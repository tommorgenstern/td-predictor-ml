{"custom_id": "scrapy#186d0e93b0614abab27c77c9e40f122ec4044f71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 11 | Churn Cumulative: 77 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,2 +1,2 @@\n-from scrapy.contrib.spiders.crawl import CrawlSpider, Rule, rule\n+from scrapy.contrib.spiders.crawl import CrawlSpider, Rule\n from scrapy.contrib.spiders.feed import XMLFeedSpider, CSVFeedSpider\n\n@@ -10,7 +10,6 @@ import copy\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n from scrapy.conf import settings\n-from scrapy.link.extractors import RegexLinkExtractor\n \n class Rule(object):\n     \"\"\"\n@@ -46,14 +45,6 @@ class Rule(object):\n         else:\n             self.follow = follow\n \n-\n-def rule(regex, callback=None, exclude=None, follow=None):\n-    \"\"\"Shorthand for creating CrawlSpider rules\"\"\"\n-    deny = [exclude] if exclude else []\n-    link_extractor = RegexLinkExtractor(allow=[regex], deny=deny) \n-    return Rule(link_extractor, callback=callback, follow=follow)\n-\n-\n class CrawlSpider(BaseSpider):\n     \"\"\"\n     Class for spiders that crawl over web pages and extract/parse their links\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#96ee661f2cb59db84419a097463a21a399917f8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 657 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from __future__ import with_statement\n import os\n import time\n import hashlib\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2852b13eade1e198c124a557ea40b96d63ad1aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 659 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -115,7 +115,7 @@ class BaseImagesPipeline(MediaPipeline):\n                 return # returning None force download\n \n             referer = request.headers.get('Referer')\n-            log.msg('Image (uptodate) type=%s at <%s> referred from <%s>' % \\\n+            log.msg('Image (uptodate): Downloaded %s from <%s> referred in <%s>' % \\\n                     (self.MEDIA_NAME, request.url, referer), level=log.DEBUG, domain=info.domain)\n             self.inc_stats(info.domain, 'uptodate')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e15c928ed20b4cd4b1a707f3760b085983fcede9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 33 | Churn Cumulative: 757 | Contributors (this commit): 4 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n-import sys\n import os\n+import sys\n import time\n import datetime\n import cPickle as pickle\n@@ -14,6 +14,7 @@ from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n \n+\n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n \n     def __init__(self, worker, domain, logfile=None, spider_settings=None):\n@@ -73,6 +74,12 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.worker.update_master(self.domain, \"running\")\n \n     def processEnded(self, status):\n+        if settings.getbool('CLUSTER_WORKER_GZIP_LOGS'):\n+            try:\n+                self.logfile = gzip_file(self.logfile)\n+            except Exception, e:\n+                log.msg(\"failed to compress %s exception=%s (domain=%s, pid=%s)\" % (self.logfile, e, self.domain, self.pid))\n+\n         if isinstance(status.value, ProcessDone):\n             st = \"done\"\n             er = \"\"\n\n@@ -2,8 +2,11 @@\n Auxiliary functions which doesn't fit anywhere else\n \"\"\"\n from __future__ import with_statement\n+from contextlib import closing\n \n+import os\n import re\n+import gzip\n import string\n import hashlib\n import csv\n@@ -202,3 +205,24 @@ def md5sum(buffer):\n     return m.hexdigest()\n \n \n+def gzip_file(logfile):\n+    \"\"\"Gzip a file in place, just like gzip unix command\n+\n+    >>> import gzip\n+    >>> import tempfile\n+    >>> logfile = tempfile.mktemp()\n+    >>> handle = open(logfile, 'wb')\n+    >>> handle.write('something to compress')\n+    >>> handle.close()\n+    >>> logfile_gz = gzip_file(logfile)\n+    >>> gzip.open(logfile_gz).read()\n+    'something to compress'\n+    \"\"\"\n+    logfile_gz = '%s.gz' % logfile\n+    with closing(gzip.open(logfile_gz, 'wb')) as f_out:\n+        with open(logfile) as f_in:\n+            f_out.writelines(f_in)\n+    os.remove(logfile)\n+    return logfile_gz\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#128de5a49bceb6fa7d2fd7c935da4283c7cdb480", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 15 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ from twisted.spread import pb\n from scrapy import log\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import load_object, gzip_file\n from scrapy.conf import settings\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ac767f84b42bf54c970d3703166642d6607900a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 254 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,6 +57,33 @@ class UrlUtilsTest(unittest.TestCase):\n                          None)\n         self.assertEqual(url_query_parameter(\"product.html?id=\", \"id\", keep_blank_values=1),\n                          '')\n+        \"\"\"\n+        This problem was seen several times in the feeds. Sometime affiliate URLs contains\n+        nested encoded affiliate URL with direct URL as parameters. For example: \n+        aff_url1 = 'http://www.tkqlhce.com/click-2590032-10294381?url=http%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FArgosCreateReferral%3FstoreId%3D10001%26langId%3D-1%26referrer%3DCOJUN%26params%3Dadref%253DGarden+and+DIY-%3EGarden+furniture-%3EChildren%26%2339%3Bs+garden+furniture%26referredURL%3Dhttp%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FProductDisplay%253FstoreId%253D10001%2526catalogId%253D1500001501%2526productId%253D1500357023%2526langId%253D-1'\n+        the typical code to extract needed URL from it is:\n+        aff_url2 = url_query_parameter(aff_url1, 'url')\n+        after this aff2_url is:\n+        'http://www.argos.co.uk/webapp/wcs/stores/servlet/ArgosCreateReferral?storeId=10001&langId=-1&referrer=COJUN&params=adref%3DGarden and DIY->Garden furniture->Children&#39;s gardenfurniture&referredURL=http://www.argos.co.uk/webapp/wcs/stores/servlet/ProductDisplay%3FstoreId%3D10001%26catalogId%3D1500001501%26productId%3D1500357023%26langId%3D-1'\n+        the direct URL extraction is\n+        url = url_query_parameter(aff_url2, 'referredURL')\n+        but this will not work, because aff_url2 contains &#39; (comma sign encoded in the feed)\n+        and the URL extraction will fail, current workaround was made in the spider, \n+        just a replace for &#39; to %27\n+        \"\"\"\n+        # correct case\n+        aff_url1 = \"http://www.anrdoezrs.net/click-2590032-10294381?url=http%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FArgosCreateReferral%3FstoreId%3D10001%26langId%3D-1%26referrer%3DCOJUN%26params%3Dadref%253DGarden+and+DIY-%3EGarden+furniture-%3EGarden+table+and+chair+sets%26referredURL%3Dhttp%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FProductDisplay%253FstoreId%253D10001%2526catalogId%253D1500001501%2526productId%253D1500357199%2526langId%253D-1\"\n+        aff_url2 = url_query_parameter(aff_url1, 'url')\n+        self.assertEqual(aff_url2, \"http://www.argos.co.uk/webapp/wcs/stores/servlet/ArgosCreateReferral?storeId=10001&langId=-1&referrer=COJUN&params=adref%3DGarden and DIY->Garden furniture->Garden table and chair sets&referredURL=http://www.argos.co.uk/webapp/wcs/stores/servlet/ProductDisplay%3FstoreId%3D10001%26catalogId%3D1500001501%26productId%3D1500357199%26langId%3D-1\")\n+        prod_url = url_query_parameter(aff_url2, 'referredURL')\n+        self.assertEqual(prod_url, \"http://www.argos.co.uk/webapp/wcs/stores/servlet/ProductDisplay?storeId=10001&catalogId=1500001501&productId=1500357199&langId=-1\")\n+        # weird case\n+        aff_url1 = \"http://www.tkqlhce.com/click-2590032-10294381?url=http%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FArgosCreateReferral%3FstoreId%3D10001%26langId%3D-1%26referrer%3DCOJUN%26params%3Dadref%253DGarden+and+DIY-%3EGarden+furniture-%3EChildren%26%2339%3Bs+garden+furniture%26referredURL%3Dhttp%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FProductDisplay%253FstoreId%253D10001%2526catalogId%253D1500001501%2526productId%253D1500357023%2526langId%253D-1\"\n+        aff_url2 = url_query_parameter(aff_url1, 'url')\n+        self.assertEqual(aff_url2, \"http://www.argos.co.uk/webapp/wcs/stores/servlet/ArgosCreateReferral?storeId=10001&langId=-1&referrer=COJUN&params=adref%3DGarden and DIY->Garden furniture->Children&#39;s garden furniture&referredURL=http://www.argos.co.uk/webapp/wcs/stores/servlet/ProductDisplay%3FstoreId%3D10001%26catalogId%3D1500001501%26productId%3D1500357023%26langId%3D-1\")\n+        prod_url = url_query_parameter(aff_url2, 'referredURL')\n+        # fails, prod_url is None now\n+        self.assertEqual(prod_url, \"http://www.argos.co.uk/webapp/wcs/stores/servlet/ProductDisplay?storeId=10001&catalogId=1500001501&productId=1500357023&langId=-1\")\n \n     def test_add_or_replace_parameter(self):\n         url = 'http://domain/test'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1683c4e9716ee0a90f178d4488caa036a4e00f37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 257 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -57,6 +57,8 @@ class UrlUtilsTest(unittest.TestCase):\n                          None)\n         self.assertEqual(url_query_parameter(\"product.html?id=\", \"id\", keep_blank_values=1),\n                          '')\n+\n+    def test_url_query_parameter_2(self):\n         \"\"\"\n         This problem was seen several times in the feeds. Sometime affiliate URLs contains\n         nested encoded affiliate URL with direct URL as parameters. For example: \n@@ -71,6 +73,7 @@ class UrlUtilsTest(unittest.TestCase):\n         and the URL extraction will fail, current workaround was made in the spider, \n         just a replace for &#39; to %27\n         \"\"\"\n+        return # FIXME: this test should pass but currently doesnt\n         # correct case\n         aff_url1 = \"http://www.anrdoezrs.net/click-2590032-10294381?url=http%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FArgosCreateReferral%3FstoreId%3D10001%26langId%3D-1%26referrer%3DCOJUN%26params%3Dadref%253DGarden+and+DIY-%3EGarden+furniture-%3EGarden+table+and+chair+sets%26referredURL%3Dhttp%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FProductDisplay%253FstoreId%253D10001%2526catalogId%253D1500001501%2526productId%253D1500357199%2526langId%253D-1\"\n         aff_url2 = url_query_parameter(aff_url1, 'url')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e7823654c1bb973e5f742f5f95bf3af38f812e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 57 | Files Changed: 2 | Hunks: 19 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/4 | Churn Δ: 110 | Churn Cumulative: 343 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,49 +1,19 @@\n-from twisted.internet import reactor\n+import os\n+import urllib\n+import urlparse\n+\n+from twisted.internet import reactor, threads\n \n-import os, urllib, urlparse\n-import scrapy\n from scrapy.command import ScrapyCommand\n from scrapy.spider import spiders\n from scrapy.xpath import XmlXPathSelector, HtmlXPathSelector\n from scrapy.utils.misc import load_object\n-from scrapy.extension import extensions\n from scrapy.conf import settings\n from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n from scrapy.http import Request\n from scrapy.fetcher import get_or_create_spider\n \n-#This code comes from twisted 8. We define here while\n-#using old twisted version.\n-def blockingCallFromThread(reactor, f, *a, **kw):\n-    \"\"\"\n-    Run a function in the reactor from a thread, and wait for the result\n-    synchronously, i.e. until the callback chain returned by the function\n-    get a result.\n-\n-    @param reactor: The L{IReactorThreads} provider which will be used to\n-        schedule the function call.\n-    @param f: the callable to run in the reactor thread\n-    @type f: any callable.\n-    @param a: the arguments to pass to C{f}.\n-    @param kw: the keyword arguments to pass to C{f}.\n-\n-    @return: the result of the callback chain.\n-    @raise: any error raised during the callback chain.\n-    \"\"\"\n-    import Queue\n-    from twisted.python import failure\n-    from twisted.internet import defer\n-    queue = Queue.Queue()\n-    def _callFromThread():\n-        result = defer.maybeDeferred(f, *a, **kw)\n-        result.addBoth(queue.put)\n-    reactor.callFromThread(_callFromThread)\n-    result = queue.get()\n-    if isinstance(result, failure.Failure):\n-        result.raiseException()\n-    return result\n-\n class Command(ScrapyCommand):\n     def syntax(self):\n         return \"[url]\"\n@@ -55,7 +25,7 @@ class Command(ScrapyCommand):\n         return \"Interactive console for scraping the given url. For scraping local files you can use a URL like file://path/to/file.html\"\n \n     def update_vars(self):\n-        \"\"\" You can use this function to update the local variables that will be available in the scrape console \"\"\"\n+        \"\"\" You can use this function to update the Scrapy objects that will be available in the shell\"\"\"\n         pass\n \n     def get_url(self, url):\n@@ -74,12 +44,11 @@ class Command(ScrapyCommand):\n             self.result = response\n             return []\n \n-        print \"Downloading URL...           \",\n+        print \"Downloading URL...\"\n         r = Request(url, callback=_get_response)\n         spider = get_or_create_spider(url)\n-        blockingCallFromThread(reactor, scrapyengine.crawl, r, spider)\n+        threads.blockingCallFromThread(reactor, scrapyengine.crawl, r, spider)\n         if self.result:\n-            print \"Done.\"\n             self.result.request = r\n             self.generate_vars(url, self.result)\n             return True\n@@ -99,17 +68,14 @@ class Command(ScrapyCommand):\n         self.print_vars()\n         \n     def print_vars(self):\n-        print '-' * 78\n-        print \"Available local variables:\"\n+        print '-' * 60\n+        print \"Available Scrapy objects:\"\n         for key, val in self.vars.iteritems():\n-            if isinstance(val, basestring):\n             print \"   %s: %s\" % (key, val)\n-            else:\n-                print \"   %s: %s\" % (key, val.__class__)\n         print \"Available commands:\"\n-        print \"   get <url>: Fetches an url and updates all variables.\"\n-        print \"   scrapehelp: Prints this help.\"\n-        print '-' * 78\n+        print \"   get <url>: Fetches a new page and updates all Scrapy objects.\"\n+        print \"   shelp: Prints this help.\"\n+        print '-' * 60\n     \n     def run(self, args, opts):\n         self.vars = {}\n@@ -118,7 +84,7 @@ class Command(ScrapyCommand):\n         if args:\n             url = args[0]\n \n-        print \"Scrapy %s - Interactive scraping console\\n\" % scrapy.__version__\n+        print \"Welcome to Scrapy shell!\"\n \n         def _console_thread():\n             \n@@ -138,7 +104,7 @@ class Command(ScrapyCommand):\n                 shell = IPython.Shell.IPShell(argv=[], user_ns=self.user_ns)\n                 ip = shell.IP.getapi()\n                 ip.expose_magic(\"get\", _get_magic)\n-                ip.expose_magic(\"scrapehelp\", _help_magic)\n+                ip.expose_magic(\"shelp\", _help_magic)\n                 shell.mainloop()\n                 reactor.callFromThread(scrapymanager.stop)\n             except ImportError:\n\n@@ -1,18 +1,13 @@\n \"\"\"\n-Monkey patches\n+Monkey patches for supporting Twisted 2.5.0\n+\"\"\"\n \n-These are generally a bad idea.\n-\"\"\"\n import twisted\n \n-\n-# Extend limit for BeautifulSoup parsing loops\n-#import sys\n-#sys.setrecursionlimit(7400)\n-\n def apply_patches():\n     if twisted.__version__ < '8.0.0':\n         patch_HTTPPageGetter_handleResponse()\n+        add_missing_blockingCallFromThread()\n \n \n # bugfix not present in twisted 2.5 for handling empty response of HEAD requests\n@@ -43,3 +38,38 @@ def patch_HTTPPageGetter_handleResponse():\n         # stupid...\n         self.transport.loseConnection()\n     setattr(HTTPPageGetter, 'handleResponse', _handleResponse)\n+\n+# This function comes bundled with Twisted 8.x and above\n+def add_missing_blockingCallFromThread():\n+    import Queue\n+    from twisted.internet import defer\n+    from twisted.python import failure\n+\n+    def blockingCallFromThread(reactor, f, *a, **kw):\n+        \"\"\"\n+        Run a function in the reactor from a thread, and wait for the result\n+        synchronously, i.e. until the callback chain returned by the function\n+        get a result.\n+\n+        @param reactor: The L{IReactorThreads} provider which will be used to\n+            schedule the function call.\n+        @param f: the callable to run in the reactor thread\n+        @type f: any callable.\n+        @param a: the arguments to pass to C{f}.\n+        @param kw: the keyword arguments to pass to C{f}.\n+\n+        @return: the result of the callback chain.\n+        @raise: any error raised during the callback chain.\n+        \"\"\"\n+        queue = Queue.Queue()\n+        def _callFromThread():\n+            result = defer.maybeDeferred(f, *a, **kw)\n+            result.addBoth(queue.put)\n+        reactor.callFromThread(_callFromThread)\n+        result = queue.get()\n+        if isinstance(result, failure.Failure):\n+            result.raiseException()\n+        return result\n+\n+    from twisted.internet import threads\n+    threads.blockingCallFromThread = blockingCallFromThread\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ef9f4961a12bd19595e75e98d0b2367396d4e38e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 236 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,9 @@\n+\"\"\"\n+XPath selectors \n+\n+See documentation in docs/ref/selectors.rst\n+\"\"\"\n+\n import libxml2\n \n from scrapy.http import TextResponse\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0fb3ee16d8c413c5084d58bbd5bbef3441cb4a53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 43 | Churn Cumulative: 225 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,9 @@\n+\"\"\"\n+Scrapy Shell\n+\n+See documentation in docs/topics/shell.rst\n+\"\"\"\n+\n import os\n import urllib\n import urlparse\n@@ -28,7 +34,15 @@ class Command(ScrapyCommand):\n         \"\"\" You can use this function to update the Scrapy objects that will be available in the shell\"\"\"\n         pass\n \n+    def _get_response(self, response):\n+        self.response = response\n+        return []\n+\n     def get_url(self, url):\n+        self.response = None\n+\n+        url = url.strip()\n+        if url:\n             u = urlparse.urlparse(url)\n             if not u.scheme:\n                 path = os.path.abspath(url).replace(os.sep, '/')\n@@ -38,22 +52,18 @@ class Command(ScrapyCommand):\n             if u.scheme not in ('http', 'https', 'file'):\n                 print \"Unsupported scheme '%s' in URL: <%s>\" % (u.scheme, url)\n                 return\n+            request = Request(url, callback=self._get_response)\n+        else:\n+            request = self.user_ns['request'].replace(callback=self._get_response)\n \n-        self.result = None\n-        def _get_response(response):\n-            self.result = response\n-            return []\n-\n-        print \"Downloading URL...\"\n-        r = Request(url, callback=_get_response)\n         spider = get_or_create_spider(url)\n-        threads.blockingCallFromThread(reactor, scrapyengine.crawl, r, spider)\n-        if self.result:\n-            self.result.request = r\n-            self.generate_vars(url, self.result)\n+        print \"Fetching %s...\" % request\n+        threads.blockingCallFromThread(reactor, scrapyengine.crawl, request, spider)\n+        if self.response:\n+            self.generate_vars(url, self.response, request)\n             return True\n \n-    def generate_vars(self, url, response):\n+    def generate_vars(self, url=None, response=None, request=None):\n         itemcls = load_object(settings['DEFAULT_ITEM_CLASS'])\n         item = itemcls()\n         self.vars['item'] = item\n@@ -62,6 +72,7 @@ class Command(ScrapyCommand):\n             self.vars['hxs'] = HtmlXPathSelector(response)\n             self.vars['url'] = url\n             self.vars['response'] = response\n+            self.vars['request'] = request\n             self.vars['spider'] = spiders.fromurl(url)\n         self.update_vars()\n         self.user_ns.update(self.vars)\n@@ -73,7 +84,7 @@ class Command(ScrapyCommand):\n         for key, val in self.vars.iteritems():\n             print \"   %s: %s\" % (key, val)\n         print \"Available commands:\"\n-        print \"   get <url>: Fetches a new page and updates all Scrapy objects.\"\n+        print \"   get [url]: Fetch a new URL or re-fetch current Request\"\n         print \"   shelp: Prints this help.\"\n         print '-' * 60\n     \n@@ -89,16 +100,16 @@ class Command(ScrapyCommand):\n         def _console_thread():\n             \n             def _get_magic(shell, arg):\n-                self.get_url(arg.strip())\n+                self.get_url(arg)\n             def _help_magic(shell, _):\n                 self.print_vars()\n                 \n             if url:\n                 result = self.get_url(url)\n                 if not result:\n-                    self.generate_vars(None, None)\n+                    self.generate_vars()\n             else:\n-                self.generate_vars(None, None)\n+                self.generate_vars()\n             try: # use IPython if available\n                 import IPython\n                 shell = IPython.Shell.IPShell(argv=[], user_ns=self.user_ns)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#324ec076788935711165ac219c73fede60867d3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 7 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d503735bf08ef673a4b327f305f723ac66fbd8cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 223 | Contributors (this commit): 3 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -96,9 +96,6 @@ EXTENSIONS = [\n     'scrapy.contrib.spider.reloader.SpiderReloader',\n     'scrapy.contrib.memusage.MemoryUsage',\n     'scrapy.contrib.memdebug.MemoryDebugger',\n-    'scrapy.contrib.pbcluster.ClusterWorker',\n-    'scrapy.contrib.pbcluster.ClusterMasterWeb',\n-    'scrapy.contrib.pbcluster.ClusterCrawler',\n     'scrapy.contrib.closedomain.CloseDomain',\n     'scrapy.contrib.debug.StackTraceDump',\n     'scrapy.contrib.response.soup.ResponseSoup',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e854af69b807318015031c22b173fa358abaf22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 34 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,16 @@\n-import unittest\n-from scrapy.utils.serialization import serialize, unserialize, serialize_funcs\n+from twisted.trial import unittest\n \n class SerializationTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        try:\n+            import simplejson\n+        except ImportError, e:\n+            raise unittest.SkipTest(e)\n+\n     def test_string_serialization(self):\n         \"\"\"Test simple string serialization\"\"\"\n+        from scrapy.utils.serialization import serialize, unserialize, serialize_funcs\n \n         s = \"string to serialize\"\n         for format in serialize_funcs.iterkeys():\n@@ -11,6 +18,7 @@ class SerializationTest(unittest.TestCase):\n \n     def test_dict_serialization(self):\n         \"\"\"Test dict serialization\"\"\"\n+        from scrapy.utils.serialization import serialize, unserialize, serialize_funcs\n \n         # using only string keys/values since it's the only data type that\n         # works with all serializers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3945c36f3023890c63cc93f3de25454f06720c2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 51 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,18 +3,8 @@ scrapy.tests: this package contains all Scrapy unittests\n \n To run all Scrapy unittests type: \n \n-    python -m scrapy.tests\n+    python -m scrapy.tests.run\n \n-Keep in mind that some tests may be skipped if you don't have a MySQL database\n-up and configured appropriately.\n+Keep in mind that some tests may be skipped if you don't have some (optional)\n+modules available like MySQLdb or simplejson.\n \"\"\"\n-\n-if __name__ == '__main__':\n-    import os\n-    from twisted.trial import runner, reporter\n-\n-    os.environ['SCRAPY_SETTINGS_DISABLED'] = '1'\n-    loader = runner.TestLoader()\n-    runner = runner.TrialRunner(reporter.TreeReporter)\n-    suite = loader.loadByNames(['scrapy'], recurse=True)\n-    runner.run(suite)\n\n@@ -0,0 +1,13 @@\n+\"\"\"\n+Module to run Scrapy tests - see scrapy.tests docstring\n+\"\"\"\n+\n+if __name__ == '__main__':\n+    import os\n+    from twisted.trial import runner, reporter\n+\n+    os.environ['SCRAPY_SETTINGS_DISABLED'] = '1'\n+    loader = runner.TestLoader()\n+    runner = runner.TrialRunner(reporter.TreeReporter)\n+    suite = loader.loadByNames(['scrapy'], recurse=True)\n+    runner.run(suite)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c005f8e7580adc682554fe8365e441498dbeb3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 70 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,13 @@\n-import unittest\n-from scrapy.contrib.pipeline.images import BaseImagesPipeline\n+from twisted.trial import unittest\n \n class ImagesPipelineTestCase(unittest.TestCase):\n     def setUp(self):\n+        try:\n+            import Image\n+        except ImportError, e:\n+            raise unittest.SkipTest(e)\n+\n+        from scrapy.contrib.pipeline.images import BaseImagesPipeline\n         self.pipeline = BaseImagesPipeline()\n \n     def tearDown(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#74f39480c6aa5dcfb476205d348d335e6866e8cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 19 | Churn Cumulative: 703 | Contributors (this commit): 2 | Commits (past 90d): 22 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,11 +39,11 @@ class FormRequest(Request):\n         forms = ParseFile(StringIO(response.body), response.url,\n                           encoding=encoding, backwards_compat=False)\n         if not forms:\n-            raise ValueError(\"No form control found in %s\" % response)\n+            raise ValueError(\"No <form> element found in %s\" % response)\n         try:\n             form = forms[formnumber]\n         except IndexError:\n-            raise IndexError(\"No such form number: %d\" % formnumber)\n+            raise IndexError(\"Form number %d not found in %s\" % (formnumber, response))\n         if formdata:\n             for k, v in formdata.iteritems():\n                 for v2 in v if hasattr(v, '__iter__') else [v]:\n\n@@ -234,6 +234,21 @@ class FormRequestTest(unittest.TestCase):\n         self.assertEqual(urlargs['test2'], ['xxx'])\n         self.assertEqual(urlargs['six'], ['seven'])\n \n+    def test_from_response_errors_noform(self):\n+        respbody = \"\"\"<html></html>\"\"\"\n+        response = Response(\"http://www.example.com/lala.html\", body=respbody)\n+        self.assertRaises(ValueError, FormRequest.from_response, response)\n+\n+    def test_from_response_errors_formnumber(self):\n+        respbody = \"\"\"\n+<form action=\"get.php\" method=\"GET\">\n+<input type=\"hidden\" name=\"test\" value=\"val1\">\n+<input type=\"hidden\" name=\"test\" value=\"val2\">\n+<input type=\"hidden\" name=\"test2\" value=\"xxx\">\n+</form>\n+        \"\"\"\n+        response = Response(\"http://www.example.com/lala.html\", body=respbody)\n+        self.assertRaises(IndexError, FormRequest.from_response, response, formnumber=1)\n \n class XmlRpcRequestTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b96a0c275689b2b38b3103c12b7245588fbc6ede", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1003 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -226,7 +226,7 @@ class ExecutionEngine(object):\n     def open_domains(self):\n         return self.downloader.sites.keys()\n \n-    def crawl(self, request, spider, priority=1, domain_priority=1):\n+    def crawl(self, request, spider, priority=0, domain_priority=0):\n         domain = spider.domain_name\n \n         def _process(response):\n@@ -290,7 +290,7 @@ class ExecutionEngine(object):\n     def scrape(self, request, response, spider):\n         return self.spidermiddleware.scrape(request, response, spider)\n \n-    def schedule(self, request, spider, priority=1, domain_priority=1):\n+    def schedule(self, request, spider, priority=0, domain_priority=0):\n         domain = spider.domain_name\n         if not self.scheduler.domain_is_open(domain):\n             if self.debug_mode: \n@@ -356,7 +356,7 @@ class ExecutionEngine(object):\n                 return response\n             elif isinstance(response, Request):\n                 redirected = response # proper alias\n-                schd = self.schedule(redirected, spider, priority=4)\n+                schd = self.schedule(redirected, spider, priority=-1)\n                 chain_deferred(schd, redirected.deferred)\n                 return schd\n \n\n@@ -79,7 +79,7 @@ class Scheduler(object) :\n             return domain\n         return None\n \n-    def add_domain(self, domain, priority=1):\n+    def add_domain(self, domain, priority=0):\n         \"\"\"This functions schedules a new domain to be scraped, with the given priority.\n \n         It doesn't check if the domain is already scheduled.\n@@ -99,7 +99,7 @@ class Scheduler(object) :\n         Priority = PriorityStack if self.dfo else PriorityQueue\n         self.pending_requests[domain] = Priority()\n \n-    def enqueue_request(self, domain, request, priority=1):\n+    def enqueue_request(self, domain, request, priority=0):\n         \"\"\"Enqueue a request to be downloaded for a domain that is currently being scraped.\"\"\"\n         dfd = defer.Deferred()\n         self.pending_requests[domain].push((request, dfd), priority)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fae3d7a8dc000927fc305c3bcdc5a1b5912e760e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1309 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -18,6 +18,7 @@ from scrapy.core.exceptions import IgnoreRequest\n from scrapy.conf import settings\n \n class RobotsTxtMiddleware(object):\n+    DOWNLOAD_PRIORITY = -1\n \n     def __init__(self):\n         if not settings.getbool('ROBOTSTXT_OBEY'):\n@@ -44,7 +45,7 @@ class RobotsTxtMiddleware(object):\n             self._parsers[urldomain] = None\n             robotsurl = \"%s://%s/robots.txt\" % parsedurl[0:2]\n             robotsreq = Request(robotsurl)\n-            dfd = scrapyengine.schedule(robotsreq, spiders.fromdomain(spiderdomain), priority=0)\n+            dfd = scrapyengine.schedule(robotsreq, spiders.fromdomain(spiderdomain), priority=self.DOWNLOAD_PRIORITY)\n             dfd.addCallbacks(callback=self._parse_robots, callbackArgs=[urldomain])\n             self._spiderdomains[spiderdomain].add(urldomain)\n \n\n@@ -10,6 +10,7 @@ from scrapy.spider import spiders\n \n \n class MediaPipeline(object):\n+    DOWNLOAD_PRIORITY = -1\n \n     class DomainInfo(object):\n         def __init__(self, domain):\n@@ -121,7 +122,7 @@ class MediaPipeline(object):\n         request fingerprint is used as cache key.\n \n         \"\"\"\n-        return scrapyengine.schedule(request, info.spider, priority=0)\n+        return scrapyengine.schedule(request, info.spider, priority=self.DOWNLOAD_PRIORITY)\n \n     def media_to_download(self, request, info):\n         \"\"\" Ongoing request hook pre-cache\n\n@@ -42,6 +42,10 @@ class ExecutionEngine(object):\n     process for that domain.\n     \"\"\"\n \n+    # Scheduler priority of redirected requests. Negative means high priority.\n+    # We use high priority to avoid hogging memory with pending redirected requests\n+    REDIRECTION_PRIORITY = -10\n+\n     def __init__(self):\n         self.configured = False\n         self.keep_alive = False\n@@ -356,7 +360,7 @@ class ExecutionEngine(object):\n                 return response\n             elif isinstance(response, Request):\n                 redirected = response # proper alias\n-                schd = self.schedule(redirected, spider, priority=-1)\n+                schd = self.schedule(redirected, spider, priority=self.REDIRECTION_PRIORITY)\n                 chain_deferred(schd, redirected.deferred)\n                 return schd\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#821f6be3ce7a57bd1e6b29dd6394593d2a0d25be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 32 | Churn Cumulative: 699 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,8 +13,10 @@ from scrapy.http.request import Request\n class XmlRpcRequest(Request):\n \n     def __init__(self, *args, **kwargs):\n+        if 'body' not in kwargs:\n             params = kwargs.pop('params')\n             methodname = kwargs.pop('methodname')\n+            kwargs['body'] = xmlrpclib.dumps(params, methodname)\n \n         # spec defines that requests must use POST method\n         kwargs.setdefault('method', 'POST')\n@@ -23,5 +25,4 @@ class XmlRpcRequest(Request):\n         kwargs.setdefault('dont_filter', True)\n \n         Request.__init__(self, *args, **kwargs)\n-        self.body = xmlrpclib.dumps(params, methodname)\n         self.headers.setdefault('Content-Type', 'text/xml')\n\n@@ -259,6 +259,35 @@ class XmlRpcRequestTest(unittest.TestCase):\n         self.assertEqual(r.method, 'POST')\n         self.assertTrue(r.dont_filter, True)\n \n+    def test_copy(self):\n+        \"\"\"Test XmlRpcRequest copy\"\"\"\n+\n+        def somecallback():\n+            pass\n+\n+        r1 = XmlRpcRequest(\"http://www.example.com\", callback=somecallback,\n+                methodname='login', params=('username', 'password'))\n+        r1.meta['foo'] = 'bar'\n+        r1.cache['lala'] = 'lolo'\n+        r2 = r1.copy()\n+\n+        assert r1.cache\n+        assert not r2.cache\n+\n+        assert r1.deferred is not r2.deferred\n+\n+        # make sure meta dict is shallow copied\n+        assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n+        self.assertEqual(r1.meta, r2.meta)\n+\n+        # make sure headers attribute is shallow copied\n+        assert r1.headers is not r2.headers, \"headers must be a shallow copy, not identical\"\n+        self.assertEqual(r1.headers, r2.headers)\n+        self.assertEqual(r1.encoding, r2.encoding)\n+        self.assertEqual(r1.dont_filter, r2.dont_filter)\n+        self.assertEqual(r1.body, r2.body)\n+\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6c52d51ed2b8118a2d68dd56f035793facd6895", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 858 | Files Changed: 23 | Hunks: 11 | Methods Changed: 61 | Complexity Δ (Sum/Max): -30/91 | Churn Δ: 863 | Churn Cumulative: 3471 | Contributors (this commit): 3 | Commits (past 90d): 33 | Contributors (cumulative): 47 | DMM Complexity: 0.47955974842767296\n\nDIFF:\n@@ -0,0 +1,3 @@\n+from scrapy.contrib.cluster.worker.manager import ClusterWorker\n+from scrapy.contrib.cluster.master.web import ClusterMasterWeb\n+from scrapy.contrib.cluster.crawler.manager import ClusterCrawler\n\n\n\n\n\n\n@@ -11,7 +11,7 @@ from scrapy.core import signals\n from scrapy import log\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.contrib_exp.cluster.worker.manager import ResponseCode\n+from scrapy.contrib.cluster.worker.manager import ResponseCode\n from scrapy.conf import settings\n \n def my_import(name):\n\n@@ -4,7 +4,7 @@ from pydispatch import dispatcher\n \n from scrapy.spider import spiders \n from scrapy.management.web import banner, webconsole_discover_module\n-from scrapy.contrib_exp.cluster.master.manager import ClusterMaster\n+from scrapy.contrib.cluster.master.manager import ClusterMaster\n from scrapy.utils.serialization import serialize\n \n class ClusterMasterWeb(ClusterMaster):\n\n\n\n\n\n@@ -1,3 +0,0 @@\n-from scrapy.contrib.pbcluster.worker.manager import ClusterWorker\n-from scrapy.contrib.pbcluster.master.web import ClusterMasterWeb\n-from scrapy.contrib.pbcluster.crawler.manager import ClusterCrawler\n\\ No newline at end of file\n\n@@ -1,38 +0,0 @@\n-import os\n-\n-from twisted.spread import pb\n-from twisted.internet import reactor\n-\n-from scrapy.conf import settings\n-from scrapy import log\n-from scrapy.core.manager import scrapymanager\n-from scrapy.core.exceptions import NotConfigured\n-\n-class Broker(pb.Referenceable):\n-    def __init__(self, crawler, remote):\n-        self.__remote = remote\n-        self.__crawler = crawler\n-        try:\n-            deferred = self.__remote.callRemote(\"register_crawler\", os.getpid(), self)\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n-        else:\n-            deferred.addCallbacks(callback=lambda x: None, errback=lambda reason: log.msg(reason, log.ERROR))\n-    def remote_stop(self):\n-        scrapymanager.stop()\n-    \n-class ClusterCrawler:\n-    def __init__(self):\n-        if not settings.getbool('CLUSTER_CRAWLER_ENABLED'):\n-            raise NotConfigured\n-\n-        self.worker = None\n-\n-        factory = pb.PBClientFactory()\n-        reactor.connectTCP(\"localhost\", settings.getint('CLUSTER_WORKER_PORT'), factory)\n-        d = factory.getRootObject()\n-        def _set_worker(obj):\n-            self.worker = Broker(self, obj)\n-        d.addCallbacks(callback=_set_worker, errback=lambda reason: log.msg(reason, log.ERROR))\n-        \n\n@@ -1,330 +0,0 @@\n-import sys, datetime\n-import pickle\n-\n-from pydispatch import dispatcher\n-\n-from twisted.spread import pb\n-from twisted.internet import reactor\n-\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-\n-DEFAULT_PRIORITY = settings.getint(\"DEFAULT_PRIORITY\", 20)\n-\n-def my_import(name):\n-    mod = __import__(name)\n-    components = name.split('.')\n-    for comp in components[1:]:\n-        mod = getattr(mod, comp)\n-    return mod\n-\n-class Broker(pb.Referenceable):\n-    def __init__(self, remote, name, master):\n-        self.__remote = remote\n-        self.alive = False\n-        self.name = name\n-        self.master = master\n-        self.available = True\n-        try:\n-            deferred = self.__remote.callRemote(\"set_master\", self)\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n-        else:\n-            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n-            \n-    def status_as_dict(self, verbosity=1):\n-        if verbosity == 0:\n-            return\n-        status = {\"alive\": self.alive}\n-        if self.alive:\n-            if verbosity == 1:\n-                #dont show spider settings\n-                status[\"running\"] = []\n-                for proc in self.running:\n-                    proccopy = proc.copy()\n-                    del proccopy[\"settings\"]\n-                    status[\"running\"].append(proccopy)\n-            elif verbosity == 2:\n-                status[\"running\"] = self.running\n-            status[\"maxproc\"] = self.maxproc\n-            status[\"freeslots\"] = self.maxproc - len(self.running)\n-            status[\"available\"] = self.available\n-            status[\"starttime\"] = self.starttime\n-            status[\"timestamp\"] = self.timestamp\n-            status[\"loadavg\"] = self.loadavg\n-        return status\n-        \n-    def _set_status(self, status):\n-        if not status:\n-            self.alive = False\n-        else:\n-            self.alive = True\n-            self.running = status['running']\n-            self.maxproc = status['maxproc']\n-            self.starttime = status['starttime']\n-            self.timestamp = status['timestamp']\n-            self.loadavg = status['loadavg']\n-            self.logdir = status['logdir']\n-            free_slots = self.maxproc - len(self.running)\n-\n-            #load domains by one, so to mix up better the domain loading between nodes. The next one in the same node will be loaded\n-            #when there is no loading domain or in the next status update. This way also we load the nodes softly\n-            if self.available and free_slots > 0 and self.master.pending:\n-                pending = self.master.pending.pop(0)\n-                #if domain already running in some node, reschedule with same priority (so will be moved to run later)\n-                if pending['domain'] in self.master.running or pending['domain'] in self.master.loading:\n-                    self.master.schedule([pending['domain']], pending['settings'], pending['priority'])\n-                else:\n-                    self.run(pending)\n-                    self.master.loading.append(pending['domain'])\n-\n-    def update_status(self):\n-        try:\n-            deferred = self.__remote.callRemote(\"status\")\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n-        else:\n-            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n-\n-    def stop(self, domain):\n-        try:\n-            deferred = self.__remote.callRemote(\"stop\", domain)\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n-        else:\n-            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n-\n-    def run(self, pending):\n-\n-        def _run_errback(reason):\n-            log.msg(reason, log.ERROR)\n-            self.master.loading.remove(pending['domain'])\n-            self.master.schedule([pending['domain']], pending['settings'], pending['priority'] - 1)\n-            log.msg(\"Domain %s rescheduled: lost connection to node.\" % pending['domain'], log.WARNING)\n-            \n-        def _run_callback(status):\n-            if status['callresponse'][0] == 1:\n-                #slots are complete. Reschedule in master with priority reduced by one.\n-                #self.master.loading check should avoid this to happen\n-                self.master.loading.remove(pending['domain'])\n-                self.master.schedule([pending['domain']], pending['settings'], pending['priority'] - 1)\n-                log.msg(\"Domain %s rescheduled: no proc space in node.\" % pending['domain'], log.WARNING)\n-            elif status['callresponse'][0] == 2:\n-                #domain already running in node. Reschedule with same priority.\n-                #self.master.loading check should avoid this to happen\n-                self.master.loading.remove(pending['domain'])\n-                self.master.schedule([pending['domain']], pending['settings'], pending['priority'])\n-                log.msg(\"Domain %s rescheduled: already running in node.\" % pending['domain'], log.WARNING)\n-\n-        try:\n-            deferred = self.__remote.callRemote(\"run\", pending[\"domain\"], pending[\"settings\"])\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n-        else:\n-            deferred.addCallbacks(callback=_run_callback, errback=_run_errback)\n-        \n-    def remote_update(self, status, domain, domain_status):\n-        self._set_status(status)\n-        if domain in self.master.loading and domain_status == \"running\":\n-            self.master.loading.remove(domain)\n-            self.master.statistics[\"domains\"][\"running\"].add(domain)\n-        elif domain_status == \"scraped\":\n-            self.master.statistics[\"domains\"][\"running\"].remove(domain)\n-            self.master.statistics[\"domains\"][\"scraped\"][domain] = self.master.statistics[\"domains\"][\"scraped\"].get(domain, 0) + 1\n-            self.master.statistics[\"scraped_count\"] = self.master.statistics.get(\"scraped_count\", 0) + 1\n-            if domain in self.master.statistics[\"domains\"][\"lost\"]:\n-                self.master.statistics[\"domains\"][\"lost\"].remove(domain)\n-\n-class ScrapyPBClientFactory(pb.PBClientFactory):\n-    def __init__(self, master, nodename):\n-        pb.PBClientFactory.__init__(self)\n-        self.master = master\n-        self.nodename = nodename\n-        \n-    def clientConnectionLost(self, *args, **kargs):\n-        pb.PBClientFactory.clientConnectionLost(self, *args, **kargs)\n-        del self.master.nodes[self.nodename]\n-        log.msg(\"Removed node %s.\" % self.nodename )\n-\n-class ClusterMaster:\n-\n-    def __init__(self):\n-\n-        if not (settings.getbool('CLUSTER_MASTER_ENABLED')):\n-            raise NotConfigured\n-\n-        #import groups settings\n-        if settings.getbool('GROUPSETTINGS_ENABLED'):\n-            self.get_spider_groupsettings = my_import(settings[\"GROUPSETTINGS_MODULE\"]).get_spider_groupsettings\n-        else:\n-            self.get_spider_groupsettings = lambda x: {}\n-        #load pending domains\n-        try:\n-            self.pending = pickle.load( open(settings[\"CLUSTER_MASTER_CACHEFILE\"], \"r\") )\n-        except IOError:\n-            self.pending = []\n-        self.loading = []\n-        self.nodes = {}\n-        self.start_time = datetime.datetime.utcnow()\n-        #on how statistics works, see self.update_nodes() and Broker.remote_update()\n-        self.statistics = {\"domains\": {\"running\": set(), \"scraped\": {}, \"lost_count\": {}, \"lost\": set()}, \"scraped_count\": 0 }\n-        self.global_settings = {}\n-        #load cluster global settings\n-        for sname in settings.getlist('GLOBAL_CLUSTER_SETTINGS'):\n-            self.global_settings[sname] = settings[sname]\n-        \n-        dispatcher.connect(self._engine_started, signal=signals.engine_started)\n-        dispatcher.connect(self._engine_stopped, signal=signals.engine_stopped)\n-        \n-    def load_nodes(self):\n-        \"\"\"Loads nodes listed in CLUSTER_MASTER_NODES setting\"\"\"\n-        for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n-            self.load_node(name, url)\n-            \n-    def load_node(self, name, url):\n-        \"\"\"Creates the remote reference for each worker node\"\"\"\n-        def _make_callback(_factory, _name, _url):\n-\n-            def _errback(_reason):\n-                log.msg(\"Could not get remote node %s in %s: %s.\" % (_name, _url, _reason), log.ERROR)\n-\n-            d = _factory.getRootObject()\n-            d.addCallbacks(callback=lambda obj: self.add_node(obj, _name), errback=_errback)\n-\n-        server, port = url.split(\":\")\n-        port = int(port)\n-        log.msg(\"Connecting to cluster worker %s...\" % name)\n-        log.msg(\"Server: %s, Port: %s\" % (server, port))\n-        factory = ScrapyPBClientFactory(self, name)\n-        try:\n-            reactor.connectTCP(server, port, factory)\n-        except Exception, err:\n-            log.msg(\"Could not connect to node %s in %s: %s.\" % (name, url, reason), log.ERROR)\n-        else:\n-            _make_callback(factory, name, url)\n-\n-    def update_nodes(self):\n-        for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n-            if name in self.nodes and self.nodes[name].alive:\n-                log.msg(\"Updating node. name: %s, url: %s\" % (name, url) )\n-                self.nodes[name].update_status()\n-            else:\n-                log.msg(\"Reloading node. name: %s, url: %s\" % (name, url) )\n-                self.load_node(name, url)\n-        \n-        real_running = set(self.running.keys())\n-        lost = self.statistics[\"domains\"][\"running\"].difference(real_running)\n-        for domain in lost:\n-            self.statistics[\"domains\"][\"lost_count\"][domain] = self.statistics[\"domains\"][\"lost_count\"].get(domain, 0) + 1\n-        self.statistics[\"domains\"][\"lost\"] = self.statistics[\"domains\"][\"lost\"].union(lost)\n-            \n-    def add_node(self, cworker, name):\n-        \"\"\"Add node given its node\"\"\"\n-        node = Broker(cworker, name, self)\n-        self.nodes[name] = node\n-        log.msg(\"Added cluster worker %s\" % name)\n-\n-    def disable_node(self, name):\n-        self.nodes[name].available = False\n-        \n-    def enable_node(self, name):\n-        self.nodes[name].available = True\n-\n-    def remove_node(self, nodename):\n-        raise NotImplemented\n-\n-    def schedule(self, domains, spider_settings=None, priority=DEFAULT_PRIORITY):\n-        i = 0\n-        for p in self.pending:\n-            if p['priority'] <= priority:\n-                i += 1\n-            else:\n-                break\n-        for domain in domains:\n-            pd = self.find_inpending(domain)\n-            if pd: #domain already pending, so just change priority if new is higher\n-                if priority < pd['priority']:\n-                    self.pending.remove(pd)\n-                    pd['priority'] = priority\n-                    self.pending.insert(i, pd)\n-            else:\n-                final_spider_settings = self.get_spider_groupsettings(domain)\n-                final_spider_settings.update(self.global_settings)\n-                final_spider_settings.update(spider_settings or {})\n-                self.pending.insert(i, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})\n-\n-    def stop(self, domains):\n-        to_stop = {}\n-        for domain in domains:\n-            node = self.running.get(domain, None)\n-            if node:\n-                if node.name not in to_stop:\n-                    to_stop[node.name] = []\n-                to_stop[node.name].append(domain)\n-\n-        for nodename, domains in to_stop.iteritems():\n-            for domain in domains:\n-                self.nodes[nodename].stop(domain)\n-\n-    def remove(self, domains):\n-        \"\"\"Remove all scheduled instances of the given domains (if it hasn't\n-        started yet). Otherwise use stop()\"\"\"\n-\n-        for domain in domains:\n-            to_remove = []\n-            for p in self.pending:\n-                if p['domain'] == domain:\n-                    to_remove.append(p)\n-    \n-            for p in to_remove:\n-                self.pending.remove(p)\n-\n-    def discard(self, domains):\n-        \"\"\"Stop and remove all running and pending instances of the given\n-        domains\"\"\"\n-        self.remove(domains)\n-        self.stop(domains)\n-\n-    @property\n-    def running(self):\n-        \"\"\"Return dict of running domains as domain -> node\"\"\"\n-        d = {}\n-        for node in self.nodes.itervalues():\n-            for proc in node.running:\n-                d[proc['domain']] = node\n-        return d\n-\n-    @property\n-    def available_nodes(self):\n-        return (node for node in self.nodes.itervalues() if node.available)\n-\n-    def find_inpending(self, domain):\n-        for p in self.pending:\n-            if domain == p['domain']:\n-                return p\n-\n-    def print_pending(self, verbosity=1):\n-        if verbosity == 1:\n-            pending = []\n-            for p in self.pending:\n-                pp = p.copy()\n-                del pp[\"settings\"]\n-                pending.append(pp)\n-            return pending\n-        elif verbosity == 2:\n-            return self.pending\n-        return\n-\n-    def _engine_started(self):\n-        self.load_nodes()\n-        scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL'))\n-    def _engine_stopped(self):\n-        pickle.dump( self.pending, open(settings[\"CLUSTER_MASTER_CACHEFILE\"], \"w\") )\n-        log.msg(\"Pending saved in %s\" % settings[\"CLUSTER_MASTER_CACHEFILE\"])\n\n@@ -1,239 +0,0 @@\n-import datetime\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.spider import spiders \n-from scrapy.management.web import banner, webconsole_discover_module\n-from scrapy.contrib.pbcluster.master.manager import ClusterMaster, DEFAULT_PRIORITY\n-from scrapy.utils.serialization import serialize\n-\n-class ClusterMasterWeb(ClusterMaster):\n-    webconsole_id = 'cluster_master'\n-    webconsole_name = 'Cluster master'\n-\n-    def __init__(self):\n-        ClusterMaster.__init__(self)\n-\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_render(self, wc_request):\n-        changes = \"\"\n-        if wc_request.path == '/cluster_master/nodes/':\n-            return self.render_nodes(wc_request)\n-        elif wc_request.path == '/cluster_master/domains/':\n-            return self.render_domains(wc_request)\n-        elif wc_request.path == '/cluster_master/ws/':\n-            return self.webconsole_control(wc_request, ws=True)\n-        elif wc_request.args:\n-            changes = self.webconsole_control(wc_request)\n-\n-        s = self.render_header()\n-\n-        s += \"<h2>Home</h2>\\n\"\n-\n-        s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>&nbsp;</th><th>Name</th><th>Available</th><th>Running</th><th>Load.avg</th></tr>\\n\"\n-        for node in self.nodes.itervalues():\n-            #chkbox = \"<input type='checkbox' name='shutdown' value='%s' />\" % domain if node.status in [\"up\", \"idle\"] else \"&nbsp;\"\n-            nodelink = \"<a href='nodes/#%s'>%s</a>\" % (node.name, node.name)\n-            chkbox = \"&nbsp;\"\n-            loadavg = \"%.2f %.2f %.2f\" % node.loadavg\n-            s += \"<tr><td>%s</td><td>%s</td><td>%s</td><td>%d/%d</td><td>%s</td></tr>\\n\" % \\\n-                 (chkbox, nodelink, node.available, len(node.running), node.maxproc, loadavg)\n-        s += \"</table>\\n\"\n-\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return str(s)\n-\n-    def webconsole_control(self, wc_request, ws=False):\n-        args = wc_request.args\n-        if \"updatenodes\" in args:\n-            self.update_nodes()\n-            if ws:\n-                return self.ws_status(wc_request)\n-\n-        if \"schedule\" in args:\n-            if ws:\n-                sep = \",\"\n-                domains = args[\"schedule\"][0].split(sep)\n-            else:\n-                sep = \"\\r\"\n-                domains = args[\"schedule\"]\n-            priority = int(args.get(\"priority\", [DEFAULT_PRIORITY])[0])\n-            \n-            #spider settings\n-            slist = args.get(\"settings\", [\"\"])[0].split(sep)\n-            spider_settings = {}\n-            for s in slist:\n-                try:\n-                    k, v = s.strip().split(\"=\")\n-                except ValueError:\n-                    pass\n-                else:\n-                    spider_settings[k] = v\n-\n-            self.schedule(domains, spider_settings, priority)\n-            if ws:\n-                return self.ws_status(wc_request, verbosity=0)\n-\n-        if \"stop\" in args:\n-            if ws:\n-                domains = args[\"stop\"][0].split(\",\")\n-            else:\n-                domains=args[\"stop\"]\n-            self.stop(domains)\n-            if ws:\n-                return self.ws_status(wc_request)\n-\n-        if \"remove\" in args:\n-            if ws:\n-                domains = args[\"remove\"][0].split(\",\")\n-            else:\n-                domains=args[\"remove\"]\n-            self.remove(domains)\n-            if ws:\n-                return self.ws_status(wc_request)\n-        if \"disable_node\" in args:\n-            self.disable_node(args[\"disable_node\"][0])\n-            if ws:\n-                return self.ws_status(wc_request)\n-        if \"enable_node\" in args:\n-            self.enable_node(args[\"enable_node\"][0])\n-            if ws:\n-                return self.ws_status(wc_request)\n-        if \"statistics\" in args:\n-            if ws:\n-                return self.ws_statistics(wc_request)\n-            \n-        if ws:\n-            return self.ws_status(wc_request)\n-        else:\n-            return \"\"\n-\n-    def render_nodes(self, wc_request):\n-        if wc_request.args:\n-            self.webconsole_control(wc_request)\n-\n-        now = datetime.datetime.utcnow()\n-\n-        s = self.render_header()\n-        for node in self.nodes.itervalues():\n-            if node.available:\n-                s += \"<h2><a name='%s'>%s</h2>\\n\" % (node.name, node.name)\n-        \n-                s += \"<h3>Running domains</h3>\\n\"\n-                if node.running:\n-                    s += \"<form method='post' action='.'>\\n\"\n-                    s += \"<table border='1'>\\n\"\n-                    s += \"<tr><th>&nbsp;</th><th>PID</th><th>Domain</th><th>Status</th><th>Running time</th><th>Log file</th></tr>\\n\"\n-                    for proc in node.running:\n-                        chkbox = \"<input type='checkbox' name='stop' value='%s' />\" % proc['domain'] if proc['status'] == \"running\" else \"&nbsp;\"\n-                        start_time = proc.get('starttime', None)\n-                        elapsed = now - start_time if start_time else None\n-                        s += \"<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % \\\n-                                (chkbox, proc['pid'], proc['domain'], proc['status'], elapsed, proc['logfile'])\n-                    s += \"</table>\\n\"\n-                    s += \"<input type='hidden' name='node' value='%s'>\\n\" % node.name\n-                    s += \"<p><input type='submit' value='Stop selected domains on %s'></p>\\n\" % node.name\n-                    s += \"</form>\\n\"\n-                else:\n-                    s += \"<p>No running domains on %s</p>\\n\" % node.name\n-\n-        return str(s)\n-\n-    def render_domains(self, wc_request):\n-        if wc_request.args:\n-            self.webconsole_control(wc_request)\n-\n-        enabled_domains = set(spiders.asdict(include_disabled=False).keys())\n-        print \"Enabled domains: %s\" % len(enabled_domains)\n-        inactive_domains = enabled_domains - set(self.running.keys() + [p['domain'] for p in self.pending])\n-\n-        s = self.render_header()\n-\n-        s += \"<h2>Schedule domains</h2>\\n\"\n-\n-        s += \"Inactive domains (not running or pending)<br />\"\n-        s += \"<form method='post' action='.'>\\n\"\n-        s += \"<select name='schedule' multiple='multiple' size='10'>\\n\"\n-        for domain in sorted(inactive_domains):\n-            s += \"<option>%s</option>\\n\" % domain\n-        s += \"</select>\\n\"\n-        s += \"<br />\\n\"\n-        \n-        s += \"Priority:<br />\\n\"\n-        s += \"<input type='text' name='priority'>%s</input>\" % DEFAULT_PRIORITY\n-        s += \"<br />\\n\"\n-        \n-        #spider settings\n-        s += \"Overrided spider settings:<br />\\n\"\n-        s += \"<textarea name='settings' rows='4'>\\n\"\n-        s += \"UNAVAILABLES_NOTIFY=2\\n\"\n-        s += \"</textarea>\\n\"\n-        s += \"<br />\\n\"\n-        \n-        s += \"<p><input type='submit' value='Schedule selected domains'></p>\\n\"\n-        s += \"</form>\\n\"\n-\n-        s += \"<h2>Domains</h2>\\n\"\n-\n-        s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>Domain</th><th>Status</th><th>Node</th></tr>\\n\"\n-        s += self._domains_table(self.running, '<b>running</b>')\n-        s += \"</table>\\n\"\n-\n-        # pending domains\n-        s += \"<h3>Pending domains</h3>\\n\"\n-        if self.pending:\n-            s += \"<form method='post' action='.'>\\n\"\n-            s += \"<select name='remove' multiple='multiple' size='10'>\\n\"\n-            for p in self.pending:\n-                s += \"<option value='%s'>%s (P:%s)</option>\\n\" % (p['domain'], p['domain'],p['priority'])\n-            s += \"</select>\\n\"\n-            s += \"<p><input type='submit' value='Remove selected pending domains'></p>\\n\"\n-            s += \"</form>\\n\"\n-        else:\n-            s += \"<p>No pending domains</p>\\n\"\n-\n-        return str(s)\n-\n-    def render_header(self):\n-        s = banner(self)\n-        s += \"<p>Nav: \"\n-        s += \"<a href='/cluster_master/'>Home</a> | \"\n-        s += \"<a href='/cluster_master/domains/'>Domains</a> | \"\n-        s += \"<a href='/cluster_master/nodes/'>Nodes</a> (<a href='/cluster_master/nodes/?updatenodes=1'>update</a>)\"\n-        s += \"</p>\"\n-        return s\n-\n-    def _domains_table(self, dict_, status):\n-        s = \"\"\n-        for domain, node in dict_.iteritems():\n-            s += \"<tr><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % (domain, status, node.name)\n-        return s\n-\n-    def webconsole_discover_module(self):\n-        return self\n-\n-    def ws_status(self, wc_request, verbosity=1):\n-        format = wc_request.args['format'][0] if 'format' in wc_request.args else 'json'\n-        verbosity = int(wc_request.args['verbosity'][0]) if 'verbosity' in wc_request.args else verbosity\n-        wc_request.setHeader('content-type', 'text/plain')\n-        status = {}\n-        nodes_status = {}\n-        if verbosity > 0:\n-            for d, n in self.nodes.iteritems():\n-                nodes_status[d] = n.status_as_dict(verbosity)\n-            status[\"nodes\"] = nodes_status\n-            status[\"pending\"] = self.print_pending(verbosity)\n-            status[\"loading\"] = self.loading\n-            content = serialize(status, format)\n-            return content\n-        return \"\"\n-\n-    def ws_statistics(self, wc_request):\n-        format = wc_request.args['format'][0] if 'format' in wc_request.args else 'json'\n-        content = serialize(self.statistics, format)\n-        return content\n\\ No newline at end of file\n\n@@ -1,138 +0,0 @@\n-import sys, os, time, datetime, pickle, gzip\n-\n-from twisted.internet import protocol, reactor\n-from twisted.spread import pb\n-\n-from scrapy import log\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-from scrapy.core.engine import scrapyengine\n-\n-class ScrapyProcessProtocol(protocol.ProcessProtocol):\n-    def __init__(self, procman, domain, logfile=None, spider_settings=None):\n-        self.procman = procman\n-        self.domain = domain\n-        self.logfile = logfile\n-        self.start_time = datetime.datetime.utcnow()\n-        self.status = \"starting\"\n-        self.pid = -1\n-        self.env = {}\n-        #We conserve original setting format for info purposes (avoid lots of unnecesary \"SCRAPY_\")\n-        self.scrapy_settings = spider_settings or {}\n-        self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': 0, 'CLUSTER_CRAWLER_ENABLED': 1, 'WEBCONSOLE_ENABLED': 0})\n-        pickled_settings = pickle.dumps(self.scrapy_settings)\n-        self.env[\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\"] = pickled_settings\n-        self.env[\"PYTHONPATH\"] = \":\".join(sys.path)#this is need so this crawl process knows where to locate local_scrapy_settings.\n-\n-    def __str__(self):\n-        return \"<ScrapyProcess domain=%s, pid=%s, status=%s>\" % (self.domain, self.pid, self.status)\n-\n-    def as_dict(self):\n-        return {\"domain\": self.domain, \"pid\": self.pid, \"status\": self.status, \"settings\": self.scrapy_settings, \"logfile\": self.logfile, \"starttime\": self.start_time}\n-\n-    def connectionMade(self):\n-        self.pid = self.transport.pid\n-        log.msg(\"ClusterWorker: started domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n-        self.transport.closeStdin()\n-        self.status = \"running\"\n-        self.procman.update_master(self.domain, \"running\")\n-\n-    def processEnded(self, reason):\n-        if settings.getbool('CLUSTER_WORKER_GZIP_LOGS'):\n-            try:\n-                f_in = open(self.logfile)\n-                f_out = gzip.open(\"%s.gz\" % self.logfile, \"wb\")\n-                f_out.writelines(f_in)\n-                f_in.close()\n-                f_out.close()\n-                os.remove(self.logfile)\n-                self.logfile = \"%s.gz\" % self.logfile\n-            except Exception, e:\n-                log.msg(\"failed to compress %s exception=%s (domain=%s, pid=%s)\" % (self.logfile, e, self.domain, self.pid))\n-        log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n-        log.msg(\"Reason type: %s. value: %s\" % (reason.type, reason.value) )\n-        del self.procman.running[self.domain]\n-        del self.procman.crawlers[self.pid]\n-        self.procman.update_master(self.domain, \"scraped\")\n-\n-class ClusterWorker(pb.Root):\n-\n-    def __init__(self):\n-        if not settings.getbool('CLUSTER_WORKER_ENABLED'):\n-            raise NotConfigured\n-\n-        self.maxproc = settings.getint('CLUSTER_WORKER_MAXPROC')\n-        self.logdir = settings['CLUSTER_LOGDIR']\n-        self.running = {}#a dict domain->ScrapyProcessControl \n-        self.crawlers = {}#a dict pid->scrapy process remote pb connection\n-        self.starttime = datetime.datetime.utcnow()\n-        port = settings.getint('CLUSTER_WORKER_PORT')\n-        scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n-        log.msg(\"PYTHONPATH: %s\" % repr(sys.path))\n-\n-    def status(self, rcode=0, rstring=None):\n-        status = {}\n-        status[\"running\"] = [ self.running[k].as_dict() for k in self.running.keys() ]\n-        status[\"starttime\"] = self.starttime\n-        status[\"timestamp\"] = datetime.datetime.utcnow()\n-        status[\"maxproc\"] = self.maxproc\n-        status[\"loadavg\"] = os.getloadavg()\n-        status[\"logdir\"] = self.logdir\n-        status[\"callresponse\"] = (rcode, rstring) if rstring else (0, \"Status Response.\")\n-        return status\n-\n-    def update_master(self, domain, domain_status):\n-        try:\n-            deferred = self.__master.callRemote(\"update\", self.status(), domain, domain_status)\n-        except pb.DeadReferenceError:\n-            self.__master = None\n-            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n-        else:\n-            deferred.addCallbacks(callback=lambda x: x, errback=lambda reason: log.msg(reason, log.ERROR))\n-        \n-    def remote_set_master(self, master):\n-        self.__master = master\n-        return self.status()\n-\n-    def remote_stop(self, domain):\n-        \"\"\"Stop running domain.\"\"\"\n-        if domain in self.running:\n-            proc = self.running[domain]\n-            log.msg(\"ClusterWorker: Sending shutdown signal to domain=%s, pid=%d\" % (domain, proc.pid))\n-            d = self.crawlers[proc.pid].callRemote(\"stop\")\n-            def _close():\n-                proc.status = \"closing\"\n-            d.addCallbacks(callback=_close, errback=lambda reason: log.msg(reason, log.ERROR))\n-            return self.status(0, \"Stopped process %s\" % proc)\n-        else:\n-            return self.status(1, \"%s: domain not running.\" % domain)\n-\n-    def remote_status(self):\n-        return self.status()\n-\n-    def remote_run(self, domain, spider_settings=None):\n-        \"\"\"Spawn process to run the given domain.\"\"\"\n-        if len(self.running) < self.maxproc:\n-            if not domain in self.running:\n-                logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n-                if not os.path.exists(os.path.dirname(logfile)):\n-                    os.makedirs(os.path.dirname(logfile))\n-                scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n-                args = [sys.executable, sys.argv[0], 'crawl', domain]\n-                self.running[domain] = scrapy_proc\n-                try:\n-                    import pysvn\n-                    c = pysvn.Client()\n-                    r = c.update(settings.get(\"CLUSTER_WORKER_SVNWORKDIR\", \".\"))\n-                    log.msg(\"Updated to revision %s.\" %r[0].number, level=log.DEBUG)\n-                except pysvn.ClientError, e:\n-                    log.msg(\"Unable to svn update: %s\" % e, level=log.WARNING)\n-                except ImportError:\n-                    log.msg(\"pysvn module not available.\", level=log.WARNING)\n-                proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n-                return self.status(0, \"Started process %s.\" % scrapy_proc)\n-            return self.status(2, \"Domain %s already running.\" % domain )\n-        return self.status(1, \"No free slot to run another process.\")\n-\n-    def remote_register_crawler(self, pid, crawler):\n-        self.crawlers[pid] = crawler\n\n@@ -1,25 +0,0 @@\n-#!/usr/bin/python2.5\n-\n-from twisted.spread import pb\n-from twisted.internet import reactor\n-from twisted.python import util\n-import sys\n-\n-factory = pb.PBClientFactory()\n-reactor.connectTCP(\"localhost\", 8789, factory)\n-d = factory.getRootObject()\n-\n-sys.argv.pop(0)\n-\n-if not sys.argv:\n-    d.addCallback(lambda object: object.callRemote(\"status\"))\n-elif sys.argv[0] == \"-s\":\n-    d.addCallback(lambda object: object.callRemote(\"stop\", sys.argv[1]))\n-elif sys.argv[0] == \"-r\":\n-    d.addCallback(lambda object: object.callRemote(\"run\", sys.argv[1]))\n-elif sys.argv[0] == \"-t\":\n-    d.addCallback(lambda object: object.callRemote(\"statistics\"))\n-\n-d.addCallbacks(callback = util.println, errback = lambda reason: 'error: '+str(reason.value))\n-d.addCallback(lambda _: reactor.stop())\n-reactor.run()\n\n@@ -1,3 +0,0 @@\n-from scrapy.contrib_exp.cluster.worker.manager import ClusterWorker\n-from scrapy.contrib_exp.cluster.master.web import ClusterMasterWeb\n-from scrapy.contrib_exp.cluster.crawler.manager import ClusterCrawler\n\n\n\n@@ -1,80 +0,0 @@\n-#!/usr/bin/env python\n-\"\"\"\n-Cluster control script\n-\"\"\"\n-\n-from optparse import OptionParser\n-import urllib\n-\n-def main():\n-    parser = OptionParser(usage=\"Usage: scrapy-cluster-ctl.py [domain [domain [...]]] [options]\" )\n-    parser.add_option(\"--disablenode\", dest=\"disable_node\", help=\"Disable given node (by name) so it will no accept more run requests.\")\n-    parser.add_option(\"--enablenode\", dest=\"enable_node\", help=\"Enable given node (by name) so it will accept again run requests\")\n-    parser.add_option(\"--format\", dest=\"format\", help=\"Output format. Default: pprint.\", default=\"pprint\")\n-    parser.add_option(\"--list\", metavar=\"FILE\", dest=\"list\", help=\"Specify a file from where to read domains, one per line.\")\n-    parser.add_option(\"--now\", action=\"store_true\", dest=\"now\", help=\"Schedule domains to run with priority now.\")\n-    parser.add_option(\"--output\", metavar=\"FILE\", dest=\"output\", help=\"Output file. If not given, output to stdout.\")\n-    parser.add_option(\"--port\", dest=\"port\", type=\"int\", help=\"Cluster master port. Default: 8060.\", default=8060)\n-    parser.add_option(\"--remove\", dest=\"remove\", action=\"store_true\", help=\"Remove from schedule domains given as args.\")\n-    parser.add_option(\"--schedule\", dest=\"schedule\", action=\"store_true\", help=\"Schedule domains given as args.\")\n-    parser.add_option(\"--server\", dest=\"server\", help=\"Cluster master server name. Default: localhost.\", default=\"localhost\")\n-    parser.add_option(\"--status\", dest=\"status\", action=\"store_true\", help=\"Print cluster master status and quit.\")\n-    parser.add_option(\"--statistics\", dest=\"statistics\", action=\"store_true\", help=\"Print cluster statistics\")\n-    parser.add_option(\"--stop\", dest=\"stop\", action=\"store_true\", help=\"Stops a running domain.\")\n-    parser.add_option(\"--verbosity\", dest=\"verbosity\", type=\"int\", help=\"Sets the report status verbosity.\")\n-    (opts, args) = parser.parse_args()\n-    \n-    output = \"\"\n-    domains = []\n-    urlstring = \"http://%s:%s/cluster_master/ws/\" % (opts.server, opts.port)\n-    post = {\"format\":opts.format}\n-    if isinstance(opts.verbosity, int):\n-        post[\"verbosity\"] = opts.verbosity\n-    \n-    if args:\n-        domains = \",\".join(args)\n-    elif opts.list:\n-        try:\n-            domainlist = []\n-            for d in open(opts.list, \"r\").readlines():\n-                domainlist.append(d.strip())\n-            domains = \",\".join(domainlist)\n-        except IOError:\n-            print \"Can't open file %s\" % opts.list\n-    \n-    if opts.status:\n-        pass\n-    elif opts.statistics:\n-        post[\"statistics\"] = True\n-    elif opts.schedule and domains:\n-        post[\"schedule\"] = domains\n-        if opts.now:\n-            post[\"priority\"] = \"0\"\n-            post[\"settings\"] = \"UNAVAILABLES_NOTIFY=2\"\n-    elif opts.remove and domains:\n-        post[\"remove\"] = domains\n-    elif opts.stop and domains:\n-        post[\"stop\"] = domains\n-    elif opts.disable_node:\n-        post[\"disable_node\"] = opts.disable_node\n-    elif opts.enable_node:\n-        post[\"enable_node\"] = opts.enable_node\n-    else:\n-        parser.print_help()\n-        return\n-\n-    f = urllib.urlopen(urlstring, urllib.urlencode(post))\n-    output=f.read()\n-    if not output:\n-        return\n-    if not opts.output:\n-        print output\n-    else:\n-        try:\n-            open(opts.output, \"w\").write(output)\n-        except IOError:\n-            open(\"/tmp/scrapy-cluster-schedule.tmp\", \"w\").write(output)\n-            print \"Could not open file %s for writing. Output dumped to /tmp/scrapy-cluster-schedule.tmp instead.\" % opts.output\n-\n-if __name__ == '__main__':\n-    main()\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
