{"custom_id": "keras#bef0a9ef40fae1b7798ec1bd758897f03fffce78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 21 | Files Changed: 4 | Hunks: 12 | Methods Changed: 11 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 78 | Churn Cumulative: 963 | Contributors (this commit): 10 | Commits (past 90d): 28 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -127,6 +127,17 @@ class MultiHeadAttention(Layer):\n         self._value_dim = value_dim if value_dim else key_dim\n         self._dropout = dropout\n         self._use_bias = use_bias\n+        if output_shape:\n+            if isinstance(output_shape, int):\n+                output_shape = (output_shape,)\n+            try:\n+                output_shape = tuple(output_shape)\n+            except:\n+                raise ValueError(\n+                    f\"Invalid `output_shape`: {output_shape}. When \"\n+                    \"specified, the `output_shape` should be of type tuple, \"\n+                    \"list, or int.\"\n+                )\n         self._output_shape = output_shape\n         self._flash_attention = flash_attention or is_flash_attention_enabled()\n         self._kernel_initializer = initializers.get(kernel_initializer)\n@@ -176,9 +187,8 @@ class MultiHeadAttention(Layer):\n     def use_bias(self):\n         return self._use_bias\n \n-    @property\n-    def output_shape(self):\n-        return self._output_shape\n+    # Avoid exposing `output_shape` as it may conflict with `Functional` and\n+    # `Sequential` models when calling `summary()`.\n \n     @property\n     def attention_axes(self):\n@@ -343,14 +353,7 @@ class MultiHeadAttention(Layer):\n         \"\"\"\n         query_rank = len(query_shape)\n         if self._output_shape:\n-            if isinstance(self._output_shape, (tuple, list)):\n             output_shape = self._output_shape\n-            elif isinstance(self._output_shape, int):\n-                output_shape = [self._output_shape]\n-            else:\n-                raise ValueError(\n-                    f\"Invalid output_shape type: {self._output_shape}\"\n-                )\n         else:\n             output_shape = [query_shape[-1]]\n         einsum_equation, bias_axes, output_rank = _build_proj_equation(\n@@ -664,8 +667,12 @@ class MultiHeadAttention(Layer):\n         value_shape,\n         key_shape=None,\n     ):\n+        query_shape = tuple(query_shape)\n+        value_shape = tuple(value_shape)\n         if key_shape is None:\n             key_shape = value_shape\n+        else:\n+            key_shape = tuple(key_shape)\n \n         if value_shape[1:-1] != key_shape[1:-1]:\n             raise ValueError(\n@@ -673,17 +680,8 @@ class MultiHeadAttention(Layer):\n                 f\"must be equal. Received: value_shape={value_shape} and \"\n                 f\"key_shape={key_shape}\"\n             )\n-\n         if self._output_shape:\n-            if isinstance(self._output_shape, (tuple, list)):\n-                return query_shape[:-1] + tuple(self._output_shape)\n-            elif isinstance(self._output_shape, int):\n-                return query_shape[:-1] + (self._output_shape,)\n-            else:\n-                raise ValueError(\n-                    f\"Invalid output_shape type: {self._output_shape}\"\n-                )\n-\n+            query_shape = query_shape[:-1] + self._output_shape\n         return query_shape\n \n     def compute_output_spec(\n\n@@ -247,6 +247,14 @@ class MultiHeadAttentionTest(testing.TestCase):\n         )\n         self.assertEqual(output.shape, comp_output_shape)\n \n+        # Test shapes as lists.\n+        comp_output_shape = layer.compute_output_shape(\n+            list(query_shape),\n+            list(value_shape),\n+            list(key_shape) if key_shape is not None else None,\n+        )\n+        self.assertEqual(output.shape, comp_output_shape)\n+\n     @parameterized.named_parameters(\n         (\"query_value_dim_mismatch\", (2, 4, 8), (2, 2, 7), (2,)),\n         (\"key_value_dim_mismatch\", (2, 4, 8), (2, 2, 8), (2, 1, 7)),\n@@ -634,3 +642,7 @@ class MultiHeadAttentionTest(testing.TestCase):\n         assert output.shape == (2, 4, 8, 8), (\n             f\"Expected shape (2, 4, 8, 8),\" f\" got {output.shape}\"\n         )\n+\n+    def test_multi_head_attention_output_shape_error(self):\n+        with self.assertRaisesRegex(ValueError, r\"Invalid `output_shape`\"):\n+            layers.MultiHeadAttention(num_heads=2, key_dim=16, output_shape=8.0)\n\n@@ -103,7 +103,7 @@ def format_layer_shape(layer):\n     else:\n         try:\n             if hasattr(layer, \"output_shape\"):\n-                output_shapes = layer.output_shape\n+                output_shapes = format_shape(layer.output_shape)\n             else:\n                 outputs = layer.compute_output_shape(**layer._build_shapes_dict)\n                 output_shapes = tree.map_shape_structure(\n\n@@ -98,3 +98,29 @@ class SummaryUtilsTest(testing.TestCase):\n         self.assertIn(\"Total params: 12\", summary_content)\n         self.assertIn(\"Trainable params: 12\", summary_content)\n         self.assertIn(\"Non-trainable params: 0\", summary_content)\n+\n+    def test_print_model_summary_with_mha(self):\n+        # In Keras <= 3.6, MHA exposes `output_shape` property which breaks this\n+        # test.\n+        class MyModel(models.Model):\n+            def __init__(self):\n+                super().__init__()\n+                self.mha = layers.MultiHeadAttention(2, 2, output_shape=(4,))\n+\n+            def call(self, inputs):\n+                return self.mha(inputs, inputs, inputs)\n+\n+        model = MyModel()\n+        model(np.ones((1, 2, 2)))\n+\n+        summary_content = []\n+\n+        def print_to_variable(text, line_break=False):\n+            summary_content.append(text)\n+\n+        summary_utils.print_summary(model, print_fn=print_to_variable)\n+        summary_content = \"\\n\".join(summary_content)\n+        self.assertIn(\"(1, 2, 4)\", summary_content)  # mha\n+        self.assertIn(\"Total params: 56\", summary_content)\n+        self.assertIn(\"Trainable params: 56\", summary_content)\n+        self.assertIn(\"Non-trainable params: 0\", summary_content)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1e8426b2f543c4bb739994bb2fc9e0b5ad117e3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 181 | Lines Deleted: 16 | Files Changed: 4 | Hunks: 18 | Methods Changed: 10 | Complexity Δ (Sum/Max): 23/11 | Churn Δ: 197 | Churn Cumulative: 2779 | Contributors (this commit): 16 | Commits (past 90d): 39 | Contributors (cumulative): 30 | DMM Complexity: 0.6875\n\nDIFF:\n@@ -44,6 +44,9 @@ class LossFunctionWrapper(Loss):\n             config = serialization_lib.deserialize_keras_object(config)\n         return cls(**config)\n \n+    def __repr__(self):\n+        return f\"<LossFunctionWrapper({self.fn}, kwargs={self._fn_kwargs})>\"\n+\n \n @keras_export(\"keras.losses.MeanSquaredError\")\n class MeanSquaredError(LossFunctionWrapper):\n\n@@ -212,7 +212,7 @@ class Functional(Function, Model):\n     def _assert_input_compatibility(self, *args):\n         return super(Model, self)._assert_input_compatibility(*args)\n \n-    def _maybe_warn_inputs_struct_mismatch(self, inputs):\n+    def _maybe_warn_inputs_struct_mismatch(self, inputs, raise_exception=False):\n         try:\n             # We first normalize to tuples before performing the check to\n             # suppress warnings when encountering mismatched tuples and lists.\n@@ -225,12 +225,17 @@ class Functional(Function, Model):\n             model_inputs_struct = tree.map_structure(\n                 lambda x: x.name, self._inputs_struct\n             )\n-            inputs_struct = tree.map_structure(lambda x: f\"type({x})\", inputs)\n-            warnings.warn(\n-                \"The structure of `inputs` doesn't match the expected \"\n-                f\"structure: {model_inputs_struct}. \"\n-                f\"Received: the structure of inputs={inputs_struct}\"\n+            inputs_struct = tree.map_structure(\n+                lambda x: f\"Tensor(shape={x.shape})\", inputs\n             )\n+            msg = (\n+                \"The structure of `inputs` doesn't match the expected \"\n+                f\"structure.\\nExpected: {model_inputs_struct}\\n\"\n+                f\"Received: inputs={inputs_struct}\"\n+            )\n+            if raise_exception:\n+                raise ValueError(msg)\n+            warnings.warn(msg)\n \n     def _convert_inputs_to_tensors(self, flat_inputs):\n         converted = []\n@@ -279,7 +284,33 @@ class Functional(Function, Model):\n         return adjusted\n \n     def _standardize_inputs(self, inputs):\n-        self._maybe_warn_inputs_struct_mismatch(inputs)\n+        raise_exception = False\n+        if isinstance(inputs, dict) and not isinstance(\n+            self._inputs_struct, dict\n+        ):\n+            # This is to avoid warning\n+            # when we have reconciable dict/list structs\n+            if hasattr(self._inputs_struct, \"__len__\") and all(\n+                isinstance(i, backend.KerasTensor) for i in self._inputs_struct\n+            ):\n+                expected_keys = set(i.name for i in self._inputs_struct)\n+                keys = set(inputs.keys())\n+                if expected_keys.issubset(keys):\n+                    inputs = [inputs[i.name] for i in self._inputs_struct]\n+                else:\n+                    raise_exception = True\n+            elif isinstance(self._inputs_struct, backend.KerasTensor):\n+                if self._inputs_struct.name in inputs:\n+                    inputs = [inputs[self._inputs_struct.name]]\n+                else:\n+                    raise_exception = True\n+            else:\n+                raise_exception = True\n+\n+        self._maybe_warn_inputs_struct_mismatch(\n+            inputs, raise_exception=raise_exception\n+        )\n+\n         flat_inputs = tree.flatten(inputs)\n         flat_inputs = self._convert_inputs_to_tensors(flat_inputs)\n         return self._adjust_input_rank(flat_inputs)\n\n@@ -137,10 +137,10 @@ class FunctionalTest(testing.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_named_input_dict_io(self):\n+        # Single input\n         input_a = Input(shape=(3,), batch_size=2, name=\"a\")\n         x = layers.Dense(5)(input_a)\n         outputs = layers.Dense(4)(x)\n-\n         model = Functional(input_a, outputs)\n \n         # Eager call\n@@ -154,6 +154,27 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(in_val)\n         self.assertEqual(out_val.shape, (2, 4))\n \n+        # Two inputs\n+        input_a = Input(shape=(3,), batch_size=2, name=\"a\")\n+        input_b = Input(shape=(4,), batch_size=2, name=\"b\")\n+        a = layers.Dense(5)(input_a)\n+        b = layers.Dense(5)(input_b)\n+        x = layers.Concatenate()([a, b])\n+        outputs = layers.Dense(4)(x)\n+        model = Functional([input_a, input_b], outputs)\n+\n+        # Eager call\n+        in_val = {\"a\": np.random.random((2, 3)), \"b\": np.random.random((2, 4))}\n+        out_val = model(in_val)\n+        self.assertEqual(out_val.shape, (2, 4))\n+\n+        # Symbolic call\n+        input_a_2 = Input(shape=(3,), batch_size=2)\n+        input_b_2 = Input(shape=(4,), batch_size=2)\n+        in_val = {\"a\": input_a_2, \"b\": input_b_2}\n+        out_val = model(in_val)\n+        self.assertEqual(out_val.shape, (2, 4))\n+\n     @pytest.mark.requires_trainable_backend\n     def test_input_dict_with_extra_field(self):\n         input_a = Input(shape=(3,), batch_size=2, name=\"a\")\n@@ -180,7 +201,7 @@ class FunctionalTest(testing.TestCase):\n         self.assertLen(record, 1)\n         self.assertStartsWith(\n             str(record[0].message),\n-            r\"The structure of `inputs` doesn't match the expected structure:\",\n+            r\"The structure of `inputs` doesn't match the expected structure\",\n         )\n \n     @parameterized.named_parameters(\n@@ -547,3 +568,92 @@ class FunctionalTest(testing.TestCase):\n             AttributeError, \"`Model.layers` attribute is reserved\"\n         ):\n             model.layers = [layers.Dense(4)]\n+\n+    def test_dict_input_to_list_model(self):\n+        vocabulary_size = 100\n+        num_tags = 10\n+        num_departments = 3\n+        num_samples = 128\n+\n+        title = layers.Input(shape=(vocabulary_size,), name=\"title\")\n+        text_body = layers.Input(shape=(vocabulary_size,), name=\"text_body\")\n+        tags = layers.Input(shape=(num_tags,), name=\"tags\")\n+        features = layers.Concatenate()([title, text_body, tags])\n+        features = layers.Dense(64, activation=\"relu\")(features)\n+        priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(\n+            features\n+        )\n+        department = layers.Dense(\n+            num_departments, activation=\"softmax\", name=\"department\"\n+        )(features)\n+        model = Functional(\n+            inputs=[title, text_body, tags], outputs=[priority, department]\n+        )\n+\n+        title_data = np.random.randint(\n+            0, 2, size=(num_samples, vocabulary_size)\n+        )\n+        text_body_data = np.random.randint(\n+            0, 2, size=(num_samples, vocabulary_size)\n+        )\n+        tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n+        priority_data = np.random.random(size=(num_samples, 1))\n+        department_data = np.random.randint(\n+            0, 2, size=(num_samples, num_departments)\n+        )\n+\n+        # List style fit\n+        model.compile(\n+            optimizer=\"adam\",\n+            loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n+            metrics=[[\"mean_absolute_error\"], [\"accuracy\"]],\n+        )\n+        model.fit(\n+            [title_data, text_body_data, tags_data],\n+            [priority_data, department_data],\n+            epochs=1,\n+        )\n+        model.evaluate(\n+            [title_data, text_body_data, tags_data],\n+            [priority_data, department_data],\n+        )\n+        priority_preds, department_preds = model.predict(\n+            [title_data, text_body_data, tags_data]\n+        )\n+\n+        # Dict style fit\n+        model.compile(\n+            optimizer=\"adam\",\n+            loss={\n+                \"priority\": \"mean_squared_error\",\n+                \"department\": \"categorical_crossentropy\",\n+            },\n+            metrics={\n+                \"priority\": [\"mean_absolute_error\"],\n+                \"department\": [\"accuracy\"],\n+            },\n+        )\n+        model.fit(\n+            {\n+                \"title\": title_data,\n+                \"text_body\": text_body_data,\n+                \"tags\": tags_data,\n+            },\n+            {\"priority\": priority_data, \"department\": department_data},\n+            epochs=1,\n+        )\n+        model.evaluate(\n+            {\n+                \"title\": title_data,\n+                \"text_body\": text_body_data,\n+                \"tags\": tags_data,\n+            },\n+            {\"priority\": priority_data, \"department\": department_data},\n+        )\n+        priority_preds, department_preds = model.predict(\n+            {\n+                \"title\": title_data,\n+                \"text_body\": text_body_data,\n+                \"tags\": tags_data,\n+            }\n+        )\n\n@@ -428,7 +428,6 @@ class CompileLoss(losses_module.Loss):\n                 f\"Received instead: loss_weights={loss_weights} \"\n                 f\"of type {type(loss_weights)}\"\n             )\n-\n         self._user_loss = loss\n         self._user_loss_weights = loss_weights\n         self.built = False\n@@ -566,6 +565,19 @@ class CompileLoss(losses_module.Loss):\n             )\n \n     def build(self, y_true, y_pred):\n+        if (\n+            self.output_names\n+            and isinstance(self._user_loss, dict)\n+            and not isinstance(y_pred, dict)\n+        ):\n+            loss = [self._user_loss[name] for name in self.output_names]\n+            if isinstance(self._user_loss_weights, dict):\n+                loss_weights = [\n+                    self._user_loss_weights[name] for name in self.output_names\n+                ]\n+            else:\n+                loss_weights = self._user_loss_weights\n+        else:\n             loss = self._user_loss\n             loss_weights = self._user_loss_weights\n         flat_output_names = self.output_names\n@@ -691,7 +703,6 @@ class CompileLoss(losses_module.Loss):\n         try:\n             tree.assert_same_structure(y_pred, y_true, check_types=False)\n         except ValueError:\n-            try:\n             # Check case where y_true is either flat or leaf\n             if (\n                 not tree.is_nested(y_true)\n@@ -699,6 +710,12 @@ class CompileLoss(losses_module.Loss):\n                 and len(y_pred) == 1\n             ):\n                 y_true = [y_true]\n+\n+            # Check case where y_pred is list/tuple and y_true is dict\n+            elif isinstance(y_pred, (list, tuple)) and isinstance(y_true, dict):\n+                if set(self.output_names) == set(y_true.keys()):\n+                    y_true = [y_true[name] for name in self.output_names]\n+\n             try:\n                 y_true = tree.pack_sequence_as(y_pred, y_true)\n             except:\n@@ -707,10 +724,9 @@ class CompileLoss(losses_module.Loss):\n                 # e.g `list` vs `tuple`.\n                 try:\n                     tree.assert_same_paths(y_true, y_pred)\n-                        y_true = tree.pack_sequence_as(\n-                            y_pred, tree.flatten(y_true)\n-                        )\n+                    y_true = tree.pack_sequence_as(y_pred, tree.flatten(y_true))\n                 except:\n+                    try:\n                         # Check case where loss is partially defined over y_pred\n                         flat_y_true = tree.flatten(y_true)\n                         flat_loss = tree.flatten(self._user_loss)\n@@ -727,8 +743,12 @@ class CompileLoss(losses_module.Loss):\n                             y_true[i] = y_t\n                         y_true = tree.pack_sequence_as(self._user_loss, y_true)\n                     except:\n-                y_true_struct = tree.map_structure(lambda _: \"*\", y_true)\n-                y_pred_struct = tree.map_structure(lambda _: \"*\", y_pred)\n+                        y_true_struct = tree.map_structure(\n+                            lambda _: \"*\", y_true\n+                        )\n+                        y_pred_struct = tree.map_structure(\n+                            lambda _: \"*\", y_pred\n+                        )\n                         raise ValueError(\n                             \"y_true and y_pred have different structures.\\n\"\n                             f\"y_true: {y_true_struct}\\n\"\n@@ -774,6 +794,7 @@ class CompileLoss(losses_module.Loss):\n                 _sample_weight = resolve_path(path, sample_weight)\n             else:\n                 _sample_weight = sample_weight\n+\n             value = ops.cast(\n                 loss_fn(y_t, y_p, _sample_weight), dtype=self.dtype\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#72e9b9d4c02b1dbcda869af311033446c15c8e3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 69 | Churn Cumulative: 2060 | Contributors (this commit): 13 | Commits (past 90d): 33 | Contributors (cumulative): 23 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -154,7 +154,8 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(in_val)\n         self.assertEqual(out_val.shape, (2, 4))\n \n-        # Two inputs\n+        # ----\n+        # Two inputs, input is list\n         input_a = Input(shape=(3,), batch_size=2, name=\"a\")\n         input_b = Input(shape=(4,), batch_size=2, name=\"b\")\n         a = layers.Dense(5)(input_a)\n@@ -175,6 +176,46 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(in_val)\n         self.assertEqual(out_val.shape, (2, 4))\n \n+        # ----\n+        # Two inputs, input is dict\n+        model = Functional({\"a\": input_a, \"b\": input_b}, outputs)\n+\n+        # Eager call\n+        in_val = {\"a\": np.random.random((2, 3)), \"b\": np.random.random((2, 4))}\n+        out_val = model(in_val)\n+        self.assertEqual(out_val.shape, (2, 4))\n+\n+        # Symbolic call\n+        input_a_2 = Input(shape=(3,), batch_size=2)\n+        input_b_2 = Input(shape=(4,), batch_size=2)\n+        in_val = {\"a\": input_a_2, \"b\": input_b_2}\n+        out_val = model(in_val)\n+        self.assertEqual(out_val.shape, (2, 4))\n+\n+        # ----\n+        # Two inputs, input is dict with incorrect names\n+        model = Functional({\"c\": input_a, \"d\": input_b}, outputs)\n+\n+        # Eager call\n+        in_val = {\"c\": np.random.random((2, 3)), \"d\": np.random.random((2, 4))}\n+        out_val = model(in_val)\n+        self.assertEqual(out_val.shape, (2, 4))\n+\n+        # Symbolic call\n+        input_a_2 = Input(shape=(3,), batch_size=2)\n+        input_b_2 = Input(shape=(4,), batch_size=2)\n+        in_val = {\"c\": input_a_2, \"d\": input_b_2}\n+        out_val = model(in_val)\n+        self.assertEqual(out_val.shape, (2, 4))\n+\n+        # Now we can't use the input names:\n+        with self.assertRaises(ValueError):\n+            in_val = {\n+                \"a\": np.random.random((2, 3)),\n+                \"b\": np.random.random((2, 4)),\n+            }\n+            out_val = model(in_val)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_input_dict_with_extra_field(self):\n         input_a = Input(shape=(3,), batch_size=2, name=\"a\")\n@@ -560,6 +601,7 @@ class FunctionalTest(testing.TestCase):\n         # TODO\n         pass\n \n+    @pytest.mark.requires_trainable_backend\n     def test_layers_setter(self):\n         inputs = Input(shape=(3,), batch_size=2, name=\"input\")\n         outputs = layers.Dense(5)(inputs)\n\n@@ -661,10 +661,11 @@ class ModelTest(testing.TestCase):\n                 \"output_c\": \"binary_crossentropy\",\n             },\n         )\n+\n         # Fit the model to make sure compile_metrics are built\n         with self.assertRaisesRegex(\n-            KeyError,\n-            \"in the `loss` argument, can't be found in the model's output\",\n+            ValueError,\n+            \"Expected keys\",\n         ):\n             model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n \n@@ -680,8 +681,8 @@ class ModelTest(testing.TestCase):\n         )\n         # Fit the model to make sure compile_metrics are built\n         with self.assertRaisesRegex(\n-            KeyError,\n-            \"in the `loss` argument, can't be found in the model's output\",\n+            ValueError,\n+            \"Expected keys\",\n         ):\n             model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n \n\n@@ -565,22 +565,26 @@ class CompileLoss(losses_module.Loss):\n             )\n \n     def build(self, y_true, y_pred):\n+        loss = self._user_loss\n+        loss_weights = self._user_loss_weights\n+        flat_output_names = self.output_names\n         if (\n             self.output_names\n             and isinstance(self._user_loss, dict)\n             and not isinstance(y_pred, dict)\n         ):\n+            if set(self.output_names) == set(self._user_loss.keys()):\n                 loss = [self._user_loss[name] for name in self.output_names]\n                 if isinstance(self._user_loss_weights, dict):\n                     loss_weights = [\n-                    self._user_loss_weights[name] for name in self.output_names\n+                        self._user_loss_weights[name]\n+                        for name in self.output_names\n                     ]\n             else:\n-                loss_weights = self._user_loss_weights\n-        else:\n-            loss = self._user_loss\n-            loss_weights = self._user_loss_weights\n-        flat_output_names = self.output_names\n+                raise ValueError(\n+                    f\"Expected keys {self.output_names} in loss dict, but found \"\n+                    f\"loss.keys()={list(self._user_loss.keys())}\"\n+                )\n \n         # Pytree leaf container\n         class WeightedLoss:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0369f6e7f8780c497f1cdcc73254a64df1c1d18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1378 | Contributors (this commit): 10 | Commits (past 90d): 25 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -601,7 +601,6 @@ class FunctionalTest(testing.TestCase):\n         # TODO\n         pass\n \n-    @pytest.mark.requires_trainable_backend\n     def test_layers_setter(self):\n         inputs = Input(shape=(3,), batch_size=2, name=\"input\")\n         outputs = layers.Dense(5)(inputs)\n@@ -611,6 +610,7 @@ class FunctionalTest(testing.TestCase):\n         ):\n             model.layers = [layers.Dense(4)]\n \n+    @pytest.mark.requires_trainable_backend\n     def test_dict_input_to_list_model(self):\n         vocabulary_size = 100\n         num_tags = 10\n\n@@ -582,8 +582,8 @@ class CompileLoss(losses_module.Loss):\n                     ]\n             else:\n                 raise ValueError(\n-                    f\"Expected keys {self.output_names} in loss dict, but found \"\n-                    f\"loss.keys()={list(self._user_loss.keys())}\"\n+                    f\"Expected keys {self.output_names} in loss dict, but \"\n+                    f\"found loss.keys()={list(self._user_loss.keys())}\"\n                 )\n \n         # Pytree leaf container\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#553521ee92f72c993ddb3c40d0ced406a3a4b7db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 53 | Files Changed: 10 | Hunks: 56 | Methods Changed: 24 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 144 | Churn Cumulative: 1818 | Contributors (this commit): 18 | Commits (past 90d): 30 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,7 @@ from keras.src.backend.common.stateless_scope import in_stateless_scope\n from keras.src.backend.common.symbolic_scope import SymbolicScope\n from keras.src.backend.common.symbolic_scope import in_symbolic_scope\n from keras.src.backend.common.variables import AutocastScope\n+from keras.src.backend.common.variables import Variable\n from keras.src.backend.common.variables import get_autocast_scope\n from keras.src.backend.common.variables import is_float_dtype\n from keras.src.backend.common.variables import is_int_dtype\n@@ -35,25 +36,26 @@ from keras.src.backend.config import standardize_data_format\n # Import backend functions.\n if backend() == \"tensorflow\":\n     from keras.src.backend.tensorflow import *  # noqa: F403\n+    from keras.src.backend.tensorflow.core import Variable as BackendVariable\n elif backend() == \"jax\":\n     from keras.src.backend.jax import *  # noqa: F403\n+    from keras.src.backend.jax.core import Variable as BackendVariable\n elif backend() == \"torch\":\n     from keras.src.backend.torch import *  # noqa: F403\n+    from keras.src.backend.torch.core import Variable as BackendVariable\n \n     distribution_lib = None\n elif backend() == \"numpy\":\n     from keras.src.backend.numpy import *  # noqa: F403\n+    from keras.src.backend.numpy.core import Variable as BackendVariable\n \n     distribution_lib = None\n else:\n     raise ValueError(f\"Unable to import backend : {backend()}\")\n \n \n-BackendVariable = Variable  # noqa: F405\n-\n-\n @keras_export(\"keras.Variable\")\n-class Variable(BackendVariable):\n+class Variable(BackendVariable):  # noqa: F811\n     pass\n \n \n\n@@ -1,7 +1,7 @@\n from keras.src.backend.common import backend_utils\n from keras.src.backend.common.dtypes import result_type\n from keras.src.backend.common.variables import AutocastScope\n-from keras.src.backend.common.variables import KerasVariable\n+from keras.src.backend.common.variables import Variable as KerasVariable\n from keras.src.backend.common.variables import get_autocast_scope\n from keras.src.backend.common.variables import is_float_dtype\n from keras.src.backend.common.variables import is_int_dtype\n\n@@ -8,7 +8,7 @@ class StatelessScope:\n \n     The values of variables to be used inside the scope\n     should be passed via the `state_mapping` argument, a\n-    list of tuples `(k, v)` where `k` is a `KerasVariable`\n+    list of tuples `(k, v)` where `k` is a `Variable`\n     and `v` is the intended value for this variable\n     (a backend tensor).\n \n@@ -39,7 +39,7 @@ class StatelessScope:\n         initialize_variables=True,\n     ):\n         from keras.src import backend\n-        from keras.src.backend.common.variables import KerasVariable\n+        from keras.src.backend.common.variables import Variable\n \n         self.collect_losses = collect_losses\n         self.initialize_variables = initialize_variables\n@@ -47,13 +47,13 @@ class StatelessScope:\n         self.state_mapping = {}\n         state_mapping = state_mapping or {}\n         for k, v in state_mapping:\n-            if not isinstance(k, KerasVariable):\n+            if not isinstance(k, Variable):\n                 raise ValueError(\n                     \"Invalid reference variable in StatelessScope: \"\n-                    \"all keys in argument `mapping` must be KerasVariable \"\n+                    \"all keys in argument `mapping` must be Variable \"\n                     f\"instances. Received instead: {k}\"\n                 )\n-            if isinstance(v, KerasVariable):\n+            if isinstance(v, Variable):\n                 v = backend.cast(v.value, dtype=k.dtype)\n             else:\n                 v = backend.convert_to_tensor(v, dtype=k.dtype)\n\n@@ -41,7 +41,7 @@ class TestStatelessScope(testing.TestCase):\n         value1 = ops.ones(shape=(2,))\n \n         with self.assertRaisesRegex(\n-            ValueError, \"all keys in argument `mapping` must be KerasVariable\"\n+            ValueError, \"all keys in argument `mapping` must be Variable\"\n         ):\n             StatelessScope(state_mapping=[(invalid_key, value1)])\n \n\n@@ -12,7 +12,7 @@ from keras.src.utils.module_utils import tensorflow as tf\n from keras.src.utils.naming import auto_name\n \n \n-class KerasVariable:\n+class Variable:\n     \"\"\"Represents a backend-agnostic variable in Keras.\n \n     A `Variable` acts as a container for state. It holds a tensor value and can\n@@ -30,17 +30,25 @@ class KerasVariable:\n             dtype type (`\"float32\"` if never configured).\n         trainable: Optional. Boolean indicating if variable is trainable.\n             Defaults to `True`.\n+        autocast: Optional. Boolean indicating whether the variable supports\n+            autocasting. If `True`, the layer may first convert the variable\n+            to the compute data type when accessed. Defaults to `True`.\n+        aggregation: Optional. String specifying how a distributed variable will\n+            be aggregated. This serves as a semantic annotation, to be taken\n+            into account by downstream backends or users. Defaults to `\"mean\"`.\n         name: Optional. A unique name for the variable. Automatically generated\n             if not set.\n \n     Attributes:\n-        name: The name of the variable (string).\n-        path: The path of the variable within the Keras model or layer (string).\n-        dtype: The data type of the variable (string).\n         shape: The shape of the variable (tuple of integers).\n         ndim: The number of dimensions of the variable (integer).\n+        dtype: The data type of the variable (string).\n         trainable: Whether the variable is trainable (boolean).\n+        autocast: Whether the variable supports autocasting (boolean).\n+        aggregation: How a distributed variable will be aggregated (string).\n         value: The current value of the variable (NumPy array or tensor).\n+        name: The name of the variable (string).\n+        path: The path of the variable within the Keras model or layer (string).\n \n     Examples:\n \n@@ -101,20 +109,19 @@ class KerasVariable:\n                 \"one of {'none', 'mean', 'sum', 'only_first_replica'}. \"\n                 f\"Received: aggregation={aggregation}\"\n             )\n-        self.name = name\n+        self._name = name\n         parent_path = current_path()\n         if parent_path:\n-            self.path = current_path() + \"/\" + self.name\n+            self._path = current_path() + \"/\" + name\n         else:\n-            self.path = self.name\n-        dtype = standardize_dtype(dtype)\n-        self._dtype = dtype\n+            self._path = name\n+        self._dtype = standardize_dtype(dtype)\n         self._shape = None\n         self._initializer = None\n         self._regularizer = None\n         self._constraint = None\n-        self._trainable = trainable\n-        self._autocast = autocast\n+        self._trainable = bool(trainable)\n+        self._autocast = bool(autocast)\n         self._aggregation = aggregation\n         # `self._overwrite_with_gradient` is an internal property to determine\n         # whether this variable should be overwritten by the computed gradient.\n@@ -163,7 +170,7 @@ class KerasVariable:\n                 self._initialize_with_initializer(initializer)\n             else:\n                 self._initialize(initializer)\n-                self._shape = tuple(self._value.shape)\n+                self._shape = self._validate_shape(self._value.shape)\n         self._ndim = len(self._shape)\n \n     def _deferred_initialize(self):\n@@ -201,10 +208,12 @@ class KerasVariable:\n \n     @property\n     def aggregation(self):\n+        \"\"\"The strategy for aggregating this variable.\"\"\"\n         return self._aggregation\n \n     @property\n     def value(self):\n+        \"\"\"The current value of the variable (numpy array or backend tensor).\"\"\"\n         if in_stateless_scope():\n             scope = get_stateless_scope()\n             value = scope.get_current_value(self)\n@@ -246,30 +255,46 @@ class KerasVariable:\n \n     @property\n     def dtype(self):\n+        \"\"\"The data type of the variable.\"\"\"\n         autocast_scope = get_autocast_scope()\n         if (\n             self._autocast\n             and autocast_scope is not None\n             and is_float_dtype(self._dtype)\n         ):\n-            return autocast_scope.dtype\n-        return self._dtype\n+            dtype = autocast_scope.dtype\n+        else:\n+            dtype = self._dtype\n+        return backend.standardize_dtype(dtype)\n \n     @property\n     def shape(self):\n+        \"\"\"The shape of the variable.\"\"\"\n         return self._shape\n \n     @property\n     def ndim(self):\n+        \"\"\"The number of dimensions of the variable.\"\"\"\n         return self._ndim\n \n     @property\n     def trainable(self):\n+        \"\"\"Whether the variable is trainable.\"\"\"\n         return self._trainable\n \n     @trainable.setter\n     def trainable(self, value):\n-        self._trainable = value\n+        self._trainable = bool(value)\n+\n+    @property\n+    def name(self):\n+        \"\"\"The name of the variable.\"\"\"\n+        return self._name\n+\n+    @property\n+    def path(self):\n+        \"\"\"The path of the variable within the Keras model or layer.\"\"\"\n+        return self._path\n \n     @property\n     def overwrite_with_gradient(self):\n@@ -326,9 +351,13 @@ class KerasVariable:\n         self._constraint = value\n \n     def __repr__(self):\n+        value = None\n+        if hasattr(self, \"_value\") and self._value is not None:\n+            value = backend.core.convert_to_numpy(self._value)\n+        value_str = f\", value={value}\" if value is not None else \"\"\n         return (\n-            f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, \"\n-            f\"path={self.path}>\"\n+            f\"<Variable path={self.path}, shape={self.shape}, \"\n+            f\"dtype={self.dtype}{value_str}>\"\n         )\n \n     def _initialize(self, value):\n@@ -573,7 +602,7 @@ def get_autocast_scope():\n class AutocastScope:\n     \"\"\"Context manager that enables the autocasting of float variables.\n \n-    Under this context manager, float `KerasVariables`s will be cast to `dtype`\n+    Under this context manager, float `Variables`s will be cast to `dtype`\n     (note that `dtype` must also be float).\n     \"\"\"\n \n\n@@ -8,7 +8,6 @@ from keras.src import backend\n from keras.src import initializers\n from keras.src.backend.common import dtypes\n from keras.src.backend.common.variables import AutocastScope\n-from keras.src.backend.common.variables import KerasVariable\n from keras.src.backend.common.variables import shape_equal\n from keras.src.backend.common.variables import standardize_dtype\n from keras.src.backend.common.variables import standardize_shape\n@@ -17,7 +16,7 @@ from keras.src.testing.test_utils import named_product\n \n \n class VariableInitializationTest(test_case.TestCase):\n-    \"\"\"Tests for KerasVariable.__init__()\"\"\"\n+    \"\"\"Tests for Variable.__init__()\"\"\"\n \n     def test_deferred_initialization(self):\n         \"\"\"Tests deferred initialization of variables.\"\"\"\n@@ -73,17 +72,16 @@ class VariableInitializationTest(test_case.TestCase):\n         self.assertAllClose(v.value, init_value)\n \n     def test_variable_without_shape_from_callable_initializer(self):\n-        \"\"\"Test that KerasVariable raises error\n+        \"\"\"Test that Variable raises error\n         if shape is not provided for callable initializer.\"\"\"\n         with self.assertRaisesRegex(\n             ValueError, \"When creating a Variable from an initializer\"\n         ):\n-            KerasVariable(initializer=lambda: np.ones((2, 2)))\n+            backend.Variable(initializer=lambda: np.ones((2, 2)))\n \n \n class VariablePropertiesTest(test_case.TestCase):\n-    \"\"\"Tests for KerasVariable._deferred_initialize\n-    KerasVariable._maybe_autocast\"\"\"\n+    \"\"\"Tests for Variable._deferred_initialize Variable._maybe_autocast\"\"\"\n \n     def test_deferred_assignment(self):\n         \"\"\"Tests deferred assignment to variables.\"\"\"\n@@ -204,10 +202,12 @@ class VariablePropertiesTest(test_case.TestCase):\n         with self.assertRaisesRegex(\n             ValueError, \"Argument `name` must be a string\"\n         ):\n-            KerasVariable(initializer=initializers.RandomNormal(), name=12345)\n+            backend.Variable(\n+                initializer=initializers.RandomNormal(), name=12345\n+            )\n \n         with self.assertRaisesRegex(ValueError, \"cannot contain character `/`\"):\n-            KerasVariable(\n+            backend.Variable(\n                 initializer=initializers.RandomNormal(), name=\"invalid/name\"\n             )\n \n@@ -272,8 +272,7 @@ class VariablePropertiesTest(test_case.TestCase):\n \n \n class VariableNumpyValueAndAssignmentTest(test_case.TestCase):\n-    \"\"\"tests for KerasVariable.numpy(), KerasVariable.value()\n-    and KerasVariable.assign()\"\"\"\n+    \"\"\"tests for Variable.numpy(), Variable.value() and Variable.assign()\"\"\"\n \n     def test_variable_numpy(self):\n         \"\"\"Test retrieving the value of a variable as a numpy array.\"\"\"\n@@ -373,7 +372,18 @@ class VariableDtypeShapeNdimRepr(test_case.TestCase):\n         \"\"\"Test the string representation of a variable.\"\"\"\n         v = backend.Variable(initializer=np.array([1, 2, 3]), name=\"test_var\")\n         expected_repr = (\n-            \"<KerasVariable shape=(3,), dtype=float32, path=test_var>\"\n+            \"<Variable path=test_var, shape=(3,), dtype=float32, \"\n+            \"value=[1. 2. 3.]>\"\n+        )\n+        self.assertEqual(repr(v), expected_repr)\n+\n+        # Test with `backend.StatelessScope()`\n+        with backend.StatelessScope():\n+            v = backend.Variable(\n+                initializer=\"zeros\", shape=(3,), name=\"test_var\"\n+            )\n+            expected_repr = (\n+                \"<Variable path=test_var, shape=(3,), dtype=float32>\"\n             )\n             self.assertEqual(repr(v), expected_repr)\n \n@@ -408,7 +418,7 @@ class VariableDtypeShapeNdimRepr(test_case.TestCase):\n \n \n class VariableOpsCorrectnessTest(test_case.TestCase):\n-    \"\"\"Tests for operations on KerasVariable.\"\"\"\n+    \"\"\"Tests for operations on Variable.\"\"\"\n \n     def test_int(self):\n         v = backend.Variable(initializer=np.array(-1.1))\n\n@@ -940,7 +940,7 @@ class JAXTrainer(base_trainer.Trainer):\n \n         During JAX training, since the training function are stateless, we have\n         to pass in and get the model weights over and over, during which the\n-        copy of the weights that attached to the KerasVariable are still and\n+        copy of the weights that attached to the Variable are still and\n         occupying extra memory. We remove those variable to save memory (for\n         better memory utilization) at the beginning of the epoch, and reattach\n         the value back to variables at the end of the epoch, via\n\n@@ -12,7 +12,6 @@ import warnings\n import tensorflow as tf\n \n from keras.src import backend\n-from keras.src.backend.common import KerasVariable\n from keras.src.backend.tensorflow.trackable import KerasAutoTrackable\n from keras.src.optimizers import base_optimizer\n \n@@ -46,7 +45,7 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n         )\n \n     def assign(self, variable, value):\n-        if isinstance(variable, KerasVariable):\n+        if isinstance(variable, backend.Variable):\n             variable = variable.value\n         value = tf.cast(value, variable.dtype)\n         if isinstance(value, tf.IndexedSlices):\n@@ -55,7 +54,7 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n             variable.assign(value)\n \n     def assign_add(self, variable, value):\n-        if isinstance(variable, KerasVariable):\n+        if isinstance(variable, backend.Variable):\n             variable = variable.value\n         value = tf.cast(value, variable.dtype)\n         if isinstance(value, tf.IndexedSlices):\n@@ -64,7 +63,7 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n             variable.assign_add(value)\n \n     def assign_sub(self, variable, value):\n-        if isinstance(variable, KerasVariable):\n+        if isinstance(variable, backend.Variable):\n             variable = variable.value\n         value = tf.cast(value, variable.dtype)\n         if isinstance(value, tf.IndexedSlices):\n\n@@ -118,13 +118,11 @@ class Variable(KerasVariable):\n     # Overload native accessor.\n     @classmethod\n     def __torch_function__(cls, func, types, args=(), kwargs=None):\n-        args = [\n-            arg.value if isinstance(arg, KerasVariable) else arg for arg in args\n-        ]\n+        args = [arg.value if isinstance(arg, Variable) else arg for arg in args]\n         if kwargs is None:\n             kwargs = {}\n         kwargs = {\n-            key: value.value if isinstance(value, KerasVariable) else value\n+            key: value.value if isinstance(value, Variable) else value\n             for key, value in kwargs.items()\n         }\n         return func(*args, **kwargs)\n@@ -275,7 +273,7 @@ def shape(x):\n \n def cast(x, dtype):\n     dtype = to_torch_dtype(dtype)\n-    if isinstance(x, KerasVariable):\n+    if isinstance(x, Variable):\n         x = x.value\n     if is_tensor(x):\n         if x.dtype == dtype:\n@@ -644,7 +642,7 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n \n def stop_gradient(variable):\n-    if isinstance(variable, KerasVariable):\n+    if isinstance(variable, Variable):\n         variable = variable.value\n     # We can't use `.requires_grad_(False)` here since it only\n     # works when the tensor is a leaf node in the graph.\n\n@@ -308,7 +308,7 @@ class Distribution:\n         \"\"\"Retrieve the `TensorLayout` for the variable.\n \n         Args:\n-            variable: A `KerasVariable` instance.\n+            variable: A `Variable` instance.\n \n         return:\n             The `TensorLayout` for the variable, which can be used by\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b6d305f82957ef57089298ebbcb01920fea4bf8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 35 | Churn Cumulative: 343 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 0.75\n\nDIFF:\n@@ -298,7 +298,7 @@ def _clone_sequential_model(model, clone_function, input_tensors=None):\n         input_dtype = None\n         input_batch_shape = None\n \n-    if input_tensors:\n+    if input_tensors is not None:\n         if isinstance(input_tensors, (list, tuple)):\n             if len(input_tensors) != 1:\n                 raise ValueError(\n@@ -310,7 +310,12 @@ def _clone_sequential_model(model, clone_function, input_tensors=None):\n                 \"Argument `input_tensors` must be a KerasTensor. \"\n                 f\"Received invalid value: input_tensors={input_tensors}\"\n             )\n-        inputs = Input(tensor=input_tensors, name=input_name)\n+        inputs = Input(\n+            tensor=input_tensors,\n+            batch_shape=input_tensors.shape,\n+            dtype=input_tensors.dtype,\n+            name=input_name,\n+        )\n         new_layers = [inputs] + new_layers\n     else:\n         if input_batch_shape is not None:\n\n@@ -61,6 +61,15 @@ def get_sequential_model(explicit_input=True):\n     return model\n \n \n+def get_cnn_sequential_model(explicit_input=True):\n+    model = models.Sequential()\n+    if explicit_input:\n+        model.add(layers.Input(shape=(7, 3)))\n+    model.add(layers.Conv1D(2, 2, padding=\"same\"))\n+    model.add(layers.Conv1D(2, 2, padding=\"same\"))\n+    return model\n+\n+\n def get_subclassed_model():\n     class ExampleModel(models.Model):\n         def __init__(self, **kwargs):\n@@ -124,6 +133,23 @@ class CloneModelTest(testing.TestCase):\n             if not isinstance(l1, layers.InputLayer):\n                 self.assertEqual(l2.name, l1.name + \"_custom\")\n \n+    @parameterized.named_parameters(\n+        (\"cnn_functional\", get_cnn_functional_model),\n+        (\"cnn_sequential\", get_cnn_sequential_model),\n+        (\n+            \"cnn_sequential_noinputlayer\",\n+            lambda: get_cnn_sequential_model(explicit_input=False),\n+        ),\n+    )\n+    def test_input_tensors(self, model_fn):\n+        ref_input = np.random.random((2, 7, 3))\n+        model = model_fn()\n+        model(ref_input)  # Maybe needed to get model inputs if no Input layer\n+        input_tensor = model.inputs[0]\n+        new_model = clone_model(model, input_tensors=input_tensor)\n+        tree.assert_same_structure(model.inputs, new_model.inputs)\n+        tree.assert_same_structure(model.outputs, new_model.outputs)\n+\n     def test_shared_layers_cloning(self):\n         model = get_mlp_functional_model(shared_layers=True)\n         new_model = clone_model(model)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ca4345afa17543e2979f2bcef9e6847f4402123", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 119 | Lines Deleted: 42 | Files Changed: 3 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): 14/13 | Churn Δ: 161 | Churn Cumulative: 395 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: 0.5342465753424658\n\nDIFF:\n@@ -22,6 +22,7 @@ class InputLayer(Layer):\n     ):\n         # TODO: support for ragged.\n         super().__init__(name=name)\n+\n         if \"input_shape\" in kwargs:\n             warnings.warn(\n                 \"Argument `input_shape` is deprecated. Use `shape` instead.\"\n@@ -30,32 +31,6 @@ class InputLayer(Layer):\n         if \"batch_input_shape\" in kwargs:\n             batch_shape = kwargs.pop(\"batch_input_shape\")\n \n-        if shape is not None and batch_shape is not None:\n-            raise ValueError(\n-                \"You cannot pass both `shape` and `batch_shape` at the \"\n-                \"same time.\"\n-            )\n-        if batch_size is not None and batch_shape is not None:\n-            raise ValueError(\n-                \"You cannot pass both `batch_size` and `batch_shape` at the \"\n-                \"same time.\"\n-            )\n-        if shape is None and batch_shape is None:\n-            raise ValueError(\"You must pass a `shape` argument.\")\n-\n-        if shape is not None:\n-            shape = backend.standardize_shape(shape)\n-            batch_shape = (batch_size,) + shape\n-        self._batch_shape = backend.standardize_shape(batch_shape)\n-        self._dtype = backend.standardize_dtype(dtype)\n-\n-        self.sparse = bool(sparse)\n-        if self.sparse and not backend.SUPPORTS_SPARSE_TENSORS:\n-            raise ValueError(\n-                \"`sparse=True` is not supported with backend: \"\n-                f\"{backend.backend()}\"\n-            )\n-\n         if input_tensor is not None:\n             if not isinstance(input_tensor, backend.KerasTensor):\n                 raise ValueError(\n@@ -63,7 +38,69 @@ class InputLayer(Layer):\n                     f\"Received invalid type: input_tensor={input_tensor} \"\n                     f\"(of type {type(input_tensor)})\"\n                 )\n+            if batch_size is not None:\n+                if (\n+                    len(input_tensor.shape) < 1\n+                    or input_tensor.shape[0] != batch_size\n+                ):\n+                    raise ValueError(\n+                        \"When providing the `input_tensor` argument, you \"\n+                        \"cannot provide an incompatible `batch_size` argument.\"\n+                    )\n+            if shape is not None:\n+                if (\n+                    len(shape) != len(input_tensor.shape) - 1\n+                    or shape != input_tensor.shape[1:]\n+                ):\n+                    raise ValueError(\n+                        \"When providing the `input_tensor` argument, you \"\n+                        \"cannot provide an incompatible `shape` argument.\"\n+                    )\n+            if batch_shape is not None and batch_shape != input_tensor.shape:\n+                raise ValueError(\n+                    \"When providing the `input_tensor` argument, you \"\n+                    \"cannot provide an incompatible `batch_shape` argument.\"\n+                )\n+            if dtype is not None and input_tensor.dtype != dtype:\n+                raise ValueError(\n+                    \"When providing the `input_tensor` argument, you \"\n+                    \"cannot provide an incompatible `dtype` argument.\"\n+                )\n+            if sparse is not None and input_tensor.sparse != sparse:\n+                raise ValueError(\n+                    \"When providing the `input_tensor` argument, you \"\n+                    \"cannot provide an incompatible `sparse` argument.\"\n+                )\n+            batch_shape = input_tensor.shape\n+            dtype = input_tensor.dtype\n+            sparse = input_tensor.sparse\n         else:\n+            if shape is not None and batch_shape is not None:\n+                raise ValueError(\n+                    \"You cannot pass both `shape` and `batch_shape` at the \"\n+                    \"same time.\"\n+                )\n+            if batch_size is not None and batch_shape is not None:\n+                raise ValueError(\n+                    \"You cannot pass both `batch_size` and `batch_shape` \"\n+                    \"at the same time.\"\n+                )\n+            if shape is None and batch_shape is None:\n+                raise ValueError(\"You must pass a `shape` argument.\")\n+\n+            if shape is not None:\n+                shape = backend.standardize_shape(shape)\n+                batch_shape = (batch_size,) + shape\n+\n+        self._batch_shape = backend.standardize_shape(batch_shape)\n+        self._dtype = backend.standardize_dtype(dtype)\n+        self.sparse = bool(sparse)\n+        if self.sparse and not backend.SUPPORTS_SPARSE_TENSORS:\n+            raise ValueError(\n+                \"`sparse=True` is not supported with backend: \"\n+                f\"{backend.backend()}\"\n+            )\n+        if input_tensor is None:\n             input_tensor = backend.KerasTensor(\n                 shape=batch_shape, dtype=dtype, sparse=sparse, name=name\n             )\n\n@@ -89,25 +89,20 @@ class InputLayerTest(testing.TestCase):\n     # Testing happy path for layer with input tensor\n     def testing_input_tensor(self):\n         input_shape = (2, 3)\n-        batch_size = 4\n         dtype = \"float32\"\n         input_tensor = KerasTensor(shape=input_shape, dtype=dtype)\n \n-        values = InputLayer(\n-            shape=input_shape,\n-            batch_size=batch_size,\n+        layer = InputLayer(\n             input_tensor=input_tensor,\n-            dtype=dtype,\n         )\n \n-        self.assertEqual(values.dtype, dtype)\n-        self.assertEqual(values.batch_shape[0], batch_size)\n-        self.assertEqual(values.batch_shape[1:], input_shape)\n-        self.assertEqual(values.trainable, True)\n-        self.assertIsInstance(values.output, KerasTensor)\n-        self.assertEqual(values.output, input_tensor)\n-        self.assertEqual(values.output.ndim, input_tensor.ndim)\n-        self.assertEqual(values.output.dtype, dtype)\n+        self.assertEqual(layer.dtype, dtype)\n+        self.assertEqual(layer.batch_shape, (2, 3))\n+        self.assertEqual(layer.trainable, True)\n+        self.assertIsInstance(layer.output, KerasTensor)\n+        self.assertEqual(layer.output, input_tensor)\n+        self.assertEqual(layer.output.ndim, input_tensor.ndim)\n+        self.assertEqual(layer.output.dtype, dtype)\n \n     def test_input_shape_deprecated(self):\n         input_shape = (2, 3)\n@@ -135,3 +130,51 @@ class InputLayerTest(testing.TestCase):\n     def test_numpy_shape(self):\n         # non-python int type shapes should be ok\n         InputLayer(shape=(np.int64(32),))\n+\n+    def test_invalid_arg_combinations(self):\n+        input_tensor = KerasTensor(shape=(2, 3), dtype=\"float32\")\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"cannot provide an incompatible `shape`\"\n+        ):\n+            _ = InputLayer(\n+                shape=(2, 4),\n+                input_tensor=input_tensor,\n+            )\n+        with self.assertRaisesRegex(\n+            ValueError, \"cannot provide an incompatible `batch_shape`\"\n+        ):\n+            _ = InputLayer(\n+                batch_shape=(2, 4),\n+                input_tensor=input_tensor,\n+            )\n+        with self.assertRaisesRegex(\n+            ValueError, \"cannot provide an incompatible `batch_size`\"\n+        ):\n+            _ = InputLayer(\n+                batch_size=5,\n+                input_tensor=input_tensor,\n+            )\n+        with self.assertRaisesRegex(\n+            ValueError, \"cannot provide an incompatible `dtype`\"\n+        ):\n+            _ = InputLayer(\n+                dtype=\"float16\",\n+                input_tensor=input_tensor,\n+            )\n+        with self.assertRaisesRegex(\n+            ValueError, \"cannot provide an incompatible `sparse`\"\n+        ):\n+            _ = InputLayer(\n+                sparse=True,\n+                input_tensor=input_tensor,\n+            )\n+\n+        # This works\n+        _ = InputLayer(\n+            shape=(3,),\n+            batch_size=2,\n+            sparse=False,\n+            dtype=\"float32\",\n+            input_tensor=input_tensor,\n+        )\n\n@@ -312,15 +312,12 @@ def _clone_sequential_model(model, clone_function, input_tensors=None):\n             )\n         inputs = Input(\n             tensor=input_tensors,\n-            batch_shape=input_tensors.shape,\n-            dtype=input_tensors.dtype,\n             name=input_name,\n         )\n         new_layers = [inputs] + new_layers\n     else:\n         if input_batch_shape is not None:\n             inputs = Input(\n-                tensor=input_tensors,\n                 batch_shape=input_batch_shape,\n                 dtype=input_dtype,\n                 name=input_name,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
