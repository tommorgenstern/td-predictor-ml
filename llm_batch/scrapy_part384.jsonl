{"custom_id": "scrapy#d13219062500eae1a6d5330ceea3502590cd89cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 362 | Contributors (this commit): 17 | Commits (past 90d): 3 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -74,4 +74,4 @@ class Debugger:\n \n     def _enter_debugger(self, signum: int, frame: Optional[FrameType]) -> None:\n         assert frame\n-        Pdb().set_trace(frame.f_back)\n+        Pdb().set_trace(frame.f_back)  # noqa: T100\n\n@@ -10,10 +10,10 @@ def _embed_ipython_shell(\n ) -> EmbedFuncT:\n     \"\"\"Start an IPython Shell\"\"\"\n     try:\n-        from IPython.terminal.embed import InteractiveShellEmbed\n+        from IPython.terminal.embed import InteractiveShellEmbed  # noqa: T100\n         from IPython.terminal.ipapp import load_default_config\n     except ImportError:\n-        from IPython.frontend.terminal.embed import (  # type: ignore[no-redef]\n+        from IPython.frontend.terminal.embed import (  # type: ignore[no-redef]  # noqa: T100\n             InteractiveShellEmbed,\n         )\n         from IPython.frontend.terminal.ipapp import (  # type: ignore[no-redef]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c70d3e60555084b4bec9dfd794adb93b24b2171", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 23 | Files Changed: 7 | Hunks: 20 | Methods Changed: 20 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 44 | Churn Cumulative: 18005 | Contributors (this commit): 77 | Commits (past 90d): 13 | Contributors (cumulative): 186 | DMM Complexity: 0.0\n\nDIFF:\n@@ -104,7 +104,7 @@ class ItemFilter:\n                 for item_class in feed_options.get(\"item_classes\") or ()\n             )\n         else:\n-            self.item_classes = tuple()\n+            self.item_classes = ()\n \n     def accepts(self, item: Any) -> bool:\n         \"\"\"\n\n@@ -200,7 +200,7 @@ def get_permissions_dict(\n \n     path_obj = Path(path)\n \n-    renamings = renamings or tuple()\n+    renamings = renamings or ()\n     permissions_dict = {\n         \".\": get_permissions(path_obj),\n     }\n\n@@ -1356,7 +1356,7 @@ class FeedExportTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_export_encoding(self):\n-        items = [dict({\"foo\": \"Test\\xd6\"})]\n+        items = [{\"foo\": \"Test\\xd6\"}]\n \n         formats = {\n             \"json\": b'[{\"foo\": \"Test\\\\u00d6\"}]',\n@@ -1401,7 +1401,7 @@ class FeedExportTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_export_multiple_configs(self):\n-        items = [dict({\"foo\": \"FOO\", \"bar\": \"BAR\"})]\n+        items = [{\"foo\": \"FOO\", \"bar\": \"BAR\"}]\n \n         formats = {\n             \"json\": b'[\\n{\"bar\": \"BAR\"}\\n]',\n@@ -2513,8 +2513,8 @@ class BatchDeliveriesTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_export_multiple_configs(self):\n         items = [\n-            dict({\"foo\": \"FOO\", \"bar\": \"BAR\"}),\n-            dict({\"foo\": \"FOO1\", \"bar\": \"BAR1\"}),\n+            {\"foo\": \"FOO\", \"bar\": \"BAR\"},\n+            {\"foo\": \"FOO1\", \"bar\": \"BAR1\"},\n         ]\n \n         formats = {\n@@ -2574,7 +2574,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_batch_item_count_feeds_setting(self):\n-        items = [dict({\"foo\": \"FOO\"}), dict({\"foo\": \"FOO1\"})]\n+        items = [{\"foo\": \"FOO\"}, {\"foo\": \"FOO1\"}]\n         formats = {\n             \"json\": [\n                 b'[{\"foo\": \"FOO\"}]',\n\n@@ -156,7 +156,7 @@ class InitializationTestMixin:\n         self.assertEqual(il.get_output_value(\"name\"), [\"foo\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(ItemAdapter(loaded_item).asdict(), dict({\"name\": [\"foo\"]}))\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\"]})\n \n     def test_get_output_value_list(self):\n         \"\"\"Getting output value must not remove value from item\"\"\"\n@@ -165,9 +165,7 @@ class InitializationTestMixin:\n         self.assertEqual(il.get_output_value(\"name\"), [\"foo\", \"bar\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(\n-            ItemAdapter(loaded_item).asdict(), dict({\"name\": [\"foo\", \"bar\"]})\n-        )\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\", \"bar\"]})\n \n     def test_values_single(self):\n         \"\"\"Values from initial item must be added to loader._values\"\"\"\n\n@@ -526,7 +526,7 @@ class InitializationFromDictTest(unittest.TestCase):\n         self.assertEqual(il.get_output_value(\"name\"), [\"foo\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(loaded_item, dict({\"name\": [\"foo\"]}))\n+        self.assertEqual(loaded_item, {\"name\": [\"foo\"]})\n \n     def test_get_output_value_list(self):\n         \"\"\"Getting output value must not remove value from item\"\"\"\n@@ -535,7 +535,7 @@ class InitializationFromDictTest(unittest.TestCase):\n         self.assertEqual(il.get_output_value(\"name\"), [\"foo\", \"bar\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(loaded_item, dict({\"name\": [\"foo\", \"bar\"]}))\n+        self.assertEqual(loaded_item, {\"name\": [\"foo\", \"bar\"]})\n \n     def test_values_single(self):\n         \"\"\"Values from initial item must be added to loader._values\"\"\"\n\n@@ -284,7 +284,7 @@ class DownloaderAwareSchedulerTestMixin:\n             downloader.decrement(slot)\n \n         self.assertTrue(\n-            _is_scheduling_fair(list(s for u, s in _URLS_WITH_SLOTS), dequeued_slots)\n+            _is_scheduling_fair([s for u, s in _URLS_WITH_SLOTS], dequeued_slots)\n         )\n         self.assertEqual(sum(len(s.active) for s in downloader.slots.values()), 0)\n \n\n@@ -244,7 +244,7 @@ class CrawlSpiderTest(SpiderTest):\n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n-        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertTrue(all(isinstance(r, Request) for r in output))\n         self.assertEqual(\n             [r.url for r in output],\n             [\n@@ -270,7 +270,7 @@ class CrawlSpiderTest(SpiderTest):\n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n-        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertTrue(all(isinstance(r, Request) for r in output))\n         self.assertEqual(\n             [r.url for r in output],\n             [\n@@ -299,7 +299,7 @@ class CrawlSpiderTest(SpiderTest):\n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 2)\n-        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertTrue(all(isinstance(r, Request) for r in output))\n         self.assertEqual(\n             [r.url for r in output],\n             [\n@@ -324,7 +324,7 @@ class CrawlSpiderTest(SpiderTest):\n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n-        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertTrue(all(isinstance(r, Request) for r in output))\n         self.assertEqual(\n             [r.url for r in output],\n             [\n@@ -352,7 +352,7 @@ class CrawlSpiderTest(SpiderTest):\n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n-        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertTrue(all(isinstance(r, Request) for r in output))\n         self.assertEqual(\n             [r.url for r in output],\n             [\n@@ -383,7 +383,7 @@ class CrawlSpiderTest(SpiderTest):\n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n-        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertTrue(all(isinstance(r, Request) for r in output))\n         self.assertEqual(\n             [r.url for r in output],\n             [\n@@ -413,7 +413,7 @@ class CrawlSpiderTest(SpiderTest):\n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n-        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertTrue(all(isinstance(r, Request) for r in output))\n         self.assertEqual(\n             [r.url for r in output],\n             [\n@@ -445,7 +445,7 @@ class CrawlSpiderTest(SpiderTest):\n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n-        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertTrue(all(isinstance(r, Request) for r in output))\n         self.assertEqual(\n             [r.url for r in output],\n             [\n@@ -637,7 +637,7 @@ Sitemap: /sitemap-relative-url.xml\n         class FilteredSitemapSpider(self.spider_class):\n             def sitemap_filter(self, entries):\n                 for entry in entries:\n-                    alternate_links = entry.get(\"alternate\", tuple())\n+                    alternate_links = entry.get(\"alternate\", ())\n                     for link in alternate_links:\n                         if \"/deutsch/\" in link:\n                             entry[\"loc\"] = link\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ef9c337cad36ac6c80eab86622f8ae9fc8d1075", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 2596 | Contributors (this commit): 23 | Commits (past 90d): 2 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -146,7 +146,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n         case_insensitive_dupefilter.close(\"finished\")\n \n     def test_seenreq_newlines(self):\n-        \"\"\"Checks against adding duplicate \\r to\n+        r\"\"\"Checks against adding duplicate \\r to\n         line endings on Windows platforms.\"\"\"\n \n         r1 = Request(\"http://scrapytest.org/1\")\n\n@@ -186,7 +186,7 @@ class Base:\n             )\n \n         def test_nofollow(self):\n-            '''Test the extractor's behaviour for links with rel=\"nofollow\"'''\n+            \"\"\"Test the extractor's behaviour for links with rel='nofollow'\"\"\"\n \n             html = b\"\"\"<html><head><title>Page title<title>\n             <body>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d8dbd5648406227c9b96736da62046b90c554e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 11 | Files Changed: 10 | Hunks: 11 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 10768 | Contributors (this commit): 83 | Commits (past 90d): 28 | Contributors (cumulative): 197 | DMM Complexity: 1.0\n\nDIFF:\n@@ -234,7 +234,7 @@ class MediaPipeline(ABC):\n             # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).\n             context = getattr(result.value, \"__context__\", None)\n             if isinstance(context, StopIteration):\n-                setattr(result.value, \"__context__\", None)\n+                result.value.__context__ = None\n \n         info.downloading.remove(fp)\n         info.downloaded[fp] = result  # cache result\n\n@@ -407,7 +407,7 @@ def maybeDeferred_coro(\n     \"\"\"Copy of defer.maybeDeferred that also converts coroutines to Deferreds.\"\"\"\n     try:\n         result = f(*args, **kw)\n-    except:  # noqa: E722\n+    except:  # noqa: E722,B001\n         return defer.fail(failure.Failure(captureVars=Deferred.debug))\n \n     if isinstance(result, Deferred):\n\n@@ -269,7 +269,7 @@ def get_spec(func: Callable[..., Any]) -> Tuple[List[str], Dict[str, Any]]:\n \n     if inspect.isfunction(func) or inspect.ismethod(func):\n         spec = inspect.getfullargspec(func)\n-    elif hasattr(func, \"__call__\"):\n+    elif hasattr(func, \"__call__\"):  # noqa: B004\n         spec = inspect.getfullargspec(func.__call__)\n     else:\n         raise TypeError(f\"{type(func)} is not callable\")\n\n@@ -100,7 +100,10 @@ def send_catch_log_deferred(\n         d.addErrback(logerror, receiver)\n         # TODO https://pylint.readthedocs.io/en/latest/user_guide/messages/warning/cell-var-from-loop.html\n         d.addBoth(\n-            lambda result: (receiver, result)  # pylint: disable=cell-var-from-loop\n+            lambda result: (\n+                receiver,  # pylint: disable=cell-var-from-loop  # noqa: B023\n+                result,\n+            )\n         )\n         dfds.append(d)\n     d = DeferredList(dfds)\n\n@@ -20,7 +20,7 @@ class CmdlineTest(unittest.TestCase):\n         self.env[\"SCRAPY_SETTINGS_MODULE\"] = \"tests.test_cmdline.settings\"\n \n     def _execute(self, *new_args, **kwargs):\n-        encoding = getattr(sys.stdout, \"encoding\") or \"utf-8\"\n+        encoding = sys.stdout.encoding or \"utf-8\"\n         args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n         comm = proc.communicate()[0].strip()\n\n@@ -12,7 +12,7 @@ class VersionTest(ProcessTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_output(self):\n-        encoding = getattr(sys.stdout, \"encoding\") or \"utf-8\"\n+        encoding = sys.stdout.encoding or \"utf-8\"\n         _, out, _ = yield self.execute([])\n         self.assertEqual(\n             out.strip().decode(encoding),\n@@ -21,7 +21,7 @@ class VersionTest(ProcessTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_verbose_output(self):\n-        encoding = getattr(sys.stdout, \"encoding\") or \"utf-8\"\n+        encoding = sys.stdout.encoding or \"utf-8\"\n         _, out, _ = yield self.execute([\"-v\"])\n         headers = [\n             line.partition(\":\")[0].strip()\n\n@@ -101,7 +101,7 @@ class ProjectTest(unittest.TestCase):\n         def kill_proc():\n             p.kill()\n             p.communicate()\n-            assert False, \"Command took too much time to complete\"\n+            raise AssertionError(\"Command took too much time to complete\")\n \n         timer = Timer(15, kill_proc)\n         try:\n\n@@ -892,7 +892,7 @@ class S3TestCase(unittest.TestCase):\n         except Exception as e:\n             self.assertIsInstance(e, (TypeError, NotConfigured))\n         else:\n-            assert False\n+            raise AssertionError()\n \n     def test_request_signing1(self):\n         # gets an object from the johnsmith bucket.\n\n@@ -459,7 +459,7 @@ class EngineTest(unittest.TestCase):\n         def kill_proc():\n             p.kill()\n             p.communicate()\n-            assert False, \"Command took too much time to complete\"\n+            raise AssertionError(\"Command took too much time to complete\")\n \n         timer = Timer(15, kill_proc)\n         try:\n\n@@ -147,7 +147,7 @@ class RequestSerializationTest(unittest.TestCase):\n \n         spider = MySpider()\n         r = Request(\"http://www.example.com\", callback=spider.parse)\n-        setattr(spider, \"parse\", None)\n+        spider.parse = None\n         self.assertRaises(ValueError, r.to_dict, spider=spider)\n \n     def test_callback_not_available(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d08f559600f0bb45b916be158a06e033753d45f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 548 | Lines Deleted: 291 | Files Changed: 102 | Hunks: 362 | Methods Changed: 21 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 839 | Churn Cumulative: 59904 | Contributors (this commit): 265 | Commits (past 90d): 249 | Contributors (cumulative): 1445 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,13 +1,16 @@\n+from __future__ import annotations\n+\n import logging\n from typing import TYPE_CHECKING, Any, List\n \n from scrapy.exceptions import NotConfigured\n-from scrapy.settings import Settings\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n+    from scrapy.settings import Settings\n+\n \n logger = logging.getLogger(__name__)\n \n@@ -15,8 +18,8 @@ logger = logging.getLogger(__name__)\n class AddonManager:\n     \"\"\"This class facilitates loading and storing :ref:`topics-addons`.\"\"\"\n \n-    def __init__(self, crawler: \"Crawler\") -> None:\n-        self.crawler: \"Crawler\" = crawler\n+    def __init__(self, crawler: Crawler) -> None:\n+        self.crawler: Crawler = crawler\n         self.addons: List[Any] = []\n \n     def load_settings(self, settings: Settings) -> None:\n\n@@ -12,7 +12,6 @@ import scrapy\n from scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter\n from scrapy.crawler import CrawlerProcess\n from scrapy.exceptions import UsageError\n-from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import get_project_settings, inside_project\n from scrapy.utils.python import garbage_collect\n@@ -21,6 +20,8 @@ if TYPE_CHECKING:\n     # typing.ParamSpec requires Python 3.10\n     from typing_extensions import ParamSpec\n \n+    from scrapy.settings import BaseSettings, Settings\n+\n     _P = ParamSpec(\"_P\")\n \n \n\n@@ -2,18 +2,22 @@\n Base class for Scrapy commands\n \"\"\"\n \n+from __future__ import annotations\n+\n import argparse\n import builtins\n import os\n from pathlib import Path\n-from typing import Any, Dict, Iterable, List, Optional\n+from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional\n \n from twisted.python import failure\n \n-from scrapy.crawler import Crawler, CrawlerProcess\n from scrapy.exceptions import UsageError\n from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n \n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler, CrawlerProcess\n+\n \n class ScrapyCommand:\n     requires_project: bool = False\n\n@@ -1,16 +1,20 @@\n+from __future__ import annotations\n+\n import argparse\n import subprocess  # nosec\n import sys\n import time\n-from typing import Any, Iterable, List\n+from typing import TYPE_CHECKING, Any, Iterable, List\n from urllib.parse import urlencode\n \n import scrapy\n-from scrapy import Request\n from scrapy.commands import ScrapyCommand\n from scrapy.http import Response, TextResponse\n from scrapy.linkextractors import LinkExtractor\n \n+if TYPE_CHECKING:\n+    from scrapy import Request\n+\n \n class Command(ScrapyCommand):\n     default_settings = {\n\n@@ -1,11 +1,15 @@\n-import argparse\n-from typing import List, cast\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, List, cast\n \n from twisted.python.failure import Failure\n \n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n \n+if TYPE_CHECKING:\n+    import argparse\n+\n \n class Command(BaseRunSpiderCommand):\n     requires_project = True\n\n@@ -1,6 +1,7 @@\n+from __future__ import annotations\n+\n import sys\n-from argparse import ArgumentParser, Namespace\n-from typing import Dict, List, Type\n+from typing import TYPE_CHECKING, Dict, List, Type\n \n from w3lib.url import is_url\n \n@@ -11,6 +12,9 @@ from scrapy.http import Request, Response\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.spider import DefaultSpider, spidercls_for_request\n \n+if TYPE_CHECKING:\n+    from argparse import ArgumentParser, Namespace\n+\n \n class Command(ScrapyCommand):\n     requires_project = False\n\n@@ -1,8 +1,12 @@\n-import argparse\n-from typing import List\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, List\n \n from scrapy.commands import ScrapyCommand\n \n+if TYPE_CHECKING:\n+    import argparse\n+\n \n class Command(ScrapyCommand):\n     requires_project = True\n\n@@ -6,6 +6,7 @@ import inspect\n import json\n import logging\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     AsyncGenerator,\n     Callable,\n@@ -22,13 +23,11 @@ from typing import (\n \n from itemadapter import ItemAdapter, is_item\n from twisted.internet.defer import Deferred, maybeDeferred\n-from twisted.python.failure import Failure\n from w3lib.url import is_url\n \n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n from scrapy.http import Request, Response\n-from scrapy.spiders import Spider\n from scrapy.utils import display\n from scrapy.utils.asyncgen import collect_asyncgen\n from scrapy.utils.defer import aiter_errback, deferred_from_coro\n@@ -36,6 +35,12 @@ from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.spider import spidercls_for_request\n \n+if TYPE_CHECKING:\n+    from twisted.python.failure import Failure\n+\n+    from scrapy.spiders import Spider\n+\n+\n logger = logging.getLogger(__name__)\n \n _T = TypeVar(\"_T\")\n\n@@ -1,17 +1,21 @@\n+from __future__ import annotations\n+\n import argparse\n import sys\n from importlib import import_module\n-from os import PathLike\n from pathlib import Path\n-from types import ModuleType\n-from typing import List, Union\n+from typing import TYPE_CHECKING, List, Union\n \n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.spider import iter_spider_classes\n \n+if TYPE_CHECKING:\n+    from os import PathLike\n+    from types import ModuleType\n \n-def _import_file(filepath: Union[str, PathLike]) -> ModuleType:\n+\n+def _import_file(filepath: Union[str, PathLike[str]]) -> ModuleType:\n     abspath = Path(filepath).resolve()\n     if abspath.suffix not in (\".py\", \".pyw\"):\n         raise ValueError(f\"Not a Python source file: {abspath}\")\n\n@@ -4,9 +4,10 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n-from argparse import ArgumentParser, Namespace\n+from __future__ import annotations\n+\n from threading import Thread\n-from typing import Any, Dict, List, Type\n+from typing import TYPE_CHECKING, Any, Dict, List, Type\n \n from scrapy import Spider\n from scrapy.commands import ScrapyCommand\n@@ -15,6 +16,9 @@ from scrapy.shell import Shell\n from scrapy.utils.spider import DefaultSpider, spidercls_for_request\n from scrapy.utils.url import guess_scheme\n \n+if TYPE_CHECKING:\n+    from argparse import ArgumentParser, Namespace\n+\n \n class Command(ScrapyCommand):\n     requires_project = False\n\n@@ -1,9 +1,12 @@\n+from __future__ import annotations\n+\n import re\n import sys\n from functools import wraps\n from inspect import getmembers\n from types import CoroutineType\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     AsyncGenerator,\n     Callable,\n@@ -16,13 +19,15 @@ from typing import (\n )\n from unittest import TestCase, TestResult\n \n-from twisted.python.failure import Failure\n-\n-from scrapy import Spider\n from scrapy.http import Request, Response\n from scrapy.utils.python import get_spec\n from scrapy.utils.spider import iterate_spider_output\n \n+if TYPE_CHECKING:\n+    from twisted.python.failure import Failure\n+\n+    from scrapy import Spider\n+\n \n class Contract:\n     \"\"\"Abstract class for contracts\"\"\"\n\n@@ -25,15 +25,16 @@ from scrapy import Request, Spider, signals\n from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.http import Response\n from scrapy.resolver import dnscache\n-from scrapy.settings import BaseSettings\n from scrapy.signalmanager import SignalManager\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.httpobj import urlparse_cached\n \n if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+    from scrapy.settings import BaseSettings\n+\n \n _T = TypeVar(\"_T\")\n \n\n@@ -21,8 +21,6 @@ from scrapy.core.downloader.tls import (\n     ScrapyClientTLSOptions,\n     openssl_methods,\n )\n-from scrapy.crawler import Crawler\n-from scrapy.settings import BaseSettings\n from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n@@ -31,6 +29,9 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.settings import BaseSettings\n+\n \n @implementer(IPolicyForHTTPS)\n class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n\n@@ -17,17 +17,19 @@ from typing import (\n )\n \n from twisted.internet import defer\n-from twisted.internet.defer import Deferred\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured, NotSupported\n-from scrapy.http import Response\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import without_none_values\n \n if TYPE_CHECKING:\n+    from twisted.internet.defer import Deferred\n+\n     from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+\n \n logger = logging.getLogger(__name__)\n \n\n@@ -1,12 +1,16 @@\n-from typing import Any, Dict\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, Dict\n \n from w3lib.url import parse_data_uri\n \n-from scrapy import Request, Spider\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.decorators import defers\n \n+if TYPE_CHECKING:\n+    from scrapy import Request, Spider\n+\n \n class DataURIDownloadHandler:\n     lazy = False\n\n@@ -1,12 +1,17 @@\n+from __future__ import annotations\n+\n from pathlib import Path\n+from typing import TYPE_CHECKING\n \n from w3lib.url import file_uri_to_path\n \n-from scrapy import Request, Spider\n-from scrapy.http import Response\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.decorators import defers\n \n+if TYPE_CHECKING:\n+    from scrapy import Request, Spider\n+    from scrapy.http import Response\n+\n \n class FileDownloadHandler:\n     lazy = False\n\n@@ -35,23 +35,25 @@ from io import BytesIO\n from typing import TYPE_CHECKING, Any, BinaryIO, Dict, Optional\n from urllib.parse import unquote\n \n-from twisted.internet.defer import Deferred\n from twisted.internet.protocol import ClientCreator, Protocol\n from twisted.protocols.ftp import CommandFailed, FTPClient\n-from twisted.python.failure import Failure\n \n-from scrapy import Request, Spider\n-from scrapy.crawler import Crawler\n from scrapy.http import Response\n from scrapy.responsetypes import responsetypes\n-from scrapy.settings import BaseSettings\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes\n \n if TYPE_CHECKING:\n+    from twisted.internet.defer import Deferred\n+    from twisted.python.failure import Failure\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request, Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.settings import BaseSettings\n+\n \n class ReceivedDataProtocol(Protocol):\n     def __init__(self, filename: Optional[str] = None):\n\n@@ -5,21 +5,21 @@ from __future__ import annotations\n \n from typing import TYPE_CHECKING, Type\n \n-from twisted.internet.defer import Deferred\n-\n-from scrapy import Request, Spider\n-from scrapy.crawler import Crawler\n-from scrapy.http import Response\n-from scrapy.settings import BaseSettings\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n+    from twisted.internet.defer import Deferred\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request, Spider\n     from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n     from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+    from scrapy.settings import BaseSettings\n \n \n class HTTP10DownloadHandler:\n\n@@ -12,11 +12,9 @@ from typing import TYPE_CHECKING, Any, List, Optional, Tuple, TypedDict, TypeVar\n from urllib.parse import urldefrag, urlunparse\n \n from twisted.internet import ssl\n-from twisted.internet.base import ReactorBase\n from twisted.internet.defer import CancelledError, Deferred, succeed\n from twisted.internet.endpoints import TCP4ClientEndpoint\n from twisted.internet.error import TimeoutError\n-from twisted.internet.interfaces import IConsumer\n from twisted.internet.protocol import Factory, Protocol, connectionDone\n from twisted.python.failure import Failure\n from twisted.web.client import URI, Agent, HTTPConnectionPool\n@@ -30,17 +28,22 @@ from zope.interface import implementer\n from scrapy import Request, Spider, signals\n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n from scrapy.core.downloader.webclient import _parse\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import StopDownload\n from scrapy.http import Headers, Response\n from scrapy.responsetypes import responsetypes\n-from scrapy.settings import BaseSettings\n from scrapy.utils.python import to_bytes, to_unicode\n \n if TYPE_CHECKING:\n+    from twisted.internet.base import ReactorBase\n+    from twisted.internet.interfaces import IConsumer\n+\n     # typing.NotRequired and typing.Self require Python 3.11\n     from typing_extensions import NotRequired, Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.settings import BaseSettings\n+\n+\n logger = logging.getLogger(__name__)\n \n _T = TypeVar(\"_T\")\n\n@@ -4,24 +4,26 @@ from time import time\n from typing import TYPE_CHECKING, Optional\n from urllib.parse import urldefrag\n \n-from twisted.internet.base import DelayedCall\n-from twisted.internet.defer import Deferred\n from twisted.internet.error import TimeoutError\n from twisted.web.client import URI\n-from twisted.web.iweb import IPolicyForHTTPS\n \n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n from scrapy.core.downloader.webclient import _parse\n from scrapy.core.http2.agent import H2Agent, H2ConnectionPool, ScrapyProxyH2Agent\n+from scrapy.utils.python import to_bytes\n+\n+if TYPE_CHECKING:\n+    from twisted.internet.base import DelayedCall\n+    from twisted.internet.defer import Deferred\n+    from twisted.web.iweb import IPolicyForHTTPS\n+\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n     from scrapy.crawler import Crawler\n     from scrapy.http import Request, Response\n     from scrapy.settings import Settings\n     from scrapy.spiders import Spider\n-from scrapy.utils.python import to_bytes\n-\n-if TYPE_CHECKING:\n-    # typing.Self requires Python 3.11\n-    from typing_extensions import Self\n \n \n class H2DownloadHandler:\n\n@@ -2,22 +2,23 @@ from __future__ import annotations\n \n from typing import TYPE_CHECKING, Any, Optional, Type\n \n-from twisted.internet.defer import Deferred\n-\n-from scrapy import Request, Spider\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import Response\n-from scrapy.settings import BaseSettings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import build_from_crawler\n \n if TYPE_CHECKING:\n+    from twisted.internet.defer import Deferred\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request, Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+    from scrapy.settings import BaseSettings\n+\n \n class S3DownloadHandler:\n     def __init__(\n\n@@ -6,19 +6,22 @@ See documentation in docs/topics/downloader-middleware.rst\n \n from __future__ import annotations\n \n-from typing import Any, Callable, Generator, List, Union, cast\n+from typing import TYPE_CHECKING, Any, Callable, Generator, List, Union, cast\n \n from twisted.internet.defer import Deferred, inlineCallbacks\n-from twisted.python.failure import Failure\n \n-from scrapy import Spider\n from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Request, Response\n from scrapy.middleware import MiddlewareManager\n-from scrapy.settings import BaseSettings\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import deferred_from_coro, mustbe_deferred\n \n+if TYPE_CHECKING:\n+    from twisted.python.failure import Failure\n+\n+    from scrapy import Spider\n+    from scrapy.settings import BaseSettings\n+\n \n class DownloaderMiddlewareManager(MiddlewareManager):\n     component_name = \"downloader middleware\"\n\n@@ -1,18 +1,22 @@\n+from __future__ import annotations\n+\n import re\n from time import time\n-from typing import Optional, Tuple\n+from typing import TYPE_CHECKING, Optional, Tuple\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n from twisted.internet import defer\n from twisted.internet.protocol import ClientFactory\n from twisted.web.http import HTTPClient\n \n-from scrapy import Request\n from scrapy.http import Headers, Response\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes, to_unicode\n \n+if TYPE_CHECKING:\n+    from scrapy import Request\n+\n \n def _parsed_url_args(parsed: ParseResult) -> Tuple[bytes, bytes, bytes, int, bytes]:\n     # Assume parsed is urlparse-d from Request.url,\n\n@@ -34,9 +34,8 @@ from scrapy.core.scraper import Scraper\n from scrapy.exceptions import CloseSpider, DontCloseSpider, IgnoreRequest\n from scrapy.http import Request, Response\n from scrapy.logformatter import LogFormatter\n-from scrapy.settings import BaseSettings, Settings\n+from scrapy.settings import Settings\n from scrapy.signalmanager import SignalManager\n-from scrapy.spiders import Spider\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import global_object_name\n@@ -46,6 +45,9 @@ if TYPE_CHECKING:\n     from scrapy.core.scheduler import BaseScheduler\n     from scrapy.core.scraper import _HandleOutputDeferred\n     from scrapy.crawler import Crawler\n+    from scrapy.settings import BaseSettings\n+    from scrapy.spiders import Spider\n+\n \n logger = logging.getLogger(__name__)\n \n\n@@ -1,10 +1,10 @@\n+from __future__ import annotations\n+\n from collections import deque\n-from typing import Deque, Dict, List, Optional, Tuple\n+from typing import TYPE_CHECKING, Deque, Dict, List, Optional, Tuple\n \n from twisted.internet import defer\n-from twisted.internet.base import ReactorBase\n from twisted.internet.defer import Deferred\n-from twisted.internet.endpoints import HostnameEndpoint\n from twisted.python.failure import Failure\n from twisted.web.client import (\n     URI,\n@@ -16,10 +16,16 @@ from twisted.web.error import SchemeNotSupported\n \n from scrapy.core.downloader.contextfactory import AcceptableProtocolsContextFactory\n from scrapy.core.http2.protocol import H2ClientFactory, H2ClientProtocol\n+\n+if TYPE_CHECKING:\n+    from twisted.internet.base import ReactorBase\n+    from twisted.internet.endpoints import HostnameEndpoint\n+\n     from scrapy.http.request import Request\n     from scrapy.settings import Settings\n     from scrapy.spiders import Spider\n \n+\n ConnectionKeyT = Tuple[bytes, bytes, int]\n \n \n\n@@ -1,9 +1,10 @@\n+from __future__ import annotations\n+\n import ipaddress\n import itertools\n import logging\n from collections import deque\n-from ipaddress import IPv4Address, IPv6Address\n-from typing import Any, Deque, Dict, List, Optional, Union\n+from typing import TYPE_CHECKING, Any, Deque, Dict, List, Optional, Union\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n@@ -20,7 +21,6 @@ from h2.events import (\n     WindowUpdated,\n )\n from h2.exceptions import FrameTooLargeError, H2Error\n-from twisted.internet.defer import Deferred\n from twisted.internet.error import TimeoutError\n from twisted.internet.interfaces import (\n     IAddress,\n@@ -30,15 +30,22 @@ from twisted.internet.interfaces import (\n from twisted.internet.protocol import Factory, Protocol, connectionDone\n from twisted.internet.ssl import Certificate\n from twisted.protocols.policies import TimeoutMixin\n-from twisted.python.failure import Failure\n-from twisted.web.client import URI\n from zope.interface import implementer\n \n from scrapy.core.http2.stream import Stream, StreamCloseReason\n from scrapy.http import Request\n+\n+if TYPE_CHECKING:\n+    from ipaddress import IPv4Address, IPv6Address\n+\n+    from twisted.internet.defer import Deferred\n+    from twisted.python.failure import Failure\n+    from twisted.web.client import URI\n+\n     from scrapy.settings import Settings\n     from scrapy.spiders import Spider\n \n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import logging\n from enum import Enum\n from io import BytesIO\n@@ -5,19 +7,20 @@ from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple\n \n from h2.errors import ErrorCodes\n from h2.exceptions import H2Error, ProtocolError, StreamClosedError\n-from hpack import HeaderTuple\n from twisted.internet.defer import CancelledError, Deferred\n from twisted.internet.error import ConnectionClosed\n from twisted.python.failure import Failure\n from twisted.web.client import ResponseFailed\n \n-from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n \n if TYPE_CHECKING:\n+    from hpack import HeaderTuple\n+\n     from scrapy.core.http2.protocol import H2ClientProtocol\n+    from scrapy.http import Request\n \n \n logger = logging.getLogger(__name__)\n@@ -87,7 +90,7 @@ class Stream:\n         self,\n         stream_id: int,\n         request: Request,\n-        protocol: \"H2ClientProtocol\",\n+        protocol: H2ClientProtocol,\n         download_maxsize: int = 0,\n         download_warnsize: int = 0,\n     ) -> None:\n@@ -99,7 +102,7 @@ class Stream:\n         \"\"\"\n         self.stream_id: int = stream_id\n         self._request: Request = request\n-        self._protocol: \"H2ClientProtocol\" = protocol\n+        self._protocol: H2ClientProtocol = protocol\n \n         self._download_maxsize = self._request.meta.get(\n             \"download_maxsize\", download_maxsize\n\n@@ -6,14 +6,10 @@ from abc import abstractmethod\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, List, Optional, Type, cast\n \n-from twisted.internet.defer import Deferred\n+# working around https://github.com/sphinx-doc/sphinx/issues/10400\n+from twisted.internet.defer import Deferred  # noqa: TC002\n \n-from scrapy.crawler import Crawler\n-from scrapy.dupefilters import BaseDupeFilter\n-from scrapy.http.request import Request\n-from scrapy.pqueues import ScrapyPriorityQueue\n-from scrapy.spiders import Spider\n-from scrapy.statscollectors import StatsCollector\n+from scrapy.spiders import Spider  # noqa: TC001\n from scrapy.utils.job import job_dir\n from scrapy.utils.misc import build_from_crawler, load_object\n \n@@ -24,6 +20,12 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.dupefilters import BaseDupeFilter\n+    from scrapy.http.request import Request\n+    from scrapy.pqueues import ScrapyPriorityQueue\n+    from scrapy.statscollectors import StatsCollector\n+\n \n logger = logging.getLogger(__name__)\n \n\n@@ -10,6 +10,7 @@ import logging\n from inspect import isasyncgenfunction, iscoroutine\n from itertools import islice\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     AsyncIterable,\n     Callable,\n@@ -30,7 +31,6 @@ from scrapy import Request, Spider\n from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Response\n from scrapy.middleware import MiddlewareManager\n-from scrapy.settings import BaseSettings\n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import (\n@@ -41,6 +41,10 @@ from scrapy.utils.defer import (\n )\n from scrapy.utils.python import MutableAsyncChain, MutableChain\n \n+if TYPE_CHECKING:\n+    from scrapy.settings import BaseSettings\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -6,16 +6,18 @@ from typing import TYPE_CHECKING, Union\n \n from w3lib import html\n \n-from scrapy import Request, Spider\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.http import HtmlResponse, Response\n-from scrapy.settings import BaseSettings\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request, Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.settings import BaseSettings\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -2,24 +2,26 @@ from __future__ import annotations\n \n import logging\n from collections import defaultdict\n-from http.cookiejar import Cookie\n from typing import TYPE_CHECKING, Any, DefaultDict, Iterable, Optional, Sequence, Union\n \n from tldextract import TLDExtract\n \n-from scrapy import Request, Spider\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n-from scrapy.http.request import VerboseCookie\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n+    from http.cookiejar import Cookie\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request, Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.http.request import VerboseCookie\n+\n \n logger = logging.getLogger(__name__)\n \n\n@@ -8,15 +8,16 @@ from __future__ import annotations\n \n from typing import TYPE_CHECKING, Iterable, Tuple, Union\n \n-from scrapy import Request, Spider\n-from scrapy.crawler import Crawler\n-from scrapy.http import Response\n from scrapy.utils.python import without_none_values\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request, Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+\n \n class DefaultHeadersMiddleware:\n     def __init__(self, headers: Iterable[Tuple[str, str]]):\n\n@@ -9,13 +9,14 @@ from __future__ import annotations\n from typing import TYPE_CHECKING, Union\n \n from scrapy import Request, Spider, signals\n-from scrapy.crawler import Crawler\n-from scrapy.http import Response\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+\n \n class DownloadTimeoutMiddleware:\n     def __init__(self, timeout: float = 180):\n\n@@ -11,14 +11,15 @@ from typing import TYPE_CHECKING, Union\n from w3lib.http import basic_auth_header\n \n from scrapy import Request, Spider, signals\n-from scrapy.crawler import Crawler\n-from scrapy.http import Response\n from scrapy.utils.url import url_is_from_any_domain\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+\n \n class HttpAuthMiddleware:\n     \"\"\"Set Basic HTTP Authorization header\n\n@@ -16,19 +16,20 @@ from twisted.internet.error import (\n from twisted.web.client import ResponseFailed\n \n from scrapy import signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n-from scrapy.http.request import Request\n-from scrapy.http.response import Response\n-from scrapy.settings import Settings\n-from scrapy.spiders import Spider\n-from scrapy.statscollectors import StatsCollector\n from scrapy.utils.misc import load_object\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.http.request import Request\n+    from scrapy.http.response import Response\n+    from scrapy.settings import Settings\n+    from scrapy.spiders import Spider\n+    from scrapy.statscollectors import StatsCollector\n+\n \n class HttpCacheMiddleware:\n     DOWNLOAD_EXCEPTIONS = (\n\n@@ -6,11 +6,9 @@ from logging import getLogger\n from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n \n from scrapy import Request, Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n-from scrapy.statscollectors import StatsCollector\n from scrapy.utils._compression import (\n     _DecompressionMaxSizeExceeded,\n     _inflate,\n@@ -24,6 +22,10 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.statscollectors import StatsCollector\n+\n+\n logger = getLogger(__name__)\n \n ACCEPTED_ENCODINGS: List[bytes] = [b\"gzip\", b\"deflate\"]\n\n@@ -9,10 +9,7 @@ from urllib.request import (  # type: ignore[attr-defined]\n     proxy_bypass,\n )\n \n-from scrapy import Request, Spider\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import Response\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes\n \n@@ -20,6 +17,10 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request, Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+\n \n class HttpProxyMiddleware:\n     def __init__(self, auth_encoding: Optional[str] = \"latin-1\"):\n\n@@ -6,15 +6,17 @@ import warnings\n from typing import TYPE_CHECKING, Set\n \n from scrapy import Request, Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest\n-from scrapy.statscollectors import StatsCollector\n from scrapy.utils.httpobj import urlparse_cached\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.statscollectors import StatsCollector\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -6,11 +6,8 @@ from urllib.parse import urljoin\n \n from w3lib.url import safe_url_string\n \n-from scrapy import Request, Spider\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import HtmlResponse, Response\n-from scrapy.settings import BaseSettings\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.response import get_meta_refresh\n \n@@ -18,6 +15,11 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request, Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.settings import BaseSettings\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -16,12 +16,8 @@ import warnings\n from logging import Logger, getLogger\n from typing import TYPE_CHECKING, Any, Optional, Tuple, Type, Union\n \n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n-from scrapy.http import Response\n-from scrapy.http.request import Request\n from scrapy.settings import BaseSettings, Settings\n-from scrapy.spiders import Spider\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import global_object_name\n from scrapy.utils.response import response_status_message\n@@ -30,6 +26,12 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+    from scrapy.http.request import Request\n+    from scrapy.spiders import Spider\n+\n+\n retry_logger = getLogger(__name__)\n \n \n\n@@ -10,22 +10,24 @@ import logging\n from typing import TYPE_CHECKING, Any, Dict, Optional, Union\n \n from twisted.internet.defer import Deferred, maybeDeferred\n-from twisted.python.failure import Failure\n \n-from scrapy import Spider\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n-from scrapy.robotstxt import RobotParser\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import load_object\n \n if TYPE_CHECKING:\n+    from twisted.python.failure import Failure\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.robotstxt import RobotParser\n+\n \n logger = logging.getLogger(__name__)\n \n\n@@ -4,11 +4,7 @@ from typing import TYPE_CHECKING, Dict, List, Tuple, Union\n \n from twisted.web import http\n \n-from scrapy import Request, Spider\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import Response\n-from scrapy.statscollectors import StatsCollector\n from scrapy.utils.python import global_object_name, to_bytes\n from scrapy.utils.request import request_httprepr\n \n@@ -16,6 +12,11 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request, Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+    from scrapy.statscollectors import StatsCollector\n+\n \n def get_header_size(\n     headers: Dict[str, Union[List[Union[str, bytes]], Tuple[Union[str, bytes], ...]]]\n\n@@ -5,13 +5,14 @@ from __future__ import annotations\n from typing import TYPE_CHECKING, Union\n \n from scrapy import Request, Spider, signals\n-from scrapy.crawler import Crawler\n-from scrapy.http import Response\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+\n \n class UserAgentMiddleware:\n     \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n\n@@ -4,11 +4,6 @@ import logging\n from pathlib import Path\n from typing import TYPE_CHECKING, Optional, Set\n \n-from twisted.internet.defer import Deferred\n-\n-from scrapy.http.request import Request\n-from scrapy.settings import BaseSettings\n-from scrapy.spiders import Spider\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import (\n     RequestFingerprinter,\n@@ -17,10 +12,15 @@ from scrapy.utils.request import (\n )\n \n if TYPE_CHECKING:\n+    from twisted.internet.defer import Deferred\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n     from scrapy.crawler import Crawler\n+    from scrapy.http.request import Request\n+    from scrapy.settings import BaseSettings\n+    from scrapy.spiders import Spider\n \n \n class BaseDupeFilter:\n\n@@ -4,12 +4,16 @@ The Extension Manager\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n-from typing import Any, List\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, List\n \n from scrapy.middleware import MiddlewareManager\n-from scrapy.settings import Settings\n from scrapy.utils.conf import build_component_list\n \n+if TYPE_CHECKING:\n+    from scrapy.settings import Settings\n+\n \n class ExtensionManager(MiddlewareManager):\n     component_name = \"extension\"\n\n@@ -10,17 +10,19 @@ import logging\n from collections import defaultdict\n from typing import TYPE_CHECKING, Any, DefaultDict, Dict\n \n-from twisted.python.failure import Failure\n-\n from scrapy import Request, Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import Response\n \n if TYPE_CHECKING:\n+    from twisted.python.failure import Failure\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -8,13 +8,14 @@ from datetime import datetime, timezone\n from typing import TYPE_CHECKING, Any, Optional\n \n from scrapy import Spider, signals\n-from scrapy.crawler import Crawler\n-from scrapy.statscollectors import StatsCollector\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.statscollectors import StatsCollector\n+\n \n class CoreStats:\n     def __init__(self, stats: StatsCollector):\n\n@@ -12,17 +12,20 @@ import sys\n import threading\n import traceback\n from pdb import Pdb\n-from types import FrameType\n from typing import TYPE_CHECKING, Optional\n \n-from scrapy.crawler import Crawler\n from scrapy.utils.engine import format_engine_status\n from scrapy.utils.trackref import format_live_refs\n \n if TYPE_CHECKING:\n+    from types import FrameType\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -31,18 +31,15 @@ from typing import (\n )\n from urllib.parse import unquote, urlparse\n \n-from twisted.internet import threads\n from twisted.internet.defer import Deferred, DeferredList, maybeDeferred\n-from twisted.python.failure import Failure\n+from twisted.internet.threads import deferToThread\n from w3lib.url import file_uri_to_path\n from zope.interface import Interface, implementer\n \n from scrapy import Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n-from scrapy.exporters import BaseItemExporter\n from scrapy.extensions.postprocessing import PostProcessingManager\n-from scrapy.settings import BaseSettings, Settings\n+from scrapy.settings import Settings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n from scrapy.utils.defer import maybe_deferred_to_future\n@@ -54,11 +51,14 @@ from scrapy.utils.python import without_none_values\n \n if TYPE_CHECKING:\n     from _typeshed import OpenBinaryMode\n+    from twisted.python.failure import Failure\n \n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n-logger = logging.getLogger(__name__)\n+    from scrapy.crawler import Crawler\n+    from scrapy.exporters import BaseItemExporter\n+    from scrapy.settings import BaseSettings\n \n try:\n     import boto3  # noqa: F401\n@@ -67,6 +67,9 @@ try:\n except ImportError:\n     IS_BOTO3_AVAILABLE = False\n \n+\n+logger = logging.getLogger(__name__)\n+\n UriParamsCallableT = Callable[[Dict[str, Any], Spider], Optional[Dict[str, Any]]]\n \n _StorageT = TypeVar(\"_StorageT\", bound=\"FeedStorageProtocol\")\n@@ -160,7 +163,7 @@ class BlockingFeedStorage:\n         return NamedTemporaryFile(prefix=\"feed-\", dir=path)\n \n     def store(self, file: IO[bytes]) -> Optional[Deferred]:\n-        return threads.deferToThread(self._store_in_thread, file)\n+        return deferToThread(self._store_in_thread, file)\n \n     def _store_in_thread(self, file: IO[bytes]) -> None:\n         raise NotImplementedError\n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import gzip\n import logging\n import os\n@@ -13,10 +15,7 @@ from weakref import WeakKeyDictionary\n from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n \n from scrapy.http import Headers, Response\n-from scrapy.http.request import Request\n from scrapy.responsetypes import responsetypes\n-from scrapy.settings import BaseSettings\n-from scrapy.spiders import Spider\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.project import data_path\n from scrapy.utils.python import to_bytes, to_unicode\n@@ -26,6 +25,10 @@ if TYPE_CHECKING:\n     # typing.Concatenate requires Python 3.10\n     from typing_extensions import Concatenate\n \n+    from scrapy.http.request import Request\n+    from scrapy.settings import BaseSettings\n+    from scrapy.spiders import Spider\n+\n \n logger = logging.getLogger(__name__)\n \n\n@@ -6,14 +6,16 @@ from typing import TYPE_CHECKING, Optional, Tuple, Union\n from twisted.internet import task\n \n from scrapy import Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n-from scrapy.statscollectors import StatsCollector\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.statscollectors import StatsCollector\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -10,15 +10,16 @@ import gc\n from typing import TYPE_CHECKING\n \n from scrapy import Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n-from scrapy.statscollectors import StatsCollector\n from scrapy.utils.trackref import live_refs\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.statscollectors import StatsCollector\n+\n \n class MemoryDebugger:\n     def __init__(self, stats: StatsCollector):\n\n@@ -16,7 +16,6 @@ from typing import TYPE_CHECKING, List\n from twisted.internet import task\n \n from scrapy import signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.utils.engine import get_engine_status\n@@ -25,6 +24,9 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -8,15 +8,17 @@ from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n from twisted.internet import task\n \n from scrapy import Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n-from scrapy.statscollectors import StatsCollector\n from scrapy.utils.serialize import ScrapyJSONEncoder\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.statscollectors import StatsCollector\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -5,7 +5,6 @@ from pathlib import Path\n from typing import TYPE_CHECKING, Optional\n \n from scrapy import Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.job import job_dir\n \n@@ -13,6 +12,8 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+\n \n class SpiderState:\n     \"\"\"Store and load spider state during a scraping job\"\"\"\n\n@@ -8,18 +8,19 @@ from __future__ import annotations\n \n from typing import TYPE_CHECKING, List, Optional\n \n-from twisted.internet.defer import Deferred\n-\n from scrapy import Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n-from scrapy.statscollectors import StatsCollector\n \n if TYPE_CHECKING:\n+    from twisted.internet.defer import Deferred\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.statscollectors import StatsCollector\n+\n \n class StatsMailer:\n     def __init__(self, stats: StatsCollector, recipients: List[str], mail: MailSender):\n\n@@ -26,7 +26,6 @@ except (ImportError, SyntaxError):\n     TWISTED_CONCH_AVAILABLE = False\n \n from scrapy import signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.decorators import defers\n from scrapy.utils.engine import print_engine_status\n@@ -36,6 +35,10 @@ from scrapy.utils.trackref import print_live_refs\n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n+\n+    from scrapy.crawler import Crawler\n+\n+\n logger = logging.getLogger(__name__)\n \n # signal to update telnet variables\n\n@@ -4,15 +4,17 @@ import logging\n from typing import TYPE_CHECKING, Optional, Tuple\n \n from scrapy import Request, Spider, signals\n-from scrapy.core.downloader import Slot\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import Response\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.core.downloader import Slot\n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -17,8 +17,6 @@ from typing import (\n     cast,\n )\n \n-from scrapy import Request\n-from scrapy.http import Response\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n@@ -26,6 +24,10 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request\n+    from scrapy.http import Response\n+\n+\n # Defined in the http.cookiejar module, but undocumented:\n # https://github.com/python/cpython/blob/v3.9.0/Lib/http/cookiejar.py#L527\n IPV4_RE = re.compile(r\"\\.\\d+$\", re.ASCII)\n\n@@ -28,13 +28,14 @@ from lxml.html import TextareaElement  # nosec\n from w3lib.html import strip_html5_whitespace\n \n from scrapy.http.request import Request\n-from scrapy.http.response.text import TextResponse\n from scrapy.utils.python import is_listlike, to_bytes\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.http.response.text import TextResponse\n+\n \n FormdataVType = Union[str, Iterable[str]]\n FormdataKVType = Tuple[str, FormdataVType]\n\n@@ -7,7 +7,6 @@ See documentation in docs/topics/request-response.rst\n \n from __future__ import annotations\n \n-from ipaddress import IPv4Address, IPv6Address\n from typing import (\n     TYPE_CHECKING,\n     Any,\n@@ -26,8 +25,6 @@ from typing import (\n )\n from urllib.parse import urljoin\n \n-from twisted.internet.ssl import Certificate\n-\n from scrapy.exceptions import NotSupported\n from scrapy.http.headers import Headers\n from scrapy.http.request import CookiesT, Request\n@@ -35,6 +32,10 @@ from scrapy.link import Link\n from scrapy.utils.trackref import object_ref\n \n if TYPE_CHECKING:\n+    from ipaddress import IPv4Address, IPv6Address\n+\n+    from twisted.internet.ssl import Certificate\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n\n@@ -35,15 +35,16 @@ from w3lib.encoding import (\n )\n from w3lib.html import strip_html5_whitespace\n \n-from scrapy.http.request import CookiesT, Request\n from scrapy.http.response import Response\n from scrapy.link import Link\n from scrapy.utils.python import memoizemethod_noargs, to_unicode\n from scrapy.utils.response import get_base_url\n \n if TYPE_CHECKING:\n+    from scrapy.http.request import CookiesT, Request\n     from scrapy.selector import Selector, SelectorList\n \n+\n _NONE = object()\n \n \n\n@@ -2,10 +2,13 @@\n Link extractor based on lxml.html\n \"\"\"\n \n+from __future__ import annotations\n+\n import logging\n import operator\n from functools import partial\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     Callable,\n     Iterable,\n@@ -20,13 +23,10 @@ from typing import (\n from urllib.parse import urljoin, urlparse\n \n from lxml import etree  # nosec\n-from lxml.html import HtmlElement  # nosec\n from parsel.csstranslator import HTMLTranslator\n from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url, safe_url_string\n \n-from scrapy import Selector\n-from scrapy.http import TextResponse\n from scrapy.link import Link\n from scrapy.linkextractors import IGNORED_EXTENSIONS, _is_valid_url, _matches, re\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n@@ -34,6 +34,13 @@ from scrapy.utils.python import unique as unique_list\n from scrapy.utils.response import get_base_url\n from scrapy.utils.url import url_has_any_extension, url_is_from_any_domain\n \n+if TYPE_CHECKING:\n+    from lxml.html import HtmlElement  # nosec\n+\n+    from scrapy import Selector\n+    from scrapy.http import TextResponse\n+\n+\n logger = logging.getLogger(__name__)\n \n # from lxml/src/lxml/html/__init__.py\n\n@@ -4,14 +4,18 @@ Item Loader\n See documentation in docs/topics/loaders.rst\n \"\"\"\n \n-from typing import Any, Optional\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, Optional\n \n import itemloaders\n \n-from scrapy.http import TextResponse\n from scrapy.item import Item\n from scrapy.selector import Selector\n \n+if TYPE_CHECKING:\n+    from scrapy.http import TextResponse\n+\n \n class ItemLoader(itemloaders.ItemLoader):\n     \"\"\"\n@@ -91,7 +95,7 @@ class ItemLoader(itemloaders.ItemLoader):\n         selector: Optional[Selector] = None,\n         response: Optional[TextResponse] = None,\n         parent: Optional[itemloaders.ItemLoader] = None,\n-        **context: Any\n+        **context: Any,\n     ):\n         if selector is None and response is not None:\n             try:\n\n@@ -6,8 +6,9 @@ from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, TypedDict, Union\n \n from twisted.python.failure import Failure\n \n-from scrapy import Request, Spider\n-from scrapy.http import Response\n+# working around https://github.com/sphinx-doc/sphinx/issues/10400\n+from scrapy import Request, Spider  # noqa: TC001\n+from scrapy.http import Response  # noqa: TC001\n from scrapy.utils.request import referer_str\n \n if TYPE_CHECKING:\n\n@@ -30,20 +30,22 @@ from typing import (\n from twisted import version as twisted_version\n from twisted.internet import ssl\n from twisted.internet.defer import Deferred\n-from twisted.python.failure import Failure\n from twisted.python.versions import Version\n \n-from scrapy.settings import BaseSettings\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n if TYPE_CHECKING:\n     # imports twisted.internet.reactor\n     from twisted.mail.smtp import ESMTPSenderFactory\n+    from twisted.python.failure import Failure\n \n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.settings import BaseSettings\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -17,19 +17,19 @@ from typing import (\n     cast,\n )\n \n-from twisted.internet.defer import Deferred\n-\n-from scrapy import Spider\n from scrapy.exceptions import NotConfigured\n-from scrapy.settings import Settings\n from scrapy.utils.defer import process_chain, process_parallel\n from scrapy.utils.misc import build_from_crawler, build_from_settings, load_object\n \n if TYPE_CHECKING:\n+    from twisted.internet.defer import Deferred\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Spider\n     from scrapy.crawler import Crawler\n+    from scrapy.settings import Settings\n \n \n logger = logging.getLogger(__name__)\n\n@@ -6,15 +6,17 @@ See documentation in docs/item-pipeline.rst\n \n from __future__ import annotations\n \n-from typing import Any, List\n+from typing import TYPE_CHECKING, Any, List\n \n+from scrapy.middleware import MiddlewareManager\n+from scrapy.utils.conf import build_component_list\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+\n+if TYPE_CHECKING:\n     from twisted.internet.defer import Deferred\n \n     from scrapy import Spider\n-from scrapy.middleware import MiddlewareManager\n     from scrapy.settings import Settings\n-from scrapy.utils.conf import build_component_list\n-from scrapy.utils.defer import deferred_f_from_coro_f\n \n \n class ItemPipelineManager(MiddlewareManager):\n\n@@ -16,7 +16,6 @@ from collections import defaultdict\n from contextlib import suppress\n from ftplib import FTP\n from io import BytesIO\n-from os import PathLike\n from pathlib import Path\n from typing import (\n     IO,\n@@ -38,11 +37,9 @@ from typing import (\n from urllib.parse import urlparse\n \n from itemadapter import ItemAdapter\n-from twisted.internet import defer, threads\n-from twisted.internet.defer import Deferred\n-from twisted.python.failure import Failure\n+from twisted.internet.defer import Deferred, maybeDeferred\n+from twisted.internet.threads import deferToThread\n \n-from scrapy import Spider\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n@@ -56,9 +53,15 @@ from scrapy.utils.python import to_bytes\n from scrapy.utils.request import referer_str\n \n if TYPE_CHECKING:\n+    from os import PathLike\n+\n+    from twisted.python.failure import Failure\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Spider\n+\n \n logger = logging.getLogger(__name__)\n \n@@ -210,7 +213,7 @@ class S3FilesStore:\n         key_name = f\"{self.prefix}{path}\"\n         return cast(\n             \"Deferred[Dict[str, Any]]\",\n-            threads.deferToThread(\n+            deferToThread(\n                 self.s3_client.head_object, Bucket=self.bucket, Key=key_name  # type: ignore[attr-defined]\n             ),\n         )\n@@ -229,7 +232,7 @@ class S3FilesStore:\n         extra = self._headers_to_botocore_kwargs(self.HEADERS)\n         if headers:\n             extra.update(self._headers_to_botocore_kwargs(headers))\n-        return threads.deferToThread(\n+        return deferToThread(\n             self.s3_client.put_object,  # type: ignore[attr-defined]\n             Bucket=self.bucket,\n             Key=key_name,\n@@ -326,9 +329,7 @@ class GCSFilesStore:\n         blob_path = self._get_blob_path(path)\n         return cast(\n             Deferred[StatInfo],\n-            threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(\n-                _onsuccess\n-            ),\n+            deferToThread(self.bucket.get_blob, blob_path).addCallback(_onsuccess),\n         )\n \n     def _get_content_type(self, headers: Optional[Dict[str, str]]) -> str:\n@@ -351,7 +352,7 @@ class GCSFilesStore:\n         blob = self.bucket.blob(blob_path)\n         blob.cache_control = self.CACHE_CONTROL\n         blob.metadata = {k: str(v) for k, v in (meta or {}).items()}\n-        return threads.deferToThread(\n+        return deferToThread(\n             blob.upload_from_string,\n             data=buf.getvalue(),\n             content_type=self._get_content_type(headers),\n@@ -388,7 +389,7 @@ class FTPFilesStore:\n         headers: Optional[Dict[str, str]] = None,\n     ) -> Deferred[Any]:\n         path = f\"{self.basedir}/{path}\"\n-        return threads.deferToThread(\n+        return deferToThread(\n             ftp_store_file,\n             path=path,\n             file=buf,\n@@ -418,7 +419,7 @@ class FTPFilesStore:\n             except Exception:\n                 return {}\n \n-        return cast(\"Deferred[StatInfo]\", threads.deferToThread(_stat_file, path))\n+        return cast(\"Deferred[StatInfo]\", deferToThread(_stat_file, path))\n \n \n class FilesPipeline(MediaPipeline):\n@@ -553,8 +554,8 @@ class FilesPipeline(MediaPipeline):\n             }\n \n         path = self.file_path(request, info=info, item=item)\n-        # defer.maybeDeferred() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n-        dfd: Deferred[StatInfo] = defer.maybeDeferred(self.store.stat_file, path, info)  # type: ignore[arg-type]\n+        # maybeDeferred() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n+        dfd: Deferred[StatInfo] = maybeDeferred(self.store.stat_file, path, info)  # type: ignore[arg-type]\n         dfd2: Deferred[Optional[FileInfo]] = dfd.addCallback(_onsuccess)\n         dfd2.addErrback(lambda _: None)\n         dfd2.addErrback(\n\n@@ -11,7 +11,6 @@ import hashlib\n import warnings\n from contextlib import suppress\n from io import BytesIO\n-from os import PathLike\n from typing import (\n     TYPE_CHECKING,\n     Any,\n@@ -28,7 +27,6 @@ from typing import (\n \n from itemadapter import ItemAdapter\n \n-from scrapy import Spider\n from scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n@@ -40,15 +38,20 @@ from scrapy.pipelines.files import (\n     S3FilesStore,\n     _md5sum,\n )\n-from scrapy.pipelines.media import FileInfoOrError, MediaPipeline\n from scrapy.settings import Settings\n from scrapy.utils.python import get_func_args, to_bytes\n \n if TYPE_CHECKING:\n-    # typing.Self requires Python 3.11\n+    from os import PathLike\n+\n     from PIL import Image\n+\n+    # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Spider\n+    from scrapy.pipelines.media import FileInfoOrError, MediaPipeline\n+\n \n class NoimagesDrop(DropItem):\n     \"\"\"Product with no images exception\"\"\"\n\n@@ -25,21 +25,23 @@ from typing import (\n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n \n-from scrapy import Spider\n-from scrapy.crawler import Crawler\n-from scrapy.http import Response\n from scrapy.http.request import NO_CALLBACK, Request\n from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import defer_result, mustbe_deferred\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.request import RequestFingerprinter\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+    from scrapy.utils.request import RequestFingerprinter\n+\n+\n _T = TypeVar(\"_T\")\n \n \n\n@@ -4,7 +4,6 @@ from typing import TYPE_CHECKING, Any, List, Optional, Sequence, Type\n \n from twisted.internet import defer\n from twisted.internet.base import ReactorBase, ThreadedResolver\n-from twisted.internet.defer import Deferred\n from twisted.internet.interfaces import (\n     IAddress,\n     IHostnameResolver,\n@@ -17,6 +16,8 @@ from zope.interface.declarations import implementer, provider\n from scrapy.utils.datatypes import LocalCache\n \n if TYPE_CHECKING:\n+    from twisted.internet.defer import Deferred\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n\n@@ -6,7 +6,6 @@ from abc import ABCMeta, abstractmethod\n from typing import TYPE_CHECKING, Optional, Union\n from warnings import warn\n \n-from scrapy import Spider\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.python import to_unicode\n \n@@ -14,8 +13,10 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Spider\n     from scrapy.crawler import Crawler\n \n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -4,7 +4,6 @@ import copy\n import json\n from importlib import import_module\n from pprint import pformat\n-from types import ModuleType\n from typing import (\n     TYPE_CHECKING,\n     Any,\n@@ -27,6 +26,8 @@ from scrapy.settings import default_settings\n _SettingsKeyT = Union[bool, float, int, str, None]\n \n if TYPE_CHECKING:\n+    from types import ModuleType\n+\n     # https://github.com/python/typing/issues/445#issuecomment-1131458824\n     from _typeshed import SupportsItems\n \n\n@@ -1,10 +1,14 @@\n-from typing import Any, List, Tuple\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, List, Tuple\n \n from pydispatch import dispatcher\n-from twisted.internet.defer import Deferred\n \n from scrapy.utils import signal as _signal\n \n+if TYPE_CHECKING:\n+    from twisted.internet.defer import Deferred\n+\n \n class SignalManager:\n     def __init__(self, sender: Any = dispatcher.Anonymous):\n\n@@ -3,21 +3,23 @@ from __future__ import annotations\n import traceback\n import warnings\n from collections import defaultdict\n-from types import ModuleType\n from typing import TYPE_CHECKING, DefaultDict, Dict, List, Tuple, Type\n \n from zope.interface import implementer\n \n-from scrapy import Request, Spider\n from scrapy.interfaces import ISpiderLoader\n-from scrapy.settings import BaseSettings\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.spider import iter_spider_classes\n \n if TYPE_CHECKING:\n+    from types import ModuleType\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request, Spider\n+    from scrapy.settings import BaseSettings\n+\n \n @implementer(ISpiderLoader)\n class SpiderLoader:\n\n@@ -9,15 +9,17 @@ from __future__ import annotations\n import logging\n from typing import TYPE_CHECKING, Any, AsyncIterable, Iterable\n \n-from scrapy import Spider\n-from scrapy.crawler import Crawler\n from scrapy.http import Request, Response\n-from scrapy.statscollectors import StatsCollector\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.statscollectors import StatsCollector\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -9,16 +9,18 @@ from __future__ import annotations\n import logging\n from typing import TYPE_CHECKING, Any, Iterable, List, Optional\n \n-from scrapy import Spider\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest\n-from scrapy.http import Response\n-from scrapy.settings import BaseSettings\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Spider\n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+    from scrapy.settings import BaseSettings\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -12,10 +12,8 @@ import warnings\n from typing import TYPE_CHECKING, Any, AsyncIterable, Iterable, Set\n \n from scrapy import Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n-from scrapy.statscollectors import StatsCollector\n from scrapy.utils.httpobj import urlparse_cached\n \n warnings.warn(\n@@ -28,6 +26,10 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.statscollectors import StatsCollector\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -23,10 +23,8 @@ from urllib.parse import urlparse\n from w3lib.url import safe_url_string\n \n from scrapy import Spider, signals\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Request, Response\n-from scrapy.settings import BaseSettings\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_unicode\n from scrapy.utils.url import strip_url\n@@ -35,6 +33,10 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+    from scrapy.settings import BaseSettings\n+\n+\n LOCAL_SCHEMES: Tuple[str, ...] = (\n     \"about\",\n     \"blob\",\n\n@@ -9,15 +9,17 @@ from __future__ import annotations\n import logging\n from typing import TYPE_CHECKING, Any, AsyncIterable, Iterable\n \n-from scrapy import Spider\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Request, Response\n-from scrapy.settings import BaseSettings\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Spider\n+    from scrapy.settings import BaseSettings\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -1,10 +1,14 @@\n-from typing import Any, Iterable, Optional, cast\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, Iterable, Optional, cast\n \n from scrapy import Request\n-from scrapy.http import Response\n from scrapy.spiders import Spider\n from scrapy.utils.spider import iterate_spider_output\n \n+if TYPE_CHECKING:\n+    from scrapy.http import Response\n+\n \n class InitSpider(Spider):\n     \"\"\"Base Spider with initialization facilities\"\"\"\n\n@@ -6,20 +6,22 @@ from __future__ import annotations\n \n import marshal\n import pickle  # nosec\n-from os import PathLike\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, Callable, Optional, Type, Union\n \n from queuelib import queue\n \n-from scrapy import Request\n-from scrapy.crawler import Crawler\n from scrapy.utils.request import request_from_dict\n \n if TYPE_CHECKING:\n+    from os import PathLike\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy import Request\n+    from scrapy.crawler import Crawler\n+\n \n def _with_mkdir(queue_class: Type[queue.BaseQueue]) -> Type[queue.BaseQueue]:\n     class DirectoriesCreated(queue_class):  # type: ignore[valid-type,misc]\n\n@@ -2,15 +2,17 @@\n Scrapy extension for collecting scraping stats\n \"\"\"\n \n+from __future__ import annotations\n+\n import logging\n import pprint\n from typing import TYPE_CHECKING, Any, Dict, Optional\n \n-from scrapy import Spider\n-\n if TYPE_CHECKING:\n+    from scrapy import Spider\n     from scrapy.crawler import Crawler\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -18,7 +20,7 @@ StatsT = Dict[str, Any]\n \n \n class StatsCollector:\n-    def __init__(self, crawler: \"Crawler\"):\n+    def __init__(self, crawler: Crawler):\n         self._dump: bool = crawler.settings.getbool(\"STATS_DUMP\")\n         self._stats: StatsT = {}\n \n@@ -67,7 +69,7 @@ class StatsCollector:\n \n \n class MemoryStatsCollector(StatsCollector):\n-    def __init__(self, crawler: \"Crawler\"):\n+    def __init__(self, crawler: Crawler):\n         super().__init__(crawler)\n         self.spider_stats: Dict[str, StatsT] = {}\n \n\n@@ -4,8 +4,8 @@ import warnings\n from functools import wraps\n from typing import TYPE_CHECKING, Any, Callable, TypeVar\n \n-from twisted.internet import defer, threads\n-from twisted.internet.defer import Deferred\n+from twisted.internet.defer import Deferred, maybeDeferred\n+from twisted.internet.threads import deferToThread\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n@@ -48,7 +48,7 @@ def defers(func: Callable[_P, _T]) -> Callable[_P, Deferred[_T]]:\n \n     @wraps(func)\n     def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n-        return defer.maybeDeferred(func, *a, **kw)\n+        return maybeDeferred(func, *a, **kw)\n \n     return wrapped\n \n@@ -60,6 +60,6 @@ def inthread(func: Callable[_P, _T]) -> Callable[_P, Deferred[_T]]:\n \n     @wraps(func)\n     def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n-        return threads.deferToThread(func, *a, **kw)\n+        return deferToThread(func, *a, **kw)\n \n     return wrapped\n\n@@ -34,12 +34,13 @@ from twisted.internet import defer\n from twisted.internet.defer import Deferred, DeferredList, ensureDeferred\n from twisted.internet.task import Cooperator\n from twisted.python import failure\n-from twisted.python.failure import Failure\n \n from scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\n from scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed\n \n if TYPE_CHECKING:\n+    from twisted.python.failure import Failure\n+\n     # typing.Concatenate and typing.ParamSpec require Python 3.10\n     from typing_extensions import Concatenate, ParamSpec\n \n\n@@ -4,8 +4,9 @@ from __future__ import annotations\n \n # used in global tests code\n from time import time  # noqa: F401\n-from typing import Any, List, Tuple\n+from typing import TYPE_CHECKING, Any, List, Tuple\n \n+if TYPE_CHECKING:\n     from scrapy.core.engine import ExecutionEngine\n \n \n\n@@ -1,11 +1,15 @@\n+from __future__ import annotations\n+\n import struct\n from gzip import GzipFile\n from io import BytesIO\n-\n-from scrapy.http import Response\n+from typing import TYPE_CHECKING\n \n from ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded\n \n+if TYPE_CHECKING:\n+    from scrapy.http import Response\n+\n \n def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n     \"\"\"Gunzip the given data and return as much data as possible.\n\n@@ -1,12 +1,16 @@\n \"\"\"Helper functions for scrapy.http objects (Request, Response)\"\"\"\n \n-from typing import Union\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Union\n from urllib.parse import ParseResult, urlparse\n from weakref import WeakKeyDictionary\n \n+if TYPE_CHECKING:\n     from scrapy.http import Request, Response\n \n-_urlparse_cache: \"WeakKeyDictionary[Union[Request, Response], ParseResult]\" = (\n+\n+_urlparse_cache: WeakKeyDictionary[Union[Request, Response], ParseResult] = (\n     WeakKeyDictionary()\n )\n \n\n@@ -1,6 +1,9 @@\n-from pathlib import Path\n-from typing import Optional\n+from __future__ import annotations\n \n+from pathlib import Path\n+from typing import TYPE_CHECKING, Optional\n+\n+if TYPE_CHECKING:\n     from scrapy.settings import BaseSettings\n \n \n\n@@ -21,12 +21,13 @@ from twisted.python import log as twisted_log\n from twisted.python.failure import Failure\n \n import scrapy\n-from scrapy.logformatter import LogFormatterResult\n from scrapy.settings import Settings, _SettingsKeyT\n from scrapy.utils.versions import scrapy_components_versions\n \n if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n+    from scrapy.logformatter import LogFormatterResult\n+\n \n logger = logging.getLogger(__name__)\n \n\n@@ -13,7 +13,6 @@ from contextlib import contextmanager\n from functools import partial\n from importlib import import_module\n from pkgutil import iter_modules\n-from types import ModuleType\n from typing import (\n     IO,\n     TYPE_CHECKING,\n@@ -35,10 +34,13 @@ from scrapy.item import Item\n from scrapy.utils.datatypes import LocalWeakReferencedCache\n \n if TYPE_CHECKING:\n+    from types import ModuleType\n+\n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n+\n _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n T = TypeVar(\"T\")\n \n\n@@ -1,7 +1,8 @@\n+from __future__ import annotations\n+\n import os\n import warnings\n from importlib import import_module\n-from os import PathLike\n from pathlib import Path\n from typing import Union\n \n@@ -46,7 +47,7 @@ def project_data_dir(project: str = \"default\") -> str:\n     return str(d)\n \n \n-def data_path(path: Union[str, PathLike], createdir: bool = False) -> str:\n+def data_path(path: Union[str, os.PathLike[str]], createdir: bool = False) -> str:\n     \"\"\"\n     Return the given path joined with the .scrapy data directory.\n     If given an absolute path, return it unmodified.\n\n@@ -2,7 +2,6 @@ from __future__ import annotations\n \n import asyncio\n import sys\n-from asyncio import AbstractEventLoop, AbstractEventLoopPolicy\n from contextlib import suppress\n from typing import (\n     TYPE_CHECKING,\n@@ -20,13 +19,16 @@ from warnings import catch_warnings, filterwarnings, warn\n \n from twisted.internet import asyncioreactor, error\n from twisted.internet.base import DelayedCall\n-from twisted.internet.protocol import ServerFactory\n-from twisted.internet.tcp import Port\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import load_object\n \n if TYPE_CHECKING:\n+    from asyncio import AbstractEventLoop, AbstractEventLoopPolicy\n+\n+    from twisted.internet.protocol import ServerFactory\n+    from twisted.internet.tcp import Port\n+\n     # typing.ParamSpec requires Python 3.10\n     from typing_extensions import ParamSpec\n \n\n@@ -2,7 +2,6 @@ from __future__ import annotations\n \n import inspect\n import logging\n-from types import CoroutineType, ModuleType\n from typing import (\n     TYPE_CHECKING,\n     Any,\n@@ -16,16 +15,19 @@ from typing import (\n     overload,\n )\n \n-from twisted.internet.defer import Deferred\n-\n-from scrapy import Request\n from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import arg_to_iter\n \n if TYPE_CHECKING:\n+    from types import CoroutineType, ModuleType\n+\n+    from twisted.internet.defer import Deferred\n+\n+    from scrapy import Request\n     from scrapy.spiderloader import SpiderLoader\n \n+\n logger = logging.getLogger(__name__)\n \n _T = TypeVar(\"_T\")\n\n@@ -1,12 +1,16 @@\n-from typing import Any, Optional\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, Optional\n \n import OpenSSL._util as pyOpenSSLutil\n import OpenSSL.SSL\n import OpenSSL.version\n-from OpenSSL.crypto import X509Name\n \n from scrapy.utils.python import to_unicode\n \n+if TYPE_CHECKING:\n+    from OpenSSL.crypto import X509Name\n+\n \n def ffi_buf_to_string(buf: Any) -> str:\n     return to_unicode(pyOpenSSLutil.ffi.string(buf))\n\n@@ -1,10 +1,14 @@\n \"\"\"Helper functions for working with templates\"\"\"\n \n+from __future__ import annotations\n+\n import re\n import string\n-from os import PathLike\n from pathlib import Path\n-from typing import Any, Union\n+from typing import TYPE_CHECKING, Any, Union\n+\n+if TYPE_CHECKING:\n+    from os import PathLike\n \n \n def render_templatefile(path: Union[str, PathLike], **kwargs: Any) -> None:\n\n@@ -2,21 +2,36 @@\n This module contains some assorted functions used in tests\n \"\"\"\n \n+from __future__ import annotations\n+\n import asyncio\n import os\n from importlib import import_module\n from pathlib import Path\n from posixpath import split\n-from typing import Any, Awaitable, Dict, List, Optional, Tuple, Type, TypeVar\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Awaitable,\n+    Dict,\n+    List,\n+    Optional,\n+    Tuple,\n+    Type,\n+    TypeVar,\n+)\n from unittest import TestCase, mock\n \n-from twisted.internet.defer import Deferred\n from twisted.trial.unittest import SkipTest\n \n from scrapy import Spider\n from scrapy.crawler import Crawler\n from scrapy.utils.boto import is_botocore_available\n \n+if TYPE_CHECKING:\n+    from twisted.internet.defer import Deferred\n+\n+\n _T = TypeVar(\"_T\")\n \n \n\n@@ -2,11 +2,13 @@ from __future__ import annotations\n \n import os\n import sys\n-from typing import Iterable, List, Optional, Tuple, cast\n+from typing import TYPE_CHECKING, Iterable, List, Optional, Tuple, cast\n \n from twisted.internet.defer import Deferred\n from twisted.internet.error import ProcessTerminated\n from twisted.internet.protocol import ProcessProtocol\n+\n+if TYPE_CHECKING:\n     from twisted.python.failure import Failure\n \n \n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import argparse\n import json\n import os\n@@ -7,12 +9,11 @@ from pathlib import Path\n from shutil import rmtree\n from subprocess import PIPE, Popen\n from tempfile import mkdtemp\n-from typing import Dict\n+from typing import TYPE_CHECKING, Dict\n from urllib.parse import urlencode\n \n from OpenSSL import SSL\n from twisted.internet import defer, reactor, ssl\n-from twisted.internet.protocol import ServerFactory\n from twisted.internet.task import deferLater\n from twisted.names import dns, error\n from twisted.names.server import DNSServerFactory\n@@ -23,6 +24,9 @@ from twisted.web.util import redirectTo\n \n from scrapy.utils.python import to_bytes, to_unicode\n \n+if TYPE_CHECKING:\n+    from twisted.internet.protocol import ServerFactory\n+\n \n def getarg(request, name, default=None, type=None):\n     if name in request.args:\n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import bz2\n import csv\n import gzip\n@@ -14,10 +16,9 @@ from collections import defaultdict\n from contextlib import ExitStack\n from io import BytesIO\n from logging import getLogger\n-from os import PathLike\n from pathlib import Path\n from string import ascii_letters, digits\n-from typing import Union\n+from typing import TYPE_CHECKING, Union\n from unittest import mock\n from urllib.parse import quote, urljoin\n from urllib.request import pathname2url\n@@ -53,6 +54,9 @@ from scrapy.utils.test import get_crawler, mock_google_cloud_storage, skip_if_no\n from tests.mockserver import MockFTPServer, MockServer\n from tests.spiders import ItemSpider\n \n+if TYPE_CHECKING:\n+    from os import PathLike\n+\n \n def path_to_url(path):\n     return urljoin(\"file:\", pathname2url(str(path)))\n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import json\n import random\n import re\n@@ -6,7 +8,7 @@ import string\n from ipaddress import IPv4Address\n from pathlib import Path\n from tempfile import mkdtemp\n-from typing import Dict\n+from typing import TYPE_CHECKING, Dict\n from unittest import mock, skipIf\n from urllib.parse import urlencode\n \n@@ -20,7 +22,6 @@ from twisted.internet.defer import (\n from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint\n from twisted.internet.error import TimeoutError\n from twisted.internet.ssl import Certificate, PrivateCertificate, optionsForClientTLS\n-from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n from twisted.web.client import URI, ResponseFailed\n from twisted.web.http import H2_ENABLED\n@@ -33,6 +34,9 @@ from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from tests.mockserver import LeafResource, Status, ssl_context_factory\n \n+if TYPE_CHECKING:\n+    from twisted.python.failure import Failure\n+\n \n def generate_random_string(size):\n     return \"\".join(random.choices(string.ascii_uppercase + string.digits, k=size))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
