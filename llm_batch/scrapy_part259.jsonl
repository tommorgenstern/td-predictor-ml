{"custom_id": "scrapy#5442c2d3c3f201c7ad069585546b403f46720df5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1353 | Contributors (this commit): 17 | Commits (past 90d): 28 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -98,7 +98,8 @@ class S3FeedStorage(BlockingFeedStorage):\n         # without using from_crawler)\n         no_defaults = access_key is None and secret_key is None\n         if no_defaults:\n-            from scrapy.conf import settings\n+            from scrapy.utils.project import get_project_settings\n+            settings = get_project_settings()\n             if 'AWS_ACCESS_KEY_ID' in settings or 'AWS_SECRET_ACCESS_KEY' in settings:\n                 import warnings\n                 from scrapy.exceptions import ScrapyDeprecationWarning\n\n@@ -26,6 +26,7 @@ from scrapy.extensions.feedexport import (\n     BlockingFeedStorage)\n from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete, get_crawler\n from scrapy.utils.python import to_native_str\n+from scrapy.utils.project import get_project_settings\n \n \n class FileFeedStorageTest(unittest.TestCase):\n@@ -134,8 +135,10 @@ class BlockingFeedStorageTest(unittest.TestCase):\n \n class S3FeedStorageTest(unittest.TestCase):\n \n-    @mock.patch('scrapy.conf.settings', new={'AWS_ACCESS_KEY_ID': 'conf_key',\n-                'AWS_SECRET_ACCESS_KEY': 'conf_secret'}, create=True)\n+    @mock.patch('scrapy.utils.project.get_project_settings',\n+                new=mock.MagicMock(return_value={'AWS_ACCESS_KEY_ID': 'conf_key',\n+                                                 'AWS_SECRET_ACCESS_KEY': 'conf_secret'}),\n+                create=True)\n     def test_parse_credentials(self):\n         try:\n             import boto\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#67a400092805ec0d643bd7de0481cc45d5ce8471", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 153 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -74,11 +74,18 @@ if twisted_version >= (14, 0, 0):\n             if where & SSL_CB_HANDSHAKE_START:\n                 set_tlsext_host_name(connection, self._hostnameBytes)\n             elif where & SSL_CB_HANDSHAKE_DONE:\n+                if hasattr(connection, 'get_cipher_name'):  # requires pyOPenSSL 0.15\n+                    if hasattr(connection, 'get_protocol_version_name'):  # requires pyOPenSSL 16.0.0\n                         logger.debug('SSL connection to %s using protocol %s, cipher %s',\n                                      self._hostnameASCII,\n                                      connection.get_protocol_version_name(),\n                                      connection.get_cipher_name(),\n                                      )\n+                    else:\n+                        logger.debug('SSL connection to %s using cipher %s',\n+                                     self._hostnameASCII,\n+                                     connection.get_cipher_name(),\n+                                     )\n                 server_cert = connection.get_peer_certificate()\n                 logger.debug('SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                              x509name_to_string(server_cert.get_issuer()),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1130711cc116b8902c8f743157299ee92b16ee96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 97 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -4,8 +4,19 @@ Transitional module for moving to the w3lib library.\n For new code, always import from w3lib.http instead of this module\n \"\"\"\n \n+import warnings\n+\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.decorators import deprecated\n from w3lib.http import *\n \n+\n+warnings.warn(\"Module `scrapy.utils.http` is deprecated, \"\n+              \"Please import from `w3lib.http` instead.\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+\n+@deprecated\n def decode_chunked_transfer(chunked_body):\n     \"\"\"Parsed body received with chunked transfer encoding, and return the\n     decoded body.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64ff3cd6aac9009ab0fe4777224b8457b6abe790", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 77 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,5 +3,13 @@ Transitional module for moving to the w3lib library.\n \n For new code, always import from w3lib.form instead of this module\n \"\"\"\n+import warnings\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from w3lib.form import *\n+\n+\n+warnings.warn(\"Module `scrapy.utils.multipart` is deprecated. \"\n+              \"If you're using `encode_multipart` function, please use \"\n+              \"`urllib3.filepost.encode_multipart_formdata` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cb4477db3e2ac4ee1972ac3832d036a08596fa7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 279 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -3,5 +3,12 @@ Transitional module for moving to the w3lib library.\n \n For new code, always import from w3lib.html instead of this module\n \"\"\"\n+import warnings\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from w3lib.html import *\n+\n+\n+warnings.warn(\"Module `scrapy.utils.markup` is deprecated. \"\n+              \"Please import from `w3lib.html` instead.\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b9dce3a6c17d8dc827df57367383e0b82fa8b07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 10 | Files Changed: 5 | Hunks: 14 | Methods Changed: 9 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 38 | Churn Cumulative: 2611 | Contributors (this commit): 54 | Commits (past 90d): 8 | Contributors (cumulative): 79 | DMM Complexity: 0.9230769230769231\n\nDIFF:\n@@ -2,6 +2,7 @@ from OpenSSL import SSL\n from twisted.internet.ssl import ClientContextFactory\n \n from scrapy import twisted_version\n+from scrapy.utils.misc import create_instance\n \n if twisted_version >= (14, 0, 0):\n \n@@ -28,9 +29,17 @@ if twisted_version >= (14, 0, 0):\n          understand the SSLv3, TLSv1, TLSv1.1 and TLSv1.2 protocols.'\n         \"\"\"\n \n-        def __init__(self, method=SSL.SSLv23_METHOD, *args, **kwargs):\n+        def __init__(self, method=SSL.SSLv23_METHOD, settings=None, *args, **kwargs):\n             super(ScrapyClientContextFactory, self).__init__(*args, **kwargs)\n             self._ssl_method = method\n+            if settings:\n+                self.tls_verbose_logging = settings['DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING']\n+            else:\n+                self.tls_verbose_logging = False\n+\n+        @classmethod\n+        def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):\n+            return cls(method=method, settings=settings, *args, **kwargs)\n \n         def getCertificateOptions(self):\n             # setting verify=True will require you to provide CAs\n@@ -56,7 +65,8 @@ if twisted_version >= (14, 0, 0):\n             return self.getCertificateOptions().getContext()\n \n         def creatorForNetloc(self, hostname, port):\n-            return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext())\n+            return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext(),\n+                                          verbose_logging=self.tls_verbose_logging)\n \n \n     @implementer(IPolicyForHTTPS)\n\n@@ -1,7 +1,7 @@\n \"\"\"Download handlers for http and https schemes\n \"\"\"\n from twisted.internet import reactor\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import load_object, create_instance\n from scrapy.utils.python import to_unicode\n \n \n@@ -11,6 +11,7 @@ class HTTP10DownloadHandler(object):\n     def __init__(self, settings):\n         self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n         self.ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n+        self._settings = settings\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n@@ -21,7 +22,7 @@ class HTTP10DownloadHandler(object):\n     def _connect(self, factory):\n         host, port = to_unicode(factory.host), factory.port\n         if factory.scheme == b'https':\n-            return reactor.connectSSL(host, port, factory,\n-                                      self.ClientContextFactory())\n+            client_context_factory = create_instance(self.ClientContextFactory, settings=self._settings, crawler=None)\n+            return reactor.connectSSL(host, port, factory, client_context_factory)\n         else:\n             return reactor.connectTCP(host, port, factory)\n\n@@ -25,7 +25,7 @@ from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import _parse\n from scrapy.core.downloader.tls import openssl_methods\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import load_object, create_instance\n from scrapy.utils.python import to_bytes, to_unicode\n from scrapy import twisted_version\n \n@@ -44,14 +44,15 @@ class HTTP11DownloadHandler(object):\n         self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n         # try method-aware context factory\n         try:\n-            self._contextFactory = self._contextFactoryClass(method=self._sslMethod)\n+            self._contextFactory = create_instance(self._contextFactoryClass, settings=settings, crawler=None,\n+                                                   method=self._sslMethod)\n         except TypeError:\n             # use context factory defaults\n-            self._contextFactory = self._contextFactoryClass()\n+            self._contextFactory = create_instance(self._contextFactoryClass, settings=settings, crawler=None)\n             msg = \"\"\"\n  '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n- e.g. OpenSSL.SSL.SSLv23_METHOD).\\\n- Please upgrade your context factory class to handle it or ignore it.\"\"\" % (\n+ e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `settings` argument.\\\n+ Please upgrade your context factory class to handle them or ignore them.\"\"\" % (\n                 settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\n             warnings.warn(msg)\n         self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')\n\n@@ -70,10 +70,15 @@ if twisted_version >= (14, 0, 0):\n         logging warnings. Also, HTTPS connection parameters logging is added.\n         \"\"\"\n \n+        def __init__(self, hostname, ctx, verbose_logging=False):\n+            super().__init__(hostname, ctx)\n+            self.verbose_logging = verbose_logging\n+\n         def _identityVerifyingInfoCallback(self, connection, where, ret):\n             if where & SSL_CB_HANDSHAKE_START:\n                 set_tlsext_host_name(connection, self._hostnameBytes)\n             elif where & SSL_CB_HANDSHAKE_DONE:\n+                if self.verbose_logging:\n                     if hasattr(connection, 'get_cipher_name'):  # requires pyOPenSSL 0.15\n                         if hasattr(connection, 'get_protocol_version_name'):  # requires pyOPenSSL 16.0.0\n                             logger.debug('SSL connection to %s using protocol %s, cipher %s',\n\n@@ -87,6 +87,7 @@ DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClien\n DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'\n DOWNLOADER_CLIENT_TLS_METHOD = 'TLS' # Use highest TLS/SSL protocol version supported by the platform,\n                                      # also allowing negotiation\n+DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = False\n \n DOWNLOADER_MIDDLEWARES = {}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a0b09e0193350426c71f9d321c9b56829ce10143", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 133 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -237,4 +237,13 @@ coverage_ignore_pyobjects = [\n     # their constructor, the methods they reimplement to achieve that purpose\n     # should be irrelevant to developers using those contracts.\n     r'\\w+Contract\\.(adjust_request_args|(pre|post)_process)$',\n+\n+    # Methods of downloader middlewares are not documented, only the classes\n+    # themselves, since downloader middlewares are controlled through Scrapy\n+    # settings.\n+    r'^scrapy\\.downloadermiddlewares\\.\\w*?\\.(\\w*?Middleware|DownloaderStats)\\.',\n+\n+    # Base classes of downloader middlewares are implementation details that\n+    # are not meant for users.\n+    r'^scrapy\\.downloadermiddlewares\\.\\w*?\\.Base\\w*?Middleware',\n ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#578bccf3bbe584ceaba01a37184bb8ad11763866", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 136 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -246,4 +246,7 @@ coverage_ignore_pyobjects = [\n     # Base classes of downloader middlewares are implementation details that\n     # are not meant for users.\n     r'^scrapy\\.downloadermiddlewares\\.\\w*?\\.Base\\w*?Middleware',\n+\n+    # Private exception used by the command-line interface implementation.\n+    r'^scrapy\\.exceptions\\.UsageError',\n ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df68c4b9b1789cc2c7f800f8e4b9f6e5aca7b7af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 33 | Churn Cumulative: 1943 | Contributors (this commit): 51 | Commits (past 90d): 6 | Contributors (cumulative): 78 | DMM Complexity: 1.0\n\nDIFF:\n@@ -157,19 +157,29 @@ class S3FeedStorage(BlockingFeedStorage):\n \n class FTPFeedStorage(BlockingFeedStorage):\n \n-    def __init__(self, uri):\n+    def __init__(self, uri, use_active_mode=False):\n         u = urlparse(uri)\n         self.host = u.hostname\n         self.port = int(u.port or '21')\n         self.username = u.username\n         self.password = u.password\n         self.path = u.path\n+        self.use_active_mode = use_active_mode\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, uri):\n+        return cls(\n+            uri=uri,\n+            use_active_mode=crawler.settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n+        )\n \n     def _store_in_thread(self, file):\n         file.seek(0)\n         ftp = FTP()\n         ftp.connect(self.host, self.port)\n         ftp.login(self.username, self.password)\n+        if self.use_active_mode:\n+            ftp.set_pasv(False)\n         dirname, filename = posixpath.split(self.path)\n         ftp_makedirs_cwd(ftp, dirname)\n         ftp.storbinary('STOR %s' % filename, file)\n\n@@ -158,6 +158,7 @@ FEED_EXPORTERS_BASE = {\n }\n FEED_EXPORT_INDENT = 0\n \n+FEED_STORAGE_FTP_ACTIVE = False\n FEED_STORAGE_S3_ACL = ''\n \n FILES_STORE_S3_ACL = 'private'\n\n@@ -71,6 +71,13 @@ class FileFeedStorageTest(unittest.TestCase):\n \n class FTPFeedStorageTest(unittest.TestCase):\n \n+    def get_test_spider(self, settings=None):\n+        class TestSpider(scrapy.Spider):\n+            name = 'test_spider'\n+        crawler = get_crawler(settings_dict=settings)\n+        spider = TestSpider.from_crawler(crawler)\n+        return spider\n+\n     def test_store(self):\n         uri = os.environ.get('FEEDTEST_FTP_URI')\n         path = os.environ.get('FEEDTEST_FTP_PATH')\n@@ -80,9 +87,20 @@ class FTPFeedStorageTest(unittest.TestCase):\n         verifyObject(IFeedStorage, st)\n         return self._assert_stores(st, path)\n \n+    def test_store_active_mode(self):\n+        uri = os.environ.get('FEEDTEST_FTP_URI')\n+        path = os.environ.get('FEEDTEST_FTP_PATH')\n+        if not (uri and path):\n+            raise unittest.SkipTest(\"No FTP server available for testing\")\n+        use_active_mode = {'FEED_STORAGE_FTP_ACTIVE': True}\n+        crawler = get_crawler(settings_dict=use_active_mode)\n+        st = FTPFeedStorage.from_crawler(crawler, uri)\n+        verifyObject(IFeedStorage, st)\n+        return self._assert_stores(st, path)\n+\n     @defer.inlineCallbacks\n     def _assert_stores(self, storage, path):\n-        spider = scrapy.Spider(\"default\")\n+        spider = self.get_test_spider()\n         file = storage.open(spider)\n         file.write(b\"content\")\n         yield storage.store(file)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0de6ffc8e1354ac7ffcdf7a75e3ec8d26ed23a01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 160 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -71,7 +71,7 @@ if twisted_version >= (14, 0, 0):\n         \"\"\"\n \n         def __init__(self, hostname, ctx, verbose_logging=False):\n-            super().__init__(hostname, ctx)\n+            super(ScrapyClientTLSOptions, self).__init__(hostname, ctx)\n             self.verbose_logging = verbose_logging\n \n         def _identityVerifyingInfoCallback(self, connection, where, ret):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a96a07bc762287a1f2056d1e142aff9f33e206fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 1143 | Contributors (this commit): 18 | Commits (past 90d): 1 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,6 +8,7 @@ try:\n except ImportError:\n     import mock\n \n+from testfixtures import LogCapture\n from twisted.trial import unittest\n from twisted.protocols.policies import WrappingFactory\n from twisted.python.filepath import FilePath\n@@ -503,6 +504,24 @@ class Http11TestCase(HttpTestCase):\n class Https11TestCase(Http11TestCase):\n     scheme = 'https'\n \n+    tls_log_message = 'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=localhost\", subject \"/C=IE/O=Scrapy/CN=localhost\"'\n+\n+    @defer.inlineCallbacks\n+    def test_tls_logging(self):\n+        download_handler = self.download_handler_cls(Settings({\n+            'DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING': True,\n+        }))\n+        try:\n+            with LogCapture() as log_capture:\n+                request = Request(self.getURL('file'))\n+                d = download_handler.download_request(request, Spider('foo'))\n+                d.addCallback(lambda r: r.body)\n+                d.addCallback(self.assertEqual, b\"0123456789\")\n+                yield d\n+                log_capture.check_present(('scrapy.core.downloader.tls', 'DEBUG', self.tls_log_message))\n+        finally:\n+            yield download_handler.close()\n+\n \n class Https11WrongHostnameTestCase(Http11TestCase):\n     scheme = 'https'\n@@ -523,6 +542,7 @@ class Https11InvalidDNSId(Https11TestCase):\n         super(Https11InvalidDNSId, self).setUp()\n         self.host = '127.0.0.1'\n \n+\n class Https11InvalidDNSPattern(Https11TestCase):\n     \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n \n@@ -534,6 +554,7 @@ class Https11InvalidDNSPattern(Https11TestCase):\n             from service_identity.exceptions import CertificateError\n         except ImportError:\n             raise unittest.SkipTest(\"cryptography lib is too old\")\n+        self.tls_log_message = 'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=127.0.0.1\", subject \"/C=IE/O=Scrapy/CN=127.0.0.1\"'\n         super(Https11InvalidDNSPattern, self).setUp()\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27e63e6890896c86fa4b11dc9951c4773bc2d629", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 254 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -14,6 +14,11 @@ if sys.version_info[0] == 2:\n         from urlparse import uses_query\n         uses_query.append('s3')\n \n+    # Prevent the DeprecationWarning about SafeConfigParser -> ConfigParser\n+    import configparser\n+    if not getattr(configparser, 'ConfigParser', None):\n+        configparser.ConfigParser = configparser.SafeConfigParser\n+\n \n # Undo what Twisted's perspective broker adds to pickle register\n # to prevent bugs like Twisted#7989 while serializing requests\n\n@@ -1,10 +1,10 @@\n import os\n import sys\n import numbers\n+import configparser\n from operator import itemgetter\n \n import six\n-from six.moves.configparser import SafeConfigParser\n \n from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n@@ -92,9 +92,9 @@ def init_env(project='default', set_syspath=True):\n \n \n def get_config(use_closest=True):\n-    \"\"\"Get Scrapy config file as a SafeConfigParser\"\"\"\n+    \"\"\"Get Scrapy config file as a ConfigParser\"\"\"\n     sources = get_sources(use_closest)\n-    cfg = SafeConfigParser()\n+    cfg = configparser.ConfigParser()\n     cfg.read(sources)\n     return cfg\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b714a372e21bed55d10539297bd90ef091f9ef43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 257 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -170,7 +170,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         status, out, stderr = yield self.execute(\n             ['--spider', self.spider_name, '-c', 'dummy', self.url('/html')]\n         )\n-        self.assertRegexpMatches(_textmode(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"Cannot find callback\"\"\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n@@ -195,7 +195,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         status, out, stderr = yield self.execute(\n             ['--spider', self.spider_name, '-r', self.url('/html')]\n         )\n-        self.assertRegexpMatches(_textmode(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"No CrawlSpider rules found\"\"\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n@@ -203,7 +203,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         status, out, stderr = yield self.execute(\n             ['--spider', 'badcrawl'+self.spider_name, '-r', self.url('/html')]\n         )\n-        self.assertRegexpMatches(_textmode(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n \n     @defer.inlineCallbacks\n     def test_crawlspider_no_matching_rule(self):\n@@ -211,5 +211,5 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         status, out, stderr = yield self.execute(\n             ['--spider', 'badcrawl'+self.spider_name, '-r', self.url('/enc-gb18030')]\n         )\n-        self.assertRegexpMatches(_textmode(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", _textmode(stderr))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c24b80e1e6df7bd04cca77d068df2656d36a8753", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1607 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -479,7 +479,7 @@ class _ResponseReader(protocol.Protocol):\n                 return\n \n             elif not self._fail_on_dataloss_warned:\n-                logger.warn(\"Got data loss in %s. If you want to process broken \"\n+                logger.warning(\"Got data loss in %s. If you want to process broken \"\n                                \"responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False\"\n                                \" -- This message won't be shown in further requests\",\n                                self._txresponse.request.absoluteURI.decode())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92d624c16189ced0d03eea8cf9db5fd400ebfabd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 54 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -44,6 +44,8 @@ import sys\n if sys.version_info[0] == 2:\n     import unittest\n     import twisted.trial.unittest\n+    if not getattr(unittest.TestCase, 'assertRegex', None):\n+        unittest.TestCase.assertRegex = unittest.TestCase.assertRegexpMatches\n     if not getattr(unittest.TestCase, 'assertRaisesRegex', None):\n         unittest.TestCase.assertRaisesRegex = unittest.TestCase.assertRaisesRegexp\n     if not getattr(twisted.trial.unittest.TestCase, 'assertRaisesRegex', None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40086dabb85a7c463f5a479e0c32d94c0b9463af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 8 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 3853 | Contributors (this commit): 40 | Commits (past 90d): 15 | Contributors (cumulative): 72 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,7 @@ class ReceivedDataProtocol(Protocol):\n     def close(self):\n         self.body.close() if self.filename else self.body.seek(0)\n \n-_CODE_RE = re.compile(\"\\d+\")\n+_CODE_RE = re.compile(r\"\\d+\")\n \n \n class FTPDownloadHandler(object):\n\n@@ -101,7 +101,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     for it.\n     \"\"\"\n \n-    _responseMatcher = re.compile(b'HTTP/1\\.. (?P<status>\\d{3})(?P<reason>.{,32})')\n+    _responseMatcher = re.compile(br'HTTP/1\\.. (?P<status>\\d{3})(?P<reason>.{,32})')\n \n     def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                  timeout=30, bindAddress=None):\n\n@@ -18,7 +18,7 @@ def render_templatefile(path, **kwargs):\n         os.remove(path)\n \n \n-CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z\\d]')\n+CAMELCASE_INVALID_CHARS = re.compile(r'[^a-zA-Z\\d]')\n def string_camelcase(string):\n     \"\"\" Convert a word  to its CamelCase version and remove invalid chars\n \n\n@@ -13,7 +13,7 @@ from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n class CookiesMiddlewareTest(TestCase):\n \n     def assertCookieValEqual(self, first, second, msg=None):\n-        cookievaleq = lambda cv: re.split(';\\s*', cv.decode('latin1'))\n+        cookievaleq = lambda cv: re.split(r';\\s*', cv.decode('latin1'))\n         return self.assertEqual(\n             sorted(cookievaleq(first)),\n             sorted(cookievaleq(second)), msg)\n\n@@ -40,9 +40,9 @@ class TestSpider(Spider):\n     name = \"scrapytest.org\"\n     allowed_domains = [\"scrapytest.org\", \"localhost\"]\n \n-    itemurl_re = re.compile(\"item\\d+.html\")\n-    name_re = re.compile(\"<h1>(.*?)</h1>\", re.M)\n-    price_re = re.compile(\">Price: \\$(.*?)<\", re.M)\n+    itemurl_re = re.compile(r\"item\\d+.html\")\n+    name_re = re.compile(r\"<h1>(.*?)</h1>\", re.M)\n+    price_re = re.compile(r\">Price: \\$(.*?)<\", re.M)\n \n     item_cls = TestItem\n \n\n@@ -288,7 +288,7 @@ class Base:\n             response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html, encoding='windows-1252')\n \n             def process_value(value):\n-                m = re.search(\"javascript:goToPage\\('(.*?)'\", value)\n+                m = re.search(r\"javascript:goToPage\\('(.*?)'\", value)\n                 if m:\n                     return m.group(1)\n \n\n@@ -691,7 +691,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n         self.assertTrue(l.selector)\n         l.add_css('url', 'a::attr(href)')\n         self.assertEqual(l.get_output_value('url'), [u'http://www.scrapy.org'])\n-        l.replace_css('url', 'a::attr(href)', re='http://www\\.(.+)')\n+        l.replace_css('url', 'a::attr(href)', re=r'http://www\\.(.+)')\n         self.assertEqual(l.get_output_value('url'), [u'scrapy.org'])\n \n \n\n@@ -234,7 +234,7 @@ for k, args in enumerate ([\n \n # TODO: the following tests do not pass with current implementation\n for k, args in enumerate([\n-            ('C:\\absolute\\path\\to\\a\\file.html',     'file://',\n+            (r'C:\\absolute\\path\\to\\a\\file.html', 'file://',\n              'Windows filepath are not supported for scrapy shell'),\n         ], start=1):\n     t_method = create_skipped_scheme_t(args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eced544d64ac7398133781ecefed178ed5178892", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 12 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 844 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 27 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,16 +4,22 @@ Scrapy Item\n See documentation in docs/topics/item.rst\n \"\"\"\n \n-from pprint import pformat\n-from collections import MutableMapping\n-from copy import deepcopy\n-\n from abc import ABCMeta\n+from pprint import pformat\n+from copy import deepcopy\n+import collections\n+\n import six\n \n from scrapy.utils.trackref import object_ref\n \n \n+if six.PY3:\n+    MutableMapping = collections.abc.MutableMapping\n+else:\n+    MutableMapping = collections.MutableMapping\n+\n+\n class BaseItem(object_ref):\n     \"\"\"Base class for all scraped items.\"\"\"\n     pass\n\n@@ -6,13 +6,20 @@ This module must not depend on any module outside the Standard Library.\n \"\"\"\n \n import copy\n-import six\n+import collections\n import warnings\n-from collections import OrderedDict, Mapping\n+\n+import six\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n+if six.PY3:\n+    Mapping = collections.abc.Mapping\n+else:\n+    Mapping = collections.Mapping\n+\n+\n class MultiValueDictKeyError(KeyError):\n     def __init__(self, *args, **kwargs):\n         warnings.warn(\n@@ -289,7 +296,7 @@ class MergeDict(object):\n         return self.__copy__()\n \n \n-class LocalCache(OrderedDict):\n+class LocalCache(collections.OrderedDict):\n     \"\"\"Dictionary with a finite number of keys.\n \n     Older items expires first.\n\n@@ -1,6 +1,5 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n-import re\n from twisted.internet import reactor, error\n from twisted.internet.defer import Deferred, DeferredList, maybeDeferred\n from twisted.python import failure\n@@ -31,18 +30,16 @@ class RobotsTxtMiddlewareTest(unittest.TestCase):\n     def _get_successful_crawler(self):\n         crawler = self.crawler\n         crawler.settings.set('ROBOTSTXT_OBEY', True)\n-        ROBOTS = re.sub(b'^\\s+(?m)', b'', u'''\n+        ROBOTS = u\"\"\"\n User-Agent: *\n Disallow: /admin/\n Disallow: /static/\n-\n # taken from https://en.wikipedia.org/robots.txt\n Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\n Disallow: /wiki/Käyttäjä:\n-\n User-Agent: UnicödeBöt\n Disallow: /some/randome/page.html\n-        '''.encode('utf-8'))\n+\"\"\".encode('utf-8')\n         response = TextResponse('http://site.local/robots.txt', body=ROBOTS)\n         def return_response(request, spider):\n             deferred = Deferred()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62f3e22481145bf023cb43b95388dbcfd75a198d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 318 | Contributors (this commit): 12 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,10 +3,13 @@ This module provides some commonly used processors for Item Loaders.\n \n See documentation in docs/topics/loaders.rst\n \"\"\"\n+try:\n+    from collections import ChainMap\n+except ImportError:\n+    from scrapy.utils.datatypes import MergeDict as ChainMap\n \n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.datatypes import MergeDict\n-from .common import wrap_loader_context\n+from scrapy.loader.common import wrap_loader_context\n \n \n class MapCompose(object):\n@@ -18,7 +21,7 @@ class MapCompose(object):\n     def __call__(self, value, loader_context=None):\n         values = arg_to_iter(value)\n         if loader_context:\n-            context = MergeDict(loader_context, self.default_loader_context)\n+            context = ChainMap(loader_context, self.default_loader_context)\n         else:\n             context = self.default_loader_context\n         wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n@@ -45,7 +48,7 @@ class Compose(object):\n \n     def __call__(self, value, loader_context=None):\n         if loader_context:\n-            context = MergeDict(loader_context, self.default_loader_context)\n+            context = ChainMap(loader_context, self.default_loader_context)\n         else:\n             context = self.default_loader_context\n         wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n\n@@ -245,6 +245,13 @@ class MergeDict(object):\n     first occurrence will be used.\n     \"\"\"\n     def __init__(self, *dicts):\n+        if six.PY3:\n+            warnings.warn(\n+                \"scrapy.utils.datatypes.MergeDict is deprecated in favor \"\n+                \"of collections.ChainMap (introduced in Python 3.3)\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n         self.dicts = dicts\n \n     def __getitem__(self, key):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ef9a61921482f1a0004616e0b2b6734b9314655d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 19 | Churn Cumulative: 1002 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,17 @@\n import six\n import json\n import copy\n-from collections import MutableMapping\n+import collections\n from importlib import import_module\n from pprint import pformat\n \n-from . import default_settings\n+from scrapy.settings import default_settings\n+\n+\n+if six.PY3:\n+    MutableMapping = collections.abc.MutableMapping\n+else:\n+    MutableMapping = collections.MutableMapping\n \n \n SETTINGS_PRIORITIES = {\n\n@@ -1,11 +1,18 @@\n import copy\n import unittest\n-from collections import Mapping, MutableMapping\n+\n+import six\n+if six.PY3:\n+    from collections.abc import Mapping, MutableMapping\n+else:\n+    from collections.abc import Mapping, MutableMapping\n \n from scrapy.utils.datatypes import CaselessDict, SequenceExclude\n \n+\n __doctests__ = ['scrapy.utils.datatypes']\n \n+\n class CaselessDictTest(unittest.TestCase):\n \n     def test_init_dict(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d5a2a7032979c6af8b074ec7fd60baa5256cc7cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 130 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ import six\n if six.PY3:\n     from collections.abc import Mapping, MutableMapping\n else:\n-    from collections.abc import Mapping, MutableMapping\n+    from collections import Mapping, MutableMapping\n \n from scrapy.utils.datatypes import CaselessDict, SequenceExclude\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e3a602d569bd81e9bd891a482f440a93dbe7bb4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 21 | Files Changed: 6 | Hunks: 12 | Methods Changed: 0 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 38 | Churn Cumulative: 1632 | Contributors (this commit): 21 | Commits (past 90d): 15 | Contributors (cumulative): 43 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,8 @@\n-import sys\n+import six\n from six.moves import copyreg\n \n-if sys.version_info[0] == 2:\n+\n+if six.PY2:\n     from urlparse import urlparse\n \n     # workaround for https://bugs.python.org/issue7904 - Python < 2.7\n@@ -14,11 +15,6 @@ if sys.version_info[0] == 2:\n         from urlparse import uses_query\n         uses_query.append('s3')\n \n-    # Prevent the DeprecationWarning about SafeConfigParser -> ConfigParser\n-    import configparser\n-    if not getattr(configparser, 'ConfigParser', None):\n-        configparser.ConfigParser = configparser.SafeConfigParser\n-\n \n # Undo what Twisted's perspective broker adds to pickle register\n # to prevent bugs like Twisted#7989 while serializing requests\n\n@@ -14,10 +14,10 @@ import six\n from scrapy.utils.trackref import object_ref\n \n \n-if six.PY3:\n-    MutableMapping = collections.abc.MutableMapping\n-else:\n+if six.PY2:\n     MutableMapping = collections.MutableMapping\n+else:\n+    MutableMapping = collections.abc.MutableMapping\n \n \n class BaseItem(object_ref):\n\n@@ -8,10 +8,10 @@ from pprint import pformat\n from scrapy.settings import default_settings\n \n \n-if six.PY3:\n-    MutableMapping = collections.abc.MutableMapping\n-else:\n+if six.PY2:\n     MutableMapping = collections.MutableMapping\n+else:\n+    MutableMapping = collections.abc.MutableMapping\n \n \n SETTINGS_PRIORITIES = {\n\n@@ -14,10 +14,10 @@ import six\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n-if six.PY3:\n-    Mapping = collections.abc.Mapping\n-else:\n+if six.PY2:\n     Mapping = collections.Mapping\n+else:\n+    Mapping = collections.abc.Mapping\n \n \n class MultiValueDictKeyError(KeyError):\n\n@@ -40,8 +40,8 @@ def get_testdata(*paths):\n # FIXME: delete after dropping py2 support\n # Monkey patch the unittest module to prevent the\n # DeprecationWarning about assertRaisesRegexp -> assertRaisesRegex\n-import sys\n-if sys.version_info[0] == 2:\n+import six\n+if six.PY2:\n     import unittest\n     import twisted.trial.unittest\n     if not getattr(unittest.TestCase, 'assertRegex', None):\n\n@@ -2,10 +2,10 @@ import copy\n import unittest\n \n import six\n-if six.PY3:\n-    from collections.abc import Mapping, MutableMapping\n-else:\n+if six.PY2:\n     from collections import Mapping, MutableMapping\n+else:\n+    from collections.abc import Mapping, MutableMapping\n \n from scrapy.utils.datatypes import CaselessDict, SequenceExclude\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d7074d86d26c936c6907dea7c550a4f251667d8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 272 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -245,7 +245,7 @@ class MergeDict(object):\n     first occurrence will be used.\n     \"\"\"\n     def __init__(self, *dicts):\n-        if six.PY3:\n+        if not six.PY2:\n             warnings.warn(\n                 \"scrapy.utils.datatypes.MergeDict is deprecated in favor \"\n                 \"of collections.ChainMap (introduced in Python 3.3)\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d51f9cc276fc6acaa815ee6d8af4e770a3ef9cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1551 | Contributors (this commit): 19 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -39,14 +39,15 @@ class Crawler(object):\n         self.settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n \n-        d = dict(overridden_settings(self.settings))\n-        logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n-\n         self.signals = SignalManager(self)\n         self.stats = load_object(self.settings['STATS_CLASS'])(self)\n \n         handler = LogCounterHandler(self, level=self.settings.get('LOG_LEVEL'))\n         logging.root.addHandler(handler)\n+\n+        d = dict(overridden_settings(self.settings))\n+        logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n+\n         if get_scrapy_root_handler() is not None:\n             # scrapy root handler already installed: update it with new settings\n             install_scrapy_root_handler(self.settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a94b5bef3a6ae658ec58a9a17bd149453aa855a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 147 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -30,6 +30,7 @@ extensions = [\n     'scrapydocs',\n     'sphinx.ext.autodoc',\n     'sphinx.ext.coverage',\n+    'sphinx.ext.intersphinx',\n ]\n \n # Add any paths that contain templates here, relative to this directory.\n@@ -74,6 +75,8 @@ language = 'en'\n # List of documents that shouldn't be included in the build.\n #unused_docs = []\n \n+exclude_patterns = ['build']\n+\n # List of directories, relative to source directory, that shouldn't be searched\n # for source files.\n exclude_trees = ['.build']\n@@ -250,3 +253,11 @@ coverage_ignore_pyobjects = [\n     # Private exception used by the command-line interface implementation.\n     r'^scrapy\\.exceptions\\.UsageError',\n ]\n+\n+\n+# Options for the InterSphinx extension\n+# -------------------------------------\n+\n+intersphinx_mapping = {\n+    'python': ('https://docs.python.org/3', None),\n+}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42743fd9dd9d7116848fd3ad6b657453dd0b117d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 1877 | Contributors (this commit): 22 | Commits (past 90d): 6 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,17 +29,18 @@ if twisted_version >= (14, 0, 0):\n          understand the SSLv3, TLSv1, TLSv1.1 and TLSv1.2 protocols.'\n         \"\"\"\n \n-        def __init__(self, method=SSL.SSLv23_METHOD, settings=None, *args, **kwargs):\n+        def __init__(self, method=SSL.SSLv23_METHOD, tls_verbose_logging=False, *args, **kwargs):\n             super(ScrapyClientContextFactory, self).__init__(*args, **kwargs)\n             self._ssl_method = method\n-            if settings:\n-                self.tls_verbose_logging = settings['DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING']\n-            else:\n-                self.tls_verbose_logging = False\n+            self.tls_verbose_logging = tls_verbose_logging\n \n         @classmethod\n         def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):\n-            return cls(method=method, settings=settings, *args, **kwargs)\n+            if settings:\n+                tls_verbose_logging = settings.getbool('DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING')\n+            else:\n+                tls_verbose_logging = False\n+            return cls(method=method, tls_verbose_logging=tls_verbose_logging, *args, **kwargs)\n \n         def getCertificateOptions(self):\n             # setting verify=True will require you to provide CAs\n\n@@ -51,7 +51,7 @@ class HTTP11DownloadHandler(object):\n             self._contextFactory = create_instance(self._contextFactoryClass, settings=settings, crawler=None)\n             msg = \"\"\"\n  '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n- e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `settings` argument.\\\n+ e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `tls_verbose_logging` argument.\\\n  Please upgrade your context factory class to handle them or ignore them.\"\"\" % (\n                 settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\n             warnings.warn(msg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
