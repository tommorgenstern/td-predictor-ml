{"custom_id": "keras#19c986adef76cd22f11e4349ef27b2a766e27f1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 160 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 160 | Churn Cumulative: 10268 | Contributors (this commit): 8 | Commits (past 90d): 12 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5187,6 +5187,107 @@ def binary_crossentropy(target, output, from_logits=False):\n   return -bce\n \n \n+@keras_export('keras.backend.binary_focal_crossentropy')\n+@tf.__internal__.dispatch.add_dispatch_support\n+@doc_controls.do_not_generate_docs\n+def binary_focal_crossentropy(\n+    target,\n+    output,\n+    gamma=2.0,\n+    from_logits=False,\n+):\n+  \"\"\"Binary focal crossentropy between an output tensor and a target tensor.\n+\n+  According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+  helps to apply a focal factor to down-weight easy examples and focus more on\n+  hard examples. By default, the focal tensor is computed as follows:\n+\n+  `focal_factor = (1 - output)**gamma` for class 1\n+  `focal_factor = output**gamma` for class 0\n+  where `gamma` is a focusing parameter. When `gamma` = 0, this function is\n+  equivalent to the binary crossentropy.\n+\n+  Args:\n+    target: A tensor with the same shape as `output`.\n+    output: A tensor.\n+    gamma: A focusing parameter used to compute the focal factor, default is 2.0\n+      as mentioned in reference.\n+    from_logits: Whether `output` is expected to be a logits tensor. By default,\n+      we consider that `output` encodes a probability distribution.\n+\n+  Returns:\n+    A tensor.\n+  \"\"\"\n+  sigmoidal = tf.__internal__.smart_cond.smart_cond(\n+      from_logits,\n+      lambda: sigmoid(output),\n+      lambda: output,\n+  )\n+  p_t = (target * sigmoidal) + ((1 - target) * (1 - sigmoidal))\n+  # Calculate focal factor\n+  focal_factor = tf.pow(1.0 - p_t, gamma)\n+  # Binary crossentropy\n+  bce = binary_crossentropy(\n+      target=target,\n+      output=output,\n+      from_logits=from_logits,\n+  )\n+  return focal_factor * bce\n+\n+\n+@keras_export('keras.backend.binary_weighted_focal_crossentropy')\n+@tf.__internal__.dispatch.add_dispatch_support\n+@doc_controls.do_not_generate_docs\n+def binary_weighted_focal_crossentropy(\n+    target,\n+    output,\n+    alpha=0.25,\n+    gamma=2.0,\n+    from_logits=False,\n+):\n+  \"\"\"Binary weighted focal crossentropy between an output tensor and a target.\n+\n+  According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+  helps to apply a focal factor to down-weight easy examples and focus more on\n+  hard examples. By default, the focal tensor is computed as follows:\n+\n+  `focal_factor = (1 - output)**gamma` for class 1\n+  `focal_factor = output**gamma` for class 0\n+  where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n+  effect on the binary crossentropy.\n+\n+  This function also takes into account a weight balancing factor for the binary\n+  classes 0 and 1 as follows:\n+\n+  `weight = alpha` for class 1 (`target` = 1)\n+  `weight = 1 - alpha` for class 0\n+  where `alpha` is a float in the range of [0, 1].\n+\n+  Args:\n+    target: A tensor with the same shape as `output`.\n+    output: A tensor.\n+    alpha: A weight balancing factor for class 1, default is 0.25 as mentioned\n+    in reference. The weight for class 0 is 1.0 - `alpha`.\n+    gamma: A focusing parameter, default is 2.0 as mentioned in reference.\n+    from_logits: Whether `output` is expected to be a logits tensor. By default,\n+      we consider that `output` encodes a probability distribution.\n+\n+  Returns:\n+    A tensor.\n+  \"\"\"\n+  # Balancing weight for the binary classes\n+  weight = target * alpha + (1 - target) * (1 - alpha)\n+\n+  # Binary focal crossentropy\n+  bfce = binary_focal_crossentropy(\n+      target=target,\n+      output=output,\n+      gamma=gamma,\n+      from_logits=from_logits,\n+  )\n+  return weight * bfce\n+\n+\n @keras_export('keras.backend.sigmoid')\n @tf.__internal__.dispatch.add_dispatch_support\n @doc_controls.do_not_generate_docs\n\n@@ -1804,6 +1804,65 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertLen(w, 1)\n       self.assertIn('received `from_logits=True`', str(w[0].message))\n \n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_binary_focal_crossentropy_with_sigmoid(self):\n+    t = backend.constant([[0, 1, 0]])\n+    logits = backend.constant([[8., 1., 1.]])\n+    p = backend.sigmoid(logits)\n+    p = tf.identity(tf.identity(p))\n+    result = self.evaluate(backend.binary_focal_crossentropy(t, p, gamma=2.0))\n+    self.assertArrayNear(result[0], [7.995, 0.022, 0.701], 1e-3)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_binary_focal_crossentropy_from_logits(self):\n+    t = backend.constant([[0, 1, 0]])\n+    logits = backend.constant([[8., 1., 1.]])\n+    result = self.evaluate(\n+        backend.binary_focal_crossentropy(\n+            target=t,\n+            output=logits,\n+            gamma=2.0,\n+            from_logits=True,\n+        ))\n+    self.assertArrayNear(result[0], [7.995, 0.022, 0.701], 1e-3)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_binary_focal_crossentropy_no_focal_effect_with_zero_gamma(self):\n+    t = backend.constant([[0, 1, 0]])\n+    logits = backend.constant([[8., 1., 1.]])\n+    p = backend.sigmoid(logits)\n+    p = tf.identity(tf.identity(p))\n+    gamma = 0\n+    focal_result = self.evaluate(\n+        backend.binary_focal_crossentropy(\n+            target=t,\n+            output=p,\n+            gamma=gamma,\n+        ))\n+    non_focal_result = self.evaluate(backend.binary_crossentropy(t, p))\n+    self.assertArrayNear(focal_result[0], non_focal_result[0], 1e-3)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_binary_weighted_focal_crossentropy_with_sigmoid(self):\n+    t = backend.constant([[0, 1, 0]])\n+    logits = backend.constant([[8., 1., 1.]])\n+    p = backend.sigmoid(logits)\n+    p = tf.identity(tf.identity(p))\n+    result = self.evaluate(backend.binary_weighted_focal_crossentropy(t, p))\n+    self.assertArrayNear(result[0], [5.996, 0.006, 0.526], 1e-3)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_binary_weighted_focal_crossentropy_from_logits(self):\n+    t = backend.constant([[0, 1, 0]])\n+    logits = backend.constant([[8., 1., 1.]])\n+    result = self.evaluate(\n+        backend.binary_weighted_focal_crossentropy(\n+            target=t,\n+            output=logits,\n+            from_logits=True,\n+        ))\n+    self.assertArrayNear(result[0], [5.996, 0.006, 0.526], 1e-3)\n+\n \n @test_util.with_control_flow_v2\n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b11e4927a0c4cf238f962b666fb74e5ef9d6a87c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 95 | Churn Cumulative: 3347 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -849,6 +849,101 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     os.remove(filepath.format(epoch=5, batch=1))\n     os.remove(filepath.format(epoch=5, batch=2))\n     \n+    # Case 12: ModelCheckpoint saves model with initial_best param\n+    mode = 'max'\n+    monitor = 'val_acc'\n+    initial_best = 0\n+    cbks = [\n+        keras.callbacks.ModelCheckpoint(\n+            filepath,\n+            monitor=monitor,\n+            save_best_only=save_best_only,\n+            initial_best=initial_best,\n+            save_best_only=True,\n+            mode=mode)\n+    ]\n+    model.fit(\n+        x_train,\n+        y_train,\n+        batch_size=BATCH_SIZE,\n+        validation_data=(x_test, y_test),\n+        callbacks=cbks,\n+        epochs=1,\n+        verbose=0)\n+    assert os.path.exists(filepath)\n+    os.remove(filepath)\n+    \n+    # Case 13: ModelCheckpoint saves model with initial_best param\n+    mode = 'auto'\n+    monitor = 'val_loss'\n+    initial_best = None\n+    cbks = [\n+        keras.callbacks.ModelCheckpoint(\n+            filepath,\n+            monitor=monitor,\n+            save_best_only=save_best_only,\n+            initial_best=initial_best,\n+            save_best_only=True,\n+            mode=mode)\n+    ]\n+    model.fit(\n+        x_train,\n+        y_train,\n+        batch_size=BATCH_SIZE,\n+        validation_data=(x_test, y_test),\n+        callbacks=cbks,\n+        epochs=1,\n+        verbose=0)\n+    assert os.path.exists(filepath)\n+    os.remove(filepath)\n+    \n+    # Case 14: ModelCheckpoint doesnt save model if loss was minimum earlier\n+    mode = 'min'\n+    monitor = 'val_loss'\n+    initial_best = 0\n+    cbks = [\n+        keras.callbacks.ModelCheckpoint(\n+            filepath,\n+            monitor=monitor,\n+            save_best_only=save_best_only,\n+            initial_best=initial_best,\n+            save_best_only=True,\n+            mode=mode)\n+    ]\n+    model.fit(\n+        x_train,\n+        y_train,\n+        batch_size=BATCH_SIZE,\n+        validation_data=(x_test, y_test),\n+        callbacks=cbks,\n+        epochs=1,\n+        verbose=0)\n+    assert not os.path.exists(filepath)\n+    \n+    # Case 15: ModelCheckpoint doesnt save model if loss was min earlier in auto mode\n+    mode = 'auto'\n+    monitor = 'val_loss'\n+    initial_best = 0\n+    cbks = [\n+        keras.callbacks.ModelCheckpoint(\n+            filepath,\n+            monitor=monitor,\n+            save_best_only=save_best_only,\n+            initial_best=initial_best,\n+            save_best_only=True,\n+            mode=mode)\n+    ]\n+    model.fit(\n+        x_train,\n+        y_train,\n+        batch_size=BATCH_SIZE,\n+        validation_data=(x_test, y_test),\n+        callbacks=cbks,\n+        epochs=1,\n+        verbose=0)\n+    assert not os.path.exists(filepath)\n+    \n+\n   @testing_utils.run_v2_only\n   def test_ModelCheckpoint_subclass_save_weights_false(self):\n     model = testing_utils.get_small_subclass_mlp(NUM_HIDDEN, NUM_CLASSES)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07e2211a62259cf0df7e4d3d34217b01e2ea18a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 72 | Churn Cumulative: 7693 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: 0.9655172413793104\n\nDIFF:\n@@ -284,9 +284,12 @@ def clear_session():\n   graph = get_graph()\n   with graph.as_default():\n     _DUMMY_EAGER_GRAPH.learning_phase_is_set = False\n-    _GRAPH_LEARNING_PHASES.clear()\n-    # Create the learning phase placeholder in graph using the default factory.\n-    _GRAPH_LEARNING_PHASES.setdefault(graph)\n+\n+    _GRAPH_LEARNING_PHASES = {}\n+    # Create the learning phase placeholder in graph using the default factory\n+    phase = _default_learning_phase()\n+    _internal_set_learning_phase(graph, phase)\n+\n     _GRAPH_VARIABLES.pop(graph, None)\n     _GRAPH_TF_OPTIMIZERS.pop(graph, None)\n   if tf.executing_eagerly():\n@@ -342,7 +345,14 @@ def learning_phase():\n       # This is because functions & defuns (both in graph & in eager mode)\n       # will always execute non-eagerly using a function-specific default\n       # subgraph.\n-      learning_phase = _GRAPH_LEARNING_PHASES[None]\n+      if context.executing_eagerly():\n+        if _DUMMY_EAGER_GRAPH.key not in _GRAPH_LEARNING_PHASES:\n+          phase = _default_learning_phase()\n+          _internal_set_learning_phase(_DUMMY_EAGER_GRAPH.key, phase)\n+          _DUMMY_EAGER_GRAPH.learning_phase_is_set = True\n+        return _internal_get_learning_phase(_DUMMY_EAGER_GRAPH.key)\n+      else:\n+        learning_phase = symbolic_learning_phase()\n   _mark_func_graph_as_unsaveable(graph, learning_phase)\n   return learning_phase\n \n@@ -373,11 +383,38 @@ def _mark_func_graph_as_unsaveable(graph, learning_phase):\n def symbolic_learning_phase():\n   graph = get_graph()\n   with graph.as_default():\n-    return _GRAPH_LEARNING_PHASES[graph]\n+    if graph not in _GRAPH_LEARNING_PHASES:\n+        phase = _default_learning_phase()\n+        _internal_set_learning_phase(graph, phase)\n+\n+    return _internal_get_learning_phase(graph)\n+\n+\n+def _internal_set_learning_phase(graph, value):\n+  global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n+\n+  if isinstance(value, tf.Tensor):\n+    \"\"\"The 'value' here is a tf.Tensor with attribute 'graph'.\n+    There is a circular reference between key 'graph' and attribute 'graph'.\n+    So we need use a weakref.ref to refer to the 'value' tensor here.\n+    Otherwise, it would lead to memory leak.\n+    \"\"\"\n+    value_ref = weakref.ref(value)\n+    _GRAPH_LEARNING_PHASES[graph] = value_ref\n+  else:\n+    _GRAPH_LEARNING_PHASES[graph] = value\n+\n+\n+def _internal_get_learning_phase(graph):\n+  phase = _GRAPH_LEARNING_PHASES.get(graph, None)\n+  if isinstance(phase, weakref.ref):\n+    return phase()\n+  else:\n+    return phase\n \n \n def _default_learning_phase():\n-  if tf.executing_eagerly():\n+  if context.executing_eagerly():\n     return 0\n   else:\n     with name_scope(''):\n@@ -439,7 +476,6 @@ def deprecated_internal_set_learning_phase(value):\n   Raises:\n       ValueError: if `value` is neither `0` nor `1`.\n   \"\"\"\n-  global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n   if value not in {0, 1}:\n     raise ValueError('Expected learning phase to be 0 or 1.')\n   with tf.init_scope():\n@@ -447,8 +483,9 @@ def deprecated_internal_set_learning_phase(value):\n       # In an eager context, the learning phase values applies to both the eager\n       # context and the internal Keras graph.\n       _DUMMY_EAGER_GRAPH.learning_phase_is_set = True\n-      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = value\n-    _GRAPH_LEARNING_PHASES[get_graph()] = value\n+      _internal_set_learning_phase(_DUMMY_EAGER_GRAPH.key, value)\n+\n+    _internal_set_learning_phase(get_graph(), value)\n \n \n @keras_export('keras.backend.learning_phase_scope')\n@@ -510,9 +547,9 @@ def deprecated_internal_learning_phase_scope(value):\n \n   with tf.init_scope():\n     if tf.executing_eagerly():\n-      previous_eager_value = _GRAPH_LEARNING_PHASES.get(\n-          _DUMMY_EAGER_GRAPH.key, None)\n-    previous_graph_value = _GRAPH_LEARNING_PHASES.get(get_graph(), None)\n+      previous_eager_value = _internal_get_learning_phase(\n+        _DUMMY_EAGER_GRAPH.key)\n+    previous_graph_value = _internal_get_learning_phase(get_graph())\n \n   learning_phase_previously_set = _DUMMY_EAGER_GRAPH.learning_phase_is_set\n   try:\n@@ -525,13 +562,14 @@ def deprecated_internal_learning_phase_scope(value):\n     with tf.init_scope():\n       if tf.executing_eagerly():\n         if previous_eager_value is not None:\n-          _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = previous_eager_value\n+          _internal_set_learning_phase(_DUMMY_EAGER_GRAPH.key,\n+                                       previous_eager_value)\n         elif _DUMMY_EAGER_GRAPH.key in _GRAPH_LEARNING_PHASES:\n           del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key]\n \n       graph = get_graph()\n       if previous_graph_value is not None:\n-        _GRAPH_LEARNING_PHASES[graph] = previous_graph_value\n+        _internal_set_learning_phase(graph, previous_graph_value)\n       elif graph in _GRAPH_LEARNING_PHASES:\n         del _GRAPH_LEARNING_PHASES[graph]\n \n@@ -557,12 +595,12 @@ def eager_learning_phase_scope(value):\n   if global_learning_phase_was_set:\n     previous_value = learning_phase()\n   try:\n-    _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = value\n+    _internal_set_learning_phase(_DUMMY_EAGER_GRAPH.key, value)\n     yield\n   finally:\n     # Restore learning phase to initial value or unset.\n     if global_learning_phase_was_set:\n-      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = previous_value\n+      _internal_set_learning_phase(_DUMMY_EAGER_GRAPH.key, previous_value)\n     else:\n       del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key]\n \n@@ -6889,7 +6927,7 @@ class ContextValueCache(weakref.WeakKeyDictionary):\n # dummy object is used.\n # A learning phase is a bool tensor used to run Keras models in\n # either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n-_GRAPH_LEARNING_PHASES = ContextValueCache(_default_learning_phase)\n+_GRAPH_LEARNING_PHASES = ContextValueCache(object_identity.ObjectIdentityWeakSet)\n \n # This dictionary holds a mapping between a graph and variables to initialize\n # in the graph.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f5efef2ef4d5bd88d5901242dc7ab2300914033c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/5 | Churn Δ: 59 | Churn Cumulative: 3221 | Contributors (this commit): 11 | Commits (past 90d): 22 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,6 +30,7 @@ from keras.engine import training_utils\n from keras.utils import data_utils\n from keras.utils import dataset_creator\n from keras.utils import tf_utils\n+from tensorflow.python.framework import type_spec\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -815,15 +816,10 @@ class GeneratorDataAdapter(DataAdapter):\n \n     self._first_batch_size = int(tf.nest.flatten(peek)[0].shape[0])\n \n-    def _get_dynamic_shape(t):\n-      shape = t.shape\n-      # Unknown number of dimensions, `as_list` cannot be called.\n-      if shape.rank is None:\n-        return shape\n-      return tf.TensorShape([None for _ in shape.as_list()])\n+    def _get_tensor_spec(t):\n+      return type_spec._type_spec_from_value(t)._with_tensor_ranks_only()\n \n-    output_shapes = tf.nest.map_structure(_get_dynamic_shape, peek)\n-    output_types = tf.nest.map_structure(lambda t: t.dtype, peek)\n+    output_signature = nest.map_structure(_get_tensor_spec, peek)\n \n     # Note that dataset API takes a callable that creates a generator object,\n     # rather than generator itself, which is why we define a function here.\n@@ -835,7 +831,7 @@ class GeneratorDataAdapter(DataAdapter):\n         yield self._standardize_batch(data)\n \n     dataset = tf.data.Dataset.from_generator(\n-        wrapped_generator, output_types, output_shapes=output_shapes)\n+        wrapped_generator, output_signature=output_signature)\n \n     if workers == 1 and not use_multiprocessing:\n       dataset = dataset.prefetch(1)\n\n@@ -98,6 +98,30 @@ class TestSequence(data_utils.Sequence):\n     return 10\n \n \n+class TestSparseSequence(TestSequence):\n+\n+  def __getitem__(self, item):\n+    indices = [[row, self.feature_shape-1] for row in range(self.batch_size)]\n+    values = [1 for row in range(self.batch_size)]\n+    st = tf.ops.sparse_tensor.SparseTensor(\n+      indices, values, (self.batch_size,self.feature_shape)\n+    )\n+    return (st, np.ones((self.batch_size,)))\n+\n+\n+class TestRaggedSequence(TestSequence):\n+\n+  def __getitem__(self, item):\n+    values = np.random.randint(\n+      0,\n+      self.feature_shape,\n+      (self.batch_size, 2)\n+    ).reshape(-1)\n+    row_lengths = np.full(self.batch_size, 2)\n+    rt = tf.ops.ragged_tensor.RaggedTensor.from_row_lengths(values, row_lengths)\n+    return (rt, np.ones((self.batch_size,)))\n+\n+\n class TensorLikeDataAdapterTest(DataAdapterTestBase):\n \n   def setUp(self):\n@@ -800,6 +824,27 @@ class KerasSequenceAdapterTest(DataAdapterTestBase):\n       self.adapter_cls(self.sequence_input, sample_weights=self.sequence_input)\n \n \n+class KerasSequenceAdapterSparseTest(KerasSequenceAdapterTest):\n+\n+  def setUp(self):\n+    super(KerasSequenceAdapterSparseTest, self).setUp()\n+    self.sequence_input = TestSparseSequence(self.batch_size, 10)\n+\n+\n+class KerasSequenceAdapterRaggedTest(KerasSequenceAdapterTest):\n+\n+  def setUp(self):\n+    super(KerasSequenceAdapterRaggedTest, self).setUp()\n+    self.sequence_input = TestRaggedSequence(self.batch_size, 10)\n+\n+    self.model = keras.models.Sequential([\n+        keras.layers.Input(shape=(None, ), ragged=True),\n+        keras.layers.Embedding(10, 10),\n+        keras.layers.Lambda(tf.ops.math_ops.reduce_mean, arguments=dict(axis=1)),\n+        keras.layers.Dense(8, input_shape=(10,), activation='relu'),\n+      ])\n+\n+\n class DataHandlerTest(keras_parameterized.TestCase):\n \n   def test_finite_dataset_with_steps_per_epoch(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3677d84ab63f929784abd3511fc3fbb0232730eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 3231 | Contributors (this commit): 11 | Commits (past 90d): 24 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -817,9 +817,9 @@ class GeneratorDataAdapter(DataAdapter):\n     self._first_batch_size = int(tf.nest.flatten(peek)[0].shape[0])\n \n     def _get_tensor_spec(t):\n-      return type_spec._type_spec_from_value(t)._with_tensor_ranks_only()\n+      return type_spec.type_spec_from_value(t)\n \n-    output_signature = nest.map_structure(_get_tensor_spec, peek)\n+    output_signature = tf.nest.map_structure(_get_tensor_spec, peek)\n \n     # Note that dataset API takes a callable that creates a generator object,\n     # rather than generator itself, which is why we define a function here.\n\n@@ -103,7 +103,7 @@ class TestSparseSequence(TestSequence):\n   def __getitem__(self, item):\n     indices = [[row, self.feature_shape-1] for row in range(self.batch_size)]\n     values = [1 for row in range(self.batch_size)]\n-    st = tf.ops.sparse_tensor.SparseTensor(\n+    st = tf.SparseTensor(\n       indices, values, (self.batch_size,self.feature_shape)\n     )\n     return (st, np.ones((self.batch_size,)))\n@@ -118,7 +118,7 @@ class TestRaggedSequence(TestSequence):\n       (self.batch_size, 2)\n     ).reshape(-1)\n     row_lengths = np.full(self.batch_size, 2)\n-    rt = tf.ops.ragged_tensor.RaggedTensor.from_row_lengths(values, row_lengths)\n+    rt = tf.RaggedTensor.from_row_lengths(values, row_lengths)\n     return (rt, np.ones((self.batch_size,)))\n \n \n@@ -840,7 +840,7 @@ class KerasSequenceAdapterRaggedTest(KerasSequenceAdapterTest):\n     self.model = keras.models.Sequential([\n         keras.layers.Input(shape=(None, ), ragged=True),\n         keras.layers.Embedding(10, 10),\n-        keras.layers.Lambda(tf.ops.math_ops.reduce_mean, arguments=dict(axis=1)),\n+        keras.layers.Lambda(tf.reduce_mean, arguments=dict(axis=1)),\n         keras.layers.Dense(8, input_shape=(10,), activation='relu'),\n       ])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6a7cdb506e4371b2b0bddd4189c019787139ade3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2024 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -817,7 +817,7 @@ class GeneratorDataAdapter(DataAdapter):\n     self._first_batch_size = int(tf.nest.flatten(peek)[0].shape[0])\n \n     def _get_tensor_spec(t):\n-      return type_spec.type_spec_from_value(t)\n+      return type_spec.type_spec_from_value(t)._with_tensor_ranks_only()\n \n     output_signature = tf.nest.map_structure(_get_tensor_spec, peek)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#65dcb3fa7827139f6a64ee698d6ccf11175353b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 6 | Churn Cumulative: 7388 | Contributors (this commit): 100 | Commits (past 90d): 14 | Contributors (cumulative): 100 | DMM Complexity: None\n\nDIFF:\n@@ -1253,9 +1253,9 @@ class ModelCheckpoint(Callback):\n       options: Optional `tf.train.CheckpointOptions` object if\n         `save_weights_only` is true or optional `tf.saved_model.SaveOptions`\n         object if `save_weights_only` is false.\n-      initial_best: Initial best value of the metric to be monitored before\n-        training begins. Only overwrites the model weights already saved, if the\n-        performance of current model is better than the previous saved model.\n+      initial_best: Initial \"best\" value of the metric to be monitored.\n+        Only overwrites the model weights already saved if the\n+        performance of current model is better than this value.\n       **kwargs: Additional arguments for backwards compatibility. Possible key\n         is `period`.\n   \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ac7da87e82177c1e652465bd1872be2fe937a92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 7392 | Contributors (this commit): 100 | Commits (past 90d): 15 | Contributors (cumulative): 100 | DMM Complexity: None\n\nDIFF:\n@@ -1269,7 +1269,7 @@ class ModelCheckpoint(Callback):\n                mode='auto',\n                save_freq='epoch',\n                options=None,\n-               initial_best=None,\n+               initial_value_threshold=None,\n                **kwargs):\n     super(ModelCheckpoint, self).__init__()\n     self._supports_tf_logs = True\n@@ -1282,7 +1282,7 @@ class ModelCheckpoint(Callback):\n     self.epochs_since_last_save = 0\n     self._batches_seen_since_last_saving = 0\n     self._last_batch_seen = 0\n-    self.best= initial_best\n+    self.best= initial_value_threshold\n \n     if save_weights_only:\n       if options is None or isinstance(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#49c5a3511ac1e00164540ede1d43554acfe06053", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 3363 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -849,16 +849,16 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     os.remove(filepath.format(epoch=5, batch=1))\n     os.remove(filepath.format(epoch=5, batch=2))\n     \n-    # Case 12: ModelCheckpoint saves model with initial_best param\n+    # Case 12: ModelCheckpoint saves model with initial_value_threshold param\n     mode = 'max'\n     monitor = 'val_acc'\n-    initial_best = 0\n+    initial_value_threshold = 0\n     cbks = [\n         keras.callbacks.ModelCheckpoint(\n             filepath,\n             monitor=monitor,\n             save_best_only=save_best_only,\n-            initial_best=initial_best,\n+            initial_value_threshold=initial_value_threshold,\n             save_best_only=True,\n             mode=mode)\n     ]\n@@ -882,7 +882,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n             filepath,\n             monitor=monitor,\n             save_best_only=save_best_only,\n-            initial_best=initial_best,\n+            initial_value_threshold=initial_value_threshold,\n             save_best_only=True,\n             mode=mode)\n     ]\n@@ -900,13 +900,13 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     # Case 14: ModelCheckpoint doesnt save model if loss was minimum earlier\n     mode = 'min'\n     monitor = 'val_loss'\n-    initial_best = 0\n+    initial_value_threshold = 0\n     cbks = [\n         keras.callbacks.ModelCheckpoint(\n             filepath,\n             monitor=monitor,\n             save_best_only=save_best_only,\n-            initial_best=initial_best,\n+            initial_value_threshold=initial_value_threshold,\n             save_best_only=True,\n             mode=mode)\n     ]\n@@ -923,13 +923,13 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     # Case 15: ModelCheckpoint doesnt save model if loss was min earlier in auto mode\n     mode = 'auto'\n     monitor = 'val_loss'\n-    initial_best = 0\n+    initial_value_threshold = 0\n     cbks = [\n         keras.callbacks.ModelCheckpoint(\n             filepath,\n             monitor=monitor,\n             save_best_only=save_best_only,\n-            initial_best=initial_best,\n+            initial_value_threshold=initial_value_threshold,\n             save_best_only=True,\n             mode=mode)\n     ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2c24bff870c73db6aeab27378b4974eb73b26ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3367 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -873,10 +873,10 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     assert os.path.exists(filepath)\n     os.remove(filepath)\n     \n-    # Case 13: ModelCheckpoint saves model with initial_best param\n+    # Case 13: ModelCheckpoint saves model with initial_value_threshold param\n     mode = 'auto'\n     monitor = 'val_loss'\n-    initial_best = None\n+    initial_value_threshold = None\n     cbks = [\n         keras.callbacks.ModelCheckpoint(\n             filepath,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4bce69b55845b399263021fe46c8d8110f24d96d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 19 | Churn Cumulative: 1330 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,7 +22,6 @@ from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import preprocessing_utils as utils\n from keras.utils import layer_utils\n-import numpy as np\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -210,8 +209,7 @@ class Hashing(base_layer.Layer):\n             f'integers, or a single integer. Received: salt={salt}.')\n \n   def call(self, inputs):\n-    if isinstance(inputs, (list, tuple, np.ndarray)):\n-      inputs = tf.convert_to_tensor(inputs)\n+    inputs = utils.ensure_tensor(inputs)\n     if isinstance(inputs, tf.SparseTensor):\n       indices = tf.SparseTensor(\n           indices=inputs.indices,\n\n@@ -23,6 +23,7 @@ from keras import testing_utils\n from keras.engine import input_layer\n from keras.engine import training\n from keras.layers.preprocessing import hashing\n+from keras.layers.preprocessing import preprocessing_test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n@@ -30,6 +31,20 @@ import tensorflow.compat.v2 as tf\n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class HashingTest(keras_parameterized.TestCase):\n \n+  @parameterized.named_parameters(\n+      ('list', list),\n+      ('tuple', tuple),\n+      ('numpy', np.array),\n+      ('array_like', preprocessing_test_utils.ArrayLike),\n+  )\n+  def test_tensor_like_inputs(self, data_fn):\n+    input_data = data_fn([0, 1, 2, 3, 4])\n+    expected_output = [1, 0, 1, 0, 2]\n+\n+    layer = hashing.Hashing(num_bins=3)\n+    output_data = layer(input_data)\n+    self.assertAllEqual(output_data, expected_output)\n+\n   def test_hash_single_bin(self):\n     layer = hashing.Hashing(num_bins=1)\n     inp = np.asarray([['A'], ['B'], ['C'], ['D'], ['E']])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1a70b70ca4f3514ae99098eed634bd01b3b7dd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 29 | Churn Cumulative: 2814 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,7 +22,6 @@ from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import preprocessing_utils as utils\n from keras.utils import layer_utils\n-import numpy as np\n import tensorflow.compat.v2 as tf\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n@@ -183,13 +182,14 @@ class CategoryEncoding(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n   def call(self, inputs, count_weights=None):\n-    if isinstance(inputs, (list, np.ndarray)):\n-      inputs = tf.convert_to_tensor(inputs)\n+    inputs = utils.ensure_tensor(inputs)\n \n-    if count_weights is not None and self.output_mode != COUNT:\n+    if count_weights is not None:\n+      if self.output_mode != COUNT:\n         raise ValueError(\n             \"`count_weights` is not used when `output_mode` is not `'count'`. \"\n             \"Received `count_weights={}`.\".format(count_weights))\n+      count_weights = utils.ensure_tensor(count_weights, self.compute_dtype)\n \n     depth = self.num_tokens\n     if isinstance(inputs, tf.SparseTensor):\n\n@@ -14,17 +14,16 @@\n # ==============================================================================\n \"\"\"Tests for Keras text category_encoding preprocessing layer.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n from keras import backend\n from keras import keras_parameterized\n from keras.layers import core\n from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import preprocessing_test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n@@ -32,6 +31,22 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n                                 preprocessing_test_utils.PreprocessingLayerTest\n                                ):\n \n+  @parameterized.named_parameters(\n+      (\"list\", list),\n+      (\"tuple\", tuple),\n+      (\"numpy\", np.array),\n+      (\"array_like\", preprocessing_test_utils.ArrayLike),\n+  )\n+  def test_tensor_like_inputs(self, data_fn):\n+    category_data = data_fn([1, 2, 3, 3, 0])\n+    weight_data = data_fn([1, 2, 3, 1, 7])\n+    expected_output = [7, 1, 2, 4, 0, 0]\n+\n+    layer = category_encoding.CategoryEncoding(\n+        num_tokens=6, output_mode=category_encoding.COUNT)\n+    output_data = layer(category_data, count_weights=weight_data)\n+    self.assertAllEqual(output_data, expected_output)\n+\n   def test_dense_input_sparse_output(self):\n     input_array = tf.constant([[1, 2, 3], [3, 3, 0]])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3fc342186ec9f4f2654af6ff2953b8ee96110ab6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 294 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,6 +53,21 @@ class Adadelta(optimizer.Optimizer):\n       no higher than this value.\n     global_clipnorm: float. If set, the gradient of all weights is clipped\n       so that their global norm is no higher than this value.\n+    use_ema: boolean, default to False. If True, exponential moving average\n+      (EMA) is applied. EMA consists of computing an exponential moving\n+      average of the weights of the model (as the weight values change after\n+      each training batch), and periodically overwriting the weights with\n+      their moving average.\n+    ema_momentum: float, default to 0.99. Only used if `use_ema=True`. This is\n+      the momentum to use when computing the EMA of the model's weights:\n+      `new_average = ema_momentum * old_average +\n+       (1 - ema_momentum) * current_variable_value`.\n+    ema_overwrite_frequency: int or None, default to 100. Only used if\n+      `use_ema=True`. Every ema_overwrite_frequency steps of iterations, we\n+      overwrite the model variable by its stored moving average. If None, we\n+      do not overwrite model variables in the middle of training, and users\n+      need to explicitly overwrite the model variable by calling\n+      `finalize_variable_update()`.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"Adadelta\"`.\n \n@@ -67,11 +82,17 @@ class Adadelta(optimizer.Optimizer):\n                clipnorm=None,\n                clipvalue=None,\n                global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n                name='Adadelta'):\n     super(Adadelta, self).__init__(\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n         global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency,\n         name=name)\n     self._learning_rate = self._build_learning_rate(learning_rate)\n     self.rho = rho\n\n@@ -47,6 +47,21 @@ class Adagrad(optimizer.Optimizer):\n       no higher than this value.\n     global_clipnorm: float. If set, the gradient of all weights is clipped\n       so that their global norm is no higher than this value.\n+    use_ema: boolean, default to False. If True, exponential moving average\n+      (EMA) is applied. EMA consists of computing an exponential moving\n+      average of the weights of the model (as the weight values change after\n+      each training batch), and periodically overwriting the weights with\n+      their moving average.\n+    ema_momentum: float, default to 0.99. Only used if `use_ema=True`. This is\n+      the momentum to use when computing the EMA of the model's weights:\n+      `new_average = ema_momentum * old_average +\n+       (1 - ema_momentum) * current_variable_value`.\n+    ema_overwrite_frequency: int or None, default to 100. Only used if\n+      `use_ema=True`. Every ema_overwrite_frequency steps of iterations, we\n+      overwrite the model variable by its stored moving average. If None, we\n+      do not overwrite model variables in the middle of training, and users\n+      need to overwrite the model variable by calling\n+      `finalize_variable_update()`.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"Adagrad\"`.\n \n@@ -62,11 +77,17 @@ class Adagrad(optimizer.Optimizer):\n                clipnorm=None,\n                clipvalue=None,\n                global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n                name='Adagrad'):\n     super(Adagrad, self).__init__(\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n         global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency,\n         name=name)\n     self._learning_rate = self._build_learning_rate(learning_rate)\n     self.initial_accumulator_value = initial_accumulator_value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e2bc66ba0aeb3a39a6b3764406a8a40f92044b6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 88 | Churn Cumulative: 20580 | Contributors (this commit): 119 | Commits (past 90d): 30 | Contributors (cumulative): 126 | DMM Complexity: 1.0\n\nDIFF:\n@@ -844,6 +844,44 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     self._validate_target_and_loss(y, loss)\n     # Run backwards pass.\n     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n+    return self.compute_metrics(x, y, y_pred, sample_weight)\n+\n+  def compute_metrics(self, x, y, y_pred, sample_weight):\n+    \"\"\"Update metric states and collect all metrics to be returned.\n+\n+    Subclasses can optionally override this method to provide custom metric\n+    updating and collection logic.\n+\n+    Example:\n+    ```python\n+    class MyModel(tf.keras.Sequential):\n+\n+      def compute_metrics(self, x, y, y_pred, sample_weight):\n+\n+        # This super call updates `self.compiled_metrics` and returns results\n+        # for all metrics listed in `self.metrics`.\n+        metric_results = super(MyModel, self).compute_metrics(\n+            x, y, y_pred, sample_weight)\n+\n+        # Note that `self.custom_metric` is not listed in `self.metrics`.\n+        self.custom_metric.update_state(x, y, y_pred, sample_weight)\n+        metric_results['custom_metric_name'] = self.custom_metric.result()\n+        return metric_results\n+    ```\n+\n+    Args:\n+      x: Input data.\n+      y: Target data.\n+      y_pred: Predictions returned by the model (output of `model.call(x)`)\n+      sample_weight: Sample weights for weighting the loss function.\n+\n+    Returns:\n+      A `dict` containing values that will be passed to\n+      `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the\n+      values of the metrics listed in `self.metrics` are returned. Example:\n+      `{'loss': 0.2, 'accuracy': 0.7}`.\n+    \"\"\"\n+    del x  # The default implementation does not use `x`.\n     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n     # Collect metrics to return\n     return_metrics = {}\n@@ -1359,16 +1397,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     # Updates stateful loss metrics.\n     self.compiled_loss(\n         y, y_pred, sample_weight, regularization_losses=self.losses)\n-    self.compiled_metrics.update_state(y, y_pred, sample_weight)\n-    # Collect metrics to return\n-    return_metrics = {}\n-    for metric in self.metrics:\n-      result = metric.result()\n-      if isinstance(result, dict):\n-        return_metrics.update(result)\n-      else:\n-        return_metrics[metric.name] = result\n-    return return_metrics\n+    return self.compute_metrics(x, y, y_pred, sample_weight)\n \n   def make_test_function(self, force=False):\n     \"\"\"Creates a function that executes one step of evaluation.\n\n@@ -1733,6 +1733,45 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertNotEqual(\n         model.make_predict_function(force=True), original_predict_function)\n \n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_custom_compute_metrics(self):\n+\n+    class CustomMetric(metrics_module.Mean):\n+\n+      def sq_diff_plus_x(self, x, y_true, y_pred):\n+        y_pred = tf.convert_to_tensor(y_pred)\n+        y_true = tf.cast(y_true, y_pred.dtype)\n+        sq_diff_plus_x = tf.add(x, tf.math.squared_difference(y_pred, y_true))\n+        return backend.mean(sq_diff_plus_x, axis=-1)\n+\n+      def update_state(self, x, y_true, y_pred, sample_weight=None):\n+        matches = self.sq_diff_plus_x(x, y_true, y_pred)\n+        return super(CustomMetric, self).update_state(matches)\n+\n+    class MyModel(sequential.Sequential):\n+\n+      def compute_metrics(self, x, y, y_pred, sample_weight):\n+        metric_results = super(MyModel,\n+                               self).compute_metrics(x, y, y_pred,\n+                                                     sample_weight)\n+        self.custom_metric.update_state(x, y, y_pred, sample_weight)\n+        metric_results['custom_metric_name'] = self.custom_metric.result()\n+        return metric_results\n+\n+    tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,))\n+    dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1)\n+    model = MyModel([layers_module.Dense(10)])\n+    model.custom_metric = CustomMetric('my_metric')\n+    initial_result = model.custom_metric.result()\n+    optimizer = optimizer_v2.gradient_descent.SGD()\n+    model.compile(optimizer, loss='mse', steps_per_execution=10)\n+    model.fit(dataset, epochs=2, steps_per_epoch=10, verbose=2)\n+    after_fit_result = model.custom_metric.result()\n+\n+    self.assertEqual(self.evaluate(initial_result), 0.0)\n+    self.assertNotEqual(self.evaluate(initial_result),\n+                        self.evaluate(after_fit_result))\n+\n \n class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8939e3908cf8937a61e4eb79ff984f339bed527c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 28 | Churn Cumulative: 2274 | Contributors (this commit): 6 | Commits (past 90d): 21 | Contributors (cumulative): 10 | DMM Complexity: 0.75\n\nDIFF:\n@@ -251,7 +251,10 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n       raise ValueError(\"Both `num_bins` and `bin_boundaries` should not be \"\n                        \"set. You passed `num_bins={}` and \"\n                        \"`bin_boundaries={}`\".format(num_bins, bin_boundaries))\n-    bin_boundaries = utils.listify_tensors(bin_boundaries)\n+    if bin_boundaries is not None:\n+      # Force everything through ensure_tensor to ensure a uniform type.\n+      bin_boundaries = utils.listify_tensors(\n+          utils.ensure_tensor(bin_boundaries))\n     self.input_bin_boundaries = bin_boundaries\n     self.bin_boundaries = bin_boundaries if bin_boundaries is not None else []\n     self.num_bins = num_bins\n@@ -284,9 +287,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     if not self.built:\n       raise RuntimeError(\"`build` must be called before `update_state`.\")\n \n-    data = tf.convert_to_tensor(data)\n-    if data.dtype != tf.float32:\n-      data = tf.cast(data, tf.float32)\n+    data = utils.ensure_tensor(data, tf.float32)\n     summary = summarize(data, self.epsilon)\n     self.summary.assign(merge_summaries(summary, self.summary, self.epsilon))\n \n@@ -326,6 +327,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     return tf.TensorSpec(shape=output_shape, dtype=self.compute_dtype)\n \n   def call(self, inputs):\n+    inputs = utils.ensure_tensor(inputs)\n     def bucketize(inputs):\n       return tf.raw_ops.Bucketize(input=inputs, boundaries=self.bin_boundaries)\n \n\n@@ -30,6 +30,21 @@ import tensorflow.compat.v2 as tf\n class DiscretizationTest(keras_parameterized.TestCase,\n                          preprocessing_test_utils.PreprocessingLayerTest):\n \n+  @parameterized.named_parameters(\n+      (\"list\", list),\n+      (\"tuple\", tuple),\n+      (\"numpy\", np.array),\n+      (\"array_like\", preprocessing_test_utils.ArrayLike),\n+  )\n+  def test_tensor_like_inputs(self, data_fn):\n+    input_data = data_fn([0.5, 1.5, 2.5])\n+    bin_boundaries = data_fn([0.5, 1.5, 2.5])\n+    expected_output = [1, 2, 3]\n+\n+    layer = discretization.Discretization(bin_boundaries=bin_boundaries)\n+    output_data = layer(input_data)\n+    self.assertAllEqual(output_data, expected_output)\n+\n   def test_bucketize_with_explicit_buckets_integer(self):\n     input_array = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n \n\n@@ -14,6 +14,7 @@\n # ==============================================================================\n \"\"\"Utils for preprocessing layers.\"\"\"\n \n+from keras import backend\n from keras.utils import tf_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -37,7 +38,7 @@ def ensure_tensor(inputs, dtype=None):\n def listify_tensors(x):\n   \"\"\"Convert any tensors or numpy arrays to lists for config serialization.\"\"\"\n   if tf.is_tensor(x):\n-    x = x.numpy()\n+    x = backend.get_value(x)\n   if isinstance(x, np.ndarray):\n     x = x.tolist()\n   return x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
