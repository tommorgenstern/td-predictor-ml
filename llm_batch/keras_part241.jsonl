{"custom_id": "keras#5422fdd38baad36730cb6aeb946e17eeae6a551c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 11/10 | Churn Δ: 85 | Churn Cumulative: 864 | Contributors (this commit): 20 | Commits (past 90d): 7 | Contributors (cumulative): 23 | DMM Complexity: 0.17647058823529413\n\nDIFF:\n@@ -326,9 +326,15 @@ class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n \n+        if self.start_index > self.end_index:\n+            raise ValueError('`start_index+length=%i > end_index=%i` '\n+                             'is disallowed, as no part of the sequence '\n+                             'would be left to be used as current step.'\n+                             % (self.start_index, self.end_index))\n+\n     def __len__(self):\n         return int(np.ceil(\n-            (self.end_index - self.start_index) /\n+            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n \n     def _empty_batch(self, num_rows):\n@@ -341,11 +347,11 @@ class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n-                self.start_index, self.end_index, size=self.batch_size)\n+                self.start_index, self.end_index + 1, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n-                                    self.stride, self.end_index), self.stride)\n+                                    self.stride, self.end_index + 1), self.stride)\n \n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):\n\n@@ -1,5 +1,7 @@\n+from math import ceil\n+\n import numpy as np\n-from numpy.testing import assert_allclose\n+from numpy.testing import assert_allclose, assert_raises\n \n import pytest\n \n@@ -152,7 +154,7 @@ def test_TimeseriesGenerator():\n                                    length=10, sampling_rate=2,\n                                    start_index=10, end_index=30,\n                                    batch_size=2)\n-    assert len(data_gen) == 5\n+    assert len(data_gen) == 6\n     assert (np.allclose(data_gen[0][0],\n                         np.array([[[10], [12], [14], [16], [18]],\n                                   [[11], [13], [15], [17], [19]]])))\n@@ -165,13 +167,76 @@ def test_TimeseriesGenerator():\n                                    length=10, sampling_rate=2,\n                                    start_index=10, end_index=30,\n                                    batch_size=2)\n-\n-    assert len(data_gen) == 5\n+    assert len(data_gen) == 6\n     assert np.allclose(data_gen[0][0], np.array(\n         [np.array(data[10:19:2]), np.array(data[11:20:2])]))\n     assert (np.allclose(data_gen[0][1],\n                         np.array([targets[20], targets[21]])))\n \n+    with assert_raises(ValueError) as context:\n+        TimeseriesGenerator(data, targets, length=50)\n+    error = str(context.exception)\n+    assert '`start_index+length=50 > end_index=49` is disallowed' in error\n+\n+\n+def test_TimeSeriesGenerator_doesnt_miss_any_sample():\n+    x = np.array([[i] for i in range(10)])\n+\n+    for length in range(3, 10):\n+        g = TimeseriesGenerator(x, x,\n+                                length=length,\n+                                batch_size=1)\n+        expected = max(0, len(x) - length)\n+        actual = len(g)\n+\n+        assert expected == actual\n+\n+        if len(g) > 0:\n+            # All elements in range(length, 10) should be used as current step\n+            expected = np.arange(length, 10).reshape(-1, 1)\n+\n+            y = np.concatenate([g[ix][1] for ix in range(len(g))], axis=0)\n+            assert_allclose(y, expected)\n+\n+    x = np.array([[i] for i in range(23)])\n+\n+    strides = (1, 1, 5, 7, 3, 5, 3)\n+    lengths = (3, 3, 4, 3, 1, 3, 7)\n+    batch_sizes = (6, 6, 6, 5, 6, 6, 6)\n+    shuffles = (False, True, True, False, False, False, False)\n+\n+    for stride, length, batch_size, shuffle in zip(strides,\n+                                                   lengths,\n+                                                   batch_sizes,\n+                                                   shuffles):\n+        g = TimeseriesGenerator(x, x,\n+                                length=length,\n+                                sampling_rate=1,\n+                                stride=stride,\n+                                start_index=0,\n+                                end_index=None,\n+                                shuffle=shuffle,\n+                                reverse=False,\n+                                batch_size=batch_size)\n+        if shuffle:\n+            # all batches have the same size when shuffle is True.\n+            expected_sequences = ceil(\n+                (23 - length) / float(batch_size * stride)) * batch_size\n+        else:\n+            # last batch will be different if `(samples - length) / stride`\n+            # is not a multiple of `batch_size`.\n+            expected_sequences = ceil((23 - length) / float(stride))\n+\n+        expected_batches = ceil(expected_sequences / float(batch_size))\n+\n+        y = [g[ix][1] for ix in range(len(g))]\n+\n+        actual_sequences = sum(len(_y) for _y in y)\n+        actual_batches = len(y)\n+\n+        assert expected_sequences == actual_sequences\n+        assert expected_batches == actual_batches\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce13af5785bbb7a2be7d42699cfe25117177ddc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 9 | Churn Cumulative: 810 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -158,6 +158,15 @@ def NASNet(input_shape=None,\n         raise ValueError('If using `weights` as ImageNet with `include_top` '\n                          'as true, `classes` should be 1000')\n \n+    if (isinstance(input_shape, tuple) and\n+            None in input_shape and\n+            weights == 'imagenet'):\n+        raise ValueError('When specifying the input shape of a NASNet'\n+                         ' and loading `ImageNet` weights, '\n+                         'the input_shape argument must be static '\n+                         '(no None entries). Got: `input_shape='\n+                         + str(input_shape) + '`.')\n+\n     if default_size is None:\n         default_size = 331\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e609dce2a97f41f816237f1a7165fe3dee31421c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 814 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -164,8 +164,8 @@ def NASNet(input_shape=None,\n         raise ValueError('When specifying the input shape of a NASNet'\n                          ' and loading `ImageNet` weights, '\n                          'the input_shape argument must be static '\n-                         '(no None entries). Got: `input_shape='\n-                         + str(input_shape) + '`.')\n+                         '(no None entries). Got: `input_shape=' +\n+                         str(input_shape) + '`.')\n \n     if default_size is None:\n         default_size = 331\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b444513b52cf05e7d40f2ffdb7ab7283bb2ce06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 29 | Churn Cumulative: 3298 | Contributors (this commit): 53 | Commits (past 90d): 15 | Contributors (cumulative): 53 | DMM Complexity: 0.0\n\nDIFF:\n@@ -423,8 +423,15 @@ class ImageDataGenerator(object):\n         zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n         zca_whitening: Boolean. Apply ZCA whitening.\n         rotation_range: Int. Degree range for random rotations.\n-        width_shift_range: Float (fraction of total width). Range for random horizontal shifts.\n-        height_shift_range: Float (fraction of total height). Range for random vertical shifts.\n+        width_shift_range: float, 1-D array-like or int\n+            float: fraction of total width, if < 1, or pixels if >= 1.\n+            1-D array-like: random elements from the array.\n+            int: integer number of pixels from interval\n+                `(-width_shift_range, +width_shift_range)`\n+            With `width_shift_range=2` possible values are integers [-1, 0, +1],\n+            same as with `width_shift_range=[-1, 0, +1]`,\n+            while with `width_shift_range=1.0` possible values are floats in\n+            the interval [-1.0, +1.0).\n         shear_range: Float. Shear Intensity (Shear angle in counter-clockwise direction in degrees)\n         zoom_range: Float or [lower, upper]. Range for random zoom. If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n         channel_shift_range: Float. Range for random channel shifts.\n@@ -839,15 +846,25 @@ class ImageDataGenerator(object):\n             theta = 0\n \n         if self.height_shift_range:\n-            tx = np.random.uniform(-self.height_shift_range, self.height_shift_range)\n-            if self.height_shift_range < 1:\n+            try:  # 1-D array-like or int\n+                tx = np.random.choice(self.height_shift_range)\n+                tx *= np.random.choice([-1, 1])\n+            except ValueError:  # floating point\n+                tx = np.random.uniform(-self.height_shift_range,\n+                                       self.height_shift_range)\n+            if np.max(self.height_shift_range) < 1:\n                 tx *= x.shape[img_row_axis]\n         else:\n             tx = 0\n \n         if self.width_shift_range:\n-            ty = np.random.uniform(-self.width_shift_range, self.width_shift_range)\n-            if self.width_shift_range < 1:\n+            try:  # 1-D array-like or int\n+                ty = np.random.choice(self.width_shift_range)\n+                ty *= np.random.choice([-1, 1])\n+            except ValueError:  # floating point\n+                ty = np.random.uniform(-self.width_shift_range,\n+                                       self.width_shift_range)\n+            if np.max(self.width_shift_range) < 1:\n                 ty *= x.shape[img_col_axis]\n         else:\n             ty = 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3578599b6499eac4dc1f320ec5e7d96433777ee8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 14 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 40 | Churn Cumulative: 3977 | Contributors (this commit): 23 | Commits (past 90d): 22 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1258,52 +1258,50 @@ class TestBackend(object):\n             assert np.min(rand) == 0\n \n     def test_conv_invalid_use(self):\n-        for k in BACKENDS:\n         with pytest.raises(ValueError):\n-                k.conv1d(k.variable(np.ones((4, 8, 2))),\n-                         k.variable(np.ones((3, 2, 3))),\n+            K.conv1d(K.variable(np.ones((4, 8, 2))),\n+                     K.variable(np.ones((3, 2, 3))),\n                      data_format='channels_middle')\n \n         with pytest.raises(ValueError):\n-                k.conv2d(k.variable(np.ones((2, 3, 4, 5))),\n-                         k.variable(np.ones((2, 2, 3, 4))),\n+            K.conv2d(K.variable(np.ones((2, 3, 4, 5))),\n+                     K.variable(np.ones((2, 2, 3, 4))),\n                      data_format='channels_middle')\n \n         with pytest.raises(ValueError):\n-                k.conv3d(k.variable(np.ones((2, 3, 4, 5, 4))),\n-                         k.variable(np.ones((2, 2, 2, 3, 4))),\n+            K.conv3d(K.variable(np.ones((2, 3, 4, 5, 4))),\n+                     K.variable(np.ones((2, 2, 2, 3, 4))),\n                      data_format='channels_middle')\n \n-            if k != KTH:\n+        if K.backend() != 'theano':\n             with pytest.raises(ValueError):\n-                    k.separable_conv2d(k.variable(np.ones((2, 3, 4, 5))),\n-                                       k.variable(np.ones((2, 2, 3, 4))),\n-                                       k.variable(np.ones((1, 1, 12, 7))),\n+                K.separable_conv2d(K.variable(np.ones((2, 3, 4, 5))),\n+                                   K.variable(np.ones((2, 2, 3, 4))),\n+                                   K.variable(np.ones((1, 1, 12, 7))),\n                                    data_format='channels_middle')\n \n         with pytest.raises(ValueError):\n-                k.depthwise_conv2d(k.variable(np.ones((2, 3, 4, 5))),\n-                                   k.variable(np.ones((2, 2, 3, 4))),\n+            K.depthwise_conv2d(K.variable(np.ones((2, 3, 4, 5))),\n+                               K.variable(np.ones((2, 2, 3, 4))),\n                                data_format='channels_middle')\n \n     def test_pooling_invalid_use(self):\n         for (input_shape, pool_size) in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n-            for k in BACKENDS:\n-                x = k.variable(np.random.random(input_shape))\n+            x = K.variable(np.random.random(input_shape))\n             if len(pool_size) == 2:\n                 with pytest.raises(ValueError):\n-                        k.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n+                    K.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n                 with pytest.raises(ValueError):\n-                        k.pool2d(x, pool_size=pool_size, padding='twice')\n+                    K.pool2d(x, pool_size=pool_size, padding='twice')\n                 with pytest.raises(ValueError):\n-                        k.pool2d(x, pool_size=pool_size, pool_mode='median')\n+                    K.pool2d(x, pool_size=pool_size, pool_mode='median')\n             else:\n                 with pytest.raises(ValueError):\n-                        k.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n+                    K.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n                 with pytest.raises(ValueError):\n-                        k.pool3d(x, pool_size=pool_size, padding='twice')\n+                    K.pool3d(x, pool_size=pool_size, padding='twice')\n                 with pytest.raises(ValueError):\n-                        k.pool3d(x, pool_size=pool_size, pool_mode='median')\n+                    K.pool3d(x, pool_size=pool_size, pool_mode='median')\n \n     def test_resize_images(self):\n         for data_format in ['channels_first', 'channels_last']:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8ca67f281bdc5d751204e3b2bf6e98731513a27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 10 | Methods Changed: 9 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 55 | Churn Cumulative: 3651 | Contributors (this commit): 27 | Commits (past 90d): 16 | Contributors (cumulative): 34 | DMM Complexity: 0.8\n\nDIFF:\n@@ -24,7 +24,10 @@ if dev.type() == 0:\n \n # A learning phase is a bool tensor used to run Keras models in\n # either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n-_LEARNING_PHASE = C.constant(shape=(), dtype=np.float32, value=1.0, name='_keras_learning_phase')\n+# LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase\n+_LEARNING_PHASE_PLACEHOLDER = C.constant(shape=(), dtype=np.float32, value=1.0, name='_keras_learning_phase')\n+# static learning phase flag, if it is not 0 or 1, we will go with dynamic learning phase tensor.\n+_LEARNING_PHASE = -1\n _UID_PREFIXES = defaultdict(int)\n \n # cntk doesn't support gradient as symbolic op, to hook up with keras model,\n@@ -49,8 +52,8 @@ def get_uid(prefix=''):\n \n \n def learning_phase():\n-    # False = test, True = train\n-    return _LEARNING_PHASE\n+    # If _LEARNING_PHASE is not 0 or 1, return dynamic learning phase tensor\n+    return _LEARNING_PHASE if _LEARNING_PHASE in {0, 1} else _LEARNING_PHASE_PLACEHOLDER\n \n \n def set_learning_phase(value):\n@@ -59,8 +62,16 @@ def set_learning_phase(value):\n         raise ValueError('CNTK Backend: Set learning phase '\n                          'with value %s is not supported, '\n                          'expected 0 or 1.' % value)\n-    v = np.asarray(value)\n-    _LEARNING_PHASE.value = v\n+    _LEARNING_PHASE = value\n+\n+\n+def clear_session():\n+    \"\"\"Reset learning phase flag for cntk backend.\n+    \"\"\"\n+    global _LEARNING_PHASE\n+    global _LEARNING_PHASE_PLACEHOLDER\n+    _LEARNING_PHASE = -1\n+    _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(1.0)\n \n \n def in_train_phase(x, alt, training=None):\n@@ -82,6 +93,10 @@ def in_train_phase(x, alt, training=None):\n     if training is True:\n         x._uses_learning_phase = uses_learning_phase\n         return x\n+    else:\n+        # if _LEARNING_PHASE is static\n+        if isinstance(training, int) or isinstance(training, bool):\n+            result = x if training == 1 or training is True else alt\n         else:\n             result = C.element_select(training, x, alt)\n         result._uses_learning_phase = uses_learning_phase\n@@ -1854,6 +1869,7 @@ class Function(object):\n         return True\n \n     def __call__(self, inputs):\n+        global _LEARNING_PHASE_PLACEHOLDER\n         global _LEARNING_PHASE\n         assert isinstance(inputs, (list, tuple))\n         feed_dict = {}\n@@ -1864,8 +1880,8 @@ class Function(object):\n                value.dtype != np.float64):\n                 value = value.astype(np.float32)\n \n-            if tensor == _LEARNING_PHASE:\n-                _LEARNING_PHASE.value = np.asarray(value)\n+            if tensor == _LEARNING_PHASE_PLACEHOLDER:\n+                _LEARNING_PHASE_PLACEHOLDER.value = np.asarray(value)\n             else:\n                 # in current version cntk can't support input with variable\n                 # length. Will support it in next release.\n@@ -1911,7 +1927,8 @@ class Function(object):\n             # \"forward\" method to let cntk know we want to evaluate them.from\n             # But the assign ops won't be executed under this mode, that's why\n             # we need this check.\n-            if self.unrelated_updates is None and _LEARNING_PHASE.value == 1.0:\n+            if (self.unrelated_updates is None and\n+                    (_LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1)):\n                 _, output_values = self.metrics_func.forward(\n                     input_dict,\n                     self.metrics_func.outputs,\n\n@@ -159,7 +159,7 @@ def keras_test(func):\n     @six.wraps(func)\n     def wrapper(*args, **kwargs):\n         output = func(*args, **kwargs)\n-        if K.backend() == 'tensorflow':\n+        if K.backend() == 'tensorflow' or K.backend() == 'cntk':\n             K.clear_session()\n         return output\n     return wrapper\n\n@@ -12,6 +12,7 @@ from keras import backend as K\n input_1 = np.arange(10)\n input_2 = np.zeros(10)\n input_3 = np.ones((10))\n+input_4 = np.expand_dims(np.arange(10.), axis=1)\n input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n \n \n@@ -223,5 +224,24 @@ def test_that_trainable_disables_updates():\n     assert_allclose(x1, x2, atol=1e-7)\n \n \n+@keras_test\n+def test_batchnorm_trainable():\n+    bn_mean = 0.5\n+    bn_std = 10.\n+\n+    def get_model(bn_mean, bn_std):\n+        input = Input(shape=(1,))\n+        x = normalization.BatchNormalization()(input)\n+        model = Model(input, x)\n+        model.set_weights([np.array([1.]), np.array([0.]),\n+                           np.array([bn_mean]), np.array([bn_std ** 2])])\n+        return model\n+    # Simulates training-mode with trainable layer. Should use mini-batch statistics.\n+    K.set_learning_phase(1)\n+    model = get_model(bn_mean, bn_std)\n+    model.compile(loss='mse', optimizer='rmsprop')\n+    out = model.predict(input_4)\n+    assert_allclose((input_4 - np.mean(input_4)) / np.std(input_4), out, atol=1e-3)\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d673afd5979a4e541266763f65bbd65fabf20b0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 172 | Lines Deleted: 163 | Files Changed: 2 | Hunks: 10 | Methods Changed: 20 | Complexity Δ (Sum/Max): 0/47 | Churn Δ: 335 | Churn Cumulative: 4312 | Contributors (this commit): 23 | Commits (past 90d): 24 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n import pytest\n from numpy.testing import assert_allclose\n import numpy as np\n-import scipy.signal as signal\n import scipy.sparse as sparse\n import warnings\n from keras.utils.test_utils import keras_test\n@@ -9,6 +8,8 @@ from keras.utils.test_utils import keras_test\n from keras import backend as K\n from keras.backend import floatx, set_floatx, variable\n from keras.utils.conv_utils import convert_kernel\n+import reference_operations\n+\n \n BACKENDS = []  # Holds a list of all available back-ends\n \n@@ -192,162 +193,6 @@ def check_composed_tensor_operations(first_function_name, first_function_args,\n     assert_list_pairwise(z_list)\n \n \n-def normalize_ref_conv(func):\n-    def wrapper(*args):\n-        x = args[0]\n-        w = args[1]\n-        if x.ndim == 3:\n-            w = np.flipud(w)\n-            w = np.transpose(w, (1, 2, 0))\n-            if args[3] == 'channels_last':\n-                x = np.transpose(x, (0, 2, 1))\n-        elif x.ndim == 4:\n-            w = np.fliplr(np.flipud(w))\n-            w = np.transpose(w, (2, 3, 0, 1))\n-            if args[3] == 'channels_last':\n-                x = np.transpose(x, (0, 3, 1, 2))\n-        else:\n-            w = np.flip(np.fliplr(np.flipud(w)), axis=2)\n-            w = np.transpose(w, (3, 4, 0, 1, 2))\n-            if args[3] == 'channels_last':\n-                x = np.transpose(x, (0, 4, 1, 2, 3))\n-\n-        y = func(x, w, args[2], args[3])\n-\n-        if args[3] == 'channels_last':\n-            if y.ndim == 3:\n-                y = np.transpose(y, (0, 2, 1))\n-            elif y.ndim == 4:\n-                y = np.transpose(y, (0, 2, 3, 1))\n-            else:\n-                y = np.transpose(y, (0, 2, 3, 4, 1))\n-\n-        return y\n-\n-    return wrapper\n-\n-\n-@normalize_ref_conv\n-def ref_conv(x, w, padding, data_format):\n-    y = []\n-    for i in range(x.shape[0]):\n-        _y = []\n-        for j in range(w.shape[1]):\n-            __y = []\n-            for k in range(w.shape[0]):\n-                __y.append(signal.convolve(x[i, k], w[k, j], mode=padding))\n-            _y.append(np.sum(np.stack(__y, axis=-1), axis=-1))\n-        y.append(_y)\n-    y = np.array(y)\n-    return y\n-\n-\n-@normalize_ref_conv\n-def ref_depthwise_conv(x, w, padding, data_format):\n-    y = []\n-    for i in range(x.shape[0]):\n-        _y = []\n-        for j in range(w.shape[0]):\n-            __y = []\n-            for k in range(w.shape[1]):\n-                __y.append(signal.convolve(x[i, j], w[j, k], mode=padding))\n-            _y.append(np.stack(__y, axis=0))\n-        y.append(np.concatenate(_y, axis=0))\n-    y = np.array(y)\n-    return y\n-\n-\n-def ref_separable_conv(x, w1, w2, padding, data_format):\n-    x2 = ref_depthwise_conv(x, w1, padding, data_format)\n-    return ref_conv(x2, w2, padding, data_format)\n-\n-\n-def ref_pool(x, pool_size, strides, padding, data_format, pool_mode):\n-    if data_format == 'channels_last':\n-        if x.ndim == 3:\n-            x = np.transpose(x, (0, 2, 1))\n-        elif x.ndim == 4:\n-            x = np.transpose(x, (0, 3, 1, 2))\n-        else:\n-            x = np.transpose(x, (0, 4, 1, 2, 3))\n-\n-    if padding == 'same':\n-        pad = [(0, 0), (0, 0)] + [(s // 2, s // 2) for s in pool_size]\n-        x = np.pad(x, pad, 'constant', constant_values=-np.inf)\n-\n-    # indexing trick\n-    x = np.pad(x, [(0, 0), (0, 0)] + [(0, 1) for _ in pool_size],\n-               'constant', constant_values=0)\n-\n-    if x.ndim == 3:\n-        y = [x[:, :, k:k1:strides[0]]\n-             for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0))]\n-    elif x.ndim == 4:\n-        y = []\n-        for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0)):\n-            for (l, l1) in zip(range(pool_size[1]), range(-pool_size[1], 0)):\n-                y.append(x[:, :, k:k1:strides[0], l:l1:strides[1]])\n-    else:\n-        y = []\n-        for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0)):\n-            for (l, l1) in zip(range(pool_size[1]), range(-pool_size[1], 0)):\n-                for (m, m1) in zip(range(pool_size[2]), range(-pool_size[2], 0)):\n-                    y.append(x[:, :, k:k1:strides[0], l:l1:strides[1], m:m1:strides[2]])\n-    y = np.stack(y, axis=-1)\n-    if pool_mode == 'avg':\n-        y = np.mean(np.ma.masked_invalid(y), axis=-1).data\n-    elif pool_mode == 'max':\n-        y = np.max(y, axis=-1)\n-\n-    if data_format == 'channels_last':\n-        if y.ndim == 3:\n-            y = np.transpose(y, (0, 2, 1))\n-        elif y.ndim == 4:\n-            y = np.transpose(y, (0, 2, 3, 1))\n-        else:\n-            y = np.transpose(y, (0, 2, 3, 4, 1))\n-\n-    return y\n-\n-\n-def ref_rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=None):\n-    w_i, w_h, w_o = w\n-    h = []\n-    o = []\n-\n-    if go_backwards:\n-        t_list = range(x.shape[1] - 1, -1, -1)\n-    else:\n-        t_list = range(x.shape[1])\n-\n-    if mask is not None:\n-        np_mask = K.eval(mask)\n-    else:\n-        np_mask = None\n-\n-    for (i, t) in enumerate(t_list):\n-        h_t = np.dot(x[:, t], w_i)\n-\n-        if w_h is not None:\n-            prev = h[i - 1] if i > 0 else init\n-            h_t1 = np.dot(prev, w_h)\n-            if np_mask is not None:\n-                h_t1[np_mask[:, t] == 0] = prev[np_mask[:, t] == 0]\n-        else:\n-            h_t1 = 0\n-\n-        o_t = h_t + h_t1\n-        if w_o is not None:\n-            o_t = np.dot(o_t, w_o)\n-        o.append(o_t)\n-\n-        if np_mask is not None:\n-            h_t = h_t * np_mask[:, t].reshape(-1, 1)\n-        h.append(h_t + h_t1)\n-\n-    return o[-1], np.stack(o, axis=1), np.stack(h, axis=1)\n-\n-\n class TestBackend(object):\n \n     def test_is_keras_tensor(self):\n@@ -724,7 +569,7 @@ class TestBackend(object):\n         ]\n \n         for (i, kwargs) in enumerate(kwargs_list):\n-            last_y1, y1, h1 = ref_rnn(x, [wi, wh, None], h0, **kwargs)\n+            last_y1, y1, h1 = reference_operations.rnn(x, [wi, wh, None], h0, **kwargs)\n             last_y2, y2, h2 = K.rnn(rnn_fn, x_k, h0_k, **kwargs)\n \n             assert len(h2) == 1\n@@ -771,7 +616,7 @@ class TestBackend(object):\n             y_k = K.dot(x_k, wi_k)\n             return y_k, []\n \n-        last_y1, y1, h1 = ref_rnn(x, [wi, None, None], None,\n+        last_y1, y1, h1 = reference_operations.rnn(x, [wi, None, None], None,\n                                                    go_backwards=False, mask=None)\n         last_y2, y2, h2 = K.rnn(rnn_fn, x_k, [],\n                                 go_backwards=False, mask=None)\n@@ -1071,7 +916,7 @@ class TestBackend(object):\n         k = K.backend()\n         _, x = parse_shape_or_val(input_shape)\n         _, w = parse_shape_or_val(kernel_shape)\n-        y1 = ref_conv(x, w, padding, data_format)\n+        y1 = reference_operations.conv(x, w, padding, data_format)\n         y2 = check_two_tensor_operation(\n             op, x, w, [KTH if k == 'theano' else KC if k == 'cntk' else KTF],\n             padding=padding, data_format=data_format,\n@@ -1088,7 +933,7 @@ class TestBackend(object):\n         k = K.backend()\n         _, x = parse_shape_or_val(input_shape)\n         _, w = parse_shape_or_val(kernel_shape)\n-        y1 = ref_depthwise_conv(x, w, padding, data_format)\n+        y1 = reference_operations.depthwise_conv(x, w, padding, data_format)\n         y2 = check_two_tensor_operation(\n             op, x, w, [KTH if k == 'theano' else KC if k == 'cntk' else KTF],\n             padding=padding, data_format=data_format,\n@@ -1108,7 +953,7 @@ class TestBackend(object):\n     def test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n         k = K.backend()\n         _, x = parse_shape_or_val(input_shape)\n-        y1 = ref_pool(x, pool_size, strides, padding, data_format, pool_mode)\n+        y1 = reference_operations.pool(x, pool_size, strides, padding, data_format, pool_mode)\n         y2 = check_single_tensor_operation(\n             op, x, [KTH if k == 'theano' else KC if k == 'cntk' else KTF],\n             pool_size=pool_size, strides=strides,\n@@ -1174,7 +1019,7 @@ class TestBackend(object):\n         _, x = parse_shape_or_val(input_shape)\n         _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n         _, pointwise = parse_shape_or_val((1, 1) + (input_depth * depth_multiplier, 7))\n-        y1 = ref_separable_conv(x, depthwise, pointwise, padding, data_format)\n+        y1 = reference_operations.separable_conv(x, depthwise, pointwise, padding, data_format)\n         if K.backend() == 'cntk':\n             y2 = cntk_func_three_tensor(\n                 op, input_shape,\n\n@@ -0,0 +1,164 @@\n+\"\"\"Utilities for backend functionality checks.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import numpy as np\n+import scipy.signal as signal\n+\n+\n+def normalize_conv(func):\n+    def wrapper(*args):\n+        x = args[0]\n+        w = args[1]\n+        if x.ndim == 3:\n+            w = np.flipud(w)\n+            w = np.transpose(w, (1, 2, 0))\n+            if args[3] == 'channels_last':\n+                x = np.transpose(x, (0, 2, 1))\n+        elif x.ndim == 4:\n+            w = np.fliplr(np.flipud(w))\n+            w = np.transpose(w, (2, 3, 0, 1))\n+            if args[3] == 'channels_last':\n+                x = np.transpose(x, (0, 3, 1, 2))\n+        else:\n+            w = np.flip(np.fliplr(np.flipud(w)), axis=2)\n+            w = np.transpose(w, (3, 4, 0, 1, 2))\n+            if args[3] == 'channels_last':\n+                x = np.transpose(x, (0, 4, 1, 2, 3))\n+\n+        y = func(x, w, args[2], args[3])\n+\n+        if args[3] == 'channels_last':\n+            if y.ndim == 3:\n+                y = np.transpose(y, (0, 2, 1))\n+            elif y.ndim == 4:\n+                y = np.transpose(y, (0, 2, 3, 1))\n+            else:\n+                y = np.transpose(y, (0, 2, 3, 4, 1))\n+\n+        return y\n+\n+    return wrapper\n+\n+\n+@normalize_conv\n+def conv(x, w, padding, data_format):\n+    y = []\n+    for i in range(x.shape[0]):\n+        _y = []\n+        for j in range(w.shape[1]):\n+            __y = []\n+            for k in range(w.shape[0]):\n+                __y.append(signal.convolve(x[i, k], w[k, j], mode=padding))\n+            _y.append(np.sum(np.stack(__y, axis=-1), axis=-1))\n+        y.append(_y)\n+    y = np.array(y)\n+    return y\n+\n+\n+@normalize_conv\n+def depthwise_conv(x, w, padding, data_format):\n+    y = []\n+    for i in range(x.shape[0]):\n+        _y = []\n+        for j in range(w.shape[0]):\n+            __y = []\n+            for k in range(w.shape[1]):\n+                __y.append(signal.convolve(x[i, j], w[j, k], mode=padding))\n+            _y.append(np.stack(__y, axis=0))\n+        y.append(np.concatenate(_y, axis=0))\n+    y = np.array(y)\n+    return y\n+\n+\n+def separable_conv(x, w1, w2, padding, data_format):\n+    x2 = depthwise_conv(x, w1, padding, data_format)\n+    return conv(x2, w2, padding, data_format)\n+\n+\n+def pool(x, pool_size, strides, padding, data_format, pool_mode):\n+    if data_format == 'channels_last':\n+        if x.ndim == 3:\n+            x = np.transpose(x, (0, 2, 1))\n+        elif x.ndim == 4:\n+            x = np.transpose(x, (0, 3, 1, 2))\n+        else:\n+            x = np.transpose(x, (0, 4, 1, 2, 3))\n+\n+    if padding == 'same':\n+        pad = [(0, 0), (0, 0)] + [(s // 2, s // 2) for s in pool_size]\n+        x = np.pad(x, pad, 'constant', constant_values=-np.inf)\n+\n+    # indexing trick\n+    x = np.pad(x, [(0, 0), (0, 0)] + [(0, 1) for _ in pool_size],\n+               'constant', constant_values=0)\n+\n+    if x.ndim == 3:\n+        y = [x[:, :, k:k1:strides[0]]\n+             for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0))]\n+    elif x.ndim == 4:\n+        y = []\n+        for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0)):\n+            for (l, l1) in zip(range(pool_size[1]), range(-pool_size[1], 0)):\n+                y.append(x[:, :, k:k1:strides[0], l:l1:strides[1]])\n+    else:\n+        y = []\n+        for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0)):\n+            for (l, l1) in zip(range(pool_size[1]), range(-pool_size[1], 0)):\n+                for (m, m1) in zip(range(pool_size[2]), range(-pool_size[2], 0)):\n+                    y.append(x[:, :, k:k1:strides[0], l:l1:strides[1], m:m1:strides[2]])\n+    y = np.stack(y, axis=-1)\n+    if pool_mode == 'avg':\n+        y = np.mean(np.ma.masked_invalid(y), axis=-1).data\n+    elif pool_mode == 'max':\n+        y = np.max(y, axis=-1)\n+\n+    if data_format == 'channels_last':\n+        if y.ndim == 3:\n+            y = np.transpose(y, (0, 2, 1))\n+        elif y.ndim == 4:\n+            y = np.transpose(y, (0, 2, 3, 1))\n+        else:\n+            y = np.transpose(y, (0, 2, 3, 4, 1))\n+\n+    return y\n+\n+\n+def rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=None):\n+    w_i, w_h, w_o = w\n+    h = []\n+    o = []\n+\n+    if go_backwards:\n+        t_list = range(x.shape[1] - 1, -1, -1)\n+    else:\n+        t_list = range(x.shape[1])\n+\n+    if mask is not None:\n+        from keras import backend as K\n+        np_mask = K.eval(mask)\n+    else:\n+        np_mask = None\n+\n+    for (i, t) in enumerate(t_list):\n+        h_t = np.dot(x[:, t], w_i)\n+\n+        if w_h is not None:\n+            prev = h[i - 1] if i > 0 else init\n+            h_t1 = np.dot(prev, w_h)\n+            if np_mask is not None:\n+                h_t1[np_mask[:, t] == 0] = prev[np_mask[:, t] == 0]\n+        else:\n+            h_t1 = 0\n+\n+        o_t = h_t + h_t1\n+        if w_o is not None:\n+            o_t = np.dot(o_t, w_o)\n+        o.append(o_t)\n+\n+        if np_mask is not None:\n+            h_t = h_t * np_mask[:, t].reshape(-1, 1)\n+        h.append(h_t + h_t1)\n+\n+    return o[-1], np.stack(o, axis=1), np.stack(h, axis=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b26fa3b63870b6160340bfeeb384b8d812dbbd15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 542 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -152,13 +152,16 @@ def preprocess_input(x, data_format=None, mode='caffe'):\n     # Arguments\n         x: Input Numpy or symbolic tensor, 3D or 4D.\n         data_format: Data format of the image tensor/array.\n-        mode: One of \"caffe\", \"tf\".\n+        mode: One of \"caffe\", \"tf\" or \"torch\".\n             - caffe: will convert the images from RGB to BGR,\n                 then will zero-center each color channel with\n                 respect to the ImageNet dataset,\n                 without scaling.\n             - tf: will scale pixels between -1 and 1,\n                 sample-wise.\n+            - torch: will scale pixels between 0 and 1 and then\n+                will normalize each channel with respect to the\n+                ImageNet dataset.\n \n     # Returns\n         Preprocessed tensor or Numpy array.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30c34ff6327d04b1296a66f6d463d8c7b2f1a96b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 444 | Contributors (this commit): 19 | Commits (past 90d): 8 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -23,4 +23,4 @@ from .layers import Input\n from .models import Model\r\n from .models import Sequential\r\n \r\n-__version__ = '2.1.5'\r\n+__version__ = '2.1.6'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='2.1.5',\n+      version='2.1.6',\n       description='Deep Learning for humans',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/keras-team/keras',\n-      download_url='https://github.com/keras-team/keras/tarball/2.1.5',\n+      download_url='https://github.com/keras-team/keras/tarball/2.1.6',\n       license='MIT',\n       install_requires=['numpy>=1.9.1',\n                         'scipy>=0.14',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b09ec1c9bf827622a5eebb94ba0af5bf202ca359", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 9675 | Contributors (this commit): 66 | Commits (past 90d): 17 | Contributors (cumulative): 75 | DMM Complexity: None\n\nDIFF:\n@@ -1895,7 +1895,7 @@ class Container(Layer):\n         \"\"\"Retrieves the model's updates.\n \n         Will only include updates that are either\n-        inconditional, or conditional on inputs to this model\n+        unconditional, or conditional on inputs to this model\n         (e.g. will not include updates that depend on tensors\n         that aren't inputs to this model).\n \n@@ -1924,7 +1924,7 @@ class Container(Layer):\n         \"\"\"Retrieves the model's losses.\n \n         Will only include losses that are either\n-        inconditional, or conditional on inputs to this model\n+        unconditional, or conditional on inputs to this model\n         (e.g. will not include losses that depend on tensors\n         that aren't inputs to this model).\n \n\n@@ -519,7 +519,7 @@ def test_saving_model_with_long_layer_names():\n \n     os.remove(fname)\n \n-    # The chunking of layer names array should have happend.\n+    # The chunking of layer names array should have happened.\n     assert n_layer_names_arrays > 0\n \n     out2 = model.predict(x)\n@@ -564,7 +564,7 @@ def test_saving_model_with_long_weights_names():\n \n     os.remove(fname)\n \n-    # The chunking of layer names array should have happend.\n+    # The chunking of layer names array should have happened.\n     assert n_weight_names_arrays > 0\n \n     out2 = model.predict(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#73c90d014f024bfa13360c34fa99fe6220cc8e98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 72 | Churn Cumulative: 10803 | Contributors (this commit): 91 | Commits (past 90d): 30 | Contributors (cumulative): 114 | DMM Complexity: None\n\nDIFF:\n@@ -1997,7 +1997,75 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n \n def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                      padding='valid', data_format=None, dilation_rate=(1, 1)):\n-    raise NotImplementedError\n+    \"\"\"2D convolution with separable filters.\n+\n+    # Arguments\n+        x: input tensor\n+        depthwise_kernel: convolution kernel for the depthwise convolution.\n+        pointwise_kernel: kernel for the 1x1 convolution.\n+        strides: strides tuple (length 2).\n+        padding: string, `\"same\"` or `\"valid\"`.\n+        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n+        dilation_rate: tuple of integers,\n+            dilation rates for the separable convolution.\n+\n+    # Returns\n+        Output tensor.\n+\n+    # Raises\n+        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n+    \"\"\"\n+    if data_format is None:\n+        data_format = image_data_format()\n+    if data_format not in {'channels_first', 'channels_last'}:\n+        raise ValueError('Unknown data_format ', data_format)\n+\n+    if hasattr(x, '_keras_shape'):\n+        image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n+    else:\n+        image_shape = None\n+    if hasattr(depthwise_kernel, '_keras_shape'):\n+        depthwise_kernel_shape = depthwise_kernel._keras_shape\n+    else:\n+        # Will only work if `depthwise_kernel` is a shared variable.\n+        depthwise_kernel_shape = depthwise_kernel.eval().shape\n+    depthwise_kernel_shape = _preprocess_conv2d_filter_shape(depthwise_kernel_shape, data_format)\n+    if hasattr(pointwise_kernel, '_keras_shape'):\n+        pointwise_kernel_shape = pointwise_kernel._keras_shape\n+    else:\n+        # Will only work if `pointwise_kernel` is a shared variable.\n+        pointwise_kernel_shape = pointwise_kernel.eval().shape\n+    pointwise_kernel_shape = _preprocess_conv2d_filter_shape(pointwise_kernel_shape, data_format)\n+\n+    x = _preprocess_conv2d_input(x, data_format)\n+    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n+    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n+    th_padding = _preprocess_padding(padding)\n+\n+    input_depth = depthwise_kernel_shape[1]\n+    output_depth = depthwise_kernel_shape[0]\n+    depthwise_kernel_shape = (input_depth * output_depth, 1) + depthwise_kernel_shape[2:]\n+    depthwise_kernel = depthwise_kernel.dimshuffle((1, 0, 2, 3))\n+    depthwise_kernel = reshape(depthwise_kernel, depthwise_kernel_shape)\n+    depthwise_kernel = depthwise_kernel[:, :, ::-1, ::-1]\n+\n+    conv_out = T.nnet.conv2d(x, depthwise_kernel,\n+                             border_mode=th_padding,\n+                             subsample=strides,\n+                             input_shape=image_shape,\n+                             filter_shape=depthwise_kernel_shape,\n+                             filter_dilation=dilation_rate,\n+                             num_groups=input_depth)\n+    conv_out = T.nnet.conv2d(conv_out, pointwise_kernel,\n+                             border_mode=th_padding,\n+                             subsample=(1, 1),\n+                             input_shape=None,\n+                             filter_shape=pointwise_kernel_shape,\n+                             filter_dilation=dilation_rate)\n+    conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n+                                          pointwise_kernel_shape,\n+                                          strides, data_format)\n+    return conv_out\n \n \n def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n\n@@ -1007,7 +1007,6 @@ class TestBackend(object):\n                                        BACKENDS, cntk_dynamicity=True,\n                                        data_format=data_format)\n \n-    @pytest.mark.skipif(K.backend() == 'theano', reason='Not supported.')\n     @pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [\n         ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'),\n         ('separable_conv2d', (2, 3, 5, 6), (4, 3), 2, 'valid', 'channels_first'),\n\n@@ -279,7 +279,6 @@ def test_separable_conv_1d():\n                                                           batch_input_shape=(None, 5, None))])\n \n \n-@pytest.mark.skipif(K.backend() == 'theano', reason='Theano does not support it yet')\n @keras_test\n def test_separable_conv_2d():\n     num_samples = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#49f5b931410bc2e56378f20a15e8ac919e0efb88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5164 | Lines Deleted: 5039 | Files Changed: 42 | Hunks: 79 | Methods Changed: 299 | Complexity Δ (Sum/Max): -1/245 | Churn Δ: 10203 | Churn Cumulative: 97107 | Contributors (this commit): 366 | Commits (past 90d): 137 | Contributors (cumulative): 806 | DMM Complexity: 0.20454545454545456\n\nDIFF:\n@@ -14,7 +14,7 @@ This is a fast Implement, just 20s/epcoh with a gtx 1070 gpu.\n \n from __future__ import print_function\n from keras import backend as K\n-from keras.engine.topology import Layer\n+from keras.layers import Layer\n from keras import activations\n from keras import utils\n from keras.datasets import cifar10\n\n@@ -33,7 +33,7 @@ from ..layers import Input\n from ..layers import MaxPooling2D\n from ..layers import ZeroPadding2D\n from ..utils.data_utils import get_file\n-from ..engine.topology import get_source_inputs\n+from ..engine import get_source_inputs\n from . import imagenet_utils\n from .imagenet_utils import decode_predictions\n from .imagenet_utils import _obtain_input_shape\n\n@@ -33,7 +33,7 @@ from ..layers import Input\n from ..layers import Lambda\n from ..layers import MaxPooling2D\n from ..utils.data_utils import get_file\n-from ..engine.topology import get_source_inputs\n+from ..engine import get_source_inputs\n from . import imagenet_utils\n from .imagenet_utils import _obtain_input_shape\n from .imagenet_utils import decode_predictions\n\n@@ -28,7 +28,7 @@ from ..layers import MaxPooling2D\n from ..layers import AveragePooling2D\n from ..layers import GlobalAveragePooling2D\n from ..layers import GlobalMaxPooling2D\n-from ..engine.topology import get_source_inputs\n+from ..engine import get_source_inputs\n from ..utils.data_utils import get_file\n from .. import backend as K\n from . import imagenet_utils\n\n@@ -72,8 +72,8 @@ from .. import regularizers\n from .. import constraints\n from ..utils import conv_utils\n from ..utils.data_utils import get_file\n-from ..engine.topology import get_source_inputs\n-from ..engine import InputSpec\n+from ..engine import get_source_inputs\n+from ..engine.base_layer import InputSpec\n from . import imagenet_utils\n from .imagenet_utils import _obtain_input_shape\n from .imagenet_utils import decode_predictions\n\n@@ -54,7 +54,7 @@ from ..layers import Cropping2D\n from ..layers import concatenate\n from ..layers import add\n from ..utils.data_utils import get_file\n-from ..engine.topology import get_source_inputs\n+from ..engine import get_source_inputs\n from ..applications.imagenet_utils import _obtain_input_shape\n from ..applications.inception_v3 import preprocess_input\n from ..applications.imagenet_utils import decode_predictions\n\n@@ -28,7 +28,7 @@ from ..layers import ZeroPadding2D\n from ..layers import BatchNormalization\n from ..models import Model\n from .. import backend as K\n-from ..engine.topology import get_source_inputs\n+from ..engine import get_source_inputs\n from ..utils import layer_utils\n from ..utils.data_utils import get_file\n from .imagenet_utils import decode_predictions\n\n@@ -21,7 +21,7 @@ from ..layers import Conv2D\n from ..layers import MaxPooling2D\n from ..layers import GlobalAveragePooling2D\n from ..layers import GlobalMaxPooling2D\n-from ..engine.topology import get_source_inputs\n+from ..engine import get_source_inputs\n from ..utils import layer_utils\n from ..utils.data_utils import get_file\n from .. import backend as K\n\n@@ -21,7 +21,7 @@ from ..layers import Conv2D\n from ..layers import MaxPooling2D\n from ..layers import GlobalAveragePooling2D\n from ..layers import GlobalMaxPooling2D\n-from ..engine.topology import get_source_inputs\n+from ..engine import get_source_inputs\n from ..utils import layer_utils\n from ..utils.data_utils import get_file\n from .. import backend as K\n\n@@ -35,7 +35,7 @@ from ..layers import SeparableConv2D\n from ..layers import MaxPooling2D\n from ..layers import GlobalAveragePooling2D\n from ..layers import GlobalMaxPooling2D\n-from ..engine.topology import get_source_inputs\n+from ..engine import get_source_inputs\n from ..utils.data_utils import get_file\n from .. import backend as K\n from . import imagenet_utils\n\n@@ -18,7 +18,6 @@ from collections import OrderedDict\n from collections import Iterable\n from .utils.generic_utils import Progbar\n from . import backend as K\n-from .engine.topology import Layer\n \n try:\n     import requests\n\n@@ -1,8 +1,8 @@\n-# note: topology.Node is an internal class,\n+# note: `Node` is an internal class,\n # it isn't meant to be used by Keras users.\n-from .topology import InputSpec\n-from .topology import Input\n-from .topology import InputLayer\n-from .topology import Layer\n-from .topology import get_source_inputs\n+from .input_layer import Input\n+from .input_layer import InputLayer\n+from .base_layer import InputSpec\n+from .base_layer import Layer\n+from .network import get_source_inputs\n from .training import Model\n\n@@ -0,0 +1,1355 @@\n+\"\"\"Contains the base Layer class, from which all layers inherit.\n+\"\"\"\n+from __future__ import print_function\n+from __future__ import absolute_import\n+from __future__ import division\n+\n+import copy\n+import re\n+from six.moves import zip\n+\n+from .. import backend as K\n+from .. import initializers\n+from ..utils.layer_utils import count_params\n+from ..utils.generic_utils import has_arg\n+from ..utils.generic_utils import object_list_uid\n+from ..utils.generic_utils import to_list\n+from ..utils.generic_utils import is_all_none\n+from ..legacy import interfaces\n+\n+\n+class Layer(object):\n+    \"\"\"Abstract base layer class.\n+\n+    # Properties\n+        name: String, must be unique within a model.\n+        input_spec: List of InputSpec class instances\n+            each entry describes one required input:\n+                - ndim\n+                - dtype\n+            A layer with `n` input tensors must have\n+            an `input_spec` of length `n`.\n+        trainable: Boolean, whether the layer weights\n+            will be updated during training.\n+        uses_learning_phase: Whether any operation\n+            of the layer uses `K.in_training_phase()`\n+            or `K.in_test_phase()`.\n+        input_shape: Shape tuple. Provided for convenience,\n+            but note that there may be cases in which this\n+            attribute is ill-defined (e.g. a shared layer\n+            with multiple input shapes), in which case\n+            requesting `input_shape` will raise an Exception.\n+            Prefer using `layer.get_input_shape_for(input_shape)`,\n+            or `layer.get_input_shape_at(node_index)`.\n+        output_shape: Shape tuple. See above.\n+        inbound_nodes: List of nodes.\n+        outbound_nodes: List of nodes.\n+        input, output: Input/output tensor(s). Note that if the layer is used\n+            more than once (shared layer), this is ill-defined\n+            and will raise an exception. In such cases, use\n+            `layer.get_input_at(node_index)`.\n+        input_mask, output_mask: Same as above, for masks.\n+        trainable_weights: List of variables.\n+        non_trainable_weights: List of variables.\n+        weights: The concatenation of the lists trainable_weights and\n+            non_trainable_weights (in this order).\n+\n+    # Methods\n+        call(x, mask=None): Where the layer's logic lives.\n+        __call__(x, mask=None): Wrapper around the layer logic (`call`).\n+            If x is a Keras tensor:\n+                - Connect current layer with last layer from tensor:\n+                    `self._add_inbound_node(last_layer)`\n+                - Add layer to tensor history\n+            If layer is not built:\n+                - Build from x._keras_shape\n+        get_weights()\n+        set_weights(weights)\n+        get_config()\n+        count_params()\n+        compute_output_shape(input_shape)\n+        compute_mask(x, mask)\n+        get_input_at(node_index)\n+        get_output_at(node_index)\n+        get_input_shape_at(node_index)\n+        get_output_shape_at(node_index)\n+        get_input_mask_at(node_index)\n+        get_output_mask_at(node_index)\n+\n+    # Class Methods\n+        from_config(config)\n+\n+    # Internal methods:\n+        build(input_shape)\n+        _add_inbound_node(layer, index=0)\n+        assert_input_compatibility()\n+    \"\"\"\n+\n+    def __init__(self, **kwargs):\n+        self.input_spec = None\n+        self.supports_masking = False\n+        self.stateful = False\n+\n+        # These properties will be set upon call of self.build()\n+        self._trainable_weights = []\n+        self._non_trainable_weights = []\n+        self._losses = []\n+        self._updates = []\n+        self._per_input_losses = {}\n+        self._per_input_updates = {}\n+        self._built = False\n+\n+        # These lists will be filled via successive calls\n+        # to self._add_inbound_node().\n+        self._inbound_nodes = []\n+        self._outbound_nodes = []\n+\n+        # These properties should be set by the user via keyword arguments.\n+        # note that 'dtype', 'input_shape' and 'batch_input_shape'\n+        # are only applicable to input layers: do not pass these keywords\n+        # to non-input layers.\n+        allowed_kwargs = {'input_shape',\n+                          'batch_input_shape',\n+                          'batch_size',\n+                          'dtype',\n+                          'name',\n+                          'trainable',\n+                          'weights',\n+                          'input_dtype',  # legacy\n+                          }\n+        for kwarg in kwargs:\n+            if kwarg not in allowed_kwargs:\n+                raise TypeError('Keyword argument not understood:', kwarg)\n+        name = kwargs.get('name')\n+        if not name:\n+            prefix = self.__class__.__name__\n+            name = _to_snake_case(prefix) + '_' + str(K.get_uid(prefix))\n+        self.name = name\n+\n+        self.trainable = kwargs.get('trainable', True)\n+        if 'input_shape' in kwargs or 'batch_input_shape' in kwargs:\n+            # In this case we will later create an input layer\n+            # to insert before the current layer\n+            if 'batch_input_shape' in kwargs:\n+                batch_input_shape = tuple(kwargs['batch_input_shape'])\n+            elif 'input_shape' in kwargs:\n+                if 'batch_size' in kwargs:\n+                    batch_size = kwargs['batch_size']\n+                else:\n+                    batch_size = None\n+                batch_input_shape = (\n+                    batch_size,) + tuple(kwargs['input_shape'])\n+            self.batch_input_shape = batch_input_shape\n+\n+            # Set dtype.\n+            dtype = kwargs.get('dtype')\n+            if dtype is None:\n+                dtype = kwargs.get('input_dtype')\n+            if dtype is None:\n+                dtype = K.floatx()\n+            self.dtype = dtype\n+\n+        if 'weights' in kwargs:\n+            self._initial_weights = kwargs['weights']\n+        else:\n+            self._initial_weights = None\n+\n+    @staticmethod\n+    def _node_key(layer, node_index):\n+        \"\"\"Converts a layer and its index to a unique (immutable type) name.\n+\n+        This function is used internally with `self._network_nodes`.\n+\n+        # Arguments\n+            layer: The layer.\n+            node_index: The layer's position (e.g. via enumerate) in a list of\n+                nodes.\n+\n+        # Returns\n+            The unique name.\n+        \"\"\"\n+        return layer.name + '_ib-' + str(node_index)\n+\n+    @property\n+    def losses(self):\n+        return self._losses\n+\n+    @property\n+    def updates(self):\n+        if not self.trainable and not self.stateful:\n+            return []\n+        return self._updates\n+\n+    @property\n+    def built(self):\n+        return self._built\n+\n+    @built.setter\n+    def built(self, value):\n+        self._built = value\n+\n+    @property\n+    def trainable_weights(self):\n+        trainable = getattr(self, 'trainable', True)\n+        if trainable:\n+            return self._trainable_weights\n+        else:\n+            return []\n+\n+    @trainable_weights.setter\n+    def trainable_weights(self, weights):\n+        self._trainable_weights = weights\n+\n+    @property\n+    def non_trainable_weights(self):\n+        trainable = getattr(self, 'trainable', True)\n+        if not trainable:\n+            return self._trainable_weights + self._non_trainable_weights\n+        else:\n+            return self._non_trainable_weights\n+\n+    @non_trainable_weights.setter\n+    def non_trainable_weights(self, weights):\n+        self._non_trainable_weights = weights\n+\n+    @interfaces.legacy_add_weight_support\n+    def add_weight(self,\n+                   name,\n+                   shape,\n+                   dtype=None,\n+                   initializer=None,\n+                   regularizer=None,\n+                   trainable=True,\n+                   constraint=None):\n+        \"\"\"Adds a weight variable to the layer.\n+\n+        # Arguments\n+            name: String, the name for the weight variable.\n+            shape: The shape tuple of the weight.\n+            dtype: The dtype of the weight.\n+            initializer: An Initializer instance (callable).\n+            regularizer: An optional Regularizer instance.\n+            trainable: A boolean, whether the weight should\n+                be trained via backprop or not (assuming\n+                that the layer itself is also trainable).\n+            constraint: An optional Constraint instance.\n+\n+        # Returns\n+            The created weight variable.\n+        \"\"\"\n+        initializer = initializers.get(initializer)\n+        if dtype is None:\n+            dtype = K.floatx()\n+        weight = K.variable(initializer(shape),\n+                            dtype=dtype,\n+                            name=name,\n+                            constraint=constraint)\n+        if regularizer is not None:\n+            self.add_loss(regularizer(weight))\n+        if trainable:\n+            self._trainable_weights.append(weight)\n+        else:\n+            self._non_trainable_weights.append(weight)\n+        return weight\n+\n+    def assert_input_compatibility(self, inputs):\n+        \"\"\"Checks compatibility between the layer and provided inputs.\n+\n+        This checks that the tensor(s) `input`\n+        verify the input assumptions of the layer\n+        (if any). If not, exceptions are raised.\n+\n+        # Arguments\n+            inputs: input tensor or list of input tensors.\n+\n+        # Raises\n+            ValueError: in case of mismatch between\n+                the provided inputs and the expectations of the layer.\n+        \"\"\"\n+        inputs = to_list(inputs)\n+        for x in inputs:\n+            try:\n+                K.is_keras_tensor(x)\n+            except ValueError:\n+                raise ValueError('Layer ' + self.name + ' was called with '\n+                                 'an input that isn\\'t a symbolic tensor. '\n+                                 'Received type: ' +\n+                                 str(type(x)) + '. Full input: ' +\n+                                 str(inputs) + '. All inputs to the layer '\n+                                 'should be tensors.')\n+\n+        if not self.input_spec:\n+            return\n+        if not isinstance(self.input_spec, (list, tuple)):\n+            input_spec = to_list(self.input_spec)\n+        else:\n+            input_spec = self.input_spec\n+        if len(inputs) != len(input_spec):\n+            raise ValueError('Layer ' + self.name + ' expects ' +\n+                             str(len(input_spec)) + ' inputs, '\n+                             'but it received ' + str(len(inputs)) +\n+                             ' input tensors. Input received: ' +\n+                             str(inputs))\n+        for input_index, (x, spec) in enumerate(zip(inputs, input_spec)):\n+            if spec is None:\n+                continue\n+\n+            # Check ndim.\n+            if spec.ndim is not None:\n+                if K.ndim(x) != spec.ndim:\n+                    raise ValueError('Input ' + str(input_index) +\n+                                     ' is incompatible with layer ' +\n+                                     self.name + ': expected ndim=' +\n+                                     str(spec.ndim) + ', found ndim=' +\n+                                     str(K.ndim(x)))\n+            if spec.max_ndim is not None:\n+                ndim = K.ndim(x)\n+                if ndim is not None and ndim > spec.max_ndim:\n+                    raise ValueError('Input ' + str(input_index) +\n+                                     ' is incompatible with layer ' +\n+                                     self.name + ': expected max_ndim=' +\n+                                     str(spec.max_ndim) + ', found ndim=' +\n+                                     str(K.ndim(x)))\n+            if spec.min_ndim is not None:\n+                ndim = K.ndim(x)\n+                if ndim is not None and ndim < spec.min_ndim:\n+                    raise ValueError('Input ' + str(input_index) +\n+                                     ' is incompatible with layer ' +\n+                                     self.name + ': expected min_ndim=' +\n+                                     str(spec.min_ndim) + ', found ndim=' +\n+                                     str(K.ndim(x)))\n+            # Check dtype.\n+            if spec.dtype is not None:\n+                if K.dtype(x) != spec.dtype:\n+                    raise ValueError('Input ' + str(input_index) +\n+                                     ' is incompatible with layer ' +\n+                                     self.name + ': expected dtype=' +\n+                                     str(spec.dtype) + ', found dtype=' +\n+                                     str(K.dtype(x)))\n+            # Check specific shape axes.\n+            if spec.axes:\n+                try:\n+                    x_shape = K.int_shape(x)\n+                except TypeError:\n+                    x_shape = None\n+                if x_shape is not None:\n+                    for axis, value in spec.axes.items():\n+                        if (value is not None and\n+                                x_shape[int(axis)] not in {value, None}):\n+                            raise ValueError(\n+                                'Input ' + str(input_index) +\n+                                ' is incompatible with layer ' +\n+                                self.name + ': expected axis ' +\n+                                str(axis) + ' of input shape to have '\n+                                'value ' + str(value) +\n+                                ' but got shape ' + str(x_shape))\n+            # Check shape.\n+            if spec.shape is not None:\n+                try:\n+                    x_shape = K.int_shape(x)\n+                except TypeError:\n+                    x_shape = None\n+                if x_shape is not None:\n+                    for spec_dim, dim in zip(spec.shape, x_shape):\n+                        if spec_dim is not None and dim is not None:\n+                            if spec_dim != dim:\n+                                raise ValueError(\n+                                    'Input ' + str(input_index) +\n+                                    ' is incompatible with layer ' +\n+                                    self.name + ': expected shape=' +\n+                                    str(spec.shape) + ', found shape=' +\n+                                    str(x_shape))\n+\n+    def call(self, inputs, **kwargs):\n+        \"\"\"This is where the layer's logic lives.\n+\n+        # Arguments\n+            inputs: Input tensor, or list/tuple of input tensors.\n+            **kwargs: Additional keyword arguments.\n+\n+        # Returns\n+            A tensor or list/tuple of tensors.\n+        \"\"\"\n+        return inputs\n+\n+    def __call__(self, inputs, **kwargs):\n+        \"\"\"Wrapper around self.call(), for handling internal references.\n+\n+        If a Keras tensor is passed:\n+            - We call self._add_inbound_node().\n+            - If necessary, we `build` the layer to match\n+                the _keras_shape of the input(s).\n+            - We update the _keras_shape of every input tensor with\n+                its new shape (obtained via self.compute_output_shape).\n+                This is done as part of _add_inbound_node().\n+            - We update the _keras_history of the output tensor(s)\n+                with the current layer.\n+                This is done as part of _add_inbound_node().\n+\n+        # Arguments\n+            inputs: Can be a tensor or list/tuple of tensors.\n+            **kwargs: Additional keyword arguments to be passed to `call()`.\n+\n+        # Returns\n+            Output of the layer's `call` method.\n+\n+        # Raises\n+            ValueError: in case the layer is missing shape information\n+                for its `build` call.\n+        \"\"\"\n+        if isinstance(inputs, list):\n+            inputs = inputs[:]\n+        with K.name_scope(self.name):\n+            # Handle laying building (weight creating, input spec locking).\n+            if not self.built:\n+                # Raise exceptions in case the input is not compatible\n+                # with the input_spec specified in the layer constructor.\n+                self.assert_input_compatibility(inputs)\n+\n+                # Collect input shapes to build layer.\n+                input_shapes = []\n+                for x_elem in to_list(inputs):\n+                    if hasattr(x_elem, '_keras_shape'):\n+                        input_shapes.append(x_elem._keras_shape)\n+                    elif hasattr(K, 'int_shape'):\n+                        input_shapes.append(K.int_shape(x_elem))\n+                    else:\n+                        raise ValueError('You tried to call layer \"' +\n+                                         self.name +\n+                                         '\". This layer has no information'\n+                                         ' about its expected input shape, '\n+                                         'and thus cannot be built. '\n+                                         'You can build it manually via: '\n+                                         '`layer.build(batch_input_shape)`')\n+                if len(input_shapes) == 1:\n+                    self.build(input_shapes[0])\n+                else:\n+                    self.build(input_shapes)\n+                self.built = True\n+\n+                # Load weights that were specified at layer instantiation.\n+                if self._initial_weights is not None:\n+                    self.set_weights(self._initial_weights)\n+\n+            # Raise exceptions in case the input is not compatible\n+            # with the input_spec set at build time.\n+            self.assert_input_compatibility(inputs)\n+\n+            # Handle mask propagation.\n+            previous_mask = _collect_previous_mask(inputs)\n+            user_kwargs = copy.copy(kwargs)\n+            if not is_all_none(previous_mask):\n+                # The previous layer generated a mask.\n+                if has_arg(self.call, 'mask'):\n+                    if 'mask' not in kwargs:\n+                        # If mask is explicitly passed to __call__,\n+                        # we should override the default mask.\n+                        kwargs['mask'] = previous_mask\n+            # Handle automatic shape inference (only useful for Theano).\n+            input_shape = _collect_input_shape(inputs)\n+\n+            # Actually call the layer,\n+            # collecting output(s), mask(s), and shape(s).\n+            output = self.call(inputs, **kwargs)\n+            output_mask = self.compute_mask(inputs, previous_mask)\n+\n+            # If the layer returns tensors from its inputs, unmodified,\n+            # we copy them to avoid loss of tensor metadata.\n+            output_ls = to_list(output)\n+            inputs_ls = to_list(inputs)\n+            output_ls_copy = []\n+            for x in output_ls:\n+                if x in inputs_ls:\n+                    x = K.identity(x)\n+                output_ls_copy.append(x)\n+            if len(output_ls_copy) == 1:\n+                output = output_ls_copy[0]\n+            else:\n+                output = output_ls_copy\n+\n+            # Inferring the output shape is only relevant for Theano.\n+            if all([s is not None\n+                    for s in to_list(input_shape)]):\n+                output_shape = self.compute_output_shape(input_shape)\n+            else:\n+                if isinstance(input_shape, list):\n+                    output_shape = [None for _ in input_shape]\n+                else:\n+                    output_shape = None\n+\n+            if (not isinstance(output_mask, (list, tuple)) and\n+                    len(output_ls) > 1):\n+                # Augment the mask to match the length of the output.\n+                output_mask = [output_mask] * len(output_ls)\n+\n+            # Add an inbound node to the layer, so that it keeps track\n+            # of the call and of all new variables created during the call.\n+            # This also updates the layer history of the output tensor(s).\n+            # If the input tensor(s) had not previous Keras history,\n+            # this does nothing.\n+            self._add_inbound_node(input_tensors=inputs,\n+                                   output_tensors=output,\n+                                   input_masks=previous_mask,\n+                                   output_masks=output_mask,\n+                                   input_shapes=input_shape,\n+                                   output_shapes=output_shape,\n+                                   arguments=user_kwargs)\n+\n+            # Apply activity regularizer if any:\n+            if (hasattr(self, 'activity_regularizer') and\n+                    self.activity_regularizer is not None):\n+                regularization_losses = [\n+                    self.activity_regularizer(x)\n+                    for x in to_list(output)]\n+                self.add_loss(regularization_losses,\n+                              inputs=to_list(inputs))\n+        return output\n+\n+    def _add_inbound_node(self, input_tensors, output_tensors,\n+                          input_masks, output_masks,\n+                          input_shapes, output_shapes, arguments=None):\n+        \"\"\"Internal method to create an inbound node for the layer.\n+\n+        # Arguments\n+            input_tensors: list of input tensors.\n+            output_tensors: list of output tensors.\n+            input_masks: list of input masks (a mask can be a tensor, or None).\n+            output_masks: list of output masks\n+                (a mask can be a tensor, or None).\n+            input_shapes: list of input shape tuples.\n+            output_shapes: list of output shape tuples.\n+            arguments: dictionary of keyword arguments that were passed to the\n+                `call` method of the layer at the call that created the node.\n+        \"\"\"\n+        input_tensors = to_list(input_tensors)\n+        output_tensors = to_list(output_tensors)\n+        input_masks = to_list(input_masks)\n+        output_masks = to_list(output_masks)\n+        input_shapes = to_list(input_shapes)\n+        output_shapes = to_list(output_shapes)\n+\n+        # Collect input tensor(s) coordinates.\n+        inbound_layers = []\n+        node_indices = []\n+        tensor_indices = []\n+        for x in input_tensors:\n+            if hasattr(x, '_keras_history'):\n+                inbound_layer, node_index, tensor_index = x._keras_history\n+                inbound_layers.append(inbound_layer)\n+                node_indices.append(node_index)\n+                tensor_indices.append(tensor_index)\n+            else:\n+                inbound_layers.append(None)\n+                node_indices.append(None)\n+                tensor_indices.append(None)\n+\n+        # Create node, add it to inbound nodes.\n+        Node(\n+            self,\n+            inbound_layers=inbound_layers,\n+            node_indices=node_indices,\n+            tensor_indices=tensor_indices,\n+            input_tensors=input_tensors,\n+            output_tensors=output_tensors,\n+            input_masks=input_masks,\n+            output_masks=output_masks,\n+            input_shapes=input_shapes,\n+            output_shapes=output_shapes,\n+            arguments=arguments\n+        )\n+\n+        # Update tensor history, _keras_shape and _uses_learning_phase.\n+        for i in range(len(output_tensors)):\n+            output_tensors[i]._keras_shape = output_shapes[i]\n+            uses_lp = any(\n+                [getattr(x, '_uses_learning_phase', False)\n+                 for x in input_tensors])\n+            uses_lp = getattr(self, 'uses_learning_phase', False) or uses_lp\n+            output_tensors[i]._uses_learning_phase = getattr(\n+                output_tensors[i], '_uses_learning_phase', False) or uses_lp\n+            output_tensors[i]._keras_history = (self,\n+                                                len(self._inbound_nodes) - 1,\n+                                                i)\n+\n+    def compute_output_shape(self, input_shape):\n+        \"\"\"Computes the output shape of the layer.\n+\n+        Assumes that the layer will be built\n+        to match that input shape provided.\n+\n+        # Arguments\n+            input_shape: Shape tuple (tuple of integers)\n+                or list of shape tuples (one per output tensor of the layer).\n+                Shape tuples can include None for free dimensions,\n+                instead of an integer.\n+\n+        # Returns\n+            An input shape tuple.\n+        \"\"\"\n+        return input_shape\n+\n+    def compute_mask(self, inputs, mask=None):\n+        \"\"\"Computes an output mask tensor.\n+\n+        # Arguments\n+            inputs: Tensor or list of tensors.\n+            mask: Tensor or list of tensors.\n+\n+        # Returns\n+            None or a tensor (or list of tensors,\n+                one per output tensor of the layer).\n+        \"\"\"\n+        if not self.supports_masking:\n+            if mask is not None:\n+                if isinstance(mask, list):\n+                    if any(m is not None for m in mask):\n+                        raise TypeError('Layer ' + self.name +\n+                                        ' does not support masking, '\n+                                        'but was passed an input_mask: ' +\n+                                        str(mask))\n+                else:\n+                    raise TypeError('Layer ' + self.name +\n+                                    ' does not support masking, '\n+                                    'but was passed an input_mask: ' +\n+                                    str(mask))\n+            # masking not explicitly supported: return None as mask\n+            return None\n+        # if masking is explicitly supported, by default\n+        # carry over the input mask\n+        return mask\n+\n+    def build(self, input_shape):\n+        \"\"\"Creates the layer weights.\n+\n+        Must be implemented on all layers that have weights.\n+\n+        # Arguments\n+            input_shape: Keras tensor (future input to layer)\n+                or list/tuple of Keras tensors to reference\n+                for weight shape computations.\n+        \"\"\"\n+        self.built = True\n+\n+    def _get_node_attribute_at_index(self, node_index, attr, attr_name):\n+        \"\"\"Retrieves an attribute (e.g. input_tensors) from a node.\n+\n+        This is used to implement the methods:\n+            - get_input_shape_at\n+            - get_output_shape_at\n+            - get_input_at\n+            etc...\n+\n+        # Arguments\n+            node_index: Integer index of the node from which\n+                to retrieve the attribute.\n+            attr: Exact node attribute name.\n+            attr_name: Human-readable attribute name, for error messages.\n+\n+        # Returns\n+            The layer's attribute `attr` at the node of index `node_index`.\n+\n+        # Raises\n+            RuntimeError: If the layer has no inbound nodes.\n+            ValueError: If the index is does not match any node.\n+        \"\"\"\n+        if not self._inbound_nodes:\n+            raise RuntimeError('The layer has never been called '\n+                               'and thus has no defined ' + attr_name + '.')\n+        if not len(self._inbound_nodes) > node_index:\n+            raise ValueError('Asked to get ' + attr_name +\n+                             ' at node ' + str(node_index) +\n+                             ', but the layer has only ' +\n+                             str(len(self._inbound_nodes)) + ' inbound nodes.')\n+        values = getattr(self._inbound_nodes[node_index], attr)\n+        if len(values) == 1:\n+            return values[0]\n+        else:\n+            return values\n+\n+    def get_input_shape_at(self, node_index):\n+        \"\"\"Retrieves the input shape(s) of a layer at a given node.\n+\n+        # Arguments\n+            node_index: Integer, index of the node\n+                from which to retrieve the attribute.\n+                E.g. `node_index=0` will correspond to the\n+                first time the layer was called.\n+\n+        # Returns\n+            A shape tuple\n+            (or list of shape tuples if the layer has multiple inputs).\n+        \"\"\"\n+        return self._get_node_attribute_at_index(node_index,\n+                                                 'input_shapes',\n+                                                 'input shape')\n+\n+    def get_output_shape_at(self, node_index):\n+        \"\"\"Retrieves the output shape(s) of a layer at a given node.\n+\n+        # Arguments\n+            node_index: Integer, index of the node\n+                from which to retrieve the attribute.\n+                E.g. `node_index=0` will correspond to the\n+                first time the layer was called.\n+\n+        # Returns\n+            A shape tuple\n+            (or list of shape tuples if the layer has multiple outputs).\n+        \"\"\"\n+        return self._get_node_attribute_at_index(node_index,\n+                                                 'output_shapes',\n+                                                 'output shape')\n+\n+    def get_input_at(self, node_index):\n+        \"\"\"Retrieves the input tensor(s) of a layer at a given node.\n+\n+        # Arguments\n+            node_index: Integer, index of the node\n+                from which to retrieve the attribute.\n+                E.g. `node_index=0` will correspond to the\n+                first time the layer was called.\n+\n+        # Returns\n+            A tensor (or list of tensors if the layer has multiple inputs).\n+        \"\"\"\n+        return self._get_node_attribute_at_index(node_index,\n+                                                 'input_tensors',\n+                                                 'input')\n+\n+    def get_output_at(self, node_index):\n+        \"\"\"Retrieves the output tensor(s) of a layer at a given node.\n+\n+        # Arguments\n+            node_index: Integer, index of the node\n+                from which to retrieve the attribute.\n+                E.g. `node_index=0` will correspond to the\n+                first time the layer was called.\n+\n+        # Returns\n+            A tensor (or list of tensors if the layer has multiple outputs).\n+        \"\"\"\n+        return self._get_node_attribute_at_index(node_index,\n+                                                 'output_tensors',\n+                                                 'output')\n+\n+    def get_input_mask_at(self, node_index):\n+        \"\"\"Retrieves the input mask tensor(s) of a layer at a given node.\n+\n+        # Arguments\n+            node_index: Integer, index of the node\n+                from which to retrieve the attribute.\n+                E.g. `node_index=0` will correspond to the\n+                first time the layer was called.\n+\n+        # Returns\n+            A mask tensor\n+            (or list of tensors if the layer has multiple inputs).\n+        \"\"\"\n+        return self._get_node_attribute_at_index(node_index,\n+                                                 'input_masks',\n+                                                 'input mask')\n+\n+    def get_output_mask_at(self, node_index):\n+        \"\"\"Retrieves the output mask tensor(s) of a layer at a given node.\n+\n+        # Arguments\n+            node_index: Integer, index of the node\n+                from which to retrieve the attribute.\n+                E.g. `node_index=0` will correspond to the\n+                first time the layer was called.\n+\n+        # Returns\n+            A mask tensor\n+            (or list of tensors if the layer has multiple outputs).\n+        \"\"\"\n+        return self._get_node_attribute_at_index(node_index,\n+                                                 'output_masks',\n+                                                 'output mask')\n+\n+    @property\n+    def input(self):\n+        \"\"\"Retrieves the input tensor(s) of a layer.\n+\n+        Only applicable if the layer has exactly one inbound node,\n+        i.e. if it is connected to one incoming layer.\n+\n+        # Returns\n+            Input tensor or list of input tensors.\n+\n+        # Raises\n+            AttributeError: if the layer is connected to\n+            more than one incoming layers.\n+        \"\"\"\n+        if len(self._inbound_nodes) > 1:\n+            raise AttributeError('Layer ' + self.name +\n+                                 ' has multiple inbound nodes, '\n+                                 'hence the notion of \"layer input\" '\n+                                 'is ill-defined. '\n+                                 'Use `get_input_at(node_index)` instead.')\n+        elif not self._inbound_nodes:\n+            raise AttributeError('Layer ' + self.name +\n+                                 ' is not connected, no input to return.')\n+        return self._get_node_attribute_at_index(0, 'input_tensors',\n+                                                 'input')\n+\n+    @property\n+    def output(self):\n+        \"\"\"Retrieves the output tensor(s) of a layer.\n+\n+        Only applicable if the layer has exactly one inbound node,\n+        i.e. if it is connected to one incoming layer.\n+\n+        # Returns\n+            Output tensor or list of output tensors.\n+\n+        # Raises\n+            AttributeError: if the layer is connected to\n+            more than one incoming layers.\n+        \"\"\"\n+        if not self._inbound_nodes:\n+            raise AttributeError('Layer ' + self.name +\n+                                 ' has no inbound nodes.')\n+        if len(self._inbound_nodes) > 1:\n+            raise AttributeError('Layer ' + self.name +\n+                                 ' has multiple inbound nodes, '\n+                                 'hence the notion of \"layer output\" '\n+                                 'is ill-defined. '\n+                                 'Use `get_output_at(node_index)` instead.')\n+        return self._get_node_attribute_at_index(0, 'output_tensors',\n+                                                 'output')\n+\n+    @property\n+    def input_mask(self):\n+        \"\"\"Retrieves the input mask tensor(s) of a layer.\n+\n+        Only applicable if the layer has exactly one inbound node,\n+        i.e. if it is connected to one incoming layer.\n+\n+        # Returns\n+            Input mask tensor (potentially None) or list of input\n+            mask tensors.\n+\n+        # Raises\n+            AttributeError: if the layer is connected to\n+            more than one incoming layers.\n+        \"\"\"\n+        if len(self._inbound_nodes) != 1:\n+            raise AttributeError('Layer ' + self.name +\n+                                 ' has multiple inbound nodes, ' +\n+                                 'hence the notion of \"layer input mask\" '\n+                                 'is ill-defined. '\n+                                 'Use `get_input_mask_at(node_index)` '\n+                                 'instead.')\n+        return self._get_node_attribute_at_index(0, 'input_masks',\n+                                                 'input mask')\n+\n+    @property\n+    def output_mask(self):\n+        \"\"\"Retrieves the output mask tensor(s) of a layer.\n+\n+        Only applicable if the layer has exactly one inbound node,\n+        i.e. if it is connected to one incoming layer.\n+\n+        # Returns\n+            Output mask tensor (potentially None) or list of output\n+            mask tensors.\n+\n+        # Raises\n+            AttributeError: if the layer is connected to\n+            more than one incoming layers.\n+        \"\"\"\n+        if len(self._inbound_nodes) != 1:\n+            raise AttributeError('Layer ' + self.name +\n+                                 ' has multiple inbound nodes, '\n+                                 'hence the notion of \"layer output mask\" '\n+                                 'is ill-defined. '\n+                                 'Use `get_output_mask_at(node_index)` '\n+                                 'instead.')\n+        return self._get_node_attribute_at_index(0, 'output_masks',\n+                                                 'output mask')\n+\n+    @property\n+    def input_shape(self):\n+        \"\"\"Retrieves the input shape tuple(s) of a layer.\n+\n+        Only applicable if the layer has exactly one inbound node,\n+        i.e. if it is connected to one incoming layer.\n+\n+        # Returns\n+            Input shape tuple\n+            (or list of input shape tuples, one tuple per input tensor).\n+\n+        # Raises\n+            AttributeError: if the layer is connected to\n+            more than one incoming layers.\n+        \"\"\"\n+        if not self._inbound_nodes:\n+            raise AttributeError('The layer has never been called '\n+                                 'and thus has no defined input shape.')\n+        all_input_shapes = set(\n+            [str(node.input_shapes) for node in self._inbound_nodes])\n+        if len(all_input_shapes) == 1:\n+            input_shapes = self._inbound_nodes[0].input_shapes\n+            if len(input_shapes) == 1:\n+                return input_shapes[0]\n+            else:\n+                return input_shapes\n+        else:\n+            raise AttributeError('The layer \"' + str(self.name) +\n+                                 ' has multiple inbound nodes, '\n+                                 'with different input shapes. Hence '\n+                                 'the notion of \"input shape\" is '\n+                                 'ill-defined for the layer. '\n+                                 'Use `get_input_shape_at(node_index)` '\n+                                 'instead.')\n+\n+    @property\n+    def output_shape(self):\n+        \"\"\"Retrieves the output shape tuple(s) of a layer.\n+\n+        Only applicable if the layer has one inbound node,\n+        or if all inbound nodes have the same output shape.\n+\n+        # Returns\n+            Output shape tuple\n+            (or list of input shape tuples, one tuple per output tensor).\n+\n+        # Raises\n+            AttributeError: if the layer is connected to\n+            more than one incoming layers.\n+        \"\"\"\n+        if not self._inbound_nodes:\n+            raise AttributeError('The layer has never been called '\n+                                 'and thus has no defined output shape.')\n+        all_output_shapes = set(\n+            [str(node.output_shapes) for node in self._inbound_nodes])\n+        if len(all_output_shapes) == 1:\n+            output_shapes = self._inbound_nodes[0].output_shapes\n+            if len(output_shapes) == 1:\n+                return output_shapes[0]\n+            else:\n+                return output_shapes\n+        else:\n+            raise AttributeError('The layer \"' + str(self.name) +\n+                                 ' has multiple inbound nodes, '\n+                                 'with different output shapes. Hence '\n+                                 'the notion of \"output shape\" is '\n+                                 'ill-defined for the layer. '\n+                                 'Use `get_output_shape_at(node_index)` '\n+                                 'instead.')\n+\n+    def add_loss(self, losses, inputs=None):\n+        \"\"\"Adds losses to the layer.\n+\n+        The loss may potentially be conditional on some inputs tensors,\n+        for instance activity losses are conditional on the layer's inputs.\n+\n+        # Arguments\n+            losses: loss tensor or list of loss tensors\n+                to add to the layer.\n+            inputs: input tensor or list of inputs tensors to mark\n+                the losses as conditional on these inputs.\n+                If None is passed, the loss is assumed unconditional\n+                (e.g. L2 weight regularization, which only depends\n+                on the layer's weights variables, not on any inputs tensors).\n+        \"\"\"\n+        if losses is None or losses == []:\n+            return\n+        # Update self.losses\n+        losses = to_list(losses)\n+        if hasattr(self, '_losses'):\n+            self._losses += losses\n+        # Update self._per_input_updates\n+        if isinstance(inputs, list) and inputs == []:\n+            inputs = None\n+        if inputs is not None:\n+            inputs_hash = object_list_uid(inputs)\n+        else:\n+            # Updates indexed by None are unconditional\n+            # rather than input-dependent\n+            inputs_hash = None\n+        if inputs_hash not in self._per_input_losses:\n+            self._per_input_losses[inputs_hash] = []\n+        self._per_input_losses[inputs_hash] += losses\n+\n+    def add_update(self, updates, inputs=None):\n+        \"\"\"Adds updates to the layer.\n+\n+        The updates may potentially be conditional on some inputs tensors,\n+        for instance batch norm updates are conditional on the layer's inputs.\n+\n+        # Arguments\n+            updates: update op or list of update ops\n+                to add to the layer.\n+            inputs: input tensor or list of inputs tensors to mark\n+                the updates as conditional on these inputs.\n+                If None is passed, the updates are assumed unconditional.\n+        \"\"\"\n+        if updates is None or updates == []:\n+            return\n+        # Update self.updates\n+        updates = to_list(updates)\n+        if hasattr(self, '_updates'):\n+            self._updates += updates\n+        # Update self._per_input_updates\n+        if isinstance(inputs, list) and inputs == []:\n+            inputs = None\n+        if inputs is not None:\n+            inputs_hash = object_list_uid(inputs)\n+        else:\n+            # Updates indexed by None are unconditional\n+            # rather than input-dependent\n+            inputs_hash = None\n+        if inputs_hash not in self._per_input_updates:\n+            self._per_input_updates[inputs_hash] = []\n+        self._per_input_updates[inputs_hash] += updates\n+\n+    def get_updates_for(self, inputs):\n+        if not self.trainable and not self.stateful:\n+            return []\n+        if inputs is not None:\n+            inputs_hash = object_list_uid(inputs)\n+        else:\n+            inputs_hash = None\n+        if inputs_hash in self._per_input_updates:\n+            return self._per_input_updates[inputs_hash]\n+        return []\n+\n+    def get_losses_for(self, inputs):\n+        if inputs is not None:\n+            inputs_hash = object_list_uid(inputs)\n+        else:\n+            inputs_hash = None\n+        if inputs_hash in self._per_input_losses:\n+            return self._per_input_losses[inputs_hash]\n+        return []\n+\n+    @property\n+    def weights(self):\n+        return self.trainable_weights + self.non_trainable_weights\n+\n+    def set_weights(self, weights):\n+        \"\"\"Sets the weights of the layer, from Numpy arrays.\n+\n+        # Arguments\n+            weights: a list of Numpy arrays. The number\n+                of arrays and their shape must match\n+                number of the dimensions of the weights\n+                of the layer (i.e. it should match the\n+                output of `get_weights`).\n+\n+        # Raises\n+            ValueError: If the provided weights list does not match the\n+                layer's specifications.\n+        \"\"\"\n+        params = self.weights\n+        if len(params) != len(weights):\n+            raise ValueError('You called `set_weights(weights)` on layer \"' +\n+                             self.name +\n+                             '\" with a  weight list of length ' +\n+                             str(len(weights)) +\n+                             ', but the layer was expecting ' +\n+                             str(len(params)) +\n+                             ' weights. Provided weights: ' +\n+                             str(weights)[:50] + '...')\n+        if not params:\n+            return\n+        weight_value_tuples = []\n+        param_values = K.batch_get_value(params)\n+        for pv, p, w in zip(param_values, params, weights):\n+            if pv.shape != w.shape:\n+                raise ValueError('Layer weight shape ' +\n+                                 str(pv.shape) +\n+                                 ' not compatible with '\n+                                 'provided weight shape ' + str(w.shape))\n+            weight_value_tuples.append((p, w))\n+        K.batch_set_value(weight_value_tuples)\n+\n+    def get_weights(self):\n+        \"\"\"Returns the current weights of the layer.\n+\n+        # Returns\n+            Weights values as a list of numpy arrays.\n+        \"\"\"\n+        params = self.weights\n+        return K.batch_get_value(params)\n+\n+    def get_config(self):\n+        \"\"\"Returns the config of the layer.\n+\n+        A layer config is a Python dictionary (serializable)\n+        containing the configuration of a layer.\n+        The same layer can be reinstantiated later\n+        (without its trained weights) from this configuration.\n+\n+        The config of a layer does not include connectivity\n+        information, nor the layer class name. These are handled\n+        by `Network` (one layer of abstraction above).\n+\n+        # Returns\n+            Python dictionary.\n+        \"\"\"\n+        config = {'name': self.name,\n+                  'trainable': self.trainable}\n+        if hasattr(self, 'batch_input_shape'):\n+            config['batch_input_shape'] = self.batch_input_shape\n+        if hasattr(self, 'dtype'):\n+            config['dtype'] = self.dtype\n+        return config\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        \"\"\"Creates a layer from its config.\n+\n+        This method is the reverse of `get_config`,\n+        capable of instantiating the same layer from the config\n+        dictionary. It does not handle layer connectivity\n+        (handled by Network), nor weights (handled by `set_weights`).\n+\n+        # Arguments\n+            config: A Python dictionary, typically the\n+                output of get_config.\n+\n+        # Returns\n+            A layer instance.\n+        \"\"\"\n+        return cls(**config)\n+\n+    def count_params(self):\n+        \"\"\"Counts the total number of scalars composing the weights.\n+\n+        # Returns\n+            An integer count.\n+\n+        # Raises\n+            RuntimeError: if the layer isn't yet built\n+                (in which case its weights aren't yet defined).\n+        \"\"\"\n+        if not self.built:\n+            if self.__class__.__name__ == 'Sequential':\n+                self.build()\n+            else:\n+                raise RuntimeError('You tried to call `count_params` on ' +\n+                                   self.name + ', but the layer isn\\'t built. '\n+                                   'You can build it manually via: `' +\n+                                   self.name + '.build(batch_input_shape)`.')\n+        return count_params(self.weights)\n+\n+\n+class InputSpec(object):\n+    \"\"\"Specifies the ndim, dtype and shape of every input to a layer.\n+\n+    Every layer should expose (if appropriate) an `input_spec` attribute:\n+    a list of instances of InputSpec (one per input tensor).\n+\n+    A None entry in a shape is compatible with any dimension,\n+    a None shape is compatible with any shape.\n+\n+    # Arguments\n+        dtype: Expected datatype of the input.\n+        shape: Shape tuple, expected shape of the input\n+            (may include None for unchecked axes).\n+        ndim: Integer, expected rank of the input.\n+        max_ndim: Integer, maximum rank of the input.\n+        min_ndim: Integer, minimum rank of the input.\n+        axes: Dictionary mapping integer axes to\n+            a specific dimension value.\n+    \"\"\"\n+\n+    def __init__(self, dtype=None,\n+                 shape=None,\n+                 ndim=None,\n+                 max_ndim=None,\n+                 min_ndim=None,\n+                 axes=None):\n+        self.dtype = dtype\n+        self.shape = shape\n+        if shape is not None:\n+            self.ndim = len(shape)\n+        else:\n+            self.ndim = ndim\n+        self.max_ndim = max_ndim\n+        self.min_ndim = min_ndim\n+        self.axes = axes or {}\n+\n+    def __repr__(self):\n+        spec = [('dtype=' + str(self.dtype)) if self.dtype else '',\n+                ('shape=' + str(self.shape)) if self.shape else '',\n+                ('ndim=' + str(self.ndim)) if self.ndim else '',\n+                ('max_ndim=' + str(self.max_ndim)) if self.max_ndim else '',\n+                ('min_ndim=' + str(self.min_ndim)) if self.min_ndim else '',\n+                ('axes=' + str(self.axes)) if self.axes else '']\n+        return 'InputSpec(%s)' % ', '.join(x for x in spec if x)\n+\n+\n+class Node(object):\n+    \"\"\"A `Node` describes the connectivity between two layers.\n+\n+    Each time a layer is connected to some new input,\n+    a node is added to `layer._inbound_nodes`.\n+    Each time the output of a layer is used by another layer,\n+    a node is added to `layer._outbound_nodes`.\n+\n+    # Arguments\n+        outbound_layer: the layer that takes\n+            `input_tensors` and turns them into `output_tensors`\n+            (the node gets created when the `call`\n+            method of the layer was called).\n+        inbound_layers: a list of layers, the same length as `input_tensors`,\n+            the layers from where `input_tensors` originate.\n+        node_indices: a list of integers, the same length as `inbound_layers`.\n+            `node_indices[i]` is the origin node of `input_tensors[i]`\n+            (necessary since each inbound layer might have several nodes,\n+            e.g. if the layer is being shared with a different data stream).\n+        tensor_indices: a list of integers,\n+            the same length as `inbound_layers`.\n+            `tensor_indices[i]` is the index of `input_tensors[i]` within the\n+            output of the inbound layer\n+            (necessary since each inbound layer might\n+            have multiple tensor outputs, with each one being\n+            independently manipulable).\n+        input_tensors: list of input tensors.\n+        output_tensors: list of output tensors.\n+        input_masks: list of input masks (a mask can be a tensor, or None).\n+        output_masks: list of output masks (a mask can be a tensor, or None).\n+        input_shapes: list of input shape tuples.\n+        output_shapes: list of output shape tuples.\n+        arguments: dictionary of keyword arguments that were passed to the\n+            `call` method of the layer at the call that created the node.\n+\n+    `node_indices` and `tensor_indices` are basically fine-grained coordinates\n+    describing the origin of the `input_tensors`, verifying the following:\n+\n+    origin_node = inbound_layers[i]._inbound_nodes[node_indices[i]]\n+    input_tensors[i] == origin_node.output_tensors[tensor_indices[i]]\n+\n+    A node from layer A to layer B is added to:\n+        A._outbound_nodes\n+        B._inbound_nodes\n+    \"\"\"\n+\n+    def __init__(self, outbound_layer,\n+                 inbound_layers, node_indices, tensor_indices,\n+                 input_tensors, output_tensors,\n+                 input_masks, output_masks,\n+                 input_shapes, output_shapes,\n+                 arguments=None):\n+        # Layer instance (NOT a list).\n+        # this is the layer that takes a list of input tensors\n+        # and turns them into a list of output tensors.\n+        # the current node will be added to\n+        # the inbound_nodes of outbound_layer.\n+        self.outbound_layer = outbound_layer\n+\n+        # The following 3 properties describe where\n+        # the input tensors come from: which layers,\n+        # and for each layer, which node and which\n+        # tensor output of each node.\n+\n+        # List of layer instances.\n+        self.inbound_layers = inbound_layers\n+        # List of integers, 1:1 mapping with inbound_layers.\n+        self.node_indices = node_indices\n+        # List of integers, 1:1 mapping with inbound_layers.\n+        self.tensor_indices = tensor_indices\n+\n+        # Following 2 properties:\n+        # tensor inputs and outputs of outbound_layer.\n+\n+        # List of tensors. 1:1 mapping with inbound_layers.\n+        self.input_tensors = input_tensors\n+        # List of tensors, created by outbound_layer.call().\n+        self.output_tensors = output_tensors\n+\n+        # Following 2 properties: input and output masks.\n+        # List of tensors, 1:1 mapping with input_tensor.\n+        self.input_masks = input_masks\n+        # List of tensors, created by outbound_layer.compute_mask().\n+        self.output_masks = output_masks\n+\n+        # Following 2 properties: input and output shapes.\n+\n+        # List of shape tuples, shapes of input_tensors.\n+        self.input_shapes = input_shapes\n+        # List of shape tuples, shapes of output_tensors.\n+        self.output_shapes = output_shapes\n+\n+        # Optional keyword arguments to layer's `call`.\n+        self.arguments = arguments\n+\n+        # Add nodes to all layers involved.\n+        for layer in inbound_layers:\n+            if layer is not None:\n+                layer._outbound_nodes.append(self)\n+        outbound_layer._inbound_nodes.append(self)\n+\n+    def get_config(self):\n+        inbound_names = []\n+        for layer in self.inbound_layers:\n+            if layer:\n+                inbound_names.append(layer.name)\n+            else:\n+                inbound_names.append(None)\n+        if self.outbound_layer:\n+            outbound_layer = self.outbound_layer.name\n+        else:\n+            outbound_layer = None\n+        return {'outbound_layer': outbound_layer,\n+                'inbound_layers': inbound_names,\n+                'node_indices': self.node_indices,\n+                'tensor_indices': self.tensor_indices}\n+\n+\n+def _collect_previous_mask(input_tensors):\n+    \"\"\"Retrieves the output mask(s) of the previous node.\n+\n+    # Arguments\n+        input_tensors: A tensor or list of tensors.\n+\n+    # Returns\n+        A mask tensor or list of mask tensors.\n+    \"\"\"\n+    input_tensors = to_list(input_tensors)\n+    masks = []\n+    for x in input_tensors:\n+        if hasattr(x, '_keras_history'):\n+            inbound_layer, node_index, tensor_index = x._keras_history\n+            node = inbound_layer._inbound_nodes[node_index]\n+            mask = node.output_masks[tensor_index]\n+            masks.append(mask)\n+        else:\n+            masks.append(None)\n+    if len(masks) == 1:\n+        return masks[0]\n+    return masks\n+\n+\n+def _to_snake_case(name):\n+    intermediate = re.sub('(.)([A-Z][a-z0-9]+)', r'\\1_\\2', name)\n+    insecure = re.sub('([a-z])([A-Z])', r'\\1_\\2', intermediate).lower()\n+    # If the class is private the name starts with \"_\" which is not secure\n+    # for creating scopes. We prefix the name with \"private\" in this case.\n+    if insecure[0] != '_':\n+        return insecure\n+    return 'private' + insecure\n+\n+\n+def _collect_input_shape(input_tensors):\n+    \"\"\"Collects the output shape(s) of a list of Keras tensors.\n+\n+    # Arguments\n+        input_tensors: list of input tensors (or single input tensor).\n+\n+    # Returns\n+        List of shape tuples (or single tuple), one tuple per input.\n+    \"\"\"\n+    input_tensors = to_list(input_tensors)\n+    shapes = []\n+    for x in input_tensors:\n+        try:\n+            shapes.append(K.int_shape(x))\n+        except TypeError:\n+            shapes.append(None)\n+    if len(shapes) == 1:\n+        return shapes[0]\n+    return shapes\n\n@@ -0,0 +1,183 @@\n+\"\"\"Input layer code (`Input` and `InputLayer`).\n+\"\"\"\n+from __future__ import print_function\n+from __future__ import absolute_import\n+from __future__ import division\n+\n+from .base_layer import Layer\n+from .base_layer import Node\n+from .. import backend as K\n+from ..legacy import interfaces\n+\n+\n+class InputLayer(Layer):\n+    \"\"\"Layer to be used as an entry point into a model.\n+\n+    It can either wrap an existing tensor (pass an `input_tensor` argument)\n+    or create its a placeholder tensor (pass arguments `input_shape`\n+    or `batch_input_shape` as well as `dtype`).\n+\n+    # Arguments\n+        input_shape: Shape tuple, not including the batch axis.\n+        batch_size: Optional input batch size (integer or None).\n+        batch_input_shape: Shape tuple, including the batch axis.\n+        dtype: Datatype of the input.\n+        input_tensor: Optional tensor to use as layer input\n+            instead of creating a placeholder.\n+        sparse: Boolean, whether the placeholder created\n+            is meant to be sparse.\n+        name: Name of the layer (string).\n+    \"\"\"\n+\n+    @interfaces.legacy_input_support\n+    def __init__(self, input_shape=None, batch_size=None,\n+                 batch_input_shape=None,\n+                 dtype=None, input_tensor=None, sparse=False, name=None):\n+        if not name:\n+            prefix = 'input'\n+            name = prefix + '_' + str(K.get_uid(prefix))\n+        super(InputLayer, self).__init__(dtype=dtype, name=name)\n+\n+        self.trainable = False\n+        self.built = True\n+        self.sparse = sparse\n+\n+        if input_shape and batch_input_shape:\n+            raise ValueError('Only provide the input_shape OR '\n+                             'batch_input_shape argument to '\n+                             'InputLayer, not both at the same time.')\n+        if input_tensor is not None and batch_input_shape is None:\n+            # If input_tensor is set, and batch_input_shape is not set:\n+            # Attempt automatic input shape inference.\n+            try:\n+                batch_input_shape = K.int_shape(input_tensor)\n+            except TypeError:\n+                if not input_shape and not batch_input_shape:\n+                    raise ValueError('InputLayer was provided '\n+                                     'an input_tensor argument, '\n+                                     'but its input shape cannot be '\n+                                     'automatically inferred. '\n+                                     'You should pass an input_shape or '\n+                                     'batch_input_shape argument.')\n+        if not batch_input_shape:\n+            if not input_shape:\n+                raise ValueError('An Input layer should be passed either '\n+                                 'a `batch_input_shape` or an `input_shape`.')\n+            else:\n+                batch_input_shape = (batch_size,) + tuple(input_shape)\n+        else:\n+            batch_input_shape = tuple(batch_input_shape)\n+\n+        if not dtype:\n+            if input_tensor is None:\n+                dtype = K.floatx()\n+            else:\n+                dtype = K.dtype(input_tensor)\n+\n+        self.batch_input_shape = batch_input_shape\n+        self.dtype = dtype\n+\n+        if input_tensor is None:\n+            self.is_placeholder = True\n+            input_tensor = K.placeholder(shape=batch_input_shape,\n+                                         dtype=dtype,\n+                                         sparse=self.sparse,\n+                                         name=self.name)\n+        else:\n+            self.is_placeholder = False\n+            input_tensor._keras_shape = batch_input_shape\n+        # Create an input node to add to self.outbound_node\n+        # and set output_tensors' _keras_history.\n+        input_tensor._uses_learning_phase = False\n+        input_tensor._keras_history = (self, 0, 0)\n+        Node(self,\n+             inbound_layers=[],\n+             node_indices=[],\n+             tensor_indices=[],\n+             input_tensors=[input_tensor],\n+             output_tensors=[input_tensor],\n+             input_masks=[None],\n+             output_masks=[None],\n+             input_shapes=[batch_input_shape],\n+             output_shapes=[batch_input_shape])\n+\n+    def get_config(self):\n+        config = {'batch_input_shape': self.batch_input_shape,\n+                  'dtype': self.dtype,\n+                  'sparse': self.sparse,\n+                  'name': self.name}\n+        return config\n+\n+\n+def Input(shape=None, batch_shape=None,\n+          name=None, dtype=None, sparse=False,\n+          tensor=None):\n+    \"\"\"`Input()` is used to instantiate a Keras tensor.\n+\n+    A Keras tensor is a tensor object from the underlying backend\n+    (Theano, TensorFlow or CNTK), which we augment with certain\n+    attributes that allow us to build a Keras model\n+    just by knowing the inputs and outputs of the model.\n+\n+    For instance, if a, b and c are Keras tensors,\n+    it becomes possible to do:\n+    `model = Model(input=[a, b], output=c)`\n+\n+    The added Keras attributes are:\n+        `_keras_shape`: Integer shape tuple propagated\n+            via Keras-side shape inference.\n+        `_keras_history`: Last layer applied to the tensor.\n+            the entire layer graph is retrievable from that layer,\n+            recursively.\n+\n+    # Arguments\n+        shape: A shape tuple (integer), not including the batch size.\n+            For instance, `shape=(32,)` indicates that the expected input\n+            will be batches of 32-dimensional vectors.\n+        batch_shape: A shape tuple (integer), including the batch size.\n+            For instance, `batch_shape=(10, 32)` indicates that\n+            the expected input will be batches of 10 32-dimensional vectors.\n+            `batch_shape=(None, 32)` indicates batches of an arbitrary number\n+            of 32-dimensional vectors.\n+        name: An optional name string for the layer.\n+            Should be unique in a model (do not reuse the same name twice).\n+            It will be autogenerated if it isn't provided.\n+        dtype: The data type expected by the input, as a string\n+            (`float32`, `float64`, `int32`...)\n+        sparse: A boolean specifying whether the placeholder\n+            to be created is sparse.\n+        tensor: Optional existing tensor to wrap into the `Input` layer.\n+            If set, the layer will not create a placeholder tensor.\n+\n+    # Returns\n+        A tensor.\n+\n+    # Example\n+\n+    ```python\n+    # this is a logistic regression in Keras\n+    x = Input(shape=(32,))\n+    y = Dense(16, activation='softmax')(x)\n+    model = Model(x, y)\n+    ```\n+    \"\"\"\n+    if not batch_shape and tensor is None:\n+        assert shape is not None, ('Please provide to Input either a `shape`'\n+                                   ' or a `batch_shape` argument. Note that '\n+                                   '`shape` does not include the batch '\n+                                   'dimension.')\n+    if shape is not None and not batch_shape:\n+        batch_shape = (None,) + tuple(shape)\n+    if not dtype:\n+        dtype = K.floatx()\n+    input_layer = InputLayer(batch_input_shape=batch_shape,\n+                             name=name, dtype=dtype,\n+                             sparse=sparse,\n+                             input_tensor=tensor)\n+    # Return tensor including _keras_shape and _keras_history.\n+    # Note that in this case train_output and test_output are the same pointer.\n+    outputs = input_layer._inbound_nodes[0].output_tensors\n+    if len(outputs) == 1:\n+        return outputs[0]\n+    else:\n+        return outputs\n\n@@ -0,0 +1,1376 @@\n+\"\"\"A `Network` is way to compose layers: the topological form of a `Model`.\n+\"\"\"\n+from __future__ import print_function\n+from __future__ import absolute_import\n+from __future__ import division\n+\n+import numpy as np\n+import json\n+import yaml\n+import warnings\n+import copy\n+import os\n+from six.moves import zip\n+\n+from . import saving\n+from .base_layer import Layer\n+from .base_layer import Node\n+from .input_layer import InputLayer\n+from .. import backend as K\n+from ..utils.io_utils import ask_to_proceed_with_overwrite\n+from ..utils.layer_utils import print_summary as print_layer_summary\n+from ..utils.generic_utils import has_arg\n+from ..utils.generic_utils import to_list\n+from ..legacy import interfaces\n+\n+try:\n+    import h5py\n+except ImportError:\n+    h5py = None\n+\n+\n+class Network(Layer):\n+    \"\"\"A Network is a directed acyclic graph of layers.\n+\n+    It is the topological form of a \"model\". A Model\n+    is simply a Network with added training routines.\n+\n+    # Properties\n+        name\n+        inputs\n+        outputs\n+        input_layers\n+        output_layers\n+        input_spec (list of class instances)\n+            each entry describes one required input:\n+                - ndim\n+                - dtype\n+        trainable (boolean)\n+        input_shape\n+        output_shape\n+        inbound_nodes: list of nodes\n+        outbound_nodes: list of nodes\n+        trainable_weights (list of variables)\n+        non_trainable_weights (list of variables)\n+\n+    # Methods\n+        summary\n+        get_layer\n+        get_weights\n+        set_weights\n+        get_config\n+        compute_output_shape\n+\n+    # Class Methods\n+        from_config\n+\n+    # Raises\n+        TypeError: if input tensors are not Keras tensors\n+            (tensors returned by `Input`).\n+    \"\"\"\n+\n+    @interfaces.legacy_model_constructor_support\n+    def __init__(self, inputs, outputs, name=None):\n+        # Handle `name` argument.\n+        if not name:\n+            prefix = self.__class__.__name__.lower()\n+            name = prefix + '_' + str(K.get_uid(prefix))\n+        self.name = name\n+\n+        self.supports_masking = False\n+        self.trainable = True\n+        self._per_input_losses = {}\n+        self._per_input_updates = {}\n+\n+        # Network-specific properties.\n+        if isinstance(inputs, (list, tuple)):\n+            self.inputs = list(inputs)  # Tensor or list of tensors.\n+        else:\n+            self.inputs = [inputs]\n+        if isinstance(outputs, (list, tuple)):\n+            self.outputs = list(outputs)\n+        else:\n+            self.outputs = [outputs]\n+\n+        # Check for redundancy in inputs.\n+        if len(set(self.inputs)) != len(self.inputs):\n+            raise ValueError('The list of inputs passed to the model '\n+                             'is redundant. '\n+                             'All inputs should only appear once.'\n+                             ' Found: ' + str(self.inputs))\n+\n+        # Check for redundancy in outputs.\n+        if len(set(self.outputs)) != len(self.outputs):\n+            warnings.warn('The list of outputs passed to the model '\n+                          'is redundant. '\n+                          'All outputs should only appear once.'\n+                          ' Found: ' + str(self.outputs))\n+\n+        # List of initial layers (1 to 1 mapping with self.inputs,\n+        # hence the same layer might appear twice)\n+        self.input_layers = []\n+        self.input_layers_node_indices = []\n+        self.input_layers_tensor_indices = []\n+        # list of layers (1 to 1 mapping with self.inputs,\n+        # hence the same layer might appear twice)\n+        self.output_layers = []\n+        self.output_layers_node_indices = []\n+        self.output_layers_tensor_indices = []\n+        # all layers in order of horizontal graph traversal.\n+        # Entries are unique. Includes input and output layers.\n+        self.layers = []\n+\n+        # This is for performance optimization\n+        # when calling the Network on new inputs.\n+        # every time the Network is called on a set on input tensors,\n+        # we compute the output tensors,\n+        # output masks and output shapes in one pass,\n+        # then cache them here. When one of these output is queried later,\n+        # we retrieve it from there instead of recomputing it.\n+        self._output_mask_cache = {}\n+        self._output_tensor_cache = {}\n+        self._output_shape_cache = {}\n+\n+        # User-provided arguments validation.\n+        for x in self.inputs:\n+            # Check that x is a Keras tensor.\n+            if not hasattr(x, '_keras_history'):\n+                cls_name = self.__class__.__name__\n+                raise TypeError('Input tensors to a ' + cls_name + ' ' +\n+                                'must be Keras tensors. Found: ' + str(x) +\n+                                ' (missing Keras metadata).')\n+            # Check that x is an input tensor.\n+            layer, node_index, tensor_index = x._keras_history\n+            if (len(layer._inbound_nodes) > 1 or\n+                    (layer._inbound_nodes and\n+                     layer._inbound_nodes[0].inbound_layers)):\n+                cls_name = self.__class__.__name__\n+                warnings.warn(cls_name + ' inputs must come from '\n+                              'a Keras Input layer, '\n+                              'they cannot be the output of '\n+                              'a previous non-Input layer. '\n+                              'Here, a tensor specified as '\n+                              'input to \"' + self.name +\n+                              '\" was not an Input tensor, '\n+                              'it was generated by layer ' +\n+                              layer.name + '.\\n'\n+                              'Note that input tensors are '\n+                              'instantiated via `tensor = Input(shape)`.\\n'\n+                              'The tensor that caused the issue was: ' +\n+                              str(x.name))\n+        for x in self.outputs:\n+            if not hasattr(x, '_keras_history'):\n+                cls_name = self.__class__.__name__\n+                raise TypeError('Output tensors to a ' + cls_name + ' must be '\n+                                'Keras tensors. Found: ' + str(x))\n+        # Build self.output_layers:\n+        for x in self.outputs:\n+            layer, node_index, tensor_index = x._keras_history\n+            self.output_layers.append(layer)\n+            self.output_layers_node_indices.append(node_index)\n+            self.output_layers_tensor_indices.append(tensor_index)\n+\n+        # Fill in the output mask cache.\n+        masks = []\n+        for x in self.inputs:\n+            layer, node_index, tensor_index = x._keras_history\n+            node = layer._inbound_nodes[node_index]\n+            mask = node.output_masks[tensor_index]\n+            masks.append(mask)\n+        mask_cache_key = ','.join([str(id(x)) for x in self.inputs])\n+        mask_cache_key += '_' + ','.join([str(id(x)) for x in masks])\n+        masks = []\n+        for x in self.outputs:\n+            layer, node_index, tensor_index = x._keras_history\n+            node = layer._inbound_nodes[node_index]\n+            mask = node.output_masks[tensor_index]\n+            masks.append(mask)\n+        if len(masks) == 1:\n+            mask = masks[0]\n+        else:\n+            mask = masks\n+        self._output_mask_cache[mask_cache_key] = mask\n+\n+        # Build self.input_layers:\n+        for x in self.inputs:\n+            layer, node_index, tensor_index = x._keras_history\n+            # It's supposed to be an input layer, so only one node\n+            # and one tensor output.\n+            assert node_index == 0\n+            assert tensor_index == 0\n+            self.input_layers.append(layer)\n+            self.input_layers_node_indices.append(node_index)\n+            self.input_layers_tensor_indices.append(tensor_index)\n+\n+        # Build self.input_names and self.output_names.\n+        self.input_names = []\n+        self.output_names = []\n+        self._feed_input_names = []\n+        self._feed_inputs = []\n+        self._feed_input_shapes = []\n+        for i, layer in enumerate(self.input_layers):\n+            # Check that layer is an InputLayer.\n+            if not isinstance(layer, InputLayer):\n+                raise TypeError(\n+                    'Input layers to a `Model` must be `InputLayer` objects. '\n+                    'Received inputs: {}. '\n+                    'Input {} (0-based) originates '\n+                    'from layer type `{}`.'.format(inputs,\n+                                                   i,\n+                                                   layer.__class__.__name__))\n+            self.input_names.append(layer.name)\n+            if layer.is_placeholder:\n+                self._feed_input_names.append(layer.name)\n+                self._feed_inputs.append(layer.input)\n+                self._feed_input_shapes.append(self.inputs[i]._keras_shape)\n+        for layer in self.output_layers:\n+            self.output_names.append(layer.name)\n+\n+        self._internal_input_shapes = [x._keras_shape for x in self.inputs]\n+        self._internal_output_shapes = [x._keras_shape for x in self.outputs]\n+\n+        # Network_nodes: set of nodes included in the graph\n+        # (not all nodes included in the layers\n+        # are relevant to the current graph).\n+        network_nodes = set()  # ids of all nodes relevant to the Network\n+        nodes_depths = {}  # dict {node: depth value}\n+        layers_depths = {}  # dict {layer: depth value}\n+        layer_indices = {}  # dict {layer: index in traversal}\n+        nodes_in_decreasing_depth = []\n+\n+        def build_map_of_graph(tensor, finished_nodes, nodes_in_progress,\n+                               layer=None, node_index=None, tensor_index=None):\n+            \"\"\"Builds a map of the graph of layers.\n+\n+            This recursively updates the map `layer_indices`,\n+            the list `nodes_in_decreasing_depth` and the set `network_nodes`.\n+\n+            # Arguments\n+                tensor: Some tensor in a graph.\n+                finished_nodes: Set of nodes whose subgraphs\n+                    have been traversed completely.\n+                    Useful to prevent duplicated work.\n+                nodes_in_progress: Set of nodes that are currently active\n+                    on the recursion stack. Useful to detect cycles.\n+                layer: Layer from which `tensor` comes from. If not provided,\n+                    will be obtained from `tensor._keras_history`.\n+                node_index: Node index from which `tensor` comes from.\n+                tensor_index: Tensor_index from which `tensor` comes from.\n+\n+            # Raises\n+                RuntimeError: if a cycle is detected.\n+            \"\"\"\n+            if not layer or node_index is None or tensor_index is None:\n+                layer, node_index, tensor_index = tensor._keras_history\n+            node = layer._inbound_nodes[node_index]\n+\n+            # Prevent cycles.\n+            if node in nodes_in_progress:\n+                raise RuntimeError(\n+                    'The tensor ' + str(tensor) + ' at layer \"' +\n+                    layer.name + '\" is part of a cycle.')\n+\n+            # Don't repeat work for shared subgraphs\n+            if node in finished_nodes:\n+                return\n+\n+            # Update network_nodes.\n+            network_nodes.add(self._node_key(layer, node_index))\n+\n+            # Store the traversal order for layer sorting.\n+            if layer not in layer_indices:\n+                layer_indices[layer] = len(layer_indices)\n+\n+            nodes_in_progress.add(node)\n+\n+            # Propagate to all previous tensors connected to this node.\n+            for i in range(len(node.inbound_layers)):\n+                x = node.input_tensors[i]\n+                layer = node.inbound_layers[i]\n+                node_index = node.node_indices[i]\n+                tensor_index = node.tensor_indices[i]\n+                build_map_of_graph(x, finished_nodes, nodes_in_progress,\n+                                   layer, node_index, tensor_index)\n+\n+            finished_nodes.add(node)\n+            nodes_in_progress.remove(node)\n+\n+            nodes_in_decreasing_depth.append(node)\n+\n+        finished_nodes = set()\n+        nodes_in_progress = set()\n+        for x in self.outputs:\n+            build_map_of_graph(x, finished_nodes, nodes_in_progress)\n+\n+        for node in reversed(nodes_in_decreasing_depth):\n+            # If the depth is not set,\n+            # the node has no outbound nodes (depth 0).\n+            depth = nodes_depths.setdefault(node, 0)\n+\n+            # Update the depth of the corresponding layer\n+            previous_depth = layers_depths.get(node.outbound_layer, 0)\n+            # If we've seen this layer before at a higher depth,\n+            # we should use that depth instead of the node depth.\n+            # This is necessary for shared layers that have inputs at different\n+            # depth levels in the graph.\n+            depth = max(depth, previous_depth)\n+            layers_depths[node.outbound_layer] = depth\n+            nodes_depths[node] = depth\n+\n+            # Update the depth of inbound nodes.\n+            for i in range(len(node.inbound_layers)):\n+                inbound_layer = node.inbound_layers[i]\n+                node_index = node.node_indices[i]\n+                inbound_node = inbound_layer._inbound_nodes[node_index]\n+                previous_depth = nodes_depths.get(inbound_node, 0)\n+                nodes_depths[inbound_node] = max(depth + 1, previous_depth)\n+\n+        # Build a dict {depth: list of nodes with this depth}\n+        nodes_by_depth = {}\n+        for node, depth in nodes_depths.items():\n+            if depth not in nodes_by_depth:\n+                nodes_by_depth[depth] = []\n+            nodes_by_depth[depth].append(node)\n+\n+        # Build a dict {depth: list of layers with this depth}\n+        layers_by_depth = {}\n+        for layer, depth in layers_depths.items():\n+            if depth not in layers_by_depth:\n+                layers_by_depth[depth] = []\n+            layers_by_depth[depth].append(layer)\n+\n+        # Get sorted list of layer depths.\n+        depth_keys = list(layers_by_depth.keys())\n+        depth_keys.sort(reverse=True)\n+\n+        # Set self.layers and self.layers_by_depth.\n+        layers = []\n+        for depth in depth_keys:\n+            layers_for_depth = layers_by_depth[depth]\n+            # Network.layers needs to have a deterministic order:\n+            # here we order them by traversal order.\n+            layers_for_depth.sort(key=lambda x: layer_indices[x])\n+            for layer in layers_for_depth:\n+                layers.append(layer)\n+        self.layers = layers\n+        self.layers_by_depth = layers_by_depth\n+\n+        # Get sorted list of node depths.\n+        depth_keys = list(nodes_by_depth.keys())\n+        depth_keys.sort(reverse=True)\n+\n+        # Check that all tensors required are computable.\n+        # computable_tensors: all tensors in the graph\n+        # that can be computed from the inputs provided.\n+        computable_tensors = []\n+        for x in self.inputs:\n+            computable_tensors.append(x)\n+\n+        layers_with_complete_input = []  # To provide a better error msg.\n+        for depth in depth_keys:\n+            for node in nodes_by_depth[depth]:\n+                layer = node.outbound_layer\n+                if layer:\n+                    for x in node.input_tensors:\n+                        if x not in computable_tensors:\n+                            raise RuntimeError(\n+                                'Graph disconnected: '\n+                                'cannot obtain value for tensor ' +\n+                                str(x) + ' at layer \"' + layer.name + '\". '\n+                                'The following previous layers '\n+                                'were accessed without issue: ' +\n+                                str(layers_with_complete_input))\n+                    for x in node.output_tensors:\n+                        computable_tensors.append(x)\n+                    layers_with_complete_input.append(layer.name)\n+\n+        # Set self._network_nodes and self._nodes_by_depth.\n+        self._network_nodes = network_nodes\n+        self._nodes_by_depth = nodes_by_depth\n+\n+        # Ensure name unicity, which will be crucial for serialization\n+        # (since serialized nodes refer to layers by their name).\n+        all_names = [layer.name for layer in self.layers]\n+        for name in all_names:\n+            if all_names.count(name) != 1:\n+                raise RuntimeError('The name \"' + name + '\" is used ' +\n+                                   str(all_names.count(name)) +\n+                                   ' times in the model. '\n+                                   'All layer names should be unique. '\n+                                   'Layer names: ', all_names)\n+\n+        # Layer parameters.\n+        # The new network starts with a single inbound node\n+        # for its inputs, and no outbound nodes.\n+        self._outbound_nodes = []  # Appended to by calls to `__call__`.\n+        self._inbound_nodes = []  # Appended to by calls to `__call__`.\n+        # Create the node linking internal inputs to internal outputs.\n+        Node(outbound_layer=self,\n+             inbound_layers=[],\n+             node_indices=[],\n+             tensor_indices=[],\n+             input_tensors=self.inputs,\n+             output_tensors=self.outputs,\n+             # No network-level masking for now.\n+             input_masks=[None for _ in self.inputs],\n+             output_masks=[None for _ in self.outputs],\n+             input_shapes=[x._keras_shape for x in self.inputs],\n+             output_shapes=[x._keras_shape for x in self.outputs])\n+        self.built = True\n+\n+        # The following are implemented as property functions:\n+        # self.trainable_weights\n+        # self.non_trainable_weights\n+        # self.input_spec\n+\n+    def get_layer(self, name=None, index=None):\n+        \"\"\"Retrieves a layer based on either its name (unique) or index.\n+\n+        If `name` and `index` are both provided, `index` will take precedence.\n+\n+        Indices are based on order of horizontal graph traversal (bottom-up).\n+\n+        # Arguments\n+            name: String, name of layer.\n+            index: Integer, index of layer.\n+\n+        # Returns\n+            A layer instance.\n+\n+        # Raises\n+            ValueError: In case of invalid layer name or index.\n+        \"\"\"\n+        # It would be unreliable to build a dictionary\n+        # based on layer names, because names can potentially\n+        # be changed at any point by the user\n+        # without the network being notified of it.\n+        if index is not None:\n+            if len(self.layers) <= index:\n+                raise ValueError('Was asked to retrieve layer at index ' +\n+                                 str(index) + ' but model only has ' +\n+                                 str(len(self.layers)) + ' layers.')\n+            else:\n+                return self.layers[index]\n+        else:\n+            if not name:\n+                raise ValueError('Provide either a layer name or layer index.')\n+\n+        for layer in self.layers:\n+            if layer.name == name:\n+                return layer\n+\n+        raise ValueError('No such layer: ' + name)\n+\n+    @property\n+    def updates(self):\n+        \"\"\"Retrieves the model's updates.\n+\n+        Will only include updates that are either\n+        unconditional, or conditional on inputs to this model\n+        (e.g. will not include updates that depend on tensors\n+        that aren't inputs to this model).\n+\n+        # Returns\n+            A list of update ops.\n+        \"\"\"\n+        if not self.trainable and not self.stateful:\n+            return []\n+        updates = []\n+        for layer in self.layers:\n+            if hasattr(layer, 'updates'):\n+                # Collect updates that are dependent on inputs\n+                # that are part of the model.\n+                for node_index, node in enumerate(layer._inbound_nodes):\n+                    node_key = self._node_key(layer, node_index)\n+                    if node_key in self._network_nodes:\n+                        # The model owns this layer node.\n+                        inputs = node.input_tensors\n+                        updates += layer.get_updates_for(inputs)\n+                # Collect unconditional updates.\n+                updates += layer.get_updates_for(None)\n+        return updates\n+\n+    @property\n+    def losses(self):\n+        \"\"\"Retrieves the model's losses.\n+\n+        Will only include losses that are either\n+        unconditional, or conditional on inputs to this model\n+        (e.g. will not include losses that depend on tensors\n+        that aren't inputs to this model).\n+\n+        # Returns\n+            A list of loss tensors.\n+        \"\"\"\n+        losses = []\n+        # Retrieve losses for all internal layers.\n+        for layer in self.layers:\n+            if hasattr(layer, 'losses'):\n+                # Collect losses that are dependent on inputs\n+                # that are part of the model.\n+                for node_index, node in enumerate(layer._inbound_nodes):\n+                    node_key = self._node_key(layer, node_index)\n+                    if node_key in self._network_nodes:\n+                        # The model owns this layer node.\n+                        inputs = node.input_tensors\n+                        losses += layer.get_losses_for(inputs)\n+                # Collect unconditional losses.\n+                losses += layer.get_losses_for(None)\n+        # Add any potential unconditional model-level loss.\n+        losses += self.get_losses_for(None)\n+\n+        unique_tensors = list(\n+            set(x for x in losses if not isinstance(x, (float, int))))\n+        non_tensors = [x for x in losses if isinstance(x, (float, int))]\n+        return unique_tensors + non_tensors\n+\n+    @property\n+    def uses_learning_phase(self):\n+        return any([x._uses_learning_phase for x in self.outputs])\n+\n+    @property\n+    def stateful(self):\n+        return any([(hasattr(layer, 'stateful') and\n+                    layer.stateful) for layer in self.layers])\n+\n+    def reset_states(self):\n+        for layer in self.layers:\n+            if hasattr(layer, 'reset_states') and getattr(layer, 'stateful', False):\n+                layer.reset_states()\n+\n+    @property\n+    def state_updates(self):\n+        \"\"\"Returns the `updates` from all layers that are stateful.\n+\n+        This is useful for separating training updates and\n+        state updates, e.g. when we need to update a layer's internal state\n+        during prediction.\n+\n+        # Returns\n+            A list of update ops.\n+        \"\"\"\n+        state_updates = []\n+        for layer in self.layers:\n+            if layer.stateful:\n+                state_updates += layer.updates\n+        return state_updates\n+\n+    @property\n+    def trainable_weights(self):\n+        if not self.trainable:\n+            return []\n+        weights = []\n+        for layer in self.layers:\n+            weights += layer.trainable_weights\n+        return weights\n+\n+    @property\n+    def non_trainable_weights(self):\n+        weights = []\n+        for layer in self.layers:\n+            weights += layer.non_trainable_weights\n+        if not self.trainable:\n+            trainable_weights = []\n+            for layer in self.layers:\n+                trainable_weights += layer.trainable_weights\n+            return trainable_weights + weights\n+        return weights\n+\n+    def get_weights(self):\n+        \"\"\"Retrieves the weights of the model.\n+\n+        # Returns\n+            A flat list of Numpy arrays.\n+        \"\"\"\n+        weights = []\n+        for layer in self.layers:\n+            weights += layer.weights\n+        return K.batch_get_value(weights)\n+\n+    def set_weights(self, weights):\n+        \"\"\"Sets the weights of the model.\n+\n+        # Arguments\n+            weights: A list of Numpy arrays with shapes and types matching\n+                the output of `model.get_weights()`.\n+        \"\"\"\n+        tuples = []\n+        for layer in self.layers:\n+            num_param = len(layer.weights)\n+            layer_weights = weights[:num_param]\n+            for sw, w in zip(layer.weights, layer_weights):\n+                tuples.append((sw, w))\n+            weights = weights[num_param:]\n+        K.batch_set_value(tuples)\n+\n+    @property\n+    def input_spec(self):\n+        \"\"\"Gets the model's input specs.\n+\n+        # Returns\n+            A list of `InputSpec` instances (one per input to the model)\n+                or a single instance if the model has only one input.\n+        \"\"\"\n+        specs = []\n+        for layer in getattr(self, 'input_layers', []):\n+            if layer.input_spec is None:\n+                specs.append(None)\n+            else:\n+                if not isinstance(layer.input_spec, list):\n+                    raise TypeError('Layer ' + layer.name +\n+                                    ' has an input_spec attribute that '\n+                                    'is not a list. We expect a list. '\n+                                    'Found input_spec = ' +\n+                                    str(layer.input_spec))\n+                specs += layer.input_spec\n+        if len(specs) == 1:\n+            return specs[0]\n+        return specs\n+\n+    def call(self, inputs, mask=None):\n+        \"\"\"Calls the model on new inputs.\n+\n+        In this case `call` just reapplies\n+        all ops in the graph to the new inputs\n+        (e.g. build a new computational graph from the provided inputs).\n+\n+        A model is callable on non-Keras tensors.\n+\n+        # Arguments\n+            inputs: A tensor or list of tensors.\n+            mask: A mask or list of masks. A mask can be\n+                either a tensor or None (no mask).\n+\n+        # Returns\n+            A tensor if there is a single output, or\n+            a list of tensors if there are more than one outputs.\n+        \"\"\"\n+        inputs = to_list(inputs)\n+        if mask is None:\n+            masks = [None for _ in range(len(inputs))]\n+        else:\n+            masks = to_list(mask)\n+        cache_key = ','.join([str(id(x)) for x in inputs])\n+        cache_key += '_' + ','.join([str(id(x)) for x in masks])\n+        if cache_key in self._output_tensor_cache:\n+            return self._output_tensor_cache[cache_key]\n+        else:\n+            output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n+            return output_tensors\n+\n+    def compute_mask(self, inputs, mask):\n+        inputs = to_list(inputs)\n+        if mask is None:\n+            masks = [None for _ in range(len(inputs))]\n+        else:\n+            masks = to_list(mask)\n+        cache_key = ','.join([str(id(x)) for x in inputs])\n+        cache_key += '_' + ','.join([str(id(x)) for x in masks])\n+        if cache_key in self._output_mask_cache:\n+            return self._output_mask_cache[cache_key]\n+        else:\n+            _, output_masks, _ = self.run_internal_graph(inputs, masks)\n+            return output_masks\n+\n+    def compute_output_shape(self, input_shape):\n+        input_shapes = to_list(input_shape)\n+        if len(input_shapes) != len(self.input_layers):\n+            raise ValueError('Invalid input_shape argument ' +\n+                             str(input_shape) + ': model has ' +\n+                             str(len(self.input_layers)) + ' tensor inputs.')\n+\n+        cache_key = ','.join([str(x) for x in input_shapes])\n+        if cache_key in self._output_shape_cache:\n+            output_shapes = self._output_shape_cache[cache_key]\n+            if isinstance(output_shapes, list) and len(output_shapes) == 1:\n+                return output_shapes[0]\n+            return output_shapes\n+        else:\n+            # Bad luck, we have to run the graph manually.\n+            layers_to_output_shapes = {}\n+            for i in range(len(input_shapes)):\n+                layer = self.input_layers[i]\n+                input_shape = input_shapes[i]\n+                # It's an input layer: compute_output_shape is identity,\n+                # and there is only one node and one tensor output.\n+                shape_key = layer.name + '_0_0'\n+                layers_to_output_shapes[shape_key] = input_shape\n+\n+            depth_keys = list(self._nodes_by_depth.keys())\n+            depth_keys.sort(reverse=True)\n+            # Iterate over nodes, by depth level.\n+            if len(depth_keys) > 1:\n+                for depth in depth_keys:\n+                    nodes = self._nodes_by_depth[depth]\n+                    for node in nodes:\n+                        # This is always a single layer, never a list.\n+                        layer = node.outbound_layer\n+                        if layer in self.input_layers:\n+                            # We've already covered the input layers\n+                            # a few lines above.\n+                            continue\n+                        # Potentially redundant list,\n+                        # same size of node.input_tensors.\n+                        input_shapes = []\n+                        for j in range(len(node.inbound_layers)):\n+                            inbound_layer = node.inbound_layers[j]\n+                            node_index = node.node_indices[j]\n+                            tensor_index = node.tensor_indices[j]\n+                            shape_key = inbound_layer.name + '_%s_%s' % (node_index, tensor_index)\n+                            input_shape = layers_to_output_shapes[shape_key]\n+                            input_shapes.append(input_shape)\n+\n+                        if len(input_shapes) == 1:\n+                            output_shape = layer.compute_output_shape(input_shapes[0])\n+                        else:\n+                            output_shape = layer.compute_output_shape(input_shapes)\n+\n+                        output_shapes = to_list(output_shape)\n+                        node_index = layer._inbound_nodes.index(node)\n+                        for j in range(len(output_shapes)):\n+                            shape_key = layer.name + '_%s_%s' % (node_index, j)\n+                            layers_to_output_shapes[shape_key] = output_shapes[j]\n+\n+            # Read final output shapes from layers_to_output_shapes.\n+            output_shapes = []\n+            output_shape_keys = []\n+            for i in range(len(self.output_layers)):\n+                layer = self.output_layers[i]\n+                node_index = self.output_layers_node_indices[i]\n+                tensor_index = self.output_layers_tensor_indices[i]\n+                shape_key = layer.name + '_%s_%s' % (node_index, tensor_index)\n+                output_shape_keys.append(shape_key)\n+\n+            for i, key in enumerate(output_shape_keys):\n+                assert key in layers_to_output_shapes\n+                output_shapes.append(layers_to_output_shapes[key])\n+            # Store in cache.\n+            self._output_shape_cache[cache_key] = output_shapes\n+            if isinstance(output_shapes, list) and len(output_shapes) == 1:\n+                return output_shapes[0]\n+            return output_shapes\n+\n+    def run_internal_graph(self, inputs, masks=None):\n+        \"\"\"Computes output tensors for new inputs.\n+\n+        # Note:\n+            - Expects `inputs` to be a list (potentially with 1 element).\n+            - Can be run on non-Keras tensors.\n+\n+        # Arguments\n+            inputs: List of tensors\n+            masks: List of masks (tensors or None).\n+\n+        # Returns\n+            Three lists: output_tensors, output_masks, output_shapes\n+        \"\"\"\n+        if masks is None:\n+            masks = [None for _ in range(len(inputs))]\n+\n+        # Dictionary mapping reference tensors to tuples\n+        # (computed tensor, compute mask)\n+        # we assume a 1:1 mapping from tensor to mask\n+        # TODO: raise exception when a `.compute_mask()` call\n+        # does not return a list the same size as `call`\n+        tensor_map = {}\n+        for x, y, mask in zip(self.inputs, inputs, masks):\n+            tensor_map[str(id(x))] = (y, mask)\n+\n+        depth_keys = list(self._nodes_by_depth.keys())\n+        depth_keys.sort(reverse=True)\n+        for depth in depth_keys:\n+            nodes = self._nodes_by_depth[depth]\n+            for node in nodes:\n+                # This is always a single layer, never a list.\n+                layer = node.outbound_layer\n+                reference_input_tensors = node.input_tensors\n+                reference_output_tensors = node.output_tensors\n+\n+                # If all previous input tensors are available in tensor_map,\n+                # then call node.inbound_layer on them.\n+                computed_data = []  # List of tuples (input, mask).\n+                for x in reference_input_tensors:\n+                    if str(id(x)) in tensor_map:\n+                        computed_data.append(tensor_map[str(id(x))])\n+\n+                if len(computed_data) == len(reference_input_tensors):\n+                    # call layer\n+                    with K.name_scope(layer.name):\n+                        if node.arguments:\n+                            kwargs = node.arguments\n+                        else:\n+                            kwargs = {}\n+                        if len(computed_data) == 1:\n+                            computed_tensor, computed_mask = computed_data[0]\n+                            if has_arg(layer.call, 'mask'):\n+                                if 'mask' not in kwargs:\n+                                    kwargs['mask'] = computed_mask\n+                            output_tensors = to_list(layer.call(computed_tensor, **kwargs))\n+                            output_masks = layer.compute_mask(computed_tensor,\n+                                                              computed_mask)\n+                            if output_masks is None:\n+                                output_masks = [None for _ in output_tensors]\n+                            else:\n+                                output_masks = to_list(output_masks)\n+                            computed_tensors = [computed_tensor]\n+                            computed_masks = [computed_mask]\n+                        else:\n+                            computed_tensors = [x[0] for x in computed_data]\n+                            computed_masks = [x[1] for x in computed_data]\n+                            if has_arg(layer.call, 'mask'):\n+                                if 'mask' not in kwargs:\n+                                    kwargs['mask'] = computed_masks\n+                            output_tensors = to_list(\n+                                layer.call(computed_tensors, **kwargs))\n+                            output_masks = layer.compute_mask(computed_tensors,\n+                                                              computed_masks)\n+                            if output_masks is None:\n+                                output_masks = [None for _ in output_tensors]\n+                            else:\n+                                output_masks = to_list(output_masks)\n+                        # Apply activity regularizer if any:\n+                        if hasattr(layer, 'activity_regularizer') and layer.activity_regularizer is not None:\n+                            regularization_losses = [\n+                                layer.activity_regularizer(x)\n+                                for x in output_tensors]\n+                            layer.add_loss(regularization_losses,\n+                                           inputs=computed_tensors)\n+\n+                        if len(output_masks) != len(output_tensors):\n+                            raise Exception(\n+                                'Layers should have equal number of output tensors '\n+                                'and output masks. Layer ' + str(layer.name) + ' has'\n+                                ' ' + str(len(output_tensors)) + ' output tensors and'\n+                                ' ' + str(len(output_masks)) + ' output masks.')\n+                    # Update model updates and losses:\n+                    # Keep track of updates that depend on the inputs\n+                    # (e.g. BN updates).\n+                    self.add_update(layer.get_updates_for(computed_tensors), inputs)\n+                    # Keep track of unconditional updates (e.g. a counter).\n+                    self.add_update(layer.get_updates_for(None), None)\n+                    # Keep track of losses that depend on the inputs\n+                    # (e.g. activity regularizers).\n+                    self.add_loss(layer.get_losses_for(computed_tensors), inputs)\n+                    # Keep track of unconditional losses\n+                    # (e.g. weight regularizers).\n+                    self.add_loss(layer.get_losses_for(None), None)\n+\n+                    # Update _keras_shape.\n+                    if all([hasattr(x, '_keras_shape') for x in computed_tensors]):\n+                        if len(computed_tensors) == 1:\n+                            shapes = to_list(layer.compute_output_shape(computed_tensors[0]._keras_shape))\n+                            uses_learning_phase = computed_tensors[0]._uses_learning_phase\n+                        else:\n+                            shapes = to_list(layer.compute_output_shape([x._keras_shape for x in computed_tensors]))\n+                            uses_learning_phase = any([x._uses_learning_phase for x in computed_tensors])\n+                        for x, s in zip(output_tensors, shapes):\n+                            x._keras_shape = s\n+                            x._uses_learning_phase = getattr(x, '_uses_learning_phase', False) or uses_learning_phase\n+\n+                    # Update tensor_map.\n+                    for x, y, mask in zip(reference_output_tensors, output_tensors, output_masks):\n+                        tensor_map[str(id(x))] = (y, mask)\n+\n+        output_tensors = []\n+        output_masks = []\n+        output_shapes = []\n+        for x in self.outputs:\n+            assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\n+            tensor, mask = tensor_map[str(id(x))]\n+            if hasattr(tensor, '_keras_shape') and output_shapes is not None:\n+                shape = tensor._keras_shape\n+                output_shapes.append(shape)\n+            else:\n+                output_shapes = None\n+            output_tensors.append(tensor)\n+            output_masks.append(mask)\n+\n+        # Update cache;\n+        # keys are based on ids on input tensors and inputs masks.\n+        cache_key = ','.join([str(id(x)) for x in inputs])\n+        cache_key += '_' + ','.join([str(id(x)) for x in masks])\n+\n+        if len(output_tensors) == 1:\n+            output_tensors = output_tensors[0]\n+            self._output_tensor_cache[cache_key] = output_tensors\n+        else:\n+            self._output_tensor_cache[cache_key] = output_tensors\n+\n+        if len(output_masks) == 1:\n+            output_masks = output_masks[0]\n+            self._output_mask_cache[cache_key] = output_masks\n+        else:\n+            self._output_mask_cache[cache_key] = output_masks\n+\n+        if output_shapes is not None:\n+            input_shapes = [x._keras_shape for x in inputs]\n+            cache_key = ','.join([str(x) for x in input_shapes])\n+            if len(output_shapes) == 1:\n+                output_shapes = output_shapes[0]\n+                self._output_shape_cache[cache_key] = output_shapes\n+            else:\n+                self._output_shape_cache[cache_key] = output_shapes\n+        return output_tensors, output_masks, output_shapes\n+\n+    def get_config(self):\n+        config = {\n+            'name': self.name,\n+        }\n+\n+        # Build a map from a layer unique name (self._node_key)\n+        # to the index of the nodes that are saved in the config.\n+        # Only nodes in network_nodes are saved.\n+        node_conversion_map = {}\n+        for layer in self.layers:\n+            if issubclass(layer.__class__, Network):\n+                # Networks start with a pre-existing node\n+                # linking their input to output.\n+                kept_nodes = 1\n+            else:\n+                kept_nodes = 0\n+            for original_node_index, node in enumerate(layer._inbound_nodes):\n+                node_key = self._node_key(layer, original_node_index)\n+                if node_key in self._network_nodes:\n+                    # i.e. we mark it to be saved\n+                    node_conversion_map[node_key] = kept_nodes\n+                    kept_nodes += 1\n+\n+        # serialize and save the layers in layer_configs\n+        layer_configs = []\n+        for layer in self.layers:  # From the earliest layers on.\n+            layer_class_name = layer.__class__.__name__\n+            layer_config = layer.get_config()\n+            filtered_inbound_nodes = []\n+            for original_node_index, node in enumerate(layer._inbound_nodes):\n+                node_key = self._node_key(layer, original_node_index)\n+                if node_key in self._network_nodes:\n+                    # The node is relevant to the model:\n+                    # add to filtered_inbound_nodes.\n+                    if node.arguments:\n+                        try:\n+                            json.dumps(node.arguments)\n+                            kwargs = node.arguments\n+                        except TypeError:\n+                            warnings.warn(\n+                                'Layer ' + layer.name +\n+                                ' was passed non-serializable '\n+                                'keyword arguments: ' +\n+                                str(node.arguments) +\n+                                '. They will not be included '\n+                                'in the serialized model '\n+                                '(and thus will be missing '\n+                                'at deserialization time).')\n+                            kwargs = {}\n+                    else:\n+                        kwargs = {}\n+                    if node.inbound_layers:\n+                        node_data = []\n+                        for i in range(len(node.inbound_layers)):\n+                            inbound_layer = node.inbound_layers[i]\n+                            node_index = node.node_indices[i]\n+                            tensor_index = node.tensor_indices[i]\n+\n+                            new_node_index = node_conversion_map.get(\n+                                self._node_key(inbound_layer, node_index), 0)\n+                            node_data.append([inbound_layer.name,\n+                                              new_node_index,\n+                                              tensor_index,\n+                                              kwargs])\n+                        filtered_inbound_nodes.append(node_data)\n+            layer_configs.append({\n+                'name': layer.name,\n+                'class_name': layer_class_name,\n+                'config': layer_config,\n+                'inbound_nodes': filtered_inbound_nodes,\n+            })\n+        config['layers'] = layer_configs\n+\n+        # Gather info about inputs and outputs.\n+        model_inputs = []\n+        for i in range(len(self.input_layers)):\n+            layer = self.input_layers[i]\n+            node_index = self.input_layers_node_indices[i]\n+\n+            node_key = self._node_key(layer, node_index)\n+            if node_key not in self._network_nodes:\n+                continue\n+            new_node_index = node_conversion_map[node_key]\n+            tensor_index = self.input_layers_tensor_indices[i]\n+            model_inputs.append([layer.name, new_node_index, tensor_index])\n+        config['input_layers'] = model_inputs\n+        model_outputs = []\n+        for i in range(len(self.output_layers)):\n+            layer = self.output_layers[i]\n+            node_index = self.output_layers_node_indices[i]\n+\n+            node_key = self._node_key(layer, node_index)\n+            if node_key not in self._network_nodes:\n+                continue\n+            new_node_index = node_conversion_map[node_key]\n+            tensor_index = self.output_layers_tensor_indices[i]\n+            model_outputs.append([layer.name, new_node_index, tensor_index])\n+        config['output_layers'] = model_outputs\n+        return copy.deepcopy(config)\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        \"\"\"Instantiates a Model from its config (output of `get_config()`).\n+\n+        # Arguments\n+            config: Model config dictionary.\n+            custom_objects: Optional dictionary mapping names\n+                (strings) to custom classes or functions to be\n+                considered during deserialization.\n+\n+        # Returns\n+            A model instance.\n+\n+        # Raises\n+            ValueError: In case of improperly formatted config dict.\n+        \"\"\"\n+        # Layer instances created during\n+        # the graph reconstruction process\n+        created_layers = {}\n+\n+        # Dictionary mapping layer instances to\n+        # node data that specifies a layer call.\n+        # It acts as a queue that maintains any unprocessed\n+        # layer call until it becomes possible to process it\n+        # (i.e. until the input tensors to the call all exist).\n+        unprocessed_nodes = {}\n+\n+        def add_unprocessed_node(layer, node_data):\n+            if layer not in unprocessed_nodes:\n+                unprocessed_nodes[layer] = [node_data]\n+            else:\n+                unprocessed_nodes[layer].append(node_data)\n+\n+        def process_node(layer, node_data):\n+            input_tensors = []\n+            for input_data in node_data:\n+                inbound_layer_name = input_data[0]\n+                inbound_node_index = input_data[1]\n+                inbound_tensor_index = input_data[2]\n+                if len(input_data) == 3:\n+                    kwargs = {}\n+                elif len(input_data) == 4:\n+                    kwargs = input_data[3]\n+                else:\n+                    raise ValueError('Improperly formatted model config.')\n+                if inbound_layer_name not in created_layers:\n+                    add_unprocessed_node(layer, node_data)\n+                    return\n+                inbound_layer = created_layers[inbound_layer_name]\n+                if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n+                    add_unprocessed_node(layer, node_data)\n+                    return\n+                inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n+                input_tensors.append(\n+                    inbound_node.output_tensors[inbound_tensor_index])\n+            # Call layer on its inputs, thus creating the node\n+            # and building the layer if needed.\n+            if input_tensors:\n+                if len(input_tensors) == 1:\n+                    layer(input_tensors[0], **kwargs)\n+                else:\n+                    layer(input_tensors, **kwargs)\n+\n+        def process_layer(layer_data):\n+            \"\"\"Deserializes a layer, then call it on appropriate inputs.\n+\n+            # Arguments\n+                layer_data: layer config dict.\n+\n+            # Raises\n+                ValueError: In case of improperly formatted `layer_data` dict.\n+            \"\"\"\n+            layer_name = layer_data['name']\n+\n+            # Instantiate layer.\n+            from ..layers import deserialize as deserialize_layer\n+\n+            layer = deserialize_layer(layer_data,\n+                                      custom_objects=custom_objects)\n+            created_layers[layer_name] = layer\n+\n+            # Gather layer inputs.\n+            inbound_nodes_data = layer_data['inbound_nodes']\n+            for node_data in inbound_nodes_data:\n+                # We don't process nodes (i.e. make layer calls)\n+                # on the fly because the inbound node may not yet exist,\n+                # in case of layer shared at different topological depths\n+                # (e.g. a model such as A(B(A(B(x)))))\n+                add_unprocessed_node(layer, node_data)\n+\n+        # First, we create all layers and enqueue nodes to be processed\n+        for layer_data in config['layers']:\n+            process_layer(layer_data)\n+        # Then we process nodes in order of layer depth.\n+        # Nodes that cannot yet be processed (if the inbound node\n+        # does not yet exist) are re-enqueued, and the process\n+        # is repeated until all nodes are processed.\n+        while unprocessed_nodes:\n+            for layer_data in config['layers']:\n+                layer = created_layers[layer_data['name']]\n+                if layer in unprocessed_nodes:\n+                    for node_data in unprocessed_nodes.pop(layer):\n+                        process_node(layer, node_data)\n+\n+        name = config.get('name')\n+        input_tensors = []\n+        output_tensors = []\n+        for layer_data in config['input_layers']:\n+            layer_name, node_index, tensor_index = layer_data\n+            assert layer_name in created_layers\n+            layer = created_layers[layer_name]\n+            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n+            input_tensors.append(layer_output_tensors[tensor_index])\n+        for layer_data in config['output_layers']:\n+            layer_name, node_index, tensor_index = layer_data\n+            assert layer_name in created_layers\n+            layer = created_layers[layer_name]\n+            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n+            output_tensors.append(layer_output_tensors[tensor_index])\n+        return cls(inputs=input_tensors, outputs=output_tensors, name=name)\n+\n+    def save(self, filepath, overwrite=True, include_optimizer=True):\n+        \"\"\"Saves the model to a single HDF5 file.\n+\n+        The savefile includes:\n+            - The model architecture, allowing to re-instantiate the model.\n+            - The model weights.\n+            - The state of the optimizer, allowing to resume training\n+                exactly where you left off.\n+\n+        This allows you to save the entirety of the state of a model\n+        in a single file.\n+\n+        Saved models can be reinstantiated via `keras.models.load_model`.\n+        The model returned by `load_model`\n+        is a compiled model ready to be used (unless the saved model\n+        was never compiled in the first place).\n+\n+        # Arguments\n+            filepath: String, path to the file to save the weights to.\n+            overwrite: Whether to silently overwrite any existing file at the\n+                target location, or provide the user with a manual prompt.\n+            include_optimizer: If True, save optimizer's state together.\n+\n+        # Example\n+\n+        ```python\n+        from keras.models import load_model\n+\n+        model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n+        del model  # deletes the existing model\n+\n+        # returns a compiled model\n+        # identical to the previous one\n+        model = load_model('my_model.h5')\n+        ```\n+        \"\"\"\n+        from ..models import save_model\n+        save_model(self, filepath, overwrite, include_optimizer)\n+\n+    def save_weights(self, filepath, overwrite=True):\n+        \"\"\"Dumps all layer weights to a HDF5 file.\n+\n+        The weight file has:\n+            - `layer_names` (attribute), a list of strings\n+                (ordered names of model layers).\n+            - For every layer, a `group` named `layer.name`\n+                - For every such layer group, a group attribute `weight_names`,\n+                    a list of strings\n+                    (ordered names of weights tensor of the layer).\n+                - For every weight in the layer, a dataset\n+                    storing the weight value, named after the weight tensor.\n+\n+        # Arguments\n+            filepath: String, path to the file to save the weights to.\n+            overwrite: Whether to silently overwrite any existing file at the\n+                target location, or provide the user with a manual prompt.\n+\n+        # Raises\n+            ImportError: If h5py is not available.\n+        \"\"\"\n+        if h5py is None:\n+            raise ImportError('`save_weights` requires h5py.')\n+        # If file exists and should not be overwritten:\n+        if not overwrite and os.path.isfile(filepath):\n+            proceed = ask_to_proceed_with_overwrite(filepath)\n+            if not proceed:\n+                return\n+        with h5py.File(filepath, 'w') as f:\n+            saving.save_weights_to_hdf5_group(f, self.layers)\n+            f.flush()\n+\n+    def load_weights(self, filepath, by_name=False,\n+                     skip_mismatch=False, reshape=False):\n+        \"\"\"Loads all layer weights from a HDF5 save file.\n+\n+        If `by_name` is False (default) weights are loaded\n+        based on the network's topology, meaning the architecture\n+        should be the same as when the weights were saved.\n+        Note that layers that don't have weights are not taken\n+        into account in the topological ordering, so adding or\n+        removing layers is fine as long as they don't have weights.\n+\n+        If `by_name` is True, weights are loaded into layers\n+        only if they share the same name. This is useful\n+        for fine-tuning or transfer-learning models where\n+        some of the layers have changed.\n+\n+        # Arguments\n+            filepath: String, path to the weights file to load.\n+            by_name: Boolean, whether to load weights by name\n+                or by topological order.\n+            skip_mismatch: Boolean, whether to skip loading of layers\n+                where there is a mismatch in the number of weights,\n+                or a mismatch in the shape of the weight\n+                (only valid when `by_name`=True).\n+            reshape: Reshape weights to fit the layer when the correct number\n+                of weight arrays is present but their shape does not match.\n+\n+\n+        # Raises\n+            ImportError: If h5py is not available.\n+        \"\"\"\n+        if h5py is None:\n+            raise ImportError('`load_weights` requires h5py.')\n+        with h5py.File(filepath, mode='r') as f:\n+            if 'layer_names' not in f.attrs and 'model_weights' in f:\n+                f = f['model_weights']\n+            if by_name:\n+                saving.load_weights_from_hdf5_group_by_name(\n+                    f, self.layers, skip_mismatch=skip_mismatch,\n+                    reshape=reshape)\n+            else:\n+                saving.load_weights_from_hdf5_group(\n+                    f, self.layers, reshape=reshape)\n+\n+    def _updated_config(self):\n+        \"\"\"Util hared between different serialization methods.\n+\n+        # Returns\n+            Model config with Keras version information added.\n+        \"\"\"\n+        from .. import __version__ as keras_version\n+\n+        config = self.get_config()\n+        model_config = {\n+            'class_name': self.__class__.__name__,\n+            'config': config,\n+            'keras_version': keras_version,\n+            'backend': K.backend()\n+        }\n+        return model_config\n+\n+    def to_json(self, **kwargs):\n+        \"\"\"Returns a JSON string containing the network configuration.\n+\n+        To load a network from a JSON save file, use\n+        `keras.models.model_from_json(json_string, custom_objects={})`.\n+\n+        # Arguments\n+            **kwargs: Additional keyword arguments\n+                to be passed to `json.dumps()`.\n+\n+        # Returns\n+            A JSON string.\n+        \"\"\"\n+        def get_json_type(obj):\n+            # If obj is any numpy type\n+            if type(obj).__module__ == np.__name__:\n+                return obj.item()\n+\n+            # If obj is a python 'type'\n+            if type(obj).__name__ == type.__name__:\n+                return obj.__name__\n+\n+            raise TypeError('Not JSON Serializable:', obj)\n+\n+        model_config = self._updated_config()\n+        return json.dumps(model_config, default=get_json_type, **kwargs)\n+\n+    def to_yaml(self, **kwargs):\n+        \"\"\"Returns a yaml string containing the network configuration.\n+\n+        To load a network from a yaml save file, use\n+        `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n+\n+        `custom_objects` should be a dictionary mapping\n+        the names of custom losses / layers / etc to the corresponding\n+        functions / classes.\n+\n+        # Arguments\n+            **kwargs: Additional keyword arguments\n+                to be passed to `yaml.dump()`.\n+\n+        # Returns\n+            A YAML string.\n+        \"\"\"\n+        return yaml.dump(self._updated_config(), **kwargs)\n+\n+    def summary(self, line_length=None, positions=None, print_fn=None):\n+        \"\"\"Prints a string summary of the network.\n+\n+        # Arguments\n+            line_length: Total length of printed lines\n+                (e.g. set this to adapt the display to different\n+                terminal window sizes).\n+            positions: Relative or absolute positions of log elements\n+                in each line. If not provided,\n+                defaults to `[.33, .55, .67, 1.]`.\n+            print_fn: Print function to use.\n+                It will be called on each line of the summary.\n+                You can set it to a custom function\n+                in order to capture the string summary.\n+                It defaults to `print` (prints to stdout).\n+        \"\"\"\n+        return print_layer_summary(self,\n+                                   line_length=line_length,\n+                                   positions=positions,\n+                                   print_fn=print_fn)\n+\n+\n+def get_source_inputs(tensor, layer=None, node_index=None):\n+    \"\"\"Returns the list of input tensors necessary to compute `tensor`.\n+\n+    Output will always be a list of tensors\n+    (potentially with 1 element).\n+\n+    # Arguments\n+        tensor: The tensor to start from.\n+        layer: Origin layer of the tensor. Will be\n+            determined via tensor._keras_history if not provided.\n+        node_index: Origin node index of the tensor.\n+\n+    # Returns\n+        List of input tensors.\n+    \"\"\"\n+    if not hasattr(tensor, '_keras_history'):\n+        return tensor\n+\n+    if layer is None or node_index:\n+        layer, node_index, _ = tensor._keras_history\n+    if not layer._inbound_nodes:\n+        return [tensor]\n+    else:\n+        node = layer._inbound_nodes[node_index]\n+        if not node.inbound_layers:\n+            # Reached an Input layer, stop recursion.\n+            return node.input_tensors\n+        else:\n+            source_tensors = []\n+            for i in range(len(node.inbound_layers)):\n+                x = node.input_tensors[i]\n+                layer = node.inbound_layers[i]\n+                node_index = node.node_indices[i]\n+                previous_sources = get_source_inputs(x,\n+                                                     layer,\n+                                                     node_index)\n+                # Avoid input redundancy.\n+                for x in previous_sources:\n+                    if x not in source_tensors:\n+                        source_tensors.append(x)\n+            return source_tensors\n\n@@ -0,0 +1,979 @@\n+\"\"\"Model saving utilities.\n+\"\"\"\n+from __future__ import print_function\n+from __future__ import absolute_import\n+from __future__ import division\n+\n+import numpy as np\n+import os\n+import json\n+import yaml\n+import warnings\n+from six.moves import zip\n+\n+from .. import backend as K\n+from .. import optimizers\n+from ..utils.io_utils import ask_to_proceed_with_overwrite\n+from ..legacy import models as legacy_models\n+from ..utils import conv_utils\n+\n+try:\n+    import h5py\n+    HDF5_OBJECT_HEADER_LIMIT = 64512\n+except ImportError:\n+    h5py = None\n+\n+\n+def save_model(model, filepath, overwrite=True, include_optimizer=True):\n+    \"\"\"Save a model to a HDF5 file.\n+\n+    Note: Please also see\n+    [How can I install HDF5 or h5py to save my models in Keras?](\n+        /getting-started/faq/\n+        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)\n+    in the FAQ for instructions on how to install `h5py`.\n+\n+    The saved model contains:\n+        - the model's configuration (topology)\n+        - the model's weights\n+        - the model's optimizer's state (if any)\n+\n+    Thus the saved model can be reinstantiated in\n+    the exact same state, without any of the code\n+    used for model definition or training.\n+\n+    # Arguments\n+        model: Keras model instance to be saved.\n+        filepath: one of the following:\n+            - string, path where to save the model, or\n+            - h5py.File object where to save the model\n+        overwrite: Whether we should overwrite any existing\n+            model at the target location, or instead\n+            ask the user with a manual prompt.\n+        include_optimizer: If True, save optimizer's state together.\n+\n+    # Raises\n+        ImportError: if h5py is not available.\n+    \"\"\"\n+\n+    if h5py is None:\n+        raise ImportError('`save_model` requires h5py.')\n+\n+    def get_json_type(obj):\n+        \"\"\"Serialize any object to a JSON-serializable structure.\n+\n+        # Arguments\n+            obj: the object to serialize\n+\n+        # Returns\n+            JSON-serializable structure representing `obj`.\n+\n+        # Raises\n+            TypeError: if `obj` cannot be serialized.\n+        \"\"\"\n+        # if obj is a serializable Keras class instance\n+        # e.g. optimizer, layer\n+        if hasattr(obj, 'get_config'):\n+            return {'class_name': obj.__class__.__name__,\n+                    'config': obj.get_config()}\n+\n+        # if obj is any numpy type\n+        if type(obj).__module__ == np.__name__:\n+            if isinstance(obj, np.ndarray):\n+                return {'type': type(obj),\n+                        'value': obj.tolist()}\n+            else:\n+                return obj.item()\n+\n+        # misc functions (e.g. loss function)\n+        if callable(obj):\n+            return obj.__name__\n+\n+        # if obj is a python 'type'\n+        if type(obj).__name__ == type.__name__:\n+            return obj.__name__\n+\n+        raise TypeError('Not JSON Serializable:', obj)\n+\n+    from .. import __version__ as keras_version\n+\n+    if not isinstance(filepath, h5py.File):\n+        # If file exists and should not be overwritten.\n+        if not overwrite and os.path.isfile(filepath):\n+            proceed = ask_to_proceed_with_overwrite(filepath)\n+            if not proceed:\n+                return\n+\n+        f = h5py.File(filepath, mode='w')\n+        opened_new_file = True\n+    else:\n+        f = filepath\n+        opened_new_file = False\n+\n+    try:\n+        f.attrs['keras_version'] = str(keras_version).encode('utf8')\n+        f.attrs['backend'] = K.backend().encode('utf8')\n+        f.attrs['model_config'] = json.dumps({\n+            'class_name': model.__class__.__name__,\n+            'config': model.get_config()\n+        }, default=get_json_type).encode('utf8')\n+\n+        model_weights_group = f.create_group('model_weights')\n+        if legacy_models.needs_legacy_support(model):\n+            model_layers = legacy_models.legacy_sequential_layers(model)\n+        else:\n+            model_layers = model.layers\n+        save_weights_to_hdf5_group(model_weights_group, model_layers)\n+\n+        if include_optimizer and hasattr(model, 'optimizer'):\n+            if isinstance(model.optimizer, optimizers.TFOptimizer):\n+                warnings.warn(\n+                    'TensorFlow optimizers do not '\n+                    'make it possible to access '\n+                    'optimizer attributes or optimizer state '\n+                    'after instantiation. '\n+                    'As a result, we cannot save the optimizer '\n+                    'as part of the model save file.'\n+                    'You will have to compile your model again '\n+                    'after loading it. '\n+                    'Prefer using a Keras optimizer instead '\n+                    '(see keras.io/optimizers).')\n+            else:\n+                f.attrs['training_config'] = json.dumps({\n+                    'optimizer_config': {\n+                        'class_name': model.optimizer.__class__.__name__,\n+                        'config': model.optimizer.get_config()\n+                    },\n+                    'loss': model.loss,\n+                    'metrics': model.metrics,\n+                    'sample_weight_mode': model.sample_weight_mode,\n+                    'loss_weights': model.loss_weights,\n+                }, default=get_json_type).encode('utf8')\n+\n+                # Save optimizer weights.\n+                symbolic_weights = getattr(model.optimizer, 'weights')\n+                if symbolic_weights:\n+                    optimizer_weights_group = f.create_group(\n+                        'optimizer_weights')\n+                    weight_values = K.batch_get_value(symbolic_weights)\n+                    weight_names = []\n+                    for i, (w, val) in enumerate(zip(symbolic_weights,\n+                                                     weight_values)):\n+                        # Default values of symbolic_weights is /variable\n+                        # for Theano and CNTK\n+                        if K.backend() == 'theano' or K.backend() == 'cntk':\n+                            if hasattr(w, 'name'):\n+                                if w.name.split('/')[-1] == 'variable':\n+                                    name = str(w.name) + '_' + str(i)\n+                                else:\n+                                    name = str(w.name)\n+                            else:\n+                                name = 'param_' + str(i)\n+                        else:\n+                            if hasattr(w, 'name') and w.name:\n+                                name = str(w.name)\n+                            else:\n+                                name = 'param_' + str(i)\n+                        weight_names.append(name.encode('utf8'))\n+                    optimizer_weights_group.attrs[\n+                        'weight_names'] = weight_names\n+                    for name, val in zip(weight_names, weight_values):\n+                        param_dset = optimizer_weights_group.create_dataset(\n+                            name,\n+                            val.shape,\n+                            dtype=val.dtype)\n+                        if not val.shape:\n+                            # scalar\n+                            param_dset[()] = val\n+                        else:\n+                            param_dset[:] = val\n+        f.flush()\n+    finally:\n+        if opened_new_file:\n+            f.close()\n+\n+\n+def load_model(filepath, custom_objects=None, compile=True):\n+    \"\"\"Loads a model saved via `save_model`.\n+\n+    # Arguments\n+        filepath: one of the following:\n+            - string, path to the saved model, or\n+            - h5py.File object from which to load the model\n+        custom_objects: Optional dictionary mapping names\n+            (strings) to custom classes or functions to be\n+            considered during deserialization.\n+        compile: Boolean, whether to compile the model\n+            after loading.\n+\n+    # Returns\n+        A Keras model instance. If an optimizer was found\n+        as part of the saved model, the model is already\n+        compiled. Otherwise, the model is uncompiled and\n+        a warning will be displayed. When `compile` is set\n+        to False, the compilation is omitted without any\n+        warning.\n+\n+    # Raises\n+        ImportError: if h5py is not available.\n+        ValueError: In case of an invalid savefile.\n+    \"\"\"\n+    if h5py is None:\n+        raise ImportError('`load_model` requires h5py.')\n+\n+    if not custom_objects:\n+        custom_objects = {}\n+\n+    def convert_custom_objects(obj):\n+        \"\"\"Handles custom object lookup.\n+\n+        # Arguments\n+            obj: object, dict, or list.\n+\n+        # Returns\n+            The same structure, where occurrences\n+                of a custom object name have been replaced\n+                with the custom object.\n+        \"\"\"\n+        if isinstance(obj, list):\n+            deserialized = []\n+            for value in obj:\n+                deserialized.append(convert_custom_objects(value))\n+            return deserialized\n+        if isinstance(obj, dict):\n+            deserialized = {}\n+            for key, value in obj.items():\n+                deserialized[key] = convert_custom_objects(value)\n+            return deserialized\n+        if obj in custom_objects:\n+            return custom_objects[obj]\n+        return obj\n+\n+    opened_new_file = not isinstance(filepath, h5py.File)\n+    if opened_new_file:\n+        f = h5py.File(filepath, mode='r')\n+    else:\n+        f = filepath\n+\n+    try:\n+        # instantiate model\n+        model_config = f.attrs.get('model_config')\n+        if model_config is None:\n+            raise ValueError('No model found in config file.')\n+        model_config = json.loads(model_config.decode('utf-8'))\n+        model = model_from_config(model_config, custom_objects=custom_objects)\n+\n+        # set weights\n+        load_weights_from_hdf5_group(f['model_weights'], model.layers)\n+\n+        # Early return if compilation is not required.\n+        if not compile:\n+            return model\n+\n+        # instantiate optimizer\n+        training_config = f.attrs.get('training_config')\n+        if training_config is None:\n+            warnings.warn('No training configuration found in save file: '\n+                          'the model was *not* compiled. Compile it manually.')\n+            return model\n+        training_config = json.loads(training_config.decode('utf-8'))\n+        optimizer_config = training_config['optimizer_config']\n+        optimizer = optimizers.deserialize(optimizer_config,\n+                                           custom_objects=custom_objects)\n+\n+        # Recover loss functions and metrics.\n+        loss = convert_custom_objects(training_config['loss'])\n+        metrics = convert_custom_objects(training_config['metrics'])\n+        sample_weight_mode = training_config['sample_weight_mode']\n+        loss_weights = training_config['loss_weights']\n+\n+        # Compile model.\n+        model.compile(optimizer=optimizer,\n+                      loss=loss,\n+                      metrics=metrics,\n+                      loss_weights=loss_weights,\n+                      sample_weight_mode=sample_weight_mode)\n+\n+        # Set optimizer weights.\n+        if 'optimizer_weights' in f:\n+            # Build train function (to get weight updates).\n+            if model.__class__.__name__ == 'Sequential':\n+                model.model._make_train_function()\n+            else:\n+                model._make_train_function()\n+            optimizer_weights_group = f['optimizer_weights']\n+            optimizer_weight_names = [\n+                n.decode('utf8') for n in\n+                optimizer_weights_group.attrs['weight_names']]\n+            optimizer_weight_values = [optimizer_weights_group[n] for n in\n+                                       optimizer_weight_names]\n+            try:\n+                model.optimizer.set_weights(optimizer_weight_values)\n+            except ValueError:\n+                warnings.warn('Error in loading the saved optimizer '\n+                              'state. As a result, your model is '\n+                              'starting with a freshly initialized '\n+                              'optimizer.')\n+        return model\n+    finally:\n+        if opened_new_file:\n+            f.close()\n+\n+\n+def model_from_config(config, custom_objects=None):\n+    \"\"\"Instantiates a Keras model from its config.\n+\n+    # Arguments\n+        config: Configuration dictionary.\n+        custom_objects: Optional dictionary mapping names\n+            (strings) to custom classes or functions to be\n+            considered during deserialization.\n+\n+    # Returns\n+        A Keras model instance (uncompiled).\n+\n+    # Raises\n+        TypeError: if `config` is not a dictionary.\n+    \"\"\"\n+    if isinstance(config, list):\n+        raise TypeError('`model_from_config` expects a dictionary, '\n+                        'not a list. Maybe you meant to use '\n+                        '`Sequential.from_config(config)`?')\n+    from ..layers import deserialize\n+    return deserialize(config, custom_objects=custom_objects)\n+\n+\n+def model_from_yaml(yaml_string, custom_objects=None):\n+    \"\"\"Parses a yaml model configuration file and returns a model instance.\n+\n+    # Arguments\n+        yaml_string: YAML string encoding a model configuration.\n+        custom_objects: Optional dictionary mapping names\n+            (strings) to custom classes or functions to be\n+            considered during deserialization.\n+\n+    # Returns\n+        A Keras model instance (uncompiled).\n+    \"\"\"\n+    config = yaml.load(yaml_string)\n+    from ..layers import deserialize\n+    return deserialize(config, custom_objects=custom_objects)\n+\n+\n+def model_from_json(json_string, custom_objects=None):\n+    \"\"\"Parses a JSON model configuration file and returns a model instance.\n+\n+    # Arguments\n+        json_string: JSON string encoding a model configuration.\n+        custom_objects: Optional dictionary mapping names\n+            (strings) to custom classes or functions to be\n+            considered during deserialization.\n+\n+    # Returns\n+        A Keras model instance (uncompiled).\n+    \"\"\"\n+    config = json.loads(json_string)\n+    from ..layers import deserialize\n+    return deserialize(config, custom_objects=custom_objects)\n+\n+\n+def save_attributes_to_hdf5_group(group, name, data):\n+    \"\"\"Saves attributes (data) of the specified name into the HDF5 group.\n+\n+    This method deals with an inherent problem of HDF5 file which is not\n+    able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n+\n+    # Arguments\n+        group: A pointer to a HDF5 group.\n+        name: A name of the attributes to save.\n+        data: Attributes data to store.\n+    \"\"\"\n+    # Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`\n+    # because in that case even chunking the array would not make the saving\n+    # possible.\n+    bad_attributes = [x for x in data if len(x) > HDF5_OBJECT_HEADER_LIMIT]\n+\n+    # Expecting this to never be true.\n+    if len(bad_attributes) > 0:\n+        raise RuntimeError('The following attributes cannot be saved to HDF5 '\n+                           'file because they are larger than %d bytes: %s'\n+                           % (HDF5_OBJECT_HEADER_LIMIT,\n+                              ', '.join([x for x in bad_attributes])))\n+\n+    data_npy = np.asarray(data)\n+\n+    num_chunks = 1\n+    chunked_data = np.array_split(data_npy, num_chunks)\n+\n+    # This will never loop forever thanks to the test above.\n+    while any(map(lambda x: x.nbytes > HDF5_OBJECT_HEADER_LIMIT, chunked_data)):\n+        num_chunks += 1\n+        chunked_data = np.array_split(data_npy, num_chunks)\n+\n+    if num_chunks > 1:\n+        for chunk_id, chunk_data in enumerate(chunked_data):\n+            group.attrs['%s%d' % (name, chunk_id)] = chunk_data\n+    else:\n+        group.attrs[name] = data\n+\n+\n+def load_attributes_from_hdf5_group(group, name):\n+    \"\"\"Loads attributes of the specified name from the HDF5 group.\n+\n+    This method deals with an inherent problem\n+    of HDF5 file which is not able to store\n+    data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n+\n+    # Arguments\n+        group: A pointer to a HDF5 group.\n+        name: A name of the attributes to load.\n+\n+    # Returns\n+        data: Attributes data.\n+    \"\"\"\n+    if name in group.attrs:\n+        data = [n.decode('utf8') for n in group.attrs[name]]\n+    else:\n+        data = []\n+        chunk_id = 0\n+        while ('%s%d' % (name, chunk_id)) in group.attrs:\n+            data.extend([n.decode('utf8')\n+                        for n in group.attrs['%s%d' % (name, chunk_id)]])\n+            chunk_id += 1\n+    return data\n+\n+\n+def save_weights_to_hdf5_group(f, layers):\n+    from .. import __version__ as keras_version\n+\n+    save_attributes_to_hdf5_group(\n+        f, 'layer_names', [layer.name.encode('utf8') for layer in layers])\n+    f.attrs['backend'] = K.backend().encode('utf8')\n+    f.attrs['keras_version'] = str(keras_version).encode('utf8')\n+\n+    for layer in layers:\n+        g = f.create_group(layer.name)\n+        symbolic_weights = layer.weights\n+        weight_values = K.batch_get_value(symbolic_weights)\n+        weight_names = []\n+        for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n+            if hasattr(w, 'name') and w.name:\n+                name = str(w.name)\n+            else:\n+                name = 'param_' + str(i)\n+            weight_names.append(name.encode('utf8'))\n+        save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\n+        for name, val in zip(weight_names, weight_values):\n+            param_dset = g.create_dataset(name, val.shape,\n+                                          dtype=val.dtype)\n+            if not val.shape:\n+                # scalar\n+                param_dset[()] = val\n+            else:\n+                param_dset[:] = val\n+\n+\n+def preprocess_weights_for_loading(layer, weights,\n+                                   original_keras_version=None,\n+                                   original_backend=None,\n+                                   reshape=False):\n+    \"\"\"Converts layers weights from Keras 1 format to Keras 2.\n+\n+    # Arguments\n+        layer: Layer instance.\n+        weights: List of weights values (Numpy arrays).\n+        original_keras_version: Keras version for the weights, as a string.\n+        original_backend: Keras backend the weights were trained with,\n+            as a string.\n+        reshape: Reshape weights to fit the layer when the correct number\n+            of values are present but the shape does not match.\n+\n+    # Returns\n+        A list of weights values (Numpy arrays).\n+    \"\"\"\n+    if layer.__class__.__name__ == 'Bidirectional':\n+        num_weights_per_layer = len(weights) // 2\n+        forward_weights = preprocess_weights_for_loading(layer.forward_layer,\n+                                                         weights[:num_weights_per_layer],\n+                                                         original_keras_version,\n+                                                         original_backend)\n+        backward_weights = preprocess_weights_for_loading(layer.backward_layer,\n+                                                          weights[num_weights_per_layer:],\n+                                                          original_keras_version,\n+                                                          original_backend)\n+        weights = forward_weights + backward_weights\n+\n+    if original_keras_version == '1':\n+        if layer.__class__.__name__ == 'TimeDistributed':\n+            weights = preprocess_weights_for_loading(layer.layer,\n+                                                     weights,\n+                                                     original_keras_version,\n+                                                     original_backend)\n+\n+        if layer.__class__.__name__ == 'Conv1D':\n+            shape = weights[0].shape\n+            # Handle Keras 1.1 format\n+            if shape[:2] != (layer.kernel_size[0], 1) or shape[3] != layer.filters:\n+                # Legacy shape:\n+                # (filters, input_dim, filter_length, 1)\n+                assert shape[0] == layer.filters and shape[2:] == (layer.kernel_size[0], 1)\n+                weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n+            weights[0] = weights[0][:, 0, :, :]\n+\n+        if layer.__class__.__name__ == 'Conv2D':\n+            if layer.data_format == 'channels_first':\n+                # old: (filters, stack_size, kernel_rows, kernel_cols)\n+                # new: (kernel_rows, kernel_cols, stack_size, filters)\n+                weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n+\n+        if layer.__class__.__name__ == 'Conv2DTranspose':\n+            if layer.data_format == 'channels_last':\n+                # old: (kernel_rows, kernel_cols, stack_size, filters)\n+                # new: (kernel_rows, kernel_cols, filters, stack_size)\n+                weights[0] = np.transpose(weights[0], (0, 1, 3, 2))\n+            if layer.data_format == 'channels_first':\n+                # old: (filters, stack_size, kernel_rows, kernel_cols)\n+                # new: (kernel_rows, kernel_cols, filters, stack_size)\n+                weights[0] = np.transpose(weights[0], (2, 3, 0, 1))\n+\n+        if layer.__class__.__name__ == 'Conv3D':\n+            if layer.data_format == 'channels_first':\n+                # old: (filters, stack_size, ...)\n+                # new: (..., stack_size, filters)\n+                weights[0] = np.transpose(weights[0], (2, 3, 4, 1, 0))\n+\n+        if layer.__class__.__name__ == 'GRU':\n+            if len(weights) == 9:\n+                kernel = np.concatenate([weights[0],\n+                                         weights[3],\n+                                         weights[6]], axis=-1)\n+                recurrent_kernel = np.concatenate([weights[1],\n+                                                   weights[4],\n+                                                   weights[7]], axis=-1)\n+                bias = np.concatenate([weights[2],\n+                                       weights[5],\n+                                       weights[8]], axis=-1)\n+                weights = [kernel, recurrent_kernel, bias]\n+\n+        if layer.__class__.__name__ == 'LSTM':\n+            if len(weights) == 12:\n+                # old: i, c, f, o\n+                # new: i, f, c, o\n+                kernel = np.concatenate([weights[0],\n+                                         weights[6],\n+                                         weights[3],\n+                                         weights[9]], axis=-1)\n+                recurrent_kernel = np.concatenate([weights[1],\n+                                                   weights[7],\n+                                                   weights[4],\n+                                                   weights[10]], axis=-1)\n+                bias = np.concatenate([weights[2],\n+                                       weights[8],\n+                                       weights[5],\n+                                       weights[11]], axis=-1)\n+                weights = [kernel, recurrent_kernel, bias]\n+\n+        if layer.__class__.__name__ == 'ConvLSTM2D':\n+            if len(weights) == 12:\n+                kernel = np.concatenate([weights[0],\n+                                         weights[6],\n+                                         weights[3],\n+                                         weights[9]], axis=-1)\n+                recurrent_kernel = np.concatenate([weights[1],\n+                                                   weights[7],\n+                                                   weights[4],\n+                                                   weights[10]], axis=-1)\n+                bias = np.concatenate([weights[2],\n+                                       weights[8],\n+                                       weights[5],\n+                                       weights[11]], axis=-1)\n+                if layer.data_format == 'channels_first':\n+                    # old: (filters, stack_size, kernel_rows, kernel_cols)\n+                    # new: (kernel_rows, kernel_cols, stack_size, filters)\n+                    kernel = np.transpose(kernel, (2, 3, 1, 0))\n+                    recurrent_kernel = np.transpose(recurrent_kernel,\n+                                                    (2, 3, 1, 0))\n+                weights = [kernel, recurrent_kernel, bias]\n+\n+        if layer.__class__.__name__ in ['Model', 'Sequential']:\n+            new_weights = []\n+            # trainable weights\n+            for sublayer in layer.layers:\n+                num_weights = len(sublayer.trainable_weights)\n+                if num_weights > 0:\n+                    new_weights.extend(preprocess_weights_for_loading(\n+                        layer=sublayer,\n+                        weights=weights[:num_weights],\n+                        original_keras_version=original_keras_version,\n+                        original_backend=original_backend))\n+                    weights = weights[num_weights:]\n+\n+            # non-trainable weights\n+            for sublayer in layer.layers:\n+                num_weights = len([l for l in sublayer.weights\n+                                  if l not in sublayer.trainable_weights])\n+                if num_weights > 0:\n+                    new_weights.extend(preprocess_weights_for_loading(\n+                        layer=sublayer,\n+                        weights=weights[:num_weights],\n+                        original_keras_version=original_keras_version,\n+                        original_backend=original_backend))\n+                    weights = weights[num_weights:]\n+            weights = new_weights\n+\n+    conv_layers = ['Conv1D',\n+                   'Conv2D',\n+                   'Conv3D',\n+                   'Conv2DTranspose',\n+                   'ConvLSTM2D']\n+    if layer.__class__.__name__ in conv_layers:\n+        layer_weights_shape = K.int_shape(layer.weights[0])\n+        if _need_convert_kernel(original_backend):\n+            weights[0] = conv_utils.convert_kernel(weights[0])\n+            if layer.__class__.__name__ == 'ConvLSTM2D':\n+                weights[1] = conv_utils.convert_kernel(weights[1])\n+        if reshape and layer_weights_shape != weights[0].shape:\n+            if weights[0].size != np.prod(layer_weights_shape):\n+                raise ValueError('Weights must be of equal size to ' +\n+                                 'apply a reshape operation. ' +\n+                                 'Layer ' + layer.name +\n+                                 '\\'s weights have shape ' +\n+                                 str(layer_weights_shape) + ' and size ' +\n+                                 str(np.prod(layer_weights_shape)) + '. ' +\n+                                 'The weights for loading have shape ' +\n+                                 str(weights[0].shape) + ' and size ' +\n+                                 str(weights[0].size) + '. ')\n+            weights[0] = np.reshape(weights[0], layer_weights_shape)\n+        elif layer_weights_shape != weights[0].shape:\n+            weights[0] = np.transpose(weights[0], (3, 2, 0, 1))\n+            if layer.__class__.__name__ == 'ConvLSTM2D':\n+                weights[1] = np.transpose(weights[1], (3, 2, 0, 1))\n+\n+    weights = _convert_rnn_weights(layer, weights)\n+\n+    return weights\n+\n+\n+def _convert_rnn_weights(layer, weights):\n+    \"\"\"Converts weights for RNN layers between native and CuDNN format.\n+\n+    Input kernels for each gate are transposed and converted between Fortran\n+    and C layout, recurrent kernels are transposed. For LSTM biases are summed/\n+    split in half, for GRU biases are reshaped.\n+\n+    Weights can be converted in both directions between `LSTM` and`CuDNNSLTM`\n+    and between `CuDNNGRU` and `GRU(reset_after=True)`. Default `GRU` is not\n+    compatible with `CuDNNGRU`.\n+\n+    For missing biases in `LSTM`/`GRU` (`use_bias=False`),\n+    no conversion is made.\n+\n+    # Arguments\n+        layer: Target layer instance.\n+        weights: List of source weights values (input kernels, recurrent\n+            kernels, [biases]) (Numpy arrays).\n+\n+    # Returns\n+        A list of converted weights values (Numpy arrays).\n+\n+    # Raises\n+        ValueError: for incompatible GRU layer/weights or incompatible biases\n+    \"\"\"\n+\n+    def transform_kernels(kernels, func, n_gates):\n+        \"\"\"Transforms kernel for each gate separately using given function.\n+\n+        # Arguments\n+            kernels: Stacked array of kernels for individual gates.\n+            func: Function applied to kernel of each gate.\n+            n_gates: Number of gates (4 for LSTM, 3 for GRU).\n+        # Returns\n+            Stacked array of transformed kernels.\n+        \"\"\"\n+        return np.hstack([func(k) for k in np.hsplit(kernels, n_gates)])\n+\n+    def transpose_input(from_cudnn):\n+        \"\"\"Makes a function that transforms input kernels from/to CuDNN format.\n+\n+        It keeps the shape, but changes between the layout (Fortran/C). Eg.:\n+\n+        ```\n+        Keras                 CuDNN\n+        [[0, 1, 2],  <--->  [[0, 2, 4],\n+         [3, 4, 5]]          [1, 3, 5]]\n+        ```\n+\n+        It can be passed to `transform_kernels()`.\n+\n+        # Arguments\n+            from_cudnn: `True` if source weights are in CuDNN format, `False`\n+                if they're in plain Keras format.\n+        # Returns\n+            Function that converts input kernel to the other format.\n+        \"\"\"\n+        order = 'F' if from_cudnn else 'C'\n+\n+        def transform(kernel):\n+            return kernel.T.reshape(kernel.shape, order=order)\n+\n+        return transform\n+\n+    target_class = layer.__class__.__name__\n+\n+    # convert the weights between CuDNNLSTM and LSTM\n+    if target_class in ['LSTM', 'CuDNNLSTM'] and len(weights) == 3:\n+        # determine if we're loading a CuDNNLSTM layer\n+        # from the number of bias weights:\n+        # CuDNNLSTM has (units * 8) weights; while LSTM has (units * 4)\n+        # if there's no bias weight in the file, skip this conversion\n+        units = weights[1].shape[0]\n+        bias_shape = weights[2].shape\n+        n_gates = 4\n+\n+        if bias_shape == (2 * units * n_gates,):\n+            source = 'CuDNNLSTM'\n+        elif bias_shape == (units * n_gates,):\n+            source = 'LSTM'\n+        else:\n+            raise ValueError('Invalid bias shape: ' + str(bias_shape))\n+\n+        def convert_weights(weights, from_cudnn=True):\n+            # transpose (and reshape) input and recurrent kernels\n+            kernels = transform_kernels(weights[0],\n+                                        transpose_input(from_cudnn),\n+                                        n_gates)\n+            recurrent_kernels = transform_kernels(weights[1], lambda k: k.T, n_gates)\n+            if from_cudnn:\n+                # merge input and recurrent biases into a single set\n+                biases = np.sum(np.split(weights[2], 2, axis=0), axis=0)\n+            else:\n+                # Split single set of biases evenly to two sets. The way of\n+                # splitting doesn't matter as long as the two sets sum is kept.\n+                biases = np.tile(0.5 * weights[2], 2)\n+            return [kernels, recurrent_kernels, biases]\n+\n+        if source != target_class:\n+            weights = convert_weights(weights, from_cudnn=source == 'CuDNNLSTM')\n+\n+    # convert the weights between CuDNNGRU and GRU(reset_after=True)\n+    if target_class in ['GRU', 'CuDNNGRU'] and len(weights) == 3:\n+        # We can determine the source of the weights from the shape of the bias.\n+        # If there is no bias we skip the conversion since CuDNNGRU always has biases.\n+\n+        units = weights[1].shape[0]\n+        bias_shape = weights[2].shape\n+        n_gates = 3\n+\n+        def convert_weights(weights, from_cudnn=True):\n+            kernels = transform_kernels(weights[0], transpose_input(from_cudnn), n_gates)\n+            recurrent_kernels = transform_kernels(weights[1], lambda k: k.T, n_gates)\n+            biases = weights[2].reshape((2, -1) if from_cudnn else -1)\n+            return [kernels, recurrent_kernels, biases]\n+\n+        if bias_shape == (2 * units * n_gates,):\n+            source = 'CuDNNGRU'\n+        elif bias_shape == (2, units * n_gates):\n+            source = 'GRU(reset_after=True)'\n+        elif bias_shape == (units * n_gates,):\n+            source = 'GRU(reset_after=False)'\n+        else:\n+            raise ValueError('Invalid bias shape: ' + str(bias_shape))\n+\n+        if target_class == 'CuDNNGRU':\n+            target = 'CuDNNGRU'\n+        elif layer.reset_after:\n+            target = 'GRU(reset_after=True)'\n+        else:\n+            target = 'GRU(reset_after=False)'\n+\n+        # only convert between different types\n+        if source != target:\n+            types = (source, target)\n+            if 'GRU(reset_after=False)' in types:\n+                raise ValueError('%s is not compatible with %s' % types)\n+            if source == 'CuDNNGRU':\n+                weights = convert_weights(weights, from_cudnn=True)\n+            elif source == 'GRU(reset_after=True)':\n+                weights = convert_weights(weights, from_cudnn=False)\n+\n+    return weights\n+\n+\n+def _need_convert_kernel(original_backend):\n+    \"\"\"Checks if conversion on kernel matrices is required during weight loading.\n+\n+    The convolution operation is implemented differently in different backends.\n+    While TH implements convolution, TF and CNTK implement the correlation operation.\n+    So the channel axis needs to be flipped when we're loading TF weights onto a TH model,\n+    or vice verca. However, there's no conversion required between TF and CNTK.\n+\n+    # Arguments\n+        original_backend: Keras backend the weights were trained with, as a string.\n+\n+    # Returns\n+        `True` if conversion on kernel matrices is required, otherwise `False`.\n+    \"\"\"\n+    if original_backend is None:\n+        # backend information not available\n+        return False\n+    uses_correlation = {'tensorflow': True,\n+                        'theano': False,\n+                        'cntk': True}\n+    return uses_correlation[original_backend] != uses_correlation[K.backend()]\n+\n+\n+def load_weights_from_hdf5_group(f, layers, reshape=False):\n+    \"\"\"Implements topological (order-based) weight loading.\n+\n+    # Arguments\n+        f: A pointer to a HDF5 group.\n+        layers: a list of target layers.\n+        reshape: Reshape weights to fit the layer when the correct number\n+            of values are present but the shape does not match.\n+\n+    # Raises\n+        ValueError: in case of mismatch between provided layers\n+            and weights file.\n+    \"\"\"\n+    if 'keras_version' in f.attrs:\n+        original_keras_version = f.attrs['keras_version'].decode('utf8')\n+    else:\n+        original_keras_version = '1'\n+    if 'backend' in f.attrs:\n+        original_backend = f.attrs['backend'].decode('utf8')\n+    else:\n+        original_backend = None\n+\n+    filtered_layers = []\n+    for layer in layers:\n+        weights = layer.weights\n+        if weights:\n+            filtered_layers.append(layer)\n+\n+    layer_names = load_attributes_from_hdf5_group(f, 'layer_names')\n+    filtered_layer_names = []\n+    for name in layer_names:\n+        g = f[name]\n+        weight_names = load_attributes_from_hdf5_group(g, 'weight_names')\n+        if weight_names:\n+            filtered_layer_names.append(name)\n+    layer_names = filtered_layer_names\n+    if len(layer_names) != len(filtered_layers):\n+        raise ValueError('You are trying to load a weight file '\n+                         'containing ' + str(len(layer_names)) +\n+                         ' layers into a model with ' +\n+                         str(len(filtered_layers)) + ' layers.')\n+\n+    # We batch weight value assignments in a single backend call\n+    # which provides a speedup in TensorFlow.\n+    weight_value_tuples = []\n+    for k, name in enumerate(layer_names):\n+        g = f[name]\n+        weight_names = load_attributes_from_hdf5_group(g, 'weight_names')\n+        weight_values = [np.asarray(g[weight_name]) for weight_name in weight_names]\n+        layer = filtered_layers[k]\n+        symbolic_weights = layer.weights\n+        weight_values = preprocess_weights_for_loading(layer,\n+                                                       weight_values,\n+                                                       original_keras_version,\n+                                                       original_backend,\n+                                                       reshape=reshape)\n+        if len(weight_values) != len(symbolic_weights):\n+            raise ValueError('Layer #' + str(k) +\n+                             ' (named \"' + layer.name +\n+                             '\" in the current model) was found to '\n+                             'correspond to layer ' + name +\n+                             ' in the save file. '\n+                             'However the new layer ' + layer.name +\n+                             ' expects ' + str(len(symbolic_weights)) +\n+                             ' weights, but the saved weights have ' +\n+                             str(len(weight_values)) +\n+                             ' elements.')\n+        weight_value_tuples += zip(symbolic_weights, weight_values)\n+    K.batch_set_value(weight_value_tuples)\n+\n+\n+def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False,\n+                                         reshape=False):\n+    \"\"\"Implements name-based weight loading.\n+\n+    (instead of topological weight loading).\n+\n+    Layers that have no matching name are skipped.\n+\n+    # Arguments\n+        f: A pointer to a HDF5 group.\n+        layers: A list of target layers.\n+        skip_mismatch: Boolean, whether to skip loading of layers\n+            where there is a mismatch in the number of weights,\n+            or a mismatch in the shape of the weights.\n+        reshape: Reshape weights to fit the layer when the correct number\n+            of values are present but the shape does not match.\n+\n+    # Raises\n+        ValueError: in case of mismatch between provided layers\n+            and weights file and skip_mismatch=False.\n+    \"\"\"\n+    if 'keras_version' in f.attrs:\n+        original_keras_version = f.attrs['keras_version'].decode('utf8')\n+    else:\n+        original_keras_version = '1'\n+    if 'backend' in f.attrs:\n+        original_backend = f.attrs['backend'].decode('utf8')\n+    else:\n+        original_backend = None\n+\n+    # New file format.\n+    layer_names = load_attributes_from_hdf5_group(f, 'layer_names')\n+\n+    # Reverse index of layer name to list of layers with name.\n+    index = {}\n+    for layer in layers:\n+        if layer.name:\n+            index.setdefault(layer.name, []).append(layer)\n+\n+    # We batch weight value assignments in a single backend call\n+    # which provides a speedup in TensorFlow.\n+    weight_value_tuples = []\n+    for k, name in enumerate(layer_names):\n+        g = f[name]\n+        weight_names = load_attributes_from_hdf5_group(g, 'weight_names')\n+        weight_values = [np.asarray(g[weight_name]) for weight_name in weight_names]\n+\n+        for layer in index.get(name, []):\n+            symbolic_weights = layer.weights\n+            weight_values = preprocess_weights_for_loading(\n+                layer,\n+                weight_values,\n+                original_keras_version,\n+                original_backend,\n+                reshape=reshape)\n+            if len(weight_values) != len(symbolic_weights):\n+                if skip_mismatch:\n+                    warnings.warn('Skipping loading of weights for layer {}'.format(layer.name) +\n+                                  ' due to mismatch in number of weights' +\n+                                  ' ({} vs {}).'.format(len(symbolic_weights), len(weight_values)))\n+                    continue\n+                else:\n+                    raise ValueError('Layer #' + str(k) +\n+                                     ' (named \"' + layer.name +\n+                                     '\") expects ' +\n+                                     str(len(symbolic_weights)) +\n+                                     ' weight(s), but the saved weights' +\n+                                     ' have ' + str(len(weight_values)) +\n+                                     ' element(s).')\n+            # Set values.\n+            for i in range(len(weight_values)):\n+                if skip_mismatch:\n+                    if K.int_shape(symbolic_weights[i]) != weight_values[i].shape:\n+                        warnings.warn('Skipping loading of weights for layer {}'.format(layer.name) +\n+                                      ' due to mismatch in shape' +\n+                                      ' ({} vs {}).'.format(\n+                                          symbolic_weights[i].shape,\n+                                          weight_values[i].shape))\n+                        continue\n+\n+                weight_value_tuples.append((symbolic_weights[i],\n+                                            weight_values[i]))\n+\n+    K.batch_set_value(weight_value_tuples)\n\n@@ -0,0 +1,1144 @@\n+\"\"\"Sequential model class.\n+\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import warnings\n+import copy\n+import os\n+\n+from . import base_layer\n+from . import network\n+from . import saving\n+from .training import Model\n+from .base_layer import Layer\n+from .input_layer import Input\n+from .input_layer import InputLayer\n+from .. import backend as K\n+from .. import layers as layer_module\n+from ..utils.io_utils import ask_to_proceed_with_overwrite\n+from ..legacy import layers as legacy_layers\n+from ..legacy import models as legacy_models\n+from ..legacy import interfaces\n+\n+try:\n+    import h5py\n+except ImportError:\n+    h5py = None\n+\n+\n+class Sequential(Model):\n+    \"\"\"Linear stack of layers.\n+\n+    # Arguments\n+        layers: list of layers to add to the model.\n+\n+    # Note\n+        The first layer passed to a Sequential model\n+        should have a defined input shape. What that\n+        means is that it should have received an `input_shape`\n+        or `batch_input_shape` argument,\n+        or for some type of layers (recurrent, Dense...)\n+        an `input_dim` argument.\n+\n+    # Example\n+\n+        ```python\n+            model = Sequential()\n+            # first layer must have a defined input shape\n+            model.add(Dense(32, input_dim=500))\n+            # afterwards, Keras does automatic shape inference\n+            model.add(Dense(32))\n+\n+            # also possible (equivalent to the above):\n+            model = Sequential()\n+            model.add(Dense(32, input_shape=(500,)))\n+            model.add(Dense(32))\n+\n+            # also possible (equivalent to the above):\n+            model = Sequential()\n+            # here the batch dimension is None,\n+            # which means any batch size will be accepted by the model.\n+            model.add(Dense(32, batch_input_shape=(None, 500)))\n+            model.add(Dense(32))\n+        ```\n+    \"\"\"\n+\n+    def __init__(self, layers=None, name=None):\n+        self.layers = []  # Stack of layers.\n+        self.model = None  # Internal Model instance.\n+        self.inputs = []  # List of input tensors\n+        self.outputs = []  # List of length 1: the output tensor (unique).\n+        self._trainable = True\n+        self._initial_weights = None\n+\n+        # Model attributes.\n+        self._inbound_nodes = []\n+        self._outbound_nodes = []\n+        self.built = False\n+\n+        # Set model name.\n+        if not name:\n+            prefix = 'sequential_'\n+            name = prefix + str(K.get_uid(prefix))\n+        self.name = name\n+\n+        # Add to the model any layers passed to the constructor.\n+        if layers:\n+            for layer in layers:\n+                self.add(layer)\n+\n+    def add(self, layer):\n+        \"\"\"Adds a layer instance on top of the layer stack.\n+\n+        # Arguments\n+            layer: layer instance.\n+\n+        # Raises\n+            TypeError: If `layer` is not a layer instance.\n+            ValueError: In case the `layer` argument does not\n+                know its input shape.\n+            ValueError: In case the `layer` argument has\n+                multiple output tensors, or is already connected\n+                somewhere else (forbidden in `Sequential` models).\n+        \"\"\"\n+        if not isinstance(layer, Layer):\n+            raise TypeError('The added layer must be '\n+                            'an instance of class Layer. '\n+                            'Found: ' + str(layer))\n+        if not self.outputs:\n+            # First layer in model: check that it is an input layer.\n+            if not isinstance(layer, (InputLayer, legacy_layers.Merge)):\n+                # Create an input layer.\n+                # First, we need to infer its expected input shape and dtype.\n+                if isinstance(layer, (Model, Sequential)):\n+                    # We were passed a model as first layer.\n+                    # This requires a specific way to figure out the\n+                    # input shape and dtype.\n+                    if not layer.layers:\n+                        raise ValueError('Cannot add an empty model '\n+                                         'to a `Sequential` model.')\n+                    # In case of nested models: recover the first layer\n+                    # of the deepest model to infer input shape and dtype.\n+                    first_layer = layer.layers[0]\n+                    while isinstance(first_layer, (Model, Sequential)):\n+                        first_layer = first_layer.layers[0]\n+                    batch_shape = first_layer.batch_input_shape\n+                    dtype = first_layer.dtype\n+                else:\n+                    # We were passed a regular layer, and it should\n+                    # know about its input shape. Otherwise, that's an error.\n+                    if not hasattr(layer, 'batch_input_shape'):\n+                        raise ValueError('The first layer in a '\n+                                         'Sequential model must '\n+                                         'get an `input_shape` or '\n+                                         '`batch_input_shape` argument.')\n+                    batch_shape = layer.batch_input_shape\n+                    dtype = layer.dtype\n+                # Instantiate the input layer.\n+                x = Input(batch_shape=batch_shape,\n+                          dtype=dtype,\n+                          name=layer.name + '_input')\n+                # This will build the current layer\n+                # and create the node connecting the current layer\n+                # to the input layer we just created.\n+                layer(x)\n+\n+            if len(layer._inbound_nodes[-1].output_tensors) != 1:\n+                raise ValueError('All layers in a Sequential model '\n+                                 'should have a single output tensor. '\n+                                 'For multi-output layers, '\n+                                 'use the functional API.')\n+\n+            self.outputs = [layer._inbound_nodes[-1].output_tensors[0]]\n+            self.inputs = network.get_source_inputs(self.outputs[0])\n+\n+            # We create an input node, which we will keep updated\n+            # as we add more layers\n+            base_layer.Node(outbound_layer=self,\n+                            inbound_layers=[],\n+                            node_indices=[],\n+                            tensor_indices=[],\n+                            input_tensors=self.inputs,\n+                            output_tensors=self.outputs,\n+                            # no model-level masking for now\n+                            input_masks=[None for _ in self.inputs],\n+                            output_masks=[None],\n+                            input_shapes=[x._keras_shape for x in self.inputs],\n+                            output_shapes=[self.outputs[0]._keras_shape])\n+        else:\n+            output_tensor = layer(self.outputs[0])\n+            if isinstance(output_tensor, list):\n+                raise TypeError('All layers in a Sequential model '\n+                                'should have a single output tensor. '\n+                                'For multi-output layers, '\n+                                'use the functional API.')\n+            self.outputs = [output_tensor]\n+            # update self._inbound_nodes\n+            self._inbound_nodes[0].output_tensors = self.outputs\n+            self._inbound_nodes[0].output_shapes = [\n+                self.outputs[0]._keras_shape]\n+\n+        self.layers.append(layer)\n+        self.built = False\n+\n+    def pop(self):\n+        \"\"\"Removes the last layer in the model.\n+\n+        # Raises\n+            TypeError: if there are no layers in the model.\n+        \"\"\"\n+        if not self.layers:\n+            raise TypeError('There are no layers in the model.')\n+\n+        self.layers.pop()\n+        if not self.layers:\n+            self.outputs = []\n+            self._inbound_nodes = []\n+            self._outbound_nodes = []\n+        else:\n+            self.layers[-1]._outbound_nodes = []\n+            self.outputs = [self.layers[-1].output]\n+            # update self._inbound_nodes\n+            self._inbound_nodes[0].output_tensors = self.outputs\n+            self._inbound_nodes[0].output_shapes = [\n+                self.outputs[0]._keras_shape]\n+        self.built = False\n+\n+    def get_layer(self, name=None, index=None):\n+        \"\"\"Retrieve a layer that is part of the model.\n+\n+        Returns a layer based on either its name (unique)\n+        or its index in the graph. Indices are based on\n+        order of horizontal graph traversal (bottom-up).\n+\n+        # Arguments\n+            name: string, name of layer.\n+            index: integer, index of layer.\n+\n+        # Returns\n+            A layer instance.\n+        \"\"\"\n+        if not self.built:\n+            self.build()\n+        return self.model.get_layer(name, index)\n+\n+    def call(self, inputs, mask=None):\n+        if not self.built:\n+            self.build()\n+        return self.model.call(inputs, mask)\n+\n+    def build(self, input_shape=None):\n+        if not self.inputs or not self.outputs:\n+            raise TypeError('Sequential model cannot be built: model is empty.'\n+                            ' Add some layers first.')\n+        # actually create the model\n+        self.model = Model(self.inputs, self.outputs[0],\n+                           name=self.name + '_model')\n+        self.model.trainable = self.trainable\n+\n+        # mirror model attributes\n+        self.supports_masking = self.model.supports_masking\n+        self._output_mask_cache = self.model._output_mask_cache\n+        self._output_tensor_cache = self.model._output_tensor_cache\n+        self._output_shape_cache = self.model._output_shape_cache\n+        self.input_layers = self.model.input_layers\n+        self.input_layers_node_indices = self.model.input_layers_node_indices\n+        self.input_layers_tensor_indices = self.model.input_layers_tensor_indices\n+        self.output_layers = self.model.output_layers\n+        self.output_layers_node_indices = self.model.output_layers_node_indices\n+        self.output_layers_tensor_indices = self.model.output_layers_tensor_indices\n+        self._nodes_by_depth = self.model._nodes_by_depth\n+        self.output_names = self.model.output_names\n+        self.input_names = self.model.input_names\n+        self._feed_input_names = self.model._feed_input_names\n+        self._feed_inputs = self.model._feed_inputs\n+        self._network_nodes = self.model._network_nodes\n+\n+        # Make sure child model callbacks\n+        # will call the parent Sequential model.\n+        self.model.callback_model = self\n+        self.built = True\n+\n+    @property\n+    def uses_learning_phase(self):\n+        if not self.built:\n+            self.build()\n+        return self.model.uses_learning_phase\n+\n+    @property\n+    def _flattened_layers(self):\n+        layers = []\n+        if self.layers:\n+            # Support for legacy models\n+            if isinstance(self.layers[0], legacy_layers.Merge):\n+                merge = self.layers[0]\n+                for layer in merge.layers:\n+                    if hasattr(layer, '_flattened_layers'):\n+                        for sublayer in layer._flattened_layers:\n+                            if sublayer not in layers:\n+                                layers.append(sublayer)\n+                    elif hasattr(layer, 'layers'):\n+                        for sublayer in layer.layers:\n+                            if sublayer not in layers:\n+                                layers.append(sublayer)\n+                    else:\n+                        if layer not in layers:\n+                            layers.append(layer)\n+            else:\n+                if self.layers[0] not in layers:\n+                    layers.append(self.layers[0])\n+            for layer in self.layers[1:]:\n+                if layer not in layers:\n+                    layers.append(layer)\n+        return layers\n+\n+    def _gather_list_attr(self, attr):\n+        all_attrs = []\n+        for layer in self._flattened_layers:\n+            all_attrs += getattr(layer, attr, [])\n+        return all_attrs\n+\n+    @property\n+    def trainable(self):\n+        return self._trainable\n+\n+    @trainable.setter\n+    def trainable(self, value):\n+        if self.built:\n+            self.model.trainable = value\n+        self._trainable = value\n+\n+    @property\n+    def trainable_weights(self):\n+        if not self.trainable:\n+            return []\n+        # Support for legacy behavior\n+        return self._gather_list_attr('trainable_weights')\n+\n+    @property\n+    def non_trainable_weights(self):\n+        # Support for legacy behavior\n+        weights = self._gather_list_attr('non_trainable_weights')\n+        if not self.trainable:\n+            trainable_weights = self._gather_list_attr('trainable_weights')\n+            return trainable_weights + weights\n+        return weights\n+\n+    @property\n+    def updates(self):\n+        if not self.built:\n+            self.build()\n+        return self.model.updates\n+\n+    @property\n+    def state_updates(self):\n+        if not self.built:\n+            self.build()\n+        return self.model.state_updates\n+\n+    def get_updates_for(self, inputs):\n+        if not self.built:\n+            self.build()\n+        return self.model.get_updates_for(inputs)\n+\n+    @property\n+    def losses(self):\n+        if not self.built:\n+            self.build()\n+        return self.model.losses\n+\n+    def get_losses_for(self, inputs):\n+        if not self.built:\n+            self.build()\n+        return self.model.get_losses_for(inputs)\n+\n+    @property\n+    def regularizers(self):\n+        if not self.built:\n+            self.build()\n+        return self.model.regularizers\n+\n+    def get_weights(self):\n+        \"\"\"Retrieves the weights of the model.\n+\n+        # Returns\n+            A flat list of Numpy arrays\n+            (one array per model weight).\n+        \"\"\"\n+        # Legacy support\n+        if legacy_models.needs_legacy_support(self):\n+            layers = legacy_models.legacy_sequential_layers(self)\n+            weights = []\n+            for layer in layers:\n+                weights.append(layer.get_weights())\n+            return weights\n+\n+        if not self.built:\n+            self.build()\n+        return self.model.get_weights()\n+\n+    def set_weights(self, weights):\n+        \"\"\"Sets the weights of the model.\n+\n+        # Arguments\n+            weights: Should be a list\n+                of Numpy arrays with shapes and types matching\n+                the output of `model.get_weights()`.\n+        \"\"\"\n+        # Legacy support\n+        if legacy_models.needs_legacy_support(self):\n+            layers = legacy_models.legacy_sequential_layers(self)\n+            for layer in layers:\n+                nb_param = len(layer.weights)\n+                layer.set_weights(weights[:nb_param])\n+                weights = weights[nb_param:]\n+\n+        if not self.built:\n+            self.build()\n+        self.model.set_weights(weights)\n+\n+    def load_weights(self, filepath, by_name=False, skip_mismatch=False, reshape=False):\n+        if h5py is None:\n+            raise ImportError('`load_weights` requires h5py.')\n+        with h5py.File(filepath, mode='r') as f:\n+            if 'layer_names' not in f.attrs and 'model_weights' in f:\n+                f = f['model_weights']\n+\n+            # Legacy support\n+            if legacy_models.needs_legacy_support(self):\n+                layers = legacy_models.legacy_sequential_layers(self)\n+            else:\n+                layers = self.layers\n+            if by_name:\n+                saving.load_weights_from_hdf5_group_by_name(f, layers,\n+                                                            skip_mismatch=skip_mismatch,\n+                                                            reshape=reshape)\n+            else:\n+                saving.load_weights_from_hdf5_group(f, layers, reshape=reshape)\n+\n+    def save_weights(self, filepath, overwrite=True):\n+        \"\"\"Saves the weights of a model.\n+\n+        Note: Please also see\n+        [How can I install HDF5 or h5py to save my models in Keras?](\n+        /getting-started/faq/\n+        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)\n+        in the FAQ for instructions on how to install `h5py`.\n+        \"\"\"\n+\n+        if h5py is None:\n+            raise ImportError('`save_weights` requires h5py.')\n+        # If file exists and should not be overwritten:\n+        if not overwrite and os.path.isfile(filepath):\n+            proceed = ask_to_proceed_with_overwrite(filepath)\n+            if not proceed:\n+                return\n+        # Legacy support\n+        if legacy_models.needs_legacy_support(self):\n+            layers = legacy_models.legacy_sequential_layers(self)\n+        else:\n+            layers = self.layers\n+\n+        with h5py.File(filepath, 'w') as f:\n+            saving.save_weights_to_hdf5_group(f, layers)\n+            f.flush()\n+\n+    def compile(self, optimizer, loss,\n+                metrics=None,\n+                sample_weight_mode=None,\n+                weighted_metrics=None,\n+                target_tensors=None,\n+                **kwargs):\n+        \"\"\"Configures the model for training.\n+\n+        # Arguments\n+            optimizer: String (name of optimizer) or optimizer object.\n+                See [optimizers](/optimizers).\n+            loss: String (name of objective function) or objective function.\n+                See [losses](/losses).\n+                If the model has multiple outputs, you can use a different loss\n+                on each output by passing a dictionary or a list of losses.\n+                The loss value that will be minimized by the model\n+                will then be the sum of all individual losses.\n+            metrics: List of metrics to be evaluated by the model\n+                during training and testing.\n+                Typically you will use `metrics=['accuracy']`.\n+                To specify different metrics for different outputs of a\n+                multi-output model, you could also pass a dictionary,\n+                such as `metrics={'output_a': 'accuracy'}`.\n+            sample_weight_mode: If you need to do timestep-wise\n+                sample weighting (2D weights), set this to `\"temporal\"`.\n+                `None` defaults to sample-wise weights (1D).\n+                If the model has multiple outputs, you can use a different\n+                `sample_weight_mode` on each output by passing a\n+                dictionary or a list of modes.\n+            weighted_metrics: List of metrics to be evaluated and weighted\n+                by sample_weight or class_weight during training and testing.\n+            target_tensors: By default, Keras will create a placeholder for the\n+                model's target, which will be fed with the target data during\n+                training. If instead you would like to use your own\n+                target tensor (in turn, Keras will not expect external\n+                Numpy data for these targets at training time), you\n+                can specify them via the `target_tensors` argument.\n+                It should be a single tensor\n+                (for a single-output `Sequential` model).\n+            **kwargs: When using the Theano/CNTK backends, these arguments\n+                are passed into `K.function`.\n+                When using the TensorFlow backend,\n+                these arguments are passed into `tf.Session.run`.\n+\n+        # Raises\n+            ValueError: In case of invalid arguments for\n+                `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n+\n+        # Example\n+            ```python\n+                model = Sequential()\n+                model.add(Dense(32, input_shape=(500,)))\n+                model.add(Dense(10, activation='softmax'))\n+                model.compile(optimizer='rmsprop',\n+                              loss='categorical_crossentropy',\n+                              metrics=['accuracy'])\n+            ```\n+        \"\"\"\n+        # create the underlying model\n+        self.build()\n+        # call compile method of Model class\n+        self.model.compile(optimizer, loss,\n+                           metrics=metrics,\n+                           sample_weight_mode=sample_weight_mode,\n+                           weighted_metrics=weighted_metrics,\n+                           target_tensors=target_tensors,\n+                           **kwargs)\n+        self.optimizer = self.model.optimizer\n+        self.loss = self.model.loss\n+        self.metrics = self.model.metrics\n+        self.loss_weights = self.model.loss_weights\n+        self.sample_weight_mode = self.model.sample_weight_mode\n+        self.weighted_metrics = self.model.weighted_metrics\n+        self.targets = self.model.targets\n+        self.metrics_tensors = self.model.metrics_tensors\n+        self.metrics_names = self.model.metrics_names\n+        self.sample_weights = self.model.sample_weights\n+        self.total_loss = self.model.total_loss\n+\n+    def fit(self,\n+            x=None,\n+            y=None,\n+            batch_size=None,\n+            epochs=1,\n+            verbose=1,\n+            callbacks=None,\n+            validation_split=0.,\n+            validation_data=None,\n+            shuffle=True,\n+            class_weight=None,\n+            sample_weight=None,\n+            initial_epoch=0,\n+            steps_per_epoch=None,\n+            validation_steps=None,\n+            **kwargs):\n+        \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n+\n+        # Arguments\n+            x: Numpy array of training data.\n+                If the input layer in the model is named, you can also pass a\n+                dictionary mapping the input name to a Numpy array.\n+                `x` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n+            y: Numpy array of target (label) data.\n+                If the output layer in the model is named, you can also pass a\n+                dictionary mapping the output name to a Numpy array.\n+                `y` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n+            batch_size: Integer or `None`.\n+                Number of samples per gradient update.\n+                If unspecified, it will default to 32.\n+            epochs: Integer. Number of epochs to train the model.\n+                An epoch is an iteration over the entire `x` and `y`\n+                data provided.\n+                Note that in conjunction with `initial_epoch`,\n+                `epochs` is to be understood as \"final epoch\".\n+                The model is not trained for a number of iterations\n+                given by `epochs`, but merely until the epoch\n+                of index `epochs` is reached.\n+            verbose: 0, 1, or 2. Verbosity mode.\n+                0 = silent, 1 = progress bar, 2 = one line per epoch.\n+            callbacks: List of `keras.callbacks.Callback` instances.\n+                List of callbacks to apply during training.\n+                See [callbacks](/callbacks).\n+            validation_split: Float between 0 and 1.\n+                Fraction of the training data to be used as validation data.\n+                The model will set apart this fraction of the training data,\n+                will not train on it, and will evaluate\n+                the loss and any model metrics\n+                on this data at the end of each epoch.\n+                The validation data is selected from the last samples\n+                in the `x` and `y` data provided, before shuffling.\n+            validation_data: tuple `(x_val, y_val)` or tuple\n+                `(x_val, y_val, val_sample_weights)` on which to evaluate\n+                the loss and any model metrics at the end of each epoch.\n+                The model will not be trained on this data.\n+                This will override `validation_split`.\n+            shuffle: Boolean (whether to shuffle the training data\n+                before each epoch) or str (for 'batch').\n+                'batch' is a special option for dealing with the\n+                limitations of HDF5 data; it shuffles in batch-sized chunks.\n+                Has no effect when `steps_per_epoch` is not `None`.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) value, used for weighting the loss function\n+                (during training only).\n+                This can be useful to tell the model to\n+                \"pay more attention\" to samples from\n+                an under-represented class.\n+            sample_weight: Optional Numpy array of weights for\n+                the training samples, used for weighting the loss function\n+                (during training only). You can either pass a flat (1D)\n+                Numpy array with the same length as the input samples\n+                (1:1 mapping between weights and samples),\n+                or in the case of temporal data,\n+                you can pass a 2D array with shape\n+                `(samples, sequence_length)`,\n+                to apply a different weight to every timestep of every sample.\n+                In this case you should make sure to specify\n+                `sample_weight_mode=\"temporal\"` in `compile()`.\n+            initial_epoch: Epoch at which to start training\n+                (useful for resuming a previous training run).\n+            steps_per_epoch: Total number of steps (batches of samples)\n+                before declaring one epoch finished and starting the\n+                next epoch. When training with input tensors such as\n+                TensorFlow data tensors, the default `None` is equal to\n+                the number of samples in your dataset divided by\n+                the batch size, or 1 if that cannot be determined.\n+            validation_steps: Only relevant if `steps_per_epoch`\n+                is specified. Total number of steps (batches of samples)\n+                to validate before stopping.\n+\n+        # Returns\n+            A `History` object. Its `History.history` attribute is\n+            a record of training loss values and metrics values\n+            at successive epochs, as well as validation loss values\n+            and validation metrics values (if applicable).\n+\n+        # Raises\n+            RuntimeError: If the model was never compiled.\n+            ValueError: In case of mismatch between the provided input data\n+                and what the model expects.\n+        \"\"\"\n+        # Legacy support\n+        if 'nb_epoch' in kwargs:\n+            warnings.warn('The `nb_epoch` argument in `fit` '\n+                          'has been renamed `epochs`.')\n+            epochs = kwargs.pop('nb_epoch')\n+        if kwargs:\n+            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n+\n+        if not self.built:\n+            raise RuntimeError('The model needs to be compiled '\n+                               'before being used.')\n+        return self.model.fit(x, y,\n+                              batch_size=batch_size,\n+                              epochs=epochs,\n+                              verbose=verbose,\n+                              callbacks=callbacks,\n+                              validation_split=validation_split,\n+                              validation_data=validation_data,\n+                              shuffle=shuffle,\n+                              class_weight=class_weight,\n+                              sample_weight=sample_weight,\n+                              initial_epoch=initial_epoch,\n+                              steps_per_epoch=steps_per_epoch,\n+                              validation_steps=validation_steps)\n+\n+    def evaluate(self, x=None, y=None,\n+                 batch_size=None,\n+                 verbose=1,\n+                 sample_weight=None,\n+                 steps=None):\n+        \"\"\"Computes the loss on some input data, batch by batch.\n+\n+        # Arguments\n+            x: input data, as a Numpy array or list of Numpy arrays\n+                (if the model has multiple inputs).\n+                `x` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n+            y: labels, as a Numpy array.\n+                `y` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n+            batch_size: Integer. If unspecified, it will default to 32.\n+            verbose: verbosity mode, 0 or 1.\n+            sample_weight: sample weights, as a Numpy array.\n+            steps: Integer or `None`.\n+                Total number of steps (batches of samples)\n+                before declaring the evaluation round finished.\n+                Ignored with the default value of `None`.\n+\n+        # Returns\n+            Scalar test loss (if the model has no metrics)\n+            or list of scalars (if the model computes other metrics).\n+            The attribute `model.metrics_names` will give you\n+            the display labels for the scalar outputs.\n+\n+        # Raises\n+            RuntimeError: if the model was never compiled.\n+        \"\"\"\n+        if not self.built:\n+            raise RuntimeError('The model needs to be compiled '\n+                               'before being used.')\n+        return self.model.evaluate(x, y,\n+                                   batch_size=batch_size,\n+                                   verbose=verbose,\n+                                   sample_weight=sample_weight,\n+                                   steps=steps)\n+\n+    def predict(self, x, batch_size=None, verbose=0, steps=None):\n+        \"\"\"Generates output predictions for the input samples.\n+\n+        The input samples are processed batch by batch.\n+\n+        # Arguments\n+            x: the input data, as a Numpy array.\n+            batch_size: Integer. If unspecified, it will default to 32.\n+            verbose: verbosity mode, 0 or 1.\n+            steps: Total number of steps (batches of samples)\n+                before declaring the prediction round finished.\n+                Ignored with the default value of `None`.\n+\n+        # Returns\n+            A Numpy array of predictions.\n+        \"\"\"\n+        if not self.built:\n+            self.build()\n+        return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n+                                  steps=steps)\n+\n+    def predict_on_batch(self, x):\n+        \"\"\"Returns predictions for a single batch of samples.\n+\n+        # Arguments\n+            x: input data, as a Numpy array or list of Numpy arrays\n+                (if the model has multiple inputs).\n+\n+        # Returns\n+            A Numpy array of predictions.\n+        \"\"\"\n+        if not self.built:\n+            self.build()\n+        return self.model.predict_on_batch(x)\n+\n+    def train_on_batch(self, x, y, class_weight=None,\n+                       sample_weight=None):\n+        \"\"\"Single gradient update over one batch of samples.\n+\n+        # Arguments\n+            x: input data, as a Numpy array or list of Numpy arrays\n+                (if the model has multiple inputs).\n+            y: labels, as a Numpy array.\n+            class_weight: dictionary mapping classes to a weight value,\n+                used for scaling the loss function (during training only).\n+            sample_weight: sample weights, as a Numpy array.\n+\n+        # Returns\n+            Scalar training loss (if the model has no metrics)\n+            or list of scalars (if the model computes other metrics).\n+            The attribute `model.metrics_names` will give you\n+            the display labels for the scalar outputs.\n+\n+        # Raises\n+            RuntimeError: if the model was never compiled.\n+        \"\"\"\n+        if not self.built:\n+            raise RuntimeError('The model needs to be compiled '\n+                               'before being used.')\n+        return self.model.train_on_batch(x, y,\n+                                         sample_weight=sample_weight,\n+                                         class_weight=class_weight)\n+\n+    def test_on_batch(self, x, y,\n+                      sample_weight=None):\n+        \"\"\"Evaluates the model over a single batch of samples.\n+\n+        # Arguments\n+            x: input data, as a Numpy array or list of Numpy arrays\n+                (if the model has multiple inputs).\n+            y: labels, as a Numpy array.\n+            sample_weight: sample weights, as a Numpy array.\n+\n+        # Returns\n+            Scalar test loss (if the model has no metrics)\n+            or list of scalars (if the model computes other metrics).\n+            The attribute `model.metrics_names` will give you\n+            the display labels for the scalar outputs.\n+\n+        # Raises\n+            RuntimeError: if the model was never compiled.\n+        \"\"\"\n+        if not self.built:\n+            raise RuntimeError('The model needs to be compiled '\n+                               'before being used.')\n+        return self.model.test_on_batch(x, y,\n+                                        sample_weight=sample_weight)\n+\n+    def predict_proba(self, x, batch_size=None, verbose=0, steps=None):\n+        \"\"\"Generates class probability predictions for the input samples.\n+\n+        The input samples are processed batch by batch.\n+\n+        # Arguments\n+            x: input data, as a Numpy array or list of Numpy arrays\n+                (if the model has multiple inputs).\n+            batch_size: Integer. If unspecified, it will default to 32.\n+            verbose: verbosity mode, 0 or 1.\n+            steps: Total number of steps (batches of samples)\n+                before declaring the prediction round finished.\n+                Ignored with the default value of `None`.\n+\n+\n+        # Returns\n+            A Numpy array of probability predictions.\n+        \"\"\"\n+        preds = self.predict(x, batch_size, verbose, steps=steps)\n+        if preds.min() < 0. or preds.max() > 1.:\n+            warnings.warn('Network returning invalid probability values. '\n+                          'The last layer might not normalize predictions '\n+                          'into probabilities '\n+                          '(like softmax or sigmoid would).')\n+        return preds\n+\n+    def predict_classes(self, x, batch_size=None, verbose=0, steps=None):\n+        \"\"\"Generate class predictions for the input samples.\n+\n+        The input samples are processed batch by batch.\n+\n+        # Arguments\n+            x: input data, as a Numpy array or list of Numpy arrays\n+                (if the model has multiple inputs).\n+            batch_size: Integer. If unspecified, it will default to 32.\n+            verbose: verbosity mode, 0 or 1.\n+            steps: Total number of steps (batches of samples)\n+                before declaring the prediction round finished.\n+                Ignored with the default value of `None`.\n+\n+        # Returns\n+            A numpy array of class predictions.\n+        \"\"\"\n+        proba = self.predict(x, batch_size=batch_size, verbose=verbose,\n+                             steps=steps)\n+        if proba.shape[-1] > 1:\n+            return proba.argmax(axis=-1)\n+        else:\n+            return (proba > 0.5).astype('int32')\n+\n+    @interfaces.legacy_generator_methods_support\n+    def fit_generator(self,\n+                      generator,\n+                      steps_per_epoch=None,\n+                      epochs=1,\n+                      verbose=1,\n+                      callbacks=None,\n+                      validation_data=None,\n+                      validation_steps=None,\n+                      class_weight=None,\n+                      max_queue_size=10,\n+                      workers=1,\n+                      use_multiprocessing=False,\n+                      shuffle=True,\n+                      initial_epoch=0):\n+        \"\"\"Fits the model on data generated batch-by-batch by a Python generator.\n+\n+        The generator is run in parallel to the model, for efficiency.\n+        For instance, this allows you to do real-time data augmentation\n+        on images on CPU in parallel to training your model on GPU.\n+\n+        The use of `keras.utils.Sequence` guarantees the ordering\n+        and guarantees the single use of every input per epoch when\n+        using `use_multiprocessing=True`.\n+\n+        # Arguments\n+            generator: A generator or an instance of `Sequence`\n+                (`keras.utils.Sequence`) object in order to avoid duplicate data\n+                    when using multiprocessing.\n+                The output of the generator must be either\n+                - a tuple `(inputs, targets)`\n+                - a tuple `(inputs, targets, sample_weights)`.\n+                This tuple (a single output of the generator) makes a single\n+                batch. Therefore, all arrays in this tuple must have the same\n+                length (equal to the size of this batch). Different batches may\n+                have different sizes. For example, the last batch of the epoch\n+                is commonly smaller than the others, if the size of the dataset\n+                is not divisible by the batch size.\n+                The generator is expected to loop over its data\n+                indefinitely. An epoch finishes when `steps_per_epoch`\n+                batches have been seen by the model.\n+            steps_per_epoch: Total number of steps (batches of samples)\n+                to yield from `generator` before declaring one epoch\n+                finished and starting the next epoch. It should typically\n+                be equal to the number of samples of your dataset\n+                divided by the batch size.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(generator)` as a number of steps.\n+            epochs: Integer, total number of iterations on the data.\n+                Note that in conjunction with initial_epoch, the parameter\n+                epochs is to be understood as \"final epoch\". The model is\n+                not trained for n steps given by epochs, but until the\n+                epoch epochs is reached.\n+            verbose: Integer. 0, 1, or 2. Verbosity mode.\n+                0 = silent, 1 = progress bar, 2 = one line per epoch.\n+            callbacks: List of `keras.callbacks.Callback` instances.\n+                List of callbacks to apply during training.\n+                See [callbacks](/callbacks).\n+            validation_data: This can be either\n+                - a generator for the validation data\n+                - a tuple `(inputs, targets)`\n+                - a tuple `(inputs, targets, sample_weights)`.\n+            validation_steps: Only relevant if `validation_data`\n+                is a generator. Total number of steps (batches of samples)\n+                to yield from `validation_data` generator before stopping\n+                at the end of every epoch. It should typically\n+                be equal to the number of samples of your\n+                validation dataset divided by the batch size.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(validation_data)` as a number of steps.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) value, used for weighting the loss function\n+                (during training only). This can be useful to tell the model to\n+                \"pay more attention\" to samples from an under-represented class.\n+            max_queue_size: Integer. Maximum size for the generator queue.\n+                If unspecified, `max_queue_size` will default to 10.\n+            workers: Integer. Maximum number of processes to spin up\n+                when using process-based threading.\n+                If unspecified, `workers` will default to 1. If 0, will\n+                execute the generator on the main thread.\n+            use_multiprocessing: Boolean.\n+                If `True`, use process-based threading.\n+                If unspecified, `use_multiprocessing` will default to `False`.\n+                Note that because this implementation relies on multiprocessing,\n+                you should not pass non-picklable arguments to the generator\n+                as they can't be passed easily to children processes.\n+            shuffle: Boolean (whether to shuffle the order of the batches at\n+                the beginning of each epoch. Only used with instances\n+                of `Sequence` (`keras.utils.Sequence`).\n+                Has no effect when `steps_per_epoch` is not `None`.\n+            initial_epoch: Integer.\n+                Epoch at which to start training\n+                (useful for resuming a previous training run).\n+\n+        # Returns\n+            A `History` object.\n+\n+        # Raises\n+            RuntimeError: if the model was never compiled.\n+            ValueError: In case the generator yields data in an invalid format.\n+\n+        # Example\n+\n+        ```python\n+            def generate_arrays_from_file(path):\n+                while True:\n+                    with open(path) as f:\n+                        for line in f:\n+                            # create Numpy arrays of input data\n+                            # and labels, from each line in the file\n+                            x, y = process_line(line)\n+                            yield (x, y)\n+\n+            model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n+                                steps_per_epoch=1000, epochs=10)\n+        ```\n+        \"\"\"\n+        if not self.built:\n+            raise RuntimeError('The model needs to be compiled '\n+                               'before being used.')\n+        return self.model.fit_generator(generator,\n+                                        steps_per_epoch,\n+                                        epochs,\n+                                        verbose=verbose,\n+                                        callbacks=callbacks,\n+                                        validation_data=validation_data,\n+                                        validation_steps=validation_steps,\n+                                        class_weight=class_weight,\n+                                        max_queue_size=max_queue_size,\n+                                        workers=workers,\n+                                        use_multiprocessing=use_multiprocessing,\n+                                        shuffle=shuffle,\n+                                        initial_epoch=initial_epoch)\n+\n+    @interfaces.legacy_generator_methods_support\n+    def evaluate_generator(self, generator, steps=None,\n+                           max_queue_size=10, workers=1,\n+                           use_multiprocessing=False):\n+        \"\"\"Evaluates the model on a data generator.\n+\n+        The generator should return the same kind of data\n+        as accepted by `test_on_batch`.\n+\n+        # Arguments\n+            generator: Generator yielding tuples (inputs, targets)\n+                or (inputs, targets, sample_weights)\n+            steps: Total number of steps (batches of samples)\n+                to yield from `generator` before stopping.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(generator)` as a number of steps.\n+            max_queue_size: maximum size for the generator queue\n+            workers: maximum number of processes to spin up\n+            use_multiprocessing: if True, use process based threading.\n+                Note that because this implementation\n+                relies on multiprocessing, you should not pass\n+                non picklable arguments to the generator\n+                as they can't be passed easily to children processes.\n+\n+        # Returns\n+            Scalar test loss (if the model has no metrics)\n+            or list of scalars (if the model computes other metrics).\n+            The attribute `model.metrics_names` will give you\n+            the display labels for the scalar outputs.\n+\n+        # Raises\n+            RuntimeError: if the model was never compiled.\n+        \"\"\"\n+        if not self.built:\n+            raise RuntimeError('The model needs to be compiled '\n+                               'before being used.')\n+        return self.model.evaluate_generator(generator,\n+                                             steps,\n+                                             max_queue_size=max_queue_size,\n+                                             workers=workers,\n+                                             use_multiprocessing=use_multiprocessing)\n+\n+    @interfaces.legacy_generator_methods_support\n+    def predict_generator(self, generator, steps=None,\n+                          max_queue_size=10, workers=1,\n+                          use_multiprocessing=False, verbose=0):\n+        \"\"\"Generates predictions for the input samples from a data generator.\n+\n+        The generator should return the same kind of data as accepted by\n+        `predict_on_batch`.\n+\n+        # Arguments\n+            generator: generator yielding batches of input samples.\n+            steps: Total number of steps (batches of samples)\n+                to yield from `generator` before stopping.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(generator)` as a number of steps.\n+            max_queue_size: maximum size for the generator queue\n+            workers: maximum number of processes to spin up\n+            use_multiprocessing: if True, use process based threading.\n+                Note that because this implementation\n+                relies on multiprocessing, you should not pass\n+                non picklable arguments to the generator\n+                as they can't be passed easily to children processes.\n+            verbose: verbosity mode, 0 or 1.\n+\n+        # Returns\n+            A Numpy array of predictions.\n+        \"\"\"\n+        if not self.built:\n+            self.build()\n+        return self.model.predict_generator(generator, steps,\n+                                            max_queue_size=max_queue_size,\n+                                            workers=workers,\n+                                            use_multiprocessing=use_multiprocessing,\n+                                            verbose=verbose)\n+\n+    def get_config(self):\n+        if isinstance(self.layers[0], legacy_layers.Merge):\n+            return self.legacy_get_config()\n+\n+        config = []\n+        for layer in self.layers:\n+            config.append({'class_name': layer.__class__.__name__,\n+                           'config': layer.get_config()})\n+        return copy.deepcopy(config)\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        if 'class_name' not in config[0] or config[0]['class_name'] == 'Merge':\n+            return cls.legacy_from_config(config)\n+\n+        model = cls()\n+        for conf in config:\n+            layer = layer_module.deserialize(conf, custom_objects=custom_objects)\n+            model.add(layer)\n+        return model\n+\n+    def legacy_get_config(self):\n+        \"\"\"Retrieves the model configuration as a Python list.\n+\n+        # Returns\n+            A list of dicts (each dict is a layer config).\n+        \"\"\"\n+        config = []\n+        if isinstance(self.layers[0], legacy_layers.Merge):\n+            assert hasattr(self.layers[0], 'layers')\n+            layers = []\n+            for layer in self.layers[0].layers:\n+                layer_config = {'class_name': layer.__class__.__name__,\n+                                'config': layer.get_config()}\n+                layers.append(layer_config)\n+            merge_config = self.layers[0].get_config()\n+            merge_config['layers'] = layers\n+            config.append({'class_name': 'Merge', 'config': merge_config})\n+        else:\n+            config.append({'class_name': self.layers[0].__class__.__name__,\n+                           'config': self.layers[0].get_config()})\n+        for layer in self.layers[1:]:\n+            config.append({'class_name': layer.__class__.__name__,\n+                           'config': layer.get_config()})\n+        return copy.deepcopy(config)\n+\n+    @classmethod\n+    def legacy_from_config(cls, config, layer_cache=None):\n+        \"\"\"Load a model from a legacy configuration.\n+\n+        # Arguments\n+            config: dictionary with configuration.\n+            layer_cache: cache to draw pre-existing layer.\n+\n+        # Returns\n+            The loaded Model.\n+        \"\"\"\n+        if not layer_cache:\n+            layer_cache = {}\n+\n+        def normalize_legacy_config(conf):\n+            if 'class_name' not in conf:\n+                class_name = conf['name']\n+                name = conf.get('custom_name')\n+                conf['name'] = name\n+                return {'class_name': class_name,\n+                        'config': conf}\n+            return conf\n+\n+        # the model we will return\n+        model = cls()\n+\n+        def get_or_create_layer(layer_data):\n+            name = layer_data['config'].get('name')\n+            if name in layer_cache:\n+                return layer_cache[name]\n+            layer = layer_module.deserialize(layer_data)\n+            layer_cache[name] = layer\n+            return layer\n+\n+        first_layer = config[0]\n+        first_layer = normalize_legacy_config(first_layer)\n+        if first_layer['class_name'] == 'Merge':\n+            merge_inputs = []\n+            first_layer_config = first_layer['config']\n+            for merge_input_config in first_layer_config.pop('layers'):\n+                merge_input = layer_module.deserialize(merge_input_config)\n+                merge_inputs.append(merge_input)\n+            first_layer_config['layers'] = merge_inputs\n+            merge = legacy_layers.Merge.from_config(first_layer_config)\n+            model.add(merge)\n+        else:\n+            layer = get_or_create_layer(first_layer)\n+            model.add(layer)\n+\n+        for conf in config[1:]:\n+            conf = normalize_legacy_config(conf)\n+            layer = get_or_create_layer(conf)\n+            model.add(layer)\n+        return model\n\n@@ -1,3479 +1,5 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"Topology-related part of the Keras engine.\n+\"\"\"This module is deprecated, but kept around for backwards compatibility.\n \"\"\"\n-from __future__ import print_function\n-from __future__ import absolute_import\n-from __future__ import division\n-\n-import numpy as np\n-import json\n-import yaml\n-import warnings\n-import copy\n-import os\n-import re\n-from six.moves import zip\n-\n-from .. import backend as K\n-from .. import initializers\n-from ..utils.io_utils import ask_to_proceed_with_overwrite\n-from ..utils.layer_utils import print_summary as print_layer_summary\n-from ..utils.layer_utils import count_params\n-from ..utils.generic_utils import has_arg\n-from ..utils import conv_utils\n-from ..legacy import interfaces\n-\n-try:\n-    import h5py\n-    HDF5_OBJECT_HEADER_LIMIT = 64512\n-except ImportError:\n-    h5py = None\n-\n-\n-class InputSpec(object):\n-    \"\"\"Specifies the ndim, dtype and shape of every input to a layer.\n-\n-    Every layer should expose (if appropriate) an `input_spec` attribute:\n-    a list of instances of InputSpec (one per input tensor).\n-\n-    A None entry in a shape is compatible with any dimension,\n-    a None shape is compatible with any shape.\n-\n-    # Arguments\n-        dtype: Expected datatype of the input.\n-        shape: Shape tuple, expected shape of the input\n-            (may include None for unchecked axes).\n-        ndim: Integer, expected rank of the input.\n-        max_ndim: Integer, maximum rank of the input.\n-        min_ndim: Integer, minimum rank of the input.\n-        axes: Dictionary mapping integer axes to\n-            a specific dimension value.\n-    \"\"\"\n-\n-    def __init__(self, dtype=None,\n-                 shape=None,\n-                 ndim=None,\n-                 max_ndim=None,\n-                 min_ndim=None,\n-                 axes=None):\n-        self.dtype = dtype\n-        self.shape = shape\n-        if shape is not None:\n-            self.ndim = len(shape)\n-        else:\n-            self.ndim = ndim\n-        self.max_ndim = max_ndim\n-        self.min_ndim = min_ndim\n-        self.axes = axes or {}\n-\n-    def __repr__(self):\n-        spec = [('dtype=' + str(self.dtype)) if self.dtype else '',\n-                ('shape=' + str(self.shape)) if self.shape else '',\n-                ('ndim=' + str(self.ndim)) if self.ndim else '',\n-                ('max_ndim=' + str(self.max_ndim)) if self.max_ndim else '',\n-                ('min_ndim=' + str(self.min_ndim)) if self.min_ndim else '',\n-                ('axes=' + str(self.axes)) if self.axes else '']\n-        return 'InputSpec(%s)' % ', '.join(x for x in spec if x)\n-\n-\n-class Node(object):\n-    \"\"\"A `Node` describes the connectivity between two layers.\n-\n-    Each time a layer is connected to some new input,\n-    a node is added to `layer._inbound_nodes`.\n-    Each time the output of a layer is used by another layer,\n-    a node is added to `layer._outbound_nodes`.\n-\n-    # Arguments\n-        outbound_layer: the layer that takes\n-            `input_tensors` and turns them into `output_tensors`\n-            (the node gets created when the `call`\n-            method of the layer was called).\n-        inbound_layers: a list of layers, the same length as `input_tensors`,\n-            the layers from where `input_tensors` originate.\n-        node_indices: a list of integers, the same length as `inbound_layers`.\n-            `node_indices[i]` is the origin node of `input_tensors[i]`\n-            (necessary since each inbound layer might have several nodes,\n-            e.g. if the layer is being shared with a different data stream).\n-        tensor_indices: a list of integers,\n-            the same length as `inbound_layers`.\n-            `tensor_indices[i]` is the index of `input_tensors[i]` within the\n-            output of the inbound layer\n-            (necessary since each inbound layer might\n-            have multiple tensor outputs, with each one being\n-            independently manipulable).\n-        input_tensors: list of input tensors.\n-        output_tensors: list of output tensors.\n-        input_masks: list of input masks (a mask can be a tensor, or None).\n-        output_masks: list of output masks (a mask can be a tensor, or None).\n-        input_shapes: list of input shape tuples.\n-        output_shapes: list of output shape tuples.\n-        arguments: dictionary of keyword arguments that were passed to the\n-            `call` method of the layer at the call that created the node.\n-\n-    `node_indices` and `tensor_indices` are basically fine-grained coordinates\n-    describing the origin of the `input_tensors`, verifying the following:\n-\n-    `input_tensors[i] == inbound_layers[i]._inbound_nodes[node_indices[i]].output_tensors[tensor_indices[i]]`\n-\n-    A node from layer A to layer B is added to:\n-        A._outbound_nodes\n-        B._inbound_nodes\n-    \"\"\"\n-\n-    def __init__(self, outbound_layer,\n-                 inbound_layers, node_indices, tensor_indices,\n-                 input_tensors, output_tensors,\n-                 input_masks, output_masks,\n-                 input_shapes, output_shapes,\n-                 arguments=None):\n-        # Layer instance (NOT a list).\n-        # this is the layer that takes a list of input tensors\n-        # and turns them into a list of output tensors.\n-        # the current node will be added to\n-        # the inbound_nodes of outbound_layer.\n-        self.outbound_layer = outbound_layer\n-\n-        # The following 3 properties describe where\n-        # the input tensors come from: which layers,\n-        # and for each layer, which node and which\n-        # tensor output of each node.\n-\n-        # List of layer instances.\n-        self.inbound_layers = inbound_layers\n-        # List of integers, 1:1 mapping with inbound_layers.\n-        self.node_indices = node_indices\n-        # List of integers, 1:1 mapping with inbound_layers.\n-        self.tensor_indices = tensor_indices\n-\n-        # Following 2 properties:\n-        # tensor inputs and outputs of outbound_layer.\n-\n-        # List of tensors. 1:1 mapping with inbound_layers.\n-        self.input_tensors = input_tensors\n-        # List of tensors, created by outbound_layer.call().\n-        self.output_tensors = output_tensors\n-\n-        # Following 2 properties: input and output masks.\n-        # List of tensors, 1:1 mapping with input_tensor.\n-        self.input_masks = input_masks\n-        # List of tensors, created by outbound_layer.compute_mask().\n-        self.output_masks = output_masks\n-\n-        # Following 2 properties: input and output shapes.\n-\n-        # List of shape tuples, shapes of input_tensors.\n-        self.input_shapes = input_shapes\n-        # List of shape tuples, shapes of output_tensors.\n-        self.output_shapes = output_shapes\n-\n-        # Optional keyword arguments to layer's `call`.\n-        self.arguments = arguments\n-\n-        # Add nodes to all layers involved.\n-        for layer in inbound_layers:\n-            if layer is not None:\n-                layer._outbound_nodes.append(self)\n-        outbound_layer._inbound_nodes.append(self)\n-\n-    def get_config(self):\n-        inbound_names = []\n-        for layer in self.inbound_layers:\n-            if layer:\n-                inbound_names.append(layer.name)\n-            else:\n-                inbound_names.append(None)\n-        return {'outbound_layer': self.outbound_layer.name if self.outbound_layer else None,\n-                'inbound_layers': inbound_names,\n-                'node_indices': self.node_indices,\n-                'tensor_indices': self.tensor_indices}\n-\n-\n-class Layer(object):\n-    \"\"\"Abstract base layer class.\n-\n-    # Properties\n-        name: String, must be unique within a model.\n-        input_spec: List of InputSpec class instances\n-            each entry describes one required input:\n-                - ndim\n-                - dtype\n-            A layer with `n` input tensors must have\n-            an `input_spec` of length `n`.\n-        trainable: Boolean, whether the layer weights\n-            will be updated during training.\n-        uses_learning_phase: Whether any operation\n-            of the layer uses `K.in_training_phase()`\n-            or `K.in_test_phase()`.\n-        input_shape: Shape tuple. Provided for convenience,\n-            but note that there may be cases in which this\n-            attribute is ill-defined (e.g. a shared layer\n-            with multiple input shapes), in which case\n-            requesting `input_shape` will raise an Exception.\n-            Prefer using `layer.get_input_shape_for(input_shape)`,\n-            or `layer.get_input_shape_at(node_index)`.\n-        output_shape: Shape tuple. See above.\n-        inbound_nodes: List of nodes.\n-        outbound_nodes: List of nodes.\n-        input, output: Input/output tensor(s). Note that if the layer is used\n-            more than once (shared layer), this is ill-defined\n-            and will raise an exception. In such cases, use\n-            `layer.get_input_at(node_index)`.\n-        input_mask, output_mask: Same as above, for masks.\n-        trainable_weights: List of variables.\n-        non_trainable_weights: List of variables.\n-        weights: The concatenation of the lists trainable_weights and\n-            non_trainable_weights (in this order).\n-\n-    # Methods\n-        call(x, mask=None): Where the layer's logic lives.\n-        __call__(x, mask=None): Wrapper around the layer logic (`call`).\n-            If x is a Keras tensor:\n-                - Connect current layer with last layer from tensor:\n-                    `self._add_inbound_node(last_layer)`\n-                - Add layer to tensor history\n-            If layer is not built:\n-                - Build from x._keras_shape\n-        get_weights()\n-        set_weights(weights)\n-        get_config()\n-        count_params()\n-        compute_output_shape(input_shape)\n-        compute_mask(x, mask)\n-        get_input_at(node_index)\n-        get_output_at(node_index)\n-        get_input_shape_at(node_index)\n-        get_output_shape_at(node_index)\n-        get_input_mask_at(node_index)\n-        get_output_mask_at(node_index)\n-\n-    # Class Methods\n-        from_config(config)\n-\n-    # Internal methods:\n-        build(input_shape)\n-        _add_inbound_node(layer, index=0)\n-        assert_input_compatibility()\n-    \"\"\"\n-\n-    def __init__(self, **kwargs):\n-        self.input_spec = None\n-        self.supports_masking = False\n-        self.stateful = False\n-\n-        # These properties will be set upon call of self.build()\n-        self._trainable_weights = []\n-        self._non_trainable_weights = []\n-        self._losses = []\n-        self._updates = []\n-        self._per_input_losses = {}\n-        self._per_input_updates = {}\n-        self._built = False\n-\n-        # These lists will be filled via successive calls\n-        # to self._add_inbound_node().\n-        self._inbound_nodes = []\n-        self._outbound_nodes = []\n-\n-        # These properties should be set by the user via keyword arguments.\n-        # note that 'dtype', 'input_shape' and 'batch_input_shape'\n-        # are only applicable to input layers: do not pass these keywords\n-        # to non-input layers.\n-        allowed_kwargs = {'input_shape',\n-                          'batch_input_shape',\n-                          'batch_size',\n-                          'dtype',\n-                          'name',\n-                          'trainable',\n-                          'weights',\n-                          'input_dtype',  # legacy\n-                          }\n-        for kwarg in kwargs:\n-            if kwarg not in allowed_kwargs:\n-                raise TypeError('Keyword argument not understood:', kwarg)\n-        name = kwargs.get('name')\n-        if not name:\n-            prefix = self.__class__.__name__\n-            name = _to_snake_case(prefix) + '_' + str(K.get_uid(prefix))\n-        self.name = name\n-\n-        self.trainable = kwargs.get('trainable', True)\n-        if 'input_shape' in kwargs or 'batch_input_shape' in kwargs:\n-            # In this case we will later create an input layer\n-            # to insert before the current layer\n-            if 'batch_input_shape' in kwargs:\n-                batch_input_shape = tuple(kwargs['batch_input_shape'])\n-            elif 'input_shape' in kwargs:\n-                if 'batch_size' in kwargs:\n-                    batch_size = kwargs['batch_size']\n-                else:\n-                    batch_size = None\n-                batch_input_shape = (batch_size,) + tuple(kwargs['input_shape'])\n-            self.batch_input_shape = batch_input_shape\n-\n-            # Set dtype.\n-            dtype = kwargs.get('dtype')\n-            if dtype is None:\n-                dtype = kwargs.get('input_dtype')\n-            if dtype is None:\n-                dtype = K.floatx()\n-            self.dtype = dtype\n-\n-        if 'weights' in kwargs:\n-            self._initial_weights = kwargs['weights']\n-        else:\n-            self._initial_weights = None\n-\n-    @staticmethod\n-    def _node_key(layer, node_index):\n-        \"\"\"Converts a layer and its index to a unique (immutable type) name.\n-\n-        This function is used internally with `self._container_nodes`.\n-\n-        # Arguments\n-            layer: The layer.\n-            node_index: The layer's position (e.g. via enumerate) in a list of\n-                nodes.\n-\n-        # Returns\n-            The unique name.\n-        \"\"\"\n-        return layer.name + '_ib-' + str(node_index)\n-\n-    @property\n-    def losses(self):\n-        return self._losses\n-\n-    @property\n-    def updates(self):\n-        if not self.trainable and not self.stateful:\n-            return []\n-        return self._updates\n-\n-    @property\n-    def built(self):\n-        return self._built\n-\n-    @built.setter\n-    def built(self, value):\n-        self._built = value\n-\n-    @property\n-    def trainable_weights(self):\n-        trainable = getattr(self, 'trainable', True)\n-        if trainable:\n-            return self._trainable_weights\n-        else:\n-            return []\n-\n-    @trainable_weights.setter\n-    def trainable_weights(self, weights):\n-        self._trainable_weights = weights\n-\n-    @property\n-    def non_trainable_weights(self):\n-        trainable = getattr(self, 'trainable', True)\n-        if not trainable:\n-            return self._trainable_weights + self._non_trainable_weights\n-        else:\n-            return self._non_trainable_weights\n-\n-    @non_trainable_weights.setter\n-    def non_trainable_weights(self, weights):\n-        self._non_trainable_weights = weights\n-\n-    @interfaces.legacy_add_weight_support\n-    def add_weight(self,\n-                   name,\n-                   shape,\n-                   dtype=None,\n-                   initializer=None,\n-                   regularizer=None,\n-                   trainable=True,\n-                   constraint=None):\n-        \"\"\"Adds a weight variable to the layer.\n-\n-        # Arguments\n-            name: String, the name for the weight variable.\n-            shape: The shape tuple of the weight.\n-            dtype: The dtype of the weight.\n-            initializer: An Initializer instance (callable).\n-            regularizer: An optional Regularizer instance.\n-            trainable: A boolean, whether the weight should\n-                be trained via backprop or not (assuming\n-                that the layer itself is also trainable).\n-            constraint: An optional Constraint instance.\n-\n-        # Returns\n-            The created weight variable.\n-        \"\"\"\n-        initializer = initializers.get(initializer)\n-        if dtype is None:\n-            dtype = K.floatx()\n-        weight = K.variable(initializer(shape),\n-                            dtype=dtype,\n-                            name=name,\n-                            constraint=constraint)\n-        if regularizer is not None:\n-            self.add_loss(regularizer(weight))\n-        if trainable:\n-            self._trainable_weights.append(weight)\n-        else:\n-            self._non_trainable_weights.append(weight)\n-        return weight\n-\n-    def assert_input_compatibility(self, inputs):\n-        \"\"\"Checks compatibility between the layer and provided inputs.\n-\n-        This checks that the tensor(s) `input`\n-        verify the input assumptions of the layer\n-        (if any). If not, exceptions are raised.\n-\n-        # Arguments\n-            inputs: input tensor or list of input tensors.\n-\n-        # Raises\n-            ValueError: in case of mismatch between\n-                the provided inputs and the expectations of the layer.\n-        \"\"\"\n-        inputs = _to_list(inputs)\n-        for x in inputs:\n-            try:\n-                K.is_keras_tensor(x)\n-            except ValueError:\n-                raise ValueError('Layer ' + self.name + ' was called with '\n-                                 'an input that isn\\'t a symbolic tensor. '\n-                                 'Received type: ' +\n-                                 str(type(x)) + '. Full input: ' +\n-                                 str(inputs) + '. All inputs to the layer '\n-                                 'should be tensors.')\n-\n-        if not self.input_spec:\n-            return\n-        if not isinstance(self.input_spec, (list, tuple)):\n-            input_spec = _to_list(self.input_spec)\n-        else:\n-            input_spec = self.input_spec\n-        if len(inputs) != len(input_spec):\n-            raise ValueError('Layer ' + self.name + ' expects ' +\n-                             str(len(input_spec)) + ' inputs, '\n-                             'but it received ' + str(len(inputs)) +\n-                             ' input tensors. Input received: ' +\n-                             str(inputs))\n-        for input_index, (x, spec) in enumerate(zip(inputs, input_spec)):\n-            if spec is None:\n-                continue\n-\n-            # Check ndim.\n-            if spec.ndim is not None:\n-                if K.ndim(x) != spec.ndim:\n-                    raise ValueError('Input ' + str(input_index) +\n-                                     ' is incompatible with layer ' +\n-                                     self.name + ': expected ndim=' +\n-                                     str(spec.ndim) + ', found ndim=' +\n-                                     str(K.ndim(x)))\n-            if spec.max_ndim is not None:\n-                ndim = K.ndim(x)\n-                if ndim is not None and ndim > spec.max_ndim:\n-                    raise ValueError('Input ' + str(input_index) +\n-                                     ' is incompatible with layer ' +\n-                                     self.name + ': expected max_ndim=' +\n-                                     str(spec.max_ndim) + ', found ndim=' +\n-                                     str(K.ndim(x)))\n-            if spec.min_ndim is not None:\n-                ndim = K.ndim(x)\n-                if ndim is not None and ndim < spec.min_ndim:\n-                    raise ValueError('Input ' + str(input_index) +\n-                                     ' is incompatible with layer ' +\n-                                     self.name + ': expected min_ndim=' +\n-                                     str(spec.min_ndim) + ', found ndim=' +\n-                                     str(K.ndim(x)))\n-            # Check dtype.\n-            if spec.dtype is not None:\n-                if K.dtype(x) != spec.dtype:\n-                    raise ValueError('Input ' + str(input_index) +\n-                                     ' is incompatible with layer ' +\n-                                     self.name + ': expected dtype=' +\n-                                     str(spec.dtype) + ', found dtype=' +\n-                                     str(K.dtype(x)))\n-            # Check specific shape axes.\n-            if spec.axes:\n-                try:\n-                    x_shape = K.int_shape(x)\n-                except TypeError:\n-                    x_shape = None\n-                if x_shape is not None:\n-                    for axis, value in spec.axes.items():\n-                        if value is not None and x_shape[int(axis)] not in {value, None}:\n-                            raise ValueError('Input ' + str(input_index) +\n-                                             ' is incompatible with layer ' +\n-                                             self.name + ': expected axis ' +\n-                                             str(axis) + ' of input shape to have '\n-                                             'value ' + str(value) +\n-                                             ' but got shape ' + str(x_shape))\n-            # Check shape.\n-            if spec.shape is not None:\n-                try:\n-                    x_shape = K.int_shape(x)\n-                except TypeError:\n-                    x_shape = None\n-                if x_shape is not None:\n-                    for spec_dim, dim in zip(spec.shape, x_shape):\n-                        if spec_dim is not None and dim is not None:\n-                            if spec_dim != dim:\n-                                raise ValueError(\n-                                    'Input ' + str(input_index) +\n-                                    ' is incompatible with layer ' +\n-                                    self.name + ': expected shape=' +\n-                                    str(spec.shape) + ', found shape=' +\n-                                    str(x_shape))\n-\n-    def call(self, inputs, **kwargs):\n-        \"\"\"This is where the layer's logic lives.\n-\n-        # Arguments\n-            inputs: Input tensor, or list/tuple of input tensors.\n-            **kwargs: Additional keyword arguments.\n-\n-        # Returns\n-            A tensor or list/tuple of tensors.\n-        \"\"\"\n-        return inputs\n-\n-    def __call__(self, inputs, **kwargs):\n-        \"\"\"Wrapper around self.call(), for handling internal references.\n-\n-        If a Keras tensor is passed:\n-            - We call self._add_inbound_node().\n-            - If necessary, we `build` the layer to match\n-                the _keras_shape of the input(s).\n-            - We update the _keras_shape of every input tensor with\n-                its new shape (obtained via self.compute_output_shape).\n-                This is done as part of _add_inbound_node().\n-            - We update the _keras_history of the output tensor(s)\n-                with the current layer.\n-                This is done as part of _add_inbound_node().\n-\n-        # Arguments\n-            inputs: Can be a tensor or list/tuple of tensors.\n-            **kwargs: Additional keyword arguments to be passed to `call()`.\n-\n-        # Returns\n-            Output of the layer's `call` method.\n-\n-        # Raises\n-            ValueError: in case the layer is missing shape information\n-                for its `build` call.\n-        \"\"\"\n-        if isinstance(inputs, list):\n-            inputs = inputs[:]\n-        with K.name_scope(self.name):\n-            # Handle laying building (weight creating, input spec locking).\n-            if not self.built:\n-                # Raise exceptions in case the input is not compatible\n-                # with the input_spec specified in the layer constructor.\n-                self.assert_input_compatibility(inputs)\n-\n-                # Collect input shapes to build layer.\n-                input_shapes = []\n-                for x_elem in _to_list(inputs):\n-                    if hasattr(x_elem, '_keras_shape'):\n-                        input_shapes.append(x_elem._keras_shape)\n-                    elif hasattr(K, 'int_shape'):\n-                        input_shapes.append(K.int_shape(x_elem))\n-                    else:\n-                        raise ValueError('You tried to call layer \"' + self.name +\n-                                         '\". This layer has no information'\n-                                         ' about its expected input shape, '\n-                                         'and thus cannot be built. '\n-                                         'You can build it manually via: '\n-                                         '`layer.build(batch_input_shape)`')\n-                if len(input_shapes) == 1:\n-                    self.build(input_shapes[0])\n-                else:\n-                    self.build(input_shapes)\n-                self.built = True\n-\n-                # Load weights that were specified at layer instantiation.\n-                if self._initial_weights is not None:\n-                    self.set_weights(self._initial_weights)\n-\n-            # Raise exceptions in case the input is not compatible\n-            # with the input_spec set at build time.\n-            self.assert_input_compatibility(inputs)\n-\n-            # Handle mask propagation.\n-            previous_mask = _collect_previous_mask(inputs)\n-            user_kwargs = copy.copy(kwargs)\n-            if not _is_all_none(previous_mask):\n-                # The previous layer generated a mask.\n-                if has_arg(self.call, 'mask'):\n-                    if 'mask' not in kwargs:\n-                        # If mask is explicitly passed to __call__,\n-                        # we should override the default mask.\n-                        kwargs['mask'] = previous_mask\n-            # Handle automatic shape inference (only useful for Theano).\n-            input_shape = _collect_input_shape(inputs)\n-\n-            # Actually call the layer, collecting output(s), mask(s), and shape(s).\n-            output = self.call(inputs, **kwargs)\n-            output_mask = self.compute_mask(inputs, previous_mask)\n-\n-            # If the layer returns tensors from its inputs, unmodified,\n-            # we copy them to avoid loss of tensor metadata.\n-            output_ls = _to_list(output)\n-            inputs_ls = _to_list(inputs)\n-            output_ls_copy = []\n-            for x in output_ls:\n-                if x in inputs_ls:\n-                    x = K.identity(x)\n-                output_ls_copy.append(x)\n-            if len(output_ls_copy) == 1:\n-                output = output_ls_copy[0]\n-            else:\n-                output = output_ls_copy\n-\n-            # Inferring the output shape is only relevant for Theano.\n-            if all([s is not None for s in _to_list(input_shape)]):\n-                output_shape = self.compute_output_shape(input_shape)\n-            else:\n-                if isinstance(input_shape, list):\n-                    output_shape = [None for _ in input_shape]\n-                else:\n-                    output_shape = None\n-\n-            if not isinstance(output_mask, (list, tuple)) and len(output_ls) > 1:\n-                # Augment the mask to match the length of the output.\n-                output_mask = [output_mask] * len(output_ls)\n-\n-            # Add an inbound node to the layer, so that it keeps track\n-            # of the call and of all new variables created during the call.\n-            # This also updates the layer history of the output tensor(s).\n-            # If the input tensor(s) had not previous Keras history,\n-            # this does nothing.\n-            self._add_inbound_node(input_tensors=inputs, output_tensors=output,\n-                                   input_masks=previous_mask, output_masks=output_mask,\n-                                   input_shapes=input_shape, output_shapes=output_shape,\n-                                   arguments=user_kwargs)\n-\n-            # Apply activity regularizer if any:\n-            if hasattr(self, 'activity_regularizer') and self.activity_regularizer is not None:\n-                regularization_losses = [self.activity_regularizer(x) for x in _to_list(output)]\n-                self.add_loss(regularization_losses, _to_list(inputs))\n-        return output\n-\n-    def _add_inbound_node(self, input_tensors, output_tensors,\n-                          input_masks, output_masks,\n-                          input_shapes, output_shapes, arguments=None):\n-        \"\"\"Internal method to create an inbound node for the layer.\n-\n-        # Arguments\n-            input_tensors: list of input tensors.\n-            output_tensors: list of output tensors.\n-            input_masks: list of input masks (a mask can be a tensor, or None).\n-            output_masks: list of output masks (a mask can be a tensor, or None).\n-            input_shapes: list of input shape tuples.\n-            output_shapes: list of output shape tuples.\n-            arguments: dictionary of keyword arguments that were passed to the\n-                `call` method of the layer at the call that created the node.\n-        \"\"\"\n-        input_tensors = _to_list(input_tensors)\n-        output_tensors = _to_list(output_tensors)\n-        input_masks = _to_list(input_masks)\n-        output_masks = _to_list(output_masks)\n-        input_shapes = _to_list(input_shapes)\n-        output_shapes = _to_list(output_shapes)\n-\n-        # Collect input tensor(s) coordinates.\n-        inbound_layers = []\n-        node_indices = []\n-        tensor_indices = []\n-        for x in input_tensors:\n-            if hasattr(x, '_keras_history'):\n-                inbound_layer, node_index, tensor_index = x._keras_history\n-                inbound_layers.append(inbound_layer)\n-                node_indices.append(node_index)\n-                tensor_indices.append(tensor_index)\n-            else:\n-                inbound_layers.append(None)\n-                node_indices.append(None)\n-                tensor_indices.append(None)\n-\n-        # Create node, add it to inbound nodes.\n-        Node(\n-            self,\n-            inbound_layers=inbound_layers,\n-            node_indices=node_indices,\n-            tensor_indices=tensor_indices,\n-            input_tensors=input_tensors,\n-            output_tensors=output_tensors,\n-            input_masks=input_masks,\n-            output_masks=output_masks,\n-            input_shapes=input_shapes,\n-            output_shapes=output_shapes,\n-            arguments=arguments\n-        )\n-\n-        # Update tensor history, _keras_shape and _uses_learning_phase.\n-        for i in range(len(output_tensors)):\n-            output_tensors[i]._keras_shape = output_shapes[i]\n-            uses_lp = any([getattr(x, '_uses_learning_phase', False) for x in input_tensors])\n-            uses_lp = getattr(self, 'uses_learning_phase', False) or uses_lp\n-            output_tensors[i]._uses_learning_phase = getattr(output_tensors[i], '_uses_learning_phase', False) or uses_lp\n-            output_tensors[i]._keras_history = (self,\n-                                                len(self._inbound_nodes) - 1,\n-                                                i)\n-\n-    def compute_output_shape(self, input_shape):\n-        \"\"\"Computes the output shape of the layer.\n-\n-        Assumes that the layer will be built\n-        to match that input shape provided.\n-\n-        # Arguments\n-            input_shape: Shape tuple (tuple of integers)\n-                or list of shape tuples (one per output tensor of the layer).\n-                Shape tuples can include None for free dimensions,\n-                instead of an integer.\n-\n-        # Returns\n-            An input shape tuple.\n-        \"\"\"\n-        if hasattr(self, 'get_output_shape_for'):\n-            msg = \"Class `{}.{}` defines `get_output_shape_for` but does not override `compute_output_shape`. \" + \\\n-                  \"If this is a Keras 1 layer, please implement `compute_output_shape` to support Keras 2.\"\n-            warnings.warn(msg.format(type(self).__module__, type(self).__name__), stacklevel=2)\n-        return input_shape\n-\n-    def compute_mask(self, inputs, mask=None):\n-        \"\"\"Computes an output mask tensor.\n-\n-        # Arguments\n-            inputs: Tensor or list of tensors.\n-            mask: Tensor or list of tensors.\n-\n-        # Returns\n-            None or a tensor (or list of tensors,\n-                one per output tensor of the layer).\n-        \"\"\"\n-        if not self.supports_masking:\n-            if mask is not None:\n-                if isinstance(mask, list):\n-                    if any(m is not None for m in mask):\n-                        raise TypeError('Layer ' + self.name +\n-                                        ' does not support masking, '\n-                                        'but was passed an input_mask: ' +\n-                                        str(mask))\n-                else:\n-                    raise TypeError('Layer ' + self.name +\n-                                    ' does not support masking, '\n-                                    'but was passed an input_mask: ' +\n-                                    str(mask))\n-            # masking not explicitly supported: return None as mask\n-            return None\n-        # if masking is explicitly supported, by default\n-        # carry over the input mask\n-        return mask\n-\n-    def build(self, input_shape):\n-        \"\"\"Creates the layer weights.\n-\n-        Must be implemented on all layers that have weights.\n-\n-        # Arguments\n-            input_shape: Keras tensor (future input to layer)\n-                or list/tuple of Keras tensors to reference\n-                for weight shape computations.\n-        \"\"\"\n-        self.built = True\n-\n-    def _get_node_attribute_at_index(self, node_index, attr, attr_name):\n-        \"\"\"Retrieves an attribute (e.g. input_tensors) from a node.\n-\n-        This is used to implement the methods:\n-            - get_input_shape_at\n-            - get_output_shape_at\n-            - get_input_at\n-            etc...\n-\n-        # Arguments\n-            node_index: Integer index of the node from which\n-                to retrieve the attribute.\n-            attr: Exact node attribute name.\n-            attr_name: Human-readable attribute name, for error messages.\n-\n-        # Returns\n-            The layer's attribute `attr` at the node of index `node_index`.\n-\n-        # Raises\n-            RuntimeError: If the layer has no inbound nodes.\n-            ValueError: If the index is does not match any node.\n-        \"\"\"\n-        if not self._inbound_nodes:\n-            raise RuntimeError('The layer has never been called '\n-                               'and thus has no defined ' + attr_name + '.')\n-        if not len(self._inbound_nodes) > node_index:\n-            raise ValueError('Asked to get ' + attr_name +\n-                             ' at node ' + str(node_index) +\n-                             ', but the layer has only ' +\n-                             str(len(self._inbound_nodes)) + ' inbound nodes.')\n-        values = getattr(self._inbound_nodes[node_index], attr)\n-        if len(values) == 1:\n-            return values[0]\n-        else:\n-            return values\n-\n-    def get_input_shape_at(self, node_index):\n-        \"\"\"Retrieves the input shape(s) of a layer at a given node.\n-\n-        # Arguments\n-            node_index: Integer, index of the node\n-                from which to retrieve the attribute.\n-                E.g. `node_index=0` will correspond to the\n-                first time the layer was called.\n-\n-        # Returns\n-            A shape tuple\n-            (or list of shape tuples if the layer has multiple inputs).\n-        \"\"\"\n-        return self._get_node_attribute_at_index(node_index,\n-                                                 'input_shapes',\n-                                                 'input shape')\n-\n-    def get_output_shape_at(self, node_index):\n-        \"\"\"Retrieves the output shape(s) of a layer at a given node.\n-\n-        # Arguments\n-            node_index: Integer, index of the node\n-                from which to retrieve the attribute.\n-                E.g. `node_index=0` will correspond to the\n-                first time the layer was called.\n-\n-        # Returns\n-            A shape tuple\n-            (or list of shape tuples if the layer has multiple outputs).\n-        \"\"\"\n-        return self._get_node_attribute_at_index(node_index,\n-                                                 'output_shapes',\n-                                                 'output shape')\n-\n-    def get_input_at(self, node_index):\n-        \"\"\"Retrieves the input tensor(s) of a layer at a given node.\n-\n-        # Arguments\n-            node_index: Integer, index of the node\n-                from which to retrieve the attribute.\n-                E.g. `node_index=0` will correspond to the\n-                first time the layer was called.\n-\n-        # Returns\n-            A tensor (or list of tensors if the layer has multiple inputs).\n-        \"\"\"\n-        return self._get_node_attribute_at_index(node_index,\n-                                                 'input_tensors',\n-                                                 'input')\n-\n-    def get_output_at(self, node_index):\n-        \"\"\"Retrieves the output tensor(s) of a layer at a given node.\n-\n-        # Arguments\n-            node_index: Integer, index of the node\n-                from which to retrieve the attribute.\n-                E.g. `node_index=0` will correspond to the\n-                first time the layer was called.\n-\n-        # Returns\n-            A tensor (or list of tensors if the layer has multiple outputs).\n-        \"\"\"\n-        return self._get_node_attribute_at_index(node_index,\n-                                                 'output_tensors',\n-                                                 'output')\n-\n-    def get_input_mask_at(self, node_index):\n-        \"\"\"Retrieves the input mask tensor(s) of a layer at a given node.\n-\n-        # Arguments\n-            node_index: Integer, index of the node\n-                from which to retrieve the attribute.\n-                E.g. `node_index=0` will correspond to the\n-                first time the layer was called.\n-\n-        # Returns\n-            A mask tensor\n-            (or list of tensors if the layer has multiple inputs).\n-        \"\"\"\n-        return self._get_node_attribute_at_index(node_index,\n-                                                 'input_masks',\n-                                                 'input mask')\n-\n-    def get_output_mask_at(self, node_index):\n-        \"\"\"Retrieves the output mask tensor(s) of a layer at a given node.\n-\n-        # Arguments\n-            node_index: Integer, index of the node\n-                from which to retrieve the attribute.\n-                E.g. `node_index=0` will correspond to the\n-                first time the layer was called.\n-\n-        # Returns\n-            A mask tensor\n-            (or list of tensors if the layer has multiple outputs).\n-        \"\"\"\n-        return self._get_node_attribute_at_index(node_index,\n-                                                 'output_masks',\n-                                                 'output mask')\n-\n-    @property\n-    def input(self):\n-        \"\"\"Retrieves the input tensor(s) of a layer.\n-\n-        Only applicable if the layer has exactly one inbound node,\n-        i.e. if it is connected to one incoming layer.\n-\n-        # Returns\n-            Input tensor or list of input tensors.\n-\n-        # Raises\n-            AttributeError: if the layer is connected to\n-            more than one incoming layers.\n-        \"\"\"\n-        if len(self._inbound_nodes) > 1:\n-            raise AttributeError('Layer ' + self.name +\n-                                 ' has multiple inbound nodes, '\n-                                 'hence the notion of \"layer input\" '\n-                                 'is ill-defined. '\n-                                 'Use `get_input_at(node_index)` instead.')\n-        elif not self._inbound_nodes:\n-            raise AttributeError('Layer ' + self.name +\n-                                 ' is not connected, no input to return.')\n-        return self._get_node_attribute_at_index(0, 'input_tensors',\n-                                                 'input')\n-\n-    @property\n-    def output(self):\n-        \"\"\"Retrieves the output tensor(s) of a layer.\n-\n-        Only applicable if the layer has exactly one inbound node,\n-        i.e. if it is connected to one incoming layer.\n-\n-        # Returns\n-            Output tensor or list of output tensors.\n-\n-        # Raises\n-            AttributeError: if the layer is connected to\n-            more than one incoming layers.\n-        \"\"\"\n-        if not self._inbound_nodes:\n-            raise AttributeError('Layer ' + self.name +\n-                                 ' has no inbound nodes.')\n-        if len(self._inbound_nodes) > 1:\n-            raise AttributeError('Layer ' + self.name +\n-                                 ' has multiple inbound nodes, '\n-                                 'hence the notion of \"layer output\" '\n-                                 'is ill-defined. '\n-                                 'Use `get_output_at(node_index)` instead.')\n-        return self._get_node_attribute_at_index(0, 'output_tensors',\n-                                                 'output')\n-\n-    @property\n-    def input_mask(self):\n-        \"\"\"Retrieves the input mask tensor(s) of a layer.\n-\n-        Only applicable if the layer has exactly one inbound node,\n-        i.e. if it is connected to one incoming layer.\n-\n-        # Returns\n-            Input mask tensor (potentially None) or list of input\n-            mask tensors.\n-\n-        # Raises\n-            AttributeError: if the layer is connected to\n-            more than one incoming layers.\n-        \"\"\"\n-        if len(self._inbound_nodes) != 1:\n-            raise AttributeError('Layer ' + self.name +\n-                                 ' has multiple inbound nodes, ' +\n-                                 'hence the notion of \"layer input mask\" '\n-                                 'is ill-defined. '\n-                                 'Use `get_input_mask_at(node_index)` '\n-                                 'instead.')\n-        return self._get_node_attribute_at_index(0, 'input_masks',\n-                                                 'input mask')\n-\n-    @property\n-    def output_mask(self):\n-        \"\"\"Retrieves the output mask tensor(s) of a layer.\n-\n-        Only applicable if the layer has exactly one inbound node,\n-        i.e. if it is connected to one incoming layer.\n-\n-        # Returns\n-            Output mask tensor (potentially None) or list of output\n-            mask tensors.\n-\n-        # Raises\n-            AttributeError: if the layer is connected to\n-            more than one incoming layers.\n-        \"\"\"\n-        if len(self._inbound_nodes) != 1:\n-            raise AttributeError('Layer ' + self.name +\n-                                 ' has multiple inbound nodes, '\n-                                 'hence the notion of \"layer output mask\" '\n-                                 'is ill-defined. '\n-                                 'Use `get_output_mask_at(node_index)` '\n-                                 'instead.')\n-        return self._get_node_attribute_at_index(0, 'output_masks',\n-                                                 'output mask')\n-\n-    @property\n-    def input_shape(self):\n-        \"\"\"Retrieves the input shape tuple(s) of a layer.\n-\n-        Only applicable if the layer has exactly one inbound node,\n-        i.e. if it is connected to one incoming layer.\n-\n-        # Returns\n-            Input shape tuple\n-            (or list of input shape tuples, one tuple per input tensor).\n-\n-        # Raises\n-            AttributeError: if the layer is connected to\n-            more than one incoming layers.\n-        \"\"\"\n-        if not self._inbound_nodes:\n-            raise AttributeError('The layer has never been called '\n-                                 'and thus has no defined input shape.')\n-        all_input_shapes = set([str(node.input_shapes) for node in self._inbound_nodes])\n-        if len(all_input_shapes) == 1:\n-            input_shapes = self._inbound_nodes[0].input_shapes\n-            if len(input_shapes) == 1:\n-                return input_shapes[0]\n-            else:\n-                return input_shapes\n-        else:\n-            raise AttributeError('The layer \"' + str(self.name) +\n-                                 ' has multiple inbound nodes, '\n-                                 'with different input shapes. Hence '\n-                                 'the notion of \"input shape\" is '\n-                                 'ill-defined for the layer. '\n-                                 'Use `get_input_shape_at(node_index)` '\n-                                 'instead.')\n-\n-    @property\n-    def output_shape(self):\n-        \"\"\"Retrieves the output shape tuple(s) of a layer.\n-\n-        Only applicable if the layer has one inbound node,\n-        or if all inbound nodes have the same output shape.\n-\n-        # Returns\n-            Output shape tuple\n-            (or list of input shape tuples, one tuple per output tensor).\n-\n-        # Raises\n-            AttributeError: if the layer is connected to\n-            more than one incoming layers.\n-        \"\"\"\n-        if not self._inbound_nodes:\n-            raise AttributeError('The layer has never been called '\n-                                 'and thus has no defined output shape.')\n-        all_output_shapes = set([str(node.output_shapes) for node in self._inbound_nodes])\n-        if len(all_output_shapes) == 1:\n-            output_shapes = self._inbound_nodes[0].output_shapes\n-            if len(output_shapes) == 1:\n-                return output_shapes[0]\n-            else:\n-                return output_shapes\n-        else:\n-            raise AttributeError('The layer \"' + str(self.name) +\n-                                 ' has multiple inbound nodes, '\n-                                 'with different output shapes. Hence '\n-                                 'the notion of \"output shape\" is '\n-                                 'ill-defined for the layer. '\n-                                 'Use `get_output_shape_at(node_index)` '\n-                                 'instead.')\n-\n-    def add_loss(self, losses, inputs=None):\n-        \"\"\"Adds losses to the layer.\n-\n-        The loss may potentially be conditional on some inputs tensors,\n-        for instance activity losses are conditional on the layer's inputs.\n-\n-        # Arguments\n-            losses: loss tensor or list of loss tensors\n-                to add to the layer.\n-            inputs: input tensor or list of inputs tensors to mark\n-                the losses as conditional on these inputs.\n-                If None is passed, the loss is assumed unconditional\n-                (e.g. L2 weight regularization, which only depends\n-                on the layer's weights variables, not on any inputs tensors).\n-        \"\"\"\n-        if losses is None or losses == []:\n-            return\n-        # Update self.losses\n-        losses = _to_list(losses)\n-        if hasattr(self, '_losses'):\n-            self._losses += losses\n-        # Update self._per_input_updates\n-        if isinstance(inputs, list) and inputs == []:\n-            inputs = None\n-        if inputs is not None:\n-            inputs_hash = _object_list_uid(inputs)\n-        else:\n-            # Updates indexed by None are unconditional\n-            # rather than input-dependent\n-            inputs_hash = None\n-        if inputs_hash not in self._per_input_losses:\n-            self._per_input_losses[inputs_hash] = []\n-        self._per_input_losses[inputs_hash] += losses\n-\n-    def add_update(self, updates, inputs=None):\n-        \"\"\"Adds updates to the layer.\n-\n-        The updates may potentially be conditional on some inputs tensors,\n-        for instance batch norm updates are conditional on the layer's inputs.\n-\n-        # Arguments\n-            updates: update op or list of update ops\n-                to add to the layer.\n-            inputs: input tensor or list of inputs tensors to mark\n-                the updates as conditional on these inputs.\n-                If None is passed, the updates are assumed unconditional.\n-        \"\"\"\n-        if updates is None or updates == []:\n-            return\n-        # Update self.updates\n-        updates = _to_list(updates)\n-        if hasattr(self, '_updates'):\n-            self._updates += updates\n-        # Update self._per_input_updates\n-        if isinstance(inputs, list) and inputs == []:\n-            inputs = None\n-        if inputs is not None:\n-            inputs_hash = _object_list_uid(inputs)\n-        else:\n-            # Updates indexed by None are unconditional\n-            # rather than input-dependent\n-            inputs_hash = None\n-        if inputs_hash not in self._per_input_updates:\n-            self._per_input_updates[inputs_hash] = []\n-        self._per_input_updates[inputs_hash] += updates\n-\n-    def get_updates_for(self, inputs):\n-        if not self.trainable and not self.stateful:\n-            return []\n-        if inputs is not None:\n-            inputs_hash = _object_list_uid(inputs)\n-        else:\n-            inputs_hash = None\n-        if inputs_hash in self._per_input_updates:\n-            return self._per_input_updates[inputs_hash]\n-        return []\n-\n-    def get_losses_for(self, inputs):\n-        if inputs is not None:\n-            inputs_hash = _object_list_uid(inputs)\n-        else:\n-            inputs_hash = None\n-        if inputs_hash in self._per_input_losses:\n-            return self._per_input_losses[inputs_hash]\n-        return []\n-\n-    @property\n-    def weights(self):\n-        return self.trainable_weights + self.non_trainable_weights\n-\n-    def set_weights(self, weights):\n-        \"\"\"Sets the weights of the layer, from Numpy arrays.\n-\n-        # Arguments\n-            weights: a list of Numpy arrays. The number\n-                of arrays and their shape must match\n-                number of the dimensions of the weights\n-                of the layer (i.e. it should match the\n-                output of `get_weights`).\n-\n-        # Raises\n-            ValueError: If the provided weights list does not match the\n-                layer's specifications.\n-        \"\"\"\n-        params = self.weights\n-        if len(params) != len(weights):\n-            raise ValueError('You called `set_weights(weights)` on layer \"' +\n-                             self.name +\n-                             '\" with a  weight list of length ' +\n-                             str(len(weights)) +\n-                             ', but the layer was expecting ' +\n-                             str(len(params)) +\n-                             ' weights. Provided weights: ' +\n-                             str(weights)[:50] + '...')\n-        if not params:\n-            return\n-        weight_value_tuples = []\n-        param_values = K.batch_get_value(params)\n-        for pv, p, w in zip(param_values, params, weights):\n-            if pv.shape != w.shape:\n-                raise ValueError('Layer weight shape ' +\n-                                 str(pv.shape) +\n-                                 ' not compatible with '\n-                                 'provided weight shape ' + str(w.shape))\n-            weight_value_tuples.append((p, w))\n-        K.batch_set_value(weight_value_tuples)\n-\n-    def get_weights(self):\n-        \"\"\"Returns the current weights of the layer.\n-\n-        # Returns\n-            Weights values as a list of numpy arrays.\n-        \"\"\"\n-        params = self.weights\n-        return K.batch_get_value(params)\n-\n-    def get_config(self):\n-        \"\"\"Returns the config of the layer.\n-\n-        A layer config is a Python dictionary (serializable)\n-        containing the configuration of a layer.\n-        The same layer can be reinstantiated later\n-        (without its trained weights) from this configuration.\n-\n-        The config of a layer does not include connectivity\n-        information, nor the layer class name. These are handled\n-        by `Container` (one layer of abstraction above).\n-\n-        # Returns\n-            Python dictionary.\n-        \"\"\"\n-        config = {'name': self.name,\n-                  'trainable': self.trainable}\n-        if hasattr(self, 'batch_input_shape'):\n-            config['batch_input_shape'] = self.batch_input_shape\n-        if hasattr(self, 'dtype'):\n-            config['dtype'] = self.dtype\n-        return config\n-\n-    @classmethod\n-    def from_config(cls, config):\n-        \"\"\"Creates a layer from its config.\n-\n-        This method is the reverse of `get_config`,\n-        capable of instantiating the same layer from the config\n-        dictionary. It does not handle layer connectivity\n-        (handled by Container), nor weights (handled by `set_weights`).\n-\n-        # Arguments\n-            config: A Python dictionary, typically the\n-                output of get_config.\n-\n-        # Returns\n-            A layer instance.\n-        \"\"\"\n-        return cls(**config)\n-\n-    def count_params(self):\n-        \"\"\"Counts the total number of scalars composing the weights.\n-\n-        # Returns\n-            An integer count.\n-\n-        # Raises\n-            RuntimeError: if the layer isn't yet built\n-                (in which case its weights aren't yet defined).\n-        \"\"\"\n-        if not self.built:\n-            if self.__class__.__name__ == 'Sequential':\n-                self.build()\n-            else:\n-                raise RuntimeError('You tried to call `count_params` on ' +\n-                                   self.name + ', but the layer isn\\'t built. '\n-                                   'You can build it manually via: `' +\n-                                   self.name + '.build(batch_input_shape)`.')\n-        return count_params(self.weights)\n-\n-\n-class InputLayer(Layer):\n-    \"\"\"Layer to be used as an entry point into a graph.\n-\n-    It can either wrap an existing tensor (pass an `input_tensor` argument)\n-    or create its a placeholder tensor (pass arguments `input_shape`\n-    or `batch_input_shape` as well as `dtype`).\n-\n-    # Arguments\n-        input_shape: Shape tuple, not including the batch axis.\n-        batch_size: Optional input batch size (integer or None).\n-        batch_input_shape: Shape tuple, including the batch axis.\n-        dtype: Datatype of the input.\n-        input_tensor: Optional tensor to use as layer input\n-            instead of creating a placeholder.\n-        sparse: Boolean, whether the placeholder created\n-            is meant to be sparse.\n-        name: Name of the layer (string).\n-    \"\"\"\n-\n-    @interfaces.legacy_input_support\n-    def __init__(self, input_shape=None, batch_size=None,\n-                 batch_input_shape=None,\n-                 dtype=None, input_tensor=None, sparse=False, name=None):\n-        if not name:\n-            prefix = 'input'\n-            name = prefix + '_' + str(K.get_uid(prefix))\n-        super(InputLayer, self).__init__(dtype=dtype, name=name)\n-\n-        self.trainable = False\n-        self.built = True\n-        self.sparse = sparse\n-\n-        if input_shape and batch_input_shape:\n-            raise ValueError('Only provide the input_shape OR '\n-                             'batch_input_shape argument to '\n-                             'InputLayer, not both at the same time.')\n-        if input_tensor is not None and batch_input_shape is None:\n-            # If input_tensor is set, and batch_input_shape is not set:\n-            # Attempt automatic input shape inference.\n-            try:\n-                batch_input_shape = K.int_shape(input_tensor)\n-            except TypeError:\n-                if not input_shape and not batch_input_shape:\n-                    raise ValueError('InputLayer was provided '\n-                                     'an input_tensor argument, '\n-                                     'but its input shape cannot be '\n-                                     'automatically inferred. '\n-                                     'You should pass an input_shape or '\n-                                     'batch_input_shape argument.')\n-        if not batch_input_shape:\n-            if not input_shape:\n-                raise ValueError('An Input layer should be passed either '\n-                                 'a `batch_input_shape` or an `input_shape`.')\n-            else:\n-                batch_input_shape = (batch_size,) + tuple(input_shape)\n-        else:\n-            batch_input_shape = tuple(batch_input_shape)\n-\n-        if not dtype:\n-            if input_tensor is None:\n-                dtype = K.floatx()\n-            else:\n-                dtype = K.dtype(input_tensor)\n-\n-        self.batch_input_shape = batch_input_shape\n-        self.dtype = dtype\n-\n-        if input_tensor is None:\n-            self.is_placeholder = True\n-            input_tensor = K.placeholder(shape=batch_input_shape,\n-                                         dtype=dtype,\n-                                         sparse=self.sparse,\n-                                         name=self.name)\n-        else:\n-            self.is_placeholder = False\n-            input_tensor._keras_shape = batch_input_shape\n-        # Create an input node to add to self.outbound_node\n-        # and set output_tensors' _keras_history.\n-        input_tensor._uses_learning_phase = False\n-        input_tensor._keras_history = (self, 0, 0)\n-        Node(self,\n-             inbound_layers=[],\n-             node_indices=[],\n-             tensor_indices=[],\n-             input_tensors=[input_tensor],\n-             output_tensors=[input_tensor],\n-             input_masks=[None],\n-             output_masks=[None],\n-             input_shapes=[batch_input_shape],\n-             output_shapes=[batch_input_shape])\n-\n-    def get_config(self):\n-        config = {'batch_input_shape': self.batch_input_shape,\n-                  'dtype': self.dtype,\n-                  'sparse': self.sparse,\n-                  'name': self.name}\n-        return config\n-\n-\n-def Input(shape=None, batch_shape=None,\n-          name=None, dtype=None, sparse=False,\n-          tensor=None):\n-    \"\"\"`Input()` is used to instantiate a Keras tensor.\n-\n-    A Keras tensor is a tensor object from the underlying backend\n-    (Theano, TensorFlow or CNTK), which we augment with certain\n-    attributes that allow us to build a Keras model\n-    just by knowing the inputs and outputs of the model.\n-\n-    For instance, if a, b and c are Keras tensors,\n-    it becomes possible to do:\n-    `model = Model(input=[a, b], output=c)`\n-\n-    The added Keras attributes are:\n-        `_keras_shape`: Integer shape tuple propagated\n-            via Keras-side shape inference.\n-        `_keras_history`: Last layer applied to the tensor.\n-            the entire layer graph is retrievable from that layer,\n-            recursively.\n-\n-    # Arguments\n-        shape: A shape tuple (integer), not including the batch size.\n-            For instance, `shape=(32,)` indicates that the expected input\n-            will be batches of 32-dimensional vectors.\n-        batch_shape: A shape tuple (integer), including the batch size.\n-            For instance, `batch_shape=(10, 32)` indicates that\n-            the expected input will be batches of 10 32-dimensional vectors.\n-            `batch_shape=(None, 32)` indicates batches of an arbitrary number\n-            of 32-dimensional vectors.\n-        name: An optional name string for the layer.\n-            Should be unique in a model (do not reuse the same name twice).\n-            It will be autogenerated if it isn't provided.\n-        dtype: The data type expected by the input, as a string\n-            (`float32`, `float64`, `int32`...)\n-        sparse: A boolean specifying whether the placeholder\n-            to be created is sparse.\n-        tensor: Optional existing tensor to wrap into the `Input` layer.\n-            If set, the layer will not create a placeholder tensor.\n-\n-    # Returns\n-        A tensor.\n-\n-    # Example\n-\n-        ```python\n-        # this is a logistic regression in Keras\n-        x = Input(shape=(32,))\n-        y = Dense(16, activation='softmax')(x)\n-        model = Model(x, y)\n-        ```\n-    \"\"\"\n-    if not batch_shape and tensor is None:\n-        assert shape is not None, ('Please provide to Input either a `shape`'\n-                                   ' or a `batch_shape` argument. Note that '\n-                                   '`shape` does not include the batch '\n-                                   'dimension.')\n-    if shape is not None and not batch_shape:\n-        batch_shape = (None,) + tuple(shape)\n-    if not dtype:\n-        dtype = K.floatx()\n-    input_layer = InputLayer(batch_input_shape=batch_shape,\n-                             name=name, dtype=dtype,\n-                             sparse=sparse,\n-                             input_tensor=tensor)\n-    # Return tensor including _keras_shape and _keras_history.\n-    # Note that in this case train_output and test_output are the same pointer.\n-    outputs = input_layer._inbound_nodes[0].output_tensors\n-    if len(outputs) == 1:\n-        return outputs[0]\n-    else:\n-        return outputs\n-\n-\n-class Container(Layer):\n-    \"\"\"A Container is a directed acyclic graph of layers.\n-\n-    It is the topological form of a \"model\". A Model\n-    is simply a Container with added training routines.\n-\n-    # Properties\n-        name\n-        inputs\n-        outputs\n-        input_layers\n-        output_layers\n-        input_spec (list of class instances)\n-            each entry describes one required input:\n-                - ndim\n-                - dtype\n-        trainable (boolean)\n-        input_shape\n-        output_shape\n-        inbound_nodes: list of nodes\n-        outbound_nodes: list of nodes\n-        trainable_weights (list of variables)\n-        non_trainable_weights (list of variables)\n-\n-    # Methods\n-        summary\n-        get_layer\n-        get_weights\n-        set_weights\n-        get_config\n-        compute_output_shape\n-\n-    # Class Methods\n-        from_config\n-\n-    # Raises\n-        TypeError: if input tensors are not Keras tensors from InputLayer objects\n-    \"\"\"\n-\n-    @interfaces.legacy_model_constructor_support\n-    def __init__(self, inputs, outputs, name=None):\n-        # Handle `name` argument.\n-        if not name:\n-            prefix = self.__class__.__name__.lower()\n-            name = prefix + '_' + str(K.get_uid(prefix))\n-        self.name = name\n-\n-        self.supports_masking = False\n-        self.trainable = True\n-        self._per_input_losses = {}\n-        self._per_input_updates = {}\n-\n-        # Container-specific properties.\n-        if isinstance(inputs, (list, tuple)):\n-            self.inputs = list(inputs)  # Tensor or list of tensors.\n-        else:\n-            self.inputs = [inputs]\n-        if isinstance(outputs, (list, tuple)):\n-            self.outputs = list(outputs)\n-        else:\n-            self.outputs = [outputs]\n-\n-        # Check for redundancy in inputs.\n-        if len(set(self.inputs)) != len(self.inputs):\n-            raise ValueError('The list of inputs passed to the model '\n-                             'is redundant. '\n-                             'All inputs should only appear once.'\n-                             ' Found: ' + str(self.inputs))\n-\n-        # Check for redundancy in outputs.\n-        if len(set(self.outputs)) != len(self.outputs):\n-            warnings.warn('The list of outputs passed to the model '\n-                          'is redundant. '\n-                          'All outputs should only appear once.'\n-                          ' Found: ' + str(self.outputs))\n-\n-        # List of initial layers (1 to 1 mapping with self.inputs,\n-        # hence the same layer might appear twice)\n-        self.input_layers = []\n-        self.input_layers_node_indices = []\n-        self.input_layers_tensor_indices = []\n-        # list of layers (1 to 1 mapping with self.inputs,\n-        # hence the same layer might appear twice)\n-        self.output_layers = []\n-        self.output_layers_node_indices = []\n-        self.output_layers_tensor_indices = []\n-        # all layers in order of horizontal graph traversal.\n-        # Entries are unique. Includes input and output layers.\n-        self.layers = []\n-\n-        # This is for performance optimization\n-        # when calling the Container on new inputs.\n-        # every time the Container is called on a set on input tensors,\n-        # we compute the output tensors,\n-        # output masks and output shapes in one pass,\n-        # then cache them here. When one of these output is queried later,\n-        # we retrieve it from there instead of recomputing it.\n-        self._output_mask_cache = {}\n-        self._output_tensor_cache = {}\n-        self._output_shape_cache = {}\n-\n-        # User-provided arguments validation.\n-        for x in self.inputs:\n-            # Check that x is a Keras tensor.\n-            if not hasattr(x, '_keras_history'):\n-                cls_name = self.__class__.__name__\n-                raise TypeError('Input tensors to a ' + cls_name + ' ' +\n-                                'must be Keras tensors. Found: ' + str(x) +\n-                                ' (missing Keras metadata).')\n-            # Check that x is an input tensor.\n-            layer, node_index, tensor_index = x._keras_history\n-            if len(layer._inbound_nodes) > 1 or (layer._inbound_nodes and layer._inbound_nodes[0].inbound_layers):\n-                cls_name = self.__class__.__name__\n-                warnings.warn(cls_name + ' inputs must come from '\n-                              'a Keras Input layer, '\n-                              'they cannot be the output of '\n-                              'a previous non-Input layer. '\n-                              'Here, a tensor specified as '\n-                              'input to \"' + self.name +\n-                              '\" was not an Input tensor, '\n-                              'it was generated by layer ' +\n-                              layer.name + '.\\n'\n-                              'Note that input tensors are '\n-                              'instantiated via `tensor = Input(shape)`.\\n'\n-                              'The tensor that caused the issue was: ' +\n-                              str(x.name))\n-        for x in self.outputs:\n-            if not hasattr(x, '_keras_history'):\n-                cls_name = self.__class__.__name__\n-                raise TypeError('Output tensors to a ' + cls_name + ' must be '\n-                                'Keras tensors. Found: ' + str(x))\n-        # Build self.output_layers:\n-        for x in self.outputs:\n-            layer, node_index, tensor_index = x._keras_history\n-            self.output_layers.append(layer)\n-            self.output_layers_node_indices.append(node_index)\n-            self.output_layers_tensor_indices.append(tensor_index)\n-\n-        # Fill in the output mask cache.\n-        masks = []\n-        for x in self.inputs:\n-            layer, node_index, tensor_index = x._keras_history\n-            node = layer._inbound_nodes[node_index]\n-            mask = node.output_masks[tensor_index]\n-            masks.append(mask)\n-        mask_cache_key = ','.join([str(id(x)) for x in self.inputs])\n-        mask_cache_key += '_' + ','.join([str(id(x)) for x in masks])\n-        masks = []\n-        for x in self.outputs:\n-            layer, node_index, tensor_index = x._keras_history\n-            node = layer._inbound_nodes[node_index]\n-            mask = node.output_masks[tensor_index]\n-            masks.append(mask)\n-        if len(masks) == 1:\n-            mask = masks[0]\n-        else:\n-            mask = masks\n-        self._output_mask_cache[mask_cache_key] = mask\n-\n-        # Build self.input_layers:\n-        for x in self.inputs:\n-            layer, node_index, tensor_index = x._keras_history\n-            # It's supposed to be an input layer, so only one node\n-            # and one tensor output.\n-            assert node_index == 0\n-            assert tensor_index == 0\n-            self.input_layers.append(layer)\n-            self.input_layers_node_indices.append(node_index)\n-            self.input_layers_tensor_indices.append(tensor_index)\n-\n-        # Build self.input_names and self.output_names.\n-        self.input_names = []\n-        self.output_names = []\n-        self._feed_input_names = []\n-        self._feed_inputs = []\n-        self._feed_input_shapes = []\n-        for i, layer in enumerate(self.input_layers):\n-            # Check that layer is an InputLayer.\n-            if not isinstance(layer, InputLayer):\n-                raise TypeError(\n-                    'Input layers to a `Model` must be `InputLayer` objects. '\n-                    'Received inputs: {}. '\n-                    'Input {} (0-based) originates '\n-                    'from layer type `{}`.'.format(inputs,\n-                                                   i,\n-                                                   layer.__class__.__name__))\n-            self.input_names.append(layer.name)\n-            if layer.is_placeholder:\n-                self._feed_input_names.append(layer.name)\n-                self._feed_inputs.append(layer.input)\n-                self._feed_input_shapes.append(self.inputs[i]._keras_shape)\n-        for layer in self.output_layers:\n-            self.output_names.append(layer.name)\n-\n-        self._internal_input_shapes = [x._keras_shape for x in self.inputs]\n-        self._internal_output_shapes = [x._keras_shape for x in self.outputs]\n-\n-        # Container_nodes: set of nodes included in the graph\n-        # (not all nodes included in the layers\n-        # are relevant to the current graph).\n-        container_nodes = set()  # ids of all nodes relevant to the Container\n-        nodes_depths = {}  # dict {node: depth value}\n-        layers_depths = {}  # dict {layer: depth value}\n-        layer_indices = {}  # dict {layer: index in traversal}\n-        nodes_in_decreasing_depth = []\n-\n-        def build_map_of_graph(tensor, finished_nodes, nodes_in_progress,\n-                               layer=None, node_index=None, tensor_index=None):\n-            \"\"\"Builds a map of the graph of layers.\n-\n-            This recursively updates the map `layer_indices`,\n-            the list `nodes_in_decreasing_depth` and the set `container_nodes`.\n-\n-            # Arguments\n-                tensor: Some tensor in a graph.\n-                finished_nodes: Set of nodes whose subgraphs have been traversed\n-                    completely. Useful to prevent duplicated work.\n-                nodes_in_progress: Set of nodes that are currently active on the\n-                    recursion stack. Useful to detect cycles.\n-                layer: Layer from which `tensor` comes from. If not provided,\n-                    will be obtained from `tensor._keras_history`.\n-                node_index: Node index from which `tensor` comes from.\n-                tensor_index: Tensor_index from which `tensor` comes from.\n-\n-            # Raises\n-                RuntimeError: if a cycle is detected.\n-            \"\"\"\n-            if not layer or node_index is None or tensor_index is None:\n-                layer, node_index, tensor_index = tensor._keras_history\n-            node = layer._inbound_nodes[node_index]\n-\n-            # Prevent cycles.\n-            if node in nodes_in_progress:\n-                raise RuntimeError(\n-                    'The tensor ' + str(tensor) + ' at layer \"' +\n-                    layer.name + '\" is part of a cycle.')\n-\n-            # Don't repeat work for shared subgraphs\n-            if node in finished_nodes:\n-                return\n-\n-            # Update container_nodes.\n-            container_nodes.add(self._node_key(layer, node_index))\n-\n-            # Store the traversal order for layer sorting.\n-            if layer not in layer_indices:\n-                layer_indices[layer] = len(layer_indices)\n-\n-            nodes_in_progress.add(node)\n-\n-            # Propagate to all previous tensors connected to this node.\n-            for i in range(len(node.inbound_layers)):\n-                x = node.input_tensors[i]\n-                layer = node.inbound_layers[i]\n-                node_index = node.node_indices[i]\n-                tensor_index = node.tensor_indices[i]\n-                build_map_of_graph(x, finished_nodes, nodes_in_progress,\n-                                   layer, node_index, tensor_index)\n-\n-            finished_nodes.add(node)\n-            nodes_in_progress.remove(node)\n-\n-            nodes_in_decreasing_depth.append(node)\n-\n-        finished_nodes = set()\n-        nodes_in_progress = set()\n-        for x in self.outputs:\n-            build_map_of_graph(x, finished_nodes, nodes_in_progress)\n-\n-        for node in reversed(nodes_in_decreasing_depth):\n-            # If the depth is not set, the node has no outbound nodes (depth 0).\n-            depth = nodes_depths.setdefault(node, 0)\n-\n-            # Update the depth of the corresponding layer\n-            previous_depth = layers_depths.get(node.outbound_layer, 0)\n-            # If we've seen this layer before at a higher depth, we should use that depth instead\n-            # of the node depth.  This is necessary for shared layers that have inputs at different\n-            # depth levels in the graph.\n-            depth = max(depth, previous_depth)\n-            layers_depths[node.outbound_layer] = depth\n-            nodes_depths[node] = depth\n-\n-            # Update the depth of inbound nodes.\n-            for i in range(len(node.inbound_layers)):\n-                inbound_layer = node.inbound_layers[i]\n-                node_index = node.node_indices[i]\n-                inbound_node = inbound_layer._inbound_nodes[node_index]\n-                previous_depth = nodes_depths.get(inbound_node, 0)\n-                nodes_depths[inbound_node] = max(depth + 1, previous_depth)\n-\n-        # Build a dict {depth: list of nodes with this depth}\n-        nodes_by_depth = {}\n-        for node, depth in nodes_depths.items():\n-            if depth not in nodes_by_depth:\n-                nodes_by_depth[depth] = []\n-            nodes_by_depth[depth].append(node)\n-\n-        # Build a dict {depth: list of layers with this depth}\n-        layers_by_depth = {}\n-        for layer, depth in layers_depths.items():\n-            if depth not in layers_by_depth:\n-                layers_by_depth[depth] = []\n-            layers_by_depth[depth].append(layer)\n-\n-        # Get sorted list of layer depths.\n-        depth_keys = list(layers_by_depth.keys())\n-        depth_keys.sort(reverse=True)\n-\n-        # Set self.layers and self.layers_by_depth.\n-        layers = []\n-        for depth in depth_keys:\n-            layers_for_depth = layers_by_depth[depth]\n-            # Container.layers needs to have a deterministic order:\n-            # here we order them by traversal order.\n-            layers_for_depth.sort(key=lambda x: layer_indices[x])\n-            for layer in layers_for_depth:\n-                layers.append(layer)\n-        self.layers = layers\n-        self.layers_by_depth = layers_by_depth\n-\n-        # Get sorted list of node depths.\n-        depth_keys = list(nodes_by_depth.keys())\n-        depth_keys.sort(reverse=True)\n-\n-        # Check that all tensors required are computable.\n-        # computable_tensors: all tensors in the graph\n-        # that can be computed from the inputs provided.\n-        computable_tensors = []\n-        for x in self.inputs:\n-            computable_tensors.append(x)\n-\n-        layers_with_complete_input = []  # To provide a better error msg.\n-        for depth in depth_keys:\n-            for node in nodes_by_depth[depth]:\n-                layer = node.outbound_layer\n-                if layer:\n-                    for x in node.input_tensors:\n-                        if x not in computable_tensors:\n-                            raise RuntimeError(\n-                                'Graph disconnected: '\n-                                'cannot obtain value for tensor ' +\n-                                str(x) + ' at layer \"' + layer.name + '\". '\n-                                'The following previous layers '\n-                                'were accessed without issue: ' +\n-                                str(layers_with_complete_input))\n-                    for x in node.output_tensors:\n-                        computable_tensors.append(x)\n-                    layers_with_complete_input.append(layer.name)\n-\n-        # Set self._container_nodes and self._nodes_by_depth.\n-        self._container_nodes = container_nodes\n-        self._nodes_by_depth = nodes_by_depth\n-\n-        # Ensure name unicity, which will be crucial for serialization\n-        # (since serialized nodes refer to layers by their name).\n-        all_names = [layer.name for layer in self.layers]\n-        for name in all_names:\n-            if all_names.count(name) != 1:\n-                raise RuntimeError('The name \"' + name + '\" is used ' +\n-                                   str(all_names.count(name)) +\n-                                   ' times in the model. '\n-                                   'All layer names should be unique. '\n-                                   'Layer names: ', all_names)\n-\n-        # Layer parameters.\n-        # The new container starts with a single inbound node\n-        # for its inputs, and no outbound nodes.\n-        self._outbound_nodes = []  # Will be appended to by future calls to __call__\n-        self._inbound_nodes = []  # Will be appended to below, and by future calls to __call__\n-        # Create the node linking internal inputs to internal outputs.\n-        Node(outbound_layer=self,\n-             inbound_layers=[],\n-             node_indices=[],\n-             tensor_indices=[],\n-             input_tensors=self.inputs,\n-             output_tensors=self.outputs,\n-             # No container-level masking for now.\n-             input_masks=[None for _ in self.inputs],\n-             output_masks=[None for _ in self.outputs],\n-             input_shapes=[x._keras_shape for x in self.inputs],\n-             output_shapes=[x._keras_shape for x in self.outputs])\n-        self.built = True\n-\n-        # The following are implemented as property functions:\n-        # self.trainable_weights\n-        # self.non_trainable_weights\n-        # self.input_spec\n-\n-    def get_layer(self, name=None, index=None):\n-        \"\"\"Retrieves a layer based on either its name (unique) or index.\n-\n-        If `name` and `index` are both provided, `index` will take precedence.\n-\n-        Indices are based on order of horizontal graph traversal (bottom-up).\n-\n-        # Arguments\n-            name: String, name of layer.\n-            index: Integer, index of layer.\n-\n-        # Returns\n-            A layer instance.\n-\n-        # Raises\n-            ValueError: In case of invalid layer name or index.\n-        \"\"\"\n-        # It would be unreliable to build a dictionary\n-        # based on layer names, because names can potentially\n-        # be changed at any point by the user\n-        # without the container being notified of it.\n-        if index is not None:\n-            if len(self.layers) <= index:\n-                raise ValueError('Was asked to retrieve layer at index ' +\n-                                 str(index) + ' but model only has ' +\n-                                 str(len(self.layers)) + ' layers.')\n-            else:\n-                return self.layers[index]\n-        else:\n-            if not name:\n-                raise ValueError('Provide either a layer name or layer index.')\n-\n-        for layer in self.layers:\n-            if layer.name == name:\n-                return layer\n-\n-        raise ValueError('No such layer: ' + name)\n-\n-    @property\n-    def updates(self):\n-        \"\"\"Retrieves the model's updates.\n-\n-        Will only include updates that are either\n-        unconditional, or conditional on inputs to this model\n-        (e.g. will not include updates that depend on tensors\n-        that aren't inputs to this model).\n-\n-        # Returns\n-            A list of update ops.\n-        \"\"\"\n-        if not self.trainable and not self.stateful:\n-            return []\n-        updates = []\n-        for layer in self.layers:\n-            if hasattr(layer, 'updates'):\n-                # Collect updates that are dependent on inputs\n-                # that are part of the model.\n-                for node_index, node in enumerate(layer._inbound_nodes):\n-                    node_key = self._node_key(layer, node_index)\n-                    if node_key in self._container_nodes:\n-                        # The model owns this layer node.\n-                        inputs = node.input_tensors\n-                        updates += layer.get_updates_for(inputs)\n-                # Collect unconditional updates.\n-                updates += layer.get_updates_for(None)\n-        return updates\n-\n-    @property\n-    def losses(self):\n-        \"\"\"Retrieves the model's losses.\n-\n-        Will only include losses that are either\n-        unconditional, or conditional on inputs to this model\n-        (e.g. will not include losses that depend on tensors\n-        that aren't inputs to this model).\n-\n-        # Returns\n-            A list of loss tensors.\n-        \"\"\"\n-        losses = []\n-        # Retrieve losses for all internal layers.\n-        for layer in self.layers:\n-            if hasattr(layer, 'losses'):\n-                # Collect losses that are dependent on inputs\n-                # that are part of the model.\n-                for node_index, node in enumerate(layer._inbound_nodes):\n-                    node_key = self._node_key(layer, node_index)\n-                    if node_key in self._container_nodes:\n-                        # The model owns this layer node.\n-                        inputs = node.input_tensors\n-                        losses += layer.get_losses_for(inputs)\n-                # Collect unconditional losses.\n-                losses += layer.get_losses_for(None)\n-        # Add any potential unconditional model-level loss.\n-        losses += self.get_losses_for(None)\n-\n-        unique_tensors = list(set(x for x in losses if not isinstance(x, (float, int))))\n-        non_tensors = [x for x in losses if isinstance(x, (float, int))]\n-        return unique_tensors + non_tensors\n-\n-    @property\n-    def uses_learning_phase(self):\n-        return any([x._uses_learning_phase for x in self.outputs])\n-\n-    @property\n-    def stateful(self):\n-        return any([(hasattr(layer, 'stateful') and layer.stateful) for layer in self.layers])\n-\n-    def reset_states(self):\n-        for layer in self.layers:\n-            if hasattr(layer, 'reset_states') and getattr(layer, 'stateful', False):\n-                layer.reset_states()\n-\n-    @property\n-    def state_updates(self):\n-        \"\"\"Returns the `updates` from all layers that are stateful.\n-\n-        This is useful for separating training updates and\n-        state updates, e.g. when we need to update a layer's internal state\n-        during prediction.\n-\n-        # Returns\n-            A list of update ops.\n-        \"\"\"\n-        state_updates = []\n-        for layer in self.layers:\n-            if layer.stateful:\n-                state_updates += layer.updates\n-        return state_updates\n-\n-    @property\n-    def trainable_weights(self):\n-        if not self.trainable:\n-            return []\n-        weights = []\n-        for layer in self.layers:\n-            weights += layer.trainable_weights\n-        return weights\n-\n-    @property\n-    def non_trainable_weights(self):\n-        weights = []\n-        for layer in self.layers:\n-            weights += layer.non_trainable_weights\n-        if not self.trainable:\n-            trainable_weights = []\n-            for layer in self.layers:\n-                trainable_weights += layer.trainable_weights\n-            return trainable_weights + weights\n-        return weights\n-\n-    def get_weights(self):\n-        \"\"\"Retrieves the weights of the model.\n-\n-        # Returns\n-            A flat list of Numpy arrays.\n-        \"\"\"\n-        weights = []\n-        for layer in self.layers:\n-            weights += layer.weights\n-        return K.batch_get_value(weights)\n-\n-    def set_weights(self, weights):\n-        \"\"\"Sets the weights of the model.\n-\n-        # Arguments\n-            weights: A list of Numpy arrays with shapes and types matching\n-                the output of `model.get_weights()`.\n-        \"\"\"\n-        tuples = []\n-        for layer in self.layers:\n-            num_param = len(layer.weights)\n-            layer_weights = weights[:num_param]\n-            for sw, w in zip(layer.weights, layer_weights):\n-                tuples.append((sw, w))\n-            weights = weights[num_param:]\n-        K.batch_set_value(tuples)\n-\n-    @property\n-    def input_spec(self):\n-        \"\"\"Gets the model's input specs.\n-\n-        # Returns\n-            A list of `InputSpec` instances (one per input to the model)\n-                or a single instance if the model has only one input.\n-        \"\"\"\n-        specs = []\n-        for layer in getattr(self, 'input_layers', []):\n-            if layer.input_spec is None:\n-                specs.append(None)\n-            else:\n-                if not isinstance(layer.input_spec, list):\n-                    raise TypeError('Layer ' + layer.name +\n-                                    ' has an input_spec attribute that '\n-                                    'is not a list. We expect a list. '\n-                                    'Found input_spec = ' +\n-                                    str(layer.input_spec))\n-                specs += layer.input_spec\n-        if len(specs) == 1:\n-            return specs[0]\n-        return specs\n-\n-    def call(self, inputs, mask=None):\n-        \"\"\"Calls the model on new inputs.\n-\n-        In this case `call` just reapplies\n-        all ops in the graph to the new inputs\n-        (e.g. build a new computational graph from the provided inputs).\n-\n-        A model is callable on non-Keras tensors.\n-\n-        # Arguments\n-            inputs: A tensor or list of tensors.\n-            mask: A mask or list of masks. A mask can be\n-                either a tensor or None (no mask).\n-\n-        # Returns\n-            A tensor if there is a single output, or\n-            a list of tensors if there are more than one outputs.\n-        \"\"\"\n-        inputs = _to_list(inputs)\n-        if mask is None:\n-            masks = [None for _ in range(len(inputs))]\n-        else:\n-            masks = _to_list(mask)\n-        cache_key = ','.join([str(id(x)) for x in inputs])\n-        cache_key += '_' + ','.join([str(id(x)) for x in masks])\n-        if cache_key in self._output_tensor_cache:\n-            return self._output_tensor_cache[cache_key]\n-        else:\n-            output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n-            return output_tensors\n-\n-    def compute_mask(self, inputs, mask):\n-        inputs = _to_list(inputs)\n-        if mask is None:\n-            masks = [None for _ in range(len(inputs))]\n-        else:\n-            masks = _to_list(mask)\n-        cache_key = ','.join([str(id(x)) for x in inputs])\n-        cache_key += '_' + ','.join([str(id(x)) for x in masks])\n-        if cache_key in self._output_mask_cache:\n-            return self._output_mask_cache[cache_key]\n-        else:\n-            _, output_masks, _ = self.run_internal_graph(inputs, masks)\n-            return output_masks\n-\n-    def compute_output_shape(self, input_shape):\n-        input_shapes = _to_list(input_shape)\n-        if len(input_shapes) != len(self.input_layers):\n-            raise ValueError('Invalid input_shape argument ' +\n-                             str(input_shape) + ': model has ' +\n-                             str(len(self.input_layers)) + ' tensor inputs.')\n-\n-        cache_key = ','.join([str(x) for x in input_shapes])\n-        if cache_key in self._output_shape_cache:\n-            output_shapes = self._output_shape_cache[cache_key]\n-            if isinstance(output_shapes, list) and len(output_shapes) == 1:\n-                return output_shapes[0]\n-            return output_shapes\n-        else:\n-            # Bad luck, we have to run the graph manually.\n-            layers_to_output_shapes = {}\n-            for i in range(len(input_shapes)):\n-                layer = self.input_layers[i]\n-                input_shape = input_shapes[i]\n-                # It's an input layer: compute_output_shape is identity,\n-                # and there is only one node and one tensor output.\n-                shape_key = layer.name + '_0_0'\n-                layers_to_output_shapes[shape_key] = input_shape\n-\n-            depth_keys = list(self._nodes_by_depth.keys())\n-            depth_keys.sort(reverse=True)\n-            # Iterate over nodes, by depth level.\n-            if len(depth_keys) > 1:\n-                for depth in depth_keys:\n-                    nodes = self._nodes_by_depth[depth]\n-                    for node in nodes:\n-                        # This is always a single layer, never a list.\n-                        layer = node.outbound_layer\n-                        if layer in self.input_layers:\n-                            # We've already covered the input layers\n-                            # a few lines above.\n-                            continue\n-                        # Potentially redundant list,\n-                        # same size of node.input_tensors.\n-                        input_shapes = []\n-                        for j in range(len(node.inbound_layers)):\n-                            inbound_layer = node.inbound_layers[j]\n-                            node_index = node.node_indices[j]\n-                            tensor_index = node.tensor_indices[j]\n-                            shape_key = inbound_layer.name + '_%s_%s' % (node_index, tensor_index)\n-                            input_shape = layers_to_output_shapes[shape_key]\n-                            input_shapes.append(input_shape)\n-\n-                        if len(input_shapes) == 1:\n-                            output_shape = layer.compute_output_shape(input_shapes[0])\n-                        else:\n-                            output_shape = layer.compute_output_shape(input_shapes)\n-\n-                        output_shapes = _to_list(output_shape)\n-                        node_index = layer._inbound_nodes.index(node)\n-                        for j in range(len(output_shapes)):\n-                            shape_key = layer.name + '_%s_%s' % (node_index, j)\n-                            layers_to_output_shapes[shape_key] = output_shapes[j]\n-\n-            # Read final output shapes from layers_to_output_shapes.\n-            output_shapes = []\n-            output_shape_keys = []\n-            for i in range(len(self.output_layers)):\n-                layer = self.output_layers[i]\n-                node_index = self.output_layers_node_indices[i]\n-                tensor_index = self.output_layers_tensor_indices[i]\n-                shape_key = layer.name + '_%s_%s' % (node_index, tensor_index)\n-                output_shape_keys.append(shape_key)\n-\n-            for i, key in enumerate(output_shape_keys):\n-                assert key in layers_to_output_shapes\n-                output_shapes.append(layers_to_output_shapes[key])\n-            # Store in cache.\n-            self._output_shape_cache[cache_key] = output_shapes\n-            if isinstance(output_shapes, list) and len(output_shapes) == 1:\n-                return output_shapes[0]\n-            return output_shapes\n-\n-    def run_internal_graph(self, inputs, masks=None):\n-        \"\"\"Computes output tensors for new inputs.\n-\n-        # Note:\n-            - Expects `inputs` to be a list (potentially with 1 element).\n-            - Can be run on non-Keras tensors.\n-\n-        # Arguments\n-            inputs: List of tensors\n-            masks: List of masks (tensors or None).\n-\n-        # Returns\n-            Three lists: output_tensors, output_masks, output_shapes\n-        \"\"\"\n-        if masks is None:\n-            masks = [None for _ in range(len(inputs))]\n-\n-        # Dictionary mapping reference tensors to tuples\n-        # (computed tensor, compute mask)\n-        # we assume a 1:1 mapping from tensor to mask\n-        # TODO: raise exception when a `.compute_mask()` call\n-        # does not return a list the same size as `call`\n-        tensor_map = {}\n-        for x, y, mask in zip(self.inputs, inputs, masks):\n-            tensor_map[str(id(x))] = (y, mask)\n-\n-        depth_keys = list(self._nodes_by_depth.keys())\n-        depth_keys.sort(reverse=True)\n-        for depth in depth_keys:\n-            nodes = self._nodes_by_depth[depth]\n-            for node in nodes:\n-                # This is always a single layer, never a list.\n-                layer = node.outbound_layer\n-                reference_input_tensors = node.input_tensors\n-                reference_output_tensors = node.output_tensors\n-\n-                # If all previous input tensors are available in tensor_map,\n-                # then call node.inbound_layer on them.\n-                computed_data = []  # List of tuples (input, mask).\n-                for x in reference_input_tensors:\n-                    if str(id(x)) in tensor_map:\n-                        computed_data.append(tensor_map[str(id(x))])\n-\n-                if len(computed_data) == len(reference_input_tensors):\n-                    # call layer\n-                    with K.name_scope(layer.name):\n-                        if node.arguments:\n-                            kwargs = node.arguments\n-                        else:\n-                            kwargs = {}\n-                        if len(computed_data) == 1:\n-                            computed_tensor, computed_mask = computed_data[0]\n-                            if has_arg(layer.call, 'mask'):\n-                                if 'mask' not in kwargs:\n-                                    kwargs['mask'] = computed_mask\n-                            output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n-                            output_masks = layer.compute_mask(computed_tensor,\n-                                                              computed_mask)\n-                            if output_masks is None:\n-                                output_masks = [None for _ in output_tensors]\n-                            else:\n-                                output_masks = _to_list(output_masks)\n-                            computed_tensors = [computed_tensor]\n-                            computed_masks = [computed_mask]\n-                        else:\n-                            computed_tensors = [x[0] for x in computed_data]\n-                            computed_masks = [x[1] for x in computed_data]\n-                            if has_arg(layer.call, 'mask'):\n-                                if 'mask' not in kwargs:\n-                                    kwargs['mask'] = computed_masks\n-                            output_tensors = _to_list(layer.call(computed_tensors, **kwargs))\n-                            output_masks = layer.compute_mask(computed_tensors,\n-                                                              computed_masks)\n-                            if output_masks is None:\n-                                output_masks = [None for _ in output_tensors]\n-                            else:\n-                                output_masks = _to_list(output_masks)\n-                        # Apply activity regularizer if any:\n-                        if hasattr(layer, 'activity_regularizer') and layer.activity_regularizer is not None:\n-                            regularization_losses = [layer.activity_regularizer(x) for x in output_tensors]\n-                            layer.add_loss(regularization_losses, computed_tensors)\n-\n-                        if len(output_masks) != len(output_tensors):\n-                            raise Exception('Layers should have equal number of output tensors '\n-                                            'and output masks. Layer ' + str(layer.name) + ' has'\n-                                            ' ' + str(len(output_tensors)) + ' output tensors and'\n-                                            ' ' + str(len(output_masks)) + ' output masks.')\n-                    # Update model updates and losses:\n-                    # Keep track of updates that depend on the inputs\n-                    # (e.g. BN updates).\n-                    self.add_update(layer.get_updates_for(computed_tensors), inputs)\n-                    # Keep track of unconditional updates (e.g. a counter).\n-                    self.add_update(layer.get_updates_for(None), None)\n-                    # Keep track of losses that depend on the inputs\n-                    # (e.g. activity regularizers).\n-                    self.add_loss(layer.get_losses_for(computed_tensors), inputs)\n-                    # Keep track of unconditional losses\n-                    # (e.g. weight regularizers).\n-                    self.add_loss(layer.get_losses_for(None), None)\n-\n-                    # Update _keras_shape.\n-                    if all([hasattr(x, '_keras_shape') for x in computed_tensors]):\n-                        if len(computed_tensors) == 1:\n-                            shapes = _to_list(layer.compute_output_shape(computed_tensors[0]._keras_shape))\n-                            uses_learning_phase = computed_tensors[0]._uses_learning_phase\n-                        else:\n-                            shapes = _to_list(layer.compute_output_shape([x._keras_shape for x in computed_tensors]))\n-                            uses_learning_phase = any([x._uses_learning_phase for x in computed_tensors])\n-                        for x, s in zip(output_tensors, shapes):\n-                            x._keras_shape = s\n-                            x._uses_learning_phase = getattr(x, '_uses_learning_phase', False) or uses_learning_phase\n-\n-                    # Update tensor_map.\n-                    for x, y, mask in zip(reference_output_tensors, output_tensors, output_masks):\n-                        tensor_map[str(id(x))] = (y, mask)\n-\n-        output_tensors = []\n-        output_masks = []\n-        output_shapes = []\n-        for x in self.outputs:\n-            assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\n-            tensor, mask = tensor_map[str(id(x))]\n-            if hasattr(tensor, '_keras_shape') and output_shapes is not None:\n-                shape = tensor._keras_shape\n-                output_shapes.append(shape)\n-            else:\n-                output_shapes = None\n-            output_tensors.append(tensor)\n-            output_masks.append(mask)\n-\n-        # Update cache;\n-        # keys are based on ids on input tensors and inputs masks.\n-        cache_key = ','.join([str(id(x)) for x in inputs])\n-        cache_key += '_' + ','.join([str(id(x)) for x in masks])\n-\n-        if len(output_tensors) == 1:\n-            output_tensors = output_tensors[0]\n-            self._output_tensor_cache[cache_key] = output_tensors\n-        else:\n-            self._output_tensor_cache[cache_key] = output_tensors\n-\n-        if len(output_masks) == 1:\n-            output_masks = output_masks[0]\n-            self._output_mask_cache[cache_key] = output_masks\n-        else:\n-            self._output_mask_cache[cache_key] = output_masks\n-\n-        if output_shapes is not None:\n-            input_shapes = [x._keras_shape for x in inputs]\n-            cache_key = ','.join([str(x) for x in input_shapes])\n-            if len(output_shapes) == 1:\n-                output_shapes = output_shapes[0]\n-                self._output_shape_cache[cache_key] = output_shapes\n-            else:\n-                self._output_shape_cache[cache_key] = output_shapes\n-        return output_tensors, output_masks, output_shapes\n-\n-    def get_config(self):\n-        config = {\n-            'name': self.name,\n-        }\n-\n-        # Build a map from a layer unique name (self._node_key)\n-        # to the index of the nodes that are saved in the config.\n-        # Only nodes in container_nodes are saved.\n-        node_conversion_map = {}\n-        for layer in self.layers:\n-            if issubclass(layer.__class__, Container):\n-                # Containers start with a pre-existing node\n-                # linking their input to output.\n-                kept_nodes = 1\n-            else:\n-                kept_nodes = 0\n-            for original_node_index, node in enumerate(layer._inbound_nodes):\n-                node_key = self._node_key(layer, original_node_index)\n-                if node_key in self._container_nodes:\n-                    # i.e. we mark it to be saved\n-                    node_conversion_map[node_key] = kept_nodes\n-                    kept_nodes += 1\n-\n-        # serialize and save the layers in layer_configs\n-        layer_configs = []\n-        for layer in self.layers:  # From the earliest layers on.\n-            layer_class_name = layer.__class__.__name__\n-            layer_config = layer.get_config()\n-            filtered_inbound_nodes = []\n-            for original_node_index, node in enumerate(layer._inbound_nodes):\n-                node_key = self._node_key(layer, original_node_index)\n-                if node_key in self._container_nodes:\n-                    # The node is relevant to the model:\n-                    # add to filtered_inbound_nodes.\n-                    if node.arguments:\n-                        try:\n-                            json.dumps(node.arguments)\n-                            kwargs = node.arguments\n-                        except TypeError:\n-                            warnings.warn(\n-                                'Layer ' + layer.name +\n-                                ' was passed non-serializable keyword arguments: ' +\n-                                str(node.arguments) + '. They will not be included '\n-                                'in the serialized model (and thus will be missing '\n-                                'at deserialization time).')\n-                            kwargs = {}\n-                    else:\n-                        kwargs = {}\n-                    if node.inbound_layers:\n-                        node_data = []\n-                        for i in range(len(node.inbound_layers)):\n-                            inbound_layer = node.inbound_layers[i]\n-                            node_index = node.node_indices[i]\n-                            tensor_index = node.tensor_indices[i]\n-\n-                            new_node_index = node_conversion_map.get(\n-                                self._node_key(inbound_layer, node_index), 0)\n-                            node_data.append([inbound_layer.name,\n-                                              new_node_index,\n-                                              tensor_index,\n-                                              kwargs])\n-                        filtered_inbound_nodes.append(node_data)\n-            layer_configs.append({\n-                'name': layer.name,\n-                'class_name': layer_class_name,\n-                'config': layer_config,\n-                'inbound_nodes': filtered_inbound_nodes,\n-            })\n-        config['layers'] = layer_configs\n-\n-        # Gather info about inputs and outputs.\n-        model_inputs = []\n-        for i in range(len(self.input_layers)):\n-            layer = self.input_layers[i]\n-            node_index = self.input_layers_node_indices[i]\n-\n-            node_key = self._node_key(layer, node_index)\n-            if node_key not in self._container_nodes:\n-                continue\n-            new_node_index = node_conversion_map[node_key]\n-            tensor_index = self.input_layers_tensor_indices[i]\n-            model_inputs.append([layer.name, new_node_index, tensor_index])\n-        config['input_layers'] = model_inputs\n-        model_outputs = []\n-        for i in range(len(self.output_layers)):\n-            layer = self.output_layers[i]\n-            node_index = self.output_layers_node_indices[i]\n-\n-            node_key = self._node_key(layer, node_index)\n-            if node_key not in self._container_nodes:\n-                continue\n-            new_node_index = node_conversion_map[node_key]\n-            tensor_index = self.output_layers_tensor_indices[i]\n-            model_outputs.append([layer.name, new_node_index, tensor_index])\n-        config['output_layers'] = model_outputs\n-        return copy.deepcopy(config)\n-\n-    @classmethod\n-    def from_config(cls, config, custom_objects=None):\n-        \"\"\"Instantiates a Model from its config (output of `get_config()`).\n-\n-        # Arguments\n-            config: Model config dictionary.\n-            custom_objects: Optional dictionary mapping names\n-                (strings) to custom classes or functions to be\n-                considered during deserialization.\n-\n-        # Returns\n-            A model instance.\n-\n-        # Raises\n-            ValueError: In case of improperly formatted config dict.\n-        \"\"\"\n-        # Layer instances created during\n-        # the graph reconstruction process\n-        created_layers = {}\n-\n-        # Dictionary mapping layer instances to\n-        # node data that specifies a layer call.\n-        # It acts as a queue that maintains any unprocessed\n-        # layer call until it becomes possible to process it\n-        # (i.e. until the input tensors to the call all exist).\n-        unprocessed_nodes = {}\n-\n-        def add_unprocessed_node(layer, node_data):\n-            if layer not in unprocessed_nodes:\n-                unprocessed_nodes[layer] = [node_data]\n-            else:\n-                unprocessed_nodes[layer].append(node_data)\n-\n-        def process_node(layer, node_data):\n-            input_tensors = []\n-            for input_data in node_data:\n-                inbound_layer_name = input_data[0]\n-                inbound_node_index = input_data[1]\n-                inbound_tensor_index = input_data[2]\n-                if len(input_data) == 3:\n-                    kwargs = {}\n-                elif len(input_data) == 4:\n-                    kwargs = input_data[3]\n-                else:\n-                    raise ValueError('Improperly formatted model config.')\n-                if inbound_layer_name not in created_layers:\n-                    add_unprocessed_node(layer, node_data)\n-                    return\n-                inbound_layer = created_layers[inbound_layer_name]\n-                if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n-                    add_unprocessed_node(layer, node_data)\n-                    return\n-                inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n-                input_tensors.append(inbound_node.output_tensors[inbound_tensor_index])\n-            # Call layer on its inputs, thus creating the node\n-            # and building the layer if needed.\n-            if input_tensors:\n-                if len(input_tensors) == 1:\n-                    layer(input_tensors[0], **kwargs)\n-                else:\n-                    layer(input_tensors, **kwargs)\n-\n-        def process_layer(layer_data):\n-            \"\"\"Deserializes a layer, then call it on appropriate inputs.\n-\n-            # Arguments\n-                layer_data: layer config dict.\n-\n-            # Raises\n-                ValueError: In case of improperly formatted `layer_data` dict.\n-            \"\"\"\n-            layer_name = layer_data['name']\n-\n-            # Instantiate layer.\n-            from ..layers import deserialize as deserialize_layer\n-\n-            layer = deserialize_layer(layer_data,\n-                                      custom_objects=custom_objects)\n-            created_layers[layer_name] = layer\n-\n-            # Gather layer inputs.\n-            inbound_nodes_data = layer_data['inbound_nodes']\n-            for node_data in inbound_nodes_data:\n-                # We don't process nodes (i.e. make layer calls)\n-                # on the fly because the inbound node may not yet exist,\n-                # in case of layer shared at different topological depths\n-                # (e.g. a model such as A(B(A(B(x)))))\n-                add_unprocessed_node(layer, node_data)\n-\n-        # First, we create all layers and enqueue nodes to be processed\n-        for layer_data in config['layers']:\n-            process_layer(layer_data)\n-        # Then we process nodes in order of layer depth.\n-        # Nodes that cannot yet be processed (if the inbound node\n-        # does not yet exist) are re-enqueued, and the process\n-        # is repeated until all nodes are processed.\n-        while unprocessed_nodes:\n-            for layer_data in config['layers']:\n-                layer = created_layers[layer_data['name']]\n-                if layer in unprocessed_nodes:\n-                    for node_data in unprocessed_nodes.pop(layer):\n-                        process_node(layer, node_data)\n-\n-        name = config.get('name')\n-        input_tensors = []\n-        output_tensors = []\n-        for layer_data in config['input_layers']:\n-            layer_name, node_index, tensor_index = layer_data\n-            assert layer_name in created_layers\n-            layer = created_layers[layer_name]\n-            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n-            input_tensors.append(layer_output_tensors[tensor_index])\n-        for layer_data in config['output_layers']:\n-            layer_name, node_index, tensor_index = layer_data\n-            assert layer_name in created_layers\n-            layer = created_layers[layer_name]\n-            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n-            output_tensors.append(layer_output_tensors[tensor_index])\n-        return cls(inputs=input_tensors, outputs=output_tensors, name=name)\n-\n-    def save(self, filepath, overwrite=True, include_optimizer=True):\n-        \"\"\"Saves the model to a single HDF5 file.\n-\n-        The savefile includes:\n-            - The model architecture, allowing to re-instantiate the model.\n-            - The model weights.\n-            - The state of the optimizer, allowing to resume training\n-                exactly where you left off.\n-\n-        This allows you to save the entirety of the state of a model\n-        in a single file.\n-\n-        Saved models can be reinstantiated via `keras.models.load_model`.\n-        The model returned by `load_model`\n-        is a compiled model ready to be used (unless the saved model\n-        was never compiled in the first place).\n-\n-        # Arguments\n-            filepath: String, path to the file to save the weights to.\n-            overwrite: Whether to silently overwrite any existing file at the\n-                target location, or provide the user with a manual prompt.\n-            include_optimizer: If True, save optimizer's state together.\n-\n-        # Example\n-\n-        ```python\n-        from keras.models import load_model\n-\n-        model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n-        del model  # deletes the existing model\n-\n-        # returns a compiled model\n-        # identical to the previous one\n-        model = load_model('my_model.h5')\n-        ```\n-        \"\"\"\n-        from ..models import save_model\n-        save_model(self, filepath, overwrite, include_optimizer)\n-\n-    def save_weights(self, filepath, overwrite=True):\n-        \"\"\"Dumps all layer weights to a HDF5 file.\n-\n-        The weight file has:\n-            - `layer_names` (attribute), a list of strings\n-                (ordered names of model layers).\n-            - For every layer, a `group` named `layer.name`\n-                - For every such layer group, a group attribute `weight_names`,\n-                    a list of strings\n-                    (ordered names of weights tensor of the layer).\n-                - For every weight in the layer, a dataset\n-                    storing the weight value, named after the weight tensor.\n-\n-        # Arguments\n-            filepath: String, path to the file to save the weights to.\n-            overwrite: Whether to silently overwrite any existing file at the\n-                target location, or provide the user with a manual prompt.\n-\n-        # Raises\n-            ImportError: If h5py is not available.\n-        \"\"\"\n-        if h5py is None:\n-            raise ImportError('`save_weights` requires h5py.')\n-        # If file exists and should not be overwritten:\n-        if not overwrite and os.path.isfile(filepath):\n-            proceed = ask_to_proceed_with_overwrite(filepath)\n-            if not proceed:\n-                return\n-        with h5py.File(filepath, 'w') as f:\n-            save_weights_to_hdf5_group(f, self.layers)\n-            f.flush()\n-\n-    def load_weights(self, filepath, by_name=False,\n-                     skip_mismatch=False, reshape=False):\n-        \"\"\"Loads all layer weights from a HDF5 save file.\n-\n-        If `by_name` is False (default) weights are loaded\n-        based on the network's topology, meaning the architecture\n-        should be the same as when the weights were saved.\n-        Note that layers that don't have weights are not taken\n-        into account in the topological ordering, so adding or\n-        removing layers is fine as long as they don't have weights.\n-\n-        If `by_name` is True, weights are loaded into layers\n-        only if they share the same name. This is useful\n-        for fine-tuning or transfer-learning models where\n-        some of the layers have changed.\n-\n-        # Arguments\n-            filepath: String, path to the weights file to load.\n-            by_name: Boolean, whether to load weights by name\n-                or by topological order.\n-            skip_mismatch: Boolean, whether to skip loading of layers\n-                where there is a mismatch in the number of weights,\n-                or a mismatch in the shape of the weight\n-                (only valid when `by_name`=True).\n-            reshape: Reshape weights to fit the layer when the correct number\n-                of weight arrays is present but their shape does not match.\n-\n-\n-        # Raises\n-            ImportError: If h5py is not available.\n-        \"\"\"\n-        if h5py is None:\n-            raise ImportError('`load_weights` requires h5py.')\n-        with h5py.File(filepath, mode='r') as f:\n-            if 'layer_names' not in f.attrs and 'model_weights' in f:\n-                f = f['model_weights']\n-            if by_name:\n-                load_weights_from_hdf5_group_by_name(\n-                    f, self.layers, skip_mismatch=skip_mismatch,\n-                    reshape=reshape)\n-            else:\n-                load_weights_from_hdf5_group(\n-                    f, self.layers, reshape=reshape)\n-\n-    def _updated_config(self):\n-        \"\"\"Util hared between different serialization methods.\n-\n-        # Returns\n-            Model config with Keras version information added.\n-        \"\"\"\n-        from .. import __version__ as keras_version\n-\n-        config = self.get_config()\n-        model_config = {\n-            'class_name': self.__class__.__name__,\n-            'config': config,\n-            'keras_version': keras_version,\n-            'backend': K.backend()\n-        }\n-        return model_config\n-\n-    def to_json(self, **kwargs):\n-        \"\"\"Returns a JSON string containing the network configuration.\n-\n-        To load a network from a JSON save file, use\n-        `keras.models.model_from_json(json_string, custom_objects={})`.\n-\n-        # Arguments\n-            **kwargs: Additional keyword arguments\n-                to be passed to `json.dumps()`.\n-\n-        # Returns\n-            A JSON string.\n-        \"\"\"\n-        def get_json_type(obj):\n-            # If obj is any numpy type\n-            if type(obj).__module__ == np.__name__:\n-                return obj.item()\n-\n-            # If obj is a python 'type'\n-            if type(obj).__name__ == type.__name__:\n-                return obj.__name__\n-\n-            raise TypeError('Not JSON Serializable:', obj)\n-\n-        model_config = self._updated_config()\n-        return json.dumps(model_config, default=get_json_type, **kwargs)\n-\n-    def to_yaml(self, **kwargs):\n-        \"\"\"Returns a yaml string containing the network configuration.\n-\n-        To load a network from a yaml save file, use\n-        `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n-\n-        `custom_objects` should be a dictionary mapping\n-        the names of custom losses / layers / etc to the corresponding\n-        functions / classes.\n-\n-        # Arguments\n-            **kwargs: Additional keyword arguments\n-                to be passed to `yaml.dump()`.\n-\n-        # Returns\n-            A YAML string.\n-        \"\"\"\n-        return yaml.dump(self._updated_config(), **kwargs)\n-\n-    def summary(self, line_length=None, positions=None, print_fn=None):\n-        \"\"\"Prints a string summary of the network.\n-\n-        # Arguments\n-            line_length: Total length of printed lines\n-                (e.g. set this to adapt the display to different\n-                terminal window sizes).\n-            positions: Relative or absolute positions of log elements\n-                in each line. If not provided,\n-                defaults to `[.33, .55, .67, 1.]`.\n-            print_fn: Print function to use.\n-                It will be called on each line of the summary.\n-                You can set it to a custom function\n-                in order to capture the string summary.\n-                It defaults to `print` (prints to stdout).\n-        \"\"\"\n-        return print_layer_summary(self,\n-                                   line_length=line_length,\n-                                   positions=positions,\n-                                   print_fn=print_fn)\n-\n-\n-def get_source_inputs(tensor, layer=None, node_index=None):\n-    \"\"\"Returns the list of input tensors necessary to compute `tensor`.\n-\n-    Output will always be a list of tensors\n-    (potentially with 1 element).\n-\n-    # Arguments\n-        tensor: The tensor to start from.\n-        layer: Origin layer of the tensor. Will be\n-            determined via tensor._keras_history if not provided.\n-        node_index: Origin node index of the tensor.\n-\n-    # Returns\n-        List of input tensors.\n-    \"\"\"\n-    if not hasattr(tensor, '_keras_history'):\n-        return tensor\n-\n-    if layer is None or node_index:\n-        layer, node_index, _ = tensor._keras_history\n-    if not layer._inbound_nodes:\n-        return [tensor]\n-    else:\n-        node = layer._inbound_nodes[node_index]\n-        if not node.inbound_layers:\n-            # Reached an Input layer, stop recursion.\n-            return node.input_tensors\n-        else:\n-            source_tensors = []\n-            for i in range(len(node.inbound_layers)):\n-                x = node.input_tensors[i]\n-                layer = node.inbound_layers[i]\n-                node_index = node.node_indices[i]\n-                previous_sources = get_source_inputs(x,\n-                                                     layer,\n-                                                     node_index)\n-                # Avoid input redundancy.\n-                for x in previous_sources:\n-                    if x not in source_tensors:\n-                        source_tensors.append(x)\n-            return source_tensors\n-\n-\n-def _to_list(x):\n-    \"\"\"Normalizes a list/tensor into a list.\n-\n-    If a tensor is passed, we return\n-    a list of size 1 containing the tensor.\n-\n-    # Arguments\n-        x: target object to be normalized.\n-\n-    # Returns\n-        A list.\n-    \"\"\"\n-    if isinstance(x, list):\n-        return x\n-    return [x]\n-\n-\n-def _object_list_uid(object_list):\n-    object_list = _to_list(object_list)\n-    return ', '.join([str(abs(id(x))) for x in object_list])\n-\n-\n-def _is_all_none(iterable_or_element):\n-    if not isinstance(iterable_or_element, (list, tuple)):\n-        iterable = [iterable_or_element]\n-    else:\n-        iterable = iterable_or_element\n-    for element in iterable:\n-        if element is not None:\n-            return False\n-    return True\n-\n-\n-def _collect_previous_mask(input_tensors):\n-    \"\"\"Retrieves the output mask(s) of the previous node.\n-\n-    # Arguments\n-        input_tensors: A tensor or list of tensors.\n-\n-    # Returns\n-        A mask tensor or list of mask tensors.\n-    \"\"\"\n-    input_tensors = _to_list(input_tensors)\n-    masks = []\n-    for x in input_tensors:\n-        if hasattr(x, '_keras_history'):\n-            inbound_layer, node_index, tensor_index = x._keras_history\n-            node = inbound_layer._inbound_nodes[node_index]\n-            mask = node.output_masks[tensor_index]\n-            masks.append(mask)\n-        else:\n-            masks.append(None)\n-    if len(masks) == 1:\n-        return masks[0]\n-    return masks\n-\n-\n-def _to_snake_case(name):\n-    intermediate = re.sub('(.)([A-Z][a-z0-9]+)', r'\\1_\\2', name)\n-    insecure = re.sub('([a-z])([A-Z])', r'\\1_\\2', intermediate).lower()\n-    # If the class is private the name starts with \"_\" which is not secure\n-    # for creating scopes. We prefix the name with \"private\" in this case.\n-    if insecure[0] != '_':\n-        return insecure\n-    return 'private' + insecure\n-\n-\n-def _collect_input_shape(input_tensors):\n-    \"\"\"Collects the output shape(s) of a list of Keras tensors.\n-\n-    # Arguments\n-        input_tensors: list of input tensors (or single input tensor).\n-\n-    # Returns\n-        List of shape tuples (or single tuple), one tuple per input.\n-    \"\"\"\n-    input_tensors = _to_list(input_tensors)\n-    shapes = []\n-    for x in input_tensors:\n-        try:\n-            shapes.append(K.int_shape(x))\n-        except TypeError:\n-            shapes.append(None)\n-    if len(shapes) == 1:\n-        return shapes[0]\n-    return shapes\n-\n-\n-def _save_attributes_to_hdf5_group(group, name, data):\n-    \"\"\"Saves attributes (data) of the specified name into the HDF5 group.\n-\n-    This method deals with an inherent problem of HDF5 file which is not\n-    able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n-\n-    # Arguments\n-        group: A pointer to a HDF5 group.\n-        name: A name of the attributes to save.\n-        data: Attributes data to store.\n-    \"\"\"\n-    # Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`\n-    # because in that case even chunking the array would not make the saving\n-    # possible.\n-    bad_attributes = [x for x in data if len(x) > HDF5_OBJECT_HEADER_LIMIT]\n-\n-    # Expecting this to never be true.\n-    if len(bad_attributes) > 0:\n-        raise RuntimeError('The following attributes cannot be saved to HDF5 '\n-                           'file because they are larger than %d bytes: %s'\n-                           % (HDF5_OBJECT_HEADER_LIMIT,\n-                              ', '.join([x for x in bad_attributes])))\n-\n-    data_npy = np.asarray(data)\n-\n-    num_chunks = 1\n-    chunked_data = np.array_split(data_npy, num_chunks)\n-\n-    # This will never loop forever thanks to the test above.\n-    while any(map(lambda x: x.nbytes > HDF5_OBJECT_HEADER_LIMIT, chunked_data)):\n-        num_chunks += 1\n-        chunked_data = np.array_split(data_npy, num_chunks)\n-\n-    if num_chunks > 1:\n-        for chunk_id, chunk_data in enumerate(chunked_data):\n-            group.attrs['%s%d' % (name, chunk_id)] = chunk_data\n-    else:\n-        group.attrs[name] = data\n-\n-\n-def _load_attributes_from_hdf5_group(group, name):\n-    \"\"\"Loads attributes of the specified name from the HDF5 group.\n-\n-    This method deals with an inherent problem\n-    of HDF5 file which is not able to store\n-    data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n-\n-    # Arguments\n-        group: A pointer to a HDF5 group.\n-        name: A name of the attributes to load.\n-\n-    # Returns\n-        data: Attributes data.\n-    \"\"\"\n-    if name in group.attrs:\n-        data = [n.decode('utf8') for n in group.attrs[name]]\n-    else:\n-        data = []\n-        chunk_id = 0\n-        while ('%s%d' % (name, chunk_id)) in group.attrs:\n-            data.extend([n.decode('utf8')\n-                        for n in group.attrs['%s%d' % (name, chunk_id)]])\n-            chunk_id += 1\n-    return data\n-\n-\n-def save_weights_to_hdf5_group(f, layers):\n-    from .. import __version__ as keras_version\n-\n-    _save_attributes_to_hdf5_group(\n-        f, 'layer_names', [layer.name.encode('utf8') for layer in layers])\n-    f.attrs['backend'] = K.backend().encode('utf8')\n-    f.attrs['keras_version'] = str(keras_version).encode('utf8')\n-\n-    for layer in layers:\n-        g = f.create_group(layer.name)\n-        symbolic_weights = layer.weights\n-        weight_values = K.batch_get_value(symbolic_weights)\n-        weight_names = []\n-        for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n-            if hasattr(w, 'name') and w.name:\n-                name = str(w.name)\n-            else:\n-                name = 'param_' + str(i)\n-            weight_names.append(name.encode('utf8'))\n-        _save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\n-        for name, val in zip(weight_names, weight_values):\n-            param_dset = g.create_dataset(name, val.shape,\n-                                          dtype=val.dtype)\n-            if not val.shape:\n-                # scalar\n-                param_dset[()] = val\n-            else:\n-                param_dset[:] = val\n-\n-\n-def preprocess_weights_for_loading(layer, weights,\n-                                   original_keras_version=None,\n-                                   original_backend=None,\n-                                   reshape=False):\n-    \"\"\"Converts layers weights from Keras 1 format to Keras 2.\n-\n-    # Arguments\n-        layer: Layer instance.\n-        weights: List of weights values (Numpy arrays).\n-        original_keras_version: Keras version for the weights, as a string.\n-        original_backend: Keras backend the weights were trained with,\n-            as a string.\n-        reshape: Reshape weights to fit the layer when the correct number\n-            of values are present but the shape does not match.\n-\n-    # Returns\n-        A list of weights values (Numpy arrays).\n-    \"\"\"\n@@ -10,8 +10,8 @@ import copy\n import numpy as np\n from scipy.sparse import issparse\n \n-from .topology import Container\n-from .topology import Layer\n+from .network import Network\n+from .base_layer import Layer\n from .. import backend as K\n from .. import optimizers\n from .. import losses\n@@ -542,8 +542,8 @@ def _standardize_weights(y, sample_weight=None, class_weight=None,\n             return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n \n \n-class Model(Container):\n-    \"\"\"The `Model` class adds training & evaluation routines to a `Container`.\n+class Model(Network):\n+    \"\"\"The `Model` class adds training & evaluation routines to a `Network`.\n     \"\"\"\n \n     def compile(self, optimizer, loss=None, metrics=None, loss_weights=None,\n\n@@ -1,10 +1,10 @@\n from __future__ import absolute_import\n \n from ..utils.generic_utils import deserialize_keras_object\n-from ..engine import Layer\n+from ..engine.base_layer import Layer\n from ..engine import Input\n from ..engine import InputLayer\n-from ..engine import InputSpec\n+from ..engine.base_layer import InputSpec\n from .merge import *\n from .core import *\n from .convolutional import *\n\n@@ -9,8 +9,8 @@ from .. import activations\n from .. import initializers\n from .. import regularizers\n from .. import constraints\n-from ..engine import Layer\n-from ..engine import InputSpec\n+from ..engine.base_layer import Layer\n+from ..engine.base_layer import InputSpec\n from .. import backend as K\n from ..legacy import interfaces\n \n\n@@ -10,8 +10,8 @@ from .. import activations\n from .. import initializers\n from .. import regularizers\n from .. import constraints\n-from ..engine import Layer\n-from ..engine import InputSpec\n+from ..engine.base_layer import Layer\n+from ..engine.base_layer import InputSpec\n from ..utils import conv_utils\n from ..legacy import interfaces\n \n\n@@ -15,7 +15,7 @@ from .recurrent import _standardize_args\n \n import numpy as np\n import warnings\n-from ..engine import InputSpec, Layer\n+from ..engine.base_layer import InputSpec, Layer\n from ..utils import conv_utils\n from ..legacy import interfaces\n from ..legacy.layers import Recurrent, ConvRecurrent2D\n\n@@ -16,8 +16,8 @@ from .. import activations\n from .. import initializers\n from .. import regularizers\n from .. import constraints\n-from ..engine import InputSpec\n-from ..engine import Layer\n+from ..engine.base_layer import InputSpec\n+from ..engine.base_layer import Layer\n from ..utils.generic_utils import func_dump\n from ..utils.generic_utils import func_load\n from ..utils.generic_utils import deserialize_keras_object\n\n@@ -8,7 +8,7 @@ from .. import backend as K\n from .. import initializers\n from .. import regularizers\n from .. import constraints\n-from ..engine import Layer\n+from ..engine.base_layer import Layer\n from ..legacy import interfaces\n \n \n\n@@ -10,8 +10,8 @@ from .. import activations\n from .. import initializers\n from .. import regularizers\n from .. import constraints\n-from ..engine import Layer\n-from ..engine import InputSpec\n+from ..engine.base_layer import Layer\n+from ..engine.base_layer import InputSpec\n from ..utils import conv_utils\n from ..legacy import interfaces\n \n\n@@ -4,7 +4,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-from ..engine.topology import Layer\n+from ..engine.base_layer import Layer\n from .. import backend as K\n \n \n\n@@ -5,7 +5,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-from ..engine import Layer\n+from ..engine.base_layer import Layer\n from .. import backend as K\n import numpy as np\n from ..legacy import interfaces\n\n@@ -5,7 +5,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-from ..engine import Layer, InputSpec\n+from ..engine.base_layer import Layer, InputSpec\n from .. import initializers\n from .. import regularizers\n from .. import constraints\n\n@@ -6,8 +6,8 @@ from __future__ import division\n from __future__ import print_function\n \n from .. import backend as K\n-from ..engine import Layer\n-from ..engine import InputSpec\n+from ..engine.base_layer import Layer\n+from ..engine.base_layer import InputSpec\n from ..utils import conv_utils\n from ..legacy import interfaces\n \n\n@@ -13,8 +13,8 @@ from .. import activations\n from .. import initializers\n from .. import regularizers\n from .. import constraints\n-from ..engine import Layer\n-from ..engine import InputSpec\n+from ..engine.base_layer import Layer\n+from ..engine.base_layer import InputSpec\n from ..utils.generic_utils import has_arg\n \n # Legacy support.\n\n@@ -6,10 +6,10 @@ from __future__ import division\n from __future__ import print_function\n \n import copy\n-from ..engine import Layer\n-from ..engine import InputSpec\n-from ..engine.topology import _object_list_uid\n+from ..engine.base_layer import Layer\n+from ..engine.base_layer import InputSpec\n from ..utils.generic_utils import has_arg\n+from ..utils.generic_utils import object_list_uid\n from .. import backend as K\n \n from . import recurrent\n@@ -71,7 +71,7 @@ class Wrapper(Layer):\n         # get the updates from the inner layer.\n         inner_inputs = inputs\n         if inputs is not None:\n-            uid = _object_list_uid(inputs)\n+            uid = object_list_uid(inputs)\n             if uid in self._input_map:\n                 inner_inputs = self._input_map[uid]\n \n@@ -206,7 +206,7 @@ class TimeDistributed(Wrapper):\n                 input_length = K.shape(inputs)[1]\n             # Shape: (num_samples * timesteps, ...). And track the\n             # transformation in self._input_map.\n-            input_uid = _object_list_uid(inputs)\n+            input_uid = object_list_uid(inputs)\n             inputs = K.reshape(inputs, (-1,) + input_shape[2:])\n             self._input_map[input_uid] = inputs\n             # (num_samples * timesteps, ...)\n\n@@ -6,7 +6,7 @@ import numpy as np\n import types as python_types\n import warnings\n \n-from ..engine.topology import Layer, InputSpec\n+from ..engine import Layer, InputSpec\n from .. import backend as K\n from ..utils.generic_utils import func_dump, func_load, has_arg\n from ..utils import conv_utils\n\n@@ -1,30 +1,21 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"Sequential model class and model-related utilities.\n+\"\"\"Model-related utilities.\n \"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import warnings\n-import copy\n-import json\n-import os\n-import yaml\n-import numpy as np\n-\n from . import backend as K\n-from . import optimizers\n-from . import layers as layer_module\n-from .utils.io_utils import ask_to_proceed_with_overwrite\n from .utils.generic_utils import has_arg\n+from .utils.generic_utils import to_list\n+from .engine.input_layer import Input\n+from .engine.input_layer import InputLayer\n from .engine.training import Model\n-from .engine import topology\n-from .engine.topology import Layer\n-from .engine.topology import Input\n-from .engine.topology import InputLayer\n-from .legacy import layers as legacy_layers\n-from .legacy import models as legacy_models\n-from .legacy import interfaces\n+from .engine.sequential import Sequential\n+from .engine.saving import save_model\n+from .engine.saving import load_model\n+from .engine.saving import model_from_config\n+from .engine.saving import model_from_yaml\n+from .engine.saving import model_from_json\n \n try:\n     import h5py\n@@ -32,1467 +23,6 @@ except ImportError:\n     h5py = None\n \n \n-def save_model(model, filepath, overwrite=True, include_optimizer=True):\n-    \"\"\"Save a model to a HDF5 file.\n-\n-    Note: Please also see\n-    [How can I install HDF5 or h5py to save my models in Keras?](\n-        /getting-started/faq/\n-        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)\n-    in the FAQ for instructions on how to install `h5py`.\n-\n-    The saved model contains:\n-        - the model's configuration (topology)\n-        - the model's weights\n-        - the model's optimizer's state (if any)\n-\n-    Thus the saved model can be reinstantiated in\n-    the exact same state, without any of the code\n-    used for model definition or training.\n-\n-    # Arguments\n-        model: Keras model instance to be saved.\n-        filepath: one of the following:\n-            - string, path where to save the model, or\n-            - h5py.File object where to save the model\n-        overwrite: Whether we should overwrite any existing\n-            model at the target location, or instead\n-            ask the user with a manual prompt.\n-        include_optimizer: If True, save optimizer's state together.\n-\n-    # Raises\n-        ImportError: if h5py is not available.\n-    \"\"\"\n-\n-    if h5py is None:\n-        raise ImportError('`save_model` requires h5py.')\n-\n-    def get_json_type(obj):\n-        \"\"\"Serialize any object to a JSON-serializable structure.\n-\n-        # Arguments\n-            obj: the object to serialize\n-\n-        # Returns\n-            JSON-serializable structure representing `obj`.\n-\n-        # Raises\n-            TypeError: if `obj` cannot be serialized.\n-        \"\"\"\n-        # if obj is a serializable Keras class instance\n-        # e.g. optimizer, layer\n-        if hasattr(obj, 'get_config'):\n-            return {'class_name': obj.__class__.__name__,\n-                    'config': obj.get_config()}\n-\n-        # if obj is any numpy type\n-        if type(obj).__module__ == np.__name__:\n-            if isinstance(obj, np.ndarray):\n-                return {'type': type(obj),\n-                        'value': obj.tolist()}\n-            else:\n-                return obj.item()\n-\n-        # misc functions (e.g. loss function)\n-        if callable(obj):\n-            return obj.__name__\n-\n-        # if obj is a python 'type'\n-        if type(obj).__name__ == type.__name__:\n-            return obj.__name__\n-\n-        raise TypeError('Not JSON Serializable:', obj)\n-\n-    from . import __version__ as keras_version\n-\n-    if not isinstance(filepath, h5py.File):\n-        # If file exists and should not be overwritten.\n-        if not overwrite and os.path.isfile(filepath):\n-            proceed = ask_to_proceed_with_overwrite(filepath)\n-            if not proceed:\n-                return\n-\n-        f = h5py.File(filepath, mode='w')\n-        opened_new_file = True\n-    else:\n-        f = filepath\n-        opened_new_file = False\n-\n-    try:\n-        f.attrs['keras_version'] = str(keras_version).encode('utf8')\n-        f.attrs['backend'] = K.backend().encode('utf8')\n-        f.attrs['model_config'] = json.dumps({\n-            'class_name': model.__class__.__name__,\n-            'config': model.get_config()\n-        }, default=get_json_type).encode('utf8')\n-\n-        model_weights_group = f.create_group('model_weights')\n-        if legacy_models.needs_legacy_support(model):\n-            model_layers = legacy_models.legacy_sequential_layers(model)\n-        else:\n-            model_layers = model.layers\n-        topology.save_weights_to_hdf5_group(model_weights_group, model_layers)\n-\n-        if include_optimizer and hasattr(model, 'optimizer'):\n-            if isinstance(model.optimizer, optimizers.TFOptimizer):\n-                warnings.warn(\n-                    'TensorFlow optimizers do not '\n-                    'make it possible to access '\n-                    'optimizer attributes or optimizer state '\n-                    'after instantiation. '\n-                    'As a result, we cannot save the optimizer '\n-                    'as part of the model save file.'\n-                    'You will have to compile your model again '\n-                    'after loading it. '\n-                    'Prefer using a Keras optimizer instead '\n-                    '(see keras.io/optimizers).')\n-            else:\n-                f.attrs['training_config'] = json.dumps({\n-                    'optimizer_config': {\n-                        'class_name': model.optimizer.__class__.__name__,\n-                        'config': model.optimizer.get_config()\n-                    },\n-                    'loss': model.loss,\n-                    'metrics': model.metrics,\n-                    'sample_weight_mode': model.sample_weight_mode,\n-                    'loss_weights': model.loss_weights,\n-                }, default=get_json_type).encode('utf8')\n-\n-                # Save optimizer weights.\n-                symbolic_weights = getattr(model.optimizer, 'weights')\n-                if symbolic_weights:\n-                    optimizer_weights_group = f.create_group('optimizer_weights')\n-                    weight_values = K.batch_get_value(symbolic_weights)\n-                    weight_names = []\n-                    for i, (w, val) in enumerate(zip(symbolic_weights,\n-                                                     weight_values)):\n-                        # Default values of symbolic_weights is /variable\n-                        # for Theano and CNTK\n-                        if K.backend() == 'theano' or K.backend() == 'cntk':\n-                            if hasattr(w, 'name'):\n-                                if w.name.split('/')[-1] == 'variable':\n-                                    name = str(w.name) + '_' + str(i)\n-                                else:\n-                                    name = str(w.name)\n-                            else:\n-                                name = 'param_' + str(i)\n-                        else:\n-                            if hasattr(w, 'name') and w.name:\n-                                name = str(w.name)\n-                            else:\n-                                name = 'param_' + str(i)\n-                        weight_names.append(name.encode('utf8'))\n-                    optimizer_weights_group.attrs['weight_names'] = weight_names\n-                    for name, val in zip(weight_names, weight_values):\n-                        param_dset = optimizer_weights_group.create_dataset(\n-                            name,\n-                            val.shape,\n-                            dtype=val.dtype)\n-                        if not val.shape:\n-                            # scalar\n-                            param_dset[()] = val\n-                        else:\n-                            param_dset[:] = val\n-        f.flush()\n-    finally:\n-        if opened_new_file:\n-            f.close()\n-\n-\n-def load_model(filepath, custom_objects=None, compile=True):\n-    \"\"\"Loads a model saved via `save_model`.\n-\n-    # Arguments\n-        filepath: one of the following:\n-            - string, path to the saved model, or\n-            - h5py.File object from which to load the model\n-        custom_objects: Optional dictionary mapping names\n-            (strings) to custom classes or functions to be\n-            considered during deserialization.\n-        compile: Boolean, whether to compile the model\n-            after loading.\n-\n-    # Returns\n-        A Keras model instance. If an optimizer was found\n-        as part of the saved model, the model is already\n-        compiled. Otherwise, the model is uncompiled and\n-        a warning will be displayed. When `compile` is set\n-        to False, the compilation is omitted without any\n-        warning.\n-\n-    # Raises\n-        ImportError: if h5py is not available.\n-        ValueError: In case of an invalid savefile.\n-    \"\"\"\n-    if h5py is None:\n-        raise ImportError('`load_model` requires h5py.')\n-\n-    if not custom_objects:\n-        custom_objects = {}\n-\n-    def convert_custom_objects(obj):\n-        \"\"\"Handles custom object lookup.\n-\n-        # Arguments\n-            obj: object, dict, or list.\n-\n-        # Returns\n-            The same structure, where occurrences\n-                of a custom object name have been replaced\n-                with the custom object.\n-        \"\"\"\n-        if isinstance(obj, list):\n-            deserialized = []\n-            for value in obj:\n-                deserialized.append(convert_custom_objects(value))\n-            return deserialized\n-        if isinstance(obj, dict):\n-            deserialized = {}\n-            for key, value in obj.items():\n-                deserialized[key] = convert_custom_objects(value)\n-            return deserialized\n-        if obj in custom_objects:\n-            return custom_objects[obj]\n-        return obj\n-\n-    opened_new_file = not isinstance(filepath, h5py.File)\n-    if opened_new_file:\n-        f = h5py.File(filepath, mode='r')\n-    else:\n-        f = filepath\n-\n-    try:\n-        # instantiate model\n-        model_config = f.attrs.get('model_config')\n-        if model_config is None:\n-            raise ValueError('No model found in config file.')\n-        model_config = json.loads(model_config.decode('utf-8'))\n-        model = model_from_config(model_config, custom_objects=custom_objects)\n-\n-        # set weights\n-        topology.load_weights_from_hdf5_group(f['model_weights'], model.layers)\n-\n-        # Early return if compilation is not required.\n-        if not compile:\n-            return model\n-\n-        # instantiate optimizer\n-        training_config = f.attrs.get('training_config')\n-        if training_config is None:\n-            warnings.warn('No training configuration found in save file: '\n-                          'the model was *not* compiled. Compile it manually.')\n-            return model\n-        training_config = json.loads(training_config.decode('utf-8'))\n-        optimizer_config = training_config['optimizer_config']\n-        optimizer = optimizers.deserialize(optimizer_config,\n-                                           custom_objects=custom_objects)\n-\n-        # Recover loss functions and metrics.\n-        loss = convert_custom_objects(training_config['loss'])\n-        metrics = convert_custom_objects(training_config['metrics'])\n-        sample_weight_mode = training_config['sample_weight_mode']\n-        loss_weights = training_config['loss_weights']\n-\n-        # Compile model.\n-        model.compile(optimizer=optimizer,\n-                      loss=loss,\n-                      metrics=metrics,\n-                      loss_weights=loss_weights,\n-                      sample_weight_mode=sample_weight_mode)\n-\n-        # Set optimizer weights.\n-        if 'optimizer_weights' in f:\n-            # Build train function (to get weight updates).\n-            if isinstance(model, Sequential):\n-                model.model._make_train_function()\n-            else:\n-                model._make_train_function()\n-            optimizer_weights_group = f['optimizer_weights']\n-            optimizer_weight_names = [n.decode('utf8') for n in\n-                                      optimizer_weights_group.attrs['weight_names']]\n-            optimizer_weight_values = [optimizer_weights_group[n] for n in\n-                                       optimizer_weight_names]\n-            try:\n-                model.optimizer.set_weights(optimizer_weight_values)\n-            except ValueError:\n-                warnings.warn('Error in loading the saved optimizer '\n-                              'state. As a result, your model is '\n-                              'starting with a freshly initialized '\n-                              'optimizer.')\n-        return model\n-    finally:\n-        if opened_new_file:\n-            f.close()\n-\n-\n-def model_from_config(config, custom_objects=None):\n-    \"\"\"Instantiates a Keras model from its config.\n-\n-    # Arguments\n-        config: Configuration dictionary.\n-        custom_objects: Optional dictionary mapping names\n-            (strings) to custom classes or functions to be\n-            considered during deserialization.\n-\n-    # Returns\n-        A Keras model instance (uncompiled).\n-\n-    # Raises\n-        TypeError: if `config` is not a dictionary.\n-    \"\"\"\n-    if isinstance(config, list):\n-        raise TypeError('`model_from_config` expects a dictionary, not a list. '\n-                        'Maybe you meant to use '\n-                        '`Sequential.from_config(config)`?')\n-    return layer_module.deserialize(config, custom_objects=custom_objects)\n-\n-\n-def model_from_yaml(yaml_string, custom_objects=None):\n-    \"\"\"Parses a yaml model configuration file and returns a model instance.\n-\n-    # Arguments\n-        yaml_string: YAML string encoding a model configuration.\n-        custom_objects: Optional dictionary mapping names\n-            (strings) to custom classes or functions to be\n-            considered during deserialization.\n-\n-    # Returns\n-        A Keras model instance (uncompiled).\n-    \"\"\"\n-    config = yaml.load(yaml_string)\n-    return layer_module.deserialize(config, custom_objects=custom_objects)\n-\n-\n-def model_from_json(json_string, custom_objects=None):\n-    \"\"\"Parses a JSON model configuration file and returns a model instance.\n-\n-    # Arguments\n-        json_string: JSON string encoding a model configuration.\n-        custom_objects: Optional dictionary mapping names\n-            (strings) to custom classes or functions to be\n-            considered during deserialization.\n-\n-    # Returns\n-        A Keras model instance (uncompiled).\n-    \"\"\"\n-    config = json.loads(json_string)\n-    return layer_module.deserialize(config, custom_objects=custom_objects)\n-\n-\n-class Sequential(Model):\n-    \"\"\"Linear stack of layers.\n-\n-    # Arguments\n-        layers: list of layers to add to the model.\n-\n-    # Note\n-        The first layer passed to a Sequential model\n-        should have a defined input shape. What that\n-        means is that it should have received an `input_shape`\n-        or `batch_input_shape` argument,\n-        or for some type of layers (recurrent, Dense...)\n-        an `input_dim` argument.\n-\n-    # Example\n-\n-        ```python\n-            model = Sequential()\n-            # first layer must have a defined input shape\n-            model.add(Dense(32, input_dim=500))\n-            # afterwards, Keras does automatic shape inference\n-            model.add(Dense(32))\n-\n-            # also possible (equivalent to the above):\n-            model = Sequential()\n-            model.add(Dense(32, input_shape=(500,)))\n-            model.add(Dense(32))\n-\n-            # also possible (equivalent to the above):\n-            model = Sequential()\n-            # here the batch dimension is None,\n-            # which means any batch size will be accepted by the model.\n-            model.add(Dense(32, batch_input_shape=(None, 500)))\n-            model.add(Dense(32))\n-        ```\n-    \"\"\"\n-\n-    def __init__(self, layers=None, name=None):\n-        self.layers = []  # Stack of layers.\n-        self.model = None  # Internal Model instance.\n-        self.inputs = []  # List of input tensors\n-        self.outputs = []  # List of length 1: the output tensor (unique).\n-        self._trainable = True\n-        self._initial_weights = None\n-\n-        # Model attributes.\n-        self._inbound_nodes = []\n-        self._outbound_nodes = []\n-        self.built = False\n-\n-        # Set model name.\n-        if not name:\n-            prefix = 'sequential_'\n-            name = prefix + str(K.get_uid(prefix))\n-        self.name = name\n-\n-        # Add to the model any layers passed to the constructor.\n-        if layers:\n-            for layer in layers:\n-                self.add(layer)\n-\n-    def add(self, layer):\n-        \"\"\"Adds a layer instance on top of the layer stack.\n-\n-        # Arguments\n-            layer: layer instance.\n-\n-        # Raises\n-            TypeError: If `layer` is not a layer instance.\n-            ValueError: In case the `layer` argument does not\n-                know its input shape.\n-            ValueError: In case the `layer` argument has\n-                multiple output tensors, or is already connected\n-                somewhere else (forbidden in `Sequential` models).\n-        \"\"\"\n-        if not isinstance(layer, Layer):\n-            raise TypeError('The added layer must be '\n-                            'an instance of class Layer. '\n-                            'Found: ' + str(layer))\n-        if not self.outputs:\n-            # First layer in model: check that it is an input layer.\n-            if not isinstance(layer, (InputLayer, legacy_layers.Merge)):\n-                # Create an input layer.\n-                # First, we need to infer its expected input shape and dtype.\n-                if isinstance(layer, (Model, Sequential)):\n-                    # We were passed a model as first layer.\n-                    # This requires a specific way to figure out the\n-                    # input shape and dtype.\n-                    if not layer.layers:\n-                        raise ValueError('Cannot add an empty model '\n-                                         'to a `Sequential` model.')\n-                    # In case of nested models: recover the first layer\n-                    # of the deepest model to infer input shape and dtype.\n-                    first_layer = layer.layers[0]\n-                    while isinstance(first_layer, (Model, Sequential)):\n-                        first_layer = first_layer.layers[0]\n-                    batch_shape = first_layer.batch_input_shape\n-                    dtype = first_layer.dtype\n-                else:\n-                    # We were passed a regular layer, and it should\n-                    # know about its input shape. Otherwise, that's an error.\n-                    if not hasattr(layer, 'batch_input_shape'):\n-                        raise ValueError('The first layer in a '\n-                                         'Sequential model must '\n-                                         'get an `input_shape` or '\n-                                         '`batch_input_shape` argument.')\n-                    batch_shape = layer.batch_input_shape\n-                    dtype = layer.dtype\n-                # Instantiate the input layer.\n-                x = Input(batch_shape=batch_shape,\n-                          dtype=dtype,\n-                          name=layer.name + '_input')\n-                # This will build the current layer\n-                # and create the node connecting the current layer\n-                # to the input layer we just created.\n-                layer(x)\n-\n-            if len(layer._inbound_nodes[-1].output_tensors) != 1:\n-                raise ValueError('All layers in a Sequential model '\n-                                 'should have a single output tensor. '\n-                                 'For multi-output layers, '\n-                                 'use the functional API.')\n-\n-            self.outputs = [layer._inbound_nodes[-1].output_tensors[0]]\n-            self.inputs = topology.get_source_inputs(self.outputs[0])\n-\n-            # We create an input node, which we will keep updated\n-            # as we add more layers\n-            topology.Node(outbound_layer=self,\n-                          inbound_layers=[],\n-                          node_indices=[],\n-                          tensor_indices=[],\n-                          input_tensors=self.inputs,\n-                          output_tensors=self.outputs,\n-                          # no model-level masking for now\n-                          input_masks=[None for _ in self.inputs],\n-                          output_masks=[None],\n-                          input_shapes=[x._keras_shape for x in self.inputs],\n-                          output_shapes=[self.outputs[0]._keras_shape])\n-        else:\n-            output_tensor = layer(self.outputs[0])\n-            if isinstance(output_tensor, list):\n-                raise TypeError('All layers in a Sequential model '\n-                                'should have a single output tensor. '\n-                                'For multi-output layers, '\n-                                'use the functional API.')\n-            self.outputs = [output_tensor]\n-            # update self._inbound_nodes\n-            self._inbound_nodes[0].output_tensors = self.outputs\n-            self._inbound_nodes[0].output_shapes = [self.outputs[0]._keras_shape]\n-\n-        self.layers.append(layer)\n-        self.built = False\n-\n-    def pop(self):\n-        \"\"\"Removes the last layer in the model.\n-\n-        # Raises\n-            TypeError: if there are no layers in the model.\n-        \"\"\"\n-        if not self.layers:\n-            raise TypeError('There are no layers in the model.')\n-\n-        self.layers.pop()\n-        if not self.layers:\n-            self.outputs = []\n-            self._inbound_nodes = []\n-            self._outbound_nodes = []\n-        else:\n-            self.layers[-1]._outbound_nodes = []\n-            self.outputs = [self.layers[-1].output]\n-            # update self._inbound_nodes\n-            self._inbound_nodes[0].output_tensors = self.outputs\n-            self._inbound_nodes[0].output_shapes = [self.outputs[0]._keras_shape]\n-        self.built = False\n-\n-    def get_layer(self, name=None, index=None):\n-        \"\"\"Retrieve a layer that is part of the model.\n-\n-        Returns a layer based on either its name (unique)\n-        or its index in the graph. Indices are based on\n-        order of horizontal graph traversal (bottom-up).\n-\n-        # Arguments\n-            name: string, name of layer.\n-            index: integer, index of layer.\n-\n-        # Returns\n-            A layer instance.\n-        \"\"\"\n-        if not self.built:\n-            self.build()\n-        return self.model.get_layer(name, index)\n-\n-    def call(self, inputs, mask=None):\n-        if not self.built:\n-            self.build()\n-        return self.model.call(inputs, mask)\n-\n-    def build(self, input_shape=None):\n-        if not self.inputs or not self.outputs:\n-            raise TypeError('Sequential model cannot be built: model is empty.'\n-                            ' Add some layers first.')\n-        # actually create the model\n-        self.model = Model(self.inputs, self.outputs[0],\n-                           name=self.name + '_model')\n-        self.model.trainable = self.trainable\n-\n-        # mirror model attributes\n-        self.supports_masking = self.model.supports_masking\n-        self._output_mask_cache = self.model._output_mask_cache\n-        self._output_tensor_cache = self.model._output_tensor_cache\n-        self._output_shape_cache = self.model._output_shape_cache\n-        self.input_layers = self.model.input_layers\n-        self.input_layers_node_indices = self.model.input_layers_node_indices\n-        self.input_layers_tensor_indices = self.model.input_layers_tensor_indices\n-        self.output_layers = self.model.output_layers\n-        self.output_layers_node_indices = self.model.output_layers_node_indices\n-        self.output_layers_tensor_indices = self.model.output_layers_tensor_indices\n-        self._nodes_by_depth = self.model._nodes_by_depth\n-        self.output_names = self.model.output_names\n-        self.input_names = self.model.input_names\n-        self._feed_input_names = self.model._feed_input_names\n-        self._feed_inputs = self.model._feed_inputs\n-        self._container_nodes = self.model._container_nodes\n-\n-        # Make sure child model callbacks\n-        # will call the parent Sequential model.\n-        self.model.callback_model = self\n-        self.built = True\n-\n-    @property\n-    def uses_learning_phase(self):\n-        if not self.built:\n-            self.build()\n-        return self.model.uses_learning_phase\n-\n-    @property\n-    def _flattened_layers(self):\n-        layers = []\n-        if self.layers:\n-            # Support for legacy models\n-            if isinstance(self.layers[0], legacy_layers.Merge):\n-                merge = self.layers[0]\n-                for layer in merge.layers:\n-                    if hasattr(layer, '_flattened_layers'):\n-                        for sublayer in layer._flattened_layers:\n-                            if sublayer not in layers:\n-                                layers.append(sublayer)\n-                    elif hasattr(layer, 'layers'):\n-                        for sublayer in layer.layers:\n-                            if sublayer not in layers:\n-                                layers.append(sublayer)\n-                    else:\n-                        if layer not in layers:\n-                            layers.append(layer)\n-            else:\n-                if self.layers[0] not in layers:\n-                    layers.append(self.layers[0])\n-            for layer in self.layers[1:]:\n-                if layer not in layers:\n-                    layers.append(layer)\n-        return layers\n-\n-    def _gather_list_attr(self, attr):\n-        all_attrs = []\n-        for layer in self._flattened_layers:\n-            all_attrs += getattr(layer, attr, [])\n-        return all_attrs\n-\n-    @property\n-    def trainable(self):\n-        return self._trainable\n-\n-    @trainable.setter\n-    def trainable(self, value):\n-        if self.built:\n-            self.model.trainable = value\n-        self._trainable = value\n-\n-    @property\n-    def trainable_weights(self):\n-        if not self.trainable:\n-            return []\n-        # Support for legacy behavior\n-        return self._gather_list_attr('trainable_weights')\n-\n-    @property\n-    def non_trainable_weights(self):\n-        # Support for legacy behavior\n-        weights = self._gather_list_attr('non_trainable_weights')\n-        if not self.trainable:\n-            trainable_weights = self._gather_list_attr('trainable_weights')\n-            return trainable_weights + weights\n-        return weights\n-\n-    @property\n-    def updates(self):\n-        if not self.built:\n-            self.build()\n-        return self.model.updates\n-\n-    @property\n-    def state_updates(self):\n-        if not self.built:\n-            self.build()\n-        return self.model.state_updates\n-\n-    def get_updates_for(self, inputs):\n-        if not self.built:\n-            self.build()\n-        return self.model.get_updates_for(inputs)\n-\n-    @property\n-    def losses(self):\n-        if not self.built:\n-            self.build()\n-        return self.model.losses\n-\n-    def get_losses_for(self, inputs):\n-        if not self.built:\n-            self.build()\n-        return self.model.get_losses_for(inputs)\n-\n-    @property\n-    def regularizers(self):\n-        if not self.built:\n-            self.build()\n-        return self.model.regularizers\n-\n-    def get_weights(self):\n-        \"\"\"Retrieves the weights of the model.\n-\n-        # Returns\n-            A flat list of Numpy arrays\n-            (one array per model weight).\n-        \"\"\"\n-        # Legacy support\n-        if legacy_models.needs_legacy_support(self):\n-            layers = legacy_models.legacy_sequential_layers(self)\n-            weights = []\n-            for layer in layers:\n-                weights.append(layer.get_weights())\n-            return weights\n-\n-        if not self.built:\n-            self.build()\n-        return self.model.get_weights()\n-\n-    def set_weights(self, weights):\n-        \"\"\"Sets the weights of the model.\n-\n-        # Arguments\n-            weights: Should be a list\n-                of Numpy arrays with shapes and types matching\n-                the output of `model.get_weights()`.\n-        \"\"\"\n-        # Legacy support\n-        if legacy_models.needs_legacy_support(self):\n-            layers = legacy_models.legacy_sequential_layers(self)\n-            for layer in layers:\n-                nb_param = len(layer.weights)\n-                layer.set_weights(weights[:nb_param])\n-                weights = weights[nb_param:]\n-\n-        if not self.built:\n-            self.build()\n-        self.model.set_weights(weights)\n-\n-    def load_weights(self, filepath, by_name=False, skip_mismatch=False, reshape=False):\n-        if h5py is None:\n-            raise ImportError('`load_weights` requires h5py.')\n-        with h5py.File(filepath, mode='r') as f:\n-            if 'layer_names' not in f.attrs and 'model_weights' in f:\n-                f = f['model_weights']\n-\n-            # Legacy support\n-            if legacy_models.needs_legacy_support(self):\n-                layers = legacy_models.legacy_sequential_layers(self)\n-            else:\n-                layers = self.layers\n-            if by_name:\n-                topology.load_weights_from_hdf5_group_by_name(f, layers,\n-                                                              skip_mismatch=skip_mismatch,\n-                                                              reshape=reshape)\n-            else:\n-                topology.load_weights_from_hdf5_group(f, layers, reshape=reshape)\n-\n-    def save_weights(self, filepath, overwrite=True):\n-        \"\"\"Saves the weights of a model.\n-\n-        Note: Please also see\n-        [How can I install HDF5 or h5py to save my models in Keras?](\n-        /getting-started/faq/\n-        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)\n-        in the FAQ for instructions on how to install `h5py`.\n-        \"\"\"\n-\n-        if h5py is None:\n-            raise ImportError('`save_weights` requires h5py.')\n-        # If file exists and should not be overwritten:\n-        if not overwrite and os.path.isfile(filepath):\n-            proceed = ask_to_proceed_with_overwrite(filepath)\n-            if not proceed:\n-                return\n-        # Legacy support\n-        if legacy_models.needs_legacy_support(self):\n-            layers = legacy_models.legacy_sequential_layers(self)\n-        else:\n-            layers = self.layers\n-\n-        with h5py.File(filepath, 'w') as f:\n-            topology.save_weights_to_hdf5_group(f, layers)\n-            f.flush()\n-\n-    def compile(self, optimizer, loss,\n-                metrics=None,\n-                sample_weight_mode=None,\n-                weighted_metrics=None,\n-                target_tensors=None,\n-                **kwargs):\n-        \"\"\"Configures the model for training.\n-\n-        # Arguments\n-            optimizer: String (name of optimizer) or optimizer object.\n-                See [optimizers](/optimizers).\n-            loss: String (name of objective function) or objective function.\n-                See [losses](/losses).\n-                If the model has multiple outputs, you can use a different loss\n-                on each output by passing a dictionary or a list of losses.\n-                The loss value that will be minimized by the model\n-                will then be the sum of all individual losses.\n-            metrics: List of metrics to be evaluated by the model\n-                during training and testing.\n-                Typically you will use `metrics=['accuracy']`.\n-                To specify different metrics for different outputs of a\n-                multi-output model, you could also pass a dictionary,\n-                such as `metrics={'output_a': 'accuracy'}`.\n-            sample_weight_mode: If you need to do timestep-wise\n-                sample weighting (2D weights), set this to `\"temporal\"`.\n-                `None` defaults to sample-wise weights (1D).\n-                If the model has multiple outputs, you can use a different\n-                `sample_weight_mode` on each output by passing a\n-                dictionary or a list of modes.\n-            weighted_metrics: List of metrics to be evaluated and weighted\n-                by sample_weight or class_weight during training and testing.\n-            target_tensors: By default, Keras will create a placeholder for the\n-                model's target, which will be fed with the target data during\n-                training. If instead you would like to use your own\n-                target tensor (in turn, Keras will not expect external\n-                Numpy data for these targets at training time), you\n-                can specify them via the `target_tensors` argument.\n-                It should be a single tensor\n-                (for a single-output `Sequential` model).\n-            **kwargs: When using the Theano/CNTK backends, these arguments\n-                are passed into `K.function`.\n-                When using the TensorFlow backend,\n-                these arguments are passed into `tf.Session.run`.\n-\n-        # Raises\n-            ValueError: In case of invalid arguments for\n-                `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n-\n-        # Example\n-            ```python\n-                model = Sequential()\n-                model.add(Dense(32, input_shape=(500,)))\n-                model.add(Dense(10, activation='softmax'))\n-                model.compile(optimizer='rmsprop',\n-                              loss='categorical_crossentropy',\n-                              metrics=['accuracy'])\n-            ```\n-        \"\"\"\n-        # create the underlying model\n-        self.build()\n-        # call compile method of Model class\n-        self.model.compile(optimizer, loss,\n-                           metrics=metrics,\n-                           sample_weight_mode=sample_weight_mode,\n-                           weighted_metrics=weighted_metrics,\n-                           target_tensors=target_tensors,\n-                           **kwargs)\n-        self.optimizer = self.model.optimizer\n-        self.loss = self.model.loss\n-        self.metrics = self.model.metrics\n-        self.loss_weights = self.model.loss_weights\n-        self.sample_weight_mode = self.model.sample_weight_mode\n-        self.weighted_metrics = self.model.weighted_metrics\n-        self.targets = self.model.targets\n-        self.metrics_tensors = self.model.metrics_tensors\n-        self.metrics_names = self.model.metrics_names\n-        self.sample_weights = self.model.sample_weights\n-        self.total_loss = self.model.total_loss\n-\n-    def fit(self,\n-            x=None,\n-            y=None,\n-            batch_size=None,\n-            epochs=1,\n-            verbose=1,\n-            callbacks=None,\n-            validation_split=0.,\n-            validation_data=None,\n-            shuffle=True,\n-            class_weight=None,\n-            sample_weight=None,\n-            initial_epoch=0,\n-            steps_per_epoch=None,\n-            validation_steps=None,\n-            **kwargs):\n-        \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n-\n-        # Arguments\n-            x: Numpy array of training data.\n-                If the input layer in the model is named, you can also pass a\n-                dictionary mapping the input name to a Numpy array.\n-                `x` can be `None` (default) if feeding from\n-                framework-native tensors (e.g. TensorFlow data tensors).\n-            y: Numpy array of target (label) data.\n-                If the output layer in the model is named, you can also pass a\n-                dictionary mapping the output name to a Numpy array.\n-                `y` can be `None` (default) if feeding from\n-                framework-native tensors (e.g. TensorFlow data tensors).\n-            batch_size: Integer or `None`.\n-                Number of samples per gradient update.\n-                If unspecified, it will default to 32.\n-            epochs: Integer. Number of epochs to train the model.\n-                An epoch is an iteration over the entire `x` and `y`\n-                data provided.\n-                Note that in conjunction with `initial_epoch`,\n-                `epochs` is to be understood as \"final epoch\".\n-                The model is not trained for a number of iterations\n-                given by `epochs`, but merely until the epoch\n-                of index `epochs` is reached.\n-            verbose: 0, 1, or 2. Verbosity mode.\n-                0 = silent, 1 = progress bar, 2 = one line per epoch.\n-            callbacks: List of `keras.callbacks.Callback` instances.\n-                List of callbacks to apply during training.\n-                See [callbacks](/callbacks).\n-            validation_split: Float between 0 and 1.\n-                Fraction of the training data to be used as validation data.\n-                The model will set apart this fraction of the training data,\n-                will not train on it, and will evaluate\n-                the loss and any model metrics\n-                on this data at the end of each epoch.\n-                The validation data is selected from the last samples\n-                in the `x` and `y` data provided, before shuffling.\n-            validation_data: tuple `(x_val, y_val)` or tuple\n-                `(x_val, y_val, val_sample_weights)` on which to evaluate\n-                the loss and any model metrics at the end of each epoch.\n-                The model will not be trained on this data.\n-                This will override `validation_split`.\n-            shuffle: Boolean (whether to shuffle the training data\n-                before each epoch) or str (for 'batch').\n-                'batch' is a special option for dealing with the\n-                limitations of HDF5 data; it shuffles in batch-sized chunks.\n-                Has no effect when `steps_per_epoch` is not `None`.\n-            class_weight: Optional dictionary mapping class indices (integers)\n-                to a weight (float) value, used for weighting the loss function\n-                (during training only).\n-                This can be useful to tell the model to\n-                \"pay more attention\" to samples from\n-                an under-represented class.\n-            sample_weight: Optional Numpy array of weights for\n-                the training samples, used for weighting the loss function\n-                (during training only). You can either pass a flat (1D)\n-                Numpy array with the same length as the input samples\n-                (1:1 mapping between weights and samples),\n-                or in the case of temporal data,\n-                you can pass a 2D array with shape\n-                `(samples, sequence_length)`,\n-                to apply a different weight to every timestep of every sample.\n-                In this case you should make sure to specify\n-                `sample_weight_mode=\"temporal\"` in `compile()`.\n-            initial_epoch: Epoch at which to start training\n-                (useful for resuming a previous training run).\n-            steps_per_epoch: Total number of steps (batches of samples)\n-                before declaring one epoch finished and starting the\n-                next epoch. When training with input tensors such as\n-                TensorFlow data tensors, the default `None` is equal to\n-                the number of samples in your dataset divided by\n-                the batch size, or 1 if that cannot be determined.\n-            validation_steps: Only relevant if `steps_per_epoch`\n-                is specified. Total number of steps (batches of samples)\n-                to validate before stopping.\n-\n-        # Returns\n-            A `History` object. Its `History.history` attribute is\n-            a record of training loss values and metrics values\n-            at successive epochs, as well as validation loss values\n-            and validation metrics values (if applicable).\n-\n-        # Raises\n-            RuntimeError: If the model was never compiled.\n-            ValueError: In case of mismatch between the provided input data\n-                and what the model expects.\n-        \"\"\"\n-        # Legacy support\n-        if 'nb_epoch' in kwargs:\n-            warnings.warn('The `nb_epoch` argument in `fit` '\n-                          'has been renamed `epochs`.')\n-            epochs = kwargs.pop('nb_epoch')\n-        if kwargs:\n-            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n-\n-        if not self.built:\n-            raise RuntimeError('The model needs to be compiled '\n-                               'before being used.')\n-        return self.model.fit(x, y,\n-                              batch_size=batch_size,\n-                              epochs=epochs,\n-                              verbose=verbose,\n-                              callbacks=callbacks,\n-                              validation_split=validation_split,\n-                              validation_data=validation_data,\n-                              shuffle=shuffle,\n-                              class_weight=class_weight,\n-                              sample_weight=sample_weight,\n-                              initial_epoch=initial_epoch,\n-                              steps_per_epoch=steps_per_epoch,\n-                              validation_steps=validation_steps)\n-\n-    def evaluate(self, x=None, y=None,\n-                 batch_size=None,\n-                 verbose=1,\n-                 sample_weight=None,\n-                 steps=None):\n-        \"\"\"Computes the loss on some input data, batch by batch.\n-\n-        # Arguments\n-            x: input data, as a Numpy array or list of Numpy arrays\n-                (if the model has multiple inputs).\n-                `x` can be `None` (default) if feeding from\n-                framework-native tensors (e.g. TensorFlow data tensors).\n-            y: labels, as a Numpy array.\n-                `y` can be `None` (default) if feeding from\n-                framework-native tensors (e.g. TensorFlow data tensors).\n-            batch_size: Integer. If unspecified, it will default to 32.\n-            verbose: verbosity mode, 0 or 1.\n-            sample_weight: sample weights, as a Numpy array.\n-            steps: Integer or `None`.\n-                Total number of steps (batches of samples)\n-                before declaring the evaluation round finished.\n-                Ignored with the default value of `None`.\n-\n-        # Returns\n-            Scalar test loss (if the model has no metrics)\n-            or list of scalars (if the model computes other metrics).\n-            The attribute `model.metrics_names` will give you\n-            the display labels for the scalar outputs.\n-\n-        # Raises\n-            RuntimeError: if the model was never compiled.\n-        \"\"\"\n-        if not self.built:\n-            raise RuntimeError('The model needs to be compiled '\n-                               'before being used.')\n-        return self.model.evaluate(x, y,\n-                                   batch_size=batch_size,\n-                                   verbose=verbose,\n-                                   sample_weight=sample_weight,\n-                                   steps=steps)\n-\n-    def predict(self, x, batch_size=None, verbose=0, steps=None):\n-        \"\"\"Generates output predictions for the input samples.\n-\n-        The input samples are processed batch by batch.\n-\n-        # Arguments\n-            x: the input data, as a Numpy array.\n-            batch_size: Integer. If unspecified, it will default to 32.\n-            verbose: verbosity mode, 0 or 1.\n-            steps: Total number of steps (batches of samples)\n-                before declaring the prediction round finished.\n-                Ignored with the default value of `None`.\n-\n-        # Returns\n-            A Numpy array of predictions.\n-        \"\"\"\n-        if not self.built:\n-            self.build()\n-        return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n-                                  steps=steps)\n-\n-    def predict_on_batch(self, x):\n-        \"\"\"Returns predictions for a single batch of samples.\n-\n-        # Arguments\n-            x: input data, as a Numpy array or list of Numpy arrays\n-                (if the model has multiple inputs).\n-\n-        # Returns\n-            A Numpy array of predictions.\n-        \"\"\"\n-        if not self.built:\n-            self.build()\n-        return self.model.predict_on_batch(x)\n-\n-    def train_on_batch(self, x, y, class_weight=None,\n-                       sample_weight=None):\n-        \"\"\"Single gradient update over one batch of samples.\n-\n-        # Arguments\n-            x: input data, as a Numpy array or list of Numpy arrays\n-                (if the model has multiple inputs).\n-            y: labels, as a Numpy array.\n-            class_weight: dictionary mapping classes to a weight value,\n-                used for scaling the loss function (during training only).\n-            sample_weight: sample weights, as a Numpy array.\n-\n-        # Returns\n-            Scalar training loss (if the model has no metrics)\n-            or list of scalars (if the model computes other metrics).\n-            The attribute `model.metrics_names` will give you\n-            the display labels for the scalar outputs.\n-\n-        # Raises\n-            RuntimeError: if the model was never compiled.\n-        \"\"\"\n-        if not self.built:\n-            raise RuntimeError('The model needs to be compiled '\n-                               'before being used.')\n-        return self.model.train_on_batch(x, y,\n-                                         sample_weight=sample_weight,\n-                                         class_weight=class_weight)\n-\n-    def test_on_batch(self, x, y,\n-                      sample_weight=None):\n-        \"\"\"Evaluates the model over a single batch of samples.\n-\n-        # Arguments\n-            x: input data, as a Numpy array or list of Numpy arrays\n-                (if the model has multiple inputs).\n-            y: labels, as a Numpy array.\n-            sample_weight: sample weights, as a Numpy array.\n-\n-        # Returns\n-            Scalar test loss (if the model has no metrics)\n-            or list of scalars (if the model computes other metrics).\n-            The attribute `model.metrics_names` will give you\n-            the display labels for the scalar outputs.\n-\n-        # Raises\n-            RuntimeError: if the model was never compiled.\n-        \"\"\"\n-        if not self.built:\n-            raise RuntimeError('The model needs to be compiled '\n-                               'before being used.')\n-        return self.model.test_on_batch(x, y,\n-                                        sample_weight=sample_weight)\n-\n-    def predict_proba(self, x, batch_size=None, verbose=0, steps=None):\n-        \"\"\"Generates class probability predictions for the input samples.\n-\n-        The input samples are processed batch by batch.\n-\n-        # Arguments\n-            x: input data, as a Numpy array or list of Numpy arrays\n-                (if the model has multiple inputs).\n-            batch_size: Integer. If unspecified, it will default to 32.\n-            verbose: verbosity mode, 0 or 1.\n-            steps: Total number of steps (batches of samples)\n-                before declaring the prediction round finished.\n-                Ignored with the default value of `None`.\n-\n-\n-        # Returns\n-            A Numpy array of probability predictions.\n-        \"\"\"\n-        preds = self.predict(x, batch_size, verbose, steps=steps)\n-        if preds.min() < 0. or preds.max() > 1.:\n-            warnings.warn('Network returning invalid probability values. '\n-                          'The last layer might not normalize predictions '\n-                          'into probabilities '\n-                          '(like softmax or sigmoid would).')\n-        return preds\n-\n-    def predict_classes(self, x, batch_size=None, verbose=0, steps=None):\n-        \"\"\"Generate class predictions for the input samples.\n-\n-        The input samples are processed batch by batch.\n-\n-        # Arguments\n-            x: input data, as a Numpy array or list of Numpy arrays\n-                (if the model has multiple inputs).\n-            batch_size: Integer. If unspecified, it will default to 32.\n-            verbose: verbosity mode, 0 or 1.\n-            steps: Total number of steps (batches of samples)\n-                before declaring the prediction round finished.\n-                Ignored with the default value of `None`.\n-\n-        # Returns\n-            A numpy array of class predictions.\n-        \"\"\"\n-        proba = self.predict(x, batch_size=batch_size, verbose=verbose,\n-                             steps=steps)\n-        if proba.shape[-1] > 1:\n-            return proba.argmax(axis=-1)\n-        else:\n-            return (proba > 0.5).astype('int32')\n-\n-    @interfaces.legacy_generator_methods_support\n-    def fit_generator(self,\n-                      generator,\n-                      steps_per_epoch=None,\n-                      epochs=1,\n-                      verbose=1,\n-                      callbacks=None,\n-                      validation_data=None,\n-                      validation_steps=None,\n-                      class_weight=None,\n-                      max_queue_size=10,\n-                      workers=1,\n-                      use_multiprocessing=False,\n-                      shuffle=True,\n-                      initial_epoch=0):\n-        \"\"\"Fits the model on data generated batch-by-batch by a Python generator.\n-\n-        The generator is run in parallel to the model, for efficiency.\n-        For instance, this allows you to do real-time data augmentation\n-        on images on CPU in parallel to training your model on GPU.\n-\n-        The use of `keras.utils.Sequence` guarantees the ordering\n-        and guarantees the single use of every input per epoch when\n-        using `use_multiprocessing=True`.\n-\n-        # Arguments\n-            generator: A generator or an instance of `Sequence`\n-                (`keras.utils.Sequence`) object in order to avoid duplicate data\n-                    when using multiprocessing.\n-                The output of the generator must be either\n-                - a tuple `(inputs, targets)`\n-                - a tuple `(inputs, targets, sample_weights)`.\n-                This tuple (a single output of the generator) makes a single\n-                batch. Therefore, all arrays in this tuple must have the same\n-                length (equal to the size of this batch). Different batches may\n-                have different sizes. For example, the last batch of the epoch\n-                is commonly smaller than the others, if the size of the dataset\n-                is not divisible by the batch size.\n-                The generator is expected to loop over its data\n-                indefinitely. An epoch finishes when `steps_per_epoch`\n-                batches have been seen by the model.\n-            steps_per_epoch: Total number of steps (batches of samples)\n-                to yield from `generator` before declaring one epoch\n-                finished and starting the next epoch. It should typically\n-                be equal to the number of samples of your dataset\n-                divided by the batch size.\n-                Optional for `Sequence`: if unspecified, will use\n-                the `len(generator)` as a number of steps.\n-            epochs: Integer, total number of iterations on the data.\n-                Note that in conjunction with initial_epoch, the parameter\n-                epochs is to be understood as \"final epoch\". The model is\n-                not trained for n steps given by epochs, but until the\n-                epoch epochs is reached.\n-            verbose: Integer. 0, 1, or 2. Verbosity mode.\n-                0 = silent, 1 = progress bar, 2 = one line per epoch.\n-            callbacks: List of `keras.callbacks.Callback` instances.\n-                List of callbacks to apply during training.\n-                See [callbacks](/callbacks).\n-            validation_data: This can be either\n-                - a generator for the validation data\n-                - a tuple `(inputs, targets)`\n-                - a tuple `(inputs, targets, sample_weights)`.\n-            validation_steps: Only relevant if `validation_data`\n-                is a generator. Total number of steps (batches of samples)\n-                to yield from `validation_data` generator before stopping\n-                at the end of every epoch. It should typically\n-                be equal to the number of samples of your\n-                validation dataset divided by the batch size.\n-                Optional for `Sequence`: if unspecified, will use\n-                the `len(validation_data)` as a number of steps.\n-            class_weight: Optional dictionary mapping class indices (integers)\n-                to a weight (float) value, used for weighting the loss function\n-                (during training only). This can be useful to tell the model to\n-                \"pay more attention\" to samples from an under-represented class.\n-            max_queue_size: Integer. Maximum size for the generator queue.\n-                If unspecified, `max_queue_size` will default to 10.\n-            workers: Integer. Maximum number of processes to spin up\n-                when using process-based threading.\n-                If unspecified, `workers` will default to 1. If 0, will\n-                execute the generator on the main thread.\n-            use_multiprocessing: Boolean.\n-                If `True`, use process-based threading.\n-                If unspecified, `use_multiprocessing` will default to `False`.\n-                Note that because this implementation relies on multiprocessing,\n-                you should not pass non-picklable arguments to the generator\n-                as they can't be passed easily to children processes.\n-            shuffle: Boolean (whether to shuffle the order of the batches at\n-                the beginning of each epoch. Only used with instances\n-                of `Sequence` (`keras.utils.Sequence`).\n-                Has no effect when `steps_per_epoch` is not `None`.\n-            initial_epoch: Integer.\n-                Epoch at which to start training\n-                (useful for resuming a previous training run).\n-\n-        # Returns\n-            A `History` object.\n-\n-        # Raises\n-            RuntimeError: if the model was never compiled.\n-            ValueError: In case the generator yields data in an invalid format.\n-\n-        # Example\n-\n-        ```python\n-            def generate_arrays_from_file(path):\n-                while True:\n-                    with open(path) as f:\n-                        for line in f:\n-                            # create Numpy arrays of input data\n-                            # and labels, from each line in the file\n-                            x, y = process_line(line)\n-                            yield (x, y)\n-\n-            model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n-                                steps_per_epoch=1000, epochs=10)\n-        ```\n-        \"\"\"\n-        if not self.built:\n-            raise RuntimeError('The model needs to be compiled '\n-                               'before being used.')\n-        return self.model.fit_generator(generator,\n-                                        steps_per_epoch,\n-                                        epochs,\n-                                        verbose=verbose,\n-                                        callbacks=callbacks,\n-                                        validation_data=validation_data,\n-                                        validation_steps=validation_steps,\n-                                        class_weight=class_weight,\n-                                        max_queue_size=max_queue_size,\n-                                        workers=workers,\n-                                        use_multiprocessing=use_multiprocessing,\n-                                        shuffle=shuffle,\n-                                        initial_epoch=initial_epoch)\n-\n-    @interfaces.legacy_generator_methods_support\n-    def evaluate_generator(self, generator, steps=None,\n-                           max_queue_size=10, workers=1,\n-                           use_multiprocessing=False):\n-        \"\"\"Evaluates the model on a data generator.\n-\n-        The generator should return the same kind of data\n-        as accepted by `test_on_batch`.\n-\n-        # Arguments\n-            generator: Generator yielding tuples (inputs, targets)\n-                or (inputs, targets, sample_weights)\n-            steps: Total number of steps (batches of samples)\n-                to yield from `generator` before stopping.\n-                Optional for `Sequence`: if unspecified, will use\n-                the `len(generator)` as a number of steps.\n-            max_queue_size: maximum size for the generator queue\n-            workers: maximum number of processes to spin up\n-            use_multiprocessing: if True, use process based threading.\n-                Note that because this implementation\n-                relies on multiprocessing, you should not pass\n-                non picklable arguments to the generator\n-                as they can't be passed easily to children processes.\n-\n-        # Returns\n-            Scalar test loss (if the model has no metrics)\n-            or list of scalars (if the model computes other metrics).\n-            The attribute `model.metrics_names` will give you\n-            the display labels for the scalar outputs.\n-\n-        # Raises\n-            RuntimeError: if the model was never compiled.\n-        \"\"\"\n-        if not self.built:\n-            raise RuntimeError('The model needs to be compiled '\n-                               'before being used.')\n-        return self.model.evaluate_generator(generator,\n-                                             steps,\n-                                             max_queue_size=max_queue_size,\n-                                             workers=workers,\n-                                             use_multiprocessing=use_multiprocessing)\n-\n-    @interfaces.legacy_generator_methods_support\n-    def predict_generator(self, generator, steps=None,\n-                          max_queue_size=10, workers=1,\n-                          use_multiprocessing=False, verbose=0):\n-        \"\"\"Generates predictions for the input samples from a data generator.\n-\n-        The generator should return the same kind of data as accepted by\n-        `predict_on_batch`.\n-\n-        # Arguments\n-            generator: generator yielding batches of input samples.\n-            steps: Total number of steps (batches of samples)\n-                to yield from `generator` before stopping.\n-                Optional for `Sequence`: if unspecified, will use\n-                the `len(generator)` as a number of steps.\n-            max_queue_size: maximum size for the generator queue\n-            workers: maximum number of processes to spin up\n-            use_multiprocessing: if True, use process based threading.\n-                Note that because this implementation\n-                relies on multiprocessing, you should not pass\n-                non picklable arguments to the generator\n-                as they can't be passed easily to children processes.\n-            verbose: verbosity mode, 0 or 1.\n-\n-        # Returns\n-            A Numpy array of predictions.\n-        \"\"\"\n-        if not self.built:\n-            self.build()\n-        return self.model.predict_generator(generator, steps,\n-                                            max_queue_size=max_queue_size,\n-                                            workers=workers,\n-                                            use_multiprocessing=use_multiprocessing,\n-                                            verbose=verbose)\n-\n-    def get_config(self):\n-        if isinstance(self.layers[0], legacy_layers.Merge):\n-            return self.legacy_get_config()\n-\n-        config = []\n-        for layer in self.layers:\n-            config.append({'class_name': layer.__class__.__name__,\n-                           'config': layer.get_config()})\n-        return copy.deepcopy(config)\n-\n-    @classmethod\n-    def from_config(cls, config, custom_objects=None):\n-        if 'class_name' not in config[0] or config[0]['class_name'] == 'Merge':\n-            return cls.legacy_from_config(config)\n-\n-        model = cls()\n-        for conf in config:\n-            layer = layer_module.deserialize(conf, custom_objects=custom_objects)\n-            model.add(layer)\n-        return model\n-\n-    def legacy_get_config(self):\n-        \"\"\"Retrieves the model configuration as a Python list.\n-\n-        # Returns\n-            A list of dicts (each dict is a layer config).\n-        \"\"\"\n-        config = []\n-        if isinstance(self.layers[0], legacy_layers.Merge):\n-            assert hasattr(self.layers[0], 'layers')\n-            layers = []\n-            for layer in self.layers[0].layers:\n-                layer_config = {'class_name': layer.__class__.__name__,\n-                                'config': layer.get_config()}\n-                layers.append(layer_config)\n-            merge_config = self.layers[0].get_config()\n-            merge_config['layers'] = layers\n-            config.append({'class_name': 'Merge', 'config': merge_config})\n-        else:\n-            config.append({'class_name': self.layers[0].__class__.__name__,\n-                           'config': self.layers[0].get_config()})\n-        for layer in self.layers[1:]:\n-            config.append({'class_name': layer.__class__.__name__,\n-                           'config': layer.get_config()})\n-        return copy.deepcopy(config)\n-\n-    @classmethod\n-    def legacy_from_config(cls, config, layer_cache=None):\n-        \"\"\"Load a model from a legacy configuration.\n-\n-        # Arguments\n-            config: dictionary with configuration.\n-            layer_cache: cache to draw pre-existing layer.\n-\n-        # Returns\n-            The loaded Model.\n-        \"\"\"\n-        if not layer_cache:\n-            layer_cache = {}\n-\n-        def normalize_legacy_config(conf):\n-            if 'class_name' not in conf:\n-                class_name = conf['name']\n-                name = conf.get('custom_name')\n-                conf['name'] = name\n-                return {'class_name': class_name,\n-                        'config': conf}\n-            return conf\n-\n-        # the model we will return\n-        model = cls()\n-\n-        def get_or_create_layer(layer_data):\n-            name = layer_data['config'].get('name')\n-            if name in layer_cache:\n-                return layer_cache[name]\n-            layer = layer_module.deserialize(layer_data)\n-            layer_cache[name] = layer\n-            return layer\n-\n-        first_layer = config[0]\n-        first_layer = normalize_legacy_config(first_layer)\n-        if first_layer['class_name'] == 'Merge':\n-            merge_inputs = []\n-            first_layer_config = first_layer['config']\n-            for merge_input_config in first_layer_config.pop('layers'):\n-                merge_input = layer_module.deserialize(merge_input_config)\n-                merge_inputs.append(merge_input)\n-            first_layer_config['layers'] = merge_inputs\n-            merge = legacy_layers.Merge.from_config(first_layer_config)\n-            model.add(merge)\n-        else:\n-            layer = get_or_create_layer(first_layer)\n-            model.add(layer)\n-\n-        for conf in config[1:]:\n-            conf = normalize_legacy_config(conf)\n-            layer = get_or_create_layer(conf)\n-            model.add(layer)\n-        return model\n-\n-\n def _clone_functional_model(model, input_tensors=None):\n     \"\"\"Clone a functional `Model` instance.\n \n@@ -1542,7 +72,7 @@ def _clone_functional_model(model, input_tensors=None):\n     else:\n         # Make sure that all input tensors come from a Keras layer.\n         # If tensor comes from an input layer: cache the input layer.\n-        input_tensors = topology._to_list(input_tensors)\n+        input_tensors = to_list(input_tensors)\n         _input_tensors = []\n         for i, x in enumerate(input_tensors):\n             if not K.is_keras_tensor(x):\n@@ -1580,7 +110,7 @@ def _clone_functional_model(model, input_tensors=None):\n                 # Reuse previously cloned layer.\n                 layer = layer_map[layer]\n                 # Don't call InputLayer multiple times.\n-                if isinstance(layer, topology.InputLayer):\n+                if isinstance(layer, InputLayer):\n                     continue\n \n             # Gather inputs to call the new layer.\n@@ -1605,9 +135,9 @@ def _clone_functional_model(model, input_tensors=None):\n                     if has_arg(layer.call, 'mask'):\n                         if 'mask' not in kwargs:\n                             kwargs['mask'] = computed_mask\n-                    output_tensors = topology._to_list(\n+                    output_tensors = to_list(\n                         layer(computed_tensor, **kwargs))\n-                    output_masks = topology._to_list(\n+                    output_masks = to_list(\n                         layer.compute_mask(computed_tensor,\n                                            computed_mask))\n                     computed_tensors = [computed_tensor]\n@@ -1618,9 +148,9 @@ def _clone_functional_model(model, input_tensors=None):\n                     if has_arg(layer.call, 'mask'):\n                         if 'mask' not in kwargs:\n                             kwargs['mask'] = computed_masks\n-                    output_tensors = topology._to_list(\n+                    output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n-                    output_masks = topology._to_list(\n+                    output_masks = to_list(\n                         layer.compute_mask(computed_tensors,\n                                            computed_masks))\n                 # Update tensor_map.\n@@ -1672,14 +202,14 @@ def _clone_sequential_model(model, input_tensors=None):\n     if input_tensors is None:\n         return Sequential(layers=layers, name=model.name)\n     else:\n-        if len(topology._to_list(input_tensors)) != 1:\n+        if len(to_list(input_tensors)) != 1:\n             raise ValueError('To clone a `Sequential` model, we expect '\n                              ' at most one tensor '\n                              'as part of `input_tensors`.')\n-        x = topology._to_list(input_tensors)[0]\n+        x = to_list(input_tensors)[0]\n         if K.is_keras_tensor(x):\n             origin_layer = x._keras_history[0]\n-            if isinstance(origin_layer, topology.InputLayer):\n+            if isinstance(origin_layer, InputLayer):\n                 return Sequential(layers=[origin_layer] + layers,\n                                   name=model.name)\n             else:\n\n@@ -139,7 +139,8 @@ def deserialize_keras_object(identifier, module_objects=None,\n         if hasattr(cls, 'from_config'):\n             custom_objects = custom_objects or {}\n             if has_arg(cls.from_config, 'custom_objects'):\n-                return cls.from_config(config['config'],\n+                return cls.from_config(\n+                    config['config'],\n                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n                                         list(custom_objects.items())))\n             with CustomObjectScope(custom_objects):\n@@ -384,7 +385,8 @@ class Progbar(object):\n             if self.target is not None and current < self.target:\n                 eta = time_per_unit * (self.target - current)\n                 if eta > 3600:\n-                    eta_format = '%d:%02d:%02d' % (eta // 3600, (eta % 3600) // 60, eta % 60)\n+                    eta_format = ('%d:%02d:%02d' %\n+                                  (eta // 3600, (eta % 3600) // 60, eta % 60))\n                 elif eta > 60:\n                     eta_format = '%d:%02d' % (eta // 60, eta % 60)\n                 else:\n@@ -440,3 +442,36 @@ class Progbar(object):\n \n     def add(self, n, values=None):\n         self.update(self._seen_so_far + n, values)\n+\n+\n+def to_list(x):\n+    \"\"\"Normalizes a list/tensor into a list.\n+\n+    If a tensor is passed, we return\n+    a list of size 1 containing the tensor.\n+\n+    # Arguments\n+        x: target object to be normalized.\n+\n+    # Returns\n+        A list.\n+    \"\"\"\n+    if isinstance(x, list):\n+        return x\n+    return [x]\n+\n+\n+def object_list_uid(object_list):\n+    object_list = to_list(object_list)\n+    return ', '.join([str(abs(id(x))) for x in object_list])\n+\n+\n+def is_all_none(iterable_or_element):\n+    if not isinstance(iterable_or_element, (list, tuple)):\n+        iterable = [iterable_or_element]\n+    else:\n+        iterable = iterable_or_element\n+    for element in iterable:\n+        if element is not None:\n+            return False\n+    return True\n\n@@ -106,7 +106,7 @@ def model_to_dot(model,\n         layer_id = str(id(layer))\n         for i, node in enumerate(layer._inbound_nodes):\n             node_key = layer.name + '_ib-' + str(i)\n-            if node_key in model._container_nodes:\n+            if node_key in model._network_nodes:\n                 for inbound_layer in node.inbound_layers:\n                     inbound_layer_id = str(id(inbound_layer))\n                     layer_id = str(id(layer))\n\n@@ -1,11 +1,10 @@\n import pytest\n import json\n import numpy as np\n-from numpy.testing import assert_allclose\n \n from keras.layers import Dense, Dropout, Conv2D, InputLayer\n from keras import layers\n-from keras.engine import Input, Layer, topology, get_source_inputs\n+from keras.engine import Input, Layer, saving, get_source_inputs\n from keras.models import Model, Sequential\n from keras import backend as K\n from keras.models import model_from_json, model_from_yaml\n@@ -541,7 +540,7 @@ def test_load_layers():\n     weight_tensor_td_conv_old.append(np.zeros((15,)))\n     td_conv_layer = model.layers[1]\n     td_conv_layer.layer.data_format = 'channels_first'\n-    weight_tensor_td_conv_new = topology.preprocess_weights_for_loading(\n+    weight_tensor_td_conv_new = saving.preprocess_weights_for_loading(\n         td_conv_layer,\n         weight_tensor_td_conv_old,\n         original_keras_version='1')\n@@ -559,7 +558,7 @@ def test_load_layers():\n             weight_tensor_bi_convlstm_old.append(np.zeros((10,)))  # bias\n \n     bi_convlstm_layer = model.layers[2]\n-    weight_tensor_bi_convlstm_new = topology.preprocess_weights_for_loading(\n+    weight_tensor_bi_convlstm_new = saving.preprocess_weights_for_loading(\n         bi_convlstm_layer,\n         weight_tensor_bi_convlstm_old,\n         original_keras_version='1')\n@@ -606,7 +605,7 @@ def test_preprocess_weights_for_loading(layer):\n     # A model is needed to initialize weights.\n     _ = Sequential([layer])\n     weights1 = layer.get_weights()\n-    weights2 = topology.preprocess_weights_for_loading(\n+    weights2 = saving.preprocess_weights_for_loading(\n         layer, convert_weights(layer, weights1),\n         original_keras_version='1')\n     assert all([np.allclose(x, y, 1e-5)\n@@ -623,7 +622,7 @@ def test_preprocess_weights_for_loading(layer):\n def test_preprocess_weights_for_loading_for_model(layer):\n     model = Sequential([layer])\n     weights1 = model.get_weights()\n-    weights2 = topology.preprocess_weights_for_loading(\n+    weights2 = saving.preprocess_weights_for_loading(\n         model, convert_weights(layer, weights1),\n         original_keras_version='1')\n     assert all([np.allclose(x, y, 1e-5)\n@@ -645,7 +644,7 @@ def test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, la\n     # A model is needed to initialize weights.\n     _ = Sequential([layer])\n     weights1 = layer.get_weights()\n-    weights2 = topology.preprocess_weights_for_loading(layer, weights1)\n+    weights2 = saving.preprocess_weights_for_loading(layer, weights1)\n     assert all([np.allclose(x, y, 1e-5) for (x, y) in zip(weights1, weights2)])\n \n \n\n@@ -8,7 +8,7 @@ import scipy.sparse as sparse\n import keras\n from keras import losses\n from keras.layers import Dense, Dropout\n-from keras.engine.topology import Input\n+from keras.engine import Input\n from keras.engine.training import Model\n from keras.engine.training import _check_loss_and_target_compatibility\n from keras.engine.training import _weighted_masked_objective\n\n@@ -5,7 +5,6 @@ from numpy.testing import assert_allclose\n from keras.utils.test_utils import layer_test\n from keras.utils.test_utils import keras_test\n from keras import backend as K\n-from keras.engine.topology import InputLayer\n from keras.layers import convolutional\n from keras.layers import pooling\n from keras.models import Sequential\n\n@@ -390,7 +390,7 @@ def test_load_weights_between_noncudnn_rnn(rnn_type, to_cudnn, bidirectional, im\n \n     def convert_weights(source_layer, target_layer):\n         weights = source_layer.get_weights()\n-        weights = keras.engine.topology.preprocess_weights_for_loading(target_layer, weights)\n+        weights = keras.engine.saving.preprocess_weights_for_loading(target_layer, weights)\n         target_layer.set_weights(weights)\n \n     input_layer = keras.layers.InputLayer(input_shape)\n@@ -490,7 +490,7 @@ def test_preprocess_weights_for_loading_gru_incompatible():\n \n     def assert_not_compatible(src, dest, message):\n         with pytest.raises(ValueError) as ex:\n-            keras.engine.topology.preprocess_weights_for_loading(\n+            keras.engine.saving.preprocess_weights_for_loading(\n                 dest, initialize_weights(src).get_weights())\n         assert message in ex.value.message\n \n\n@@ -9,8 +9,8 @@ from keras.layers import recurrent\n from keras.layers import embeddings\n from keras.models import Sequential\n from keras.models import Model\n-from keras.engine.topology import Input\n-from keras.layers.core import Masking\n+from keras.engine import Input\n+from keras.layers import Masking\n from keras import regularizers\n from keras import backend as K\n \n\n@@ -9,7 +9,7 @@ from keras.layers import RNN\n from keras import layers\n from keras.models import Sequential, Model, model_from_json\n from keras import backend as K\n-from keras.engine.topology import _object_list_uid, _to_list\n+from keras.utils.generic_utils import object_list_uid, to_list\n \n \n @keras_test\n@@ -121,7 +121,7 @@ def test_TimeDistributed():\n     assert not np.array_equal(td.get_weights()[2], np.array([0, 0]))\n     assert not np.array_equal(td.get_weights()[3], np.array([1, 1]))\n     # Verify input_map has one mapping from inputs to reshaped inputs.\n-    uid = _object_list_uid(model.inputs)\n+    uid = object_list_uid(model.inputs)\n     assert len(td._input_map.keys()) == 1\n     assert uid in td._input_map\n     assert K.int_shape(td._input_map[uid]) == (None, 2)\n@@ -280,12 +280,12 @@ def test_Bidirectional_merged_value(merge_mode):\n     # basic case\n     inputs = Input((timesteps, dim))\n     layer = wrappers.Bidirectional(rnn(units, return_sequences=True), merge_mode=merge_mode)\n-    f_merged = K.function([inputs], _to_list(layer(inputs)))\n+    f_merged = K.function([inputs], to_list(layer(inputs)))\n     f_forward = K.function([inputs], [layer.forward_layer.call(inputs)])\n     f_backward = K.function([inputs], [K.reverse(layer.backward_layer.call(inputs), 1)])\n \n     y_merged = f_merged(X)\n-    y_expected = _to_list(merge_func(f_forward(X)[0], f_backward(X)[0]))\n+    y_expected = to_list(merge_func(f_forward(X)[0], f_backward(X)[0]))\n     assert len(y_merged) == len(y_expected)\n     for x1, x2 in zip(y_merged, y_expected):\n         assert_allclose(x1, x2, atol=1e-5)\n@@ -301,7 +301,7 @@ def test_Bidirectional_merged_value(merge_mode):\n     y_merged = f_merged(X)\n     y_forward = f_forward(X)\n     y_backward = f_backward(X)\n-    y_expected = _to_list(merge_func(y_forward[0], y_backward[0]))\n+    y_expected = to_list(merge_func(y_forward[0], y_backward[0]))\n     assert len(y_merged) == len(y_expected) + n_states * 2\n     for x1, x2 in zip(y_merged, y_expected):\n         assert_allclose(x1, x2, atol=1e-5)\n@@ -328,19 +328,19 @@ def test_Bidirectional_dropout(merge_mode):\n     inputs = Input((timesteps, dim))\n     wrapped = wrappers.Bidirectional(rnn(units, dropout=0.2, recurrent_dropout=0.2),\n                                      merge_mode=merge_mode)\n-    outputs = _to_list(wrapped(inputs, training=True))\n+    outputs = to_list(wrapped(inputs, training=True))\n     assert all(not getattr(x, '_uses_learning_phase') for x in outputs)\n \n     inputs = Input((timesteps, dim))\n     wrapped = wrappers.Bidirectional(rnn(units, dropout=0.2, return_state=True),\n                                      merge_mode=merge_mode)\n-    outputs = _to_list(wrapped(inputs))\n+    outputs = to_list(wrapped(inputs))\n     assert all(x._uses_learning_phase for x in outputs)\n \n     model = Model(inputs, outputs)\n     assert model.uses_learning_phase\n-    y1 = _to_list(model.predict(X))\n-    y2 = _to_list(model.predict(X))\n+    y1 = to_list(model.predict(X))\n+    y2 = to_list(model.predict(X))\n     for x1, x2 in zip(y1, y2):\n         assert_allclose(x1, x2, atol=1e-5)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
