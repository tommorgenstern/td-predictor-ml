{"custom_id": "scrapy#5f5962e6a481a88537e2f53663389033fdfa6ed4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 354 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -38,15 +38,6 @@ class Scheduler(object) :\n         necesarily the order you put them in.\n \n     ``pending_domains_count`` contains the names of all domains that are to be scheduled.\n-\n-    Two crawling orders are available by default, which can be set with the\n-    SCHEDULER_ORDER settings:\n-\n-        * BFO - breath-first order (default). Consumes more memory than DFO but reaches\n-                most relevant pages faster.\n-\n-        * DFO - depth-first order. Consumes less memory than BFO but usually takes\n-                longer to reach the most relevant pages.\n     \"\"\"\n \n     def __init__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4d16571988c06315d29eac0a2c7cc264fa819d81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 665 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -481,6 +481,7 @@ class ExecutionEngine(object):\n         try:\n             dispatcher.send(signal=signals.domain_idle, sender=self.__class__, domain=domain, spider=spider)\n         except DontCloseDomain:\n+            self.next_request(spider)\n             return\n         except:\n             log.exc(\"Exception catched on domain_idle signal dispatch\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#be5106d7302b1c4f8eb9c3a37b9358647face59d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/6 | Churn Δ: 56 | Churn Cumulative: 721 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,37 @@\n+\"\"\"\n+DelayedCloseDomain is an extension that keeps open a domain until a\n+configurable amount of idle time is reached\n+\"\"\"\n+\n+from datetime import datetime\n+\n+from pydispatch import dispatcher\n+from collections import defaultdict\n+\n+from scrapy.core import signals\n+from scrapy.core.engine import scrapyengine\n+from scrapy.core.exceptions import NotConfigured, DontCloseDomain\n+from scrapy.conf import settings\n+\n+\n+class DelayedCloseDomain(object):\n+    def __init__(self):\n+        self.delay = settings.getint('DOMAIN_CLOSE_DELAY')\n+        if not self.delay:\n+            raise NotConfigured\n+\n+        self.opened_at = defaultdict(datetime.now)\n+        dispatcher.connect(self.domain_idle, signal=signals.domain_idle)\n+        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+\n+    def domain_idle(self, domain):\n+        lastseen = scrapyengine.downloader.lastseen(domain)\n+        if not lastseen:\n+            lastseen = self.opened_at[domain]\n+\n+        delta = datetime.now() - lastseen\n+        if delta.seconds < self.delay:\n+            raise DontCloseDomain\n+\n+    def domain_closed(self, domain):\n+        self.opened_at.pop(domain, None)\n\n@@ -49,7 +49,6 @@ class ExecutionEngine(object):\n     def __init__(self):\n         self.configured = False\n         self.keep_alive = False\n-        self.domain_close_delay = settings.getint('DOMAIN_CLOSE_DELAY') # seconds after an idle domain will be closed\n         self.initializing = set() # domais in intialization state\n         self.cancelled = set() # domains in cancelation state\n         self.debug_mode = settings.getbool('ENGINE_DEBUG')\n@@ -211,17 +210,7 @@ class ExecutionEngine(object):\n         downloading = not self.downloader.domain_is_idle(domain)\n         haspipe = not self.pipeline.domain_is_idle(domain)\n         oninit = domain in self.initializing\n-        if pending or downloading or haspipe or oninit or scraping:\n-            return False\n-        elif self.domain_close_delay:\n-            lastseen = self.downloader.lastseen(domain)\n-            if not lastseen: # domain not yet started\n-                return False\n-            now = datetime.now()\n-            delta = now - lastseen\n-            return (delta.seconds >= self.domain_close_delay)\n-        else:\n-            return True\n+        return not (pending or downloading or haspipe or oninit or scraping)\n \n     def domain_is_open(self, domain):\n         return domain in self.downloader.sites\n@@ -319,12 +308,6 @@ class ExecutionEngine(object):\n             if not self.next_domain():\n                 return self._stop_if_idle()\n \n-        # purge idle domains - (domain_close_delay support)\n-        if self.domain_close_delay:\n-            for domain in self.downloader.sites:\n-                if self.domain_is_idle(domain):\n-                    self._domain_idle(domain)\n-\n     def _add_starter(self, request, spider, domain_priority):\n         domain = spider.domain_name\n         if not self.scheduler.is_pending(domain):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#558a1a5033a2c01074af10a3a5fc374f064c0169", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 23 | Churn Cumulative: 755 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 4 | DMM Complexity: 0.9375\n\nDIFF:\n@@ -45,9 +45,14 @@ class FormRequest(Request):\n         except IndexError:\n             raise IndexError(\"Form number %d not found in %s\" % (formnumber, response))\n         if formdata:\n+            # remove all existing fields with the same name before, so that\n+            # formdata fields properly can properly override existing ones,\n+            # which is the desired behaviour\n+            form.controls = [c for c in form.controls if c.name not in formdata.keys()]\n             for k, v in formdata.iteritems():\n                 for v2 in v if hasattr(v, '__iter__') else [v]:\n                     form.new_control('text', k, {'value': v2})\n+                    \n         url, body, headers = form.click_request_data()       \n-        request = cls(url, body=body, headers=headers, **kwargs)\n+        request = cls(url, method=form.method, body=body, headers=headers, **kwargs)\n         return request\n\n@@ -209,6 +209,8 @@ class FormRequestTest(unittest.TestCase):\n         \"\"\"\n         response = Response(\"http://www.example.com/this/list.html\", body=respbody)\n         r1 = FormRequest.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'}, callback=lambda x: x)\n+        self.assertEqual(r1.method, 'POST')\n+        self.assertEqual(r1.headers['Content-type'], 'application/x-www-form-urlencoded')\n         fs = cgi.FieldStorage(StringIO(r1.body), r1.headers, environ={\"REQUEST_METHOD\": \"POST\"})\n         self.assertEqual(r1.url, \"http://www.example.com/this/post.php\")\n         self.assertEqual(set([f.value for f in fs[\"test\"]]), set([\"val1\", \"val2\"]))\n@@ -226,6 +228,7 @@ class FormRequestTest(unittest.TestCase):\n         \"\"\"\n         response = Response(\"http://www.example.com/this/list.html\", body=respbody)\n         r1 = FormRequest.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n+        self.assertEqual(r1.method, 'GET')\n         self.assertEqual(r1.url.hostname, \"www.example.com\")\n         self.assertEqual(r1.url.path, \"/this/get.php\")\n         urlargs = cgi.parse_qs(r1.url.query)\n@@ -234,6 +237,19 @@ class FormRequestTest(unittest.TestCase):\n         self.assertEqual(urlargs['test2'], ['xxx'])\n         self.assertEqual(urlargs['six'], ['seven'])\n \n+    def test_from_response_override_params(self):\n+        respbody = \"\"\"\n+<form action=\"get.php\" method=\"POST\">\n+<input type=\"hidden\" name=\"one\" value=\"1\">\n+<input type=\"hidden\" name=\"two\" value=\"3\">\n+</form>\n+        \"\"\"\n+        response = Response(\"http://www.example.com/this/list.html\", body=respbody)\n+        r1 = FormRequest.from_response(response, formdata={'two': '2'})\n+        fs = cgi.FieldStorage(StringIO(r1.body), r1.headers, environ={\"REQUEST_METHOD\": \"POST\"})\n+        self.assertEqual(fs['one'].value, '1')\n+        self.assertEqual(fs['two'].value, '2')\n+\n     def test_from_response_errors_noform(self):\n         respbody = \"\"\"<html></html>\"\"\"\n         response = Response(\"http://www.example.com/lala.html\", body=respbody)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f22dfecb834cf68916e9906c68503c2a664d38a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 388 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -213,6 +213,7 @@ class ClusterMaster(object):\n             self.get_spider_groupsettings = my_import(settings[\"GROUPSETTINGS_MODULE\"]).get_spider_groupsettings\n         else:\n             self.get_spider_groupsettings = lambda x: {}\n+\n         # load pending domains\n         try:\n             statefile = open(self.statefile, \"r\")\n@@ -220,13 +221,16 @@ class ClusterMaster(object):\n             log.msg(\"ClusterMaster: Loaded state from %s\" % self.statefile)\n         except IOError:\n             self.pending = []\n+\n         self.loading = []\n         self.nodes = {}\n         self.nodesconf = settings.get('CLUSTER_MASTER_NODES', {})\n         self.start_time = datetime.datetime.utcnow()\n+\n         # for more info about statistics see self.update_nodes() and ClusterNodeBroker.remote_update()\n         self.statistics = {\"domains\": {\"running\": set(), \"scraped\": {}, \"lost_count\": {}, \"lost\": set()}, \"scraped_count\": 0 }\n         self.global_settings = {}\n+\n         # load cluster global settings\n         for sname in settings.getlist('GLOBAL_CLUSTER_SETTINGS'):\n             self.global_settings[sname] = settings[sname]\n@@ -291,7 +295,7 @@ class ClusterMaster(object):\n     def _schedule(self, domains, spider_settings=None, priority=20):\n         \"\"\"Private method which performs the schedule of the given domains,\n         with the given priority. Used for both scheduling and rescheduling.\"\"\"\n-        insert_pos = len([p for p in self.pending if ['priority'] <= priority])\n+        insert_pos = len([p for p in self.pending if p['priority'] <= priority])\n         for domain in domains:\n             pd = self.get_first_pending(domain)\n             if pd: # domain already pending, so just change priority if new is higher\n@@ -341,7 +345,7 @@ class ClusterMaster(object):\n         \"\"\"Remove all scheduled instances of the given domains (if they haven't\n         started yet). Otherwise use stop() to stop running domains\"\"\"\n \n-        self.pending = [p for p in self.pending if ['domain'] not in domains]\n+        self.pending = [p for p in self.pending if p['domain'] not in domains]\n \n     def discard(self, domains):\n         \"\"\"Stop and remove all running and pending instances of the given\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5861b345c72c817a53148316fdc1dc351af5d186", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 118 | Lines Deleted: 67 | Files Changed: 2 | Hunks: 11 | Methods Changed: 40 | Complexity Δ (Sum/Max): 20/28 | Churn Δ: 185 | Churn Cumulative: 301 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,9 @@\n from pydispatch import dispatcher\n \n from collections import defaultdict\n-from cookielib import CookieJar\n \n from scrapy.core import signals\n-from scrapy.utils.misc import dict_updatedefault\n+from scrapy.utils.cookies import CookieJar\n from scrapy import log\n \n class CookiesMiddleware(object):\n@@ -19,84 +18,29 @@ class CookiesMiddleware(object):\n             return\n \n         jar = self.cookies[spider.domain_name]\n-        wreq = _WrappedRequest(request)\n+        for name, cookie in request.cookies.items():\n+            jar.set_cookie_if_ok(cookie, request)\n \n-        # TODO: Merge cookies in request with jar here\n-        # for key, value in request.cookies.items():\n-        #     jar.set_cookie(..)\n-\n-        # set Cookie header with cookies in jar\n+        # set Cookie header\n         jar.add_cookie_header(wreq)\n \n-        print request.url.netloc, '    Cookie: ', request.headers.get('Cookie'), request.cookies\n-\n     def process_response(self, request, response, spider):\n-        print request.url.netloc, 'Set-Cookie:', response.headers.get('Set-Cookie'), request.cookies\n+        if request.meta.get('dont_merge_cookies', False):\n+            return\n \n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n-        wreq = _WrappedRequest(request)\n-        wrsp = _WrappedResponse(response)\n         jar = self.cookies[spider.domain_name]\n-\n         jar.extract_cookies(wrsp, wreq)\n \n         # TODO: set current cookies in jar to response.cookies?\n         return response\n \n+    # cookies should be set on non-200 responses too\n+    def process_exception(self, request, exception, spider):\n+        if isinstance(exception, HttpException):\n+            self.process_response(request, exception.response, spider)\n+\n     def domain_closed(self, domain):\n         if domain in self.cookies:\n             del self.cookies[domain]\n \n-\n-class _WrappedRequest(object):\n-    \"\"\"Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class\n-\n-    see http://docs.python.org/library/urllib2.html#urllib2.Request\n-    \"\"\"\n-\n-    def __init__(self, request):\n-        self.request = request\n-\n-    def get_full_url(self):\n-        return self.request.url\n-\n-    def get_host(self):\n-        return self.request.url.netloc\n-\n-    def get_type(self):\n-        return self.request.url.scheme\n-\n-    def is_unverifiable(self):\n-        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n-\n-        It defaults to False. An unverifiable request is one whose URL the user did not have the\n-        option to approve. For example, if the request is for an image in an\n-        HTML document, and the user had no option to approve the automatic\n-        fetching of the image, this should be true.\n-        \"\"\"\n-        return self.request.meta.get('is_unverifiable', False)\n-\n-    def get_origin_req_host(self):\n-        return self.request.hostname\n-\n-    def has_header(self, name):\n-        return name in self.request.headers\n-\n-    def header_items(self):\n-        return self.request.headers.items()\n-\n-    def add_unredirected_header(self, name, value):\n-        # XXX: review please, not sure how to handle this\n-        self.request.headers[name] = value\n-\n-\n-class _WrappedResponse(object):\n-\n-    def __init__(self, response):\n-        self.response = response\n-\n-    def info(self):\n-        return self\n-\n-    def getheaders(self, name):\n-        return self.response.headers.getlist(name)\n\n@@ -0,0 +1,107 @@\n+from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy\n+\n+\n+class CookieJar(object):\n+    def __init__(self, policy=None):\n+        self.jar = _CookieJar(policy or DefaultCookiePolicy())\n+\n+    def extract_cookies(self, response, request):\n+        wreq = WrappedRequest(request)\n+        wrsp = WrappedResponse(response)\n+        return self.jar.extract_cookies(wrsp, wreq)\n+\n+    def add_cookie_header(self, request):\n+        wreq = WrappedRequest(request)\n+        self.jar.add_cookie_header(wreq)\n+        for hdr in ('Cookie', 'Cookie2'):\n+            v = wreq.get_header(hdr)\n+            if hdr:\n+                request.headers[hdr] = v\n+\n+\n+    @property\n+    def _cookies(self):\n+        return self.jar._cookies\n+\n+    def clear_session_cookies(self, *args, **kwargs):\n+        return self.jar.clear_session_cookies(*args, **kwargs)\n+\n+    def clear(self):\n+        return self.jar.clear()\n+\n+    def __iter__(self):\n+        return iter(self.jar)\n+\n+    def __len__(self):\n+        return len(self.jar)\n+\n+    def set_policy(self, pol):\n+        return self.jar.set_policy(pol)\n+\n+    def make_cookies(self, response, request):\n+        wreq = WrappedRequest(request)\n+        wrsp = WrappedResponse(response)\n+        return self.jar.make_cookies(wrsp, wreq)\n+\n+    def set_cookie(self, cookie):\n+        self.jar.set_cookie(cookie)\n+\n+    def set_cookie_if_ok(self, cookie, request):\n+        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))\n+\n+\n+class WrappedRequest(object):\n+    \"\"\"Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class\n+\n+    see http://docs.python.org/library/urllib2.html#urllib2.Request\n+    \"\"\"\n+\n+    def __init__(self, request):\n+        self.request = request\n+\n+    def get_full_url(self):\n+        return self.request.url\n+\n+    def get_host(self):\n+        return self.request.url.netloc\n+\n+    def get_type(self):\n+        return self.request.url.scheme\n+\n+    def is_unverifiable(self):\n+        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n+\n+        It defaults to False. An unverifiable request is one whose URL the user did not have the\n+        option to approve. For example, if the request is for an image in an\n+        HTML document, and the user had no option to approve the automatic\n+        fetching of the image, this should be true.\n+        \"\"\"\n+        return self.request.meta.get('is_unverifiable', False)\n+\n+    def get_origin_req_host(self):\n+        return self.request.hostname\n+\n+    def has_header(self, name):\n+        return name in self.request.headers\n+\n+    def get_header(self, name, default=None):\n+        return self.request.headers.get(name, default)\n+\n+    def header_items(self):\n+        return self.request.headers.items()\n+\n+    def add_unredirected_header(self, name, value):\n+        self.request.headers[name] = value\n+        #print 'add_unredirected_header', self.request.headers\n+\n+\n+class WrappedResponse(object):\n+\n+    def __init__(self, response):\n+        self.response = response\n+\n+    def info(self):\n+        return self\n+\n+    def getheaders(self, name):\n+        return self.response.headers.getlist(name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3170dd9fdbce4baf377d0ea4751e57ec97ae6c37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 329 | Contributors (this commit): 3 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,7 +23,9 @@ class RedirectMiddleware(object):\n \n         if status in [302, 303]:\n             redirected_url = urljoin(request.url, response.headers['location'])\n-            redirected = request.replace(url=redirected_url, method='GET', body=None)\n+            redirected = request.replace(url=redirected_url, method='GET', body='')\n+            redirected.headers.pop('Content-Type', None)\n+            redirected.headers.pop('Content-Length', None)\n             return self._redirect(redirected, request, spider, status)\n \n         if status in [301, 307]:\n\n@@ -18,7 +18,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n     def tearDown(self):\n         dupefilter.close('scrapytest.org')\n \n-    def test_process_exception(self):\n+    def test_redirect_301(self):\n         url = 'http://www.example.com/301'\n         url2 = 'http://www.example.com/redirected'\n         req = Request(url)\n@@ -30,9 +30,11 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, url2)\n \n+    def test_redirect_302(self):\n         url = 'http://www.example.com/302'\n         url2 = 'http://www.example.com/redirected2'\n-        req = Request(url, method='POST')\n+        req = Request(url, method='POST', body='test', \n+            headers={'Content-Type': 'text/plain', 'Content-length': '4'})\n         hdr = Headers({'Location': [url2]})\n         rsp = Response(url, headers=hdr)\n         exc = HttpException('302', None, rsp)\n@@ -41,9 +43,14 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, url2)\n         self.assertEqual(req2.method, 'GET')\n-        assert not req2.body\n+        assert 'Content-Type' not in req2.headers, \\\n+            \"Content-Type header must not be present in redirected request\"\n+        assert 'Content-Length' not in req2.headers, \\\n+            \"Content-Length header must not be present in redirected request\"\n+        assert not req2.body, \\\n+            \"Redirected body must be empty, not '%s'\" % req2.body\n \n-    def test_process_response(self):\n+    def test_meta_refresh(self):\n         body = \"\"\"<html>\n             <head><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n             </html>\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#799301b865772da7b7f5b0917e28cfe5c42815c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 370 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,7 +51,9 @@ class Headers(CaselessDict):\n         self.setdefault(key, default_list)\n \n     def appendlist(self, key, value):\n-        self.getlist(key).extend(self.normvalue(value))\n+        lst = self.getlist(key)\n+        lst.extend(self.normvalue(value))\n+        self[key] = lst\n \n     def items(self):\n         return list(self.iteritems())\n\n@@ -87,6 +87,11 @@ class HeadersTest(unittest.TestCase):\n         h1.appendlist('header1', 'value3')\n         self.assertEqual(h1.getlist('header1'), ['value1', 'value3'])\n \n+        h1 = Headers()\n+        h1.appendlist('header1', 'value1')\n+        h1.appendlist('header1', 'value3')\n+        self.assertEqual(h1.getlist('header1'), ['value1', 'value3'])\n+\n     def test_setlist(self):\n         h1 = Headers({'header1': 'value1'})\n         self.assertEqual(h1.getlist('header1'), ['value1'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#48b436e81b43d84696342f44d4928c6408ca0403", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 12 | Churn Cumulative: 313 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ from collections import defaultdict\n \n from scrapy.core import signals\n from scrapy.utils.cookies import CookieJar\n+from scrapy.core.exceptions import HttpException\n from scrapy import log\n \n class CookiesMiddleware(object):\n@@ -22,7 +23,7 @@ class CookiesMiddleware(object):\n             jar.set_cookie_if_ok(cookie, request)\n \n         # set Cookie header\n-        jar.add_cookie_header(wreq)\n+        jar.add_cookie_header(request)\n \n     def process_response(self, request, response, spider):\n         if request.meta.get('dont_merge_cookies', False):\n@@ -30,7 +31,7 @@ class CookiesMiddleware(object):\n \n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n         jar = self.cookies[spider.domain_name]\n-        jar.extract_cookies(wrsp, wreq)\n+        jar.extract_cookies(response, request)\n \n         # TODO: set current cookies in jar to response.cookies?\n         return response\n\n@@ -1,4 +1,4 @@\n-from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy\n+from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy, Cookie\n \n \n class CookieJar(object):\n@@ -13,11 +13,6 @@ class CookieJar(object):\n     def add_cookie_header(self, request):\n         wreq = WrappedRequest(request)\n         self.jar.add_cookie_header(wreq)\n-        for hdr in ('Cookie', 'Cookie2'):\n-            v = wreq.get_header(hdr)\n-            if hdr:\n-                request.headers[hdr] = v\n-\n \n     @property\n     def _cookies(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#391dde86ee7126a6dc4109433e527160a146f785", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 209 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,14 +11,14 @@ class CookiesMiddleware(object):\n     \"\"\"This middleware enables working with sites that need cookies\"\"\"\n \n     def __init__(self):\n-        self.cookies = defaultdict(CookieJar)\n+        self.jars = defaultdict(CookieJar)\n         dispatcher.connect(self.domain_closed, signals.domain_closed)\n \n     def process_request(self, request, spider):\n         if request.meta.get('dont_merge_cookies', False):\n             return\n \n-        jar = self.cookies[spider.domain_name]\n+        jar = self.jars[spider.domain_name]\n         for name, cookie in request.cookies.items():\n             jar.set_cookie_if_ok(cookie, request)\n \n@@ -30,7 +30,7 @@ class CookiesMiddleware(object):\n             return\n \n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n-        jar = self.cookies[spider.domain_name]\n+        jar = self.jars[spider.domain_name]\n         jar.extract_cookies(response, request)\n \n         # TODO: set current cookies in jar to response.cookies?\n@@ -42,6 +42,4 @@ class CookiesMiddleware(object):\n             self.process_response(request, exception.response, spider)\n \n     def domain_closed(self, domain):\n-        if domain in self.cookies:\n-            del self.cookies[domain]\n-\n+        self.jars.pop(domain, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ce1481eabf7da35e63b5828d223bc9e741cc118", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 123 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy, Cookie\n class CookieJar(object):\n     def __init__(self, policy=None):\n         self.jar = _CookieJar(policy or DefaultCookiePolicy())\n+        self.jar._cookies_jar = _DummyLock()\n \n     def extract_cookies(self, response, request):\n         wreq = WrappedRequest(request)\n@@ -45,6 +46,14 @@ class CookieJar(object):\n         self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))\n \n \n+class _DummyLock(object):\n+    def acquire(self):\n+        pass\n+\n+    def release(self):\n+        pass\n+\n+\n class WrappedRequest(object):\n     \"\"\"Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3de3c2a331c5fcd221d0e317f0bb74c654e6d5fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 321 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,12 +1,15 @@\n+import operator\n+from itertools import groupby\n+from collections import defaultdict\n from pydispatch import dispatcher\n \n-from collections import defaultdict\n-\n from scrapy.core import signals\n+from scrapy.http import Response\n from scrapy.utils.cookies import CookieJar\n from scrapy.core.exceptions import HttpException\n from scrapy import log\n \n+\n class CookiesMiddleware(object):\n     \"\"\"This middleware enables working with sites that need cookies\"\"\"\n \n@@ -19,7 +22,8 @@ class CookiesMiddleware(object):\n             return\n \n         jar = self.jars[spider.domain_name]\n-        for name, cookie in request.cookies.items():\n+        cookies = _get_cookies(jar, request)\n+        for cookie in cookies:\n             jar.set_cookie_if_ok(cookie, request)\n \n         # set Cookie header\n@@ -43,3 +47,12 @@ class CookiesMiddleware(object):\n \n     def domain_closed(self, domain):\n         self.jars.pop(domain, None)\n+\n+\n+def _get_cookies(jar, request):\n+    headers = {'Set-Cookie': ['%s=%s;' % (k, v) for k, v in request.cookies.iteritems()]}\n+    response = Response(request.url, headers=headers)\n+    cookies = jar.make_cookies(response, request)\n+    return cookies\n+    \n+\n\n@@ -73,16 +73,16 @@ class CookiesMiddlewareTest(TestCase):\n         self.assertEquals(req.headers.get('Cookie'), 'C1=value1')\n \n     def test_merge_request_cookies(self):\n-        headers = {'Set-Cookie': 'C1=value1; path=/'}\n         req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'})\n         self.mw.process_request(req, self.spider)\n         self.assertEquals(req.headers.get('Cookie'), 'galleta=salada')\n \n+        headers = {'Set-Cookie': 'C1=value1; path=/'}\n         res = Response('http://scrapytest.org/', headers=headers)\n         self.mw.process_response(req, res, self.spider)\n         req2 = Request('http://scrapytest.org/sub1/')\n         self.mw.process_request(req2, self.spider)\n-        self.assertEquals(req2.headers.get('Cookie'), \"C1=value1\")\n+        self.assertEquals(req2.headers.get('Cookie'), \"C1=value1; galleta=salada\")\n \n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ecf874a755d9c1e91314c27f0656adf107770a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 190 | Lines Deleted: 157 | Files Changed: 12 | Hunks: 31 | Methods Changed: 42 | Complexity Δ (Sum/Max): 13/17 | Churn Δ: 347 | Churn Cumulative: 3604 | Contributors (this commit): 5 | Commits (past 90d): 64 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -61,12 +61,12 @@ DOWNLOADER_MIDDLEWARES = [\n     # Engine side\n     'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware',\n     'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n-    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n     'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n     'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n     'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n+    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n     'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',\n     'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n     'scrapy.contrib.downloadermiddleware.stats.DownloaderStats',\n\n@@ -1,27 +1,81 @@\n+import operator\n+from itertools import groupby\n+from collections import defaultdict\n from pydispatch import dispatcher\n \n from scrapy.core import signals\n-from scrapy.utils.misc import dict_updatedefault\n+from scrapy.http import Response\n+from scrapy.utils.cookies import CookieJar\n+from scrapy.core.exceptions import HttpException\n+from scrapy.conf import settings\n+from scrapy import log\n+\n \n class CookiesMiddleware(object):\n     \"\"\"This middleware enables working with sites that need cookies\"\"\"\n+    debug = settings.getbool('COOKIES_DEBUG')\n \n     def __init__(self):\n-        self.cookies = {}\n-        dispatcher.connect(self.domain_open, signals.domain_open)\n+        self.jars = defaultdict(CookieJar)\n         dispatcher.connect(self.domain_closed, signals.domain_closed)\n \n     def process_request(self, request, spider):\n-        if not request.meta.get('dont_merge_cookies', False):\n-            dict_updatedefault(request.cookies, self.cookies[spider.domain_name])\n+        if request.meta.get('dont_merge_cookies', False):\n+            return\n+\n+        jar = self.jars[spider.domain_name]\n+        cookies = self._get_request_cookies(jar, request)\n+        for cookie in cookies:\n+            jar.set_cookie_if_ok(cookie, request)\n+\n+        # set Cookie header\n+        request.headers.pop('Cookie', None)\n+        jar.add_cookie_header(request)\n+        self._debug_cookie(request)\n \n     def process_response(self, request, response, spider):\n-        cookies = self.cookies[spider.domain_name]\n-        cookies.update(request.cookies)\n+        if request.meta.get('dont_merge_cookies', False):\n+            return\n+\n+        # extract cookies from Set-Cookie and drop invalid/expired cookies\n+        jar = self.jars[spider.domain_name]\n+        jar.extract_cookies(response, request)\n+        self._debug_set_cookie(response)\n+\n+        # TODO: set current cookies in jar to response.cookies?\n         return response\n \n-    def domain_open(self, domain):\n-        self.cookies[domain] = {}\n+    # cookies should be set on non-200 responses too\n+    def process_exception(self, request, exception, spider):\n+        if isinstance(exception, HttpException):\n+            self.process_response(request, exception.response, spider)\n \n     def domain_closed(self, domain):\n-        del self.cookies[domain]\n+        self.jars.pop(domain, None)\n+\n+    def _debug_cookie(self, request):\n+        \"\"\"log Cookie header for request\"\"\"\n+        if self.debug:\n+            c = request.headers.get('Cookie')\n+            c = c and [p.split('=')[0] for p in c.split(';')]\n+            log.msg('Cookie: %s for %s' % (c, request.url), level=log.DEBUG)\n+\n+    def _debug_set_cookie(self, response):\n+        \"\"\"log Set-Cookies headers but exclude cookie values\"\"\"\n+        if self.debug:\n+            cl = response.headers.getlist('Set-Cookie')\n+            res = []\n+            for c in cl:\n+                kv, tail = c.split(';', 1)\n+                k = kv.split('=', 1)[0]\n+                res.append('%s %s' % (k, tail))\n+            log.msg('Set-Cookie: %s from %s' % (res, response.url))\n+\n+\n+    def _get_request_cookies(self, jar, request):\n+        headers = {'Set-Cookie': ['%s=%s;' % (k, v) for k, v in request.cookies.iteritems()]}\n+        response = Response(request.url, headers=headers)\n+        cookies = jar.make_cookies(response, request)\n+        return cookies\n+\n+\n\n@@ -1,58 +0,0 @@\n-import operator\n-from itertools import groupby\n-from collections import defaultdict\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy.http import Response\n-from scrapy.utils.cookies import CookieJar\n-from scrapy.core.exceptions import HttpException\n-from scrapy import log\n-\n-\n-class CookiesMiddleware(object):\n-    \"\"\"This middleware enables working with sites that need cookies\"\"\"\n-\n-    def __init__(self):\n-        self.jars = defaultdict(CookieJar)\n-        dispatcher.connect(self.domain_closed, signals.domain_closed)\n-\n-    def process_request(self, request, spider):\n-        if request.meta.get('dont_merge_cookies', False):\n-            return\n-\n-        jar = self.jars[spider.domain_name]\n-        cookies = _get_cookies(jar, request)\n-        for cookie in cookies:\n-            jar.set_cookie_if_ok(cookie, request)\n-\n-        # set Cookie header\n-        jar.add_cookie_header(request)\n-\n-    def process_response(self, request, response, spider):\n-        if request.meta.get('dont_merge_cookies', False):\n-            return\n-\n-        # extract cookies from Set-Cookie and drop invalid/expired cookies\n-        jar = self.jars[spider.domain_name]\n-        jar.extract_cookies(response, request)\n-\n-        # TODO: set current cookies in jar to response.cookies?\n-        return response\n-\n-    # cookies should be set on non-200 responses too\n-    def process_exception(self, request, exception, spider):\n-        if isinstance(exception, HttpException):\n-            self.process_response(request, exception.response, spider)\n-\n-    def domain_closed(self, domain):\n-        self.jars.pop(domain, None)\n-\n-\n-def _get_cookies(jar, request):\n-    headers = {'Set-Cookie': ['%s=%s;' % (k, v) for k, v in request.cookies.iteritems()]}\n-    response = Response(request.url, headers=headers)\n-    cookies = jar.make_cookies(response, request)\n-    return cookies\n-    \n-\n\n@@ -55,7 +55,6 @@ def create_factory(request, spider):\n                                 postdata=request.body or None, # see http://dev.scrapy.org/ticket/60\n                                 headers=request.headers,\n                                 agent=agent,\n-                                cookies=request.cookies,\n                                 timeout=getattr(spider, \"download_timeout\", None) or default_timeout,\n                                 followRedirect=False)\n \n\n@@ -1,8 +1,13 @@\n from urlparse import urlunparse\n \n-from twisted.web.client import HTTPClientFactory\n+from twisted.web.client import HTTPClientFactory, HTTPPageGetter\n+from twisted.web import http\n+from twisted.python import failure\n+from twisted.web import error\n+from twisted.internet import defer\n \n-from scrapy.http import Url\n+from scrapy.http import Url, Headers\n+from scrapy.utils.misc import arg_to_iter\n \n def _parse(url, defaultPort=None):\n     url = url.strip()\n@@ -27,6 +32,93 @@ def _parse(url, defaultPort=None):\n     return scheme, host, port, path\n \n \n+class ScrapyHTTPPageGetter(HTTPPageGetter):\n+    quietLoss = 0\n+    failed = 0\n+\n+    _specialHeaders = set(('host', 'user-agent', 'content-length'))\n+\n+    def connectionMade(self):\n+        headers = self.factory.headers\n+        method = getattr(self.factory, 'method', 'GET')\n+\n+        self.sendCommand(method, self.factory.path)\n+        self.sendHeader('Host', headers.get(\"host\", self.factory.host))\n+        self.sendHeader('User-Agent', headers.get('User-Agent', self.factory.agent))\n+\n+        data = getattr(self.factory, 'postdata', None)\n+        if data is not None:\n+            self.sendHeader(\"Content-Length\", str(len(data)))\n+\n+        for key, value in self.factory.headers.items():\n+            if key.lower() not in self._specialHeaders:\n+                self.sendHeader(key, value)\n+\n+        self.endHeaders()\n+        self.headers = Headers()\n+\n+        if data is not None:\n+            self.transport.write(data)\n+\n+    def sendHeader(self, name, value):\n+        for v in arg_to_iter(value):\n+            self.transport.write('%s: %s\\r\\n' % (name, v))\n+\n+    def handleHeader(self, key, value):\n+        self.headers.appendlist(key, value)\n+\n+    def handleStatus(self, version, status, message):\n+        self.version, self.status, self.message = version, status, message\n+        self.factory.gotStatus(version, status, message)\n+\n+    def handleEndHeaders(self):\n+        self.factory.gotHeaders(self.headers)\n+        m = getattr(self, 'handleStatus_'+self.status, self.handleStatusDefault)\n+        m()\n+\n+    def handleStatus_200(self):\n+        pass\n+\n+    handleStatus_201 = lambda self: self.handleStatus_200()\n+    handleStatus_202 = lambda self: self.handleStatus_200()\n+\n+    def handleStatusDefault(self):\n+        self.failed = 1\n+\n+    def connectionLost(self, reason):\n+        if not self.quietLoss:\n+            http.HTTPClient.connectionLost(self, reason)\n+            self.factory.noPage(reason)\n+\n+    def handleResponse(self, response):\n+        if self.quietLoss:\n+            return\n+\n+        if self.failed:\n+            self.factory.noPage(failure.Failure(error.Error(self.status, self.message, response)))\n+\n+        if self.factory.method.upper() == 'HEAD':\n+            # Callback with empty string, since there is never a response\n+            # body for HEAD requests.\n+            self.factory.page('')\n+        elif self.length != None and self.length != 0:\n+            self.factory.noPage(failure.Failure(\n+                PartialDownloadError(self.status, self.message, response)))\n+        else:\n+            self.factory.page(response)\n+\n+        # server might be stupid and not close connection. admittedly\n+        # the fact we do only one request per connection is also\n+        # stupid...\n+        self.transport.loseConnection()\n+\n+    def timeout(self):\n+        self.quietLoss = True\n+        self.transport.loseConnection()\n+        self.factory.noPage(defer.TimeoutError(\"Getting %s took longer than %s seconds.\" % (self.factory.url, self.factory.timeout)))\n+\n+\n+\n class ScrapyHTTPClientFactory(HTTPClientFactory):\n     \"\"\"Scrapy implementation of the HTTPClientFactory overwriting the\n     serUrl method to make use of our Url object that cache the parse \n@@ -34,6 +126,8 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     cookies.\n     \"\"\"\n \n+    protocol = ScrapyHTTPPageGetter\n+\n     def setURL(self, url):\n         self.url = url\n         scheme, host, port, path = _parse(url)\n@@ -55,18 +149,4 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         values that could be managed correctly by twisted.\n         \"\"\"\n         self.response_headers = headers\n-        if 'set-cookie' in headers:\n-            goodcookies = []\n-            for cookie in headers['set-cookie']:\n-                cookparts = cookie.split(';')\n-                cook = cookparts[0].lstrip()\n-                t = cook.split('=', 1)\n-                if len(t) == 2: #Good cookie\n-                    goodcookies.append(cookie)\n-                    k, v = t                \n-                    self.cookies[k.lstrip()] = v.lstrip()\n-            if goodcookies:\n-                self.response_headers['set-cookie'] = goodcookies\n-            else:\n-                del self.response_headers['set-cookie']\n \n\n@@ -48,7 +48,7 @@ class Headers(CaselessDict):\n         self[key] = list_\n \n     def setlistdefault(self, key, default_list=()):\n-        self.setdefault(key, default_list)\n+        return self.setdefault(key, default_list)\n \n     def appendlist(self, key, value):\n         lst = self.getlist(key)\n@@ -59,19 +59,16 @@ class Headers(CaselessDict):\n         return list(self.iteritems())\n \n     def iteritems(self):\n-        return ((k, self[k]) for k in self.keys())\n+        return ((k, self.getlist(k)) for k in self.keys())\n \n     def values(self):\n         return [self[k] for k in self.keys()]\n \n-    def lists(self):\n-        return super(Headers, self).items()\n-\n     def to_string(self):\n         return headers_dict_to_raw(self)\n \n     def __copy__(self):\n-        return self.__class__(self.lists())\n+        return self.__class__(self)\n     copy = __copy__\n \n \n\n@@ -60,13 +60,11 @@ class HeadersTest(unittest.TestCase):\n         h = Headers(idict)\n         self.assertEqual(dict(h), {'Content-Type': ['text/html'], 'X-Forwarded-For': ['ip1', 'ip2']})\n         self.assertEqual(h.keys(), ['X-Forwarded-For', 'Content-Type'])\n-        self.assertEqual(h.items(), [('X-Forwarded-For', 'ip2'), ('Content-Type', 'text/html')])\n+        self.assertEqual(h.items(), [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])\n         self.assertEqual(list(h.iteritems()),\n-                [('X-Forwarded-For', 'ip2'), ('Content-Type', 'text/html')])\n+                [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])\n \n         self.assertEqual(h.values(), ['ip2', 'text/html'])\n-        self.assertEqual(h.lists(),\n-                [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])\n \n     def test_update(self):\n         h = Headers()\n\n@@ -52,7 +52,8 @@ class RequestTest(unittest.TestCase):\n         h[u'newkey'] = u'newval'\n         for k, v in h.iteritems():\n             self.assert_(isinstance(k, str))\n-            self.assert_(isinstance(v, str))\n+            for s in v:\n+                self.assert_(isinstance(s, str))\n \n     def test_eq(self):\n         url = 'http://www.scrapy.org'\n\n@@ -1081,6 +1081,23 @@ class LWPCookieTests(TestCase):\n             # we didn't have session cookies in the first place\n             counter[\"session_before\"] == 0))\n \n+    def testMalformedCookieHeaderParsing(self):\n+        headers = Headers({'Set-Cookie': [\n+            'CUSTOMER=WILE_E_COYOTE; path=/; expires=Wednesday, 09-Nov-2100 23:12:40 GMT',\n+            'PART_NUMBER=ROCKET_LAUNCHER_0001; path=/',\n+            'SHIPPING=FEDEX; path=/foo',\n+            'COUNTRY=UY; path=/foo',\n+            'GOOD_CUSTOMER;',\n+            'NO_A_BOT;']})\n+        res = Response('http://www.perlmeister.com/foo', headers=headers)\n+        req = Request('http://www.perlmeister.com/foo')\n+\n+        c = CookieJar()\n+        c.extract_cookies(res, req)\n+        c.add_cookie_header(req)\n+        self.assertEquals(req.headers.get('Cookie'),\n+                'COUNTRY=UY; SHIPPING=FEDEX; CUSTOMER=WILE_E_COYOTE; '\n+                'PART_NUMBER=ROCKET_LAUNCHER_0001; NO_A_BOT; GOOD_CUSTOMER')\n \n \n class WrapperRequestResponse(TestCase):\n@@ -1130,3 +1147,4 @@ class WrapperRequestResponse(TestCase):\n         req = self.FakeRequest(\"http://www.acme.com:2345/resource.html\", headers={\"Host\": \"www.acme.com:5432\"})\n         self.assertEquals(request_host(req), \"www.acme.com\")\n \n+\n\n@@ -1,4 +1,5 @@\n \"\"\"\n+from twisted.internet import defer\n Tests borrowed from the twisted.web.client tests.\n \"\"\"\n from urlparse import urlparse\n@@ -52,61 +53,3 @@ class ParseUrlTestCase(unittest.TestCase):\n         self.assertTrue(isinstance(path, str))\n \n \n-class FakeTransport:\n-    disconnecting = False\n-\n-    def __init__(self):\n-        self.data = []\n-\n-    def write(self, stuff):\n-        self.data.append(stuff)\n-\n-\n-class CookieTestCase(unittest.TestCase):\n-\n-    def _listen(self, site):\n-        return reactor.listenTCP(0, site, interface=\"127.0.0.1\")\n-\n-    def setUp(self):\n-        root = static.Data('El toro!', 'text/plain')\n-        site = server.Site(root, timeout=None)\n-        self.port = self._listen(site)\n-        self.portno = self.port.getHost().port\n-\n-    def tearDown(self):\n-        return self.port.stopListening()\n-\n-    def testMalformedCookieHeaderParsing(self):\n-        d = defer.Deferred()\n-        factory = ScrapyHTTPClientFactory('http://foo.example.com/')\n-        proto = factory.buildProtocol('127.42.42.42')\n-        proto.transport = FakeTransport()\n-        proto.connectionMade()\n-        for line in [\n-            '200 Ok',\n-            'Squash: yes',\n-            'Hands: stolen',\n-            'Set-Cookie: CUSTOMER=WILE_E_COYOTE; path=/; expires=Wednesday, 09-Nov-99 23:12:40 GMT',\n-            'Set-Cookie: PART_NUMBER=ROCKET_LAUNCHER_0001; path=/',\n-            'Set-Cookie: SHIPPING=FEDEX; path=/foo',\n-            'Set-Cookie: COUNTRY=UY; path=/foo',\n-            'Set-Cookie: GOOD_CUSTOMER;',\n-            'Set-Cookie: NO_A_BOT;',\n-            '',\n-            'body',\n-            'more body',\n-            ]:\n-            proto.dataReceived(line + '\\r\\n')\n-        self.assertEquals(proto.transport.data,\n-                          ['GET / HTTP/1.0\\r\\n',\n-                           'Host: foo.example.com\\r\\n',\n-                           'User-Agent: Twisted PageGetter\\r\\n',\n-                           '\\r\\n'])\n-        self.assertEquals(factory.cookies,\n-                          {\n-            'CUSTOMER': 'WILE_E_COYOTE',\n-            'PART_NUMBER': 'ROCKET_LAUNCHER_0001',\n-            'SHIPPING': 'FEDEX',\n-            'COUNTRY': 'UY',\n-            })\n-\n\n@@ -95,7 +95,7 @@ class WrappedRequest(object):\n         return self.request.headers.items()\n \n     def add_unredirected_header(self, name, value):\n-        self.request.headers[name] = value\n+        self.request.headers.appendlist(name, value)\n         #print 'add_unredirected_header', self.request.headers\n \n \n\n@@ -52,7 +52,8 @@ def request_fingerprint(request, include_headers=()):\n         for hdr in include_headers:\n             if hdr in request.headers:\n                 fp.update(hdr)\n-                fp.update(request.headers.get(hdr, ''))\n+                for v in request.headers.getlist(hdr):\n+                    fp.update(v)\n         fphash = fp.hexdigest()\n         request.cache[cachekey] = fphash\n         return fphash\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
