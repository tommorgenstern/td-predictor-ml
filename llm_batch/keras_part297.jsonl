{"custom_id": "keras#e2c0da4071e1ff1785f423a0ea22225b6ae9c8e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 11 | Churn Cumulative: 1807 | Contributors (this commit): 23 | Commits (past 90d): 11 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -61,7 +61,7 @@ class Metric(Layer):\n         super(Metric, self).__init__(name=name, dtype=dtype, **kwargs)\n         self.stateful = True  # All metric layers are stateful.\n         self.built = True\n-        self.dtype = K.floatx() if dtype is None else dtype\n+        self.dtype = dtype or K.floatx()\n \n     def __new__(cls, *args, **kwargs):\n         obj = super(Metric, cls).__new__(cls)\n@@ -71,11 +71,6 @@ class Metric(Layer):\n             metrics_utils.update_state_wrapper(update_state_fn), obj)\n         return obj\n \n-    def __call__(self, *args, **kwargs):\n-        \"\"\"Accumulates statistics and then computes metric result value.\"\"\"\n-        update_op = self.update_state(*args, **kwargs)\n-        return self.result()\n-\n     def get_config(self):\n         \"\"\"Returns the serializable config of the metric.\"\"\"\n         return {'name': self.name, 'dtype': self.dtype}\n\n@@ -6,7 +6,7 @@ from keras import metrics\n from keras import backend as K\n \n \n-class TestSum:\n+class TestSum(object):\n \n     def test_sum(self):\n         m = metrics.Sum(name='my_sum', dtype='float32')\n@@ -71,7 +71,7 @@ class TestSum:\n         assert np.isclose(K.eval(m.total), 63.75, 2)\n \n \n-class TestMean:\n+class TestMean(object):\n \n     def test_mean(self):\n         m = metrics.Mean(name='my_mean')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c93bd654be9cbf500f8db06544ee8f2afdf14cd0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 25 | Churn Cumulative: 16299 | Contributors (this commit): 144 | Commits (past 90d): 18 | Contributors (cumulative): 164 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -150,6 +150,7 @@ from .load_backend import name_scope\n from .load_backend import symbolic\r\n from .load_backend import eager\r\n from .load_backend import size\r\n+from .load_backend import control_dependencies\r\n \r\n if backend() == 'theano':\r\n     from .load_backend import pattern_broadcast\r\n\n@@ -4502,7 +4502,21 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n     for st in decoded:\n         dense_tensor = tf.sparse.to_dense(st, default_value=-1)\n         decoded_dense.append(dense_tensor)\n-    return (decoded_dense, log_prob)\n+\n+\n+def control_dependencies(control_inputs):\n+    \"\"\"A context manager that specifies control dependencies for all operations\n+    constructed within the context.\n+\n+    # Arguments\n+        control_inputs: A list of Operation or Tensor objects which must be executed\n+        or computed before running the operations defined in the context. Can also\n+        be None to clear the control dependencies.\n+\n+    # Returns\n+        A context manager.\n+    \"\"\"\n+    return tf.control_dependencies(control_inputs)\n \n \n # HIGH ORDER FUNCTIONS\n\n@@ -71,6 +71,12 @@ class Metric(Layer):\n             metrics_utils.update_state_wrapper(update_state_fn), obj)\n         return obj\n \n+    def __call__(self, *args, **kwargs):\n+        \"\"\"Accumulates statistics and then computes metric result value.\"\"\"\n+        update_op = self.update_state(*args, **kwargs)\n+        with K.control_dependencies([update_op]):\n+            return self.result()\n+\n     def get_config(self):\n         \"\"\"Returns the serializable config of the metric.\"\"\"\n         return {'name': self.name, 'dtype': self.dtype}\n@@ -157,6 +163,7 @@ class Reduce(Metric):\n             values = values * sample_weight\n \n         value_sum = K.sum(values)\n+        with K.control_dependencies([value_sum]):\n             update_total_op = K.update_add(self.total, value_sum)\n \n         # Exit early if the reduction doesn't have a denominator.\n@@ -175,6 +182,7 @@ class Reduce(Metric):\n             raise NotImplementedError(\n                 'reduction [%s] not implemented' % self.reduction)\n \n+        with K.control_dependencies([update_total_op]):\n             return K.update_add(self.count, num_values)\n \n     def result(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#62c6b778c2b8dd8bec188024644669f92ab1a49a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 17 | Churn Cumulative: 10639 | Contributors (this commit): 108 | Commits (past 90d): 4 | Contributors (cumulative): 129 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2899,3 +2899,11 @@ def foldr(fn, elems, initializer=None, name=None):\n         accumulator.name = str(name)\n \n     return reshape(accumulator, shape(initializer)[1:])\n+\n+\n+def control_dependencies(control_inputs):\n+    @contextmanager\n+    def nullcontextmanager():\n+        yield\n+\n+    return nullcontextmanager()\n\\ No newline at end of file\n\n@@ -2954,3 +2954,12 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1,\n                merge_repeated=False):\n     raise NotImplementedError\n+\n+\n+def control_dependencies(control_inputs):\n+    @contextmanager\n+    def nullcontextmanager():\n+        yield\n+\n+    return nullcontextmanager()\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3e77ab67790e2ff22ca423662d6ec23c01600640", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 8 | Churn Cumulative: 2571 | Contributors (this commit): 32 | Commits (past 90d): 2 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -784,21 +784,17 @@ def test_check_not_failing():\n \n def test_check_last_is_one():\n     a = np.random.random((2, 3, 1))\n-    with pytest.raises(ValueError) as exc:\n+    with pytest.raises(ValueError, match='You are passing a target array'):\n         training_utils.check_loss_and_target_compatibility(\n             [a], [losses.categorical_crossentropy], [a.shape])\n \n-    assert 'You are passing a target array' in str(exc)\n-\n \n def test_check_bad_shape():\n     a = np.random.random((2, 3, 5))\n-    with pytest.raises(ValueError) as exc:\n+    with pytest.raises(ValueError, match='targets to have the same shape'):\n         training_utils.check_loss_and_target_compatibility(\n             [a], [losses.categorical_crossentropy], [(2, 3, 6)])\n \n-    assert 'targets to have the same shape' in str(exc)\n-\n \n @pytest.mark.parametrize('input_metrics,expected_output', [\n     (None, [[], []]),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e99fcfefc1edd8ca9f423d3f374dd96ffe2e636", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 30 | Churn Cumulative: 7770 | Contributors (this commit): 101 | Commits (past 90d): 14 | Contributors (cumulative): 111 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2962,4 +2962,3 @@ def control_dependencies(control_inputs):\n         yield\n \n     return nullcontextmanager()\n-\n\n@@ -65,10 +65,12 @@ class Metric(Layer):\n \n     def __new__(cls, *args, **kwargs):\n         obj = super(Metric, cls).__new__(cls)\n-        update_state_fn = obj.update_state\n \n         obj.update_state = types.MethodType(\n-            metrics_utils.update_state_wrapper(update_state_fn), obj)\n+            metrics_utils.update_state_wrapper(obj.update_state), obj)\n+\n+        obj.result = types.MethodType(\n+            metrics_utils.result_wrapper(obj.result), obj)\n         return obj\n \n     def __call__(self, *args, **kwargs):\n\n@@ -3,6 +3,8 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+from .. import backend as K\n+\n \n class Reduction(object):\n     \"\"\"Types of metrics reduction.\n@@ -36,3 +38,24 @@ def update_state_wrapper(update_state_fn):\n         return update_op\n \n     return decorated\n+\n+\n+def result_wrapper(result_fn):\n+    \"\"\"Decorator to wrap metric `result()` with identity op.\n+\n+    Wrapping result in identity so that control dependency between\n+    update_op from `update_state` and result works in case result returns\n+    a tensor.\n+\n+    # Arguments\n+        result_fn: function that computes the metric result.\n+\n+    # Returns\n+        Decorated function that wraps `result()` with identity op.\n+    \"\"\"\n+    def decorated(metric_obj, *args, **kwargs):\n+        \"\"\"Decorated function with `add_update()`.\"\"\"\n+\n+        return K.identity(result_fn(*args, **kwargs))\n+\n+    return decorated\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0cdff54b639544085705f4996169ce31bf509754", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 63 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -54,8 +54,6 @@ def result_wrapper(result_fn):\n         Decorated function that wraps `result()` with identity op.\n     \"\"\"\n     def decorated(metric_obj, *args, **kwargs):\n-        \"\"\"Decorated function with `add_update()`.\"\"\"\n-\n         return K.identity(result_fn(*args, **kwargs))\n \n     return decorated\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#72b55d20899fae554b453a9e67682bb26c2e60d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 180 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -62,13 +62,6 @@ def _test_application_notop(app, last_dim):\n     assert output_shape[-1] == last_dim\n \n \n-def test_mobilenet_v2_legacy_import():\n-    from keras.applications import mobilenetv2\n-    assert hasattr(mobilenetv2, 'MobileNetV2')\n-    from keras.applications import mobilenet_v2\n-    assert hasattr(mobilenet_v2, 'MobileNetV2')\n-\n-\n def test_applications():\n     for _ in range(3):\n         app, last_dim = random.choice(MODEL_LIST)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f06524c44e5f6926968cb2bb3ddd1e523f5474c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 2386 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1215,6 +1215,10 @@ class Network(Layer):\n             else:\n                 saving.load_weights_from_hdf5_group(\n                     f, self.layers, reshape=reshape)\n+            if hasattr(f, 'close'):\n+                f.close()\n+            elif hasattr(f.file, 'close'):\n+                f.file.close()\n \n     def _updated_config(self):\n         \"\"\"Util hared between different serialization methods.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0fc33feb5f4efe3bb823c57a8390f52932a966ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9776 | Contributors (this commit): 63 | Commits (past 90d): 3 | Contributors (cumulative): 63 | DMM Complexity: None\n\nDIFF:\n@@ -255,7 +255,7 @@ class RNN(Layer):\n                 for the cell, the value will be inferred by the first element\n                 of the `state_size`.\n             It is also possible for `cell` to be a list of RNN cell instances,\n-            in which cases the cells get stacked on after the other in the RNN,\n+            in which cases the cells get stacked one after the other in the RNN,\n             implementing an efficient stacked RNN.\n         return_sequences: Boolean. Whether to return the last output\n             in the output sequence, or the full sequence.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b365dcfa144f562c4d0e552cbf5db6bcd5920aa6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 14704 | Contributors (this commit): 131 | Commits (past 90d): 9 | Contributors (cumulative): 131 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4502,11 +4502,11 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n     for st in decoded:\n         dense_tensor = tf.sparse.to_dense(st, default_value=-1)\n         decoded_dense.append(dense_tensor)\n+    return decoded_dense, log_prob\n \n \n def control_dependencies(control_inputs):\n-    \"\"\"A context manager that specifies control dependencies for all operations\n-    constructed within the context.\n+    \"\"\"A context manager that specifies control dependencies.\n \n     # Arguments\n         control_inputs: A list of Operation or Tensor objects which must be executed\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#85e530fe3dbdf0f547f41d85342090ee29f848fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 48 | Churn Cumulative: 1130 | Contributors (this commit): 17 | Commits (past 90d): 9 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -263,6 +263,54 @@ class Mean(Reduce):\n             reduction=metrics_utils.Reduction.WEIGHTED_MEAN, name=name, dtype=dtype)\n \n \n+class MeanMetricWrapper(Mean):\n+    \"\"\"Wraps a stateless metric function with the Mean metric.\"\"\"\n+\n+    def __init__(self, fn, name=None, dtype=None, **kwargs):\n+        \"\"\"Creates a `MeanMetricWrapper` instance.\n+\n+        # Arguments\n+            fn: The metric function to wrap, with signature\n+                `fn(y_true, y_pred, **kwargs)`.\n+            name: (Optional) string name of the metric instance.\n+            dtype: (Optional) data type of the metric result.\n+            **kwargs: The keyword arguments that are passed on to `fn`.\n+        \"\"\"\n+        super(MeanMetricWrapper, self).__init__(name=name, dtype=dtype)\n+        self._fn = fn\n+        self._fn_kwargs = kwargs\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates metric statistics.\n+\n+        `y_true` and `y_pred` should have the same shape.\n+\n+        # Arguments\n+            y_true: The ground truth values.\n+            y_pred: The predicted values.\n+            sample_weight: Optional weighting of each example. Defaults to 1. Can be\n+                a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+                and must be broadcastable to `y_true`.\n+\n+        # Returns\n+            Update op.\n+        \"\"\"\n+        y_true = K.cast(y_true, self._dtype)\n+        y_pred = K.cast(y_pred, self._dtype)\n+        y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(y_pred, y_true)\n+\n+        matches = self._fn(y_true, y_pred, **self._fn_kwargs)\n+        return super(MeanMetricWrapper, self).update_state(\n+            matches, sample_weight=sample_weight)\n+\n+    def get_config(self):\n+        config = {}\n+        for k, v in six.iteritems(self._fn_kwargs):\n+            config[k] = K.eval(v) if is_tensor_or_variable(v) else v\n+        base_config = super(MeanMetricWrapper, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n def binary_accuracy(y_true, y_pred):\n     return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3e8a7ca98963869665161cf4071e885d45937b60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 59 | Churn Cumulative: 1928 | Contributors (this commit): 23 | Commits (past 90d): 16 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -295,8 +295,8 @@ class MeanMetricWrapper(Mean):\n         # Returns\n             Update op.\n         \"\"\"\n-        y_true = K.cast(y_true, self._dtype)\n-        y_pred = K.cast(y_pred, self._dtype)\n+        y_true = K.cast(y_true, self.dtype)\n+        y_pred = K.cast(y_pred, self.dtype)\n         y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(y_pred, y_true)\n \n         matches = self._fn(y_true, y_pred, **self._fn_kwargs)\n@@ -311,6 +311,30 @@ class MeanMetricWrapper(Mean):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class MeanSquaredError(MeanMetricWrapper):\n+    \"\"\"Computes the mean squared error between `y_true` and `y_pred`.\n+\n+    Standalone usage:\n+\n+    ```python\n+    m = keras.metrics.MeanSquaredError()\n+    m.update_state([0., 0., 1., 1.], [1., 1., 1., 0.])\n+    m.result()\n+    ```\n+\n+    Usage with compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', metrics=[keras.metrics.MeanSquaredError()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name='mean_squared_error', dtype=None):\n+        super(MeanSquaredError, self).__init__(\n+            mean_squared_error, name, dtype=dtype)\n+\n+\n def binary_accuracy(y_true, y_pred):\n     return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)\n \n\n@@ -171,3 +171,34 @@ class TestMean(object):\n         assert K.eval(m.result()) == 100\n         assert K.eval(m.total) == 100\n         assert K.eval(m.count) == 1\n+\n+\n+class TestMeanSquaredErrorTest(object):\n+\n+    def test_config(self):\n+        mse_obj = metrics.MeanSquaredError(name='my_mse', dtype='int32')\n+        assert mse_obj.name == 'my_mse'\n+        assert mse_obj.dtype == 'int32'\n+\n+        # Check save and restore config\n+        mse_obj2 = metrics.MeanSquaredError.from_config(mse_obj.get_config())\n+        assert mse_obj2.name == 'my_mse'\n+        assert mse_obj2.dtype == 'int32'\n+\n+    def test_unweighted(self):\n+        mse_obj = metrics.MeanSquaredError()\n+        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+\n+        update_op = mse_obj.update_state(y_true, y_pred)\n+        K.eval(update_op)\n+        result = mse_obj.result()\n+        np.isclose(0.5, K.eval(result), atol=1e-5)\n+\n+    def test_weighted(self):\n+        mse_obj = metrics.MeanSquaredError()\n+        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        sample_weight = (1., 1.5, 2., 2.5)\n+        result = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n+        np.isclose(0.54285, K.eval(result), atol=1e-5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#87b2b00c976d26451f149d7365ee194912930cde", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 263 | Lines Deleted: 118 | Files Changed: 3 | Hunks: 23 | Methods Changed: 21 | Complexity Δ (Sum/Max): 22/22 | Churn Δ: 381 | Churn Cumulative: 11085 | Contributors (this commit): 99 | Commits (past 90d): 6 | Contributors (cumulative): 113 | DMM Complexity: 0.45901639344262296\n\nDIFF:\n@@ -49,11 +49,14 @@ class Model(Network):\n                 The loss value that will be minimized by the model\n                 will then be the sum of all individual losses.\n             metrics: List of metrics to be evaluated by the model\n-                during training and testing.\n-                Typically you will use `metrics=['accuracy']`.\n-                To specify different metrics for different outputs of a\n-                multi-output model, you could also pass a dictionary,\n-                such as `metrics={'output_a': 'accuracy'}`.\n+                during training and testing. Typically you will use\n+                `metrics=['accuracy']`. To specify different metrics for different\n+                outputs of a multi-output model, you could also pass a dictionary,\n+                such as\n+                `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n+                You can also pass a list (len = len(outputs)) of lists of metrics\n+                such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n+                `metrics=['accuracy', ['accuracy', 'mse']]`.\n             loss_weights: Optional list or dictionary specifying scalar\n                 coefficients (Python floats) to weight the loss contributions\n                 of different model outputs.\n@@ -90,10 +93,16 @@ class Model(Network):\n         \"\"\"\n         self.optimizer = optimizers.get(optimizer)\n         self.loss = loss or {}\n-        self.metrics = metrics or []\n+        self._compile_metrics = metrics or []\n         self.loss_weights = loss_weights\n         self.sample_weight_mode = sample_weight_mode\n-        self.weighted_metrics = weighted_metrics\n+        self._compile_weighted_metrics = weighted_metrics\n+\n+        # List of stateful metric functions. Used for resetting metric state during\n+        # training/eval.\n+        self._compile_metric_functions = []\n+        # List of metric wrappers on output losses.\n+        self._output_loss_metrics = None\n \n         if not self.built:\n             # Model is not compilable because\n@@ -197,106 +206,18 @@ class Model(Network):\n         self._set_sample_weight_attributes(\n             sample_weight_mode, skip_target_weighing_indices)\n \n-        # Prepare metrics.\n-        self.metrics_names = ['loss']\n-        self.metrics_tensors = []\n+        # Save all metric attributes per output of the model.\n+        self._cache_output_metric_attributes(metrics, weighted_metrics)\n \n-        # List of same size as output_names.\n-        # contains tuples (metrics for output, names of metrics).\n-        nested_metrics = training_utils.collect_metrics(metrics, self.output_names)\n-        nested_weighted_metrics = training_utils.collect_metrics(\n-            weighted_metrics, self.output_names)\n-        self.metrics_updates = []\n-        self.stateful_metric_names = []\n-        self.stateful_metric_functions = []\n+        # Set metric attributes on model.\n+        self._set_metric_attributes()\n \n-        def handle_metrics(metrics, weights=None):\n-            metric_name_prefix = 'weighted_' if weights is not None else ''\n-\n-            for metric in metrics:\n-                if metric in ('accuracy', 'acc', 'crossentropy', 'ce'):\n-                    # custom handling of accuracy/crossentropy\n-                    # (because of class mode duality)\n-                    output_shape = K.int_shape(self.outputs[i])\n-                    loss_fn = self.loss_functions[i]\n-                    if isinstance(loss_fn, losses.LossFunctionWrapper):\n-                        loss_fn = loss_fn.fn\n-\n-                    if (output_shape[-1] == 1 or\n-                       loss_fn == losses.binary_crossentropy):\n-                        # case: binary accuracy/crossentropy\n-                        if metric in ('accuracy', 'acc'):\n-                            metric_fn = metrics_module.binary_accuracy\n-                        elif metric in ('crossentropy', 'ce'):\n-                            metric_fn = metrics_module.binary_crossentropy\n-                    elif (loss_fn == losses.sparse_categorical_crossentropy):\n-                        # case: categorical accuracy/crossentropy\n-                        # with sparse targets\n-                        if metric in ('accuracy', 'acc'):\n-                            metric_fn = metrics_module.sparse_categorical_accuracy\n-                        elif metric in ('crossentropy', 'ce'):\n-                            metric_fn = (\n-                                metrics_module.sparse_categorical_crossentropy)\n-                    else:\n-                        # case: categorical accuracy/crossentropy\n-                        if metric in ('accuracy', 'acc'):\n-                            metric_fn = metrics_module.categorical_accuracy\n-                        elif metric in ('crossentropy', 'ce'):\n-                            metric_fn = metrics_module.categorical_crossentropy\n-                    if metric in ('accuracy', 'acc'):\n-                            suffix = 'acc'\n-                    elif metric in ('crossentropy', 'ce'):\n-                            suffix = 'ce'\n-                    weighted_metric_fn = training_utils.weighted_masked_objective(\n-                        metric_fn)\n-                    metric_name = metric_name_prefix + suffix\n-                else:\n-                    metric_fn = metrics_module.get(metric)\n-                    weighted_metric_fn = training_utils.weighted_masked_objective(\n-                        metric_fn)\n-                    # Get metric name as string\n-                    if hasattr(metric_fn, 'name'):\n-                        metric_name = metric_fn.name\n-                    else:\n-                        metric_name = metric_fn.__name__\n-                    metric_name = metric_name_prefix + metric_name\n-\n-                with K.name_scope(metric_name):\n-                    metric_result = weighted_metric_fn(y_true, y_pred,\n-                                                       weights=weights,\n-                                                       mask=masks[i])\n-\n-                # Append to self.metrics_names, self.metric_tensors,\n-                # self.stateful_metric_names\n-                if len(self.output_names) > 1:\n-                    metric_name = self.output_names[i] + '_' + metric_name\n-                # Dedupe name\n-                j = 1\n-                base_metric_name = metric_name\n-                while metric_name in self.metrics_names:\n-                    metric_name = base_metric_name + '_' + str(j)\n-                    j += 1\n-                self.metrics_names.append(metric_name)\n-                self.metrics_tensors.append(metric_result)\n-\n-                # Keep track of state updates created by\n-                # stateful metrics (i.e. metrics layers).\n-                if isinstance(metric_fn, Layer) and metric_fn.stateful:\n-                    self.stateful_metric_names.append(metric_name)\n-                    self.stateful_metric_functions.append(metric_fn)\n-                    self.metrics_updates += metric_fn.updates\n-        with K.name_scope('metrics'):\n-            for i in range(len(self.outputs)):\n-                if i in skip_target_indices:\n-                    continue\n-\n-                y_true = self.targets[i]\n-                y_pred = self.outputs[i]\n-                weights = self.sample_weights[i]\n-                output_metrics = nested_metrics[i]\n-                output_weighted_metrics = nested_weighted_metrics[i]\n-                handle_metrics(output_metrics)\n-                handle_metrics(output_weighted_metrics, weights=weights)\n+        # Invoke metric functions (unweighted) for all the outputs.\n+        self._handle_metrics(\n+            self.outputs,\n+            targets=self._targets,\n+            skip_target_masks=[l is None for l in self.loss_functions],\n+            masks=masks)\n \n         # Compute total loss.\n         # Used to keep track of the total loss value (stateless).\n@@ -318,6 +239,37 @@ class Model(Network):\n         trainable_weights = self.trainable_weights\n         self._collected_trainable_weights = trainable_weights\n \n+    @property\n+    def metrics(self):\n+        \"\"\"Returns the model's metrics added using `compile`, `add_metric` APIs.\"\"\"\n+        metrics = []\n+        if self._is_compiled:\n+            metrics += self._compile_metric_functions\n+        return metrics\n+\n+    @property\n+    def metrics_names(self):\n+        \"\"\"Returns the model's display labels for all outputs.\"\"\"\n+        metrics_names = ['loss']\n+        if self._is_compiled:\n+            # Add output loss metric names to the metric names list.\n+            if len(self._training_endpoints) > 1:\n+                metrics_names.extend([\n+                    self.output_names[i] + '_loss'\n+                    for i in range(len(self.outputs))\n+                    if i not in skip_target_indices\n+                ])\n+\n+            # Add compile metrics/weighted metrics' names to the metric names list.\n+            metrics_names.extend([m.name for m in self._compile_metric_functions])\n+        return metrics_names\n+\n+    def reset_metrics(self):\n+        \"\"\"Resets the state of metrics.\"\"\"\n+        metrics = self._get_training_eval_metrics()\n+        for m in metrics:\n+            m.reset_states()\n+\n     def _check_trainable_weights_consistency(self):\n         \"\"\"Check trainable weights count consistency.\n \n@@ -354,12 +306,17 @@ class Model(Network):\n                         params=self._collected_trainable_weights,\n                         loss=self.total_loss)\n                 updates = (self.updates +\n-                           training_updates +\n-                           self.metrics_updates)\n+                           training_updates)\n+\n+                metrics = self._get_training_eval_metrics()\n+                metrics_tensors = [\n+                    m._call_result for m in metrics if hasattr(m, '_call_result')\n+                ]\n+\n                 # Gets loss and metrics. Updates weights at each call.\n                 self.train_function = K.function(\n                     inputs,\n-                    [self.total_loss] + self.metrics_tensors,\n+                    [self.total_loss] + metrics_tensors,\n                     updates=updates,\n                     name='train_function',\n                     **self._function_kwargs)\n@@ -373,12 +330,17 @@ class Model(Network):\n                       self._feed_sample_weights)\n             if self._uses_dynamic_learning_phase():\n                 inputs += [K.learning_phase()]\n+\n+            metrics = self._get_training_eval_metrics()\n+            metrics_tensors = [\n+                m._call_result for m in metrics if hasattr(m, '_call_result')\n+            ]\n             # Return loss and metrics, no gradient updates.\n             # Does update the network states.\n             self.test_function = K.function(\n                 inputs,\n-                [self.total_loss] + self.metrics_tensors,\n-                updates=self.state_updates + self.metrics_updates,\n+                [self.total_loss] + metrics_tensors,\n+                updates=self.state_updates,\n                 name='test_function',\n                 **self._function_kwargs)\n \n@@ -714,8 +676,7 @@ class Model(Network):\n                         y_true, y_pred, sample_weight=sample_weight)\n \n                 if len(self.outputs) > 1:\n-                    self.metrics_tensors.append(output_loss)\n-                    self.metrics_names.append(self.output_names[i] + '_loss')\n+                    self._output_loss_metrics[i](output_loss)\n \n                 if total_loss is None:\n                     total_loss = loss_weight * output_loss\n@@ -735,6 +696,177 @@ class Model(Network):\n \n         return total_loss\n \n+    def _get_training_eval_metrics(self):\n+        \"\"\"Returns all the metrics that are to be reported.\n+\n+        This includes the output loss metrics, compile metrics/weighted metrics.\n+        \"\"\"\n+        metrics = []\n+        if getattr(self, '_output_loss_metrics', None) is not None:\n+            metrics.extend(self._output_loss_metrics)\n+        if hasattr(self, 'metrics'):\n+            metrics.extend(self.metrics)\n+        return metrics\n+\n+    def _cache_output_metric_attributes(self, metrics, weighted_metrics):\n+        \"\"\"Caches metric name and function attributes for every model output.\"\"\"\n+        output_shapes = []\n+        for output in self.outputs:\n+            if output is None or output.shape.rank is None:\n+                output_shapes.append(None)\n+            else:\n+                output_shapes.append(output.shape.as_list())\n+            self._per_output_metrics = training_utils.collect_per_output_metric_info(\n+                metrics, self.output_names, output_shapes, self.loss_functions)\n+            self._per_output_weighted_metrics = (\n+                training_utils.collect_per_output_metric_info(\n+                    weighted_metrics,\n+                    self.output_names,\n+                    output_shapes,\n+                    self.loss_functions,\n+                    is_weighted=True))\n+\n+    def _add_unique_metric_name(self, metric_name, output_index):\n+        \"\"\"Makes the metric name unique and adds it to the model's metric name list.\n+\n+        If there are multiple outputs for which the metrics are calculated, the\n+        metric names have to be made unique by appending an integer.\n+\n+        # Arguments\n+            metric_name: Metric name that corresponds to the metric specified by the\n+                user. For example: 'acc'.\n+            output_index: The index of the model output for which the metric name is\n+                being added.\n+\n+        # Returns\n+            string, name of the model's unique metric name\n+        \"\"\"\n+        if len(self.output_names) > 1:\n+            metric_name = '%s_%s' % (self.output_names[output_index], metric_name)\n+        j = 1\n+        base_metric_name = metric_name\n+        while metric_name in self.metrics_names:\n+            metric_name = '%s_%d' % (base_metric_name, j)\n+            j += 1\n+        return metric_name\n+\n+    def _set_per_output_metric_attributes(self, metrics_dict, output_index):\n+        \"\"\"Sets the metric attributes on the model for the given output.\n+\n+        # Arguments\n+            metrics_dict: A dict with metric names as keys and metric fns as values.\n+            output_index: The index of the model output for which the metric\n+                attributes are added.\n+\n+        # Returns\n+            Metrics dict updated with unique metric names as keys.\n+        \"\"\"\n+        updated_metrics_dict = collections.OrderedDict()\n+        for metric_name, metric_fn in metrics_dict.items():\n+            metric_name = self._add_unique_metric_name(metric_name, output_index)\n+\n+            # Update the name on the metric class to be the unique generated name.\n+            metric_fn._name = metric_name\n+            updated_metrics_dict[metric_name] = metric_fn\n+            # Keep track of metric function.\n+            self._compile_metric_functions.append(metric_fn)\n+        return updated_metrics_dict\n+\n+    def _set_metric_attributes(self):\n+        \"\"\"Sets the metric attributes on the model for all the model outputs.\"\"\"\n+        updated_per_output_metrics = []\n+        updated_per_output_weighted_metrics = []\n+        for i in range(len(self.outputs)):\n+            if i in skip_target_indices:\n+                updated_per_output_metrics.append(self._per_output_metrics[i])\n+                updated_per_output_weighted_metrics.append(\n+                    self._per_output_weighted_metrics[i])\n+                continue\n+            updated_per_output_metrics.append(\n+                self._set_per_output_metric_attributes(\n+                    self._per_output_metrics[i], i))\n+            updated_per_output_weighted_metrics.append(\n+                self._set_per_output_metric_attributes(\n+                    self._per_output_weighted_metrics[i], i))\n+\n+        # Create a metric wrapper for each output loss. This computes mean of an\n+        # output loss across mini-batches (irrespective of how we reduce within a\n+        # batch).\n+        if len(self.outputs) > 1:\n+            self._output_loss_metrics = [\n+                metrics_module.Mean(name=self.output_names[i] + '_loss')\n+                for i in range(len(self.loss_functions))\n+                if i not in skip_target_indices\n+            ]\n+\n+        self._per_output_metrics = updated_per_output_metrics\n+        self._per_output_weighted_metrics = updated_per_output_weighted_metrics\n+\n+    def _handle_per_output_metrics(self,\n+                                   metrics_dict,\n+                                   y_true,\n+                                   y_pred,\n+                                   mask,\n+                                   weights=None):\n+        \"\"\"Calls metric functions for a single output.\n+\n+        # Arguments\n+            metrics_dict: A dict with metric names as keys and metric fns as values.\n+            y_true: Target output.\n+            y_pred: Predicted output.\n+            mask: Computed mask value for the current output.\n+            weights: Weights to be applied on the current output.\n+\n+        # Returns\n+            A list of metric result tensors.\n+        \"\"\"\n+        metric_results = []\n+        for metric_name, metric_fn in metrics_dict.items():\n+            with K.name_scope(metric_name):\n+                metric_result = training_utils.call_metric_function(\n+                    metric_fn, y_true, y_pred, weights=weights, mask=mask)\n+                metric_results.append(metric_result)\n+        return metric_results\n+\n+    def _handle_metrics(self,\n+                        outputs,\n+                        targets=None,\n+                        skip_target_masks=None,\n+                        sample_weights=None,\n+                        masks=None):\n+        \"\"\"Handles calling metric functions.\n+\n+        # Arguments\n+            outputs: List of outputs (predictions).\n+            targets: List of targets.\n+            skip_target_masks: Optional. List of boolean for whether the\n+                corresponding target should be ignored or not.\n+            sample_weights: Optional list of sample weight arrays.\n+            masks: List of computed output mask values.\n+\n+        Returns:\n+          A list of metric result tensors.\n+        \"\"\"\n+        skip_target_masks = skip_target_masks or [False] * len(outputs)\n+        with K.name_scope('metrics'):\n+            # Invoke all metrics added using `compile`.\n+            for i in range(len(outputs)):\n+                if skip_target_masks[i]:\n+                    continue\n+                output = outputs[i] if outputs else None\n+                target = targets[i] if targets else None\n+                output_mask = masks[i] if masks else None\n+\n+                self._handle_per_output_metrics(\n+                    self._per_output_metrics[i], target, output, output_mask)\n+                self._handle_per_output_metrics(\n+                    self._per_output_weighted_metrics[i],\n+                    target,\n+                    output,\n+                    output_mask,\n+                    weights=sample_weights[i] if sample_weights else None)\n+        return metric_results\n+\n     def _get_callback_model(self):\n         \"\"\"Returns the Callback Model for this Model.\"\"\"\n         if hasattr(self, 'callback_model') and self.callback_model:\n@@ -1326,7 +1458,8 @@ class Model(Network):\n \n     def train_on_batch(self, x, y,\n                        sample_weight=None,\n-                       class_weight=None):\n+                       class_weight=None,\n+                       reset_metrics=True):\n         \"\"\"Runs a single gradient update on a single batch of data.\n \n         # Arguments\n@@ -1353,6 +1486,9 @@ class Model(Network):\n                 from this class during training.\n                 This can be useful to tell the model to \"pay more attention\" to\n                 samples from an under-represented class.\n+            reset_metrics: If `True`, the metrics returned will be only for this\n+                batch. If `False`, the metrics will be statefully accumulated across\n+                batches.\n \n         # Returns\n             Scalar training loss\n@@ -1371,9 +1507,12 @@ class Model(Network):\n             ins = x + y + sample_weights\n         self._make_train_function()\n         outputs = self.train_function(ins)\n+\n+        if reset_metrics:\n+            self.reset_metrics()\n         return unpack_singleton(outputs)\n \n-    def test_on_batch(self, x, y, sample_weight=None):\n+    def test_on_batch(self, x, y, sample_weight=None, reset_metrics=True):\n         \"\"\"Test the model on a single batch of samples.\n \n         # Arguments\n@@ -1394,6 +1533,9 @@ class Model(Network):\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify\n                 sample_weight_mode=\"temporal\" in compile().\n+            reset_metrics: If `True`, the metrics returned will be only for this\n+                batch. If `False`, the metrics will be statefully accumulated across\n+                batches.\n \n         # Returns\n             Scalar test loss (if the model has a single output and no metrics)\n@@ -1410,6 +1552,9 @@ class Model(Network):\n             ins = x + y + sample_weights\n         self._make_test_function()\n         outputs = self.test_function(ins)\n+\n+        if reset_metrics:\n+            self.reset_metrics()\n         return unpack_singleton(outputs)\n \n     def predict_on_batch(self, x):\n\n@@ -104,7 +104,7 @@ def fit_loop(model, fit_function, fit_inputs,\n \n     model.history = cbks.History()\n     _callbacks = [cbks.BaseLogger(\n-        stateful_metrics=model.stateful_metric_names)]\n+        stateful_metrics=model.metrics[1:])]\n     if verbose:\n         if steps_per_epoch is not None:\n             count_mode = 'steps'\n@@ -113,7 +113,7 @@ def fit_loop(model, fit_function, fit_inputs,\n         _callbacks.append(\n             cbks.ProgbarLogger(\n                 count_mode,\n-                stateful_metrics=model.stateful_metric_names))\n+                stateful_metrics=model.metrics[1:]))\n     _callbacks += (callbacks or []) + [model.history]\n     callbacks = cbks.CallbackList(_callbacks)\n     out_labels = out_labels or []\n\n@@ -80,12 +80,12 @@ def fit_generator(model,\n     # prepare callbacks\n     model.history = cbks.History()\n     _callbacks = [cbks.BaseLogger(\n-        stateful_metrics=model.stateful_metric_names)]\n+        stateful_metrics=model.metrics[1:])]\n     if verbose:\n         _callbacks.append(\n             cbks.ProgbarLogger(\n                 count_mode='steps',\n-                stateful_metrics=model.stateful_metric_names))\n+                stateful_metrics=model.metrics[1:]))\n     _callbacks += (callbacks or []) + [model.history]\n     callbacks = cbks.CallbackList(_callbacks)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b78e3ff9263a6c3b28c869a73f74a1fbd6fd491c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 580 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -157,7 +157,7 @@ def visualize_layer(model,\n                 if loss_value <= K.epsilon():\n                     return None\n \n-            # Calulate upscaled dimension\n+            # Calculate upscaled dimension\n             intermediate_dim = tuple(\n                 int(x / (upscaling_factor ** up)) for x in output_dim)\n             # Upscale\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b938b91b527760cef3af29031e74ab181f2fc2b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 225 | Lines Deleted: 398 | Files Changed: 13 | Hunks: 62 | Methods Changed: 41 | Complexity Δ (Sum/Max): 3/28 | Churn Δ: 623 | Churn Cumulative: 21757 | Contributors (this commit): 143 | Commits (past 90d): 41 | Contributors (cumulative): 235 | DMM Complexity: 0.08088235294117647\n\nDIFF:\n@@ -179,8 +179,8 @@ def _serialize_model(model, h5dict, include_optimizer=True):\n                     'config': model.optimizer.get_config()\n                 },\n                 'loss': model.loss,\n-                'metrics': model.metrics,\n-                'weighted_metrics': model.weighted_metrics,\n+                'metrics': model._compile_metrics,\n+                'weighted_metrics': model._compile_weighted_metrics,\n                 'sample_weight_mode': model.sample_weight_mode,\n                 'loss_weights': model.loss_weights,\n             }, default=get_json_type).encode('utf8')\n\n@@ -4,6 +4,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import collections\n import warnings\n import copy\n import numpy as np\n@@ -124,11 +125,11 @@ class Model(Network):\n \n         # if loss function is None, then this output will be skipped during total\n         # loss calculation and feed targets preparation.\n-        skip_target_indices = []\n+        self.skip_target_indices = []\n         skip_target_weighing_indices = []\n         for i, loss_function in enumerate(self.loss_functions):\n             if loss_function is None:\n-                skip_target_indices.append(i)\n+                self.skip_target_indices.append(i)\n                 skip_target_weighing_indices.append(i)\n \n         # Prepare output masks.\n@@ -177,7 +178,7 @@ class Model(Network):\n                                 target_tensors)\n \n         for i in range(len(self.outputs)):\n-            if i in skip_target_indices:\n+            if i in self.skip_target_indices:\n                 self.targets.append(None)\n             else:\n                 shape = K.int_shape(self.outputs[i])\n@@ -215,8 +216,9 @@ class Model(Network):\n         # Invoke metric functions (unweighted) for all the outputs.\n         self._handle_metrics(\n             self.outputs,\n-            targets=self._targets,\n+            targets=self.targets,\n             skip_target_masks=[l is None for l in self.loss_functions],\n+            sample_weights=self.sample_weights,\n             masks=masks)\n \n         # Compute total loss.\n@@ -224,7 +226,7 @@ class Model(Network):\n         # eg., total_loss = loss_weight_1 * output_1_loss_fn(...) +\n         #                   loss_weight_2 * output_2_loss_fn(...) +\n         #                   layer losses.\n-        self.total_loss = self._prepare_total_loss(skip_target_indices, masks)\n+        self.total_loss = self._prepare_total_loss(masks)\n \n         # Functions for train, test and predict will\n         # be compiled lazily when required.\n@@ -253,11 +255,11 @@ class Model(Network):\n         metrics_names = ['loss']\n         if self._is_compiled:\n             # Add output loss metric names to the metric names list.\n-            if len(self._training_endpoints) > 1:\n+            if len(self.outputs) > 1:\n                 metrics_names.extend([\n                     self.output_names[i] + '_loss'\n                     for i in range(len(self.outputs))\n-                    if i not in skip_target_indices\n+                    if i not in self.skip_target_indices\n                 ])\n \n             # Add compile metrics/weighted metrics' names to the metric names list.\n@@ -528,7 +530,8 @@ class Model(Network):\n                     target_tensors = None\n                 self.compile(optimizer=self.optimizer,\n                              loss=self.loss,\n-                             metrics=self.metrics,\n+                             metrics=self._compile_metrics,\n+                             weighted_metrics=self._compile_weighted_metrics,\n                              loss_weights=self.loss_weights,\n                              target_tensors=target_tensors)\n \n@@ -638,7 +641,7 @@ class Model(Network):\n                                  str(x[0].shape[0]) + ' samples')\n         return x, y, sample_weights\n \n-    def _prepare_total_loss(self, skip_target_indices=None, masks=None):\n+    def _prepare_total_loss(self, masks=None):\n         \"\"\"Computes total loss from loss functions.\n \n         # Arguments\n@@ -649,14 +652,13 @@ class Model(Network):\n         # Returns\n             A list of loss weights of python floats.\n         \"\"\"\n-        skip_target_indices = skip_target_indices or []\n         total_loss = None\n         with K.name_scope('loss'):\n             zipped_inputs = zip(self.targets, self.outputs, self.loss_functions,\n                                 self.sample_weights, masks, self.loss_weights_list)\n             for i, (y_true, y_pred, loss_fn, sample_weight, mask,\n                     loss_weight) in enumerate(zipped_inputs):\n-                if i in skip_target_indices:\n+                if i in self.skip_target_indices:\n                     continue\n                 loss_name = self.output_names[i] + '_loss'\n                 with K.name_scope(loss_name):\n@@ -743,6 +745,7 @@ class Model(Network):\n         \"\"\"\n         if len(self.output_names) > 1:\n             metric_name = '%s_%s' % (self.output_names[output_index], metric_name)\n+\n         j = 1\n         base_metric_name = metric_name\n         while metric_name in self.metrics_names:\n@@ -766,7 +769,7 @@ class Model(Network):\n             metric_name = self._add_unique_metric_name(metric_name, output_index)\n \n             # Update the name on the metric class to be the unique generated name.\n-            metric_fn._name = metric_name\n+            metric_fn.name = metric_name\n             updated_metrics_dict[metric_name] = metric_fn\n             # Keep track of metric function.\n             self._compile_metric_functions.append(metric_fn)\n@@ -777,7 +780,7 @@ class Model(Network):\n         updated_per_output_metrics = []\n         updated_per_output_weighted_metrics = []\n         for i in range(len(self.outputs)):\n-            if i in skip_target_indices:\n+            if i in self.skip_target_indices:\n                 updated_per_output_metrics.append(self._per_output_metrics[i])\n                 updated_per_output_weighted_metrics.append(\n                     self._per_output_weighted_metrics[i])\n@@ -796,7 +799,6 @@ class Model(Network):\n             self._output_loss_metrics = [\n                 metrics_module.Mean(name=self.output_names[i] + '_loss')\n                 for i in range(len(self.loss_functions))\n-                if i not in skip_target_indices\n             ]\n \n         self._per_output_metrics = updated_per_output_metrics\n@@ -816,17 +818,12 @@ class Model(Network):\n             y_pred: Predicted output.\n             mask: Computed mask value for the current output.\n             weights: Weights to be applied on the current output.\n-\n-        # Returns\n-            A list of metric result tensors.\n         \"\"\"\n-        metric_results = []\n+\n         for metric_name, metric_fn in metrics_dict.items():\n             with K.name_scope(metric_name):\n                 metric_result = training_utils.call_metric_function(\n                     metric_fn, y_true, y_pred, weights=weights, mask=mask)\n-                metric_results.append(metric_result)\n-        return metric_results\n \n     def _handle_metrics(self,\n                         outputs,\n@@ -843,9 +840,6 @@ class Model(Network):\n                 corresponding target should be ignored or not.\n             sample_weights: Optional list of sample weight arrays.\n             masks: List of computed output mask values.\n-\n-        Returns:\n-          A list of metric result tensors.\n         \"\"\"\n         skip_target_masks = skip_target_masks or [False] * len(outputs)\n         with K.name_scope('metrics'):\n@@ -865,7 +859,6 @@ class Model(Network):\n                     output,\n                     output_mask,\n                     weights=sample_weights[i] if sample_weights else None)\n-        return metric_results\n \n     def _get_callback_model(self):\n         \"\"\"Returns the Callback Model for this Model.\"\"\"\n@@ -1210,10 +1203,7 @@ class Model(Network):\n         if do_validation:\n             self._make_test_function()\n             val_function = self.test_function\n-            callback_metrics = copy.copy(out_labels) + [\n-                'val_' + n for n in out_labels]\n         else:\n-            callback_metrics = copy.copy(out_labels)\n             val_function = None\n             val_inputs = []\n \n@@ -1227,7 +1217,6 @@ class Model(Network):\n                                         val_function=val_function,\n                                         val_inputs=val_inputs,\n                                         shuffle=shuffle,\n-                                        callback_metrics=callback_metrics,\n                                         initial_epoch=initial_epoch,\n                                         steps_per_epoch=steps_per_epoch,\n                                         validation_steps=validation_steps,\n\n@@ -28,7 +28,6 @@ def fit_loop(model, fit_function, fit_inputs,\n              val_function=None,\n              val_inputs=None,\n              shuffle=True,\n-             callback_metrics=None,\n              initial_epoch=0,\n              steps_per_epoch=None,\n              validation_steps=None,\n@@ -51,11 +50,6 @@ def fit_loop(model, fit_function, fit_inputs,\n         val_function: Keras function to call for validation\n         val_inputs: List of tensors to be fed to `val_function`\n         shuffle: Whether to shuffle the data at the beginning of each epoch\n-        callback_metrics: List of strings, the display names of the metrics\n-            passed to the callbacks. They should be the\n-            concatenation of list the display names of the outputs of\n-             `fit_function` and the list of display names\n-             of the outputs of `fit_inputs`.\n         initial_epoch: Epoch at which to start training\n             (useful for resuming a previous training run)\n         steps_per_epoch: Total number of steps (batches of samples)\n@@ -103,17 +97,14 @@ def fit_loop(model, fit_function, fit_inputs,\n         index_array = np.arange(num_train_samples)\n \n     model.history = cbks.History()\n-    _callbacks = [cbks.BaseLogger(\n-        stateful_metrics=model.metrics[1:])]\n+    _callbacks = [cbks.BaseLogger(stateful_metrics=model.metrics_names[1:])]\n     if verbose:\n         if steps_per_epoch is not None:\n             count_mode = 'steps'\n         else:\n             count_mode = 'samples'\n         _callbacks.append(\n-            cbks.ProgbarLogger(\n-                count_mode,\n-                stateful_metrics=model.metrics[1:]))\n+            cbks.ProgbarLogger(count_mode, stateful_metrics=model.metrics_names[1:]))\n     _callbacks += (callbacks or []) + [model.history]\n     callbacks = cbks.CallbackList(_callbacks)\n     out_labels = out_labels or []\n@@ -121,6 +112,9 @@ def fit_loop(model, fit_function, fit_inputs,\n     # it's possible to callback a different model than itself\n     # (used by Sequential models)\n     callback_model = model._get_callback_model()\n+    callback_metrics = list(model.metrics_names)\n+    if do_validation:\n+        callback_metrics += ['val_' + n for n in model.metrics_names]\n \n     callbacks.set_model(callback_model)\n     callbacks.set_params({\n@@ -130,7 +124,7 @@ def fit_loop(model, fit_function, fit_inputs,\n         'samples': num_train_samples,\n         'verbose': verbose,\n         'do_validation': do_validation,\n-        'metrics': callback_metrics or [],\n+        'metrics': callback_metrics,\n     })\n     callbacks._call_begin_hook('train')\n     callbacks.model.stop_training = False\n@@ -148,9 +142,7 @@ def fit_loop(model, fit_function, fit_inputs,\n             indices_for_conversion_to_dense.append(i)\n \n     for epoch in range(initial_epoch, epochs):\n-        # Reset stateful metrics\n-        for m in model.stateful_metric_functions:\n-            m.reset_states()\n+        model.reset_metrics()\n         callbacks.on_epoch_begin(epoch)\n         epoch_logs = {}\n         if steps_per_epoch is not None:\n@@ -373,15 +365,7 @@ def test_loop(model, f, ins,\n         the display labels for the scalar outputs.\n     \"\"\"\n \n-    if hasattr(model, 'metrics'):\n-        for m in model.stateful_metric_functions:\n-            m.reset_states()\n-        stateful_metric_indices = [\n-            i for i, name in enumerate(model.metrics_names)\n-            if str(name) in model.stateful_metric_names]\n-    else:\n-        stateful_metric_indices = []\n-\n+    model.reset_metrics()\n     num_samples = check_num_samples(ins,\n                                     batch_size=batch_size,\n                                     steps=steps,\n@@ -392,8 +376,6 @@ def test_loop(model, f, ins,\n         callbacks = cbks.CallbackList(callbacks)\n         callback_model = model._get_callback_model()\n         callbacks.set_model(callback_model)\n-        callback_metrics = []\n-        if hasattr(model, 'metrics_names'):\n         callback_metrics = list(model.metrics_names)\n         callback_params = {\n             'batch_size': batch_size,\n@@ -433,7 +415,7 @@ def test_loop(model, f, ins,\n                 if step == 0:\n                     outs.extend([0.] * len(batch_outs))\n                 for i, batch_out in enumerate(batch_outs):\n-                    if i in stateful_metric_indices:\n+                    if i == 0:  # Index 0 == `Loss`\n                         outs[i] = float(batch_out)\n                     else:\n                         outs[i] += batch_out\n@@ -442,16 +424,13 @@ def test_loop(model, f, ins,\n                     outs.append(0.)\n                 outs[0] += batch_outs\n \n-            if hasattr(model, 'metrics_names'):\n             for l, o in zip(model.metrics_names, batch_outs):\n                 batch_logs[l] = o\n             callbacks._call_batch_hook('test', 'end', step, batch_logs)\n \n             if verbose == 1:\n                 progbar.update(step + 1)\n-        for i in range(len(outs)):\n-            if i not in stateful_metric_indices:\n-                outs[i] /= steps\n+        outs[0] /= steps  # Index 0 == `Loss`\n     else:\n         batches = make_batches(num_samples, batch_size)\n         index_array = np.arange(num_samples)\n@@ -472,24 +451,21 @@ def test_loop(model, f, ins,\n                 if batch_index == 0:\n                     outs.extend([0.] * len(batch_outs))\n                 for i, batch_out in enumerate(batch_outs):\n-                    if i in stateful_metric_indices:\n-                        outs[i] = batch_out\n-                    else:\n+                    if i == 0:  # Index 0 == `Loss`\n                         outs[i] += batch_out * len(batch_ids)\n+                    else:\n+                        outs[i] = batch_out\n             else:\n                 if batch_index == 0:\n                     outs.append(0.)\n                 outs[0] += batch_outs * len(batch_ids)\n \n-            if hasattr(model, 'metrics_names'):\n             for l, o in zip(model.metrics_names, batch_outs):\n                 batch_logs[l] = o\n             callbacks._call_batch_hook('test', 'end', batch_index, batch_logs)\n \n             if verbose == 1:\n                 progbar.update(batch_end)\n-        for i in range(len(outs)):\n-            if i not in stateful_metric_indices:\n-                outs[i] /= num_samples\n+        outs[0] /= num_samples  # Index 0 == `Loss`\n     callbacks._call_end_hook('test')\n     return unpack_singleton(outs)\n\n@@ -80,12 +80,12 @@ def fit_generator(model,\n     # prepare callbacks\n     model.history = cbks.History()\n     _callbacks = [cbks.BaseLogger(\n-        stateful_metrics=model.metrics[1:])]\n+        stateful_metrics=model.metrics_names[1:])]\n     if verbose:\n         _callbacks.append(\n             cbks.ProgbarLogger(\n                 count_mode='steps',\n-                stateful_metrics=model.metrics[1:]))\n+                stateful_metrics=model.metrics_names[1:]))\n     _callbacks += (callbacks or []) + [model.history]\n     callbacks = cbks.CallbackList(_callbacks)\n \n@@ -172,8 +172,7 @@ def fit_generator(model,\n         # Construct epoch logs.\n         epoch_logs = {}\n         while epoch < epochs:\n-            for m in model.stateful_metric_functions:\n-                m.reset_states()\n+            model.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n             steps_done = 0\n             batch_index = 0\n@@ -278,15 +277,7 @@ def evaluate_generator(model, generator,\n                        verbose=0):\n     \"\"\"See docstring for `Model.evaluate_generator`.\"\"\"\n     model._make_test_function()\n-\n-    if hasattr(model, 'metrics'):\n-        for m in model.stateful_metric_functions:\n-            m.reset_states()\n-        stateful_metric_indices = [\n-            i for i, name in enumerate(model.metrics_names)\n-            if str(name) in model.stateful_metric_names]\n-    else:\n-        stateful_metric_indices = []\n+    model.reset_metrics()\n \n     steps_done = 0\n     outs_per_batch = []\n@@ -313,8 +304,6 @@ def evaluate_generator(model, generator,\n         callbacks = cbks.CallbackList(callbacks)\n         callback_model = model._get_callback_model()\n         callbacks.set_model(callback_model)\n-        callback_metrics = []\n-        if hasattr(model, 'metrics_names'):\n         callback_metrics = list(model.metrics_names)\n         callback_params = {\n             'steps': steps,\n@@ -386,7 +375,6 @@ def evaluate_generator(model, generator,\n             outs = to_list(outs)\n             outs_per_batch.append(outs)\n \n-            if hasattr(model, 'metrics_names'):\n             for l, o in zip(model.metrics_names, outs):\n                 batch_logs[l] = o\n             callbacks._call_batch_hook('test', 'end', steps_done, batch_logs)\n@@ -402,12 +390,8 @@ def evaluate_generator(model, generator,\n         if enqueuer is not None:\n             enqueuer.stop()\n \n-    averages = []\n-    for i in range(len(outs)):\n-        if i not in stateful_metric_indices:\n-            averages.append(np.average([out[i] for out in outs_per_batch],\n-                                       weights=batch_sizes))\n-        else:\n+    averages = [float(outs_per_batch[-1][0])]  # index 0 = 'loss'\n+    for i in range(1, len(outs)):\n         averages.append(np.float64(outs_per_batch[-1][i]))\n     return unpack_singleton(averages)\n \n\n@@ -10,9 +10,11 @@ import copy\n import numpy as np\n import six\n import warnings\n+from collections import OrderedDict\n \n from .. import backend as K\n from .. import losses\n+from .. import metrics as metrics_module\n from ..utils import Sequence\n from ..utils import generic_utils\n from ..utils import losses_utils\n@@ -326,47 +328,6 @@ def check_generator_arguments(y=None, sample_weight=None,\n                          'you cannot use `validation_split`.')\n \n \n-def collect_metrics(metrics, output_names):\n-    \"\"\"Maps metric functions to model outputs.\n-\n-    # Arguments\n-        metrics: a list or dict of metric functions.\n-        output_names: a list of the names (strings) of model outputs.\n-\n-    # Returns\n-        A list (one entry per model output) of lists of metric functions.\n-        For instance, if the model has 2 outputs, and for the first output\n-        we want to compute \"binary_accuracy\" and \"binary_crossentropy\",\n-        and just \"binary_accuracy\" for the second output,\n-        the list would look like:\n-            `[[binary_accuracy, binary_crossentropy], [binary_accuracy]]`\n-\n-    # Raises\n-        TypeError: if an incorrect type is passed for the `metrics` argument.\n-    \"\"\"\n-    if not metrics:\n-        return [[] for _ in output_names]\n-    if isinstance(metrics, list):\n-        # we then apply all metrics to all outputs.\n-        return [copy.copy(metrics) for _ in output_names]\n-    elif isinstance(metrics, dict):\n-        nested_metrics = []\n-        if not set(metrics.keys()).issubset(set(output_names)):\n-            unknown_output_names = list(set(metrics.keys()) - set(output_names))\n-            warnings.warn('Invalid layer name for metric computations: '\n-                          '{}. Available names are {}.'\n-                          .format(unknown_output_names, output_names))\n-        for name in output_names:\n-            output_metrics = metrics.get(name, [])\n-            output_metrics = generic_utils.to_list(output_metrics)\n-            nested_metrics.append(output_metrics)\n-        return nested_metrics\n-    else:\n-        raise TypeError('Type of `metrics` argument not understood. '\n-                        'Expected a list or dictionary, found: ' +\n-                        str(metrics))\n-\n-\n def batch_shuffle(index_array, batch_size):\n     \"\"\"Shuffles an array in a batch-wise fashion.\n \n@@ -910,3 +871,165 @@ def prepare_loss_weights(output_names, loss_weights=None):\n                         str(loss_weights) + ' - expected a list of dicts.')\n \n     return weights_list\n+\n+\n+def collect_per_output_metric_info(metrics,\n+                                   output_names,\n+                                   output_shapes,\n+                                   loss_fns,\n+                                   is_weighted=False):\n+    \"\"\"Maps metric names and functions to model outputs.\n+\n+    # Arguments\n+        metrics: a list or a list of lists or a dict of metric functions.\n+        output_names: a list of the names (strings) of model outputs.\n+        output_shapes: a list of the shapes (strings) of model outputs.\n+        loss_fns: a list of the loss functions corresponding to the model outputs.\n+        is_weighted: Boolean indicating whether the given metrics are weighted.\n+\n+    # Returns\n+        A list (one entry per model output) of dicts.\n+        For instance, if the model has 2 outputs, and for the first output\n+        we want to compute \"binary_accuracy\" and \"binary_crossentropy\",\n+        and just \"binary_accuracy\" for the second output,\n+        the list would look like: `[{\n+            'acc': binary_accuracy(),\n+            'ce': binary_crossentropy(),\n+        }, {\n+            'acc': binary_accuracy(),\n+        }]`\n+\n+    # Raises\n+        TypeError: if an incorrect type is passed for the `metrics` argument.\n+    \"\"\"\n+    if not metrics:\n+        return [{} for _ in output_names]\n+\n+    if isinstance(metrics, list):\n+        any_sub_list = any(isinstance(m, list) for m in metrics)\n+        if any_sub_list:\n+            if len(metrics) != len(output_names):\n+                raise ValueError('When passing a list of lists as `metrics`, '\n+                                 'it should have one entry per model output. '\n+                                 'The model has ' + str(len(output_names)) +\n+                                 ' outputs, but you passed metrics=' + str(metrics))\n+            # User has provided a list of len = len(outputs).\n+            nested_metrics = [generic_utils.to_list(m) for m in metrics]\n+        else:\n+            # If it is a single list we then apply all metrics to all outputs.\n+            if len(output_names) > 1:\n+                nested_metrics = []\n+                for _ in output_names:\n+                    nested_metrics.append(\n+                        [metrics_module.clone_metric(m) for m in metrics])\n+            else:\n+                nested_metrics = [metrics]\n+    elif isinstance(metrics, collections.Mapping):\n+        generic_utils.check_for_unexpected_keys('metrics', metrics, output_names)\n+        nested_metrics = []\n+        for name in output_names:\n+            output_metrics = generic_utils.to_list(metrics.get(name, []))\n+            nested_metrics.append(output_metrics)\n+    else:\n+        raise TypeError('Type of `metrics` argument not understood. '\n+                        'Expected a list or dictionary, found: ' + str(metrics))\n+\n+    per_output_metrics = []\n+    for i, metrics in enumerate(nested_metrics):\n+        metrics_dict = OrderedDict()\n+        for metric in metrics:\n+            metric_name = get_metric_name(metric, is_weighted)\n+            metric_fn = get_metric_function(\n+                metric, output_shape=output_shapes[i], loss_fn=loss_fns[i])\n+\n+            # If the metric function is not stateful, we create a stateful version.\n+            if not isinstance(metric_fn, metrics_module.Metric):\n+                metric_fn = metrics_module.MeanMetricWrapper(\n+                    metric_fn, name=metric_name)\n+            metrics_dict[metric_name] = metric_fn\n+        per_output_metrics.append(metrics_dict)\n+\n+    return per_output_metrics\n+\n+\n+def get_metric_name(metric, weighted=False):\n+    \"\"\"Returns the name corresponding to the given metric input.\n+\n+    # Arguments\n+        metric: Metric function name or reference.\n+        weighted: Boolean indicating if the given metric is weighted.\n+\n+    # Returns\n+        The metric name.\n+    \"\"\"\n+    # We keep the string that the user has set in compile as the metric name.\n+    if isinstance(metric, six.string_types):\n+        return metric\n+\n+    metric = metrics_module.get(metric)\n+    return metric.name if hasattr(metric, 'name') else metric.__name__\n+\n+\n+def get_metric_function(metric, output_shape=None, loss_fn=None):\n+    \"\"\"Returns the metric function corresponding to the given metric input.\n+\n+    # Arguments\n+        metric: Metric function name or reference.\n+        output_shape: The shape of the output that this metric will be calculated\n+            for.\n+        loss_fn: The loss function used.\n+\n+    # Returns\n+        The metric function.\n+    \"\"\"\n+    if metric not in ['accuracy', 'acc', 'crossentropy', 'ce']:\n+        return metrics_module.get(metric)\n+\n+    is_sparse_categorical_crossentropy = (\n+        isinstance(loss_fn, losses.SparseCategoricalCrossentropy) or\n+        (isinstance(loss_fn, losses.LossFunctionWrapper) and\n+         loss_fn.fn == losses.sparse_categorical_crossentropy))\n+\n+    is_binary_crossentropy = (\n+        isinstance(loss_fn, losses.BinaryCrossentropy) or\n+        (isinstance(loss_fn, losses.LossFunctionWrapper) and\n+         loss_fn.fn == losses.binary_crossentropy))\n+\n+    if metric in ['accuracy', 'acc']:\n+        if output_shape[-1] == 1 or is_binary_crossentropy:\n+            return metrics_module.binary_accuracy\n+        elif is_sparse_categorical_crossentropy:\n+            return metrics_module.sparse_categorical_accuracy\n+        # If the output_shape[-1] is not 1, then we know output is `categorical`.\n+        # We assume it is sparse categorical only if loss is explicitly given\n+        # as sparse categorical crossentropy loss.\n+        return metrics_module.categorical_accuracy\n+    else:\n+        if output_shape[-1] == 1 or is_binary_crossentropy:\n+            return metrics_module.binary_crossentropy\n+        elif is_sparse_categorical_crossentropy:\n+            return metrics_module.sparse_categorical_crossentropy\n+        return metrics_module.categorical_crossentropy\n+\n+\n+def call_metric_function(metric_fn,\n+                         y_true,\n+                         y_pred=None,\n+                         weights=None,\n+                         mask=None):\n+    \"\"\"Invokes metric function and returns the metric result tensor.\"\"\"\n+    if mask is not None:\n+        mask = math_ops.cast(mask, y_pred.dtype)\n+        if weights is None:\n+            # Use mask as sample weight.\n+            weights = mask\n+        else:\n+            # Update dimensions of weights to match with mask.\n+            mask, _, weights = tf_losses_utils.squeeze_or_expand_dimensions(\n+                mask, sample_weight=weights)\n+            weights *= mask\n+\n+    if y_pred is not None:\n+        return metric_fn(y_true, y_pred, sample_weight=weights)\n+    # `Mean` metric only takes a single value.\n+    return metric_fn(y_true, sample_weight=weights)\n\n@@ -365,6 +365,22 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n                   K.floatx())\n \n \n+def clone_metric(metric):\n+    \"\"\"Returns a clone of the metric if stateful, otherwise returns it as is.\"\"\"\n+    if isinstance(metric, Metric):\n+        return metric.__class__.from_config(metric.get_config())\n+    return metric\n+\n+\n+def clone_metrics(metrics):\n+    \"\"\"Clones the given metric list/dict.\"\"\"\n+    if metrics is None:\n+        return None\n+    if isinstance(metrics, dict):\n+        return {key: clone_metric(value) for key, value in metrics.items()}\n+    return [clone_metric(metric) for metric in metrics]\n+\n+\n # Aliases\n \n mse = MSE = mean_squared_error\n\n@@ -54,6 +54,8 @@ def result_wrapper(result_fn):\n         Decorated function that wraps `result()` with identity op.\n     \"\"\"\n     def decorated(metric_obj, *args, **kwargs):\n-        return K.identity(result_fn(*args, **kwargs))\n+        result_t = K.identity(result_fn(*args, **kwargs))\n+        metric_obj._call_result = result_t\n+        return result_t\n \n     return decorated\n\n@@ -293,7 +293,7 @@ class KerasClassifier(BaseWrapper):\n         outputs = self.model.evaluate(x, y, **kwargs)\n         outputs = to_list(outputs)\n         for name, output in zip(self.model.metrics_names, outputs):\n-            if name == 'acc':\n+            if name in ['accuracy', 'acc']:\n                 return output\n         raise ValueError('The model is not configured to compute accuracy. '\n                          'You should pass `metrics=[\"accuracy\"]` to '\n\n@@ -466,7 +466,7 @@ def test_ModelCheckpoint(tmpdir):\n \n     # case 3\n     mode = 'max'\n-    monitor = 'val_acc'\n+    monitor = 'val_accuracy'\n     cbks = [callbacks.ModelCheckpoint(filepath,\n                                       monitor=monitor,\n                                       save_best_only=save_best_only,\n\n@@ -830,32 +830,6 @@ def test_check_bad_shape():\n     assert 'targets to have the same shape' in str(exc)\n \n \n-@pytest.mark.parametrize('input_metrics,expected_output', [\n-    (None, [[], []]),\n-    (['mse', 'mae'], [['mse', 'mae'], ['mse', 'mae']]),\n-    ({'layer_1': 'mae', 'layer_2': 'mse'}, [['mae'], ['mse']]),\n-])\n-def test_collect_metrics(input_metrics, expected_output):\n-    output_names = ['layer_1', 'layer_2']\n-\n-    output_metrics = training_utils.collect_metrics(input_metrics,\n-                                                    output_names)\n-    assert output_metrics == expected_output\n-\n-\n-def test_collect_metrics_with_invalid_metrics_format():\n-    with pytest.raises(TypeError):\n-        training_utils.collect_metrics({'a', 'set', 'type'}, [])\n-\n-\n-def test_collect_metrics_with_invalid_layer_name():\n-    with pytest.warns(Warning) as w:\n-        training_utils.collect_metrics({'unknown_layer': 'mse'}, ['layer_1'])\n-\n-    warning_raised = all(['unknown_layer' in str(w_.message) for w_ in w])\n-    assert warning_raised, 'Warning was raised for unknown_layer'\n-\n-\n @pytest.mark.skipif(K.backend() != 'tensorflow',\n                     reason='Requires TensorFlow backend')\n def test_model_with_input_feed_tensor():\n\n@@ -121,118 +121,5 @@ def test_sparse_top_k_categorical_accuracy(y_pred, y_true):\n     assert np.mean(failure_result) == 0\n \n \n-# TODO: resolve flakyness issue. Tracked with #11064\n-@pytest.mark.parametrize('metrics_mode', ['list', 'dict'])\n-@flaky(rerun_filter=lambda err, *args: issubclass(err[0], AssertionError))\n-def test_stateful_metrics(metrics_mode):\n-    np.random.seed(1334)\n-\n-    class BinaryTruePositives(keras.layers.Layer):\n-        \"\"\"Stateful Metric to count the total true positives over all batches.\n-\n-        Assumes predictions and targets of shape `(samples, 1)`.\n-\n-        # Arguments\n-            name: String, name for the metric.\n-        \"\"\"\n-\n-        def __init__(self, name='true_positives', **kwargs):\n-            super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n-            self.stateful = True\n-            self.true_positives = K.variable(value=0, dtype='int32')\n-\n-        def reset_states(self):\n-            K.set_value(self.true_positives, 0)\n-\n-        def __call__(self, y_true, y_pred):\n-            \"\"\"Computes the number of true positives in a batch.\n-\n-            # Arguments\n-                y_true: Tensor, batch_wise labels\n-                y_pred: Tensor, batch_wise predictions\n-\n-            # Returns\n-                The total number of true positives seen this epoch at the\n-                    completion of the batch.\n-            \"\"\"\n-            y_true = K.cast(y_true, 'int32')\n-            y_pred = K.cast(K.round(y_pred), 'int32')\n-            correct_preds = K.cast(K.equal(y_pred, y_true), 'int32')\n-            true_pos = K.cast(K.sum(correct_preds * y_true), 'int32')\n-            current_true_pos = self.true_positives * 1\n-            self.add_update(K.update_add(self.true_positives,\n-                                         true_pos),\n-                            inputs=[y_true, y_pred])\n-            return current_true_pos + true_pos\n-\n-    metric_fn = BinaryTruePositives()\n-    config = metrics.serialize(metric_fn)\n-    metric_fn = metrics.deserialize(\n-        config, custom_objects={'BinaryTruePositives': BinaryTruePositives})\n-\n-    # Test on simple model\n-    inputs = keras.Input(shape=(2,))\n-    outputs = keras.layers.Dense(1, activation='sigmoid', name='out')(inputs)\n-    model = keras.Model(inputs, outputs)\n-\n-    if metrics_mode == 'list':\n-        model.compile(optimizer='sgd',\n-                      loss='binary_crossentropy',\n-                      metrics=['acc', metric_fn])\n-    elif metrics_mode == 'dict':\n-        model.compile(optimizer='sgd',\n-                      loss='binary_crossentropy',\n-                      metrics={'out': ['acc', metric_fn]})\n-\n-    samples = 1000\n-    x = np.random.random((samples, 2))\n-    y = np.random.randint(2, size=(samples, 1))\n-\n-    val_samples = 10\n-    val_x = np.random.random((val_samples, 2))\n-    val_y = np.random.randint(2, size=(val_samples, 1))\n-\n-    # Test fit and evaluate\n-    history = model.fit(x, y, validation_data=(val_x, val_y),\n-                        epochs=1, batch_size=10)\n-    outs = model.evaluate(x, y, batch_size=10)\n-    preds = model.predict(x)\n-\n-    def ref_true_pos(y_true, y_pred):\n-        return np.sum(np.logical_and(y_pred > 0.5, y_true == 1))\n-\n-    # Test correctness (e.g. updates should have been run)\n-    np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)\n-\n-    # Test correctness of the validation metric computation\n-    val_preds = model.predict(val_x)\n-    val_outs = model.evaluate(val_x, val_y, batch_size=10)\n-    assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)\n-    assert_allclose(val_outs[2], history.history['val_true_positives'][-1],\n-                    atol=1e-5)\n-\n-    # Test with generators\n-    gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(x, y)]\n-    val_gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(val_x, val_y)]\n-    history = model.fit_generator(iter(gen), epochs=1, steps_per_epoch=samples,\n-                                  validation_data=iter(val_gen),\n-                                  validation_steps=val_samples)\n-    outs = model.evaluate_generator(iter(gen), steps=samples, workers=0)\n-    preds = model.predict_generator(iter(gen), steps=samples, workers=0)\n-\n-    # Test correctness of the metric re ref_true_pos()\n-    np.testing.assert_allclose(outs[2], ref_true_pos(y, preds),\n-                               atol=1e-5)\n-\n-    # Test correctness of the validation metric computation\n-    val_preds = model.predict_generator(iter(val_gen), steps=val_samples, workers=0)\n-    val_outs = model.evaluate_generator(iter(val_gen), steps=val_samples, workers=0)\n-    np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds),\n-                               atol=1e-5)\n-    np.testing.assert_allclose(val_outs[2],\n-                               history.history['val_true_positives'][-1],\n-                               atol=1e-5)\n-\n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -39,7 +39,7 @@ def _test_optimizer(optimizer, target=0.75):\n                   metrics=['accuracy'])\n \n     history = model.fit(x_train, y_train, epochs=3, batch_size=16, verbose=0)\n-    assert history.history['acc'][-1] >= target\n+    assert history.history['accuracy'][-1] >= target\n     config = optimizers.serialize(optimizer)\n     optim = optimizers.deserialize(config)\n     new_config = optimizers.serialize(optim)\n\n@@ -20,7 +20,6 @@ test_samples = 1000\n timesteps = 3\n input_dim = 10\n loss = 'mse'\n-loss_full_name = 'mean_squared_error'\n standard_weight = 1\n standard_score_sequential = 0.5\n \n@@ -158,129 +157,6 @@ def test_sequential_temporal_sample_weights():\n     assert(score < standard_score_sequential)\n \n \n-def test_weighted_metrics_with_sample_weight():\n-    decimal = decimal_precision[K.backend()]\n-\n-    model = create_sequential_model()\n-    model.compile(loss=loss, optimizer='rmsprop',\n-                  metrics=[loss], weighted_metrics=[loss])\n-\n-    ((x_train, y_train), (x_test, y_test),\n-     (sample_weight, class_weight, test_ids)) = _get_test_data()\n-\n-    history = model.fit(x_train, y_train, batch_size=batch_size,\n-                        epochs=epochs // 3, verbose=0,\n-                        sample_weight=sample_weight)\n-\n-    h = history.history\n-    assert_array_almost_equal(h['loss'], h['weighted_' + loss_full_name],\n-                              decimal=decimal)\n-\n-    history = model.fit(x_train, y_train, batch_size=batch_size,\n-                        epochs=epochs // 3, verbose=0,\n-                        sample_weight=sample_weight,\n-                        validation_split=0.1)\n-\n-    h = history.history\n-    assert_almost_equal(h['val_loss'], h['val_weighted_' + loss_full_name],\n-                        decimal=decimal)\n-\n-    model.train_on_batch(x_train[:32], y_train[:32],\n-                         sample_weight=sample_weight[:32])\n-    model.test_on_batch(x_train[:32], y_train[:32],\n-                        sample_weight=sample_weight[:32])\n-\n-    test_sample_weight = np.ones((y_test.shape[0])) * standard_weight\n-    test_sample_weight[test_ids] = high_weight\n-\n-    scores = model.evaluate(x_test, y_test, verbose=0,\n-                            sample_weight=test_sample_weight)\n-    loss_score, metric_score, weighted_metric_score = scores\n-\n-    assert loss_score < standard_score_sequential\n-    assert loss_score != metric_score\n-    assert_almost_equal(loss_score, weighted_metric_score, decimal=decimal)\n-\n-\n-def test_weighted_metrics_with_no_sample_weight():\n-    decimal = decimal_precision[K.backend()]\n-\n-    model = create_sequential_model()\n-    model.compile(loss=loss, optimizer='rmsprop',\n-                  metrics=[loss], weighted_metrics=[loss])\n-\n-    (x_train, y_train), (x_test, y_test), _ = _get_test_data()\n-\n-    history = model.fit(x_train, y_train, batch_size=batch_size,\n-                        epochs=epochs // 3, verbose=0)\n-\n-    h = history.history\n-    assert_array_almost_equal(h['loss'], h[loss_full_name], decimal=decimal)\n-    assert_array_almost_equal(h['loss'], h['weighted_' + loss_full_name],\n-                              decimal=decimal)\n-\n-    history = model.fit(x_train, y_train, batch_size=batch_size,\n-                        epochs=epochs // 3, verbose=0, validation_split=0.1)\n-\n-    h = history.history\n-    assert_array_almost_equal(h['val_loss'], h['val_' + loss_full_name],\n-                              decimal=decimal)\n-    assert_array_almost_equal(h['val_loss'], h['val_weighted_' + loss_full_name],\n-                              decimal=decimal)\n-\n-    model.train_on_batch(x_train[:32], y_train[:32])\n-    model.test_on_batch(x_train[:32], y_train[:32])\n-\n-    scores = model.evaluate(x_test, y_test, verbose=0)\n-    loss_score, metric_score, weighted_metric_score = scores\n-\n-    assert_almost_equal(loss_score, metric_score, decimal=decimal)\n-    assert_almost_equal(loss_score, weighted_metric_score, decimal=decimal)\n-\n-\n-def test_weighted_metrics_with_weighted_accuracy_metric():\n-    model = create_sequential_model()\n-    model.compile(loss=loss, optimizer='rmsprop',\n-                  metrics=['acc'], weighted_metrics=['acc'])\n-\n-    (x_train, y_train), _, (sample_weight, _, _) = _get_test_data()\n-\n-    history = model.fit(x_train, y_train, batch_size=batch_size,\n-                        epochs=epochs // 3, verbose=0,\n-                        sample_weight=sample_weight)\n-\n-    assert history.history['acc'] != history.history['weighted_acc']\n-\n-\n-def test_weighted_metrics_with_multiple_outputs():\n-    decimal = decimal_precision[K.backend()]\n-\n-    inputs = Input(shape=(5,))\n-    x = Dense(5)(inputs)\n-    output1 = Dense(1, name='output1')(x)\n-    output2 = Dense(1, name='output2')(x)\n-\n-    model = Model(inputs=inputs, outputs=[output1, output2])\n-\n-    metrics = {'output1': [loss], 'output2': [loss]}\n-    weighted_metrics = {'output2': [loss]}\n-    loss_map = {'output1': loss, 'output2': loss}\n-\n-    model.compile(loss=loss_map, optimizer='sgd',\n-                  metrics=metrics, weighted_metrics=weighted_metrics)\n-\n-    x = np.array([[1, 1, 1, 1, 1]])\n-    y = {'output1': np.array([0]), 'output2': np.array([1])}\n-    weight = 5\n-\n-    history = model.fit(x, y, sample_weight={'output2': np.array([weight])})\n-\n-    unweighted_metric = history.history['output2_' + loss_full_name][0]\n-    weighted_metric = history.history['output2_weighted_' + loss_full_name][0]\n-\n-    assert_almost_equal(unweighted_metric * weight, weighted_metric, decimal=decimal)\n-\n-\n def test_class_weight_wrong_classes():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
