{"custom_id": "keras#71c6c83e301e5bc3339df67bb36b973ea3c82e87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 207 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,6 @@ import numpy as np\n np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n-from keras.optimizers import RMSprop\n from keras.models import Sequential\n from keras.layers.core import Dense, Dropout, Activation, Flatten\n from keras.layers.embeddings import Embedding\n@@ -17,7 +16,7 @@ from keras.datasets import imdb\n \n     Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_cnn.py\n \n-    Get to 0.8330 test accuracy after 3 epochs. 100s/epoch on K520 GPU.\n+    Get to 0.835 test accuracy after 2 epochs. 100s/epoch on K520 GPU.\n '''\n \n # set parameters:\n@@ -28,7 +27,7 @@ embedding_dims = 100\n nb_filter = 250\n filter_length = 3\n hidden_dims = 250\n-nb_epoch = 3\n+nb_epoch = 2\n \n print(\"Loading data...\")\n (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n@@ -60,7 +59,8 @@ model.add(Convolution1D(nb_filter=nb_filter,\n # we use standard max pooling (halving the output of the previous layer):\n model.add(MaxPooling1D(pool_length=2))\n \n-# We flatten the output of the conv layer, so that we can add a vanilla dense layer:\n+# We flatten the output of the conv layer,\n+# so that we can add a vanilla dense layer:\n model.add(Flatten())\n \n # We add a vanilla hidden layer:\n@@ -72,5 +72,9 @@ model.add(Activation('relu'))\n model.add(Dense(1))\n model.add(Activation('sigmoid'))\n \n-model.compile(loss='binary_crossentropy', optimizer='rmsprop', class_mode=\"binary\")\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, validation_data=(X_test, y_test))\n+model.compile(loss='binary_crossentropy',\n+              optimizer='rmsprop',\n+              class_mode=\"binary\")\n+model.fit(X_train, y_train, batch_size=batch_size,\n+          nb_epoch=nb_epoch, show_accuracy=True,\n+          validation_data=(X_test, y_test))\n\n@@ -4,12 +4,11 @@ import numpy as np\n np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n-from keras.optimizers import SGD, RMSprop, Adagrad\n from keras.utils import np_utils\n from keras.models import Sequential\n from keras.layers.core import Dense, Dropout, Activation\n from keras.layers.embeddings import Embedding\n-from keras.layers.recurrent import LSTM, GRU\n+from keras.layers.recurrent import LSTM\n from keras.datasets import imdb\n \n '''\n@@ -36,7 +35,8 @@ maxlen = 100  # cut texts after this number of words (among top max_features mos\n batch_size = 32\n \n print(\"Loading data...\")\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n+                                                      test_split=0.2)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n@@ -55,10 +55,15 @@ model.add(Dense(1))\n model.add(Activation('sigmoid'))\n \n # try using different optimizers and different optimizer configs\n-model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=\"binary\")\n+model.compile(loss='binary_crossentropy',\n+              optimizer='adam',\n+              class_mode=\"binary\")\n \n print(\"Train...\")\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=4, validation_data=(X_test, y_test), show_accuracy=True)\n-score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3,\n+          validation_data=(X_test, y_test), show_accuracy=True)\n+score, acc = model.evaluate(X_test, y_test,\n+                            batch_size=batch_size,\n+                            show_accuracy=True)\n print('Test score:', score)\n print('Test accuracy:', acc)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#69a8acc05a40a523aa4e51b2e281b94a2e8bdc43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 12 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 52 | Churn Cumulative: 1444 | Contributors (this commit): 1 | Commits (past 90d): 21 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,35 @@\n from __future__ import absolute_import\n+from __future__ import print_function\n+import os\n+import json\n from .common import epsilon, floatx, set_epsilon, set_floatx\n \n _BACKEND = 'theano'\n+_config_path = os.path.expanduser(os.path.join('~', '.keras', 'keras.json'))\n+if os.path.exists(_config_path):\n+    _config = json.load(open(_config_path))\n+    _floatx = _config.get('floatx', floatx())\n+    assert _floatx in {'float32', 'float64'}\n+    _epsilon = _config.get('epsilon', epsilon())\n+    assert type(_epsilon) == float\n+    _backend = _config.get('backend', _BACKEND)\n+    assert _backend in {'theano', 'tensorflow'}\n+\n+    set_floatx(_floatx)\n+    set_epsilon(_epsilon)\n+    _BACKEND = _backend\n+else:\n+    # save config file, for easy edition\n+    _config = {'floatx': floatx(),\n+               'epsilon': epsilon(),\n+               'backend': _BACKEND}\n+    json.dump(_config, open(_config_path, 'w'))\n \n if _BACKEND == 'theano':\n+    print('Using Theano backend.')\n     from .theano_backend import *\n elif _BACKEND == 'tensorflow':\n+    print('Using TensorFlow backend.')\n     from .tensorflow_backend import *\n else:\n     raise Exception('Unknown backend: ' + str(backend))\n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n # the type of float to use throughout the session.\n-_FLOATX = 'float64'\n+_FLOATX = 'float32'\n _EPSILON = 10e-8\n \n \n\n@@ -282,12 +282,17 @@ def temporal_padding(x, padding=1):\n     return tf.pad(x, pattern)\n \n \n-def spatial_2d_padding(x, padding=(1, 1)):\n+def spatial_2d_padding(x, padding=(1, 1), dim_ordering='th'):\n     '''Pad the 2nd and 3rd dimensions of a 4D tensor\n     with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n     '''\n+    if dim_ordering == 'th':\n         pattern = [[0, 0], [0, 0],\n                    [padding[0], padding[0]], [padding[1], padding[1]]]\n+    else:\n+        pattern = [[0, 0],\n+                   [padding[0], padding[0]], [padding[1], padding[1]],\n+                   [0, 0]]\n     return tf.pad(x, pattern)\n \n \n@@ -380,6 +385,9 @@ def rnn(step_function, inputs, initial_states,\n     for input in input_list:\n         output, new_states = step_function(input, states)\n         if masking:\n+            # for now we raise an exception because tf.reduce_any will not work\n+            raise Exception(\"Masking is Theano-only for the time being.\")\n+\n             # if all-zero input timestep, return\n             # all-zero output and unchanged states\n             switch = tf.reduce_any(input)\n\n@@ -301,11 +301,12 @@ def temporal_padding(x, padding=1):\n     return T.set_subtensor(output[:, padding:x.shape[1] + padding, :], x)\n \n \n-def spatial_2d_padding(x, padding=(1, 1)):\n+def spatial_2d_padding(x, padding=(1, 1), dim_ordering='th'):\n     '''Pad the 2nd and 3rd dimensions of a 4D tensor\n     with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n     '''\n     input_shape = x.shape\n+    if dim_ordering == 'th':\n         output_shape = (input_shape[0],\n                         input_shape[1],\n                         input_shape[2] + 2 * padding[0],\n@@ -315,6 +316,19 @@ def spatial_2d_padding(x, padding=(1, 1)):\n                    slice(None),\n                    slice(padding[0], input_shape[2] + padding[0]),\n                    slice(padding[1], input_shape[3] + padding[1]))\n+\n+    elif dim_ordering == 'tf':\n+        output_shape = (input_shape[0],\n+                        input_shape[1] + 2 * padding[0],\n+                        input_shape[2] + 2 * padding[1],\n+                        input_shape[3])\n+        output = T.zeros(output_shape)\n+        indices = (slice(None),\n+                   slice(padding[0], input_shape[1] + padding[0]),\n+                   slice(padding[1], input_shape[2] + padding[1]),\n+                   slice(None))\n+    else:\n+        raise Exception('Invalid dim_ordering: ' + dim_ordering)\n     return T.set_subtensor(output[indices], x)\n \n # VALUE MANIPULATION\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c458ff28103580bbc3ba3d1acb38dc8e7eb12b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 185 | Files Changed: 6 | Hunks: 48 | Methods Changed: 21 | Complexity Δ (Sum/Max): 122/116 | Churn Δ: 296 | Churn Cumulative: 10245 | Contributors (this commit): 45 | Commits (past 90d): 144 | Contributors (cumulative): 73 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,15 +1,18 @@\n from __future__ import absolute_import\n from __future__ import print_function\n \n-import tarfile, inspect, os\n+import tarfile\n+import os\n from six.moves.urllib.request import FancyURLopener\n \n from ..utils.generic_utils import Progbar\n \n+\n class ParanoidURLopener(FancyURLopener):\n     def http_error_default(self, url, fp, errcode, errmsg, headers):\n         raise Exception('URL fetch failure on {}: {} -- {}'.format(url, errcode, errmsg))\n \n+\n def get_file(fname, origin, untar=False):\n     datadir = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n     if not os.path.exists(datadir):\n@@ -21,12 +24,8 @@ def get_file(fname, origin, untar=False):\n     else:\n         fpath = os.path.join(datadir, fname)\n \n-    try:\n-        f = open(fpath)\n-    except:\n+    if not os.path.exists(fpath):\n         print('Downloading data from',  origin)\n-\n-        global progbar\n         progbar = None\n \n         def dl_progress(count, block_size, total_size):\n\n@@ -61,7 +61,9 @@ class ELU(MaskedLayer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        return ((X + abs(X)) / 2.0) + self.alpha * (T.exp((X - abs(X)) / 2.0) - 1)\n+        pos = K.relu(X)\n+        neg = (X - abs(X)) * 0.5\n+        return pos + self.alpha * (K.exp(neg) - 1.)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n\n@@ -24,7 +24,8 @@ class Convolution1D(Layer):\n                  init='uniform', activation='linear', weights=None,\n                  border_mode='valid', subsample_length=1,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, input_dim=None, input_length=None, **kwargs):\n+                 W_constraint=None, b_constraint=None,\n+                 input_dim=None, input_length=None, **kwargs):\n \n         if border_mode not in {'valid', 'same'}:\n             raise Exception('Invalid border mode for Convolution1D:', border_mode)\n@@ -32,6 +33,7 @@ class Convolution1D(Layer):\n         self.filter_length = filter_length\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n+        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n         self.border_mode = border_mode\n         self.subsample_length = subsample_length\n \n@@ -80,7 +82,10 @@ class Convolution1D(Layer):\n \n     @property\n     def output_shape(self):\n-        length = conv_output_length(self.input_shape[1], self.filter_length, self.border_mode, self.subsample[0])\n+        length = conv_output_length(self.input_shape[1],\n+                                    self.filter_length,\n+                                    self.border_mode,\n+                                    self.subsample[0])\n         return (self.input_shape[0], length, self.nb_filter)\n \n     def get_output(self, train=False):\n@@ -131,8 +136,10 @@ class Convolution2D(Layer):\n         self.nb_col = nb_col\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n+        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n         self.border_mode = border_mode\n         self.subsample = tuple(subsample)\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n@@ -148,8 +155,14 @@ class Convolution2D(Layer):\n         super(Convolution2D, self).__init__(**kwargs)\n \n     def build(self):\n+        if self.dim_ordering == 'th':\n             stack_size = self.input_shape[1]\n             self.W_shape = (self.nb_filter, stack_size, self.nb_row, self.nb_col)\n+        elif self.dim_ordering == 'tf':\n+            stack_size = self.input_shape[3]\n+            self.W_shape = (self.nb_row, self.nb_col, stack_size, self.nb_filter)\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n         self.W = self.init(self.W_shape)\n         self.b = K.zeros((self.nb_filter,))\n         self.params = [self.W, self.b]\n@@ -174,11 +187,26 @@ class Convolution2D(Layer):\n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n+        if self.dim_ordering == 'th':\n             rows = input_shape[2]\n             cols = input_shape[3]\n-        rows = conv_output_length(rows, self.nb_row, self.border_mode, self.subsample[0])\n-        cols = conv_output_length(cols, self.nb_col, self.border_mode, self.subsample[1])\n+        elif self.dim_ordering == 'tf':\n+            rows = input_shape[1]\n+            cols = input_shape[2]\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+        rows = conv_output_length(rows, self.nb_row,\n+                                  self.border_mode, self.subsample[0])\n+        cols = conv_output_length(cols, self.nb_col,\n+                                  self.border_mode, self.subsample[1])\n+\n+        if self.dim_ordering == 'th':\n             return (input_shape[0], self.nb_filter, rows, cols)\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0], rows, cols, self.nb_filter)\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n@@ -220,9 +248,9 @@ class MaxPooling1D(Layer):\n         self.pool_length = pool_length\n         self.stride = stride\n         self.st = (self.stride, 1)\n-\n         self.input = K.placeholder(ndim=3)\n         self.pool_size = (pool_length, 1)\n+        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n         self.border_mode = border_mode\n \n     @property\n@@ -262,17 +290,34 @@ class MaxPooling2D(Layer):\n         if strides is None:\n             strides = self.pool_size\n         self.strides = tuple(strides)\n+        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n         self.border_mode = border_mode\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n \n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n-        rows = conv_output_length(input_shape[2], self.pool_size[0],\n+        if self.dim_ordering == 'th':\n+            rows = input_shape[2]\n+            cols = input_shape[3]\n+        elif self.dim_ordering == 'tf':\n+            rows = input_shape[1]\n+            cols = input_shape[2]\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+        rows = conv_output_length(rows, self.pool_size[0],\n                                   self.border_mode, self.strides[0])\n-        cols = conv_output_length(input_shape[3], self.pool_size[1],\n+        cols = conv_output_length(cols, self.pool_size[1],\n                                   self.border_mode, self.strides[1])\n+\n+        if self.dim_ordering == 'th':\n             return (input_shape[0], input_shape[1], rows, cols)\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0], rows, cols, input_shape[3])\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n@@ -292,11 +337,11 @@ class MaxPooling2D(Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class UpSample1D(Layer):\n+class UpSampling1D(Layer):\n     input_ndim = 3\n \n     def __init__(self, length=2, **kwargs):\n-        super(UpSample1D, self).__init__(**kwargs)\n+        super(UpSampling1D, self).__init__(**kwargs)\n         self.length = length\n         self.input = K.placeholder(ndim=3)\n \n@@ -313,35 +358,52 @@ class UpSample1D(Layer):\n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n                   \"length\": self.length}\n-        base_config = super(UpSample1D, self).get_config()\n+        base_config = super(UpSampling1D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class UpSample2D(Layer):\n+class UpSampling2D(Layer):\n     input_ndim = 4\n \n-    def __init__(self, size=(2, 2), **kwargs):\n-        super(UpSample2D, self).__init__(**kwargs)\n+    def __init__(self, size=(2, 2), dim_ordering='th', **kwargs):\n+        super(UpSampling2D, self).__init__(**kwargs)\n         self.input = K.placeholder(ndim=4)\n         self.size = tuple(size)\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n \n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n-        return (input_shape[0], input_shape[1],\n+        if self.dim_ordering == 'th':\n+            return (input_shape[0],\n+                    input_shape[1],\n                     self.size[0] * input_shape[2],\n                     self.size[1] * input_shape[3])\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0],\n+                    self.size[0] * input_shape[1],\n+                    self.size[1] * input_shape[2],\n+                    input_shape[3])\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n+        if self.dim_ordering == 'th':\n             output = K.concatenate([X] * self.size[0], axis=2)\n             output = K.concatenate([output] * self.size[1], axis=3)\n+        elif self.dim_ordering == 'tf':\n+            output = K.concatenate([X] * self.size[0], axis=1)\n+            output = K.concatenate([output] * self.size[1], axis=2)\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n         return output\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n                   \"size\": self.size}\n-        base_config = super(UpSample2D, self).get_config()\n+        base_config = super(UpSampling2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -408,22 +470,33 @@ class ZeroPadding2D(Layer):\n     \"\"\"\n     input_ndim = 4\n \n-    def __init__(self, padding=(1, 1), **kwargs):\n+    def __init__(self, padding=(1, 1), dim_ordering='th', **kwargs):\n         super(ZeroPadding2D, self).__init__(**kwargs)\n         self.padding = tuple(padding)\n         self.input = K.placeholder(ndim=4)\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n \n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n+        if self.dim_ordering == 'th':\n             return (input_shape[0],\n                     input_shape[1],\n                     input_shape[2] + 2 * self.padding[0],\n                     input_shape[3] + 2 * self.padding[1])\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0],\n+                    input_shape[1] + 2 * self.padding[0],\n+                    input_shape[2] + 2 * self.padding[1],\n+                    input_shape[3])\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        return K.spatial_2d_padding(X, padding=self.padding)\n+        return K.spatial_2d_padding(X, padding=self.padding,\n+                                    dim_ordering=self.dim_ordering)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n\n@@ -220,6 +220,8 @@ class Masking(MaskedLayer):\n         self.input = K.placeholder(ndim=3)\n \n     def get_output_mask(self, train=False):\n+        if K._BACKEND == \"tensorflow\":\n+            raise Exception(\"Masking is Theano-only for the time being.\")\n         X = self.get_input(train)\n         return K.any(K.ones_like(X) * (1. - K.equal(X, self.mask_value)),\n                      axis=-1)\n@@ -976,7 +978,6 @@ class MaxoutDense(Layer):\n \n \n class Lambda(Layer):\n-\n     \"\"\"Lambda layer for evaluating arbitrary function\n \n     Input shape\n@@ -1156,7 +1157,6 @@ class LambdaMerge(Lambda):\n \n \n class Siamese(Layer):\n-\n     '''Shared layer with multiple inputs\n \n     Output shape\n@@ -1171,7 +1171,6 @@ class Siamese(Layer):\n     concat_axis - Similar to concat_axis argument of Merge layer\n     dot_axes - Similar to dot_axes argument of Merge layer\n     '''\n-\n     def __init__(self, layer, inputs, merge_mode='concat',\n                  concat_axis=1, dot_axes=-1):\n         if merge_mode not in ['sum', 'mul', 'concat', 'ave',\n\n@@ -58,6 +58,8 @@ class Embedding(Layer):\n         if not self.mask_zero:\n             return None\n         else:\n+            if K._BACKEND == \"tensorflow\":\n+                raise Exception(\"Masking is Theano-only for the time being.\")\n             return K.ones_like(X) * (1 - K.equal(X, 0))\n \n     @property\n\n@@ -11,9 +11,8 @@ class Recurrent(MaskedLayer):\n     input_ndim = 3\n \n     def __init__(self, weights=None,\n-                 return_sequences=False, input_dim=None,\n-                 input_length=None, go_backwards=False,\n-                 stateful=False, **kwargs):\n+                 return_sequences=False, go_backwards=False, stateful=False,\n+                 input_dim=None, input_length=None, **kwargs):\n         self.return_sequences = return_sequences\n         self.initial_weights = weights\n         self.go_backwards = go_backwards\n@@ -46,6 +45,13 @@ class Recurrent(MaskedLayer):\n         # input shape: (nb_samples, time (padded with zeros), input_dim)\n         X = self.get_input(train)\n         assert K.ndim(X) == 3\n+        if K._BACKEND == 'tensorflow':\n+            if not self.input_shape[1]:\n+                raise Exception('When using TensorFlow, you should define ' +\n+                                'explicitely the number of timesteps of ' +\n+                                'your sequences. Make sure the first layer ' +\n+                                'has an \"input_shape\" argument with a defined ' +\n+                                'first dimension.')\n \n         mask = self.get_output_mask(train)\n         if mask:\n@@ -346,158 +352,3 @@ class LSTM(Recurrent):\n                   \"inner_activation\": self.inner_activation.__name__}\n         base_config = super(LSTM, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class JZS(Recurrent):\n-    '''\n-        Evolved recurrent neural network architectures\n-        from the evaluation of thousands\n-        of models, serving as alternatives to LSTMs and GRUs.\n-        See Jozefowicz et al. 2015.\n-        References:\n-            An Empirical Exploration of Recurrent Network Architectures\n-                http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n-    '''\n-    def __init__(self, output_dim,\n-                 init='glorot_uniform', inner_init='orthogonal',\n-                 activation='tanh', inner_activation='sigmoid',\n-                 **kwargs):\n-        self.output_dim = output_dim\n-        self.init = initializations.get(init)\n-        self.inner_init = initializations.get(inner_init)\n-        self.activation = activations.get(activation)\n-        self.inner_activation = activations.get(inner_activation)\n-        super(JZS, self).__init__(**kwargs)\n-\n-    def build(self):\n-        input_shape = self.input_shape\n-        input_dim = input_shape[2]\n-        self.input_dim = input_dim\n-        self.input = K.placeholder(input_shape)\n-\n-        if self.stateful:\n-            if not input_shape[0]:\n-                raise Exception('If a RNN is stateful, a complete ' +\n-                                'input_shape must be provided ' +\n-                                '(including batch size).')\n-            self.states = [K.zeros(input_shape[0], self.output_dim)]\n-        else:\n-            # initial states: all-zero tensor of shape (output_dim)\n-            self.states = [None]\n-\n-        self.W_z = self.init((input_dim, self.output_dim))\n-        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_z = K.zeros((self.output_dim))\n-\n-        self.W_r = self.init((input_dim, self.output_dim))\n-        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_r = K.zeros((self.output_dim))\n-\n-        self.W_h = self.init((input_dim, self.output_dim))\n-        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_h = K.zeros((self.output_dim))\n-\n-        # P matrix used to project X onto different dimension,\n-        # using sparse random projections\n-        if input_dim == self.output_dim:\n-            self.P = K.variable(np.identity(self.output_dim))\n-        else:\n-            P = np.random.binomial(1, 0.5, size=(input_dim, self.output_dim))\n-            P = 1. / np.sqrt(input_dim) * (P * 2. - 1.)\n-            self.P = K.variable(P)\n-\n-        self.params = [self.W_z, self.b_z,\n-                       self.W_r, self.U_r, self.b_r,\n-                       self.U_h, self.b_h,\n-                       self.P]\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-\n-    def get_config(self):\n-        config = {\"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"inner_init\": self.inner_init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__}\n-        base_config = super(JZS, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class JZS1(JZS):\n-    '''\n-        Evolved recurrent neural network architectures\n-        from the evaluation of thousands\n-        of models, serving as alternatives to LSTMs and GRUs.\n-        See Jozefowicz et al. 2015.\n-        This corresponds to the `MUT1` architecture described in the paper.\n-    '''\n-    def __init__(self, *args, **kwargs):\n-        super(JZS1, self).__init__(*args, **kwargs)\n-\n-    def step(self, x, states):\n-        assert len(states) == 1\n-        h_tm1 = states[0]\n-\n-        x_z = K.dot(x, self.W_z) + self.b_z\n-        x_r = K.dot(x, self.P) + self.b_r\n-        x_h = K.dot(x, self.W_h) + self.b_h\n-\n-        z = self.inner_activation(x_z)\n-        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n-        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n-        h = hh * z + h_tm1 * (1. - z)\n-        return h, [h]\n-\n-\n-class JZS2(JZS):\n-    '''\n-        Evolved recurrent neural network architectures\n-        from the evaluation of thousands\n-        of models, serving as alternatives to LSTMs and GRUs.\n-        See Jozefowicz et al. 2015.\n-        This corresponds to the `MUT2` architecture described in the paper.\n-    '''\n-    def __init__(self, *args, **kwargs):\n-        super(JZS2, self).__init__(*args, **kwargs)\n-\n-    def step(self, x, states):\n-        assert len(states) == 1\n-        h_tm1 = states[0]\n-\n-        x_z = K.dot(x, self.W_z) + self.b_z\n-        x_r = K.dot(x, self.P) + self.b_r\n-        x_h = K.dot(x, self.W_h) + self.b_h\n-\n-        z = self.inner_activation(x_z + K.dot(h_tm1, self.U_z))\n-        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n-        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n-        h = hh * z + h_tm1 * (1. - z)\n-        return h, [h]\n-\n-\n-class JZS3(Recurrent):\n-    '''\n-        Evolved recurrent neural network architectures\n-        from the evaluation of thousands\n-        of models, serving as alternatives to LSTMs and GRUs.\n-        See Jozefowicz et al. 2015.\n-        This corresponds to the `MUT3` architecture described in the paper.\n-    '''\n-    def __init__(self, *args, **kwargs):\n-        super(JZS3, self).__init__(*args, **kwargs)\n-\n-    def step(self, x, states):\n-        assert len(states) == 1\n-        h_tm1 = states[0]\n-\n-        x_z = K.dot(x, self.W_z) + self.b_z\n-        x_r = K.dot(x, self.P) + self.b_r\n-        x_h = K.dot(x, self.W_h) + self.b_h\n-\n-        z = self.inner_activation(x_z + K.dot(K.tanh(h_tm1), self.U_z))\n-        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n-        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n-        h = hh * z + h_tm1 * (1. - z)\n-        return h, [h]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e32436144be933486182db6baee76c8746810488", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2368 | Lines Deleted: 0 | Files Changed: 17 | Hunks: 17 | Methods Changed: 132 | Complexity Δ (Sum/Max): 222/40 | Churn Δ: 2368 | Churn Cumulative: 2368 | Contributors (this commit): 1 | Commits (past 90d): 17 | Contributors (cumulative): 17 | DMM Complexity: 0.9243002544529262\n\nDIFF:\n@@ -0,0 +1,334 @@\n+import sys\n+import unittest\n+from numpy.testing import assert_allclose\n+import numpy as np\n+import pytest\n+\n+from keras.backend import theano_backend as KTH\n+from keras.backend import tensorflow_backend as KTF\n+\n+\n+def check_single_tensor_operation(function_name, input_shape, **kwargs):\n+    val = np.random.random(input_shape) - 0.5\n+    xth = KTH.variable(val)\n+    xtf = KTF.variable(val)\n+\n+    zth = KTH.eval(getattr(KTH, function_name)(xth, **kwargs))\n+    ztf = KTF.eval(getattr(KTF, function_name)(xtf, **kwargs))\n+\n+    assert zth.shape == ztf.shape\n+    assert_allclose(zth, ztf, atol=1e-06)\n+\n+\n+def check_two_tensor_operation(function_name, x_input_shape,\n+                               y_input_shape, **kwargs):\n+    xval = np.random.random(x_input_shape) - 0.5\n+    xth = KTH.variable(xval)\n+    xtf = KTF.variable(xval)\n+\n+    yval = np.random.random(y_input_shape) - 0.5\n+    yth = KTH.variable(yval)\n+    ytf = KTF.variable(yval)\n+\n+    zth = KTH.eval(getattr(KTH, function_name)(xth, yth, **kwargs))\n+    ztf = KTF.eval(getattr(KTF, function_name)(xtf, ytf, **kwargs))\n+\n+    assert zth.shape == ztf.shape\n+    assert_allclose(zth, ztf, atol=1e-06)\n+\n+\n+@pytest.mark.skipif(sys.version_info < (2, 7), reason=\"Requires Python 2.7\")\n+class TestBackend(unittest.TestCase):\n+\n+    def test_linear_operations(self):\n+        check_two_tensor_operation('dot', (4, 2), (2, 4))\n+        check_single_tensor_operation('transpose', (4, 2))\n+\n+    def test_shape_operations(self):\n+        # concatenate\n+        xval = np.random.random((4, 3))\n+        xth = KTH.variable(xval)\n+        xtf = KTF.variable(xval)\n+        yval = np.random.random((4, 2))\n+        yth = KTH.variable(yval)\n+        ytf = KTF.variable(yval)\n+        zth = KTH.eval(KTH.concatenate([xth, yth], axis=-1))\n+        ztf = KTF.eval(KTF.concatenate([xtf, ytf], axis=-1))\n+        assert zth.shape == ztf.shape\n+        assert_allclose(zth, ztf, atol=1e-06)\n+\n+        check_single_tensor_operation('reshape', (4, 2), shape=(8, 1))\n+        check_single_tensor_operation('permute_dimensions', (4, 2, 3),\n+                                      pattern=(2, 0, 1))\n+        check_single_tensor_operation('repeat', (4, 1), n=3)\n+        check_single_tensor_operation('flatten', (4, 1))\n+        check_single_tensor_operation('expand_dims', (4, 3), dim=-1)\n+        check_single_tensor_operation('expand_dims', (4, 3, 2), dim=1)\n+        check_single_tensor_operation('squeeze', (4, 3, 1), axis=2)\n+\n+    def test_value_manipulation(self):\n+        val = np.random.random((4, 2))\n+        xth = KTH.variable(val)\n+        xtf = KTF.variable(val)\n+\n+        # get_value\n+        valth = KTH.get_value(xth)\n+        valtf = KTF.get_value(xtf)\n+        assert valtf.shape == valth.shape\n+        assert_allclose(valth, valtf, atol=1e-06)\n+\n+        # set_value\n+        val = np.random.random((4, 2))\n+        KTH.set_value(xth, val)\n+        KTF.set_value(xtf, val)\n+\n+        valth = KTH.get_value(xth)\n+        valtf = KTF.get_value(xtf)\n+        assert valtf.shape == valth.shape\n+        assert_allclose(valth, valtf, atol=1e-06)\n+\n+        # count_params\n+        assert KTH.count_params(xth) == KTF.count_params(xtf)\n+\n+    def test_elementwise_operations(self):\n+        check_single_tensor_operation('max', (4, 2))\n+        check_single_tensor_operation('max', (4, 2), axis=1, keepdims=True)\n+\n+        check_single_tensor_operation('min', (4, 2))\n+        check_single_tensor_operation('min', (4, 2), axis=1, keepdims=True)\n+\n+        check_single_tensor_operation('mean', (4, 2))\n+        check_single_tensor_operation('mean', (4, 2), axis=1, keepdims=True)\n+        check_single_tensor_operation('mean', (4, 2, 3), axis=-1, keepdims=True)\n+\n+        check_single_tensor_operation('std', (4, 2))\n+        check_single_tensor_operation('std', (4, 2), axis=1, keepdims=True)\n+\n+        check_single_tensor_operation('prod', (4, 2))\n+        check_single_tensor_operation('prod', (4, 2), axis=1, keepdims=True)\n+\n+        # does not work yet, wait for bool <-> int casting in TF (coming soon)\n+        # check_single_tensor_operation('any', (4, 2))\n+        # check_single_tensor_operation('any', (4, 2), axis=1, keepdims=True)\n+\n+        check_single_tensor_operation('argmax', (4, 2))\n+        check_single_tensor_operation('argmax', (4, 2), axis=1)\n+\n+        check_single_tensor_operation('argmin', (4, 2))\n+        check_single_tensor_operation('argmin', (4, 2), axis=1)\n+\n+        check_single_tensor_operation('square', (4, 2))\n+        check_single_tensor_operation('abs', (4, 2))\n+        check_single_tensor_operation('sqrt', (4, 2))\n+        check_single_tensor_operation('exp', (4, 2))\n+        check_single_tensor_operation('log', (4, 2))\n+        check_single_tensor_operation('round', (4, 2))\n+        check_single_tensor_operation('pow', (4, 2), a=3)\n+        check_single_tensor_operation('clip', (4, 2), min_value=0.4,\n+                                      max_value=0.6)\n+\n+        # two-tensor ops\n+        check_two_tensor_operation('equal', (4, 2), (4, 2))\n+        check_two_tensor_operation('maximum', (4, 2), (4, 2))\n+        check_two_tensor_operation('minimum', (4, 2), (4, 2))\n+\n+    def test_gradient(self):\n+        val = np.random.random((4, 2))\n+        xth = KTH.variable(val)\n+        xtf = KTF.variable(val)\n+\n+        expth = xth * KTH.exp(xth)\n+        exptf = xtf * KTF.exp(xtf)\n+        lossth = KTH.sum(expth)\n+        losstf = KTF.sum(exptf)\n+\n+        gradth = KTH.gradients(lossth, [expth])\n+        gradtf = KTF.gradients(losstf, [exptf])\n+\n+        zth = KTH.eval(gradth[0])\n+        ztf = KTF.eval(gradtf[0])\n+        assert zth.shape == ztf.shape\n+        assert_allclose(zth, ztf, atol=1e-06)\n+\n+    def test_function(self):\n+        val = np.random.random((4, 2))\n+        input_val = np.random.random((4, 2))\n+\n+        xth = KTH.variable(val)\n+        xtf = KTF.variable(val)\n+        yth = KTH.placeholder(ndim=2)\n+        ytf = KTF.placeholder(ndim=2)\n+\n+        exp_th = KTH.square(xth) + yth\n+        exp_tf = KTF.square(xtf) + ytf\n+\n+        update_th = xth * 2\n+        update_tf = xtf * 2\n+        fth = KTH.function([yth], [exp_th], updates=[(xth, update_th)])\n+        ftf = KTF.function([ytf], [exp_tf], updates=[(xtf, update_tf)])\n+\n+        function_outputs_th = fth([input_val])[0]\n+        function_outputs_tf = ftf([input_val])[0]\n+        assert function_outputs_th.shape == function_outputs_tf.shape\n+        assert_allclose(function_outputs_th, function_outputs_tf, atol=1e-06)\n+\n+        new_val_th = KTH.get_value(xth)\n+        new_val_tf = KTF.get_value(xtf)\n+        assert new_val_th.shape == new_val_tf.shape\n+        assert_allclose(new_val_th, new_val_tf, atol=1e-06)\n+\n+    def test_rnn(self):\n+        # implement a simple RNN\n+        input_dim = 8\n+        output_dim = 4\n+        timesteps = 5\n+\n+        input_val = np.random.random((32, timesteps, input_dim))\n+        init_state_val = np.random.random((32, output_dim))\n+        W_i_val = np.random.random((input_dim, output_dim))\n+        W_o_val = np.random.random((output_dim, output_dim))\n+\n+        def rnn_step_fn(input_dim, output_dim, K):\n+            W_i = K.variable(W_i_val)\n+            W_o = K.variable(W_o_val)\n+\n+            def step_function(x, states):\n+                assert len(states) == 1\n+                prev_output = states[0]\n+                output = K.dot(x, W_i) + K.dot(prev_output, W_o)\n+                return output, [output]\n+            return step_function\n+\n+        th_rnn_step_fn = rnn_step_fn(input_dim, output_dim, KTH)\n+        inputs = KTH.variable(input_val)\n+        initial_states = [KTH.variable(init_state_val)]\n+        last_output, outputs, new_states = KTH.rnn(th_rnn_step_fn, inputs,\n+                                                   initial_states,\n+                                                   go_backwards=False,\n+                                                   masking=False)\n+        th_last_output = KTH.eval(last_output)\n+        th_outputs = KTH.eval(outputs)\n+        assert len(new_states) == 1\n+        th_state = KTH.eval(new_states[0])\n+\n+        tf_rnn_step_fn = rnn_step_fn(input_dim, output_dim, KTF)\n+        inputs = KTF.variable(input_val)\n+        initial_states = [KTF.variable(init_state_val)]\n+        last_output, outputs, new_states = KTF.rnn(tf_rnn_step_fn, inputs,\n+                                                   initial_states,\n+                                                   go_backwards=False,\n+                                                   masking=False)\n+        tf_last_output = KTF.eval(last_output)\n+        tf_outputs = KTF.eval(outputs)\n+        assert len(new_states) == 1\n+        tf_state = KTF.eval(new_states[0])\n+\n+        assert_allclose(tf_last_output, th_last_output, atol=1e-06)\n+        assert_allclose(tf_outputs, th_outputs, atol=1e-06)\n+        assert_allclose(tf_state, th_state, atol=1e-06)\n+\n+    def test_switch(self):\n+        val = np.random.random()\n+        xth = KTH.variable(val)\n+        xth = KTH.switch(xth >= 0.5, xth * 0.1, xth * 0.2)\n+\n+        xtf = KTF.variable(val)\n+        xtf = KTF.switch(xtf >= 0.5, xtf * 0.1, xtf * 0.2)\n+\n+        zth = KTH.eval(xth)\n+        ztf = KTF.eval(xtf)\n+\n+        assert zth.shape == ztf.shape\n+        assert_allclose(zth, ztf, atol=1e-06)\n+\n+    def test_nn_operations(self):\n+        check_single_tensor_operation('relu', (4, 2), alpha=0.1, max_value=0.5)\n+        check_single_tensor_operation('softmax', (4, 10))\n+        check_single_tensor_operation('softplus', (4, 10))\n+\n+        check_single_tensor_operation('sigmoid', (4, 2))\n+        check_single_tensor_operation('hard_sigmoid', (4, 2))\n+        check_single_tensor_operation('tanh', (4, 2))\n+\n+        # dropout\n+        val = np.random.random((20, 20))\n+        xth = KTH.variable(val)\n+        xtf = KTF.variable(val)\n+        zth = KTH.eval(KTH.dropout(xth, level=0.2))\n+        ztf = KTF.eval(KTF.dropout(xtf, level=0.2))\n+        assert zth.shape == ztf.shape\n+        # dropout patterns are different, only check mean\n+        assert np.abs(zth.mean() - ztf.mean()) < 0.05\n+\n+        check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), from_logits=True)\n+        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), from_logits=True)\n+        check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), from_logits=False)\n+\n+        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), from_logits=False)\n+\n+    # def test_conv2d(self):\n+    #     '''conv2d works \"properly\" with Theano and TF but outputs different\n+    #     values in each case. Cause unclear (input / kernel shape format?)\n+    #     '''\n+    #     # TH kernel shape: (depth, input_depth, rows, cols)\n+    #     check_two_tensor_operation('conv2d', (5, 3, 10, 12), (4, 3, 2, 2),\n+    #                                strides=(1, 1), border_mode='valid')\n+    #     check_two_tensor_operation('conv2d', (5, 3, 10, 12), (4, 3, 2, 2),\n+    #                                strides=(1, 1), border_mode='same')\n+\n+    #     # TF kernel shape: (rows, cols, input_depth, depth)\n+    #     check_two_tensor_operation('conv2d', (5, 10, 12, 3), (2, 2, 3, 4),\n+    #                                strides=(1, 1), border_mode='valid', dim_ordering='tf')\n+    #     check_two_tensor_operation('conv2d', (5, 10, 12, 3), (2, 2, 3, 4),\n+    #                                strides=(1, 1), border_mode='same', dim_ordering='tf')\n+\n+    #     check_two_tensor_operation('conv2d', (5, 3, 10, 12), (4, 3, 3, 3),\n+    #                                strides=(1, 1), border_mode='valid')\n+    #     check_two_tensor_operation('conv2d', (5, 3, 10, 12), (4, 3, 3, 3),\n+    #                                strides=(1, 1), border_mode='same')\n+\n+    #     check_two_tensor_operation('conv2d', (5, 3, 10, 12), (4, 3, 3, 3),\n+    #                                strides=(2, 2), border_mode='valid')\n+\n+    # def test_maxpool2d(self):\n+    #     '''maxpool2d works \"properly\" with Theano and TF but outputs different\n+    #     values in each case. Cause unclear (input shape format?)\n+    #     '''\n+    #     check_single_tensor_operation('maxpool2d', (5, 3, 10, 12), pool_size=(2, 2),\n+    #                                   strides=(1, 1), border_mode='valid')\n+\n+    #     check_single_tensor_operation('maxpool2d', (5, 3, 9, 11), pool_size=(2, 2),\n+    #                                   strides=(1, 1), border_mode='valid')\n+\n+    #     check_single_tensor_operation('maxpool2d', (5, 3, 9, 11), pool_size=(2, 3),\n+    #                                   strides=(1, 1), border_mode='valid')\n+\n+    def test_random_normal(self):\n+        mean = 0.\n+        std = 1.\n+        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n+        assert(rand.shape == (1000, 1000))\n+        assert(np.abs(np.mean(rand) - mean) < 0.01)\n+        assert(np.abs(np.std(rand) - std) < 0.01)\n+\n+        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n+        assert(rand.shape == (1000, 1000))\n+        assert(np.abs(np.mean(rand) - mean) < 0.01)\n+        assert(np.abs(np.std(rand) - std) < 0.01)\n+\n+    def test_random_uniform(self):\n+        mean = 0.\n+        std = 1.\n+        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n+        assert(rand.shape == (1000, 1000))\n+        assert(np.abs(np.mean(rand) - mean) < 0.01)\n+        assert(np.abs(np.std(rand) - std) < 0.01)\n+\n+        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n+        assert(rand.shape == (1000, 1000))\n+        assert(np.abs(np.mean(rand) - mean) < 0.01)\n+        assert(np.abs(np.std(rand) - std) < 0.01)\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n@@ -0,0 +1,166 @@\n+import unittest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+\n+from keras import backend as K\n+from keras.layers import convolutional\n+\n+\n+class TestConvolutions(unittest.TestCase):\n+    def test_convolution_1d(self):\n+        nb_samples = 9\n+        nb_steps = 7\n+        input_dim = 10\n+        filter_length = 6\n+        nb_filter = 5\n+\n+        weights_in = [np.ones((nb_filter, input_dim, filter_length, 1)),\n+                      np.ones(nb_filter)]\n+\n+        input = np.ones((nb_samples, nb_steps, input_dim))\n+        for weight in [None, weights_in]:\n+            for border_mode in ['valid', 'same']:\n+                for subsample_length in [1]:\n+                    if border_mode == 'same' and subsample_length != 1:\n+                        continue\n+                    for W_regularizer in [None, 'l2']:\n+                        for b_regularizer in [None, 'l2']:\n+                            for act_regularizer in [None, 'l2']:\n+                                layer = convolutional.Convolution1D(\n+                                    nb_filter, filter_length,\n+                                    weights=weight,\n+                                    border_mode=border_mode,\n+                                    W_regularizer=W_regularizer,\n+                                    b_regularizer=b_regularizer,\n+                                    activity_regularizer=act_regularizer,\n+                                    subsample_length=subsample_length,\n+                                    input_shape=(None, input_dim))\n+\n+                            layer.input = K.variable(input)\n+                            for train in [True, False]:\n+                                out = K.eval(layer.get_output(train))\n+                                assert input.shape[0] == out.shape[0]\n+                                if border_mode == 'same' and subsample_length == 1:\n+                                    assert input.shape[1] == out.shape[1]\n+                            layer.get_config()\n+\n+    def test_maxpooling_1d(self):\n+        nb_samples = 9\n+        nb_steps = 7\n+        input_dim = 10\n+\n+        input = np.ones((nb_samples, nb_steps, input_dim))\n+        for stride in [1, 2]:\n+            layer = convolutional.MaxPooling1D(stride=stride,\n+                                               border_mode='valid')\n+            layer.input = K.variable(input)\n+            for train in [True, False]:\n+                K.eval(layer.get_output(train))\n+            layer.get_config()\n+\n+    def test_convolution_2d(self):\n+        nb_samples = 8\n+        nb_filter = 9\n+        stack_size = 7\n+        nb_row = 10\n+        nb_col = 6\n+\n+        input_nb_row = 11\n+        input_nb_col = 12\n+\n+        weights_in = [np.ones((nb_filter, stack_size, nb_row, nb_col)), np.ones(nb_filter)]\n+\n+        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+        for weight in [None, weights_in]:\n+            for border_mode in ['valid', 'same']:\n+                for subsample in [(1, 1), (2, 2)]:\n+                    if border_mode == 'same' and subsample != (1, 1):\n+                        continue\n+                    for W_regularizer in [None, 'l2']:\n+                        for b_regularizer in [None, 'l2']:\n+                            for act_regularizer in [None, 'l2']:\n+                                layer = convolutional.Convolution2D(\n+                                    nb_filter, nb_row, nb_col,\n+                                    weights=weight,\n+                                    border_mode=border_mode,\n+                                    W_regularizer=W_regularizer,\n+                                    b_regularizer=b_regularizer,\n+                                    activity_regularizer=act_regularizer,\n+                                    subsample=subsample,\n+                                    input_shape=(stack_size, None, None))\n+\n+                                layer.input = K.variable(input)\n+                                for train in [True, False]:\n+                                    out = K.eval(layer.get_output(train))\n+                                    if border_mode == 'same' and subsample == (1, 1):\n+                                        assert out.shape[2:] == input.shape[2:]\n+                                layer.get_config()\n+\n+    def test_maxpooling_2d(self):\n+        nb_samples = 9\n+        stack_size = 7\n+        input_nb_row = 11\n+        input_nb_col = 12\n+        pool_size = (3, 3)\n+\n+        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+        for strides in [(1, 1), (2, 2)]:\n+            layer = convolutional.MaxPooling2D(strides=strides,\n+                                               border_mode='valid',\n+                                               pool_size=pool_size)\n+            layer.input = K.variable(input)\n+            for train in [True, False]:\n+                K.eval(layer.get_output(train))\n+            layer.get_config()\n+\n+    def test_zero_padding_2d(self):\n+        nb_samples = 9\n+        stack_size = 7\n+        input_nb_row = 11\n+        input_nb_col = 12\n+\n+        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+        layer = convolutional.ZeroPadding2D(padding=(2, 2))\n+        layer.input = K.variable(input)\n+        for train in [True, False]:\n+            out = K.eval(layer.get_output(train))\n+            for offset in [0, 1, -1, -2]:\n+                assert_allclose(out[:, :, offset, :], 0.)\n+                assert_allclose(out[:, :, :, offset], 0.)\n+            assert_allclose(out[:, :, 2:-2, 2:-2], 1.)\n+        layer.get_config()\n+\n+    def test_upsampling_1d(self):\n+        nb_samples = 9\n+        nb_steps = 7\n+        input_dim = 10\n+\n+        input = np.ones((nb_samples, nb_steps, input_dim))\n+        for length in [2, 3, 9]:\n+            layer = convolutional.UpSampling1D(length=length)\n+            layer.input = K.variable(input)\n+            for train in [True, False]:\n+                out = K.eval(layer.get_output(train))\n+                assert out.shape[1] == length * nb_steps\n+            layer.get_config()\n+\n+    def test_upsampling_2d(self):\n+        nb_samples = 9\n+        stack_size = 7\n+        input_nb_row = 11\n+        input_nb_col = 12\n+\n+        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+\n+        for length_row in [2, 3, 9]:\n+            for length_col in [2, 3, 9]:\n+                layer = convolutional.UpSampling2D(size=(length_row, length_col))\n+                layer.input = K.variable(input)\n+                for train in [True, False]:\n+                    out = K.eval(layer.get_output(train))\n+                    assert out.shape[2] == length_row * input_nb_row\n+                    assert out.shape[3] == length_col * input_nb_col\n+            layer.get_config()\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n@@ -0,0 +1,166 @@\n+import unittest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+\n+from keras import backend as K\n+from keras.layers import core\n+\n+\n+class TestLayerBase(unittest.TestCase):\n+    def test_input_output(self):\n+        nb_samples = 10\n+        input_dim = 5\n+        layer = core.Layer()\n+\n+        # Once an input is provided, it should be reachable through the\n+        # appropriate getters\n+        input = np.ones((nb_samples, input_dim))\n+        layer.input = K.variable(input)\n+        for train in [True, False]:\n+            assert_allclose(K.eval(layer.get_input(train)), input)\n+            assert_allclose(K.eval(layer.get_output(train)), input)\n+\n+    def test_connections(self):\n+        nb_samples = 10\n+        input_dim = 5\n+        layer1 = core.Layer()\n+        layer2 = core.Layer()\n+\n+        input = np.ones((nb_samples, input_dim))\n+        layer1.input = K.variable(input)\n+\n+        # After connecting, input of layer1 should be passed through\n+        layer2.set_previous(layer1)\n+        for train in [True, False]:\n+            assert_allclose(K.eval(layer2.get_input(train)), input)\n+            assert_allclose(K.eval(layer2.get_output(train)), input)\n+\n+\n+class TestConfigParams(unittest.TestCase):\n+    \"\"\"\n+    Test the constructor, config and params functions of all layers in core.\n+    \"\"\"\n+\n+    def _runner(self, layer):\n+        conf = layer.get_config()\n+        assert (type(conf) == dict)\n+\n+        param = layer.get_params()\n+        # Typically a list or a tuple, but may be any iterable\n+        assert hasattr(param, '__iter__')\n+\n+    def test_base(self):\n+        layer = core.Layer()\n+        self._runner(layer)\n+\n+    def test_masked(self):\n+        layer = core.MaskedLayer()\n+        self._runner(layer)\n+\n+    def test_merge(self):\n+        layer_1 = core.Layer()\n+        layer_2 = core.Layer()\n+        layer_1.set_input_shape((None,))\n+        layer_2.set_input_shape((None,))\n+        layer = core.Merge([layer_1, layer_2])\n+        self._runner(layer)\n+\n+    def test_dropout(self):\n+        layer = core.Dropout(0.5)\n+        self._runner(layer)\n+\n+    def test_activation(self):\n+        layer = core.Activation('linear')\n+        self._runner(layer)\n+\n+    def test_reshape(self):\n+        layer = core.Reshape(dims=(10, 10))\n+        self._runner(layer)\n+\n+    def test_flatten(self):\n+        layer = core.Flatten()\n+        self._runner(layer)\n+\n+    def test_repeat_vector(self):\n+        layer = core.RepeatVector(10)\n+        self._runner(layer)\n+\n+    def test_dense(self):\n+        layer = core.Dense(10, input_shape=(10,))\n+        self._runner(layer)\n+\n+    def test_act_reg(self):\n+        layer = core.ActivityRegularization(0.5, 0.5)\n+        self._runner(layer)\n+\n+    def test_time_dist_dense(self):\n+        layer = core.TimeDistributedDense(10, input_shape=(None, 10))\n+        self._runner(layer)\n+\n+    def test_time_dist_merge(self):\n+        layer = core.TimeDistributedMerge()\n+        self._runner(layer)\n+\n+    def test_autoencoder(self):\n+        layer_1 = core.Layer()\n+        layer_2 = core.Layer()\n+\n+        layer = core.AutoEncoder(layer_1, layer_2)\n+        self._runner(layer)\n+\n+    def test_maxout_dense(self):\n+        layer = core.MaxoutDense(10, 10)\n+        self._runner(layer)\n+\n+\n+class TestMasking(unittest.TestCase):\n+    \"\"\"Test the Masking class\"\"\"\n+\n+    def test_sequences(self):\n+        \"\"\"Test masking sequences with zeroes as padding\"\"\"\n+        if K._BACKEND == \"tensorflow\":\n+            return\n+        # integer inputs, one per timestep, like embeddings\n+        layer = core.Masking()\n+        func = K.function([layer.input], [layer.get_output_mask()])\n+        input_data = np.array([[[1], [2], [3], [0]],\n+                              [[0], [4], [5], [0]]], dtype=np.int32)\n+\n+        # This is the expected output mask, one dimension less\n+        expected = np.array([[1, 1, 1, 0], [0, 1, 1, 0]])\n+\n+        # get mask for this input\n+        output = func([input_data])[0]\n+        self.assertTrue(np.all(output == expected))\n+\n+    def test_non_zero(self):\n+        \"\"\"Test masking with non-zero mask value\"\"\"\n+        if K._BACKEND == \"tensorflow\":\n+            return\n+        layer = core.Masking(5)\n+        func = K.function([layer.input], [layer.get_output_mask()])\n+        input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n+                              [[1, 5], [5, 0], [0, 0], [0, 0]]],\n+                              dtype=np.int32)\n+        output = func([input_data])[0]\n+        expected = np.array([[1, 1, 1, 0], [1, 1, 1, 1]])\n+        self.assertTrue(np.all(output == expected))\n+\n+    def test_non_zero_output(self):\n+        \"\"\"Test output of masking layer with non-zero mask value\"\"\"\n+        if K._BACKEND == \"tensorflow\":\n+            return\n+        layer = core.Masking(5)\n+        func = K.function([layer.input], [layer.get_output()])\n+\n+        input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n+                              [[1, 5], [5, 0], [0, 0], [0, 0]]],\n+                              dtype=np.int32)\n+        output = func([input_data])[0]\n+        expected = np.array([[[1, 1], [2, 1], [3, 1], [0, 0]],\n+                            [[1, 5], [5, 0], [0, 0], [0, 0]]])\n+        self.assertTrue(np.all(output == expected))\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n@@ -0,0 +1,47 @@\n+import unittest\n+import numpy as np\n+\n+from keras.layers import recurrent\n+from keras import backend as K\n+\n+nb_samples, timesteps, input_dim, output_dim = 3, 3, 10, 5\n+\n+\n+def _runner(layer_class):\n+    \"\"\"\n+    All the recurrent layers share the same interface,\n+    so we can run through them with a single function.\n+    \"\"\"\n+    for ret_seq in [True, False]:\n+        layer = layer_class(output_dim, return_sequences=ret_seq,\n+                            weights=None, input_shape=(timesteps, input_dim))\n+        layer.input = K.variable(np.ones((nb_samples, timesteps, input_dim)))\n+        layer.get_config()\n+\n+        for train in [True, False]:\n+            out = K.eval(layer.get_output(train))\n+            # Make sure the output has the desired shape\n+            if ret_seq:\n+                assert(out.shape == (nb_samples, timesteps, output_dim))\n+            else:\n+                assert(out.shape == (nb_samples, output_dim))\n+\n+            mask = layer.get_output_mask(train)\n+\n+\n+class TestRNNS(unittest.TestCase):\n+    \"\"\"\n+    Test all the RNNs using a generic test runner function defined above.\n+    \"\"\"\n+    def test_simple(self):\n+        _runner(recurrent.SimpleRNN)\n+\n+    def test_gru(self):\n+        _runner(recurrent.GRU)\n+\n+    def test_lstm(self):\n+        _runner(recurrent.LSTM)\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n@@ -0,0 +1,76 @@\n+import unittest\n+from keras import backend as K\n+import numpy as np\n+from numpy.testing import assert_allclose\n+\n+\n+def get_standard_values():\n+    '''\n+    These are just a set of floats used for testing the activation\n+    functions, and are useful in multiple tests.\n+    '''\n+    return np.array([[0, 0.1, 0.5, 0.9, 1.0]], dtype=K.floatx())\n+\n+\n+class TestActivations(unittest.TestCase):\n+\n+    def test_softmax(self):\n+        from keras.activations import softmax as s\n+\n+        # Test using a reference implementation of softmax\n+        def softmax(values):\n+            m = np.max(values)\n+            e = np.exp(values - m)\n+            return e / np.sum(e)\n+\n+        x = K.placeholder(ndim=2)\n+        exp = s(x)\n+        f = K.function([x], [exp])\n+        test_values = get_standard_values()\n+\n+        result = f([test_values])[0]\n+        expected = softmax(test_values)\n+        assert_allclose(result, expected, rtol=1e-05)\n+\n+    def test_relu(self):\n+        '''\n+        Relu implementation doesn't depend on the value being\n+        a theano variable. Testing ints, floats and theano tensors.\n+        '''\n+        from keras.activations import relu as r\n+\n+        x = K.placeholder(ndim=2)\n+        exp = r(x)\n+        f = K.function([x], [exp])\n+\n+        test_values = get_standard_values()\n+        result = f([test_values])[0]\n+\n+        # because no negatives in test values\n+        assert_allclose(result, test_values, rtol=1e-05)\n+\n+    def test_tanh(self):\n+        from keras.activations import tanh as t\n+        test_values = get_standard_values()\n+\n+        x = K.placeholder(ndim=2)\n+        exp = t(x)\n+        f = K.function([x], [exp])\n+\n+        result = f([test_values])[0]\n+        expected = np.tanh(test_values)\n+        assert_allclose(result, expected, rtol=1e-05)\n+\n+    def test_linear(self):\n+        '''\n+        This function does no input validation, it just returns the thing\n+        that was passed in.\n+        '''\n+        from keras.activations import linear as l\n+\n+        xs = [1, 5, True, None, 'foo']\n+        for x in xs:\n+            assert x == l(x)\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n@@ -0,0 +1,72 @@\n+import unittest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+from keras import backend as K\n+\n+\n+class TestConstraints(unittest.TestCase):\n+    def setUp(self):\n+        self.some_values = [0.1, 0.5, 3, 8, 1e-7]\n+        np.random.seed(3537)\n+        self.example_array = np.random.random((100, 100)) * 100. - 50.\n+        self.example_array[0, 0] = 0.  # 0 could possibly cause trouble\n+\n+    def test_maxnorm(self):\n+        from keras.constraints import maxnorm\n+\n+        for m in self.some_values:\n+            norm_instance = maxnorm(m)\n+            normed = norm_instance(K.variable(self.example_array))\n+            assert (np.all(K.eval(normed) < m))\n+\n+        # a more explicit example\n+        norm_instance = maxnorm(2.0)\n+        x = np.array([[0, 0, 0], [1.0, 0, 0], [3, 0, 0], [3, 3, 3]]).T\n+        x_normed_target = np.array([[0, 0, 0], [1.0, 0, 0],\n+                                    [2.0, 0, 0],\n+                                    [2. / np.sqrt(3), 2. / np.sqrt(3), 2. / np.sqrt(3)]]).T\n+        x_normed_actual = K.eval(norm_instance(K.variable(x)))\n+        assert_allclose(x_normed_actual, x_normed_target, rtol=1e-05)\n+\n+    def test_nonneg(self):\n+        from keras.constraints import nonneg\n+\n+        nonneg_instance = nonneg()\n+\n+        normed = nonneg_instance(K.variable(self.example_array))\n+        assert (np.all(np.min(K.eval(normed), axis=1) == 0.))\n+\n+    def test_identity(self):\n+        from keras.constraints import identity\n+\n+        identity_instance = identity()\n+\n+        normed = identity_instance(self.example_array)\n+        assert (np.all(normed == self.example_array))\n+\n+    def test_identity_oddballs(self):\n+        \"\"\"\n+        test the identity constraint on some more exotic input.\n+        this does not need to pass for the desired real life behaviour,\n+        but it should in the current implementation.\n+        \"\"\"\n+        from keras.constraints import identity\n+        identity_instance = identity()\n+\n+        oddball_examples = [\"Hello\", [1], -1, None]\n+        assert(oddball_examples == identity_instance(oddball_examples))\n+\n+    def test_unitnorm(self):\n+        from keras.constraints import unitnorm\n+        unitnorm_instance = unitnorm()\n+\n+        normalized = unitnorm_instance(K.variable(self.example_array))\n+\n+        norm_of_normalized = np.sqrt(np.sum(K.eval(normalized)**2, axis=1))\n+        # in the unit norm constraint, it should be equal to 1.\n+        difference = norm_of_normalized - 1.\n+        largest_difference = np.max(np.abs(difference))\n+        assert np.abs(largest_difference) < 10e-5\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n@@ -0,0 +1,102 @@\n+import unittest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+from keras.layers import normalization\n+from keras.models import Sequential\n+from keras import backend as K\n+\n+\n+class TestBatchNormalization(unittest.TestCase):\n+    def setUp(self):\n+        self.input_1 = np.arange(10)\n+        self.input_2 = np.zeros(10)\n+        self.input_3 = np.ones((10))\n+\n+        self.input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n+\n+    def test_setup(self):\n+        norm_m0 = normalization.BatchNormalization(input_shape=(10, 10))\n+        norm_m1 = normalization.BatchNormalization(input_shape=(10, 10), mode=1)\n+\n+        # mode 3 does not exist\n+        self.assertRaises(Exception,\n+                          normalization.BatchNormalization(input_shape=(10, 10), mode=3))\n+\n+    def test_mode_0(self):\n+        model = Sequential()\n+        norm_m0 = normalization.BatchNormalization(input_shape=(10,))\n+        model.add(norm_m0)\n+        model.compile(loss='mse', optimizer='sgd')\n+\n+        # centered on 5.0, variance 10.0\n+        X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10))\n+        model.fit(X, X, nb_epoch=5, verbose=0)\n+        norm_m0.input = K.variable(X)\n+        out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n+\n+        self.assertAlmostEqual(K.eval(K.mean(out)), 0.0, places=1)\n+        self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=1)\n+\n+    def test_mode_1(self):\n+        norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1)\n+\n+        for inp in [self.input_1, self.input_2, self.input_3]:\n+            norm_m1.input = K.variable(inp)\n+            out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n+            self.assertAlmostEqual(K.eval(K.mean(out)), 0.0)\n+            if inp.std() > 0.:\n+                self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=2)\n+            else:\n+                self.assertAlmostEqual(K.eval(K.std(out)), 0.0, places=2)\n+\n+    def test_shapes(self):\n+        \"\"\"\n+        Test batch normalization with various input shapes\n+        \"\"\"\n+        for inp in self.input_shapes:\n+            norm_m0 = normalization.BatchNormalization(input_shape=inp.shape, mode=0)\n+            norm_m0.input = K.variable(inp)\n+            out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n+\n+            norm_m1 = normalization.BatchNormalization(input_shape=inp.shape, mode=1)\n+            norm_m1.input = K.variable(inp)\n+            out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n+\n+    def test_weight_init(self):\n+        \"\"\"\n+        Test weight initialization\n+        \"\"\"\n+        norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1,\n+                                                   weights=[np.ones(10), np.ones(10), np.zeros(10), np.zeros(10)])\n+\n+        for inp in [self.input_1, self.input_2, self.input_3]:\n+            norm_m1.input = K.variable(inp)\n+            out = (norm_m1.get_output(train=True) - np.ones(10)) / 1.\n+            self.assertAlmostEqual(K.eval(K.mean(out)), 0.0)\n+            if inp.std() > 0.:\n+                self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=2)\n+            else:\n+                self.assertAlmostEqual(K.eval(K.std(out)), 0.0, places=2)\n+\n+        assert_allclose(K.eval(norm_m1.gamma), np.ones(10))\n+        assert_allclose(K.eval(norm_m1.beta), np.ones(10))\n+\n+    def test_config(self):\n+        norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n+                                                epsilon=0.1, momentum=0.9)\n+        conf = norm.get_config()\n+        conf_target = {\"input_shape\": (10, 10),\n+                       \"name\": normalization.BatchNormalization.__name__,\n+                       \"epsilon\": 0.1, \"mode\": 1, \"momentum\": 0.9}\n+        self.assertDictEqual(conf, conf_target)\n+\n+    def test_save_weights(self):\n+        norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n+                                                epsilon=0.1)\n+        weights = norm.get_weights()\n+        assert(len(weights) == 4)\n+        norm.set_weights(weights)\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n@@ -0,0 +1,48 @@\n+from __future__ import print_function\n+import unittest\n+from keras.datasets import cifar10, cifar100, reuters, imdb, mnist\n+\n+\n+class TestDatasets(unittest.TestCase):\n+    def test_cifar(self):\n+        print('cifar10')\n+        (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n+        print(X_train.shape)\n+        print(X_test.shape)\n+        print(y_train.shape)\n+        print(y_test.shape)\n+\n+        print('cifar100 fine')\n+        (X_train, y_train), (X_test, y_test) = cifar100.load_data('fine')\n+        print(X_train.shape)\n+        print(X_test.shape)\n+        print(y_train.shape)\n+        print(y_test.shape)\n+\n+        print('cifar100 coarse')\n+        (X_train, y_train), (X_test, y_test) = cifar100.load_data('coarse')\n+        print(X_train.shape)\n+        print(X_test.shape)\n+        print(y_train.shape)\n+        print(y_test.shape)\n+\n+    def test_reuters(self):\n+        print('reuters')\n+        (X_train, y_train), (X_test, y_test) = reuters.load_data()\n+\n+    def test_mnist(self):\n+        print('mnist')\n+        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n+        print(X_train.shape)\n+        print(X_test.shape)\n+        print(y_train.shape)\n+        print(y_test.shape)\n+\n+    def test_imdb(self):\n+        print('imdb')\n+        (X_train, y_train), (X_test, y_test) = imdb.load_data()\n+\n+\n+if __name__ == '__main__':\n+    print('Test datasets')\n+    unittest.main()\n\n@@ -0,0 +1,30 @@\n+import unittest\n+import numpy as np\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation, Flatten\n+from keras.layers.embeddings import Embedding\n+from keras.constraints import unitnorm\n+from keras import backend as K\n+\n+\n+class TestEmbedding(unittest.TestCase):\n+    def setUp(self):\n+        self.X1 = np.array([[1], [2]], dtype='int32')\n+        self.W1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype='float32')\n+\n+    def test_unitnorm_constraint(self):\n+        lookup = Sequential()\n+        lookup.add(Embedding(3, 2, weights=[self.W1],\n+                             W_constraint=unitnorm(),\n+                             input_length=1))\n+        lookup.add(Flatten())\n+        lookup.add(Dense(1))\n+        lookup.add(Activation('sigmoid'))\n+        lookup.compile(loss='binary_crossentropy', optimizer='sgd',\n+                       class_mode='binary')\n+        lookup.train_on_batch(self.X1, np.array([[1], [0]], dtype='int32'))\n+        norm = np.linalg.norm(K.get_value(lookup.params[0]), axis=1)\n+        self.assertTrue(np.allclose(norm, np.ones_like(norm).astype('float32')))\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n@@ -0,0 +1,270 @@\n+from __future__ import print_function\n+import unittest\n+import numpy as np\n+np.random.seed(1337)\n+\n+from keras.models import Graph, Sequential\n+from keras.layers import containers\n+from keras.layers.core import Dense, Activation\n+from keras.utils.test_utils import get_test_data\n+\n+X = np.random.random((100, 32))\n+X2 = np.random.random((100, 32))\n+y = np.random.random((100, 4))\n+y2 = np.random.random((100,))\n+\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n+                                                     nb_test=200,\n+                                                     input_shape=(32,),\n+                                                     classification=False,\n+                                                     output_shape=(4,))\n+(X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000,\n+                                                         nb_test=200,\n+                                                         input_shape=(32,),\n+                                                         classification=False,\n+                                                         output_shape=(1,))\n+\n+\n+class TestGraph(unittest.TestCase):\n+    def test_1o_1i(self):\n+        print('test a non-sequential graph with 1 input and 1 output')\n+        graph = Graph()\n+        graph.add_input(name='input1', input_shape=(32,))\n+\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(4), name='dense3', input='dense1')\n+\n+        graph.add_output(name='output1',\n+                         inputs=['dense2', 'dense3'],\n+                         merge_mode='sum')\n+        graph.compile('rmsprop', {'output1': 'mse'})\n+\n+        history = graph.fit({'input1': X_train, 'output1': y_train},\n+                            nb_epoch=10)\n+        out = graph.predict({'input1': X_test})\n+        assert(type(out == dict))\n+        assert(len(out) == 1)\n+        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n+        loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n+        loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n+        print(loss)\n+        assert(loss < 2.5)\n+\n+        # test validation split\n+        history = graph.fit({'input1': X_train, 'output1': y_train},\n+                            validation_split=0.2, nb_epoch=1)\n+        # test validation data\n+        history = graph.fit({'input1': X_train, 'output1': y_train},\n+                            validation_data={'input1': X_train, 'output1': y_train},\n+                            nb_epoch=1)\n+\n+    def test_1o_1i_2(self):\n+        print('test a more complex non-sequential graph with 1 input and 1 output')\n+        graph = Graph()\n+        graph.add_input(name='input1', input_shape=(32,))\n+\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2-0', input='input1')\n+        graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n+\n+        graph.add_node(Dense(16), name='dense3', input='dense2')\n+        graph.add_node(Dense(4), name='dense4', inputs=['dense1', 'dense3'],\n+                       merge_mode='sum')\n+\n+        graph.add_output(name='output1', inputs=['dense2', 'dense4'],\n+                         merge_mode='sum')\n+        graph.compile('rmsprop', {'output1': 'mse'})\n+\n+        history = graph.fit({'input1': X_train, 'output1': y_train},\n+                            nb_epoch=10)\n+        out = graph.predict({'input1': X_train})\n+        assert(type(out == dict))\n+        assert(len(out) == 1)\n+        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n+        loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n+        loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n+        print(loss)\n+        assert(loss < 2.5)\n+        graph.get_config(verbose=1)\n+\n+    def test_1o_2i(self):\n+        print('test a non-sequential graph with 2 inputs and 1 output')\n+        graph = Graph()\n+        graph.add_input(name='input1', input_shape=(32,))\n+        graph.add_input(name='input2', input_shape=(32,))\n+\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input2')\n+        graph.add_node(Dense(4), name='dense3', input='dense1')\n+\n+        graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+                         merge_mode='sum')\n+        graph.compile('rmsprop', {'output1': 'mse'})\n+\n+        history = graph.fit({'input1': X_train, 'input2': X2_train, 'output1': y_train},\n+                            nb_epoch=10)\n+        out = graph.predict({'input1': X_test, 'input2': X2_test})\n+        assert(type(out == dict))\n+        assert(len(out) == 1)\n+        loss = graph.test_on_batch({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n+        loss = graph.train_on_batch({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n+        loss = graph.evaluate({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n+        print(loss)\n+        assert(loss < 3.0)\n+        graph.get_config(verbose=1)\n+\n+    def test_2o_1i_weights(self):\n+        print('test a non-sequential graph with 1 input and 2 outputs')\n+        graph = Graph()\n+        graph.add_input(name='input1', input_shape=(32,))\n+\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(1), name='dense3', input='dense1')\n+\n+        graph.add_output(name='output1', input='dense2')\n+        graph.add_output(name='output2', input='dense3')\n+        graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n+\n+        history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n+                            nb_epoch=10)\n+        out = graph.predict({'input1': X_test})\n+        assert(type(out == dict))\n+        assert(len(out) == 2)\n+        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n+        loss = graph.train_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n+        loss = graph.evaluate({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n+        print(loss)\n+        assert(loss < 4.)\n+\n+        print('test weight saving')\n+        graph.save_weights('temp.h5', overwrite=True)\n+        graph = Graph()\n+        graph.add_input(name='input1', input_shape=(32,))\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(1), name='dense3', input='dense1')\n+        graph.add_output(name='output1', input='dense2')\n+        graph.add_output(name='output2', input='dense3')\n+        graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n+        graph.load_weights('temp.h5')\n+        nloss = graph.evaluate({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n+        print(nloss)\n+        assert(loss == nloss)\n+\n+    def test_2o_1i_sample_weights(self):\n+        print('test a non-sequential graph with 1 input and 2 outputs with sample weights')\n+        graph = Graph()\n+        graph.add_input(name='input1', input_shape=(32,))\n+\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(1), name='dense3', input='dense1')\n+\n+        graph.add_output(name='output1', input='dense2')\n+        graph.add_output(name='output2', input='dense3')\n+\n+        weights1 = np.random.uniform(size=y_train.shape[0])\n+        weights2 = np.random.uniform(size=y2_train.shape[0])\n+        weights1_test = np.random.uniform(size=y_test.shape[0])\n+        weights2_test = np.random.uniform(size=y2_test.shape[0])\n+\n+        graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n+\n+        history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n+                            nb_epoch=10,\n+                            sample_weight={'output1': weights1, 'output2': weights2})\n+        out = graph.predict({'input1': X_test})\n+        assert(type(out == dict))\n+        assert(len(out) == 2)\n+        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test},\n+                                   sample_weight={'output1': weights1_test, 'output2': weights2_test})\n+        loss = graph.train_on_batch({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n+                                    sample_weight={'output1': weights1, 'output2': weights2})\n+        loss = graph.evaluate({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n+                              sample_weight={'output1': weights1, 'output2': weights2})\n+        print(loss)\n+\n+    def test_recursive(self):\n+        print('test layer-like API')\n+\n+        graph = containers.Graph()\n+        graph.add_input(name='input1', input_shape=(32,))\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(4), name='dense3', input='dense1')\n+        graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+                         merge_mode='sum')\n+\n+        seq = Sequential()\n+        seq.add(Dense(32, input_shape=(32,)))\n+        seq.add(graph)\n+        seq.add(Dense(4))\n+\n+        seq.compile('rmsprop', 'mse')\n+\n+        history = seq.fit(X_train, y_train, batch_size=10, nb_epoch=10)\n+        loss = seq.evaluate(X_test, y_test)\n+        print(loss)\n+        assert(loss < 2.5)\n+\n+        loss = seq.evaluate(X_test, y_test, show_accuracy=True)\n+        pred = seq.predict(X_test)\n+        seq.get_config(verbose=1)\n+\n+    def test_create_output(self):\n+        print('test create_output argument')\n+        graph = Graph()\n+        graph.add_input(name='input1', input_shape=(32,))\n+\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(4), name='dense3', input='dense1')\n+        graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'],\n+                       merge_mode='sum', create_output=True)\n+        graph.compile('rmsprop', {'output1': 'mse'})\n+\n+        history = graph.fit({'input1': X_train, 'output1': y_train},\n+                            nb_epoch=10)\n+        out = graph.predict({'input1': X_test})\n+        assert(type(out == dict))\n+        assert(len(out) == 1)\n+        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n+        loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n+        loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n+        print(loss)\n+        assert(loss < 2.5)\n+\n+    def test_count_params(self):\n+        print('test count params')\n+\n+        nb_units = 100\n+        nb_classes = 2\n+\n+        graph = Graph()\n+        graph.add_input(name='input1', input_shape=(32,))\n+        graph.add_input(name='input2', input_shape=(32,))\n+        graph.add_node(Dense(nb_units),\n+                       name='dense1', input='input1')\n+        graph.add_node(Dense(nb_classes),\n+                       name='dense2', input='input2')\n+        graph.add_node(Dense(nb_classes),\n+                       name='dense3', input='dense1')\n+        graph.add_output(name='output', inputs=['dense2', 'dense3'],\n+                         merge_mode='sum')\n+\n+        n = 32 * nb_units + nb_units\n+        n += 32 * nb_classes + nb_classes\n+        n += nb_units * nb_classes + nb_classes\n+\n+        self.assertEqual(n, graph.count_params())\n+\n+        graph.compile('rmsprop', {'output': 'binary_crossentropy'})\n+\n+        self.assertEqual(n, graph.count_params())\n+\n+\n+if __name__ == '__main__':\n+    print('Test graph model')\n+    unittest.main()\n\n@@ -0,0 +1,47 @@\n+import numpy as np\n+np.random.seed(1337)\n+\n+import unittest\n+from keras.models import Sequential, weighted_objective\n+from keras.layers.core import TimeDistributedDense, Masking\n+from keras import objectives\n+from keras import backend as K\n+\n+\n+class TestMasking(unittest.TestCase):\n+    def test_masking(self):\n+        if K._BACKEND == 'tensorflow':\n+            # skip this test for TF: not supported yet\n+            return\n+        X = np.array(\n+            [[[1, 1], [2, 1], [3, 1], [5, 5]],\n+             [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)\n+        model = Sequential()\n+        model.add(Masking(mask_value=0, input_shape=(None, 2)))\n+        model.add(TimeDistributedDense(1, init='one'))\n+        model.compile(loss='mse', optimizer='sgd')\n+        y = model.predict(X)\n+        history = model.fit(X, 4 * y, nb_epoch=1, batch_size=2, verbose=1)\n+        assert history.history['loss'][0] == 285.\n+\n+    def test_loss_masking(self):\n+        weighted_loss = weighted_objective(objectives.get('mae'))\n+        shape = (3, 4, 2)\n+        X = np.arange(24).reshape(shape)\n+        Y = 2 * X\n+\n+        # Normally the trailing 1 is added by standardize_weights\n+        weights = np.ones((3,))\n+        mask = np.ones((3, 4))\n+        mask[1, 0] = 0\n+\n+        out = K.eval(weighted_loss(K.variable(X),\n+                                   K.variable(Y),\n+                                   K.variable(weights),\n+                                   K.variable(mask)))\n+        print(out)\n+\n+\n+if __name__ == '__main__':\n+    print('Test masking')\n+    unittest.main()\n\n@@ -0,0 +1,159 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+\n+import numpy as np\n+np.random.seed(1336)  # for reproducibility\n+\n+from keras.datasets import mnist\n+from keras.models import Sequential, Graph\n+from keras.layers.core import Dense, Activation\n+from keras.utils import np_utils\n+import unittest\n+\n+nb_classes = 10\n+batch_size = 128\n+nb_epoch = 10\n+weighted_class = 9\n+standard_weight = 1\n+high_weight = 5\n+max_train_samples = 5000\n+max_test_samples = 1000\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+X_train = X_train.reshape(60000, 784)[:max_train_samples]\n+X_test = X_test.reshape(10000, 784)[:max_test_samples]\n+X_train = X_train.astype(\"float32\") / 255\n+X_test = X_test.astype(\"float32\") / 255\n+\n+# convert class vectors to binary class matrices\n+y_train = y_train[:max_train_samples]\n+y_test = y_test[:max_test_samples]\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+test_ids = np.where(y_test == np.array(weighted_class))[0]\n+\n+class_weight = dict([(i, standard_weight) for i in range(nb_classes)])\n+class_weight[weighted_class] = high_weight\n+\n+sample_weight = np.ones((y_train.shape[0])) * standard_weight\n+sample_weight[y_train == weighted_class] = high_weight\n+\n+\n+def create_sequential_model():\n+    model = Sequential()\n+    model.add(Dense(50, input_shape=(784,)))\n+    model.add(Activation('relu'))\n+    model.add(Dense(10))\n+    model.add(Activation('softmax'))\n+    return model\n+\n+\n+def create_graph_model():\n+    model = Graph()\n+    model.add_input(name='input', input_shape=(784,))\n+    model.add_node(Dense(50, activation='relu'), name='d1', input='input')\n+    model.add_node(Dense(10, activation='softmax'), name='d2', input='d1')\n+    model.add_output(name='output', input='d2')\n+    return model\n+\n+\n+def _test_weights_sequential(model, class_weight=None, sample_weight=None):\n+    if sample_weight is not None:\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch // 3, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight)\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch // 3, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight,\n+                  validation_split=0.1)\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch // 3, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight,\n+                  validation_data=(X_train, Y_train, sample_weight))\n+    else:\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch // 2, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight)\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch // 2, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight,\n+                  validation_split=0.1)\n+\n+    model.train_on_batch(X_train[:32], Y_train[:32],\n+                         class_weight=class_weight,\n+                         sample_weight=sample_weight[:32] if sample_weight is not None else None)\n+    model.test_on_batch(X_train[:32], Y_train[:32],\n+                        sample_weight=sample_weight[:32] if sample_weight is not None else None)\n+    score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n+    return score\n+\n+\n+def _test_weights_graph(model, class_weight=None, sample_weight=None):\n+    model.fit({'input': X_train, 'output': Y_train},\n+              batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n+              class_weight={'output': class_weight},\n+              sample_weight={'output': sample_weight})\n+    model.fit({'input': X_train, 'output': Y_train},\n+              batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n+              class_weight={'output': class_weight},\n+              sample_weight={'output': sample_weight}, validation_split=0.1)\n+\n+    model.train_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n+                         class_weight={'output': class_weight},\n+                         sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n+    model.test_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n+                        sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n+    score = model.evaluate({'input': X_test[test_ids, :],\n+                            'output': Y_test[test_ids, :]},\n+                           verbose=0)\n+    return score\n+\n+\n+class TestLossWeighting(unittest.TestCase):\n+    def test_sequential(self):\n+        for loss in ['mae', 'mse']:\n+            print('loss:', loss)\n+            print('sequential')\n+            # no weights: reference point\n+            model = create_sequential_model()\n+            model.compile(loss=loss, optimizer='rmsprop')\n+            standard_score = _test_weights_sequential(model)\n+            # test class_weight\n+            model = create_sequential_model()\n+            model.compile(loss=loss, optimizer='rmsprop')\n+            score = _test_weights_sequential(model, class_weight=class_weight)\n+            print('score:', score, ' vs.', standard_score)\n+            self.assertTrue(score < standard_score)\n+            # test sample_weight\n+            model = create_sequential_model()\n+            model.compile(loss=loss, optimizer='rmsprop')\n+            score = _test_weights_sequential(model, sample_weight=sample_weight)\n+            print('score:', score, ' vs.', standard_score)\n+            self.assertTrue(score < standard_score)\n+\n+    def test_graph(self):\n+        for loss in ['mae', 'mse']:\n+            print('loss:', loss)\n+            print('graph')\n+            # no weights: reference point\n+            model = create_graph_model()\n+            model.compile(loss={'output': loss}, optimizer='rmsprop')\n+            standard_score = _test_weights_graph(model)\n+            # test class_weight\n+            model = create_graph_model()\n+            model.compile(loss={'output': loss}, optimizer='rmsprop')\n+            score = _test_weights_graph(model, class_weight=class_weight)\n+            print('score:', score, ' vs.', standard_score)\n+            self.assertTrue(score < standard_score)\n+            # test sample_weight\n+            model = create_graph_model()\n+            model.compile(loss={'output': loss}, optimizer='rmsprop')\n+            score = _test_weights_graph(model, sample_weight=sample_weight)\n+            print('score:', score, ' vs.', standard_score)\n+            self.assertTrue(score < standard_score)\n+\n+\n+if __name__ == '__main__':\n+    print('Test class_weight and sample_weight')\n+    unittest.main()\n\n@@ -0,0 +1,59 @@\n+from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337)\n+\n+from keras.utils.test_utils import get_test_data\n+from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation\n+from keras.utils.np_utils import to_categorical\n+import unittest\n+\n+\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(10,),\n+                                                     classification=True, nb_class=2)\n+y_train = to_categorical(y_train)\n+y_test = to_categorical(y_test)\n+\n+\n+def get_model(input_dim, nb_hidden, output_dim):\n+    model = Sequential()\n+    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    model.add(Activation('relu'))\n+    model.add(Dense(output_dim))\n+    model.add(Activation('softmax'))\n+    return model\n+\n+\n+def _test_optimizer(optimizer, target=0.9):\n+    model = get_model(X_train.shape[1], 10, y_train.shape[1])\n+    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n+    history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n+    return history.history['val_acc'][-1] > target\n+\n+\n+class TestOptimizers(unittest.TestCase):\n+    def test_sgd(self):\n+        print('test SGD')\n+        sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n+        self.assertTrue(_test_optimizer(sgd))\n+\n+    def test_rmsprop(self):\n+        print('test RMSprop')\n+        self.assertTrue(_test_optimizer(RMSprop()))\n+\n+    def test_adagrad(self):\n+        print('test Adagrad')\n+        self.assertTrue(_test_optimizer(Adagrad()))\n+\n+    def test_adadelta(self):\n+        print('test Adadelta')\n+        self.assertTrue(_test_optimizer(Adadelta()))\n+\n+    def test_adam(self):\n+        print('test Adam')\n+        self.assertTrue(_test_optimizer(Adam()))\n+\n+if __name__ == '__main__':\n+    print('Test optimizers')\n+    unittest.main()\n\n@@ -0,0 +1,62 @@\n+import unittest\n+import numpy as np\n+np.random.seed(1337) # for reproducibility\n+\n+from keras.models import Sequential\n+from keras.layers.core import Merge, Dense, Activation, Flatten, ActivityRegularization\n+from keras.layers.embeddings import Embedding\n+from keras.datasets import mnist\n+from keras.utils import np_utils\n+from keras import regularizers\n+\n+nb_classes = 10\n+batch_size = 128\n+nb_epoch = 5\n+weighted_class = 9\n+standard_weight = 1\n+high_weight = 5\n+max_train_samples = 5000\n+max_test_samples = 1000\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+X_train = X_train.reshape(60000, 784)[:max_train_samples]\n+X_test = X_test.reshape(10000, 784)[:max_test_samples]\n+X_train = X_train.astype(\"float32\") / 255\n+X_test = X_test.astype(\"float32\") / 255\n+\n+# convert class vectors to binary class matrices\n+y_train = y_train[:max_train_samples]\n+y_test = y_test[:max_test_samples]\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+test_ids = np.where(y_test == np.array(weighted_class))[0]\n+\n+\n+def create_model(weight_reg=None, activity_reg=None):\n+    model = Sequential()\n+    model.add(Dense(50, input_shape=(784,)))\n+    model.add(Activation('relu'))\n+    model.add(Dense(10, W_regularizer=weight_reg, activity_regularizer=activity_reg))\n+    model.add(Activation('softmax'))\n+    return model\n+\n+\n+class TestRegularizers(unittest.TestCase):\n+    def test_W_reg(self):\n+        for reg in [regularizers.identity(), regularizers.l1(), regularizers.l2(), regularizers.l1l2()]:\n+            model = create_model(weight_reg=reg)\n+            model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+            model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+            model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n+\n+    def test_A_reg(self):\n+        for reg in [regularizers.activity_l1(), regularizers.activity_l2()]:\n+            model = create_model(activity_reg=reg)\n+            model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+            model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+            model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n+\n+if __name__ == '__main__':\n+    print('Test weight and activity regularizers')\n+    unittest.main()\n\n@@ -0,0 +1,405 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import unittest\n+import numpy as np\n+np.random.seed(1337)\n+\n+from keras import backend as K\n+from keras.models import Sequential, model_from_json, model_from_yaml\n+from keras.layers.core import Dense, Activation, Merge, Lambda, LambdaMerge\n+from keras.utils import np_utils\n+from keras.utils.test_utils import get_test_data\n+import pickle\n+import sys\n+\n+input_dim = 32\n+nb_hidden = 16\n+nb_class = 4\n+batch_size = 64\n+nb_epoch = 1\n+\n+train_samples = 5000\n+test_samples = 1000\n+\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n+                                                     nb_test=test_samples,\n+                                                     input_shape=(input_dim,),\n+                                                     classification=True,\n+                                                     nb_class=4)\n+y_test = np_utils.to_categorical(y_test)\n+y_train = np_utils.to_categorical(y_train)\n+print(X_train.shape)\n+print(y_train.shape)\n+\n+\n+class TestSequential(unittest.TestCase):\n+    def test_sequential(self):\n+        print('Test sequential')\n+        model = Sequential()\n+        model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        model.add(Activation('relu'))\n+        model.add(Dense(nb_class))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n+\n+        model.train_on_batch(X_train[:32], y_train[:32])\n+\n+        loss = model.evaluate(X_train, y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.7:\n+            raise Exception('Score too low, learning issue.')\n+        model.predict(X_test, verbose=0)\n+        model.predict_classes(X_test, verbose=0)\n+        model.predict_proba(X_test, verbose=0)\n+        model.get_config(verbose=0)\n+\n+        print('test weight saving')\n+        model.save_weights('temp.h5', overwrite=True)\n+        model = Sequential()\n+        model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        model.add(Activation('relu'))\n+        model.add(Dense(nb_class))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        model.load_weights('temp.h5')\n+\n+        nloss = model.evaluate(X_train, y_train, verbose=0)\n+        assert(loss == nloss)\n+\n+        # test json serialization\n+        json_data = model.to_json()\n+        model = model_from_json(json_data)\n+\n+        # test yaml serialization\n+        yaml_data = model.to_yaml()\n+        model = model_from_yaml(yaml_data)\n+\n+    def test_merge_sum(self):\n+        print('Test merge: sum')\n+        left = Sequential()\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        right.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([left, right], mode='sum'))\n+        model.add(Dense(nb_class))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+        loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.7:\n+            raise Exception('Score too low, learning issue.')\n+        model.predict([X_test, X_test], verbose=0)\n+        model.predict_classes([X_test, X_test], verbose=0)\n+        model.predict_proba([X_test, X_test], verbose=0)\n+        model.get_config(verbose=0)\n+\n+        print('test weight saving')\n+        model.save_weights('temp.h5', overwrite=True)\n+        left = Sequential()\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        left.add(Activation('relu'))\n+        right = Sequential()\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        right.add(Activation('relu'))\n+        model = Sequential()\n+        model.add(Merge([left, right], mode='sum'))\n+        model.add(Dense(nb_class))\n+        model.add(Activation('softmax'))\n+        model.load_weights('temp.h5')\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print(nloss)\n+        assert(loss == nloss)\n+\n+    def test_merge_dot1(self):\n+        if K._BACKEND == 'tensorflow':\n+            return\n+\n+        print('Test merge: dot')\n+        left = Sequential()\n+        left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+        right.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([left, right], mode='dot', dot_axes=1))\n+        model.add(Dense(nb_class))\n+        model.add(Activation('softmax'))\n+\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    def test_merge_dot2(self):\n+        if K._BACKEND == 'tensorflow':\n+            return\n+\n+        print('Test merge: dot')\n+        left = Sequential()\n+        left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+        right.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([left, right], mode='dot', dot_axes=([1], [1])))\n+        model.add(Dense(nb_class))\n+        model.add(Activation('softmax'))\n+\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    def test_merge_concat(self):\n+        print('Test merge: concat')\n+        left = Sequential()\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        right.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([left, right], mode='concat'))\n+        model.add(Dense(nb_class))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+        loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.7:\n+            raise Exception('Score too low, learning issue.')\n+        model.predict([X_test, X_test], verbose=0)\n+        model.predict_classes([X_test, X_test], verbose=0)\n+        model.predict_proba([X_test, X_test], verbose=0)\n+        model.get_config(verbose=0)\n+\n+        print('test weight saving')\n+        model.save_weights('temp.h5', overwrite=True)\n+        left = Sequential()\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        right.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([left, right], mode='concat'))\n+\n+        model.add(Dense(nb_class))\n+        model.add(Activation('softmax'))\n+\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        model.load_weights('temp.h5')\n+\n+        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        assert(loss == nloss)\n+\n+    def test_merge_recursivity(self):\n+        print('Test merge recursivity')\n+\n+        left = Sequential()\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        right.add(Activation('relu'))\n+\n+        righter = Sequential()\n+        righter.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        righter.add(Activation('relu'))\n+\n+        intermediate = Sequential()\n+        intermediate.add(Merge([left, right], mode='sum'))\n+        intermediate.add(Dense(nb_hidden))\n+        intermediate.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([intermediate, righter], mode='sum'))\n+        model.add(Dense(nb_class))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+        loss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.7:\n+            raise Exception('Score too low, learning issue.')\n+        model.predict([X_test, X_test, X_test], verbose=0)\n+        model.predict_classes([X_test, X_test, X_test], verbose=0)\n+        model.predict_proba([X_test, X_test, X_test], verbose=0)\n+        model.get_config(verbose=0)\n+\n+        model.save_weights('temp.h5', overwrite=True)\n+        model.load_weights('temp.h5')\n+\n+        nloss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n+        print(nloss)\n+        assert(loss == nloss)\n+\n+    def test_merge_overlap(self):\n+        print('Test merge overlap')\n+        left = Sequential()\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        left.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([left, left], mode='sum'))\n+        model.add(Dense(nb_class))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n+\n+        model.train_on_batch(X_train[:32], y_train[:32])\n+\n+        loss = model.evaluate(X_train, y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.7:\n+            raise Exception('Score too low, learning issue.')\n+        model.predict(X_test, verbose=0)\n+        model.predict_classes(X_test, verbose=0)\n+        model.predict_proba(X_test, verbose=0)\n+        model.get_config(verbose=0)\n+\n+        model.save_weights('temp.h5', overwrite=True)\n+        model.load_weights('temp.h5')\n+\n+        nloss = model.evaluate(X_train, y_train, verbose=0)\n+        print(nloss)\n+        assert(loss == nloss)\n+\n+    def test_lambda(self):\n+        print('Test lambda: sum')\n+\n+        def func(X):\n+            s = X[0]\n+            for i in range(1, len(X)):\n+                s += X[i]\n+            return s\n+\n+        def activation(X):\n+            return K.softmax(X)\n+\n+        def output_shape(input_shapes):\n+            return input_shapes[0]\n+\n+        left = Sequential()\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        right.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(LambdaMerge([left, right], function=func,\n+                              output_shape=output_shape))\n+        model.add(Dense(nb_class))\n+        model.add(Lambda(activation))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+        loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.7:\n+            raise Exception('Score too low, learning issue.')\n+        model.predict([X_test, X_test], verbose=0)\n+        model.predict_classes([X_test, X_test], verbose=0)\n+        model.predict_proba([X_test, X_test], verbose=0)\n+        model.get_config(verbose=0)\n+\n+        print('test weight saving')\n+        model.save_weights('temp.h5', overwrite=True)\n+        left = Sequential()\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        left.add(Activation('relu'))\n+        right = Sequential()\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        right.add(Activation('relu'))\n+        model = Sequential()\n+        model.add(LambdaMerge([left, right], function=func,\n+                              output_shape=output_shape))\n+        model.add(Dense(nb_class))\n+        model.add(Lambda(activation))\n+        model.load_weights('temp.h5')\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        assert(loss == nloss)\n+\n+    def test_count_params(self):\n+        print('test count params')\n+        input_dim = 20\n+        nb_units = 10\n+        nb_classes = 2\n+\n+        n = input_dim * nb_units + nb_units\n+        n += nb_units * nb_units + nb_units\n+        n += nb_units * nb_classes + nb_classes\n+\n+        model = Sequential()\n+        model.add(Dense(nb_units, input_shape=(input_dim,)))\n+        model.add(Dense(nb_units))\n+        model.add(Dense(nb_classes))\n+        model.add(Activation('softmax'))\n+\n+        self.assertEqual(n, model.count_params())\n+\n+        model.compile('sgd', 'binary_crossentropy')\n+\n+        self.assertEqual(n, model.count_params())\n+\n+\n+if __name__ == '__main__':\n+    print('Test Sequential model')\n+    unittest.main()\n\n@@ -0,0 +1,167 @@\n+import unittest\n+import numpy as np\n+\n+from keras import backend as K\n+from keras.layers.core import *\n+from keras.layers.convolutional import *\n+from keras.layers.recurrent import SimpleRNN\n+\n+\n+def check_layer_output_shape(layer, input_data):\n+    ndim = len(input_data.shape)\n+    layer.input = K.placeholder(ndim=ndim)\n+    layer.set_input_shape(input_data.shape[1:])\n+    expected_output_shape = layer.output_shape[1:]\n+\n+    function = K.function([layer.input], [layer.get_output()])\n+    output = function([input_data])[0]\n+    assert output.shape[1:] == expected_output_shape, str(output.shape[1:]) + ' != ' + str(expected_output_shape)\n+\n+\n+class TestShapeInference(unittest.TestCase):\n+    # ########\n+    # # Core #\n+    # ########\n+    def test_Reshape(self):\n+        layer = Reshape(dims=(2, 3))\n+        input_data = np.random.random((2, 6))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_Permute(self):\n+        layer = Permute(dims=(1, 3, 2))\n+        input_data = np.random.random((2, 2, 4, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_Flatten(self):\n+        layer = Flatten()\n+        input_data = np.random.random((2, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_RepeatVector(self):\n+        layer = RepeatVector(2)\n+        input_data = np.random.random((2, 2))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_Dense(self):\n+        layer = Dense(3)\n+        input_data = np.random.random((2, 2))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_TimeDistributedDense(self):\n+        layer = TimeDistributedDense(2)\n+        input_data = np.random.random((2, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+    #################\n+    # Convolutional #\n+    #################\n+    def test_Convolution1D(self):\n+        for border_mode in ['same', 'valid']:\n+            for filter_length in [2, 3]:\n+                for subsample_length in [1]:\n+                    if subsample_length > 1 and border_mode == 'same':\n+                        continue\n+                    for input_data_shape in [(2, 3, 4), (2, 4, 4)]:\n+                        layer = Convolution1D(nb_filter=1,\n+                                              filter_length=filter_length,\n+                                              border_mode=border_mode,\n+                                              subsample_length=subsample_length)\n+                        input_data = np.random.random(input_data_shape)\n+                        check_layer_output_shape(layer, input_data)\n+\n+    def test_Convolution2D(self):\n+        for border_mode in ['same', 'valid']:\n+            for nb_row, nb_col in [(2, 2), (3, 3)]:\n+                for subsample in [(1, 1), (2, 2)]:\n+                    if (subsample[0] > 1 or subsample[1] > 1) and border_mode == 'same':\n+                        continue\n+                    for input_data_shape in [(2, 1, 3, 3), (2, 1, 4, 4)]:\n+                        layer = Convolution2D(nb_filter=1, nb_row=nb_row,\n+                                              nb_col=nb_row,\n+                                              border_mode=border_mode,\n+                                              subsample=subsample,\n+                                              dim_ordering='th')\n+                        input_data = np.random.random(input_data_shape)\n+                        check_layer_output_shape(layer, input_data)\n+\n+                    for input_data_shape in [(2, 3, 3, 1)]:\n+                        layer = Convolution2D(nb_filter=1, nb_row=nb_row,\n+                                              nb_col=nb_row,\n+                                              border_mode=border_mode,\n+                                              subsample=subsample,\n+                                              dim_ordering='tf')\n+                        input_data = np.random.random(input_data_shape)\n+                        check_layer_output_shape(layer, input_data)\n+\n+    def test_MaxPooling1D(self):\n+        for ignore_border in [True, False]:\n+            for pool_length in [1, 2]:\n+                for stride in [1]:\n+                    for input_data_shape in [(2, 3, 4), (2, 4, 4)]:\n+                        layer = MaxPooling1D(pool_length=pool_length,\n+                                             stride=stride,\n+                                             border_mode='valid')\n+                        input_data = np.random.random(input_data_shape)\n+                        check_layer_output_shape(layer, input_data)\n+\n+    def test_MaxPooling2D(self):\n+        for ignore_border in [True, False]:\n+            for strides in [(1, 1), (2, 2)]:\n+                for pool_size in [(2, 2), (3, 3), (4, 4)]:\n+                    for input_data_shape in [(2, 1, 4, 4), (2, 1, 5, 5), (2, 1, 6, 6)]:\n+                        layer = MaxPooling2D(pool_size=pool_size,\n+                                             strides=strides,\n+                                             border_mode='valid',\n+                                             dim_ordering='th')\n+                        input_data = np.random.random(input_data_shape)\n+                        check_layer_output_shape(layer, input_data)\n+\n+                    for input_data_shape in [(2, 4, 4, 1)]:\n+                        layer = MaxPooling2D(pool_size=pool_size,\n+                                             strides=strides,\n+                                             border_mode='valid',\n+                                             dim_ordering='tf')\n+                        input_data = np.random.random(input_data_shape)\n+                        check_layer_output_shape(layer, input_data)\n+\n+    def test_UpSampling1D(self):\n+        layer = UpSampling1D(length=2)\n+        input_data = np.random.random((2, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_UpSampling2D(self):\n+        layer = UpSampling2D(size=(2, 2), dim_ordering='th')\n+        input_data = np.random.random((2, 1, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+        layer = UpSampling2D(size=(2, 2), dim_ordering='tf')\n+        input_data = np.random.random((2, 2, 3, 1))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_ZeroPadding1D(self):\n+        layer = ZeroPadding1D(1)\n+        input_data = np.random.random((2, 2, 1))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_ZeroPadding2D(self):\n+        layer = ZeroPadding2D((1, 2), dim_ordering='th')\n+        input_data = np.random.random((2, 1, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+        layer = ZeroPadding2D((1, 2), dim_ordering='tf')\n+        input_data = np.random.random((2, 2, 3, 1))\n+        check_layer_output_shape(layer, input_data)\n+\n+    # #############\n+    # # Recurrent #\n+    # #############\n+    def test_SimpleRNN(self):\n+        # all recurrent layers inherit output_shape\n+        # from the same base recurrent layer\n+        layer = SimpleRNN(2)\n+        input_data = np.random.random((2, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -0,0 +1,158 @@\n+from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337)\n+\n+from keras.utils.test_utils import get_test_data\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation, TimeDistributedDense, Flatten\n+from keras.layers.recurrent import GRU\n+from keras.layers.convolutional import Convolution2D\n+from keras.utils.np_utils import to_categorical\n+import unittest\n+\n+\n+class TestTasks(unittest.TestCase):\n+    def test_vector_clf(self):\n+        nb_hidden = 10\n+\n+        print('vector classification data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n+                                                             nb_test=200,\n+                                                             input_shape=(10,),\n+                                                             classification=True,\n+                                                             nb_class=2)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        y_train = to_categorical(y_train)\n+        y_test = to_categorical(y_test)\n+\n+        model = Sequential()\n+        model.add(Dense(nb_hidden, input_shape=(X_train.shape[-1],)))\n+        model.add(Activation('relu'))\n+        model.add(Dense(y_train.shape[-1]))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        history = model.fit(X_train, y_train, nb_epoch=15, batch_size=16,\n+                            validation_data=(X_test, y_test),\n+                            show_accuracy=True, verbose=0)\n+        self.assertTrue(history.history['val_acc'][-1] > 0.8)\n+\n+    def test_vector_reg(self):\n+        nb_hidden = 10\n+        print('vector regression data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n+                                                             nb_test=200,\n+                                                             input_shape=(10,),\n+                                                             output_shape=(2,),\n+                                                             classification=False)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        model = Sequential()\n+        model.add(Dense(nb_hidden, input_shape=(X_train.shape[-1],)))\n+        model.add(Activation('tanh'))\n+        model.add(Dense(y_train.shape[-1]))\n+        model.compile(loss='hinge', optimizer='adagrad')\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n+                            validation_data=(X_test, y_test), verbose=0)\n+        self.assertTrue(history.history['val_loss'][-1] < 0.9)\n+\n+    def test_temporal_clf(self):\n+        print('temporal classification data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n+                                                             nb_test=200,\n+                                                             input_shape=(3, 5),\n+                                                             classification=True,\n+                                                             nb_class=2)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        y_train = to_categorical(y_train)\n+        y_test = to_categorical(y_test)\n+\n+        model = Sequential()\n+        model.add(GRU(y_train.shape[-1], input_shape=(X_train.shape[1], X_train.shape[2])))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n+                            validation_data=(X_test, y_test),\n+                            show_accuracy=True, verbose=0)\n+        self.assertTrue(history.history['val_acc'][-1] > 0.9)\n+\n+    def test_temporal_reg(self):\n+        print('temporal regression data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n+                                                             nb_test=200,\n+                                                             input_shape=(3, 5),\n+                                                             output_shape=(2,),\n+                                                             classification=False)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        model = Sequential()\n+        model.add(GRU(y_train.shape[-1], input_shape=(X_train.shape[1], X_train.shape[2])))\n+        model.compile(loss='hinge', optimizer='adam')\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n+                            validation_data=(X_test, y_test), verbose=0)\n+        self.assertTrue(history.history['val_loss'][-1] < 0.8)\n+\n+    def test_seq_to_seq(self):\n+        print('sequence to sequence data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n+                                                             nb_test=200,\n+                                                             input_shape=(3, 5),\n+                                                             output_shape=(3, 5),\n+                                                             classification=False)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        model = Sequential()\n+        model.add(TimeDistributedDense(y_train.shape[-1], input_shape=(X_train.shape[1], X_train.shape[2])))\n+        model.compile(loss='hinge', optimizer='rmsprop')\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n+                            validation_data=(X_test, y_test), verbose=0)\n+        self.assertTrue(history.history['val_loss'][-1] < 0.8)\n+\n+    def test_img_clf(self):\n+        print('image classification data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n+                                                             nb_test=200,\n+                                                             input_shape=(3, 8, 8),\n+                                                             classification=True,\n+                                                             nb_class=2)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        y_train = to_categorical(y_train)\n+        y_test = to_categorical(y_test)\n+\n+        model = Sequential()\n+        model.add(Convolution2D(8, 8, 8, input_shape=(3, 8, 8)))\n+        model.add(Activation('sigmoid'))\n+        model.add(Flatten())\n+        model.add(Dense(y_test.shape[-1]))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='sgd')\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n+                            validation_data=(X_test, y_test),\n+                            show_accuracy=True, verbose=0)\n+        print(history.history['val_acc'][-1])\n+        self.assertTrue(history.history['val_acc'][-1] > 0.9)\n+\n+\n+if __name__ == '__main__':\n+    print('Test different types of classification and regression tasks')\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
