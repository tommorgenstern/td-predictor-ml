{"custom_id": "scrapy#c31441a273ec93c2db9a94d7353e022524fef472", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 190 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -143,7 +143,7 @@ HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n HTTPCACHE_DBM_MODULE = 'anydbm'\n-HTTPCACHE_POLICY = 'scrapy.contrib.httpcache.RFC2616Policy'\n+HTTPCACHE_POLICY = 'scrapy.contrib.httpcache.DummyPolicy'\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b7eeeff4102d411cd74636df5355a5cd9100c270", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 166 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -75,7 +75,7 @@ class DjangoItemTest(unittest.TestCase):\n         long_name = 'z' * 300\n         i = BasePersonItem(name=long_name)\n         self.assertFalse(i.is_valid())\n-        self.assertDictEqual(\n+        self.assertEqual(\n             {\n                 'age': [u'This field cannot be null.'],\n                 'name': [u'Ensure this value has at most 255 characters (it has 300).']\n@@ -84,7 +84,7 @@ class DjangoItemTest(unittest.TestCase):\n \n         i = BasePersonItem(name='John')\n         self.assertTrue(i.is_valid(exclude=['age']))\n-        self.assertDictEqual({}, i.errors)\n+        self.assertEqual({}, i.errors)\n \n         # once the item is validated, it does not validate again\n         i['name'] = long_name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ba04b1fc3c95201ca6eebc31b41ddf9382bda14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 18 | Churn Cumulative: 885 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,6 +73,11 @@ class XPathSelector(object_ref):\n             self.namespaces = {}\n         self.namespaces[prefix] = uri\n \n+    def remove_namespaces(self):\n+        for el in self._root.iter('*'):\n+            if el.tag.startswith('{'):\n+                el.tag = el.tag.split('}', 1)[1]\n+\n     def __nonzero__(self):\n         return bool(self.extract())\n \n\n@@ -4,7 +4,7 @@ Selectors tests, specific for lxml backend\n \n import unittest\n from scrapy.tests import test_selector\n-from scrapy.http import TextResponse, HtmlResponse\n+from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n from scrapy.selector.lxmldocument import LxmlDocument\n from scrapy.selector.lxmlsel import XmlXPathSelector, HtmlXPathSelector, XPathSelector\n \n@@ -15,6 +15,17 @@ class LxmlXPathSelectorTestCase(test_selector.XPathSelectorTestCase):\n     hxs_cls = HtmlXPathSelector\n     xxs_cls = XmlXPathSelector\n \n+    def test_remove_namespaces(self):\n+        xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<feed xmlns=\"http://www.w3.org/2005/Atom\" xml:lang=\"en-US\" xmlns:media=\"http://search.yahoo.com/mrss/\">\n+  <link type=\"text/html\">\n+  <link type=\"application/atom+xml\">\n+</feed>\n+\"\"\"\n+        xxs = XmlXPathSelector(XmlResponse(\"http://example.com/feed.atom\", body=xml))\n+        self.assertEqual(len(xxs.select(\"//link\")), 0)\n+        xxs.remove_namespaces()\n+        self.assertEqual(len(xxs.select(\"//link\")), 2)\n \n class Libxml2DocumentTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2c51266a40a89f9fff3096d74f074f38961fcfda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1241 | Contributors (this commit): 7 | Commits (past 90d): 12 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -100,4 +100,4 @@ class FilesystemCacheStorage(_FilesystemCacheStorage):\n                       'scrapy.contrib.downloadermiddlware.httpcache is '\n                       'deprecated, use scrapy.contrib.httpcache instead.',\n                       category=ScrapyDeprecationWarning, stacklevel=1)\n-        super(_FilesystemCacheStorage, self).__init__(*args, **kwargs)\n+        super(FilesystemCacheStorage, self).__init__(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27583922a702e808d544534bdb920e31eb6a9e9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1245 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,7 @@\n-from time import time\n from email.utils import formatdate\n-from weakref import WeakKeyDictionary\n from scrapy import signals\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n-from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n-from scrapy.contrib.httpcache import rfc1123_to_epoch, parse_cachecontrol\n \n \n class HttpCacheMiddleware(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#71d7df9273dae5da8765ca4dd7e7ed4dacad5309", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 23 | Churn Cumulative: 71 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,31 +1,18 @@\n from scrapy.spider import BaseSpider\n+from scrapy.utils.spider import iterate_spider_output\n \n class InitSpider(BaseSpider):\n     \"\"\"Base Spider with initialization facilities\"\"\"\n     \n-    def __init__(self, *a, **kw):\n-        super(InitSpider, self).__init__(*a, **kw)\n-        self._postinit_reqs = []\n-        self._init_complete = False\n-        self._init_started = False\n-\n-    def make_requests_from_url(self, url):\n-        req = super(InitSpider, self).make_requests_from_url(url)\n-        if self._init_complete:\n-            return req\n-        self._postinit_reqs.append(req)\n-        if not self._init_started:\n-            self._init_started = True\n-            return self.init_request()\n+    def start_requests(self):\n+        self._postinit_reqs = super(InitSpider, self).start_requests()\n+        return iterate_spider_output(self.init_request())\n \n     def initialized(self, response=None):\n         \"\"\"This method must be set as the callback of your last initialization\n         request. See self.init_request() docstring for more info.\n         \"\"\"\n-        self._init_complete = True\n-        reqs = self._postinit_reqs[:]\n-        del self._postinit_reqs\n-        return reqs\n+        return self.__dict__.pop('_postinit_reqs')\n \n     def init_request(self):\n         \"\"\"This function should return one initialization request, with the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ff044806759b82bfe38af32b905791f2e993702a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 46 | Churn Cumulative: 1105 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -134,7 +134,10 @@ class XmlItemExporter(BaseItemExporter):\n \n     def _export_xml_field(self, name, serialized_value):\n         self.xg.startElement(name, {})\n-        if hasattr(serialized_value, '__iter__'):\n+        if hasattr(serialized_value, 'items'):\n+            for subname, value in serialized_value.items():\n+                self._export_xml_field(subname, value)\n+        elif hasattr(serialized_value, '__iter__'):\n             for value in serialized_value:\n                 self._export_xml_field('value', value)\n         else:\n\n@@ -171,6 +171,47 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         expected_value = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><name><value>John\\xc2\\xa3</value><value>Doe</value></name></item></items>'\n         self.assertEqual(output.getvalue(), expected_value)\n \n+    def test_nested_item(self):\n+        output = StringIO()\n+        i1 = TestItem(name=u'foo\\xa3hoo', age='22')\n+        i2 = TestItem(name=u'bar', age=i1)\n+        i3 = TestItem(name=u'buz', age=i2)\n+        ie = XmlItemExporter(output)\n+        ie.start_exporting()\n+        ie.export_item(i3)\n+        ie.finish_exporting()\n+        expected_value = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\\\n+                '<items><item>'\\\n+                    '<age>'\\\n+                        '<age>'\\\n+                            '<age>22</age>'\\\n+                            '<name>foo\\xc2\\xa3hoo</name>'\\\n+                        '</age>'\\\n+                        '<name>bar</name>'\\\n+                    '</age>'\\\n+                    '<name>buz</name>'\\\n+                '</item></items>'\n+        self.assertEqual(output.getvalue(), expected_value)\n+\n+    def test_nested_list_item(self):\n+        output = StringIO()\n+        i1 = TestItem(name=u'foo')\n+        i2 = TestItem(name=u'bar')\n+        i3 = TestItem(name=u'buz', age=[i1, i2])\n+        ie = XmlItemExporter(output)\n+        ie.start_exporting()\n+        ie.export_item(i3)\n+        ie.finish_exporting()\n+        expected_value =  '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\\\n+                '<items><item>'\\\n+                    '<age>'\\\n+                        '<value><name>foo</name></value>'\\\n+                        '<value><name>bar</name></value>'\\\n+                    '</age>'\\\n+                    '<name>buz</name>'\\\n+                '</item></items>'\n+        self.assertEqual(output.getvalue(), expected_value)\n+\n class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n     _expected_nested = {'name': u'Jesus', 'age': {'name': 'Maria', 'age': {'name': 'Joseph', 'age': '22'}}}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#95fde0a4987acaa75a6749223c8b7f9bd7081c23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 63 | Churn Cumulative: 342 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,14 +4,15 @@ for scraping from an XML feed.\n \n See documentation in docs/topics/spiders.rst\n \"\"\"\n-\n from scrapy.spider import BaseSpider\n from scrapy.item import BaseItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n+from scrapy.utils.spider import iterate_spider_output\n from scrapy.selector import XmlXPathSelector, HtmlXPathSelector\n from scrapy.exceptions import NotConfigured, NotSupported\n \n+\n class XMLFeedSpider(BaseSpider):\n     \"\"\"\n     This class intends to be the base class for spiders that scrape\n@@ -58,11 +59,7 @@ class XMLFeedSpider(BaseSpider):\n         \"\"\"\n \n         for selector in nodes:\n-            ret = self.parse_node(response, selector)\n-            if isinstance(ret, (BaseItem, Request)):\n-                ret = [ret]\n-            if not isinstance(ret, (list, tuple)):\n-                raise TypeError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n+            ret = iterate_spider_output(self.parse_node(response, selector))\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n \n@@ -72,7 +69,7 @@ class XMLFeedSpider(BaseSpider):\n \n         response = self.adapt_response(response)\n         if self.iterator == 'iternodes':\n-            nodes = xmliter(response, self.itertag)\n+            nodes = self._iternodes(response)\n         elif self.iterator == 'xml':\n             selector = XmlXPathSelector(response)\n             self._register_namespaces(selector)\n@@ -86,6 +83,11 @@ class XMLFeedSpider(BaseSpider):\n \n         return self.parse_nodes(response, nodes)\n \n+    def _iternodes(self, response):\n+        for node in xmliter(response, self.itertag):\n+            self._register_namespaces(node)\n+            yield node\n+\n     def _register_namespaces(self, selector):\n         for (prefix, uri) in self.namespaces:\n             selector.register_namespace(prefix, uri)\n\n@@ -1,4 +1,6 @@\n-import gzip, warnings, inspect\n+import gzip\n+import inspect\n+import warnings\n from cStringIO import StringIO\n \n from twisted.trial import unittest\n@@ -45,18 +47,61 @@ class InitSpiderTest(BaseSpiderTest):\n \n     spider_class = InitSpider\n \n+\n class XMLFeedSpiderTest(BaseSpiderTest):\n \n     spider_class = XMLFeedSpider\n \n+    def test_register_namespace(self):\n+        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        <urlset xmlns:x=\"http://www.google.com/schemas/sitemap/0.84\"\n+                xmlns:y=\"http://www.example.com/schemas/extras/1.0\">\n+        <url><x:loc>http://www.example.com/Special-Offers.html</loc><y:updated>2009-08-16</updated><other value=\"bar\" y:custom=\"fuu\"/></url>\n+        <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</updated><other value=\"foo\"/></url>\n+        </urlset>\"\"\"\n+        response = XmlResponse(url='http://example.com/sitemap.xml', body=body)\n+\n+        class _XMLSpider(self.spider_class):\n+            itertag = 'url'\n+            namespaces = (\n+                ('a', 'http://www.google.com/schemas/sitemap/0.84'),\n+                ('b', 'http://www.example.com/schemas/extras/1.0'),\n+            )\n+\n+            def parse_node(self, response, selector):\n+                yield {\n+                    'loc': selector.select('a:loc/text()').extract(),\n+                    'updated': selector.select('b:updated/text()').extract(),\n+                    'other': selector.select('other/@value').extract(),\n+                    'custom': selector.select('other/@b:custom').extract(),\n+                }\n+\n+        for iterator in ('iternodes', 'xml'):\n+            spider = _XMLSpider('example', iterator=iterator)\n+            output = list(spider.parse(response))\n+            self.assertEqual(len(output), 2, iterator)\n+            self.assertEqual(output, [\n+                {'loc': [u'http://www.example.com/Special-Offers.html'],\n+                 'updated': [u'2009-08-16'],\n+                 'custom': [u'fuu'],\n+                 'other': [u'bar']},\n+                {'loc': [],\n+                 'updated': [u'2009-08-16'],\n+                 'other': [u'foo'],\n+                 'custom': []},\n+            ], iterator)\n+\n+\n class CSVFeedSpiderTest(BaseSpiderTest):\n \n     spider_class = CSVFeedSpider\n \n+\n class CrawlSpiderTest(BaseSpiderTest):\n \n     spider_class = CrawlSpider\n \n+\n class SitemapSpiderTest(BaseSpiderTest):\n \n     spider_class = SitemapSpider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc69b3aa4c88c60ca41a792a60f7e80d75f1b7f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 472 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -115,13 +115,15 @@ class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n         base_url = None\n         if self.restrict_xpaths:\n             hxs = HtmlXPathSelector(response)\n-            html = ''.join(''.join(html_fragm for html_fragm in hxs.select(xpath_expr).extract()) \\\n-                for xpath_expr in self.restrict_xpaths)\n             base_url = get_base_url(response)\n+            body = u''.join(f\n+                            for x in self.restrict_xpaths\n+                            for f in hxs.select(x).extract()\n+                            ).encode(response.encoding)\n         else:\n-            html = response.body\n+            body = response.body\n \n-        links = self._extract_links(html, response.url, response.encoding, base_url)\n+        links = self._extract_links(body, response.url, response.encoding, base_url)\n         links = self._process_links(links)\n         return links\n \n\n@@ -217,6 +217,15 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://example.org/about.html', text=u'About us\\xa3')])\n \n+    def test_restrict_xpaths_concat_in_handle_data(self):\n+        \"\"\"html entities cause SGMLParser to call handle_data hook twice\"\"\"\n+        body = \"\"\"<html><body><div><a href=\"/foo\">&gt;\\xbe\\xa9&lt;\\xb6\\xab</a></body></html>\"\"\"\n+        response = HtmlResponse(\"http://example.org\", body=body, encoding='gb18030')\n+        lx = SgmlLinkExtractor(restrict_xpaths=\"//div\")\n+        self.assertEqual(lx.extract_links(response),\n+                         [Link(url='http://example.org/foo', text=u'>\\u4eac<\\u4e1c',\n+                               fragment='', nofollow=False)])\n+\n     def test_deny_extensions(self):\n         html = \"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n         response = HtmlResponse(\"http://example.org/\", body=html)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#872a22df68ab2e776c67b11006098d2d10e5e4c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 53 | Files Changed: 2 | Hunks: 31 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 125 | Churn Cumulative: 597 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 0.7058823529411765\n\nDIFF:\n@@ -1,12 +1,9 @@\n \"\"\"\n SGMLParser-based Link extractors\n \"\"\"\n-\n import re\n from urlparse import urlparse, urljoin\n-\n from w3lib.url import safe_url_string\n-\n from scrapy.selector import HtmlXPathSelector\n from scrapy.link import Link\n from scrapy.linkextractor import IGNORED_EXTENSIONS\n@@ -15,6 +12,7 @@ from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_u\n from scrapy.utils.url import canonicalize_url, url_is_from_any_domain, url_has_any_extension\n from scrapy.utils.response import get_base_url\n \n+\n class BaseSgmlLinkExtractor(FixedSGMLParser):\n \n     def __init__(self, tag=\"a\", attr=\"href\", unique=False, process_value=None):\n@@ -92,6 +90,7 @@ _re_type = type(re.compile(\"\", 0))\n _matches = lambda url, regexs: any((r.search(url) for r in regexs))\n _is_valid_url = lambda url: url.split('://', 1)[0] in set(['http', 'https', 'file'])\n \n+\n class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n@@ -108,8 +107,11 @@ class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n         self.deny_extensions = set(['.' + e for e in deny_extensions])\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n-        BaseSgmlLinkExtractor.__init__(self, tag=tag_func, attr=attr_func, \n-            unique=unique, process_value=process_value)\n+        BaseSgmlLinkExtractor.__init__(self,\n+                                       tag=tag_func,\n+                                       attr=attr_func,\n+                                       unique=unique,\n+                                       process_value=process_value)\n \n     def extract_links(self, response):\n         base_url = None\n\n@@ -1,11 +1,11 @@\n import re\n import unittest\n-\n from scrapy.http import HtmlResponse\n from scrapy.link import Link\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor, BaseSgmlLinkExtractor\n from scrapy.tests import get_testdata\n \n+\n class LinkExtractorTestCase(unittest.TestCase):\n     def test_basic(self):\n         html = \"\"\"<html><head><title>Page title<title>\n@@ -54,8 +54,9 @@ class LinkExtractorTestCase(unittest.TestCase):\n         html = \"\"\"<body><p><a href=\"item/12.html\">Wrong: \\xed</a></p></body></html>\"\"\"\n         response = HtmlResponse(\"http://www.example.com\", body=html, encoding='utf-8')\n         lx = BaseSgmlLinkExtractor()\n-        self.assertEqual(lx.extract_links(response),\n-             [Link(url='http://www.example.com/item/12.html', text=u'Wrong: \\ufffd')])\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://www.example.com/item/12.html', text=u'Wrong: \\ufffd'),\n+        ])\n \n     def test_extraction_encoding(self):\n         body = get_testdata('link_extractor', 'linkextractor_noenc.html')\n@@ -65,17 +66,20 @@ class LinkExtractorTestCase(unittest.TestCase):\n         response_latin1 = HtmlResponse(url='http://example.com/latin1', body=body)\n \n         lx = BaseSgmlLinkExtractor()\n-        self.assertEqual(lx.extract_links(response_utf8),\n-            [ Link(url='http://example.com/sample_%C3%B1.html', text=''),\n-              Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')) ])\n+        self.assertEqual(lx.extract_links(response_utf8), [\n+            Link(url='http://example.com/sample_%C3%B1.html', text=''),\n+            Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')),\n+        ])\n \n-        self.assertEqual(lx.extract_links(response_noenc),\n-            [ Link(url='http://example.com/sample_%C3%B1.html', text=''),\n-              Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')) ])\n+        self.assertEqual(lx.extract_links(response_noenc), [\n+            Link(url='http://example.com/sample_%C3%B1.html', text=''),\n+            Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')),\n+        ])\n \n-        self.assertEqual(lx.extract_links(response_latin1),\n-            [ Link(url='http://example.com/sample_%F1.html', text=''),\n-              Link(url='http://example.com/sample_%E1.html', text='sample \\xe1 text'.decode('latin1')) ])\n+        self.assertEqual(lx.extract_links(response_latin1), [\n+            Link(url='http://example.com/sample_%F1.html', text=''),\n+            Link(url='http://example.com/sample_%E1.html', text='sample \\xe1 text'.decode('latin1')),\n+        ])\n \n     def test_matches(self):\n         url1 = 'http://lotsofstuff.com/stuff1/index'\n@@ -92,9 +96,11 @@ class LinkExtractorTestCase(unittest.TestCase):\n         \"\"\"\n         response = HtmlResponse(\"http://example.org/page.html\", body=html)\n         lx = SgmlLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(response)],\n-            [ Link(url='http://example.org/page.html?action=print', text=u'Printer-friendly page', nofollow=True),\n-              Link(url='http://example.org/about.html', text=u'About us', nofollow=False) ])\n+        self.assertEqual([link for link in lx.extract_links(response)], [\n+            Link(url='http://example.org/page.html?action=print', text=u'Printer-friendly page', nofollow=True),\n+            Link(url='http://example.org/about.html', text=u'About us', nofollow=False),\n+        ])\n+\n \n class SgmlLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n@@ -110,62 +116,71 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         '''Test the extractor's behaviour among different situations'''\n \n         lx = SgmlLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text=u''),\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://example.com/sample1.html', text=u''),\n             Link(url='http://example.com/sample2.html', text=u'sample 2'),\n             Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n-              Link(url='http://www.google.com/something', text=u'') ])\n+            Link(url='http://www.google.com/something', text=u''),\n+        ])\n \n         lx = SgmlLinkExtractor(allow=('sample', ))\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text=u''),\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://example.com/sample1.html', text=u''),\n             Link(url='http://example.com/sample2.html', text=u'sample 2'),\n-              Link(url='http://example.com/sample3.html', text=u'sample 3 text') ])\n+            Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+        ])\n \n         lx = SgmlLinkExtractor(allow=('sample', ), unique=False)\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text=u''),\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://example.com/sample1.html', text=u''),\n             Link(url='http://example.com/sample2.html', text=u'sample 2'),\n             Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n-              Link(url='http://example.com/sample3.html', text=u'sample 3 repetition') ])\n+            Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n+        ])\n \n         lx = SgmlLinkExtractor(allow=('sample', ))\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text=u''),\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://example.com/sample1.html', text=u''),\n             Link(url='http://example.com/sample2.html', text=u'sample 2'),\n             Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n         ])\n \n         lx = SgmlLinkExtractor(allow=('sample', ), deny=('3', ))\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text=u''),\n-              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://example.com/sample1.html', text=u''),\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+        ])\n \n         lx = SgmlLinkExtractor(allow_domains=('google.com', ))\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://www.google.com/something', text=u'') ])\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://www.google.com/something', text=u''),\n+        ])\n \n     def test_extraction_using_single_values(self):\n         '''Test the extractor's behaviour among different situations'''\n \n         lx = SgmlLinkExtractor(allow='sample')\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text=u''),\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://example.com/sample1.html', text=u''),\n             Link(url='http://example.com/sample2.html', text=u'sample 2'),\n-              Link(url='http://example.com/sample3.html', text=u'sample 3 text') ])\n+            Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+        ])\n \n         lx = SgmlLinkExtractor(allow='sample', deny='3')\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text=u''),\n-              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://example.com/sample1.html', text=u''),\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+        ])\n \n         lx = SgmlLinkExtractor(allow_domains='google.com')\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://www.google.com/something', text=u'') ])\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://www.google.com/something', text=u''),\n+        ])\n \n         lx = SgmlLinkExtractor(deny_domains='example.com')\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://www.google.com/something', text=u'') ])\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://www.google.com/something', text=u''),\n+        ])\n \n     def test_matches(self):\n         url1 = 'http://lotsofstuff.com/stuff1/index'\n@@ -188,7 +203,8 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.matches(url2), True)\n \n         lx = SgmlLinkExtractor(allow=('blah1',), deny=('blah2',),\n-            allow_domains=('blah1.com', ), deny_domains=('blah2.com', ))\n+                               allow_domains=('blah1.com',),\n+                               deny_domains=('blah2.com',))\n         self.assertEqual(lx.matches('http://blah1.com/blah1'), True)\n         self.assertEqual(lx.matches('http://blah1.com/blah2'), False)\n         self.assertEqual(lx.matches('http://blah2.com/blah1'), False)\n@@ -196,9 +212,10 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n \n     def test_restrict_xpaths(self):\n         lx = SgmlLinkExtractor(restrict_xpaths=('//div[@id=\"subwrapper\"]', ))\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text=u''),\n-              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://example.com/sample1.html', text=u''),\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+        ])\n \n     def test_restrict_xpaths_encoding(self):\n         \"\"\"Test restrict_xpaths with encodings\"\"\"\n@@ -230,8 +247,9 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         html = \"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n         response = HtmlResponse(\"http://example.org/\", body=html)\n         lx = SgmlLinkExtractor()\n-        self.assertEqual(lx.extract_links(response),\n-            [Link(url='http://example.org/page.html', text=u'asd')])\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://example.org/page.html', text=u'asd'),\n+        ])\n \n     def test_process_value(self):\n         \"\"\"Test restrict_xpaths with encodings\"\"\"\n@@ -260,6 +278,5 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n \n-\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d3e4b4c43711c4b02db6813ebad99933ee8858d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 31 | Churn Cumulative: 1080 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -243,6 +243,22 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://example.org/foo', text=u'>\\u4eac<\\u4e1c',\n                                fragment='', nofollow=False)])\n \n+    def test_encoded_url(self):\n+        body = \"\"\"<html><body><div><a href=\"?page=2\">BinB</a></body></html>\"\"\"\n+        response = HtmlResponse(\"http://known.fm/AC%2FDC/\", body=body, encoding='utf8')\n+        lx = SgmlLinkExtractor()\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://known.fm/AC%2FDC/?page=2', text=u'BinB', fragment='', nofollow=False),\n+        ])\n+\n+    def test_encoded_url_in_restricted_xpath(self):\n+        body = \"\"\"<html><body><div><a href=\"?page=2\">BinB</a></body></html>\"\"\"\n+        response = HtmlResponse(\"http://known.fm/AC%2FDC/\", body=body, encoding='utf8')\n+        lx = SgmlLinkExtractor(restrict_xpaths=\"//div\")\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://known.fm/AC%2FDC/?page=2', text=u'BinB', fragment='', nofollow=False),\n+        ])\n+\n     def test_deny_extensions(self):\n         html = \"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n         response = HtmlResponse(\"http://example.org/\", body=html)\n\n@@ -151,6 +151,12 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(\"http://www.EXAMPLE.com/\"),\n                                           \"http://www.example.com/\")\n \n+        # quoted slash and question sign\n+        self.assertEqual(canonicalize_url(\"http://foo.com/AC%2FDC+rocks%3f/?yeah=1\"),\n+                         \"http://foo.com/AC%2FDC+rocks%3F/?yeah=1\")\n+        self.assertEqual(canonicalize_url(\"http://foo.com/AC%2FDC/\"),\n+                         \"http://foo.com/AC%2FDC/\")\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -52,10 +52,17 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n     keyvals = cgi.parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urllib.urlencode(keyvals)\n-    path = safe_url_string(urllib.unquote(path)) or '/'\n+    path = safe_url_string(_unquotepath(path)) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlparse.urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n \n+\n+def _unquotepath(path):\n+    for reserved in ('2f', '2F', '3f', '3F'):\n+        path = path.replace('%' + reserved, '%25' + reserved.upper())\n+    return urllib.unquote(path)\n+\n+\n def parse_url(url, encoding=None):\n     \"\"\"Return urlparsed url from the given argument (which could be an already\n     parsed url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2602df90b6954b6e764ab2b49d897fc2cdb0457", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 684 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -163,7 +163,8 @@ class DbmCacheStorage(object):\n     def __init__(self, settings):\n         self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n-        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'])\n+        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'],\n+                                   {}, {}, [''])\n         self.db = None\n \n     def open_spider(self, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ed44a3a67e6e11063a53be9c0a8684cf2f688b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 40 | Churn Cumulative: 1706 | Contributors (this commit): 7 | Commits (past 90d): 18 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -163,8 +163,7 @@ class DbmCacheStorage(object):\n     def __init__(self, settings):\n         self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n-        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'],\n-                                   {}, {}, [''])\n+        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'], {}, {}, [''])\n         self.db = None\n \n     def open_spider(self, spider):\n\n\n@@ -0,0 +1,23 @@\n+\"\"\"DBM-like dummy module\"\"\"\n+import collections\n+\n+\n+class DummyDB(dict):\n+    \"\"\"Provide dummy DBM-like interface.\"\"\"\n+    def close(self):\n+        pass\n+\n+\n+class error(Exception):\n+    pass\n+\n+\n+_DATABASES = collections.defaultdict(DummyDB)\n+\n+def open(file, flag='r', mode=0666):\n+    \"\"\"Open or create a dummy database compatible.\n+\n+    Arguments `flag` and `mode` are ignored.\n+    \"\"\"\n+    # return same instance for same file argument\n+    return _DATABASES[file]\n\n@@ -116,6 +116,20 @@ class DbmStorageTest(DefaultStorageTest):\n     storage_class = 'scrapy.contrib.httpcache.DbmCacheStorage'\n \n \n+class DbmStorageWithCustomDbmModuleTest(DbmStorageTest):\n+\n+    dbm_module = 'scrapy.tests.mocks.dummydbm'\n+\n+    def _get_settings(self, **new_settings):\n+        new_settings.setdefault('HTTPCACHE_DBM_MODULE', self.dbm_module)\n+        return super(DbmStorageWithCustomDbmModuleTest, self)._get_settings(**new_settings)\n+\n+    def test_custom_dbm_module_loaded(self):\n+        # make sure our dbm module has been loaded\n+        with self._storage() as storage:\n+            self.assertEqual(storage.dbmodule.__name__, self.dbm_module)\n+\n+\n class FilesystemStorageTest(DefaultStorageTest):\n \n     storage_class = 'scrapy.contrib.httpcache.FilesystemCacheStorage'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d00e6cf1d7e7c2d8bb6c9822f6094e164a8478e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 26 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -8,8 +8,7 @@ class DummyDB(dict):\n         pass\n \n \n-class error(Exception):\n-    pass\n+error = KeyError\n \n \n _DATABASES = collections.defaultdict(DummyDB)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5db45b382522be9f9b77009892ec304fb2d83018", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1610 | Files Changed: 29 | Hunks: 29 | Methods Changed: 147 | Complexity Δ (Sum/Max): -245/0 | Churn Δ: 1613 | Churn Cumulative: 3748 | Contributors (this commit): 9 | Commits (past 90d): 32 | Contributors (cumulative): 68 | DMM Complexity: 0.06494845360824743\n\nDIFF:\n@@ -2,7 +2,7 @@ import sys, os, glob, shutil\n from subprocess import check_call\n \n def build(suffix):\n-    for ifn in glob.glob(\"debian/scrapy.*\") + glob.glob(\"debian/scrapyd.*\"):\n+    for ifn in glob.glob(\"debian/scrapy.*\"):\n         s = open(ifn).read()\n         s = s.replace('SUFFIX', suffix)\n         pre, suf = ifn.split('.', 1)\n@@ -21,8 +21,7 @@ def build(suffix):\n     check_call('debuild -us -uc -b', shell=True)\n \n def clean(suffix):\n-    for f in glob.glob(\"debian/python-scrapy%s*\" % suffix) + \\\n-            glob.glob(\"debian/scrapyd%s*\" % suffix):\n+    for f in glob.glob(\"debian/python-scrapy%s*\" % suffix):\n         if os.path.isdir(f):\n             shutil.rmtree(f)\n         else:\n\n@@ -1,22 +0,0 @@\n-from __future__ import absolute_import\n-\n-from scrapy.command import ScrapyCommand\n-from scrapy.exceptions import UsageError\n-\n-class Command(ScrapyCommand):\n-\n-    requires_project = True\n-\n-    def short_desc(self):\n-        return \"Start Scrapyd server for this project\"\n-\n-    def long_desc(self):\n-        return \"Start Scrapyd server for this project, which can be referred \" \\\n-            \"from the JSON API with the name 'default'\"\n-\n-    def run(self, args, opts):\n-        try:\n-            from scrapyd.script import execute\n-            execute()\n-        except ImportError:\n-            raise UsageError(\"Scrapyd is not available in this system\")\n\n@@ -1,9 +0,0 @@\n-from scrapy.utils.misc import load_object\n-from .config import Config\n-\n-def get_application(config=None):\n-    if config is None:\n-        config = Config()\n-    apppath = config.get('application', 'scrapyd.app.application')\n-    appfunc = load_object(apppath)\n-    return appfunc(config)\n\n@@ -1,44 +0,0 @@\n-from twisted.application.service import Application\n-from twisted.application.internet import TimerService, TCPServer\n-from twisted.web import server\n-from twisted.python import log\n-\n-from scrapy.utils.misc import load_object\n-\n-from .interfaces import IEggStorage, IPoller, ISpiderScheduler, IEnvironment\n-from .eggstorage import FilesystemEggStorage\n-from .scheduler import SpiderScheduler\n-from .poller import QueuePoller\n-from .environ import Environment\n-from .website import Root\n-from .config import Config\n-\n-def application(config):\n-    app = Application(\"Scrapyd\")\n-    http_port = config.getint('http_port', 6800)\n-    bind_address = config.get('bind_address', '0.0.0.0')\n-\n-    poller = QueuePoller(config)\n-    eggstorage = FilesystemEggStorage(config)\n-    scheduler = SpiderScheduler(config)\n-    environment = Environment(config)\n-\n-    app.setComponent(IPoller, poller)\n-    app.setComponent(IEggStorage, eggstorage)\n-    app.setComponent(ISpiderScheduler, scheduler)\n-    app.setComponent(IEnvironment, environment)\n-\n-    laupath = config.get('launcher', 'scrapyd.launcher.Launcher')\n-    laucls = load_object(laupath)\n-    launcher = laucls(config, app)\n-\n-    timer = TimerService(5, poller.poll)\n-    webservice = TCPServer(http_port, server.Site(Root(config, app)), interface=bind_address)\n-    log.msg(format=\"Scrapyd web console available at http://%(bind_address)s:%(http_port)s/\",\n-            bind_address=bind_address, http_port=http_port)\n-\n-    launcher.setServiceParent(app)\n-    timer.setServiceParent(app)\n-    webservice.setServiceParent(app)\n-\n-    return app\n\n@@ -1,62 +0,0 @@\n-import glob\n-from cStringIO import StringIO\n-from pkgutil import get_data\n-from ConfigParser import SafeConfigParser, NoSectionError, NoOptionError\n-\n-from scrapy.utils.conf import closest_scrapy_cfg\n-\n-class Config(object):\n-    \"\"\"A ConfigParser wrapper to support defaults when calling instance\n-    methods, and also tied to a single section\"\"\"\n-\n-    SECTION = 'scrapyd'\n-\n-    def __init__(self, values=None, extra_sources=()):\n-        if values is None:\n-            sources = self._getsources()\n-            default_config = get_data(__package__, 'default_scrapyd.conf')\n-            self.cp = SafeConfigParser()\n-            self.cp.readfp(StringIO(default_config))\n-            self.cp.read(sources)\n-            for fp in extra_sources:\n-                self.cp.readfp(fp)\n-        else:\n-            self.cp = SafeConfigParser(values)\n-            self.cp.add_section(self.SECTION)\n-\n-    def _getsources(self):\n-        sources = ['/etc/scrapyd/scrapyd.conf', r'c:\\scrapyd\\scrapyd.conf']\n-        sources += sorted(glob.glob('/etc/scrapyd/conf.d/*'))\n-        sources += ['scrapyd.conf']\n-        scrapy_cfg = closest_scrapy_cfg()\n-        if scrapy_cfg:\n-            sources.append(scrapy_cfg)\n-        return sources\n-\n-    def _getany(self, method, option, default):\n-        try:\n-            return method(self.SECTION, option)\n-        except (NoSectionError, NoOptionError):\n-            if default is not None:\n-                return default\n-            raise\n-\n-    def get(self, option, default=None):\n-        return self._getany(self.cp.get, option, default)\n-\n-    def getint(self, option, default=None):\n-        return self._getany(self.cp.getint, option, default)\n-\n-    def getfloat(self, option, default=None):\n-        return self._getany(self.cp.getfloat, option, default)\n-\n-    def getboolean(self, option, default=None):\n-        return self._getany(self.cp.getboolean, option, default)\n-\n-    def items(self, section, default=None):\n-        try:\n-            return self.cp.items(section)\n-        except (NoSectionError, NoOptionError):\n-            if default is not None:\n-                return default\n-            raise\n\n@@ -1,49 +0,0 @@\n-from glob import glob\n-from os import path, makedirs, remove\n-from shutil import copyfileobj, rmtree\n-from distutils.version import LooseVersion\n-\n-from zope.interface import implements\n-\n-from .interfaces import IEggStorage\n-\n-class FilesystemEggStorage(object):\n-\n-    implements(IEggStorage)\n-\n-    def __init__(self, config):\n-        self.basedir = config.get('eggs_dir', 'eggs')\n-\n-    def put(self, eggfile, project, version):\n-        eggpath = self._eggpath(project, version)\n-        eggdir = path.dirname(eggpath)\n-        if not path.exists(eggdir):\n-            makedirs(eggdir)\n-        with open(eggpath, 'wb') as f:\n-            copyfileobj(eggfile, f)\n-\n-    def get(self, project, version=None):\n-        if version is None:\n-            try:\n-                version = self.list(project)[-1]\n-            except IndexError:\n-                return None, None\n-        return version, open(self._eggpath(project, version), 'rb')\n-\n-    def list(self, project):\n-        eggdir = path.join(self.basedir, project)\n-        versions = [path.splitext(path.basename(x))[0] \\\n-            for x in glob(\"%s/*.egg\" % eggdir)]\n-        return sorted(versions, key=LooseVersion)\n-\n-    def delete(self, project, version=None):\n-        if version is None:\n-            rmtree(path.join(self.basedir, project))\n-        else:\n-            remove(self._eggpath(project, version))\n-            if not self.list(project): # remove project if no versions left\n-                self.delete(project)\n-\n-    def _eggpath(self, project, version):\n-        x = path.join(self.basedir, project, \"%s.egg\" % version)\n-        return x\n\n@@ -1,14 +0,0 @@\n-import os, pkg_resources\n-\n-def activate_egg(eggpath):\n-    \"\"\"Activate a Scrapy egg file. This is meant to be used from egg runners\n-    to activate a Scrapy egg file. Don't use it from other code as it may\n-    leave unwanted side effects.\n-    \"\"\"\n-    try:\n-        d = pkg_resources.find_distributions(eggpath).next()\n-    except StopIteration:\n-        raise ValueError(\"Unknown or corrupt egg\")\n-    d.activate()\n-    settings_module = d.get_entry_info('scrapy', 'settings').module_name\n-    os.environ.setdefault('SCRAPY_SETTINGS_MODULE', settings_module)\n\n@@ -1,46 +0,0 @@\n-import os\n-\n-from zope.interface import implements\n-\n-from .interfaces import IEnvironment\n-\n-class Environment(object):\n-\n-    implements(IEnvironment)\n-\n-    def __init__(self, config, initenv=os.environ):\n-        self.dbs_dir = config.get('dbs_dir', 'dbs')\n-        self.logs_dir = config.get('logs_dir', 'logs')\n-        self.items_dir = config.get('items_dir', 'items')\n-        self.jobs_to_keep = config.getint('jobs_to_keep', 5)\n-        if config.cp.has_section('settings'):\n-            self.settings = dict(config.cp.items('settings'))\n-        else:\n-            self.settings = {}\n-        self.initenv = initenv\n-\n-    def get_environment(self, message, slot):\n-        project = message['_project']\n-        env = self.initenv.copy()\n-        env['SCRAPY_SLOT'] = str(slot)\n-        env['SCRAPY_PROJECT'] = project\n-        env['SCRAPY_SPIDER'] = message['_spider']\n-        env['SCRAPY_JOB'] = message['_job']\n-        if project in self.settings:\n-            env['SCRAPY_SETTINGS_MODULE'] = self.settings[project]\n-        if self.logs_dir:\n-            env['SCRAPY_LOG_FILE'] = self._get_file(message, self.logs_dir, 'log')\n-        if self.items_dir:\n-            env['SCRAPY_FEED_URI'] = self._get_file(message, self.items_dir, 'jl')\n-        return env\n-\n-    def _get_file(self, message, dir, ext):\n-        logsdir = os.path.join(dir, message['_project'], \\\n-            message['_spider'])\n-        if not os.path.exists(logsdir):\n-            os.makedirs(logsdir)\n-        to_delete = sorted((os.path.join(logsdir, x) for x in \\\n-            os.listdir(logsdir)), key=os.path.getmtime)[:-self.jobs_to_keep]\n-        for x in to_delete:\n-            os.remove(x)\n-        return os.path.join(logsdir, \"%s.%s\" % (message['_job'], ext))\n\n@@ -1,109 +0,0 @@\n-from zope.interface import Interface\n-\n-class IEggStorage(Interface):\n-    \"\"\"A component that handles storing and retrieving eggs\"\"\"\n-\n-    def put(eggfile, project, version):\n-        \"\"\"Store the egg (passed in the file object) under the given project and\n-        version\"\"\"\n-\n-    def get(project, version=None):\n-        \"\"\"Return a tuple (version, file) with the the egg for the specified\n-        project and version. If version is None, the latest version is\n-        returned. If no egg is found for the given project/version (None, None)\n-        should be returned.\"\"\"\n-\n-    def list(project):\n-        \"\"\"Return the list of versions which have eggs stored (for the given\n-        project) in order (the latest version is the currently used).\"\"\"\n-\n-    def delete(project, version=None):\n-        \"\"\"Delete the egg stored for the given project and version. If should\n-        also delete the project if no versions are left\"\"\"\n-\n-\n-class IPoller(Interface):\n-    \"\"\"A component that polls for projects that need to run\"\"\"\n-\n-    def poll():\n-        \"\"\"Called periodically to poll for projects\"\"\"\n-\n-    def next():\n-        \"\"\"Return the next message.\n-\n-        It should return a Deferred which will get fired when there is a new\n-        project that needs to run, or already fired if there was a project\n-        waiting to run already.\n-\n-        The message is a dict containing (at least):\n-        * the name of the project to be run in the '_project' key\n-        * the name of the spider to be run in the '_spider' key\n-        * a unique identifier for this run in the `_job` key\n-        This message will be passed later to IEnvironment.get_environment().\n-        \"\"\"\n-\n-    def update_projects():\n-        \"\"\"Called when projects may have changed, to refresh the available\n-        projects\"\"\"\n-\n-\n-class ISpiderQueue(Interface):\n-\n-    def add(name, **spider_args):\n-        \"\"\"Add a spider to the queue given its name a some spider arguments.\n-\n-        This method can return a deferred. \"\"\"\n-\n-    def pop():\n-        \"\"\"Pop the next mesasge from the queue. The messages is a dict\n-        conaining a key 'name' with the spider name and other keys as spider\n-        attributes.\n-\n-        This method can return a deferred. \"\"\"\n-\n-    def list():\n-        \"\"\"Return a list with the messages in the queue. Each message is a dict\n-        which must have a 'name' key (with the spider name), and other optional\n-        keys that will be used as spider arguments, to create the spider.\n-\n-        This method can return a deferred. \"\"\"\n-\n-    def count():\n-        \"\"\"Return the number of spiders in the queue.\n-\n-        This method can return a deferred. \"\"\"\n-\n-    def remove(func):\n-        \"\"\"Remove all elements from the queue for which func(element) is true,\n-        and return the number of removed elements.\n-        \"\"\"\n-\n-    def clear():\n-        \"\"\"Clear the queue.\n-\n-        This method can return a deferred. \"\"\"\n-\n-\n-class ISpiderScheduler(Interface):\n-    \"\"\"A component to schedule spider runs\"\"\"\n-\n-    def schedule(project, spider_name, **spider_args):\n-        \"\"\"Schedule a spider for the given project\"\"\"\n-\n-    def list_projects():\n-        \"\"\"Return the list of available projects\"\"\"\n-\n-    def update_projects():\n-        \"\"\"Called when projects may have changed, to refresh the available\n-        projects\"\"\"\n-\n-\n-class IEnvironment(Interface):\n-    \"\"\"A component to generate the environment of crawler processes\"\"\"\n-\n-    def get_environment(message, slot):\n-        \"\"\"Return the environment variables to use for running the process.\n-\n-        `message` is the message received from the IPoller.next() method\n-        `slot` is the Launcher slot where the process will be running.\n-        \"\"\"\n\n@@ -1,102 +0,0 @@\n-import sys\n-from datetime import datetime\n-from multiprocessing import cpu_count\n-\n-from twisted.internet import reactor, defer, protocol, error\n-from twisted.application.service import Service\n-from twisted.python import log\n-\n-from scrapy.utils.python import stringify_dict\n-from scrapyd.utils import get_crawl_args\n-from .interfaces import IPoller, IEnvironment\n-\n-class Launcher(Service):\n-\n-    name = 'launcher'\n-\n-    def __init__(self, config, app):\n-        self.processes = {}\n-        self.finished = []\n-        self.finished_to_keep = config.getint('finished_to_keep', 100)\n-        self.max_proc = self._get_max_proc(config)\n-        self.runner = config.get('runner', 'scrapyd.runner')\n-        self.app = app\n-\n-    def startService(self):\n-        for slot in range(self.max_proc):\n-            self._wait_for_project(slot)\n-        log.msg(format='%(parent)s started: max_proc=%(max_proc)r, runner=%(runner)r',\n-                parent=self.parent.name, max_proc=self.max_proc,\n-                runner=self.runner, system='Launcher')\n-\n-    def _wait_for_project(self, slot):\n-        poller = self.app.getComponent(IPoller)\n-        poller.next().addCallback(self._spawn_process, slot)\n-\n-    def _spawn_process(self, message, slot):\n-        msg = stringify_dict(message, keys_only=False)\n-        project = msg['_project']\n-        args = [sys.executable, '-m', self.runner, 'crawl']\n-        args += get_crawl_args(msg)\n-        e = self.app.getComponent(IEnvironment)\n-        env = e.get_environment(msg, slot)\n-        env = stringify_dict(env, keys_only=False)\n-        pp = ScrapyProcessProtocol(slot, project, msg['_spider'], \\\n-            msg['_job'], env)\n-        pp.deferred.addBoth(self._process_finished, slot)\n-        reactor.spawnProcess(pp, sys.executable, args=args, env=env)\n-        self.processes[slot] = pp\n-\n-    def _process_finished(self, _, slot):\n-        process = self.processes.pop(slot)\n-        process.end_time = datetime.now()\n-        self.finished.append(process)\n-        del self.finished[:-self.finished_to_keep] # keep last 100 finished jobs\n-        self._wait_for_project(slot)\n-\n-    def _get_max_proc(self, config):\n-        max_proc = config.getint('max_proc', 0)\n-        if not max_proc:\n-            try:\n-                cpus = cpu_count()\n-            except NotImplementedError:\n-                cpus = 1\n-            max_proc = cpus * config.getint('max_proc_per_cpu', 4)\n-        return max_proc\n-\n-class ScrapyProcessProtocol(protocol.ProcessProtocol):\n-\n-    def __init__(self, slot, project, spider, job, env):\n-        self.slot = slot\n-        self.pid = None\n-        self.project = project\n-        self.spider = spider\n-        self.job = job\n-        self.start_time = datetime.now()\n-        self.end_time = None\n-        self.env = env\n-        self.logfile = env.get('SCRAPY_LOG_FILE')\n-        self.itemsfile = env.get('SCRAPY_FEED_URI')\n-        self.deferred = defer.Deferred()\n-\n-    def outReceived(self, data):\n-        log.msg(data.rstrip(), system=\"Launcher,%d/stdout\" % self.pid)\n-\n-    def errReceived(self, data):\n-        log.msg(data.rstrip(), system=\"Launcher,%d/stderr\" % self.pid)\n-\n-    def connectionMade(self):\n-        self.pid = self.transport.pid\n-        self.log(\"Process started: \")\n-\n-    def processEnded(self, status):\n-        if isinstance(status.value, error.ProcessDone):\n-            self.log(\"Process finished: \")\n-        else:\n-            self.log(\"Process died: exitstatus=%r \" % status.value.exitCode)\n-        self.deferred.callback(self)\n-\n-    def log(self, action):\n-        fmt = '%(action)s project=%(project)r spider=%(spider)r job=%(job)r pid=%(pid)r log=%(log)r items=%(items)r'\n-        log.msg(format=fmt, action=action, project=self.project, spider=self.spider,\n-                job=self.job, pid=self.pid, log=self.logfile, items=self.itemsfile)\n\n@@ -1,36 +0,0 @@\n-from zope.interface import implements\n-from twisted.internet.defer import DeferredQueue, inlineCallbacks, maybeDeferred, returnValue\n-\n-from .utils import get_spider_queues\n-from .interfaces import IPoller\n-\n-class QueuePoller(object):\n-\n-    implements(IPoller)\n-\n-    def __init__(self, config):\n-        self.config = config\n-        self.update_projects()\n-        self.dq = DeferredQueue(size=1)\n-\n-    @inlineCallbacks\n-    def poll(self):\n-        if self.dq.pending:\n-            return\n-        for p, q in self.queues.iteritems():\n-            c = yield maybeDeferred(q.count)\n-            if c:\n-                msg = yield maybeDeferred(q.pop)\n-                returnValue(self.dq.put(self._message(msg, p)))\n-\n-    def next(self):\n-        return self.dq.get()\n-\n-    def update_projects(self):\n-        self.queues = get_spider_queues(self.config)\n-\n-    def _message(self, queue_msg, project):\n-        d = queue_msg.copy()\n-        d['_project'] = project\n-        d['_spider'] = d.pop('name')\n-        return d\n\n@@ -1,39 +0,0 @@\n-import sys\n-import os\n-import shutil\n-import tempfile\n-from contextlib import contextmanager\n-\n-from scrapyd import get_application\n-from scrapyd.interfaces import IEggStorage\n-from scrapyd.eggutils import activate_egg\n-\n-@contextmanager\n-def project_environment(project):\n-    app = get_application()\n-    eggstorage = app.getComponent(IEggStorage)\n-    version, eggfile = eggstorage.get(project)\n-    if eggfile:\n-        prefix = '%s-%s-' % (project, version)\n-        fd, eggpath = tempfile.mkstemp(prefix=prefix, suffix='.egg')\n-        lf = os.fdopen(fd, 'wb')\n-        shutil.copyfileobj(eggfile, lf)\n-        lf.close()\n-        activate_egg(eggpath)\n-    else:\n-        eggpath = None\n-    try:\n-        assert 'scrapy.conf' not in sys.modules, \"Scrapy settings already loaded\"\n-        yield\n-    finally:\n-        if eggpath:\n-            os.remove(eggpath)\n-\n-def main():\n-    project = os.environ['SCRAPY_PROJECT']\n-    with project_environment(project):\n-        from scrapy.cmdline import execute\n-        execute()\n-\n-if __name__ == '__main__':\n-    main()\n\n@@ -1,22 +0,0 @@\n-from zope.interface import implements\n-\n-from .interfaces import ISpiderScheduler\n-from .utils import get_spider_queues\n-\n-class SpiderScheduler(object):\n-\n-    implements(ISpiderScheduler)\n-\n-    def __init__(self, config):\n-        self.config = config\n-        self.update_projects()\n-\n-    def schedule(self, project, spider_name, **spider_args):\n-        q = self.queues[project]\n-        q.add(spider_name, **spider_args)\n-\n-    def list_projects(self):\n-        return self.queues.keys()\n-\n-    def update_projects(self):\n-        self.queues = get_spider_queues(self.config)\n\n@@ -1,42 +0,0 @@\n-\"\"\"This module can be used to execute Scrapyd from a Scrapy command\"\"\"\n-\n-import sys\n-import os\n-from cStringIO import StringIO\n-\n-from twisted.python import log\n-from twisted.internet import reactor\n-from twisted.application import app\n-\n-from scrapy.utils.project import project_data_dir\n-\n-from scrapyd import get_application\n-from scrapyd.config import Config\n-\n-def _get_config():\n-    datadir = os.path.join(project_data_dir(), 'scrapyd')\n-    conf = {\n-        'eggs_dir': os.path.join(datadir, 'eggs'),\n-        'logs_dir': os.path.join(datadir, 'logs'),\n-        'items_dir': os.path.join(datadir, 'items'),\n-        'dbs_dir': os.path.join(datadir, 'dbs'),\n-    }\n-    for k in ['eggs_dir', 'logs_dir', 'items_dir', 'dbs_dir']: # create dirs\n-        d = conf[k]\n-        if not os.path.exists(d):\n-            os.makedirs(d)\n-    scrapyd_conf = \"\"\"\n-[scrapyd]\n-eggs_dir = %(eggs_dir)s\n-logs_dir = %(logs_dir)s\n-items_dir = %(items_dir)s\n-dbs_dir  = %(dbs_dir)s\n-    \"\"\" % conf\n-    return Config(extra_sources=[StringIO(scrapyd_conf)])\n-\n-def execute():\n-    config = _get_config()\n-    log.startLogging(sys.stderr)\n-    application = get_application(config)\n-    app.startApplication(application, False)\n-    reactor.run()\n\n@@ -1,33 +0,0 @@\n-from zope.interface import implements\n-\n-from scrapyd.interfaces import ISpiderQueue\n-from scrapyd.sqlite import JsonSqlitePriorityQueue\n-\n-\n-class SqliteSpiderQueue(object):\n-\n-    implements(ISpiderQueue)\n-\n-    def __init__(self, database=None, table='spider_queue'):\n-        self.q = JsonSqlitePriorityQueue(database, table)\n-\n-    def add(self, name, **spider_args):\n-        d = spider_args.copy()\n-        d['name'] = name\n-        priority = float(d.pop('priority', 0))\n-        self.q.put(d, priority)\n-\n-    def pop(self):\n-        return self.q.pop()\n-\n-    def count(self):\n-        return len(self.q)\n-\n-    def list(self):\n-        return [x[0] for x in self.q]\n-\n-    def remove(self, func):\n-        return self.q.remove(func)\n-\n-    def clear(self):\n-        self.q.clear()\n\n@@ -1,172 +0,0 @@\n-import sqlite3\n-import cPickle\n-import json\n-from UserDict import DictMixin\n-\n-\n-class SqliteDict(DictMixin):\n-    \"\"\"SQLite-backed dictionary\"\"\"\n-\n-    def __init__(self, database=None, table=\"dict\"):\n-        self.database = database or ':memory:'\n-        self.table = table\n-        # about check_same_thread: http://twistedmatrix.com/trac/ticket/4040\n-        self.conn = sqlite3.connect(self.database, check_same_thread=False)\n-        q = \"create table if not exists %s (key text primary key, value blob)\" \\\n-            % table\n-        self.conn.execute(q)\n-\n-    def __getitem__(self, key):\n-        key = self.encode(key)\n-        q = \"select value from %s where key=?\" % self.table\n-        value = self.conn.execute(q, (key,)).fetchone()\n-        if value:\n-            return self.decode(value[0])\n-        raise KeyError(key)\n-\n-    def __setitem__(self, key, value):\n-        key, value = self.encode(key), self.encode(value)\n-        q = \"insert or replace into %s (key, value) values (?,?)\" % self.table\n-        self.conn.execute(q, (key, value))\n-        self.conn.commit()\n-\n-    def __delitem__(self, key):\n-        key = self.encode(key)\n-        q = \"delete from %s where key=?\" % self.table\n-        self.conn.execute(q, (key,))\n-        self.conn.commit()\n-\n-    def iterkeys(self):\n-        q = \"select key from %s\" % self.table\n-        return (self.decode(x[0]) for x in self.conn.execute(q))\n-\n-    def keys(self):\n-        return list(self.iterkeys())\n-\n-    def itervalues(self):\n-        q = \"select value from %s\" % self.table\n-        return (self.decode(x[0]) for x in self.conn.execute(q))\n-\n-    def values(self):\n-        return list(self.itervalues())\n-\n-    def iteritems(self):\n-        q = \"select key, value from %s\" % self.table\n-        return ((self.decode(x[0]), self.decode(x[1])) for x in self.conn.execute(q))\n-\n-    def items(self):\n-        return list(self.iteritems())\n-\n-    def encode(self, obj):\n-        return obj\n-\n-    def decode(self, text):\n-        return text\n-\n-\n-class PickleSqliteDict(SqliteDict):\n-\n-    def encode(self, obj):\n-        return buffer(cPickle.dumps(obj, protocol=2))\n-\n-    def decode(self, text):\n-        return cPickle.loads(str(text))\n-\n-\n-class JsonSqliteDict(SqliteDict):\n-\n-    def encode(self, obj):\n-        return json.dumps(obj)\n-\n-    def decode(self, text):\n-        return json.loads(text)\n-\n-\n-\n-class SqlitePriorityQueue(object):\n-    \"\"\"SQLite priority queue. It relies on SQLite concurrency support for\n-    providing atomic inter-process operations.\n-    \"\"\"\n-\n-    def __init__(self, database=None, table=\"queue\"):\n-        self.database = database or ':memory:'\n-        self.table = table\n-        # about check_same_thread: http://twistedmatrix.com/trac/ticket/4040\n-        self.conn = sqlite3.connect(self.database, check_same_thread=False)\n-        q = \"create table if not exists %s (id integer primary key, \" \\\n-            \"priority real key, message blob)\" % table\n-        self.conn.execute(q)\n-\n-    def put(self, message, priority=0.0):\n-        args = (priority, self.encode(message))\n-        q = \"insert into %s (priority, message) values (?,?)\" % self.table\n-        self.conn.execute(q, args)\n-        self.conn.commit()\n-\n-    def pop(self):\n-        q = \"select id, message from %s order by priority desc limit 1\" \\\n-            % self.table\n-        idmsg = self.conn.execute(q).fetchone()\n-        if idmsg is None:\n-            return\n-        id, msg = idmsg\n-        q = \"delete from %s where id=?\" % self.table\n-        c = self.conn.execute(q, (id,))\n-        if not c.rowcount: # record vanished, so let's try again\n-            self.conn.rollback()\n-            return self.pop()\n-        self.conn.commit()\n-        return self.decode(msg)\n-\n-    def remove(self, func):\n-        q = \"select id, message from %s\" % self.table\n-        n = 0\n-        for id, msg in self.conn.execute(q):\n-            if func(self.decode(msg)):\n-                q = \"delete from %s where id=?\" % self.table\n-                c = self.conn.execute(q, (id,))\n-                if not c.rowcount: # record vanished, so let's try again\n-                    self.conn.rollback()\n-                    return self.remove(func)\n-                n += 1\n-        self.conn.commit()\n-        return n\n-\n-    def clear(self):\n-        self.conn.execute(\"delete from %s\" % self.table)\n-        self.conn.commit()\n-\n-    def __len__(self):\n-        q = \"select count(*) from %s\" % self.table\n-        return self.conn.execute(q).fetchone()[0]\n-\n-    def __iter__(self):\n-        q = \"select message, priority from %s order by priority desc\" % \\\n-            self.table\n-        return ((self.decode(x), y) for x, y in self.conn.execute(q))\n-\n-    def encode(self, obj):\n-        return obj\n-\n-    def decode(self, text):\n-        return text\n-\n-\n-class PickleSqlitePriorityQueue(SqlitePriorityQueue):\n-\n-    def encode(self, obj):\n-        return buffer(cPickle.dumps(obj, protocol=2))\n-\n-    def decode(self, text):\n-        return cPickle.loads(str(text))\n-\n-\n-class JsonSqlitePriorityQueue(SqlitePriorityQueue):\n-\n-    def encode(self, obj):\n-        return json.dumps(obj)\n-\n-    def decode(self, text):\n-        return json.loads(text)\n-\n-\n\n\n@@ -1,22 +0,0 @@\n-import sys\n-import unittest\n-\n-class SettingsSafeModulesTest(unittest.TestCase):\n-\n-    # these modules must not load scrapy.conf\n-    SETTINGS_SAFE_MODULES = [\n-        'scrapy.utils.project',\n-        'scrapy.utils.conf',\n-        'scrapyd.interfaces',\n-        'scrapyd.eggutils',\n-    ]\n-\n-    def test_modules_that_shouldnt_load_settings(self):\n-        sys.modules.pop('scrapy.conf', None)\n-        for m in self.SETTINGS_SAFE_MODULES:\n-            __import__(m)\n-            assert 'scrapy.conf' not in sys.modules, \\\n-                \"Module %r must not cause the scrapy.conf module to be loaded\" % m\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,43 +0,0 @@\n-from cStringIO import StringIO\n-\n-from twisted.trial import unittest\n-\n-from zope.interface.verify import verifyObject\n-\n-from scrapyd.interfaces import IEggStorage\n-from scrapyd.config import Config\n-from scrapyd.eggstorage import FilesystemEggStorage\n-\n-class EggStorageTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        d = self.mktemp()\n-        config = Config(values={'eggs_dir': d})\n-        self.eggst = FilesystemEggStorage(config)\n-\n-    def test_interface(self):\n-        verifyObject(IEggStorage, self.eggst)\n-\n-    def test_put_get_list_delete(self):\n-        self.eggst.put(StringIO(\"egg01\"), 'mybot', '01')\n-        self.eggst.put(StringIO(\"egg03\"), 'mybot', '03')\n-        self.eggst.put(StringIO(\"egg02\"), 'mybot', '02')\n-\n-        self.assertEqual(self.eggst.list('mybot'), ['01', '02', '03'])\n-        self.assertEqual(self.eggst.list('mybot2'), [])\n-\n-        v, f = self.eggst.get('mybot')\n-        self.assertEqual(v, \"03\")\n-        self.assertEqual(f.read(), \"egg03\")\n-        f.close()\n-\n-        v, f = self.eggst.get('mybot', '02')\n-        self.assertEqual(v, \"02\")\n-        self.assertEqual(f.read(), \"egg02\")\n-        f.close()\n-\n-        self.eggst.delete('mybot', '02')\n-        self.assertEqual(self.eggst.list('mybot'), ['01', '03'])\n-\n-        self.eggst.delete('mybot')\n-        self.assertEqual(self.eggst.list('mybot'), [])\n\n@@ -1,45 +0,0 @@\n-import os\n-\n-from twisted.trial import unittest\n-\n-from zope.interface.verify import verifyObject\n-\n-from scrapyd.interfaces import IEnvironment\n-from scrapyd.config import Config\n-from scrapyd.environ import Environment\n-\n-class EnvironmentTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        d = self.mktemp()\n-        os.mkdir(d)\n-        config = Config(values={'eggs_dir': d, 'logs_dir': d})\n-        config.cp.add_section('settings')\n-        config.cp.set('settings', 'newbot', 'newbot.settings')\n-        self.environ = Environment(config, initenv={})\n-\n-    def test_interface(self):\n-        verifyObject(IEnvironment, self.environ)\n-\n-    def test_get_environment_with_eggfile(self):\n-        msg = {'_project': 'mybot', '_spider': 'myspider', '_job': 'ID'}\n-        slot = 3\n-        env = self.environ.get_environment(msg, slot)\n-        self.assertEqual(env['SCRAPY_PROJECT'], 'mybot')\n-        self.assertEqual(env['SCRAPY_SLOT'], '3')\n-        self.assertEqual(env['SCRAPY_SPIDER'], 'myspider')\n-        self.assertEqual(env['SCRAPY_JOB'], 'ID')\n-        self.assert_(env['SCRAPY_LOG_FILE'].endswith(os.path.join('mybot', 'myspider', 'ID.log')))\n-        self.assert_(env['SCRAPY_FEED_URI'].endswith(os.path.join('mybot', 'myspider', 'ID.jl')))\n-        self.failIf('SCRAPY_SETTINGS_MODULE' in env)\n-\n-    def test_get_environment_with_no_items_dir(self):\n-        config = Config(values={'items_dir': '', 'logs_dir': ''})\n-        config.cp.add_section('settings')\n-        config.cp.set('settings', 'newbot', 'newbot.settings')\n-        msg = {'_project': 'mybot', '_spider': 'myspider', '_job': 'ID'}\n-        slot = 3\n-        environ = Environment(config, initenv={})\n-        env = environ.get_environment(msg, slot)\n-        self.failUnless('SCRAPY_FEED_URI' not in env)\n-        self.failUnless('SCRAPY_LOG_FILE' not in env)\n\n@@ -1,41 +0,0 @@\n-import os\n-\n-from twisted.trial import unittest\n-from twisted.internet.defer import Deferred\n-\n-from zope.interface.verify import verifyObject\n-\n-from scrapyd.interfaces import IPoller\n-from scrapyd.config import Config\n-from scrapyd.poller import QueuePoller\n-from scrapyd.utils import get_spider_queues\n-\n-class QueuePollerTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        d = self.mktemp()\n-        eggs_dir = os.path.join(d, 'eggs')\n-        dbs_dir = os.path.join(d, 'dbs')\n-        os.makedirs(eggs_dir)\n-        os.makedirs(dbs_dir)\n-        os.makedirs(os.path.join(eggs_dir, 'mybot1'))\n-        os.makedirs(os.path.join(eggs_dir, 'mybot2'))\n-        config = Config(values={'eggs_dir': eggs_dir, 'dbs_dir': dbs_dir})\n-        self.queues = get_spider_queues(config)\n-        self.poller = QueuePoller(config)\n-\n-    def test_interface(self):\n-        verifyObject(IPoller, self.poller)\n-\n-    def test_poll_next(self):\n-        self.queues['mybot1'].add('spider1')\n-        self.queues['mybot2'].add('spider2')\n-        d1 = self.poller.next()\n-        d2 = self.poller.next()\n-        self.failUnless(isinstance(d1, Deferred))\n-        self.failIf(hasattr(d1, 'result'))\n-        self.poller.poll()\n-        self.queues['mybot1'].pop()\n-        self.poller.poll()\n-        self.failUnlessEqual(d1.result, {'_project': 'mybot1', '_spider': 'spider1'})\n-        self.failUnlessEqual(d2.result, {'_project': 'mybot2', '_spider': 'spider2'})\n\n@@ -1,44 +0,0 @@\n-import os\n-\n-from twisted.trial import unittest\n-\n-from zope.interface.verify import verifyObject\n-\n-from scrapyd.interfaces import ISpiderScheduler\n-from scrapyd.config import Config\n-from scrapyd.scheduler import SpiderScheduler\n-from scrapyd.utils import get_spider_queues\n-\n-class SpiderSchedulerTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        d = self.mktemp()\n-        eggs_dir = self.eggs_dir = os.path.join(d, 'eggs')\n-        dbs_dir = os.path.join(d, 'dbs')\n-        os.mkdir(d)\n-        os.makedirs(eggs_dir)\n-        os.makedirs(dbs_dir)\n-        os.makedirs(os.path.join(eggs_dir, 'mybot1'))\n-        os.makedirs(os.path.join(eggs_dir, 'mybot2'))\n-        config = Config(values={'eggs_dir': eggs_dir, 'dbs_dir': dbs_dir})\n-        self.queues = get_spider_queues(config)\n-        self.sched = SpiderScheduler(config)\n-\n-    def test_interface(self):\n-        verifyObject(ISpiderScheduler, self.sched)\n-\n-    def test_list_update_projects(self):\n-        self.assertEqual(sorted(self.sched.list_projects()), sorted(['mybot1', 'mybot2']))\n-        os.makedirs(os.path.join(self.eggs_dir, 'mybot3'))\n-        self.sched.update_projects()\n-        self.assertEqual(sorted(self.sched.list_projects()), sorted(['mybot1', 'mybot2', 'mybot3']))\n-\n-    def test_schedule(self):\n-        q = self.queues['mybot1']\n-        self.failIf(q.count())\n-        self.sched.schedule('mybot1', 'myspider1', a='b')\n-        self.sched.schedule('mybot2', 'myspider2', c='d')\n-        self.assertEqual(q.pop(), {'name': 'myspider1', 'a': 'b'})\n-        q = self.queues['mybot2']\n-        self.assertEqual(q.pop(), {'name': 'myspider2', 'c': 'd'})\n-\n\n@@ -1,66 +0,0 @@\n-from twisted.internet.defer import inlineCallbacks, maybeDeferred\n-from twisted.trial import unittest\n-\n-from zope.interface.verify import verifyObject\n-\n-from scrapyd.interfaces import ISpiderQueue\n-from scrapyd.spiderqueue import SqliteSpiderQueue\n-\n-class SpiderQueueTest(unittest.TestCase):\n-    \"\"\"This test case can be used easily for testing other SpiderQueue's by\n-    just changing the _get_queue() method. It also supports queues with\n-    deferred methods.\n-    \"\"\"\n-\n-    def setUp(self):\n-        self.q = self._get_queue()\n-        self.name = 'spider1'\n-        self.args = {'arg1': 'val1', 'arg2': 2}\n-        self.msg = self.args.copy()\n-        self.msg['name'] = self.name\n-\n-    def _get_queue(self):\n-        return SqliteSpiderQueue(':memory:')\n-\n-    def test_interface(self):\n-        verifyObject(ISpiderQueue, self.q)\n-\n-    @inlineCallbacks\n-    def test_add_pop_count(self):\n-        c = yield maybeDeferred(self.q.count)\n-        self.assertEqual(c, 0)\n-\n-        yield maybeDeferred(self.q.add, self.name, **self.args)\n-\n-        c = yield maybeDeferred(self.q.count)\n-        self.assertEqual(c, 1)\n-\n-        m = yield maybeDeferred(self.q.pop)\n-        self.assertEqual(m, self.msg)\n-\n-        c = yield maybeDeferred(self.q.count)\n-        self.assertEqual(c, 0)\n-\n-    @inlineCallbacks\n-    def test_list(self):\n-        l = yield maybeDeferred(self.q.list)\n-        self.assertEqual(l, [])\n-\n-        yield maybeDeferred(self.q.add, self.name, **self.args)\n-        yield maybeDeferred(self.q.add, self.name, **self.args)\n-\n-        l = yield maybeDeferred(self.q.list)\n-        self.assertEqual(l, [self.msg, self.msg])\n-\n-    @inlineCallbacks\n-    def test_clear(self):\n-        yield maybeDeferred(self.q.add, self.name, **self.args)\n-        yield maybeDeferred(self.q.add, self.name, **self.args)\n-\n-        c = yield maybeDeferred(self.q.count)\n-        self.assertEqual(c, 2)\n-\n-        yield maybeDeferred(self.q.clear)\n-\n-        c = yield maybeDeferred(self.q.count)\n-        self.assertEqual(c, 0)\n\n@@ -1,173 +0,0 @@\n-import unittest\n-from datetime import datetime\n-from decimal import Decimal\n-\n-from scrapy.http import Request\n-from scrapyd.sqlite import SqlitePriorityQueue, JsonSqlitePriorityQueue, \\\n-    PickleSqlitePriorityQueue, SqliteDict, JsonSqliteDict, PickleSqliteDict\n-\n-\n-class SqliteDictTest(unittest.TestCase):\n-\n-    dict_class = SqliteDict\n-    test_dict = {'hello': 'world', 'int': 1, 'float': 1.5}\n-\n-    def test_basic_types(self):\n-        test = self.test_dict\n-        d = self.dict_class()\n-        d.update(test)\n-        self.failUnlessEqual(d.items(), test.items())\n-        d.clear()\n-        self.failIf(d.items())\n-\n-    def test_in(self):\n-        d = self.dict_class()\n-        self.assertFalse('test' in d)\n-        d['test'] = 123\n-        self.assertTrue('test' in d)\n-\n-    def test_keyerror(self):\n-        d = self.dict_class()\n-        self.assertRaises(KeyError, d.__getitem__, 'test')\n-\n-    def test_replace(self):\n-        d = self.dict_class()\n-        self.assertEqual(d.get('test'), None)\n-        d['test'] = 123\n-        self.assertEqual(d.get('test'), 123)\n-        d['test'] = 456\n-        self.assertEqual(d.get('test'), 456)\n-\n-\n-class JsonSqliteDictTest(SqliteDictTest):\n-\n-    dict_class = JsonSqliteDict\n-    test_dict = SqliteDictTest.test_dict.copy()\n-    test_dict.update({'list': ['a', 'world'], 'dict': {'some': 'dict'}})\n-\n-\n-class PickleSqliteDictTest(JsonSqliteDictTest):\n-\n-    dict_class = PickleSqliteDict\n-    test_dict = JsonSqliteDictTest.test_dict.copy()\n-    test_dict.update({'decimal': Decimal(\"10\"), 'datetime': datetime.now()})\n-\n-    def test_request_persistance(self):\n-        r1 = Request(\"http://www.example.com\", body=\"some\")\n-        d = self.dict_class()\n-        d['request'] = r1\n-        r2 = d['request']\n-        self.failUnless(isinstance(r2, Request))\n-        self.failUnlessEqual(r1.url, r2.url)\n-        self.failUnlessEqual(r1.body, r2.body)\n-\n-\n-class SqlitePriorityQueueTest(unittest.TestCase):\n-\n-    queue_class = SqlitePriorityQueue\n-\n-    supported_values = [\"bytes\", u\"\\xa3\", 123, 1.2, True]\n-\n-    def setUp(self):\n-        self.q = self.queue_class()\n-\n-    def test_empty(self):\n-        self.failUnless(self.q.pop() is None)\n-\n-    def test_one(self):\n-        msg = \"a message\"\n-        self.q.put(msg)\n-        self.failIf(\"_id\" in msg)\n-        self.failUnlessEqual(self.q.pop(), msg)\n-        self.failUnless(self.q.pop() is None)\n-\n-    def test_multiple(self):\n-        msg1 = \"first message\"\n-        msg2 = \"second message\"\n-        self.q.put(msg1)\n-        self.q.put(msg2)\n-        out = []\n-        out.append(self.q.pop())\n-        out.append(self.q.pop())\n-        self.failUnless(msg1 in out)\n-        self.failUnless(msg2 in out)\n-        self.failUnless(self.q.pop() is None)\n-\n-    def test_priority(self):\n-        msg1 = \"message 1\"\n-        msg2 = \"message 2\"\n-        msg3 = \"message 3\"\n-        msg4 = \"message 4\"\n-        self.q.put(msg1, priority=1.0)\n-        self.q.put(msg2, priority=5.0)\n-        self.q.put(msg3, priority=3.0)\n-        self.q.put(msg4, priority=2.0)\n-        self.failUnlessEqual(self.q.pop(), msg2)\n-        self.failUnlessEqual(self.q.pop(), msg3)\n-        self.failUnlessEqual(self.q.pop(), msg4)\n-        self.failUnlessEqual(self.q.pop(), msg1)\n-\n-    def test_iter_len_clear(self):\n-        self.failUnlessEqual(len(self.q), 0)\n-        self.failUnlessEqual(list(self.q), [])\n-        msg1 = \"message 1\"\n-        msg2 = \"message 2\"\n-        msg3 = \"message 3\"\n-        msg4 = \"message 4\"\n-        self.q.put(msg1, priority=1.0)\n-        self.q.put(msg2, priority=5.0)\n-        self.q.put(msg3, priority=3.0)\n-        self.q.put(msg4, priority=2.0)\n-        self.failUnlessEqual(len(self.q), 4)\n-        self.failUnlessEqual(list(self.q), \\\n-            [(msg2, 5.0), (msg3, 3.0), (msg4, 2.0), (msg1, 1.0)])\n-        self.q.clear()\n-        self.failUnlessEqual(len(self.q), 0)\n-        self.failUnlessEqual(list(self.q), [])\n-\n-    def test_remove(self):\n-        self.failUnlessEqual(len(self.q), 0)\n-        self.failUnlessEqual(list(self.q), [])\n-        msg1 = \"good message 1\"\n-        msg2 = \"bad message 2\"\n-        msg3 = \"good message 3\"\n-        msg4 = \"bad message 4\"\n-        self.q.put(msg1)\n-        self.q.put(msg2)\n-        self.q.put(msg3)\n-        self.q.put(msg4)\n-        self.q.remove(lambda x: x.startswith(\"bad\"))\n-        self.failUnlessEqual(list(self.q), [(msg1, 0.0), (msg3, 0.0)])\n-\n-    def test_types(self):\n-        for x in self.supported_values:\n-            self.q.put(x)\n-            self.failUnlessEqual(self.q.pop(), x)\n-\n-\n-class JsonSqlitePriorityQueueTest(SqlitePriorityQueueTest):\n-\n-    queue_class = JsonSqlitePriorityQueue\n-\n-    supported_values = SqlitePriorityQueueTest.supported_values + [\n-        [\"a\", \"list\", 1],\n-        {\"a\": \"dict\"},\n-    ]\n-\n-\n-class PickleSqlitePriorityQueueTest(JsonSqlitePriorityQueueTest):\n-\n-    queue_class = PickleSqlitePriorityQueue\n-\n-    supported_values = JsonSqlitePriorityQueueTest.supported_values + [\n-        Decimal(\"10\"),\n-        datetime.now(),\n-    ]\n-\n-    def test_request_persistance(self):\n-        r1 = Request(\"http://www.example.com\", body=\"some\")\n-        self.q.put(r1)\n-        r2 = self.q.pop()\n-        self.failUnless(isinstance(r2, Request))\n-        self.failUnlessEqual(r1.url, r2.url)\n-        self.failUnlessEqual(r1.body, r2.body)\n\n@@ -1,51 +0,0 @@\n-import os\n-from pkgutil import get_data\n-from cStringIO import StringIO\n-\n-from twisted.trial import unittest\n-\n-from scrapy.utils.test import get_pythonpath\n-from scrapyd.interfaces import IEggStorage\n-from scrapyd.utils import get_crawl_args, get_spider_list\n-from scrapyd import get_application\n-\n-class UtilsTest(unittest.TestCase):\n-\n-    def test_get_crawl_args(self):\n-        msg = {'_project': 'lolo', '_spider': 'lala'}\n-        self.assertEqual(get_crawl_args(msg), ['lala'])\n-        msg = {'_project': 'lolo', '_spider': 'lala', 'arg1': u'val1'}\n-        cargs = get_crawl_args(msg)\n-        self.assertEqual(cargs, ['lala', '-a', 'arg1=val1'])\n-        assert all(isinstance(x, str) for x in cargs), cargs\n-\n-    def test_get_crawl_args_with_settings(self):\n-        msg = {'_project': 'lolo', '_spider': 'lala', 'arg1': u'val1', 'settings': {'ONE': 'two'}}\n-        cargs = get_crawl_args(msg)\n-        self.assertEqual(cargs, ['lala', '-a', 'arg1=val1', '-s', 'ONE=two'])\n-        assert all(isinstance(x, str) for x in cargs), cargs\n-\n-class GetSpiderListTest(unittest.TestCase):\n-\n-    def test_get_spider_list(self):\n-        path = os.path.abspath(self.mktemp())\n-        j = os.path.join\n-        eggs_dir = j(path, 'eggs')\n-        os.makedirs(eggs_dir)\n-        dbs_dir = j(path, 'dbs')\n-        os.makedirs(dbs_dir)\n-        logs_dir = j(path, 'logs')\n-        os.makedirs(logs_dir)\n-        os.chdir(path)\n-        with open('scrapyd.conf', 'w') as f:\n-            f.write(\"[scrapyd]\\n\")\n-            f.write(\"eggs_dir = %s\\n\" % eggs_dir)\n-            f.write(\"dbs_dir = %s\\n\" % dbs_dir)\n-            f.write(\"logs_dir = %s\\n\" % logs_dir)\n-        app = get_application()\n-        eggstorage = app.getComponent(IEggStorage)\n-        eggfile = StringIO(get_data(__package__, 'mybot.egg'))\n-        eggstorage.put(eggfile, 'mybot', 'r1')\n-        spiders = get_spider_list('mybot', pythonpath=get_pythonpath())\n-        self.assertEqual(sorted(spiders), ['spider1', 'spider2'])\n-\n\n@@ -1,67 +0,0 @@\n-import sys\n-import os\n-from subprocess import Popen, PIPE\n-from ConfigParser import NoSectionError\n-\n-from scrapyd.spiderqueue import SqliteSpiderQueue\n-from scrapy.utils.python import stringify_dict, unicode_to_str\n-from scrapyd.config import Config\n-\n-def get_spider_queues(config):\n-    \"\"\"Return a dict of Spider Quees keyed by project name\"\"\"\n-    dbsdir = config.get('dbs_dir', 'dbs')\n-    if not os.path.exists(dbsdir):\n-        os.makedirs(dbsdir)\n-    d = {}\n-    for project in get_project_list(config):\n-        dbpath = os.path.join(dbsdir, '%s.db' % project)\n-        d[project] = SqliteSpiderQueue(dbpath)\n-    return d\n-\n-def get_project_list(config):\n-    \"\"\"Get list of projects by inspecting the eggs dir and the ones defined in\n-    the scrapyd.conf [settings] section\n-    \"\"\"\n-    eggs_dir = config.get('eggs_dir', 'eggs')\n-    if os.path.exists(eggs_dir):\n-        projects = os.listdir(eggs_dir)\n-    else:\n-        projects = []\n-    try:\n-        projects += [x[0] for x in config.cp.items('settings')]\n-    except NoSectionError:\n-        pass\n-    return projects\n-\n-def get_crawl_args(message):\n-    \"\"\"Return the command-line arguments to use for the scrapy crawl process\n-    that will be started for this message\n-    \"\"\"\n-    msg = message.copy()\n-    args = [unicode_to_str(msg['_spider'])]\n-    del msg['_project'], msg['_spider']\n-    settings = msg.pop('settings', {})\n-    for k, v in stringify_dict(msg, keys_only=False).items():\n-        args += ['-a']\n-        args += ['%s=%s' % (k, v)]\n-    for k, v in stringify_dict(settings, keys_only=False).items():\n-        args += ['-s']\n-        args += ['%s=%s' % (k, v)]\n-    return args\n-\n-def get_spider_list(project, runner=None, pythonpath=None):\n-    \"\"\"Return the spider list from the given project, using the given runner\"\"\"\n-    if runner is None:\n-        runner = Config().get('runner')\n-    env = os.environ.copy()\n-    env['SCRAPY_PROJECT'] = project\n-    if pythonpath:\n-        env['PYTHONPATH'] = pythonpath\n-    pargs = [sys.executable, '-m', runner, 'list']\n-    proc = Popen(pargs, stdout=PIPE, stderr=PIPE, env=env)\n-    out, err = proc.communicate()\n-    if proc.returncode:\n-        msg = err or out or 'unknown error'\n-        raise RuntimeError(msg.splitlines()[-1])\n-    return out.splitlines()\n-\n\n@@ -1,122 +0,0 @@\n-import traceback\n-import uuid\n-from cStringIO import StringIO\n-\n-from twisted.python import log\n-\n-from scrapy.utils.txweb import JsonResource\n-from .utils import get_spider_list\n-\n-class WsResource(JsonResource):\n-\n-    def __init__(self, root):\n-        JsonResource.__init__(self)\n-        self.root = root\n-\n-    def render(self, txrequest):\n-        try:\n-            return JsonResource.render(self, txrequest)\n-        except Exception, e:\n-            if self.root.debug:\n-                return traceback.format_exc()\n-            log.err()\n-            r = {\"status\": \"error\", \"message\": str(e)}\n-            return self.render_object(r, txrequest)\n-\n-class Schedule(WsResource):\n-\n-    def render_POST(self, txrequest):\n-        settings = txrequest.args.pop('setting', [])\n-        settings = dict(x.split('=', 1) for x in settings)\n-        args = dict((k, v[0]) for k, v in txrequest.args.items())\n-        project = args.pop('project')\n-        spider = args.pop('spider')\n-        args['settings'] = settings\n-        jobid = uuid.uuid1().hex\n-        args['_job'] = jobid\n-        self.root.scheduler.schedule(project, spider, **args)\n-        return {\"status\": \"ok\", \"jobid\": jobid}\n-\n-class Cancel(WsResource):\n-\n-    def render_POST(self, txrequest):\n-        args = dict((k, v[0]) for k, v in txrequest.args.items())\n-        project = args['project']\n-        jobid = args['job']\n-        signal = args.get('signal', 'TERM')\n-        prevstate = None\n-        queue = self.root.poller.queues[project]\n-        c = queue.remove(lambda x: x[\"_job\"] == jobid)\n-        if c:\n-            prevstate = \"pending\"\n-        spiders = self.root.launcher.processes.values()\n-        for s in spiders:\n-            if s.job == jobid:\n-                s.transport.signalProcess(signal)\n-                prevstate = \"running\"\n-        return {\"status\": \"ok\", \"prevstate\": prevstate}\n-\n-class AddVersion(WsResource):\n-\n-    def render_POST(self, txrequest):\n-        project = txrequest.args['project'][0]\n-        version = txrequest.args['version'][0]\n-        eggf = StringIO(txrequest.args['egg'][0])\n-        self.root.eggstorage.put(eggf, project, version)\n-        spiders = get_spider_list(project)\n-        self.root.update_projects()\n-        return {\"status\": \"ok\", \"project\": project, \"version\": version, \\\n-            \"spiders\": len(spiders)}\n-\n-class ListProjects(WsResource):\n-\n-    def render_GET(self, txrequest):\n-        projects = self.root.scheduler.list_projects()\n-        return {\"status\": \"ok\", \"projects\": projects}\n-\n-class ListVersions(WsResource):\n-\n-    def render_GET(self, txrequest):\n-        project = txrequest.args['project'][0]\n-        versions = self.root.eggstorage.list(project)\n-        return {\"status\": \"ok\", \"versions\": versions}\n-\n-class ListSpiders(WsResource):\n-\n-    def render_GET(self, txrequest):\n-        project = txrequest.args['project'][0]\n-        spiders = get_spider_list(project, runner=self.root.runner)\n-        return {\"status\": \"ok\", \"spiders\": spiders}\n-\n-class ListJobs(WsResource):\n-\n-    def render_GET(self, txrequest):\n-        project = txrequest.args['project'][0]\n-        spiders = self.root.launcher.processes.values()\n-        running = [{\"id\": s.job, \"spider\": s.spider} for s in spiders if s.project == project]\n-        queue = self.root.poller.queues[project]\n-        pending = [{\"id\": x[\"_job\"], \"spider\": x[\"name\"]} for x in queue.list()]\n-        finished = [{\"id\": s.job, \"spider\": s.spider,\n-            \"start_time\": s.start_time.isoformat(' '),\n-            \"end_time\": s.end_time.isoformat(' ')} for s in self.root.launcher.finished\n-            if s.project == project]\n-        return {\"status\":\"ok\", \"pending\": pending, \"running\": running, \"finished\": finished}\n-\n-class DeleteProject(WsResource):\n-\n-    def render_POST(self, txrequest):\n-        project = txrequest.args['project'][0]\n-        self._delete_version(project)\n-        return {\"status\": \"ok\"}\n-\n-    def _delete_version(self, project, version=None):\n-        self.root.eggstorage.delete(project, version)\n-        self.root.update_projects()\n-\n-class DeleteVersion(DeleteProject):\n-\n-    def render_POST(self, txrequest):\n-        project = txrequest.args['project'][0]\n-        version = txrequest.args['version'][0]\n-        self._delete_version(project, version)\n-        return {\"status\": \"ok\"}\n\n@@ -1,131 +0,0 @@\n-from datetime import datetime\n-\n-from twisted.web import resource, static\n-from twisted.application.service import IServiceCollection\n-\n-from scrapy.utils.misc import load_object\n-\n-from .interfaces import IPoller, IEggStorage, ISpiderScheduler\n-\n-class Root(resource.Resource):\n-\n-    def __init__(self, config, app):\n-        resource.Resource.__init__(self)\n-        self.debug = config.getboolean('debug', False)\n-        self.runner = config.get('runner')\n-        logsdir = config.get('logs_dir')\n-        itemsdir = config.get('items_dir')\n-        self.app = app\n-        self.putChild('', Home(self))\n-        self.putChild('logs', static.File(logsdir, 'text/plain'))\n-        self.putChild('items', static.File(itemsdir, 'text/plain'))\n-        self.putChild('jobs', Jobs(self))\n-        services = config.items('services', ())\n-        for servName, servClsName in services:\n-          servCls = load_object(servClsName)\n-          self.putChild(servName, servCls(self))\n-        self.update_projects()\n-\n-    def update_projects(self):\n-        self.poller.update_projects()\n-        self.scheduler.update_projects()\n-\n-    @property\n-    def launcher(self):\n-        app = IServiceCollection(self.app, self.app)\n-        return app.getServiceNamed('launcher')\n-\n-    @property\n-    def scheduler(self):\n-        return self.app.getComponent(ISpiderScheduler)\n-\n-    @property\n-    def eggstorage(self):\n-        return self.app.getComponent(IEggStorage)\n-\n-    @property\n-    def poller(self):\n-        return self.app.getComponent(IPoller)\n-\n-\n-class Home(resource.Resource):\n-\n-    def __init__(self, root):\n-        resource.Resource.__init__(self)\n-        self.root = root\n-\n-    def render_GET(self, txrequest):\n-        vars = {\n-            'projects': ', '.join(self.root.scheduler.list_projects()),\n-        }\n-        return \"\"\"\n-<html>\n-<head><title>Scrapyd</title></head>\n-<body>\n-<h1>Scrapyd</h1>\n-<p>Available projects: <b>%(projects)s</b></p>\n-<ul>\n-<li><a href=\"/jobs\">Jobs</a></li>\n-<li><a href=\"/items/\">Items</li>\n-<li><a href=\"/logs/\">Logs</li>\n-<li><a href=\"http://doc.scrapy.org/en/latest/topics/scrapyd.html\">Documentation</a></li>\n-</ul>\n-\n-<h2>How to schedule a spider?</h2>\n-\n-<p>To schedule a spider you need to use the API (this web UI is only for\n-monitoring)</p>\n-\n-<p>Example using <a href=\"http://curl.haxx.se/\">curl</a>:</p>\n-<p><code>curl http://localhost:6800/schedule.json -d project=default -d spider=somespider</code></p>\n-\n-<p>For more information about the API, see the <a href=\"http://doc.scrapy.org/en/latest/topics/scrapyd.html\">Scrapyd documentation</a></p>\n-</body>\n-</html>\n-\"\"\" % vars\n-\n-\n-class Jobs(resource.Resource):\n-\n-    def __init__(self, root):\n-        resource.Resource.__init__(self)\n-        self.root = root\n-\n-    def render(self, txrequest):\n-        s = \"<html><head><title>Scrapyd</title></title>\"\n-        s += \"<body>\"\n-        s += \"<h1>Jobs</h1>\"\n-        s += \"<p><a href='..'>Go back</a></p>\"\n-        s += \"<table border='1'>\"\n-        s += \"<th>Project</th><th>Spider</th><th>Job</th><th>PID</th><th>Runtime</th><th>Log</th><th>Items</th>\"\n-        s += \"<tr><th colspan='7' style='background-color: #ddd'>Pending</th></tr>\"\n-        for project, queue in self.root.poller.queues.items():\n-            for m in queue.list():\n-                s += \"<tr>\"\n-                s += \"<td>%s</td>\" % project\n-                s += \"<td>%s</td>\" % str(m['name'])\n-                s += \"<td>%s</td>\" % str(m['_job'])\n-                s += \"</tr>\"\n-        s += \"<tr><th colspan='7' style='background-color: #ddd'>Running</th></tr>\"\n-        for p in self.root.launcher.processes.values():\n-            s += \"<tr>\"\n-            for a in ['project', 'spider', 'job', 'pid']:\n-                s += \"<td>%s</td>\" % getattr(p, a)\n-            s += \"<td>%s</td>\" % (datetime.now() - p.start_time)\n-            s += \"<td><a href='/logs/%s/%s/%s.log'>Log</a></td>\" % (p.project, p.spider, p.job)\n-            s += \"<td><a href='/items/%s/%s/%s.jl'>Items</a></td>\" % (p.project, p.spider, p.job)\n-            s += \"</tr>\"\n-        s += \"<tr><th colspan='7' style='background-color: #ddd'>Finished</th></tr>\"\n-        for p in self.root.launcher.finished:\n-            s += \"<tr>\"\n-            for a in ['project', 'spider', 'job']:\n-                s += \"<td>%s</td>\" % getattr(p, a)\n-            s += \"<td></td>\"\n-            s += \"<td>%s</td>\" % (p.end_time - p.start_time)\n-            s += \"<td><a href='/logs/%s/%s/%s.log'>Log</a></td>\" % (p.project, p.spider, p.job)\n-            s += \"<td><a href='/items/%s/%s/%s.jl'>Items</a></td>\" % (p.project, p.spider, p.job)\n-            s += \"</tr>\"\n-        s += \"</table>\"\n-        s += \"</body>\"\n-        s += \"</html>\"\n-        return s\n\n@@ -57,7 +57,7 @@ if root_dir != '':\n def is_not_module(filename):\n     return os.path.splitext(f)[1] not in ['.py', '.pyc', '.pyo']\n \n-for scrapy_dir in ['scrapy', 'scrapyd']:\n+for scrapy_dir in ['scrapy']:\n     for dirpath, dirnames, filenames in os.walk(scrapy_dir):\n         # Ignore dirnames that start with '.'\n         for i, dirname in enumerate(dirnames):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
