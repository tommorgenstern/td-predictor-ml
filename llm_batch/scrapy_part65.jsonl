{"custom_id": "scrapy#db90e26a8b469197abeabc6d6dd782def0015551", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 49 | Files Changed: 2 | Hunks: 47 | Methods Changed: 20 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 98 | Churn Cumulative: 901 | Contributors (this commit): 1 | Commits (past 90d): 15 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ from scrapy.newitem import Item\n from scrapy.newitem.loader.reducers import TakeFirst\n from scrapy.newitem.loader.expanders import IdentityExpander\n \n-class ItemLoader(object):\n+class Loader(object):\n \n     default_item_class = Item\n     default_expander = IdentityExpander()\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.newitem.loader import ItemLoader\n+from scrapy.newitem.loader import Loader\n from scrapy.newitem.loader.expanders import TreeExpander, IdentityExpander\n from scrapy.newitem.loader.reducers import Join, Identity\n from scrapy.newitem import Item, Field\n@@ -16,13 +16,13 @@ class TestItem(NameItem):\n \n # test loaders\n \n-class NameItemLoader(ItemLoader):\n+class NameLoader(Loader):\n     default_item_class = TestItem\n \n-class TestItemLoader(NameItemLoader):\n+class TestLoader(NameLoader):\n     name_exp = TreeExpander(lambda v: v.title())\n \n-class DefaultedItemLoader(NameItemLoader):\n+class DefaultedLoader(NameLoader):\n     default_expander = TreeExpander(lambda v: v[:-1])\n \n # test expanders\n@@ -32,12 +32,12 @@ def expander_func_with_args(value, other=None, loader_args=None):\n         return loader_args['key']\n     return value\n \n-class ItemLoaderTest(unittest.TestCase):\n+class Loader(unittest.TestCase):\n \n     def test_get_item_using_default_loader(self):\n         i = TestItem()\n         i['summary'] = u'lala'\n-        il = ItemLoader(item=i)\n+        il = Loader(item=i)\n         il.add_value('name', u'marta')\n         item = il.get_item()\n         assert item is i\n@@ -45,13 +45,13 @@ class ItemLoaderTest(unittest.TestCase):\n         self.assertEqual(item['name'], u'marta')\n \n     def test_get_item_using_custom_loader(self):\n-        il = TestItemLoader()\n+        il = TestLoader()\n         il.add_value('name', u'marta')\n         item = il.get_item()\n         self.assertEqual(item['name'], u'Marta')\n \n     def test_add_value(self):\n-        il = TestItemLoader()\n+        il = TestLoader()\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_expanded_value('name'), [u'Marta'])\n         self.assertEqual(il.get_reduced_value('name'), u'Marta')\n@@ -60,7 +60,7 @@ class ItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_reduced_value('name'), u'Marta')\n \n     def test_replace_value(self):\n-        il = TestItemLoader()\n+        il = TestLoader()\n         il.replace_value('name', u'marta')\n         self.assertEqual(il.get_expanded_value('name'), [u'Marta'])\n         self.assertEqual(il.get_reduced_value('name'), u'Marta')\n@@ -69,144 +69,144 @@ class ItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_reduced_value('name'), u'Pepe')\n \n     def test_tree_expander_multiple_functions(self):\n-        class TestItemLoader(NameItemLoader):\n+        class TestLoader(NameLoader):\n             name_exp = TreeExpander(lambda v: v.title(), lambda v: v[:-1])\n \n-        il = TestItemLoader()\n+        il = TestLoader()\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_reduced_value('name'), u'Mart')\n         item = il.get_item()\n         self.assertEqual(item['name'], u'Mart')\n \n     def test_default_expander(self):\n-        il = DefaultedItemLoader()\n+        il = DefaultedLoader()\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_reduced_value('name'), u'mart')\n \n     def test_inherited_default_expander(self):\n-        class InheritDefaultedItemLoader(DefaultedItemLoader):\n+        class InheritDefaultedLoader(DefaultedLoader):\n             pass\n \n-        il = InheritDefaultedItemLoader()\n+        il = InheritDefaultedLoader()\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_reduced_value('name'), u'mart')\n \n     def test_expander_inheritance(self):\n-        class ChildItemLoader(TestItemLoader):\n+        class ChildLoader(TestLoader):\n             url_exp = TreeExpander(lambda v: v.lower())\n \n-        il = ChildItemLoader()\n+        il = ChildLoader()\n         il.add_value('url', u'HTTP://scrapy.ORG')\n         self.assertEqual(il.get_reduced_value('url'), u'http://scrapy.org')\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_reduced_value('name'), u'Marta')\n \n-        class ChildChildItemLoader(ChildItemLoader):\n+        class ChildChildLoader(ChildLoader):\n             url_exp = TreeExpander(lambda v: v.upper())\n             summary_exp = TreeExpander(lambda v: v)\n \n-        il = ChildChildItemLoader()\n+        il = ChildChildLoader()\n         il.add_value('url', u'http://scrapy.org')\n         self.assertEqual(il.get_reduced_value('url'), u'HTTP://SCRAPY.ORG')\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_reduced_value('name'), u'Marta')\n \n     def test_empty_tree_expander(self):\n-        class IdentityDefaultedItemLoader(DefaultedItemLoader):\n+        class IdentityDefaultedLoader(DefaultedLoader):\n             name_exp = TreeExpander()\n \n-        il = IdentityDefaultedItemLoader()\n+        il = IdentityDefaultedLoader()\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_reduced_value('name'), u'marta')\n \n     def test_identity_expander(self):\n-        class IdentityDefaultedItemLoader(DefaultedItemLoader):\n+        class IdentityDefaultedLoader(DefaultedLoader):\n             name_exp = IdentityExpander()\n \n-        il = IdentityDefaultedItemLoader()\n+        il = IdentityDefaultedLoader()\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_reduced_value('name'), u'marta')\n \n     def test_extend_custom_expanders(self):\n-        class ChildItemLoader(TestItemLoader):\n-            name_exp = TreeExpander(TestItemLoader.name_exp, unicode.swapcase)\n+        class ChildLoader(TestLoader):\n+            name_exp = TreeExpander(TestLoader.name_exp, unicode.swapcase)\n \n-        il = ChildItemLoader()\n+        il = ChildLoader()\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_reduced_value('name'), u'mARTA')\n \n     def test_extend_default_expanders(self):\n-        class ChildDefaultedItemLoader(DefaultedItemLoader):\n-            name_exp = TreeExpander(DefaultedItemLoader.default_expander, unicode.swapcase)\n+        class ChildDefaultedLoader(DefaultedLoader):\n+            name_exp = TreeExpander(DefaultedLoader.default_expander, unicode.swapcase)\n \n-        il = ChildDefaultedItemLoader()\n+        il = ChildDefaultedLoader()\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_reduced_value('name'), u'MART')\n \n     def test_reducer_using_function(self):\n-        il = TestItemLoader()\n+        il = TestLoader()\n         il.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(il.get_reduced_value('name'), u'Mar')\n \n-        class TakeFirstItemLoader(TestItemLoader):\n+        class TakeFirstLoader(TestLoader):\n             name_red = u\" \".join\n \n-        il = TakeFirstItemLoader()\n+        il = TakeFirstLoader()\n         il.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(il.get_reduced_value('name'), u'Mar Ta')\n \n     def test_reducer_using_classes(self):\n-        il = TestItemLoader()\n+        il = TestLoader()\n         il.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(il.get_reduced_value('name'), u'Mar')\n \n-        class TakeFirstItemLoader(TestItemLoader):\n+        class TakeFirstLoader(TestLoader):\n             name_red = Join()\n \n-        il = TakeFirstItemLoader()\n+        il = TakeFirstLoader()\n         il.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(il.get_reduced_value('name'), u'Mar Ta')\n \n-        class TakeFirstItemLoader(TestItemLoader):\n+        class TakeFirstLoader(TestLoader):\n             name_red = Join(\"<br>\")\n \n-        il = TakeFirstItemLoader()\n+        il = TakeFirstLoader()\n         il.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(il.get_reduced_value('name'), u'Mar<br>Ta')\n \n     def test_default_reducer(self):\n-        il = TestItemLoader()\n+        il = TestLoader()\n         il.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(il.get_reduced_value('name'), u'Mar')\n \n-        class LalaItemLoader(TestItemLoader):\n+        class LalaLoader(TestLoader):\n             default_reducer = Identity()\n \n-        il = LalaItemLoader()\n+        il = LalaLoader()\n         il.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(il.get_reduced_value('name'), [u'Mar', u'Ta'])\n \n     def test_expander_args_on_declaration(self):\n-        class ChildItemLoader(TestItemLoader):\n+        class ChildLoader(TestLoader):\n             url_exp = TreeExpander(expander_func_with_args, key=u'val')\n \n-        il = ChildItemLoader()\n+        il = ChildLoader()\n         il.add_value('url', u'text', key=u'val')\n         self.assertEqual(il.get_reduced_value('url'), 'val')\n \n     def test_expander_args_on_instantiation(self):\n-        class ChildItemLoader(TestItemLoader):\n+        class ChildLoader(TestLoader):\n             url_exp = TreeExpander(expander_func_with_args)\n \n-        il = ChildItemLoader(key=u'val')\n+        il = ChildLoader(key=u'val')\n         il.add_value('url', u'text')\n         self.assertEqual(il.get_reduced_value('url'), 'val')\n \n     def test_expander_args_on_assign(self):\n-        class ChildItemLoader(TestItemLoader):\n+        class ChildLoadeLoader(TestLoader):\n             url_exp = TreeExpander(expander_func_with_args)\n \n-        il = ChildItemLoader()\n+        il = ChildLoader()\n         il.add_value('url', u'text', key=u'val')\n         self.assertEqual(il.get_reduced_value('url'), 'val')\n \n@@ -214,16 +214,16 @@ class ItemLoaderTest(unittest.TestCase):\n         def exp_func(value, loader_args):\n             return loader_args['item']['name']\n \n-        class ChildItemLoader(TestItemLoader):\n+        class ChildLoader(TestLoader):\n             url_exp = TreeExpander(exp_func)\n \n         it = TestItem(name='marta')\n-        il = ChildItemLoader(item=it)\n+        il = ChildLoader(item=it)\n         il.add_value('url', u'text', key=u'val')\n         self.assertEqual(il.get_reduced_value('url'), 'marta')\n \n     def test_add_value_on_unknown_field(self):\n-        il = TestItemLoader()\n+        il = TestLoader()\n         self.assertRaises(KeyError, il.add_value, 'wrong_field', [u'lala', u'lolo'])\n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#14148b6ed8be99c813fab6b30142a716c210fc57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 628 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ def expander_func_with_args(value, other=None, loader_args=None):\n         return loader_args['key']\n     return value\n \n-class Loader(unittest.TestCase):\n+class LoaderTest(unittest.TestCase):\n \n     def test_get_item_using_default_loader(self):\n         i = TestItem()\n@@ -203,7 +203,7 @@ class Loader(unittest.TestCase):\n         self.assertEqual(il.get_reduced_value('url'), 'val')\n \n     def test_expander_args_on_assign(self):\n-        class ChildLoadeLoader(TestLoader):\n+        class ChildLoader(TestLoader):\n             url_exp = TreeExpander(expander_func_with_args)\n \n         il = ChildLoader()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9aaf010af3abe57b31a48dd0c6dd6850107faf06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 24 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,6 @@ See documentation in docs/ref/extensions.rst\n \n import sys\n import os\n-import pprint\n import socket\n \n from scrapy.xlib.pydispatch import dispatcher\n@@ -18,6 +17,7 @@ from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n+from scrapy.stats import stats\n \n class MemoryUsage(object):\n     \n@@ -68,10 +68,12 @@ class MemoryUsage(object):\n \n     def engine_started(self):\n         self.data['startup'] = self.virtual\n+        stats.set_value('memusage/startup', int(self.virtual))\n \n     def update(self):\n         if self.virtual > self.data['max']:\n             self.data['max'] = self.virtual\n+            stats.set_value('memusage/max', int(self.virtual))\n \n     def _vmvalue(self, VmKey):\n         # get pseudo file  /proc/<pid>/status\n@@ -91,22 +93,30 @@ class MemoryUsage(object):\n \n     def _check_limit(self):\n         if self.virtual > self.limit:\n+            stats.set_value('memusage/limit_reached', 1)\n             mem = self.limit/1024/1024\n             log.msg(\"Memory usage exceeded %dM. Shutting down Scrapy...\" % mem, level=log.ERROR)\n             if self.notify_mails:\n-                subj = \"%s terminated: memory usage exceeded %dM at %s\" % (settings['BOT_NAME'], mem, socket.gethostname())\n+                subj = \"%s terminated: memory usage exceeded %dM at %s\" % \\\n+                        (settings['BOT_NAME'], mem, socket.gethostname())\n                 self._send_report(self.notify_mails, subj)\n+                stats.set_value('memusage/limit_notified', 1)\n             scrapymanager.stop()\n \n     def _check_warning(self):\n         if self.warned: # warn only once\n             return\n         if self.virtual > self.warning:\n+            stats.set_value('memusage/warning_reached', 1)\n             mem = self.warning/1024/1024\n             log.msg(\"Memory usage reached %dM\" % mem, level=log.WARNING)\n             if self.notify_mails:\n-                subj = \"%s warning: memory usage reached %dM at %s\" % (settings['BOT_NAME'], mem, socket.gethostname())\n+                subj = \"%s warning: memory usage reached %dM at %s\" % \\\n+                        (settings['BOT_NAME'], mem, socket.gethostname())\n                 self._send_report(self.notify_mails, subj)\n+                stats.set_value('memusage/warning_notified', 1)\n+            else:\n+                stats.set_value('memusage/warning_notified', 0)\n             self.warned = True\n \n     def _send_report(self, rcpts, subject):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#417f0a3a20bcb56380a38a3be2b29d4ab7538e4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 26 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -115,8 +115,6 @@ class MemoryUsage(object):\n                         (settings['BOT_NAME'], mem, socket.gethostname())\n                 self._send_report(self.notify_mails, subj)\n                 stats.set_value('memusage/warning_notified', 1)\n-            else:\n-                stats.set_value('memusage/warning_notified', 0)\n             self.warned = True\n \n     def _send_report(self, rcpts, subject):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1339b182189596c3022c6d88037ae34490c45d4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 37 | Churn Cumulative: 80 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -106,7 +106,7 @@ def execute_with_args(argv):\n         print usage(argv)\n         sys.exit(2)\n \n-    parser = optparse.OptionParser()\n+    parser = optparse.OptionParser(conflict_handler='resolve')\n \n     if cmdname in cmds:\n         cmd = cmds[cmdname]\n\n@@ -1,6 +1,9 @@\n \"\"\"\n Base class for Scrapy commands\n \"\"\"\n+\n+from __future__ import with_statement\n+\n import os\n import sys\n from scrapy.conf import settings\n@@ -38,15 +41,22 @@ class ScrapyCommand(object):\n         \"\"\"\n         Populate option parse with options available for this command\n         \"\"\"\n-        parser.add_option(\"-f\", \"--logfile\", dest=\"logfile\", help=\"logfile to use. if omitted stderr will be used\", metavar=\"FILE\")\n-        parser.add_option(\"-o\", \"--loglevel\", dest=\"loglevel\", default=None, help=\"log level\")\n-        parser.add_option(\"--default-spider\", dest=\"default_spider\", default=None, help=\"default spider (domain) to use if no spider is found\")\n-        parser.add_option(\"--spider\", dest=\"spider\", default=None, help=\"Force using the given spider when the arguments are urls\")\n-        parser.add_option(\"--nolog\", dest=\"nolog\", action=\"store_true\", help=\"disable all log messages\")\n-        parser.add_option(\"--profile\", dest=\"profile\", default=None, help=\"write profiling stats in FILE, to analyze later with: python -m pstats FILE\", metavar=\"FILE\")\n-        parser.add_option(\"--pidfile\", dest=\"pidfile\", help=\"Write process pid to file FILE\", metavar=\"FILE\")\n-        parser.add_option(\"--set\", dest=\"settings\", action=\"append\", metavar=\"SETTING=VALUE\", default=[],\n-                            help=\"Override Scrapy setting SETTING with VALUE. May be repeated.\")\n+        parser.add_option(\"--logfile\", dest=\"logfile\", metavar=\"FILE\", \\\n+            help=\"log file. if omitted stderr will be used\")\n+        parser.add_option(\"-L\", \"--loglevel\", dest=\"loglevel\", metavar=\"LEVEL\", \\\n+            default=None, \\\n+            help=\"log level. use SILENT level to diasble all log messages\")\n+        parser.add_option(\"--default-spider\", dest=\"default_spider\", default=None, \\\n+            help=\"use this spider when arguments are urls and no spider is found\")\n+        parser.add_option(\"--spider\", dest=\"spider\", default=None, \\\n+            help=\"always use this spider when arguments are urls\")\n+        parser.add_option(\"--profile\", dest=\"profile\", metavar=\"FILE\", default=None, \\\n+            help=\"write python cProfile stats to FILE\")\n+        parser.add_option(\"--pidfile\", dest=\"pidfile\", metavar=\"FILE\", \\\n+            help=\"write process ID to FILE\")\n+        parser.add_option(\"--set\", dest=\"settings\", action=\"append\", \\\n+            metavar=\"SETTING=VALUE\", default=[], \\\n+            help=\"set/override setting (may be repeated)\")\n         \n     def process_options(self, args, opts):\n         if opts.logfile:\n@@ -57,9 +67,6 @@ class ScrapyCommand(object):\n             settings.overrides['LOG_ENABLED'] = True\n             settings.overrides['LOGLEVEL'] = opts.loglevel\n \n-        if opts.nolog:\n-            settings.overrides['LOG_ENABLED'] = False\n-\n         if opts.default_spider:\n             from scrapy.spider import spiders\n             spiders.default_domain = opts.default_spider\n@@ -69,8 +76,8 @@ class ScrapyCommand(object):\n             spiders.force_domain = opts.spider\n \n         if opts.pidfile:\n-            pid = os.getpid()\n-            open(opts.pidfile, \"w\").write(str(pid))\n+            with open(opts.pidfile, \"w\") as f:\n+                f.write(str(os.getpid()))\n \n         for setting in opts.settings:\n             if '=' in setting:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b20f1f7d93df14620f471f84adf17058262e782", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 25 | Files Changed: 2 | Hunks: 23 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 72 | Churn Cumulative: 152 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 0.6842105263157895\n\nDIFF:\n@@ -8,6 +8,7 @@ import cProfile\n import scrapy\n from scrapy import log\n from scrapy.spider import spiders\n+from scrapy.xlib import lsprofcalltree\n from scrapy.conf import settings, SETTINGS_MODULE\n \n def find_commands(dir):\n@@ -48,10 +49,15 @@ def getcmdname(argv):\n         if not arg.startswith('-'):\n             return arg\n \n-def usage(argv):\n-    s  = \"usage: %s <command> [options] [args]\\n\" % argv[0]\n-    s += \"       %s <command> -h\\n\\n\" % argv[0]\n-    s += \"Available commands:\\n\\n\"\n+def usage(prog):\n+    s = \"Usage\\n\"\n+    s += \"=====\\n\"\n+    s += \"%s <command> [options] [args]\\n\" % prog\n+    s += \"  Run a command\\n\\n\"\n+    s += \"%s <command> -h\\n\" % prog\n+    s += \"  Print command help and options\\n\\n\"\n+    s += \"Available commands\\n\"\n+    s += \"===================\\n\"\n \n     cmds = builtin_commands_dict()\n     cmds.update(custom_commands_dict())\n@@ -103,10 +109,11 @@ def execute_with_args(argv):\n \n     if not cmdname:\n         print \"Scrapy %s\\n\" % scrapy.__version__\n-        print usage(argv)\n+        print usage(argv[0])\n         sys.exit(2)\n \n-    parser = optparse.OptionParser(conflict_handler='resolve')\n+    parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \\\n+        conflict_handler='resolve', add_help_option=False)\n \n     if cmdname in cmds:\n         cmd = cmds[cmdname]\n@@ -122,6 +129,10 @@ def execute_with_args(argv):\n     (opts, args) = parser.parse_args(args=argv[1:])\n     del args[0]  # args[0] is cmdname\n \n+    if opts.help:\n+        parser.print_help()\n+        sys.exit()\n+\n     # storing command executed info for later reference\n     command_executed['name'] = cmdname\n     command_executed['class'] = cmd\n@@ -131,20 +142,20 @@ def execute_with_args(argv):\n     cmd.process_options(args, opts)\n     spiders.load()\n     log.start()\n+    if opts.profile or opts.lsprof:\n         if opts.profile:\n-        log.msg(\"Profiling enabled. Analyze later with: python -m pstats %s\" % opts.profile)\n+            log.msg(\"writing cProfile stats to %r\" % opts.profile)\n+        if opts.lsprof:\n+            log.msg(\"writing lsprof stats to %r\" % opts.lsprof)\n         loc = locals()\n         p = cProfile.Profile()\n         p.runctx('ret = cmd.run(args, opts)', globals(), loc)\n+        if opts.profile:\n             p.dump_stats(opts.profile)\n-        try:\n-            from scrapy.xlib import lsprofcalltree\n-            fn = opts.profile + \".cachegrind\"\n         k = lsprofcalltree.KCacheGrind(p)\n-            with open(fn, 'w') as f:\n+        if opts.lsprof:\n+            with open(opts.lsprof, 'w') as f:\n                 k.output(f)\n-        except ImportError:\n-            pass\n         ret = loc['ret']\n     else:\n         ret = cmd.run(args, opts)\n\n@@ -6,6 +6,8 @@ from __future__ import with_statement\n \n import os\n import sys\n+from optparse import OptionGroup\n+\n from scrapy.conf import settings\n \n class ScrapyCommand(object):\n@@ -22,16 +24,14 @@ class ScrapyCommand(object):\n         return \"\"\n \n     def long_desc(self):\n-        \"\"\"\n-        A long description of the command. Return short description when not\n+        \"\"\"A long description of the command. Return short description when not\n         available. It cannot contain newlines, since contents will be formatted\n         by optparser which removes newlines and wraps text.\n         \"\"\"\n         return self.short_desc()\n \n     def help(self):\n-        \"\"\"\n-        An extensive help for the command. It will be shown when using the\n+        \"\"\"An extensive help for the command. It will be shown when using the\n         \"help\" command. It can contain newlines, since not post-formatting will\n         be applied to its contents.\n         \"\"\"\n@@ -41,22 +41,30 @@ class ScrapyCommand(object):\n         \"\"\"\n         Populate option parse with options available for this command\n         \"\"\"\n-        parser.add_option(\"--logfile\", dest=\"logfile\", metavar=\"FILE\", \\\n+        group = OptionGroup(parser, \"Global Options\")\n+        group.add_option(\"-h\", \"--help\", action=\"store_true\", dest=\"help\", \\\n+            help=\"print command help and options\")\n+        group.add_option(\"--logfile\", dest=\"logfile\", metavar=\"FILE\", \\\n             help=\"log file. if omitted stderr will be used\")\n-        parser.add_option(\"-L\", \"--loglevel\", dest=\"loglevel\", metavar=\"LEVEL\", \\\n+        group.add_option(\"-L\", \"--loglevel\", dest=\"loglevel\", metavar=\"LEVEL\", \\\n             default=None, \\\n-            help=\"log level. use SILENT level to diasble all log messages\")\n-        parser.add_option(\"--default-spider\", dest=\"default_spider\", default=None, \\\n+            help=\"log level (default: %s)\" % settings['LOGLEVEL'])\n+        group.add_option(\"--nolog\", action=\"store_true\", dest=\"nolog\", \\\n+            help=\"disable logging completely\")\n+        group.add_option(\"--default-spider\", dest=\"default_spider\", default=None, \\\n             help=\"use this spider when arguments are urls and no spider is found\")\n-        parser.add_option(\"--spider\", dest=\"spider\", default=None, \\\n+        group.add_option(\"--spider\", dest=\"spider\", default=None, \\\n             help=\"always use this spider when arguments are urls\")\n-        parser.add_option(\"--profile\", dest=\"profile\", metavar=\"FILE\", default=None, \\\n+        group.add_option(\"--profile\", dest=\"profile\", metavar=\"FILE\", default=None, \\\n             help=\"write python cProfile stats to FILE\")\n-        parser.add_option(\"--pidfile\", dest=\"pidfile\", metavar=\"FILE\", \\\n+        group.add_option(\"--lsprof\", dest=\"lsprof\", metavar=\"FILE\", default=None, \\\n+            help=\"write lsprof profiling stats to FILE\")\n+        group.add_option(\"--pidfile\", dest=\"pidfile\", metavar=\"FILE\", \\\n             help=\"write process ID to FILE\")\n-        parser.add_option(\"--set\", dest=\"settings\", action=\"append\", \\\n+        group.add_option(\"--set\", dest=\"settings\", action=\"append\", \\\n             metavar=\"SETTING=VALUE\", default=[], \\\n             help=\"set/override setting (may be repeated)\")\n+        parser.add_option_group(group)\n         \n     def process_options(self, args, opts):\n         if opts.logfile:\n@@ -67,6 +75,9 @@ class ScrapyCommand(object):\n             settings.overrides['LOG_ENABLED'] = True\n             settings.overrides['LOGLEVEL'] = opts.loglevel\n \n+        if opts.nolog:\n+            settings.overrides['LOG_ENABLED'] = False\n+\n         if opts.default_spider:\n             from scrapy.spider import spiders\n             spiders.default_domain = opts.default_spider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5786bbd26a596e8daf4bb4be90eede0b117ab18a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 55 | Files Changed: 1 | Hunks: 16 | Methods Changed: 14 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 91 | Churn Cumulative: 165 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,40 +11,36 @@ from scrapy.spider import spiders\n from scrapy.xlib import lsprofcalltree\n from scrapy.conf import settings, SETTINGS_MODULE\n \n+# This dict holds information about the executed command for later use\n+command_executed = {}\n+\n def find_commands(dir):\n     try:\n-        return [f[:-3] for f in os.listdir(dir) if not f.startswith('_') and f.endswith('.py')]\n+        return [f[:-3] for f in os.listdir(dir) if not f.startswith('_') and \\\n+            f.endswith('.py')]\n     except OSError:\n         return []\n \n-def builtin_commands_dict():\n+def get_commands_from_module(module):\n     d = {}\n-    scrapy_dir = scrapy.__path__[0]\n-    commands_dir = os.path.join(scrapy_dir, 'command', 'commands')\n-    for cmdname in find_commands(commands_dir):\n-        modname = 'scrapy.command.commands.%s' % cmdname\n-        command = getattr(__import__(modname, {}, {}, [cmdname]), 'Command', None)\n-        if callable(command):\n-            d[cmdname] = command()\n-        else:\n-            print 'WARNING: Builtin command module %s exists but Command class not found' % modname\n-    return d\n-\n-def custom_commands_dict():\n-    d = {}\n-    cmdsmod = settings['COMMANDS_MODULE']\n-    if cmdsmod:\n-        mod = __import__(cmdsmod, {}, {}, [''])\n+    mod = __import__(module, {}, {}, [''])\n     for cmdname in find_commands(mod.__path__[0]):\n-            modname = '%s.%s' % (cmdsmod, cmdname)\n+        modname = '%s.%s' % (module, cmdname)\n         command = getattr(__import__(modname, {}, {}, [cmdname]), 'Command', None)\n         if callable(command):\n             d[cmdname] = command()\n         else:\n-                print 'WARNING: Custom command module %s exists but Command class not found' % modname\n+            print 'WARNING: Module %r does not define a Command class' % modname\n     return d\n \n-def getcmdname(argv):\n+def get_commands_dict():\n+    cmds = get_commands_from_module('scrapy.command.commands')\n+    cmds_module = settings['COMMANDS_MODULE']\n+    if cmds_module:\n+        cmds.update(get_commands_from_module(cmds_module))\n+    return cmds\n+\n+def get_command_name(argv):\n     for arg in argv[1:]:\n         if not arg.startswith('-'):\n             return arg\n@@ -58,54 +54,35 @@ def usage(prog):\n     s += \"  Print command help and options\\n\\n\"\n     s += \"Available commands\\n\"\n     s += \"===================\\n\"\n-\n-    cmds = builtin_commands_dict()\n-    cmds.update(custom_commands_dict())\n-\n+    cmds = get_commands_dict()\n     for cmdname, cmdclass in sorted(cmds.iteritems()):\n         s += \"%s %s\\n\" % (cmdname, cmdclass.syntax())\n         s += \"  %s\\n\" % cmdclass.short_desc()\n-\n     return s\n \n-\n-def update_defaults(defaults, module):\n-    settingsdict = vars(module)\n+def update_default_settings(module, cmdname):\n+    try:\n+        mod = __import__('%s.%s' % (module, cmdname), {}, {}, [''])\n+    except ImportError:\n+        return\n+    settingsdict = vars(mod)\n     for k, v in settingsdict.iteritems():\n         if not k.startswith(\"_\"):\n-            defaults[k] = v\n-\n-def command_settings(cmdname):\n-    try:\n-        module = __import__('%s.%s' % ('scrapy.conf.commands', cmdname), {}, {}, [''])\n-        update_defaults(settings.defaults, module)\n-    except ImportError:\n-        pass\n-\n-    basepath = settings['COMMANDS_SETTINGS_MODULE']\n-    if basepath:\n-        try:\n-            module = __import__('%s.%s' % (basepath, cmdname), {}, {}, [''])\n-            update_defaults(settings.defaults, module)\n-        except ImportError:\n-            pass\n-\n-# This dict holds information about the executed command for later use\n-command_executed = {}\n+            settings.defaults[k] = v\n \n def execute():\n     if not settings.settings_module:\n         print \"Scrapy %s\\n\" % scrapy.__version__\n-        print \"Error: Cannot find %r module in python path.\" % SETTINGS_MODULE\n+        print \"Error: Cannot find %r module in python path\" % SETTINGS_MODULE\n         sys.exit(1)\n     execute_with_args(sys.argv)\n \n def execute_with_args(argv):\n-    cmds = builtin_commands_dict()\n-    cmds.update(custom_commands_dict())\n+    cmds = get_commands_dict()\n \n-    cmdname = getcmdname(argv)\n-    command_settings(cmdname)\n+    cmdname = get_command_name(argv)\n+    update_default_settings('scrapy.conf.commands', cmdname)\n+    update_default_settings(settings['COMMANDS_SETTINGS_MODULE'], cmdname)\n \n     if not cmdname:\n         print \"Scrapy %s\\n\" % scrapy.__version__\n@@ -142,6 +119,11 @@ def execute_with_args(argv):\n     cmd.process_options(args, opts)\n     spiders.load()\n     log.start()\n+    ret = run_command(cmd, args, opts)\n+    if ret is False:\n+        parser.print_help()\n+\n+def run_command(cmd, args, opts):\n     if opts.profile or opts.lsprof:\n         if opts.profile:\n             log.msg(\"writing cProfile stats to %r\" % opts.profile)\n@@ -159,5 +141,4 @@ def execute_with_args(argv):\n         ret = loc['ret']\n     else:\n         ret = cmd.run(args, opts)\n-    if ret is False:\n-        parser.print_help()\n+    return ret\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a67dc6c73cca7511840dfb180d2ae1d69b01e05b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#174ef14d0a2bef94d6247a3c93b48b670d157402", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 26 | Churn Cumulative: 113 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,8 @@\n+\"\"\"\n+Item pipeline manager\n+\n+See documentation in docs/item-pipeline.rst\n+\"\"\"\n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.item.models import BaseItem\n@@ -9,6 +14,8 @@ class ItemPipelineManager(object):\n \n     def __init__(self):\n         self.loaded = False\n+        self.enabled = {}\n+        self.disabled = {}\n         self.pipeline = []\n         self.load()\n \n@@ -16,15 +23,20 @@ class ItemPipelineManager(object):\n         \"\"\"\n         Load pipelines stages defined in settings module\n         \"\"\"\n-        for stage in settings.getlist('ITEM_PIPELINES') or ():\n-            cls = load_object(stage)\n+        self.enabled.clear()\n+        self.disabled.clear()\n+        for pipepath in settings.getlist('ITEM_PIPELINES'):\n+            cls = load_object(pipepath)\n             if cls:\n                 try:\n-                    stageinstance = cls()\n-                    self.pipeline.append(stageinstance)\n-                except NotConfigured:\n-                    pass\n-        log.msg(\"Enabled item pipelines: %s\" % \", \".join([type(p).__name__ for p in self.pipeline]),\n+                    pipe = cls()\n+                    self.pipeline.append(pipe)\n+                    self.enabled[cls.__name__] = pipe\n+                except NotConfigured, e:\n+                    self.disabled[cls.__name__] = pipepath\n+                    if e.args:\n+                        log.msg(e)\n+        log.msg(\"Enabled item pipelines: %s\" % \", \".join(self.enabled.keys()),\n             level=log.DEBUG)\n         self.loaded = True\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#661aeb5cda2d9e8665a14c4ef8c655329a6a5531", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 65 | Files Changed: 3 | Hunks: 3 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/12 | Churn Δ: 131 | Churn Cumulative: 389 | Contributors (this commit): 4 | Commits (past 90d): 35 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -110,7 +110,7 @@ HTTPCACHE_IGNORE_MISSING = False\n HTTPCACHE_SECTORIZE = True\n HTTPCACHE_EXPIRATION_SECS = 0\n \n-ITEM_PROCESSOR = 'scrapy.item.pipeline.ItemPipelineManager'\n+ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n # Item pipelines are typically set in specific commands settings\n ITEM_PIPELINES = []\n\n@@ -0,0 +1,65 @@\n+\"\"\"\n+Item pipeline\n+\n+See documentation in docs/item-pipeline.rst\n+\"\"\"\n+\n+from scrapy import log\n+from scrapy.core.exceptions import NotConfigured\n+from scrapy.item.models import BaseItem\n+from scrapy.utils.misc import load_object\n+from scrapy.utils.defer import defer_succeed, mustbe_deferred\n+from scrapy.conf import settings\n+\n+class ItemPipelineManager(object):\n+\n+    def __init__(self):\n+        self.loaded = False\n+        self.enabled = {}\n+        self.disabled = {}\n+        self.pipeline = []\n+        self.load()\n+\n+    def load(self):\n+        \"\"\"\n+        Load pipelines stages defined in settings module\n+        \"\"\"\n+        self.enabled.clear()\n+        self.disabled.clear()\n+        for pipepath in settings.getlist('ITEM_PIPELINES'):\n+            cls = load_object(pipepath)\n+            if cls:\n+                try:\n+                    pipe = cls()\n+                    self.pipeline.append(pipe)\n+                    self.enabled[cls.__name__] = pipe\n+                except NotConfigured, e:\n+                    self.disabled[cls.__name__] = pipepath\n+                    if e.args:\n+                        log.msg(e)\n+        log.msg(\"Enabled item pipelines: %s\" % \", \".join(self.enabled.keys()),\n+            level=log.DEBUG)\n+        self.loaded = True\n+\n+    def open_domain(self, domain):\n+        pass\n+\n+    def close_domain(self, domain):\n+        pass\n+\n+    def process_item(self, item, spider):\n+        if not self.pipeline:\n+            return defer_succeed(item)\n+\n+        def next_stage(item, stages_left):\n+            assert isinstance(item, BaseItem), \\\n+                'Item pipelines must return a BaseItem, got %s' % type(item).__name__\n+            if not stages_left:\n+                return item\n+            current_stage = stages_left.pop(0)\n+            d = mustbe_deferred(current_stage.process_item, spider.domain_name, item)\n+            d.addCallback(next_stage, stages_left)\n+            return d\n+\n+        deferred = mustbe_deferred(next_stage, item, self.pipeline[:])\n+        return deferred\n\n@@ -1,64 +0,0 @@\n-\"\"\"\n-Item pipeline manager\n-\n-See documentation in docs/item-pipeline.rst\n-\"\"\"\n-from scrapy import log\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.item.models import BaseItem\n-from scrapy.utils.misc import load_object\n-from scrapy.utils.defer import defer_succeed, mustbe_deferred\n-from scrapy.conf import settings\n-\n-class ItemPipelineManager(object):\n-\n-    def __init__(self):\n-        self.loaded = False\n-        self.enabled = {}\n-        self.disabled = {}\n-        self.pipeline = []\n-        self.load()\n-\n-    def load(self):\n-        \"\"\"\n-        Load pipelines stages defined in settings module\n-        \"\"\"\n-        self.enabled.clear()\n-        self.disabled.clear()\n-        for pipepath in settings.getlist('ITEM_PIPELINES'):\n-            cls = load_object(pipepath)\n-            if cls:\n-                try:\n-                    pipe = cls()\n-                    self.pipeline.append(pipe)\n-                    self.enabled[cls.__name__] = pipe\n-                except NotConfigured, e:\n-                    self.disabled[cls.__name__] = pipepath\n-                    if e.args:\n-                        log.msg(e)\n-        log.msg(\"Enabled item pipelines: %s\" % \", \".join(self.enabled.keys()),\n-            level=log.DEBUG)\n-        self.loaded = True\n-\n-    def open_domain(self, domain):\n-        pass\n-\n-    def close_domain(self, domain):\n-        pass\n-\n-    def process_item(self, item, spider):\n-        if not self.pipeline:\n-            return defer_succeed(item)\n-\n-        def next_stage(item, stages_left):\n-            assert isinstance(item, BaseItem), \\\n-                'Item pipelines must return a BaseItem, got %s' % type(item).__name__\n-            if not stages_left:\n-                return item\n-            current_stage = stages_left.pop(0)\n-            d = mustbe_deferred(current_stage.process_item, spider.domain_name, item)\n-            d.addCallback(next_stage, stages_left)\n-            return d\n-\n-        deferred = mustbe_deferred(next_stage, item, self.pipeline[:])\n-        return deferred\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#10cdc70fc4c2ce441fb3c0a7a9ee9621c0d2eb63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 22 | Files Changed: 4 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 46 | Churn Cumulative: 65 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,12 +3,12 @@ This module contains a simple class (Libxml2Document) to wrap libxml2 documents\n (xmlDoc) for proper garbage collection.\n \"\"\"\n \n-from scrapy.xpath.constructors import xmlDoc_from_html\n+from scrapy.xpath.factories import xmlDoc_from_html\n \n class Libxml2Document(object):\n \n-    def __init__(self, response, constructor=xmlDoc_from_html):\n-        self.xmlDoc = constructor(response)\n+    def __init__(self, response, factory=xmlDoc_from_html):\n+        self.xmlDoc = factory(response)\n         self.xpathContext = self.xmlDoc.xpathNewContext()\n \n     def __del__(self):\n\n@@ -5,16 +5,16 @@ The ResponseLibxml2 extension causes the Response objects to grow a new method\n \n from scrapy.http import Response\n from scrapy.xpath.document import Libxml2Document\n-from scrapy.xpath.constructors import xmlDoc_from_html\n+from scrapy.xpath.factories import xmlDoc_from_html\n \n class ResponseLibxml2(object):\n     def __init__(self):\n         setattr(Response, 'getlibxml2doc', getlibxml2doc)\n \n-def getlibxml2doc(response, constructor=xmlDoc_from_html):\n-    cachekey = 'lx2doc_%s' % constructor.__name__\n+def getlibxml2doc(response, factory=xmlDoc_from_html):\n+    cachekey = 'lx2doc_%s' % factory.__name__\n     if cachekey not in response.cache:\n-        lx2doc = Libxml2Document(response, constructor=constructor)\n+        lx2doc = Libxml2Document(response, factory=factory)\n         response.cache[cachekey] = lx2doc\n     return response.cache[cachekey]\n \n\n\n@@ -8,7 +8,7 @@ import libxml2\n \n from scrapy.http import TextResponse\n from scrapy.xpath.extension import Libxml2Document\n-from scrapy.xpath.constructors import xmlDoc_from_html, xmlDoc_from_xml\n+from scrapy.xpath.factories import xmlDoc_from_html, xmlDoc_from_xml\n from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.misc import extract_regex\n \n@@ -23,19 +23,23 @@ class XPathSelector(object):\n     - HtmlXPathSelector (for HTML content)\n     \"\"\"\n \n-    def __init__(self, response=None, text=None, node=None, parent=None, expr=None, constructor=xmlDoc_from_html):\n+    xmlDoc_factory = staticmethod(xmlDoc_from_html)\n+\n+    def __init__(self, response=None, text=None, node=None, parent=None, expr=None):\n         if parent:\n             self.doc = parent.doc\n             self.xmlNode = node\n         elif response:\n             try:\n-                self.doc = response.getlibxml2doc(constructor=constructor)  # try with cached version first\n+                # try with cached version first\n+                self.doc = response.getlibxml2doc(factory=self.xmlDoc_factory)\n             except AttributeError:\n-                self.doc = Libxml2Document(response, constructor=constructor)\n+                self.doc = Libxml2Document(response, factory=self.xmlDoc_factory)\n             self.xmlNode = self.doc.xmlDoc\n         elif text:\n-            response = TextResponse(url=None, body=unicode_to_str(text), encoding='utf-8')\n-            self.doc = Libxml2Document(response, constructor=constructor)\n+            response = TextResponse(url=None, body=unicode_to_str(text), \\\n+                encoding='utf-8')\n+            self.doc = Libxml2Document(response, factory=self.xmlDoc_factory)\n             self.xmlNode = self.doc.xmlDoc\n         self.expr = expr\n         self.response = response\n@@ -51,10 +55,11 @@ class XPathSelector(object):\n                 raise ValueError(\"Invalid XPath: %s\" % xpath)\n             cls = type(self)\n             if hasattr(xpath_result, '__iter__'):\n-                return XPathSelectorList([cls(node=node, parent=self, expr=xpath, response=self.response)\n-                                          for node in xpath_result])\n+                return XPathSelectorList([cls(node=node, parent=self, expr=xpath, \\\n+                    response=self.response) for node in xpath_result])\n             else:\n-                return XPathSelectorList([cls(node=xpath_result, parent=self, expr=xpath, response=self.response)])\n+                return XPathSelectorList([cls(node=xpath_result, parent=self, \\\n+                    expr=xpath, response=self.response)])\n         else:\n             return XPathSelectorList([])\n     __call__ = x\n@@ -132,12 +137,9 @@ class XPathSelectorList(list):\n \n class XmlXPathSelector(XPathSelector):\n     \"\"\"XPathSelector for XML content\"\"\"\n-    def __init__(self, *args, **kwargs):\n-        kwargs['constructor'] = xmlDoc_from_xml\n-        XPathSelector.__init__(self, *args, **kwargs)\n+    xmlDoc_factory = staticmethod(xmlDoc_from_xml)\n+\n \n class HtmlXPathSelector(XPathSelector):\n     \"\"\"XPathSelector for HTML content\"\"\"\n-    def __init__(self, *args, **kwargs):\n-        kwargs['constructor'] = xmlDoc_from_html\n-        XPathSelector.__init__(self, *args, **kwargs)\n+    xmlDoc_factory = staticmethod(xmlDoc_from_html)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91eea82eefbc60e8c9824faf49fb510ad4aabc50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 52 | Churn Cumulative: 957 | Contributors (this commit): 1 | Commits (past 90d): 18 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ from scrapy.utils.datatypes import MergeDict\n from scrapy.newitem import Item\n from scrapy.newitem.loader.reducers import TakeFirst\n from scrapy.newitem.loader.expanders import IdentityExpander\n+from scrapy.xpath import HtmlXPathSelector\n \n class Loader(object):\n \n@@ -12,7 +13,7 @@ class Loader(object):\n     default_reducer = TakeFirst()\n \n     def __init__(self, item=None, **loader_args):\n-        if not item:\n+        if item is None:\n             item = self.default_item_class()\n         self._item = loader_args['item'] = item\n         self._loader_args = loader_args\n@@ -59,3 +60,26 @@ class Loader(object):\n             loader_args = MergeDict(new_loader_args, self._loader_args)\n         expander = self.get_expander(field_name)\n         return expander(value, loader_args=loader_args)\n+\n+\n+class XPathLoader(Loader):\n+\n+    default_selector_class = HtmlXPathSelector\n+\n+    def __init__(self, item=None, selector=None, response=None, **loader_args):\n+        if selector is None and response is None:\n+            raise RuntimeError(\"%s must be instantiated with a selector\" \\\n+                \"or response\" % self.__class__.__name__)\n+        if selector is None:\n+            selector = self.default_selector_class(response)\n+        self._selector = selector\n+        loader_args.update(selector=selector, response=response)\n+        super(XPathLoader, self).__init__(item, **loader_args)\n+\n+    def add_xpath(self, field_name, xpath, **new_loader_args):\n+        self.add_value(field_name, self._selector.x(xpath).extract(), \\\n+            **new_loader_args)\n+\n+    def replace_xpath(self, field_name, xpath, **new_loader_args):\n+        self.replace_value(field_name, self._selector.x(xpath).extract(), \\\n+            **new_loader_args)\n\n@@ -1,9 +1,11 @@\n import unittest\n \n-from scrapy.newitem.loader import Loader\n+from scrapy.newitem.loader import Loader, XPathLoader\n from scrapy.newitem.loader.expanders import TreeExpander, IdentityExpander\n from scrapy.newitem.loader.reducers import Join, Identity\n from scrapy.newitem import Item, Field\n+from scrapy.xpath import HtmlXPathSelector\n+from scrapy.http import HtmlResponse\n \n # test items\n \n@@ -226,5 +228,27 @@ class LoaderTest(unittest.TestCase):\n         il = TestLoader()\n         self.assertRaises(KeyError, il.add_value, 'wrong_field', [u'lala', u'lolo'])\n \n+\n+class TestXPathLoader(XPathLoader):\n+    default_item_class = TestItem\n+    name_exp = TreeExpander(lambda v: v.title())\n+\n+class XPathLoaderTest(unittest.TestCase):\n+\n+    def test_constructor_errors(self):\n+        self.assertRaises(RuntimeError, XPathLoader)\n+\n+    def test_constructor_with_selector(self):\n+        sel = HtmlXPathSelector(text=u\"<html><body><div>marta</div></body></html>\")\n+        l = TestXPathLoader(selector=sel)\n+        l.add_xpath('name', '//div/text()')\n+        self.assertEqual(l.get_reduced_value('name'), u'Marta')\n+\n+    def test_constructor_with_response(self):\n+        response = HtmlResponse(url=\"\", body=\"<html><body><div>marta</div></body></html>\")\n+        l = TestXPathLoader(response=response)\n+        l.add_xpath('name', '//div/text()')\n+        self.assertEqual(l.get_reduced_value('name'), u'Marta')\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
