{"custom_id": "keras#17d2a03ab00326cd35fe69f1ab850acad7e8f146", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 234 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 13/11 | Churn Δ: 234 | Churn Cumulative: 324 | Contributors (this commit): 7 | Commits (past 90d): 42 | Contributors (cumulative): 9 | DMM Complexity: 0.5581395348837209\n\nDIFF:\n@@ -54,6 +54,7 @@ from keras_core.layers.pooling.max_pooling2d import MaxPooling2D\n from keras_core.layers.pooling.max_pooling3d import MaxPooling3D\n from keras_core.layers.preprocessing.center_crop import CenterCrop\n from keras_core.layers.preprocessing.discretization import Discretization\n+from keras_core.layers.preprocessing.hashing import Hashing\n from keras_core.layers.preprocessing.normalization import Normalization\n from keras_core.layers.preprocessing.rescaling import Rescaling\n from keras_core.layers.preprocessing.resizing import Resizing\n\n@@ -0,0 +1,194 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.Hashing\")\n+class Hashing(Layer):\n+    \"\"\"A preprocessing layer which hashes and bins categorical features.\n+\n+    This layer transforms categorical inputs to hashed output. It element-wise\n+    converts a ints or strings to ints in a fixed range. The stable hash\n+    function uses `tensorflow::ops::Fingerprint` to produce the same output\n+    consistently across all platforms.\n+\n+    This layer uses [FarmHash64](https://github.com/google/farmhash) by default,\n+    which provides a consistent hashed output across different platforms and is\n+    stable across invocations, regardless of device and context, by mixing the\n+    input bits thoroughly.\n+\n+    If you want to obfuscate the hashed output, you can also pass a random\n+    `salt` argument in the constructor. In that case, the layer will use the\n+    [SipHash64](https://github.com/google/highwayhash) hash function, with\n+    the `salt` value serving as additional input to the hash function.\n+\n+    **Note:** This layer wraps `tf.keras.layers.Hashing`. It cannot\n+    be used as part of the compiled computation graph of a model with\n+    any backend other than TensorFlow.\n+    It can however be used with any backend when running eagerly.\n+    It can also always be used as part of an input preprocessing pipeline\n+    with any backend (outside the model itself), which is how we recommend\n+    to use this layer.\n+\n+    **Example (FarmHash64)**\n+\n+    >>> layer = keras_core.layers.Hashing(num_bins=3)\n+    >>> inp = [['A'], ['B'], ['C'], ['D'], ['E']]\n+    >>> layer(inp)\n+    array([[1],\n+            [0],\n+            [1],\n+            [1],\n+            [2]])>\n+\n+    **Example (FarmHash64) with a mask value**\n+\n+    >>> layer = keras_core.layers.Hashing(num_bins=3, mask_value='')\n+    >>> inp = [['A'], ['B'], [''], ['C'], ['D']]\n+    >>> layer(inp)\n+    array([[1],\n+            [1],\n+            [0],\n+            [2],\n+            [2]])\n+\n+    **Example (SipHash64)**\n+\n+    >>> layer = keras_core.layers.Hashing(num_bins=3, salt=[133, 137])\n+    >>> inp = [['A'], ['B'], ['C'], ['D'], ['E']]\n+    >>> layer(inp)\n+    array([[1],\n+            [2],\n+            [1],\n+            [0],\n+            [2]])\n+\n+    **Example (Siphash64 with a single integer, same as `salt=[133, 133]`)**\n+\n+    >>> layer = keras_core.layers.Hashing(num_bins=3, salt=133)\n+    >>> inp = [['A'], ['B'], ['C'], ['D'], ['E']]\n+    >>> layer(inp)\n+    array([[0],\n+            [0],\n+            [2],\n+            [1],\n+            [0]])\n+\n+    Args:\n+        num_bins: Number of hash bins. Note that this includes the `mask_value`\n+            bin, so the effective number of bins is `(num_bins - 1)`\n+            if `mask_value` is set.\n+        mask_value: A value that represents masked inputs, which are mapped to\n+            index 0. `None` means no mask term will be added and the\n+            hashing will start at index 0. Defaults to `None`.\n+        salt: A single unsigned integer or None.\n+            If passed, the hash function used will be SipHash64,\n+            with these values used as an additional input\n+            (known as a \"salt\" in cryptography).\n+            These should be non-zero. If `None`, uses the FarmHash64 hash\n+            function. It also supports tuple/list of 2 unsigned\n+            integer numbers, see reference paper for details.\n+            Defaults to `None`.\n+        output_mode: Specification for the output of the layer. Values can be\n+            `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or\n+            `\"count\"` configuring the layer as follows:\n+            - `\"int\"`: Return the integer bin indices directly.\n+            - `\"one_hot\"`: Encodes each individual element in the input into an\n+                array the same size as `num_bins`, containing a 1\n+                at the input's bin index. If the last dimension is size 1,\n+                will encode on that dimension.\n+                If the last dimension is not size 1, will append a new\n+                dimension for the encoded output.\n+            - `\"multi_hot\"`: Encodes each sample in the input into a\n+                single array the same size as `num_bins`,\n+                containing a 1 for each bin index\n+                index present in the sample. Treats the last dimension\n+                as the sample dimension, if input shape is\n+                `(..., sample_length)`, output shape will be\n+                `(..., num_tokens)`.\n+            - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of\n+                the number of times the bin index appeared in the sample.\n+            Defaults to `\"int\"`.\n+        sparse: Boolean. Only applicable to `\"one_hot\"`, `\"multi_hot\"`,\n+            and `\"count\"` output modes. Only supported with TensorFlow\n+            backend. If `True`, returns a `SparseTensor` instead of\n+            a dense `Tensor`. Defaults to `False`.\n+        **kwargs: Keyword arguments to construct a layer.\n+\n+    Input shape:\n+        A single string, a list of strings, or an `int32` or `int64` tensor\n+        of shape `(batch_size, ...,)`.\n+\n+    Output shape:\n+        An `int64` tensor of shape `(batch_size, ...)`.\n+\n+    Reference:\n+\n+    - [SipHash with salt](https://www.131002.net/siphash/siphash.pdf)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        num_bins,\n+        mask_value=None,\n+        salt=None,\n+        output_mode=\"int\",\n+        sparse=False,\n+        name=None,\n+        **kwargs,\n+    ):\n+        super().__init__(name=name)\n+        self.layer = tf.keras.layers.Hashing(\n+            num_bins=num_bins,\n+            mask_value=mask_value,\n+            salt=salt,\n+            output_mode=output_mode,\n+            sparse=sparse,\n+            name=name,\n+            **kwargs,\n+        )\n+        self.num_bins = num_bins\n+        self.mask_value = mask_value\n+        self.strong_hash = True if salt is not None else False\n+        self.output_mode = output_mode\n+        self.sparse = sparse\n+        self.salt = None\n+        if salt is not None:\n+            if isinstance(salt, (tuple, list)) and len(salt) == 2:\n+                self.salt = salt\n+            elif isinstance(salt, int):\n+                self.salt = [salt, salt]\n+            else:\n+                raise ValueError(\n+                    \"The `salt` argument for `Hashing` can only be a tuple \"\n+                    \"of 2 integers, or a single integer. \"\n+                    f\"Received: salt={salt}.\"\n+                )\n+        self._allow_non_tensor_positional_args = True\n+\n+    def call(self, inputs):\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(np.array(inputs))\n+        outputs = self.layer.call(inputs)\n+        if backend.backend() != \"tensorflow\":\n+            outputs = backend.convert_to_tensor(outputs)\n+        return outputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"num_bins\": self.num_bins,\n+                \"salt\": self.salt,\n+                \"mask_value\": self.mask_value,\n+                \"output_mode\": self.output_mode,\n+                \"sparse\": self.sparse,\n+            }\n+        )\n+        return config\n\n@@ -0,0 +1,39 @@\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class TextVectorizationTest(testing.TestCase):\n+    def test_config(self):\n+        layer = layers.Hashing(\n+            num_bins=8,\n+            output_mode=\"int\",\n+        )\n+        self.run_class_serialization_test(layer)\n+\n+    def test_correctness(self):\n+        layer = layers.Hashing(num_bins=3)\n+        inp = [[\"A\"], [\"B\"], [\"C\"], [\"D\"], [\"E\"]]\n+        output = layer(inp)\n+        self.assertTrue(backend.is_tensor(output))\n+        self.assertAllClose(output, np.array([[1], [0], [1], [1], [2]]))\n+\n+        layer = layers.Hashing(num_bins=3, mask_value=\"\")\n+        inp = [[\"A\"], [\"B\"], [\"\"], [\"C\"], [\"D\"]]\n+        output = layer(inp)\n+        self.assertTrue(backend.is_tensor(output))\n+        self.assertAllClose(output, np.array([[1], [1], [0], [2], [2]]))\n+\n+        layer = layers.Hashing(num_bins=3, salt=[133, 137])\n+        inp = [[\"A\"], [\"B\"], [\"C\"], [\"D\"], [\"E\"]]\n+        output = layer(inp)\n+        self.assertTrue(backend.is_tensor(output))\n+        self.assertAllClose(output, np.array([[1], [2], [1], [0], [2]]))\n+\n+        layer = layers.Hashing(num_bins=3, salt=133)\n+        inp = [[\"A\"], [\"B\"], [\"C\"], [\"D\"], [\"E\"]]\n+        output = layer(inp)\n+        self.assertTrue(backend.is_tensor(output))\n+        self.assertAllClose(output, np.array([[0], [0], [2], [1], [0]]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cf98a0e32fade62fd5e8b9fcde5fe3a0eb640564", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 235 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 10 | Complexity Δ (Sum/Max): 21/8 | Churn Δ: 235 | Churn Cumulative: 1139 | Contributors (this commit): 8 | Commits (past 90d): 38 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ from keras_core.callbacks.csv_logger import CSVLogger\n from keras_core.callbacks.early_stopping import EarlyStopping\n from keras_core.callbacks.history import History\n from keras_core.callbacks.lambda_callback import LambdaCallback\n+from keras_core.callbacks.learning_rate_scheduler import LearningRateScheduler\n from keras_core.callbacks.progbar_logger import ProgbarLogger\n from keras_core.callbacks.remote_monitor import RemoteMonitor\n from keras_core.callbacks.terminate_on_nan import TerminateOnNaN\n\n@@ -0,0 +1,79 @@\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.callbacks.callback import Callback\n+from keras_core.utils import io_utils\n+\n+\n+@keras_core_export(\"keras_core.callbacks.LearningRateScheduler\")\n+class LearningRateScheduler(Callback):\n+    \"\"\"Learning rate scheduler.\n+\n+    At the beginning of every epoch, this callback gets the updated learning\n+    rate value from `schedule` function provided at `__init__`, with the current\n+    epoch and current learning rate, and applies the updated learning rate on\n+    the optimizer.\n+\n+    Args:\n+        schedule: A function that takes an epoch index (integer, indexed from 0)\n+            and current learning rate (float) as inputs and returns a new\n+            learning rate as output (float).\n+        verbose: Integer. 0: quiet, 1: log update messages.\n+\n+    Example:\n+\n+    >>> # This function keeps the initial learning rate for the first ten epochs\n+    >>> # and decreases it exponentially after that.\n+    >>> def scheduler(epoch, lr):\n+    ...     if epoch < 10:\n+    ...         return lr\n+    ...     else:\n+    ...         return lr * ops.exp(-0.1)\n+    >>>\n+    >>> model = keras_core.models.Sequential([keras_core.layers.Dense(10)])\n+    >>> model.compile(keras_core.optimizers.SGD(), loss='mse')\n+    >>> round(model.optimizer.learning_rate, 5)\n+    0.01\n+\n+    >>> callback = keras_core.callbacks.LearningRateScheduler(scheduler)\n+    >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n+    ...                     epochs=15, callbacks=[callback], verbose=0)\n+    >>> round(model.optimizer.learning_rate, 5)\n+    0.00607\n+\n+    \"\"\"\n+\n+    def __init__(self, schedule, verbose=0):\n+        super().__init__()\n+        self.schedule = schedule\n+        self.verbose = verbose\n+\n+    def on_epoch_begin(self, epoch, logs=None):\n+        if not hasattr(self.model.optimizer, \"learning_rate\"):\n+            raise ValueError('Optimizer must have a \"learning_rate\" attribute.')\n+\n+        try:  # new API\n+            learning_rate = backend.Variable(\n+                self.model.optimizer.learning_rate\n+            ).numpy()\n+            learning_rate = self.schedule(epoch, learning_rate)\n+        except TypeError:  # Support for old API for backward compatibility\n+            learning_rate = self.schedule(epoch)\n+\n+        if not isinstance(learning_rate, (float, np.float32, np.float64)):\n+            raise ValueError(\n+                \"The output of the `schedule` function should be a float. \"\n+                f\"Got: {learning_rate}\"\n+            )\n+\n+        self.model.optimizer.learning_rate = learning_rate\n+        if self.verbose > 0:\n+            io_utils.print_msg(\n+                f\"\\nEpoch {epoch + 1}: LearningRateScheduler setting learning \"\n+                f\"rate to {learning_rate}.\"\n+            )\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n+        logs[\"learning_rate\"] = self.model.optimizer.learning_rate.value\n\n@@ -0,0 +1,102 @@\n+from keras_core import callbacks\n+from keras_core import layers\n+from keras_core import optimizers\n+from keras_core import testing\n+from keras_core.models import Sequential\n+from keras_core.testing import test_utils\n+from keras_core.utils import io_utils\n+from keras_core.utils import numerical_utils\n+\n+\n+class LearningRateSchedulerTest(testing.TestCase):\n+    def setUp(self):\n+        (x_train, y_train), _ = test_utils.get_test_data(\n+            train_samples=10,\n+            test_samples=10,\n+            input_shape=(3,),\n+            num_classes=2,\n+        )\n+        y_train = numerical_utils.to_categorical(y_train)\n+\n+        model = Sequential([layers.Dense(5), layers.Dense(2)])\n+\n+        model.compile(\n+            loss=\"mse\",\n+            optimizer=\"sgd\",\n+        )\n+\n+        self.model = model\n+        self.x_train = x_train\n+        self.y_train = y_train\n+\n+    def test_updates_learning_rate(self):\n+        lr_scheduler = callbacks.LearningRateScheduler(\n+            lambda step: 1.0 / (2.0 + step), verbose=1\n+        )\n+\n+        self.model.fit(\n+            self.x_train,\n+            self.y_train,\n+            callbacks=[lr_scheduler],\n+            epochs=1,\n+        )\n+\n+        self.assertEqual(self.model.optimizer.learning_rate.value, 0.5)\n+\n+    def test_verbose_logging(self):\n+        lr_scheduler = callbacks.LearningRateScheduler(\n+            lambda step: 1.0 / (1.0 + step), verbose=1\n+        )\n+        io_utils.disable_interactive_logging()\n+\n+        with self.assertLogs(level=\"INFO\") as logs:\n+            self.model.fit(\n+                self.x_train,\n+                self.y_train,\n+                callbacks=[lr_scheduler],\n+                epochs=1,\n+            )\n+            expected_log = \"LearningRateScheduler setting learning rate to 1.0\"\n+            self.assertTrue(any(expected_log in log for log in logs.output))\n+\n+    def test_schedule_dependent_on_previous_learning_rate(self):\n+        lr_scheduler = callbacks.LearningRateScheduler(lambda step, lr: lr / 2)\n+\n+        initial_lr = 0.03\n+        self.model.compile(\n+            loss=\"mse\",\n+            optimizer=optimizers.Adam(initial_lr),\n+        )\n+\n+        self.model.fit(\n+            self.x_train,\n+            self.y_train,\n+            callbacks=[lr_scheduler],\n+            epochs=2,\n+        )\n+        self.assertEqual(\n+            self.model.optimizer.learning_rate.value, initial_lr / 4.0\n+        )\n+\n+    def test_throws_when_optimizer_has_schedule(self):\n+        lr_scheduler = callbacks.LearningRateScheduler(lambda step, lr: lr / 2)\n+\n+        self.model.compile(\n+            loss=\"mse\",\n+            optimizer=optimizers.Adam(\n+                optimizers.schedules.PolynomialDecay(\n+                    initial_learning_rate=0.1, decay_steps=10\n+                )\n+            ),\n+        )\n+\n+        with self.assertRaisesRegex(\n+            TypeError,\n+            \"This optimizer was created with a `LearningRateSchedule`\",\n+        ):\n+            self.model.fit(\n+                self.x_train,\n+                self.y_train,\n+                callbacks=[lr_scheduler],\n+                epochs=2,\n+            )\n\n@@ -281,6 +281,27 @@ class Optimizer:\n     def learning_rate(self):\n         return self._get_current_learning_rate()\n \n+    @learning_rate.setter\n+    def learning_rate(self, learning_rate):\n+        if isinstance(\n+            learning_rate, learning_rate_schedule.LearningRateSchedule\n+        ):\n+            self._learning_rate = learning_rate\n+        elif callable(learning_rate):\n+            self._learning_rate = learning_rate\n+        else:\n+            if isinstance(\n+                self._learning_rate, learning_rate_schedule.LearningRateSchedule\n+            ):\n+                raise TypeError(\n+                    \"This optimizer was created with a `LearningRateSchedule`\"\n+                    \" object as its `learning_rate` constructor argument, \"\n+                    \"hence its learning rate is not settable. If you need the\"\n+                    \" learning rate to be settable, you should instantiate \"\n+                    \"the optimizer with a float `learning_rate` argument.\"\n+                )\n+            self._learning_rate.assign(learning_rate)\n+\n     def save_own_variables(self, store):\n         \"\"\"Get the state of this optimizer object.\"\"\"\n         for i, variable in enumerate(self.variables):\n\n@@ -0,0 +1,32 @@\n+import numpy as np\n+\n+\n+def get_test_data(\n+    train_samples, test_samples, input_shape, num_classes, random_seed=None\n+):\n+    \"\"\"Generates test data to train a model on.\n+\n+    Args:\n+        train_samples: Integer, how many training samples to generate.\n+        test_samples: Integer, how many test samples to generate.\n+        input_shape: Tuple of integers, shape of the inputs.\n+        num_classes: Integer, number of classes for the data and targets.\n+        random_seed: Integer, random seed used by Numpy to generate data.\n+\n+    Returns:\n+        A tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+    \"\"\"\n+    if random_seed is not None:\n+        np.random.seed(random_seed)\n+    num_sample = train_samples + test_samples\n+    templates = 2 * num_classes * np.random.random((num_classes,) + input_shape)\n+    y = np.random.randint(0, num_classes, size=(num_sample,))\n+    x = np.zeros((num_sample,) + input_shape, dtype=np.float32)\n+    for i in range(num_sample):\n+        x[i] = templates[y[i]] + np.random.normal(\n+            loc=0, scale=1.0, size=input_shape\n+        )\n+    return (\n+        (x[:train_samples], y[:train_samples]),\n+        (x[train_samples:], y[train_samples:]),\n+    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a42ea974171f908709c4ec9e519531c954eb49b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 84 | Churn Cumulative: 175 | Contributors (this commit): 7 | Commits (past 90d): 43 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,5 +69,6 @@ from keras_core.layers.regularization.spatial_dropout import SpatialDropout1D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout2D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout3D\n from keras_core.layers.reshaping.flatten import Flatten\n+from keras_core.layers.reshaping.repeat_vector import RepeatVector\n from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n\n@@ -0,0 +1,44 @@\n+from keras_core import operations as ops\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+class RepeatVector(Layer):\n+    \"\"\"Repeats the input n times.\n+\n+    Example:\n+\n+    >>> x = keras_core.Input(shape=(32,))\n+    >>> y = keras_core.layers.RepeatVector(3)(x)\n+    >>> y.shape\n+    (None, 3, 32)\n+\n+    Args:\n+        n: Integer, repetition factor.\n+    Input shape:\n+        2D tensor of shape `(batch_size, features)`.\n+    Output shape:\n+        3D tensor of shape `(batch_size, n, features)`.\n+    \"\"\"\n+\n+    def __init__(self, n, name=None, dtype=None):\n+        super().__init__(name=name, dtype=dtype)\n+        self.n = n\n+        if not isinstance(n, int):\n+            raise TypeError(\n+                f\"Expected an integer value for `n`, got {type(n)}.\"\n+            )\n+        self.input_spec = InputSpec(ndim=2)\n+\n+    def compute_output_shape(self, input_shape):\n+        return (input_shape[0], self.n, input_shape[1])\n+\n+    def call(self, inputs):\n+        input_shape = inputs.shape\n+        reshaped = ops.reshape(inputs, (input_shape[0], 1, input_shape[1]))\n+        return ops.repeat(reshaped, self.n, axis=1)\n+\n+    def get_config(self):\n+        config = {\"n\": self.n}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,39 @@\n+import numpy as np\n+import pytest\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core import testing\n+\n+\n+class FlattenTest(testing.TestCase):\n+    def test_repeat_vector(self):\n+        inputs = np.random.random((2, 5)).astype(\"float32\")\n+        expected_output = ops.convert_to_tensor(\n+            np.repeat(np.reshape(inputs, (2, 1, 5)), 3, axis=1)\n+        )\n+        self.run_layer_test(\n+            layers.RepeatVector,\n+            init_kwargs={\"n\": 3},\n+            input_data=inputs,\n+            expected_output=expected_output,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_repeat_vector_with_dynamic_batch_size(self):\n+        input_layer = layers.Input(batch_shape=(None, 5))\n+        repeated = layers.RepeatVector(n=3)(input_layer)\n+        self.assertEqual(repeated.shape, (None, 3, 5))\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_repeat_vector_with_dynamic_dimension(self):\n+        input_layer = layers.Input(batch_shape=(2, None))\n+        repeated = layers.RepeatVector(n=3)(input_layer)\n+        self.assertEqual(repeated.shape, (2, 3, None))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e1558381f464a68d7c6a87cb144dfba98b650ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 516 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 6 | Methods Changed: 22 | Complexity Δ (Sum/Max): 41/21 | Churn Δ: 516 | Churn Cumulative: 608 | Contributors (this commit): 7 | Commits (past 90d): 46 | Contributors (cumulative): 11 | DMM Complexity: 0.861198738170347\n\nDIFF:\n@@ -30,9 +30,15 @@ from keras_core.layers.merging.subtract import subtract\n from keras_core.layers.normalization.batch_normalization import (\n     BatchNormalization,\n )\n+from keras_core.layers.normalization.group_normalization import (\n+    GroupNormalization,\n+)\n from keras_core.layers.normalization.layer_normalization import (\n     LayerNormalization,\n )\n+from keras_core.layers.normalization.spectral_normalization import (\n+    SpectralNormalization,\n+)\n from keras_core.layers.normalization.unit_normalization import UnitNormalization\n from keras_core.layers.pooling.average_pooling1d import AveragePooling1D\n from keras_core.layers.pooling.average_pooling2d import AveragePooling2D\n\n@@ -0,0 +1,236 @@\n+from keras_core import constraints\n+from keras_core import initializers\n+from keras_core import operations as ops\n+from keras_core import regularizers\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.GroupNormalization\")\n+class GroupNormalization(Layer):\n+    \"\"\"Group normalization layer.\n+\n+    Group Normalization divides the channels into groups and computes\n+    within each group the mean and variance for normalization.\n+    Empirically, its accuracy is more stable than batch norm in a wide\n+    range of small batch sizes, if learning rate is adjusted linearly\n+    with batch sizes.\n+\n+    Relation to Layer Normalization:\n+    If the number of groups is set to 1, then this operation becomes nearly\n+    identical to Layer Normalization (see Layer Normalization docs for details).\n+\n+    Relation to Instance Normalization:\n+    If the number of groups is set to the input dimension (number of groups is\n+    equal to number of channels), then this operation becomes identical to\n+    Instance Normalization.\n+\n+    Args:\n+        groups: Integer, the number of groups for Group Normalization. Can be in\n+            the range `[1, N]` where N is the input dimension. The input\n+            dimension must be divisible by the number of groups.\n+            Defaults to 32.\n+        axis: Integer or List/Tuple. The axis or axes to normalize across.\n+            Typically, this is the features axis/axes. The left-out axes are\n+            typically the batch axis/axes. -1 is the last dimension in the\n+            input. Defaults to -1.\n+        epsilon: Small float added to variance to avoid dividing by zero.\n+            Defaults to 1e-3.\n+        center: If `True`, add offset of `beta` to normalized tensor.\n+            If `False`, `beta` is ignored. Defaults to `True`.\n+        scale: If `True`, multiply by `gamma`. If `False`, `gamma` is not used.\n+            When the next layer is linear (also e.g. `relu`), this can be\n+            disabled since the scaling will be done by the next layer.\n+            Defaults to `True`.\n+        beta_initializer: Initializer for the beta weight. Defaults to zeros.\n+        gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n+        beta_regularizer: Optional regularizer for the beta weight. None by\n+            default.\n+        gamma_regularizer: Optional regularizer for the gamma weight. None by\n+            default.\n+        beta_constraint: Optional constraint for the beta weight.\n+            None by default.\n+        gamma_constraint: Optional constraint for the gamma weight. None by\n+            default.  Input shape: Arbitrary. Use the keyword argument\n+            `input_shape` (tuple of integers, does not include the samples\n+            axis) when using this layer as the first layer in a model.\n+            Output shape: Same shape as input.\n+        **kwargs: Base layer keyword arguments (e.g. `name` and `dtype`).\n+\n+    Reference:\n+\n+    - [Yuxin Wu & Kaiming He, 2018](https://arxiv.org/abs/1803.08494)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        groups=32,\n+        axis=-1,\n+        epsilon=1e-3,\n+        center=True,\n+        scale=True,\n+        beta_initializer=\"zeros\",\n+        gamma_initializer=\"ones\",\n+        beta_regularizer=None,\n+        gamma_regularizer=None,\n+        beta_constraint=None,\n+        gamma_constraint=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+        self.groups = groups\n+        self.axis = axis\n+        self.epsilon = epsilon\n+        self.center = center\n+        self.scale = scale\n+        self.beta_initializer = initializers.get(beta_initializer)\n+        self.gamma_initializer = initializers.get(gamma_initializer)\n+        self.beta_regularizer = regularizers.get(beta_regularizer)\n+        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n+        self.beta_constraint = constraints.get(beta_constraint)\n+        self.gamma_constraint = constraints.get(gamma_constraint)\n+\n+    def build(self, input_shape):\n+        dim = input_shape[self.axis]\n+\n+        if dim is None:\n+            raise ValueError(\n+                f\"Axis {self.axis} of input tensor should have a defined \"\n+                \"dimension but the layer received an input with shape \"\n+                f\"{input_shape}.\"\n+            )\n+\n+        if self.groups == -1:\n+            self.groups = dim\n+\n+        if dim < self.groups:\n+            raise ValueError(\n+                f\"Number of groups ({self.groups}) cannot be more than the \"\n+                f\"number of channels ({dim}).\"\n+            )\n+\n+        if dim % self.groups != 0:\n+            raise ValueError(\n+                f\"Number of groups ({self.groups}) must be a multiple \"\n+                f\"of the number of channels ({dim}).\"\n+            )\n+\n+        self.input_spec = InputSpec(\n+            ndim=len(input_shape), axes={self.axis: dim}\n+        )\n+\n+        if self.scale:\n+            self.gamma = self.add_weight(\n+                shape=(dim,),\n+                name=\"gamma\",\n+                initializer=self.gamma_initializer,\n+                regularizer=self.gamma_regularizer,\n+                constraint=self.gamma_constraint,\n+            )\n+        else:\n+            self.gamma = None\n+\n+        if self.center:\n+            self.beta = self.add_weight(\n+                shape=(dim,),\n+                name=\"beta\",\n+                initializer=self.beta_initializer,\n+                regularizer=self.beta_regularizer,\n+                constraint=self.beta_constraint,\n+            )\n+        else:\n+            self.beta = None\n+\n+        super().build(input_shape)\n+\n+    def call(self, inputs):\n+        input_shape = inputs.shape\n+\n+        reshaped_inputs = self._reshape_into_groups(inputs)\n+\n+        normalized_inputs = self._apply_normalization(\n+            reshaped_inputs, input_shape\n+        )\n+\n+        return ops.reshape(normalized_inputs, input_shape)\n+\n+    def _reshape_into_groups(self, inputs):\n+        input_shape = inputs.shape\n+        group_shape = [input_shape[i] for i in range(len(input_shape))]\n+\n+        group_shape[self.axis] = input_shape[self.axis] // self.groups\n+        group_shape.insert(self.axis, self.groups)\n+        group_shape = ops.stack(group_shape)\n+        reshaped_inputs = ops.reshape(inputs, group_shape)\n+        return reshaped_inputs\n+\n+    def _apply_normalization(self, reshaped_inputs, input_shape):\n+        group_reduction_axes = list(range(1, len(reshaped_inputs.shape)))\n+\n+        axis = -2 if self.axis == -1 else self.axis - 1\n+        group_reduction_axes.pop(axis)\n+\n+        mean = ops.mean(\n+            reshaped_inputs, axis=group_reduction_axes, keepdims=True\n+        )\n+        variance = ops.var(\n+            reshaped_inputs, axis=group_reduction_axes, keepdims=True\n+        )\n+\n+        gamma, beta = self._get_reshaped_weights(input_shape)\n+\n+        # Compute the batch normalization.\n+        inv = 1 / ops.sqrt(variance + self.epsilon)\n+        if gamma is not None:\n+            inv *= gamma\n+\n+        x = beta - mean * inv if beta is not None else -mean * inv\n+        normalized_inputs = reshaped_inputs * ops.cast(\n+            inv, reshaped_inputs.dtype\n+        ) + ops.cast(x, reshaped_inputs.dtype)\n+\n+        normalized_inputs = ops.cast(normalized_inputs, reshaped_inputs.dtype)\n+\n+        return normalized_inputs\n+\n+    def _get_reshaped_weights(self, input_shape):\n+        broadcast_shape = self._create_broadcast_shape(input_shape)\n+        gamma = None\n+        beta = None\n+        if self.scale:\n+            gamma = ops.reshape(self.gamma, broadcast_shape)\n+\n+        if self.center:\n+            beta = ops.reshape(self.beta, broadcast_shape)\n+        return gamma, beta\n+\n+    def _create_broadcast_shape(self, input_shape):\n+\n+        broadcast_shape = [1] * len(input_shape)\n+\n+        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n+        broadcast_shape.insert(self.axis, self.groups)\n+\n+        return broadcast_shape\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        config = {\n+            \"groups\": self.groups,\n+            \"axis\": self.axis,\n+            \"epsilon\": self.epsilon,\n+            \"center\": self.center,\n+            \"scale\": self.scale,\n+            \"beta_initializer\": initializers.serialize(self.beta_initializer),\n+            \"gamma_initializer\": initializers.serialize(self.gamma_initializer),\n+            \"beta_regularizer\": regularizers.serialize(self.beta_regularizer),\n+            \"gamma_regularizer\": regularizers.serialize(self.gamma_regularizer),\n+            \"beta_constraint\": constraints.serialize(self.beta_constraint),\n+            \"gamma_constraint\": constraints.serialize(self.gamma_constraint),\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,114 @@\n+import numpy as np\n+\n+from keras_core import constraints\n+from keras_core import layers\n+from keras_core import regularizers\n+from keras_core import testing\n+\n+\n+class GroupNormalizationTest(testing.TestCase):\n+    def test_groupnorm(self):\n+        self.run_layer_test(\n+            layers.GroupNormalization,\n+            init_kwargs={\n+                \"gamma_regularizer\": regularizers.L2(0.01),\n+                \"beta_regularizer\": regularizers.L2(0.01),\n+            },\n+            input_shape=(3, 4, 32),\n+            expected_output_shape=(3, 4, 32),\n+            expected_num_trainable_weights=2,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=2,\n+            supports_masking=True,\n+        )\n+\n+        self.run_layer_test(\n+            layers.GroupNormalization,\n+            init_kwargs={\n+                \"groups\": 4,\n+                \"gamma_constraint\": constraints.UnitNorm(),\n+                \"beta_constraint\": constraints.UnitNorm(),\n+            },\n+            input_shape=(3, 4, 4),\n+            expected_output_shape=(3, 4, 4),\n+            expected_num_trainable_weights=2,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    def test_correctness_instance_norm(self):\n+        instance_norm_layer = layers.GroupNormalization(\n+            groups=4, axis=-1, scale=False, center=False\n+        )\n+\n+        inputs = np.array([[[-1.0, 1.0, 0, 2.0], [1.0, 3.0, -4, -2.0]]])\n+\n+        expected_instance_norm_output = np.array(\n+            [[[-1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0]]]\n+        )\n+\n+        self.assertAllClose(\n+            instance_norm_layer(inputs),\n+            expected_instance_norm_output,\n+            atol=1e-3,\n+        )\n+\n+    def test_correctness_1d(self):\n+        layer_with_1_group = layers.GroupNormalization(\n+            groups=1, axis=-1, scale=False, center=False\n+        )\n+        layer_with_2_groups = layers.GroupNormalization(\n+            groups=2, axis=1, scale=False, center=False\n+        )\n+\n+        inputs = np.array([[-1.0, -1.0, 1.0, 1.0, 2.0, 2.0, 0, -2.0]])\n+\n+        expected_output_1_group = np.array(\n+            [[-0.898, -0.898, 0.539, 0.539, 1.257, 1.257, -0.180, -1.616]],\n+        )\n+        self.assertAllClose(\n+            layer_with_1_group(inputs),\n+            expected_output_1_group,\n+            atol=1e-3,\n+        )\n+\n+        expected_output_2_groups = np.array(\n+            [[-1.0, -1.0, 1.0, 1.0, 0.904, 0.904, -0.301, -1.507]]\n+        )\n+        self.assertAllClose(\n+            layer_with_2_groups(inputs),\n+            expected_output_2_groups,\n+            atol=1e-3,\n+        )\n+\n+    def test_correctness_2d(self):\n+        layer_with_1_group = layers.GroupNormalization(\n+            groups=1, axis=-1, scale=False, center=False\n+        )\n+        layer_with_2_groups = layers.GroupNormalization(\n+            groups=2, axis=2, scale=False, center=False\n+        )\n+\n+        inputs = np.array([[[-1.0, -1.0, 2.0, 2.0], [1.0, 1.0, 0, -2.0]]])\n+\n+        expected_output_1_group = np.array(\n+            [[[-0.898, -0.898, 1.257, 1.257], [0.539, 0.539, -0.180, -1.616]]]\n+        )\n+\n+        self.assertAllClose(\n+            layer_with_1_group(inputs),\n+            expected_output_1_group,\n+            atol=1e-3,\n+        )\n+\n+        expected_output_2_groups = np.array(\n+            [[[-1.0, -1.0, 0.904, 0.904], [1.0, 1.0, -0.301, -1.507]]]\n+        )\n+        self.assertAllClose(\n+            layer_with_2_groups(inputs),\n+            expected_output_2_groups,\n+            atol=1e-3,\n+        )\n\n@@ -0,0 +1,124 @@\n+from keras_core import initializers\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers import Wrapper\n+from keras_core.layers.input_spec import InputSpec\n+\n+\n+@keras_core_export(\"keras_core.layers.SpectralNormalization\")\n+class SpectralNormalization(Wrapper):\n+    \"\"\"Performs spectral normalization on the weights of a target layer.\n+\n+    This wrapper controls the Lipschitz constant of the weights of a layer by\n+    constraining their spectral norm, which can stabilize the training of GANs.\n+\n+    Args:\n+        layer: A `keras_core.layers.Layer` instance that\n+            has either a `kernel` (e.g. `Conv2D`, `Dense`...)\n+            or an `embeddings` attribute (`Embedding` layer).\n+        power_iterations: int, the number of iterations during normalization.\n+        **kwargs: Base wrapper keyword arguments.\n+\n+    Examples:\n+\n+    Wrap `keras_core.layers.Conv2D`:\n+    >>> x = np.random.rand(1, 10, 10, 1)\n+    >>> conv2d = SpectralNormalization(keras_core.layers.Conv2D(2, 2))\n+    >>> y = conv2d(x)\n+    >>> y.shape\n+    (1, 9, 9, 2)\n+\n+    Wrap `keras_core.layers.Dense`:\n+    >>> x = np.random.rand(1, 10, 10, 1)\n+    >>> dense = SpectralNormalization(keras_core.layers.Dense(10))\n+    >>> y = dense(x)\n+    >>> y.shape\n+    (1, 10, 10, 10)\n+\n+    Reference:\n+\n+    - [Spectral Normalization for GAN](https://arxiv.org/abs/1802.05957).\n+    \"\"\"\n+\n+    def __init__(self, layer, power_iterations=1, **kwargs):\n+        super().__init__(layer, **kwargs)\n+        if power_iterations <= 0:\n+            raise ValueError(\n+                \"`power_iterations` should be greater than zero. Received: \"\n+                f\"`power_iterations={power_iterations}`\"\n+            )\n+        self.power_iterations = power_iterations\n+\n+    def build(self, input_shape):\n+        super().build(input_shape)\n+        self.input_spec = InputSpec(shape=[None] + list(input_shape[1:]))\n+\n+        if hasattr(self.layer, \"kernel\"):\n+            self.kernel = self.layer.kernel\n+        elif hasattr(self.layer, \"embeddings\"):\n+            self.kernel = self.layer.embeddings\n+        else:\n+            raise ValueError(\n+                f\"{type(self.layer).__name__} object has no attribute 'kernel' \"\n+                \"nor 'embeddings'\"\n+            )\n+\n+        self.kernel_shape = self.kernel.shape\n+\n+        self.vector_u = self.add_weight(\n+            shape=(1, self.kernel_shape[-1]),\n+            initializer=initializers.TruncatedNormal(stddev=0.02),\n+            trainable=False,\n+            name=\"vector_u\",\n+            dtype=self.kernel.dtype,\n+        )\n+\n+    def call(self, inputs, training=False):\n+        if training:\n+            self.normalize_weights()\n+\n+        output = self.layer(inputs)\n+        return output\n+\n+    def compute_output_shape(self, input_shape):\n+        return self.layer.compute_output_shape(input_shape)\n+\n+    def normalize_weights(self):\n+        \"\"\"Generate spectral normalized weights.\n+\n+        This method will update the value of `self.kernel` with the\n+        spectral normalized value, so that the layer is ready for `call()`.\n+        \"\"\"\n+\n+        weights = ops.reshape(self.kernel, [-1, self.kernel_shape[-1]])\n+        vector_u = self.vector_u\n+\n+        # check for zeroes weights\n+        if not all([w == 0.0 for w in weights]):\n+            for _ in range(self.power_iterations):\n+                vector_v = self._l2_normalize(\n+                    ops.matmul(vector_u, ops.transpose(weights))\n+                )\n+                vector_u = self._l2_normalize(ops.matmul(vector_v, weights))\n+            # vector_u = tf.stop_gradient(vector_u)\n+            # vector_v = tf.stop_gradient(vector_v)\n+            sigma = ops.matmul(\n+                ops.matmul(vector_v, weights), ops.transpose(vector_u)\n+            )\n+            self.vector_u.assign(ops.cast(vector_u, self.vector_u.dtype))\n+            self.kernel.assign(\n+                ops.cast(\n+                    ops.reshape(self.kernel / sigma, self.kernel_shape),\n+                    self.kernel.dtype,\n+                )\n+            )\n+\n+    def _l2_normalize(self, x):\n+        square_sum = ops.sum(ops.square(x), keepdims=True)\n+        x_inv_norm = 1 / ops.sqrt(ops.maximum(square_sum, 1e-12))\n+        return ops.multiply(x, x_inv_norm)\n+\n+    def get_config(self):\n+        config = {\"power_iterations\": self.power_iterations}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,36 @@\n+import numpy as np\n+\n+from keras_core import initializers\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class SpectralNormalizationTest(testing.TestCase):\n+    def test_basic_spectralnorm(self):\n+        self.run_layer_test(\n+            layers.SpectralNormalization,\n+            init_kwargs={\"layer\": layers.Dense(2)},\n+            input_shape=None,\n+            input_data=np.random.uniform(size=(10, 3, 4)),\n+            expected_output_shape=(10, 3, 2),\n+            expected_num_trainable_weights=2,\n+            expected_num_non_trainable_weights=2,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    def test_apply_layer(self):\n+        images = np.ones((1, 2, 2, 1))\n+        sn_wrapper = layers.SpectralNormalization(\n+            layers.Conv2D(\n+                1, (2, 2), kernel_initializer=initializers.Constant(value=1)\n+            ),\n+        )\n+\n+        result = sn_wrapper(images, training=False)\n+        result_train = sn_wrapper(images, training=True)\n+        expected_output = np.array([[[[4.0]]]], dtype=np.float32)\n+        self.assertAllClose(result, expected_output)\n+        # max eigen value of 2x2 matrix of ones is 2\n+        self.assertAllClose(result_train, expected_output / 2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0322038b8a0e75771886b2092636435fc0154973", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 147 | Lines Deleted: 60 | Files Changed: 9 | Hunks: 26 | Methods Changed: 22 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 207 | Churn Cumulative: 4504 | Contributors (this commit): 9 | Commits (past 90d): 165 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -151,7 +151,14 @@ def softmax(x, axis=-1):\n         x : Input tensor.\n         axis: Integer, axis along which the softmax is applied.\n     \"\"\"\n-    return ops.softmax(x, axis=axis)\n+    output = ops.softmax(x, axis=axis)\n+    # Cache the logits to use for crossentropy loss.\n+    try:\n+        output._keras_logits = x\n+    except AttributeError:\n+        # We're dealing with a C-type.\n+        pass\n+    return output\n \n \n @keras_core_export(\"keras_core.activations.elu\")\n@@ -324,7 +331,11 @@ def sigmoid(x):\n     \"\"\"\n     output = ops.sigmoid(x)\n     # Cache the logits to use for crossentropy loss.\n+    try:\n         output._keras_logits = x\n+    except AttributeError:\n+        # We're dealing with a C-type.\n+        pass\n     return output\n \n \n\n@@ -399,10 +399,10 @@ class AutocastScope:\n         self.original_scope = None\n \n     def maybe_cast(self, value):\n-        from keras_core import operations as ops\n+        from keras_core import backend\n \n         if is_float_dtype(value.dtype):\n-            return ops.cast(value, dtype=self.dtype)\n+            return backend.cast(value, dtype=self.dtype)\n         return value\n \n     def __enter__(self):\n\n@@ -96,10 +96,7 @@ class Variable(KerasVariable):\n             # in during shape inference with JAX tracer objects\n             # (anything else would be a bug, to be fixed.)\n             return self._maybe_autocast(\n-                jnp.array(\n-                    self._initializer(self._shape, dtype=self._dtype),\n-                    dtype=self._dtype,\n-                )\n+                self._initializer(self._shape, dtype=self._dtype)\n             )\n         return self._maybe_autocast(self._value)\n \n@@ -158,19 +155,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                 return KerasTensor(x.shape, x.dtype)\n             return x\n \n-        return nest.map_structure(convert_jax_spec_to_keras_tensor, jax_out)\n-\n-\n-def traceable_tensor(shape, dtype=None):\n-    \"\"\"Create a \"traceable tensor\".\n-\n-    That's a tensor that can be passed as input\n-    to a stateful backend-native function to\n-    create state during the trace.\n-    \"\"\"\n-    shape = list(shape)\n-    dtype = dtype or \"float32\"\n-    for i, x in enumerate(shape):\n-        if x is None:\n-            shape[i] = 1\n-    return jnp.ones(shape, dtype=dtype)\n+        output_shape = nest.map_structure(\n+            convert_jax_spec_to_keras_tensor, jax_out\n+        )\n+    return output_shape\n\n@@ -52,12 +52,8 @@ class Variable(KerasVariable, tf.__internal__.types.Tensor):\n             # This is fine because it's only ever used\n             # during shape inference in a scratch graph\n             # (anything else would be a bug, to be fixed.)\n-            return self._maybe_autocast(\n-                tf.constant(\n-                    self._initializer(self._shape, dtype=self._dtype),\n-                    dtype=self._dtype,\n-                )\n-            )\n+            init_val = self._initializer(self._shape, dtype=self._dtype)\n+            return self._maybe_autocast(init_val)\n         return self._maybe_autocast(self._value)\n \n     def numpy(self):  # noqa: F811\n@@ -109,9 +105,9 @@ def vectorized_map(function, elements):\n \n \n def compute_output_spec(fn, *args, **kwargs):\n+    with StatelessScope():\n         graph_name = auto_name(\"scratch_graph\")\n         with tf.__internal__.FuncGraph(graph_name).as_default():\n-        with StatelessScope():\n \n             def convert_keras_tensor_to_tf(x):\n                 if isinstance(x, KerasTensor):\n@@ -130,21 +126,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                     return KerasTensor(x.shape, x.dtype)\n                 return x\n \n-            return tf.nest.map_structure(convert_tf_to_keras_tensor, tf_out)\n-\n-\n-def traceable_tensor(shape, dtype=None):\n-    \"\"\"Create a \"traceable tensor\".\n-\n-    That's a tensor that can be passed as input\n-    to a stateful backend-native function to\n-    create state during the trace.\n-\n-    TODO: get rid of this.\n-    \"\"\"\n-    shape = list(shape)\n-    dtype = dtype or \"float32\"\n-    for i, x in enumerate(shape):\n-        if x is None:\n-            shape[i] = 1\n-    return tf.ones(shape, dtype=dtype)\n+            output_shape = tf.nest.map_structure(\n+                convert_tf_to_keras_tensor, tf_out\n+            )\n+    return output_shape\n\n@@ -198,8 +198,12 @@ class Layer(Operation):\n             value: Boolean with the desired state for the layer's trainable\n                 attribute.\n         \"\"\"\n-        for layer in self._layers():\n-            layer._trainable = value\n+        value = bool(value)\n+        self._trainable = value\n+        for v in self._trainable_variables:\n+            v.trainable = value\n+        for layer in self._layers:\n+            layer.trainable = value\n \n     @property\n     def variables(self):\n@@ -225,10 +229,14 @@ class Layer(Operation):\n \n     @property\n     def trainable_variables(self):\n+        if not self.trainable:\n+            return []\n         return [v for v in self.variables if v.trainable]\n \n     @property\n     def non_trainable_variables(self):\n+        if not self.trainable:\n+            return self.variables\n         return [v for v in self.variables if not v.trainable]\n \n     @property\n@@ -250,10 +258,14 @@ class Layer(Operation):\n \n     @property\n     def trainable_weights(self):\n+        if not self.trainable:\n+            return []\n         return [v for v in self.weights if v.trainable]\n \n     @property\n     def non_trainable_weights(self):\n+        if not self.trainable:\n+            return self.weights\n         return [v for v in self.weights if not v.trainable]\n \n     def get_weights(self):\n@@ -773,18 +785,17 @@ class Layer(Operation):\n             input_shape = tuple(input_shape)\n         if isinstance(input_shape, list):\n             input_tensors = [\n-                backend.traceable_tensor(shape) for shape in input_shape\n+                backend.KerasTensor(shape) for shape in input_shape\n             ]\n         elif isinstance(input_shape, dict):\n             input_tensors = {\n-                k: backend.traceable_tensor(shape)\n+                k: backend.KerasTensor(shape)\n                 for k, shape in input_shape.items()\n             }\n         else:\n-            input_tensors = backend.traceable_tensor(input_shape)\n+            input_tensors = backend.KerasTensor(input_shape)\n         try:\n-            # TODO: make this work without relying on eager tensors.\n-            self.call(input_tensors)\n+            backend.compute_output_spec(self.call, input_tensors)\n             return True\n         except Exception as e:\n             warnings.warn(\n@@ -799,12 +810,11 @@ class Layer(Operation):\n             # Case: all input keyword arguments were plain tensors.\n             input_tensors = {\n                 # We strip the `_shape` suffix to recover kwarg names.\n-                k[:-6]: backend.traceable_tensor(shape)\n+                k[:-6]: backend.KerasTensor(shape)\n                 for k, shape in shapes_dict.items()\n             }\n             try:\n-                # TODO: make this work without relying on eager tensors.\n-                self.call(**input_tensors)\n+                backend.compute_output_spec(self.call, **input_tensors)\n                 return True\n             except Exception as e:\n                 warnings.warn(\n\n@@ -400,3 +400,75 @@ class LayerTest(testing.TestCase):\n             self.assertAllClose(ref_v, v)\n         for ref_loss, loss in zip(layer1.losses, losses):\n             self.assertAllClose(ref_loss, loss)\n+\n+    def test_trainable_setting(self):\n+        class NonTrainableWeightsLayer(layers.Layer):\n+            def build(self, _):\n+                self.w1 = self.add_weight(\n+                    shape=(),\n+                    initializer=\"ones\",\n+                    trainable=True,\n+                )\n+                self.w2 = self.add_weight(\n+                    shape=(),\n+                    initializer=\"ones\",\n+                    trainable=False,\n+                )\n+                self.seed = backend.random.SeedGenerator(123)\n+\n+            def call(self, inputs):\n+                return inputs\n+\n+        class NestedNonTrainableWeightsLayer(layers.Layer):\n+            def build(self, _):\n+                self.w1 = self.add_weight(\n+                    shape=(),\n+                    initializer=\"ones\",\n+                    trainable=True,\n+                )\n+                self.w2 = self.add_weight(\n+                    shape=(),\n+                    initializer=\"ones\",\n+                    trainable=False,\n+                )\n+                self.nested = NonTrainableWeightsLayer()\n+                self.nested.build(None)\n+\n+            def call(self, inputs):\n+                return inputs\n+\n+        layer = NestedNonTrainableWeightsLayer()\n+        layer.build(None)\n+        self.assertEqual(len(layer.trainable_weights), 2)\n+        self.assertEqual(len(layer.trainable_variables), 2)\n+        self.assertEqual(len(layer.non_trainable_weights), 2)\n+        self.assertEqual(len(layer.non_trainable_variables), 3)\n+\n+        layer.trainable = False\n+        self.assertEqual(len(layer.trainable_weights), 0)\n+        self.assertEqual(len(layer.trainable_variables), 0)\n+        self.assertEqual(len(layer.non_trainable_weights), 4)\n+        self.assertEqual(len(layer.non_trainable_variables), 5)\n+        self.assertFalse(layer.w1.trainable)\n+        self.assertFalse(layer.nested.w1.trainable)\n+\n+        layer.trainable = True\n+        self.assertEqual(len(layer.trainable_weights), 2)\n+        self.assertEqual(len(layer.trainable_variables), 2)\n+        self.assertEqual(len(layer.non_trainable_weights), 2)\n+        self.assertEqual(len(layer.non_trainable_variables), 3)\n+        self.assertTrue(layer.w1.trainable)\n+        self.assertTrue(layer.nested.w1.trainable)\n+\n+        layer = NestedNonTrainableWeightsLayer(trainable=False)\n+        layer.build(None)\n+        self.assertEqual(len(layer.trainable_weights), 0)\n+        self.assertEqual(len(layer.trainable_variables), 0)\n+        self.assertEqual(len(layer.non_trainable_weights), 4)\n+        self.assertEqual(len(layer.non_trainable_variables), 5)\n+\n+        layer.trainable = True\n+        self.assertEqual(len(layer.trainable_weights), 2)\n+        self.assertEqual(len(layer.trainable_variables), 2)\n+        self.assertEqual(len(layer.non_trainable_weights), 2)\n+        self.assertEqual(len(layer.non_trainable_variables), 3)\n\n@@ -190,7 +190,7 @@ class BatchNormalization(Layer):\n \n     def call(self, inputs, training=None, mask=None):\n         # TODO: support masking during stats computation.\n-        if training:\n+        if training and self.trainable:\n             mean = ops.mean(inputs, axis=self._reduction_axes)\n             variance = ops.var(inputs, axis=self._reduction_axes)\n             outputs = (inputs - mean) / ops.sqrt(variance + self.epsilon)\n\n@@ -78,6 +78,34 @@ class BatchNormalizationTest(testing.TestCase):\n         self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-1)\n         self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-1)\n \n+    def test_trainable_behavior(self):\n+        layer = layers.BatchNormalization(axis=-1, momentum=0.8, epsilon=1e-7)\n+        layer.build((1, 4, 4, 3))\n+        layer.trainable = False\n+        self.assertEqual(len(layer.weights), 4)\n+        self.assertEqual(len(layer.trainable_weights), 0)\n+        self.assertEqual(len(layer.non_trainable_weights), 4)\n+\n+        # Random data centered on 5.0, variance 10.0\n+        x = np.random.normal(loc=5.0, scale=10.0, size=(200, 4, 4, 3))\n+\n+        out = layer(x, training=True)\n+        self.assertAllClose(out, x)\n+\n+        layer.trainable = True\n+        self.assertEqual(len(layer.weights), 4)\n+        self.assertEqual(len(layer.trainable_weights), 2)\n+        self.assertEqual(len(layer.non_trainable_weights), 2)\n+\n+        for _ in range(10):\n+            out = layer(x, training=True)\n+\n+        out -= np.reshape(np.array(layer.beta), (1, 1, 1, 3))\n+        out /= np.reshape(np.array(layer.gamma), (1, 1, 1, 3))\n+\n+        self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-3)\n+        self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-3)\n+\n     def test_masking_correctness(self):\n         # TODO\n         pass\n\n@@ -207,7 +207,6 @@ class GroupNormalization(Layer):\n         return gamma, beta\n \n     def _create_broadcast_shape(self, input_shape):\n-\n         broadcast_shape = [1] * len(input_shape)\n \n         broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
