{"custom_id": "keras#f3c60dc571a242960a9189b3d11addb1d76b5e53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 3720 | Contributors (this commit): 49 | Commits (past 90d): 34 | Contributors (cumulative): 49 | DMM Complexity: None\n\nDIFF:\n@@ -1507,6 +1507,11 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid',\n                                  input_shape=image_shape,\n                                  filter_shape=filter_shape)\n     else:\n+        # T.nnet.conv2d uses **kwargs, so the filter_dilation parameter will be\n+        # ignored by versions that do not support it\n+        if 'filter_dilation' not in inspect.getargspec(T.nnet.conv2d).args:\n+            raise ValueError('conv2d with filter dilation requires Theano '\n+                             '0.9.0dev2 or newer.')\n         conv_out = T.nnet.conv2d(x, kernel,\n                                  border_mode=th_border_mode,\n                                  subsample=strides,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#82ca6d418588ccd61d663ec8029937290b62d583", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 281 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -120,7 +120,7 @@ X = X[indices]\n y = y[indices]\n \n # Explicitly set apart 10% for validation data that we never train over\n-split_at = len(X) - len(X) / 10\n+split_at = len(X) - len(X) // 10\n (X_train, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n (y_train, y_val) = (y[:split_at], y[split_at:])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e54d7951f2a82629c71c77ca842ba032b523fe84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 3723 | Contributors (this commit): 50 | Commits (past 90d): 34 | Contributors (cumulative): 50 | DMM Complexity: None\n\nDIFF:\n@@ -1545,6 +1545,9 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n     if dim_ordering not in {'th', 'tf'}:\n         raise ValueError('Unknown dim_ordering ' + dim_ordering)\n \n+    if dim_ordering == 'tf':\n+        output_shape = (output_shape[0], output_shape[3], output_shape[1], output_shape[2])\n+\n     x = _preprocess_conv2d_input(x, dim_ordering)\n     kernel = _preprocess_conv2d_kernel(kernel, dim_ordering)\n     kernel = kernel.dimshuffle((1, 0, 2, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c6db0815853f7c5421b8057e9167f19b5b94812", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 15 | Churn Cumulative: 2844 | Contributors (this commit): 30 | Commits (past 90d): 17 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1573,6 +1573,11 @@ class Model(Container):\n                 if p.is_alive():\n                     p.terminate()\n             data_gen_queue.close()\n+        else:\n+            # Wait for all threads to finish\n+            for p in generator_threads:\n+                if p.is_alive():\n+                    p.join()\n         callbacks.on_train_end()\n         return self.history\n \n@@ -1665,6 +1670,11 @@ class Model(Container):\n                 if p.is_alive():\n                     p.terminate()\n             data_gen_queue.close()\n+        else:\n+            # Wait for all threads to finish\n+            for p in generator_threads:\n+                if p.is_alive():\n+                    p.join()\n         if not isinstance(outs, list):\n             return np.average(np.asarray(all_outs),\n                               weights=weights)\n@@ -1766,6 +1776,11 @@ class Model(Container):\n                 if p.is_alive():\n                     p.terminate()\n             data_gen_queue.close()\n+        else:\n+            # Wait for all threads to finish\n+            for p in generator_threads:\n+                if p.is_alive():\n+                    p.join()\n         if len(all_outs) == 1:\n             return all_outs[0]\n         return all_outs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#55487f33b135849cdda49bcc75778976d94a2575", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 9626 | Contributors (this commit): 73 | Commits (past 90d): 77 | Contributors (cumulative): 105 | DMM Complexity: None\n\nDIFF:\n@@ -534,15 +534,17 @@ def eye(size, dtype=None, name=None):\n     return variable(np.eye(size), dtype, name)\n \n \n-def zeros_like(x, name=None):\n+def zeros_like(x, dtype=None, name=None):\n     \"\"\"Instantiates an all-zeros Keras variable\n     of the same shape as another Keras variable or tensor and returns it.\n \n     # Arguments\n         x: Keras variable or Keras tensor.\n+        dtype: String, dtype of returned Keras variable.\n+             None uses the dtype of x.\n \n     # Returns\n-        A Keras variable, filled with `0.0`.\n+        A Keras variable with the shape of x filled with zeros.\n \n     # Example\n     ```python\n@@ -554,18 +556,20 @@ def zeros_like(x, name=None):\n                [ 0.,  0.,  0.]], dtype=float32)\n     ```\n     \"\"\"\n-    return tf.zeros_like(x, name=name)\n+    return tf.zeros_like(x, dtype=dtype, name=name)\n \n \n-def ones_like(x, name=None):\n+def ones_like(x, dtype=None, name=None):\n     \"\"\"Instantiates an all-ones Keras variable\n     of the same shape as another Keras variable or tensor and returns it.\n \n     # Arguments\n         x: Keras variable or tensor.\n+        dtype: String, dtype of returned Keras variable.\n+             None uses the dtype of x.\n \n     # Returns\n-        A Keras variable, filled with `1.0`.\n+        A Keras variable with the shape of x filled with ones.\n \n     # Example\n     ```python\n@@ -577,7 +581,7 @@ def ones_like(x, name=None):\n                [ 1.,  1.,  1.]], dtype=float32)\n     ```\n     \"\"\"\n-    return tf.ones_like(x, name=name)\n+    return tf.ones_like(x, dtype=dtype, name=name)\n \n \n def random_uniform_variable(shape, low, high, dtype=None,\n\n@@ -176,12 +176,12 @@ def eye(size, dtype=None, name=None):\n     return variable(np.eye(size), dtype, name)\n \n \n-def ones_like(x, name=None):\n-    return T.ones_like(x)\n+def ones_like(x, dtype=None, name=None):\n+    return T.ones_like(x, dtype=dtype)\n \n \n-def zeros_like(x, name=None):\n-    return T.zeros_like(x)\n+def zeros_like(x, dtype=None, name=None):\n+    return T.zeros_like(x, dtype=dtype)\n \n \n def random_uniform_variable(shape, low, high, dtype=None, name=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8911c28850d374177a945878c223d00ad70ff94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 588 | Lines Deleted: 247 | Files Changed: 7 | Hunks: 24 | Methods Changed: 68 | Complexity Δ (Sum/Max): 34/41 | Churn Δ: 835 | Churn Cumulative: 15860 | Contributors (this commit): 93 | Commits (past 90d): 119 | Contributors (cumulative): 144 | DMM Complexity: 0.7321428571428571\n\nDIFF:\n@@ -272,6 +272,12 @@ def _initialize_variables():\n             sess.run(tf.initialize_variables(uninitialized_variables))\n \n \n+def constant(value, dtype=None, shape=None, name=None):\n+    if dtype is None:\n+        dtype = floatx()\n+    return tf.constant(value, dtype=dtype, shape=shape, name=name)\n+\n+\n def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n     \"\"\"Instantiates a placeholder tensor and returns it.\n \n@@ -2804,13 +2810,13 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n \n # RANDOMNESS\n \n-def random_normal(shape, mean=0.0, std=1.0, dtype=None, seed=None):\n-    \"\"\"Returns a tensor with normal distribution\n+def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    \"\"\"Returns a tensor with normal distribution of values.\n \n     # Arguments\n         shape: A tuple of integers, the shape of tensor to create.\n         mean: A float, mean of the normal distribution to draw samples.\n-        std: A float, standard deviation of the normal distribution\n+        stddev: A float, standard deviation of the normal distribution\n             to draw samples.\n         dtype: String, dtype of returned tensor.\n         seed: Integer, random seed.\n@@ -2822,18 +2828,18 @@ def random_normal(shape, mean=0.0, std=1.0, dtype=None, seed=None):\n         dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(10e6)\n-    return tf.random_normal(shape, mean=mean, stddev=std,\n+    return tf.random_normal(shape, mean=mean, stddev=stddev,\n                             dtype=dtype, seed=seed)\n \n \n-def random_uniform(shape, low=0.0, high=1.0, dtype=None, seed=None):\n-    \"\"\"Returns a tensor with uniform distribution\n+def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n+    \"\"\"Returns a tensor with uniform distribution of values.\n \n     # Arguments\n         shape: A tuple of integers, the shape of tensor to create.\n-        low: A float, lower boundary of the uniform distribution\n+        minval: A float, lower boundary of the uniform distribution\n             to draw samples.\n-        high: A float, upper boundary of the uniform distribution\n+        maxval: A float, upper boundary of the uniform distribution\n             to draw samples.\n         dtype: String, dtype of returned tensor.\n         seed: Integer, random seed.\n@@ -2845,12 +2851,12 @@ def random_uniform(shape, low=0.0, high=1.0, dtype=None, seed=None):\n         dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(10e6)\n-    return tf.random_uniform(shape, minval=low, maxval=high,\n+    return tf.random_uniform(shape, minval=minval, maxval=maxval,\n                              dtype=dtype, seed=seed)\n \n \n def random_binomial(shape, p=0.0, dtype=None, seed=None):\n-    \"\"\"Returns a tensor with binomlai distribution\n+    \"\"\"Returns a tensor with random binomial distribution of values.\n \n     # Arguments\n         shape: A tuple of integers, the shape of tensor to create.\n@@ -2870,6 +2876,31 @@ def random_binomial(shape, p=0.0, dtype=None, seed=None):\n                     tf.zeros(shape, dtype=dtype))\n \n \n+def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    \"\"\"Returns a tensor with truncated random normal distribution of values.\n+\n+    The generated values follow a normal distribution\n+    with specified mean and standard deviation,\n+    except that values whose magnitude is more than\n+    two standard deviations from the mean are dropped and re-picked.\n+\n+    # Arguments\n+        shape: A tuple of integers, the shape of tensor to create.\n+        mean: Mean of the values.\n+        stddev: Standard deviation of the values.\n+        dtype: String, dtype of returned tensor.\n+        seed: Integer, random seed.\n+\n+    # Returns\n+        A tensor.\n+    \"\"\"\n+    if dtype is None:\n+        dtype = floatx()\n+    if seed is None:\n+        seed = np.random.randint(10e6)\n+    return tf.truncated_normal(shape, mean, stddev, dtype=dtype, seed=seed)\n+\n+\n # CTC\n # tensorflow has a native implemenation, but it uses sparse tensors\n # and therefore requires a wrapper for Keras. The functions below convert\n\n@@ -83,6 +83,11 @@ def variable(value, dtype=None, name=None):\n         _assert_sparse_module()\n         variable = th_sparse_module.as_sparse_variable(value)\n     else:\n+        if isinstance(value, (theano.tensor.TensorVariable,\n+                              theano.tensor.sharedvar.TensorSharedVariable,\n+                              theano.tensor.TensorConstant)):\n+            # Support for RandomStreams().normal(), .uniform().\n+            value = value.eval()\n         value = np.asarray(value, dtype=dtype)\n         variable = theano.shared(value=value, name=name, strict=False)\n     variable._keras_shape = value.shape\n@@ -90,6 +95,18 @@ def variable(value, dtype=None, name=None):\n     return variable\n \n \n+def constant(value, dtype=None, shape=None, name=None):\n+    if dtype is None:\n+        dtype = floatx()\n+    if shape is None:\n+        shape = ()\n+    np_value = value * np.ones(shape)\n+    const = T.constant(np_value, dtype=dtype, name=name)\n+    const._keras_shape = shape\n+    const._uses_learning_phase = False\n+    return const\n+\n+\n def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n     \"\"\"Instantiate an input data placeholder variable.\n     \"\"\"\n@@ -1893,22 +1910,22 @@ def _old_theano_pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n # RANDOMNESS\n \n \n-def random_normal(shape, mean=0.0, std=1.0, dtype=None, seed=None):\n+def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     if dtype is None:\n         dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(1, 10e6)\n     rng = RandomStreams(seed=seed)\n-    return rng.normal(size=shape, avg=mean, std=std, dtype=dtype)\n+    return rng.normal(size=shape, avg=mean, std=stddev, dtype=dtype)\n \n \n-def random_uniform(shape, low=0.0, high=1.0, dtype=None, seed=None):\n+def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     if dtype is None:\n         dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(1, 10e6)\n     rng = RandomStreams(seed=seed)\n-    return rng.uniform(shape, low=low, high=high, dtype=dtype)\n+    return rng.uniform(shape, low=minval, high=maxval, dtype=dtype)\n \n \n def random_binomial(shape, p=0.0, dtype=None, seed=None):\n@@ -1919,6 +1936,18 @@ def random_binomial(shape, p=0.0, dtype=None, seed=None):\n     rng = RandomStreams(seed=seed)\n     return rng.binomial(shape, p=p, dtype=dtype)\n \n+\n+def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    if dtype is None:\n+        dtype = floatx()\n+    if seed is None:\n+        seed = np.random.randint(1, 10e6)\n+    rng = RandomStreams(seed=seed)\n+    normal_tensor = rng.normal(size=shape, avg=mean, std=stddev, dtype=dtype)\n+    # Poor man's truncated normal: we literally clip the tensor\n+    return T.clip(normal_tensor, mean - 2 * stddev, mean + 2 * stddev)\n+\n+\n # Theano implementation of CTC\n # Used with permission from Shawn Tan\n # https://github.com/shawntan/\n\n@@ -13,7 +13,7 @@ import inspect\n from six.moves import zip\n \n from .. import backend as K\n-from .. import initializations\n+from .. import initializers\n from ..utils.io_utils import ask_to_proceed_with_overwrite\n from ..utils.generic_utils import func_dump, func_load\n \n@@ -400,7 +400,8 @@ class Layer(object):\n         # to the input layer we just created.\n         self(x)\n \n-    def add_weight(self, shape, initializer, name=None,\n+    def add_weight(self, shape, initializer,\n+                   name=None,\n                    trainable=True,\n                    regularizer=None,\n                    constraint=None):\n@@ -414,8 +415,8 @@ class Layer(object):\n                 that the layer itself is also trainable).\n             regularizer: An optional Regularizer instance.\n         \"\"\"\n-        initializer = initializations.get(initializer)\n-        weight = initializer(shape, name=name)\n+        initializer = initializers.get(initializer)\n+        weight = K.variable(initializer(shape), dtype=K.floatx())\n         if regularizer is not None:\n             self.add_loss(regularizer(weight))\n         if constraint is not None:\n\n@@ -1,121 +0,0 @@\n-from __future__ import absolute_import\n-import numpy as np\n-from . import backend as K\n-from .utils.generic_utils import get_from_module\n-\n-\n-def get_fans(shape, data_format='channels_first'):\n-    if len(shape) == 2:\n-        fan_in = shape[0]\n-        fan_out = shape[1]\n-    elif len(shape) == 4 or len(shape) == 5:\n-        # Assuming convolution kernels (2D or 3D).\n-        # TH kernel shape: (depth, input_depth, ...)\n-        # TF kernel shape: (..., input_depth, depth)\n-        if data_format == 'channels_first':\n-            receptive_field_size = np.prod(shape[2:])\n-            fan_in = shape[1] * receptive_field_size\n-            fan_out = shape[0] * receptive_field_size\n-        elif data_format == 'channels_last':\n-            receptive_field_size = np.prod(shape[:2])\n-            fan_in = shape[-2] * receptive_field_size\n-            fan_out = shape[-1] * receptive_field_size\n-        else:\n-            raise ValueError('Invalid data_format: ' + data_format)\n-    else:\n-        # No specific assumptions.\n-        fan_in = np.sqrt(np.prod(shape))\n-        fan_out = np.sqrt(np.prod(shape))\n-    return fan_in, fan_out\n-\n-\n-def uniform(shape, scale=0.05, name=None, data_format='channels_first'):\n-    return K.random_uniform_variable(shape, -scale, scale, name=name)\n-\n-\n-def normal(shape, scale=0.05, name=None, data_format='channels_first'):\n-    return K.random_normal_variable(shape, 0.0, scale, name=name)\n-\n-\n-def lecun_uniform(shape, name=None, data_format='channels_first'):\n-    \"\"\"LeCun uniform variance scaling initializer.\n-\n-    # References\n-        LeCun 98, Efficient Backprop,\n-        http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n-    \"\"\"\n-    fan_in, fan_out = get_fans(shape, data_format=data_format)\n-    scale = np.sqrt(3. / fan_in)\n-    return uniform(shape, scale, name=name)\n-\n-\n-def glorot_normal(shape, name=None, data_format='channels_first'):\n-    \"\"\"Glorot normal variance scaling initializer.\n-\n-    # References\n-        Glorot & Bengio, AISTATS 2010\n-    \"\"\"\n-    fan_in, fan_out = get_fans(shape, data_format=data_format)\n-    s = np.sqrt(2. / (fan_in + fan_out))\n-    return normal(shape, s, name=name)\n-\n-\n-def glorot_uniform(shape, name=None, data_format='channels_first'):\n-    fan_in, fan_out = get_fans(shape, data_format=data_format)\n-    s = np.sqrt(6. / (fan_in + fan_out))\n-    return uniform(shape, s, name=name)\n-\n-\n-def he_normal(shape, name=None, data_format='channels_first'):\n-    \"\"\"He normal variance scaling initializer.\n-\n-    # References\n-        He et al., http://arxiv.org/abs/1502.01852\n-    \"\"\"\n-    fan_in, fan_out = get_fans(shape, data_format=data_format)\n-    s = np.sqrt(2. / fan_in)\n-    return normal(shape, s, name=name)\n-\n-\n-def he_uniform(shape, name=None, data_format='channels_first'):\n-    \"\"\"He uniform variance scaling initializer.\n-    \"\"\"\n-    fan_in, fan_out = get_fans(shape, data_format=data_format)\n-    s = np.sqrt(6. / fan_in)\n-    return uniform(shape, s, name=name)\n-\n-\n-def orthogonal(shape, scale=1.1, name=None, data_format='channels_first'):\n-    \"\"\"Orthogonal initializer.\n-\n-    # References\n-        Saxe et al., http://arxiv.org/abs/1312.6120\n-    \"\"\"\n-    flat_shape = (shape[0], np.prod(shape[1:]))\n-    a = np.random.normal(0.0, 1.0, flat_shape)\n-    u, _, v = np.linalg.svd(a, full_matrices=False)\n-    # Pick the one with the correct shape.\n-    q = u if u.shape == flat_shape else v\n-    q = q.reshape(shape)\n-    return K.variable(scale * q[:shape[0], :shape[1]], name=name)\n-\n-\n-def identity(shape, scale=1, name=None, data_format='channels_first'):\n-    if len(shape) != 2 or shape[0] != shape[1]:\n-        raise ValueError('Identity matrix initialization can only be used '\n-                         'for 2D square matrices.')\n-    else:\n-        return K.variable(scale * np.identity(shape[0]), name=name)\n-\n-\n-def zero(shape, name=None, data_format='channels_first'):\n-    return K.zeros(shape, name=name)\n-\n-\n-def one(shape, name=None, data_format='channels_first'):\n-    return K.ones(shape, name=name)\n-\n-\n-def get(identifier, **kwargs):\n-    return get_from_module(identifier, globals(),\n-                           'initialization', kwargs=kwargs)\n\n@@ -0,0 +1,386 @@\n+from __future__ import absolute_import\n+import numpy as np\n+from . import backend as K\n+from .utils.generic_utils import get_from_module\n+\n+\n+class Initializer(object):\n+    \"\"\"Initializer base class: all initializers inherit from this class.\n+    \"\"\"\n+\n+    def __call__(self, shape, dtype=None):\n+        raise NotImplementedError\n+\n+\n+class Zeros(Initializer):\n+    \"\"\"Initializer that generates tensors initialized to 0.\"\"\"\n+\n+    def __call__(self, shape, dtype=None):\n+        return K.constant(0, shape=shape, dtype=dtype)\n+\n+\n+class Ones(Initializer):\n+    \"\"\"Initializer that generates tensors initialized to 1.\"\"\"\n+\n+    def __call__(self, shape, dtype=None):\n+        return K.constant(1, shape=shape, dtype=dtype)\n+\n+\n+class Constant(Initializer):\n+    \"\"\"Initializer that generates tensors initialized to a constant value.\n+\n+    # Arguments\n+        value: float; the value of the generator tensors.\n+    \"\"\"\n+\n+    def __init__(self, value=0):\n+        self.value = value\n+\n+    def __call__(self, shape, dtype=None):\n+        return K.constant(self.value, shape=shape, dtype=dtype)\n+\n+\n+class RandomNormal(Initializer):\n+    \"\"\"Initializer that generates tensors with a normal distribution.\n+\n+    # Arguments\n+        mean: a python scalar or a scalar tensor. Mean of the random values\n+          to generate.\n+        stddev: a python scalar or a scalar tensor. Standard deviation of the\n+          random values to generate.\n+        seed: A Python integer. Used to seed the random generator.\n+    \"\"\"\n+\n+    def __init__(self, mean=0., stddev=0.05, seed=None):\n+        self.mean = mean\n+        self.stddev = stddev\n+        self.seed = seed\n+\n+    def __call__(self, shape, dtype=None):\n+        return K.random_normal(shape, self.mean, self.stddev,\n+                               dtype=dtype, seed=self.seed)\n+\n+\n+class RandomUniform(Initializer):\n+    \"\"\"Initializer that generates tensors with a uniform distribution.\n+\n+    # Arguments\n+        minval: A python scalar or a scalar tensor. Lower bound of the range\n+          of random values to generate.\n+        maxval: A python scalar or a scalar tensor. Upper bound of the range\n+          of random values to generate.  Defaults to 1 for float types.\n+        seed: A Python integer. Used to seed the random generator.\n+    \"\"\"\n+\n+    def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n+        self.minval = minval\n+        self.maxval = maxval\n+        self.seed = seed\n+\n+    def __call__(self, shape, dtype=None):\n+        return K.random_uniform(shape, self.minval, self.maxval,\n+                                dtype=dtype, seed=self.seed)\n+\n+\n+class TruncatedNormal(Initializer):\n+    \"\"\"Initializer that generates a truncated normal distribution.\n+\n+    These values are similar to values from a `random_normal_initializer`\n+    except that values more than two standard deviations from the mean\n+    are discarded and re-drawn. This is the recommended initializer for\n+    neural network weights and filters.\n+\n+    # Arguments\n+        mean: a python scalar or a scalar tensor. Mean of the random values\n+          to generate.\n+        stddev: a python scalar or a scalar tensor. Standard deviation of the\n+          random values to generate.\n+        seed: A Python integer. Used to seed the random generator.\n+      \"\"\"\n+\n+    def __init__(self, mean=0., stddev=0.05, seed=None):\n+        self.mean = mean\n+        self.stddev = stddev\n+        self.seed = seed\n+\n+    def __call__(self, shape, dtype=None):\n+        return K.truncated_normal(shape, self.mean, self.stddev,\n+                                  dtype=dtype, seed=self.seed)\n+\n+\n+class VarianceScaling(Initializer):\n+    \"\"\"Initializer capable of adapting its scale to the shape of weights.\n+\n+    With `distribution=\"normal\"`, samples are drawn from a truncated normal\n+    distribution centered on zero, with `stddev = sqrt(scale / n)` where n is:\n+        - number of input units in the weight tensor, if mode = \"fan_in\"\n+        - number of output units, if mode = \"fan_out\"\n+        - average of the numbers of input and output units, if mode = \"fan_avg\"\n+\n+    With `distribution=\"uniform\"`,\n+    samples are drawn from a uniform distribution\n+    within [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n+\n+    # Arguments\n+        scale: Scaling factor (positive float).\n+        mode: One of \"fan_in\", \"fan_out\", \"fan_avg\".\n+        distribution: Random distribution to use. One of \"normal\", \"uniform\".\n+        seed: A Python integer. Used to seed the random generator.\n+\n+    # Raises\n+        ValueError: In case of an invalid value for the \"scale\", mode\" or\n+          \"distribution\" arguments.\n+    \"\"\"\n+    def __init__(self, scale=1.0,\n+                 mode='fan_in',\n+                 distribution='normal',\n+                 seed=None,\n+                 data_format=None):\n+        if scale <= 0.:\n+            raise ValueError('`scale` must be a positive float. Got:', scale)\n+        mode = mode.lower()\n+        if mode not in {'fan_in', 'fan_out', 'fan_avg'}:\n+            raise ValueError('Invalid `mode` argument: '\n+                             'expected on of {\"fan_in\", \"fan_out\", \"fan_avg\"} '\n+                             'but got', mode)\n+        distribution = distribution.lower()\n+        if distribution not in {'normal', 'uniform'}:\n+            raise ValueError('Invalid `distribution` argument: '\n+                             'expected one of {\"normal\", \"uniform\"} '\n+                             'but got', distribution)\n+        if data_format is None:\n+            data_format = K.image_data_format()\n+        else:\n+            data_format = data_format.lower()\n+        if data_format not in {'channels_first', 'channels_last'}:\n+            raise ValueError('Invalid `data_format` argument: '\n+                             'expected one of {\"channels_first\", '\n+                             '\"channels_last\"} but got', data_format)\n+        self.scale = scale\n+        self.mode = mode\n+        self.distribution = distribution\n+        self.seed = seed\n+        self.data_format = data_format\n+\n+    def __call__(self, shape, dtype=None):\n+        fan_in, fan_out = _compute_fans(shape, self.data_format)\n+        scale = self.scale\n+        if self.mode == 'fan_in':\n+            scale /= max(1., fan_in)\n+        elif self.mode == 'fan_out':\n+            scale /= max(1., fan_out)\n+        else:\n+            scale /= max(1., float(fan_in + fan_out) / 2)\n+        if self.distribution == 'normal':\n+            stddev = np.sqrt(scale)\n+            return K.truncated_normal(shape, 0., stddev,\n+                                      dtype=dtype, seed=self.seed)\n+        else:\n+            limit = np.sqrt(3. * scale)\n+            return K.random_uniform(shape, -limit, limit,\n+                                    dtype=dtype, seed=self.seed)\n+\n+\n+class Orthogonal(Initializer):\n+    \"\"\"Initializer that generates a random orthogonal matrix.\n+\n+    # Arguments\n+        gain: Multiplicative factor to apply to the orthogonal matrix.\n+        seed: A Python integer. Used to seed the random generator.\n+\n+    # References\n+        Saxe et al., http://arxiv.org/abs/1312.6120\n+    \"\"\"\n+\n+    def __init__(self, gain=1., seed=None):\n+        self.gain = gain\n+        self.seed = seed\n+\n+    def __call__(self, shape, dtype=None):\n+        flat_shape = (shape[0], np.prod(shape[1:]))\n+        if self.seed is not None:\n+            np.random.seed(self.seed)\n+        a = np.random.normal(0.0, 1.0, flat_shape)\n+        u, _, v = np.linalg.svd(a, full_matrices=False)\n+        # Pick the one with the correct shape.\n+        q = u if u.shape == flat_shape else v\n+        q = q.reshape(shape)\n+        return self.gain * q[:shape[0], :shape[1]]\n+\n+\n+class Identity(Initializer):\n+    \"\"\"Initializer that generates the identity matrix.\n+\n+    Only use for square 2D matrices.\n+\n+    # Arguments\n+        gain: Multiplicative factor to apply to the identity matrix.\n+    \"\"\"\n+\n+    def __init__(self, gain=1.):\n+        self.gain = gain\n+\n+    def __call__(self, shape, dtype=None):\n+        if len(shape) != 2 or shape[0] != shape[1]:\n+            raise ValueError('Identity matrix initializer can only be used '\n+                             'for 2D square matrices.')\n+        else:\n+            return self.gain * np.identity(shape[0])\n+\n+\n+def lecun_uniform(seed=None, data_format=None):\n+    \"\"\"LeCun uniform initializer.\n+\n+    It draws samples from a uniform distribution within [-limit, limit]\n+    where `limit` is `sqrt(3 / fan_in)`\n+    where `fan_in` is the number of input units in the weight tensor.\n+\n+    # Arguments\n+        seed: A Python integer. Used to seed the random generator.\n+        data_format: Image data format to use for convolution kernels.\n+\n+    # Returns\n+        An initializer.\n+\n+    # References\n+        LeCun 98, Efficient Backprop,\n+        http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n+    \"\"\"\n+    return VarianceScaling(scale=1., mode='fan_in', distribution='uniform')\n+\n+\n+def glorot_normal(seed=None, data_format=None):\n+    \"\"\"Glorot normal initializer, also called Xavier normal initializer.\n+\n+    It draws samples from a truncated normal distribution centered on 0\n+    with `stddev = sqrt(2 / (fan_in + fan_out))`\n+    where `fan_in` is the number of input units in the weight tensor\n+    and `fan_out` is the number of output units in the weight tensor.\n+\n+    # Arguments\n+        seed: A Python integer. Used to seed the random generator.\n+        data_format: Image data format to use for convolution kernels.\n+\n+    # Returns\n+        An initializer.\n+\n+    # References\n+        Glorot & Bengio, AISTATS 2010\n+        http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n+    \"\"\"\n+    return VarianceScaling(scale=1., mode='fan_avg', distribution='normal')\n+\n+\n+def glorot_uniform(seed=None, data_format=None):\n+    \"\"\"Glorot uniform initializer, also called Xavier uniform initializer.\n+\n+    It draws samples from a uniform distribution within [-limit, limit]\n+    where `limit` is `sqrt(6 / (fan_in + fan_out))`\n+    where `fan_in` is the number of input units in the weight tensor\n+    and `fan_out` is the number of output units in the weight tensor.\n+\n+    # Arguments\n+        seed: A Python integer. Used to seed the random generator.\n+        data_format: Image data format to use for convolution kernels.\n+\n+    # Returns\n+        An initializer.\n+\n+    # References\n+        Glorot & Bengio, AISTATS 2010\n+        http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n+    \"\"\"\n+    return VarianceScaling(scale=1., mode='fan_avg', distribution='uniform')\n+\n+\n+def he_normal(seed=None, data_format=None):\n+    \"\"\"He normal initializer.\n+\n+    It draws samples from a truncated normal distribution centered on 0\n+    with `stddev = sqrt(2 / fan_in)`\n+    where `fan_in` is the number of input units in the weight tensor.\n+\n+    # Arguments\n+        seed: A Python integer. Used to seed the random generator.\n+        data_format: Image data format to use for convolution kernels.\n+\n+    # Returns\n+        An initializer.\n+\n+    # References\n+        He et al., http://arxiv.org/abs/1502.01852\n+    \"\"\"\n+    return VarianceScaling(scale=2., mode='fan_in', distribution='normal')\n+\n+\n+def he_uniform(seed=None, data_format=None):\n+    \"\"\"He uniform variance scaling initializer.\n+\n+    It draws samples from a uniform distribution within [-limit, limit]\n+    where `limit` is `sqrt(6 / fan_in)`\n+    where `fan_in` is the number of input units in the weight tensor.\n+\n+    # Arguments\n+        seed: A Python integer. Used to seed the random generator.\n+        data_format: Image data format to use for convolution kernels.\n+\n+    # Returns\n+        An initializer.\n+\n+    # References\n+        He et al., http://arxiv.org/abs/1502.01852\n+    \"\"\"\n+    return VarianceScaling(scale=2., mode='fan_in', distribution='uniform')\n+\n+\n+# Compatibility aliases\n+\n+zero = zeros = Zeros\n+one = ones = Ones\n+constant = Constant\n+uniform = random_uniform = RandomUniform\n+normal = random_normal = RandomNormal\n+truncated_normal = TruncatedNormal\n+identity = Identity\n+orthogonal = Orthogonal\n+\n+# Utility functions\n+\n+\n+def _compute_fans(shape, data_format='channels_first'):\n+    \"\"\"Computes the number of input and output units for a weight shape.\n+\n+    # Arguments\n+        shape: Integer shape tuple.\n+        data_format: Image data format to use for convolution kernels.\n+\n+    # Returns\n+        A tuple of scalars, `(fan_in, fan_out)`.\n+    \"\"\"\n+    if len(shape) == 2:\n+        fan_in = shape[0]\n+        fan_out = shape[1]\n+    elif len(shape) == 4 or len(shape) == 5:\n+        # Assuming convolution kernels (2D or 3D).\n+        # TH kernel shape: (depth, input_depth, ...)\n+        # TF kernel shape: (..., input_depth, depth)\n+        if data_format == 'channels_first':\n+            receptive_field_size = np.prod(shape[2:])\n+            fan_in = shape[1] * receptive_field_size\n+            fan_out = shape[0] * receptive_field_size\n+        elif data_format == 'channels_last':\n+            receptive_field_size = np.prod(shape[:2])\n+            fan_in = shape[-2] * receptive_field_size\n+            fan_out = shape[-1] * receptive_field_size\n+        else:\n+            raise ValueError('Invalid data_format: ' + data_format)\n+    else:\n+        # No specific assumptions.\n+        fan_in = np.sqrt(np.prod(shape))\n+        fan_out = np.sqrt(np.prod(shape))\n+    return fan_in, fan_out\n+\n+\n+def get(identifier, **kwargs):\n+    return get_from_module(identifier, globals(),\n+                           'initializer', instantiate=True, kwargs=kwargs)\n\n@@ -1,108 +0,0 @@\n-import pytest\n-import numpy as np\n-\n-from keras import initializations\n-from keras import backend as K\n-\n-# 2D tensor test fixture\n-FC_SHAPE = (100, 100)\n-\n-# 4D convolution in th order. This shape has the same effective shape as FC_SHAPE\n-CONV_SHAPE = (25, 25, 2, 2)\n-\n-# The equivalent shape of both test fixtures\n-SHAPE = (100, 100)\n-\n-\n-def _runner(init, shape, target_mean=None, target_std=None,\n-            target_max=None, target_min=None):\n-    variable = init(shape)\n-    output = K.get_value(variable)\n-    lim = 1e-2\n-    if target_std is not None:\n-        assert abs(output.std() - target_std) < lim\n-    if target_mean is not None:\n-        assert abs(output.mean() - target_mean) < lim\n-    if target_max is not None:\n-        assert abs(output.max() - target_max) < lim\n-    if target_min is not None:\n-        assert abs(output.min() - target_min) < lim\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_uniform(tensor_shape):\n-    _runner(initializations.uniform, tensor_shape, target_mean=0.,\n-            target_max=0.05, target_min=-0.05)\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_normal(tensor_shape):\n-    _runner(initializations.normal, tensor_shape, target_mean=0., target_std=0.05)\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_lecun_uniform(tensor_shape):\n-    scale = np.sqrt(3. / SHAPE[0])\n-    _runner(initializations.lecun_uniform, tensor_shape,\n-            target_mean=0., target_max=scale, target_min=-scale)\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_glorot_uniform(tensor_shape):\n-    scale = np.sqrt(6. / (SHAPE[0] + SHAPE[1]))\n-    _runner(initializations.glorot_uniform, tensor_shape, target_mean=0.,\n-            target_max=scale, target_min=-scale)\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_glorot_normal(tensor_shape):\n-    scale = np.sqrt(2. / (SHAPE[0] + SHAPE[1]))\n-    _runner(initializations.glorot_normal, tensor_shape,\n-            target_mean=0., target_std=scale)\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_he_uniform(tensor_shape):\n-    scale = np.sqrt(6. / SHAPE[0])\n-    _runner(initializations.he_uniform, tensor_shape, target_mean=0.,\n-            target_max=scale, target_min=-scale)\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_he_normal(tensor_shape):\n-    scale = np.sqrt(2. / SHAPE[0])\n-    _runner(initializations.he_normal, tensor_shape,\n-            target_mean=0., target_std=scale)\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_orthogonal(tensor_shape):\n-    _runner(initializations.orthogonal, tensor_shape,\n-            target_mean=0.)\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_identity(tensor_shape):\n-    if len(tensor_shape) > 2:\n-        with pytest.raises(Exception):\n-            _runner(initializations.identity, tensor_shape,\n-                    target_mean=1. / SHAPE[0], target_max=1.)\n-    else:\n-        _runner(initializations.identity, tensor_shape,\n-                target_mean=1. / SHAPE[0], target_max=1.)\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_zero(tensor_shape):\n-    _runner(initializations.zero, tensor_shape,\n-            target_mean=0., target_max=0.)\n-\n-\n-@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n-def test_one(tensor_shape):\n-    _runner(initializations.one, tensor_shape,\n-            target_mean=1., target_max=1.)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -0,0 +1,123 @@\n+import pytest\n+import numpy as np\n+\n+from keras import initializers\n+from keras import backend as K\n+\n+# 2D tensor test fixture\n+FC_SHAPE = (200, 100)\n+\n+# 4D convolution in th order. This shape has the same effective shape as FC_SHAPE\n+CONV_SHAPE = (25, 25, 20, 20)\n+\n+\n+def _runner(init, shape, target_mean=None, target_std=None,\n+            target_max=None, target_min=None):\n+    variable = K.variable(init(shape))\n+    output = K.get_value(variable)\n+    lim = 3e-2\n+    if target_std is not None:\n+        assert abs(output.std() - target_std) < lim\n+    if target_mean is not None:\n+        assert abs(output.mean() - target_mean) < lim\n+    if target_max is not None:\n+        assert abs(output.max() - target_max) < lim\n+    if target_min is not None:\n+        assert abs(output.min() - target_min) < lim\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_uniform(tensor_shape):\n+    _runner(initializers.RandomUniform(minval=-1, maxval=1), tensor_shape,\n+            target_mean=0., target_max=1, target_min=-1)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_normal(tensor_shape):\n+    _runner(initializers.RandomNormal(mean=0, stddev=1), tensor_shape,\n+            target_mean=0., target_std=1)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_truncated_normal(tensor_shape):\n+    _runner(initializers.TruncatedNormal(mean=0, stddev=1), tensor_shape,\n+            target_mean=0., target_std=None, target_max=2)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_constant(tensor_shape):\n+    _runner(initializers.Constant(2), tensor_shape,\n+            target_mean=2, target_max=2, target_min=2)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_lecun_uniform(tensor_shape):\n+    fan_in, _ = initializers._compute_fans(tensor_shape, data_format='channels_last')\n+    scale = np.sqrt(3. / fan_in)\n+    _runner(initializers.lecun_uniform(data_format='channels_last'), tensor_shape,\n+            target_mean=0., target_max=scale, target_min=-scale)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_glorot_uniform(tensor_shape):\n+    fan_in, fan_out = initializers._compute_fans(tensor_shape, data_format='channels_last')\n+    scale = np.sqrt(6. / (fan_in + fan_out))\n+    _runner(initializers.glorot_uniform(data_format='channels_last'), tensor_shape,\n+            target_mean=0., target_max=scale, target_min=-scale)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_he_uniform(tensor_shape):\n+    fan_in, _ = initializers._compute_fans(tensor_shape, data_format='channels_last')\n+    scale = np.sqrt(6. / fan_in)\n+    _runner(initializers.he_uniform(data_format='channels_last'), tensor_shape,\n+            target_mean=0., target_max=scale, target_min=-scale)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_glorot_normal(tensor_shape):\n+    fan_in, fan_out = initializers._compute_fans(tensor_shape, data_format='channels_last')\n+    scale = np.sqrt(2. / (fan_in + fan_out))\n+    _runner(initializers.glorot_normal(data_format='channels_last'), tensor_shape,\n+            target_mean=0., target_std=None, target_max=2 * scale)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_he_normal(tensor_shape):\n+    fan_in, _ = initializers._compute_fans(tensor_shape, data_format='channels_last')\n+    scale = np.sqrt(2. / fan_in)\n+    _runner(initializers.he_normal(data_format='channels_last'), tensor_shape,\n+            target_mean=0., target_std=None, target_max=2 * scale)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_orthogonal(tensor_shape):\n+    _runner(initializers.orthogonal(), tensor_shape,\n+            target_mean=0.)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [(100, 100), (1, 2, 3, 4)], ids=['FC', 'CONV'])\n+def test_identity(tensor_shape):\n+    if len(tensor_shape) > 2:\n+        with pytest.raises(Exception):\n+            _runner(initializers.identity(), tensor_shape,\n+                    target_mean=1. / tensor_shape[0], target_max=1.)\n+    else:\n+        _runner(initializers.identity(), tensor_shape,\n+                target_mean=1. / tensor_shape[0], target_max=1.)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_zero(tensor_shape):\n+    _runner(initializers.zeros(), tensor_shape,\n+            target_mean=0., target_max=0.)\n+\n+\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_one(tensor_shape):\n+    _runner(initializers.ones(), tensor_shape,\n+            target_mean=1., target_max=1.)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c630c3e3c8969b40a47d07b9f2edda50ec69720", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 70 | Files Changed: 1 | Hunks: 8 | Methods Changed: 11 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 76 | Churn Cumulative: 877 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n from __future__ import absolute_import\n from . import backend as K\n from .utils.generic_utils import get_from_module\n-import warnings\n \n \n class Regularizer(object):\n@@ -9,58 +8,10 @@ class Regularizer(object):\n     \"\"\"\n \n     def __call__(self, x):\n-        return 0\n-\n-    def get_config(self):\n-        return {'name': self.__class__.__name__}\n-\n-    def set_param(self, _):\n-        warnings.warn('The `set_param` method on regularizers is deprecated. '\n-                      'It no longer does anything, '\n-                      'and it will be removed after 06/2017.')\n-\n-    def set_layer(self, _):\n-        warnings.warn('The `set_layer` method on regularizers is deprecated. '\n-                      'It no longer does anything, '\n-                      'and it will be removed after 06/2017.')\n+        return 0.\n \n \n-class EigenvalueRegularizer(Regularizer):\n-    \"\"\"Regularizer based on the eignvalues of a weight matrix.\n-\n-    Only available for tensors of rank 2.\n-\n-    # Arguments\n-        k: Float; modulates the amount of regularization to apply.\n-    \"\"\"\n-\n-    def __init__(self, k):\n-        self.k = k\n-\n-    def __call__(self, x):\n-        if K.ndim(x) != 2:\n-            raise ValueError('EigenvalueRegularizer '\n-                             'is only available for tensors of rank 2.')\n-        covariance = K.dot(K.transpose(x), x)\n-        dim1, dim2 = K.eval(K.shape(covariance))\n-\n-        # Power method for approximating the dominant eigenvector:\n-        power = 9  # Number of iterations of the power method.\n-        o = K.ones([dim1, 1])  # Initial values for the dominant eigenvector.\n-        main_eigenvect = K.dot(covariance, o)\n-        for n in range(power - 1):\n-            main_eigenvect = K.dot(covariance, main_eigenvect)\n-        covariance_d = K.dot(covariance, main_eigenvect)\n-\n-        # The corresponding dominant eigenvalue:\n-        main_eigenval = (K.dot(K.transpose(covariance_d), main_eigenvect) /\n-                         K.dot(K.transpose(main_eigenvect), main_eigenvect))\n-        # Multiply by the given regularization gain.\n-        regularization = (main_eigenval ** 0.5) * self.k\n-        return K.sum(regularization)\n-\n-\n-class L1L2Regularizer(Regularizer):\n+class L1L2(Regularizer):\n     \"\"\"Regularizer for L1 and L2 regularization.\n \n     # Arguments\n@@ -73,7 +24,7 @@ class L1L2Regularizer(Regularizer):\n         self.l2 = K.cast_to_floatx(l2)\n \n     def __call__(self, x):\n-        regularization = 0\n+        regularization = 0.\n         if self.l1:\n             regularization += K.sum(self.l1 * K.abs(x))\n         if self.l2:\n@@ -88,32 +39,17 @@ class L1L2Regularizer(Regularizer):\n \n # Aliases.\n \n-WeightRegularizer = L1L2Regularizer\n-ActivityRegularizer = L1L2Regularizer\n-\n \n def l1(l=0.01):\n-    return L1L2Regularizer(l1=l)\n+    return L1L2(l1=l)\n \n \n def l2(l=0.01):\n-    return L1L2Regularizer(l2=l)\n+    return L1L2(l2=l)\n \n \n def l1l2(l1=0.01, l2=0.01):\n-    return L1L2Regularizer(l1=l1, l2=l2)\n-\n-\n-def activity_l1(l=0.01):\n-    return L1L2Regularizer(l1=l)\n-\n-\n-def activity_l2(l=0.01):\n-    return L1L2Regularizer(l2=l)\n-\n-\n-def activity_l1l2(l1=0.01, l2=0.01):\n-    return L1L2Regularizer(l1=l1, l2=l2)\n+    return L1L2(l1=l1, l2=l2)\n \n \n def get(identifier, kwargs=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a56b1a55182acf061b1eb2e2c86b48193a0e88f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 95 | Files Changed: 1 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 96 | Churn Cumulative: 630 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -77,107 +77,13 @@ def cosine_proximity(y_true, y_pred):\n     return - K.mean(y_true * y_pred)\n \n \n-def matthews_correlation(y_true, y_pred):\n-    \"\"\"Matthews correlation metric.\n+# Aliases\n \n-    It is only computed as a batch-wise average, not globally.\n-\n-    Computes the Matthews correlation coefficient measure for quality\n-    of binary classification problems.\n-    \"\"\"\n-    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n-    y_pred_neg = 1 - y_pred_pos\n-\n-    y_pos = K.round(K.clip(y_true, 0, 1))\n-    y_neg = 1 - y_pos\n-\n-    tp = K.sum(y_pos * y_pred_pos)\n-    tn = K.sum(y_neg * y_pred_neg)\n-\n-    fp = K.sum(y_neg * y_pred_pos)\n-    fn = K.sum(y_pos * y_pred_neg)\n-\n-    numerator = (tp * tn - fp * fn)\n-    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n-\n-    return numerator / (denominator + K.epsilon())\n-\n-\n-def precision(y_true, y_pred):\n-    \"\"\"Precision metric.\n-\n-    Only computes a batch-wise average of precision.\n-\n-    Computes the precision, a metric for multi-label classification of\n-    how many selected items are relevant.\n-    \"\"\"\n-    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n-    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n-    precision = true_positives / (predicted_positives + K.epsilon())\n-    return precision\n-\n-\n-def recall(y_true, y_pred):\n-    \"\"\"Recall metric.\n-\n-    Only computes a batch-wise average of recall.\n-\n-    Computes the recall, a metric for multi-label classification of\n-    how many relevant items are selected.\n-    \"\"\"\n-    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n-    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n-    recall = true_positives / (possible_positives + K.epsilon())\n-    return recall\n-\n-\n-def fbeta_score(y_true, y_pred, beta=1):\n-    \"\"\"Computes the F score.\n-\n-    The F score is the weighted harmonic mean of precision and recall.\n-    Here it is only computed as a batch-wise average, not globally.\n-\n-    This is useful for multi-label classification, where input samples can be\n-    classified as sets of labels. By only using accuracy (precision) a model\n-    would achieve a perfect score by simply assigning every class to every\n-    input. In order to avoid this, a metric should penalize incorrect class\n-    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n-    computes this, as a weighted mean of the proportion of correct class\n-    assignments vs. the proportion of incorrect class assignments.\n-\n-    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n-    correct classes becomes more important, and with beta > 1 the metric is\n-    instead weighted towards penalizing incorrect class assignments.\n-    \"\"\"\n-    if beta < 0:\n-        raise ValueError('The lowest choosable beta is zero (only precision).')\n-\n-    # If there are no true positives, fix the F score at 0 like sklearn.\n-    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n-        return 0\n-\n-    p = precision(y_true, y_pred)\n-    r = recall(y_true, y_pred)\n-    bb = beta ** 2\n-    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n-    return fbeta_score\n-\n-\n-def fmeasure(y_true, y_pred):\n-    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n-\n-    Here it is only computed as a batch-wise average, not globally.\n-    \"\"\"\n-    return fbeta_score(y_true, y_pred, beta=1)\n-\n-\n-# aliases\n mse = MSE = mean_squared_error\n mae = MAE = mean_absolute_error\n mape = MAPE = mean_absolute_percentage_error\n msle = MSLE = mean_squared_logarithmic_error\n cosine = cosine_proximity\n-fscore = f1score = fmeasure\n \n \n def get(identifier):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
