{"custom_id": "keras#b0b3ff13b02c82db3f9afe566327bbc2cf061538", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 1371 | Contributors (this commit): 16 | Commits (past 90d): 63 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -325,7 +325,7 @@ class Sequential(Model, containers.Sequential):\n         self.params = [] # learnable\n         self.regularizers = [] # same size as params\n         self.constraints = [] # same size as params\n-        self.cost_updates = [] # NOT the same size as params\n+        self.cost_updates = [] # size can vary, no 1-to-1 mapping to params\n \n \n     def get_config(self, verbose=0):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c274cb990b33b77cc30d787e3c6705aa0695157b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 61 | Contributors (this commit): 5 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ import time\n import sys\n \n def get_from_module(identifier, module_params, module_name, instantiate=False):\n-    if type(identifier) is str:\n+    if isinstance(identifier, basestring):\n         res = module_params.get(identifier)\n         if not res:\n             raise Exception('Invalid ' + str(module_name) + ': ' + str(identifier))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83419e644f6fb223701c36a08ff2235566e3d37f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 33 | Files Changed: 4 | Hunks: 15 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 79 | Churn Cumulative: 1361 | Contributors (this commit): 11 | Commits (past 90d): 70 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ import theano\n import theano.tensor as T\n \n from .. import activations, initializations\n-from ..utils.theano_utils import shared_zeros, floatX\n+from ..utils.theano_utils import shared_zeros, floatX, shared_scalar, get_mask\n from ..utils.generic_utils import make_tuple\n from .. import regularizers\n from .. import constraints\n@@ -144,19 +144,21 @@ class Dropout(Layer):\n     '''\n         Hinton's dropout.\n     '''\n-    def __init__(self, p):\n+    def __init__(self, p, mask_val=default_mask_val):\n         super(Dropout,self).__init__()\n         self.p = p\n+        self.mask_val = shared_scalar(mask_val)\n \n     def get_output(self, train):\n         X = self.get_input(train)\n+        mask = get_mask(X, self.mask_val)\n         if self.p > 0.:\n             retain_prob = 1. - self.p\n             if train:\n                 X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n             else:\n                 X *= retain_prob\n-        return X\n+        return mask*X + (1-mask)*self.mask_val\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n\n@@ -5,28 +5,10 @@ import theano.tensor as T\n import numpy as np\n \n from .. import activations, initializations\n-from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix\n+from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix, get_mask\n from ..layers.core import Layer, default_mask_val\n from six.moves import range\n \n-def get_mask(X, mask_val, steps_back=0):\n-    '''\n-        Given X, a (timesteps, nb_samples, n_dimensions) tensor, returns a mask\n-        tensor and, if steps_back>0, a padded_mask tensor  with dimension (timesteps + steps_back, nb_samples, 1).\n-        This tensor is left-padded with `steps_back` zeros in the time dimension.\n-        \n-        The mask has a 1 for every entry except for those corresponding to a vector in X that has every entry equal to mask_val.\n-    '''\n-    mask = T.neq(X, mask_val).sum(axis=2) > 0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n-    mask = T.addbroadcast(mask[:, :, np.newaxis], 2) # (time, nb_samples, 1) matrix.\n-    if steps_back > 0:\n-        # left-pad in time with 0\n-        pad = alloc_zeros_matrix(steps_back, mask.shape[1], 1).astype('uint8')\n-        return mask, T.concatenate([pad, mask], axis=0)\n-    return mask\n-\n-    \n-\n class SimpleRNN(Layer):\n     '''\n         Fully connected RNN where output is to fed back to input.\n\n@@ -20,3 +20,29 @@ def shared_ones(shape, dtype=theano.config.floatX, name=None):\n \n def alloc_zeros_matrix(*dims):\n     return T.alloc(np.cast[theano.config.floatX](0.), *dims)\n+\n+\n+\n+def get_mask(X, mask_val, steps_back=0):\n+    '''\n+        Given X, a tensor or matrix, return mask tension of same number of dimensions thus:\n+                    X.shape                     mask.shape\n+            -------------------------------------------------\n+            (time, samples, dimensions)  -> (time, samples, 1)\n+            (samples, dimensions)        -> (samples, 1)\n+        \n+        and, if steps_back>0, a padded_mask tensor that is left-padded with `steps_back` zeros\n+        along the time dimension.\n+        \n+        The mask has a 1 for every entry except for those corresponding to a vector in X that has every entry equal to mask_val.\n+    '''\n+    mask = T.neq(X, mask_val).sum(axis=-1) > 0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n+    mask = T.shape_padright(mask)\n+    mask = T.addbroadcast(mask, -1) # (time, nb_samples, 1) matrix.\n+\n+    if steps_back > 0:\n+        # left-pad in time with 0\n+        pad = alloc_zeros_matrix(steps_back, mask.shape[1], 1).astype('uint8')\n+        return mask, T.concatenate([pad, mask], axis=0)\n+    return mask\n+\n\n@@ -4,7 +4,7 @@\n import numpy as np\n from keras.utils.theano_utils import sharedX\n from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Merge\n+from keras.layers.core import Dense, Activation, Merge, Dropout\n from keras.layers.embeddings import Embedding\n from keras.layers.recurrent import SimpleRNN, SimpleDeepRNN\n from keras.layers.core import default_mask_val\n@@ -13,13 +13,15 @@ import theano\n theano.config.exception_verbosity='high' \n \n # (nb_samples, timesteps, dimensions)\n-X = np.random.random_integers(1, 4, size=(400000,4))\n+X = np.random.random_integers(1, 4, size=(500000,15))\n \n model = Sequential()\n-model.add(Embedding(5, 2, zero_is_mask=True))\n-model.add(SimpleRNN(2,3, activation='relu', return_sequences=True))\n-model.add(SimpleDeepRNN(3,3, depth=2, activation='relu')) \n-model.add(Dense(3,4, activation='softmax'))\n+model.add(Embedding(5, 4, zero_is_mask=True))\n+model.add(SimpleRNN(4,4, activation='relu', return_sequences=True))\n+model.add(Dropout(0.5))\n+model.add(SimpleDeepRNN(4,4, depth=2, activation='relu')) \n+model.add(Dropout(0.5))\n+model.add(Dense(4,4, activation='softmax'))\n model.compile(loss='categorical_crossentropy',\n         optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n print(\"Compiled model\")\n@@ -32,18 +34,19 @@ if (W0 != default_mask_val).any():\n W = model.get_weights() # We'll save these so we can reset it later\n \n \n+X[:,:10] = 0\n Xmask0 = X.copy()\n-Xmask0[:,0] = 0\n+Xmask0[:,10] = 0\n \n Xmask12 = X.copy()\n-Xmask12[:,1] = 0\n-Xmask12[:,2] = 0\n+Xmask12[:,11] = 0\n+Xmask12[:,12] = 0\n \n X0_onehot = np.zeros((X.shape[0], 4))\n X1_onehot = np.zeros((X.shape[0], 4))\n for i, row in enumerate(X):\n-    X0_onehot[i, row[0]-1] = 1\n-    X1_onehot[i, row[1]-1] = 1\n+    X0_onehot[i, row[10]-1] = 1\n+    X1_onehot[i, row[11]-1] = 1\n \n # Uniform score: 4 options = ln(4) nats (2 bits)\n # we should not do better than this when we mask out the part of the input\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f8ee81d89e9f35352fa882d7b7624899de344da8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 43 | Files Changed: 2 | Hunks: 17 | Methods Changed: 17 | Complexity Δ (Sum/Max): -5/1 | Churn Δ: 56 | Churn Cumulative: 1528 | Contributors (this commit): 16 | Commits (past 90d): 80 | Contributors (cumulative): 19 | DMM Complexity: 0.12\n\nDIFF:\n@@ -75,10 +75,7 @@ class Model(object):\n         # target of model\n         self.y = T.zeros_like(self.y_train)\n \n-        # parameter for rescaling the objective function\n-        self.loss_weights = T.vector()\n-\n-        train_loss = self.loss(self.y, self.y_train, self.loss_weights)\n+        train_loss = self.loss(self.y, self.y_train)\n         test_score = self.loss(self.y, self.y_test)\n \n         if class_mode == \"categorical\":\n@@ -95,11 +92,11 @@ class Model(object):\n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints, train_loss)\n \n         if type(self.X_train) == list:\n-            train_ins = self.X_train + [self.y, self.loss_weights]\n+            train_ins = self.X_train + [self.y]\n             test_ins = self.X_test + [self.y]\n             predict_ins = self.X_test\n         else:\n-            train_ins = [self.X_train, self.y, self.loss_weights]\n+            train_ins = [self.X_train, self.y]\n             test_ins = [self.X_test, self.y]\n             predict_ins = [self.X_test]\n \n@@ -115,13 +112,11 @@ class Model(object):\n             allow_input_downcast=True, mode=theano_mode)\n \n \n-    def train(self, X, y, accuracy=False, sample_weight=None, class_weight=None):\n+    def train(self, X, y, accuracy=False):\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n-        w = calculate_loss_weights(y, sample_weight=sample_weight, class_weight=class_weight)\n-\n-        ins = X + [y, w]\n+        ins = X + [y]\n         if accuracy:\n             return self._train_with_acc(*ins)\n         else:\n@@ -139,12 +134,10 @@ class Model(object):\n \n \n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n-            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False,\n-            sample_weight=None, class_weight=None):\n+            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False):\n \n         X = standardize_X(X)\n         y = standardize_y(y)\n-        sample_weight = calculate_loss_weights(y, sample_weight=sample_weight, class_weight=class_weight)\n \n         do_validation = False\n         if validation_data:\n@@ -199,14 +192,12 @@ class Model(object):\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n \n-                w = sample_weight[batch_ids]\n-\n                 batch_logs = {}\n                 batch_logs['batch'] = batch_index\n                 batch_logs['size'] = len(batch_ids)\n                 callbacks.on_batch_begin(batch_index, batch_logs)\n \n-                ins = X_batch + [y_batch, w]\n+                ins = X_batch + [y_batch]\n                 if show_accuracy:\n                     loss, acc = self._train_with_acc(*ins)\n                     batch_logs['accuracy'] = acc\n\n@@ -6,51 +6,30 @@ from six.moves import range\n \n epsilon = 1.0e-9\n \n-def mean_squared_error(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        return T.sqr(weight.reshape((weight.shape[0], 1)) * (y_pred - y_true)).mean()\n-    else:\n+def mean_squared_error(y_true, y_pred):\n     return T.sqr(y_pred - y_true).mean()\n \n-def mean_absolute_error(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        return T.abs_(weight.reshape((weight.shape[0], 1)) * (y_pred - y_true)).mean()\n-    else:\n+def mean_absolute_error(y_true, y_pred):\n     return T.abs_(y_pred - y_true).mean()\n \n-def squared_hinge(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        weight = weight.reshape((weight.shape[0], 1))\n-        return T.sqr(weight * T.maximum(1. - (y_true * y_pred), 0.)).mean()\n-    else:\n+def squared_hinge(y_true, y_pred):\n     return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean()\n \n-def hinge(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        weight = weight.reshape((weight.shape[0], 1))\n-        return (weight * T.maximum(1. - (y_true * y_pred), 0.)).mean()\n-    else:\n+def hinge(y_true, y_pred):\n     return T.maximum(1. - y_true * y_pred, 0.).mean()\n \n-def categorical_crossentropy(y_true, y_pred, weight=None):\n+def categorical_crossentropy(y_true, y_pred):\n     '''Expects a binary class matrix instead of a vector of scalar classes\n     '''\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     # scale preds so that the class probas of each sample sum to 1\n     y_pred /= y_pred.sum(axis=1, keepdims=True) \n     cce = T.nnet.categorical_crossentropy(y_pred, y_true)\n-    if weight is not None:\n-        # return avg. of scaled cat. crossentropy\n-        return (weight * cce).mean()\n-    else:\n     return cce.mean()\n \n-def binary_crossentropy(y_true, y_pred, weight=None):\n+def binary_crossentropy(y_true, y_pred):\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     bce = T.nnet.binary_crossentropy(y_pred, y_true)\n-    if weight is not None:\n-        return (weight.reshape((weight.shape[0], 1)) * bce).mean()\n-    else:\n     return bce.mean()\n \n # aliases\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef4250fc05b23522b356e83ca8368197804923a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1ab9769ec6ff045a2f8d1dbea26169c27498797", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#852f5d977f51ece05dfafff8722bd2cffd7c330a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 930 | Contributors (this commit): 9 | Commits (past 90d): 41 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,7 +15,7 @@ from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n srng = RandomStreams(seed=np.random.randint(10e6))\n \n-default_mask_val = -999.\n+default_mask_val = -999\n \n class Layer(object):\n     def __init__(self):\n@@ -291,7 +291,7 @@ class TimeDistributedDense(Layer):\n \n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, \n-        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, mask_val=default_mask_val):\n \n         super(TimeDistributedDense,self).__init__()\n         self.init = initializations.get(init)\n@@ -307,18 +307,21 @@ class TimeDistributedDense(Layer):\n \n         self.regularizers = [W_regularizer, b_regularizer]\n         self.constraints = [W_constraint, b_constraint]\n+        self.mask_val = shared_scalar(mask_val)\n \n         if weights is not None:\n             self.set_weights(weights)\n \n     def get_output(self, train):\n         X = self.get_input(train)\n+        X = X.dimshuffle(1,0,2)\n+        mask = get_mask(X, self.mask_val)\n \n-        def act_func(X):\n-            return self.activation(T.dot(X, self.W) + self.b)\n+        def act_func(X, mask):\n+            return mask * self.activation(T.dot(X, self.W) + self.b) + (1 - mask) * self.mask_val\n \n         output, _ = theano.scan(fn = act_func,\n-                                sequences = X.dimshuffle(1,0,2),\n+                                sequences = [X, mask],\n                                 outputs_info=None)\n         return output.dimshuffle(1,0,2)\n \n\n@@ -4,7 +4,7 @@\n import numpy as np\n from keras.utils.theano_utils import sharedX\n from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Merge, Dropout\n+from keras.layers.core import Dense, Activation, Merge, Dropout, TimeDistributedDense\n from keras.layers.embeddings import Embedding\n from keras.layers.recurrent import SimpleRNN, SimpleDeepRNN\n from keras.layers.core import default_mask_val\n@@ -17,6 +17,7 @@ X = np.random.random_integers(1, 4, size=(500000,15))\n \n model = Sequential()\n model.add(Embedding(5, 4, zero_is_mask=True))\n+model.add(TimeDistributedDense(4,4)) # obviously this is redundant. Just testing.\n model.add(SimpleRNN(4,4, activation='relu', return_sequences=True))\n model.add(Dropout(0.5))\n model.add(SimpleDeepRNN(4,4, depth=2, activation='relu')) \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f3a1f0e9bc55630acac3efdafb3c1c9fa719dec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 252 | Contributors (this commit): 5 | Commits (past 90d): 23 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -39,14 +39,13 @@ class SimpleRNN(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def _step(self, x_t, mask_t, mask_tm1, h_tm1, u):\n+    def _step(self, x_t, mask_tm1, h_tm1, u):\n         '''\n             Variable names follow the conventions from: \n             http://deeplearning.net/software/theano/library/scan.html\n \n         '''\n-        normal_output = self.activation(x_t + mask_tm1 * T.dot(h_tm1, u))\n-        return mask_t * normal_output\n+        return self.activation(x_t + mask_tm1 * T.dot(h_tm1, u))\n \n     def get_output(self, train):\n         X = self.get_input(train) # shape: (nb_samples, time (padded with zeros at the end), input_dim)\n@@ -62,7 +61,7 @@ class SimpleRNN(Layer):\n         # Iterate over the first dimension of the x array (=time).\n         outputs, updates = theano.scan(\n             self._step, # this will be called with arguments (sequences[i], outputs[i-1], non_sequences[i])\n-            sequences=[x, dict(input=padded_mask,taps=[0, -1])], # tensors to iterate over, inputs to _step\n+            sequences=[x, dict(input=padded_mask, taps=[-1])], # tensors to iterate over, inputs to _step\n             # initialization of the output. Input to _step with default tap=-1.\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=self.U, # static inputs to _step\n@@ -120,14 +119,14 @@ class SimpleDeepRNN(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def _step(self, x_t, mask_t, *args):\n+    def _step(self, x_t, *args):\n         o = x_t\n         for i in range(self.depth):\n             mask_tmi = args[i]\n             h_tmi = args[i + self.depth]\n             U_tmi = args[i + 2*self.depth]\n             o += mask_tmi*self.inner_activation(T.dot(h_tmi, U_tmi))\n-        return mask_t*self.activation(o)\n+        return self.activation(o)\n \n     def get_output(self, train):\n         X = self.get_input(train)\n@@ -146,7 +145,7 @@ class SimpleDeepRNN(Layer):\n             self._step,\n             sequences=[x, dict(\n                 input = padded_mask,\n-                taps = [(-i) for i in range(self.depth+1)]\n+                taps = [(-i) for i in range(self.depth)]\n             )],\n             outputs_info=[dict(\n                 initial = initial,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3519221b401eabd5b01b0bc754ea833645feb2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 17 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 68 | Churn Cumulative: 323 | Contributors (this commit): 5 | Commits (past 90d): 27 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -198,7 +198,7 @@ class GRU(Layer):\n     def __init__(self, input_dim, output_dim=128, \n         init='glorot_uniform', inner_init='orthogonal',\n         activation='sigmoid', inner_activation='hard_sigmoid',\n-        weights=None, truncate_gradient=-1, return_sequences=False):\n+        weights=None, truncate_gradient=-1, return_sequences=False, mask_val=default_mask_val):\n \n         super(GRU,self).__init__()\n         self.input_dim = input_dim\n@@ -211,6 +211,7 @@ class GRU(Layer):\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n         self.input = T.tensor3()\n+        self.mask_val = shared_scalar(default_mask_val)\n \n         self.W_z = self.init((self.input_dim, self.output_dim))\n         self.U_z = self.inner_init((self.output_dim, self.output_dim))\n@@ -234,29 +235,35 @@ class GRU(Layer):\n             self.set_weights(weights)\n \n     def _step(self, \n-        xz_t, xr_t, xh_t, \n+        xz_t, xr_t, xh_t, mask_tm1,\n         h_tm1, \n         u_z, u_r, u_h):\n-        z = self.inner_activation(xz_t + T.dot(h_tm1, u_z))\n-        r = self.inner_activation(xr_t + T.dot(h_tm1, u_r))\n-        hh_t = self.activation(xh_t + T.dot(r * h_tm1, u_h))\n-        h_t = z * h_tm1 + (1 - z) * hh_t\n+        h_mask_tm1 = mask_tm1 * h_tm1\n+        z = self.inner_activation(xz_t + T.dot(h_mask_tm1, u_z))\n+        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n+        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n+        h_t = z * h_mask_tm1 + (1 - z) * hh_t\n+        #return theano.printing.Print(\"h_t\")(h_t)\n         return h_t\n \n     def get_output(self, train):\n         X = self.get_input(train) \n         X = X.dimshuffle((1,0,2)) \n+        mask, padded_mask = get_mask(X, self.mask_val, steps_back=1)\n \n         x_z = T.dot(X, self.W_z) + self.b_z\n         x_r = T.dot(X, self.W_r) + self.b_r\n         x_h = T.dot(X, self.W_h) + self.b_h\n         outputs, updates = theano.scan(\n             self._step, \n-            sequences=[x_z, x_r, x_h], \n+            sequences=[x_z, x_r, x_h, padded_mask], \n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=[self.U_z, self.U_r, self.U_h],\n             truncate_gradient=self.truncate_gradient\n         )\n+\n+        outputs = mask * outputs + (1 - mask) * self.mask_val\n+\n         if self.return_sequences:\n             return outputs.dimshuffle((1,0,2))\n         return outputs[-1]\n@@ -302,13 +309,14 @@ class LSTM(Layer):\n     def __init__(self, input_dim, output_dim=128, \n         init='glorot_uniform', inner_init='orthogonal', \n         activation='tanh', inner_activation='hard_sigmoid',\n-        weights=None, truncate_gradient=-1, return_sequences=False):\n+        weights=None, truncate_gradient=-1, return_sequences=False, mask_val=default_mask_val):\n     \n         super(LSTM,self).__init__()\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n         self.return_sequences = return_sequences\n+        self.mask_val = shared_scalar(mask_val)\n \n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -343,19 +351,23 @@ class LSTM(Layer):\n             self.set_weights(weights)\n \n     def _step(self, \n-        xi_t, xf_t, xo_t, xc_t, \n+        xi_t, xf_t, xo_t, xc_t, mask_tm1,\n         h_tm1, c_tm1, \n         u_i, u_f, u_o, u_c): \n-        i_t = self.inner_activation(xi_t + T.dot(h_tm1, u_i))\n-        f_t = self.inner_activation(xf_t + T.dot(h_tm1, u_f))\n-        c_t = f_t * c_tm1 + i_t * self.activation(xc_t + T.dot(h_tm1, u_c))\n-        o_t = self.inner_activation(xo_t + T.dot(h_tm1, u_o))\n+        h_mask_tm1 = mask_tm1 * h_tm1\n+        c_mask_tm1 = mask_tm1 * c_tm1\n+\n+        i_t = self.inner_activation(xi_t + T.dot(h_mask_tm1, u_i))\n+        f_t = self.inner_activation(xf_t + T.dot(h_mask_tm1, u_f))\n+        c_t = f_t * c_mask_tm1 + i_t * self.activation(xc_t + T.dot(h_mask_tm1, u_c))\n+        o_t = self.inner_activation(xo_t + T.dot(h_mask_tm1, u_o))\n         h_t = o_t * self.activation(c_t)\n         return h_t, c_t\n \n     def get_output(self, train):\n         X = self.get_input(train) \n         X = X.dimshuffle((1,0,2))\n+        mask, padded_mask = get_mask(X, self.mask_val, steps_back=1)\n \n         xi = T.dot(X, self.W_i) + self.b_i\n         xf = T.dot(X, self.W_f) + self.b_f\n@@ -364,7 +376,7 @@ class LSTM(Layer):\n         \n         [outputs, memories], updates = theano.scan(\n             self._step, \n-            sequences=[xi, xf, xo, xc],\n+            sequences=[xi, xf, xo, xc, padded_mask],\n             outputs_info=[\n                 T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n                 T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1)\n@@ -372,6 +384,8 @@ class LSTM(Layer):\n             non_sequences=[self.U_i, self.U_f, self.U_o, self.U_c], \n             truncate_gradient=self.truncate_gradient \n         )\n+\n+        outputs = mask * outputs + (1 - mask) * self.mask_val\n         if self.return_sequences:\n             return outputs.dimshuffle((1,0,2))\n         return outputs[-1]\n\n@@ -6,7 +6,7 @@ from keras.utils.theano_utils import sharedX\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation, Merge, Dropout, TimeDistributedDense\n from keras.layers.embeddings import Embedding\n-from keras.layers.recurrent import SimpleRNN, SimpleDeepRNN\n+from keras.layers.recurrent import SimpleRNN, SimpleDeepRNN, LSTM, GRU\n from keras.layers.core import default_mask_val\n import theano\n \n@@ -15,6 +15,7 @@ theano.config.exception_verbosity='high'\n # (nb_samples, timesteps, dimensions)\n X = np.random.random_integers(1, 4, size=(500000,15))\n \n+print(\"About to compile the first model\")\n model = Sequential()\n model.add(Embedding(5, 4, zero_is_mask=True))\n model.add(TimeDistributedDense(4,4)) # obviously this is redundant. Just testing.\n@@ -108,3 +109,26 @@ score = model.evaluate(Xmask12, X0_onehot, batch_size=batch_size)\n if score < uniform_score*0.9:\n     raise Exception('Somehow learned to copy timestep 0 despite masking 1, score %f' % score)\n \n+\n+# Another testing approach, just initialize models and make sure that prepending zeros doesn't affect\n+# their output\n+print(\"About to compile the second model\")\n+model2 = Sequential()\n+model2.add(Embedding(5, 4, zero_is_mask=True))\n+model2.add(TimeDistributedDense(4,4)) # obviously this is redundant. Just testing.\n+model2.add(LSTM(4,4, return_sequences=True))\n+model2.add(GRU(4,4, activation='softmax', return_sequences=True))\n+model2.add(SimpleDeepRNN(4,4, depth=2, activation='relu', return_sequences=True)) \n+model2.add(SimpleRNN(4,4, activation='relu', return_sequences=False))\n+model2.compile(loss='categorical_crossentropy',\n+        optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n+print(\"Compiled model2\")\n+\n+X2 = np.random.random_integers(1, 4, size=(1,5))\n+ref = model2.predict(X2)\n+for pre_zeros in range(1,10):\n+    padded = np.concatenate((np.zeros((1, pre_zeros)), X2), axis=1)\n+    pred = model2.predict(padded)\n+    if not np.allclose(ref, pred):\n+        raise Exception(\"Different result after left-padding %d zeros. Ref: %s, Pred: %s\" % (pre_zeros, ref, pred))\n+        \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9fbc458ed70e6bffdfc5af9d9f0ad554b6c0cec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 130 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -39,12 +39,12 @@ for i in range(0, len(text) - maxlen, step):\n print('nb sequences:', len(sentences))\n \n print('Vectorization...')\n-X = np.zeros((len(sentences), maxlen, len(chars)))\n-y = np.zeros((len(sentences), len(chars)))\n+X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n+y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n for i, sentence in enumerate(sentences):\n     for t, char in enumerate(sentence):\n-        X[i, t, char_indices[char]] = 1.\n-    y[i, char_indices[next_chars[i]]] = 1.\n+        X[i, t, char_indices[char]] = 1\n+    y[i, char_indices[next_chars[i]]] = 1\n \n \n # build the model: 2 stacked LSTM\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2384aa4c97d267b6b01ff662838731c24332ba33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 11 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 1311 | Contributors (this commit): 12 | Commits (past 90d): 77 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,15 +1,19 @@\n from __future__ import absolute_import\n import theano\n import theano.tensor as T\n+import numpy as np\n+from .utils.theano_utils import default_mask_val, get_mask\n import types\n \n def softmax(x):\n     return T.nnet.softmax(x)\n \n-def time_distributed_softmax(x):\n+def time_distributed_softmax(x, mask_val=default_mask_val):\n     xshape = x.shape\n     X = x.reshape((xshape[0] * xshape[1], xshape[2]))\n-    return T.nnet.softmax(X).reshape(xshape)\n+    mask = get_mask(X, mask_val)\n+    r =  mask * T.nnet.softmax(X) + (1 - mask) * mask_val\n+    return r.reshape(xshape)\n \n def softplus(x):\n     return T.nnet.softplus(x)\n\n@@ -6,7 +6,7 @@ import theano.tensor as T\n import numpy as np\n \n from .. import activations, initializations\n-from ..utils.theano_utils import shared_zeros, floatX, shared_scalar, get_mask\n+from ..utils.theano_utils import shared_zeros, floatX, shared_scalar, get_mask, default_mask_val\n from ..utils.generic_utils import make_tuple\n from .. import regularizers\n from .. import constraints\n@@ -15,8 +15,6 @@ from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n srng = RandomStreams(seed=np.random.randint(10e6))\n \n-default_mask_val = -999\n-\n class Layer(object):\n     def __init__(self):\n         self.params = []\n\n@@ -5,8 +5,8 @@ import theano.tensor as T\n import numpy as np\n \n from .. import activations, initializations\n-from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix, get_mask\n-from ..layers.core import Layer, default_mask_val\n+from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix, get_mask, default_mask_val\n+from ..layers.core import Layer\n from six.moves import range\n \n class SimpleRNN(Layer):\n\n@@ -3,6 +3,8 @@ import numpy as np\n import theano\n import theano.tensor as T\n \n+default_mask_val = -999\n+\n def floatX(X):\n     return np.asarray(X, dtype=theano.config.floatX)\n \n\n@@ -109,13 +109,13 @@ score = model.evaluate(Xmask12, X0_onehot, batch_size=batch_size)\n if score < uniform_score*0.9:\n     raise Exception('Somehow learned to copy timestep 0 despite masking 1, score %f' % score)\n \n-\n # Another testing approach, just initialize models and make sure that prepending zeros doesn't affect\n # their output\n print(\"About to compile the second model\")\n model2 = Sequential()\n model2.add(Embedding(5, 4, zero_is_mask=True))\n-model2.add(TimeDistributedDense(4,4)) # obviously this is redundant. Just testing.\n+model2.add(TimeDistributedDense(4,4))\n+model2.add(Activation('time_distributed_softmax'))\n model2.add(LSTM(4,4, return_sequences=True))\n model2.add(GRU(4,4, activation='softmax', return_sequences=True))\n model2.add(SimpleDeepRNN(4,4, depth=2, activation='relu', return_sequences=True)) \n@@ -124,10 +124,10 @@ model2.compile(loss='categorical_crossentropy',\n         optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n print(\"Compiled model2\")\n \n-X2 = np.random.random_integers(1, 4, size=(1,5))\n+X2 = np.random.random_integers(1, 4, size=(2,5))\n ref = model2.predict(X2)\n for pre_zeros in range(1,10):\n-    padded = np.concatenate((np.zeros((1, pre_zeros)), X2), axis=1)\n+    padded = np.concatenate((np.zeros((2, pre_zeros)), X2), axis=1)\n     pred = model2.predict(padded)\n     if not np.allclose(ref, pred):\n         raise Exception(\"Different result after left-padding %d zeros. Ref: %s, Pred: %s\" % (pre_zeros, ref, pred))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1912b0774672753cff0f44e4b6457a9689441cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3dd4b2e8ab2db2c04cf349b580886521295f386e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 9 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 18 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n\n\n\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a9f94c94a70367dc4e0449180844b41954397369", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 43 | Files Changed: 2 | Hunks: 17 | Methods Changed: 17 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 56 | Churn Cumulative: 1584 | Contributors (this commit): 16 | Commits (past 90d): 82 | Contributors (cumulative): 19 | DMM Complexity: 0.12\n\nDIFF:\n@@ -75,10 +75,7 @@ class Model(object):\n         # target of model\n         self.y = T.zeros_like(self.y_train)\n \n-        # parameter for rescaling the objective function\n-        self.loss_weights = T.vector()\n-\n-        train_loss = self.loss(self.y, self.y_train, self.loss_weights)\n+        train_loss = self.loss(self.y, self.y_train)\n         test_score = self.loss(self.y, self.y_test)\n \n         if class_mode == \"categorical\":\n@@ -95,11 +92,11 @@ class Model(object):\n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints, train_loss)\n \n         if type(self.X_train) == list:\n-            train_ins = self.X_train + [self.y, self.loss_weights]\n+            train_ins = self.X_train + [self.y]\n             test_ins = self.X_test + [self.y]\n             predict_ins = self.X_test\n         else:\n-            train_ins = [self.X_train, self.y, self.loss_weights]\n+            train_ins = [self.X_train, self.y]\n             test_ins = [self.X_test, self.y]\n             predict_ins = [self.X_test]\n \n@@ -115,13 +112,11 @@ class Model(object):\n             allow_input_downcast=True, mode=theano_mode)\n \n \n-    def train(self, X, y, accuracy=False, sample_weight=None, class_weight=None):\n+    def train(self, X, y, accuracy=False):\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n-        w = calculate_loss_weights(y, sample_weight=sample_weight, class_weight=class_weight)\n-\n-        ins = X + [y, w]\n+        ins = X + [y]\n         if accuracy:\n             return self._train_with_acc(*ins)\n         else:\n@@ -139,12 +134,10 @@ class Model(object):\n \n \n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n-            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False,\n-            sample_weight=None, class_weight=None):\n+            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False):\n \n         X = standardize_X(X)\n         y = standardize_y(y)\n-        sample_weight = calculate_loss_weights(y, sample_weight=sample_weight, class_weight=class_weight)\n \n         do_validation = False\n         if validation_data:\n@@ -199,14 +192,12 @@ class Model(object):\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n \n-                w = sample_weight[batch_ids]\n-\n                 batch_logs = {}\n                 batch_logs['batch'] = batch_index\n                 batch_logs['size'] = len(batch_ids)\n                 callbacks.on_batch_begin(batch_index, batch_logs)\n \n-                ins = X_batch + [y_batch, w]\n+                ins = X_batch + [y_batch]\n                 if show_accuracy:\n                     loss, acc = self._train_with_acc(*ins)\n                     batch_logs['accuracy'] = acc\n\n@@ -6,51 +6,30 @@ from six.moves import range\n \n epsilon = 1.0e-9\n \n-def mean_squared_error(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        return T.sqr(weight.reshape((weight.shape[0], 1)) * (y_pred - y_true)).mean()\n-    else:\n+def mean_squared_error(y_true, y_pred):\n     return T.sqr(y_pred - y_true).mean()\n \n-def mean_absolute_error(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        return T.abs_(weight.reshape((weight.shape[0], 1)) * (y_pred - y_true)).mean()\n-    else:\n+def mean_absolute_error(y_true, y_pred):\n     return T.abs_(y_pred - y_true).mean()\n \n-def squared_hinge(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        weight = weight.reshape((weight.shape[0], 1))\n-        return T.sqr(weight * T.maximum(1. - (y_true * y_pred), 0.)).mean()\n-    else:\n+def squared_hinge(y_true, y_pred):\n     return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean()\n \n-def hinge(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        weight = weight.reshape((weight.shape[0], 1))\n-        return (weight * T.maximum(1. - (y_true * y_pred), 0.)).mean()\n-    else:\n+def hinge(y_true, y_pred):\n     return T.maximum(1. - y_true * y_pred, 0.).mean()\n \n-def categorical_crossentropy(y_true, y_pred, weight=None):\n+def categorical_crossentropy(y_true, y_pred):\n     '''Expects a binary class matrix instead of a vector of scalar classes\n     '''\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     # scale preds so that the class probas of each sample sum to 1\n     y_pred /= y_pred.sum(axis=1, keepdims=True) \n     cce = T.nnet.categorical_crossentropy(y_pred, y_true)\n-    if weight is not None:\n-        # return avg. of scaled cat. crossentropy\n-        return (weight * cce).mean()\n-    else:\n     return cce.mean()\n \n-def binary_crossentropy(y_true, y_pred, weight=None):\n+def binary_crossentropy(y_true, y_pred):\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     bce = T.nnet.binary_crossentropy(y_pred, y_true)\n-    if weight is not None:\n-        return (weight.reshape((weight.shape[0], 1)) * bce).mean()\n-    else:\n     return bce.mean()\n \n # aliases\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be22591e326028b5642b1b7d2cecb58137dcb921", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 138 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -39,12 +39,12 @@ for i in range(0, len(text) - maxlen, step):\n print('nb sequences:', len(sentences))\n \n print('Vectorization...')\n-X = np.zeros((len(sentences), maxlen, len(chars)))\n-y = np.zeros((len(sentences), len(chars)))\n+X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n+y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n for i, sentence in enumerate(sentences):\n     for t, char in enumerate(sentence):\n-        X[i, t, char_indices[char]] = 1.\n-    y[i, char_indices[next_chars[i]]] = 1.\n+        X[i, t, char_indices[char]] = 1\n+    y[i, char_indices[next_chars[i]]] = 1\n \n \n # build the model: 2 stacked LSTM\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9e76bdeca26f7f9ccdd15f066514cb1b7530b2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 64 | Contributors (this commit): 5 | Commits (past 90d): 14 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -2,9 +2,10 @@ from __future__ import absolute_import\n import numpy as np\n import time\n import sys\n+import six\n \n def get_from_module(identifier, module_params, module_name, instantiate=False):\n-    if isinstance(identifier, basestring):\n+    if isinstance(identifier, six.string_types):\n         res = module_params.get(identifier)\n         if not res:\n             raise Exception('Invalid ' + str(module_name) + ': ' + str(identifier))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce6728d9ceec93422c91f823dbefc5c541dc34ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 17 | Churn Cumulative: 948 | Contributors (this commit): 10 | Commits (past 90d): 44 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -356,7 +356,7 @@ class AutoEncoder(Layer):\n             self.set_weights(weights)\n \n     def connect(self, node):\n-        self.encoder.previous = node\n+        self.encoder.connect(node)\n \n     def get_weights(self):\n         weights = []\n\n@@ -12,7 +12,8 @@ from keras.utils import np_utils\n import numpy as np\n \n # Try different things here: 'lstm' or 'classical' or 'denoising'\n-autoencoder_type = 'denoising'\n+# or 'deep_denoising'\n+autoencoder_type = 'deep_denoising'\n \n nb_classes = 10\n batch_size = 128\n@@ -88,6 +89,15 @@ def build_denoising_autoencoder(autoencoder):\n \t\t\t\t\t\t\t\t\t\t output_reconstruction=False, tie_weights=True, corruption_level=0.3))\n \treturn autoencoder\n \n+def build_deep_denoising_autoencoder(autoencoder):\n+\tencoder = containers.Sequential([Dense(input_dim, hidden_dim, activation=activation), Dense(hidden_dim, hidden_dim/2, activation=activation)])\n+\tdecoder = containers.Sequential([Dense(hidden_dim/2, hidden_dim, activation=activation), Dense(hidden_dim, input_dim, activation=activation)])\n+\tautoencoder.add(Dense(input_dim, input_dim))\n+\tautoencoder.add(DenoisingAutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False, tie_weights=True))\n+\treturn autoencoder\n+\n+\n+\n # Build our autoencoder model\n autoencoder = Sequential()\n if autoencoder_type == 'lstm':\n@@ -96,6 +106,9 @@ if autoencoder_type == 'lstm':\n elif autoencoder_type == 'denoising':\n \tprint(\"Training Denoising AutoEncoder\")\n \tautoencoder = build_denoising_autoencoder(autoencoder)\n+elif autoencoder_type == 'deep_denoising':\n+    print (\"Training Deep Denoising AutoEncoder\")\n+    autoencoder = build_deep_denoising_autoencoder(autoencoder)\n elif autoencoder_type == 'classical':\n \tprint(\"Training Classical AutoEncoder\")\n \tautoencoder = build_deep_classical_autoencoder(autoencoder)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#271e2b7f844c53a1fa3f8448cc7f0d0852005baf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 999 | Contributors (this commit): 11 | Commits (past 90d): 52 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -11,9 +11,7 @@ def softmax(x):\n def time_distributed_softmax(x, mask_val=default_mask_val):\n     xshape = x.shape\n     X = x.reshape((xshape[0] * xshape[1], xshape[2]))\n-    mask = get_mask(X, mask_val)\n-    r =  mask * T.nnet.softmax(X) + (1 - mask) * mask_val\n-    return r.reshape(xshape)\n+    return T.nnet.softmax(X).reshape(xshape)\n \n def softplus(x):\n     return T.nnet.softplus(x)\n\n@@ -168,15 +168,17 @@ class Activation(Layer):\n     '''\n         Apply an activation function to an output.\n     '''\n-    def __init__(self, activation, target=0, beta=0.1):\n+    def __init__(self, activation, target=0, beta=0.1, mask_val=default_mask_val):\n         super(Activation,self).__init__()\n         self.activation = activations.get(activation)\n         self.target = target\n         self.beta = beta\n+        self.mask_val = shared_scalar(mask_val)\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        return self.activation(X)\n+        mask = get_mask(X, self.mask_val)\n+        return mask * self.activation(X) + (1 - mask) * self.mask_val\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n\n@@ -117,11 +117,12 @@ model2.add(Embedding(5, 4, zero_is_mask=True))\n model2.add(TimeDistributedDense(4,4))\n model2.add(Activation('time_distributed_softmax'))\n model2.add(LSTM(4,4, return_sequences=True))\n+model2.add(Activation('tanh'))\n model2.add(GRU(4,4, activation='softmax', return_sequences=True))\n model2.add(SimpleDeepRNN(4,4, depth=2, activation='relu', return_sequences=True)) \n model2.add(SimpleRNN(4,4, activation='relu', return_sequences=False))\n model2.compile(loss='categorical_crossentropy',\n-        optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n+        optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_COMPILE)\n print(\"Compiled model2\")\n \n X2 = np.random.random_integers(1, 4, size=(2,5))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86a9445ffd5a789b359e49057771332138cc2bfe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 22 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,7 @@ import types\n def softmax(x):\n     return T.nnet.softmax(x)\n \n-def time_distributed_softmax(x, mask_val=default_mask_val):\n+def time_distributed_softmax(x):\n     xshape = x.shape\n     X = x.reshape((xshape[0] * xshape[1], xshape[2]))\n     return T.nnet.softmax(X).reshape(xshape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
