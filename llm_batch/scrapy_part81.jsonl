{"custom_id": "scrapy#0643010f0cb27d7b904c4f790dfff5d588f9e003", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1002 | Contributors (this commit): 4 | Commits (past 90d): 52 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -65,6 +65,8 @@ class ExecutionEngine(object):\n                 self.close_domain, domain, reason='shutdown')\n         if self._mainloop_task.running:\n             self._mainloop_task.stop()\n+        if reactor.running:\n+            reactor.stop()\n \n     def kill(self):\n         \"\"\"Forces shutdown without waiting for pending transfers to finish.\n@@ -299,9 +301,8 @@ class ExecutionEngine(object):\n         stats.close_domain(domain, reason=reason)\n         log.msg(\"Domain closed (%s)\" % reason, domain=domain) \n         spiders.close_domain(domain)\n-        if self.running:\n         self._mainloop()\n-        elif not self.open_domains:\n+        if not self.open_domains:\n             send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n \n scrapyengine = ExecutionEngine()\n\n@@ -68,6 +68,7 @@ class CrawlingSession(object):\n             dispatcher.connect(self.request_received, signals.request_received)\n             dispatcher.connect(self.response_downloaded, signals.response_downloaded)\n \n+            scrapymanager.configure()\n             scrapymanager.runonce(self.domain)\n             self.port.stopListening()\n             self.wasrun = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#be76490002c3f478882ffd52ba8e88b8ce5ab551", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 975 | Contributors (this commit): 4 | Commits (past 90d): 47 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -65,8 +65,10 @@ class ExecutionEngine(object):\n                 self.close_domain, domain, reason='shutdown')\n         if self._mainloop_task.running:\n             self._mainloop_task.stop()\n-        if reactor.running:\n+        try:\n             reactor.stop()\n+        except RuntimeError: # raised if already stopped or in shutdown stage\n+            pass\n \n     def kill(self):\n         \"\"\"Forces shutdown without waiting for pending transfers to finish.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c33d6bbdbd9edba050b18f0e1e27db70bd83ec78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 2 | Churn Cumulative: 247 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -113,8 +113,6 @@ class ExecutionManager(object):\n         \"\"\"Stop the scrapy server, shutting down the execution engine\"\"\"\n         self.interrupted = True\n         scrapyengine.stop()\n-        if self.control_reactor and reactor.running:\n-            reactor.stop()\n \n     def _signal_shutdown(self, signum, _):\n         signame = signal_names[signum]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e7b2a6da5836a95fecebed5db0f009ed3eddf66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 50 | Churn Cumulative: 844 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -101,25 +101,29 @@ class XmlItemExporter(BaseItemExporter):\n \n class CsvItemExporter(BaseItemExporter):\n \n-    def __init__(self, file, include_headers_line=False, **kwargs):\n+    def __init__(self, file, include_headers_line=True, **kwargs):\n         self._configure(kwargs, dont_fail=True)\n         self.include_headers_line = include_headers_line\n         self.csv_writer = csv.writer(file, **kwargs)\n-\n-    def start_exporting(self):\n-        if self.include_headers_line:\n-            if not self.fields_to_export:\n-                raise RuntimeError(\"You must set fields_to_export in order\" + \\\n-                                   \" to use include_headers_line\")\n-            self.csv_writer.writerow(self.fields_to_export)\n+        self._header_written = False\n \n     def export_item(self, item):\n+        if not self._header_written:\n+            self._write_header(item)\n+\n         fields = self._get_serialized_fields(item, default_value='', \\\n             include_empty=True)\n \n         values = [x[1] for x in fields]\n         self.csv_writer.writerow(values)\n \n+    def _write_header(self, item):\n+        self._header_written = True\n+        if self.include_headers_line:\n+            if not self.fields_to_export:\n+                self.fields_to_export = item.fields.keys()\n+            self.csv_writer.writerow(self.fields_to_export)\n+\n \n class PickleItemExporter(BaseItemExporter):\n \n\n@@ -95,20 +95,38 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         return CsvItemExporter(self.output, **kwargs)\n \n     def _check_output(self):\n-        self.assertEqual(self.output.getvalue(), '22,John\\xc2\\xa3\\r\\n')\n+        self.assertEqual(self.output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n     def test_header(self):\n         output = StringIO()\n-        ie = CsvItemExporter(output, include_headers_line=True)\n-        self.assertRaises(RuntimeError, ie.start_exporting)\n-\n-        ie = CsvItemExporter(output, include_headers_line=True, \\\n-            fields_to_export=self.i.fields.keys())\n+        ie = CsvItemExporter(output, fields_to_export=self.i.fields.keys())\n         ie.start_exporting()\n         ie.export_item(self.i)\n         ie.finish_exporting()\n         self.assertEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n+        output = StringIO()\n+        ie = CsvItemExporter(output, fields_to_export=['age'])\n+        ie.start_exporting()\n+        ie.export_item(self.i)\n+        ie.finish_exporting()\n+        self.assertEqual(output.getvalue(), 'age\\r\\n22\\r\\n')\n+\n+        output = StringIO()\n+        ie = CsvItemExporter(output)\n+        ie.start_exporting()\n+        ie.export_item(self.i)\n+        ie.export_item(self.i)\n+        ie.finish_exporting()\n+        self.assertEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n')\n+\n+        output = StringIO()\n+        ie = CsvItemExporter(output, include_headers_line=False)\n+        ie.start_exporting()\n+        ie.export_item(self.i)\n+        ie.finish_exporting()\n+        self.assertEqual(output.getvalue(), '22,John\\xc2\\xa3\\r\\n')\n+\n \n class XmlItemExporterTest(BaseItemExporterTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d493615abe89e1e3c086fc69bf569c4c34d37ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 301 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -105,21 +105,21 @@ class CsvItemExporter(BaseItemExporter):\n         self._configure(kwargs, dont_fail=True)\n         self.include_headers_line = include_headers_line\n         self.csv_writer = csv.writer(file, **kwargs)\n-        self._header_written = False\n+        self._headers_written = False\n \n     def export_item(self, item):\n-        if not self._header_written:\n-            self._write_header(item)\n-\n+        self._write_headers_and_set_fields_to_export(item)\n         fields = self._get_serialized_fields(item, default_value='', \\\n             include_empty=True)\n-\n         values = [x[1] for x in fields]\n         self.csv_writer.writerow(values)\n \n-    def _write_header(self, item):\n-        self._header_written = True\n-        if self.include_headers_line:\n+    def _write_headers_and_set_fields_to_export(self, item):\n+        if self._headers_written:\n+            return\n+        self._headers_written = True\n+        if not self.include_headers_line:\n+            return\n         if not self.fields_to_export:\n             self.fields_to_export = item.fields.keys()\n         self.csv_writer.writerow(self.fields_to_export)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ff8ed8686b0f5888f4f90a0d1cf45c43ab2a201", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 91 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,9 +43,6 @@ class MediaPipeline(object):\n                     callbackArgs=(item, request, info),\n                     errback=self.item_media_failed,\n                     errbackArgs=(item, request, info),)\n-            dfd.addErrback(log.err, \\\n-                    'Unhandled ERROR in %s.item_media_{downloaded,failed} for %s' \\\n-                    % (type(self).__name__, request), domain=domain)\n             dlist.append(dfd)\n \n         return DeferredList(dlist).addCallback(self.item_completed, item, info)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b4304d5eb80e8506aa19dc412c0c5bade993eea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 312 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -105,21 +105,20 @@ class CsvItemExporter(BaseItemExporter):\n         self._configure(kwargs, dont_fail=True)\n         self.include_headers_line = include_headers_line\n         self.csv_writer = csv.writer(file, **kwargs)\n-        self._headers_written = False\n+        self._headers_not_written = True\n \n     def export_item(self, item):\n+        if self._headers_not_written:\n+            self._headers_not_written = False\n             self._write_headers_and_set_fields_to_export(item)\n+\n         fields = self._get_serialized_fields(item, default_value='', \\\n             include_empty=True)\n         values = [x[1] for x in fields]\n         self.csv_writer.writerow(values)\n \n     def _write_headers_and_set_fields_to_export(self, item):\n-        if self._headers_written:\n-            return\n-        self._headers_written = True\n-        if not self.include_headers_line:\n-            return\n+        if self.include_headers_line:\n             if not self.fields_to_export:\n                 self.fields_to_export = item.fields.keys()\n             self.csv_writer.writerow(self.fields_to_export)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ae11e92201a20c3ac94584fded1ee1bb66b2af0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 336 | Contributors (this commit): 3 | Commits (past 90d): 19 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ class SiteInfo(object):\n         return self.max_concurrent_requests - len(self.transferring)\n \n     def needs_backout(self):\n-        return len(self.queue) > 2 * self.max_concurrent_requests\n+        return len(self.active) > 2 * self.max_concurrent_requests\n \n \n class Downloader(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dad05957f3f40c30da78ef5db6f06a5b7310dfc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 337 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -42,6 +42,7 @@ class SiteInfo(object):\n         return self.max_concurrent_requests - len(self.transferring)\n \n     def needs_backout(self):\n+        # use self.active to include requests in the downloader middleware\n         return len(self.active) > 2 * self.max_concurrent_requests\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#86de5180fcb1be576dcb44ce4ca072e8a719fbdb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 79 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,11 +31,12 @@ class RobotsTxtMiddleware(object):\n \n     def process_request(self, request, spider):\n         useragent = self._useragents[spider]\n-        rp = self.robot_parser(request.url, spider)\n+        rp = self.robot_parser(request, spider)\n         if rp and not rp.can_fetch(useragent, request.url):\n             raise IgnoreRequest(\"URL forbidden by robots.txt: %s\" % request.url)\n \n-    def robot_parser(self, url, spider):\n+    def robot_parser(self, request, spider):\n+        url = urlparse_cached(request)\n         netloc = url.netloc\n         if netloc not in self._parsers:\n             self._parsers[netloc] = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#96bb223c135702e079ea3e475698d81b5dc5568b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 24 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 24 | Churn Cumulative: 54 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,24 +0,0 @@\n-from scrapy import log\n-from scrapy.core.exceptions import NotConfigured\n-\n-class DebugMiddleware(object):\n-\n-    def process_request(self, request, spider):\n-        log.msg(\"process_request %r\" % request, domain=spider.domain_name, level=log.DEBUG)\n-\n-    def process_exception(self, request, exception, spider):\n-        log.msg(\"process_exception %s in %r\" % (exception, request), domain=spider.domain_name, level=log.DEBUG)\n-\n-    def process_response(self, request, response, spider):\n-        log.msg(\"process_response %s from %r\" % (response, request), domain=spider.domain_name, level=log.DEBUG)\n-        return response\n-\n-# FIXME: backwards compatibility - will be removed before 0.7 release\n-\n-import warnings\n-\n-class CrawlDebug(object):\n-\n-    def __init__(self):\n-        warnings.warn(\"scrapy.contrib.downloadermiddleware.debug.CrawlDebug has been replaced by scrapy.contrib.downloadermiddleware.debug.DebugDownloaderMiddleware\")\n-        DebugMiddleware.__init__(self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d127d7fcfe11e831f1cb38818fc8f1d5444f568", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 8 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 69 | Churn Cumulative: 69 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,30 @@\n+from unittest import TestCase\n+\n+from scrapy.conf import settings\n+from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware\n+from scrapy.http import Response, Request\n+from scrapy.spider import BaseSpider\n+\n+\n+class TestDefaultHeadersMiddleware(TestCase):\n+\n+    def setUp(self):\n+        self.spider = BaseSpider()\n+        self.mw = DefaultHeadersMiddleware()\n+        self.default_headers = dict([(k, [v]) for k, v in \\\n+            settings.get('DEFAULT_REQUEST_HEADERS').iteritems()])\n+\n+    def test_process_request(self):\n+        req = Request('http://www.scrapytest.org')\n+        self.mw.process_request(req, self.spider)\n+        self.assertEquals(req.headers, self.default_headers)\n+\n+    def test_update_headers(self):\n+        headers = {'Accept-Language': ['es'], 'Test-Header': ['test']}\n+        req = Request('http://www.scrapytest.org', headers=headers)\n+        self.assertEquals(req.headers, headers)\n+\n+        self.mw.process_request(req, self.spider)\n+        self.default_headers.update(headers)\n+        self.assertEquals(req.headers, self.default_headers)\n+\n\n@@ -0,0 +1,39 @@\n+from unittest import TestCase\n+\n+from scrapy.conf import settings\n+from scrapy.contrib.downloadermiddleware.stats import DownloaderStats\n+from scrapy.http import Request, Response\n+from scrapy.spider import BaseSpider\n+from scrapy.stats import stats\n+\n+\n+class TestDownloaderStats(TestCase):\n+\n+    def setUp(self):\n+        self.spider = BaseSpider()\n+        self.spider.domain_name = 'scrapytest.org'\n+        self.mw = DownloaderStats()\n+\n+        stats.open_domain(self.spider.domain_name)\n+\n+        self.req = Request('scrapytest.org')\n+        self.res = Response('scrapytest.org', status=400)\n+\n+    def test_process_request(self):\n+        self.mw.process_request(self.req, self.spider)\n+        self.assertEqual(stats.get_value('downloader/request_count', \\\n+            domain=self.spider.domain_name), 1)\n+        \n+    def test_process_response(self):\n+        self.mw.process_response(self.req, self.res, self.spider)\n+        self.assertEqual(stats.get_value('downloader/response_count', \\\n+            domain=self.spider.domain_name), 1)\n+\n+    def test_process_exception(self):\n+        self.mw.process_exception(self.req, Exception(), self.spider)\n+        self.assertEqual(stats.get_value('downloader/exception_count', \\\n+            domain=self.spider.domain_name), 1)\n+\n+    def tearUp(self):\n+        stats.close_domain(self.spider.domain_name)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#043e7355f75f6b822dbcb34f42a036ec3ef43712", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 38 | Churn Cumulative: 38 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,38 @@\n+from unittest import TestCase\n+\n+from scrapy.http import Response, Request\n+from scrapy.spider import BaseSpider\n+from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware\n+\n+\n+class TestHttpErrorMiddleware(TestCase):\n+\n+    def setUp(self):\n+        self.spider = BaseSpider()\n+        self.mw = HttpErrorMiddleware()\n+        self.req = Request('http://scrapytest.org')\n+\n+        self.res200 = Response('http://scrapytest.org', status=200)\n+        self.res200.request = self.req\n+        self.res404 = Response('http://scrapytest.org', status=404)\n+        self.res404.request = self.req\n+\n+    def test_process_spider_input(self):\n+        self.assertEquals(self.mw.process_spider_input(self.res200, self.spider),\n+                          None)\n+\n+        self.assertEquals(self.mw.process_spider_input(self.res404, self.spider),\n+                          [])\n+\n+    def test_handle_httpstatus_list(self):\n+        res = self.res404.copy()\n+        res.request = Request('http://scrapytest.org',\n+                              meta={'handle_httpstatus_list': [404]})\n+\n+        self.assertEquals(self.mw.process_spider_input(res, self.spider),\n+                          None)\n+\n+        self.spider.handle_httpstatus_list = [404]\n+        self.assertEquals(self.mw.process_spider_input(self.res404, self.spider),\n+                          None)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#06311021538eee4516ca225608bae687e16acf66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 23 | Churn Cumulative: 53 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,23 +0,0 @@\n-import warnings\n-\n-from scrapy.core.exceptions import IgnoreRequest\n-from scrapy.utils.response import response_status_message\n-\n-class ErrorPagesMiddleware(object):\n-    \"\"\"This middleware filters out responses with status code others than 2XX\n-    or defined in spider handle_httpstatus_list attribute.\n-\n-    TODO: move this mw to spidermiddleware and remove me\n-    \"\"\"\n-\n-    def __init__(self):\n-        warnings.warn(\"scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware downloader middleware is deprecated - use scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware spider middleware instead\",\n-            DeprecationWarning, stacklevel=2)\n-\n-    def process_response(self, request, response, spider):\n-        status = response.status\n-        if 200 <= status < 300 or status in getattr(spider, 'handle_httpstatus_list', []):\n-            return response\n-        else:\n-            raise IgnoreRequest(response_status_message(status))\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#861a803cc3cbbd7bff4273ea86697fe59f167da9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 23 | Churn Cumulative: 259 | Contributors (this commit): 3 | Commits (past 90d): 37 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -154,7 +154,6 @@ SPIDER_MIDDLEWARES_BASE = {\n     'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n     'scrapy.contrib.itemsampler.ItemSamplerMiddleware': 100,\n     'scrapy.contrib.spidermiddleware.requestlimit.RequestLimitMiddleware': 200,\n-    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware': 300,\n     'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n     'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n     'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n\n@@ -1,22 +0,0 @@\n-\"\"\"\n-Restrict Spider Middleware\n-\n-See documentation in docs/topics/spider-middleware.rst\n-\"\"\"\n-\n-from itertools import ifilter\n-\n-from scrapy.http import Request\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-\n-class RestrictMiddleware(object):\n-    def __init__(self):\n-        self.allowed_urls = set(settings.getlist('RESTRICT_TO_URLS'))\n-        if not self.allowed_urls:\n-            raise NotConfigured\n-\n-    def process_spider_output(self, response, result, spider):\n-        return ifilter(lambda r: isinstance(r, Request) \\\n-            and r.url not in self.allowed_urls, result or ())\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7466150286cad0d056adeea61e251dcacc0e2093", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 25 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 26 | Churn Cumulative: 465 | Contributors (this commit): 3 | Commits (past 90d): 37 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -62,7 +62,7 @@ DOWNLOADER_MIDDLEWARES_BASE = {\n     'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n     'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 800,\n     'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n-    'scrapy.contrib.downloadermiddleware.cache.HttpCacheMiddleware': 900,\n+    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n     # Downloader side\n }\n \n\n@@ -1,14 +0,0 @@\n-# FIXME: code below is for backwards compatibility and should be removed before\n-# the 0.7 release\n-\n-import warnings\n-\n-from scrapy.contrib.downloadermiddleware.httpcache import HttpCacheMiddleware\n-\n-class CacheMiddleware(HttpCacheMiddleware):\n-\n-    def __init__(self, *args, **kwargs):\n-        warnings.warn(\"scrapy.contrib.downloadermiddleware.cache.CacheMiddleware was moved to scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware\",\n-            DeprecationWarning, stacklevel=2)\n-        HttpCacheMiddleware.__init__(self, *args, **kwargs)\n-\n\n@@ -1,10 +0,0 @@\n-\n-import warnings\n-\n-from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware\n-\n-class CommonMiddleware(DefaultHeadersMiddleware):\n-\n-    def __init__(self):\n-        warnings.warn(\"scrapy.contrib.downloadermiddleware.common.CommonMiddleware has been replaced by scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware\")\n-        DefaultHeadersMiddleware.__init__(self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#827aa19c6ee5140bd03b31485cc53a9ffc6b77fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 115 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ import cPickle as pickle\n from datetime import datetime\n \n from scrapy.stats.collector import StatsCollector\n-from scrapy.utils.db import mysql_connect\n+from scrapy.utils.mysql import mysql_connect\n from scrapy.conf import settings\n \n class MysqlStatsCollector(StatsCollector):\n\n@@ -1,6 +0,0 @@\n-import warnings\n-\n-from scrapy.utils.mysql import *\n-\n-warnings.warn(\"scrapy.utils.db module is depreacted, use scrapy.utils.mysql instead\",\n-    DeprecationWarning, stacklevel=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2bd1be9956cff99feec84a4ef8317d394191788", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 81 | Files Changed: 3 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/15 | Churn Δ: 169 | Churn Cumulative: 179 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.75\n\nDIFF:\n@@ -1,91 +1,27 @@\n+\"\"\"\n+A downloader middleware for signing AWS requests just before they get into the\n+downloader. It is important to sign as close to the downloader as possible\n+because Amazon Web Service use timestamps for authentication.\n+\"\"\"\n+\n import os\n-import re\n import time\n-import hmac\n-import base64\n-import hashlib\n-from urlparse import urlsplit\n \n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.aws import sign_request\n from scrapy.conf import settings\n \n-\n-METADATA_PREFIX = 'x-amz-meta-'\n-AMAZON_HEADER_PREFIX = 'x-amz-'\n-\n-\n-# generates the aws canonical string for the given parameters\n-def canonical_string(method, path, headers, expires=None):\n-    interesting_headers = {}\n-    for key in headers:\n-        lk = key.lower()\n-        if lk in set(['content-md5', 'content-type', 'date']) or lk.startswith(AMAZON_HEADER_PREFIX):\n-            interesting_headers[lk] = headers[key].strip()\n-\n-    # these keys get empty strings if they don't exist\n-    interesting_headers.setdefault('content-type', '')\n-    interesting_headers.setdefault('content-md5', '')\n-\n-    # just in case someone used this.  it's not necessary in this lib.\n-    if 'x-amz-date' in interesting_headers:\n-        interesting_headers['date'] = ''\n-\n-    # if you're using expires for query string auth, then it trumps date\n-    # (and x-amz-date)\n-    if expires:\n-        interesting_headers['date'] = str(expires)\n-\n-    sorted_header_keys = interesting_headers.keys()\n-    sorted_header_keys.sort()\n-\n-    buf = \"%s\\n\" % method\n-    for key in sorted_header_keys:\n-        if key.startswith(AMAZON_HEADER_PREFIX):\n-            buf += \"%s:%s\\n\" % (key, interesting_headers[key])\n-        else:\n-            buf += \"%s\\n\" % interesting_headers[key]\n-\n-    # don't include anything after the first ? in the resource...\n-    buf += \"%s\" % path.split('?')[0]\n-\n-    # ...unless there is an acl or torrent parameter\n-    if re.search(\"[&?]acl($|=|&)\", path):\n-        buf += \"?acl\"\n-    elif re.search(\"[&?]logging($|=|&)\", path):\n-        buf += \"?logging\"\n-    elif re.search(\"[&?]torrent($|=|&)\", path):\n-        buf += \"?torrent\"\n-    elif re.search(\"[&?]location($|=|&)\", path):\n-        buf += \"?location\"\n-\n-    return buf\n-\n-\n-\n-def sign_request(req, accesskey, secretkey):\n-    if 'Date' not in req.headers:\n-        req.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n-\n-    parsed = urlsplit(req.url)\n-    bucket = parsed.hostname.replace('.s3.amazonaws.com','')\n-    key = '%s?%s' % (parsed.path, parsed.query) if parsed.query else parsed.path\n-    fqkey = '/%s%s' % (bucket, key)\n-\n-    c_string = canonical_string(req.method, fqkey, req.headers)\n-    _hmac = hmac.new(secretkey, digestmod=hashlib.sha1)\n-    _hmac.update(c_string)\n-    b64_hmac = base64.encodestring(_hmac.digest()).strip()\n-    req.headers['Authorization'] = \"AWS %s:%s\" % (accesskey, b64_hmac)\n-\n-\n class AWSMiddleware(object):\n     def __init__(self):\n-        self.access_key = settings['AWS_ACCESS_KEY_ID'] or os.environ.get('AWS_ACCESS_KEY_ID')\n-        self.secret_key = settings['AWS_SECRET_ACCESS_KEY'] or os.environ.get('AWS_SECRET_ACCESS_KEY')\n+        self.access_key = settings['AWS_ACCESS_KEY_ID'] or \\\n+            os.environ.get('AWS_ACCESS_KEY_ID')\n+        self.secret_key = settings['AWS_SECRET_ACCESS_KEY'] or \\\n+            os.environ.get('AWS_SECRET_ACCESS_KEY')\n \n     def process_request(self, request, spider):\n         hostname = urlparse_cached(request).hostname\n         if spider.domain_name == 's3.amazonaws.com' \\\n                 or (hostname and hostname.endswith('s3.amazonaws.com')):\n-            request.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n+            request.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", \\\n+                time.gmtime())\n             sign_request(request, self.access_key, self.secret_key)\n\n@@ -1,8 +1,9 @@\n from unittest import TestCase, main\n-from scrapy.contrib import aws\n+\n+from scrapy.utils import aws\n from scrapy.http import Request\n \n-# keys are provided by amazon developer guide at\n+# just some random keys. keys are provided by amazon developer guide at\n # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n # and the tests described here are the examples from that manual\n \n@@ -103,7 +104,5 @@ class ScrapyAWSTest(TestCase):\n                 'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')\n \n \n-\n-\n if __name__ == '__main__':\n     main()\n\n@@ -0,0 +1,72 @@\n+\"\"\"Helper function for working with Amazon Web Services\"\"\"\n+\n+import re\n+import time\n+import hmac\n+import base64\n+import hashlib\n+from urlparse import urlsplit\n+\n+AMAZON_HEADER_PREFIX = 'x-amz-'\n+\n+# generates the aws canonical string for the given parameters\n+def canonical_string(method, path, headers, expires=None):\n+    interesting_headers = {}\n+    for key in headers:\n+        lk = key.lower()\n+        if lk in set(['content-md5', 'content-type', 'date']) \\\n+                or lk.startswith(AMAZON_HEADER_PREFIX):\n+            interesting_headers[lk] = headers[key].strip()\n+\n+    # these keys get empty strings if they don't exist\n+    interesting_headers.setdefault('content-type', '')\n+    interesting_headers.setdefault('content-md5', '')\n+\n+    # just in case someone used this.  it's not necessary in this lib.\n+    if 'x-amz-date' in interesting_headers:\n+        interesting_headers['date'] = ''\n+\n+    # if you're using expires for query string auth, then it trumps date\n+    # (and x-amz-date)\n+    if expires:\n+        interesting_headers['date'] = str(expires)\n+\n+    sorted_header_keys = interesting_headers.keys()\n+    sorted_header_keys.sort()\n+\n+    buf = \"%s\\n\" % method\n+    for key in sorted_header_keys:\n+        if key.startswith(AMAZON_HEADER_PREFIX):\n+            buf += \"%s:%s\\n\" % (key, interesting_headers[key])\n+        else:\n+            buf += \"%s\\n\" % interesting_headers[key]\n+\n+    # don't include anything after the first ? in the resource...\n+    buf += \"%s\" % path.split('?')[0]\n+\n+    # ...unless there is an acl or torrent parameter\n+    if re.search(\"[&?]acl($|=|&)\", path):\n+        buf += \"?acl\"\n+    elif re.search(\"[&?]logging($|=|&)\", path):\n+        buf += \"?logging\"\n+    elif re.search(\"[&?]torrent($|=|&)\", path):\n+        buf += \"?torrent\"\n+    elif re.search(\"[&?]location($|=|&)\", path):\n+        buf += \"?location\"\n+\n+    return buf\n+\n+def sign_request(req, accesskey, secretkey):\n+    if 'Date' not in req.headers:\n+        req.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n+\n+    parsed = urlsplit(req.url)\n+    bucket = parsed.hostname.replace('.s3.amazonaws.com','')\n+    key = '%s?%s' % (parsed.path, parsed.query) if parsed.query else parsed.path\n+    fqkey = '/%s%s' % (bucket, key)\n+\n+    c_string = canonical_string(req.method, fqkey, req.headers)\n+    _hmac = hmac.new(secretkey, digestmod=hashlib.sha1)\n+    _hmac.update(c_string)\n+    b64_hmac = base64.encodestring(_hmac.digest()).strip()\n+    req.headers['Authorization'] = \"AWS %s:%s\" % (accesskey, b64_hmac)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#82ca5e26f5e51f01816c9d8ea3d1cd4cb65c2648", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f0b5892aa5acb5bd8451f0b73a0ac06502c18222", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 43 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -34,6 +34,6 @@ class TestDownloaderStats(TestCase):\n         self.assertEqual(stats.get_value('downloader/exception_count', \\\n             domain=self.spider.domain_name), 1)\n \n-    def tearUp(self):\n-        stats.close_domain(self.spider.domain_name)\n+    def tearDown(self):\n+        stats.close_domain(self.spider.domain_name, '')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#083635ebafe19403cae7945310b13e8acb0b9f4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 53 | Churn Cumulative: 53 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,53 @@\n+from unittest import TestCase\n+\n+from scrapy.contrib.spidermiddleware.depth import DepthMiddleware \n+from scrapy.conf import settings\n+from scrapy.http import Response, Request\n+from scrapy.spider import BaseSpider\n+from scrapy.stats import stats\n+\n+\n+class TestDepthMiddleware(TestCase):\n+\n+    def setUp(self):\n+        settings.disabled = False\n+        settings.overrides['DEPTH_LIMIT'] = 1\n+        settings.overrides['DEPTH_STATS'] = True\n+\n+        self.spider = BaseSpider()\n+        self.spider.domain_name = 'scrapytest.org'\n+\n+        stats.open_domain(self.spider.domain_name)\n+\n+        self.mw = DepthMiddleware()\n+        self.assertEquals(stats.get_value('envinfo/request_depth_limit'), 1)\n+\n+    def test_process_spider_output(self):\n+        req = Request('http://scrapytest.org')\n+        resp = Response('http://scrapytest.org')\n+        resp.request = req\n+        result = [Request('http://scrapytest.org')]\n+\n+        out = list(self.mw.process_spider_output(resp, result, self.spider))\n+        self.assertEquals(out, result)\n+\n+        rdc = stats.get_value('request_depth_count/1',\n+                              domain=self.spider.domain_name)\n+        self.assertEquals(rdc, 1)\n+\n+        req.meta['depth'] = 1\n+\n+        out2 = list(self.mw.process_spider_output(resp, result, self.spider))\n+        self.assertEquals(out2, [])\n+\n+        rdm = stats.get_value('request_depth_max',\n+                              domain=self.spider.domain_name)\n+        self.assertEquals(rdm, 1)\n+ \n+    def tearDown(self):\n+        del settings.overrides['DEPTH_LIMIT']\n+        del settings.overrides['DEPTH_STATS']\n+        settings.disabled = True\n+\n+        stats.close_domain(self.spider.domain_name, '')\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c700749aef200d8a0d06a81a2870064557d6b8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 32 | Churn Cumulative: 32 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,32 @@\n+from unittest import TestCase\n+\n+from scrapy.http import Response, Request\n+from scrapy.spider import BaseSpider\n+from scrapy.contrib.spidermiddleware.offsite import OffsiteMiddleware\n+\n+\n+class TestOffsiteMiddleware(TestCase):\n+\n+    def setUp(self):\n+        self.spider = BaseSpider()\n+        self.spider.domain_name = 'scrapytest.org'\n+        self.spider.extra_domain_names = ['scrapy.org']\n+\n+        self.mw = OffsiteMiddleware()\n+        self.mw.domain_opened(self.spider)\n+\n+    def test_process_spider_output(self):\n+        res = Response('http://scrapytest.org')\n+\n+        onsite_reqs = [Request('http://scrapytest.org/1'),\n+                       Request('http://scrapy.org/1'),\n+                       Request('http://sub.scrapy.org/1')]\n+        offsite_reqs = [Request('http://scrapy2.org')]\n+        reqs = onsite_reqs + offsite_reqs\n+\n+        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n+        self.assertEquals(out, onsite_reqs)\n+\n+    def tearDown(self):\n+        self.mw.domain_closed(self.spider)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#43bd00dea2226ab7fd1ba36476bfea462111bfc3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 21 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,21 @@\n+from unittest import TestCase\n+\n+from scrapy.http import Response, Request\n+from scrapy.spider import BaseSpider\n+from scrapy.contrib.spidermiddleware.referer import RefererMiddleware\n+\n+\n+class TestRefererMiddleware(TestCase):\n+\n+    def setUp(self):\n+        self.spider = BaseSpider()\n+        self.mw = RefererMiddleware()\n+\n+    def test_process_spider_output(self):\n+        res = Response('http://scrapytest.org')\n+        reqs = [Request('http://scrapytest.org/')]\n+\n+        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n+        self.assertEquals(out[0].headers.get('Referer'),\n+                          'http://scrapytest.org')\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c4ad2bea5d0a601d1fcee09589630a79cfda661d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 22 | Churn Cumulative: 22 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,22 @@\n+from unittest import TestCase\n+\n+from scrapy.contrib.spidermiddleware.urlfilter import UrlFilterMiddleware\n+from scrapy.http import Response, Request\n+from scrapy.spider import BaseSpider\n+from scrapy.utils.url import canonicalize_url\n+\n+\n+class TestUrlFilterMiddleware(TestCase):\n+\n+    def setUp(self):\n+        self.spider = BaseSpider()\n+        self.mw = UrlFilterMiddleware()\n+\n+    def test_process_spider_output(self):\n+        res = Response('http://scrapytest.org')\n+        req_url = 'http://scrapytest.org/?last=1&first=2'\n+        reqs = [Request(req_url)]\n+\n+        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n+        self.assertEquals(out[0].url, canonicalize_url(req_url))\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#30c2ad3f0c7d9d32bb131d832d240d8b78814bd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 31 | Churn Cumulative: 31 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,31 @@\n+from unittest import TestCase\n+\n+from scrapy.conf import settings\n+from scrapy.contrib.spidermiddleware.urllength import UrlLengthMiddleware\n+from scrapy.http import Response, Request\n+from scrapy.spider import BaseSpider\n+\n+\n+class TestUrlLengthMiddleware(TestCase):\n+\n+    def setUp(self):\n+        settings.disabled = False\n+        settings.overrides['URLLENGTH_LIMIT'] = 25\n+\n+        self.spider = BaseSpider()\n+        self.mw = UrlLengthMiddleware()\n+\n+    def test_process_spider_output(self):\n+        res = Response('http://scrapytest.org')\n+\n+        short_url_req = Request('http://scrapytest.org/')\n+        long_url_req = Request('http://scrapytest.org/this_is_a_long_url')\n+        reqs = [short_url_req, long_url_req] \n+\n+        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n+        self.assertEquals(out, [short_url_req])\n+\n+    def tearDown(self):\n+        del settings.overrides['URLLENGTH_LIMIT']\n+        settings.disabled = True\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7a88c0d8e584c130ce8701f3cae535499746d59a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 185 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -120,6 +120,7 @@ class Shell(object):\n         self._run_console()\n \n     def _run_console(self):\n+        try:\n             try: # use IPython if available\n                 import IPython\n                 shell = IPython.Shell.IPShell(argv=[], user_ns=self.vars)\n@@ -135,6 +136,8 @@ class Shell(object):\n                     import rlcompleter\n                     readline.parse_and_bind(\"tab:complete\")\n                 code.interact(local=self.vars)\n+        except SystemExit: # raised when using exit() in python code.interact\n+            pass\n \n     def _console_thread(self, url=None):\n         self.populate_vars()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2974c2c4b56f73aac40232d49d38aa92e17c0239", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 30 | Churn Cumulative: 380 | Contributors (this commit): 2 | Commits (past 90d): 24 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,9 +49,14 @@ class Request(object_ref):\n         return self._url\n \n     def _set_url(self, url):\n-        if isinstance(url, basestring):\n-            decoded_url = url if isinstance(url, unicode) else url.decode(self.encoding)\n-            self._url = safe_url_string(decoded_url, self.encoding)\n+        if isinstance(url, str):\n+            self._url = safe_url_string(url)\n+        elif isinstance(url, unicode):\n+            if self.encoding is None:\n+                raise TypeError('Cannot convert unicode url - %s has no encoding' %\n+                    type(self).__name__)\n+            unicode_url = url if isinstance(url, unicode) else url.decode(self.encoding)\n+            self._url = safe_url_string(unicode_url, self.encoding)\n         else:\n             raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n \n@@ -64,6 +69,9 @@ class Request(object_ref):\n         if isinstance(body, str):\n             self._body = body\n         elif isinstance(body, unicode):\n+            if self.encoding is None:\n+                raise TypeError('Cannot convert unicode body - %s has no encoding' %\n+                    type(self).__name__)\n             self._body = body.encode(self.encoding)\n         elif body is None:\n             self._body = ''\n\n@@ -29,6 +29,9 @@ class TextResponse(Response):\n \n     def _set_url(self, url):\n         if isinstance(url, unicode):\n+            if self.encoding is None:\n+                raise TypeError('Cannot convert unicode url - %s has no encoding' %\n+                    type(self).__name__)\n             self._url = url.encode(self.encoding)\n         else:\n             super(TextResponse, self)._set_url(url)\n@@ -36,10 +39,11 @@ class TextResponse(Response):\n     url = property(_get_url, _set_url)\n \n     def _set_body(self, body):\n+        self._body = ''\n         if isinstance(body, unicode):\n-            if self._encoding is None:\n-                raise TypeError(\"To instantiate a %s with unicode body you \" \\\n-                    \"must specify the encoding\" % self.__class__.__name__)\n+            if self.encoding is None:\n+                raise TypeError('Cannot convert unicode body - %s has no encoding' %\n+                    type(self).__name__)\n             self._body = body.encode(self._encoding)\n         else:\n             super(TextResponse, self)._set_body(body)\n\n@@ -138,6 +138,12 @@ class TextResponseTest(BaseResponseTest):\n         self.assertEqual(r3.encoding, \"latin1\")\n \n     def test_unicode_url(self):\n+        # instantiate with unicode url without encoding\n+        self.assertRaises(TypeError, self.response_class, u\"http://www.example.com/\")\n+        # make sure urls are converted to str\n+        resp = self.response_class(url=u\"http://www.example.com/\", encoding='utf-8')\n+        assert isinstance(resp.url, str)\n+\n         resp = self.response_class(url=u\"http://www.example.com/price/\\xa3\", encoding='utf-8')\n         self.assertEqual(resp.url, 'http://www.example.com/price/\\xc2\\xa3')\n         resp = self.response_class(url=u\"http://www.example.com/price/\\xa3\", encoding='latin-1')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
