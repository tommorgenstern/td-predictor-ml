{"custom_id": "keras#710d8e4dd323cbb5dbadf988d153f1c799cae125", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 2315 | Contributors (this commit): 38 | Commits (past 90d): 19 | Contributors (cumulative): 38 | DMM Complexity: 0.0\n\nDIFF:\n@@ -299,7 +299,7 @@ class ModelCheckpoint(Callback):\n             self.monitor_op = np.greater\n             self.best = -np.Inf\n         else:\n-            if self.monitor.startswith(('acc', 'fmeasure')):\n+            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n                 self.monitor_op = np.greater\n                 self.best = -np.Inf\n             else:\n@@ -385,7 +385,7 @@ class EarlyStopping(Callback):\n         elif mode == 'max':\n             self.monitor_op = np.greater\n         else:\n-            if self.monitor.startswith(('acc', 'fmeasure')):\n+            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n                 self.monitor_op = np.greater\n             else:\n                 self.monitor_op = np.less\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c3c634f4b1f76dc6ab64904deaa871650ceb0de5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 4902 | Contributors (this commit): 27 | Commits (past 90d): 28 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -662,7 +662,7 @@ class Layer(object):\n         if not hasattr(self, 'supports_masking') or not self.supports_masking:\n             if input_mask is not None:\n                 if isinstance(input_mask, list):\n-                    if any(input_mask):\n+                    if any(mask is not None for mask in input_mask):\n                         raise ValueError('Layer ' + self.name +\n                                          ' does not support masking, '\n                                          'but was passed an input_mask: ' +\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a50f5dfc80030b864ef24d6c562661c24fe7c2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 81 | Files Changed: 4 | Hunks: 33 | Methods Changed: 28 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 135 | Churn Cumulative: 5071 | Contributors (this commit): 69 | Commits (past 90d): 43 | Contributors (cumulative): 77 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,6 +16,14 @@ from .utils.generic_utils import Progbar\n from keras import backend as K\n from pkg_resources import parse_version\n \n+try:\n+    import requests\n+except ImportError:\n+    requests = None\n+\n+if K.backend() == 'tensorflow':\n+    import tensorflow as tf\n+\n \n class CallbackList(object):\n     \"\"\"Container abstracting a list of callbacks.\n@@ -43,6 +51,12 @@ class CallbackList(object):\n             callback.set_model(model)\n \n     def on_epoch_begin(self, epoch, logs=None):\n+        \"\"\"Called at the start of an epoch.\n+\n+        # Arguments\n+            epoch: integer, index of epoch.\n+            logs: dictionary of logs.\n+        \"\"\"\n         logs = logs or {}\n         for callback in self.callbacks:\n             callback.on_epoch_begin(epoch, logs)\n@@ -51,11 +65,23 @@ class CallbackList(object):\n         self._delta_ts_batch_end = deque([], maxlen=self.queue_length)\n \n     def on_epoch_end(self, epoch, logs=None):\n+        \"\"\"Called at the end of an epoch.\n+\n+        # Arguments\n+            epoch: integer, index of epoch.\n+            logs: dictionary of logs.\n+        \"\"\"\n         logs = logs or {}\n         for callback in self.callbacks:\n             callback.on_epoch_end(epoch, logs)\n \n     def on_batch_begin(self, batch, logs=None):\n+        \"\"\"Called right before processing a batch.\n+\n+        # Arguments\n+            batch: integer, index of batch within the current epoch.\n+            logs: dictionary of logs.\n+        \"\"\"\n         logs = logs or {}\n         t_before_callbacks = time.time()\n         for callback in self.callbacks:\n@@ -71,6 +97,12 @@ class CallbackList(object):\n         self._t_enter_batch = time.time()\n \n     def on_batch_end(self, batch, logs=None):\n+        \"\"\"Called at the end of a batch.\n+\n+        # Arguments\n+            batch: integer, index of batch within the current epoch.\n+            logs: dictionary of logs.\n+        \"\"\"\n         logs = logs or {}\n         if not hasattr(self, '_t_enter_batch'):\n             self._t_enter_batch = time.time()\n@@ -87,11 +119,21 @@ class CallbackList(object):\n                           % delta_t_median)\n \n     def on_train_begin(self, logs=None):\n+        \"\"\"Called at the beginning of training.\n+\n+        # Arguments\n+            logs: dictionary of logs.\n+        \"\"\"\n         logs = logs or {}\n         for callback in self.callbacks:\n             callback.on_train_begin(logs)\n \n     def on_train_end(self, logs=None):\n+        \"\"\"Called at the end of training.\n+\n+        # Arguments\n+            logs: dictionary of logs.\n+        \"\"\"\n         logs = logs or {}\n         for callback in self.callbacks:\n             callback.on_train_end(logs)\n@@ -452,7 +494,9 @@ class RemoteMonitor(Callback):\n         self.headers = headers\n \n     def on_epoch_end(self, epoch, logs=None):\n-        import requests\n+        if requests is None:\n+            raise ImportError('RemoteMonitor requires '\n+                              'the `requests` library.')\n         logs = logs or {}\n         send = {}\n         send['epoch'] = epoch\n@@ -534,11 +578,8 @@ class TensorBoard(Callback):\n         self.write_images = write_images\n \n     def set_model(self, model):\n-        import tensorflow as tf\n-        import keras.backend.tensorflow_backend as KTF\n-\n         self.model = model\n-        self.sess = KTF.get_session()\n+        self.sess = K.get_session()\n         if self.histogram_freq and self.merged is None:\n             for layer in self.model.layers:\n \n@@ -595,7 +636,6 @@ class TensorBoard(Callback):\n                 self.writer = tf.train.SummaryWriter(self.log_dir)\n \n     def on_epoch_end(self, epoch, logs=None):\n-        import tensorflow as tf\n         logs = logs or {}\n \n         if self.model.validation_data and self.histogram_freq:\n@@ -683,9 +723,11 @@ class ReduceLROnPlateau(Callback):\n         self.best = 0\n         self.mode = mode\n         self.monitor_op = None\n-        self.reset()\n+        self._reset()\n \n-    def reset(self):\n+    def _reset(self):\n+        \"\"\"Resets wait counter and cooldown counter.\n+        \"\"\"\n         if self.mode not in ['auto', 'min', 'max']:\n             warnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '\n                           'fallback to auto mode.' % (self.mode),\n@@ -703,7 +745,7 @@ class ReduceLROnPlateau(Callback):\n         self.lr_epsilon = self.min_lr * 1e-4\n \n     def on_train_begin(self, logs=None):\n-        self.reset()\n+        self._reset()\n \n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}\n\n@@ -3,67 +3,32 @@ from .utils.generic_utils import get_from_module\n \n \n def binary_accuracy(y_true, y_pred):\n-    \"\"\"Binary accuracy metric.\n-\n-    Computes the mean accuracy rate across all predictions for binary\n-    classification problems.\n-    \"\"\"\n     return K.mean(K.equal(y_true, K.round(y_pred)))\n \n \n def categorical_accuracy(y_true, y_pred):\n-    \"\"\"Categorical accuracy metric.\n-\n-    Computes the mean accuracy rate across all predictions for\n-    multiclass classification problems.\n-    \"\"\"\n     return K.mean(K.equal(K.argmax(y_true, axis=-1),\n                           K.argmax(y_pred, axis=-1)))\n \n \n def sparse_categorical_accuracy(y_true, y_pred):\n-    \"\"\"Sparse version of the categorical accuracy metric.\n-\n-    Same as categorical_accuracy, but useful when the predictions are for\n-    sparse targets.\n-    \"\"\"\n     return K.mean(K.equal(K.max(y_true, axis=-1),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())))\n \n \n def top_k_categorical_accuracy(y_true, y_pred, k=5):\n-    \"\"\"Categorical accuracy metric for top-k accuracy.\n-\n-    Computes the top-k categorical accuracy rate, i.e. success when the\n-    target class is within the top-k predictions provided.\n-    \"\"\"\n     return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k))\n \n \n def mean_squared_error(y_true, y_pred):\n-    \"\"\"Mean squared error metric.\n-\n-    Computes the mean squared error (mse) rate\n-    between predicted and target values.\n-    \"\"\"\n     return K.mean(K.square(y_pred - y_true))\n \n \n def mean_absolute_error(y_true, y_pred):\n-    \"\"\"Mean absolute error metric.\n-\n-    Computes the mean absolute error (mae) rate\n-    between predicted and target values.\n-    \"\"\"\n     return K.mean(K.abs(y_pred - y_true))\n \n \n def mean_absolute_percentage_error(y_true, y_pred):\n-    \"\"\"Mean absolute percentage error metric.\n-\n-    Computes the mean absolute percentage error (mape) rate\n-    between predicted and target values.\n-    \"\"\"\n     diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n                                             K.epsilon(),\n                                             None))\n@@ -71,11 +36,6 @@ def mean_absolute_percentage_error(y_true, y_pred):\n \n \n def mean_squared_logarithmic_error(y_true, y_pred):\n-    \"\"\"Mean squared logarithmic error metric.\n-\n-    Computes the mean squared logarithmic error (msle) rate\n-    between predicted and target values.\n-    \"\"\"\n     first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n     second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n     return K.mean(K.square(first_log - second_log))\n@@ -97,50 +57,28 @@ def squared_hinge(y_true, y_pred):\n \n \n def categorical_crossentropy(y_true, y_pred):\n-    \"\"\"Categorical cross-entropy metric.\n-\n-    Computes the cross-entropy value for multiclass classification\n-    problems. Note: Expects a binary class matrix instead of a vector\n-    of scalar classes.\n-    \"\"\"\n     return K.mean(K.categorical_crossentropy(y_pred, y_true))\n \n \n def sparse_categorical_crossentropy(y_true, y_pred):\n-    \"\"\"Sparse version of the categorical cross-entropy metric.\n-\n-    Computes the cross-entropy value for multiclass classification\n-    problems with sparse targets. Note: Expects an array of integer\n-    classes. Labels shape must have the same number of dimensions as\n-    output shape. If you get a shape error, add a length-1 dimension\n-    to labels.\n-    \"\"\"\n     return K.mean(K.sparse_categorical_crossentropy(y_pred, y_true))\n \n \n def binary_crossentropy(y_true, y_pred):\n-    \"\"\"Computes the cross-entropy value for binary classification problems.\n-    \"\"\"\n     return K.mean(K.binary_crossentropy(y_pred, y_true))\n \n \n def kullback_leibler_divergence(y_true, y_pred):\n-    \"\"\"Computes the KLdivergence between prediction and target values.\n-    \"\"\"\n     y_true = K.clip(y_true, K.epsilon(), 1)\n     y_pred = K.clip(y_pred, K.epsilon(), 1)\n     return K.mean(K.sum(y_true * K.log(y_true / y_pred), axis=-1))\n \n \n def poisson(y_true, y_pred):\n-    \"\"\"Computes the poisson function over prediction and target values.\n-    \"\"\"\n     return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()))\n \n \n def cosine_proximity(y_true, y_pred):\n-    \"\"\"Computes the cosine similarity between the prediction and target values.\n-    \"\"\"\n     y_true = K.l2_normalize(y_true, axis=-1)\n     y_pred = K.l2_normalize(y_pred, axis=-1)\n     return -K.mean(y_true * y_pred)\n\n@@ -34,17 +34,10 @@ def hinge(y_true, y_pred):\n \n \n def categorical_crossentropy(y_true, y_pred):\n-    \"\"\"Expects a binary class matrix instead of a vector of scalar classes.\n-    \"\"\"\n     return K.categorical_crossentropy(y_pred, y_true)\n \n \n def sparse_categorical_crossentropy(y_true, y_pred):\n-    \"\"\"Expects an array of integer classes.\n-\n-    Note: labels shape must have the same number of dimensions as output shape.\n-    If you get a shape error, add a length-1 dimension to labels.\n-    \"\"\"\n     return K.sparse_categorical_crossentropy(y_pred, y_true)\n \n \n\n@@ -305,7 +305,7 @@ def load_img(path, grayscale=False, target_size=None):\n \n def list_pictures(directory, ext='jpg|jpeg|bmp|png'):\n     return [os.path.join(root, f)\n-            for root, dirs, files in os.walk(directory) for f in files\n+            for root, _, files in os.walk(directory) for f in files\n             if re.match('([\\w]+\\.(?:' + ext + '))', f)]\n \n \n@@ -613,7 +613,7 @@ class ImageDataGenerator(object):\n         if self.zca_whitening:\n             flat_x = np.reshape(x, (x.shape[0], x.shape[1] * x.shape[2] * x.shape[3]))\n             sigma = np.dot(flat_x.T, flat_x) / flat_x.shape[0]\n-            u, s, v = linalg.svd(sigma)\n+            u, s, _ = linalg.svd(sigma)\n             self.principal_components = np.dot(np.dot(u, np.diag(1. / np.sqrt(s + 10e-7))), u.T)\n \n \n@@ -785,7 +785,7 @@ class DirectoryIterator(Iterator):\n \n         for subdir in classes:\n             subpath = os.path.join(directory, subdir)\n-            for root, dirs, files in _recursive_list(subpath):\n+            for root, _, files in _recursive_list(subpath):\n                 for fname in files:\n                     is_valid = False\n                     for extension in white_list_formats:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#53e541f7bf55de036f4f5641bd2947b96dd8c4c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 33 | Files Changed: 5 | Hunks: 21 | Methods Changed: 11 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 117 | Churn Cumulative: 3378 | Contributors (this commit): 30 | Commits (past 90d): 31 | Contributors (cumulative): 38 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,11 @@\n import numpy as np\n from .. import backend as K\n \n+try:\n+    import librosa\n+except ImportError:\n+    librosa = None\n+\n \n TAGS = ['rock', 'pop', 'alternative', 'indie', 'electronic',\n         'female vocalists', 'dance', '00s', 'alternative rock', 'jazz',\n@@ -15,51 +20,47 @@ TAGS = ['rock', 'pop', 'alternative', 'indie', 'electronic',\n         'sad', 'House', 'happy']\n \n \n-def librosa_exists():\n-    try:\n-        __import__('librosa')\n-    except ImportError:\n-        return False\n-    else:\n-        return True\n-\n-\n def preprocess_input(audio_path, dim_ordering='default'):\n     \"\"\"Reads an audio file and outputs a Mel-spectrogram.\n+\n+    # Arguments\n+        audio_path: path to the target audio file.\n+        dim_ordering: data format for the output spectrogram image.\n+\n+    # Returns\n+        3D Numpy tensor encoding the Mel-spectrogram.\n     \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = K.image_dim_ordering()\n     assert dim_ordering in {'tf', 'th'}\n \n-    if librosa_exists():\n-        import librosa\n-    else:\n-        raise RuntimeError('Librosa is required to process audio files.\\n' +\n+    if librosa is None:\n+        raise ImportError('Librosa is required to process audio files.\\n' +\n                           'Install it via `pip install librosa` \\nor visit ' +\n                           'http://librosa.github.io/librosa/ for details.')\n \n     # mel-spectrogram parameters\n-    SR = 12000\n-    N_FFT = 512\n-    N_MELS = 96\n-    HOP_LEN = 256\n-    DURA = 29.12\n+    sr = 12000\n+    n_fft = 512\n+    n_mels = 96\n+    hop_length = 256\n+    duration = 29.12\n \n-    src, sr = librosa.load(audio_path, sr=SR)\n+    src, sr = librosa.load(audio_path, sr=sr)\n     n_sample = src.shape[0]\n-    n_sample_wanted = int(DURA * SR)\n+    n_sample_wanted = int(duration * sr)\n \n     # trim the signal at the center\n     if n_sample < n_sample_wanted:  # if too short\n-        src = np.hstack((src, np.zeros((int(DURA * SR) - n_sample,))))\n+        src = np.hstack((src, np.zeros((int(duration * sr) - n_sample,))))\n     elif n_sample > n_sample_wanted:  # if too long\n         src = src[(n_sample - n_sample_wanted) / 2:\n                   (n_sample + n_sample_wanted) / 2]\n \n     logam = librosa.logamplitude\n     melgram = librosa.feature.melspectrogram\n-    x = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n-                      n_fft=N_FFT, n_mels=N_MELS) ** 2,\n+    x = logam(melgram(y=src, sr=sr, hop_lengthgth=hop_length,\n+                      n_fft=n_fft, n_mels=n_mels) ** 2,\n               ref_power=1.0)\n \n     if dim_ordering == 'th':\n@@ -74,8 +75,10 @@ def decode_predictions(preds, top_n=5):\n \n     # Arguments\n         preds: 2-dimensional numpy array\n-        top_n: integer in [0, 50], number of items to show\n+        top_n: integer in [0, 50], number of items to show.\n \n+    # Returns\n+        Decoded output.\n     \"\"\"\n     assert len(preds.shape) == 2 and preds.shape[1] == 50\n     results = []\n\n@@ -9,6 +9,15 @@ CLASS_INDEX_PATH = 'https://s3.amazonaws.com/deep-learning-models/image-models/i\n \n \n def preprocess_input(x, dim_ordering='default'):\n+    \"\"\"Preprocesses a tensor encoding a batch of images.\n+\n+    # Arguments\n+        x: input Numpy tensor, 4D.\n+        dim_ordering: data format of the image tensor.\n+\n+    # Returns\n+        Preprocessed tensor.\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = K.image_dim_ordering()\n     assert dim_ordering in {'tf', 'th'}\n@@ -31,6 +40,21 @@ def preprocess_input(x, dim_ordering='default'):\n \n \n def decode_predictions(preds, top=5):\n+    \"\"\"Decodes the prediction of an ImageNet model.\n+\n+    # Arguments\n+        preds: Numpy tensor encoding a batch of predictions.\n+        top: integer, how many top-guesses to return.\n+\n+    # Returns\n+        A list of lists of top class prediction tuples\n+        `(class_name, class_description, score)`.\n+        One list of tuples per sample in batch input.\n+\n+    # Raises\n+        ValueError: in case of invalid shape of the `pred` array\n+            (must be 2D).\n+    \"\"\"\n     global CLASS_INDEX\n     if len(preds.shape) != 2 or preds.shape[1] != 1000:\n         raise ValueError('`decode_predictions` expects '\n@@ -51,7 +75,28 @@ def decode_predictions(preds, top=5):\n     return results\n \n \n-def _obtain_input_shape(input_shape, default_size, min_size, dim_ordering, include_top):\n+def _obtain_input_shape(input_shape,\n+                        default_size,\n+                        min_size,\n+                        dim_ordering,\n+                        include_top):\n+    \"\"\"Internal utility to compute/validate an ImageNet model's input shape.\n+\n+    # Arguments\n+        input_shape: either None (will return the default network input shape),\n+            or a user-provided shape to be validated.\n+        default_size: default input width/height for the model.\n+        min_size: minimum input width/height accepted by the model.\n+        dim_ordering: image data format to use.\n+        include_top: whether the model is expected to\n+            be linked to a classifier via a Flatten layer.\n+\n+    # Returns\n+        An integer shape tuple (may include None entries).\n+\n+    # Raises\n+        ValueError: in case of invalid argument values.\n+    \"\"\"\n     if dim_ordering == 'th':\n         default_shape = (3, default_size, default_size)\n     else:\n\n@@ -42,17 +42,10 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n \n \n def hinge(y_true, y_pred):\n-    \"\"\"Hinge loss metric.\n-\n-    Computes the hinge loss, which is defined as\n-    `max(1 - y_true * y_pred, 0)`.\n-    \"\"\"\n     return K.mean(K.maximum(1. - y_true * y_pred, 0.))\n \n \n def squared_hinge(y_true, y_pred):\n-    \"\"\"Computes the squared value of the hinge loss.\n-    \"\"\"\n     return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)))\n \n \n\n@@ -5,6 +5,9 @@ from six.moves import zip\n from . import backend as K\n from .utils.generic_utils import get_from_module\n \n+if K.backend() == 'tensorflow':\n+    import tensorflow as tf\n+\n \n def clip_norm(g, c, n):\n     if c > 0:\n@@ -22,6 +25,9 @@ def optimizer_from_config(config, custom_objects=None):\n \n     # Returns\n         An optimizer instance.\n+\n+    # Raises\n+        ValueError: in case of invalid optimizer config.\n     \"\"\"\n     all_classes = {\n         'sgd': SGD,\n@@ -91,6 +97,9 @@ class Optimizer(object):\n                 number of the dimensions of the weights\n                 of the optimizer (i.e. it should match the\n                 output of `get_weights`).\n+\n+        # Raises\n+            ValueError: in case of incompatible weight shapes.\n         \"\"\"\n         params = self.weights\n         weight_value_tuples = []\n@@ -641,7 +650,6 @@ nadam = Nadam\n def get(identifier, kwargs=None):\n     if K.backend() == 'tensorflow':\n         # Wrap TF optimizer instances\n-        import tensorflow as tf\n         if isinstance(identifier, tf.train.Optimizer):\n             return TFOptimizer(identifier)\n     # Instantiate a Keras optimizer\n\n@@ -5,6 +5,8 @@ import warnings\n \n \n class Regularizer(object):\n+    \"\"\"Regularizer base class.\n+    \"\"\"\n \n     def __call__(self, x):\n         return 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3e933ca0ed1c526c0a9b8643ca84129db96ecc17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 97 | Lines Deleted: 61 | Files Changed: 7 | Hunks: 53 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 158 | Churn Cumulative: 1212 | Contributors (this commit): 12 | Commits (past 90d): 12 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -5,18 +5,28 @@ from six.moves import cPickle\n \n \n def load_batch(fpath, label_key='labels'):\n+    \"\"\"Internal utility for parsing CIFAR data.\n+\n+    # Arguments\n+        fpath: path the file to parse.\n+        label_key: key for label data in the retrieve\n+            dictionary.\n+\n+    # Returns\n+        A tuple `(data, labels)`.\n+    \"\"\"\n     f = open(fpath, 'rb')\n     if sys.version_info < (3,):\n         d = cPickle.load(f)\n     else:\n-        d = cPickle.load(f, encoding=\"bytes\")\n+        d = cPickle.load(f, encoding='bytes')\n         # decode utf8\n         d_decoded = {}\n         for k, v in d.items():\n-            d_decoded[k.decode(\"utf8\")] = v\n+            d_decoded[k.decode('utf8')] = v\n         d = d_decoded\n     f.close()\n-    data = d[\"data\"]\n+    data = d['data']\n     labels = d[label_key]\n \n     data = data.reshape(data.shape[0], 3, 32, 32)\n\n@@ -7,29 +7,34 @@ import os\n \n \n def load_data():\n-    dirname = \"cifar-10-batches-py\"\n-    origin = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n+    \"\"\"Loads CIFAR10 dataset.\n+\n+    # Returns\n+        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+    \"\"\"\n+    dirname = 'cifar-10-batches-py'\n+    origin = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n     path = get_file(dirname, origin=origin, untar=True)\n \n     nb_train_samples = 50000\n \n-    X_train = np.zeros((nb_train_samples, 3, 32, 32), dtype=\"uint8\")\n-    y_train = np.zeros((nb_train_samples,), dtype=\"uint8\")\n+    x_train = np.zeros((nb_train_samples, 3, 32, 32), dtype='uint8')\n+    y_train = np.zeros((nb_train_samples,), dtype='uint8')\n \n     for i in range(1, 6):\n         fpath = os.path.join(path, 'data_batch_' + str(i))\n         data, labels = load_batch(fpath)\n-        X_train[(i - 1) * 10000: i * 10000, :, :, :] = data\n+        x_train[(i - 1) * 10000: i * 10000, :, :, :] = data\n         y_train[(i - 1) * 10000: i * 10000] = labels\n \n     fpath = os.path.join(path, 'test_batch')\n-    X_test, y_test = load_batch(fpath)\n+    x_test, y_test = load_batch(fpath)\n \n     y_train = np.reshape(y_train, (len(y_train), 1))\n     y_test = np.reshape(y_test, (len(y_test), 1))\n \n     if K.image_dim_ordering() == 'tf':\n-        X_train = X_train.transpose(0, 2, 3, 1)\n-        X_test = X_test.transpose(0, 2, 3, 1)\n+        x_train = x_train.transpose(0, 2, 3, 1)\n+        x_test = x_test.transpose(0, 2, 3, 1)\n \n-    return (X_train, y_train), (X_test, y_test)\n+    return (x_train, y_train), (x_test, y_test)\n\n@@ -7,6 +7,14 @@ import os\n \n \n def load_data(label_mode='fine'):\n+    \"\"\"Loads CIFAR100 dataset.\n+\n+    # Arguments\n+        label_mode: one of \"fine\", \"coarse\".\n+\n+    # Returns\n+        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+    \"\"\"\n     if label_mode not in ['fine', 'coarse']:\n         raise ValueError('label_mode must be one of \"fine\" \"coarse\".')\n \n@@ -15,16 +23,16 @@ def load_data(label_mode='fine'):\n     path = get_file(dirname, origin=origin, untar=True)\n \n     fpath = os.path.join(path, 'train')\n-    X_train, y_train = load_batch(fpath, label_key=label_mode + '_labels')\n+    x_train, y_train = load_batch(fpath, label_key=label_mode + '_labels')\n \n     fpath = os.path.join(path, 'test')\n-    X_test, y_test = load_batch(fpath, label_key=label_mode + '_labels')\n+    x_test, y_test = load_batch(fpath, label_key=label_mode + '_labels')\n \n     y_train = np.reshape(y_train, (len(y_train), 1))\n     y_test = np.reshape(y_test, (len(y_test), 1))\n \n     if K.image_dim_ordering() == 'tf':\n-        X_train = X_train.transpose(0, 2, 3, 1)\n-        X_test = X_test.transpose(0, 2, 3, 1)\n+        x_train = x_train.transpose(0, 2, 3, 1)\n+        x_test = x_test.transpose(0, 2, 3, 1)\n \n-    return (X_train, y_train), (X_test, y_test)\n+    return (x_train, y_train), (x_test, y_test)\n\n@@ -1,4 +0,0 @@\n-from ..utils.data_utils import *\n-import warnings\n-\n-warnings.warn('data_utils has been moved to keras.utils.data_utils.')\n\n@@ -10,7 +10,7 @@ import sys\n def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n               maxlen=None, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n-    \"\"\"Loads IMDB dataset.\n+    \"\"\"Loads the IMDB dataset.\n \n     # Arguments\n         path: where to store the data (in `/.keras/dataset`)\n@@ -27,6 +27,9 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n             or `skip_top` limit will be replaced with this character.\n         index_from: index actual words with this index and higher.\n \n+    # Returns\n+        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n     Note that the 'out of vocabulary' character is only used for\n     words that were present in the training set but are not included\n     because they're not making the `nb_words` cut here.\n@@ -55,51 +58,52 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n     np.random.seed(seed * 2)\n     np.random.shuffle(labels_test)\n \n-    X = x_train + x_test\n+    xs = x_train + x_test\n     labels = labels_train + labels_test\n \n     if start_char is not None:\n-        X = [[start_char] + [w + index_from for w in x] for x in X]\n+        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n     elif index_from:\n-        X = [[w + index_from for w in x] for x in X]\n+        xs = [[w + index_from for w in x] for x in xs]\n \n     if maxlen:\n-        new_X = []\n+        new_xs = []\n         new_labels = []\n-        for x, y in zip(X, labels):\n+        for x, y in zip(xs, labels):\n             if len(x) < maxlen:\n-                new_X.append(x)\n+                new_xs.append(x)\n                 new_labels.append(y)\n-        X = new_X\n+        xs = new_xs\n         labels = new_labels\n-    if not X:\n+    if not xs:\n         raise ValueError('After filtering for sequences shorter than maxlen=' +\n                          str(maxlen) + ', no sequence was kept. '\n                          'Increase maxlen.')\n     if not nb_words:\n-        nb_words = max([max(x) for x in X])\n+        nb_words = max([max(x) for x in xs])\n \n     # by convention, use 2 as OOV word\n-    # reserve 'index_from' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n+    # reserve 'index_from' (=3 by default) characters:\n+    # 0 (padding), 1 (start), 2 (OOV)\n     if oov_char is not None:\n-        X = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n+        xs = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in xs]\n     else:\n-        nX = []\n-        for x in X:\n+        new_xs = []\n+        for x in xs:\n             nx = []\n             for w in x:\n                 if (w >= nb_words or w < skip_top):\n                     nx.append(w)\n-            nX.append(nx)\n-        X = nX\n+            new_xs.append(nx)\n+        xs = new_xs\n \n-    X_train = np.array(X[:len(x_train)])\n+    x_train = np.array(xs[:len(x_train)])\n     y_train = np.array(labels[:len(x_train)])\n \n-    X_test = np.array(X[len(x_train):])\n+    x_test = np.array(xs[len(x_train):])\n     y_test = np.array(labels[len(x_train):])\n \n-    return (X_train, y_train), (X_test, y_test)\n+    return (x_train, y_train), (x_test, y_test)\n \n \n def get_word_index(path='imdb_word_index.pkl'):\n\n@@ -5,6 +5,15 @@ import sys\n \n \n def load_data(path='mnist.pkl.gz'):\n+    \"\"\"Loads the MNIST dataset.\n+\n+    # Arguments\n+        path: path where to cache the dataset locally\n+            (relative to ~/.keras/datasets).\n+\n+    # Returns\n+        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+    \"\"\"\n     path = get_file(path, origin='https://s3.amazonaws.com/img-datasets/mnist.pkl.gz')\n \n     if path.endswith('.gz'):\n@@ -18,4 +27,4 @@ def load_data(path='mnist.pkl.gz'):\n         data = cPickle.load(f, encoding='bytes')\n \n     f.close()\n-    return data  # (X_train, y_train), (X_test, y_test)\n+    return data  # (x_train, y_train), (x_test, y_test)\n\n@@ -28,6 +28,9 @@ def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n             or `skip_top` limit will be replaced with this character.\n         index_from: index actual words with this index and higher.\n \n+    # Returns\n+        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n     Note that the 'out of vocabulary' character is only used for\n     words that were present in the training set but are not included\n     because they're not making the `nb_words` cut here.\n@@ -37,53 +40,54 @@ def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n \n     path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters.pkl')\n     f = open(path, 'rb')\n-    X, labels = cPickle.load(f)\n+    xs, labels = cPickle.load(f)\n     f.close()\n \n     np.random.seed(seed)\n-    np.random.shuffle(X)\n+    np.random.shuffle(xs)\n     np.random.seed(seed)\n     np.random.shuffle(labels)\n \n     if start_char is not None:\n-        X = [[start_char] + [w + index_from for w in x] for x in X]\n+        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n     elif index_from:\n-        X = [[w + index_from for w in x] for x in X]\n+        xs = [[w + index_from for w in x] for x in xs]\n \n     if maxlen:\n-        new_X = []\n+        new_xs = []\n         new_labels = []\n-        for x, y in zip(X, labels):\n+        for x, y in zip(xs, labels):\n             if len(x) < maxlen:\n-                new_X.append(x)\n+                new_xs.append(x)\n                 new_labels.append(y)\n-        X = new_X\n+        xs = new_xs\n         labels = new_labels\n \n     if not nb_words:\n-        nb_words = max([max(x) for x in X])\n+        nb_words = max([max(x) for x in xs])\n \n     # by convention, use 2 as OOV word\n-    # reserve 'index_from' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n+    # reserve 'index_from' (=3 by default) characters:\n+    # 0 (padding), 1 (start), 2 (OOV)\n     if oov_char is not None:\n-        X = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n+        xs = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in xs]\n     else:\n-        nX = []\n-        for x in X:\n+        new_xs = []\n+        for x in xs:\n             nx = []\n             for w in x:\n                 if (w >= nb_words or w < skip_top):\n                     nx.append(w)\n-            nX.append(nx)\n-        X = nX\n+            new_xs.append(nx)\n+        xs = new_xs\n \n-    X_train = X[:int(len(X) * (1 - test_split))]\n-    y_train = labels[:int(len(X) * (1 - test_split))]\n+    x_train = xs[:int(len(xs) * (1 - test_split))]\n+    y_train = labels[:int(len(xs) * (1 - test_split))]\n \n-    X_test = X[int(len(X) * (1 - test_split)):]\n-    y_test = labels[int(len(X) * (1 - test_split)):]\n+    x_test = xs[int(len(xs) * (1 - test_split)):]\n+    y_test = labels[int(len(xs) * (1 - test_split)):]\n \n-    return (X_train, y_train), (X_test, y_test)\n+    return (x_train, y_train), (x_test, y_test)\n \n \n def get_word_index(path='reuters_word_index.pkl'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5746331f6c214f048467e29a0c5656aefd97950", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 746 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -29,6 +29,9 @@ def preprocess_input(audio_path, dim_ordering='default'):\n \n     # Returns\n         3D Numpy tensor encoding the Mel-spectrogram.\n+\n+    # Raises\n+        ImportError: if librosa is not available.\n     \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = K.image_dim_ordering()\n\n@@ -14,6 +14,9 @@ def load_data(label_mode='fine'):\n \n     # Returns\n         Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n+    # Raises\n+        ValueError: in case of invalid `label_mode`.\n     \"\"\"\n     if label_mode not in ['fine', 'coarse']:\n         raise ValueError('label_mode must be one of \"fine\" \"coarse\".')\n\n@@ -13,7 +13,7 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n     \"\"\"Loads the IMDB dataset.\n \n     # Arguments\n-        path: where to store the data (in `/.keras/dataset`)\n+        path: where to cache the data (relative to `~/.keras/dataset`).\n         nb_words: max number of words to include. Words are ranked\n             by how often they occur (in the training set) and only\n             the most frequent words are kept\n@@ -30,6 +30,10 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n     # Returns\n         Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n \n+    # Raises\n+        ValueError: in case `maxlen` is so low\n+            that no input sequence could be kept.\n+\n     Note that the 'out of vocabulary' character is only used for\n     words that were present in the training set but are not included\n     because they're not making the `nb_words` cut here.\n@@ -92,7 +96,7 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n         for x in xs:\n             nx = []\n             for w in x:\n-                if (w >= nb_words or w < skip_top):\n+                if w >= nb_words or w < skip_top:\n                     nx.append(w)\n             new_xs.append(nx)\n         xs = new_xs\n@@ -107,6 +111,11 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n \n \n def get_word_index(path='imdb_word_index.pkl'):\n+    \"\"\"Retrieves the dictionary mapping word indices back to words.\n+\n+    # Arguments\n+        path: where to cache the data (relative to `~/.keras/dataset`).\n+    \"\"\"\n     path = get_file(path,\n                     origin='https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl',\n                     md5_hash='72d94b01291be4ff843198d3b0e1e4d7')\n\n@@ -13,7 +13,7 @@ def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n     \"\"\"Loads the Reuters newswire classification dataset.\n \n     # Arguments\n-        path: where to store the data (in `/.keras/dataset`)\n+        path: where to cache the data (relative to `~/.keras/dataset`).\n         nb_words: max number of words to include. Words are ranked\n             by how often they occur (in the training set) and only\n             the most frequent words are kept\n@@ -76,7 +76,7 @@ def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n         for x in xs:\n             nx = []\n             for w in x:\n-                if (w >= nb_words or w < skip_top):\n+                if w >= nb_words or w < skip_top:\n                     nx.append(w)\n             new_xs.append(nx)\n         xs = new_xs\n@@ -91,6 +91,11 @@ def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n \n \n def get_word_index(path='reuters_word_index.pkl'):\n+    \"\"\"Retrieves the dictionary mapping word indices back to words.\n+\n+    # Arguments\n+        path: where to cache the data (relative to `~/.keras/dataset`).\n+    \"\"\"\n     path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl')\n     f = open(path, 'rb')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1b7800aceb1b2672630e067425790c3c9769e5f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 39 | Files Changed: 10 | Hunks: 32 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 120 | Churn Cumulative: 14464 | Contributors (this commit): 76 | Commits (past 90d): 80 | Contributors (cumulative): 118 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,8 +38,8 @@ def preprocess_input(audio_path, dim_ordering='default'):\n     assert dim_ordering in {'tf', 'th'}\n \n     if librosa is None:\n-        raise ImportError('Librosa is required to process audio files.\\n' +\n-                          'Install it via `pip install librosa` \\nor visit ' +\n+        raise ImportError('Librosa is required to process audio files. '\n+                          'Install it via `pip install librosa` or visit '\n                           'http://librosa.github.io/librosa/ for details.')\n \n     # mel-spectrogram parameters\n\n@@ -115,6 +115,9 @@ def get_word_index(path='imdb_word_index.pkl'):\n \n     # Arguments\n         path: where to cache the data (relative to `~/.keras/dataset`).\n+\n+    # Returns\n+        The word index dictionary.\n     \"\"\"\n     path = get_file(path,\n                     origin='https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl',\n\n@@ -95,6 +95,9 @@ def get_word_index(path='reuters_word_index.pkl'):\n \n     # Arguments\n         path: where to cache the data (relative to `~/.keras/dataset`).\n+\n+    # Returns\n+        The word index dictionary.\n     \"\"\"\n     path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl')\n     f = open(path, 'rb')\n\n@@ -5,8 +5,9 @@ import numpy as np\n \n \n class LeakyReLU(Layer):\n-    \"\"\"Special version of a Rectified Linear Unit\n-    that allows a small gradient when the unit is not active:\n+    \"\"\"Leaky version of a Rectified Linear Unit.\n+\n+    It allows a small gradient when the unit is not active:\n     `f(x) = alpha * x for x < 0`,\n     `f(x) = x for x >= 0`.\n \n@@ -40,7 +41,9 @@ class LeakyReLU(Layer):\n \n \n class PReLU(Layer):\n-    \"\"\"Parametric Rectified Linear Unit:\n+    \"\"\"Parametric Rectified Linear Unit.\n+\n+    It follows:\n     `f(x) = alphas * x for x < 0`,\n     `f(x) = x for x >= 0`,\n     where `alphas` is a learned array with the same shape as x.\n@@ -111,7 +114,9 @@ class PReLU(Layer):\n \n \n class ELU(Layer):\n-    \"\"\"Exponential Linear Unit:\n+    \"\"\"Exponential Linear Unit.\n+\n+    It follows:\n     `f(x) =  alpha * (exp(x) - 1.) for x < 0`,\n     `f(x) = x for x >= 0`.\n \n@@ -145,7 +150,9 @@ class ELU(Layer):\n \n \n class ParametricSoftplus(Layer):\n-    \"\"\"Parametric Softplus:\n+    \"\"\"Parametric Softplus.\n+\n+    It follows:\n     `f(x) = alpha * log(1 + exp(beta * x))`\n \n     # Input shape\n@@ -219,7 +226,9 @@ class ParametricSoftplus(Layer):\n \n \n class ThresholdedReLU(Layer):\n-    \"\"\"Thresholded Rectified Linear Unit:\n+    \"\"\"Thresholded Rectified Linear Unit.\n+\n+    It follows:\n     `f(x) = x for x > theta`,\n     `f(x) = 0 otherwise`.\n \n@@ -253,7 +262,9 @@ class ThresholdedReLU(Layer):\n \n \n class SReLU(Layer):\n-    \"\"\"S-shaped Rectified Linear Unit:\n+    \"\"\"S-shaped Rectified Linear Unit.\n+\n+    It follows:\n     `f(x) = t^r + a^r(x - t^r) for x >= t^r`,\n     `f(x) = x for t^r > x > t^l`,\n     `f(x) = t^l + a^l(x - t^l) for x <= t^l`.\n@@ -337,11 +348,11 @@ class SReLU(Layer):\n             a_right = self.a_right\n             t_right_actual = self.t_right_actual\n \n-        Y_left_and_center = t_left + K.relu(x - t_left,\n+        y_left_and_center = t_left + K.relu(x - t_left,\n                                             a_left,\n                                             t_right_actual - t_left)\n-        Y_right = K.relu(x - t_right_actual) * a_right\n-        return Y_left_and_center + Y_right\n+        y_right = K.relu(x - t_right_actual) * a_right\n+        return y_left_and_center + y_right\n \n     def get_config(self):\n         config = {'t_left_init': self.t_left_init,\n\n@@ -558,19 +558,9 @@ class Deconvolution2D(Convolution2D):\n         output_shape: Output shape of the transposed convolution operation.\n             tuple of integers\n             `(nb_samples, nb_filter, nb_output_rows, nb_output_cols)`.\n-            Formula for calculation of the output shape [1], [2]:\n-                o = s (i - 1) + a + k - 2p, \\quad a \\in \\{0, \\ldots, s - 1\\}\n-                where:\n-                    i - input size (rows or cols),\n-                    k - kernel size (nb_filter),\n-                    s - stride (subsample for rows or cols respectively),\n-                    p - padding size,\n-                    a - user-specified quantity used to distinguish between\n-                        the s different possible output sizes.\n-             Because a is not specified explicitly and Theano and Tensorflow\n-             use different values, it is better to use\n+             It is better to use\n              a dummy input and observe the actual output shape of\n-             a layer as specified in the examples.\n+             a layer, as specified in the examples.\n         init: name of initialization function for the weights of the layer\n             (see [initializations](../initializations.md)), or alternatively,\n             Theano function to use for weights initialization.\n@@ -1267,7 +1257,9 @@ class Convolution3D(Layer):\n \n \n class UpSampling1D(Layer):\n-    \"\"\"Repeats each temporal step `length` times along the time axis.\n+    \"\"\"Upsampling layer for 1D inputs.\n+\n+    Repeats each temporal step `length` times along the time axis.\n \n     # Arguments\n         length: integer. Upsampling factor.\n@@ -1299,7 +1291,9 @@ class UpSampling1D(Layer):\n \n \n class UpSampling2D(Layer):\n-    \"\"\"Repeats the rows and columns of the data\n+    \"\"\"Upsampling layer for 2D inputs.\n+\n+    Repeats the rows and columns of the data\n     by size[0] and size[1] respectively.\n \n     # Arguments\n@@ -1363,8 +1357,10 @@ class UpSampling2D(Layer):\n \n \n class UpSampling3D(Layer):\n-    \"\"\"Repeats the first, second and third dimension of the data\n-    by size[0], size[1] and size[2] respectively.\n+    \"\"\"Upsampling layer for 3D inputs.\n+\n+    Repeats the 1st, 2nd and 3rd dimensions\n+    of the data by size[0], size[1] and size[2] respectively.\n \n     # Arguments\n         size: tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3.\n@@ -1670,6 +1666,7 @@ class ZeroPadding3D(Layer):\n \n class Cropping1D(Layer):\n     \"\"\"Cropping layer for 1D input (e.g. temporal sequence).\n+\n     It crops along the time dimension (axis 1).\n \n     # Arguments\n@@ -1718,6 +1715,7 @@ class Cropping1D(Layer):\n \n class Cropping2D(Layer):\n     \"\"\"Cropping layer for 2D input (e.g. picture).\n+\n     It crops along spatial dimensions, i.e. width and height.\n \n     # Arguments\n\n@@ -9,6 +9,7 @@ import warnings\n \n class ConvRecurrent2D(Layer):\n     \"\"\"Abstract base class for convolutional recurrent layers.\n+\n     Do not use in a model -- it's not a functional layer!\n \n     ConvLSTM2D\n@@ -478,7 +479,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n             ones = K.sum(ones, axis=1)\n             ones = self.conv_step(ones, K.zeros(self.W_shape),\n                                   border_mode=self.border_mode)\n-            ones = ones + 1\n+            ones += 1\n             B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones)\n                    for _ in range(4)]\n             constants.append(B_U)\n@@ -488,7 +489,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n         if 0 < self.dropout_W < 1:\n             ones = K.zeros_like(x)\n             ones = K.sum(ones, axis=1)\n-            ones = ones + 1\n+            ones += 1\n             B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones)\n                    for _ in range(4)]\n             constants.append(B_W)\n\n@@ -1,7 +1,9 @@\n from __future__ import absolute_import\n \n from .. import backend as K\n-from .. import initializations, regularizers, constraints\n+from .. import initializations\n+from .. import regularizers\n+from .. import constraints\n from ..engine import Layer\n \n \n\n@@ -8,7 +8,9 @@ from ..utils.np_utils import conv_output_length\n \n \n class LocallyConnected1D(Layer):\n-    \"\"\"The `LocallyConnected1D` layer works similarly to\n+    \"\"\"Locally-connected layer for 1D inputs.\n+\n+    The `LocallyConnected1D` layer works similarly to\n     the `Convolution1D` layer, except that weights are unshared,\n     that is, a different set of filters is applied at each different patch\n     of the input.\n@@ -179,7 +181,9 @@ class LocallyConnected1D(Layer):\n \n \n class LocallyConnected2D(Layer):\n-    \"\"\"The `LocallyConnected2D` layer works similarly\n+    \"\"\"Locally-connected layer for 2D inputs.\n+\n+    The `LocallyConnected2D` layer works similarly\n     to the `Convolution2D` layer, except that weights are unshared,\n     that is, a different set of filters is applied at each\n     different patch of the input.\n@@ -346,7 +350,7 @@ class LocallyConnected2D(Layer):\n         _, feature_dim, nb_filter = self.W_shape\n \n         if self.dim_ordering == 'th':\n-            if K._backend == 'theano':\n+            if K.backend() == 'theano':\n                 output = []\n                 for i in range(self.output_row):\n                     for j in range(self.output_col):\n\n@@ -29,7 +29,7 @@ class _Pooling1D(Layer):\n                                     self.border_mode, self.stride)\n         return (input_shape[0], length, input_shape[2])\n \n-    def _pooling_function(self, back_end, inputs, pool_size, strides,\n+    def _pooling_function(self, inputs, pool_size, strides,\n                           border_mode, dim_ordering):\n         raise NotImplementedError\n \n@@ -401,6 +401,8 @@ class AveragePooling3D(_Pooling3D):\n \n \n class _GlobalPooling1D(Layer):\n+    \"\"\"Abstract class for different global pooling 1D layers.\n+    \"\"\"\n \n     def __init__(self, **kwargs):\n         super(_GlobalPooling1D, self).__init__(**kwargs)\n@@ -442,6 +444,8 @@ class GlobalMaxPooling1D(_GlobalPooling1D):\n \n \n class _GlobalPooling2D(Layer):\n+    \"\"\"Abstract class for different global pooling 2D layers.\n+    \"\"\"\n \n     def __init__(self, dim_ordering='default', **kwargs):\n         super(_GlobalPooling2D, self).__init__(**kwargs)\n@@ -522,6 +526,8 @@ class GlobalMaxPooling2D(_GlobalPooling2D):\n \n \n class _GlobalPooling3D(Layer):\n+    \"\"\"Abstract class for different global pooling 3D layers.\n+    \"\"\"\n \n     def __init__(self, dim_ordering='default', **kwargs):\n         super(_GlobalPooling3D, self).__init__(**kwargs)\n\n@@ -9,7 +9,20 @@ from ..engine import Layer, InputSpec\n \n def time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None, timesteps=None):\n-    \"\"\"Apply y.w + b for every temporal slice y of x.\n+    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n+\n+    # Arguments\n+        x: input tensor.\n+        w: weight matrix.\n+        b: optional bias vector.\n+        dropout: wether to apply dropout (same dropout mask\n+            for every temporal slice of the input).\n+        input_dim: integer; optional dimensionality of the input.\n+        output_dim: integer; optional dimensionality of the output.\n+        timesteps: integer; optional number of timesteps.\n+\n+    # Returns\n+        Output tensor.\n     \"\"\"\n     if not input_dim:\n         input_dim = K.shape(x)[2]\n@@ -29,7 +42,7 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n     x = K.reshape(x, (-1, input_dim))\n     x = K.dot(x, w)\n     if b:\n-        x = x + b\n+        x += b\n     # reshape to 3D tensor\n     if K.backend() == 'tensorflow':\n         x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n@@ -524,8 +537,9 @@ class GRU(Recurrent):\n         assert self.stateful, 'Layer must be stateful.'\n         input_shape = self.input_spec[0].shape\n         if not input_shape[0]:\n-            raise ValueError('If a RNN is stateful, a complete ' +\n-                             'input_shape must be provided (including batch size).')\n+            raise ValueError('If a RNN is stateful, a complete '\n+                             'input_shape must be provided '\n+                             '(including batch size).')\n         if hasattr(self, 'states'):\n             K.set_value(self.states[0],\n                         np.zeros((input_shape[0], self.output_dim)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
