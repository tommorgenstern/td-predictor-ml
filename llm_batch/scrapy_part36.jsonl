{"custom_id": "scrapy#3f13b388c711da1a1d41cc3add763ffbe613e65d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 41 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,11 @@ class ResponseSoupTest(unittest.TestCase):\n         # make sure it's cached\n         assert soup1 is soup2\n \n+        # when body is None, an empty soup should be returned\n+        r1 = Response('http://www.example.com')\n+        assert r1.body is None\n+        assert isinstance(r1.getsoup(), BeautifulSoup)\n+\n     def test_response_soup_caching(self):\n         r1 = Response('http://www.example.com', body='')\n         soup1 = r1.getsoup()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12d0bd4dbb83fa9753145208cb0778ac8f9bb88d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 30 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,4 +17,6 @@ class CommonMiddleware(object):\n         request.headers.setdefault('Accept-Language', self.header_accept_language)\n         if request.method == 'POST':\n             request.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')\n+            if request.body:\n+                request.headers.setdefault('Content-Length', '%d' % len(request.body))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bef1fa967aeeddb34fc86a73d12a3f9ae90f81c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 7 | Churn Cumulative: 488 | Contributors (this commit): 2 | Commits (past 90d): 26 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -92,7 +92,7 @@ class CrawlSpider(BaseSpider):\n             for link in links:\n                 r = Request(url=link.url)\n                 r.meta['link_text'] = link.text\n-                r.append_callback(self._response_downloaded, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)\n+                r.deferred.addCallback(self._response_downloaded, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)\n                 requests.append(r)\n         return requests\n \n\n@@ -36,11 +36,6 @@ class Request(object):\n         self.meta = {} if meta is None else dict(meta)\n         self.cache = {}\n         \n-    def append_callback(self, callback, *args, **kwargs):\n-        if isinstance(callback, defer.Deferred):\n-            return chain_deferred(self.deferred, callback)\n-        return self.deferred.addCallback(callback, *args, **kwargs)\n-\n     def set_url(self, url):\n         assert isinstance(url, basestring), \\\n             'Request url argument must be str or unicode, got %s:' % type(url).__name__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ff4dc4d02cb8c75bdb053b9023d8655759ab1b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 120 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,8 @@ About HTTP errors to consider:\n \"\"\"\n \n from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookupError, \\\n-                                   ConnectionRefusedError, ConnectionDone, ConnectError\n+                                   ConnectionRefusedError, ConnectionDone, ConnectError, \\\n+                                   ConnectionLost\n from twisted.internet.defer import TimeoutError as UserTimeoutError\n \n from scrapy import log\n@@ -36,7 +37,8 @@ from scrapy.conf import settings\n class RetryMiddleware(object):\n \n     EXCEPTIONS_TO_RETRY = (ServerTimeoutError, UserTimeoutError, DNSLookupError,\n-                           ConnectionRefusedError, ConnectionDone, ConnectError)\n+                           ConnectionRefusedError, ConnectionDone, ConnectError,\n+                           ConnectionLost)\n \n     def __init__(self):\n         self.failed_count = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#86923cc694ca18eff832aa54672d422c1a89ad1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 44 | Files Changed: 2 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 96 | Churn Cumulative: 564 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: 0.875\n\nDIFF:\n@@ -1,14 +1,13 @@\n \"\"\"\n This module provides additional LinkExtractors, apart from the ones in scrapy.link\n and scrapy.link.extractors.\n-\n \"\"\"\n import urlparse\n \n from scrapy.link import Link\n from scrapy.utils.url import canonicalize_url\n-from scrapy.utils.python import unicode_to_str\n-from scrapy.xpath import HtmlXPathSelector\n+from scrapy.utils.python import unicode_to_str, flatten\n+from scrapy.xpath.selector import XPathSelectorList, HtmlXPathSelector\n \n class HTMLImageLinkExtractor(object):\n     '''HTMLImageLinkExtractor objects are intended to extract image links from HTML pages\n@@ -21,15 +20,15 @@ class HTMLImageLinkExtractor(object):\n     both locations will be used for that call of extract_links'''\n \n     def __init__(self, locations=None, unique=True, canonicalize=True):\n-        self.locations = tuple(locations) if hasattr(locations, '__iter__') else tuple()\n+        self.locations = flatten([locations])\n         self.unique = unique\n         self.canonicalize = canonicalize\n \n     def extract_from_selector(self, selector, parent=None):\n         ret = []\n         def _add_link(url_sel, alt_sel=None):\n-            url = url_sel.extract()\n-            alt = alt_sel.extract() if alt_sel else ('', )\n+            url = flatten([url_sel.extract()])\n+            alt = flatten([alt_sel.extract()]) if alt_sel else (u'', )\n             if url:\n                 ret.append(Link(unicode_to_str(url[0]), alt[0]))\n \n@@ -55,8 +54,14 @@ class HTMLImageLinkExtractor(object):\n \n         links = []\n         for location in self.locations:\n-            selector_res = xs.x(location)\n-            for selector in selector_res:\n+            if isinstance(location, basestring):\n+                selectors = xs.x(location)\n+            elif isinstance(location, (XPathSelectorList, HtmlXPathSelector)):\n+                selectors = [location] if isinstance(location, HtmlXPathSelector) else location\n+            else:\n+                continue\n+\n+            for selector in selectors:\n                 links.extend(self.extract_from_selector(selector))\n \n         seen, ret = set(), []\n\n@@ -142,48 +142,51 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.matches('http://blah2.com/blah1'), False)\n         self.assertEqual(lx.matches('http://blah2.com/blah2'), False)\n \n-#class HTMLImageLinkExtractorTestCase(unittest.TestCase):\n-#    def setUp(self):\n-#        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n-#        body = open(os.path.join(base_path 'image_linkextractor.html'), 'r').read()\n-#        self.response = Response(url='http://example.com/index', body=body)\n+class HTMLImageLinkExtractorTestCase(unittest.TestCase):\n+    def setUp(self):\n+        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n+        body = open(os.path.join(base_path, 'image_linkextractor.html'), 'r').read()\n+        self.response = Response(url='http://example.com/index', body=body)\n \n-#    def test_urls_type(self):\n-#        '''Test that the resulting urls are regular strings and not a unicode objects'''\n-#        lx = ImageLinkExtractor()\n-#        links = lx.extract_links(self.response)\n-#        self.assertTrue(all(isinstance(link.url, str) for link in links))\n+    def tearDown(self):\n+        del self.response\n \n-#    def test_extraction(self):\n-#        '''Test the extractor's behaviour among different situations'''\n+    def test_urls_type(self):\n+        '''Test that the resulting urls are regular strings and not a unicode objects'''\n+        lx = HTMLImageLinkExtractor()\n+        links = lx.extract_links(self.response)\n+        self.assertTrue(all(isinstance(link.url, str) for link in links))\n \n-#        lx = HTMLImageLinkExtractor(locations=('//img', ))\n-#        links_1 = lx.extract_links(self.response)\n-#        self.assertEqual(links_1,\n-#            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),\n-#              Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n-#              Link(url='http://example.com/sample4.jpg', text=u'sample 4') ])\n+    def test_extraction(self):\n+        '''Test the extractor's behaviour among different situations'''\n \n-#        lx = HTMLImageLinkExtractor(locations=('//img', ), unique=False)\n-#        links_2 = lx.extract_links(self.response, unique=False)\n-#        self.assertEqual(links_2,\n-#            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),\n-#              Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n-#              Link(url='http://example.com/sample4.jpg', text=u'sample 4'),\n-#              Link(url='http://example.com/sample4.jpg', text=u'sample 4 repetition') ])\n+        lx = HTMLImageLinkExtractor(locations=('//img', ))\n+        links_1 = lx.extract_links(self.response)\n+        self.assertEqual(links_1,\n+            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),\n+              Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n+              Link(url='http://example.com/sample4.jpg', text=u'sample 4') ])\n \n-#        lx = HTMLImageLinkExtractor(locations=('//div[@id=\"wrapper\"]', )\n-#        links_3 = lx.extract_links(self.response)\n-#        self.assertEqual(links_3,\n-#            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),\n-#              Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n-#              Link(url='http://example.com/sample4.jpg', text=u'sample 4') ])\n+        lx = HTMLImageLinkExtractor(locations=('//img', ), unique=False)\n+        links_2 = lx.extract_links(self.response)\n+        self.assertEqual(links_2,\n+            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),\n+              Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n+              Link(url='http://example.com/sample4.jpg', text=u'sample 4'),\n+              Link(url='http://example.com/sample4.jpg', text=u'sample 4 repetition') ])\n \n-#        lx = HTMLImageLinkExtractor(locations=('//a', )\n-#        links_4 = lx.extract_links(self.response)\n-#        self.assertEqual(links_4,\n-#            [ Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n-#              Link(url='http://example.com/sample3.html', text=u'sample 3') ])\n+        lx = HTMLImageLinkExtractor(locations=('//div[@id=\"wrapper\"]', ))\n+        links_3 = lx.extract_links(self.response)\n+        self.assertEqual(links_3,\n+            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),\n+              Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n+              Link(url='http://example.com/sample4.jpg', text=u'sample 4') ])\n+\n+        lx = HTMLImageLinkExtractor(locations=('//a', ))\n+        links_4 = lx.extract_links(self.response)\n+        self.assertEqual(links_4,\n+            [ Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n+              Link(url='http://example.com/sample3.html', text=u'sample 3') ])\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4142fd0d5df5c7eb0b3753150069a1bd4a826682", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 29 | Churn Cumulative: 98 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,20 +1,20 @@\n import unittest\n from scrapy.http.response import Response\n-from scrapy.utils.response import body_or_str\n+from scrapy.utils.response import body_or_str, get_base_url\n \n class ResponseUtilsTest(unittest.TestCase):\n     dummy_response = Response(url='http://example.org/', body='dummy_response')\n \n-    def test_input(self):\n+    def test_body_or_str_input(self):\n         self.assertTrue(isinstance(body_or_str(self.dummy_response), basestring))\n         self.assertTrue(isinstance(body_or_str('text'), basestring))\n         self.assertRaises(Exception, body_or_str, 2)\n \n-    def test_extraction(self):\n+    def test_body_or_str_extraction(self):\n         self.assertEqual(body_or_str(self.dummy_response), 'dummy_response')\n         self.assertEqual(body_or_str('text'), 'text')\n \n-    def test_encoding(self):\n+    def test_body_or_str_encoding(self):\n         self.assertTrue(isinstance(body_or_str(self.dummy_response, unicode=False), str))\n         self.assertTrue(isinstance(body_or_str(self.dummy_response, unicode=True), unicode))\n \n@@ -24,5 +24,14 @@ class ResponseUtilsTest(unittest.TestCase):\n         self.assertTrue(isinstance(body_or_str(u'text', unicode=False), str))\n         self.assertTrue(isinstance(body_or_str(u'text', unicode=True), unicode))\n \n+    def test_get_base_url(self):\n+        response = Response(url='http://example.org', body=\"\"\"\\\n+            <html>\\\n+            <head><title>Dummy</title><base href='http://example.org/something' /></head>\\\n+            <body>blahablsdfsal&amp;</body>\\\n+            </html>\"\"\")\n+        self.assertEqual(get_base_url(response), 'http://example.org/something')\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -3,6 +3,7 @@ This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n \n+import re\n from scrapy.http.response import Response\n \n def body_or_str(obj, unicode=True):\n@@ -13,3 +14,14 @@ def body_or_str(obj, unicode=True):\n         return obj.decode('utf-8') if unicode else obj\n     else:\n         return obj if unicode else obj.encode('utf-8')\n+\n+BASEURL_RE = re.compile(r'<base\\s+href\\s*=\\s*[\\\"\\']\\s*([^\\\"\\'\\s]+)\\s*[\\\"\\']', re.I)\n+def get_base_url(response):\n+    \"\"\" Return the base url of the given response used to resolve relative links. \"\"\"\n+    # re is used instead of xpath for eficiency reasons. In a quick\n+    # benchmark using timeit we got (for 50 repetitions) 0.0017 seconds\n+    # using re and 0.7452 using xpath\n+    if 'base_url' not in response.cache:\n+        match = BASEURL_RE.search(response.body.to_string()[0:4096])\n+        response.cache['base_url'] = match.group(1) if match else response.url\n+    return response.cache['base_url']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f6021cad2cf6260ec78b3923875f0389343121b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 470 | Lines Deleted: 507 | Files Changed: 16 | Hunks: 68 | Methods Changed: 83 | Complexity Δ (Sum/Max): 10/28 | Churn Δ: 977 | Churn Cumulative: 4192 | Contributors (this commit): 3 | Commits (past 90d): 87 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,85 +0,0 @@\n-from scrapy.item.adaptors import AdaptorPipe as BaseAdaptorPipe\n-from scrapy.utils.serialization import serialize\n-\n-class DuplicatedAdaptorName(Exception): pass\n-\n-class _Adaptor(object):\n-    \"\"\"\n-    Adaptors instances should be instantiated and used only\n-    inside the AdaptorPipe.\n-    \"\"\"\n-    def __init__(self, function, match_function):\n-        self.basefunction = function\n-        self.match_function = match_function\n-    def __repr__(self):\n-        return self.basefunction.func_name\n-    def __call__(self, *args):\n-        return self.basefunction(*args)\n-        \n-class AdaptorPipe(BaseAdaptorPipe):\n-\n-    def __init__(self, attribute_names, adaptors=None):\n-        \"\"\"\n-        If \"adaptors\" is given, constructs pipeline from this.\n-        \"adaptors\" is an ordered tuple of 2-elements tuples, each of which\n-        has the same parameters you give to the insertadaptor method, except \n-        'after' and 'before', because you define the adaptors order in the tuple.\n-        Example:\n-        (\n-          (my_function, lambda x: x in my_list)\n-          ...\n-        )\n-        \"\"\"\n-        self.__attribute_names = [ n for n in attribute_names ]\n-        self.__adaptorspipe = []\n-        self.pipes = {}\n-        if adaptors:\n-            for entry in adaptors:\n-                self.insertadaptor(compile_pipe=False, *entry)\n-            self._compile_pipe()\n-\n-    @property\n-    def adaptors_names(self):\n-        _adaptors = []\n-        for a in self.__adaptorspipe:\n-            _adaptors.append(a.basefunction.func_name)\n-        return _adaptors\n-    \n-    def insertadaptor(self, function, match_function=lambda x: True, compile_pipe=True, after=None, before=None):\n-        \"\"\"\n-        Inserts a \"function\" as an adaptor that will apply when match_function returns True (by\n-        default always apply)\n-        If \"after\" is given, inserts the adaptor after the already inserted adaptor\n-        of the name given in this parameter, If \"before\" is given, inserts it before\n-        the adaptor of the given name. \"name\" is the name of the adaptor.\n-        \"\"\"\n-        if function.func_name in self.adaptors_names:\n-            raise DuplicatedAdaptorName(function.func_name)\n-        else:\n-            adaptor = _Adaptor(function, match_function)\n-            #by default append adaptor at end of pipe\n-            pos = len(self.adaptors_names)\n-            if after:\n-                pos = self.adaptors_names.index(after) + 1\n-            elif before:\n-                pos = self.adaptors_names.index(before)\n-            self.__adaptorspipe.insert(pos, adaptor)\n-            if compile_pipe:\n-                self._compile_pipe()\n-            return pos\n-\n-    def removeadaptor(self, adaptorname):\n-        pos = self.adaptors_names.index(adaptorname)\n-        self.__adaptorspipe.pop(pos)\n-        self._compile_pipe()\n-\n-    def _compile_pipe(self):\n-        for attrname in self.__attribute_names:\n-            adaptors_pipe = []\n-            for adaptor in self.__adaptorspipe:\n-                if adaptor.match_function(attrname):\n-                    adaptors_pipe.append(adaptor)\n-            self.pipes[attrname] = adaptors_pipe\n-\n-    def __repr__(self):\n-        return serialize(self.pipes, \"pprint\")\n\\ No newline at end of file\n\n@@ -1,4 +0,0 @@\n-from scrapy.contrib.adaptors.extraction import extract, ExtractImages\n-from scrapy.contrib.adaptors.markup import remove_tags, remove_root, Unquote\n-from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip, drop_empty, canonicalize_urls, Delist, Regex\n-from scrapy.utils.python import unique, flatten\n\n@@ -1,106 +0,0 @@\n-\"\"\"\n-Adaptors related with extraction of data\n-\"\"\"\n-\n-import urlparse\n-import re\n-from scrapy import log\n-from scrapy.http import Response\n-from scrapy.utils.url import is_url\n-from scrapy.utils.python import flatten\n-from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n-\n-def extract(locations, adaptor_args=None):\n-    \"\"\"\n-    This adaptor tries to extract data from the given locations.\n-    Any XPathSelector in it will be extracted, and any other data\n-    will be added as-is to the result.\n-\n-    If an XPathSelector is a text/cdata node, and `use_unquote`\n-    is True, that selector will be extracted using the `extract_unquoted`\n-    method; otherwise, the `extract` method will be used.\n-\n-    Input: anything\n-    Output: list of extracted selectors plus anything else in the input\n-    \"\"\"\n-    locations = flatten([locations])\n-    use_unquote = adaptor_args.get('use_unquote', True) if adaptor_args else True\n-\n-    result = []\n-    for location in locations:\n-        if isinstance(location, XPathSelector):\n-            if location.xmlNode.type in ('text', 'cdata') and use_unquote:\n-                result.append(location.extract_unquoted())\n-            else:\n-                result.append(location.extract())\n-        else:\n-            result.append(location)\n-    return result\n-\n-class ExtractImages(object):\n-    \"\"\"\n-    This adaptor may receive either an XPathSelector containing\n-    the desired locations for finding urls, a list of relative\n-    links to be resolved, or simply a link (relative or not).\n-\n-    Input: XPathSelector, XPathSelectorList, iterable\n-    Output: list of unicodes\n-    \"\"\"\n-    def __init__(self, response=None, base_url=None):\n-        BASETAG_RE = re.compile(r'<base\\s+href\\s*=\\s*[\\\"\\']\\s*([^\\\"\\'\\s]+)\\s*[\\\"\\']', re.I)\n-\n-        if response:\n-            match = BASETAG_RE.search(response.body.to_string()[0:4096])\n-            if match:\n-                self.base_url = match.group(1)\n-            else:\n-                self.base_url = response.url\n-        else:\n-            self.base_url = base_url\n-\n-        if not self.base_url:\n-            log.msg('No base URL was found for ExtractImages adaptor, will only extract absolute URLs', log.WARNING)\n-\n-    def extract_from_xpath(self, selector):\n-        ret = []\n-        if selector.xmlNode.type == 'element':\n-            if selector.xmlNode.name == 'a':\n-                children = selector.x('child::*')\n-                if len(children) > 1:\n-                    ret.extend(selector.x('.//@href'))\n-                    ret.extend(selector.x('.//@src'))\n-                elif len(children) == 1 and children[0].xmlNode.name == 'img':\n-                    ret.extend(children.x('@src'))\n-                else:\n-                    ret.extend(selector.x('@href'))\n-            elif selector.xmlNode.name == 'img':\n-                ret.extend(selector.x('@src'))\n-            elif selector.xmlNode.name == 'text':\n-                ret.extend(selector)\n-            else:\n-                ret.extend(selector.x('.//@href'))\n-                ret.extend(selector.x('.//@src'))\n-                ret.extend(selector.x('.//text()'))\n-        else:\n-            ret.append(selector)\n-\n-        return ret\n-\n-    def __call__(self, locations):\n-        if not locations:\n-            return []\n-        elif isinstance(locations, basestring):\n-            locations = [locations]\n-\n-        rel_urls = []\n-        for location in flatten(locations):\n-            if isinstance(location, (XPathSelector, XPathSelectorList)):\n-                rel_urls.extend(self.extract_from_xpath(location))\n-            else:\n-                rel_urls.append(location)\n-        rel_urls = extract(rel_urls)\n-\n-        if self.base_url:\n-            return [urlparse.urljoin(self.base_url, url) for url in rel_urls]\n-        else:\n-            return filter(rel_urls, is_url)\n\n@@ -1,43 +0,0 @@\n-import re\n-from scrapy.utils.markup import replace_tags, unquote_markup\n-from scrapy.item.adaptors import adaptize\n-\n-def remove_tags(value):\n-    \"\"\"\n-    Removes any tags found in each of the provided list's string.\n-    E.g:\n-      >> remove_tags(['<head>my header</head>', '<body>my <b>body</b></body>'])\n-      [u'my header', u'my body']\n-    Input: iterable with strings\n-    Output: list of unicodes\n-    \"\"\"\n-    return [ replace_tags(v) for v in value ]\n-\n-def remove_root(value):\n-    \"\"\"\n-    Input: iterable with strings\n-    Output: list of strings\n-    \"\"\"\n-    def _remove_root(value):\n-        _remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n-        m = _remove_root_re.search(value)\n-        if m:\n-            value = m.group(1)\n-        return unicode(value)\n-    return [ _remove_root(v) for v in value ]\n-\n-class Unquote(object):\n-    \"\"\"\n-    Receives a list of strings, removes all of the\n-    CDATAs and entities (except the ones in CDATAs) the strings\n-    may have, and returns a new list.\n-\n-    Input: iterable with strings\n-    Output: list of strings\n-    \"\"\"\n-    def __init__(self, keep=None):\n-        self.keep = [] if keep is None else keep\n-\n-    def __call__(self, value, keep=None):\n-        keep = keep if keep is not None else self.keep\n-        return [ unquote_markup(v, keep=keep) for v in value ]\n\n@@ -9,9 +9,11 @@ from pydispatch import dispatcher\n from pprint import PrettyPrinter\n \n from scrapy.item import ScrapedItem, ItemDelta\n+from scrapy.item.adaptors import AdaptorPipe\n from scrapy.spider import spiders\n from scrapy.core import signals\n from scrapy.core.exceptions import UsageError, DropItem\n+from scrapy.utils.python import unique\n \n class ValidationError(DropItem):\n     \"\"\"Indicates a data validation error\"\"\"\n@@ -41,8 +43,9 @@ class RobustScrapedItem(ScrapedItem):\n         'url': basestring,  # the main URL where this item was scraped from\n     }\n \n-    def __init__(self, data=None):\n+    def __init__(self, data=None, adaptors_dict=None):\n         super(RobustScrapedItem, self).__init__(data)\n+        self.__dict__['_adaptors_dict'] = adaptors_dict or {}\n         self.__dict__['_version'] = None\n \n     def __getattr__(self, attr):\n@@ -107,16 +110,104 @@ class RobustScrapedItem(ScrapedItem):\n     def __str__(self) :\n         return \"%s: GUID=%s, url=%s\" % ( self.__class__.__name__ , self.guid, self.url )\n \n+    def _add_single_attributes(self, attrname, attrtype, attributes):\n+        if len(attributes) == 1:\n+            return attributes[0]\n+\n+        if attrtype is basestring:\n+            return ''.join(attributes)\n+        else:\n+           raise NotImplementedError('You must override _add_single_attributes method in order to join %s values into a single value.' % attrtype.__name__)\n+\n+    def _clean_values(self, values):\n+        ret = []\n+        for val in values:\n+            if isinstance(val, tuple):\n+                ret.extend(val)\n+            elif val:\n+                ret.append(val)\n+        return ret\n+\n+    def attribute(self, attrname, *values, **kwargs):\n+        \"\"\"\n+        Set the provided values to the provided attribute (`attrname`) by filtering them\n+        through its adaptor pipeline first (if any).\n+\n+        If the attribute had been already set, it won't be overwritten unless the parameters\n+        `override` or `add` are True.\n+\n+        If `override` is True, the old value simply will be replaced with the new one.\n+\n+        If add is True (and there was an old value, of course), for multivalued attributes the values\n+        will be appended to the list of the already existing ones.\n+        For this same situation in single-valued attributes, the method _add_single_attributes will be\n+        called with the attribute's name and type, and the list of values to join as parameters.\n+\n+        The kwargs parameter is passed to the adaptors pipeline, which manages to transmit\n+        it to the adaptors themselves.\n+        \"\"\"\n+        if not values:\n+            raise UsageError(\"You must specify at least one value when setting an attribute\")\n+        if attrname not in self.ATTRIBUTES:\n+            raise AttributeError('Attribute \"%s\" is not a valid attribute name. You must add it to %s.ATTRIBUTES' % (attrname, self.__class__.__name__))\n+\n+        add = kwargs.pop('add', False)\n+        override = kwargs.pop('override', False)\n+        unique_vals = kwargs.pop('unique', False)\n+        old_value = getattr(self, attrname, None)\n+        if old_value and not any([override, add]):\n+            return\n+\n+        attrtype = self.ATTRIBUTES.get(attrname)\n+        multivalued = isinstance(attrtype, list)\n+        adaptors_pipe = self._adaptors_dict.get(attrname)\n+        new_values = [adaptors_pipe(value, kwargs) for value in values] if adaptors_pipe else [values]\n+        new_values = self._clean_values(new_values)\n+\n+        if old_value and not override:\n+            if multivalued:\n+                new_values = old_value + new_values\n+            else:\n+                new_values.insert(0, old_value)\n+\n+        if not multivalued:\n+            if add and len(new_values) >= 1:\n+                new_values = self._add_single_attributes(attrname, attrtype, new_values)\n+            else:\n+                new_values = new_values[0] if new_values else None\n+        elif multivalued and unique_vals:\n+            new_values = unique(new_values)\n+\n+        setattr(self, attrname, new_values)\n+\n+    def set_attrib_adaptors(self, attrib, pipe):\n+        \"\"\" Set the adaptors (from a list or tuple) to be used for a specific attribute. \"\"\"\n+        self._adaptors_dict[attrib] = pipe if isinstance(pipe, AdaptorPipe) else AdaptorPipe(pipe)\n+\n+    def set_adaptors(self, adaptors_dict):\n+        \"\"\"\n+        Set the adaptors to use for this item. Receives a dict of the adaptors\n+        desired for each attribute and returns the item itself.\n+        \"\"\"\n+        self._adaptors_dict = {}\n+        for attrib, pipe in adaptors_dict.items():\n+            self.set_attrib_adaptors(attrib, pipe)\n+        return self\n+\n+    def add_adaptor(self, attrib, adaptor, position=None):\n+        \"\"\"\n+        Add an adaptor for the specified attribute at the given position.\n+        If position = None, then the adaptor is appended at the end of the pipeline.\n+        \"\"\"\n+        pipe = self._adaptors_dict.get(attrib, AdaptorPipe([]))\n+        pipe.add_adaptor(adaptor, position)\n+        self.set_attrib_adaptors(attrib, pipe)\n+\n     def validate(self):\n         \"\"\"Method used to validate item attributes data\"\"\"\n         if not self.guid:\n             raise ValidationError('A guid is required')\n \n-    def copy(self):\n-        \"\"\"Create a new ScrapedItem object based on the current one\"\"\"\n-        import copy\n-        return copy.deepcopy(self)\n-\n     @property\n     def version(self):\n         \"\"\"\n\n\n@@ -0,0 +1,4 @@\n+from scrapy.contrib_exp.adaptors.extraction import extract, ExtractImageLinks\n+from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, Unquote\n+from scrapy.contrib_exp.adaptors.misc import to_unicode, clean_spaces, strip, drop_empty, Delist, Regex\n+from scrapy.utils.python import flatten\n\n@@ -0,0 +1,78 @@\n+\"\"\"\n+Adaptors related with extraction of data\n+\"\"\"\n+\n+import urlparse\n+import re\n+from scrapy import log\n+from scrapy.http import Response\n+from scrapy.utils.url import is_url\n+from scrapy.utils.response import get_base_url\n+from scrapy.utils.python import flatten, unicode_to_str\n+from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n+from scrapy.contrib.link_extractors import HTMLImageLinkExtractor\n+\n+def extract(location, adaptor_args=None):\n+    \"\"\"\n+    This adaptor tries to extract data from the given locations.\n+    Any XPathSelector in it will be extracted, and any other data\n+    will be added as-is to the result.\n+\n+    If an XPathSelector is a text/cdata node, and `use_unquote`\n+    is True, that selector will be extracted using the `extract_unquoted`\n+    method; otherwise, the `extract` method will be used.\n+\n+    Input: anything\n+    Output: tuple of extracted selectors plus anything else in the input\n+    \"\"\"\n+    use_unquote = adaptor_args.get('use_unquote', True) if adaptor_args else True\n+\n+    if isinstance(location, XPathSelectorList):\n+        ret = location.extract()\n+    elif isinstance(location, XPathSelector):\n+        if location.xmlNode.type in ('text', 'cdata') and use_unquote:\n+            ret = location.extract_unquoted()\n+        else:\n+            ret = location.extract()\n+        ret = ret if hasattr(ret, '__iter__') else [ret]\n+    elif isinstance(location, list):\n+        ret = tuple(location)\n+    else:\n+        ret = tuple([location])\n+\n+    return tuple(x for x in ret if x is not None)\n+\n+class ExtractImageLinks(object):\n+    \"\"\"\n+    This adaptor may receive either XPathSelectors pointing to\n+    the desired locations for finding image urls, or just a list of\n+    XPath expressions (which will be turned into selectors anyway).\n+\n+    Input: XPathSelector, XPathSelectorList, iterable\n+    Output: tuple of urls (strings)\n+    \"\"\"\n+    def __init__(self, response=None, canonicalize=True):\n+        self.response = response\n+        self.canonicalize = canonicalize\n+\n+    def __call__(self, locations):\n+        if not locations:\n+            return tuple()\n+        elif not hasattr(locations, '__iter__'):\n+            locations = [locations]\n+\n+        selectors, raw_links = [], []\n+        for location in locations:\n+            if isinstance(location, (XPathSelector, XPathSelectorList)):\n+                selectors.append(location)\n+            else:\n+                raw_links.append(location)\n+\n+        if raw_links:\n+            base_url = get_base_url(self.response)\n+            raw_links = [urlparse.urljoin(base_url, unicode_to_str(rel_url)) for rel_url in raw_links]\n+\n+        lx = HTMLImageLinkExtractor(locations=selectors, canonicalize=self.canonicalize)\n+        urls = map(lambda link: link.url, lx.extract_links(self.response))\n+        return tuple(urls + raw_links)\n+\n\n@@ -0,0 +1,42 @@\n+import re\n+from scrapy.utils.markup import replace_tags, unquote_markup\n+from scrapy.utils.python import str_to_unicode\n+from scrapy.item.adaptors import adaptize\n+\n+def remove_tags(value):\n+    \"\"\"\n+    Removes any tags found in each of the provided list's string.\n+    E.g:\n+      >> remove_tags('<head>my header</head>', '<body>my <b>body</b></body>')\n+      u'my header', u'my body'\n+    Input: string/unicode\n+    Output: unicode\n+    \"\"\"\n+    return replace_tags(value)\n+\n+_remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n+def remove_root(value):\n+    \"\"\"\n+    Input: string/unicode\n+    Output: unicode\n+    \"\"\"\n+    m = _remove_root_re.search(value)\n+    if m:\n+        value = m.group(1)\n+    return str_to_unicode(value)\n+\n+class Unquote(object):\n+    \"\"\"\n+    Receives a list of strings, removes all of the\n+    CDATAs and entities (except the ones in CDATAs) the strings\n+    may have, and returns a new list.\n+\n+    Input: string/unicode\n+    Output: string/unicode\n+    \"\"\"\n+    def __init__(self, keep=None):\n+        self.keep = [] if keep is None else keep\n+\n+    def __call__(self, value, adaptor_args):\n+        keep = adaptor_args.get('keep_entities', self.keep)\n+        return unquote_markup(value, keep=keep)\n\n@@ -2,12 +2,11 @@\n \n import re\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n-from scrapy.utils.url import canonicalize_url\n from scrapy.utils.misc import extract_regex\n from scrapy.utils.python import flatten, str_to_unicode, unicode_to_str\n from scrapy.item.adaptors import adaptize\n \n-def to_unicode(value):\n+def to_unicode(value, adaptor_args):\n     \"\"\"\n     Receives a list of strings, converts\n     it to unicode, and returns a new list.\n@@ -18,11 +17,11 @@ def to_unicode(value):\n     Input: iterable of strings\n     Output: list of unicodes\n     \"\"\"\n-    if hasattr(value, '__iter__'):\n-        return [ str_to_unicode(v) if isinstance(v, basestring) else str_to_unicode(str(v)) for v in value ]\n-    else:\n-        raise TypeError('to_unicode must receive an iterable.')\n+    if not isinstance(value, basestring):\n+        value = str(value)\n+    return str_to_unicode(value, adaptor_args.get('encoding', 'utf-8'))\n \n+_clean_spaces_re = re.compile(\"\\s+\", re.U)\n def clean_spaces(value):\n     \"\"\"\n     Converts multispaces into single spaces for each string\n@@ -34,26 +33,20 @@ def clean_spaces(value):\n     Input: iterable of unicodes\n     Output: list of unicodes\n     \"\"\"\n-    _clean_spaces_re = re.compile(\"\\s+\", re.U)\n-    return [ _clean_spaces_re.sub(' ', str_to_unicode(v)) for v in value ]\n+    return _clean_spaces_re.sub(' ', str_to_unicode(value))\n \n def strip(value):\n     \"\"\"\n     Removes any spaces at both the start and the ending\n-    of the provided string or list.\n+    of the provided string.\n     E.g:\n-      >> strip([' hi   ', 'buddies  '])\n-      [u'hi', u'buddies']\n       >> strip(' hi buddies    ')\n       u'hi buddies'\n \n-    Input: unicode or iterable of unicodes\n-    Output: unicode or list of unicodes\n+    Input: string/unicode\n+    Output: string/unicode\n     \"\"\"\n-    if isinstance(value, basestring):\n-        return unicode(value.strip())\n-    elif hasattr(value, '__iter__'):\n-        return [ unicode(v.strip()) for v in value ]\n+    return value.strip()\n \n def drop_empty(value):\n     \"\"\"\n@@ -68,36 +61,20 @@ def drop_empty(value):\n     \"\"\"\n     return [ v for v in value if v ]\n \n-def canonicalize_urls(value):\n-    \"\"\"\n-    Canonicalizes each url in the list you provide.\n-    To see what this implies, check out canonicalize_url's\n-    docstring, at scrapy.utils.url.py\n-\n-    Input: iterable of unicodes(urls)\n-    Output: list of unicodes(urls)\n-    \"\"\"\n-    if hasattr(value, '__iter__'):\n-        return [canonicalize_url(unicode_to_str(url)) for url in value]\n-    elif isinstance(value, basestring):\n-        return canonicalize_url(unicode_to_str(value))\n-    return ''\n-\n class Delist(object):\n     \"\"\"\n     Joins a list with the specified delimiter\n     in the adaptor's constructor.\n \n-    Input: iterable of strings\n-    Output: unicode\n+    Input: iterable of strings/unicodes\n+    Output: string/unicode\n     \"\"\"\n     def __init__(self, delimiter=''):\n         self.delimiter = delimiter\n \n-    def __call__(self, value, delimiter=None):\n-        if delimiter is not None:\n-            self.delimiter = delimiter\n-        return self.delimiter.join(value)\n+    def __call__(self, value, adaptor_args):\n+        delimiter = adaptor_args.get('join_delimiter', self.delimiter)\n+        return delimiter.join(value)\n \n class Regex(object):\n     \"\"\"\n\n@@ -3,6 +3,38 @@ import inspect\n from traceback import format_exc\n from scrapy.conf import settings\n \n+class AdaptorFunc(object):\n+    \"\"\"\n+    This is a private class that the AdaptorPipe uses to store\n+    its adaptation functions and their attributes.\n+    \"\"\"\n+    def __init__(self, adaptor):\n+        self.adaptor = adaptor\n+\n+        if inspect.isfunction(adaptor):\n+            self.func_args, _, _, _ = inspect.getargspec(adaptor)\n+        elif hasattr(adaptor, '__call__'):\n+            try:\n+                self.func_args, _, _, _ = inspect.getargspec(adaptor.__call__)\n+            except Exception:\n+                self.func_args = []\n+\n+        self.name = getattr(adaptor, 'func_name', None)\n+        if not self.name:\n+            if hasattr(adaptor, '__class__') and adaptor.__class__ is not type:\n+                self.name = adaptor.__class__.__name__\n+            else:\n+                self.name = adaptor.__name__\n+\n+    def __call__(self, value, kwargs):\n+        if 'adaptor_args' in self.func_args:\n+            return self.adaptor(value, kwargs)\n+        else:\n+            return self.adaptor(value)\n+\n+    def __repr__(self):\n+        return '<Adaptor %s >' % self.name\n+\n class AdaptorPipe(list):\n     \"\"\"\n     Class that represents an item's attribute pipeline.\n@@ -11,44 +43,46 @@ class AdaptorPipe(list):\n     in order to filter the input.\n     \"\"\"\n     def __init__(self, adaptors):\n+        def _filter_adaptor(adaptor):\n+            return AdaptorFunc(adaptor) if not isinstance(adaptor, AdaptorFunc) else adaptor\n+\n+        if not hasattr(adaptors, '__iter__'):\n+            raise TypeError('You must provide AdaptorPipe a list of adaptors')\n+\n         for adaptor in adaptors:\n             if not callable(adaptor):\n                 raise TypeError(\"%s is not a callable\" % adaptor)\n-        super(AdaptorPipe, self).__init__(adaptors)\n+        super(AdaptorPipe, self).__init__(map(_filter_adaptor, adaptors))\n         self.debug = settings.getbool('ADAPTORS_DEBUG')\n \n-    def __call__(self, value, **kwargs):\n+    def __call__(self, value, kwargs=None):\n         \"\"\"\n         Execute the adaptor pipeline for this attribute.\n         \"\"\"\n \n+        values = [value]\n         for adaptor in self:\n-            if inspect.isfunction(adaptor):\n-                func_args, _, _ ,_ = inspect.getargspec(adaptor)\n-            else:\n-                func_args = []\n-\n-            name = getattr(adaptor, 'func_name', None)\n-            if not name:\n-                name = adaptor.__class__.__name__ if hasattr(adaptor, '__class__') else adaptor.__name__\n-\n+            next_round = []\n+            for val in values:\n                 try:\n                     if self.debug:\n-                    print \"  %07s | input >\" % name, repr(value)\n+                        print \"  %07s | input >\" % adaptor.name, repr(val)\n \n-                if 'adaptor_args' in func_args:\n-                    value = adaptor(value, kwargs)\n+                    val = adaptor(val, kwargs or {})\n+                    if isinstance(val, tuple):\n+                        next_round.extend(val)\n                     else:\n-                    value = adaptor(value)\n+                        next_round.append(val)\n \n                     if self.debug:\n-                    print \"  %07s | output >\" % name, repr(value)\n+                        print \"  %07s | output >\" % adaptor.name, repr(val)\n                 except Exception:\n-                print \"Error in '%s' adaptor. Traceback text:\" % name\n+                    print \"Error in '%s' adaptor. Traceback text:\" % adaptor.name\n                     print format_exc()\n                     return\n+            values = next_round\n \n-        return value\n+        return tuple(values)\n \n     def __add__(self, other):\n         if callable(other):\n@@ -60,6 +94,15 @@ class AdaptorPipe(list):\n     def __repr__(self):\n         return '<AdaptorPipe %s >' % super(AdaptorPipe, self).__repr__()\n \n+    def add_adaptor(self, adaptor, position=None):\n+        if callable(adaptor):\n+            if not isinstance(adaptor, AdaptorFunc):\n+                adaptor = AdaptorFunc(adaptor)\n+            if position is None:\n+                self.append(adaptor)\n+            else:\n+                self.insert(position, adaptor)\n+\n def adaptize(adaptor, my_args=None):\n     \"\"\"\n     This decorator helps you add to your pipelines adaptors that are able\n\n@@ -1,5 +1,4 @@\n-from scrapy.conf import settings\n-from scrapy.item.adaptors import AdaptorPipe\n+import copy\n \n class ScrapedItem(object):\n     \"\"\"\n@@ -13,8 +12,6 @@ class ScrapedItem(object):\n         A ScrapedItem can be initialised with a dictionary that will be\n         squirted directly into the object.\n         \"\"\"\n-        self._adaptors_dict = {}\n-\n         if isinstance(data, dict):\n             for attr, value in data.iteritems():\n                 setattr(self, attr, value)\n@@ -32,69 +29,9 @@ class ScrapedItem(object):\n     def __sub__(self, other):\n         raise NotImplementedError\n \n-    def set_adaptors(self, adaptors_dict):\n-        \"\"\"\n-        Set the adaptors to use for this item. Receives a dict of the adaptors\n-        desired for each attribute and returns the item itself.\n-        \"\"\"\n-        _adaptors_dict = dict((attrib, AdaptorPipe(pipe)) for attrib, pipe in adaptors_dict.items() if hasattr(pipe, '__iter__'))\n-        self.__dict__['_adaptors_dict'] = _adaptors_dict\n-        return self\n-    \n-    def set_attrib_adaptors(self, attrib, pipe):\n-        \"\"\" Set the adaptors (from a list or tuple) to be used for a specific attribute. \"\"\"\n-        self._adaptors_dict[attrib] = AdaptorPipe(pipe) if hasattr(pipe, '__iter__') else None\n-\n-    def add_adaptor(self, attrib, adaptor, position=None):\n-        \"\"\"\n-        Add an adaptor for the specified attribute at the given position.\n-        If position = None, then the adaptor is appended at the end of the pipeline.\n-        \"\"\"\n-        if callable(adaptor):\n-            pipe = self._adaptors_dict.get(attrib, [])\n-            if position is None:\n-                pipe = pipe + [adaptor]\n-            else:\n-                pipe.insert(position, adaptor)\n-            self.set_attrib_adaptors(attrib, pipe)\n-\n-    def attribute(self, attrname, value, override=False, add=False, **kwargs):\n-        \"\"\"\n-        Set the given value to the provided attribute (`attrname`) by filtering it\n-        through its adaptor pipeline first (if it has any).\n-\n-        If the attribute has been already set, it won't be overwritten unless\n-        `override` is True.\n-        If there was an old value and `add` is True (but `override` isn't), `value`\n-        will be appended/extended (depending on its type), to the old value, as long as\n-        the old value is a list.\n-        If both of the values are strings they will be joined by using the `add` delimiter\n-        (which may be a string, or True, in which case '' will be used as the delimiter).\n-\n-        The kwargs parameter is passed to the adaptors pipeline, which manages to transmit\n-        it to the adaptors themselves.\n-        \"\"\"\n-        pipe = self._adaptors_dict.get(attrname)\n-        old_value = getattr(self, attrname, None)\n-\n-        if pipe:\n-            value = pipe(value, **kwargs)\n-\n-        if old_value:\n-            if override:\n-                setattr(self, attrname, value)\n-            elif add:\n-                if hasattr(old_value, '__iter__'):\n-                    if hasattr(value, '__iter__'):\n-                        self.__dict__[attrname].extend(list(value))\n-                    else:\n-                        self.__dict__[attrname].append(value)\n-                elif isinstance(old_value, basestring) and isinstance(value, basestring):\n-                    delimiter = add if isinstance(add, basestring) else ''\n-                    setattr(self, attrname, '%s%s%s' % (old_value, delimiter, value))\n-        else:\n-            setattr(self, attrname, value)\n-\n+    def copy(self):\n+        \"\"\"Create a new ScrapedItem based on the current one\"\"\"\n+        return copy.deepcopy(self)\n \n class ItemDelta(object):\n     pass\n\n@@ -4,9 +4,10 @@ import unittest\n import re\n \n from scrapy.item.adaptors import AdaptorPipe\n-from scrapy.contrib import adaptors\n+from scrapy.contrib_exp import adaptors\n from scrapy.http import Response, Headers\n from scrapy.xpath.selector import HtmlXPathSelector, XmlXPathSelector\n+from scrapy.link import Link\n \n class AdaptorPipeTestCase(unittest.TestCase):\n     def test_pipe_init(self):\n@@ -20,9 +21,9 @@ class AdaptorPipeTestCase(unittest.TestCase):\n \n         sample_value = 'hi, this is my text:'\n         sample_pipe = AdaptorPipe([sample_adaptor])\n-        self.assertEqual(sample_pipe(sample_value), 'hi, this is my text: \"sample text 1\"')\n-        self.assertEqual(sample_pipe(sample_value, sample_arg='foobarfoobar'),\n-            'hi, this is my text: \"foobarfoobar\"')\n+        self.assertEqual(sample_pipe(sample_value), ('hi, this is my text: \"sample text 1\"', ))\n+        self.assertEqual(sample_pipe(sample_value, {'sample_arg': 'foobarfoobar'}),\n+            ('hi, this is my text: \"foobarfoobar\"', ))\n \n     def test_add(self):\n         pipe1 = AdaptorPipe([adaptors.extract])\n@@ -51,21 +52,21 @@ class AdaptorsTestCase(unittest.TestCase):\n             eurore = re.compile(r'<span class=\"euro\" .*?>(.*?)</span>')\n \n             if pound:\n-                self.assertEqual(adaptors.extract(x.x(\"//span[@class='pound']/text()\")), [u'\\xa3'])\n-                self.assertEqual(adaptors.extract(x.x(\"//span[@class='pound']/@value\")), [u'\\xa3'])\n-                self.assertEqual(adaptors.extract(x.re(poundre)), [u'\\xa3'])\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundent']/text()\")), [u'\\xa3'])\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundent']/@value\")), [u'\\xa3'])\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundnum']/text()\")), [u'\\xa3'])\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundnum']/@value\")), [u'\\xa3'])\n+                self.assertEqual(adaptors.extract(x.x(\"//span[@class='pound']/text()\")), (u'\\xa3', ))\n+                self.assertEqual(adaptors.extract(x.x(\"//span[@class='pound']/@value\")), (u'\\xa3', ))\n+                self.assertEqual(adaptors.extract(x.re(poundre)), (u'\\xa3', ))\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundent']/text()\")), (u'\\xa3', ))\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundent']/@value\")), (u'\\xa3', ))\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundnum']/text()\")), (u'\\xa3', ))\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundnum']/@value\")), (u'\\xa3', ))\n             if euro:\n-                self.assertEqual(adaptors.extract(x.x(\"//span[@class='euro']/text()\")), [u'\\u20ac'])\n-                self.assertEqual(adaptors.extract(x.x(\"//span[@class='euro']/@value\")), [u'\\u20ac'])\n-                self.assertEqual(adaptors.extract(x.re(eurore)), [u'\\u20ac'])\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euroent']/text()\")), [u'\\u20ac'])\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euroent']/@value\")), [u'\\u20ac'])\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euronum']/text()\")), [u'\\u20ac'])\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euronum']/@value\")), [u'\\u20ac'])\n+                self.assertEqual(adaptors.extract(x.x(\"//span[@class='euro']/text()\")), (u'\\u20ac', ))\n+                self.assertEqual(adaptors.extract(x.x(\"//span[@class='euro']/@value\")), (u'\\u20ac', ))\n+                self.assertEqual(adaptors.extract(x.re(eurore)), (u'\\u20ac', ))\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euroent']/text()\")), (u'\\u20ac', ))\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euroent']/@value\")), (u'\\u20ac', ))\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euronum']/text()\")), (u'\\u20ac', ))\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euronum']/@value\")), (u'\\u20ac', ))\n \n         x = self.get_selector('example.com',\n                          'http://www.example.com/test/utf8',\n@@ -105,15 +106,15 @@ class AdaptorsTestCase(unittest.TestCase):\n         x = self.get_selector('example.com', 'http://www.example.com/test_unquoted', 'extr_unquoted.xml', selector=XmlXPathSelector)\n \n         # test unquoting\n-        self.assertEqual(adaptors.extract(x.x('//tag1/text()')), [u'test text & &', u'more test text &amp; &gt;', u'blah&blah'])\n-        self.assertEqual(adaptors.extract(x.x('//tag2/text()')), [u'blaheawfds<'])\n+        self.assertEqual(adaptors.extract(x.x('//tag1/text()[2]')[0]), (u'more test text &amp; &gt;', ))\n+        self.assertEqual(adaptors.extract(x.x('//tag2/text()[1]')[0]), (u'blaheawfds<', ))\n \n         # test without unquoting\n         self.assertEqual(adaptors.extract(x.x('//tag1/text()'), {'use_unquote': False}),\n-            [u'test text &amp; &amp;', u'<![CDATA[more test text &amp; &gt;]]>', u'blah&amp;blah'])\n-        self.assertEqual(adaptors.extract(x.x('//tag2/text()'), {'use_unquote': False}), [u'blaheawfds&lt;'])\n+            (u'test text &amp; &amp;', u'<![CDATA[more test text &amp; &gt;]]>', u'blah&amp;blah'))\n+        self.assertEqual(adaptors.extract(x.x('//tag2/text()'), {'use_unquote': False}), (u'blaheawfds&lt;', ))\n \n-    def test_extract_links(self):\n+    def test_extract_image_links(self):\n         test_data = \"\"\"<html><body>\n                          <div>\n                            <a href=\"lala1/lala1.html\">lala1</a>\n@@ -125,26 +126,34 @@ class AdaptorsTestCase(unittest.TestCase):\n                          </div>\n                        </body></html>\"\"\"\n         sample_response = Response('http://foobar.com/dummy', body=test_data)\n-        sample_xsel = XmlXPathSelector(sample_response)\n-        sample_adaptor = adaptors.ExtractImages(response=sample_response)\n-\n-        self.assertEqual(sample_adaptor(None), [])\n-        self.assertEqual(sample_adaptor([]), [])\n-        self.assertEqual(sample_adaptor('http://foobar.com/my_image.jpg'), ['http://foobar.com/my_image.jpg'])\n-        self.assertEqual(sample_adaptor([sample_xsel.x('//@href'), 'my_image.gif']),\n-                         [u'http://foobar.com/lala1/lala1.html', u'http://foobar.com/lala2.html',\n-                          u'http://foobar.com/pepepe/papapa/lala3.html', u'http://foobar.com/lala4.html', u'http://foobar.com/my_image.gif'])\n-        self.assertEqual(sample_adaptor(sample_xsel.x('//a')),\n-                         [u'http://foobar.com/lala1/lala1.html', u'http://foobar.com/lala2.html',\n-                          u'http://foobar.com/pepepe/papapa/lala3.html', u'http://foobar.com/imgs/lala4.jpg'])\n-        self.assertEqual(sample_adaptor(sample_xsel.x('//a[@onclick]').re(r'opensomething\\(\\'(.*?)\\'\\)')),\n-                         [u'http://foobar.com/my_html1.html', u'http://foobar.com/dummy/my_html2.html'])\n+        sample_selector = HtmlXPathSelector(sample_response)\n+        sample_adaptor = adaptors.ExtractImageLinks(response=sample_response)\n \n+        self.assertEqual(sample_adaptor(None), tuple())\n+        self.assertEqual(sample_adaptor([]), tuple())\n+        self.assertEqual(sample_adaptor([sample_selector.x('//@href'), 'http://foobar.com/my_image.gif']), (\n+            'http://foobar.com/lala1/lala1.html',\n+            'http://foobar.com/lala2.html',\n+            'http://foobar.com/pepepe/papapa/lala3.html',\n+            'http://foobar.com/lala4.html',\n+            'http://foobar.com/my_image.gif',\n+        ))\n+        self.assertEqual(sample_adaptor(sample_selector.x('//a')), (\n+            'http://foobar.com/lala1/lala1.html',\n+            'http://foobar.com/lala2.html',\n+            'http://foobar.com/pepepe/papapa/lala3.html',\n+            'http://foobar.com/imgs/lala4.jpg',\n+        ))\n+        self.assertEqual(sample_adaptor(sample_selector.x('//a[@onclick]').re(r'opensomething\\(\\'(.*?)\\'\\)')), (\n+            'http://foobar.com/my_html1.html',\n+            'http://foobar.com/dummy/my_html2.html'\n+        ))\n \n     def test_to_unicode(self):\n-        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'lulu\\xc3\\xb1', 1, '\\xc3\\xa1\\xc3\\xa9']),\n-                         [u'lala', u'lele', u'lulu\\xf1', u'1', u'\\xe1\\xe9'])\n-\n+        self.assertEqual(adaptors.to_unicode('lelelulu\\xc3\\xb1', {}), u'lelelulu\\xf1')\n+        self.assertEqual(adaptors.to_unicode(1, {}), u'1')\n+        self.assertEqual(adaptors.to_unicode('\\xc3\\xa1\\xc3\\xa9', {'encoding': 'utf-8'}), u'\\xe1\\xe9')\n+        self.assertEqual(adaptors.to_unicode('\\xf1', {'encoding': 'latin1'}), u'\\xf1')\n \n     def test_regex(self):\n         adaptor = adaptors.Regex(regex=r'href=\"(.*?)\"')\n@@ -152,44 +161,40 @@ class AdaptorsTestCase(unittest.TestCase):\n                                   '<a href=\"das.biz\">href=\"lelelel.net\"</a>']),\n                                   ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])\n \n-\n     def test_unquote_all(self):\n-        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n-\n+        unquote = adaptors.Unquote()\n+        self.assertEqual(unquote(u'hello&copy;&amp;welcome&lt;br /&gt;&amp;', {}),\n+            u'hello\\xa9&welcome<br />&')\n \n     def test_unquote(self):\n-        self.assertEqual(adaptors.Unquote(keep=['amp', 'lt'])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n-\n+        unquote = adaptors.Unquote(keep=['amp', 'lt'])\n+        self.assertEqual(unquote(u'hello&copy;&amp;welcome&lt;br /&gt;&amp;', {}),\n+            u'hello\\xa9&amp;welcome&lt;br />&amp;')\n \n     def test_remove_tags(self):\n-        test_data = ['<a href=\"lala\">adsaas<br /></a>', '<div id=\"1\"><table>dsadasf</table></div>']\n-        self.assertEqual(adaptors.remove_tags(test_data), ['adsaas', 'dsadasf'])\n-\n+        self.assertEqual(adaptors.remove_tags('<a href=\"lala\">adsaas<br /></a>'), 'adsaas')\n+        self.assertEqual(adaptors.remove_tags('<div id=\"1\"><table>dsadasf</table></div>'), 'dsadasf')\n \n     def test_remove_root(self):\n-        self.assertEqual(adaptors.remove_root(['<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>']),\n-                         ['lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br />'])\n-\n+        self.assertEqual(adaptors.remove_root('<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>'),\n+            'lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br />')\n \n     def test_remove_multispaces(self):\n-        self.assertEqual(adaptors.clean_spaces(['  hello,  whats     up?', 'testing testingtesting      testing']),\n-                         [' hello, whats up?', 'testing testingtesting testing'])\n-\n+        self.assertEqual(adaptors.clean_spaces('  hello,  whats     up?'), ' hello, whats up?')\n \n     def test_strip(self):\n-        self.assertEqual(adaptors.strip([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),\n-                         ['hi there, sweety ;D', 'I CAN HAZ TEST??'])\n         self.assertEqual(adaptors.strip('      hello there, this is my test     '),\n             'hello there, this is my test')\n \n-\n     def test_drop_empty_elements(self):\n         self.assertEqual(adaptors.drop_empty([1, 2, None, 5, 0, 6, False, 'hi']),\n             [1, 2, 5, 6, 'hi'])\n \n-\n     def test_delist(self):\n-        self.assertEqual(adaptors.Delist()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n+        delist = adaptors.Delist(' ')\n+        self.assertEqual(delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.'], {}),\n             'hi there fellas. this is my test.')\n+        self.assertEqual(delist(['hi', 'there', 'fellas,', 'this', 'is', 'my', 'test.'], {'join_delimiter': '.'}),\n+            'hi.there.fellas,.this.is.my.test.')\n \n \n\n@@ -0,0 +1,94 @@\n+# -*- coding: utf8 -*-\n+import unittest\n+\n+from scrapy.contrib.item import RobustScrapedItem\n+from scrapy.item.adaptors import AdaptorPipe, AdaptorFunc\n+from scrapy.contrib_exp import adaptors\n+from scrapy.core.exceptions import UsageError\n+\n+class MyItem(RobustScrapedItem):\n+    ATTRIBUTES = {\n+        'guid': basestring,\n+        'name': basestring,\n+        'children': [basestring],\n+        'age': int,\n+        'alive': bool,\n+    }\n+\n+class RobustScrapedItemTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.item = MyItem()\n+\n+    def test_attribute_basic(self):\n+        self.assertRaises(UsageError, self.item.attribute, 'foo')\n+        self.assertRaises(AttributeError, self.item.attribute, 'foo', 'something')\n+\n+        self.item.attribute('name', 'John')\n+        self.assertEqual(self.item.name, 'John')\n+\n+        self.assertRaises(TypeError, self.item.attribute, 'children', ['Peter'])\n+        self.item.attribute('children', *['Peter'])\n+        self.assertEqual(self.item.children, ['Peter'])\n+\n+        self.item.attribute('age', 40)\n+        self.assertEqual(self.item.age, 40)\n+\n+        self.item.attribute('alive', False)\n+        self.assertEqual(self.item.alive, False)\n+\n+    def test_attribute_override(self):\n+        self.item.attribute('name', 'John')\n+        self.item.attribute('name', 'Charlie')\n+        self.assertEqual(self.item.name, 'John')\n+\n+        self.item.attribute('name', 'Charlie', override=True)\n+        self.assertEqual(self.item.name, 'Charlie')\n+\n+    def test_attribute_add(self):\n+        self.item.attribute('name', 'John')\n+\n+        self.item.attribute('name', 'Doe', add=True)\n+        self.assertEqual(self.item.name, 'JohnDoe')\n+\n+        self.item.attribute('children', *['Ken', 'Tom'])\n+        self.item.attribute('children', 'Bobby')\n+        self.item.attribute('children', 'Jimmy', add=True)\n+        self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy'])\n+\n+        self.item.attribute('children', *['Johnny', 'Rodrigo'], add=True)\n+        self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy', 'Johnny', 'Rodrigo'])\n+\n+    def test_set_adaptors(self):\n+        self.assertEqual(self.item._adaptors_dict, {})\n+\n+        delist = adaptors.Delist()\n+        self.item.set_adaptors({'name': [adaptors.extract, delist]})\n+        self.assertTrue(isinstance(self.item._adaptors_dict['name'], AdaptorPipe))\n+        self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n+        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"Delist\")\n+\n+        self.item.set_adaptors({'description': [adaptors.extract]})\n+        self.assertEqual(self.item._adaptors_dict['description'][0].name, \"extract\")\n+\n+    def test_set_attrib_adaptors(self):\n+        self.assertEqual(self.item._adaptors_dict, {})\n+\n+        self.item.set_attrib_adaptors('name', [adaptors.extract, adaptors.strip])\n+        self.assertTrue(isinstance(self.item._adaptors_dict['name'], AdaptorPipe))\n+        self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n+        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"strip\")\n+\n+        unquote = adaptors.Unquote()\n+        self.item.set_attrib_adaptors('name', [adaptors.extract, unquote])\n+        self.assertTrue(isinstance(self.item._adaptors_dict['name'], AdaptorPipe))\n+        self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n+        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"Unquote\")\n+\n+    def test_add_adaptor(self):\n+        self.assertEqual(self.item._adaptors_dict, {})\n+\n+        self.item.add_adaptor('name', adaptors.strip)\n+        self.assertEqual(self.item._adaptors_dict['name'][0].name, \"strip\")\n+        self.item.add_adaptor('name', adaptors.extract, position=0)\n+        self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n+        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"strip\")\n\n@@ -1,70 +0,0 @@\n-# -*- coding: utf8 -*-\n-import unittest\n-\n-from scrapy.item.models import ScrapedItem\n-from scrapy.item.adaptors import AdaptorPipe\n-from scrapy.contrib import adaptors\n-\n-class ScrapedItemTestCase(unittest.TestCase):\n-    def setUp(self):\n-        self.item = ScrapedItem()\n-\n-    def test_attribute_basic(self):\n-        self.item.attribute('name', 'John')\n-        self.assertEqual(self.item.name, 'John')\n-\n-    def test_attribute_override(self):\n-        self.item.attribute('name', 'John')\n-        self.item.attribute('name', 'Charlie')\n-        self.assertEqual(self.item.name, 'John')\n-\n-        self.item.attribute('name', 'Charlie', override=True)\n-        self.assertEqual(self.item.name, 'Charlie')\n-\n-    def test_attribute_add(self):\n-        self.item.attribute('name', 'John')\n-      \n-        self.item.attribute('name', 'Doe', add=True)\n-        self.assertEqual(self.item.name, 'JohnDoe')\n-\n-        self.item.attribute('name', 'Smith', add=' ')\n-        self.assertEqual(self.item.name, 'JohnDoe Smith')\n-\n-        self.item.attribute('children', ['Ken', 'Tom'])\n-        self.item.attribute('children', 'Bobby')\n-        self.item.attribute('children', 'Jimmy', add=True)\n-        self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy'])\n-\n-        self.item.attribute('children', ['Johnny', 'Rodrigo'], add=True)\n-        self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy', 'Johnny', 'Rodrigo'])\n-\n-    def test_set_adaptors(self):\n-        self.assertEqual(self.item._adaptors_dict, {})\n-\n-        delist = adaptors.Delist()\n-        self.item.set_adaptors({'name': [adaptors.extract, delist]})\n-        self.assertEqual(self.item._adaptors_dict, {'name': [adaptors.extract, delist]})\n-\n-        self.item.set_adaptors({'description': [adaptors.extract]})\n-        self.assertEqual(self.item._adaptors_dict, {'description': [adaptors.extract]})\n-\n-    def test_set_attrib_adaptors(self):\n-        self.assertEqual(self.item._adaptors_dict, {})\n-\n-        self.item.set_attrib_adaptors('name', [adaptors.extract, adaptors.strip])\n-        self.assertEqual(self.item._adaptors_dict['name'],\n-            AdaptorPipe([adaptors.extract, adaptors.strip]))\n-\n-        unquote = adaptors.Unquote()\n-        self.item.set_attrib_adaptors('name', [adaptors.extract, unquote])\n-        self.assertEqual(self.item._adaptors_dict['name'],\n-            AdaptorPipe([adaptors.extract, unquote]))\n-\n-    def test_add_adaptor(self):\n-        self.assertEqual(self.item._adaptors_dict, {})\n-\n-        self.item.add_adaptor('name', adaptors.strip)\n-        self.assertEqual(self.item._adaptors_dict['name'], [adaptors.strip])\n-        self.item.add_adaptor('name', adaptors.extract, position=0)\n-        self.assertEqual(self.item._adaptors_dict['name'], [adaptors.extract, adaptors.strip])\n-\n\n@@ -14,19 +14,19 @@ class UtilsMiscTestCase(unittest.TestCase):\n \n     def test_items_to_csv(self):\n         item_1 = ScrapedItem()\n-        item_1.attribute('_hidden', '543565')\n-        item_1.attribute('id', '3213')\n-        item_1.attribute('name', 'Item 1')\n-        item_1.attribute('description', 'Really cute item')\n-        item_1.attribute('url', 'http://dummyurl.com')\n+        item_1._hidden = '543565'\n+        item_1.id = '3213'\n+        item_1.name = 'Item 1'\n+        item_1.description = 'Really cute item'\n+        item_1.url = 'http://dummyurl.com'\n \n         item_2 = ScrapedItem()\n-        item_2.attribute('_hidden', 'lala')\n-        item_2.attribute('id', '1234')\n-        item_2.attribute('name', 'Item 2')\n-        item_2.attribute('description', 'This item rocks!')\n-        item_2.attribute('url', 'http://dummyurl.com/2')\n-        item_2.attribute('supplier', 'A random supplier')\n+        item_2._hidden = 'lala'\n+        item_2.id = '1234'\n+        item_2.name = 'Item 2'\n+        item_2.description = 'This item rocks!'\n+        item_2.url = 'http://dummyurl.com/2'\n+        item_2.supplier = 'A random supplier'\n \n         file = StringIO()\n         items_to_csv(file, [item_1, item_2])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
