{"custom_id": "keras#e5ccf535312626a2727cbfd15455c592357f7568", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 118 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -49,7 +49,7 @@ class TimeDistributed(MaskedLayer):\n \n         trainable_weights, regularizers, constraints, updates = self.layer.get_params()\n         self.trainable_weights = trainable_weights\n-        self.non_trainable_weights = self.layer.non_traible_weights\n+        self.non_trainable_weights = self.layer.non_trainable_weights\n         self.regularizers = regularizers\n         self.constraints = constraints\n         self.updates = updates\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be4a86f6dc61e7cffe3887666b8152fec81fded6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 70 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -60,6 +60,5 @@ def test_TimeDistributed():\n     model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 3)), nb_epoch=1, batch_size=10)\n \n \n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ebeff8ee304833a3df421d0de3b9f9480570eb9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 107 | Lines Deleted: 102 | Files Changed: 11 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/22 | Churn Δ: 209 | Churn Cumulative: 1908 | Contributors (this commit): 14 | Commits (past 90d): 29 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,7 @@ from keras.models import Sequential\n from keras.layers.embeddings import Embedding\n from keras.layers.core import Activation, Dense, Merge, Permute, Dropout\n from keras.layers.recurrent import LSTM\n-from keras.datasets.data_utils import get_file\n+from keras.utils.data_utils import get_file\n from keras.preprocessing.sequence import pad_sequences\n from functools import reduce\n import tarfile\n\n@@ -64,7 +64,7 @@ import tarfile\n import numpy as np\n np.random.seed(1337)  # for reproducibility\n \n-from keras.datasets.data_utils import get_file\n+from keras.utils.data_utils import get_file\n from keras.layers.embeddings import Embedding\n from keras.layers.core import Dense, Merge, Dropout, RepeatVector\n from keras.layers import recurrent\n\n@@ -14,7 +14,7 @@ from __future__ import print_function\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation, Dropout\n from keras.layers.recurrent import LSTM\n-from keras.datasets.data_utils import get_file\n+from keras.utils.data_utils import get_file\n import numpy as np\n import random\n import sys\n\n@@ -4,6 +4,7 @@ import sys\n from six.moves import cPickle\n from six.moves import range\n \n+\n def load_batch(fpath, label_key='labels'):\n     f = open(fpath, 'rb')\n     if sys.version_info < (3,):\n\n@@ -1,6 +1,6 @@\n from __future__ import absolute_import\n from .cifar import load_batch\n-from .data_utils import get_file\n+from ..utils.data_utils import get_file\n import numpy as np\n import os\n \n\n@@ -1,6 +1,6 @@\n from __future__ import absolute_import\n from .cifar import load_batch\n-from .data_utils import get_file\n+from ..utils.data_utils import get_file\n import numpy as np\n import os\n \n\n@@ -1,95 +1,4 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n+from ..utils.data_utils import *\n+import warnings\n \n-import tarfile\n-import os\n-import sys\n-import shutil\n-from six.moves.urllib.request import urlopen\n-from six.moves.urllib.error import URLError, HTTPError\n-\n-from ..utils.generic_utils import Progbar\n-\n-\n-# Under Python 2, 'urlretrieve' relies on FancyURLopener from legacy\n-# urllib module, known to have issues with proxy management\n-if sys.version_info[0] == 2:\n-    def urlretrieve(url, filename, reporthook=None, data=None):\n-        def chunk_read(response, chunk_size=8192, reporthook=None):\n-            total_size = response.info().get('Content-Length').strip()\n-            total_size = int(total_size)\n-            count = 0\n-            while 1:\n-                chunk = response.read(chunk_size)\n-                if not chunk:\n-                    break\n-                count += 1\n-                if reporthook:\n-                    reporthook(count, chunk_size, total_size)\n-                yield chunk\n-\n-        response = urlopen(url, data)\n-        with open(filename, 'wb') as fd:\n-            for chunk in chunk_read(response, reporthook=reporthook):\n-                fd.write(chunk)\n-else:\n-    from six.moves.urllib.request import urlretrieve\n-\n-\n-def get_file(fname, origin, untar=False):\n-    datadir_base = os.path.expanduser(os.path.join('~', '.keras'))\n-    if not os.access(datadir_base, os.W_OK):\n-        datadir_base = os.path.join('/tmp', '.keras')\n-    datadir = os.path.join(datadir_base, 'datasets')\n-    if not os.path.exists(datadir):\n-        os.makedirs(datadir)\n-\n-    if untar:\n-        untar_fpath = os.path.join(datadir, fname)\n-        fpath = untar_fpath + '.tar.gz'\n-    else:\n-        fpath = os.path.join(datadir, fname)\n-\n-    if not os.path.exists(fpath):\n-        print('Downloading data from',  origin)\n-        global progbar\n-        progbar = None\n-\n-        def dl_progress(count, block_size, total_size):\n-            global progbar\n-            if progbar is None:\n-                progbar = Progbar(total_size)\n-            else:\n-                progbar.update(count*block_size)\n-\n-        error_msg = 'URL fetch failure on {}: {} -- {}'\n-        try:\n-            try:\n-                urlretrieve(origin, fpath, dl_progress)\n-            except URLError as e:\n-                raise Exception(error_msg.format(origin, e.errno, e.reason))\n-            except HTTPError as e:\n-                raise Exception(error_msg.format(origin, e.code, e.msg))\n-        except (Exception, KeyboardInterrupt) as e:\n-            if os.path.exists(fpath):\n-                os.remove(fpath)\n-            raise e\n-        progbar = None\n-\n-    if untar:\n-        if not os.path.exists(untar_fpath):\n-            print('Untaring file...')\n-            tfile = tarfile.open(fpath, 'r:gz')\n-            try:\n-                tfile.extractall(path=datadir)\n-            except (Exception, KeyboardInterrupt) as e:\n-                if os.path.exists(untar_fpath):\n-                    if os.path.isfile(untar_fpath):\n-                        os.remove(untar_fpath)\n-                    else:\n-                        shutil.rmtree(untar_fpath)\n-                raise e\n-            tfile.close()\n-        return untar_fpath\n-\n-    return fpath\n+warnings.warn('data_utils has been moved to keras.utils.data_utils.')\n\\ No newline at end of file\n\n@@ -1,7 +1,7 @@\n from __future__ import absolute_import\n from six.moves import cPickle\n import gzip\n-from .data_utils import get_file\n+from ..utils.data_utils import get_file\n from six.moves import zip\n import numpy as np\n \n\n@@ -1,6 +1,6 @@\n # -*- coding: utf-8 -*-\n import gzip\n-from .data_utils import get_file\n+from ..utils.data_utils import get_file\n from six.moves import cPickle\n import sys\n \n\n@@ -1,6 +1,6 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n-from .data_utils import get_file\n+from ..utils.data_utils import get_file\n from six.moves import cPickle\n from six.moves import zip\n import numpy as np\n\n@@ -0,0 +1,95 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+\n+import tarfile\n+import os\n+import sys\n+import shutil\n+from six.moves.urllib.request import urlopen\n+from six.moves.urllib.error import URLError, HTTPError\n+\n+from ..utils.generic_utils import Progbar\n+\n+\n+# Under Python 2, 'urlretrieve' relies on FancyURLopener from legacy\n+# urllib module, known to have issues with proxy management\n+if sys.version_info[0] == 2:\n+    def urlretrieve(url, filename, reporthook=None, data=None):\n+        def chunk_read(response, chunk_size=8192, reporthook=None):\n+            total_size = response.info().get('Content-Length').strip()\n+            total_size = int(total_size)\n+            count = 0\n+            while 1:\n+                chunk = response.read(chunk_size)\n+                if not chunk:\n+                    break\n+                count += 1\n+                if reporthook:\n+                    reporthook(count, chunk_size, total_size)\n+                yield chunk\n+\n+        response = urlopen(url, data)\n+        with open(filename, 'wb') as fd:\n+            for chunk in chunk_read(response, reporthook=reporthook):\n+                fd.write(chunk)\n+else:\n+    from six.moves.urllib.request import urlretrieve\n+\n+\n+def get_file(fname, origin, untar=False):\n+    datadir_base = os.path.expanduser(os.path.join('~', '.keras'))\n+    if not os.access(datadir_base, os.W_OK):\n+        datadir_base = os.path.join('/tmp', '.keras')\n+    datadir = os.path.join(datadir_base, 'datasets')\n+    if not os.path.exists(datadir):\n+        os.makedirs(datadir)\n+\n+    if untar:\n+        untar_fpath = os.path.join(datadir, fname)\n+        fpath = untar_fpath + '.tar.gz'\n+    else:\n+        fpath = os.path.join(datadir, fname)\n+\n+    if not os.path.exists(fpath):\n+        print('Downloading data from',  origin)\n+        global progbar\n+        progbar = None\n+\n+        def dl_progress(count, block_size, total_size):\n+            global progbar\n+            if progbar is None:\n+                progbar = Progbar(total_size)\n+            else:\n+                progbar.update(count*block_size)\n+\n+        error_msg = 'URL fetch failure on {}: {} -- {}'\n+        try:\n+            try:\n+                urlretrieve(origin, fpath, dl_progress)\n+            except URLError as e:\n+                raise Exception(error_msg.format(origin, e.errno, e.reason))\n+            except HTTPError as e:\n+                raise Exception(error_msg.format(origin, e.code, e.msg))\n+        except (Exception, KeyboardInterrupt) as e:\n+            if os.path.exists(fpath):\n+                os.remove(fpath)\n+            raise e\n+        progbar = None\n+\n+    if untar:\n+        if not os.path.exists(untar_fpath):\n+            print('Untaring file...')\n+            tfile = tarfile.open(fpath, 'r:gz')\n+            try:\n+                tfile.extractall(path=datadir)\n+            except (Exception, KeyboardInterrupt) as e:\n+                if os.path.exists(untar_fpath):\n+                    if os.path.isfile(untar_fpath):\n+                        os.remove(untar_fpath)\n+                    else:\n+                        shutil.rmtree(untar_fpath)\n+                raise e\n+            tfile.close()\n+        return untar_fpath\n+\n+    return fpath\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a86057d91c5f82a40bf70cbb342dd93d6be631f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1009 | Contributors (this commit): 8 | Commits (past 90d): 31 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -378,9 +378,9 @@ def set_value(x, value):\n class Function(object):\n \n     def __init__(self, inputs, outputs, updates=[]):\n-        assert type(inputs) in {list, tuple}\n-        assert type(outputs) in {list, tuple}\n-        assert type(updates) in {list, tuple}\n+        assert type(inputs) in {list, tuple}, 'Input to a TensorFlow backend function should be a list or tuple.'\n+        assert type(outputs) in {list, tuple}, 'Output to a TensorFlow backend function should be a list or tuple.'\n+        assert type(updates) in {list, tuple}, 'Updates in a TensorFlow backend function should be a list or tuple.'\n         self.inputs = list(inputs)\n         self.outputs = list(outputs)\n         with tf.control_dependencies(self.outputs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cfcb1e87038e33d22fa4d8f38ec96c949767ebb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 19 | Files Changed: 3 | Hunks: 21 | Methods Changed: 14 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 45 | Churn Cumulative: 2867 | Contributors (this commit): 20 | Commits (past 90d): 55 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ from .common import _FLOATX, _EPSILON\n _SESSION = None\n \n \n-def _get_session():\n+def get_session():\n     global _SESSION\n     if _SESSION is None:\n         if not os.environ.get('OMP_NUM_THREADS'):\n@@ -19,7 +19,7 @@ def _get_session():\n     return _SESSION\n \n \n-def _set_session(session):\n+def set_session(session):\n     global _SESSION\n     _SESSION = session\n \n@@ -28,7 +28,7 @@ def _set_session(session):\n \n def variable(value, dtype=_FLOATX, name=None):\n     v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n-    _get_session().run(v.initializer)\n+    get_session().run(v.initializer)\n     return v\n \n \n@@ -40,7 +40,8 @@ def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n \n \n def shape(x):\n-    return x.get_shape()\n+    # symbolic shape\n+    return tf.shape(x)\n \n \n def int_shape(x):\n@@ -55,7 +56,7 @@ def ndim(x):\n def eval(x):\n     '''Run a graph.\n     '''\n-    return x.eval(session=_get_session())\n+    return x.eval(session=get_session())\n \n \n def zeros(shape, dtype=_FLOATX, name=None):\n@@ -244,7 +245,10 @@ def minimum(x, y):\n \n def concatenate(tensors, axis=-1):\n     if axis < 0:\n+        if len(tensors[0].get_shape()):\n             axis = axis % len(tensors[0].get_shape())\n+        else:\n+            axis = 0\n     return tf.concat(axis, tensors)\n \n \n@@ -270,15 +274,15 @@ def resize_images(X, height_factor, width_factor, dim_ordering):\n     positive integers.\n     '''\n     if dim_ordering == 'th':\n-        new_height = shape(X)[2].value * height_factor\n-        new_width = shape(X)[3].value * width_factor\n+        new_shape = tf.shape(X)[2:]\n+        new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\n         X = permute_dimensions(X, [0, 2, 3, 1])\n-        X = tf.image.resize_nearest_neighbor(X, (new_height, new_width))\n+        X = tf.image.resize_nearest_neighbor(X, new_shape)\n         return permute_dimensions(X, [0, 3, 1, 2])\n     elif dim_ordering == 'tf':\n-        new_height = shape(X)[1].value * height_factor\n-        new_width = shape(X)[2].value * width_factor\n-        return tf.image.resize_nearest_neighbor(X, (new_height, new_width))\n+        new_shape = tf.shape(X)[1:3]\n+        new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\n+        return tf.image.resize_nearest_neighbor(X, new_shape)\n     else:\n         raise Exception('Invalid dim_ordering: ' + dim_ordering)\n \n@@ -358,19 +362,22 @@ def spatial_2d_padding(x, padding=(1, 1), dim_ordering='th'):\n                    [0, 0]]\n     return tf.pad(x, pattern)\n \n+\n def pack(x):\n     return tf.pack(x)\n \n+\n # VALUE MANIPULATION\n \n+\n def get_value(x):\n     '''Technically the same as eval() for TF.\n     '''\n-    return x.eval(session=_get_session())\n+    return x.eval(session=get_session())\n \n \n def set_value(x, value):\n-    tf.assign(x, np.asarray(value)).op.run(session=_get_session())\n+    tf.assign(x, np.asarray(value)).op.run(session=get_session())\n \n \n # GRAPH MANIPULATION\n@@ -390,7 +397,7 @@ class Function(object):\n         assert type(inputs) in {list, tuple}\n         names = [v.name for v in self.inputs]\n         feed_dict = dict(zip(names, inputs))\n-        session = _get_session()\n+        session = get_session()\n         updated = session.run(self.outputs + self.updates, feed_dict=feed_dict)\n         return updated[:len(self.outputs)]\n \n\n@@ -446,7 +446,7 @@ class TensorBoard(Callback):\n         import keras.backend.tensorflow_backend as KTF\n \n         self.model = model\n-        self.sess = KTF._get_session()\n+        self.sess = KTF.get_session()\n         if self.histogram_freq and not self.merged:\n             mod_type = self.model.get_config()['name']\n             if mod_type == 'Sequential':\n\n@@ -127,7 +127,7 @@ def test_TensorBoard():\n     import shutil\n     import tensorflow as tf\n     import keras.backend.tensorflow_backend as KTF\n-    old_session = KTF._get_session()\n+    old_session = KTF.get_session()\n     filepath = './logs'\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n                                                          nb_test=test_samples,\n@@ -162,7 +162,7 @@ def test_TensorBoard():\n \n     with tf.Graph().as_default():\n         session = tf.Session('')\n-        KTF._set_session(session)\n+        KTF.set_session(session)\n         model = Sequential()\n         model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n         model.add(Dense(nb_class, activation='softmax'))\n@@ -208,7 +208,7 @@ def test_TensorBoard():\n \n     with tf.Graph().as_default():\n         session = tf.Session('')\n-        KTF._set_session(session)\n+        KTF.set_session(session)\n         model = Graph()\n         model.add_input(name='X_vars', input_shape=(input_dim, ))\n \n@@ -246,7 +246,7 @@ def test_TensorBoard():\n         assert os.path.exists(filepath)\n         shutil.rmtree(filepath)\n \n-    KTF._set_session(old_session)\n+    KTF.set_session(old_session)\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fc476840fa835e916ca046f7df5220773bfb8cfe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 1326 | Contributors (this commit): 14 | Commits (past 90d): 38 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -418,6 +418,7 @@ def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='th'):\n         raise Exception('Invalid dim_ordering: ' + dim_ordering)\n     return T.set_subtensor(output[indices], x)\n \n+\n def pack(x):\n     return T.stack(*x)\n \n@@ -461,8 +462,8 @@ def gradients(loss, variables):\n def rnn(step_function, inputs, initial_states,\n         go_backwards=False, mask=None, constants=None):\n     '''Iterates over the time dimension of a tensor.\n-    Parameters\n-    ----------\n+\n+    # Arguments\n         inputs: tensor of temporal data of shape (samples, time, ...)\n             (at least 3D).\n         step_function:\n@@ -483,8 +484,9 @@ def rnn(step_function, inputs, initial_states,\n         mask: binary tensor with shape (samples, time),\n             with a zero for every element that is masked.\n         constants: a list of constant values passed at each step.\n-    Returns\n-    -------\n+\n+\n+    # Returns\n         A tuple (last_output, outputs, new_states).\n             last_output: the latest output of the rnn, of shape (samples, ...)\n             outputs: tensor with shape (samples, time, ...) where each\n@@ -709,7 +711,8 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n     return conv_out\n \n \n-def conv3d(x, kernel, strides=(1, 1, 1), border_mode='valid', dim_ordering='th',\n+def conv3d(x, kernel, strides=(1, 1, 1),\n+           border_mode='valid', dim_ordering='th',\n            volume_shape=None, filter_shape=None):\n     '''\n     Run on cuDNN if available.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bde45eff87ef40ce02da98c56a2ad59ae9170d4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 406 | Lines Deleted: 393 | Files Changed: 3 | Hunks: 5 | Methods Changed: 27 | Complexity Δ (Sum/Max): 41/26 | Churn Δ: 799 | Churn Cumulative: 1202 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: 0.75\n\nDIFF:\n@@ -57,6 +57,7 @@ def container_from_config(original_layer_dict, custom_objects={}):\n         for node in nodes:\n             layer = container_from_config(layer_dict['nodes'].get(node['name']))\n             node['layer'] = layer\n+            node['create_output'] = False  # outputs will be added below\n             graph_layer.add_node(**node)\n \n         outputs = layer_dict.get('output_config')\n\n@@ -0,0 +1,404 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import pytest\n+import os\n+import numpy as np\n+np.random.seed(1337)\n+\n+from keras import backend as K\n+from keras.models import Graph, Sequential, model_from_json, model_from_yaml\n+from keras.layers.core import Dense, Activation, Merge, Lambda, LambdaMerge, Siamese, add_shared_layer\n+from keras.layers import containers\n+from keras.utils.test_utils import get_test_data\n+\n+\n+batch_size = 32\n+\n+(X_train_graph, y_train_graph), (X_test_graph, y_test_graph) = get_test_data(nb_train=1000,\n+                                                                             nb_test=200,\n+                                                                             input_shape=(32,),\n+                                                                             classification=False,\n+                                                                             output_shape=(4,))\n+(X2_train_graph, y2_train_graph), (X2_test_graph, y2_test_graph) = get_test_data(nb_train=1000,\n+                                                                                 nb_test=200,\n+                                                                                 input_shape=(32,),\n+                                                                                 classification=False,\n+                                                                                 output_shape=(1,))\n+\n+\n+def test_graph_fit_generator():\n+    def data_generator_graph(train):\n+        while 1:\n+            if train:\n+                yield {'input1': X_train_graph, 'output1': y_train_graph}\n+            else:\n+                yield {'input1': X_test_graph, 'output1': y_test_graph}\n+\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(4), name='dense3', input='dense1')\n+\n+    graph.add_output(name='output1',\n+                     inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4)\n+    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4)\n+    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4, validation_data={'input1': X_test_graph, 'output1': y_test_graph})\n+    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4, validation_data={'input1': X_test_graph, 'output1': y_test_graph})\n+    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n+                        validation_data=data_generator_graph(False), nb_val_samples=batch_size * 3)\n+    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n+                        validation_data=data_generator_graph(False), nb_val_samples=batch_size * 3)\n+\n+    gen_loss = graph.evaluate_generator(data_generator_graph(True), 128, verbose=0)\n+    assert(gen_loss < 3.)\n+\n+    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph}, verbose=0)\n+    assert(loss < 3.)\n+\n+\n+def test_1o_1i():\n+    # test a non-sequential graph with 1 input and 1 output\n+    np.random.seed(1337)\n+\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(4), name='dense3', input='dense1')\n+\n+    graph.add_output(name='output1',\n+                     inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph}, verbose=0)\n+    assert(loss < 2.5)\n+\n+    # test validation split\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+              validation_split=0.2, nb_epoch=1)\n+    # test validation data\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+              validation_data={'input1': X_train_graph, 'output1': y_train_graph},\n+              nb_epoch=1)\n+\n+\n+def test_1o_1i_2():\n+    # test a more complex non-sequential graph with 1 input and 1 output\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2-0', input='input1')\n+    graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n+\n+    graph.add_node(Dense(16), name='dense3', input='dense2')\n+    graph.add_node(Dense(4), name='dense4', inputs=['dense1', 'dense3'],\n+                   merge_mode='sum')\n+\n+    graph.add_output(name='output1', inputs=['dense2', 'dense4'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_train_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph})\n+    assert(loss < 2.5)\n+\n+    graph.get_config(verbose=1)\n+    graph.summary()\n+\n+\n+def test_1o_2i():\n+    # test a non-sequential graph with 2 inputs and 1 output\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input2')\n+    graph.add_node(Dense(4), name='dense3', input='dense1')\n+\n+    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    assert(loss < 3.0)\n+\n+    graph.get_config(verbose=1)\n+\n+\n+def test_siamese_3():\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n+\n+    graph.add_shared_node(Dense(16), name='shared', inputs=['input1', 'input2'], merge_mode='sum')\n+    graph.add_node(Dense(4), name='dense1', input='shared')\n+    graph.add_node(Dense(4), name='dense2', input='dense1')\n+\n+    graph.add_output(name='output1', input='dense2')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    assert(loss < 3.0)\n+\n+    graph.get_config(verbose=1)\n+\n+\n+def test_siamese_4():\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n+\n+    graph.add_shared_node(Dense(16), name='shared1', inputs=['input1', 'input2'])\n+    graph.add_shared_node(Dense(4), name='shared2', inputs=['shared1'])\n+    graph.add_shared_node(Dense(4), name='shared3', inputs=['shared2'], merge_mode='sum')\n+    graph.add_node(Dense(4), name='dense', input='shared3')\n+\n+    graph.add_output(name='output1', input='dense',\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    assert(loss < 3.0)\n+\n+    graph.get_config(verbose=1)\n+\n+\n+def test_siamese_5():\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n+\n+    graph.add_shared_node(Dense(16), name='shared1', inputs=['input1', 'input2'])\n+    graph.add_shared_node(Dense(4), name='shared2', inputs=['shared1'])\n+    graph.add_shared_node(Dense(4), name='shared3', inputs=['shared2'], outputs=['shared_output1','shared_output2'])\n+    graph.add_node(Dense(4), name='dense1',  input='shared_output1')\n+    graph.add_node(Dense(4), name='dense2',  input='shared_output2')\n+\n+    graph.add_output(name='output1', inputs=['dense1', 'dense2'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    assert(loss < 3.0)\n+\n+    graph.get_config(verbose=1)\n+\n+\n+def test_2o_1i_weights():\n+    # test a non-sequential graph with 1 input and 2 outputs\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(1), name='dense3', input='dense1')\n+\n+    graph.add_output(name='output1', input='dense2')\n+    graph.add_output(name='output2', input='dense3')\n+    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 2)\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n+    assert(loss < 4.)\n+\n+    # test weight saving\n+    fname = 'test_2o_1i_weights_temp.h5'\n+    graph.save_weights(fname, overwrite=True)\n+\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(1), name='dense3', input='dense1')\n+    graph.add_output(name='output1', input='dense2')\n+    graph.add_output(name='output2', input='dense3')\n+    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n+    graph.load_weights('test_2o_1i_weights_temp.h5')\n+    os.remove(fname)\n+\n+    nloss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n+    assert(loss == nloss)\n+\n+\n+def test_2o_1i_sample_weights():\n+    # test a non-sequential graph with 1 input and 2 outputs with sample weights\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(1), name='dense3', input='dense1')\n+\n+    graph.add_output(name='output1', input='dense2')\n+    graph.add_output(name='output2', input='dense3')\n+\n+    weights1 = np.random.uniform(size=y_train_graph.shape[0])\n+    weights2 = np.random.uniform(size=y2_train_graph.shape[0])\n+    weights1_test = np.random.uniform(size=y_test_graph.shape[0])\n+    weights2_test = np.random.uniform(size=y2_test_graph.shape[0])\n+\n+    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n+              nb_epoch=10,\n+              sample_weight={'output1': weights1, 'output2': weights2})\n+    out = graph.predict({'input1': X_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 2)\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph},\n+                               sample_weight={'output1': weights1_test, 'output2': weights2_test})\n+    loss = graph.train_on_batch({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n+                                sample_weight={'output1': weights1, 'output2': weights2})\n+    loss = graph.evaluate({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n+                          sample_weight={'output1': weights1, 'output2': weights2})\n+\n+\n+def test_recursive():\n+    # test layer-like API\n+\n+    graph = containers.Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(4), name='dense3', input='dense1')\n+    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+\n+    seq = Sequential()\n+    seq.add(Dense(32, input_shape=(32,)))\n+    seq.add(graph)\n+    seq.add(Dense(4))\n+\n+    seq.compile('rmsprop', 'mse')\n+\n+    seq.fit(X_train_graph, y_train_graph, batch_size=10, nb_epoch=10)\n+    loss = seq.evaluate(X_test_graph, y_test_graph)\n+    assert(loss < 2.5)\n+\n+    loss = seq.evaluate(X_test_graph, y_test_graph, show_accuracy=True)\n+    seq.predict(X_test_graph)\n+    seq.get_config(verbose=1)\n+\n+\n+def test_create_output():\n+    # test create_output argument\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(4), name='dense3', input='dense1')\n+    graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'],\n+                   merge_mode='sum', create_output=True)\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+                        nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph})\n+    assert(loss < 2.5)\n+\n+    # test serialization\n+    config = graph.to_json()\n+    del graph\n+    graph = model_from_json(config)\n+\n+\n+def test_count_params():\n+    # test count params\n+\n+    nb_units = 100\n+    nb_classes = 2\n+\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n+    graph.add_node(Dense(nb_units),\n+                   name='dense1', input='input1')\n+    graph.add_node(Dense(nb_classes),\n+                   name='dense2', input='input2')\n+    graph.add_node(Dense(nb_classes),\n+                   name='dense3', input='dense1')\n+    graph.add_output(name='output', inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+\n+    n = 32 * nb_units + nb_units\n+    n += 32 * nb_classes + nb_classes\n+    n += nb_units * nb_classes + nb_classes\n+\n+    assert(n == graph.count_params())\n+\n+    graph.compile('rmsprop', {'output': 'binary_crossentropy'})\n+\n+    assert(n == graph.count_params())\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -36,10 +36,6 @@ def _get_test_data():\n     return (X_train, y_train), (X_test, y_test)\n \n \n-####################\n-# SEQUENTIAL TEST  #\n-####################\n-\n def test_sequential_fit_generator():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n \n@@ -667,393 +663,5 @@ def test_siamese_2():\n     assert(loss == nloss)\n \n \n-###############\n-# GRAPH TEST  #\n-###############\n-\n-(X_train_graph, y_train_graph), (X_test_graph, y_test_graph) = get_test_data(nb_train=1000,\n-                                                                             nb_test=200,\n-                                                                             input_shape=(32,),\n-                                                                             classification=False,\n-                                                                             output_shape=(4,))\n-(X2_train_graph, y2_train_graph), (X2_test_graph, y2_test_graph) = get_test_data(nb_train=1000,\n-                                                                                 nb_test=200,\n-                                                                                 input_shape=(32,),\n-                                                                                 classification=False,\n-                                                                                 output_shape=(1,))\n-\n-\n-def test_graph_fit_generator():\n-    def data_generator_graph(train):\n-        while 1:\n-            if train:\n-                yield {'input1': X_train_graph, 'output1': y_train_graph}\n-            else:\n-                yield {'input1': X_test_graph, 'output1': y_test_graph}\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1',\n-                     inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4)\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4)\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4, validation_data={'input1': X_test_graph, 'output1': y_test_graph})\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4, validation_data={'input1': X_test_graph, 'output1': y_test_graph})\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n-                        validation_data=data_generator_graph(False), nb_val_samples=batch_size * 3)\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n-                        validation_data=data_generator_graph(False), nb_val_samples=batch_size * 3)\n-\n-    gen_loss = graph.evaluate_generator(data_generator_graph(True), 128, verbose=0)\n-    assert(gen_loss < 3.)\n-\n-    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph}, verbose=0)\n-    assert(loss < 3.)\n-\n-\n-def test_1o_1i():\n-    # test a non-sequential graph with 1 input and 1 output\n-    np.random.seed(1337)\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1',\n-                     inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-              nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph}, verbose=0)\n-    assert(loss < 2.5)\n-\n-    # test validation split\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-              validation_split=0.2, nb_epoch=1)\n-    # test validation data\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-              validation_data={'input1': X_train_graph, 'output1': y_train_graph},\n-              nb_epoch=1)\n-\n-\n-def test_1o_1i_2():\n-    # test a more complex non-sequential graph with 1 input and 1 output\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2-0', input='input1')\n-    graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n-\n-    graph.add_node(Dense(16), name='dense3', input='dense2')\n-    graph.add_node(Dense(4), name='dense4', inputs=['dense1', 'dense3'],\n-                   merge_mode='sum')\n-\n-    graph.add_output(name='output1', inputs=['dense2', 'dense4'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-              nb_epoch=10)\n-    out = graph.predict({'input1': X_train_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph})\n-    assert(loss < 2.5)\n-\n-    graph.get_config(verbose=1)\n-    graph.summary()\n-\n-\n-def test_1o_2i():\n-    # test a non-sequential graph with 2 inputs and 1 output\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_input(name='input2', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input2')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n-              nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    assert(loss < 3.0)\n-\n-    graph.get_config(verbose=1)\n-\n-\n-def test_siamese_3():\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_input(name='input2', input_shape=(32,))\n-\n-    graph.add_shared_node(Dense(16), name='shared', inputs=['input1', 'input2'], merge_mode='sum')\n-    graph.add_node(Dense(4), name='dense1', input='shared')\n-    graph.add_node(Dense(4), name='dense2', input='dense1')\n-\n-    graph.add_output(name='output1', input='dense2')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n-              nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    assert(loss < 3.0)\n-\n-    graph.get_config(verbose=1)\n-\n-\n-def test_siamese_4():\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_input(name='input2', input_shape=(32,))\n-\n-    graph.add_shared_node(Dense(16), name='shared1', inputs=['input1', 'input2'])\n-    graph.add_shared_node(Dense(4), name='shared2', inputs=['shared1'])\n-    graph.add_shared_node(Dense(4), name='shared3', inputs=['shared2'], merge_mode='sum')\n-    graph.add_node(Dense(4), name='dense', input='shared3')\n-\n-    graph.add_output(name='output1', input='dense',\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n-              nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    assert(loss < 3.0)\n-\n-    graph.get_config(verbose=1)\n-\n-\n-def test_siamese_5():\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_input(name='input2', input_shape=(32,))\n-\n-    graph.add_shared_node(Dense(16), name='shared1', inputs=['input1', 'input2'])\n-    graph.add_shared_node(Dense(4), name='shared2', inputs=['shared1'])\n-    graph.add_shared_node(Dense(4), name='shared3', inputs=['shared2'], outputs=['shared_output1','shared_output2'])\n-    graph.add_node(Dense(4), name='dense1',  input='shared_output1')\n-    graph.add_node(Dense(4), name='dense2',  input='shared_output2')\n-\n-    graph.add_output(name='output1', inputs=['dense1', 'dense2'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n-              nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    assert(loss < 3.0)\n-\n-    graph.get_config(verbose=1)\n-\n-\n-def test_2o_1i_weights():\n-    # test a non-sequential graph with 1 input and 2 outputs\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(1), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1', input='dense2')\n-    graph.add_output(name='output2', input='dense3')\n-    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n-              nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 2)\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n-    assert(loss < 4.)\n-\n-    # test weight saving\n-    fname = 'test_2o_1i_weights_temp.h5'\n-    graph.save_weights(fname, overwrite=True)\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(1), name='dense3', input='dense1')\n-    graph.add_output(name='output1', input='dense2')\n-    graph.add_output(name='output2', input='dense3')\n-    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-    graph.load_weights('test_2o_1i_weights_temp.h5')\n-    os.remove(fname)\n-\n-    nloss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n-    assert(loss == nloss)\n-\n-\n-def test_2o_1i_sample_weights():\n-    # test a non-sequential graph with 1 input and 2 outputs with sample weights\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(1), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1', input='dense2')\n-    graph.add_output(name='output2', input='dense3')\n-\n-    weights1 = np.random.uniform(size=y_train_graph.shape[0])\n-    weights2 = np.random.uniform(size=y2_train_graph.shape[0])\n-    weights1_test = np.random.uniform(size=y_test_graph.shape[0])\n-    weights2_test = np.random.uniform(size=y2_test_graph.shape[0])\n-\n-    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n-              nb_epoch=10,\n-              sample_weight={'output1': weights1, 'output2': weights2})\n-    out = graph.predict({'input1': X_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 2)\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph},\n-                               sample_weight={'output1': weights1_test, 'output2': weights2_test})\n-    loss = graph.train_on_batch({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n-                                sample_weight={'output1': weights1, 'output2': weights2})\n-    loss = graph.evaluate({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n-                          sample_weight={'output1': weights1, 'output2': weights2})\n-\n-\n-def test_recursive():\n-    # test layer-like API\n-\n-    graph = containers.Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-\n-    seq = Sequential()\n-    seq.add(Dense(32, input_shape=(32,)))\n-    seq.add(graph)\n-    seq.add(Dense(4))\n-\n-    seq.compile('rmsprop', 'mse')\n-\n-    seq.fit(X_train_graph, y_train_graph, batch_size=10, nb_epoch=10)\n-    loss = seq.evaluate(X_test_graph, y_test_graph)\n-    assert(loss < 2.5)\n-\n-    loss = seq.evaluate(X_test_graph, y_test_graph, show_accuracy=True)\n-    seq.predict(X_test_graph)\n-    seq.get_config(verbose=1)\n-\n-\n-def test_create_output():\n-    # test create_output argument\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-    graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'],\n-                   merge_mode='sum', create_output=True)\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-                        nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph})\n-    assert(loss < 2.5)\n-\n-\n-def test_count_params():\n-    # test count params\n-\n-    nb_units = 100\n-    nb_classes = 2\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_input(name='input2', input_shape=(32,))\n-    graph.add_node(Dense(nb_units),\n-                   name='dense1', input='input1')\n-    graph.add_node(Dense(nb_classes),\n-                   name='dense2', input='input2')\n-    graph.add_node(Dense(nb_classes),\n-                   name='dense3', input='dense1')\n-    graph.add_output(name='output', inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-\n-    n = 32 * nb_units + nb_units\n-    n += 32 * nb_classes + nb_classes\n-    n += nb_units * nb_classes + nb_classes\n-\n-    assert(n == graph.count_params())\n-\n-    graph.compile('rmsprop', {'output': 'binary_crossentropy'})\n-\n-    assert(n == graph.count_params())\n-\n-\n if __name__ == '__main__':\n-    # pytest.main([__file__])\n-    test_sequential()\n-    test_sequential_fit_generator()\n-    test_graph_fit_generator()\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
