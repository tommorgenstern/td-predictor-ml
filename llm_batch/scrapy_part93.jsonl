{"custom_id": "scrapy#025b34e122b7ca615c304266cee7afcebf4bce86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 105 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ in Python 2.5. The Python 2.6 function is used when available.\n import sys\n import os\n import fnmatch\n-from shutil import copytree, ignore_patterns, copy2, copystat\n+from shutil import copy2, copystat\n \n __all__ = ['cpu_count', 'copytree', 'ignore_patterns']\n \n@@ -39,7 +39,9 @@ except ImportError:\n         else:\n             raise NotImplementedError('cannot determine number of cpus')\n \n-if sys.version_info < (2, 6):\n+if sys.version_info >= (2, 6):\n+    from shutil import copytree, ignore_patterns\n+else:\n     try:\n         WindowsError\n     except NameError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cd6aa72d7f13a7169a4ce0204559f194fdf229f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 117 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ from os.path import join, exists\n import scrapy\n from scrapy.command import ScrapyCommand\n from scrapy.utils.template import render_templatefile, string_camelcase\n-from scrapy.utils.python import ignore_patterns, copytree\n+from scrapy.utils.py26 import ignore_patterns, copytree\n \n TEMPLATES_PATH = join(scrapy.__path__[0], 'templates', 'project')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d731ba641f58e67ca5af9419ff6293163e2186b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 25 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 61 | Churn Cumulative: 183 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -136,12 +136,22 @@ class UrlUtilsTest(unittest.TestCase):\n                          'http://rmc-offers.co.uk/productlist.asp?BCat=newvalue&CatID=60')\n \n     def test_url_query_cleaner(self):\n-        self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", 'id'),\n-                         'product.html?id=200')\n-        self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id', 'name']),\n-                         'product.html?id=200&name=wired')\n-        self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired#id20\", ['id', 'foo']),\n-                         'product.html?id=200&foo=bar')\n+        self.assertEqual('product.html?id=200',\n+                url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id']))\n+        self.assertEqual('product.html?id=200&name=wired',\n+                url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id', 'name']))\n+        self.assertEqual('product.html?id',\n+                url_query_cleaner(\"product.html?id&other=3&novalue=\", ['id']))\n+        self.assertEqual('product.html?d=1&d=2&d=3',\n+                url_query_cleaner(\"product.html?d=1&e=b&d=2&d=3&other=other\", ['d'], unique=False))\n+        self.assertEqual('product.html?id=200&foo=bar',\n+                url_query_cleaner(\"product.html?id=200&foo=bar&name=wired#id20\", ['id', 'foo']))\n+        self.assertEqual('product.html?foo=bar&name=wired',\n+                url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id'], remove=True))\n+        self.assertEqual('product.html?name=wired',\n+                url_query_cleaner(\"product.html?id=2&foo=bar&name=wired\", ['id', 'foo'], remove=True))\n+        self.assertEqual('product.html?foo=bar&name=wired',\n+                url_query_cleaner(\"product.html?id=2&foo=bar&name=wired\", ['id', 'footo'], remove=True))\n \n     def test_canonicalize_url(self):\n         # simplest case\n\n@@ -85,29 +85,30 @@ def url_query_parameter(url, parameter, default=None, keep_blank_values=0):\n         keep_blank_values=keep_blank_values)\n     return queryparams.get(parameter, [default])[0]\n \n-def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='='):\n-    \"\"\"Clean url arguments leaving only those passed in the parameterlist\"\"\"\n-    try:\n+def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='=', remove=False, unique=True):\n+    \"\"\"Clean url arguments leaving only those passed in the parameterlist.\n+\n+    If remove is True, leave only those not in parameterlist.\n+    If unique is False, do not remove duplicated keys\n+    \"\"\"\n     url = urlparse.urldefrag(url)[0]\n-        base, query = url.split('?', 1)\n-        parameters = [pair.split(kvsep, 1) for pair in query.split(sep)]\n-    except:\n-        base = url\n-        query = \"\"\n-        parameters = []\n+    base, _, query = url.partition('?')\n+    parameters = [ksv.partition(kvsep) for ksv in query.split(sep)]\n \n     # unique parameters while keeping order\n-    unique = {}\n+    seen = set()\n     querylist = []\n-    for pair in parameters:\n-        k = pair[0]\n-        if not unique.get(k):\n-            querylist += [pair]\n-            unique[k] = 1\n-\n-    query = sep.join([kvsep.join(pair) for pair in querylist if pair[0] in \\\n-        parameterlist])\n-    return '?'.join([base, query])\n+    for k, s, v in parameters:\n+        if unique and k in seen:\n+            continue\n+        elif remove and k in parameterlist:\n+            continue\n+        elif not remove and k not in parameterlist:\n+            continue\n+        else:\n+            querylist.append([k, s, v])\n+            seen.add(k)\n+    return base + '?' + sep.join(''.join(ksv) for ksv in querylist)\n \n def add_or_replace_parameter(url, name, new_value, sep='&', url_is_quoted=False):\n     \"\"\"Add or remove a parameter to a given url\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ac646a3b4784e8f15f29357d3760750cc7b29753", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 188 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -138,6 +138,8 @@ class UrlUtilsTest(unittest.TestCase):\n     def test_url_query_cleaner(self):\n         self.assertEqual('product.html?id=200',\n                 url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id']))\n+        self.assertEqual('product.html',\n+                url_query_cleaner(\"product.html?foo=bar&name=wired\", ['id']))\n         self.assertEqual('product.html?id=200&name=wired',\n                 url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id', 'name']))\n         self.assertEqual('product.html?id',\n\n@@ -108,7 +108,8 @@ def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='=', remove=False, u\n         else:\n             querylist.append([k, s, v])\n             seen.add(k)\n-    return base + '?' + sep.join(''.join(ksv) for ksv in querylist)\n+    query = '?' + sep.join(''.join(ksv) for ksv in querylist)\n+    return urlparse.urljoin(base, query)\n \n def add_or_replace_parameter(url, name, new_value, sep='&', url_is_quoted=False):\n     \"\"\"Add or remove a parameter to a given url\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1750e233f7133610a09a937db81877d39844aab5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 59 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,6 +8,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy.core.exceptions import NotConfigured\n from scrapy.contrib import exporter\n+from scrapy.contrib.exporter import jsonlines\n from scrapy.conf import settings\n \n class FileExportPipeline(object):\n@@ -48,7 +49,6 @@ class FileExportPipeline(object):\n         elif format == 'pickle':\n             exp = exporter.PickleItemExporter(file, **exp_kwargs)\n         elif format == 'json':\n-            from scrapy.contrib.exporter import jsonlines\n             exp = jsonlines.JsonLinesItemExporter(file, **exp_kwargs)\n         else:\n             raise NotConfigured(\"Unsupported export format: %s\" % format)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3ab3cf85c13385533f92c9e2bf6cb216d7df977", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 15 | Churn Cumulative: 203 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -138,6 +138,8 @@ class UrlUtilsTest(unittest.TestCase):\n     def test_url_query_cleaner(self):\n         self.assertEqual('product.html?id=200',\n                 url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id']))\n+        self.assertEqual('product.html?id=200',\n+                url_query_cleaner(\"product.html?&id=200&&foo=bar&name=wired\", ['id']))\n         self.assertEqual('product.html',\n                 url_query_cleaner(\"product.html?foo=bar&name=wired\", ['id']))\n         self.assertEqual('product.html?id=200&name=wired',\n\n@@ -86,19 +86,17 @@ def url_query_parameter(url, parameter, default=None, keep_blank_values=0):\n     return queryparams.get(parameter, [default])[0]\n \n def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='=', remove=False, unique=True):\n-    \"\"\"Clean url arguments leaving only those passed in the parameterlist.\n+    \"\"\"Clean url arguments leaving only those passed in the parameterlist keeping order\n \n     If remove is True, leave only those not in parameterlist.\n     If unique is False, do not remove duplicated keys\n     \"\"\"\n     url = urlparse.urldefrag(url)[0]\n     base, _, query = url.partition('?')\n-    parameters = [ksv.partition(kvsep) for ksv in query.split(sep)]\n-\n-    # unique parameters while keeping order\n     seen = set()\n     querylist = []\n-    for k, s, v in parameters:\n+    for ksv in query.split(sep):\n+        k, _, _ = ksv.partition(kvsep)\n         if unique and k in seen:\n             continue\n         elif remove and k in parameterlist:\n@@ -106,10 +104,9 @@ def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='=', remove=False, u\n         elif not remove and k not in parameterlist:\n             continue\n         else:\n-            querylist.append([k, s, v])\n+            querylist.append(ksv)\n             seen.add(k)\n-    query = '?' + sep.join(''.join(ksv) for ksv in querylist)\n-    return urlparse.urljoin(base, query)\n+    return urlparse.urljoin(base, '?'+sep.join(querylist))\n \n def add_or_replace_parameter(url, name, new_value, sep='&', url_is_quoted=False):\n     \"\"\"Add or remove a parameter to a given url\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c925c9e9a0e45b0f95c8c1181bc072f72eed6705", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 47 | Churn Cumulative: 233 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,16 @@ HttpError Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+from scrapy.core.exceptions import IgnoreRequest\n+\n+\n+class HttpErrorException(IgnoreRequest):\n+    \"\"\"A non-200 response was filtered\"\"\"\n+\n+    def __init__(self, response, *args, **kwargs):\n+        self.response = response\n+        super(HttpErrorException, self).__init__(*args, **kwargs)\n+\n \n class HttpErrorMiddleware(object):\n \n@@ -15,4 +25,5 @@ class HttpErrorMiddleware(object):\n             allowed_statuses = getattr(spider, 'handle_httpstatus_list', ())\n         if response.status in allowed_statuses:\n             return\n-        return []\n+        raise HttpErrorException(response, 'Ignoring non-200 response')\n+\n\n@@ -7,11 +7,11 @@ docs/topics/spider-middleware.rst\n \"\"\"\n \n from scrapy import log\n+from twisted.python.failure import Failure\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import mustbe_deferred\n-from scrapy.http import Request\n from scrapy.conf import settings\n \n def _isiterable(possible_iterator):\n@@ -60,12 +60,14 @@ class SpiderMiddlewareManager(object):\n \n         def process_spider_input(response):\n             for method in self.spider_middleware:\n+                try:\n                     result = method(response=response, spider=spider)\n-                assert result is None or _isiterable(result), \\\n-                    'Middleware %s must returns None or an iterable object, got %s ' % \\\n-                    (fname(method), type(result))\n-                if result is not None:\n-                    return result\n+                    assert result is None, \\\n+                            'Middleware %s must returns None or ' \\\n+                            'raise an exception, got %s ' \\\n+                            % (fname(method), type(result))\n+                except:\n+                    return scrape_func(Failure(), request, spider)\n             return scrape_func(response, request, spider)\n \n         def process_spider_exception(_failure):\n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.http import Response, Request\n from scrapy.spider import BaseSpider\n-from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware\n+from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpErrorException\n \n \n class TestHttpErrorMiddleware(TestCase):\n@@ -18,21 +18,19 @@ class TestHttpErrorMiddleware(TestCase):\n         self.res404.request = self.req\n \n     def test_process_spider_input(self):\n-        self.assertEquals(self.mw.process_spider_input(self.res200, self.spider),\n-                          None)\n-\n-        self.assertEquals(self.mw.process_spider_input(self.res404, self.spider),\n-                          [])\n+        self.assertEquals(None,\n+                self.mw.process_spider_input(self.res200, self.spider))\n+        self.assertRaises(HttpErrorException,\n+                self.mw.process_spider_input, self.res404, self.spider)\n \n     def test_handle_httpstatus_list(self):\n         res = self.res404.copy()\n         res.request = Request('http://scrapytest.org',\n                               meta={'handle_httpstatus_list': [404]})\n-\n-        self.assertEquals(self.mw.process_spider_input(res, self.spider),\n-                          None)\n+        self.assertEquals(None,\n+            self.mw.process_spider_input(res, self.spider))\n \n         self.spider.handle_httpstatus_list = [404]\n-        self.assertEquals(self.mw.process_spider_input(self.res404, self.spider),\n-                          None)\n+        self.assertEquals(None,\n+            self.mw.process_spider_input(self.res404, self.spider))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94e6acebabd4e38f47cc493eedbf739664abaf9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 115 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -87,6 +87,10 @@ class UtilsMarkupTest(unittest.TestCase):\n         self.assertEqual(remove_tags(u'<p align=\"center\" class=\"one\">texty</p>', which_ones=('b',)),\n                          u'<p align=\"center\" class=\"one\">texty</p>')\n \n+        # text with empty tags\n+        self.assertEqual(remove_tags(u'a<br />b<br/>c'), u'abc')\n+        self.assertEqual(remove_tags(u'a<br />b<br/>c', which_ones=('br',)), u'abc')\n+\n     def test_remove_tags_with_content(self):\n         # make sure it always return unicode\n         assert isinstance(remove_tags_with_content('no tags'), unicode)\n@@ -105,6 +109,9 @@ class UtilsMarkupTest(unittest.TestCase):\n         self.assertEqual(remove_tags_with_content(u'<b>not will removed</b><i>i will removed</i>', which_ones=('i',)),\n                          u'<b>not will removed</b>')\n \n+        # text with empty tags\n+        self.assertEqual(remove_tags_with_content(u'<br/>a<br />', which_ones=('br',)), u'a')\n+\n     def test_replace_escape_chars(self):\n         # make sure it always return unicode\n         assert isinstance(replace_escape_chars('no ec'), unicode)\n\n@@ -85,7 +85,7 @@ def remove_tags(text, which_ones=(), encoding=None):\n                       if is empty remove all tags.\n     \"\"\"\n     if which_ones:\n-        tags = ['<%s>|<%s .*?>|</%s>' % (tag, tag, tag) for tag in which_ones]\n+        tags = ['<%s/?>|<%s .*?>|</%s>' % (tag, tag, tag) for tag in which_ones]\n         regex = '|'.join(tags)\n     else:\n         regex = '<.*?>'\n@@ -101,7 +101,7 @@ def remove_tags_with_content(text, which_ones=(), encoding=None):\n     \"\"\"\n     text = str_to_unicode(text, encoding)\n     if which_ones:\n-        tags = '|'.join(['<%s.*?</%s>' % (tag, tag) for tag in which_ones])\n+        tags = '|'.join([r'<%s.*?</%s>|<%s\\s*/>' % (tag, tag, tag) for tag in which_ones])\n         retags = re.compile(tags, re.DOTALL | re.IGNORECASE)\n         text = retags.sub(u'', text)\n     return text\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c60ef69ab50c22e77cd5442fa98ee82c5a0b1db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 33 | Churn Cumulative: 148 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -91,6 +91,11 @@ class UtilsMarkupTest(unittest.TestCase):\n         self.assertEqual(remove_tags(u'a<br />b<br/>c'), u'abc')\n         self.assertEqual(remove_tags(u'a<br />b<br/>c', which_ones=('br',)), u'abc')\n \n+        # test keep arg\n+        self.assertEqual(remove_tags(u'<p>a<br />b<br/>c</p>', keep=('br',)), u'a<br />b<br/>c')\n+        self.assertEqual(remove_tags(u'<p>a<br />b<br/>c</p>', keep=('p',)), u'<p>abc</p>')\n+        self.assertEqual(remove_tags(u'<p>a<br />b<br/>c</p>', keep=('p','br','div')), u'<p>a<br />b<br/>c</p>')\n+\n     def test_remove_tags_with_content(self):\n         # make sure it always return unicode\n         assert isinstance(remove_tags_with_content('no tags'), unicode)\n\n@@ -78,20 +78,34 @@ def remove_comments(text, encoding=None):\n     \"\"\" Remove HTML Comments. \"\"\"\n     return re.sub('<!--.*?-->', u'', str_to_unicode(text, encoding), re.DOTALL)\n       \n-def remove_tags(text, which_ones=(), encoding=None):\n+def remove_tags(text, which_ones=(), keep=(), encoding=None):\n     \"\"\" Remove HTML Tags only. \n \n-        which_ones -- is a tuple of which tags we want to remove.\n-                      if is empty remove all tags.\n+        which_ones and keep are both tuples, there are four cases:\n+\n+        which_ones, keep (1 - not empty, 0 - empty)\n+        1, 0 - remove all tags in which_ones\n+        0, 1 - remove all tags except the ones in keep\n+        0, 0 - remove all tags\n+        1, 1 - not allowd\n     \"\"\"\n+\n+    assert not (which_ones and keep), 'which_ones and keep can not be given at the same time'\n+\n+    def will_remove(tag):\n         if which_ones:\n-        tags = ['<%s/?>|<%s .*?>|</%s>' % (tag, tag, tag) for tag in which_ones]\n-        regex = '|'.join(tags)\n+            return tag in which_ones\n         else:\n-        regex = '<.*?>'\n+            return tag not in keep\n+\n+    def remove_tag(m):\n+        tag = m.group(1)\n+        return u'' if will_remove(tag) else m.group(0)\n+\n+    regex = '</?([^ >/]+).*?>'\n     retags = re.compile(regex, re.DOTALL | re.IGNORECASE)\n \n-    return retags.sub(u'', str_to_unicode(text, encoding))\n+    return retags.sub(remove_tag, str_to_unicode(text, encoding))\n \n def remove_tags_with_content(text, which_ones=(), encoding=None):\n     \"\"\" Remove tags and its content.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90fef3cbcdd57357998c29c679ad72e24b45337f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 473 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -206,8 +206,8 @@ class ImagesPipeline(MediaPipeline):\n         referer = request.headers.get('Referer')\n \n         if response.status != 200:\n-            log.msg('Image (http-error): Error downloading image from %s referred in <%s>' \\\n-                    % (request, referer), level=log.WARNING, spider=info.spider)\n+            log.msg('Image (code: %s): Error downloading image from %s referred in <%s>' \\\n+                    % (response.status, request, referer), level=log.WARNING, spider=info.spider)\n             raise ImageException\n \n         if not response.body:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c5cd8b9d3d160251e5c352a1aee44ee295f9a8bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 153 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,12 +3,11 @@ This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n \n-from __future__ import with_statement\n-\n+import os\n import re\n import weakref\n import webbrowser\n-from tempfile import NamedTemporaryFile\n+import tempfile\n \n from twisted.web import http\n from twisted.web.http import RESPONSES\n@@ -104,6 +103,7 @@ def open_in_browser(response):\n     body = response.body\n     if '<base' not in body:\n         body = body.replace('<head>', '<head><base href=\"%s\">' % response.url)\n-    with NamedTemporaryFile(suffix='.html', delete=False) as f:\n-        f.write(body)\n-        webbrowser.open(\"file://%s\" % f.name)\n+    fd, fname = tempfile.mkstemp('.html')\n+    os.write(fd, body)\n+    os.close(fd)\n+    webbrowser.open(\"file://%s\" % fname)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f75839e7a9a92e8f01943492d56e454809b2123", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 53 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -5,12 +5,15 @@ Scrapy - a screen scraping framework written in Python\n version_info = (0, 8, 0, '', 0)\n __version__ = \"0.8\"\n \n-import sys, os\n+import sys, os, warnings\n \n if sys.version_info < (2,5):\n     print \"Scrapy %s requires Python 2.5 or above\" % __version__\n     sys.exit(1)\n \n+# ignore noisy twisted deprecation warnings\n+warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n+\n # monkey patches to fix external library issues\n from scrapy.xlib import twisted_250_monkeypatches\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0aaa74d2bd1460bacef9f88539469e1599b3cc39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 166 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ def load_object(path):\n \n     return obj\n \n-def extract_regex(regex, text, encoding):\n+def extract_regex(regex, text, encoding='utf-8'):\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n \n     * if the regex contains a named group called \"extract\" that will be returned\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b3bf5c6f6cb0c58591a1b87b574975fd152dc86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 216 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -71,6 +71,13 @@ class HttpTestCase(unittest.TestCase):\n         d.addCallback(self.assertEquals, \"0123456789\")\n         return d\n \n+    def test_download_head(self):\n+        request = Request(self.getURL('file'), method='HEAD')\n+        d = download_http(request, BaseSpider('foo'))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEquals, '')\n+        return d\n+\n     def test_redirect_status(self):\n         request = Request(self.getURL('redirect'))\n         d = download_http(request, BaseSpider('foo'))\n@@ -78,6 +85,13 @@ class HttpTestCase(unittest.TestCase):\n         d.addCallback(self.assertEquals, 302)\n         return d\n \n+    def test_redirect_status_head(self):\n+        request = Request(self.getURL('redirect'), method='HEAD')\n+        d = download_http(request, BaseSpider('foo'))\n+        d.addCallback(lambda r: r.status)\n+        d.addCallback(self.assertEquals, 302)\n+        return d\n+\n     def test_timeout_download_from_spider(self):\n         spider = BaseSpider('foo')\n         spider.download_timeout = 0.000001\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c87a29eb9edda23d70861ab041e1816efbdfa01a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -7,4 +7,10 @@ A hierarchical approach to wrapper induction\n \n     Extracting web data using instance based learning\n     http://portal.acm.org/citation.cfm?id=1265174 \n+\n+This code has some additional dependencies too:\n+\n+* numpy\n+* nltk\n+\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b2f58207a4365d9886e6293539b1aad9c5ca8183", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 143 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -106,7 +106,7 @@ def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='=', remove=False, u\n         else:\n             querylist.append(ksv)\n             seen.add(k)\n-    return urlparse.urljoin(base, '?'+sep.join(querylist))\n+    return '?'.join([base, sep.join(querylist)]) if querylist else base\n \n def add_or_replace_parameter(url, name, new_value, sep='&', url_is_quoted=False):\n     \"\"\"Add or remove a parameter to a given url\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#02b7ca7e8c87cc94034593910213d9ca6ee7b8f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 136 | Lines Deleted: 89 | Files Changed: 1 | Hunks: 53 | Methods Changed: 34 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 225 | Churn Cumulative: 313 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,7 +42,7 @@ http://www.crummy.com/software/BeautifulSoup/documentation.html\n \n Here, have some legalese:\n \n-Copyright (c) 2004-2008, Leonard Richardson\n+Copyright (c) 2004-2010, Leonard Richardson\n \n All rights reserved.\n \n@@ -79,8 +79,8 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, DAMMIT.\n from __future__ import generators\n \n __author__ = \"Leonard Richardson (leonardr@segfault.org)\"\n-__version__ = \"3.0.7a\"\n-__copyright__ = \"Copyright (c) 2004-2008 Leonard Richardson\"\n+__version__ = \"3.0.8.1\"\n+__copyright__ = \"Copyright (c) 2004-2010 Leonard Richardson\"\n __license__ = \"New-style BSD\"\n \n from sgmllib import SGMLParser, SGMLParseError\n@@ -104,9 +104,13 @@ markupbase._declname_match = re.compile(r'[a-zA-Z][-_.:a-zA-Z0-9]*\\s*').match\n \n DEFAULT_OUTPUT_ENCODING = \"utf-8\"\n \n+def _match_css_class(str):\n+    \"\"\"Build a RE to match the given CSS class.\"\"\"\n+    return re.compile(r\"(^|.*\\s)%s($|\\s)\" % str)\n+\n # First, the classes that represent markup elements.\n \n-class PageElement:\n+class PageElement(object):\n     \"\"\"Contains the navigational information for some part of the page\n     (either a tag or a piece of text)\"\"\"\n \n@@ -124,10 +128,11 @@ class PageElement:\n \n     def replaceWith(self, replaceWith):\n         oldParent = self.parent\n-        myIndex = self.parent.contents.index(self)\n-        if hasattr(replaceWith, 'parent') and replaceWith.parent == self.parent:\n+        myIndex = self.parent.index(self)\n+        if hasattr(replaceWith, \"parent\")\\\n+                  and replaceWith.parent is self.parent:\n             # We're replacing this element with one of its siblings.\n-            index = self.parent.contents.index(replaceWith)\n+            index = replaceWith.parent.index(replaceWith)\n             if index and index < myIndex:\n                 # Furthermore, it comes before this element. That\n                 # means that when we extract it, the index of this\n@@ -136,11 +141,20 @@ class PageElement:\n         self.extract()\n         oldParent.insert(myIndex, replaceWith)\n \n+    def replaceWithChildren(self):\n+        myParent = self.parent\n+        myIndex = self.parent.index(self)\n+        self.extract()\n+        reversedChildren = list(self.contents)\n+        reversedChildren.reverse()\n+        for child in reversedChildren:\n+            myParent.insert(myIndex, child)\n+\n     def extract(self):\n         \"\"\"Destructively rips this element out of the tree.\"\"\"\n         if self.parent:\n             try:\n-                self.parent.contents.remove(self)\n+                del self.parent.contents[self.parent.index(self)]\n             except ValueError:\n                 pass\n \n@@ -173,18 +187,17 @@ class PageElement:\n         return lastChild\n \n     def insert(self, position, newChild):\n-        if (isinstance(newChild, basestring)\n-            or isinstance(newChild, unicode)) \\\n+        if isinstance(newChild, basestring) \\\n             and not isinstance(newChild, NavigableString):\n             newChild = NavigableString(newChild)\n \n         position =  min(position, len(self.contents))\n-        if hasattr(newChild, 'parent') and newChild.parent != None:\n+        if hasattr(newChild, 'parent') and newChild.parent is not None:\n             # We're 'inserting' an element that's already one\n             # of this object's children.\n-            if newChild.parent == self:\n-                index = self.find(newChild)\n-                if index and index < position:\n+            if newChild.parent is self:\n+                index = self.index(newChild)\n+                if index > position:\n                     # Furthermore we're moving it further down the\n                     # list of this object's children. That means that\n                     # when we extract this element, our target index\n@@ -322,8 +335,21 @@ class PageElement:\n \n         if isinstance(name, SoupStrainer):\n             strainer = name\n+        # (Possibly) special case some findAll*(...) searches\n+        elif text is None and not limit and not attrs and not kwargs:\n+            # findAll*(True)\n+            if name is True:\n+                return [element for element in generator()\n+                        if isinstance(element, Tag)]\n+            # findAll*('tag-name')\n+            elif isinstance(name, basestring):\n+                return [element for element in generator()\n+                        if isinstance(element, Tag) and\n+                        element.name == name]\n             else:\n+                strainer = SoupStrainer(name, attrs, text, **kwargs)\n         # Build a SoupStrainer\n+        else:\n             strainer = SoupStrainer(name, attrs, text, **kwargs)\n         results = ResultSet(strainer)\n         g = generator()\n@@ -344,31 +370,31 @@ class PageElement:\n     #NavigableStrings and Tags.\n     def nextGenerator(self):\n         i = self\n-        while i:\n+        while i is not None:\n             i = i.next\n             yield i\n \n     def nextSiblingGenerator(self):\n         i = self\n-        while i:\n+        while i is not None:\n             i = i.nextSibling\n             yield i\n \n     def previousGenerator(self):\n         i = self\n-        while i:\n+        while i is not None:\n             i = i.previous\n             yield i\n \n     def previousSiblingGenerator(self):\n         i = self\n-        while i:\n+        while i is not None:\n             i = i.previousSibling\n             yield i\n \n     def parentGenerator(self):\n         i = self\n-        while i:\n+        while i is not None:\n             i = i.parent\n             yield i\n \n@@ -503,7 +529,7 @@ class Tag(PageElement):\n         self.parserClass = parser.__class__\n         self.isSelfClosing = parser.isSelfClosingTag(name)\n         self.name = name\n-        if attrs == None:\n+        if attrs is None:\n             attrs = []\n         self.attrs = attrs\n         self.contents = []\n@@ -521,12 +547,49 @@ class Tag(PageElement):\n                                           val))\n         self.attrs = map(convert, self.attrs)\n \n+    def getString(self):\n+        if (len(self.contents) == 1\n+            and isinstance(self.contents[0], NavigableString)):\n+            return self.contents[0]\n+\n+    def setString(self, string):\n+        \"\"\"Replace the contents of the tag with a string\"\"\"\n+        self.clear()\n+        self.append(string)\n+\n+    string = property(getString, setString)\n+\n+    def getText(self, separator=u\"\"):\n+        if not len(self.contents):\n+            return u\"\"\n+        stopNode = self._lastRecursiveChild().next\n+        strings = []\n+        current = self.contents[0]\n+        while current is not stopNode:\n+            if isinstance(current, NavigableString):\n+                strings.append(current.strip())\n+            current = current.next\n+        return separator.join(strings)\n+\n+    text = property(getText)\n+\n     def get(self, key, default=None):\n         \"\"\"Returns the value of the 'key' attribute for the tag, or\n         the value given for 'default' if it doesn't have that\n         attribute.\"\"\"\n         return self._getAttrMap().get(key, default)\n \n+    def clear(self):\n+        \"\"\"Extract all children.\"\"\"\n+        for child in self.contents[:]:\n+            child.extract()\n+\n+    def index(self, element):\n+        for i, child in enumerate(self.contents):\n+            if child is element:\n+                return i\n+        raise ValueError(\"Tag.index: element not in tag\")\n+\n     def has_key(self, key):\n         return self._getAttrMap().has_key(key)\n \n@@ -595,6 +658,8 @@ class Tag(PageElement):\n \n         NOTE: right now this will return false if two tags have the\n         same attributes in a different order. Should this be fixed?\"\"\"\n+        if other is self:\n+            return True\n         if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n             return False\n         for i in range(0, len(self.contents)):\n@@ -638,7 +703,7 @@ class Tag(PageElement):\n         if self.attrs:\n             for key, val in self.attrs:\n                 fmt = '%s=\"%s\"'\n-                if isString(val):\n+                if isinstance(val, basestring):\n                     if self.containsSubstitutions and '%SOUP-ENCODING%' in val:\n                         val = self.substituteEncoding(val, encoding)\n \n@@ -710,13 +775,20 @@ class Tag(PageElement):\n \n     def decompose(self):\n         \"\"\"Recursively destroys the contents of this tree.\"\"\"\n-        contents = [i for i in self.contents]\n-        for i in contents:\n-            if isinstance(i, Tag):\n-                i.decompose()\n-            else:\n-                i.extract()\n         self.extract()\n+        if len(self.contents) == 0:\n+            return\n+        current = self.contents[0]\n+        while current is not None:\n+            next = current.next\n+            if isinstance(current, Tag):\n+                del current.contents[:]\n+            current.parent = None\n+            current.previous = None\n+            current.previousSibling = None\n+            current.next = None\n+            current.nextSibling = None\n+            current = next\n \n     def prettify(self, encoding=DEFAULT_OUTPUT_ENCODING):\n         return self.__str__(encoding, True)\n@@ -795,24 +867,18 @@ class Tag(PageElement):\n \n     #Generator methods\n     def childGenerator(self):\n-        for i in range(0, len(self.contents)):\n-            yield self.contents[i]\n-        raise StopIteration\n+        # Just use the iterator from the contents\n+        return iter(self.contents)\n \n     def recursiveChildGenerator(self):\n-        stack = [(self, 0)]\n-        while stack:\n-            tag, start = stack.pop()\n-            if isinstance(tag, Tag):\n-                for i in range(start, len(tag.contents)):\n-                    a = tag.contents[i]\n-                    yield a\n-                    if isinstance(a, Tag) and tag.contents:\n-                        if i < len(tag.contents) - 1:\n-                            stack.append((tag, i+1))\n-                        stack.append((a, 0))\n-                        break\n+        if not len(self.contents):\n             raise StopIteration\n+        stopNode = self._lastRecursiveChild().next\n+        current = self.contents[0]\n+        while current is not stopNode:\n+            yield current\n+            current = current.next\n+\n \n # Next, a couple classes to represent queries and their results.\n class SoupStrainer:\n@@ -821,8 +887,8 @@ class SoupStrainer:\n \n     def __init__(self, name=None, attrs={}, text=None, **kwargs):\n         self.name = name\n-        if isString(attrs):\n-            kwargs['class'] = attrs\n+        if isinstance(attrs, basestring):\n+            kwargs['class'] = _match_css_class(attrs)\n             attrs = None\n         if kwargs:\n             if attrs:\n@@ -881,7 +947,8 @@ class SoupStrainer:\n         found = None\n         # If given a list of items, scan it for a text element that\n         # matches.\n-        if isList(markup) and not isinstance(markup, Tag):\n+        if hasattr(markup, \"__iter__\") \\\n+                and not isinstance(markup, Tag):\n             for element in markup:\n                 if isinstance(element, NavigableString) \\\n                        and self.search(element):\n@@ -894,7 +961,7 @@ class SoupStrainer:\n                 found = self.searchTag(markup)\n         # If it's text, make sure the text matches.\n         elif isinstance(markup, NavigableString) or \\\n-                 isString(markup):\n+                 isinstance(markup, basestring):\n             if self._matches(markup, self.text):\n                 found = markup\n         else:\n@@ -905,8 +972,8 @@ class SoupStrainer:\n     def _matches(self, markup, matchAgainst):\n         #print \"Matching %s against %s\" % (markup, matchAgainst)\n         result = False\n-        if matchAgainst == True and type(matchAgainst) == types.BooleanType:\n-            result = markup != None\n+        if matchAgainst is True:\n+            result = markup is not None\n         elif callable(matchAgainst):\n             result = matchAgainst(markup)\n         else:\n@@ -914,17 +981,17 @@ class SoupStrainer:\n             #other ways of matching match the tag name as a string.\n             if isinstance(markup, Tag):\n                 markup = markup.name\n-            if markup and not isString(markup):\n+            if markup and not isinstance(markup, basestring):\n                 markup = unicode(markup)\n             #Now we know that chunk is either a string, or None.\n             if hasattr(matchAgainst, 'match'):\n                 # It's a regexp object.\n                 result = markup and matchAgainst.search(markup)\n-            elif isList(matchAgainst):\n+            elif hasattr(matchAgainst, '__iter__'): # list-like\n                 result = markup in matchAgainst\n             elif hasattr(matchAgainst, 'items'):\n                 result = markup.has_key(matchAgainst)\n-            elif matchAgainst and isString(markup):\n+            elif matchAgainst and isinstance(markup, basestring):\n                 if isinstance(markup, unicode):\n                     matchAgainst = unicode(matchAgainst)\n                 else:\n@@ -943,20 +1010,6 @@ class ResultSet(list):\n \n # Now, some helper functions.\n \n-def isList(l):\n-    \"\"\"Convenience method that works with all 2.x versions of Python\n-    to determine whether or not something is listlike.\"\"\"\n-    return hasattr(l, '__iter__') \\\n-           or (type(l) in (types.ListType, types.TupleType))\n-\n-def isString(s):\n-    \"\"\"Convenience method that works with all 2.x versions of Python\n-    to determine whether or not something is stringlike.\"\"\"\n-    try:\n-        return isinstance(s, unicode) or isinstance(s, basestring)\n-    except NameError:\n-        return isinstance(s, str)\n-\n def buildTagMap(default, *args):\n     \"\"\"Turns a list of maps, lists, or scalars into a single map.\n     Used to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\n@@ -967,7 +1020,7 @@ def buildTagMap(default, *args):\n             #It's a map. Merge it.\n             for k,v in portion.items():\n                 built[k] = v\n-        elif isList(portion):\n+        elif hasattr(portion, '__iter__'): # is a list\n             #It's a list. Map each item to the default.\n             for k in portion:\n                 built[k] = default\n@@ -1116,7 +1169,7 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n             self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\n         if markup:\n             if self.markupMassage:\n-                if not isList(self.markupMassage):\n+                if not hasattr(self.markupMassage, \"__iter__\"):\n                     self.markupMassage = self.MARKUP_MASSAGE\n                 for fix, m in self.markupMassage:\n                     markup = fix.sub(m, markup)\n@@ -1139,10 +1192,10 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n         superclass or the Tag superclass, depending on the method name.\"\"\"\n         #print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n \n-        if methodName.find('start_') == 0 or methodName.find('end_') == 0 \\\n-               or methodName.find('do_') == 0:\n+        if methodName.startswith('start_') or methodName.startswith('end_') \\\n+               or methodName.startswith('do_'):\n             return SGMLParser.__getattr__(self, methodName)\n-        elif methodName.find('__') != 0:\n+        elif not methodName.startswith('__'):\n             return Tag.__getattr__(self, methodName)\n         else:\n             raise AttributeError\n@@ -1165,12 +1218,6 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n \n     def popTag(self):\n         tag = self.tagStack.pop()\n-        # Tags with just one string-owning child get the child as a\n-        # 'string' property, so that soup.tag.string is shorthand for\n-        # soup.tag.contents[0]\n-        if len(self.currentTag.contents) == 1 and \\\n-           isinstance(self.currentTag.contents[0], NavigableString):\n-            self.currentTag.string = self.currentTag.contents[0]\n \n         #print \"Pop\", tag.name\n         if self.tagStack:\n@@ -1259,9 +1306,9 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n                 #last occurance.\n                 popTo = name\n                 break\n-            if (nestingResetTriggers != None\n+            if (nestingResetTriggers is not None\n                 and p.name in nestingResetTriggers) \\\n-                or (nestingResetTriggers == None and isResetNesting\n+                or (nestingResetTriggers is None and isResetNesting\n                     and self.RESET_NESTING_TAGS.has_key(p.name)):\n \n                 #If we encounter one of the nesting reset triggers\n@@ -1280,7 +1327,7 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n         if self.quoteStack:\n             #This is not a real tag.\n             #print \"<%s> is not real!\" % name\n-            attrs = ''.join(map(lambda(x, y): ' %s=\"%s\"' % (x, y), attrs))\n+            attrs = ''.join([' %s=\"%s\"' % (x, y) for x, y in attrs])\n             self.handle_data('<%s%s>' % (name, attrs))\n             return\n         self.endData()\n@@ -1470,8 +1517,8 @@ class BeautifulSoup(BeautifulStoneSoup):\n         BeautifulStoneSoup.__init__(self, *args, **kwargs)\n \n     SELF_CLOSING_TAGS = buildTagMap(None,\n-                                    ['br' , 'hr', 'input', 'img', 'meta',\n-                                    'spacer', 'link', 'frame', 'base'])\n+                                    ('br' , 'hr', 'input', 'img', 'meta',\n+                                    'spacer', 'link', 'frame', 'base', 'col'))\n \n     PRESERVE_WHITESPACE_TAGS = set(['pre', 'textarea'])\n \n@@ -1480,13 +1527,13 @@ class BeautifulSoup(BeautifulStoneSoup):\n     #According to the HTML standard, each of these inline tags can\n     #contain another tag of the same type. Furthermore, it's common\n     #to actually use these tags this way.\n-    NESTABLE_INLINE_TAGS = ['span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',\n-                            'center']\n+    NESTABLE_INLINE_TAGS = ('span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',\n+                            'center')\n \n     #According to the HTML standard, these block tags can contain\n     #another tag of the same type. Furthermore, it's common\n     #to actually use these tags this way.\n-    NESTABLE_BLOCK_TAGS = ['blockquote', 'div', 'fieldset', 'ins', 'del']\n+    NESTABLE_BLOCK_TAGS = ('blockquote', 'div', 'fieldset', 'ins', 'del')\n \n     #Lists can contain other lists, but there are restrictions.\n     NESTABLE_LIST_TAGS = { 'ol' : [],\n@@ -1506,7 +1553,7 @@ class BeautifulSoup(BeautifulStoneSoup):\n                            'tfoot' : ['table'],\n                            }\n \n-    NON_NESTABLE_BLOCK_TAGS = ['address', 'form', 'p', 'pre']\n+    NON_NESTABLE_BLOCK_TAGS = ('address', 'form', 'p', 'pre')\n \n     #If one of these tags is encountered, all tags up to the next tag of\n     #this type are popped.\n@@ -1597,11 +1644,11 @@ class ICantBelieveItsBeautifulSoup(BeautifulSoup):\n     wouldn't be.\"\"\"\n \n     I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS = \\\n-     ['em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',\n+     ('em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',\n       'cite', 'code', 'dfn', 'kbd', 'samp', 'strong', 'var', 'b',\n-      'big']\n+      'big')\n \n-    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ['noscript']\n+    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ('noscript',)\n \n     NESTABLE_TAGS = buildTagMap([], BeautifulSoup.NESTABLE_TAGS,\n                                 I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS,\n@@ -1752,7 +1799,7 @@ class UnicodeDammit:\n         \"\"\"Changes a MS smart quote character to an XML or HTML\n         entity.\"\"\"\n         sub = self.MS_CHARS.get(orig)\n-        if type(sub) == types.TupleType:\n+        if isinstance(sub, tuple):\n             if self.smartQuotesTo == 'xml':\n                 sub = '&#x%s;' % sub[1]\n             else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
