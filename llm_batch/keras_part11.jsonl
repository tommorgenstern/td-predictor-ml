{"custom_id": "keras#4e83b739f8a5ce4c3b6e83b7708e7a37cd26556e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -41/0 | Churn Δ: 8 | Churn Cumulative: 1243 | Contributors (this commit): 16 | Commits (past 90d): 53 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -320,9 +320,13 @@ class Sequential(Model, containers.Sequential):\n         import os.path\n         # if file exists and should not be overwritten\n         if not overwrite and os.path.isfile(filepath):\n-            overwrite = input('[WARNING] %s already exists - overwrite? [y/n]' % (filepath))\n+            import sys\n+            get_input = input\n+            if sys.version_info[:2] <= (2, 7):\n+                get_input = raw_input\n+            overwrite = get_input('[WARNING] %s already exists - overwrite? [y/n]' % (filepath))\n             while overwrite not in ['y', 'n']:\n-                overwrite = input('Enter \"y\" (overwrite) or \"n\" (cancel).')\n+                overwrite = get_input('Enter \"y\" (overwrite) or \"n\" (cancel).')\n             if overwrite == 'n':\n                 return\n             print('[TIP] Next time specify overwrite=True in save_weights!')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ee179f7da155c3cb8129b954db828906886ad4e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 72 | Files Changed: 3 | Hunks: 33 | Methods Changed: 18 | Complexity Δ (Sum/Max): -12/2 | Churn Δ: 132 | Churn Cumulative: 1231 | Contributors (this commit): 8 | Commits (past 90d): 39 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,6 +23,9 @@ class Sequential(Layer):\n         for layer in layers:\n             self.add(layer)\n \n+    def connect(self, layer):\n+        self.layers[0].previous = layer\n+\n     def add(self, layer):\n         self.layers.append(layer)\n         if len(self.layers) > 1:\n@@ -45,6 +48,10 @@ class Sequential(Layer):\n             self.layers[0].input = ndim_tensor(ndim)\n         return self.layers[0].get_input(train)\n \n+    @property\n+    def input(self):\n+        return self.get_input()\n+\n     def get_weights(self):\n         weights = []\n         for layer in self.layers:\n\n@@ -324,95 +324,76 @@ class TimeDistributedDense(Layer):\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__}\n \n-\n class AutoEncoder(Layer):\n     '''\n         A customizable autoencoder model.\n-          - Supports deep architectures by passing appropriate encoders/decoders list\n-          - If output_reconstruction then dim(input) = dim(output) else dim(output) = dim(hidden)\n+        If output_reconstruction then dim(input) = dim(output)\n+        else dim(output) = dim(hidden)\n     '''\n-    def __init__(self, encoders=[], decoders=[], output_reconstruction=True, tie_weights=False, weights=None):\n+    def __init__(self, encoder, decoder, output_reconstruction=True, tie_weights=False, weights=None):\n \n         super(AutoEncoder,self).__init__()\n-        if not encoders or not decoders:\n-            raise Exception(\"Please specify the encoder/decoder layers\")\n \n-        if not len(encoders) == len(decoders):\n-            raise Exception(\"There need to be an equal number of encoders and decoders\")\n-\n-        # connect all encoders & decoders to their previous (respectively)\n-        for i in range(len(encoders)-1, 0, -1):\n-            encoders[i].connect(encoders[i-1])\n-            decoders[i].connect(decoders[i-1])\n-        decoders[0].connect(encoders[-1])  # connect the first to the last\n-\n-        self.input_dim = encoders[0].input_dim\n-        self.hidden_dim = reversed([d.input_dim for d in decoders])\n         self.output_reconstruction = output_reconstruction\n         self.tie_weights = tie_weights\n-        self.encoders = encoders\n-        self.decoders = decoders\n+        self.encoder = encoder\n+        self.decoder = decoder\n+\n+        self.decoder.connect(self.encoder)\n \n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n-        for m in encoders + decoders:\n-            self.params += m.params\n-            if hasattr(m, 'regularizers'):\n-                self.regularizers += m.regularizers\n-            if hasattr(m, 'constraints'):\n-                self.constraints += m.constraints\n+        for layer in [self.encoder, self.decoder]:\n+            self.params += layer.params\n+            if hasattr(layer, 'regularizers'):\n+                self.regularizers += layer.regularizers\n+            if hasattr(layer, 'constraints'):\n+                self.constraints += layer.constraints\n \n         if weights is not None:\n             self.set_weights(weights)\n \n     def connect(self, node):\n-        self.encoders[0].previous = node\n+        self.encoder.previous = node\n \n     def get_weights(self):\n         weights = []\n-        for m in self.encoders + self.decoders:\n-            weights += m.get_weights()\n+        for layer in [self.encoder, self.decoder]:\n+            weights += layer.get_weights()\n         return weights\n \n     def set_weights(self, weights):\n-        models = self.encoders + self.decoders\n-        for i in range(len(models)):\n-            nb_param = len(models[i].params)\n-            models[i].set_weights(weights[:nb_param])\n-            weights = weights[nb_param:]\n+        nb_param = len(self.encoder.params)\n+        self.encoder.set_weights(weights[:nb_param])\n+        self.decoder.set_weights(weights[nb_param:])\n \n     def get_input(self, train=False):\n-        if hasattr(self.encoders[0], 'previous'):\n-            return  self.encoders[0].previous.get_output(train=train)\n-        else:\n-            return self.encoders[0].input\n+        return self.encoder.get_input(train)\n \n     @property\n     def input(self):\n-        return self.get_input()\n+        return self.encoder.input\n \n     def _get_hidden(self, train):\n-        return self.encoders[-1].get_output(train)\n-\n-    def _tranpose_weights(self, src, dest):\n-        if len(dest.shape) > 1 and len(src.shape) > 1:\n-            dest = src.T\n+        return self.encoder.get_output(train)\n \n     def get_output(self, train):\n-        if not train and not self.output_reconstruction:\n-            return self._get_hidden(train)\n+        decoded = self.decoder.get_output(train)\n \n         if self.tie_weights:\n-            for e,d in zip(self.encoders, self.decoders):\n-                map(self._tranpose_weights, e.get_weights(), d.get_weights())\n+            encoder_params = self.encoder.get_weights()\n+            decoder_params = self.decoder.get_weights()\n+            for dec_param, enc_param in zip(decoder_params, encoder_params):\n+                if len(dec_param.shape) > 1:\n+                    enc_param = dec_param.T\n \n-        return self.decoders[-1].get_output(train)\n+        return decoded\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-                \"encoder_config\":[e.get_config() for e in self.encoders],\n-                \"decoder_config\":[d.get_config() for d in self.decoders],\n+                \"encoder_config\":self.encoder.get_config(),\n+                \"decoder_config\":self.decoder.get_config(),\n                 \"output_reconstruction\":self.output_reconstruction,\n                 \"tie_weights\":self.tie_weights}\n \n@@ -421,8 +402,8 @@ class DenoisingAutoEncoder(AutoEncoder):\n     '''\n         A denoising autoencoder model that inherits the base features from autoencoder\n     '''\n-    def __init__(self, encoders=[], decoders=[], output_reconstruction=True, tie_weights=False, weights=None, corruption_level=0.3):\n-        super(DenoisingAutoEncoder, self).__init__(encoders, decoders, output_reconstruction, tie_weights, weights)\n+    def __init__(self, encoder=None, decoder=None, output_reconstruction=True, tie_weights=False, weights=None, corruption_level=0.3):\n+        super(DenoisingAutoEncoder, self).__init__(encoder, decoder, output_reconstruction, tie_weights, weights)\n         self.corruption_level = corruption_level\n \n     def _get_corrupted_input(self, input):\n@@ -439,8 +420,8 @@ class DenoisingAutoEncoder(AutoEncoder):\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-                \"encoder_config\":[e.get_config() for e in self.encoders],\n-                \"decoder_config\":[d.get_config() for d in self.decoders],\n+                \"encoder_config\":self.encoder.get_config(),\n+                \"decoder_config\":self.decoder.get_config(),\n                 \"corruption_level\":self.corruption_level,\n                 \"output_reconstruction\":self.output_reconstruction,\n                 \"tie_weights\":self.tie_weights}\n\n@@ -6,16 +6,17 @@ from keras.layers.core import DenoisingAutoEncoder, AutoEncoder, Dense, Activati\n from keras.layers.recurrent import LSTM\n from keras.layers.embeddings import Embedding\n from keras.layers.core import Layer\n+from keras.layers import containers\n from keras.utils import np_utils\n \n import numpy as np\n \n # Try different things here: 'lstm' or 'classical' or 'denoising'\n-autoencoder_type = 'classical'\n+autoencoder_type = 'denoising'\n \n nb_classes = 10\n batch_size = 128\n-nb_epoch = 10\n+nb_epoch = 5\n activation = 'linear'\n \n input_dim = 784\n@@ -52,7 +53,7 @@ model_classical.add(Activation('softmax'))\n model_classical.get_config(verbose=1)\n model_classical.compile(loss='categorical_crossentropy', optimizer='adam')\n model_classical.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=(X_test, Y_test))\n-classical_score = model_classical.evaluate(X_test, Y_test, verbose=0)\n+classical_score = model_classical.evaluate(X_test, Y_test, verbose=0, show_accuracy=True)\n print('\\nclassical_score:', classical_score)\n \n ##########################\n@@ -67,23 +68,23 @@ def build_lstm_autoencoder(autoencoder, X_train, X_test):\n \n \t# The TimeDistributedDense isn't really necessary, however you need a lot of GPU memory to do 784x394-394x784\n \tautoencoder.add(TimeDistributedDense(input_dim, 16))\n-\tautoencoder.add(AutoEncoder(encoders=[LSTM(16, 8, activation=activation, return_sequences=True)],\n-\t\t\t\t\t\t\t\tdecoders=[LSTM(8, input_dim, activation=activation, return_sequences=True)],\n+\tautoencoder.add(AutoEncoder(encoder=LSTM(16, 8, activation=activation, return_sequences=True),\n+\t\t\t\t\t\t\t\tdecoder=LSTM(8, input_dim, activation=activation, return_sequences=True),\n \t\t\t\t\t\t\t\toutput_reconstruction=False, tie_weights=True))\n \treturn autoencoder, X_train, X_test\n \n def build_deep_classical_autoencoder(autoencoder):\n-\tencoders = [Dense(input_dim, hidden_dim, activation=activation), Dense(hidden_dim, hidden_dim/2, activation=activation)]\n-\tdecoders = [Dense(hidden_dim/2, hidden_dim, activation=activation), Dense(hidden_dim, input_dim, activation=activation)]\n-\tautoencoder.add(AutoEncoder(encoders=encoders, decoders=decoders, output_reconstruction=False, tie_weights=True))\n+\tencoder = containers.Sequential([Dense(input_dim, hidden_dim, activation=activation), Dense(hidden_dim, hidden_dim/2, activation=activation)])\n+\tdecoder = containers.Sequential([Dense(hidden_dim/2, hidden_dim, activation=activation), Dense(hidden_dim, input_dim, activation=activation)])\n+\tautoencoder.add(AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False, tie_weights=True))\n \treturn autoencoder\n \n def build_denoising_autoencoder(autoencoder):\n \t# You need another layer before a denoising autoencoder\n \t# This is similar to the dropout layers, etc..\n \tautoencoder.add(Dense(input_dim, input_dim))\n-\tautoencoder.add(DenoisingAutoEncoder(encoders=[Dense(input_dim, hidden_dim, activation=activation)],\n-\t\t\t\t\t\t\t\t\t\t decoders=[Dense(hidden_dim, input_dim, activation=activation)],\n+\tautoencoder.add(DenoisingAutoEncoder(encoder=Dense(input_dim, hidden_dim, activation=activation),\n+\t\t\t\t\t\t\t\t\t\t decoder=Dense(hidden_dim, input_dim, activation=activation),\n \t\t\t\t\t\t\t\t\t\t output_reconstruction=False, tie_weights=True, corruption_level=0.3))\n \treturn autoencoder\n \n@@ -117,12 +118,12 @@ print(\"prefilter_test: \", prefilter_test.shape)\n print(\"Building classical fully connected layer for classification\")\n model = Sequential()\n if autoencoder_type == 'lstm':\n-\tmodel.add(TimeDistributedDense(8, 10, activation=activation))\n+\tmodel.add(TimeDistributedDense(8, nb_classes, activation=activation))\n \tmodel.add(Flatten())\n elif autoencoder_type == 'classical':\n-\tmodel.add(Dense(196, 10, activation=activation))\n+\tmodel.add(Dense(prefilter_train.shape[1], nb_classes, activation=activation))\n else:\n-\tmodel.add(Dense(392, 10, activation=activation))\n+\tmodel.add(Dense(prefilter_train.shape[1], nb_classes, activation=activation))\n \n model.add(Activation('softmax'))\n \n@@ -130,10 +131,9 @@ model.get_config(verbose=1)\n model.compile(loss='categorical_crossentropy', optimizer='adam')\n model.fit(prefilter_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=(prefilter_test, Y_test))\n \n-score = model.evaluate(prefilter_test, Y_test, verbose=0, show_accuracy=False)\n+score = model.evaluate(prefilter_test, Y_test, verbose=0, show_accuracy=True)\n print('\\nscore:', score)\n \n-if score < classical_score:\n-\tprint(\"error: classical score > autoencoder score!\")\n-else:\n-\tprint(\"autoencoder improvement: \", 100.0*(float(score)-float(classical_score))/float(classical_score), \"%\")\n+print('Loss change:', (score[0] - classical_score[0])/classical_score[0], '%')\n+print('Accuracy change:', (score[1] - classical_score[1])/classical_score[1], '%')\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7db26e678153bd48c901658ff9dfb64e3609acdc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 98 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -31,7 +31,7 @@ class SimpleRNN(Layer):\n         self.input = T.tensor3()\n \n         self.W = self.init((self.input_dim, self.output_dim))\n-        self.U = self.init((self.output_dim, self.output_dim))\n+        self.U = self.inner_init((self.output_dim, self.output_dim))\n         self.b = shared_zeros((self.output_dim))\n         self.params = [self.W, self.U, self.b]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ffeefb2a1b20acab10f026ef2a63507ccd30cc22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 87 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -53,14 +53,16 @@ class_weight = {0:1,1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:high_weight}\n #test different methods#\n ########################\n \n+print(\"Testing fit methods with and without classweights\")\n # fit\n-model_classweights_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=3, validation_data=(X_test, Y_test), class_weight=class_weight)\n-model_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=3, validation_data=(X_test, Y_test))\n+model_classweights_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test), class_weight=class_weight)\n+model_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test))\n+print(\"Testing train methods with and without classweights\")\n # train\n model_classweights_train.train(X_train, Y_train, class_weight=class_weight)\n model_train.train(X_train, Y_train)\n \n-print('Classification accuracies on test set:')\n+print('MNIST Classification accuracies on test set for fitted models:')\n for nb in range(nb_classes):\n     testIdcs = np.where(y_test == np.array(nb))[0]\n     X_temp = X_test[testIdcs, :]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0856266613bec67dc17b220136c3ccc079d68131", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 496 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -11,8 +11,8 @@ from .utils.generic_utils import Progbar\n \n class CallbackList(object):\n \n-    def __init__(self, callbacks, queue_length=10):\n-        self.callbacks = callbacks\n+    def __init__(self, callbacks=[], queue_length=10):\n+        self.callbacks = [c for c in callbacks]\n         self.queue_length = queue_length\n \n     def append(self, callback):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eda2970b3b9a1dfc98b8d8342f80eb432ee527d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 192 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -51,12 +51,7 @@ model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer='adam')\n \n-history = model.fit(X_train, Y_train, nb_epoch=3, batch_size=batch_size, verbose=1, show_accuracy=False, validation_split=0.1)\n-print(history.epoch)\n-print(history.loss)\n-print(history.accuracy)\n-print(history.validation_loss)\n-print(history.validation_accuracy)\n-score = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1, show_accuracy=False)\n+history = model.fit(X_train, Y_train, nb_epoch=4, batch_size=batch_size, verbose=1, show_accuracy=True, validation_split=0.1)\n+score = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1, show_accuracy=True)\n print('Test score:', score[0])\n print('Test accuracy:', score[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa682535dcc80eed8a5ff1db9553c2e285d424bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 137 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,8 @@ from ..layers.core import Layer\n class Convolution1D(Layer):\n     def __init__(self, nb_filter, stack_size, filter_length,\n         init='uniform', activation='linear', weights=None,\n-        image_shape=None, border_mode='valid', subsample_length=1):\n+        image_shape=None, border_mode='valid', subsample_length=1,\n+        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n \n         nb_row = 1\n         nb_col = filter_length\n@@ -35,6 +36,9 @@ class Convolution1D(Layer):\n \n         self.params = [self.W, self.b]\n \n+        self.regularizers = [W_regularizer, b_regularizer]\n+        self.constraints = [W_constraint, b_constraint]\n+\n         if weights is not None:\n             self.set_weights(weights)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9dbbe4c2cc4d0eef09322b442c0ff55962c8fe2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 143 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -86,7 +86,8 @@ class MaxPooling1D(Layer):\n class Convolution2D(Layer):\n     def __init__(self, nb_filter, stack_size, nb_row, nb_col, \n         init='glorot_uniform', activation='linear', weights=None, \n-        image_shape=None, border_mode='valid', subsample=(1,1)):\n+        image_shape=None, border_mode='valid', subsample=(1,1),\n+        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n         super(Convolution2D,self).__init__()\n \n         self.init = initializations.get(init)\n@@ -106,6 +107,9 @@ class Convolution2D(Layer):\n \n         self.params = [self.W, self.b]\n \n+        self.regularizers = [W_regularizer, b_regularizer]\n+        self.constraints = [W_constraint, b_constraint]\n+\n         if weights is not None:\n             self.set_weights(weights)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b82ab804daf24d6091c449148a938083425155d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 68 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -2,11 +2,13 @@ from setuptools import setup\n from setuptools import find_packages\n \n setup(name = 'Keras',\n-      version='0.0.1',\n-      description='Theano-based Deep Learning',\n+      version = '0.1.0',\n+      description = 'Theano-based Deep Learning library',\n+      long_description = open('README.md').read(),\n       author = 'Francois Chollet',\n       author_email = 'francois.chollet@gmail.com',\n       url = 'https://github.com/fchollet/keras',\n+      download_url = 'https://github.com/fchollet/keras/tarball/0.1.0',\n       license = 'MIT',\n       install_requires = ['theano'],\n       packages = find_packages(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fabfdb868eb5d6b032dcaa9f71844743e57f69ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 69 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,6 @@ from setuptools import find_packages\n setup(name = 'Keras',\n       version = '0.1.0',\n       description = 'Theano-based Deep Learning library',\n-      long_description = open('README.md').read(),\n       author = 'Francois Chollet',\n       author_email = 'francois.chollet@gmail.com',\n       url = 'https://github.com/fchollet/keras',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce209553792dcf5c640690269accbf1512d8c80f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 73 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -2,12 +2,12 @@ from setuptools import setup\n from setuptools import find_packages\n \n setup(name = 'Keras',\n-      version = '0.1.0',\n+      version = '0.1.1',\n       description = 'Theano-based Deep Learning library',\n       author = 'Francois Chollet',\n       author_email = 'francois.chollet@gmail.com',\n       url = 'https://github.com/fchollet/keras',\n-      download_url = 'https://github.com/fchollet/keras/tarball/0.1.0',\n+      download_url = 'https://github.com/fchollet/keras/tarball/0.1.1',\n       license = 'MIT',\n       install_requires = ['theano'],\n       packages = find_packages(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#829b7ab289b0ce2d927dfe79020b0fb1272c14ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 73 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -45,7 +45,7 @@ def categorical_crossentropy(y_true, y_pred, weight=None):\n \n def binary_crossentropy(y_true, y_pred, weight=None):\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n-    bce = T.nnet.binary_crossentropy(y_pred, y_true).mean()\n+    bce = T.nnet.binary_crossentropy(y_pred, y_true)\n     if weight is not None:\n         return (weight*bce).mean()\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#368ad61236efc708bcaee8d653f14a7e14a956a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 10 | Churn Cumulative: 1253 | Contributors (this commit): 16 | Commits (past 90d): 54 | Contributors (cumulative): 16 | DMM Complexity: 0.5\n\nDIFF:\n@@ -111,11 +111,6 @@ class Model(object):\n \n \n     def train(self, X, y, accuracy=False, class_weight=None):\n-        if class_weight is not None and (self.loss is objectives.get('mean_absolute_error') or self.loss is objectives.get('mean_squared_error')):\n-            import warnings\n-            warnings.warn(\"Using class_weight with incompatible loss, ignoring class_weight parameter\", RuntimeWarning)\n-            class_weight = None\n-\n         X = standardize_X(X)\n         y = standardize_y(y)\n         # calculate the weight vector for the loss function\n@@ -140,11 +135,6 @@ class Model(object):\n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True, show_accuracy=False, class_weight=None):\n \n-        if class_weight is not None and (self.loss is objectives.get('mean_absolute_error') or self.loss is objectives.get('mean_squared_error')):\n-            import warnings\n-            warnings.warn(\"Using class_weight with incompatible loss, ignoring class_weight parameter\", RuntimeWarning)\n-            class_weight = None\n-\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf4822f675325f5cfc60911ee30878bdf9f2b98c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -4/2 | Churn Δ: 5 | Churn Cumulative: 963 | Contributors (this commit): 9 | Commits (past 90d): 45 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -379,6 +379,9 @@ class AutoEncoder(Layer):\n         return self.encoder.get_output(train)\n \n     def get_output(self, train):\n+        if not train and not self.output_reconstruction:\n+            return self.encoder.get_output(train)\n+\n         decoded = self.decoder.get_output(train)\n \n         if self.tie_weights:\n\n@@ -4,7 +4,7 @@ import theano.tensor as T\n import numpy as np\n from six.moves import range\n \n-epsilon = 1.0e-15\n+epsilon = 1.0e-9\n \n def mean_squared_error(y_true, y_pred):\n     return T.sqr(y_pred - y_true).mean()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c93ba860a4c9aec572e68b2157a355dc168ba9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 10 | Churn Cumulative: 85 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -8,25 +8,25 @@ epsilon = 1.0e-15\n \n def mean_squared_error(y_true, y_pred, weight=None):\n     if weight is not None:\n-        return T.sqr(weight*(y_pred - y_true)).mean()\n+        return T.sqr(weight.reshape((weight.shape[0], 1))*(y_pred - y_true)).mean()\n     else:\n         return T.sqr(y_pred - y_true).mean()\n \n def mean_absolute_error(y_true, y_pred, weight=None):\n     if weight is not None:\n-        return T.abs_(weight*(y_pred - y_true)).mean()\n+        return T.abs_(weight.reshape((weight.shape[0], 1))*(y_pred - y_true)).mean()\n     else:\n         return T.abs_(y_pred - y_true).mean()\n \n def squared_hinge(y_true, y_pred, weight=None):\n     if weight is not None:\n-        return T.sqr(T.maximum(1. - weight*(y_true * y_pred), 0.)).mean()\n+        return T.sqr(T.maximum(1. - weight.reshape((weight.shape[0], 1))*(y_true * y_pred), 0.)).mean()\n     else:\n         return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean()\n \n def hinge(y_true, y_pred, weight=None):\n     if weight is not None:\n-        return T.maximum(1. - weight*(y_true * y_pred), 0.).mean()\n+        return T.maximum(1. - weight.reshape((weight.shape[0], 1))*(y_true * y_pred), 0.).mean()\n     else:\n         return T.maximum(1. - y_true * y_pred, 0.).mean()\n \n@@ -47,7 +47,7 @@ def binary_crossentropy(y_true, y_pred, weight=None):\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     bce = T.nnet.binary_crossentropy(y_pred, y_true)\n     if weight is not None:\n-        return (weight*bce).mean()\n+        return (weight.reshape((weight.shape[0], 1))*bce).mean()\n     else:\n         return bce.mean()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35c2f36759d418f453e25d06bfcb8d7d0bdfeb94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 91 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,13 +20,15 @@ def mean_absolute_error(y_true, y_pred, weight=None):\n \n def squared_hinge(y_true, y_pred, weight=None):\n     if weight is not None:\n-        return T.sqr(T.maximum(1. - weight.reshape((weight.shape[0], 1))*(y_true * y_pred), 0.)).mean()\n+        weight = weight.reshape((weight.shape[0], 1))\n+        return T.sqr(weight*T.maximum(1. - (y_true * y_pred), 0.)).mean()\n     else:\n         return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean()\n \n def hinge(y_true, y_pred, weight=None):\n     if weight is not None:\n-        return T.maximum(1. - weight.reshape((weight.shape[0], 1))*(y_true * y_pred), 0.).mean()\n+        weight = weight.reshape((weight.shape[0], 1))\n+        return (weight*T.maximum(1. - (y_true * y_pred), 0.)).mean()\n     else:\n         return T.maximum(1. - y_true * y_pred, 0.).mean()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d2b229df2ea0bab712379c418115bc44508bc6f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 103 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 103 | Churn Cumulative: 103 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,103 @@\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation\n+from keras.layers.recurrent import LSTM\n+from keras.datasets.data_utils import get_file\n+import numpy as np\n+import random, sys\n+\n+'''\n+    Example script to generate text from Nietzsche's writings.\n+\n+    At least 20 epochs are required before the generated text\n+    starts sounding coherent.\n+\n+    It is recommended to run this script on GPU, as recurrent\n+    networks are quite computationally intensive.\n+\n+    If you try this script on new data, make sure your corpus \n+    has at least ~100k characters. ~1M is better.\n+'''\n+\n+path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n+text = open(path).read().lower()\n+print('corpus length:', len(text))\n+\n+chars = set(text)\n+print('total chars:', len(chars))\n+char_indices = dict((c, i) for i, c in enumerate(chars))\n+indices_char = dict((i, c) for i, c in enumerate(chars))\n+\n+# cut the text in semi-redundant sequences of 30 characters\n+maxlen = 30\n+step = 5\n+sentences = []\n+next_chars = []\n+for i in range(0, len(text) - maxlen, step):\n+    sentences.append(text[i : i + maxlen])\n+    next_chars.append(text[i + maxlen])\n+print('nb sequences:', len(sentences))\n+\n+print('Vectorization...')\n+X = np.zeros((len(sentences), maxlen, len(chars)))\n+y = np.zeros((len(sentences), len(chars)))\n+for i, sentence in enumerate(sentences):\n+    for t, char in enumerate(sentence):\n+        X[i, t, char_indices[char]] = 1.\n+    y[i, char_indices[next_chars[i]]] = 1.\n+\n+\n+# build the model: 2 stacked LSTM\n+print('Build model...')\n+model = Sequential()\n+model.add(LSTM(len(chars), 512, return_sequences=True))\n+model.add(LSTM(512, 512, return_sequences=False))\n+model.add(Dense(512, len(chars)))\n+model.add(Activation('softmax'))\n+\n+model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+# helper function to sample an index from a probability array\n+def sample(a, diversity=0.75):\n+    if random.random() > diversity:\n+        return np.argmax(a)\n+    while 1:\n+        i = random.randint(0, len(a)-1)\n+        if a[i] > random.random():\n+            return i\n+\n+# train the model, output generated text after each epoch\n+for iteration in range(1, 30):\n+    print()\n+    print('-' * 50)\n+    print('Iteration', iteration)\n+    model.fit(X, y, batch_size=128, nb_epoch=1)\n+\n+    start_index = random.randint(0, len(text) - maxlen - 1)\n+\n+    for diversity in [0.4, 0.7, 1.]:\n+        print()\n+        print('----- diversity:', diversity)\n+\n+        generated = ''\n+        sentence = text[start_index : start_index + maxlen]\n+        generated += sentence\n+        print('----- Generating with seed: \"' + sentence + '\"')\n+        sys.stdout.write(generated)\n+\n+        for iteration in range(400):\n+            x = np.zeros((1, maxlen, len(chars)))\n+            for t, char in enumerate(sentence):\n+                x[0, t, char_indices[char]] = 1.\n+\n+            preds = model.predict(x, verbose=0)[0]\n+            next_index = sample(preds, diversity)\n+            next_char = indices_char[next_index]\n+\n+            generated += next_char\n+            sentence = sentence[1:] + next_char\n+\n+            sys.stdout.write(next_char)\n+            sys.stdout.flush()\n+        print()\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#872a9faf180e1257e63c577c5f0273d12cd6ee9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 104 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n from keras.layers.recurrent import LSTM\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2165fd03dcbbe1361444c0e61e57553ef767a276", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 91 | Churn Cumulative: 178 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,8 @@ from __future__ import absolute_import\n from __future__ import print_function\n from keras.datasets import mnist\n from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Merge\n+from keras.layers.core import Dense, Activation, Merge, Dropout\n+from keras.optimizers import SGD\n from keras.utils import np_utils\n import numpy as np\n \n@@ -49,9 +50,9 @@ model_train = createMNISTModel()\n high_weight = 100\n class_weight = {0:1,1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:high_weight}\n \n-########################\n-#test different methods#\n-########################\n+############################\n+# categorical crossentropy #\n+############################\n \n print(\"Testing fit methods with and without classweights\")\n # fit\n@@ -78,4 +79,86 @@ for nb in range(nb_classes):\n     if class_weight[nb] == high_weight and (score_cw[1] <= score[1] or score_cw_train[1] <= score_train[1]):\n         raise Exception('Class weights are not implemented correctly')\n \n+####################################################\n+# test cases for all remaining objective functions #\n+####################################################\n+batch_size = 64\n+nb_epoch = 10\n+\n+np.random.seed(1337) # for reproducibility\n+\n+def generateData(n_samples, n_dim):\n+    A_feats = np.random.randn(n_samples, n_dim)\n+    B_feats = np.random.randn(n_samples, n_dim)\n+    A_label = np.zeros((n_samples,1))\n+    B_label = np.ones((n_samples,1))\n+    X = np.vstack((A_feats, B_feats))\n+    y = np.vstack((A_label, B_label)).squeeze()\n+    return X, y\n+\n+n_dim = 100\n+X_train, y_train = generateData(1000, n_dim)\n+X_test, y_test = generateData(5000, n_dim)\n+\n+def createModel(ls, n_dim, activation=\"sigmoid\"):\n+    model = Sequential()\n+    model.add(Dense(n_dim, 50))\n+    model.add(Activation('relu'))\n+    model.add(Dropout(0.5))\n+    model.add(Dense(50, 1))\n+    model.add(Activation(activation))\n+    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n+    model.compile(loss=ls, optimizer=sgd, class_mode=\"binary\")\n+    return model\n+\n+verbosity = 0\n+cw = {0: 1.5, 1: 1}\n+# binary crossentropy\n+model = createModel('binary_crossentropy', n_dim)\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), class_weight=cw)\n+res = model.predict(X_test, verbose=verbosity).round()\n+neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n+assert(neg_preds > pos_preds)\n+print(\"binary crossentropy: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n+\n+# MAE\n+model = createModel('mean_absolute_error', n_dim)\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), class_weight=cw)\n+res = model.predict(X_test, verbose=verbosity).round()\n+neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n+assert(neg_preds > pos_preds)\n+print(\"MAE: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n+\n+# MSE\n+model = createModel('mean_squared_error', n_dim)\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), class_weight=cw)\n+res = model.predict(X_test, verbose=verbosity).round()\n+neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n+assert(neg_preds > pos_preds)\n+print(\"MSE: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n+\n+# hinge losses, map labels\n+y_train[y_train == 0] = -1\n+y_test[y_test == 0] = -1\n+cw = {-1: 1.5, 1: 1}\n+\n+# hinge\n+model = createModel('hinge', n_dim, \"tanh\")\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), class_weight=cw)\n+res = model.predict(X_test, verbose=verbosity)\n+res[res < 0] = -1\n+res[res >= 0] = 1\n+neg_preds, pos_preds = (1.0*np.sum(res == -1)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n+assert(neg_preds > pos_preds)\n+print(\"hinge: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n+\n+# squared hinge\n+model = createModel('squared_hinge', n_dim, \"tanh\")\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), class_weight=cw)\n+res = model.predict(X_test, verbose=verbosity)\n+res[res < 0] = -1\n+res[res >= 0] = 1\n+neg_preds, pos_preds = (1.0*np.sum(res == -1)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n+assert(neg_preds > pos_preds)\n+print(\"sqr hinge: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1857a6af5ed6592552559b15c190708e912be139", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 122 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n from __future__ import print_function\n from keras.models import Sequential\n-from keras.layers.core import Dense, Activation\n+from keras.layers.core import Dense, Activation, Dropout\n from keras.layers.recurrent import LSTM\n from keras.datasets.data_utils import get_file\n import numpy as np\n@@ -28,9 +28,9 @@ print('total chars:', len(chars))\n char_indices = dict((c, i) for i, c in enumerate(chars))\n indices_char = dict((i, c) for i, c in enumerate(chars))\n \n-# cut the text in semi-redundant sequences of 30 characters\n-maxlen = 30\n-step = 5\n+# cut the text in semi-redundant sequences of maxlen characters\n+maxlen = 20\n+step = 3\n sentences = []\n next_chars = []\n for i in range(0, len(text) - maxlen, step):\n@@ -51,7 +51,9 @@ for i, sentence in enumerate(sentences):\n print('Build model...')\n model = Sequential()\n model.add(LSTM(len(chars), 512, return_sequences=True))\n+model.add(Dropout(0.2))\n model.add(LSTM(512, 512, return_sequences=False))\n+model.add(Dropout(0.2))\n model.add(Dense(512, len(chars)))\n model.add(Activation('softmax'))\n \n@@ -66,8 +68,8 @@ def sample(a, diversity=0.75):\n         if a[i] > random.random():\n             return i\n \n-# train the model, output generated text after each epoch\n-for iteration in range(1, 30):\n+# train the model, output generated text after each iteration\n+for iteration in range(1, 60):\n     print()\n     print('-' * 50)\n     print('Iteration', iteration)\n@@ -75,7 +77,7 @@ for iteration in range(1, 30):\n \n     start_index = random.randint(0, len(text) - maxlen - 1)\n \n-    for diversity in [0.4, 0.7, 1.]:\n+    for diversity in [0.2, 0.4, 0.6, 0.8]:\n         print()\n         print('----- diversity:', diversity)\n \n@@ -100,5 +102,3 @@ for iteration in range(1, 30):\n             sys.stdout.write(next_char)\n             sys.stdout.flush()\n         print()\n\\ No newline at end of file\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#45775e36b1a911be13a7c30b8f4d3a9908115aa4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 498 | Contributors (this commit): 4 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -201,7 +201,7 @@ class ModelCheckpoint(Callback):\n             self.val_loss.append(cur_val_loss)\n             if cur_val_loss < self.best_val_loss:\n                 if self.verbose > 0:\n-                    print(\"Epoch %05d: valdidation loss improved from %0.5f to %0.5f, saving model to %s\"\n+                    print(\"Epoch %05d: validation loss improved from %0.5f to %0.5f, saving model to %s\"\n                         % (epoch, self.best_val_loss, cur_val_loss, self.filepath))\n                 self.best_val_loss = cur_val_loss\n                 self.model.save_weights(self.filepath, overwrite=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fecedb02cf17cc7b96f01a145d1b69c064f8141e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 6 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 23 | Churn Cumulative: 2614 | Contributors (this commit): 18 | Commits (past 90d): 108 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,6 +20,8 @@ class Sequential(Layer):\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n+        self.cost_updates = []\n+\n         for layer in layers:\n             self.add(layer)\n \n@@ -36,6 +38,9 @@ class Sequential(Layer):\n         self.regularizers += regularizers\n         self.constraints += constraints\n \n+        if hasattr(layer,'activity_regularizer'):\n+            self.cost_updates.append(layer.activity_regularizer)\n+\n     def get_output(self, train=False):\n         return self.layers[-1].get_output(train)\n \n\n@@ -276,6 +276,17 @@ class Dense(Layer):\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__}\n \n+class ActivityRegularization(Layer):\n+    '''\n+        Layer that passes through its input unchanged, but applies an update\n+        to the cost function based on the activity.\n+    '''\n+    def __init__(self, activity_regularizer):\n+        super(ActivityRegularization,self).__init__()\n+        self.activity_regularizer = activity_regularizer(self.get_output())\n+\n+    def get_output(self, train):\n+        return self.get_input(train)\n \n class TimeDistributedDense(Layer):\n     '''\n\n@@ -72,6 +72,10 @@ class Model(object):\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n \n+        if hasattr(self,'cost_updates'):\n+            for u in self.cost_updates:\n+                train_loss += u\n+\n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints,  train_loss)\n \n         if type(self.X_train) == list:\n\n@@ -16,10 +16,11 @@ def kl_divergence(p, p_hat):\n \n class Optimizer(object):\n     \n-    def get_updates(self, params, grads):\n+    def get_updates(self, params, regularizers, constraints,  cost):\n         raise NotImplementedError\n \n     def get_gradients(self, cost, params, regularizers):\n+\n         grads = T.grad(cost, params)\n \n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
