{"custom_id": "scrapy#3e726b9df721f2288f4ae9a685de9bdff0762c06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 19 | Churn Cumulative: 484 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,11 +73,10 @@ def request_from_dict(d, spider=None):\n def _find_method(obj, func):\n     if obj:\n         try:\n-            func_self = func.__self__\n-        except AttributeError:  # func has no __self__\n+            func.__func__\n+        except AttributeError:  # func is not a instance method. Not supported.\n             pass\n         else:\n-            if func_self is obj:\n             members = inspect.getmembers(obj, predicate=inspect.ismethod)\n             for name, obj_func in members:\n                 # We need to use __func__ to access the original\n@@ -89,7 +88,7 @@ def _find_method(obj, func):\n                 # https://docs.python.org/3/reference/datamodel.html\n                 if obj_func.__func__ is func.__func__:\n                     return name\n-    raise ValueError(\"Function %s is not a method of: %s\" % (func, obj))\n+    raise ValueError(\"Function %s is not an instance method in: %s\" % (func, obj))\n \n \n def _get_method(obj, name):\n\n@@ -102,6 +102,12 @@ class RequestSerializationTest(unittest.TestCase):\n                     errback=self.spider.handle_error)\n         self._assert_serializes_ok(r, spider=self.spider)\n \n+    def test_delegated_callback_serialization(self):\n+        r = Request(\"http://www.example.com\",\n+                    callback=self.spider.delegated_callback,\n+                    errback=self.spider.handle_error)\n+        self._assert_serializes_ok(r, spider=self.spider)\n+\n     def test_unserializable_callback1(self):\n         r = Request(\"http://www.example.com\", callback=lambda x: x)\n         self.assertRaises(ValueError, request_to_dict, r)\n@@ -131,6 +137,9 @@ class TestSpiderMixin:\n     def __mixin_callback(self, response):\n         pass\n \n+class TestSpiderDelegation:\n+    def delegated_callback(self, response):\n+        pass\n \n def parse_item(response):\n     pass\n@@ -155,6 +164,9 @@ class TestSpider(Spider, TestSpiderMixin):\n     __parse_item_reference = private_parse_item\n     __handle_error_reference = private_handle_error\n \n+    def __init__(self):\n+        self.delegated_callback = TestSpiderDelegation().delegated_callback\n+\n     def parse_item(self, response):\n         pass\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2d6fa5adc17035cebb8dad4a3bd2020236b4f71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 22 | Churn Cumulative: 1284 | Contributors (this commit): 17 | Commits (past 90d): 8 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -195,7 +195,7 @@ class XmlItemExporter(BaseItemExporter):\n \n class CsvItemExporter(BaseItemExporter):\n \n-    def __init__(self, file, include_headers_line=True, join_multivalued=',', **kwargs):\n+    def __init__(self, file, include_headers_line=True, join_multivalued=',', errors=None, **kwargs):\n         super().__init__(dont_fail=True, **kwargs)\n         if not self.encoding:\n             self.encoding = 'utf-8'\n@@ -205,7 +205,8 @@ class CsvItemExporter(BaseItemExporter):\n             line_buffering=False,\n             write_through=True,\n             encoding=self.encoding,\n-            newline=''  # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n+            newline='',  # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n+            errors=errors,\n         )\n         self.csv_writer = csv.writer(self.stream, **self._kwargs)\n         self._headers_not_written = True\n\n@@ -355,6 +355,23 @@ class CsvItemExporterTest(BaseItemExporterTest):\n             expected='22,False,3.14,2015-01-01 01:01:01\\r\\n'\n         )\n \n+    def test_errors_default(self):\n+        with self.assertRaises(UnicodeEncodeError):\n+            self.assertExportResult(\n+                item=dict(text=u'W\\u0275\\u200Brd'),\n+                expected=None,\n+                encoding='windows-1251',\n+            )\n+\n+    def test_errors_xmlcharrefreplace(self):\n+        self.assertExportResult(\n+            item=dict(text=u'W\\u0275\\u200Brd'),\n+            include_headers_line=False,\n+            expected='W&#629;&#8203;rd\\r\\n',\n+            encoding='windows-1251',\n+            errors='xmlcharrefreplace',\n+        )\n+\n \n class CsvItemExporterDataclassTest(CsvItemExporterTest):\n     item_class = TestDataClass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39affea93c5b60cde89e000486c39c3c0ce87dc9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 279 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -137,10 +137,12 @@ class TestSpiderMixin:\n     def __mixin_callback(self, response):\n         pass\n \n+\n class TestSpiderDelegation:\n     def delegated_callback(self, response):\n         pass\n \n+\n def parse_item(response):\n     pass\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0524df866936506ae9438a5565fc4733fa5ba5b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 215 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -71,12 +71,8 @@ def request_from_dict(d, spider=None):\n \n \n def _find_method(obj, func):\n-    if obj:\n-        try:\n-            func.__func__\n-        except AttributeError:  # func is not a instance method. Not supported.\n-            pass\n-        else:\n+    # Only instance methods contain ``__func__``\n+    if obj and hasattr(func, '__func__'):\n         members = inspect.getmembers(obj, predicate=inspect.ismethod)\n         for name, obj_func in members:\n             # We need to use __func__ to access the original\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f28cee3ce482a9380c816b689fc6a5dabc60295", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 527 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -175,6 +175,30 @@ class XmliterTestCase(unittest.TestCase):\n         node.register_namespace('g', 'http://base.google.com/ns/1.0')\n         self.assertEqual(node.xpath('text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])\n \n+    def test_xmliter_namespaced_nodename_missing(self):\n+        body = b\"\"\"\n+            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+            <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n+                <channel>\n+                <title>My Dummy Company</title>\n+                <link>http://www.mydummycompany.com</link>\n+                <description>This is a dummy company. We do nothing.</description>\n+                <item>\n+                    <title>Item 1</title>\n+                    <description>This is item 1</description>\n+                    <link>http://www.mydummycompany.com/items/1</link>\n+                    <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>\n+                    <g:id>ITEM_1</g:id>\n+                    <g:price>400</g:price>\n+                </item>\n+                </channel>\n+            </rss>\n+        \"\"\"\n+        response = XmlResponse(url='http://mydummycompany.com', body=body)\n+        my_iter = self.xmliter(response, 'g:link_image')\n+        with self.assertRaises(StopIteration):\n+            next(my_iter)\n+\n     def test_xmliter_exception(self):\n         body = (\n             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#58ca8bbf6d1589bd0c8cc1ebda52299346f55e8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 446 | Lines Deleted: 453 | Files Changed: 121 | Hunks: 355 | Methods Changed: 269 | Complexity Δ (Sum/Max): -19/11 | Churn Δ: 899 | Churn Cumulative: 50156 | Contributors (this commit): 210 | Commits (past 90d): 314 | Contributors (cumulative): 1379 | DMM Complexity: 0.42857142857142855\n\nDIFF:\n@@ -49,7 +49,7 @@ master_doc = 'index'\n \n # General information about the project.\n project = 'Scrapy'\n-copyright = '2008–{}, Scrapy developers'.format(datetime.now().year)\n+copyright = f'2008–{datetime.now().year}, Scrapy developers'\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n\n@@ -37,7 +37,7 @@ class Root(Resource):\n         if now - self.lastmark >= 3:\n             self.lastmark = now\n             qps = len(self.tail) / sum(self.tail)\n-            print('samplesize={0} concurrent={1} qps={2:0.2f}'.format(len(self.tail), self.concurrent, qps))\n+            print(f'samplesize={len(self.tail)} concurrent={self.concurrent} qps={qps:0.2f}')\n \n         if 'latency' in request.args:\n             latency = float(request.args['latency'][0])\n\n@@ -37,11 +37,11 @@ class QPSSpider(Spider):\n     def start_requests(self):\n         url = self.benchurl\n         if self.latency is not None:\n-            url += '?latency={0}'.format(self.latency)\n+            url += f'?latency={self.latency}'\n \n         slots = int(self.slots)\n         if slots > 1:\n-            urls = [url.replace('localhost', '127.0.0.%d' % (x + 1)) for x in range(slots)]\n+            urls = [url.replace('localhost', f'127.0.0.{x + 1}') for x in range(slots)]\n         else:\n             urls = [url]\n \n\n@@ -44,7 +44,7 @@ def _get_commands_from_entry_points(inproject, group='scrapy.commands'):\n         if inspect.isclass(obj):\n             cmds[entry_point.name] = obj()\n         else:\n-            raise Exception(\"Invalid entry point %s\" % entry_point.name)\n+            raise Exception(f\"Invalid entry point {entry_point.name}\")\n     return cmds\n \n \n@@ -67,11 +67,11 @@ def _pop_command_name(argv):\n \n \n def _print_header(settings, inproject):\n+    version = scrapy.__version__\n     if inproject:\n-        print(\"Scrapy %s - project: %s\\n\" % (scrapy.__version__,\n-                                             settings['BOT_NAME']))\n+        print(f\"Scrapy {version} - project: {settings['BOT_NAME']}\\n\")\n     else:\n-        print(\"Scrapy %s - no active project\\n\" % scrapy.__version__)\n+        print(f\"Scrapy {version} - no active project\\n\")\n \n \n def _print_commands(settings, inproject):\n@@ -81,7 +81,7 @@ def _print_commands(settings, inproject):\n     print(\"Available commands:\")\n     cmds = _get_commands_dict(settings, inproject)\n     for cmdname, cmdclass in sorted(cmds.items()):\n-        print(\"  %-13s %s\" % (cmdname, cmdclass.short_desc()))\n+        print(f\"  {cmdname:<13} {cmdclass.short_desc()}\")\n     if not inproject:\n         print()\n         print(\"  [ more ]      More commands available when run from project directory\")\n@@ -91,7 +91,7 @@ def _print_commands(settings, inproject):\n \n def _print_unknown_command(settings, cmdname, inproject):\n     _print_header(settings, inproject)\n-    print(\"Unknown command: %s\\n\" % cmdname)\n+    print(f\"Unknown command: {cmdname}\\n\")\n     print('Use \"scrapy\" to see available commands')\n \n \n@@ -133,7 +133,7 @@ def execute(argv=None, settings=None):\n         sys.exit(2)\n \n     cmd = cmds[cmdname]\n-    parser.usage = \"scrapy %s %s\" % (cmdname, cmd.syntax())\n+    parser.usage = f\"scrapy {cmdname} {cmd.syntax()}\"\n     parser.description = cmd.long_desc()\n     settings.setdict(cmd.default_settings, priority='command')\n     cmd.settings = settings\n@@ -155,7 +155,7 @@ def _run_command(cmd, args, opts):\n \n def _run_command_profiled(cmd, args, opts):\n     if opts.profile:\n-        sys.stderr.write(\"scrapy: writing cProfile stats to %r\\n\" % opts.profile)\n+        sys.stderr.write(f\"scrapy: writing cProfile stats to {opts.profile!r}\\n\")\n     loc = locals()\n     p = cProfile.Profile()\n     p.runctx('cmd.run(args, opts)', globals(), loc)\n\n@@ -61,7 +61,7 @@ class ScrapyCommand:\n         group.add_option(\"--logfile\", metavar=\"FILE\",\n                          help=\"log file. if omitted stderr will be used\")\n         group.add_option(\"-L\", \"--loglevel\", metavar=\"LEVEL\", default=None,\n-                         help=\"log level (default: %s)\" % self.settings['LOG_LEVEL'])\n+                         help=f\"log level (default: {self.settings['LOG_LEVEL']})\")\n         group.add_option(\"--nolog\", action=\"store_true\",\n                          help=\"disable logging completely\")\n         group.add_option(\"--profile\", metavar=\"FILE\", default=None,\n\n@@ -50,7 +50,7 @@ class _BenchSpider(scrapy.Spider):\n \n     def start_requests(self):\n         qargs = {'total': self.total, 'show': self.show}\n-        url = '{}?{}'.format(self.baseurl, urlencode(qargs, doseq=1))\n+        url = f'{self.baseurl}?{urlencode(qargs, doseq=1)}'\n         return [scrapy.Request(url, dont_filter=True)]\n \n     def parse(self, response):\n\n@@ -17,7 +17,7 @@ class TextTestResult(_TextTestResult):\n         plural = \"s\" if run != 1 else \"\"\n \n         writeln(self.separator2)\n-        writeln(\"Ran %d contract%s in %.3fs\" % (run, plural, stop - start))\n+        writeln(f\"Ran {run} contract{plural} in {stop - start:.3f}\")\n         writeln()\n \n         infos = []\n@@ -25,14 +25,14 @@ class TextTestResult(_TextTestResult):\n             write(\"FAILED\")\n             failed, errored = map(len, (self.failures, self.errors))\n             if failed:\n-                infos.append(\"failures=%d\" % failed)\n+                infos.append(f\"failures={failed}\")\n             if errored:\n-                infos.append(\"errors=%d\" % errored)\n+                infos.append(f\"errors={errored}\")\n         else:\n             write(\"OK\")\n \n         if infos:\n-            writeln(\" (%s)\" % (\", \".join(infos),))\n+            writeln(f\" ({', '.join(infos)})\")\n         else:\n             write(\"\\n\")\n \n@@ -85,7 +85,7 @@ class Command(ScrapyCommand):\n                         continue\n                     print(spider)\n                     for method in sorted(methods):\n-                        print('  * %s' % method)\n+                        print(f'  * {method}')\n             else:\n                 start = time.time()\n                 self.crawler_process.start()\n\n@@ -32,8 +32,8 @@ class Command(ScrapyCommand):\n         try:\n             spidercls = self.crawler_process.spider_loader.load(args[0])\n         except KeyError:\n-            return self._err(\"Spider not found: %s\" % args[0])\n+            return self._err(f\"Spider not found: {args[0]}\")\n \n         sfile = sys.modules[spidercls.__module__].__file__\n         sfile = sfile.replace('.pyc', '.py')\n-        self.exitcode = os.system('%s \"%s\"' % (editor, sfile))\n+        self.exitcode = os.system(f'{editor} \"{sfile}\"')\n\n@@ -73,17 +73,18 @@ class Command(ScrapyCommand):\n         if template_file:\n             self._genspider(module, name, domain, opts.template, template_file)\n             if opts.edit:\n-                self.exitcode = os.system('scrapy edit \"%s\"' % name)\n+                self.exitcode = os.system(f'scrapy edit \"{name}\"')\n \n     def _genspider(self, module, name, domain, template_name, template_file):\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n+        capitalized_module = ''.join(s.capitalize() for s in module.split('_'))\n         tvars = {\n             'project_name': self.settings.get('BOT_NAME'),\n             'ProjectName': string_camelcase(self.settings.get('BOT_NAME')),\n             'module': module,\n             'name': name,\n             'domain': domain,\n-            'classname': '%sSpider' % ''.join(s.capitalize() for s in module.split('_'))\n+            'classname': f'{capitalized_module}Spider'\n         }\n         if self.settings.get('NEWSPIDER_MODULE'):\n             spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n@@ -91,32 +92,32 @@ class Command(ScrapyCommand):\n         else:\n             spiders_module = None\n             spiders_dir = \".\"\n-        spider_file = \"%s.py\" % join(spiders_dir, module)\n+        spider_file = f\"{join(spiders_dir, module)}.py\"\n         shutil.copyfile(template_file, spider_file)\n         render_templatefile(spider_file, **tvars)\n-        print(\"Created spider %r using template %r \"\n-              % (name, template_name), end=('' if spiders_module else '\\n'))\n+        print(f\"Created spider {name!r} using template {template_name!r} \",\n+              end=('' if spiders_module else '\\n'))\n         if spiders_module:\n-            print(\"in module:\\n  %s.%s\" % (spiders_module.__name__, module))\n+            print(\"in module:\\n  {spiders_module.__name__}.{module}\")\n \n     def _find_template(self, template):\n-        template_file = join(self.templates_dir, '%s.tmpl' % template)\n+        template_file = join(self.templates_dir, f'{template}.tmpl')\n         if exists(template_file):\n             return template_file\n-        print(\"Unable to find template: %s\\n\" % template)\n+        print(f\"Unable to find template: {template}\\n\")\n         print('Use \"scrapy genspider --list\" to see all available templates.')\n \n     def _list_templates(self):\n         print(\"Available templates:\")\n         for filename in sorted(os.listdir(self.templates_dir)):\n             if filename.endswith('.tmpl'):\n-                print(\"  %s\" % splitext(filename)[0])\n+                print(f\"  {splitext(filename)[0]}\")\n \n     def _spider_exists(self, name):\n         if not self.settings.get('NEWSPIDER_MODULE'):\n             # if run as a standalone command and file with same filename already exists\n             if exists(name + \".py\"):\n-                print(\"%s already exists\" % (abspath(name + \".py\")))\n+                print(f\"{abspath(name + '.py')} already exists\")\n                 return True\n             return False\n \n@@ -126,8 +127,8 @@ class Command(ScrapyCommand):\n             pass\n         else:\n             # if spider with same name exists\n-            print(\"Spider %r already exists in module:\" % name)\n-            print(\"  %s\" % spidercls.__module__)\n+            print(f\"Spider {name!r} already exists in module:\")\n+            print(f\"  {spidercls.__module__}\")\n             return True\n \n         # a file with the same name exists in the target directory\n@@ -135,7 +136,7 @@ class Command(ScrapyCommand):\n         spiders_dir = dirname(spiders_module.__file__)\n         spiders_dir_abs = abspath(spiders_dir)\n         if exists(join(spiders_dir_abs, name + \".py\")):\n-            print(\"%s already exists\" % (join(spiders_dir_abs, (name + \".py\"))))\n+            print(f\"{join(spiders_dir_abs, (name + '.py'))} already exists\")\n             return True\n \n         return False\n\n@@ -96,13 +96,13 @@ class Command(BaseRunSpiderCommand):\n \n         if opts.verbose:\n             for level in range(1, self.max_level + 1):\n-                print('\\n>>> DEPTH LEVEL: %s <<<' % level)\n+                print(f'\\n>>> DEPTH LEVEL: {level} <<<')\n                 if not opts.noitems:\n                     self.print_items(level, colour)\n                 if not opts.nolinks:\n                     self.print_requests(level, colour)\n         else:\n-            print('\\n>>> STATUS DEPTH LEVEL %s <<<' % self.max_level)\n+            print(f'\\n>>> STATUS DEPTH LEVEL {self.max_level} <<<')\n             if not opts.noitems:\n                 self.print_items(colour=colour)\n             if not opts.nolinks:\n\n@@ -12,7 +12,7 @@ def _import_file(filepath):\n     dirname, file = os.path.split(abspath)\n     fname, fext = os.path.splitext(file)\n     if fext != '.py':\n-        raise ValueError(\"Not a Python source file: %s\" % abspath)\n+        raise ValueError(f\"Not a Python source file: {abspath}\")\n     if dirname:\n         sys.path = [dirname] + sys.path\n     try:\n@@ -42,14 +42,14 @@ class Command(BaseRunSpiderCommand):\n             raise UsageError()\n         filename = args[0]\n         if not os.path.exists(filename):\n-            raise UsageError(\"File not found: %s\\n\" % filename)\n+            raise UsageError(f\"File not found: {filename}\\n\")\n         try:\n             module = _import_file(filename)\n         except (ImportError, ValueError) as e:\n-            raise UsageError(\"Unable to load %r: %s\\n\" % (filename, e))\n+            raise UsageError(f\"Unable to load {filename!r}: {e}\\n\")\n         spclasses = list(iter_spider_classes(module))\n         if not spclasses:\n-            raise UsageError(\"No spider found in file: %s\\n\" % filename)\n+            raise UsageError(f\"No spider found in file: {filename}\\n\")\n         spidercls = spclasses.pop()\n \n         self.crawler_process.crawl(spidercls, **opts.spargs)\n\n@@ -52,7 +52,7 @@ class Command(ScrapyCommand):\n             print('Error: Project names must begin with a letter and contain'\n                   ' only\\nletters, numbers and underscores')\n         elif _module_exists(project_name):\n-            print('Error: Module %r already exists' % project_name)\n+            print(f'Error: Module {project_name!r} already exists')\n         else:\n             return True\n         return False\n@@ -100,7 +100,7 @@ class Command(ScrapyCommand):\n \n         if exists(join(project_dir, 'scrapy.cfg')):\n             self.exitcode = 1\n-            print('Error: scrapy.cfg already exists in %s' % abspath(project_dir))\n+            print(f'Error: scrapy.cfg already exists in {abspath(project_dir)}')\n             return\n \n         if not self._is_valid_name(project_name):\n@@ -113,11 +113,11 @@ class Command(ScrapyCommand):\n             path = join(*paths)\n             tplfile = join(project_dir, string.Template(path).substitute(project_name=project_name))\n             render_templatefile(tplfile, project_name=project_name, ProjectName=string_camelcase(project_name))\n-        print(\"New Scrapy project '%s', using template directory '%s', \"\n-              \"created in:\" % (project_name, self.templates_dir))\n-        print(\"    %s\\n\" % abspath(project_dir))\n+        print(f\"New Scrapy project '{project_name}', using template directory \"\n+              f\"'{self.templates_dir}', created in:\")\n+        print(f\"    {abspath(project_dir)}\\n\")\n         print(\"You can start your first spider with:\")\n-        print(\"    cd %s\" % project_dir)\n+        print(f\"    cd {project_dir}\")\n         print(\"    scrapy genspider example example.com\")\n \n     @property\n\n@@ -23,8 +23,8 @@ class Command(ScrapyCommand):\n         if opts.verbose:\n             versions = scrapy_components_versions()\n             width = max(len(n) for (n, _) in versions)\n-            patt = \"%-{}s : %s\".format(width)\n+            patt = f\"%-{width}s : %s\"\n             for name, version in versions:\n                 print(patt % (name, version))\n         else:\n-            print(\"Scrapy %s\" % scrapy.__version__)\n+            print(f\"Scrapy {scrapy.__version__}\")\n\n@@ -112,8 +112,8 @@ class Contract:\n     request_cls = None\n \n     def __init__(self, method, *args):\n-        self.testcase_pre = _create_testcase(method, '@%s pre-hook' % self.name)\n-        self.testcase_post = _create_testcase(method, '@%s post-hook' % self.name)\n+        self.testcase_pre = _create_testcase(method, f'@{self.name} pre-hook')\n+        self.testcase_post = _create_testcase(method, f'@{self.name} post-hook')\n         self.args = args\n \n     def add_pre_hook(self, request, results):\n@@ -172,8 +172,8 @@ def _create_testcase(method, desc):\n \n     class ContractTestCase(TestCase):\n         def __str__(_self):\n-            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n+            return f\"[{spider}] {method.__name__} ({desc})\"\n \n-    name = '%s_%s' % (spider, method.__name__)\n+    name = f'{spider}_{method.__name__}'\n     setattr(ContractTestCase, name, lambda x: x)\n     return ContractTestCase(name)\n\n@@ -60,8 +60,7 @@ class ReturnsContract(Contract):\n \n         if len(self.args) not in [1, 2, 3]:\n             raise ValueError(\n-                \"Incorrect argument quantity: expected 1, 2 or 3, got %i\"\n-                % len(self.args)\n+                f\"Incorrect argument quantity: expected 1, 2 or 3, got {len(self.args)}\"\n             )\n         self.obj_name = self.args[0] or None\n         self.obj_type_verifier = self.object_type_verifiers[self.obj_name]\n@@ -88,10 +87,9 @@ class ReturnsContract(Contract):\n             if self.min_bound == self.max_bound:\n                 expected = self.min_bound\n             else:\n-                expected = '%s..%s' % (self.min_bound, self.max_bound)\n+                expected = f'{self.min_bound}..{self.max_bound}'\n \n-            raise ContractFail(\"Returned %s %s, expected %s\" %\n-                               (occurrences, self.obj_name, expected))\n+            raise ContractFail(f\"Returned {occurrences} {self.obj_name}, expected {expected}\")\n \n \n class ScrapesContract(Contract):\n@@ -106,5 +104,5 @@ class ScrapesContract(Contract):\n             if is_item(x):\n                 missing = [arg for arg in self.args if arg not in ItemAdapter(x)]\n                 if missing:\n-                    missing_str = \", \".join(missing)\n-                    raise ContractFail(\"Missing fields: %s\" % missing_str)\n+                    missing_fields = \", \".join(missing)\n+                    raise ContractFail(f\"Missing fields: {missing_fields}\")\n\n@@ -41,17 +41,17 @@ class Slot:\n \n     def __repr__(self):\n         cls_name = self.__class__.__name__\n-        return \"%s(concurrency=%r, delay=%0.2f, randomize_delay=%r)\" % (\n-            cls_name, self.concurrency, self.delay, self.randomize_delay)\n+        return (f\"{cls_name}(concurrency={self.concurrency!r}, \"\n+                f\"delay={self.delay:.2f}, \"\n+                f\"randomize_delay={self.randomize_delay!r}\")\n \n     def __str__(self):\n         return (\n-            \"<downloader.Slot concurrency=%r delay=%0.2f randomize_delay=%r \"\n-            \"len(active)=%d len(queue)=%d len(transferring)=%d lastseen=%s>\" % (\n-                self.concurrency, self.delay, self.randomize_delay,\n-                len(self.active), len(self.queue), len(self.transferring),\n-                datetime.fromtimestamp(self.lastseen).isoformat()\n-            )\n+            f\"<downloader.Slot concurrency={self.concurrency!r} \"\n+            f\"delay={self.delay:.2f} randomize_delay={self.randomize_delay!r} \"\n+            f\"len(active)={len(self.active)} len(queue)={len(self.queue)} \"\n+            f\"len(transferring)={len(self.transferring)} \"\n+            f\"lastseen={datetime.fromtimestamp(self.lastseen).isoformat()}>\"\n         )\n \n \n\n@@ -71,8 +71,7 @@ class DownloadHandlers:\n         scheme = urlparse_cached(request).scheme\n         handler = self._get_handler(scheme)\n         if not handler:\n-            raise NotSupported(\"Unsupported URL scheme '%s': %s\" %\n-                               (scheme, self._notconfigured[scheme]))\n+            raise NotSupported(f\"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}\")\n         return handler.download_request(request, spider)\n \n     @defer.inlineCallbacks\n\n@@ -60,11 +60,11 @@ class HTTP11DownloadHandler:\n                 settings=settings,\n                 crawler=crawler,\n             )\n-            msg = \"\"\"\n- '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n- e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `tls_verbose_logging` argument and/or `tls_ciphers` argument.\\\n- Please upgrade your context factory class to handle them or ignore them.\"\"\" % (\n-                settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\n+            msg = f\"\"\"\n+ '{settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"]}' does not accept `method` \\\n+ argument (type OpenSSL.SSL method, e.g. OpenSSL.SSL.SSLv23_METHOD) and/or \\\n+ `tls_verbose_logging` argument and/or `tls_ciphers` argument.\\\n+ Please upgrade your context factory class to handle them or ignore them.\"\"\"\n             warnings.warn(msg)\n         self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')\n         self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')\n@@ -169,8 +169,9 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n             else:\n                 extra = rcvd_bytes[:32]\n             self._tunnelReadyDeferred.errback(\n-                TunnelError('Could not open CONNECT tunnel with proxy %s:%s [%r]' % (\n-                    self._host, self._port, extra)))\n+                TunnelError('Could not open CONNECT tunnel with proxy '\n+                            f'{self._host}:{self._port} [{extra!r}]')\n+            )\n \n     def connectFailed(self, reason):\n         \"\"\"Propagates the errback to the appropriate deferred.\"\"\"\n@@ -371,7 +372,7 @@ class ScrapyAgent:\n         if self._txresponse:\n             self._txresponse._transport.stopProducing()\n \n-        raise TimeoutError(\"Getting %s took longer than %s seconds.\" % (url, timeout))\n+        raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n \n     def _cb_latency(self, result, request, start_time):\n         request.meta['download_latency'] = time() - start_time\n\n@@ -56,7 +56,7 @@ class S3DownloadHandler:\n             import botocore.credentials\n             kw.pop('anon', None)\n             if kw:\n-                raise TypeError('Unexpected keyword arguments: %s' % kw)\n+                raise TypeError(f'Unexpected keyword arguments: {kw}')\n             if not self.anon:\n                 SignerCls = botocore.auth.AUTH_TYPE_MAPS['s3']\n                 self._signer = SignerCls(botocore.credentials.Credentials(\n@@ -85,14 +85,14 @@ class S3DownloadHandler:\n         scheme = 'https' if request.meta.get('is_secure') else 'http'\n         bucket = p.hostname\n         path = p.path + '?' + p.query if p.query else p.path\n-        url = '%s://%s.s3.amazonaws.com%s' % (scheme, bucket, path)\n+        url = f'{scheme}://{bucket}.s3.amazonaws.com{path}'\n         if self.anon:\n             request = request.replace(url=url)\n         elif self._signer is not None:\n             import botocore.awsrequest\n             awsrequest = botocore.awsrequest.AWSRequest(\n                 method=request.method,\n-                url='%s://s3.amazonaws.com/%s%s' % (scheme, bucket, path),\n+                url=f'{scheme}://s3.amazonaws.com/{bucket}{path}',\n                 headers=request.headers.to_unicode_dict(),\n                 data=request.body)\n             self._signer.add_auth(awsrequest)\n\n@@ -36,8 +36,9 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 response = yield deferred_from_coro(method(request=request, spider=spider))\n                 if response is not None and not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput(\n-                        \"Middleware %s.process_request must return None, Response or Request, got %s\"\n-                        % (method.__self__.__class__.__name__, response.__class__.__name__)\n+                        f\"Middleware {method.__self__.__class__.__name__}\"\n+                        \".process_request must return None, Response or \"\n+                        f\"Request, got {response.__class__.__name__}\"\n                     )\n                 if response:\n                     return response\n@@ -54,8 +55,9 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 response = yield deferred_from_coro(method(request=request, response=response, spider=spider))\n                 if not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput(\n-                        \"Middleware %s.process_response must return Response or Request, got %s\"\n-                        % (method.__self__.__class__.__name__, type(response))\n+                        f\"Middleware {method.__self__.__class__.__name__}\"\n+                        \".process_response must return Response or Request, \"\n+                        f\"got {type(response)}\"\n                     )\n                 if isinstance(response, Request):\n                     return response\n@@ -68,8 +70,9 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))\n                 if response is not None and not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput(\n-                        \"Middleware %s.process_exception must return None, Response or Request, got %s\"\n-                        % (method.__self__.__class__.__name__, type(response))\n+                        f\"Middleware {method.__self__.__class__.__name__}\"\n+                        \".process_exception must return None, Response or \"\n+                        f\"Request, got {type(response)}\"\n                     )\n                 if response:\n                     return response\n\n@@ -88,8 +88,8 @@ class ScrapyHTTPPageGetter(HTTPClient):\n             self.transport.stopProducing()\n \n         self.factory.noPage(\n-            defer.TimeoutError(\"Getting %s took longer than %s seconds.\"\n-                               % (self.factory.url, self.factory.timeout)))\n+            defer.TimeoutError(f\"Getting {self.factory.url} took longer \"\n+                               f\"than {self.factory.timeout} seconds.\"))\n \n \n # This class used to inherit from Twisted’s\n@@ -155,7 +155,7 @@ class ScrapyHTTPClientFactory(ClientFactory):\n             self.headers['Content-Length'] = 0\n \n     def __repr__(self):\n-        return \"<%s: %s>\" % (self.__class__.__name__, self.url)\n+        return f\"<{self.__class__.__name__}: {self.url}>\"\n \n     def _cancelTimeout(self, result, timeoutCall):\n         if timeoutCall.active():\n\n@@ -171,8 +171,8 @@ class ExecutionEngine:\n     def _handle_downloader_output(self, response, request, spider):\n         if not isinstance(response, (Request, Response, Failure)):\n             raise TypeError(\n-                \"Incorrect type: expected Request, Response or Failure, got %s: %r\"\n-                % (type(response), response)\n+                \"Incorrect type: expected Request, Response or Failure, got \"\n+                f\"{type(response)}: {response!r}\"\n             )\n         # downloader middleware can return requests (for example, redirects)\n         if isinstance(response, Request):\n@@ -214,7 +214,7 @@ class ExecutionEngine:\n \n     def crawl(self, request, spider):\n         if spider not in self.open_spiders:\n-            raise RuntimeError(\"Spider %r not opened when crawling: %s\" % (spider.name, request))\n+            raise RuntimeError(f\"Spider {spider.name!r} not opened when crawling: {request}\")\n         self.schedule(request, spider)\n         self.slot.nextcall.schedule()\n \n@@ -239,8 +239,8 @@ class ExecutionEngine:\n         def _on_success(response):\n             if not isinstance(response, (Response, Request)):\n                 raise TypeError(\n-                    \"Incorrect type: expected Response or Request, got %s: %r\"\n-                    % (type(response), response)\n+                    \"Incorrect type: expected Response or Request, got \"\n+                    f\"{type(response)}: {response!r}\"\n                 )\n             if isinstance(response, Response):\n                 if response.request is None:\n@@ -268,7 +268,7 @@ class ExecutionEngine:\n     @defer.inlineCallbacks\n     def open_spider(self, spider, start_requests=(), close_if_idle=True):\n         if not self.has_capacity():\n-            raise RuntimeError(\"No free spider slot when opening %r\" % spider.name)\n+            raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n         logger.info(\"Spider opened\", extra={'spider': spider})\n         nextcall = CallLaterOnce(self._next_request, spider)\n         scheduler = self.scheduler_cls.from_crawler(self.crawler)\n\n@@ -125,7 +125,7 @@ class Scraper:\n         Handle the downloaded response or failure through the spider callback/errback\n         \"\"\"\n         if not isinstance(result, (Response, Failure)):\n-            raise TypeError(\"Incorrect type: expected Response or Failure, got %s: %r\" % (type(result), result))\n+            raise TypeError(f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\")\n         dfd = self._scrape2(result, request, spider)  # returns spider's processed output\n         dfd.addErrback(self.handle_spider_error, request, result, spider)\n         dfd.addCallback(self.handle_spider_output, request, result, spider)\n@@ -173,7 +173,7 @@ class Scraper:\n             spider=spider\n         )\n         self.crawler.stats.inc_value(\n-            \"spider_exceptions/%s\" % _failure.value.__class__.__name__,\n+            f\"spider_exceptions/{_failure.value.__class__.__name__}\",\n             spider=spider\n         )\n \n\n@@ -19,10 +19,7 @@ def _isiterable(possible_iterator):\n \n \n def _fname(f):\n-    return \"{}.{}\".format(\n-        f.__self__.__class__.__name__,\n-        f.__func__.__name__\n-    )\n+    return f\"{f.__self__.__class__.__name__}.{f.__func__.__name__}\"\n \n \n class SpiderMiddlewareManager(MiddlewareManager):\n@@ -51,8 +48,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 try:\n                     result = method(response=response, spider=spider)\n                     if result is not None:\n-                        msg = \"Middleware {} must return None or raise an exception, got {}\"\n-                        raise _InvalidOutput(msg.format(_fname(method), type(result)))\n+                        msg = (f\"Middleware {_fname(method)} must return None \"\n+                               f\"or raise an exception, got {type(result)}\")\n+                        raise _InvalidOutput(msg)\n                 except _InvalidOutput:\n                     raise\n                 except Exception:\n@@ -86,8 +84,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 elif result is None:\n                     continue\n                 else:\n-                    msg = \"Middleware {} must return None or an iterable, got {}\"\n-                    raise _InvalidOutput(msg.format(_fname(method), type(result)))\n+                    msg = (f\"Middleware {_fname(method)} must return None \"\n+                           f\"or an iterable, got {type(result)}\")\n+                    raise _InvalidOutput(msg)\n             return _failure\n \n         def process_spider_output(result, start_index=0):\n@@ -110,8 +109,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 if _isiterable(result):\n                     result = _evaluate_iterable(result, method_index + 1, recovered)\n                 else:\n-                    msg = \"Middleware {} must return an iterable, got {}\"\n-                    raise _InvalidOutput(msg.format(_fname(method), type(result)))\n+                    msg = (f\"Middleware {_fname(method)} must return an \"\n+                           f\"iterable, got {type(result)}\")\n+                    raise _InvalidOutput(msg)\n \n             return MutableChain(result, recovered)\n \n\n@@ -54,8 +54,8 @@ class CookiesMiddleware:\n             cl = [to_unicode(c, errors='replace')\n                   for c in request.headers.getlist('Cookie')]\n             if cl:\n-                cookies = \"\\n\".join(\"Cookie: {}\\n\".format(c) for c in cl)\n-                msg = \"Sending cookies to: {}\\n{}\".format(request, cookies)\n+                cookies = \"\\n\".join(f\"Cookie: {c}\\n\" for c in cl)\n+                msg = f\"Sending cookies to: {request}\\n{cookies}\"\n                 logger.debug(msg, extra={'spider': spider})\n \n     def _debug_set_cookie(self, response, spider):\n@@ -63,8 +63,8 @@ class CookiesMiddleware:\n             cl = [to_unicode(c, errors='replace')\n                   for c in response.headers.getlist('Set-Cookie')]\n             if cl:\n-                cookies = \"\\n\".join(\"Set-Cookie: {}\\n\".format(c) for c in cl)\n-                msg = \"Received cookies from: {}\\n{}\".format(response, cookies)\n+                cookies = \"\\n\".join(f\"Set-Cookie: {c}\\n\" for c in cl)\n+                msg = f\"Received cookies from: {response}\\n{cookies}\"\n                 logger.debug(msg, extra={'spider': spider})\n \n     def _format_cookie(self, cookie, request):\n@@ -90,9 +90,9 @@ class CookiesMiddleware:\n                                    request, cookie)\n                     decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")\n \n-        cookie_str = \"{}={}\".format(decoded.pop(\"name\"), decoded.pop(\"value\"))\n+        cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n         for key, value in decoded.items():  # path, domain\n-            cookie_str += \"; {}={}\".format(key.capitalize(), value)\n+            cookie_str += f\"; {key.capitalize()}={value}\"\n         return cookie_str\n \n     def _get_request_cookies(self, jar, request):\n\n@@ -24,7 +24,7 @@ class HttpProxyMiddleware:\n \n     def _basic_auth_header(self, username, password):\n         user_pass = to_bytes(\n-            '%s:%s' % (unquote(username), unquote(password)),\n+            f'{unquote(username)}:{unquote(password)}',\n             encoding=self.auth_encoding)\n         return base64.b64encode(user_pass)\n \n\n@@ -88,7 +88,7 @@ class RetryMiddleware:\n                 reason = global_object_name(reason.__class__)\n \n             stats.inc_value('retry/count')\n-            stats.inc_value('retry/reason_count/%s' % reason)\n+            stats.inc_value(f'retry/reason_count/{reason}')\n             return retryreq\n         else:\n             stats.inc_value('retry/max_reached')\n\n@@ -61,7 +61,7 @@ class RobotsTxtMiddleware:\n \n         if netloc not in self._parsers:\n             self._parsers[netloc] = Deferred()\n-            robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n+            robotsurl = f\"{url.scheme}://{url.netloc}/robots.txt\"\n             robotsreq = Request(\n                 robotsurl,\n                 priority=self.DOWNLOAD_PRIORITY,\n@@ -94,7 +94,7 @@ class RobotsTxtMiddleware:\n \n     def _parse_robots(self, response, netloc, spider):\n         self.crawler.stats.inc_value('robotstxt/response_count')\n-        self.crawler.stats.inc_value('robotstxt/response_status_count/{}'.format(response.status))\n+        self.crawler.stats.inc_value(f'robotstxt/response_status_count/{response.status}')\n         rp = self._parserimpl.from_crawler(self.crawler, response.body)\n         rp_dfd = self._parsers[netloc]\n         self._parsers[netloc] = rp\n@@ -102,7 +102,7 @@ class RobotsTxtMiddleware:\n \n     def _robots_error(self, failure, netloc):\n         if failure.type is not IgnoreRequest:\n-            key = 'robotstxt/exception_count/{}'.format(failure.type)\n+            key = f'robotstxt/exception_count/{failure.type}'\n             self.crawler.stats.inc_value(key)\n         rp_dfd = self._parsers[netloc]\n         self._parsers[netloc] = None\n\n@@ -17,13 +17,13 @@ class DownloaderStats:\n \n     def process_request(self, request, spider):\n         self.stats.inc_value('downloader/request_count', spider=spider)\n-        self.stats.inc_value('downloader/request_method_count/%s' % request.method, spider=spider)\n+        self.stats.inc_value(f'downloader/request_method_count/{request.method}', spider=spider)\n         reqlen = len(request_httprepr(request))\n         self.stats.inc_value('downloader/request_bytes', reqlen, spider=spider)\n \n     def process_response(self, request, response, spider):\n         self.stats.inc_value('downloader/response_count', spider=spider)\n-        self.stats.inc_value('downloader/response_status_count/%s' % response.status, spider=spider)\n+        self.stats.inc_value(f'downloader/response_status_count/{response.status}', spider=spider)\n         reslen = len(response_httprepr(response))\n         self.stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n         return response\n@@ -31,4 +31,4 @@ class DownloaderStats:\n     def process_exception(self, request, exception, spider):\n         ex_class = global_object_name(exception.__class__)\n         self.stats.inc_value('downloader/exception_count', spider=spider)\n-        self.stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)\n+        self.stats.inc_value(f'downloader/exception_type_count/{ex_class}', spider=spider)\n\n@@ -39,7 +39,7 @@ class BaseItemExporter:\n         self.export_empty_fields = options.pop('export_empty_fields', False)\n         self.indent = options.pop('indent', None)\n         if not dont_fail and options:\n-            raise TypeError(\"Unexpected options: %s\" % ', '.join(options.keys()))\n+            raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")\n \n     def export_item(self, item):\n         raise NotImplementedError\n\n@@ -43,4 +43,4 @@ class CoreStats:\n     def item_dropped(self, item, spider, exception):\n         reason = exception.__class__.__name__\n         self.stats.inc_value('item_dropped_count', spider=spider)\n-        self.stats.inc_value('item_dropped_reasons_count/%s' % reason, spider=spider)\n+        self.stats.inc_value(f'item_dropped_reasons_count/{reason}', spider=spider)\n\n@@ -48,7 +48,7 @@ class StackTraceDump:\n         for id_, frame in sys._current_frames().items():\n             name = id2name.get(id_, '')\n             dump = ''.join(traceback.format_stack(frame))\n-            dumps += \"# Thread: {0}({1})\\n{2}\\n\".format(name, id_, dump)\n+            dumps += f\"# Thread: {name}({id_})\\n{dump}\\n\"\n         return dumps\n \n \n\n@@ -223,7 +223,7 @@ class DbmCacheStorage:\n         self.db = None\n \n     def open_spider(self, spider):\n-        dbpath = os.path.join(self.cachedir, '%s.db' % spider.name)\n+        dbpath = os.path.join(self.cachedir, f'{spider.name}.db')\n         self.db = self.dbmodule.open(dbpath, 'c')\n \n         logger.debug(\"Using DBM cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})\n@@ -251,13 +251,13 @@ class DbmCacheStorage:\n             'headers': dict(response.headers),\n             'body': response.body,\n         }\n-        self.db['%s_data' % key] = pickle.dumps(data, protocol=4)\n-        self.db['%s_time' % key] = str(time())\n+        self.db[f'{key}_data'] = pickle.dumps(data, protocol=4)\n+        self.db[f'{key}_time'] = str(time())\n \n     def _read_data(self, spider, request):\n         key = self._request_key(request)\n         db = self.db\n-        tkey = '%s_time' % key\n+        tkey = f'{key}_time'\n         if tkey not in db:\n             return  # not found\n \n@@ -265,7 +265,7 @@ class DbmCacheStorage:\n         if 0 < self.expiration_secs < time() - float(ts):\n             return  # expired\n \n-        return pickle.loads(db['%s_data' % key])\n+        return pickle.loads(db[f'{key}_data'])\n \n     def _request_key(self, request):\n         return request_fingerprint(request)\n\n@@ -30,4 +30,4 @@ class MemoryDebugger:\n         for cls, wdict in live_refs.items():\n             if not wdict:\n                 continue\n-            self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict), spider=spider)\n+            self.stats.set_value(f'memdebug/live_refs/{cls.__name__}', len(wdict), spider=spider)\n\n@@ -82,8 +82,8 @@ class MemoryUsage:\n                          {'memusage': mem}, extra={'crawler': self.crawler})\n             if self.notify_mails:\n                 subj = (\n-                    \"%s terminated: memory usage exceeded %dM at %s\"\n-                    % (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n+                    f\"{self.crawler.settings['BOT_NAME']} terminated: \"\n+                    f\"memory usage exceeded {mem}M at {socket.gethostname()}\"\n                 )\n                 self._send_report(self.notify_mails, subj)\n                 self.crawler.stats.set_value('memusage/limit_notified', 1)\n@@ -105,8 +105,8 @@ class MemoryUsage:\n                            {'memusage': mem}, extra={'crawler': self.crawler})\n             if self.notify_mails:\n                 subj = (\n-                    \"%s warning: memory usage reached %dM at %s\"\n-                    % (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n+                    f\"{self.crawler.settings['BOT_NAME']} warning: \"\n+                    f\"memory usage reached {mem}M at {socket.gethostname()}\"\n                 )\n                 self._send_report(self.notify_mails, subj)\n                 self.crawler.stats.set_value('memusage/warning_notified', 1)\n@@ -115,9 +115,9 @@ class MemoryUsage:\n     def _send_report(self, rcpts, subject):\n         \"\"\"send notification mail with some additional useful info\"\"\"\n         stats = self.crawler.stats\n-        s = \"Memory usage at engine startup : %dM\\r\\n\" % (stats.get_value('memusage/startup')/1024/1024)\n-        s += \"Maximum memory usage           : %dM\\r\\n\" % (stats.get_value('memusage/max')/1024/1024)\n-        s += \"Current memory usage           : %dM\\r\\n\" % (self.get_virtual_size()/1024/1024)\n+        s = f\"Memory usage at engine startup : {stats.get_value('memusage/startup')/1024/1024}M\\r\\n\"\n+        s += f\"Maximum memory usage          : {stats.get_value('memusage/max')/1024/1024}M\\r\\n\"\n+        s += f\"Current memory usage          : {self.get_virtual_size()/1024/1024}M\\r\\n\"\n \n         s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n         s += \"\\r\\n\"\n\n@@ -28,7 +28,7 @@ class StatsMailer:\n     def spider_closed(self, spider):\n         spider_stats = self.stats.get_stats(spider)\n         body = \"Global stats\\n\\n\"\n-        body += \"\\n\".join(\"%-50s : %s\" % i for i in self.stats.get_stats().items())\n-        body += \"\\n\\n%s stats\\n\\n\" % spider.name\n-        body += \"\\n\".join(\"%-50s : %s\" % i for i in spider_stats.items())\n-        return self.mail.send(self.recipients, \"Scrapy stats for: %s\" % spider.name, body)\n+        body += \"\\n\".join(f\"{i:<50} : {self.stats.get_stats()[i]}\" for i in self.stats.get_stats())\n+        body += f\"\\n\\n{spider.name} stats\\n\\n\"\n+        body += \"\\n\".join(f\"{i:<50} : {spider_stats[i]}\" for i in spider_stats)\n+        return self.mail.send(self.recipients, f\"Scrapy stats for: {spider.name}\", body)\n\n@@ -1,6 +1,6 @@\n def obsolete_setter(setter, attrname):\n     def newsetter(self, value):\n         c = self.__class__.__name__\n-        msg = \"%s.%s is not modifiable, use %s.replace() instead\" % (c, attrname, c)\n+        msg = f\"{c}.{attrname} is not modifiable, use {c}.replace() instead\"\n         raise AttributeError(msg)\n     return newsetter\n\n@@ -33,7 +33,7 @@ class Headers(CaselessDict):\n         elif isinstance(x, int):\n             return str(x).encode(self.encoding)\n         else:\n-            raise TypeError('Unsupported value type: {}'.format(type(x)))\n+            raise TypeError(f'Unsupported value type: {type(x)}')\n \n     def __getitem__(self, key):\n         try:\n\n@@ -25,13 +25,13 @@ class Request(object_ref):\n         self._set_url(url)\n         self._set_body(body)\n         if not isinstance(priority, int):\n-            raise TypeError(\"Request priority not an integer: %r\" % priority)\n+            raise TypeError(f\"Request priority not an integer: {priority!r}\")\n         self.priority = priority\n \n         if callback is not None and not callable(callback):\n-            raise TypeError('callback must be a callable, got %s' % type(callback).__name__)\n+            raise TypeError(f'callback must be a callable, got {type(callback).__name__}')\n         if errback is not None and not callable(errback):\n-            raise TypeError('errback must be a callable, got %s' % type(errback).__name__)\n+            raise TypeError(f'errback must be a callable, got {type(errback).__name__}')\n         self.callback = callback\n         self.errback = errback\n \n@@ -60,13 +60,13 @@ class Request(object_ref):\n \n     def _set_url(self, url):\n         if not isinstance(url, str):\n-            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n+            raise TypeError(f'Request url must be str or unicode, got {type(url).__name__}')\n \n         s = safe_url_string(url, self.encoding)\n         self._url = escape_ajax(s)\n \n         if ('://' not in self._url) and (not self._url.startswith('data:')):\n-            raise ValueError('Missing scheme in request url: %s' % self._url)\n+            raise ValueError(f'Missing scheme in request url: {self._url}')\n \n     url = property(_get_url, obsolete_setter(_set_url, 'url'))\n \n@@ -86,7 +86,7 @@ class Request(object_ref):\n         return self._encoding\n \n     def __str__(self):\n-        return \"<%s %s>\" % (self.method, self.url)\n+        return f\"<{self.method} {self.url}>\"\n \n     __repr__ = __str__\n \n\n@@ -80,15 +80,15 @@ def _get_form(response, formname, formid, formnumber, formxpath):\n                             base_url=get_base_url(response))\n     forms = root.xpath('//form')\n     if not forms:\n-        raise ValueError(\"No <form> element found in %s\" % response)\n+        raise ValueError(f\"No <form> element found in {response}\")\n \n     if formname is not None:\n-        f = root.xpath('//form[@name=\"%s\"]' % formname)\n+        f = root.xpath(f'//form[@name=\"{formname}\"]')\n         if f:\n             return f[0]\n \n     if formid is not None:\n-        f = root.xpath('//form[@id=\"%s\"]' % formid)\n+        f = root.xpath(f'//form[@id=\"{formid}\"]')\n         if f:\n             return f[0]\n \n@@ -103,7 +103,7 @@ def _get_form(response, formname, formid, formnumber, formxpath):\n                 el = el.getparent()\n                 if el is None:\n                     break\n-        raise ValueError('No <form> element found with %s' % formxpath)\n+        raise ValueError(f'No <form> element found with {formxpath}')\n \n     # If we get here, it means that either formname was None\n     # or invalid\n@@ -111,8 +111,7 @@ def _get_form(response, formname, formid, formnumber, formxpath):\n         try:\n             form = forms[formnumber]\n         except IndexError:\n-            raise IndexError(\"Form number %d not found in %s\" %\n-                             (formnumber, response))\n+            raise IndexError(f\"Form number {formnumber} not found in {response}\")\n         else:\n             return form\n \n@@ -205,12 +204,12 @@ def _get_clickable(clickdata, form):\n \n     # We didn't find it, so now we build an XPath expression out of the other\n     # arguments, because they can be used as such\n-    xpath = './/*' + ''.join('[@%s=\"%s\"]' % c for c in clickdata.items())\n+    xpath = './/*' + ''.join(f'[@{key}=\"{clickdata[key]}\"]' for key in clickdata)\n     el = form.xpath(xpath)\n     if len(el) == 1:\n         return (el[0].get('name'), el[0].get('value') or '')\n     elif len(el) > 1:\n-        raise ValueError(\"Multiple elements found (%r) matching the criteria \"\n-                         \"in clickdata: %r\" % (el, clickdata))\n+        raise ValueError(f\"Multiple elements found ({el!r}) matching the \"\n+                         f\"criteria in clickdata: {clickdata!r}\")\n     else:\n-        raise ValueError('No clickable element matching clickdata: %r' % (clickdata,))\n+        raise ValueError(f'No clickable element matching clickdata: {clickdata!r}')\n\n@@ -55,8 +55,8 @@ class Response(object_ref):\n         if isinstance(url, str):\n             self._url = url\n         else:\n-            raise TypeError('%s url must be str, got %s:' %\n-                            (type(self).__name__, type(url).__name__))\n+            raise TypeError(f'{type(self).__name__} url must be str, '\n+                            f'got {type(url).__name__}')\n \n     url = property(_get_url, obsolete_setter(_set_url, 'url'))\n \n@@ -77,7 +77,7 @@ class Response(object_ref):\n     body = property(_get_body, obsolete_setter(_set_body, 'body'))\n \n     def __str__(self):\n-        return \"<%d %s>\" % (self.status, self.url)\n+        return f\"<{self.status} {self.url}>\"\n \n     __repr__ = __str__\n \n\n@@ -47,8 +47,8 @@ class TextResponse(Response):\n         self._body = b''  # used by encoding detection\n         if isinstance(body, str):\n             if self._encoding is None:\n-                raise TypeError('Cannot convert unicode body - %s has no encoding' %\n-                                type(self).__name__)\n+                raise TypeError('Cannot convert unicode body - '\n+                                f'{type(self).__name__} has no encoding')\n             self._body = body.encode(self._encoding)\n         else:\n             super()._set_body(body)\n@@ -92,7 +92,7 @@ class TextResponse(Response):\n         # _body_inferred_encoding is called\n         benc = self.encoding\n         if self._cached_ubody is None:\n-            charset = 'charset=%s' % benc\n+            charset = f'charset={benc}'\n             self._cached_ubody = html_to_unicode(charset, self.body)[1]\n         return self._cached_ubody\n \n@@ -255,12 +255,11 @@ def _url_from_selector(sel):\n         # e.g. ::attr(href) result\n         return strip_html5_whitespace(sel.root)\n     if not hasattr(sel.root, 'tag'):\n-        raise _InvalidSelector(\"Unsupported selector: %s\" % sel)\n+        raise _InvalidSelector(f\"Unsupported selector: {sel}\")\n     if sel.root.tag not in ('a', 'link'):\n-        raise _InvalidSelector(\"Only <a> and <link> elements are supported; got <%s>\" %\n-                               sel.root.tag)\n+        raise _InvalidSelector(\"Only <a> and <link> elements are supported; \"\n+                               f\"got <{sel.root.tag}>\")\n     href = sel.root.get('href')\n     if href is None:\n-        raise _InvalidSelector(\"<%s> element has no href attribute: %s\" %\n-                               (sel.root.tag, sel))\n+        raise _InvalidSelector(f\"<{sel.root.tag}> element has no href attribute: {sel}\")\n     return strip_html5_whitespace(href)\n\n@@ -96,19 +96,19 @@ class DictItem(MutableMapping, BaseItem):\n         if key in self.fields:\n             self._values[key] = value\n         else:\n-            raise KeyError(\"%s does not support field: %s\" % (self.__class__.__name__, key))\n+            raise KeyError(f\"{self.__class__.__name__} does not support field: {key}\")\n \n     def __delitem__(self, key):\n         del self._values[key]\n \n     def __getattr__(self, name):\n         if name in self.fields:\n-            raise AttributeError(\"Use item[%r] to get field value\" % name)\n+            raise AttributeError(f\"Use item[{name!r}] to get field value\")\n         raise AttributeError(name)\n \n     def __setattr__(self, name, value):\n         if not name.startswith('_'):\n-            raise AttributeError(\"Use item[%r] = %r to set field value\" % (name, value))\n+            raise AttributeError(f\"Use item[{name!r}] = {value!r} to set field value\")\n         super().__setattr__(name, value)\n \n     def __len__(self):\n\n@@ -14,7 +14,7 @@ class Link:\n     def __init__(self, url, text='', fragment='', nofollow=False):\n         if not isinstance(url, str):\n             got = url.__class__.__name__\n-            raise TypeError(\"Link urls must be str objects, got %s\" % got)\n+            raise TypeError(f\"Link urls must be str objects, got {got}\")\n         self.url = url\n         self.text = text\n         self.fragment = fragment\n@@ -33,6 +33,6 @@ class Link:\n \n     def __repr__(self):\n         return (\n-            'Link(url=%r, text=%r, fragment=%r, nofollow=%r)'\n-            % (self.url, self.text, self.fragment, self.nofollow)\n+            f'Link(url={self.url!r}, text={self.text!r}, '\n+            f'fragment={self.fragment!r}, nofollow={self.nofollow!r})'\n         )\n\n@@ -54,8 +54,8 @@ class LogFormatter:\n \n     def crawled(self, request, response, spider):\n         \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n-        request_flags = ' %s' % str(request.flags) if request.flags else ''\n-        response_flags = ' %s' % str(response.flags) if response.flags else ''\n+        request_flags = f' {str(request.flags)}' if request.flags else ''\n+        response_flags = f' {str(response.flags)}' if response.flags else ''\n         return {\n             'level': logging.DEBUG,\n             'msg': CRAWLEDMSG,\n\n@@ -108,7 +108,7 @@ class S3FilesStore:\n             from boto.s3.connection import S3Connection\n             self.S3Connection = S3Connection\n         if not uri.startswith(\"s3://\"):\n-            raise ValueError(\"Incorrect URI scheme in %s, expected 's3'\" % uri)\n+            raise ValueError(f\"Incorrect URI scheme in {uri}, expected 's3'\")\n         self.bucket, self.prefix = uri[5:].split('/', 1)\n \n     def stat_file(self, path, info):\n@@ -133,7 +133,7 @@ class S3FilesStore:\n         return c.get_bucket(self.bucket, validate=False)\n \n     def _get_boto_key(self, path):\n-        key_name = '%s%s' % (self.prefix, path)\n+        key_name = f'{self.prefix}{path}'\n         if self.is_botocore:\n             return threads.deferToThread(\n                 self.s3_client.head_object,\n@@ -145,7 +145,7 @@ class S3FilesStore:\n \n     def persist_file(self, path, buf, info, meta=None, headers=None):\n         \"\"\"Upload file to S3 storage\"\"\"\n-        key_name = '%s%s' % (self.prefix, path)\n+        key_name = f'{self.prefix}{path}'\n         buf.seek(0)\n         if self.is_botocore:\n             extra = self._headers_to_botocore_kwargs(self.HEADERS)\n@@ -208,8 +208,7 @@ class S3FilesStore:\n             try:\n                 kwarg = mapping[key]\n             except KeyError:\n-                raise TypeError(\n-                    'Header \"%s\" is not supported by botocore' % key)\n+                raise TypeError(f'Header \"{key}\" is not supported by botocore')\n             else:\n                 extra[kwarg] = value\n         return extra\n@@ -283,7 +282,7 @@ class FTPFilesStore:\n \n     def __init__(self, uri):\n         if not uri.startswith(\"ftp://\"):\n-            raise ValueError(\"Incorrect URI scheme in %s, expected 'ftp'\" % uri)\n+            raise ValueError(f\"Incorrect URI scheme in {uri}, expected 'ftp'\")\n         u = urlparse(uri)\n         self.port = u.port\n         self.host = u.hostname\n@@ -293,7 +292,7 @@ class FTPFilesStore:\n         self.basedir = u.path.rstrip('/')\n \n     def persist_file(self, path, buf, info, meta=None, headers=None):\n-        path = '%s/%s' % (self.basedir, path)\n+        path = f'{self.basedir}/{path}'\n         return threads.deferToThread(\n             ftp_store_file, path=path, file=buf,\n             host=self.host, port=self.port, username=self.username,\n@@ -308,10 +307,10 @@ class FTPFilesStore:\n                 ftp.login(self.username, self.password)\n                 if self.USE_ACTIVE_MODE:\n                     ftp.set_pasv(False)\n-                file_path = \"%s/%s\" % (self.basedir, path)\n-                last_modified = float(ftp.voidcmd(\"MDTM %s\" % file_path)[4:].strip())\n+                file_path = f\"{self.basedir}/{path}\"\n+                last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n                 m = hashlib.md5()\n-                ftp.retrbinary('RETR %s' % file_path, m.update)\n+                ftp.retrbinary(f'RETR {file_path}', m.update)\n                 return {'last_modified': last_modified, 'checksum': m.hexdigest()}\n             # The file doesn't exist\n             except Exception:\n@@ -515,7 +514,7 @@ class FilesPipeline(MediaPipeline):\n \n     def inc_stats(self, spider, status):\n         spider.crawler.stats.inc_value('file_count', spider=spider)\n-        spider.crawler.stats.inc_value('file_status_count/%s' % status, spider=spider)\n+        spider.crawler.stats.inc_value(f'file_status_count/{status}', spider=spider)\n \n     # Overridable Interface\n     def get_media_requests(self, item, info):\n@@ -545,4 +544,4 @@ class FilesPipeline(MediaPipeline):\n             media_type = mimetypes.guess_type(request.url)[0]\n             if media_type:\n                 media_ext = mimetypes.guess_extension(media_type)\n-        return 'full/%s%s' % (media_guid, media_ext)\n+        return f'full/{media_guid}{media_ext}'\n\n@@ -125,8 +125,9 @@ class ImagesPipeline(FilesPipeline):\n \n         width, height = orig_image.size\n         if width < self.min_width or height < self.min_height:\n-            raise ImageException(\"Image too small (%dx%d < %dx%d)\" %\n-                                 (width, height, self.min_width, self.min_height))\n+            raise ImageException(\"Image too small \"\n+                                 f\"({width}x{height} < \"\n+                                 f\"{self.min_width}x{self.min_height})\")\n \n         image, buf = self.convert_image(orig_image)\n         yield path, image, buf\n@@ -168,8 +169,8 @@ class ImagesPipeline(FilesPipeline):\n \n     def file_path(self, request, response=None, info=None, *, item=None):\n         image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n-        return 'full/%s.jpg' % (image_guid)\n+        return f'full/{image_guid}.jpg'\n \n     def thumb_path(self, request, thumb_id, response=None, info=None):\n         thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n-        return 'thumbs/%s/%s.jpg' % (thumb_id, thumb_guid)\n+        return f'thumbs/{thumb_id}/{thumb_guid}.jpg'\n\n@@ -61,7 +61,7 @@ class MediaPipeline:\n         'MYPIPE_IMAGES'\n         \"\"\"\n         class_name = self.__class__.__name__\n-        formatted_key = \"{}_{}\".format(class_name.upper(), key)\n+        formatted_key = f\"{class_name.upper()}_{key}\"\n         if (\n             not base_class_name\n             or class_name == base_class_name\n@@ -151,9 +151,8 @@ class MediaPipeline:\n         if 'item' not in sig.parameters:\n             old_params = str(sig)[1:-1]\n             new_params = old_params + \", *, item=None\"\n-            warn('%s(self, %s) is deprecated, '\n-                 'please use %s(self, %s)'\n-                 % (func.__name__, old_params, func.__name__, new_params),\n+            warn(f'{func.__name__}(self, {old_params}) is deprecated, '\n+                 f'please use {func.__name__}(self, {new_params})',\n                  ScrapyDeprecationWarning, stacklevel=2)\n             self._expects_item[func.__name__] = False\n \n\n@@ -141,17 +141,16 @@ class DownloaderAwarePriorityQueue:\n \n     def __init__(self, crawler, downstream_queue_cls, key, slot_startprios=()):\n         if crawler.settings.getint('CONCURRENT_REQUESTS_PER_IP') != 0:\n-            raise ValueError('\"%s\" does not support CONCURRENT_REQUESTS_PER_IP'\n-                             % (self.__class__,))\n+            raise ValueError(f'\"{self.__class__}\" does not support CONCURRENT_REQUESTS_PER_IP')\n \n         if slot_startprios and not isinstance(slot_startprios, dict):\n             raise ValueError(\"DownloaderAwarePriorityQueue accepts \"\n-                             \"``slot_startprios`` as a dict; %r instance \"\n+                             \"``slot_startprios`` as a dict; \"\n+                             f\"{slot_startprios.__class__!r} instance \"\n                              \"is passed. Most likely, it means the state is\"\n                              \"created by an incompatible priority queue. \"\n                              \"Only a crawl started with the same priority \"\n-                             \"queue class can be resumed.\" %\n-                             slot_startprios.__class__)\n+                             \"queue class can be resumed.\")\n \n         self._downloader_interface = DownloaderInterface(crawler)\n         self.downstream_queue_cls = downstream_queue_cls\n\n@@ -45,7 +45,7 @@ class ResponseTypes:\n         elif mimetype in self.classes:\n             return self.classes[mimetype]\n         else:\n-            basetype = \"%s/*\" % mimetype.split('/')[0]\n+            basetype = f\"{mimetype.split('/')[0]}/*\"\n             return self.classes.get(basetype, Response)\n \n     def from_content_type(self, content_type, content_encoding=None):\n\n@@ -66,8 +66,8 @@ class Selector(_ParselSelector, object_ref):\n \n     def __init__(self, response=None, text=None, type=None, root=None, **kwargs):\n         if response is not None and text is not None:\n-            raise ValueError('%s.__init__() received both response and text'\n-                             % self.__class__.__name__)\n+            raise ValueError(f'{self.__class__.__name__}.__init__() received '\n+                             'both response and text')\n \n         st = _st(response, type or self._default_type)\n \n\n@@ -52,7 +52,7 @@ class SettingsAttribute:\n             self.priority = priority\n \n     def __str__(self):\n-        return \"<SettingsAttribute value={self.value!r} priority={self.priority}>\".format(self=self)\n+        return f\"<SettingsAttribute value={self.value!r} priority={self.priority}>\"\n \n     __repr__ = __str__\n \n\n@@ -287,7 +287,7 @@ TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))\n \n URLLENGTH_LIMIT = 2083\n \n-USER_AGENT = 'Scrapy/%s (+https://scrapy.org)' % import_module('scrapy').__version__\n+USER_AGENT = f'Scrapy/{import_module(\"scrapy\").__version__} (+https://scrapy.org)'\n \n TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n\n@@ -140,7 +140,7 @@ class Shell:\n         b.append(\"  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\")\n         for k, v in sorted(self.vars.items()):\n             if self._is_relevant(v):\n-                b.append(\"  %-10s %s\" % (k, v))\n+                b.append(f\"  {k:<10} {v}\")\n         b.append(\"Useful shortcuts:\")\n         if self.inthread:\n             b.append(\"  fetch(url[, redirect=True]) \"\n@@ -150,7 +150,7 @@ class Shell:\n         b.append(\"  shelp()           Shell help (print this help)\")\n         b.append(\"  view(response)    View response in a browser\")\n \n-        return \"\\n\".join(\"[s] %s\" % line for line in b)\n+        return \"\\n\".join(f\"[s] {line}\" for line in b)\n \n     def _is_relevant(self, value):\n         return isinstance(value, self.relevant_classes) or is_item(value)\n\n@@ -27,7 +27,7 @@ class SpiderLoader:\n         dupes = []\n         for name, locations in self._found.items():\n             dupes.extend([\n-                \"  {cls} named {name!r} (in {module})\".format(module=mod, cls=cls, name=name)\n+                f\"  {cls} named {name!r} (in {mod})\"\n                 for mod, cls in locations\n                 if len(locations) > 1\n             ])\n@@ -36,7 +36,7 @@ class SpiderLoader:\n             dupes_string = \"\\n\\n\".join(dupes)\n             warnings.warn(\n                 \"There are several spiders with the same name:\\n\\n\"\n-                \"{}\\n\\n  This can cause unexpected behavior.\".format(dupes_string),\n+                f\"{dupes_string}\\n\\n  This can cause unexpected behavior.\",\n                 category=UserWarning,\n             )\n \n@@ -53,10 +53,9 @@ class SpiderLoader:\n             except ImportError:\n                 if self.warn_only:\n                     warnings.warn(\n-                        \"\\n{tb}Could not load spiders from module '{modname}'. \"\n-                        \"See above traceback for details.\".format(\n-                            modname=name, tb=traceback.format_exc()\n-                        ),\n+                        f\"\\n{traceback.format_exc()}Could not load spiders \"\n+                        f\"from module '{name}'. \"\n+                        \"See above traceback for details.\",\n                         category=RuntimeWarning,\n                     )\n                 else:\n@@ -75,7 +74,7 @@ class SpiderLoader:\n         try:\n             return self._spiders[spider_name]\n         except KeyError:\n-            raise KeyError(\"Spider not found: {}\".format(spider_name))\n+            raise KeyError(f\"Spider not found: {spider_name}\")\n \n     def find_by_request(self, request):\n         \"\"\"\n\n@@ -43,7 +43,7 @@ class DepthMiddleware:\n                     return False\n                 else:\n                     if self.verbose_stats:\n-                        self.stats.inc_value('request_depth_count/%s' % depth,\n+                        self.stats.inc_value(f'request_depth_count/{depth}',\n                                              spider=spider)\n                     self.stats.max_value('request_depth_max', depth,\n                                          spider=spider)\n\n@@ -48,7 +48,7 @@ class HttpErrorMiddleware:\n         if isinstance(exception, HttpError):\n             spider.crawler.stats.inc_value('httperror/response_ignored_count')\n             spider.crawler.stats.inc_value(\n-                'httperror/response_ignored_status_count/%s' % response.status\n+                f'httperror/response_ignored_status_count/{response.status}'\n             )\n             logger.info(\n                 \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n\n@@ -61,15 +61,15 @@ class OffsiteMiddleware:\n                 continue\n             elif url_pattern.match(domain):\n                 message = (\"allowed_domains accepts only domains, not URLs. \"\n-                           \"Ignoring URL entry %s in allowed_domains.\" % domain)\n+                           f\"Ignoring URL entry {domain} in allowed_domains.\")\n                 warnings.warn(message, URLWarning)\n             elif port_pattern.search(domain):\n                 message = (\"allowed_domains accepts only domains without ports. \"\n-                           \"Ignoring entry %s in allowed_domains.\" % domain)\n+                           f\"Ignoring entry {domain} in allowed_domains.\")\n                 warnings.warn(message, PortWarning)\n             else:\n                 domains.append(re.escape(domain))\n-        regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n+        regex = fr'^(.*\\.)?({\"|\".join(domains)})$'\n         return re.compile(regex)\n \n     def spider_opened(self, spider):\n\n@@ -278,7 +278,7 @@ def _load_policy_class(policy, warning_only=False):\n         try:\n             return _policy_classes[policy.lower()]\n         except KeyError:\n-            msg = \"Could not load referrer policy %r\" % policy\n+            msg = f\"Could not load referrer policy {policy!r}\"\n             if not warning_only:\n                 raise RuntimeError(msg)\n             else:\n\n@@ -25,7 +25,7 @@ class Spider(object_ref):\n         if name is not None:\n             self.name = name\n         elif not getattr(self, 'name', None):\n-            raise ValueError(\"%s must have a name\" % type(self).__name__)\n+            raise ValueError(f\"{type(self).__name__} must have a name\")\n         self.__dict__.update(kwargs)\n         if not hasattr(self, 'start_urls'):\n             self.start_urls = []\n@@ -66,9 +66,8 @@ class Spider(object_ref):\n             warnings.warn(\n                 \"Spider.make_requests_from_url method is deprecated; it \"\n                 \"won't be called in future Scrapy releases. Please \"\n-                \"override Spider.start_requests method instead (see %s.%s).\" % (\n-                    cls.__module__, cls.__name__\n-                ),\n+                \"override Spider.start_requests method instead \"\n+                f\"(see {cls.__module__}.{cls.__name__}).\",\n             )\n             for url in self.start_urls:\n                 yield self.make_requests_from_url(url)\n@@ -90,7 +89,7 @@ class Spider(object_ref):\n         return self.parse(response, **kwargs)\n \n     def parse(self, response, **kwargs):\n-        raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))\n+        raise NotImplementedError(f'{self.__class__.__name__}.parse callback is not defined')\n \n     @classmethod\n     def update_settings(cls, settings):\n@@ -107,7 +106,7 @@ class Spider(object_ref):\n             return closed(reason)\n \n     def __str__(self):\n-        return \"<%s %r at 0x%0x>\" % (type(self).__name__, self.name, id(self))\n+        return f\"<{type(self).__name__} {self.name!r} at 0x{id(self):0x}>\"\n \n     __repr__ = __str__\n \n\n@@ -71,11 +71,11 @@ class XMLFeedSpider(Spider):\n         elif self.iterator == 'xml':\n             selector = Selector(response, type='xml')\n             self._register_namespaces(selector)\n-            nodes = selector.xpath('//%s' % self.itertag)\n+            nodes = selector.xpath(f'//{self.itertag}')\n         elif self.iterator == 'html':\n             selector = Selector(response, type='html')\n             self._register_namespaces(selector)\n-            nodes = selector.xpath('//%s' % self.itertag)\n+            nodes = selector.xpath(f'//{self.itertag}')\n         else:\n             raise NotSupported('Unsupported node iterator')\n \n\n@@ -21,8 +21,8 @@ class Root(Resource):\n         for nl in nlist:\n             args['n'] = nl\n             argstr = urlencode(args, doseq=True)\n-            request.write(\"<a href='/follow?{0}'>follow {1}</a><br>\"\n-                          .format(argstr, nl).encode('utf8'))\n+            request.write(f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\"\n+                          .encode('utf8'))\n         request.write(b\"</body></html>\")\n         return b''\n \n@@ -39,6 +39,6 @@ if __name__ == '__main__':\n \n     def _print_listening():\n         httpHost = httpPort.getHost()\n-        print(\"Bench server at http://{}:{}\".format(httpHost.host, httpHost.port))\n+        print(f\"Bench server at http://{httpHost.host}:{httpHost.port}\")\n     reactor.callWhenRunning(_print_listening)\n     reactor.run()\n\n@@ -17,8 +17,8 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n \n     def _check_components(complist):\n         if len({convert(c) for c in complist}) != len(complist):\n-            raise ValueError('Some paths in {!r} convert to the same object, '\n-                             'please update your settings'.format(complist))\n+            raise ValueError('Some paths in {complist!r} convert to the same object, '\n+                             'please update your settings')\n \n     def _map_keys(compdict):\n         if isinstance(compdict, BaseSettings):\n@@ -26,9 +26,10 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n             for k, v in compdict.items():\n                 prio = compdict.getpriority(k)\n                 if compbs.getpriority(convert(k)) == prio:\n-                    raise ValueError('Some paths in {!r} convert to the same '\n+                    raise ValueError(f'Some paths in {list(compdict.keys())!r} '\n+                                     'convert to the same '\n                                      'object, please update your settings'\n-                                     ''.format(list(compdict.keys())))\n+                                     )\n                 else:\n                     compbs.set(convert(k), v, priority=prio)\n             return compbs\n@@ -40,8 +41,9 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n         \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n         for name, value in compdict.items():\n             if value is not None and not isinstance(value, numbers.Real):\n-                raise ValueError('Invalid value {} for component {}, please provide '\n-                                 'a real number or None instead'.format(value, name))\n+                raise ValueError(f'Invalid value {value} for component {name}, '\n+                                 'please provide a real number or None instead'\n+                                 )\n \n     # BEGIN Backward compatibility for old (base, custom) call signature\n     if isinstance(custom, (list, tuple)):\n@@ -141,12 +143,10 @@ def feed_process_params_from_cli(settings, output, output_format=None,\n     def check_valid_format(output_format):\n         if output_format not in valid_output_formats:\n             raise UsageError(\n-                \"Unrecognized output format '%s'. Set a supported one (%s) \"\n+                f\"Unrecognized output format '{output_format}'. \"\n+                f\"Set a supported one ({tuple(valid_output_formats)}) \"\n                 \"after a colon at the end of the output URI (i.e. -o/-O \"\n-                \"<URI>:<FORMAT>) or as a file extension.\" % (\n-                    output_format,\n-                    tuple(valid_output_formats),\n-                )\n+                \"<URI>:<FORMAT>) or as a file extension.\"\n             )\n \n     overwrite = False\n\n@@ -9,7 +9,7 @@ from w3lib.http import basic_auth_header\n \n class CurlParser(argparse.ArgumentParser):\n     def error(self, message):\n-        error_msg = 'There was an error parsing the curl command: {}'.format(message)\n+        error_msg = f'There was an error parsing the curl command: {message}'\n         raise ValueError(error_msg)\n \n \n@@ -52,7 +52,7 @@ def curl_to_request_kwargs(curl_command, ignore_unknown_options=True):\n     parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])\n \n     if argv:\n-        msg = 'Unrecognized options: {}'.format(', '.join(argv))\n+        msg = f'Unrecognized options: {\", \".join(argv)}'\n         if ignore_unknown_options:\n             warnings.warn(msg)\n         else:\n\n@@ -14,9 +14,9 @@ def deprecated(use_instead=None):\n     def deco(func):\n         @wraps(func)\n         def wrapped(*args, **kwargs):\n-            message = \"Call to deprecated function %s.\" % func.__name__\n+            message = f\"Call to deprecated function {func.__name__}.\"\n             if use_instead:\n-                message += \" Use %s instead.\" % use_instead\n+                message += f\" Use {use_instead} instead.\"\n             warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n             return func(*args, **kwargs)\n         return wrapped\n\n@@ -8,9 +8,8 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n def attribute(obj, oldattr, newattr, version='0.12'):\n     cname = obj.__class__.__name__\n     warnings.warn(\n-        \"%s.%s attribute is deprecated and will be no longer supported \"\n-        \"in Scrapy %s, use %s.%s attribute instead\"\n-        % (cname, oldattr, version, cname, newattr),\n+        f\"{cname}.{oldattr} attribute is deprecated and will be no longer supported \"\n+        f\"in Scrapy {version}, use {cname}.{newattr} attribute instead\",\n         ScrapyDeprecationWarning,\n         stacklevel=3)\n \n@@ -116,7 +115,7 @@ def create_deprecated_class(\n         # deprecated class is in jinja2 template). __module__ attribute is not\n         # important enough to raise an exception as users may be unable\n         # to fix inspect.stack() errors.\n-        warnings.warn(\"Error detecting parent module: %r\" % e)\n+        warnings.warn(f\"Error detecting parent module: {e!r}\")\n \n     return deprecated_cls\n \n@@ -124,7 +123,7 @@ def create_deprecated_class(\n def _clspath(cls, forced=None):\n     if forced is not None:\n         return forced\n-    return '{}.{}'.format(cls.__module__, cls.__name__)\n+    return f'{cls.__module__}.{cls.__name__}'\n \n \n DEPRECATION_RULES = [\n@@ -137,7 +136,7 @@ def update_classpath(path):\n     for prefix, replacement in DEPRECATION_RULES:\n         if path.startswith(prefix):\n             new_path = path.replace(prefix, replacement, 1)\n-            warnings.warn(\"`{}` class is deprecated, use `{}` instead\".format(path, new_path),\n+            warnings.warn(f\"`{path}` class is deprecated, use `{new_path}` instead\",\n                           ScrapyDeprecationWarning)\n             return new_path\n     return path\n\n@@ -29,7 +29,7 @@ def get_engine_status(engine):\n         try:\n             checks += [(test, eval(test))]\n         except Exception as e:\n-            checks += [(test, \"%s (exception)\" % type(e).__name__)]\n+            checks += [(test, f\"{type(e).__name__} (exception)\")]\n \n     return checks\n \n@@ -38,7 +38,7 @@ def format_engine_status(engine=None):\n     checks = get_engine_status(engine)\n     s = \"Execution engine status\\n\\n\"\n     for test, result in checks:\n-        s += \"%-47s : %s\\n\" % (test, result)\n+        s += f\"{test:<47} : {result}\\n\"\n     s += \"\\n\"\n \n     return s\n\n@@ -33,5 +33,5 @@ def ftp_store_file(\n         dirname, filename = posixpath.split(path)\n         ftp_makedirs_cwd(ftp, dirname)\n         command = 'STOR' if overwrite else 'APPE'\n-        ftp.storbinary('%s %s' % (command, filename), file)\n+        ftp.storbinary(f'{command} {filename}', file)\n         file.close()\n\n@@ -22,8 +22,8 @@ def xmliter(obj, nodename):\n     \"\"\"\n     nodename_patt = re.escape(nodename)\n \n-    HEADER_START_RE = re.compile(r'^(.*?)<\\s*%s(?:\\s|>)' % nodename_patt, re.S)\n-    HEADER_END_RE = re.compile(r'<\\s*/%s\\s*>' % nodename_patt, re.S)\n+    HEADER_START_RE = re.compile(fr'^(.*?)<\\s*{nodename_patt}(?:\\s|>)', re.S)\n+    HEADER_END_RE = re.compile(fr'<\\s*/{nodename_patt}\\s*>', re.S)\n     text = _body_or_str(obj)\n \n     header_start = re.search(HEADER_START_RE, text)\n@@ -31,7 +31,7 @@ def xmliter(obj, nodename):\n     header_end = re_rsearch(HEADER_END_RE, text)\n     header_end = text[header_end[1]:].strip() if header_end else ''\n \n-    r = re.compile(r'<%(np)s[\\s>].*?</%(np)s>' % {'np': nodename_patt}, re.DOTALL)\n+    r = re.compile(fr'<{nodename_patt}[\\s>].*?</{nodename_patt}>', re.DOTALL)\n     for match in r.finditer(text):\n         nodetext = header_start + match.group() + header_end\n         yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]\n@@ -40,9 +40,9 @@ def xmliter(obj, nodename):\n def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n     from lxml import etree\n     reader = _StreamReader(obj)\n-    tag = '{%s}%s' % (namespace, nodename) if namespace else nodename\n+    tag = f'{{{namespace}}}{nodename}'if namespace else nodename\n     iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n-    selxpath = '//' + ('%s:%s' % (prefix, nodename) if namespace else nodename)\n+    selxpath = '//' + (f'{prefix}:{nodename}' if namespace else nodename)\n     for _, node in iterable:\n         nodetext = etree.tostring(node, encoding='unicode')\n         node.clear()\n@@ -131,8 +131,7 @@ def _body_or_str(obj, unicode=True):\n     if not isinstance(obj, expected_types):\n         expected_types_str = \" or \".join(t.__name__ for t in expected_types)\n         raise TypeError(\n-            \"Object %r must be %s, not %s\"\n-            % (obj, expected_types_str, type(obj).__name__)\n+            f\"Object {obj!r} must be {expected_types_str}, not {type(obj).__name__}\"\n         )\n     if isinstance(obj, Response):\n         if not unicode:\n\n@@ -143,7 +143,7 @@ def log_scrapy_info(settings):\n     logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                 {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n     versions = [\n-        \"%s %s\" % (name, version)\n+        f\"{name} {version}\"\n         for name, version in scrapy_components_versions()\n         if name != \"Scrapy\"\n     ]\n@@ -187,7 +187,7 @@ class LogCounterHandler(logging.Handler):\n         self.crawler = crawler\n \n     def emit(self, record):\n-        sname = 'log_count/{}'.format(record.levelname)\n+        sname = f'log_count/{record.levelname}'\n         self.crawler.stats.inc_value(sname)\n \n \n\n@@ -46,7 +46,7 @@ def load_object(path):\n     try:\n         dot = path.rindex('.')\n     except ValueError:\n-        raise ValueError(\"Error loading object '%s': not a full path\" % path)\n+        raise ValueError(f\"Error loading object '{path}': not a full path\")\n \n     module, name = path[:dot], path[dot + 1:]\n     mod = import_module(module)\n@@ -54,7 +54,7 @@ def load_object(path):\n     try:\n         obj = getattr(mod, name)\n     except AttributeError:\n-        raise NameError(\"Module '%s' doesn't define any object named '%s'\" % (module, name))\n+        raise NameError(f\"Module '{module}' doesn't define any object named '{name}'\")\n \n     return obj\n \n@@ -163,7 +163,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n         instance = objcls(*args, **kwargs)\n         method_name = '__new__'\n     if instance is None:\n-        raise TypeError(\"%s.%s returned None\" % (objcls.__qualname__, method_name))\n+        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n     return instance\n \n \n@@ -234,9 +234,10 @@ def warn_on_generator_with_return_value(spider, callable):\n     \"\"\"\n     if is_generator_with_return_value(callable):\n         warnings.warn(\n-            'The \"{}.{}\" method is a generator and includes a \"return\" statement with a '\n-            'value different than None. This could lead to unexpected behaviour. Please see '\n+            f'The \"{spider.__class__.__name__}.{callable.__name__}\" method is '\n+            'a generator and includes a \"return\" statement with a value '\n+            'different than None. This could lead to unexpected behaviour. Please see '\n             'https://docs.python.org/3/reference/simple_stmts.html#the-return-statement '\n-            'for details about the semantics of the \"return\" statement within generators'\n-            .format(spider.__class__.__name__, callable.__name__), stacklevel=2,\n+            'for details about the semantics of the \"return\" statement within generators',\n+            stacklevel=2,\n         )\n\n@@ -20,7 +20,7 @@ def inside_project():\n         try:\n             import_module(scrapy_module)\n         except ImportError as exc:\n-            warnings.warn(\"Cannot import scrapy settings module %s: %s\" % (scrapy_module, exc))\n+            warnings.warn(f\"Cannot import scrapy settings module {scrapy_module}: {exc}\")\n         else:\n             return True\n     return bool(closest_scrapy_cfg())\n@@ -90,7 +90,7 @@ def get_project_settings():\n         warnings.warn(\n             'Use of environment variables prefixed with SCRAPY_ to override '\n             'settings is deprecated. The following environment variables are '\n-            'currently defined: {}'.format(setting_envvar_list),\n+            f'currently defined: {setting_envvar_list}',\n             ScrapyDeprecationWarning\n         )\n     settings.setdict(scrapy_envvars, priority='project')\n\n@@ -91,7 +91,7 @@ def to_unicode(text, encoding=None, errors='strict'):\n         return text\n     if not isinstance(text, (bytes, str)):\n         raise TypeError('to_unicode must receive a bytes or str '\n-                        'object, got %s' % type(text).__name__)\n+                        f'object, got {type(text).__name__}')\n     if encoding is None:\n         encoding = 'utf-8'\n     return text.decode(encoding, errors)\n@@ -104,7 +104,7 @@ def to_bytes(text, encoding=None, errors='strict'):\n         return text\n     if not isinstance(text, str):\n         raise TypeError('to_bytes must receive a str or bytes '\n-                        'object, got %s' % type(text).__name__)\n+                        f'object, got {type(text).__name__}')\n     if encoding is None:\n         encoding = 'utf-8'\n     return text.encode(encoding, errors)\n@@ -174,7 +174,7 @@ def binary_is_text(data):\n     does not contain unprintable control characters.\n     \"\"\"\n     if not isinstance(data, bytes):\n-        raise TypeError(\"data must be bytes, got '%s'\" % type(data).__name__)\n+        raise TypeError(f\"data must be bytes, got '{type(data).__name__}'\")\n     return all(c not in _BINARYCHARS for c in data)\n \n \n@@ -217,7 +217,7 @@ def get_func_args(func, stripself=False):\n         else:\n             return get_func_args(func.__call__, True)\n     else:\n-        raise TypeError('%s is not callable' % type(func))\n+        raise TypeError(f'{type(func)} is not callable')\n     if stripself:\n         func_args.pop(0)\n     return func_args\n@@ -250,7 +250,7 @@ def get_spec(func):\n     elif hasattr(func, '__call__'):\n         spec = _getargspec_py23(func.__call__)\n     else:\n-        raise TypeError('%s is not callable' % type(func))\n+        raise TypeError(f'{type(func)} is not callable')\n \n     defaults = spec.defaults or []\n \n@@ -322,7 +322,7 @@ def global_object_name(obj):\n     >>> global_object_name(Request)\n     'scrapy.http.request.Request'\n     \"\"\"\n-    return \"%s.%s\" % (obj.__module__, obj.__name__)\n+    return f\"{obj.__module__}.{obj.__name__}\"\n \n \n if hasattr(sys, \"pypy_version_info\"):\n\n@@ -10,7 +10,7 @@ def listen_tcp(portrange, host, factory):\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n     from twisted.internet import reactor\n     if len(portrange) > 2:\n-        raise ValueError(\"invalid portrange: %s\" % portrange)\n+        raise ValueError(f\"invalid portrange: {portrange}\")\n     if not portrange:\n         return reactor.listenTCP(0, factory, interface=host)\n     if not hasattr(portrange, '__iter__'):\n@@ -78,9 +78,9 @@ def verify_installed_reactor(reactor_path):\n     from twisted.internet import reactor\n     reactor_class = load_object(reactor_path)\n     if not isinstance(reactor, reactor_class):\n-        msg = \"The installed reactor ({}.{}) does not match the requested one ({})\".format(\n-            reactor.__module__, reactor.__class__.__name__, reactor_path\n-        )\n+        msg = (\"The installed reactor \"\n+               f\"({reactor.__module__}.{reactor.__class__.__name__}) does not \"\n+               f\"match the requested one ({reactor_path})\")\n         raise Exception(msg)\n \n \n\n@@ -84,7 +84,7 @@ def _find_method(obj, func):\n             # https://docs.python.org/3/reference/datamodel.html\n             if obj_func.__func__ is func.__func__:\n                 return name\n-    raise ValueError(\"Function %s is not an instance method in: %s\" % (func, obj))\n+    raise ValueError(f\"Function {func} is not an instance method in: {obj}\")\n \n \n def _get_method(obj, name):\n@@ -92,4 +92,4 @@ def _get_method(obj, name):\n     try:\n         return getattr(obj, name)\n     except AttributeError:\n-        raise ValueError(\"Method %r not found in: %s\" % (name, obj))\n+        raise ValueError(f\"Method {name!r} not found in: {obj}\")\n\n@@ -39,7 +39,7 @@ def response_status_message(status):\n     \"\"\"Return status code plus status text descriptive message\n     \"\"\"\n     message = http.RESPONSES.get(int(status), \"Unknown Status\")\n-    return '%s %s' % (status, to_unicode(message))\n+    return f'{status} {to_unicode(message)}'\n \n \n def response_httprepr(response):\n@@ -69,15 +69,15 @@ def open_in_browser(response, _openfunc=webbrowser.open):\n     body = response.body\n     if isinstance(response, HtmlResponse):\n         if b'<base' not in body:\n-            repl = '<head><base href=\"%s\">' % response.url\n+            repl = f'<head><base href=\"{response.url}\">'\n             body = body.replace(b'<head>', to_bytes(repl))\n         ext = '.html'\n     elif isinstance(response, TextResponse):\n         ext = '.txt'\n     else:\n-        raise TypeError(\"Unsupported response type: %s\" %\n-                        response.__class__.__name__)\n+        raise TypeError(\"Unsupported response type: \"\n+                        f\"{response.__class__.__name__}\")\n     fd, fname = tempfile.mkstemp(ext)\n     os.write(fd, body)\n     os.close(fd)\n-    return _openfunc(\"file://%s\" % fname)\n+    return _openfunc(f\"file://{fname}\")\n\n@@ -17,7 +17,7 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n         if isinstance(o, set):\n             return list(o)\n         elif isinstance(o, datetime.datetime):\n-            return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n+            return o.strftime(f\"{self.DATE_FORMAT} {self.TIME_FORMAT}\")\n         elif isinstance(o, datetime.date):\n             return o.strftime(self.DATE_FORMAT)\n         elif isinstance(o, datetime.time):\n@@ -29,9 +29,9 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n         elif is_item(o):\n             return ItemAdapter(o).asdict()\n         elif isinstance(o, Request):\n-            return \"<%s %s %s>\" % (type(o).__name__, o.method, o.url)\n+            return f\"<{type(o).__name__} {o.method} {o.url}>\"\n         elif isinstance(o, Response):\n-            return \"<%s %s %s>\" % (type(o).__name__, o.status, o.url)\n+            return f\"<{type(o).__name__} {o.status} {o.url}>\"\n         else:\n             return super().default(o)\n \n\n@@ -50,7 +50,7 @@ def get_temp_key_info(ssl_object):\n         key_info.append(ffi_buf_to_string(cname))\n     else:\n         key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n-    key_info.append('%s bits' % pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key))\n+    key_info.append(f'{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits')\n     return ', '.join(key_info)\n \n \n@@ -58,4 +58,4 @@ def get_openssl_version():\n     system_openssl = OpenSSL.SSL.SSLeay_version(\n         OpenSSL.SSL.SSLEAY_VERSION\n     ).decode('ascii', errors='replace')\n-    return '{} ({})'.format(OpenSSL.version.__version__, system_openssl)\n+    return f'{OpenSSL.version.__version__} ({system_openssl})'\n\n@@ -79,7 +79,7 @@ def get_ftp_content_and_delete(\n \n     def buffer_data(data):\n         ftp_data.append(data)\n-    ftp.retrbinary('RETR %s' % path, buffer_data)\n+    ftp.retrbinary(f'RETR {path}', buffer_data)\n     dirname, filename = split(path)\n     ftp.cwd(dirname)\n     ftp.delete(filename)\n\n@@ -23,10 +23,10 @@ class ProcessTest:\n \n     def _process_finished(self, pp, cmd, check_code):\n         if pp.exitcode and check_code:\n-            msg = \"process %s exit with code %d\" % (cmd, pp.exitcode)\n-            msg += \"\\n>>> stdout <<<\\n%s\" % pp.out\n+            msg = f\"process {cmd} exit with code {pp.exitcode}\"\n+            msg += f\"\\n>>> stdout <<<\\n{pp.out}\"\n             msg += \"\\n\"\n-            msg += \"\\n>>> stderr <<<\\n%s\" % pp.err\n+            msg += f\"\\n>>> stderr <<<\\n{pp.err}\"\n             raise RuntimeError(msg)\n         return pp.exitcode, pp.out, pp.err\n \n\n@@ -9,7 +9,7 @@ class SiteTest:\n         from twisted.internet import reactor\n         super().setUp()\n         self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n-        self.baseurl = \"http://localhost:%d/\" % self.site.getHost().port\n+        self.baseurl = f\"http://localhost:{self.site.getHost().port}/\"\n \n     def tearDown(self):\n         super().tearDown()\n@@ -40,5 +40,5 @@ def test_site():\n if __name__ == '__main__':\n     from twisted.internet import reactor\n     port = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n-    print(\"http://localhost:%d/\" % port.getHost().port)\n+    print(f\"http://localhost:{port.getHost().port}/\")\n     reactor.run()\n\n@@ -41,9 +41,7 @@ def format_live_refs(ignore=NoneType):\n         if issubclass(cls, ignore):\n             continue\n         oldest = min(wdict.values())\n-        s += \"%-30s %6d   oldest: %ds ago\\n\" % (\n-            cls.__name__, len(wdict), now - oldest\n-        )\n+        s += f\"{cls.__name__:<30} {len(wdict):6}   oldest: {int(now - oldest)}s ago\\n\"\n     return s\n \n \n\n@@ -22,7 +22,7 @@ def url_is_from_any_domain(url, domains):\n     if not host:\n         return False\n     domains = [d.lower() for d in domains]\n-    return any((host == d) or (host.endswith('.%s' % d)) for d in domains)\n+    return any((host == d) or (host.endswith(f'.{d}')) for d in domains)\n \n \n def url_is_from_spider(url, spider):\n@@ -153,7 +153,7 @@ def strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=\n         if (parsed_url.scheme, parsed_url.port) in (('http', 80),\n                                                     ('https', 443),\n                                                     ('ftp', 21)):\n-            netloc = netloc.replace(':{p.port}'.format(p=parsed_url), '')\n+            netloc = netloc.replace(f':{parsed_url.port}', '')\n     return urlunparse((\n         parsed_url.scheme,\n         netloc,\n\n@@ -38,7 +38,7 @@ class LocalhostSpider(Spider):\n if __name__ == \"__main__\":\n     with MockServer() as mock_http_server, MockDNSServer() as mock_dns_server:\n         port = urlparse(mock_http_server.http_address).port\n-        url = \"http://not.a.real.domain:{port}/echo\".format(port=port)\n+        url = f\"http://not.a.real.domain:{port}/echo\"\n \n         servers = [(mock_dns_server.host, mock_dns_server.port)]\n         reactor.installResolver(createResolver(servers=servers))\n\n@@ -73,7 +73,7 @@ class Follow(LeafResource):\n         for nl in nlist:\n             args[b\"n\"] = [to_bytes(str(nl))]\n             argstr = urlencode(args, doseq=True)\n-            s += \"<a href='/follow?%s'>follow %d</a><br>\" % (argstr, nl)\n+            s += f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\"\n         s += \"\"\"</body>\"\"\"\n         request.write(to_bytes(s))\n         request.finish()\n@@ -91,7 +91,7 @@ class Delay(LeafResource):\n         return NOT_DONE_YET\n \n     def _delayedRender(self, request, n):\n-        request.write(to_bytes(\"Response delayed for %0.3f seconds\\n\" % n))\n+        request.write(to_bytes(f\"Response delayed for {n:.3f} seconds\\n\"))\n         request.finish()\n \n \n@@ -310,8 +310,8 @@ if __name__ == \"__main__\":\n         def print_listening():\n             httpHost = httpPort.getHost()\n             httpsHost = httpsPort.getHost()\n-            httpAddress = \"http://%s:%d\" % (httpHost.host, httpHost.port)\n-            httpsAddress = \"https://%s:%d\" % (httpsHost.host, httpsHost.port)\n+            httpAddress = f'http://{httpHost.host}:{httpHost.port}'\n+            httpsAddress = f'https://{httpsHost.host}:{httpsHost.port}'\n             print(httpAddress)\n             print(httpsAddress)\n \n@@ -323,7 +323,7 @@ if __name__ == \"__main__\":\n \n         def print_listening():\n             host = listener.getHost()\n-            print(\"%s:%s\" % (host.host, host.port))\n+            print(f\"{host.host}:{host.port}\")\n \n     reactor.callWhenRunning(print_listening)\n     reactor.run()\n\n@@ -33,7 +33,7 @@ class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n     depth = 2\n \n     def _get_req(self, index, cb=None):\n-        return Request(self.mockserver.url(\"/status?n=200&request=%d\" % index),\n+        return Request(self.mockserver.url(f\"/status?n=200&request={index}\"),\n                        meta={'index': index},\n                        dont_filter=True,\n                        callback=cb)\n\n@@ -45,7 +45,7 @@ class FollowAllSpider(MetaSpider):\n         self.urls_visited = []\n         self.times = []\n         qargs = {'total': total, 'show': show, 'order': order, 'maxlatency': maxlatency}\n-        url = self.mockserver.url(\"/follow?%s\" % urlencode(qargs, doseq=1))\n+        url = self.mockserver.url(f\"/follow?{urlencode(qargs, doseq=1)}\")\n         self.start_urls = [url]\n \n     def parse(self, response):\n@@ -67,7 +67,7 @@ class DelaySpider(MetaSpider):\n \n     def start_requests(self):\n         self.t1 = time.time()\n-        url = self.mockserver.url(\"/delay?n=%s&b=%s\" % (self.n, self.b))\n+        url = self.mockserver.url(f\"/delay?n={self.n}&b={self.b}\")\n         yield Request(url, callback=self.parse, errback=self.errback)\n \n     def parse(self, response):\n@@ -192,7 +192,7 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n \n         for s in range(100):\n             qargs = {'total': 10, 'seed': s}\n-            url = self.mockserver.url(\"/follow?%s\") % urlencode(qargs, doseq=1)\n+            url = self.mockserver.url(f\"/follow?{urlencode(qargs, doseq=1)}\")\n             yield Request(url, meta={'seed': s})\n             if self.fail_yielding:\n                 2 / 0\n@@ -239,7 +239,7 @@ class DuplicateStartRequestsSpider(MockServerSpider):\n     def start_requests(self):\n         for i in range(0, self.distinct_urls):\n             for j in range(0, self.dupe_factor):\n-                url = self.mockserver.url(\"/echo?headers=1&body=test%d\" % i)\n+                url = self.mockserver.url(f\"/echo?headers=1&body=test{i}\")\n                 yield Request(url, dont_filter=self.dont_filter)\n \n     def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n\n@@ -4,7 +4,7 @@\n class TestExtension:\n \n     def __init__(self, settings):\n-        settings.set('TEST1', \"%s + %s\" % (settings['TEST1'], 'started'))\n+        settings.set('TEST1', f\"{settings['TEST1']} + started\")\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -14,20 +14,20 @@ class CheckCommandTest(CommandTest):\n \n     def _write_contract(self, contracts, parse_def):\n         with open(self.spider, 'w') as file:\n-            file.write(\"\"\"\n+            file.write(f\"\"\"\n import scrapy\n \n class CheckSpider(scrapy.Spider):\n-    name = '{0}'\n+    name = '{self.spider_name}'\n     start_urls = ['http://example.com']\n \n     def parse(self, response, **cb_kwargs):\n         \\\"\\\"\\\"\n         @url http://example.com\n-        {1}\n+        {contracts}\n         \\\"\\\"\\\"\n-        {2}\n-            \"\"\".format(self.spider_name, contracts, parse_def))\n+        {parse_def}\n+            \"\"\")\n \n     def _test_contract(self, contracts='', parse_def='pass'):\n         self._write_contract(contracts, parse_def)\n\n@@ -21,14 +21,14 @@ class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n         self.spider_name = 'parse_spider'\n         fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))\n         with open(fname, 'w') as f:\n-            f.write(\"\"\"\n+            f.write(f\"\"\"\n import scrapy\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import CrawlSpider, Rule\n \n \n class MySpider(scrapy.Spider):\n-    name = '{0}'\n+    name = '{self.spider_name}'\n \n     def parse(self, response):\n         if getattr(self, 'test_arg', None):\n@@ -58,7 +58,7 @@ class MySpider(scrapy.Spider):\n             self.logger.debug('It Does Not Work :(')\n \n class MyGoodCrawlSpider(CrawlSpider):\n-    name = 'goodcrawl{0}'\n+    name = 'goodcrawl{self.spider_name}'\n \n     rules = (\n         Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n@@ -74,7 +74,7 @@ class MyGoodCrawlSpider(CrawlSpider):\n \n class MyBadCrawlSpider(CrawlSpider):\n     '''Spider which doesn't define a parse_item callback while using it in a rule.'''\n-    name = 'badcrawl{0}'\n+    name = 'badcrawl{self.spider_name}'\n \n     rules = (\n         Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n@@ -82,7 +82,7 @@ class MyBadCrawlSpider(CrawlSpider):\n \n     def parse(self, response):\n         return [scrapy.Item(), dict(foo='bar')]\n-\"\"\".format(self.spider_name))\n+\"\"\")\n \n         fname = abspath(join(self.proj_mod_path, 'pipelines.py'))\n         with open(fname, 'w') as f:\n@@ -99,9 +99,9 @@ class MyPipeline:\n \n         fname = abspath(join(self.proj_mod_path, 'settings.py'))\n         with open(fname, 'a') as f:\n-            f.write(\"\"\"\n-ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n-\"\"\" % self.project_name)\n+            f.write(f\"\"\"\n+ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n+\"\"\")\n \n     @defer.inlineCallbacks\n     def test_spider_arguments(self):\n\n@@ -65,8 +65,8 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n     def test_fetch_redirect_follow_302(self):\n         \"\"\"Test that calling ``fetch(url)`` follows HTTP redirects by default.\"\"\"\n         url = self.url('/redirect-no-meta-refresh')\n-        code = \"fetch('{0}')\"\n-        errcode, out, errout = yield self.execute(['-c', code.format(url)])\n+        code = f\"fetch('{url}')\"\n+        errcode, out, errout = yield self.execute(['-c', code])\n         self.assertEqual(errcode, 0, out)\n         assert b'Redirecting (302)' in errout\n         assert b'Crawled (200)' in errout\n@@ -75,23 +75,23 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n     def test_fetch_redirect_not_follow_302(self):\n         \"\"\"Test that calling ``fetch(url, redirect=False)`` disables automatic redirects.\"\"\"\n         url = self.url('/redirect-no-meta-refresh')\n-        code = \"fetch('{0}', redirect=False)\"\n-        errcode, out, errout = yield self.execute(['-c', code.format(url)])\n+        code = f\"fetch('{url}', redirect=False)\"\n+        errcode, out, errout = yield self.execute(['-c', code])\n         self.assertEqual(errcode, 0, out)\n         assert b'Crawled (302)' in errout\n \n     @defer.inlineCallbacks\n     def test_request_replace(self):\n         url = self.url('/text')\n-        code = \"fetch('{0}') or fetch(response.request.replace(method='POST'))\"\n-        errcode, out, _ = yield self.execute(['-c', code.format(url)])\n+        code = f\"fetch('{url}') or fetch(response.request.replace(method='POST'))\"\n+        errcode, out, _ = yield self.execute(['-c', code])\n         self.assertEqual(errcode, 0, out)\n \n     @defer.inlineCallbacks\n     def test_scrapy_import(self):\n         url = self.url('/text')\n-        code = \"fetch(scrapy.Request('{0}'))\"\n-        errcode, out, _ = yield self.execute(['-c', code.format(url)])\n+        code = f\"fetch(scrapy.Request('{url}'))\"\n+        errcode, out, _ = yield self.execute(['-c', code])\n         self.assertEqual(errcode, 0, out)\n \n     @defer.inlineCallbacks\n\n@@ -16,7 +16,7 @@ class VersionTest(ProcessTest, unittest.TestCase):\n         _, out, _ = yield self.execute([])\n         self.assertEqual(\n             out.strip().decode(encoding),\n-            \"Scrapy %s\" % scrapy.__version__,\n+            f\"Scrapy {scrapy.__version__}\",\n         )\n \n     @defer.inlineCallbacks\n\n@@ -42,7 +42,7 @@ class CommandSettings(unittest.TestCase):\n \n     def test_settings_json_string(self):\n         feeds_json = '{\"data.json\": {\"format\": \"json\"}, \"data.xml\": {\"format\": \"xml\"}}'\n-        opts, args = self.parser.parse_args(args=['-s', 'FEEDS={}'.format(feeds_json), 'spider.py'])\n+        opts, args = self.parser.parse_args(args=['-s', f'FEEDS={feeds_json}', 'spider.py'])\n         self.command.process_options(args, opts)\n         self.assertIsInstance(self.command.settings['FEEDS'], scrapy.settings.BaseSettings)\n         self.assertEqual(dict(self.command.settings['FEEDS']), json.loads(feeds_json))\n@@ -163,10 +163,10 @@ class StartprojectTemplatesTest(ProjectTest):\n             pass\n         assert exists(join(self.tmpl_proj, 'root_template'))\n \n-        args = ['--set', 'TEMPLATES_DIR=%s' % self.tmpl]\n+        args = ['--set', f'TEMPLATES_DIR={self.tmpl}']\n         p, out, err = self.proc('startproject', self.project_name, *args)\n-        self.assertIn(\"New Scrapy project '%s', using template directory\"\n-                      % self.project_name, out)\n+        self.assertIn(f\"New Scrapy project '{self.project_name}', \"\n+                      \"using template directory\", out)\n         self.assertIn(self.tmpl_proj, out)\n         assert exists(join(self.proj_path, 'root_template'))\n \n@@ -247,7 +247,7 @@ class StartprojectTemplatesTest(ProjectTest):\n                 'startproject',\n                 project_name,\n                 '--set',\n-                'TEMPLATES_DIR={}'.format(read_only_templates_dir),\n+                f'TEMPLATES_DIR={read_only_templates_dir}',\n             ),\n             cwd=destination,\n             env=self.env,\n@@ -320,7 +320,7 @@ class CommandTest(ProjectTest):\n         super().setUp()\n         self.call('startproject', self.project_name)\n         self.cwd = join(self.temp_path, self.project_name)\n-        self.env['SCRAPY_SETTINGS_MODULE'] = '%s.settings' % self.project_name\n+        self.env['SCRAPY_SETTINGS_MODULE'] = f'{self.project_name}.settings'\n \n \n class GenspiderCommandTest(CommandTest):\n@@ -334,14 +334,14 @@ class GenspiderCommandTest(CommandTest):\n         assert exists(join(self.proj_mod_path, 'spiders', 'test_name.py'))\n \n     def test_template(self, tplname='crawl'):\n-        args = ['--template=%s' % tplname] if tplname else []\n+        args = [f'--template={tplname}'] if tplname else []\n         spname = 'test_spider'\n         p, out, err = self.proc('genspider', spname, 'test.com', *args)\n-        self.assertIn(\"Created spider %r using template %r in module\" % (spname, tplname), out)\n+        self.assertIn(f\"Created spider {spname!r} using template {tplname!r} in module\", out)\n         self.assertTrue(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n         modify_time_before = getmtime(join(self.proj_mod_path, 'spiders', 'test_spider.py'))\n         p, out, err = self.proc('genspider', spname, 'test.com', *args)\n-        self.assertIn(\"Spider %r already exists in module\" % spname, out)\n+        self.assertIn(f\"Spider {spname!r} already exists in module\", out)\n         modify_time_after = getmtime(join(self.proj_mod_path, 'spiders', 'test_spider.py'))\n         self.assertEqual(modify_time_after, modify_time_before)\n \n@@ -363,11 +363,11 @@ class GenspiderCommandTest(CommandTest):\n \n     def test_same_name_as_project(self):\n         self.assertEqual(2, self.call('genspider', self.project_name))\n-        assert not exists(join(self.proj_mod_path, 'spiders', '%s.py' % self.project_name))\n+        assert not exists(join(self.proj_mod_path, 'spiders', f'{self.project_name}.py'))\n \n     def test_same_filename_as_existing_spider(self, force=False):\n         file_name = 'example'\n-        file_path = join(self.proj_mod_path, 'spiders', '%s.py' % file_name)\n+        file_path = join(self.proj_mod_path, 'spiders', f'{file_name}.py')\n         self.assertEqual(0, self.call('genspider', file_name, 'example.com'))\n         assert exists(file_path)\n \n@@ -383,14 +383,14 @@ class GenspiderCommandTest(CommandTest):\n \n         if force:\n             p, out, err = self.proc('genspider', '--force', file_name, 'example.com')\n-            self.assertIn(\"Created spider %r using template \\'basic\\' in module\" % file_name, out)\n+            self.assertIn(f\"Created spider {file_name!r} using template \\'basic\\' in module\", out)\n             modify_time_after = getmtime(file_path)\n             self.assertNotEqual(modify_time_after, modify_time_before)\n             file_contents_after = open(file_path, 'r').read()\n             self.assertNotEqual(file_contents_after, file_contents_before)\n         else:\n             p, out, err = self.proc('genspider', file_name, 'example.com')\n-            self.assertIn(\"%s already exists\" % (file_path), out)\n+            self.assertIn(f\"{file_path} already exists\", out)\n             modify_time_after = getmtime(file_path)\n             self.assertEqual(modify_time_after, modify_time_before)\n             file_contents_after = open(file_path, 'r').read()\n@@ -410,7 +410,7 @@ class GenspiderStandaloneCommandTest(ProjectTest):\n         file_name = 'example'\n         file_path = join(self.temp_path, file_name + '.py')\n         p, out, err = self.proc('genspider', file_name, 'example.com')\n-        self.assertIn(\"Created spider %r using template \\'basic\\' \" % file_name, out)\n+        self.assertIn(f\"Created spider {file_name!r} using template \\'basic\\' \", out)\n         assert exists(file_path)\n         modify_time_before = getmtime(file_path)\n         file_contents_before = open(file_path, 'r').read()\n@@ -418,14 +418,14 @@ class GenspiderStandaloneCommandTest(ProjectTest):\n         if force:\n             # use different template to ensure contents were changed\n             p, out, err = self.proc('genspider', '--force', '-t', 'crawl', file_name, 'example.com')\n-            self.assertIn(\"Created spider %r using template \\'crawl\\' \" % file_name, out)\n+            self.assertIn(f\"Created spider {file_name!r} using template \\'crawl\\' \", out)\n             modify_time_after = getmtime(file_path)\n             self.assertNotEqual(modify_time_after, modify_time_before)\n             file_contents_after = open(file_path, 'r').read()\n             self.assertNotEqual(file_contents_after, file_contents_before)\n         else:\n             p, out, err = self.proc('genspider', file_name, 'example.com')\n-            self.assertIn(\"%s already exists\" % join(self.temp_path, file_name + \".py\"), out)\n+            self.assertIn(f\"{join(self.temp_path, file_name + '.py')} already exists\", out)\n             modify_time_after = getmtime(file_path)\n             self.assertEqual(modify_time_after, modify_time_before)\n             file_contents_after = open(file_path, 'r').read()\n\n@@ -393,7 +393,7 @@ class ContractsManagerTest(unittest.TestCase):\n                 return TestItem()\n \n         with MockServer() as mockserver:\n-            contract_doc = '@url {}'.format(mockserver.url('/status?n=200'))\n+            contract_doc = f'@url {mockserver.url(\"/status?n=200\")}'\n \n             TestSameUrlSpider.parse_first.__doc__ = contract_doc\n             TestSameUrlSpider.parse_second.__doc__ = contract_doc\n\n@@ -79,7 +79,7 @@ class CrawlTestCase(TestCase):\n         total_time = times[-1] - times[0]\n         average = total_time / (len(times) - 1)\n         self.assertTrue(average > delay * tolerance,\n-                        \"download delay too small: %s\" % average)\n+                        f\"download delay too small: {average}\")\n \n         # Ensure that the same test parameters would cause a failure if no\n         # download delay is set. Otherwise, it means we are using a combination\n@@ -204,7 +204,7 @@ with multiples lines\n '''})\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/raw?{0}\".format(query)), mockserver=self.mockserver)\n+            yield crawler.crawl(self.mockserver.url(f\"/raw?{query}\"), mockserver=self.mockserver)\n         self.assertEqual(str(log).count(\"Got response 200\"), 1)\n \n     @defer.inlineCallbacks\n@@ -465,7 +465,7 @@ class CrawlSpiderTestCase(TestCase):\n         with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n         for req_id in range(3):\n-            self.assertIn(\"Got response 200, req_id %d\" % req_id, str(log))\n+            self.assertIn(f\"Got response 200, req_id {req_id}\", str(log))\n \n     @defer.inlineCallbacks\n     def test_response_ssl_certificate_none(self):\n\n@@ -121,7 +121,7 @@ class FileTestCase(unittest.TestCase):\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n     def test_non_existent(self):\n-        request = Request('file://%s' % self.mktemp())\n+        request = Request(f'file://{self.mktemp()}')\n         d = self.download_request(request, Spider('foo'))\n         return self.assertFailure(d, IOError)\n \n@@ -249,7 +249,7 @@ class HttpTestCase(unittest.TestCase):\n         shutil.rmtree(self.tmpname)\n \n     def getURL(self, path):\n-        return \"%s://%s:%d/%s\" % (self.scheme, self.host, self.portno, path)\n+        return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n \n     def test_download(self):\n         request = Request(self.getURL('file'))\n@@ -300,7 +300,7 @@ class HttpTestCase(unittest.TestCase):\n     def test_host_header_not_in_request_headers(self):\n         def _test(response):\n             self.assertEqual(\n-                response.body, to_bytes('%s:%d' % (self.host, self.portno)))\n+                response.body, to_bytes(f'{self.host}:{self.portno}'))\n             self.assertEqual(request.headers, {})\n \n         request = Request(self.getURL('host'))\n@@ -583,7 +583,7 @@ class Https11CustomCiphers(unittest.TestCase):\n         shutil.rmtree(self.tmpname)\n \n     def getURL(self, path):\n-        return \"%s://%s:%d/%s\" % (self.scheme, self.host, self.portno, path)\n+        return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n \n     def test_download(self):\n         request = Request(self.getURL('file'))\n@@ -678,7 +678,7 @@ class HttpProxyTestCase(unittest.TestCase):\n             yield self.download_handler.close()\n \n     def getURL(self, path):\n-        return \"http://127.0.0.1:%d/%s\" % (self.portno, path)\n+        return f\"http://127.0.0.1:{self.portno}/{path}\"\n \n     def test_download_with_proxy(self):\n         def _test(response):\n@@ -696,7 +696,7 @@ class HttpProxyTestCase(unittest.TestCase):\n             self.assertEqual(response.url, request.url)\n             self.assertEqual(response.body, b'https://example.com')\n \n-        http_proxy = '%s?noconnect' % self.getURL('')\n+        http_proxy = f'{self.getURL(\"\")}?noconnect'\n         request = Request('https://example.com', meta={'proxy': http_proxy})\n         with self.assertWarnsRegex(ScrapyDeprecationWarning,\n                                    r'Using HTTPS proxies in the noconnect mode is deprecated'):\n@@ -977,7 +977,7 @@ class BaseFTPTestCase(unittest.TestCase):\n         return deferred\n \n     def test_ftp_download_success(self):\n-        request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n+        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\",\n                           meta=self.req_meta)\n         d = self.download_handler.download_request(request, None)\n \n@@ -989,7 +989,7 @@ class BaseFTPTestCase(unittest.TestCase):\n \n     def test_ftp_download_path_with_spaces(self):\n         request = Request(\n-            url=\"ftp://127.0.0.1:%s/file with spaces.txt\" % self.portNum,\n+            url=f\"ftp://127.0.0.1:{self.portNum}/file with spaces.txt\",\n             meta=self.req_meta\n         )\n         d = self.download_handler.download_request(request, None)\n@@ -1001,7 +1001,7 @@ class BaseFTPTestCase(unittest.TestCase):\n         return self._add_test_callbacks(d, _test)\n \n     def test_ftp_download_notexist(self):\n-        request = Request(url=\"ftp://127.0.0.1:%s/notexist.txt\" % self.portNum,\n+        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/notexist.txt\",\n                           meta=self.req_meta)\n         d = self.download_handler.download_request(request, None)\n \n@@ -1015,7 +1015,7 @@ class BaseFTPTestCase(unittest.TestCase):\n         os.close(f)\n         meta = {\"ftp_local_filename\": local_fname}\n         meta.update(self.req_meta)\n-        request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n+        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\",\n                           meta=meta)\n         d = self.download_handler.download_request(request, None)\n \n@@ -1037,7 +1037,7 @@ class FTPTestCase(BaseFTPTestCase):\n \n         meta = dict(self.req_meta)\n         meta.update({\"ftp_password\": 'invalid'})\n-        request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n+        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\",\n                           meta=meta)\n         d = self.download_handler.download_request(request, None)\n \n\n@@ -84,7 +84,7 @@ class DefaultsTest(ManagerTestCase):\n         })\n         ret = self._download(request=req, response=resp)\n         self.assertTrue(isinstance(ret, Request),\n-                        \"Not redirected: {0!r}\".format(ret))\n+                        f\"Not redirected: {ret!r}\")\n         self.assertEqual(to_bytes(ret.url), resp.headers['Location'],\n                          \"Not redirected to location header\")\n \n\n@@ -28,7 +28,7 @@ class DecompressionMiddlewareTest(TestCase):\n         for fmt in self.test_formats:\n             rsp = self.test_responses[fmt]\n             new = self.mw.process_response(None, rsp, self.spider)\n-            error_msg = 'Failed %s, response type %s' % (fmt, type(new).__name__)\n+            error_msg = f'Failed {fmt}, response type {type(new).__name__}'\n             assert isinstance(new, XmlResponse), error_msg\n             assert_samelines(self, new.body, self.uncompressed_body, fmt)\n \n\n@@ -324,7 +324,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n         ]\n         with self._middleware() as mw:\n             for idx, (shouldcache, status, headers) in enumerate(responses):\n-                req0 = Request('http://example-%d.com' % idx)\n+                req0 = Request(f'http://example-{idx}.com')\n                 res0 = Response(req0.url, status=status, headers=headers)\n                 res1 = self._process_requestresponse(mw, req0, res0)\n                 res304 = res0.replace(status=304)\n@@ -343,7 +343,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n         with self._middleware(HTTPCACHE_ALWAYS_STORE=True) as mw:\n             for idx, (_, status, headers) in enumerate(responses):\n                 shouldcache = 'no-store' not in headers.get('Cache-Control', '') and status != 304\n-                req0 = Request('http://example2-%d.com' % idx)\n+                req0 = Request(f'http://example2-{idx}.com')\n                 res0 = Response(req0.url, status=status, headers=headers)\n                 res1 = self._process_requestresponse(mw, req0, res0)\n                 res304 = res0.replace(status=304)\n@@ -386,7 +386,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n         ]\n         with self._middleware() as mw:\n             for idx, (status, headers) in enumerate(sampledata):\n-                req0 = Request('http://example-%d.com' % idx)\n+                req0 = Request(f'http://example-{idx}.com')\n                 res0 = Response(req0.url, status=status, headers=headers)\n                 # cache fresh response\n                 res1 = self._process_requestresponse(mw, req0, res0)\n@@ -423,7 +423,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n         ]\n         with self._middleware() as mw:\n             for idx, (status, headers) in enumerate(sampledata):\n-                req0 = Request('http://example-%d.com' % idx)\n+                req0 = Request(f'http://example-{idx}.com')\n                 res0a = Response(req0.url, status=status, headers=headers)\n                 # cache expired response\n                 res1 = self._process_requestresponse(mw, req0, res0a)\n@@ -490,7 +490,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n         ]\n         with self._middleware(HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS=['no-cache', 'no-store']) as mw:\n             for idx, (status, headers) in enumerate(sampledata):\n-                req0 = Request('http://example-%d.com' % idx)\n+                req0 = Request(f'http://example-{idx}.com')\n                 res0 = Response(req0.url, status=status, headers=headers)\n                 # cache fresh response\n                 res1 = self._process_requestresponse(mw, req0, res0)\n\n@@ -22,7 +22,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n \n     def test_redirect_3xx_permanent(self):\n         def _test(method, status=301):\n-            url = 'http://www.example.com/{}'.format(status)\n+            url = f'http://www.example.com/{status}'\n             url2 = 'http://www.example.com/redirected'\n             req = Request(url, method=method)\n             rsp = Response(url, headers={'Location': url2}, status=status)\n@@ -79,7 +79,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req2.method, 'GET')\n         assert 'Content-Type' not in req2.headers, \"Content-Type header must not be present in redirected request\"\n         assert 'Content-Length' not in req2.headers, \"Content-Length header must not be present in redirected request\"\n-        assert not req2.body, \"Redirected body must be empty, not '%s'\" % req2.body\n+        assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n \n         # response without Location header but with status code is 3XX should be ignored\n         del rsp.headers['Location']\n@@ -207,8 +207,8 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n         self.mw = MetaRefreshMiddleware.from_crawler(crawler)\n \n     def _body(self, interval=5, url='http://example.org/newpage'):\n-        html = \"\"\"<html><head><meta http-equiv=\"refresh\" content=\"{0};url={1}\"/></head></html>\"\"\"\n-        return html.format(interval, url).encode('utf-8')\n+        html = f\"\"\"<html><head><meta http-equiv=\"refresh\" content=\"{interval};url={url}\"/></head></html>\"\"\"\n+        return html.encode('utf-8')\n \n     def test_priority_adjust(self):\n         req = Request('http://a.com')\n@@ -243,7 +243,7 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req2.method, 'GET')\n         assert 'Content-Type' not in req2.headers, \"Content-Type header must not be present in redirected request\"\n         assert 'Content-Length' not in req2.headers, \"Content-Length header must not be present in redirected request\"\n-        assert not req2.body, \"Redirected body must be empty, not '%s'\" % req2.body\n+        assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n \n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n\n@@ -94,7 +94,7 @@ class RetryTest(unittest.TestCase):\n         ]\n \n         for exc in exceptions:\n-            req = Request('http://www.scrapytest.org/%s' % exc.__name__)\n+            req = Request(f'http://www.scrapytest.org/{exc.__name__}')\n             self._test_retry_exception(req, exc('foo'))\n \n         stats = self.crawler.stats\n\n@@ -127,8 +127,8 @@ def start_test_site(debug=False):\n \n     port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n     if debug:\n-        print(\"Test server running at http://localhost:%d/ - hit Ctrl-C to finish.\"\n-              % port.getHost().port)\n+        print(f\"Test server running at http://localhost:{port.getHost().port}/ \"\n+              \"- hit Ctrl-C to finish.\")\n     return port\n \n \n@@ -185,7 +185,7 @@ class CrawlerRun:\n         self.deferred.callback(None)\n \n     def geturl(self, path):\n-        return \"http://localhost:%s%s\" % (self.portno, path)\n+        return f\"http://localhost:{self.portno}{path}\"\n \n     def getpath(self, url):\n         u = urlparse(url)\n@@ -265,7 +265,7 @@ class EngineTest(unittest.TestCase):\n                            \"/item1.html\", \"/item2.html\", \"/item999.html\"]\n         urls_visited = {rp[0].url for rp in self.run.respplug}\n         urls_expected = {self.run.geturl(p) for p in must_be_visited}\n-        assert urls_expected <= urls_visited, \"URLs not visited: %s\" % list(urls_expected - urls_visited)\n+        assert urls_expected <= urls_visited, f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n \n     def _assert_scheduled_requests(self, urls_to_visit=None):\n         self.assertEqual(urls_to_visit, len(self.run.reqplug))\n@@ -413,16 +413,19 @@ class StopDownloadEngineTest(EngineTest):\n                 yield self.run.run()\n                 log.check_present((\"scrapy.core.downloader.handlers.http11\",\n                                    \"DEBUG\",\n-                                   \"Download stopped for <GET http://localhost:{}/redirected> from signal handler\"\n-                                   \" StopDownloadCrawlerRun.bytes_received\".format(self.run.portno)))\n+                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/redirected> \"\n+                                   \"from signal handler\"\n+                                   \" StopDownloadCrawlerRun.bytes_received\"))\n                 log.check_present((\"scrapy.core.downloader.handlers.http11\",\n                                    \"DEBUG\",\n-                                   \"Download stopped for <GET http://localhost:{}/> from signal handler\"\n-                                   \" StopDownloadCrawlerRun.bytes_received\".format(self.run.portno)))\n+                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/> \"\n+                                   \"from signal handler\"\n+                                   \" StopDownloadCrawlerRun.bytes_received\"))\n                 log.check_present((\"scrapy.core.downloader.handlers.http11\",\n                                    \"DEBUG\",\n-                                   \"Download stopped for <GET http://localhost:{}/numbers> from signal handler\"\n-                                   \" StopDownloadCrawlerRun.bytes_received\".format(self.run.portno)))\n+                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/numbers> \"\n+                                   \"from signal handler\"\n+                                   \" StopDownloadCrawlerRun.bytes_received\"))\n             self._assert_visited_urls()\n             self._assert_scheduled_requests(urls_to_visit=9)\n             self._assert_downloaded_responses()\n\n@@ -184,8 +184,7 @@ class FTPFeedStorageTest(unittest.TestCase):\n     def test_uri_auth_quote(self):\n         # RFC3986: 3.2.1. User Information\n         pw_quoted = quote(string.punctuation, safe='')\n-        st = FTPFeedStorage('ftp://foo:%s@example.com/some_path' % pw_quoted,\n-                            {})\n+        st = FTPFeedStorage(f'ftp://foo:{pw_quoted}@example.com/some_path', {})\n         self.assertEqual(st.password, string.punctuation)\n \n \n@@ -1230,7 +1229,7 @@ class FeedExportTest(FeedExportTestBase):\n \n         print(log)\n         for fmt in ['json', 'xml', 'csv']:\n-            self.assertIn('Stored %s feed (2 items)' % fmt, str(log))\n+            self.assertIn(f'Stored {fmt} feed (2 items)', str(log))\n \n     @defer.inlineCallbacks\n     def test_multiple_feeds_failing_logs_blocking_feed_storage(self):\n@@ -1251,7 +1250,7 @@ class FeedExportTest(FeedExportTestBase):\n \n         print(log)\n         for fmt in ['json', 'xml', 'csv']:\n-            self.assertIn('Error storing %s feed (2 items)' % fmt, str(log))\n+            self.assertIn(f'Error storing {fmt} feed (2 items)', str(log))\n \n \n class BatchDeliveriesTest(FeedExportTestBase):\n@@ -1582,10 +1581,8 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n         chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n         filename = ''.join(chars)\n-        prefix = 'tmp/{filename}'.format(filename=filename)\n-        s3_test_file_uri = 's3://{bucket_name}/{prefix}/%(batch_time)s.json'.format(\n-            bucket_name=s3_test_bucket_name, prefix=prefix\n-        )\n+        prefix = f'tmp/{filename}'\n+        s3_test_file_uri = f's3://{s3_test_bucket_name}/{prefix}/%(batch_time)s.json'\n         storage = S3FeedStorage(s3_test_bucket_name, access_key, secret_key)\n         settings = Settings({\n             'FEEDS': {\n\n@@ -657,7 +657,7 @@ class SelectJmesTestCase(unittest.TestCase):\n             self.assertEqual(\n                 test,\n                 expected,\n-                msg='test \"{}\" got {} expected {}'.format(tl, test, expected)\n+                msg=f'test \"{tl}\" got {test} expected {expected}'\n             )\n \n \n\n@@ -20,7 +20,7 @@ class CustomItem(Item):\n     name = Field()\n \n     def __str__(self):\n-        return \"name: %s\" % self['name']\n+        return f\"name: {self['name']}\"\n \n \n class LogFormatterTestCase(unittest.TestCase):\n\n@@ -50,7 +50,7 @@ class TestMiddlewareManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return ['tests.test_middleware.%s' % x for x in ['M1', 'MOff', 'M3']]\n+        return [f'tests.test_middleware.{x}' for x in ['M1', 'MOff', 'M3']]\n \n     def _add_middleware(self, mw):\n         super()._add_middleware(mw)\n\n@@ -123,10 +123,10 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.assertEqual(crawler.stats.get_value('downloader/request_method_count/GET'), 4)\n         self.assertEqual(crawler.stats.get_value('downloader/response_count'), 4)\n         self.assertEqual(crawler.stats.get_value('downloader/response_status_count/200'), 1)\n-        self.assertEqual(crawler.stats.get_value('downloader/response_status_count/%d' % code), 3)\n+        self.assertEqual(crawler.stats.get_value(f'downloader/response_status_count/{code}'), 3)\n \n         # check that logs do show the failure on the file downloads\n-        file_dl_failure = 'File (code: %d): Error downloading file from' % code\n+        file_dl_failure = f'File (code: {code}): Error downloading file from'\n         self.assertEqual(logs.count(file_dl_failure), 3)\n \n         # check that no files were written to the media store\n\n@@ -167,7 +167,7 @@ class FilesPipelineTestCase(unittest.TestCase):\n         \"\"\"\n         class CustomFilesPipeline(FilesPipeline):\n             def file_path(self, request, response=None, info=None, item=None):\n-                return 'full/%s' % item.get('path')\n+                return f'full/{item.get(\"path\")}'\n \n         file_path = CustomFilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir})).file_path\n         item = dict(path='path-to-store-file')\n@@ -495,7 +495,7 @@ class TestFTPFileStore(unittest.TestCase):\n         self.assertIn('last_modified', stat)\n         self.assertIn('checksum', stat)\n         self.assertEqual(stat['checksum'], 'd113d66b2ec7258724a268bd88eef6b6')\n-        path = '%s/%s' % (store.basedir, path)\n+        path = f'{store.basedir}/{path}'\n         content = get_ftp_content_and_delete(\n             path, store.host, store.port,\n             store.username, store.password, store.USE_ACTIVE_MODE)\n\n@@ -128,11 +128,11 @@ class DeprecatedImagesPipeline(ImagesPipeline):\n \n     def image_key(self, url):\n         image_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n-        return 'empty/%s.jpg' % (image_guid)\n+        return f'empty/{image_guid}.jpg'\n \n     def thumb_key(self, url, thumb_id):\n         thumb_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n-        return 'thumbsup/%s/%s.jpg' % (thumb_id, thumb_guid)\n+        return f'thumbsup/{thumb_id}/{thumb_guid}.jpg'\n \n \n class ImagesPipelineTestCaseFieldsMixin:\n\n@@ -37,14 +37,14 @@ sys.exit(mitmdump())\n                            '-c', script,\n                            '--listen-host', '127.0.0.1',\n                            '--listen-port', '0',\n-                           '--proxyauth', '%s:%s' % (self.auth_user, self.auth_pass),\n+                           '--proxyauth', f'{self.auth_user}:{self.auth_pass}',\n                            '--certs', cert_path,\n                            '--ssl-insecure',\n                            ],\n                           stdout=PIPE, env=get_testenv())\n         line = self.proc.stdout.readline().decode('utf-8')\n         host_port = re.search(r'listening at http://([^:]+:\\d+)', line).group(1)\n-        address = 'http://%s:%s@%s' % (self.auth_user, self.auth_pass, host_port)\n+        address = f'http://{self.auth_user}:{self.auth_pass}@{host_port}'\n         return address\n \n     def stop(self):\n@@ -118,7 +118,7 @@ class ProxyConnectTestCase(TestCase):\n \n     def _assert_got_response_code(self, code, log):\n         print(log)\n-        self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n+        self.assertEqual(str(log).count(f'Crawled ({code})'), 1)\n \n     def _assert_got_tunnel_error(self, log):\n         print(log)\n\n@@ -79,7 +79,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_response_error(self):\n         for status in (\"404\", \"500\"):\n-            url = self.mockserver.url(\"/status?n={}\".format(status))\n+            url = self.mockserver.url(f\"/status?n={status}\")\n             crawler = CrawlerRunner().create_crawler(SingleRequestSpider)\n             yield crawler.crawl(seed=url, mockserver=self.mockserver)\n             failure = crawler.spider.meta[\"failure\"]\n@@ -135,7 +135,7 @@ class CrawlTestCase(TestCase):\n         self.assertEqual(signal_params[\"request\"].url, OVERRIDEN_URL)\n \n         log.check_present(\n-            (\"scrapy.core.engine\", \"DEBUG\", \"Crawled (200) <GET {}> (referer: None)\".format(OVERRIDEN_URL)),\n+            (\"scrapy.core.engine\", \"DEBUG\", f\"Crawled (200) <GET {OVERRIDEN_URL}> (referer: None)\"),\n         )\n \n     @defer.inlineCallbacks\n\n@@ -17,7 +17,7 @@ class ResponseTypesTest(unittest.TestCase):\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_filename(source)\n-            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n \n     def test_from_content_disposition(self):\n         mappings = [\n@@ -32,7 +32,7 @@ class ResponseTypesTest(unittest.TestCase):\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_disposition(source)\n-            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n \n     def test_from_content_type(self):\n         mappings = [\n@@ -47,7 +47,7 @@ class ResponseTypesTest(unittest.TestCase):\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_type(source)\n-            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n \n     def test_from_body(self):\n         mappings = [\n@@ -58,7 +58,7 @@ class ResponseTypesTest(unittest.TestCase):\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_body(source)\n-            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n \n     def test_from_headers(self):\n         mappings = [\n@@ -70,7 +70,7 @@ class ResponseTypesTest(unittest.TestCase):\n         for source, cls in mappings:\n             source = Headers(source)\n             retcls = responsetypes.from_headers(source)\n-            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n \n     def test_from_args(self):\n         # TODO: add more tests that check precedence between the different arguments\n@@ -86,7 +86,7 @@ class ResponseTypesTest(unittest.TestCase):\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_args(**source)\n-            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n \n     def test_custom_mime_types_loaded(self):\n         # check that mime.types files shipped with scrapy are loaded\n\n@@ -88,7 +88,7 @@ class SelectorTestCase(unittest.TestCase):\n         \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n         x = Selector(text='')\n         weakref.ref(x)\n-        assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % x.__class__.__name__\n+        assert not hasattr(x, '__dict__'), f\"{x.__class__.__name__} does not use __slots__\"\n \n     def test_selector_bad_args(self):\n         with self.assertRaisesRegex(ValueError, 'received both response and text'):\n\n@@ -13,7 +13,7 @@ class ItemSpider(Spider):\n \n     def start_requests(self):\n         for index in range(10):\n-            yield Request(self.mockserver.url('/status?n=200&id=%d' % index),\n+            yield Request(self.mockserver.url(f'/status?n=200&id={index}'),\n                           meta={'index': index})\n \n     def parse(self, response):\n\n@@ -163,11 +163,11 @@ class GeneratorOutputChainSpider(Spider):\n class _GeneratorDoNothingMiddleware:\n     def process_spider_output(self, response, result, spider):\n         for r in result:\n-            r['processed'].append('{}.process_spider_output'.format(self.__class__.__name__))\n+            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n             yield r\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = '{}.process_spider_exception'.format(self.__class__.__name__)\n+        method = f'{self.__class__.__name__}.process_spider_exception'\n         spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n         return None\n \n@@ -175,12 +175,12 @@ class _GeneratorDoNothingMiddleware:\n class GeneratorFailMiddleware:\n     def process_spider_output(self, response, result, spider):\n         for r in result:\n-            r['processed'].append('{}.process_spider_output'.format(self.__class__.__name__))\n+            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n             yield r\n             raise LookupError()\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = '{}.process_spider_exception'.format(self.__class__.__name__)\n+        method = f'{self.__class__.__name__}.process_spider_exception'\n         spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n         yield {'processed': [method]}\n \n@@ -192,11 +192,11 @@ class GeneratorDoNothingAfterFailureMiddleware(_GeneratorDoNothingMiddleware):\n class GeneratorRecoverMiddleware:\n     def process_spider_output(self, response, result, spider):\n         for r in result:\n-            r['processed'].append('{}.process_spider_output'.format(self.__class__.__name__))\n+            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n             yield r\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = '{}.process_spider_exception'.format(self.__class__.__name__)\n+        method = f'{self.__class__.__name__}.process_spider_exception'\n         spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n         yield {'processed': [method]}\n \n@@ -229,12 +229,12 @@ class _NotGeneratorDoNothingMiddleware:\n     def process_spider_output(self, response, result, spider):\n         out = []\n         for r in result:\n-            r['processed'].append('{}.process_spider_output'.format(self.__class__.__name__))\n+            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n             out.append(r)\n         return out\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = '{}.process_spider_exception'.format(self.__class__.__name__)\n+        method = f'{self.__class__.__name__}.process_spider_exception'\n         spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n         return None\n \n@@ -243,13 +243,13 @@ class NotGeneratorFailMiddleware:\n     def process_spider_output(self, response, result, spider):\n         out = []\n         for r in result:\n-            r['processed'].append('{}.process_spider_output'.format(self.__class__.__name__))\n+            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n             out.append(r)\n         raise ReferenceError()\n         return out\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = '{}.process_spider_exception'.format(self.__class__.__name__)\n+        method = f'{self.__class__.__name__}.process_spider_exception'\n         spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n         return [{'processed': [method]}]\n \n@@ -262,12 +262,12 @@ class NotGeneratorRecoverMiddleware:\n     def process_spider_output(self, response, result, spider):\n         out = []\n         for r in result:\n-            r['processed'].append('{}.process_spider_output'.format(self.__class__.__name__))\n+            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n             out.append(r)\n         return out\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = '{}.process_spider_exception'.format(self.__class__.__name__)\n+        method = f'{self.__class__.__name__}.process_spider_exception'\n         spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n         return [{'processed': [method]}]\n \n\n@@ -16,7 +16,7 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n         try:\n             Request(**result)\n         except TypeError as e:\n-            self.fail(\"Request kwargs are not correct {}\".format(e))\n+            self.fail(f\"Request kwargs are not correct {e}\")\n \n     def test_get(self):\n         curl_command = \"curl http://example.org/\"\n\n@@ -299,7 +299,7 @@ class LocalWeakReferencedCacheTest(unittest.TestCase):\n         cache = LocalWeakReferencedCache()\n         refs = []\n         for x in range(max):\n-            refs.append(Request('https://example.org/{}'.format(x)))\n+            refs.append(Request(f'https://example.org/{x}'))\n             cache[refs[-1]] = x\n         self.assertEqual(len(cache), max)\n         for i, r in enumerate(refs):\n\n@@ -40,15 +40,15 @@ class MustbeDeferredTest(unittest.TestCase):\n \n \n def cb1(value, arg1, arg2):\n-    return \"(cb1 %s %s %s)\" % (value, arg1, arg2)\n+    return f\"(cb1 {value} {arg1} {arg2})\"\n \n \n def cb2(value, arg1, arg2):\n-    return defer.succeed(\"(cb2 %s %s %s)\" % (value, arg1, arg2))\n+    return defer.succeed(f\"(cb2 {value} {arg1} {arg2})\")\n \n \n def cb3(value, arg1, arg2):\n-    return \"(cb3 %s %s %s)\" % (value, arg1, arg2)\n+    return f\"(cb3 {value} {arg1} {arg2})\"\n \n \n def cb_fail(value, arg1, arg2):\n@@ -56,7 +56,7 @@ def cb_fail(value, arg1, arg2):\n \n \n def eb1(failure, arg1, arg2):\n-    return \"(eb1 %s %s %s)\" % (failure.value.__class__.__name__, arg1, arg2)\n+    return f\"(eb1 {failure.value.__class__.__name__} {arg1} {arg2})\"\n \n \n class DeferUtilsTest(unittest.TestCase):\n\n@@ -409,7 +409,7 @@ class TestHelper(unittest.TestCase):\n \n     def _assert_type_and_value(self, a, b, obj):\n         self.assertTrue(type(a) is type(b),\n-                        'Got {}, expected {} for {!r}'.format(type(a), type(b), obj))\n+                        f'Got {type(a)}, expected {type(b)} for { obj!r}')\n         self.assertEqual(a, b)\n \n \n\n@@ -213,7 +213,7 @@ def create_guess_scheme_t(args):\n     def do_expected(self):\n         url = guess_scheme(args[0])\n         assert url.startswith(args[1]), \\\n-            'Wrong scheme guessed: for `%s` got `%s`, expected `%s...`' % (args[0], url, args[1])\n+            f'Wrong scheme guessed: for `{args[0]}` got `{url}`, expected `{args[1]}...`'\n     return do_expected\n \n \n@@ -254,7 +254,7 @@ for k, args in enumerate(\n     start=1,\n ):\n     t_method = create_guess_scheme_t(args)\n-    t_method.__name__ = 'test_uri_%03d' % k\n+    t_method.__name__ = f'test_uri_{k:03}'\n     setattr(GuessSchemeTest, t_method.__name__, t_method)\n \n # TODO: the following tests do not pass with current implementation\n@@ -269,7 +269,7 @@ for k, args in enumerate(\n     start=1,\n ):\n     t_method = create_skipped_scheme_t(args)\n-    t_method.__name__ = 'test_uri_skipped_%03d' % k\n+    t_method.__name__ = f'test_uri_skipped_{k:03}'\n     setattr(GuessSchemeTest, t_method.__name__, t_method)\n \n \n\n@@ -253,7 +253,7 @@ class WebClientTestCase(unittest.TestCase):\n         shutil.rmtree(self.tmpname)\n \n     def getURL(self, path):\n-        return \"http://127.0.0.1:%d/%s\" % (self.portno, path)\n+        return f\"http://127.0.0.1:{self.portno}/{path}\"\n \n     def testPayload(self):\n         s = \"0123456789\" * 10\n@@ -265,7 +265,7 @@ class WebClientTestCase(unittest.TestCase):\n         # it should extract from url\n         return defer.gatherResults([\n             getPage(self.getURL(\"host\")).addCallback(\n-                self.assertEqual, to_bytes(\"127.0.0.1:%d\" % self.portno)),\n+                self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\")),\n             getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}).addCallback(\n                 self.assertEqual, to_bytes(\"www.example.com\"))])\n \n@@ -298,7 +298,7 @@ class WebClientTestCase(unittest.TestCase):\n         \"\"\"\n         d = getPage(self.getURL(\"host\"), timeout=100)\n         d.addCallback(\n-            self.assertEqual, to_bytes(\"127.0.0.1:%d\" % self.portno))\n+            self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\"))\n         return d\n \n     def test_timeoutTriggering(self):\n@@ -376,7 +376,7 @@ class WebClientSSLTestCase(unittest.TestCase):\n             interface=\"127.0.0.1\")\n \n     def getURL(self, path):\n-        return \"https://127.0.0.1:%d/%s\" % (self.portno, path)\n+        return f\"https://127.0.0.1:{self.portno}/{path}\"\n \n     def setUp(self):\n         self.tmpname = self.mktemp()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
