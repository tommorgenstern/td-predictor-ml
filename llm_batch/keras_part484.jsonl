{"custom_id": "keras#ef26e315dd9135eaa12ad01c38fe57459dbfeb12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 115 | Lines Deleted: 105 | Files Changed: 5 | Hunks: 58 | Methods Changed: 29 | Complexity Δ (Sum/Max): 4/7 | Churn Δ: 220 | Churn Cumulative: 33460 | Contributors (this commit): 151 | Commits (past 90d): 66 | Contributors (cumulative): 190 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3433,22 +3433,22 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                             state[f\"{child_attr}-{contained_obj}\"]\n                         )\n \n-    def _save_state(self, dir_path):\n-        file_path = tf.io.gfile.join(dir_path, saving_lib.STATE_FILENAME)\n+    def _save_state(self, dirpath):\n+        filepath = tf.io.gfile.join(dirpath, \"weights.npz\")\n         weights = self._get_state()\n         if weights:\n             # Only save the state if that of the trackable is available.\n-            np.savez(file_path, **weights)\n-            logging.debug(f\"Saved state to {file_path}\")\n+            np.savez(filepath, **weights)\n+            logging.debug(f\"Saved state to {filepath}\")\n \n-    def _load_state(self, dir_path):\n-        file_path = tf.io.gfile.join(dir_path, saving_lib.STATE_FILENAME)\n-        if tf.io.gfile.exists(file_path):\n-            loaded_npz = np.load(file_path)\n-            logging.debug(f\"Loaded state from {file_path}\")\n+    def _load_state(self, dirpath):\n+        filepath = tf.io.gfile.join(dirpath, \"weights.npz\")\n+        if tf.io.gfile.exists(filepath):\n+            loaded_npz = np.load(filepath)\n             self._set_state(\n                 {file: loaded_npz[file] for file in loaded_npz.files}\n             )\n+            logging.debug(f\"Loaded state from {filepath}\")\n \n \n class TensorFlowOpLayer(Layer):\n\n@@ -3834,8 +3834,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     def _compile_was_called(self):\n         return self._is_compiled\n \n-    def _save_new(self, dirpath):\n-        return saving_lib.save_model(self, dirpath)\n+    def _save_experimental(self, filepath):\n+        return saving_lib.save_model(self, filepath)\n \n \n def reduce_per_replica(values, strategy, reduction=\"auto\"):\n\n@@ -715,10 +715,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n             variable.assign(state[variable.name])\n \n     def _save_state(self, dir_path):\n-        # To avoid circular import\n-        from keras.saving.experimental import saving_lib\n-\n-        file_path = tf.io.gfile.join(dir_path, saving_lib.STATE_FILENAME)\n+        file_path = tf.io.gfile.join(dir_path, \"state.npz\")\n         weights = self._get_state()\n         if weights:\n             # Only save the state if that of the trackable is available.\n@@ -726,10 +723,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n             logging.debug(f\"Saved state to {file_path}\")\n \n     def _load_state(self, dir_path):\n-        # To avoid circular import\n-        from keras.saving.experimental import saving_lib\n-\n-        file_path = tf.io.gfile.join(dir_path, saving_lib.STATE_FILENAME)\n+        file_path = tf.io.gfile.join(dir_path, \"state.npz\")\n         if tf.io.gfile.exists(file_path):\n             loaded_npz = np.load(file_path)\n             logging.debug(f\"Loaded state from {file_path}\")\n\n@@ -15,7 +15,9 @@\n \"\"\"Python-based idempotent model-saving functionality.\"\"\"\n \n import json\n+import os\n import tempfile\n+import uuid\n import zipfile\n \n import tensorflow.compat.v2 as tf\n@@ -30,8 +32,6 @@ from keras.utils import io_utils\n \n # isort: off\n \n-_ARCHIVE_FILENAME = \"archive.keras\"\n-STATE_FILENAME = \"states.npz\"\n _SELF_DIRNAME = \"self\"\n _CONFIG_FILENAME = \"config.json\"\n _STATES_ROOT_DIRNAME = \"model\"\n@@ -83,18 +83,18 @@ def _extract_dir(zipfile_to_load, root_system_path, zip_dir):\n             )\n \n \n-def _load_state(trackable, zip_dir_path, temp_path, zipfile_to_load):\n-    states_dir_path = tf.io.gfile.join(zip_dir_path, _SELF_DIRNAME)\n+def _load_state(trackable, zip_dirpath, temp_path, zipfile_to_load):\n+    states_dirpath = tf.io.gfile.join(zip_dirpath, _SELF_DIRNAME)\n     # Extract the whole directory that represents the states of the trackable\n-    # into a temporary path.\n-    _extract_dir(zipfile_to_load, temp_path, states_dir_path)\n-    dir_path_to_load_state = tf.io.gfile.join(temp_path, states_dir_path)\n+    # into a temporary directory to be removed at the end.\n+    _extract_dir(zipfile_to_load, temp_path, states_dirpath)\n+    dirpath_to_load_state = tf.io.gfile.join(temp_path, states_dirpath)\n     # TODO(rchao): Make `.set_state()` and `.load_state()` exported methods\n     # and remove the attr check.\n     if hasattr(trackable, \"_load_state\"):\n-        trackable._load_state(dir_path_to_load_state)\n-    if tf.io.gfile.exists(dir_path_to_load_state):\n-        tf.io.gfile.rmtree(dir_path_to_load_state)\n+        trackable._load_state(dirpath_to_load_state)\n+    if tf.io.gfile.exists(dirpath_to_load_state):\n+        tf.io.gfile.rmtree(dirpath_to_load_state)\n \n     # Recursively load states for Keras trackables such as layers/optimizers.\n     for child_attr in dir(trackable):\n@@ -109,41 +109,45 @@ def _load_state(trackable, zip_dir_path, temp_path, zipfile_to_load):\n         try:\n             child_obj = getattr(trackable, child_attr)\n         except Exception:\n-            # Avoid raising the exception when visiting the attributes.\n+            # Avoid raising exceptions when visiting attributes.\n             continue\n         if _is_keras_trackable(child_obj):\n             _load_state(\n                 child_obj,\n-                tf.io.gfile.join(zip_dir_path, child_attr),\n+                tf.io.gfile.join(zip_dirpath, child_attr),\n                 temp_path,\n                 zipfile_to_load,\n             )\n         elif is_container(child_obj):\n             _load_container_state(\n                 child_obj,\n-                tf.io.gfile.join(zip_dir_path, child_attr),\n+                tf.io.gfile.join(zip_dirpath, child_attr),\n                 temp_path,\n                 zipfile_to_load,\n             )\n \n \n-def _load_container_state(container, zip_dir_path, temp_path, zipfile_to_load):\n+def _load_container_state(container, zip_dirpath, temp_path, zipfile_to_load):\n     for trackable in container:\n         if _is_keras_trackable(trackable):\n             _load_state(\n                 trackable,\n-                tf.io.gfile.join(zip_dir_path, trackable.name),\n+                tf.io.gfile.join(zip_dirpath, trackable.name),\n                 temp_path,\n                 zipfile_to_load,\n             )\n \n \n-def load_model(dirpath, custom_objects=None):\n-    \"\"\"Load a zip-archive representing a Keras model given the container dir.\"\"\"\n-    file_path = tf.io.gfile.join(dirpath, _ARCHIVE_FILENAME)\n-    temp_path = tempfile.mkdtemp(dir=dirpath)\n-\n-    with zipfile.ZipFile(file_path, \"r\") as zipfile_to_load:\n+def load_model(filepath, custom_objects=None):\n+    \"\"\"Load a zip archive representing a Keras model.\"\"\"\n+    if not filepath.endswith(\".keras\"):\n+        raise ValueError(\n+            \"Invalid filename: expected a `.keras` extension. \"\n+            f\"Received: filepath={filepath}\"\n+        )\n+    temp_path = _get_temp_dir()\n+    try:\n+        with zipfile.ZipFile(filepath, \"r\") as zipfile_to_load:\n             _print_archive(zipfile_to_load, \"loaded from\")\n             with zipfile_to_load.open(_CONFIG_FILENAME, \"r\") as c:\n                 config_json = c.read()\n@@ -151,13 +155,16 @@ def load_model(dirpath, custom_objects=None):\n             # Note: we should NOT use a custom JSON decoder. Anything that\n             # needs custom decoding must be handled in deserialize_keras_object.\n             config_dict = json.loads(config_json)\n-        # Construct the model from the configuration file saved in the archive.\n+            # Construct the model from the configuration file in the archive.\n             model = deserialize_keras_object(config_dict, custom_objects)\n             _load_state(model, _STATES_ROOT_DIRNAME, temp_path, zipfile_to_load)\n-\n+    except Exception as e:\n+        raise e\n+    else:\n+        return model\n+    finally:\n         if tf.io.gfile.exists(temp_path):\n             tf.io.gfile.rmtree(temp_path)\n-    return model\n \n \n def _write_recursively(zipfile_to_save, system_path, zip_path):\n@@ -172,7 +179,7 @@ def _write_recursively(zipfile_to_save, system_path, zip_path):\n \n \n def _save_state(\n-    trackable, zip_dir_path, temp_path, zipfile_to_save, saved_trackables\n+    trackable, zip_dirpath, temp_path, zipfile_to_save, saved_trackables\n ):\n     # Check whether this trackable has been saved; if so, do not duplicate the\n     # saving.\n@@ -183,19 +190,19 @@ def _save_state(\n     # and remove the attr check.\n     if hasattr(trackable, \"_save_state\"):\n         # Designate a `self` directory for the trackable object to save.\n-        states_dir_path = tf.io.gfile.join(temp_path, _SELF_DIRNAME)\n-        if not tf.io.gfile.exists(states_dir_path):\n-            tf.io.gfile.mkdir(states_dir_path)\n-        trackable._save_state(states_dir_path)\n-        if states_dir_path is not None:\n+        states_dirpath = tf.io.gfile.join(temp_path, _SELF_DIRNAME)\n+        if not tf.io.gfile.exists(states_dirpath):\n+            tf.io.gfile.mkdir(states_dirpath)\n+        trackable._save_state(states_dirpath)\n+        if states_dirpath is not None:\n             # Recursively write the states (represented by files inside the\n             # directory) into the zip file.\n             _write_recursively(\n                 zipfile_to_save,\n-                states_dir_path,\n-                tf.io.gfile.join(zip_dir_path, _SELF_DIRNAME),\n+                states_dirpath,\n+                tf.io.gfile.join(zip_dirpath, _SELF_DIRNAME),\n             )\n-            tf.io.gfile.rmtree(states_dir_path)\n+            tf.io.gfile.rmtree(states_dirpath)\n         saved_trackables.add(trackable)\n \n     # Recursively ask contained trackable (layers, optimizers,\n@@ -217,7 +224,7 @@ def _save_state(\n         if _is_keras_trackable(child_obj):\n             _save_state(\n                 child_obj,\n-                tf.io.gfile.join(zip_dir_path, child_attr),\n+                tf.io.gfile.join(zip_dirpath, child_attr),\n                 temp_path,\n                 zipfile_to_save,\n                 saved_trackables,\n@@ -225,7 +232,7 @@ def _save_state(\n         elif is_container(child_obj):\n             _save_container_state(\n                 child_obj,\n-                tf.io.gfile.join(zip_dir_path, child_attr),\n+                tf.io.gfile.join(zip_dirpath, child_attr),\n                 temp_path,\n                 zipfile_to_save,\n                 saved_trackables,\n@@ -233,21 +240,21 @@ def _save_state(\n \n \n def _save_container_state(\n-    container, zip_dir_path, temp_path, zipfile_to_save, saved_trackables\n+    container, zip_dirpath, temp_path, zipfile_to_save, saved_trackables\n ):\n     for trackable in container:\n         if _is_keras_trackable(trackable):\n             _save_state(\n                 trackable,\n-                tf.io.gfile.join(zip_dir_path, trackable.name),\n+                tf.io.gfile.join(zip_dirpath, trackable.name),\n                 temp_path,\n                 zipfile_to_save,\n                 saved_trackables,\n             )\n \n \n-def save_model(model, dirpath):\n-    \"\"\"Save a zip-archive representing a Keras model given the container dir.\n+def save_model(model, filepath):\n+    \"\"\"Save a zip-archive representing a Keras model to the given filepath.\n \n     The zip-based archive contains the following structure:\n \n@@ -270,22 +277,23 @@ def save_model(model, dirpath):\n     container (list, tuple, or dict), and the container is referenced via a\n     layer attribute.\n     \"\"\"\n-    if not tf.io.gfile.exists(dirpath):\n-        tf.io.gfile.mkdir(dirpath)\n-    file_path = tf.io.gfile.join(dirpath, _ARCHIVE_FILENAME)\n+    if not filepath.endswith(\".keras\"):\n+        raise ValueError(\n+            \"Invalid filename: expected a `.keras` extension. \"\n+            f\"Received: filepath={filepath}\"\n+        )\n \n     # TODO(rchao): Save the model's metadata (e.g. Keras version) in a separate\n     # file in the archive.\n     serialized_model_dict = serialize_keras_object(model)\n     config_json = json.dumps(serialized_model_dict).encode()\n \n-    # Utilize a temporary directory for the interim npz files.\n-    temp_path = tempfile.mkdtemp(dir=dirpath)\n-    if not tf.io.gfile.exists(temp_path):\n-        tf.io.gfile.mkdir(temp_path)\n+    # Utilize a temporary directory for the storing files prior to zipping.\n+    temp_path = _get_temp_dir()\n \n-    # Save the configuration json and state npz's.\n-    with zipfile.ZipFile(file_path, \"x\") as zipfile_to_save:\n+    try:\n+        # Save the configuration json and state files.\n+        with zipfile.ZipFile(filepath, \"x\") as zipfile_to_save:\n             with zipfile_to_save.open(_CONFIG_FILENAME, \"w\") as c:\n                 c.write(config_json)\n                 logging.debug(f\"Written config: {config_json} into {c}.\")\n@@ -293,6 +301,25 @@ def save_model(model, dirpath):\n                 model, _STATES_ROOT_DIRNAME, temp_path, zipfile_to_save, set()\n             )\n             _print_archive(zipfile_to_save, \"saved in\")\n-\n+    except Exception as e:\n+        raise e\n+    finally:\n         # Remove the directory temporarily used.\n         tf.io.gfile.rmtree(temp_path)\n+\n+\n+def _get_temp_dir():\n+    temp_dir = tempfile.mkdtemp()\n+    try:\n+        testfile = tempfile.TemporaryFile(dir=temp_dir)\n+        testfile.close()\n+        stats = os.statvfs(temp_dir)\n+        available_space = stats.f_frsize * stats.f_bavail\n+    except OSError:\n+        # Non-writable\n+        available_space = 0\n+    if available_space < 2000000000:\n+        # Fallback on RAM if disk is nonwritable or if less than 2GB available.\n+        temp_dir = f\"ram://{uuid.uuid4()}\"\n+        tf.io.gfile.mkdir(temp_dir)\n+    return temp_dir\n\n@@ -177,9 +177,9 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n         return functional_model\n \n     def test_saving_after_compile_but_before_fit(self):\n-        temp_dir = os.path.join(self.get_temp_dir(), \"my_model\")\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n         subclassed_model = self._get_subclassed_model()\n-        subclassed_model._save_new(temp_dir)\n+        subclassed_model._save_experimental(temp_filepath)\n \n         # This is so that we can register another function with the same custom\n         # object key, and make sure the newly registered function is used while\n@@ -197,7 +197,7 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n                 tf.math.squared_difference(y_pred, y_true), axis=-1\n             )\n \n-        loaded_model = saving_lib.load_model(temp_dir)\n+        loaded_model = saving_lib.load_model(temp_filepath)\n         self.assertEqual(\n             subclassed_model._is_compiled, loaded_model._is_compiled\n         )\n@@ -238,14 +238,14 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertIsNot(module_my_mean_squared_error, my_mean_squared_error)\n \n     def test_saving_after_fit(self):\n-        temp_dir = os.path.join(self.get_temp_dir(), \"my_model\")\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n         subclassed_model = self._get_subclassed_model()\n \n         x = np.random.random((100, 32))\n         y = np.random.random((100, 1))\n         subclassed_model.fit(x, y, epochs=1)\n-        subclassed_model._save_new(temp_dir)\n-        loaded_model = saving_lib.load_model(temp_dir)\n+        subclassed_model._save_experimental(temp_filepath)\n+        loaded_model = saving_lib.load_model(temp_filepath)\n         self.assertEqual(\n             subclassed_model._is_compiled, loaded_model._is_compiled\n         )\n@@ -297,10 +297,10 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n             )\n \n     def test_saving_preserve_unbuilt_state(self):\n-        temp_dir = os.path.join(self.get_temp_dir(), \"my_model\")\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n         subclassed_model = CustomModelX()\n-        subclassed_model._save_new(temp_dir)\n-        loaded_model = saving_lib.load_model(temp_dir)\n+        subclassed_model._save_experimental(temp_filepath)\n+        loaded_model = saving_lib.load_model(temp_filepath)\n         self.assertEqual(\n             subclassed_model._is_compiled, loaded_model._is_compiled\n         )\n@@ -308,13 +308,13 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertFalse(loaded_model.built)\n \n     def test_saving_preserve_built_state(self):\n-        temp_dir = os.path.join(self.get_temp_dir(), \"my_model\")\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n         subclassed_model = self._get_subclassed_model()\n         x = np.random.random((100, 32))\n         y = np.random.random((100, 1))\n         subclassed_model.fit(x, y, epochs=1)\n-        subclassed_model._save_new(temp_dir)\n-        loaded_model = saving_lib.load_model(temp_dir)\n+        subclassed_model._save_experimental(temp_filepath)\n+        loaded_model = saving_lib.load_model(temp_filepath)\n         self.assertEqual(\n             subclassed_model._is_compiled, loaded_model._is_compiled\n         )\n@@ -328,15 +328,14 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n         )\n \n     def test_saved_module_paths_and_class_names(self):\n-        temp_dir = os.path.join(self.get_temp_dir(), \"my_model\")\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n         subclassed_model = self._get_subclassed_model()\n         x = np.random.random((100, 32))\n         y = np.random.random((100, 1))\n         subclassed_model.fit(x, y, epochs=1)\n-        subclassed_model._save_new(temp_dir)\n+        subclassed_model._save_experimental(temp_filepath)\n \n-        file_path = tf.io.gfile.join(temp_dir, saving_lib._ARCHIVE_FILENAME)\n-        with zipfile.ZipFile(file_path, \"r\") as z:\n+        with zipfile.ZipFile(temp_filepath, \"r\") as z:\n             with z.open(saving_lib._CONFIG_FILENAME, \"r\") as c:\n                 config_json = c.read()\n         config_dict = json_utils.decode(config_json)\n@@ -372,7 +371,7 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n             def __call__(self, msg):\n                 self.contents += msg + \"\\n\"\n \n-        temp_dir = os.path.join(self.get_temp_dir(), \"my_model\")\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n \n         if layer == \"lambda\":\n             func = tf.function(lambda x: tf.math.cos(x) + tf.math.sin(x))\n@@ -393,8 +392,8 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n         x = np.random.random((1000, 32))\n         y = np.random.random((1000, 1))\n         functional_model.fit(x, y, epochs=3)\n-        functional_model._save_new(temp_dir)\n-        loaded_model = saving_lib.load_model(temp_dir)\n+        functional_model._save_experimental(temp_filepath)\n+        loaded_model = saving_lib.load_model(temp_filepath)\n         self.assertEqual(\n             functional_model._is_compiled, loaded_model._is_compiled\n         )\n@@ -426,16 +425,14 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n         )\n     )\n     def test_saving_model_state(self, model_type):\n-        temp_dir = os.path.join(self.get_temp_dir(), \"my_model\")\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n         model = getattr(self, f\"_get_{model_type}_model\")()\n         x = np.random.random((100, 32))\n         y = np.random.random((100, 1))\n         model.fit(x, y, epochs=1)\n \n         # Assert that the archive has not been saved.\n-        self.assertFalse(\n-            os.path.exists(os.path.join(temp_dir, saving_lib._ARCHIVE_FILENAME))\n-        )\n+        self.assertFalse(os.path.exists(temp_filepath))\n \n         # Mutate the `Dense` layer custom weights to ensure that list and\n         # dict-contained weights get restored.\n@@ -443,19 +440,12 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n         model.layers[1].weights_in_dict[\"my_weight\"].assign(2)\n         model.layers[1].nested_layer.kernel.assign([[1]])\n \n-        model._save_new(temp_dir)\n+        model._save_experimental(temp_filepath)\n \n         # Assert that the archive has been saved.\n-        self.assertTrue(\n-            os.path.exists(os.path.join(temp_dir, saving_lib._ARCHIVE_FILENAME))\n-        )\n-\n-        # Assert the temporarily created dir does not persist before and after\n-        # loading.\n-        self.assertFalse(os.path.exists(os.path.join(temp_dir, \"tmp\")))\n-        loaded_model = saving_lib.load_model(temp_dir)\n+        self.assertTrue(os.path.exists(temp_filepath))\n+        loaded_model = saving_lib.load_model(temp_filepath)\n         self.assertEqual(model._is_compiled, loaded_model._is_compiled)\n-        self.assertFalse(os.path.exists(os.path.join(temp_dir, \"tmp\")))\n \n         # The weights are supposed to be the same (between original and loaded\n         # models).\n@@ -479,8 +469,7 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n     def test_compile_overridden_model_raises_if_no_from_config_overridden(\n         self, model_type\n     ):\n-\n-        temp_dir = os.path.join(self.get_temp_dir(), \"my_model\")\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n         model = (\n             CompileOverridingModel()\n             if model_type == \"subclassed\"\n@@ -488,10 +477,10 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n                 [keras.layers.Embedding(4, 1), MyDense(1), MyDense(1)]\n             )\n         )\n-        model._save_new(temp_dir)\n+        model._save_experimental(temp_filepath)\n \n         with mock.patch.object(logging, \"warning\") as mock_warn:\n-            saving_lib.load_model(temp_dir)\n+            saving_lib.load_model(temp_filepath)\n         self.assertIn(\n             \"`compile()` was not called as part of model loading \"\n             \"because the model's `compile()` method is custom. \",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b85964a3559c456926822a6fd76690384bb1296f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 15611 | Contributors (this commit): 114 | Commits (past 90d): 17 | Contributors (cumulative): 129 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,6 +31,7 @@ import tensorflow.compat.v2 as tf\n from keras import backend\n from keras.distribute import distributed_file_utils\n from keras.distribute import worker_training_state\n+from keras.optimizers import optimizer_experimental\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.utils import generic_utils\n from keras.utils import io_utils\n@@ -2768,6 +2769,9 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         self._is_tracing = False\n \n     def _collect_learning_rate(self, logs):\n+        if isinstance(self.model.optimizer, optimizer_experimental.Optimizer):\n+            lr_schedule = getattr(self.model.optimizer, \"_learning_rate\", None)\n+        else:\n             lr_schedule = getattr(self.model.optimizer, \"lr\", None)\n         if isinstance(lr_schedule, learning_rate_schedule.LearningRateSchedule):\n             logs[\"learning_rate\"] = lr_schedule(self.model.optimizer.iterations)\n\n@@ -38,6 +38,7 @@ from keras.callbacks import Callback\n from keras.engine import sequential\n from keras.layers import Activation\n from keras.layers import Dense\n+from keras.optimizers import sgd_experimental\n from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_combinations\n@@ -428,7 +429,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n                     raise RuntimeError(\"Interruption\")\n \n         model = keras.Sequential([keras.layers.Dense(10)])\n-        optimizer = gradient_descent.SGD()\n+        optimizer = sgd_experimental.SGD()\n         model.compile(optimizer, loss=\"mse\")\n \n         x = tf.random.uniform((24, 10))\n@@ -505,7 +506,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n                     )\n \n         model = keras.Sequential([keras.layers.Dense(10)])\n-        optimizer = gradient_descent.SGD()\n+        optimizer = sgd_experimental.SGD()\n         model.compile(optimizer, loss=\"mse\")\n \n         x = tf.random.uniform((24, 10))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d38e0c4fc0805e80fad4c63b69e4efab9c0f8609", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 366 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -315,14 +315,8 @@ def _retrieve_class_or_fn(\n ):\n     # If there is a custom object registered via\n     # `register_keras_serializable`, that takes precedence.\n-    custom_obj = generic_utils.get_custom_objects_by_name(registered_name)\n-    if custom_obj is not None:\n-        return custom_obj\n-\n-    # If there is a custom object by this name in `custom_objects`,\n-    # that takes precedence.\n     custom_obj = generic_utils.get_custom_objects_by_name(\n-        name, custom_objects=custom_objects\n+        registered_name, custom_objects=custom_objects\n     )\n     if custom_obj is not None:\n         return custom_obj\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#12bbd222729f75a60b8692598f9ca45ae769230c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 22 | Churn Cumulative: 1083 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.7272727272727273\n\nDIFF:\n@@ -18,6 +18,7 @@ import json\n import os\n import tempfile\n import uuid\n+import warnings\n import zipfile\n \n import tensorflow.compat.v2 as tf\n@@ -207,12 +208,15 @@ def _save_state(\n \n     # Recursively ask contained trackable (layers, optimizers,\n     # etc.) to save states.\n+    attr_skiplist = {\n+        \"_self_tracked_trackables\",\n+        \"_layer_call_argspecs\",\n+        \"_output_layers\",\n+        \"updates\",  # Would raise a warning if visited.\n+        \"state_updates\",  # Would raise a warning if visited.\n+    }\n     for child_attr in dir(trackable):\n-        if (\n-            child_attr == \"_self_tracked_trackables\"\n-            or child_attr == \"_layer_call_argspecs\"\n-            or child_attr == \"_output_layers\"\n-        ):\n+        if child_attr in attr_skiplist:\n             # Avoid certain attribute names to allow readable state file paths,\n             # e.g., `layers`.\n             continue\n@@ -282,6 +286,14 @@ def save_model(model, filepath):\n             \"Invalid filename: expected a `.keras` extension. \"\n             f\"Received: filepath={filepath}\"\n         )\n+    if not model.built:\n+        warnings.warn(\n+            \"You are saving a model that has not yet been built. \"\n+            \"It might not contain any weights yet. \"\n+            \"Consider building the model first by calling it \"\n+            \"on some data.\",\n+            stacklevel=2,\n+        )\n \n     # TODO(rchao): Save the model's metadata (e.g. Keras version) in a separate\n     # file in the archive.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83b118f44ea7891a6f553e91aa2915109e1068c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 1871 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,7 @@\n # ==============================================================================\n \"\"\"Python-based idempotent model-saving functionality.\"\"\"\n \n+import datetime\n import json\n import os\n import tempfile\n@@ -24,6 +25,7 @@ import zipfile\n import tensorflow.compat.v2 as tf\n from absl import logging\n \n+import keras\n from keras import losses\n from keras.engine import base_layer\n from keras.optimizers.optimizer_experimental import optimizer\n@@ -35,6 +37,7 @@ from keras.utils import io_utils\n \n _SELF_DIRNAME = \"self\"\n _CONFIG_FILENAME = \"config.json\"\n+_METADATA_FILENAME = \"metadata.json\"\n _STATES_ROOT_DIRNAME = \"model\"\n \n # A temporary flag to enable the new idempotent saving framework.\n@@ -295,20 +298,25 @@ def save_model(model, filepath):\n             stacklevel=2,\n         )\n \n-    # TODO(rchao): Save the model's metadata (e.g. Keras version) in a separate\n-    # file in the archive.\n     serialized_model_dict = serialize_keras_object(model)\n     config_json = json.dumps(serialized_model_dict).encode()\n+    # TODO(fchollet): consider saving dependencies list / versions in metadata.\n+    metadata_json = json.dumps(\n+        {\n+            \"keras_version\": keras.__version__,\n+            \"date_saved\": datetime.datetime.now().strftime(\"%Y-%m-%d@%H:%M:%S\"),\n+        }\n+    ).encode()\n \n     # Utilize a temporary directory for the storing files prior to zipping.\n     temp_path = _get_temp_dir()\n-\n     try:\n         # Save the configuration json and state files.\n         with zipfile.ZipFile(filepath, \"x\") as zipfile_to_save:\n+            with zipfile_to_save.open(_METADATA_FILENAME, \"w\") as c:\n+                c.write(metadata_json)\n             with zipfile_to_save.open(_CONFIG_FILENAME, \"w\") as c:\n                 c.write(config_json)\n-                logging.debug(f\"Written config: {config_json} into {c}.\")\n             _save_state(\n                 model, _STATES_ROOT_DIRNAME, temp_path, zipfile_to_save, set()\n             )\n\n@@ -487,6 +487,17 @@ class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n             mock_warn.call_args_list[0][0][0],\n         )\n \n+    def test_metadata(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n+        model = CompileOverridingModel()\n+        model._save_experimental(temp_filepath)\n+        with zipfile.ZipFile(temp_filepath, \"r\") as z:\n+            with z.open(saving_lib._METADATA_FILENAME, \"r\") as c:\n+                metadata_json = c.read()\n+        metadata = json_utils.decode(metadata_json)\n+        self.assertIn(\"keras_version\", metadata)\n+        self.assertIn(\"date_saved\", metadata)\n+\n \n if __name__ == \"__main__\":\n     if tf.__internal__.tf2.enabled():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6888ae0d824830bb478fa4e513c7ac4baa47c7ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2264 | Files Changed: 8 | Hunks: 8 | Methods Changed: 78 | Complexity Δ (Sum/Max): -204/0 | Churn Δ: 2264 | Churn Cumulative: 5803 | Contributors (this commit): 15 | Commits (past 90d): 11 | Contributors (cumulative): 50 | DMM Complexity: 0.3673655423883318\n\nDIFF:\n@@ -1,524 +0,0 @@\n-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Deprecated experimental Keras SavedModel implementation.\"\"\"\n-\n-import warnings\n-\n-import tensorflow.compat.v2 as tf\n-\n-from keras import backend\n-from keras.optimizers import optimizer_v1\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n-from keras.saving import model_config\n-from keras.saving import saving_utils\n-from keras.saving import utils_v1 as model_utils\n-from keras.utils import mode_keys\n-from keras.utils.generic_utils import LazyLoader\n-\n-# isort: off\n-from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.util.tf_export import keras_export\n-\n-# To avoid circular dependencies between keras/engine and keras/saving,\n-# code in keras/saving must delay imports.\n-\n-# TODO(b/134426265): Switch back to single-quotes to match the rest of the file\n-# once the issue with copybara is fixed.\n-\n-metrics_lib = LazyLoader(\"metrics_lib\", globals(), \"keras.metrics\")\n-models_lib = LazyLoader(\"models_lib\", globals(), \"keras.models\")\n-sequential = LazyLoader(\"sequential\", globals(), \"keras.engine.sequential\")\n-\n-\n-# File name for json format of SavedModel.\n-SAVED_MODEL_FILENAME_JSON = \"saved_model.json\"\n-\n-\n-@keras_export(v1=[\"keras.experimental.export_saved_model\"])\n-def export_saved_model(\n-    model,\n-    saved_model_path,\n-    custom_objects=None,\n-    as_text=False,\n-    input_signature=None,\n-    serving_only=False,\n-):\n-    \"\"\"Exports a `tf.keras.Model` as a Tensorflow SavedModel.\n-\n-    Note that at this time, subclassed models can only be saved using\n-    `serving_only=True`.\n-\n-    The exported `SavedModel` is a standalone serialization of Tensorflow\n-    objects, and is supported by TF language APIs and the Tensorflow Serving\n-    system.  To load the model, use the function\n-    `tf.compat.v1.keras.experimental.load_from_saved_model`.\n-\n-    The `SavedModel` contains:\n-\n-    1. a checkpoint containing the model weights.\n-    2. a `SavedModel` proto containing the Tensorflow backend graph. Separate\n-       graphs are saved for prediction (serving), train, and evaluation. If\n-       the model has not been compiled, then only the graph computing\n-       predictions will be exported.\n-    3. the model's json config. If the model is subclassed, this will only be\n-       included if the model's `get_config()` method is overwritten.\n-\n-    Example:\n-\n-    ```python\n-    import tensorflow as tf\n-\n-    # Create a tf.keras model.\n-    model = tf.keras.Sequential()\n-    model.add(tf.keras.layers.Dense(1, input_shape=[10]))\n-    model.summary()\n-\n-    # Save the tf.keras model in the SavedModel format.\n-    path = '/tmp/simple_keras_model'\n-    tf.compat.v1.keras.experimental.export_saved_model(model, path)\n-\n-    # Load the saved keras model back.\n-    new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)\n-    new_model.summary()\n-    ```\n-\n-    Args:\n-      model: A `tf.keras.Model` to be saved. If the model is subclassed, the\n-        flag `serving_only` must be set to True.\n-      saved_model_path: a string specifying the path to the SavedModel\n-        directory.\n-      custom_objects: Optional dictionary mapping string names to custom classes\n-        or functions (e.g. custom loss functions).\n-      as_text: bool, `False` by default. Whether to write the `SavedModel` proto\n-        in text format. Currently unavailable in serving-only mode.\n-      input_signature: A possibly nested sequence of `tf.TensorSpec` objects,\n-        used to specify the expected model inputs. See `tf.function` for more\n-        details.\n-      serving_only: bool, `False` by default. When this is true, only the\n-        prediction graph is saved.\n-\n-    Raises:\n-      NotImplementedError: If the model is a subclassed model, and serving_only\n-        is False.\n-      ValueError: If the input signature cannot be inferred from the model.\n-      AssertionError: If the SavedModel directory already exists and isn't\n-        empty.\n-    \"\"\"\n-    warnings.warn(\n-        \"`tf.keras.experimental.export_saved_model` is deprecated\"\n-        \"and will be removed in a future version. \"\n-        'Please use `model.save(..., save_format=\"tf\")` or '\n-        '`tf.keras.models.save_model(..., save_format=\"tf\")`.',\n-        stacklevel=2,\n-    )\n-    if serving_only:\n-        tf.saved_model.save(\n-            model,\n-            saved_model_path,\n-            signatures=saving_utils.trace_model_call(model, input_signature),\n-        )\n-    else:\n-        _save_v1_format(\n-            model, saved_model_path, custom_objects, as_text, input_signature\n-        )\n-\n-    try:\n-        _export_model_json(model, saved_model_path)\n-    except NotImplementedError:\n-        logging.warning(\n-            \"Skipped saving model JSON, subclassed model does not have \"\n-            \"get_config() defined.\"\n-        )\n-\n-\n-def _export_model_json(model, saved_model_path):\n-    \"\"\"Saves model configuration as a json string under assets folder.\"\"\"\n-    model_json = model.to_json()\n-    model_json_filepath = tf.io.gfile.join(\n-        _get_or_create_assets_dir(saved_model_path),\n-        tf.compat.as_text(SAVED_MODEL_FILENAME_JSON),\n-    )\n-    with tf.io.gfile.GFile(model_json_filepath, \"w\") as f:\n-        f.write(model_json)\n-\n-\n-def _export_model_variables(model, saved_model_path):\n-    \"\"\"Saves model weights in checkpoint format under variables folder.\"\"\"\n-    _get_or_create_variables_dir(saved_model_path)\n-    checkpoint_prefix = _get_variables_path(saved_model_path)\n-    model.save_weights(checkpoint_prefix, save_format=\"tf\", overwrite=True)\n-    return checkpoint_prefix\n-\n-\n-def _save_v1_format(model, path, custom_objects, as_text, input_signature):\n-    \"\"\"Exports model to v1 SavedModel format.\"\"\"\n-    if not model._is_graph_network:\n-        if isinstance(model, sequential.Sequential):\n-            # If input shape is not directly set in the model, the exported\n-            # model will infer the expected shapes of the input from the model.\n-            if not model.built:\n-                raise ValueError(\n-                    \"Weights for sequential model have not yet been \"\n-                    \"created. Weights are created when the Model is first \"\n-                    \"called on inputs or `build()` is called with an \"\n-                    \"`input_shape`, or the first layer in the model has \"\n-                    \"`input_shape` during construction.\"\n-                )\n-            # TODO(kathywu): Build the model with input_signature to create the\n-            # weights before _export_model_variables().\n-        else:\n-            raise NotImplementedError(\n-                \"Subclassed models can only be exported for serving. Please \"\n-                \"set argument serving_only=True.\"\n-            )\n-\n-    builder = tf.__internal__.saved_model.SavedModelBuilder(path)\n-\n-    # Manually save variables to export them in an object-based checkpoint. This\n-    # skips the `builder.add_meta_graph_and_variables()` step, which saves a\n-    # named-based checkpoint.\n-    # TODO(b/113134168): Add fn to Builder to save with object-based saver.\n-    # TODO(b/113178242): This should only export the model json structure. Only\n-    # one save is needed once the weights can be copied from the model to clone.\n-    checkpoint_path = _export_model_variables(model, path)\n-\n-    # Export each mode. Use ModeKeys enums defined for `Estimator` to ensure\n-    # that Keras models and `Estimator`s are exported with the same format.\n-    # Every time a mode is exported, the code checks to see if new variables\n-    # have been created (e.g. optimizer slot variables). If that is the case,\n-    # the checkpoint is re-saved to include the new variables.\n-    export_args = {\n-        \"builder\": builder,\n-        \"model\": model,\n-        \"custom_objects\": custom_objects,\n-        \"checkpoint_path\": checkpoint_path,\n-        \"input_signature\": input_signature,\n-    }\n-\n-    has_saved_vars = False\n-    if model.optimizer:\n-        if isinstance(\n-            model.optimizer,\n-            (optimizer_v1.TFOptimizer, optimizer_v2.OptimizerV2),\n-        ):\n-            _export_mode(\n-                mode_keys.ModeKeys.TRAIN, has_saved_vars, **export_args\n-            )\n-            has_saved_vars = True\n-            _export_mode(mode_keys.ModeKeys.TEST, has_saved_vars, **export_args)\n-        else:\n-            logging.warning(\n-                \"Model was compiled with an optimizer, but the optimizer is \"\n-                \"not from `tf.train` (e.g. `tf.train.AdagradOptimizer`). \"\n-                \"Only the serving graph was exported. The train and evaluate \"\n-                \"graphs were not added to the SavedModel.\"\n-            )\n-    _export_mode(mode_keys.ModeKeys.PREDICT, has_saved_vars, **export_args)\n-\n-    builder.save(as_text)\n-\n-\n-def _get_var_list(model):\n-    \"\"\"Returns list of all checkpointed saveable objects in the model.\"\"\"\n-    var_list, _, _ = tf.__internal__.tracking.ObjectGraphView(\n-        model\n-    ).serialize_object_graph()\n-    return var_list\n-\n-\n-def create_placeholder(spec):\n-    return backend.placeholder(\n-        shape=spec.shape, dtype=spec.dtype, name=spec.name\n-    )\n-\n-\n-def _export_mode(\n-    mode,\n-    has_saved_vars,\n-    builder,\n-    model,\n-    custom_objects,\n-    checkpoint_path,\n-    input_signature,\n-):\n-    \"\"\"Exports a model, and optionally saves new vars from the clone model.\n-\n-    Args:\n-      mode: A `tf.estimator.ModeKeys` string.\n-      has_saved_vars: A `boolean` indicating whether the SavedModel has already\n-        exported variables.\n-      builder: A `SavedModelBuilder` object.\n-      model: A `tf.keras.Model` object.\n-      custom_objects: A dictionary mapping string names to custom classes\n-        or functions.\n-      checkpoint_path: String path to checkpoint.\n-      input_signature: Nested TensorSpec containing the expected inputs. Can be\n-        `None`, in which case the signature will be inferred from the model.\n-\n-    Raises:\n-      ValueError: If the train/eval mode is being exported, but the model does\n-        not have an optimizer.\n-    \"\"\"\n-    compile_clone = mode != mode_keys.ModeKeys.PREDICT\n-    if compile_clone and not model.optimizer:\n-        raise ValueError(\n-            f\"Model {model.name} does not have an optimizer. \"\n-            f\"Cannot export mode {mode}.\"\n-        )\n-\n-    model_graph = tf.compat.v1.get_default_graph()\n-    with tf.Graph().as_default() as g, backend.learning_phase_scope(\n-        mode == mode_keys.ModeKeys.TRAIN\n-    ):\n-\n-        if input_signature is None:\n-            input_tensors = None\n-        else:\n-            input_tensors = tf.nest.map_structure(\n-                create_placeholder, input_signature\n-            )\n-\n-        # Clone the model into blank graph. This will create placeholders for\n-        # inputs and targets.\n-        clone = models_lib.clone_and_build_model(\n-            model,\n-            input_tensors=input_tensors,\n-            custom_objects=custom_objects,\n-            compile_clone=compile_clone,\n-        )\n-\n-        # Make sure that iterations variable is added to the global step\n-        # collection, to ensure that, when the SavedModel graph is loaded, the\n-        # iterations variable is returned by\n-        # `tf.compat.v1.train.get_global_step()`. This is required for\n-        # compatibility with the SavedModelEstimator.\n-        if compile_clone:\n-            g.add_to_collection(\n-                tf.compat.v1.GraphKeys.GLOBAL_STEP, clone.optimizer.iterations\n-            )\n-\n-        # Extract update and train ops from train/test/predict functions.\n-        train_op = None\n-        if mode == mode_keys.ModeKeys.TRAIN:\n-            clone._make_train_function()\n-            train_op = clone.train_function.updates_op\n-        elif mode == mode_keys.ModeKeys.TEST:\n-            clone._make_test_function()\n-        else:\n-            clone._make_predict_function()\n-        g.get_collection_ref(tf.compat.v1.GraphKeys.UPDATE_OPS).extend(\n-            clone.state_updates\n-        )\n-\n-        with tf.compat.v1.Session().as_default():\n-            clone_var_list = _get_var_list(clone)\n-            if has_saved_vars:\n-                # Confirm all variables in the clone have an entry in the\n-                # checkpoint.\n-                status = clone.load_weights(checkpoint_path)\n-                status.assert_existing_objects_matched()\n-            else:\n-                # Confirm that variables between the clone and model match up\n-                # exactly, not counting optimizer objects. Optimizer objects are\n-                # ignored because if the model has not trained, the slot\n-                # variables will not have been created yet.\n-                # TODO(b/113179535): Replace with trackable equivalence.\n-                _assert_same_non_optimizer_objects(model, model_graph, clone, g)\n-\n-                # TODO(b/113178242): Use value transfer for trackable objects.\n-                clone.load_weights(checkpoint_path)\n-\n-                # Add graph and variables to SavedModel.\n-                # TODO(b/113134168): Switch to add_meta_graph_and_variables.\n-                clone.save_weights(\n-                    checkpoint_path, save_format=\"tf\", overwrite=True\n-                )\n-                builder._has_saved_variables = True\n-\n-            # Add graph to the SavedModel builder.\n-            builder.add_meta_graph(\n-                model_utils.EXPORT_TAG_MAP[mode],\n-                signature_def_map=_create_signature_def_map(clone, mode),\n-                saver=tf.compat.v1.train.Saver(\n-                    clone_var_list,\n-                    # Allow saving Models with no variables. This is somewhat\n-                    # odd, but it's not necessarily a bug.\n-                    allow_empty=True,\n-                ),\n-                init_op=tf.compat.v1.local_variables_initializer(),\n-                train_op=train_op,\n-            )\n-        return None\n-\n-\n-def _create_signature_def_map(model, mode):\n-    \"\"\"Creates a SignatureDef map from a Keras model.\"\"\"\n-    inputs_dict = {name: x for name, x in zip(model.input_names, model.inputs)}\n-    if model.optimizer:\n-        targets_dict = {\n-            x.name.split(\":\")[0]: x for x in model._targets if x is not None\n-        }\n-        inputs_dict.update(targets_dict)\n-    outputs_dict = {\n-        name: x for name, x in zip(model.output_names, model.outputs)\n-    }\n-    metrics = saving_utils.extract_model_metrics(model)\n-\n-    # Add metric variables to the `LOCAL_VARIABLES` collection. Metric variables\n-    # are by default not added to any collections. We are doing this here, so\n-    # that metric variables get initialized.\n-    local_vars = set(\n-        tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.LOCAL_VARIABLES)\n-    )\n-    vars_to_add = set()\n-    if metrics is not None:\n-        for key, value in metrics.items():\n-            if isinstance(value, metrics_lib.Metric):\n-                vars_to_add.update(value.variables)\n-                # Convert Metric instances to (value_tensor, update_op) tuple.\n-                metrics[key] = (value.result(), value.updates[0])\n-    # Remove variables that are in the local variables collection already.\n-    vars_to_add = vars_to_add.difference(local_vars)\n-    for v in vars_to_add:\n-        tf.compat.v1.add_to_collection(\n-            tf.compat.v1.GraphKeys.LOCAL_VARIABLES, v\n-        )\n-\n-    export_outputs = model_utils.export_outputs_for_mode(\n-        mode,\n-        predictions=outputs_dict,\n-        loss=model.total_loss if model.optimizer else None,\n-        metrics=metrics,\n-    )\n-    return model_utils.build_all_signature_defs(\n-        inputs_dict,\n-        export_outputs=export_outputs,\n-        serving_only=(mode == mode_keys.ModeKeys.PREDICT),\n-    )\n-\n-\n-def _assert_same_non_optimizer_objects(model, model_graph, clone, clone_graph):\n-    \"\"\"Asserts model and clone contain the same trackable objects.\"\"\"\n-\n-    # TODO(fchollet, kathywu): make sure this works in eager mode.\n-    return True\n-\n-\n-@keras_export(v1=[\"keras.experimental.load_from_saved_model\"])\n-def load_from_saved_model(saved_model_path, custom_objects=None):\n-    \"\"\"Loads a keras Model from a SavedModel created by `export_saved_model()`.\n-\n-    This function reinstantiates model state by:\n-    1) loading model topology from json (this will eventually come\n-       from metagraph).\n-    2) loading model weights from checkpoint.\n-\n-    Example:\n-\n-    ```python\n-    import tensorflow as tf\n-\n-    # Create a tf.keras model.\n-    model = tf.keras.Sequential()\n-    model.add(tf.keras.layers.Dense(1, input_shape=[10]))\n-    model.summary()\n-\n-    # Save the tf.keras model in the SavedModel format.\n-    path = '/tmp/simple_keras_model'\n-    tf.compat.v1.keras.experimental.export_saved_model(model, path)\n-\n-    # Load the saved keras model back.\n-    new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)\n-    new_model.summary()\n-    ```\n-\n-    Args:\n-      saved_model_path: a string specifying the path to an existing SavedModel.\n-      custom_objects: Optional dictionary mapping names\n-          (strings) to custom classes or functions to be\n-          considered during deserialization.\n-\n-    Returns:\n-      a keras.Model instance.\n-    \"\"\"\n-    warnings.warn(\n-        \"`tf.keras.experimental.load_from_saved_model` is deprecated\"\n-        \"and will be removed in a future version. \"\n-        \"Please switch to `tf.keras.models.load_model`.\",\n-        stacklevel=2,\n-    )\n-    # restore model topology from json string\n-    model_json_filepath = tf.io.gfile.join(\n-        tf.compat.as_bytes(saved_model_path),\n-        tf.compat.as_bytes(tf.saved_model.ASSETS_DIRECTORY),\n-        tf.compat.as_bytes(SAVED_MODEL_FILENAME_JSON),\n-    )\n-    with tf.io.gfile.GFile(model_json_filepath, \"r\") as f:\n-        model_json = f.read()\n-    model = model_config.model_from_json(\n-        model_json, custom_objects=custom_objects\n-    )\n-\n-    # restore model weights\n-    checkpoint_prefix = tf.io.gfile.join(\n-        tf.compat.as_text(saved_model_path),\n-        tf.compat.as_text(tf.saved_model.VARIABLES_DIRECTORY),\n-        tf.compat.as_text(tf.saved_model.VARIABLES_FILENAME),\n-    )\n-    model.load_weights(checkpoint_prefix)\n-    return model\n-\n-\n-#### Directory / path helpers\n-\n-\n-def _get_or_create_variables_dir(export_dir):\n-    \"\"\"Return variables sub-directory, or create one if it doesn't exist.\"\"\"\n-    variables_dir = _get_variables_dir(export_dir)\n-    tf.io.gfile.makedirs(variables_dir)\n-    return variables_dir\n-\n-\n-def _get_variables_dir(export_dir):\n-    \"\"\"Return variables sub-directory in the SavedModel.\"\"\"\n-    return tf.io.gfile.join(\n-        tf.compat.as_text(export_dir),\n-        tf.compat.as_text(tf.saved_model.VARIABLES_DIRECTORY),\n-    )\n-\n-\n-def _get_variables_path(export_dir):\n-    \"\"\"Return the variables path, used as the prefix for checkpoint files.\"\"\"\n-    return tf.io.gfile.join(\n-        tf.compat.as_text(_get_variables_dir(export_dir)),\n-        tf.compat.as_text(tf.saved_model.VARIABLES_FILENAME),\n-    )\n-\n-\n-def _get_or_create_assets_dir(export_dir):\n-    \"\"\"Return assets sub-directory, or create one if it doesn't exist.\"\"\"\n-    assets_destination_dir = _get_assets_dir(export_dir)\n-\n-    tf.io.gfile.makedirs(assets_destination_dir)\n-\n-    return assets_destination_dir\n-\n-\n-def _get_assets_dir(export_dir):\n-    \"\"\"Return path to asset directory in the SavedModel.\"\"\"\n-    return tf.io.gfile.join(\n-        tf.compat.as_text(export_dir),\n-        tf.compat.as_text(tf.saved_model.ASSETS_DIRECTORY),\n-    )\n\n@@ -1,612 +0,0 @@\n-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\n-\"\"\"Tests for saving/loading function for keras Model.\"\"\"\n-\n-import os\n-import shutil\n-\n-import numpy as np\n-import tensorflow.compat.v2 as tf\n-from absl.testing import parameterized\n-\n-import keras\n-from keras.engine import training as model_lib\n-from keras.optimizers import optimizer_v1\n-from keras.optimizers.optimizer_v2 import adadelta\n-from keras.optimizers.optimizer_v2 import rmsprop\n-from keras.saving import saved_model_experimental as keras_saved_model\n-from keras.saving import utils_v1 as model_utils\n-from keras.utils import control_flow_util\n-from keras.utils import mode_keys\n-\n-\n-class TestModelSavingandLoading(parameterized.TestCase, tf.test.TestCase):\n-    def _save_model_dir(self, dirname=\"saved_model\"):\n-        temp_dir = self.get_temp_dir()\n-        self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n-        return os.path.join(temp_dir, dirname)\n-\n-    def test_saving_sequential_model(self):\n-        with self.cached_session():\n-            model = keras.models.Sequential()\n-            model.add(keras.layers.Dense(2, input_shape=(3,)))\n-            model.add(keras.layers.RepeatVector(3))\n-            model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n-            model.compile(\n-                loss=keras.losses.MSE,\n-                optimizer=rmsprop.RMSprop(lr=0.0001),\n-                metrics=[keras.metrics.categorical_accuracy],\n-                sample_weight_mode=\"temporal\",\n-            )\n-            x = np.random.random((1, 3))\n-            y = np.random.random((1, 3, 3))\n-            model.train_on_batch(x, y)\n-\n-            ref_y = model.predict(x)\n-\n-            saved_model_dir = self._save_model_dir()\n-            keras_saved_model.export_saved_model(model, saved_model_dir)\n-\n-            loaded_model = keras_saved_model.load_from_saved_model(\n-                saved_model_dir\n-            )\n-            y = loaded_model.predict(x)\n-            self.assertAllClose(ref_y, y, atol=1e-05)\n-\n-    def test_saving_sequential_model_without_compile(self):\n-        with self.cached_session():\n-            model = keras.models.Sequential()\n-            model.add(keras.layers.Dense(2, input_shape=(3,)))\n-            model.add(keras.layers.RepeatVector(3))\n-            model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n-\n-            x = np.random.random((1, 3))\n-            ref_y = model.predict(x)\n-\n-            saved_model_dir = self._save_model_dir()\n-            keras_saved_model.export_saved_model(model, saved_model_dir)\n-            loaded_model = keras_saved_model.load_from_saved_model(\n-                saved_model_dir\n-            )\n-\n-            y = loaded_model.predict(x)\n-            self.assertAllClose(ref_y, y, atol=1e-05)\n-\n-    def test_saving_functional_model(self):\n-        with self.cached_session():\n-            inputs = keras.layers.Input(shape=(3,))\n-            x = keras.layers.Dense(2)(inputs)\n-            output = keras.layers.Dense(3)(x)\n-\n-            model = keras.models.Model(inputs, output)\n-            model.compile(\n-                loss=keras.losses.MSE,\n-                optimizer=rmsprop.RMSprop(lr=0.0001),\n-                metrics=[keras.metrics.categorical_accuracy],\n-            )\n-            x = np.random.random((1, 3))\n-            y = np.random.random((1, 3))\n-            model.train_on_batch(x, y)\n-\n-            ref_y = model.predict(x)\n-\n-            saved_model_dir = self._save_model_dir()\n-            keras_saved_model.export_saved_model(model, saved_model_dir)\n-            loaded_model = keras_saved_model.load_from_saved_model(\n-                saved_model_dir\n-            )\n-\n-            y = loaded_model.predict(x)\n-            self.assertAllClose(ref_y, y, atol=1e-05)\n-\n-    def test_saving_functional_model_without_compile(self):\n-        with self.cached_session():\n-            inputs = keras.layers.Input(shape=(3,))\n-            x = keras.layers.Dense(2)(inputs)\n-            output = keras.layers.Dense(3)(x)\n-\n-            model = keras.models.Model(inputs, output)\n-\n-            x = np.random.random((1, 3))\n-            y = np.random.random((1, 3))\n-\n-            ref_y = model.predict(x)\n-\n-            saved_model_dir = self._save_model_dir()\n-            keras_saved_model.export_saved_model(model, saved_model_dir)\n-            loaded_model = keras_saved_model.load_from_saved_model(\n-                saved_model_dir\n-            )\n-\n-            y = loaded_model.predict(x)\n-            self.assertAllClose(ref_y, y, atol=1e-05)\n-\n-    def test_saving_with_tf_optimizer(self):\n-        model = keras.models.Sequential()\n-        model.add(keras.layers.Dense(2, input_shape=(3,)))\n-        model.add(keras.layers.Dense(3))\n-        model.compile(\n-            loss=\"mse\",\n-            optimizer=tf.compat.v1.train.RMSPropOptimizer(0.1),\n-            metrics=[\"acc\"],\n-        )\n-\n-        x = np.random.random((1, 3))\n-        y = np.random.random((1, 3))\n-        model.train_on_batch(x, y)\n-        ref_y = model.predict(x)\n-\n-        saved_model_dir = self._save_model_dir()\n-        keras_saved_model.export_saved_model(model, saved_model_dir)\n-        loaded_model = keras_saved_model.load_from_saved_model(saved_model_dir)\n-        loaded_model.compile(\n-            loss=\"mse\",\n-            optimizer=tf.compat.v1.train.RMSPropOptimizer(0.1),\n-            metrics=[\"acc\"],\n-        )\n-        y = loaded_model.predict(x)\n-        self.assertAllClose(ref_y, y, atol=1e-05)\n-\n-        # test that new updates are the same with both models\n-        x = np.random.random((1, 3))\n-        y = np.random.random((1, 3))\n-\n-        ref_loss = model.train_on_batch(x, y)\n-        loss = loaded_model.train_on_batch(x, y)\n-        self.assertAllClose(ref_loss, loss, atol=1e-05)\n-\n-        ref_y = model.predict(x)\n-        y = loaded_model.predict(x)\n-        self.assertAllClose(ref_y, y, atol=1e-05)\n-\n-        # test saving/loading again\n-        saved_model_dir2 = self._save_model_dir(\"saved_model_2\")\n-        keras_saved_model.export_saved_model(loaded_model, saved_model_dir2)\n-        loaded_model = keras_saved_model.load_from_saved_model(saved_model_dir2)\n-        y = loaded_model.predict(x)\n-        self.assertAllClose(ref_y, y, atol=1e-05)\n-\n-    def test_saving_subclassed_model_raise_error(self):\n-        # For now, saving subclassed model should raise an error. It should be\n-        # avoided later with loading from SavedModel.pb.\n-\n-        class SubclassedModel(model_lib.Model):\n-            def __init__(self):\n-                super().__init__()\n-                self.layer1 = keras.layers.Dense(3)\n-                self.layer2 = keras.layers.Dense(1)\n-\n-            def call(self, inp):\n-                return self.layer2(self.layer1(inp))\n-\n-        model = SubclassedModel()\n-\n-        saved_model_dir = self._save_model_dir()\n-        with self.assertRaises(NotImplementedError):\n-            keras_saved_model.export_saved_model(model, saved_model_dir)\n-\n-\n-class LayerWithLearningPhase(keras.engine.base_layer.Layer):\n-    def build(self, input_shape):\n-        self.input_spec = keras.layers.InputSpec(\n-            shape=[None] * len(input_shape)\n-        )\n-        self.built = True\n-\n-    def call(self, x, training=None):\n-        if training is None:\n-            training = keras.backend.learning_phase()\n-        output = control_flow_util.smart_cond(\n-            training, lambda: x * 0, lambda: tf.identity(x)\n-        )\n-        if not tf.executing_eagerly():\n-            output._uses_learning_phase = True\n-        return output\n-\n-    def compute_output_shape(self, input_shape):\n-        return input_shape\n-\n-\n-def functional_model(uses_learning_phase=True):\n-    inputs = keras.layers.Input(shape=(3,))\n-    x = keras.layers.Dense(2)(inputs)\n-    x = keras.layers.Dense(3)(x)\n-    if uses_learning_phase:\n-        x = LayerWithLearningPhase()(x)\n-    return keras.models.Model(inputs, x)\n-\n-\n-def sequential_model(uses_learning_phase=True):\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Dense(2, input_shape=(3,)))\n-    model.add(keras.layers.Dense(3))\n-    if uses_learning_phase:\n-        model.add(LayerWithLearningPhase())\n-    return model\n-\n-\n-def sequential_model_without_input_shape(uses_learning_phase=True):\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Dense(2))\n-    model.add(keras.layers.Dense(3))\n-    if uses_learning_phase:\n-        model.add(LayerWithLearningPhase())\n-    return model\n-\n-\n-class Subclassed(keras.models.Model):\n-    def __init__(self):\n-        super().__init__()\n-        self.dense1 = keras.layers.Dense(2)\n-        self.dense2 = keras.layers.Dense(3)\n-\n-    def call(self, inputs):\n-        x = self.dense1(inputs)\n-        x = self.dense2(x)\n-        return x\n-\n-\n-def subclassed_model():\n-    return Subclassed()\n-\n-\n-def load_model(sess, path, mode):\n-    tags = model_utils.EXPORT_TAG_MAP[mode]\n-    sig_def_key = model_utils.SIGNATURE_KEY_MAP[mode]\n-\n-    meta_graph_def = tf.compat.v1.saved_model.load(sess, tags, path)\n-    inputs = {\n-        k: sess.graph.get_tensor_by_name(v.name)\n-        for k, v in meta_graph_def.signature_def[sig_def_key].inputs.items()\n-    }\n-    outputs = {\n-        k: sess.graph.get_tensor_by_name(v.name)\n-        for k, v in meta_graph_def.signature_def[sig_def_key].outputs.items()\n-    }\n-    return inputs, outputs, meta_graph_def\n-\n-\n-def get_train_op(meta_graph_def):\n-    graph = tf.compat.v1.get_default_graph()\n-    signature_def = meta_graph_def.signature_def[\"__saved_model_train_op\"]\n-    op_name = signature_def.outputs[\"__saved_model_train_op\"].name\n-    return graph.as_graph_element(op_name)\n-\n-\n-class TestModelSavedModelExport(tf.test.TestCase, parameterized.TestCase):\n-    def _save_model_dir(self, dirname=\"saved_model\"):\n-        temp_dir = self.get_temp_dir()\n-        self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n-        return os.path.join(temp_dir, dirname)\n-\n-    @parameterized.parameters(\n-        {\n-            \"model_builder\": functional_model,\n-            \"uses_learning_phase\": True,\n-            \"optimizer_cls\": adadelta.Adadelta,\n-            \"train_before_export\": True,\n-        },\n-        {\n-            \"model_builder\": functional_model,\n-            \"uses_learning_phase\": True,\n-            \"optimizer_cls\": tf.compat.v1.train.AdadeltaOptimizer,\n-            \"train_before_export\": False,\n-        },\n-        {\n-            \"model_builder\": functional_model,\n-            \"uses_learning_phase\": False,\n-            \"optimizer_cls\": None,\n-            \"train_before_export\": False,\n-        },\n-        {\n-            \"model_builder\": sequential_model,\n-            \"uses_learning_phase\": True,\n-            \"optimizer_cls\": tf.compat.v1.train.AdadeltaOptimizer,\n-            \"train_before_export\": True,\n-        },\n-        {\n-            \"model_builder\": sequential_model,\n-            \"uses_learning_phase\": True,\n-            \"optimizer_cls\": adadelta.Adadelta,\n-            \"train_before_export\": False,\n-        },\n-        {\n-            \"model_builder\": sequential_model,\n-            \"uses_learning_phase\": False,\n-            \"optimizer_cls\": None,\n-            \"train_before_export\": False,\n-        },\n-        {\n-            \"model_builder\": sequential_model_without_input_shape,\n-            \"uses_learning_phase\": True,\n-            \"optimizer_cls\": tf.compat.v1.train.AdadeltaOptimizer,\n-            \"train_before_export\": False,\n-        },\n-    )\n-    def testSaveAndLoadSavedModelExport(\n-        self,\n-        model_builder,\n-        uses_learning_phase,\n-        optimizer_cls,\n-        train_before_export,\n-    ):\n-        optimizer = None if optimizer_cls is None else optimizer_cls()\n-\n-        saved_model_dir = self._save_model_dir()\n-\n-        np.random.seed(130)\n-        input_arr = np.random.random((1, 3))\n-        target_arr = np.random.random((1, 3))\n-\n-        model = model_builder(uses_learning_phase)\n-        if optimizer is not None:\n-            model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\"])\n-            if train_before_export:\n-                model.train_on_batch(input_arr, target_arr)\n-\n-            ref_loss, ref_mae = model.evaluate(input_arr, target_arr)\n-\n-        ref_predict = model.predict(input_arr)\n-\n-        # Export SavedModel\n-        keras_saved_model.export_saved_model(model, saved_model_dir)\n-\n-        input_name = model.input_names[0]\n-        output_name = model.output_names[0]\n-        target_name = output_name + \"_target\"\n-\n-        # Load predict graph, and test predictions\n-        with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n-            inputs, outputs, _ = load_model(\n-                sess, saved_model_dir, mode_keys.ModeKeys.PREDICT\n-            )\n-\n-            predictions = sess.run(\n-                outputs[output_name], {inputs[input_name]: input_arr}\n-            )\n-            self.assertAllClose(ref_predict, predictions, atol=1e-05)\n-\n-        if optimizer:\n-            # Load eval graph, and test predictions, loss and metric values\n-            with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n-                inputs, outputs, _ = load_model(\n-                    sess, saved_model_dir, mode_keys.ModeKeys.TEST\n-                )\n-\n-                # First obtain the loss and predictions, and run the metric\n-                # update op by feeding in the inputs and targets.\n-                metrics_name = (\n-                    \"mae\"\n-                    if tf.__internal__.tf2.enabled()\n-                    else \"mean_absolute_error\"\n-                )\n-                metrics_update_op_key = \"metrics/\" + metrics_name + \"/update_op\"\n-                metrics_value_op_key = \"metrics/\" + metrics_name + \"/value\"\n-\n-                loss, predictions, _ = sess.run(\n-                    (\n-                        outputs[\"loss\"],\n-                        outputs[\"predictions/\" + output_name],\n-                        outputs[metrics_update_op_key],\n-                    ),\n-                    {\n-                        inputs[input_name]: input_arr,\n-                        inputs[target_name]: target_arr,\n-                    },\n-                )\n-\n-                # The metric value should be run after the update op, to ensure\n-                # that it reflects the correct value.\n-                metric_value = sess.run(outputs[metrics_value_op_key])\n-\n-                self.assertEqual(\n-                    int(train_before_export),\n-                    sess.run(tf.compat.v1.train.get_global_step()),\n-                )\n-                self.assertAllClose(ref_loss, loss, atol=1e-05)\n-                self.assertAllClose(ref_mae, metric_value, atol=1e-05)\n-                self.assertAllClose(ref_predict, predictions, atol=1e-05)\n-\n-            # Load train graph, and check for the train op, and prediction\n-            # values\n-            with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n-                inputs, outputs, meta_graph_def = load_model(\n-                    sess, saved_model_dir, mode_keys.ModeKeys.TRAIN\n-                )\n-                self.assertEqual(\n-                    int(train_before_export),\n-                    sess.run(tf.compat.v1.train.get_global_step()),\n-                )\n-                self.assertIn(\"loss\", outputs)\n-                self.assertIn(metrics_update_op_key, outputs)\n-                self.assertIn(metrics_value_op_key, outputs)\n-                self.assertIn(\"predictions/\" + output_name, outputs)\n-\n-                # Train for a step\n-                train_op = get_train_op(meta_graph_def)\n-                train_outputs, _ = sess.run(\n-                    [outputs, train_op],\n-                    {\n-                        inputs[input_name]: input_arr,\n-                        inputs[target_name]: target_arr,\n-                    },\n-                )\n-                self.assertEqual(\n-                    int(train_before_export) + 1,\n-                    sess.run(tf.compat.v1.train.get_global_step()),\n-                )\n-\n-                if uses_learning_phase:\n-                    self.assertAllClose(\n-                        [[0, 0, 0]],\n-                        train_outputs[\"predictions/\" + output_name],\n-                        atol=1e-05,\n-                    )\n-                else:\n-                    self.assertNotAllClose(\n-                        [[0, 0, 0]],\n-                        train_outputs[\"predictions/\" + output_name],\n-                        atol=1e-05,\n-                    )\n-\n-    def testSaveAndLoadSavedModelWithCustomObject(self):\n-        saved_model_dir = self._save_model_dir()\n-        with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n-\n-            def relu6(x):\n-                return keras.backend.relu(x, max_value=6)\n-\n-            inputs = keras.layers.Input(shape=(1,))\n-            outputs = keras.layers.Activation(relu6)(inputs)\n-            model = keras.models.Model(inputs, outputs)\n-            keras_saved_model.export_saved_model(\n-                model, saved_model_dir, custom_objects={\"relu6\": relu6}\n-            )\n-        with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n-            inputs, outputs, _ = load_model(\n-                sess, saved_model_dir, mode_keys.ModeKeys.PREDICT\n-            )\n-            input_name = model.input_names[0]\n-            output_name = model.output_names[0]\n-            predictions = sess.run(\n-                outputs[output_name], {inputs[input_name]: [[7], [-3], [4]]}\n-            )\n-            self.assertAllEqual([[6], [0], [4]], predictions)\n-\n-    def testAssertModelCloneSameObjectsIgnoreOptimizer(self):\n-        input_arr = np.random.random((1, 3))\n-        target_arr = np.random.random((1, 3))\n-\n-        model_graph = tf.Graph()\n-        clone_graph = tf.Graph()\n-\n-        # Create two models with the same layers but different optimizers.\n-        with tf.compat.v1.Session(graph=model_graph):\n-            inputs = keras.layers.Input(shape=(3,))\n-            x = keras.layers.Dense(2)(inputs)\n-            x = keras.layers.Dense(3)(x)\n-            model = keras.models.Model(inputs, x)\n-\n-            model.compile(\n-                loss=\"mse\", optimizer=tf.compat.v1.train.AdadeltaOptimizer()\n-            )\n-            model.train_on_batch(input_arr, target_arr)\n-\n-        with tf.compat.v1.Session(graph=clone_graph):\n-            inputs = keras.layers.Input(shape=(3,))\n-            x = keras.layers.Dense(2)(inputs)\n-            x = keras.layers.Dense(3)(x)\n-            clone = keras.models.Model(inputs, x)\n-            clone.compile(loss=\"mse\", optimizer=optimizer_v1.RMSprop(lr=0.0001))\n-            clone.train_on_batch(input_arr, target_arr)\n-\n-        keras_saved_model._assert_same_non_optimizer_objects(\n-            model, model_graph, clone, clone_graph\n-        )\n-\n-    def testAssertModelCloneSameObjectsThrowError(self):\n-        input_arr = np.random.random((1, 3))\n-        target_arr = np.random.random((1, 3))\n-\n-        model_graph = tf.Graph()\n-        clone_graph = tf.Graph()\n-\n-        # Create two models with the same layers but different optimizers.\n-        with tf.compat.v1.Session(graph=model_graph):\n-            inputs = keras.layers.Input(shape=(3,))\n-            x = keras.layers.Dense(2)(inputs)\n-            x = keras.layers.Dense(3)(x)\n-            model = keras.models.Model(inputs, x)\n-\n-            model.compile(\n-                loss=\"mse\", optimizer=tf.compat.v1.train.AdadeltaOptimizer()\n-            )\n-            model.train_on_batch(input_arr, target_arr)\n-\n-        with tf.compat.v1.Session(graph=clone_graph):\n-            inputs = keras.layers.Input(shape=(3,))\n-            x = keras.layers.Dense(2)(inputs)\n-            x = keras.layers.Dense(4)(x)\n-            x = keras.layers.Dense(3)(x)\n-            clone = keras.models.Model(inputs, x)\n-            clone.compile(loss=\"mse\", optimizer=optimizer_v1.RMSprop(lr=0.0001))\n-            clone.train_on_batch(input_arr, target_arr)\n-\n-    def testSaveSequentialModelWithoutInputShapes(self):\n-        model = sequential_model_without_input_shape(True)\n-        # A Sequential model that hasn't been built should raise an error.\n-        with self.assertRaisesRegex(\n-            ValueError, \"Weights for sequential model have not yet been created\"\n-        ):\n-            keras_saved_model.export_saved_model(model, \"\")\n-\n-        # Even with input_signature, the model's weights has not been created.\n-        with self.assertRaisesRegex(\n-            ValueError, \"Weights for sequential model have not yet been created\"\n-        ):\n-            saved_model_dir = self._save_model_dir()\n-            keras_saved_model.export_saved_model(\n-                model,\n-                saved_model_dir,\n-                input_signature=tf.TensorSpec(\n-                    shape=(10, 11, 12, 13, 14),\n-                    dtype=tf.float32,\n-                    name=\"spec_input\",\n-                ),\n-            )\n-\n-    @parameterized.parameters(\n-        {\n-            \"model_builder\": sequential_model_without_input_shape,\n-            \"input_signature\": [\n-                tf.TensorSpec(shape=[None, 3], dtype=tf.float32)\n-            ],\n-        },\n-        {\n-            \"model_builder\": subclassed_model,\n-            \"input_signature\": [\n-                tf.TensorSpec(shape=[None, 3], dtype=tf.float32)\n-            ],\n-        },\n-    )\n-    def testServingOnly(self, model_builder, input_signature):\n-        if tf.executing_eagerly():\n-            saved_model_dir = self._save_model_dir()\n-            input_arr = np.random.random((5, 3)).astype(np.float32)\n-            model = model_builder()\n-            ref_predict = model.predict(input_arr)\n-\n-            keras_saved_model.export_saved_model(\n-                model,\n-                saved_model_dir,\n-                serving_only=True,\n-                input_signature=input_signature,\n-            )\n-\n-            # Load predict graph, and test predictions\n-            with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n-                inputs, outputs, _ = load_model(\n-                    sess, saved_model_dir, mode_keys.ModeKeys.PREDICT\n-                )\n-                predictions = sess.run(\n-                    outputs[next(iter(outputs.keys()))],\n-                    {inputs[next(iter(inputs.keys()))]: input_arr},\n-                )\n-                self.assertAllClose(ref_predict, predictions, atol=1e-05)\n-\n-\n-if __name__ == \"__main__\":\n-    tf.test.main()\n\n@@ -1,30 +0,0 @@\n-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-# LINT.IfChange\n-\"\"\"Utils for saving a Keras Model or Estimator to the SavedModel format.\"\"\"\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from keras.saving.utils_v1.export_output import *  # noqa: F403\n-from keras.saving.utils_v1.export_utils import EXPORT_TAG_MAP\n-from keras.saving.utils_v1.export_utils import SIGNATURE_KEY_MAP\n-from keras.saving.utils_v1.export_utils import build_all_signature_defs\n-from keras.saving.utils_v1.export_utils import export_outputs_for_mode\n-from keras.saving.utils_v1.export_utils import get_export_outputs\n-from keras.saving.utils_v1.export_utils import get_temp_export_dir\n-from keras.saving.utils_v1.export_utils import get_timestamped_export_dir\n-\n-# LINT.ThenChange(//tensorflow/python/saved_model/model_utils/__init__.py)\n\n@@ -1,458 +0,0 @@\n-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-# LINT.IfChange\n-\"\"\"Classes for different types of export output.\"\"\"\n-\n-import abc\n-\n-import tensorflow.compat.v2 as tf\n-\n-from keras.saving.utils_v1 import (\n-    signature_def_utils as unexported_signature_utils,\n-)\n-\n-\n-class ExportOutput:\n-    \"\"\"Represents an output of a model that can be served.\n-\n-    These typically correspond to model heads.\n-    \"\"\"\n-\n-    __metaclass__ = abc.ABCMeta\n-\n-    _SEPARATOR_CHAR = \"/\"\n-\n-    @abc.abstractmethod\n-    def as_signature_def(self, receiver_tensors):\n-        \"\"\"Generate a SignatureDef proto for inclusion in a MetaGraphDef.\n-\n-        The SignatureDef will specify outputs as described in this ExportOutput,\n-        and will use the provided receiver_tensors as inputs.\n-\n-        Args:\n-          receiver_tensors: a `Tensor`, or a dict of string to `Tensor`,\n-            specifying input nodes that will be fed.\n-        \"\"\"\n-        pass\n-\n-    def _check_output_key(self, key, error_label):\n-        # For multi-head models, the key can be a tuple.\n-        if isinstance(key, tuple):\n-            key = self._SEPARATOR_CHAR.join(key)\n-\n-        if not isinstance(key, str):\n-            raise ValueError(\n-                f\"{error_label} output key must be a string; got {key}.\"\n-            )\n-        return key\n-\n-    def _wrap_and_check_outputs(\n-        self, outputs, single_output_default_name, error_label=None\n-    ):\n-        \"\"\"Wraps raw tensors as dicts and checks type.\n-\n-        Note that we create a new dict here so that we can overwrite the keys\n-        if necessary.\n-\n-        Args:\n-          outputs: A `Tensor` or a dict of string to `Tensor`.\n-          single_output_default_name: A string key for use in the output dict\n-            if the provided `outputs` is a raw tensor.\n-          error_label: descriptive string for use in error messages. If none,\n-            single_output_default_name will be used.\n-\n-        Returns:\n-          A dict of tensors\n-\n-        Raises:\n-          ValueError: if the outputs dict keys are not strings or tuples of\n-            strings or the values are not Tensors.\n-        \"\"\"\n-        if not isinstance(outputs, dict):\n-            outputs = {single_output_default_name: outputs}\n-\n-        output_dict = {}\n-        for key, value in outputs.items():\n-            error_name = error_label or single_output_default_name\n-            key = self._check_output_key(key, error_name)\n-            if not isinstance(value, tf.Tensor):\n-                raise ValueError(\n-                    f\"{error_name} output value must be a Tensor; got {value}.\"\n-                )\n-\n-            output_dict[key] = value\n-        return output_dict\n-\n-\n-class ClassificationOutput(ExportOutput):\n-    \"\"\"Represents the output of a classification head.\n-\n-    Either classes or scores or both must be set.\n-\n-    The classes `Tensor` must provide string labels, not integer class IDs.\n-\n-    If only classes is set, it is interpreted as providing top-k results in\n-    descending order.\n-\n-    If only scores is set, it is interpreted as providing a score for every\n-    class in order of class ID.\n-\n-    If both classes and scores are set, they are interpreted as zipped, so each\n-    score corresponds to the class at the same index.  Clients should not depend\n-    on the order of the entries.\n-    \"\"\"\n-\n-    def __init__(self, scores=None, classes=None):\n-        \"\"\"Constructor for `ClassificationOutput`.\n-\n-        Args:\n-          scores: A float `Tensor` giving scores (sometimes but not always\n-              interpretable as probabilities) for each class.  May be `None`,\n-              but only if `classes` is set.  Interpretation varies-- see class\n-              doc.\n-          classes: A string `Tensor` giving predicted class labels. May be\n-              `None`, but only if `scores` is set.  Interpretation varies-- see\n-              class doc.\n-\n-        Raises:\n-          ValueError: if neither classes nor scores is set, or one of them is\n-              not a `Tensor` with the correct dtype.\n-        \"\"\"\n-        if scores is not None and not (\n-            isinstance(scores, tf.Tensor) and scores.dtype.is_floating\n-        ):\n-            raise ValueError(\n-                f\"Classification scores must be a float32 Tensor; got {scores}\"\n-            )\n-        if classes is not None and not (\n-            isinstance(classes, tf.Tensor)\n-            and tf.as_dtype(classes.dtype) == tf.string\n-        ):\n-            raise ValueError(\n-                f\"Classification classes must be a string Tensor; got {classes}\"\n-            )\n-        if scores is None and classes is None:\n-            raise ValueError(\n-                \"Cannot create a ClassificationOutput with empty \"\n-                \"arguments. At least one of `scores` and `classes` \"\n-                \"must be defined.\"\n-            )\n-        self._scores = scores\n-        self._classes = classes\n-\n-    @property\n-    def scores(self):\n-        return self._scores\n-\n-    @property\n-    def classes(self):\n-        return self._classes\n-\n-    def as_signature_def(self, receiver_tensors):\n-        if len(receiver_tensors) != 1:\n-            raise ValueError(\n-                \"Classification signatures can only accept a single tensor \"\n-                \"input of type tf.string. Please check to make sure that \"\n-                \"you have structured the serving_input_receiver_fn so that it \"\n-                \"creates a single string placeholder. If your model function \"\n-                \"expects multiple inputs, then use `tf.io.parse_example()` to \"\n-                \"parse the string into multiple \"\n-                f\"tensors.\\n Received: {receiver_tensors}\"\n-            )\n-        ((_, examples),) = receiver_tensors.items()\n-        if tf.as_dtype(examples.dtype) != tf.string:\n-            raise ValueError(\n-                \"Classification signatures can only accept a single tensor \"\n-                \"input of type tf.string. Please check to make sure that you \"\n-                \"have structured the serving_input_receiver_fn so that it \"\n-                \"creates a single string placeholder. If your model function \"\n-                \"expects multiple inputs, then use `tf.io.parse_example()` to \"\n-                \"parse the string into multiple \"\n-                f\"tensors.\\n Received: {receiver_tensors}\"\n-            )\n-        return tf.compat.v1.saved_model.classification_signature_def(\n-            examples, self.classes, self.scores\n-        )\n-\n-\n-class RegressionOutput(ExportOutput):\n-    \"\"\"Represents the output of a regression head.\"\"\"\n-\n-    def __init__(self, value):\n-        \"\"\"Constructor for `RegressionOutput`.\n-\n-        Args:\n-          value: a float `Tensor` giving the predicted values.  Required.\n-\n-        Raises:\n-          ValueError: if the value is not a `Tensor` with dtype tf.float32.\n-        \"\"\"\n-        if not (isinstance(value, tf.Tensor) and value.dtype.is_floating):\n-            raise ValueError(\n-                f\"Regression output value must be a float32 Tensor; got {value}\"\n-            )\n-        self._value = value\n-\n-    @property\n-    def value(self):\n-        return self._value\n-\n-    def as_signature_def(self, receiver_tensors):\n-        if len(receiver_tensors) != 1:\n-            raise ValueError(\n-                \"Regression signatures can only accept a single tensor input \"\n-                \"of type tf.string. Please check to make sure that you have \"\n-                \"structured the serving_input_receiver_fn so that it creates \"\n-                \"a single string placeholder. If your model function expects \"\n-                \"multiple inputs, then use `tf.io.parse_example()` to parse \"\n-                \"the string into multiple \"\n-                f\"tensors.\\n Received: {receiver_tensors}\"\n-            )\n-        ((_, examples),) = receiver_tensors.items()\n-        if tf.as_dtype(examples.dtype) != tf.string:\n-            raise ValueError(\n-                \"Regression signatures can only accept a single tensor input \"\n-                \"of type tf.string. Please check to make sure that you have \"\n-                \"structured the serving_input_receiver_fn so that it creates \"\n-                \"a single string placeholder. If your model function expects \"\n-                \"multiple inputs, then use `tf.io.parse_example()` to parse \"\n-                \"the string into multiple \"\n-                f\"tensors.\\n Received: {receiver_tensors}\"\n-            )\n-        return tf.compat.v1.saved_model.regression_signature_def(\n-            examples, self.value\n-        )\n-\n-\n-class PredictOutput(ExportOutput):\n-    \"\"\"Represents the output of a generic prediction head.\n-\n-    A generic prediction need not be either a classification or a regression.\n-\n-    Named outputs must be provided as a dict from string to `Tensor`,\n-    \"\"\"\n-\n-    _SINGLE_OUTPUT_DEFAULT_NAME = \"output\"\n-\n-    def __init__(self, outputs):\n-        \"\"\"Constructor for PredictOutput.\n-\n-        Args:\n-          outputs: A `Tensor` or a dict of string to `Tensor` representing the\n-            predictions.\n-\n-        Raises:\n-          ValueError: if the outputs is not dict, or any of its keys are not\n-              strings, or any of its values are not `Tensor`s.\n-        \"\"\"\n-\n-        self._outputs = self._wrap_and_check_outputs(\n-            outputs, self._SINGLE_OUTPUT_DEFAULT_NAME, error_label=\"Prediction\"\n-        )\n-\n-    @property\n-    def outputs(self):\n-        return self._outputs\n-\n-    def as_signature_def(self, receiver_tensors):\n-        return tf.compat.v1.saved_model.predict_signature_def(\n-            receiver_tensors, self.outputs\n-        )\n-\n-\n-class _SupervisedOutput(ExportOutput):\n-    \"\"\"Represents the output of a supervised training or eval process.\"\"\"\n-\n-    __metaclass__ = abc.ABCMeta\n-\n-    LOSS_NAME = \"loss\"\n-    PREDICTIONS_NAME = \"predictions\"\n-    METRICS_NAME = \"metrics\"\n-\n-    METRIC_VALUE_SUFFIX = \"value\"\n-    METRIC_UPDATE_SUFFIX = \"update_op\"\n-\n-    _loss = None\n-    _predictions = None\n-    _metrics = None\n-\n-    def __init__(self, loss=None, predictions=None, metrics=None):\n-        \"\"\"Constructor for SupervisedOutput (ie, Train or Eval output).\n-\n-        Args:\n-          loss: dict of Tensors or single Tensor representing calculated loss.\n-          predictions: dict of Tensors or single Tensor representing model\n-            predictions.\n-          metrics: Dict of metric results keyed by name.\n-            The values of the dict can be one of the following:\n-            (1) instance of `Metric` class.\n-            (2) (metric_value, update_op) tuples, or a single tuple.\n-            metric_value must be a Tensor, and update_op must be a Tensor or Op.\n-\n-        Raises:\n-          ValueError: if any of the outputs' dict keys are not strings or tuples\n-            of strings or the values are not Tensors (or Operations in the case\n-            of update_op).\n-        \"\"\"\n-\n-        if loss is not None:\n-            loss_dict = self._wrap_and_check_outputs(loss, self.LOSS_NAME)\n-            self._loss = self._prefix_output_keys(loss_dict, self.LOSS_NAME)\n-        if predictions is not None:\n-            pred_dict = self._wrap_and_check_outputs(\n-                predictions, self.PREDICTIONS_NAME\n-            )\n-            self._predictions = self._prefix_output_keys(\n-                pred_dict, self.PREDICTIONS_NAME\n-            )\n-        if metrics is not None:\n-            self._metrics = self._wrap_and_check_metrics(metrics)\n-\n-    def _prefix_output_keys(self, output_dict, output_name):\n-        \"\"\"Prepend output_name to the output_dict keys if it doesn't exist.\n-\n-        This produces predictable prefixes for the pre-determined outputs\n-        of SupervisedOutput.\n-\n-        Args:\n-          output_dict: dict of string to Tensor, assumed valid.\n-          output_name: prefix string to prepend to existing keys.\n-\n-        Returns:\n-          dict with updated keys and existing values.\n-        \"\"\"\n-\n-        new_outputs = {}\n-        for key, val in output_dict.items():\n-            key = self._prefix_key(key, output_name)\n-            new_outputs[key] = val\n-        return new_outputs\n-\n-    def _prefix_key(self, key, output_name):\n-        if key.find(output_name) != 0:\n-            key = output_name + self._SEPARATOR_CHAR + key\n-        return key\n-\n-    def _wrap_and_check_metrics(self, metrics):\n-        \"\"\"Handle the saving of metrics.\n-\n-        Metrics is either a tuple of (value, update_op), or a dict of such\n-        tuples.  Here, we separate out the tuples and create a dict with names\n-        to tensors.\n-\n-        Args:\n-          metrics: Dict of metric results keyed by name.\n-            The values of the dict can be one of the following:\n-            (1) instance of `Metric` class.\n-            (2) (metric_value, update_op) tuples, or a single tuple.\n-            metric_value must be a Tensor, and update_op must be a Tensor or Op.\n-\n-        Returns:\n-          dict of output_names to tensors\n-\n-        Raises:\n-          ValueError: if the dict key is not a string, or the metric values or\n-            ops are not tensors.\n-        \"\"\"\n-        if not isinstance(metrics, dict):\n-            metrics = {self.METRICS_NAME: metrics}\n-\n-        outputs = {}\n-        for key, value in metrics.items():\n-            if isinstance(value, tuple):\n-                metric_val, metric_op = value\n-            else:  # value is a keras.Metrics object\n-                metric_val = value.result()\n-                assert len(value.updates) == 1  # We expect only one update op.\n-                metric_op = value.updates[0]\n-            key = self._check_output_key(key, self.METRICS_NAME)\n-            key = self._prefix_key(key, self.METRICS_NAME)\n-\n-            val_name = key + self._SEPARATOR_CHAR + self.METRIC_VALUE_SUFFIX\n-            op_name = key + self._SEPARATOR_CHAR + self.METRIC_UPDATE_SUFFIX\n-            if not isinstance(metric_val, tf.Tensor):\n-                raise ValueError(\n-                    f\"{key} output value must be a Tensor; got {metric_val}.\"\n-                )\n-            if not (\n-                tf.is_tensor(metric_op) or isinstance(metric_op, tf.Operation)\n-            ):\n-                raise ValueError(\n-                    f\"{key} update_op must be a \"\n-                    f\"Tensor or Operation; got {metric_op}.\"\n-                )\n-\n-            # We must wrap any ops (or variables) in a Tensor before export, as\n-            # the SignatureDef proto expects tensors only. See b/109740581\n-            metric_op_tensor = metric_op\n-            if not isinstance(metric_op, tf.Tensor):\n-                with tf.control_dependencies([metric_op]):\n-                    metric_op_tensor = tf.constant([], name=\"metric_op_wrapper\")\n-\n-            outputs[val_name] = metric_val\n-            outputs[op_name] = metric_op_tensor\n-\n-        return outputs\n-\n-    @property\n-    def loss(self):\n-        return self._loss\n-\n-    @property\n-    def predictions(self):\n-        return self._predictions\n-\n-    @property\n-    def metrics(self):\n-        return self._metrics\n-\n-    @abc.abstractmethod\n-    def _get_signature_def_fn(self):\n-        \"\"\"Returns a function that produces a SignatureDef given desired\n-        outputs.\"\"\"\n-        pass\n-\n-    def as_signature_def(self, receiver_tensors):\n-        signature_def_fn = self._get_signature_def_fn()\n-        return signature_def_fn(\n-            receiver_tensors, self.loss, self.predictions, self.metrics\n-        )\n-\n-\n-class TrainOutput(_SupervisedOutput):\n-    \"\"\"Represents the output of a supervised training process.\n-\n-    This class generates the appropriate signature def for exporting\n-    training output by type-checking and wrapping loss, predictions, and metrics\n-    values.\n-    \"\"\"\n-\n-    def _get_signature_def_fn(self):\n-        return unexported_signature_utils.supervised_train_signature_def\n-\n-\n-class EvalOutput(_SupervisedOutput):\n-    \"\"\"Represents the output of a supervised eval process.\n-\n-    This class generates the appropriate signature def for exporting\n-    eval output by type-checking and wrapping loss, predictions, and metrics\n-    values.\n-    \"\"\"\n-\n-    def _get_signature_def_fn(self):\n-        return unexported_signature_utils.supervised_eval_signature_def\n-\n-\n-# LINT.ThenChange(//tensorflow/python/saved_model/model_utils/export_output.py)\n\n@@ -1,405 +0,0 @@\n-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-# LINT.IfChange\n-\"\"\"Utilities for creating SavedModels.\"\"\"\n-\n-import collections\n-import os\n-import time\n-\n-import tensorflow.compat.v2 as tf\n-\n-from keras.saving.utils_v1 import export_output as export_output_lib\n-from keras.saving.utils_v1 import mode_keys\n-from keras.saving.utils_v1 import unexported_constants\n-from keras.saving.utils_v1.mode_keys import KerasModeKeys as ModeKeys\n-\n-# isort: off\n-from tensorflow.python.platform import tf_logging as logging\n-\n-# Mapping of the modes to appropriate MetaGraph tags in the SavedModel.\n-EXPORT_TAG_MAP = mode_keys.ModeKeyMap(\n-    **{\n-        ModeKeys.PREDICT: [tf.saved_model.SERVING],\n-        ModeKeys.TRAIN: [tf.saved_model.TRAINING],\n-        ModeKeys.TEST: [unexported_constants.EVAL],\n-    }\n-)\n-\n-# For every exported mode, a SignatureDef map should be created using the\n-# functions `export_outputs_for_mode` and `build_all_signature_defs`. By\n-# default, this map will contain a single Signature that defines the input\n-# tensors and output predictions, losses, and/or metrics (depending on the mode)\n-# The default keys used in the SignatureDef map are defined below.\n-SIGNATURE_KEY_MAP = mode_keys.ModeKeyMap(\n-    **{\n-        ModeKeys.PREDICT: tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY,\n-        ModeKeys.TRAIN: unexported_constants.DEFAULT_TRAIN_SIGNATURE_DEF_KEY,\n-        ModeKeys.TEST: unexported_constants.DEFAULT_EVAL_SIGNATURE_DEF_KEY,\n-    }\n-)\n-\n-# Default names used in the SignatureDef input map, which maps strings to\n-# TensorInfo protos.\n-SINGLE_FEATURE_DEFAULT_NAME = \"feature\"\n-SINGLE_RECEIVER_DEFAULT_NAME = \"input\"\n-SINGLE_LABEL_DEFAULT_NAME = \"label\"\n-\n-### Below utilities are specific to SavedModel exports.\n-\n-\n-def build_all_signature_defs(\n-    receiver_tensors,\n-    export_outputs,\n-    receiver_tensors_alternatives=None,\n-    serving_only=True,\n-):\n-    \"\"\"Build `SignatureDef`s for all export outputs.\n-\n-    Args:\n-      receiver_tensors: a `Tensor`, or a dict of string to `Tensor`, specifying\n-        input nodes where this receiver expects to be fed by default. Typically,\n-        this is a single placeholder expecting serialized `tf.Example` protos.\n-      export_outputs: a dict of ExportOutput instances, each of which has\n-        an as_signature_def instance method that will be called to retrieve\n-        the signature_def for all export output tensors.\n-      receiver_tensors_alternatives: a dict of string to additional\n-        groups of receiver tensors, each of which may be a `Tensor` or a dict of\n-        string to `Tensor`.  These named receiver tensor alternatives generate\n-        additional serving signatures, which may be used to feed inputs at\n-        different points within the input receiver subgraph.  A typical usage is\n-        to allow feeding raw feature `Tensor`s *downstream* of the\n-        tf.io.parse_example() op.  Defaults to None.\n-      serving_only: boolean; if true, resulting signature defs will only include\n-        valid serving signatures. If false, all requested signatures will be\n-        returned.\n-\n-    Returns:\n-      signature_def representing all passed args.\n-\n-    Raises:\n-      ValueError: if export_outputs is not a dict\n-    \"\"\"\n-    if not isinstance(receiver_tensors, dict):\n-        receiver_tensors = {SINGLE_RECEIVER_DEFAULT_NAME: receiver_tensors}\n-    if export_outputs is None or not isinstance(export_outputs, dict):\n-        raise ValueError(\n-            \"`export_outputs` must be a dict. Received \"\n-            f\"{export_outputs} with type \"\n-            f\"{type(export_outputs).__name__}.\"\n-        )\n-\n-    signature_def_map = {}\n-    excluded_signatures = {}\n-    for output_key, export_output in export_outputs.items():\n-        signature_name = f\"{output_key or 'None'}\"\n-        try:\n-            signature = export_output.as_signature_def(receiver_tensors)\n-            signature_def_map[signature_name] = signature\n-        except ValueError as e:\n-            excluded_signatures[signature_name] = str(e)\n-\n-    if receiver_tensors_alternatives:\n-        for (\n-            receiver_name,\n-            receiver_tensors_alt,\n-        ) in receiver_tensors_alternatives.items():\n-            if not isinstance(receiver_tensors_alt, dict):\n-                receiver_tensors_alt = {\n-                    SINGLE_RECEIVER_DEFAULT_NAME: receiver_tensors_alt\n-                }\n-            for output_key, export_output in export_outputs.items():\n-                signature_name = \"{}:{}\".format(\n-                    receiver_name or \"None\", output_key or \"None\"\n-                )\n-                try:\n-                    signature = export_output.as_signature_def(\n-                        receiver_tensors_alt\n-                    )\n-                    signature_def_map[signature_name] = signature\n-                except ValueError as e:\n-                    excluded_signatures[signature_name] = str(e)\n-\n-    _log_signature_report(signature_def_map, excluded_signatures)\n-\n-    # The above calls to export_output_lib.as_signature_def should return only\n-    # valid signatures; if there is a validity problem, they raise a ValueError,\n-    # in which case we exclude that signature from signature_def_map above.\n-    # The is_valid_signature check ensures that the signatures produced are\n-    # valid for serving, and acts as an additional sanity check for export\n-    # signatures produced for serving. We skip this check for training and eval\n-    # signatures, which are not intended for serving.\n-    if serving_only:\n-        signature_def_map = {\n-            k: v\n-            for k, v in signature_def_map.items()\n-            if tf.compat.v1.saved_model.is_valid_signature(v)\n-        }\n-    return signature_def_map\n-\n-\n-_FRIENDLY_METHOD_NAMES = {\n-    tf.saved_model.CLASSIFY_METHOD_NAME: \"Classify\",\n-    tf.saved_model.REGRESS_METHOD_NAME: \"Regress\",\n-    tf.saved_model.PREDICT_METHOD_NAME: \"Predict\",\n-    unexported_constants.SUPERVISED_TRAIN_METHOD_NAME: \"Train\",\n-    unexported_constants.SUPERVISED_EVAL_METHOD_NAME: \"Eval\",\n-}\n-\n-\n-def _log_signature_report(signature_def_map, excluded_signatures):\n-    \"\"\"Log a report of which signatures were produced.\"\"\"\n-    sig_names_by_method_name = collections.defaultdict(list)\n-\n-    # We'll collect whatever method_names are present, but also we want to make\n-    # sure to output a line for each of the three standard methods even if they\n-    # have no signatures.\n-    for method_name in _FRIENDLY_METHOD_NAMES:\n-        sig_names_by_method_name[method_name] = []\n-\n-    for signature_name, sig in signature_def_map.items():\n-        sig_names_by_method_name[sig.method_name].append(signature_name)\n-\n-    # TODO(b/67733540): consider printing the full signatures, not just names\n-    for method_name, sig_names in sig_names_by_method_name.items():\n-        if method_name in _FRIENDLY_METHOD_NAMES:\n-            method_name = _FRIENDLY_METHOD_NAMES[method_name]\n-        logging.info(\n-            \"Signatures INCLUDED in export for {}: {}\".format(\n-                method_name, sig_names if sig_names else \"None\"\n-            )\n-        )\n-\n-    if excluded_signatures:\n-        logging.info(\n-            \"Signatures EXCLUDED from export because they cannot be \"\n-            \"be served via TensorFlow Serving APIs:\"\n-        )\n-        for signature_name, message in excluded_signatures.items():\n-            logging.info(f\"'{signature_name}' : {message}\")\n-\n-    if not signature_def_map:\n-        logging.warning(\"Export includes no signatures!\")\n-    elif (\n-        tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n-        not in signature_def_map\n-    ):\n-        logging.warning(\"Export includes no default signature!\")\n-\n-\n-# When we create a timestamped directory, there is a small chance that the\n-# directory already exists because another process is also creating these\n-# directories. In this case we just wait one second to get a new timestamp and\n-# try again. If this fails several times in a row, then something is seriously\n-# wrong.\n-MAX_DIRECTORY_CREATION_ATTEMPTS = 10\n-\n-\n-def get_timestamped_export_dir(export_dir_base):\n-    \"\"\"Builds a path to a new subdirectory within the base directory.\n-\n-    Each export is written into a new subdirectory named using the\n-    current time.  This guarantees monotonically increasing version\n-    numbers even across multiple runs of the pipeline.\n-    The timestamp used is the number of seconds since epoch UTC.\n-\n-    Args:\n-      export_dir_base: A string containing a directory to write the exported\n-          graph and checkpoints.\n-    Returns:\n-      The full path of the new subdirectory (which is not actually created yet).\n-\n-    Raises:\n-      RuntimeError: if repeated attempts fail to obtain a unique timestamped\n-        directory name.\n-    \"\"\"\n-    attempts = 0\n-    while attempts < MAX_DIRECTORY_CREATION_ATTEMPTS:\n-        timestamp = int(time.time())\n-\n-        result_dir = tf.io.gfile.join(\n-            tf.compat.as_bytes(export_dir_base),\n-            tf.compat.as_bytes(str(timestamp)),\n-        )\n-        if not tf.compat.v1.gfile.Exists(result_dir):\n-            # Collisions are still possible (though extremely unlikely): this\n-            # directory is not actually created yet, but it will be almost\n-            # instantly on return from this function.\n-            return result_dir\n-        time.sleep(1)\n-        attempts += 1\n-        logging.warning(\n-            \"Directory {} already exists; retrying (attempt {}/{})\".format(\n-                tf.compat.as_str(result_dir),\n-                attempts,\n-                MAX_DIRECTORY_CREATION_ATTEMPTS,\n-            )\n-        )\n-    raise RuntimeError(\n-        \"Failed to obtain a unique export directory name after \"\n-        f\"{MAX_DIRECTORY_CREATION_ATTEMPTS} attempts.\"\n-    )\n-\n-\n-def get_temp_export_dir(timestamped_export_dir):\n-    \"\"\"Builds a directory name based on the argument but starting with 'temp-'.\n-\n-    This relies on the fact that TensorFlow Serving ignores subdirectories of\n-    the base directory that can't be parsed as integers.\n-\n-    Args:\n-      timestamped_export_dir: the name of the eventual export directory, e.g.\n-        /foo/bar/<timestamp>\n-\n-    Returns:\n-      A sister directory prefixed with 'temp-', e.g. /foo/bar/temp-<timestamp>.\n-    \"\"\"\n-    (dirname, basename) = os.path.split(timestamped_export_dir)\n-    if isinstance(basename, bytes):\n-        str_name = basename.decode(\"utf-8\")\n-    else:\n-        str_name = str(basename)\n-    temp_export_dir = tf.io.gfile.join(\n-        tf.compat.as_bytes(dirname),\n-        tf.compat.as_bytes(f\"temp-{str_name}\"),\n-    )\n-    return temp_export_dir\n-\n-\n-def export_outputs_for_mode(\n-    mode, serving_export_outputs=None, predictions=None, loss=None, metrics=None\n-):\n-    \"\"\"Util function for constructing a `ExportOutput` dict given a mode.\n-\n-    The returned dict can be directly passed to `build_all_signature_defs`\n-    helper function as the `export_outputs` argument, used for generating a\n-    SignatureDef map.\n-\n-    Args:\n-      mode: A `ModeKeys` specifying the mode.\n-      serving_export_outputs: Describes the output signatures to be exported to\n-        `SavedModel` and used during serving. Should be a dict or None.\n-      predictions: A dict of Tensors or single Tensor representing model\n-        predictions. This argument is only used if serving_export_outputs is\n-        not set.\n-      loss: A dict of Tensors or single Tensor representing calculated loss.\n-      metrics: A dict of (metric_value, update_op) tuples, or a single tuple.\n-        metric_value must be a Tensor, and update_op must be a Tensor or Op\n-\n-    Returns:\n-      Dictionary mapping the a key to an `tf.estimator.export.ExportOutput`\n-      object The key is the expected SignatureDef key for the mode.\n-\n-    Raises:\n-      ValueError: if an appropriate ExportOutput cannot be found for the mode.\n-    \"\"\"\n-    if mode not in SIGNATURE_KEY_MAP:\n-        raise ValueError(\n-            f\"Export output type not found for `mode`: {mode}. Expected one \"\n-            f\"of: {list(SIGNATURE_KEY_MAP.keys())}.\\n\"\n-            \"One likely error is that V1 Estimator Modekeys were somehow \"\n-            \"passed to this function. Please ensure that you are using the new \"\n-            \"ModeKeys.\"\n-        )\n-    signature_key = SIGNATURE_KEY_MAP[mode]\n-    if mode_keys.is_predict(mode):\n-        return get_export_outputs(serving_export_outputs, predictions)\n-    elif mode_keys.is_train(mode):\n-        return {\n-            signature_key: export_output_lib.TrainOutput(\n-                loss=loss, predictions=predictions, metrics=metrics\n-            )\n-        }\n-    else:\n-        return {\n-            signature_key: export_output_lib.EvalOutput(\n-                loss=loss, predictions=predictions, metrics=metrics\n-            )\n-        }\n-\n-\n-def get_export_outputs(export_outputs, predictions):\n-    \"\"\"Validate export_outputs or create default export_outputs.\n-\n-    Args:\n-      export_outputs: Describes the output signatures to be exported to\n-        `SavedModel` and used during serving. Should be a dict or None.\n-      predictions:  Predictions `Tensor` or dict of `Tensor`.\n-\n-    Returns:\n-      Valid export_outputs dict\n-\n-    Raises:\n-      TypeError: if export_outputs is not a dict or its values are not\n-        ExportOutput instances.\n-    \"\"\"\n-    if export_outputs is None:\n-        default_output = export_output_lib.PredictOutput(predictions)\n-        export_outputs = {\n-            tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY: default_output\n-        }\n-\n-    if not isinstance(export_outputs, dict):\n-        raise TypeError(\n-            f\"`export_outputs` must be dict, received: {export_outputs}.\"\n-        )\n-    for v in export_outputs.values():\n-        if not isinstance(v, export_output_lib.ExportOutput):\n-            raise TypeError(\n-                \"Values in `export_outputs` must be ExportOutput objects, \"\n-                f\"received: {export_outputs}.\"\n-            )\n-\n-    _maybe_add_default_serving_output(export_outputs)\n-\n-    return export_outputs\n-\n-\n-def _maybe_add_default_serving_output(export_outputs):\n-    \"\"\"Add a default serving output to the export_outputs if not present.\n-\n-    Args:\n-      export_outputs: Describes the output signatures to be exported to\n-        `SavedModel` and used during serving. Should be a dict.\n-\n-    Returns:\n-      export_outputs dict with default serving signature added if necessary\n-\n-    Raises:\n-      ValueError: if multiple export_outputs were provided without a default\n-        serving key.\n-    \"\"\"\n-    if len(export_outputs) == 1:\n-        ((key, value),) = export_outputs.items()\n-        if key != tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n-            export_outputs[\n-                tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n-            ] = value\n-    if len(export_outputs) > 1:\n-        if (\n-            tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n-            not in export_outputs\n-        ):\n-            raise ValueError(\n-                \"Multiple `export_outputs` were provided, but none of them are \"\n-                \"specified as the default. Use\"\n-                \"`tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY` to \"\n-                \"specify a default.\"\n-            )\n-\n-    return export_outputs\n-\n-\n-# LINT.ThenChange(//tensorflow/python/saved_model/model_utils/export_utils.py)\n\n@@ -1,110 +0,0 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-# LINT.IfChange\n-\"\"\"Utils for managing different mode strings used by Keras and Estimator models.\n-\"\"\"\n-\n-import collections\n-\n-\n-class KerasModeKeys:\n-    \"\"\"Standard names for model modes.\n-\n-    The following standard keys are defined:\n-\n-    * `TRAIN`: training/fitting mode.\n-    * `TEST`: testing/evaluation mode.\n-    * `PREDICT`: prediction/inference mode.\n-    \"\"\"\n-\n-    TRAIN = \"train\"\n-    TEST = \"test\"\n-    PREDICT = \"predict\"\n-\n-\n-# TODO(kathywu): Remove copy in Estimator after nightlies\n-class EstimatorModeKeys:\n-    \"\"\"Standard names for Estimator model modes.\n-\n-    The following standard keys are defined:\n-\n-    * `TRAIN`: training/fitting mode.\n-    * `EVAL`: testing/evaluation mode.\n-    * `PREDICT`: predication/inference mode.\n-    \"\"\"\n-\n-    TRAIN = \"train\"\n-    EVAL = \"eval\"\n-    PREDICT = \"infer\"\n-\n-\n-def is_predict(mode):\n-    return mode in [KerasModeKeys.PREDICT, EstimatorModeKeys.PREDICT]\n-\n-\n-def is_eval(mode):\n-    return mode in [KerasModeKeys.TEST, EstimatorModeKeys.EVAL]\n-\n-\n-def is_train(mode):\n-    return mode in [KerasModeKeys.TRAIN, EstimatorModeKeys.TRAIN]\n-\n-\n-class ModeKeyMap(collections.abc.Mapping):\n-    \"\"\"Map using ModeKeys as keys.\n-\n-    This class creates an immutable mapping from modes to values. For example,\n-    SavedModel export of Keras and Estimator models use this to map modes to\n-    their corresponding MetaGraph tags/SignatureDef keys.\n-\n-    Since this class uses modes, rather than strings, as keys, both \"predict\"\n-    (Keras's PREDICT ModeKey) and \"infer\" (Estimator's PREDICT ModeKey) map to\n-    the same value.\n-    \"\"\"\n-\n-    def __init__(self, **kwargs):\n-        self._internal_dict = {}\n-        self._keys = []\n-        for key in kwargs:\n-            self._keys.append(key)\n-            dict_key = self._get_internal_key(key)\n-            if dict_key in self._internal_dict:\n-                raise ValueError(\n-                    \"Error creating ModeKeyMap. \"\n-                    f\"Multiple keys/values found for {dict_key} mode.\"\n-                )\n-            self._internal_dict[dict_key] = kwargs[key]\n-\n-    def _get_internal_key(self, key):\n-        \"\"\"Return keys used for the internal dictionary.\"\"\"\n-        if is_train(key):\n-            return KerasModeKeys.TRAIN\n-        if is_eval(key):\n-            return KerasModeKeys.TEST\n-        if is_predict(key):\n-            return KerasModeKeys.PREDICT\n-        raise ValueError(f\"Invalid mode key: {key}.\")\n-\n-    def __getitem__(self, key):\n-        return self._internal_dict[self._get_internal_key(key)]\n-\n-    def __iter__(self):\n-        return iter(self._keys)\n-\n-    def __len__(self):\n-        return len(self._keys)\n-\n-\n-# LINT.ThenChange(//tensorflow/python/saved_model/model_utils/mode_keys.py)\n\n@@ -1,93 +0,0 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"SignatureDef utility functions implementation.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-from keras.saving.utils_v1 import unexported_constants\n-\n-\n-# LINT.IfChange\n-def supervised_train_signature_def(\n-    inputs, loss, predictions=None, metrics=None\n-):\n-    return _supervised_signature_def(\n-        unexported_constants.SUPERVISED_TRAIN_METHOD_NAME,\n-        inputs,\n-        loss=loss,\n-        predictions=predictions,\n-        metrics=metrics,\n-    )\n-\n-\n-def supervised_eval_signature_def(inputs, loss, predictions=None, metrics=None):\n-    return _supervised_signature_def(\n-        unexported_constants.SUPERVISED_EVAL_METHOD_NAME,\n-        inputs,\n-        loss=loss,\n-        predictions=predictions,\n-        metrics=metrics,\n-    )\n-\n-\n-def _supervised_signature_def(\n-    method_name, inputs, loss=None, predictions=None, metrics=None\n-):\n-    \"\"\"Creates a signature for training and eval data.\n-\n-    This function produces signatures that describe the inputs and outputs\n-    of a supervised process, such as training or evaluation, that\n-    results in loss, metrics, and the like. Note that this function only\n-    requires inputs to be not None.\n-\n-    Args:\n-      method_name: Method name of the SignatureDef as a string.\n-      inputs: dict of string to `Tensor`.\n-      loss: dict of string to `Tensor` representing computed loss.\n-      predictions: dict of string to `Tensor` representing the output\n-        predictions.\n-      metrics: dict of string to `Tensor` representing metric ops.\n-\n-    Returns:\n-      A train- or eval-flavored signature_def.\n-\n-    Raises:\n-      ValueError: If inputs or outputs is `None`.\n-    \"\"\"\n-    if inputs is None or not inputs:\n-        raise ValueError(f\"{method_name} `inputs` cannot be None or empty.\")\n-\n-    signature_inputs = {\n-        key: tf.compat.v1.saved_model.build_tensor_info(tensor)\n-        for key, tensor in inputs.items()\n-    }\n-\n-    signature_outputs = {}\n-    for output_set in (loss, predictions, metrics):\n-        if output_set is not None:\n-            sig_out = {\n-                key: tf.compat.v1.saved_model.build_tensor_info(tensor)\n-                for key, tensor in output_set.items()\n-            }\n-            signature_outputs.update(sig_out)\n-\n-    signature_def = tf.compat.v1.saved_model.build_signature_def(\n-        signature_inputs, signature_outputs, method_name\n-    )\n-\n-    return signature_def\n-\n-\n-# LINT.ThenChange(//keras/saving/utils_v1/signature_def_utils.py)\n\n@@ -1,32 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Signature constants for SavedModel save and restore operations.\n-\n-These are the private constants that have not been exported.\n-\"\"\"\n-\n-# LINT.IfChange\n-DEFAULT_TRAIN_SIGNATURE_DEF_KEY = \"train\"\n-\n-DEFAULT_EVAL_SIGNATURE_DEF_KEY = \"eval\"\n-\n-SUPERVISED_TRAIN_METHOD_NAME = \"tensorflow/supervised/training\"\n-\n-SUPERVISED_EVAL_METHOD_NAME = \"tensorflow/supervised/eval\"\n-# LINT.ThenChange(//tensorflow/python/saved_model/signature_constants.py)\n-\n-# LINT.IfChange\n-EVAL = \"eval\"\n-# LINT.ThenChange(//tensorflow/python/saved_model/tag_constants.py)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
