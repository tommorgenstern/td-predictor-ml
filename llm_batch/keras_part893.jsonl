{"custom_id": "keras#81c509799723d815ce0ad59e08d661dd3ce6941f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 202 | Lines Deleted: 44 | Files Changed: 4 | Hunks: 23 | Methods Changed: 30 | Complexity Δ (Sum/Max): 23/16 | Churn Δ: 246 | Churn Cumulative: 1930 | Contributors (this commit): 17 | Commits (past 90d): 17 | Contributors (cumulative): 39 | DMM Complexity: 0.37037037037037035\n\nDIFF:\n@@ -312,6 +312,17 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         self._call_has_training_arg = \"training\" in call_signature_parameters\n         self._call_has_mask_arg = \"mask\" in call_signature_parameters\n \n+        # 1. collect names that should be auto‑propagated\n+        builtin_context_args = {\"training\"}\n+        custom_context_args = set(getattr(self, \"call_context_args\", ()))\n+        self._call_context_args = builtin_context_args | custom_context_args\n+\n+        # 2. remember which of them exist in *this* call signature\n+        self._call_has_context_arg = {\n+            arg: (arg in call_signature_parameters)\n+            for arg in self._call_context_args\n+        }\n+\n         self._supports_masking = not utils.is_default(self.compute_mask)\n         # Whether to automatically convert (+ auto-cast) inputs to `call()`.\n         self._convert_input_args = True\n@@ -835,7 +846,9 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n                     )\n \n         # Caches info about `call()` signature, args, kwargs.\n-        call_spec = CallSpec(self._call_signature, args, kwargs)\n+        call_spec = CallSpec(\n+            self._call_signature, self._call_context_args, args, kwargs\n+        )\n \n         ############################################\n         # 3. Check input spec for 1st positional arg.\n@@ -859,18 +872,10 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         # across nested calls.\n         call_context = self._get_call_context()\n \n-        # This is the value explicitly passed by the user\n-        training = call_spec.user_arguments_dict.get(\"training\", None)\n-        if training is None:\n-            # Wasn't passed explicitly: use context value\n-            training = call_context.training\n-            if training is None:\n-                # Get signature default value\n-                training = call_spec.arguments_dict.get(\"training\", None)\n-        call_context.training = training\n-        if self._call_has_training_arg and training is not None:\n-            # Only populate arg if it has a concrete value\n-            kwargs[\"training\"] = training\n+        for context_arg in self._call_context_args:\n+            self._resolve_and_populate_arg(\n+                context_arg, call_spec, call_context, kwargs\n+            )\n \n         ##############################\n         # 6. Populate mask argument(s)\n@@ -978,6 +983,29 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n     def call(self, *args, **kwargs):\n         raise self._not_implemented_error(self.call)\n \n+    def _resolve_and_populate_arg(\n+        self, arg_name, call_spec, call_context, kwargs\n+    ):\n+        # 1) user explicitly passed it?\n+        if arg_name in call_spec.user_arguments_dict:\n+            value = call_spec.user_arguments_dict[arg_name]\n+        # 2) else: inherited from outer layer call?\n+        elif call_context.get_value(arg_name) is not None:\n+            value = call_context.get_value(arg_name)\n+        # 3) else: default from the call() signature\n+        else:\n+            value = call_spec.arguments_dict.get(arg_name, None)\n+\n+        # stash it for downstream layers\n+        call_context.set_value(arg_name, value)\n+\n+        # only inject it if this layer actually accepts it and it's not None\n+        if (\n+            self._call_has_context_arg.get(arg_name, False)\n+            and value is not None\n+        ):\n+            kwargs[arg_name] = value\n+\n     @traceback_utils.filter_traceback\n     def stateless_call(\n         self,\n@@ -1097,7 +1125,9 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n             return super().compute_output_spec(*args, **kwargs)\n         else:\n             # Use compute_output_shape() to return the right output spec\n-            call_spec = CallSpec(self._call_signature, args, kwargs)\n+            call_spec = CallSpec(\n+                self._call_signature, self._call_context_args, args, kwargs\n+            )\n             shapes_dict = get_shapes_dict(call_spec)\n             shapes_dict = update_shapes_dict_for_target_fn(\n                 self.compute_output_shape,\n@@ -1675,20 +1705,21 @@ def is_backend_tensor_or_symbolic(x, allow_none=False):\n \n \n class CallSpec:\n-    def __init__(self, signature, args, kwargs):\n-        # `training` and `mask` are special kwargs that are always available in\n-        # a layer, if user specifies them in their call without adding to spec,\n-        # we remove them to be able to bind variables. User is not using\n-        # `training` anyway so we can ignore.\n-        # TODO: If necessary use workaround for `mask`\n-        if \"training\" in kwargs and \"training\" not in signature.parameters:\n-            kwargs.pop(\"training\")\n-            bound_args = signature.bind(*args, **kwargs)\n-        else:\n-            bound_args = signature.bind(*args, **kwargs)\n-        self.user_arguments_dict = {\n-            k: v for k, v in bound_args.arguments.items()\n+    def __init__(self, signature, call_context_args, args, kwargs):\n+        # Strip out user-supplied call-context args that this layer’s `call()`\n+        # does not accept (otherwise `signature.bind` would raise).\n+        # This includes built-in args like `training`, and user-defined args.\n+        call_args = {\n+            context_arg: kwargs.pop(context_arg)\n+            for context_arg in call_context_args\n+            if context_arg in kwargs and context_arg not in signature.parameters\n         }\n+\n+        bound_args = signature.bind(*args, **kwargs)\n+\n+        # Combine the two dicts.\n+        self.user_arguments_dict = {**call_args, **bound_args.arguments}\n+\n         bound_args.apply_defaults()\n         arg_dict = {}\n         arg_names = []\n@@ -1850,7 +1881,14 @@ def update_shapes_dict_for_target_fn(\n class CallContext:\n     def __init__(self, entry_layer):\n         self.entry_layer = entry_layer\n-        self.training = None\n+\n+    def get_value(self, arg_name, default=None):\n+        \"\"\"Get the context value for `arg_name`, or `default` if unset.\"\"\"\n+        return getattr(self, arg_name, default)\n+\n+    def set_value(self, arg_name, value):\n+        \"\"\"Set `arg_name` = `value` on this context object.\"\"\"\n+        setattr(self, arg_name, value)\n \n \n def is_shape_tuple(s):\n\n@@ -1544,3 +1544,107 @@ class LayerTest(testing.TestCase):\n         layer = MyDenseLayer(10)\n         output = layer(inputs)\n         self.assertAllEqual(output.shape, (10, 10))\n+\n+    def test_call_context_args_with_custom_layers(self):\n+        class Inner(layers.Layer):\n+            call_context_args = (\"foo_mode\",)\n+\n+            def call(self, x, foo_mode=None):\n+                return x + (1 if foo_mode else 0)\n+\n+        class Outer(layers.Layer):\n+            call_context_args = (\"foo_mode\",)\n+\n+            def __init__(self):\n+                super().__init__()\n+                self.inner = Inner()\n+\n+            def call(self, x):\n+                # Outer doesn’t even need to re‑inject explicitly:\n+                # our base class will propagate foo_mode automatically\n+                return self.inner(x)\n+\n+        layer = Outer()\n+        self.assertEqual(int(layer(np.array(0), foo_mode=True)), 1)\n+        self.assertEqual(int(layer(np.array(0))), 0)\n+\n+    def test_context_args_with_triple_nesting_and_priority(self):\n+        class Inner(layers.Layer):\n+            call_context_args = (\"foo_mode\",)\n+\n+            def call(self, x, foo_mode=None):\n+                return x + (1 if foo_mode else 0)\n+\n+        class Middle(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.inner = Inner()\n+\n+            def call(self, x):\n+                return self.inner(x)\n+\n+        class Outer(layers.Layer):\n+            call_context_args = (\"foo_mode\",)\n+\n+            def __init__(self):\n+                super().__init__()\n+                self.middle = Middle()\n+\n+            def call(self, x):\n+                # Outer explicitly sets foo_mode=False when calling Inner,\n+                # so the value being passed here should be ignored.\n+                return self.middle(x)\n+\n+        layer = Outer()\n+\n+        # The value of foo_mode is set to True in the call to Outer,\n+        # so it should automatically propagate to Inner through Middle.\n+        self.assertEqual(int(layer(np.array(0), foo_mode=True)), 1)\n+        self.assertEqual(int(layer(np.array(0))), 0)\n+\n+    def test_call_context_args_with_func_seq_models_as_layers(self):\n+        class Inner(layers.Layer):\n+            call_context_args = (\"foo_mode\",)\n+\n+            def call(self, x, foo_mode=False):\n+                # If foo_mode=True add 1, otherwise add 0\n+                add_val = ops.where(foo_mode, 1.0, 0.0)\n+                return x + add_val\n+\n+        class Outer(layers.Layer):\n+            call_context_args = (\"foo_mode\",)\n+\n+            def __init__(self):\n+                super().__init__()\n+                self.inner = Inner()\n+\n+            def call(self, x):\n+                # We don’t explicitly pass foo_mode here—Base Layer.__call__\n+                # should inject it into `self.inner`\n+                return self.inner(x)\n+\n+        sample_input = np.array([[1.0], [2.0]])\n+\n+        # Sequential model\n+        seq = models.Sequential([Outer()])\n+\n+        # foo_mode=True -> input + 1\n+        out_true = seq(sample_input, foo_mode=True)\n+        self.assertAllClose(out_true, sample_input + 1.0)\n+\n+        # foo_mode omitted -> foo_mode defaults to False -> no change\n+        out_false = seq(sample_input)\n+        self.assertAllClose(out_false, sample_input)\n+\n+        # Functional model\n+        inp = Input(shape=(1,))\n+        out = Outer()(inp)\n+        model = models.Model(inp, out)\n+\n+        # foo_mode=True -> input + 1\n+        y1 = model(sample_input, foo_mode=True)\n+        self.assertAllClose(y1, sample_input + 1.0)\n+\n+        # foo_mode omitted -> foo_mode defaults to False -> no change\n+        y2 = model(sample_input)\n+        self.assertAllClose(y2, sample_input)\n\n@@ -170,7 +170,7 @@ class Functional(Function, Model):\n             \"Please use another name.\"\n         )\n \n-    def call(self, inputs, training=None, mask=None):\n+    def call(self, inputs, training=None, mask=None, **kwargs):\n         # Add support for training, masking\n         inputs = self._standardize_inputs(inputs)\n         if mask is None:\n@@ -181,7 +181,10 @@ class Functional(Function, Model):\n                 if mask is not None:\n                     backend.set_keras_mask(x, mask)\n         outputs = self._run_through_graph(\n-            inputs, operation_fn=lambda op: operation_fn(op, training=training)\n+            inputs,\n+            operation_fn=lambda op: operation_fn(\n+                op, training=training, **kwargs\n+            ),\n         )\n         return unpack_singleton(outputs)\n \n@@ -624,14 +627,18 @@ def functional_from_config(cls, config, custom_objects=None):\n     )\n \n \n-def operation_fn(operation, training):\n+def operation_fn(operation, **call_context_args):\n+    \"\"\"Wraps each op to inject the call-context args.\"\"\"\n+\n     def call(*args, **kwargs):\n+        # Propagate all registered call-context args\n+        for name, value in call_context_args.items():\n             if (\n-            hasattr(operation, \"_call_has_training_arg\")\n-            and operation._call_has_training_arg\n-            and training is not None\n+                name in getattr(operation, \"_call_context_args\", {})\n+                and value is not None\n             ):\n-            kwargs[\"training\"] = training\n+                kwargs[name] = value\n+\n         return operation(*args, **kwargs)\n \n     return call\n\n@@ -215,9 +215,11 @@ class Sequential(Model):\n         outputs = x\n         self._functional = Functional(inputs=inputs, outputs=outputs)\n \n-    def call(self, inputs, training=None, mask=None):\n+    def call(self, inputs, training=None, mask=None, **kwargs):\n         if self._functional:\n-            return self._functional.call(inputs, training=training, mask=mask)\n+            return self._functional.call(\n+                inputs, training=training, mask=mask, **kwargs\n+            )\n \n         # Fallback: Just apply the layer sequence.\n         # This typically happens if `inputs` is a nested struct.\n@@ -226,12 +228,17 @@ class Sequential(Model):\n             # `outputs` are the outputs of `layer` applied to `inputs`. At the\n             # end of each iteration `inputs` is set to `outputs` to prepare for\n             # the next layer.\n-            kwargs = {}\n+            layer_kwargs = {\n+                k: kwargs[k]\n+                # only inject if this layer’s signature actually has that arg\n+                for k in getattr(layer, \"_call_has_context_arg\", {})\n+                if k in kwargs\n+            }\n             if layer._call_has_mask_arg:\n-                kwargs[\"mask\"] = mask\n+                layer_kwargs[\"mask\"] = mask\n             if layer._call_has_training_arg and training is not None:\n-                kwargs[\"training\"] = training\n-            outputs = layer(inputs, **kwargs)\n+                layer_kwargs[\"training\"] = training\n+            outputs = layer(inputs, **layer_kwargs)\n             inputs = outputs\n \n             mask = tree.map_structure(backend.get_keras_mask, outputs)\n@@ -254,15 +261,17 @@ class Sequential(Model):\n             \"Use `add()` and `pop()` to change the layers in this model.\"\n         )\n \n-    def compute_output_spec(self, inputs, training=None, mask=None):\n+    def compute_output_spec(self, inputs, training=None, mask=None, **kwargs):\n         if self._functional:\n             return self._functional.compute_output_spec(\n-                inputs, training=training, mask=mask\n+                inputs, training=training, mask=mask, **kwargs\n             )\n         # Direct application\n         for layer in self.layers:\n             outputs = layer.compute_output_spec(\n-                inputs, training=training\n+                inputs,\n+                training=training,\n+                **kwargs,\n             )  # Ignore mask\n             inputs = outputs\n         return outputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3fcdbe0d86b811ed6b087a4179e69924a91a2009", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 184 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -415,7 +415,7 @@ def is_remote_path(filepath):\n \n     This function checks if the filepath represents a known remote pattern\n     such as GCS (`/gcs`), CNS (`/cns`), CFS (`/cfs`), HDFS (`/hdfs`), Placer\n-    (`/placer`), or a URL (`.*://`).\n+    (`/placer`), TFHub (`/tfhub`), or a URL (`.*://`).\n \n     Args:\n         filepath (str): The path to be checked.\n@@ -424,7 +424,8 @@ def is_remote_path(filepath):\n         bool: True if the filepath is a recognized remote path, otherwise False\n     \"\"\"\n     if re.match(\n-        r\"^(/cns|/cfs|/gcs|/hdfs|/readahead|/placer|.*://).*$\", str(filepath)\n+        r\"^(/cns|/cfs|/gcs|/hdfs|/readahead|/placer|/tfhub|.*://).*$\",\n+        str(filepath),\n     ):\n         return True\n     return False\n\n@@ -726,6 +726,9 @@ class IsRemotePathTest(test_case.TestCase):\n             file_utils.is_remote_path(\"/placer/prod/scratch/home/some/path\")\n         )\n \n+    def test_tfhub_remote_path(self):\n+        self.assertTrue(file_utils.is_remote_path(\"/tfhub/some/path\"))\n+\n     def test_cfs_remote_path(self):\n         self.assertTrue(file_utils.is_remote_path(\"/cfs/some/path\"))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#55adc5780f2326c4f7f98a2aaa75c1873984a56d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 591 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -945,6 +945,7 @@ class Trainer:\n         \"\"\"\n         if self.compiled and hasattr(self, \"_compile_config\"):\n             return self._compile_config.serialize()\n+        return {}\n \n     def compile_from_config(self, config):\n         \"\"\"Compiles the model with the information given in config.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#525c65bdbdfc9a651d0200be0314eb194f843393", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 0 | Files Changed: 10 | Hunks: 13 | Methods Changed: 9 | Complexity Δ (Sum/Max): 20/6 | Churn Δ: 113 | Churn Cumulative: 7682 | Contributors (this commit): 28 | Commits (past 90d): 48 | Contributors (cumulative): 118 | DMM Complexity: 1.0\n\nDIFF:\n@@ -118,6 +118,7 @@ from keras.src.ops.numpy import add as add\n from keras.src.ops.numpy import all as all\n from keras.src.ops.numpy import amax as amax\n from keras.src.ops.numpy import amin as amin\n+from keras.src.ops.numpy import angle as angle\n from keras.src.ops.numpy import any as any\n from keras.src.ops.numpy import append as append\n from keras.src.ops.numpy import arange as arange\n\n@@ -10,6 +10,7 @@ from keras.src.ops.numpy import add as add\n from keras.src.ops.numpy import all as all\n from keras.src.ops.numpy import amax as amax\n from keras.src.ops.numpy import amin as amin\n+from keras.src.ops.numpy import angle as angle\n from keras.src.ops.numpy import any as any\n from keras.src.ops.numpy import append as append\n from keras.src.ops.numpy import arange as arange\n\n@@ -118,6 +118,7 @@ from keras.src.ops.numpy import add as add\n from keras.src.ops.numpy import all as all\n from keras.src.ops.numpy import amax as amax\n from keras.src.ops.numpy import amin as amin\n+from keras.src.ops.numpy import angle as angle\n from keras.src.ops.numpy import any as any\n from keras.src.ops.numpy import append as append\n from keras.src.ops.numpy import arange as arange\n\n@@ -10,6 +10,7 @@ from keras.src.ops.numpy import add as add\n from keras.src.ops.numpy import all as all\n from keras.src.ops.numpy import amax as amax\n from keras.src.ops.numpy import amin as amin\n+from keras.src.ops.numpy import angle as angle\n from keras.src.ops.numpy import any as any\n from keras.src.ops.numpy import append as append\n from keras.src.ops.numpy import arange as arange\n\n@@ -246,6 +246,16 @@ def all(x, axis=None, keepdims=False):\n     return jnp.all(x, axis=axis, keepdims=keepdims)\n \n \n+def angle(x):\n+    x = convert_to_tensor(x)\n+    if standardize_dtype(x.dtype) == \"int64\":\n+        dtype = config.floatx()\n+    else:\n+        dtype = dtypes.result_type(x.dtype, float)\n+    x = cast(x, dtype)\n+    return jnp.angle(x)\n+\n+\n def any(x, axis=None, keepdims=False):\n     return jnp.any(x, axis=axis, keepdims=keepdims)\n \n\n@@ -138,6 +138,16 @@ def all(x, axis=None, keepdims=False):\n     return np.all(x, axis=axis, keepdims=keepdims)\n \n \n+def angle(x):\n+    x = convert_to_tensor(x)\n+    if standardize_dtype(x.dtype) == \"int64\":\n+        dtype = config.floatx()\n+    else:\n+        dtype = dtypes.result_type(x.dtype, float)\n+    x = x.astype(dtype)\n+    return np.angle(x)\n+\n+\n def any(x, axis=None, keepdims=False):\n     axis = standardize_axis_for_numpy(axis)\n     return np.any(x, axis=axis, keepdims=keepdims)\n\n@@ -728,6 +728,16 @@ def all(x, axis=None, keepdims=False):\n     return tf.reduce_all(x, axis=axis, keepdims=keepdims)\n \n \n+def angle(x):\n+    x = convert_to_tensor(x)\n+    if standardize_dtype(x.dtype) == \"int64\":\n+        dtype = config.floatx()\n+    else:\n+        dtype = dtypes.result_type(x.dtype, float)\n+    x = tf.cast(x, dtype)\n+    return tf.math.angle(x)\n+\n+\n def any(x, axis=None, keepdims=False):\n     x = tf.cast(x, \"bool\")\n     return tf.reduce_any(x, axis=axis, keepdims=keepdims)\n\n@@ -264,6 +264,17 @@ def all(x, axis=None, keepdims=False):\n     return cast(x, \"bool\")\n \n \n+def angle(x):\n+    x = convert_to_tensor(x)\n+    ori_dtype = standardize_dtype(x.dtype)\n+\n+    # torch.angle doesn't support float16 with cuda\n+    if get_device() != \"cpu\" and ori_dtype == \"float16\":\n+        x = cast(x, \"float32\")\n+        return cast(torch.angle(x), \"float16\")\n+    return torch.angle(x)\n+\n+\n def any(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n     if axis is None:\n\n@@ -328,6 +328,41 @@ class Any(Operation):\n         )\n \n \n+class Angle(Operation):\n+    def call(self, x):\n+        return backend.numpy.angle(x)\n+\n+    def compute_output_spec(self, x):\n+        dtype = backend.standardize_dtype(getattr(x, \"dtype\", backend.floatx()))\n+        if dtype == \"int64\":\n+            dtype = backend.floatx()\n+        else:\n+            dtype = dtypes.result_type(dtype, float)\n+        return KerasTensor(x.shape, dtype=dtype)\n+\n+\n+@keras_export([\"keras.ops.angle\", \"keras.ops.numpy.angle\"])\n+def angle(x):\n+    \"\"\"Element-wise angle of a complex tensor.\n+\n+    Arguments:\n+        x: Input tensor. Can be real or complex.\n+\n+    Returns:\n+        Output tensor of same shape as x. containing the angle of each element\n+        (in radians).\n+\n+    Example:\n+    >>> x = keras.ops.convert_to_tensor([[1 + 3j, 2 - 5j], [4 - 3j, 3 + 2j]])\n+    >>> keras.ops.angle(x)\n+    array([[ 1.2490457, -1.19029  ],\n+       [-0.6435011,  0.5880026]], dtype=float32)\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Angle().symbolic_call(x)\n+    return backend.numpy.angle(x)\n+\n+\n @keras_export([\"keras.ops.any\", \"keras.ops.numpy.any\"])\n def any(x, axis=None, keepdims=False):\n     \"\"\"Test whether any array element along a given axis evaluates to `True`.\n\n@@ -1712,6 +1712,10 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         with self.assertRaises(ValueError):\n             knp.argpartition(x, (1, 3))\n \n+    def test_angle(self):\n+        x = KerasTensor((None, 3))\n+        self.assertEqual(knp.angle(x).shape, (None, 3))\n+\n \n class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n     def test_mean(self):\n@@ -2281,6 +2285,10 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         with self.assertRaises(ValueError):\n             knp.argpartition(x, (1, 3))\n \n+    def test_angle(self):\n+        x = KerasTensor((2, 3))\n+        self.assertEqual(knp.angle(x).shape, (2, 3))\n+\n \n class NumpyTwoInputOpsCorrectnessTest(testing.TestCase):\n     def test_add(self):\n@@ -4850,6 +4858,12 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.argpartition(x, 1), np.argpartition(x, 1))\n         self.assertAllClose(knp.Argpartition(1)(x), np.argpartition(x, 1))\n \n+    def test_angle(self):\n+        x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n+        self.assertAllClose(knp.angle(x), np.angle(x))\n+\n+        self.assertAllClose(knp.Angle()(x), np.angle(x))\n+\n \n class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n     def test_ones(self):\n@@ -8742,6 +8756,25 @@ class NumpyDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_angle(self, dtype):\n+        import jax.numpy as jnp\n+\n+        if dtype == \"bfloat16\":\n+            self.skipTest(\"Weirdness with numpy\")\n+\n+        x = knp.ones((1,), dtype=dtype)\n+        x_jax = jnp.ones((1,), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.angle(x_jax).dtype)\n+        if dtype == \"int64\":\n+            expected_dtype = backend.floatx()\n+\n+        self.assertEqual(standardize_dtype(knp.angle(x).dtype), expected_dtype)\n+        self.assertEqual(\n+            standardize_dtype(knp.Angle().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n \n @pytest.mark.skipif(\n     testing.torch_uses_gpu(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d94e91f831a331edca80f2bb726fe478d53f9569", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 17 | Churn Cumulative: 516 | Contributors (this commit): 15 | Commits (past 90d): 7 | Contributors (cumulative): 20 | DMM Complexity: 0.16666666666666666\n\nDIFF:\n@@ -248,11 +248,10 @@ class ModelCheckpoint(Callback):\n             current = logs.get(self.monitor)\n             if current is None:\n                 warnings.warn(\n-                    f\"Can save best model only with {self.monitor} \"\n-                    \"available, skipping.\",\n+                    f\"Can save best model only with {self.monitor} available.\",\n                     stacklevel=2,\n                 )\n-                return False\n+                return True\n             elif (\n                 isinstance(current, np.ndarray) or backend.is_tensor(current)\n             ) and len(current.shape) > 0:\n\n@@ -167,6 +167,7 @@ class ModelCheckpointTest(testing.TestCase):\n                 filepath, monitor=\"unknown\", save_best_only=True\n             )\n         ]\n+        with pytest.warns(UserWarning):\n             model.fit(\n                 x_train,\n                 y_train,\n@@ -176,8 +177,7 @@ class ModelCheckpointTest(testing.TestCase):\n                 epochs=1,\n                 verbose=0,\n             )\n-        # File won't be written.\n-        self.assertFalse(os.path.exists(filepath))\n+        self.assertTrue(os.path.exists(filepath))\n \n         # Case 6\n         with warnings.catch_warnings(record=True) as warning_logs:\n\n@@ -288,7 +288,13 @@ class Functional(Function, Model):\n \n     def _standardize_inputs(self, inputs):\n         raise_exception = False\n-        if isinstance(inputs, dict) and not isinstance(\n+        if (\n+            isinstance(self._inputs_struct, list)\n+            and len(self._inputs_struct) == 1\n+            and ops.is_tensor(inputs)\n+        ):\n+            inputs = [inputs]\n+        elif isinstance(inputs, dict) and not isinstance(\n             self._inputs_struct, dict\n         ):\n             # This is to avoid warning\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#37eacb012dff7bc6ffece2d5992faa4d279ed244", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 67 | Churn Cumulative: 2402 | Contributors (this commit): 20 | Commits (past 90d): 29 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -41,6 +41,38 @@ OPENVINO_DTYPES = {\n     \"string\": ov.Type.string,\n }\n \n+DTYPES_MAX = {\n+    ov.Type.bf16: 3.38953139e38,\n+    ov.Type.f16: np.finfo(np.float16).max,\n+    ov.Type.f32: np.finfo(np.float32).max,\n+    ov.Type.f64: np.finfo(np.float64).max,\n+    ov.Type.u8: np.iinfo(np.uint8).max,\n+    ov.Type.u16: np.iinfo(np.uint16).max,\n+    ov.Type.u32: np.iinfo(np.uint32).max,\n+    ov.Type.u64: np.iinfo(np.uint64).max,\n+    ov.Type.i8: np.iinfo(np.int8).max,\n+    ov.Type.i16: np.iinfo(np.int16).max,\n+    ov.Type.i32: np.iinfo(np.int32).max,\n+    ov.Type.i64: np.iinfo(np.int64).max,\n+    ov.Type.boolean: 1,\n+}\n+\n+DTYPES_MIN = {\n+    ov.Type.bf16: -3.38953139e38,\n+    ov.Type.f16: np.finfo(np.float16).min,\n+    ov.Type.f32: np.finfo(np.float32).min,\n+    ov.Type.f64: np.finfo(np.float64).min,\n+    ov.Type.u8: np.iinfo(np.uint8).min,\n+    ov.Type.u16: np.iinfo(np.uint16).min,\n+    ov.Type.u32: np.iinfo(np.uint32).min,\n+    ov.Type.u64: np.iinfo(np.uint64).min,\n+    ov.Type.i8: np.iinfo(np.int8).min,\n+    ov.Type.i16: np.iinfo(np.int16).min,\n+    ov.Type.i32: np.iinfo(np.int32).min,\n+    ov.Type.i64: np.iinfo(np.int64).min,\n+    ov.Type.boolean: 0,\n+}\n+\n \n def align_operand_types(x1, x2, op_name):\n     x1_type = x1.element_type\n\n@@ -5,6 +5,8 @@ from openvino import Type\n from keras.src.backend import config\n from keras.src.backend.common import dtypes\n from keras.src.backend.common.variables import standardize_dtype\n+from keras.src.backend.openvino.core import DTYPES_MAX\n+from keras.src.backend.openvino.core import DTYPES_MIN\n from keras.src.backend.openvino.core import OPENVINO_DTYPES\n from keras.src.backend.openvino.core import OpenVINOKerasTensor\n from keras.src.backend.openvino.core import (\n@@ -1079,9 +1081,36 @@ def moveaxis(x, source, destination):\n \n \n def nan_to_num(x, nan=0.0, posinf=None, neginf=None):\n-    raise NotImplementedError(\n-        \"`nan_to_num` is not supported with openvino backend\"\n-    )\n+    x = get_ov_output(x)\n+    dtype = x.get_element_type()\n+    if dtype.is_integral():\n+        return OpenVINOKerasTensor(x)\n+    isfloat64 = True if dtype == Type.f64 else False\n+    if isfloat64:  # conversion to f32 due to https://github.com/openvinotoolkit/openvino/issues/30264\n+        x = ov_opset.convert(x, Type.f32).output(0)\n+        dtype = Type.f32\n+    nan_val = ov_opset.constant(nan, dtype).output(0)\n+    posinf_val = ov_opset.constant(\n+        posinf if posinf is not None else DTYPES_MAX[dtype], dtype\n+    ).output(0)\n+    neginf_val = ov_opset.constant(\n+        neginf if neginf is not None else DTYPES_MIN[dtype], dtype\n+    ).output(0)\n+    posinf_mask = ov_opset.is_inf(\n+        x,\n+        {\"detect_positive\": True, \"detect_negative\": False},\n+    ).output(0)\n+    neginf_mask = ov_opset.is_inf(\n+        x,\n+        {\"detect_positive\": False, \"detect_negative\": True},\n+    ).output(0)\n+    nan_mask = ov_opset.is_nan(x).output(0)\n+    x = ov_opset.select(nan_mask, nan_val, x).output(0)\n+    x = ov_opset.select(posinf_mask, posinf_val, x).output(0)\n+    x = ov_opset.select(neginf_mask, neginf_val, x).output(0)\n+    if isfloat64:\n+        x = ov_opset.convert(x, Type.f64).output(0)\n+    return OpenVINOKerasTensor(x)\n \n \n def ndim(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06392a17c8932d341e8fdd95787e148dcc367a05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 119 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,6 +60,9 @@ class LossScaleOptimizer(optimizer.Optimizer):\n         self.inner_optimizer = inner_optimizer\n         self.initial_scale = initial_scale\n         self.dynamic_growth_steps = dynamic_growth_steps\n+        # Disable the inner optimizer's loss scaling, otherwise\n+        # gradients will be scaled twice.\n+        self.inner_optimizer.loss_scale_factor = None\n \n     @tracking.no_automatic_dependency_tracking\n     def build(self, var_list):\n\n@@ -48,6 +48,26 @@ class LossScaleOptimizerTest(testing.TestCase):\n             vars, [[0.5, -1.0, -0.5, 3.0]], rtol=1e-4, atol=1e-4\n         )\n \n+    @parameterized.named_parameters((\"stateless\", True), (\"stateful\", False))\n+    def test_finite_step_with_inner_loss_scale(self, stateless):\n+        self._skip_test_for_stateless(stateless)\n+\n+        # Ensure that the inner loss scale does not interfere with the update.\n+        inner_optimizer = SGD(learning_rate=0.5, loss_scale_factor=100)\n+        optimizer = LossScaleOptimizer(inner_optimizer)\n+        grads = [ops.array([1.0, 6.0, 7.0, 2.0]) * optimizer.initial_scale]\n+        vars = [backend.Variable([1.0, 2.0, 3.0, 4.0])]\n+        if stateless:\n+            optimizer.build(vars)\n+            vars, _ = optimizer.stateless_apply(\n+                optimizer.variables, grads, vars\n+            )\n+        else:\n+            optimizer.apply(grads, vars)\n+        self.assertAllClose(\n+            vars, [[0.5, -1.0, -0.5, 3.0]], rtol=1e-4, atol=1e-4\n+        )\n+\n     @parameterized.named_parameters((\"stateless\", True), (\"stateful\", False))\n     def test_infinite_step(self, stateless):\n         self._skip_test_for_stateless(stateless)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3a3f025e01b0340141c4556f6d5984c90888ae52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 89 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -38,7 +38,7 @@ def pytest_collection_modifyitems(config, items):\n             ]\n \n     requires_trainable_backend = pytest.mark.skipif(\n-        backend() == \"numpy\" or backend() == \"openvino\",\n+        backend() in [\"numpy\", \"openvino\"],\n         reason=\"Trainer not implemented for NumPy and OpenVINO backend.\",\n     )\n     for item in items:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff42a375bb838f3fc74bf46bc901c409f4324d96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 155 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 19 | Methods Changed: 16 | Complexity Δ (Sum/Max): 15/12 | Churn Δ: 174 | Churn Cumulative: 1626 | Contributors (this commit): 11 | Commits (past 90d): 13 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -306,20 +306,20 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         self._losses_override = []\n \n         self._call_signature = inspect.signature(self.call)\n-        call_signature_parameters = [\n+        self.call_signature_parameters = [\n             p.name for p in self._call_signature.parameters.values()\n         ]\n-        self._call_has_training_arg = \"training\" in call_signature_parameters\n-        self._call_has_mask_arg = \"mask\" in call_signature_parameters\n+        self._call_has_training_arg = (\n+            \"training\" in self.call_signature_parameters\n+        )\n+        self._call_has_mask_arg = \"mask\" in self.call_signature_parameters\n \n         # 1. collect names that should be auto‑propagated\n-        builtin_context_args = {\"training\"}\n-        custom_context_args = set(getattr(self, \"call_context_args\", ()))\n-        self._call_context_args = builtin_context_args | custom_context_args\n+        self._call_context_args = {\"training\"}\n \n         # 2. remember which of them exist in *this* call signature\n         self._call_has_context_arg = {\n-            arg: (arg in call_signature_parameters)\n+            arg: (arg in self.call_signature_parameters)\n             for arg in self._call_context_args\n         }\n \n@@ -1697,6 +1697,68 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n                 return rematerialized_activation_call_wrapper\n         return layer_call\n \n+    def _register_call_context_args(self, *names):\n+        \"\"\"Registers call-context args for this layer.\n+\n+        If this layer declares a `call()` method that accepts\n+        one or more of the given args, those args will be\n+        automatically injected into the call signature of this\n+        layer. This layer will also propagate the args to any\n+        nested sublayers that are called from within this layer.\n+\n+        If this layer doesn't declare a `call()` method that\n+        accepts one or more of the given args, these args will\n+        simply be propagated to any nested sublayers without\n+        being injected into the call signature of this layer.\n+        This is useful for propagating custom arguments\n+        from top-level layers/models to sublayers.\n+\n+        Example:\n+        ```\n+        class Inner(layers.Layer):\n+\n+            def __init__(self):\n+                super().__init__()\n+                # Register `foo_mode` as a call-context arg\n+                self._register_call_context_args(\"foo_mode\")\n+\n+            def call(self, x, foo_mode=False):\n+                # If foo_mode=True add 1, otherwise add 0\n+                add_val = ops.where(foo_mode, 1.0, 0.0)\n+                return x + add_val\n+\n+        class Outer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.inner = Inner()\n+\n+            def call(self, x):\n+                # We don't explicitly pass foo_mode here—Base Layer.__call__\n+                # should inject it into `self.inner`\n+                return self.inner(x)\n+\n+        sample_input = np.array([[1.0], [2.0]])\n+\n+        # Sequential model\n+        seq = models.Sequential([Outer()])\n+\n+        # Tell the Sequential model to propagate foo_mode down\n+        # the call-stack\n+        seq.register_call_context_args(\"foo_mode\")\n+\n+        # foo_mode=True -> input + 1\n+        out_true = seq(sample_input, foo_mode=True)\n+        \"\"\"\n+        if self._called:\n+            raise RuntimeError(\n+                \"Cannot add call-context args after the layer has been called.\"\n+            )\n+        self._call_context_args = self._call_context_args | set(names)\n+\n+        self._call_has_context_arg.update(\n+            {arg: (arg in self.call_signature_parameters) for arg in names}\n+        )\n+\n \n def is_backend_tensor_or_symbolic(x, allow_none=False):\n     if allow_none and x is None:\n\n@@ -1547,16 +1547,17 @@ class LayerTest(testing.TestCase):\n \n     def test_call_context_args_with_custom_layers(self):\n         class Inner(layers.Layer):\n-            call_context_args = (\"foo_mode\",)\n+            def __init__(self):\n+                super().__init__()\n+                self._register_call_context_args(\"foo_mode\")\n \n             def call(self, x, foo_mode=None):\n                 return x + (1 if foo_mode else 0)\n \n         class Outer(layers.Layer):\n-            call_context_args = (\"foo_mode\",)\n-\n             def __init__(self):\n                 super().__init__()\n+                self._register_call_context_args(\"foo_mode\")\n                 self.inner = Inner()\n \n             def call(self, x):\n@@ -1568,9 +1569,47 @@ class LayerTest(testing.TestCase):\n         self.assertEqual(int(layer(np.array(0), foo_mode=True)), 1)\n         self.assertEqual(int(layer(np.array(0))), 0)\n \n+    def test_register_call_context_arguments(self):\n+        \"\"\"Validate that registering call-context args works as expected.\"\"\"\n+\n+        class MyLayer(layers.Layer):\n+            def call(self, x):\n+                return x\n+\n+        layer = MyLayer()\n+\n+        layer._register_call_context_args(\"foo_mode\")\n+\n+        self.assertCountEqual(\n+            layer._call_context_args, (\"foo_mode\", \"training\")\n+        )\n+\n+    def test_register_call_context_arguments_after_call(self):\n+        \"\"\"Validate that registering call-context args after the layer has\n+        been called raises an error.\"\"\"\n+\n+        class MyLayer(layers.Layer):\n+            def call(self, x):\n+                return x\n+\n+        layer = MyLayer()\n+        layer(np.array(0))\n+        with self.assertRaisesRegex(\n+            RuntimeError,\n+            \"Cannot add call-context args after the layer has been called.\",\n+        ):\n+            layer._register_call_context_args(\"foo_mode\")\n+\n     def test_context_args_with_triple_nesting_and_priority(self):\n+        \"\"\"Validate that call-context args are propagated correctly\n+        through multiple layers, and that the most specific value is used\n+        when multiple values are passed down the call-stack.\n+        \"\"\"\n+\n         class Inner(layers.Layer):\n-            call_context_args = (\"foo_mode\",)\n+            def __init__(self):\n+                super().__init__()\n+                self._register_call_context_args(\"foo_mode\")\n \n             def call(self, x, foo_mode=None):\n                 return x + (1 if foo_mode else 0)\n@@ -1584,8 +1623,6 @@ class LayerTest(testing.TestCase):\n                 return self.inner(x)\n \n         class Outer(layers.Layer):\n-            call_context_args = (\"foo_mode\",)\n-\n             def __init__(self):\n                 super().__init__()\n                 self.middle = Middle()\n@@ -1596,15 +1633,47 @@ class LayerTest(testing.TestCase):\n                 return self.middle(x)\n \n         layer = Outer()\n+        layer._register_call_context_args(\"foo_mode\")\n \n         # The value of foo_mode is set to True in the call to Outer,\n         # so it should automatically propagate to Inner through Middle.\n         self.assertEqual(int(layer(np.array(0), foo_mode=True)), 1)\n         self.assertEqual(int(layer(np.array(0))), 0)\n \n-    def test_call_context_args_with_func_seq_models_as_layers(self):\n+    def test_context_arg_propagation_without_declaration(self):\n+        \"\"\"Validate that layer does not resolve a propagated arg if it is not\n+        declared as a call-context arg in the layer itself.\"\"\"\n+\n         class Inner(layers.Layer):\n-            call_context_args = (\"foo_mode\",)\n+            def call(self, x, foo_mode=None):\n+                return x + (1 if foo_mode else 0)\n+\n+        class Wrapper(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.inner = Inner()\n+\n+            def call(self, x):\n+                return self.inner(x)\n+\n+        layer = Wrapper()\n+        layer._register_call_context_args(\"foo_mode\")\n+\n+        # The value of foo_mode is set to True in the call to Wrapper,\n+        # However, it is not declared as a call-context arg in Inner,\n+        # so it should not resolve to True inside Inner (and instead\n+        # default to False).\n+        self.assertEqual(int(layer(np.array(0), foo_mode=True)), 0)\n+\n+    def test_call_context_args_with_func_seq_models_as_layers(self):\n+        \"\"\"Validate that call-context args are propagated correctly\n+        through functional and sequential models when used as layers.\n+        \"\"\"\n+\n+        class Inner(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self._register_call_context_args(\"foo_mode\")\n \n             def call(self, x, foo_mode=False):\n                 # If foo_mode=True add 1, otherwise add 0\n@@ -1612,8 +1681,6 @@ class LayerTest(testing.TestCase):\n                 return x + add_val\n \n         class Outer(layers.Layer):\n-            call_context_args = (\"foo_mode\",)\n-\n             def __init__(self):\n                 super().__init__()\n                 self.inner = Inner()\n@@ -1626,7 +1693,10 @@ class LayerTest(testing.TestCase):\n         sample_input = np.array([[1.0], [2.0]])\n \n         # Sequential model\n-        seq = models.Sequential([Outer()])\n+        seq = models.Sequential([layers.Identity(), Outer()])\n+        # Tell the Sequential model to propagate foo_mode down\n+        # the call-stack\n+        seq._register_call_context_args(\"foo_mode\")\n \n         # foo_mode=True -> input + 1\n         out_true = seq(sample_input, foo_mode=True)\n@@ -1638,8 +1708,12 @@ class LayerTest(testing.TestCase):\n \n         # Functional model\n         inp = Input(shape=(1,))\n-        out = Outer()(inp)\n+        out = layers.Identity()(inp)\n+        out = Outer()(out)\n         model = models.Model(inp, out)\n+        # Tell the Functional model to propagate foo_mode down\n+        # the call-stack\n+        model._register_call_context_args(\"foo_mode\")\n \n         # foo_mode=True -> input + 1\n         y1 = model(sample_input, foo_mode=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16ef8fb54576482bca10aa90ca2a7431de90b15b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 23 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -726,7 +726,7 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n     model.compile(\n         optimizer='sgd',\n         loss='binary_crossentropy',\n-        metrics=[keras.metrics.SensitivityAtSpecificity()])\n+        metrics=[keras.metrics.SensitivityAtSpecificity(specificity=0.5)])\n     ```\n     \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c0017df36478328e9ccaeeac535ef7cf75ce6913", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 108 | Lines Deleted: 154 | Files Changed: 15 | Hunks: 36 | Methods Changed: 22 | Complexity Δ (Sum/Max): -5/6 | Churn Δ: 262 | Churn Cumulative: 1151 | Contributors (this commit): 12 | Commits (past 90d): 23 | Contributors (cumulative): 54 | DMM Complexity: 0.07042253521126761\n\nDIFF:\n@@ -15,7 +15,9 @@ class TorchParallelOptimizer(BaseOptimizer):\n \n     @torch_utils.no_grad\n     def _backend_reset_gradient_accumulators(self):\n-        acc_list = [v.value for v in self._accumulated_gradients]\n+        acc_list = [\n+            v.value for v in self._accumulated_gradients if v is not None\n+        ]\n         torch._foreach_mul_(acc_list, 0.0)\n \n     @torch_utils.no_grad\n\n@@ -75,14 +75,11 @@ class Adadelta(optimizer.Optimizer):\n         if self.built:\n             return\n         super().build(var_list)\n-        self._accumulated_grads = []\n-        self._accumulated_delta_vars = []\n-        for var in var_list:\n-            self._accumulated_grads.append(\n-                self.add_variable_from_reference(var, \"accumulated_grad\")\n+        self._accumulated_grads = self.add_optimizer_variables(\n+            var_list, \"accumulated_grad\"\n         )\n-            self._accumulated_delta_vars.append(\n-                self.add_variable_from_reference(var, \"accumulated_delta_var\")\n+        self._accumulated_delta_vars = self.add_optimizer_variables(\n+            var_list, \"accumulated_delta_var\"\n         )\n \n     def update_step(self, grad, variable, learning_rate):\n\n@@ -1,4 +1,3 @@\n-from keras.src import backend\n from keras.src import ops\n from keras.src.api_export import keras_export\n from keras.src.optimizers import optimizer\n@@ -97,16 +96,13 @@ class Adafactor(optimizer.Optimizer):\n         self._c = []\n         self._v = []\n         for var in var_list:\n-            if len(var.shape) < 2:\n-                # Don't factor if variable is of dimension < 2, but we still\n-                # need to create dummy variables as placeholder.\n-                with backend.name_scope(self.name, caller=self):\n-                    self._r.append(\n-                        backend.Variable(0, name=var.name, trainable=False)\n-                    )\n-                    self._c.append(\n-                        backend.Variable(0, name=var.name, trainable=False)\n-                    )\n+            if (\n+                self._overwrite_variable_with_gradient(var)\n+                or len(var.shape) < 2\n+            ):\n+                # Don't factor if variable is of dimension < 2.\n+                self._r.append(None)\n+                self._c.append(None)\n             else:\n                 # Always factor the last 2 dimensions.\n                 r_shape = var.shape[:-1]\n@@ -125,6 +121,10 @@ class Adafactor(optimizer.Optimizer):\n                         name=var.name,\n                     )\n                 )\n+\n+            if self._overwrite_variable_with_gradient(var):\n+                self._v.append(None)\n+            else:\n                 self._v.append(\n                     self.add_variable_from_reference(\n                         reference_variable=var, name=\"velocity\"\n\n@@ -70,16 +70,9 @@ class Adagrad(optimizer.Optimizer):\n         if self.built:\n             return\n         super().build(var_list)\n-        self._accumulators = []\n         initializer = initializers.Constant(self.initial_accumulator_value)\n-        for var in var_list:\n-            self._accumulators.append(\n-                self.add_variable(\n-                    shape=var.shape,\n-                    initializer=initializer,\n-                    dtype=var.dtype,\n-                    name=\"accumulator\",\n-                )\n+        self._accumulators = self.add_optimizer_variables(\n+            var_list, \"accumulator\", initializer=initializer\n         )\n \n     def update_step(self, gradient, variable, learning_rate):\n\n@@ -90,26 +90,12 @@ class Adam(optimizer.Optimizer):\n         if self.built:\n             return\n         super().build(var_list)\n-        self._momentums = []\n-        self._velocities = []\n-        for var in var_list:\n-            self._momentums.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"momentum\"\n-                )\n-            )\n-            self._velocities.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"velocity\"\n-                )\n-            )\n+        self._momentums = self.add_optimizer_variables(var_list, \"momentum\")\n+        self._velocities = self.add_optimizer_variables(var_list, \"velocity\")\n+\n         if self.amsgrad:\n-            self._velocity_hats = []\n-            for var in var_list:\n-                self._velocity_hats.append(\n-                    self.add_variable_from_reference(\n-                        reference_variable=var, name=\"velocity_hat\"\n-                    )\n+            self._velocity_hats = self.add_optimizer_variables(\n+                var_list, \"velocity_hat\"\n             )\n \n     def update_step(self, gradient, variable, learning_rate):\n\n@@ -98,19 +98,8 @@ class Adamax(optimizer.Optimizer):\n         if self.built:\n             return\n         super().build(var_list)\n-        self._m = []\n-        self._u = []\n-        for var in var_list:\n-            self._m.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"momentum\"\n-                )\n-            )\n-            self._u.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"norm\"\n-                )\n-            )\n+        self._m = self.add_optimizer_variables(var_list, \"momentum\")\n+        self._u = self.add_optimizer_variables(var_list, \"norm\")\n \n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n\n@@ -204,21 +204,19 @@ class BaseOptimizer(KerasSaveable):\n     def _track_variable(self, variable):\n         self._tracker.add_to_store(\"variables\", variable)\n \n+    def _overwrite_variable_with_gradient(self, variable):\n+        return getattr(variable, \"overwrite_with_gradient\", False)\n+\n     @tracking.no_automatic_dependency_tracking\n     def build(self, variables):\n         if self.use_ema:\n-            self._model_variables_moving_average = []\n+            self._model_variables_moving_average = self.add_optimizer_variables(\n+                variables, \"average\"\n+            )\n         if self.gradient_accumulation_steps:\n             self._accumulated_gradients = []\n         for i, variable in enumerate(variables):\n             self._trainable_variables_indices[self._var_key(variable)] = i\n-            if self.use_ema:\n-                self._model_variables_moving_average.append(\n-                    self.add_variable_from_reference(\n-                        variable,\n-                        name=\"average\",\n-                    )\n-                )\n             if self.gradient_accumulation_steps:\n                 self._accumulated_gradients.append(\n                     self.add_variable_from_reference(\n@@ -323,6 +321,49 @@ class BaseOptimizer(KerasSaveable):\n             name=name,\n         )\n \n+    def add_optimizer_variables(\n+        self, trainable_variables, name, initializer=\"zeros\"\n+    ):\n+        \"\"\"Add optimizer variables from the list of trainable model variables.\n+\n+        Create an optimizer variable based on the information of the supplied\n+        model variables.  For example, in SGD optimizer momemtum, for each model\n+        variable, a corresponding momemtum variable is created of the same shape\n+        and dtype.\n+\n+        Note that trainable variables with `v.overwrite_with_gradient == True`\n+        will insert `None`, into the output list, since the optimizer variable\n+        will not be used anyways, and could be wasteful.\n+\n+        Args:\n+            trainable_variables: `keras.Variable`, the corresponding model\n+                variable to the optimizer variable to be created.\n+            name: The name prefix of the optimizer variable to be created. The\n+                variable name will follow the pattern\n+                `{variable_name}_{trainable_variable.name}`, e.g.,\n+                `momemtum/dense_1`. Defaults to `None`.\n+            initializer: Initializer object to use to populate the initial\n+                variable value, or string name of a built-in initializer (e.g.\n+                `\"random_normal\"`). If unspecified, defaults to `\"zeros\"`.\n+\n+        Returns:\n+            A list of optimizer variables, in the format of `keras.Variable`s.\n+        \"\"\"\n+        optimizer_variables = []\n+        for variable in trainable_variables:\n+            if not self._overwrite_variable_with_gradient(variable):\n+                optimizer_variables.append(\n+                    self.add_variable_from_reference(\n+                        variable,\n+                        name=name,\n+                        initializer=initializer,\n+                    )\n+                )\n+            else:\n+                optimizer_variables.append(None)\n+\n+        return optimizer_variables\n+\n     def _check_variables_are_known(self, variables):\n         for v in variables:\n             if self._var_key(v) not in self._trainable_variables_indices:\n@@ -544,6 +585,7 @@ class BaseOptimizer(KerasSaveable):\n \n     def _backend_reset_gradient_accumulators(self):\n         for g_acc in self._accumulated_gradients:\n+            if g_acc is not None:\n                 g_acc.assign(ops.zeros(g_acc.shape, dtype=g_acc.dtype))\n \n     def _backend_increment_gradient_accumulators(self, grads, acc_grads):\n@@ -711,8 +753,8 @@ class BaseOptimizer(KerasSaveable):\n         After the update, the processed pairs will be filtered out.\n         \"\"\"\n         # Shortcut for `tf.Variable` because it doesn't have a\n-        # `overwrite_with_gradient` attr\n-        if any(not hasattr(v, \"overwrite_with_gradient\") for v in vars):\n+        # `overwrite_with_gradient` attr.\n+        if not any(self._overwrite_variable_with_gradient(v) for v in vars):\n             return grads, vars\n \n         # Shallow copies\n@@ -722,7 +764,7 @@ class BaseOptimizer(KerasSaveable):\n         # Iterate from right to left for safe popping\n         for i in range(len(filtered_grads) - 1, -1, -1):\n             g, v = filtered_grads[i], filtered_vars[i]\n-            if v.overwrite_with_gradient:\n+            if self._overwrite_variable_with_gradient(v):\n                 if self.gradient_accumulation_steps:\n                     # Utilize a stateless manner for JAX compatibility\n                     steps = self.gradient_accumulation_steps\n@@ -886,6 +928,7 @@ class BaseOptimizer(KerasSaveable):\n             for var, average in zip(\n                 trainable_variables, self._model_variables_moving_average\n             ):\n+                if average is not None:\n                     not_first_step = ops.not_equal(self.iterations, 0)\n                     momentum = (\n                         ops.cast(not_first_step, var.dtype) * self.ema_momentum\n@@ -909,6 +952,7 @@ class BaseOptimizer(KerasSaveable):\n         for var, average_var in zip(\n             trainable_variables, self._model_variables_moving_average\n         ):\n+            if average_var is not None:\n                 var.assign(average_var)\n \n     def finalize_variable_values(self, var_list):\n\n@@ -159,24 +159,13 @@ class Ftrl(optimizer.Optimizer):\n         if self.built:\n             return\n         super().build(var_list)\n-        self._accumulators = []\n-        self._linears = []\n-        for var in var_list:\n-            self._accumulators.append(\n-                self.add_variable(\n-                    shape=var.shape,\n-                    dtype=var.dtype,\n-                    name=\"accumulator\",\n-                    initializer=initializers.Constant(\n+        accumulator_initializer = initializers.Constant(\n             self.initial_accumulator_value,\n-                    ),\n-                )\n-            )\n-            self._linears.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"linear\"\n         )\n+        self._accumulators = self.add_optimizer_variables(\n+            var_list, \"accumulator\", initializer=accumulator_initializer\n         )\n+        self._linears = self.add_optimizer_variables(var_list, \"linear\")\n \n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n\n@@ -82,19 +82,8 @@ class Lamb(optimizer.Optimizer):\n         if self.built:\n             return\n         super().build(var_list)\n-        self._momentums = []\n-        self._velocities = []\n-        for var in var_list:\n-            self._momentums.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"momentum\"\n-                )\n-            )\n-            self._velocities.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"velocity\"\n-                )\n-            )\n+        self._momentums = self.add_optimizer_variables(var_list, \"momentum\")\n+        self._velocities = self.add_optimizer_variables(var_list, \"velocity\")\n \n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n\n@@ -91,13 +91,7 @@ class Lion(optimizer.Optimizer):\n         if self.built:\n             return\n         super().build(var_list)\n-        self._momentums = []\n-        for var in var_list:\n-            self._momentums.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"momentum\"\n-                )\n-            )\n+        self._momentums = self.add_optimizer_variables(var_list, \"momentum\")\n \n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n\n@@ -105,12 +105,6 @@ class LossScaleOptimizer(optimizer.Optimizer):\n             ),\n         )\n \n-    def _overwrite_variable_with_gradient(self, variable):\n-        return (\n-            hasattr(variable, \"overwrite_with_gradient\")\n-            and variable.overwrite_with_gradient\n-        )\n-\n     def _stateless_handle_finite_grads(\n         self, optimizer_variables, grads, trainable_variables\n     ):\n\n@@ -160,9 +160,12 @@ class Muon(optimizer.Optimizer):\n         self.muon_velocities = {}\n \n         for var in var_list:\n-            self.adam_momentums[var.path] = self.add_variable_from_reference(\n+            if not self._overwrite_variable_with_gradient(var):\n+                self.adam_momentums[var.path] = (\n+                    self.add_variable_from_reference(\n                         reference_variable=var, name=\"momentum\"\n                     )\n+                )\n                 if self._should_use_adamw(var):\n                     self.adam_velocities[var.path] = (\n                         self.add_variable_from_reference(\n@@ -237,15 +240,14 @@ class Muon(optimizer.Optimizer):\n         return X\n \n     def zeropower_via_newtonschulz5(self, x, steps: int):\n-        \"\"\"\n-        We apply the Newton-Schulz iteration to compute matrix G.\n-        We select a quintic iteration that maximizes the slope at zero.\n-        This approach helps minimize steps, even if the iteration doesn't fully\n-        converge across the interval.\n-        The result isn't exactly UV^T (from the SVD of G),\n-        but rather an approximation like US'V^T. Despite this approximation,\n-        model performance remains unaffected\n-        compared to using the exact UV^T from the SVD.\n+        \"\"\"We apply the Newton-Schulz iteration to compute matrix G.\n+\n+        We select a quintic iteration that maximizes the slope at zero. This\n+        approach helps minimize steps, even if the iteration doesn't fully\n+        converge across the interval. The result isn't exactly UV^T (from the\n+        SVD of G), but rather an approximation like US'V^T. Despite this\n+        approximation, model performance remains unaffected compared to using\n+        the exact UV^T from the SVD.\n         \"\"\"\n         shape = ops.shape(x)\n         assert len(shape) >= 2\n\n@@ -87,22 +87,10 @@ class Nadam(optimizer.Optimizer):\n         else:\n             dtype = backend.floatx()\n         super().build(var_list)\n-        self._momentums = []\n-        self._velocities = []\n+        self._momentums = self.add_optimizer_variables(var_list, \"momentum\")\n+        self._velocities = self.add_optimizer_variables(var_list, \"velocity\")\n         self._u_product = backend.Variable(1.0, dtype=dtype)\n \n-        for var in var_list:\n-            self._momentums.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"momentum\"\n-                )\n-            )\n-            self._velocities.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"velocity\"\n-                )\n-            )\n-\n     def _backend_update_step(self, grads, trainable_variables, learning_rate):\n         dtype = self._u_product.dtype\n         self.assign(\n\n@@ -94,24 +94,16 @@ class RMSprop(optimizer.Optimizer):\n \n         super().build(var_list)\n \n-        self._velocities = []\n-        for var in var_list:\n-            self._velocities.append(\n-                self.add_variable_from_reference(var, \"velocity\")\n-            )\n+        self._velocities = self.add_optimizer_variables(var_list, \"velocity\")\n \n         self._momentums = []\n         if self.momentum > 0:\n-            for var in var_list:\n-                self._momentums.append(\n-                    self.add_variable_from_reference(var, \"momentum\")\n-                )\n+            self._momentums = self.add_optimizer_variables(var_list, \"momentum\")\n \n         self._average_gradients = []\n         if self.centered:\n-            for var in var_list:\n-                self._average_gradients.append(\n-                    self.add_variable_from_reference(var, \"average_gradient\")\n+            self._average_gradients = self.add_optimizer_variables(\n+                var_list, \"average_gradient\"\n             )\n \n     def update_step(self, gradient, variable, learning_rate):\n\n@@ -90,12 +90,7 @@ class SGD(optimizer.Optimizer):\n         super().build(variables)\n         self.momentums = []\n         if self.momentum != 0:\n-            for variable in variables:\n-                self.momentums.append(\n-                    self.add_variable_from_reference(\n-                        reference_variable=variable, name=\"momentum\"\n-                    )\n-                )\n+            self.momentums = self.add_optimizer_variables(variables, \"momentum\")\n \n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
