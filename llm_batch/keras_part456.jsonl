{"custom_id": "keras#f3cafc77c269f7ecbf80bb4cf4b54e28c153f4e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 841 | Lines Deleted: 682 | Files Changed: 13 | Hunks: 329 | Methods Changed: 157 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1523 | Churn Cumulative: 46367 | Contributors (this commit): 160 | Commits (past 90d): 72 | Contributors (cumulative): 268 | DMM Complexity: 0.4166666666666667\n\nDIFF:\n@@ -74,7 +74,8 @@ def softmax(x, axis=-1):\n \n     **Example 2: usage in a `Dense` layer**\n \n-    >>> layer = tf.keras.layers.Dense(32, activation=tf.keras.activations.softmax)\n+    >>> layer = tf.keras.layers.Dense(32,\n+    ...                               activation=tf.keras.activations.softmax)\n     \"\"\"\n     if x.shape.rank > 1:\n         if isinstance(axis, int):\n@@ -130,12 +131,12 @@ def elu(x, alpha=1.0):\n \n     Args:\n         x: Input tensor.\n-        alpha: A scalar, slope of negative section. `alpha` controls the value to\n-          which an ELU saturates for negative net inputs.\n+        alpha: A scalar, slope of negative section. `alpha` controls the value\n+          to which an ELU saturates for negative net inputs.\n \n     Returns:\n-        The exponential linear unit (ELU) activation function: `x` if `x > 0` and\n-        `alpha * (exp(x) - 1)` if `x < 0`.\n+        The exponential linear unit (ELU) activation function: `x` if `x > 0`\n+          and `alpha * (exp(x) - 1)` if `x < 0`.\n \n \n     Reference:\n@@ -301,10 +302,10 @@ def relu(x, alpha=0.0, max_value=None, threshold=0.0):\n         x: Input `tensor` or `variable`.\n         alpha: A `float` that governs the slope for values lower than the\n           threshold.\n-        max_value: A `float` that sets the saturation threshold (the largest value\n-          the function will return).\n-        threshold: A `float` giving the threshold value of the activation function\n-          below which values will be damped or set to zero.\n+        max_value: A `float` that sets the saturation threshold (the largest\n+          value the function will return).\n+        threshold: A `float` giving the threshold value of the activation\n+          function below which values will be damped or set to zero.\n \n     Returns:\n         A `Tensor` representing the input tensor,\n\n@@ -105,10 +105,11 @@ class _DummyEagerGraph(threading.local):\n     class _WeakReferencableClass:\n         \"\"\"This dummy class is needed for two reasons.\n \n-        - We need something that supports weak references. Basic types like string\n-        and ints don't.\n+        - We need something that supports weak references. Basic types like\n+        string and ints don't.\n         - We need something whose hash and equality are based on object identity\n-        to make sure they are treated as different keys to _GRAPH_LEARNING_PHASES.\n+        to make sure they are treated as different keys to\n+        _GRAPH_LEARNING_PHASES.\n \n         An empty Python class satisfies both of these requirements.\n         \"\"\"\n@@ -168,8 +169,8 @@ def cast_to_floatx(x):\n         x: Numpy array or TensorFlow tensor.\n \n     Returns:\n-        The same array (Numpy array if `x` was a Numpy array, or TensorFlow tensor\n-        if `x` was a tensor), cast to its new type.\n+        The same array (Numpy array if `x` was a Numpy array, or TensorFlow\n+        tensor if `x` was a tensor), cast to its new type.\n \n     Example:\n \n@@ -233,8 +234,8 @@ def clear_session():\n \n     If you are creating many models in a loop, this global state will consume\n     an increasing amount of memory over time, and you may want to clear it.\n-    Calling `clear_session()` releases the global state: this helps avoid clutter\n-    from old models and layers, especially when memory is limited.\n+    Calling `clear_session()` releases the global state: this helps avoid\n+    clutter from old models and layers, especially when memory is limited.\n \n     Example 1: calling `clear_session()` when creating models in a loop\n \n@@ -242,14 +243,16 @@ def clear_session():\n     for _ in range(100):\n       # Without `clear_session()`, each iteration of this loop will\n       # slightly increase the size of the global state managed by Keras\n-      model = tf.keras.Sequential([tf.keras.layers.Dense(10) for _ in range(10)])\n+      model = tf.keras.Sequential([\n+          tf.keras.layers.Dense(10) for _ in range(10)])\n \n     for _ in range(100):\n       # With `clear_session()` called at the beginning,\n       # Keras starts with a blank state at each iteration\n       # and memory consumption is constant over time.\n       tf.keras.backend.clear_session()\n-      model = tf.keras.Sequential([tf.keras.layers.Dense(10) for _ in range(10)])\n+      model = tf.keras.Sequential([\n+          tf.keras.layers.Dense(10) for _ in range(10)])\n     ```\n \n     Example 2: resetting the layer name generation counter\n@@ -268,9 +271,9 @@ def clear_session():\n     dense\n     \"\"\"\n     global _SESSION\n-    global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n-    global _GRAPH_VARIABLES  # pylint: disable=global-variable-not-assigned\n-    global _GRAPH_TF_OPTIMIZERS  # pylint: disable=global-variable-not-assigned\n+    global _GRAPH_LEARNING_PHASES\n+    global _GRAPH_VARIABLES\n+    global _GRAPH_TF_OPTIMIZERS\n     global _GRAPH\n     _GRAPH.graph = None\n     tf.compat.v1.reset_default_graph()\n@@ -283,14 +286,16 @@ def clear_session():\n         _DUMMY_EAGER_GRAPH.learning_phase_is_set = False\n \n         _GRAPH_LEARNING_PHASES = {}\n-        # Create the learning phase placeholder in graph using the default factory\n+        # Create the learning phase placeholder in graph using the default\n+        # factory\n         phase = _default_learning_phase()\n         _internal_set_learning_phase(graph, phase)\n \n         _GRAPH_VARIABLES.pop(graph, None)\n         _GRAPH_TF_OPTIMIZERS.pop(graph, None)\n     if tf.executing_eagerly():\n-        # Clear pending nodes in eager executors, kernel caches and step_containers.\n+        # Clear pending nodes in eager executors, kernel caches and\n+        # step_containers.\n         context.context().clear_kernel_cache()\n \n \n@@ -337,8 +342,8 @@ def learning_phase():\n     else:\n         with tf.init_scope():\n             # We always check & set the learning phase inside the init_scope,\n-            # otherwise the wrong default_graph will be used to look up the learning\n-            # phase inside of functions & defuns.\n+            # otherwise the wrong default_graph will be used to look up the\n+            # learning phase inside of functions & defuns.\n             #\n             # This is because functions & defuns (both in graph & in eager mode)\n             # will always execute non-eagerly using a function-specific default\n@@ -363,8 +368,8 @@ def _mark_func_graph_as_unsaveable(graph, learning_phase):\n     \"\"\"Mark func graph as unsaveable due to use of symbolic keras learning phase.\n \n     Functions that capture the symbolic learning phase cannot be exported to\n-    SavedModel. Mark the funcgraph as unsaveable, so that an error will be raised\n-    if it is exported.\n+    SavedModel. Mark the funcgraph as unsaveable, so that an error will be\n+    raised if it is exported.\n \n     Args:\n       graph: Graph or FuncGraph object.\n@@ -373,9 +378,9 @@ def _mark_func_graph_as_unsaveable(graph, learning_phase):\n     if graph.building_function and is_placeholder(learning_phase):\n         graph.mark_as_unsaveable(\n             \"The keras learning phase placeholder was used inside a function. \"\n-            \"Exporting placeholders is not supported when saving out a SavedModel. \"\n-            \"Please call `tf.keras.backend.set_learning_phase(0)` in the function \"\n-            \"to set the learning phase to a constant value.\"\n+            \"Exporting placeholders is not supported when saving out a \"\n+            \"SavedModel. Please call `tf.keras.backend.set_learning_phase(0)` \"\n+            \"in the function to set the learning phase to a constant value.\"\n         )\n \n \n@@ -390,13 +395,13 @@ def symbolic_learning_phase():\n \n \n def _internal_set_learning_phase(graph, value):\n-    global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n+    global _GRAPH_LEARNING_PHASES\n \n     if isinstance(value, tf.Tensor):\n         # The 'value' here is a tf.Tensor with attribute 'graph'.\n-        # There is a circular reference between key 'graph' and attribute 'graph'.\n-        # So we need use a weakref.ref to refer to the 'value' tensor here.\n-        # Otherwise, it would lead to memory leak.\n+        # There is a circular reference between key 'graph' and attribute\n+        # 'graph'.  So we need use a weakref.ref to refer to the 'value' tensor\n+        # here.  Otherwise, it would lead to memory leak.\n         value_ref = weakref.ref(value)\n         _GRAPH_LEARNING_PHASES[graph] = value_ref\n     else:\n@@ -428,8 +433,8 @@ def set_learning_phase(value):\n \n     The backend learning phase affects any code that calls\n     `backend.learning_phase()`\n-    In particular, all Keras built-in layers use the learning phase as the default\n-    for the `training` arg to `Layer.__call__`.\n+    In particular, all Keras built-in layers use the learning phase as the\n+    default for the `training` arg to `Layer.__call__`.\n \n     User-written layers and models can achieve the same behavior with code that\n     looks like:\n@@ -462,7 +467,8 @@ def deprecated_internal_set_learning_phase(value):\n     This method is an internal-only version of `set_learning_phase` that\n     does not raise a deprecation error. It is required because\n     saved_model needs to keep working with user code that uses the deprecated\n-    learning phase methods until those APIs are fully removed from the public API.\n+    learning phase methods until those APIs are fully removed from the public\n+    API.\n \n     Specifically SavedModel saving needs to make sure the learning phase is 0\n     during tracing even if users overwrote it to a different value.\n@@ -472,7 +478,8 @@ def deprecated_internal_set_learning_phase(value):\n     explicitly setting the learning phase for other values.\n \n     Args:\n-        value: Learning phase value, either 0 or 1 (integers). 0 = test, 1 = train\n+        value: Learning phase value, either 0 or 1 (integers).\n+            0 = test, 1 = train\n \n     Raises:\n         ValueError: if `value` is neither `0` nor `1`.\n@@ -481,8 +488,8 @@ def deprecated_internal_set_learning_phase(value):\n         raise ValueError(\"Expected learning phase to be 0 or 1.\")\n     with tf.init_scope():\n         if tf.executing_eagerly():\n-            # In an eager context, the learning phase values applies to both the eager\n-            # context and the internal Keras graph.\n+            # In an eager context, the learning phase values applies to both the\n+            # eager context and the internal Keras graph.\n             _DUMMY_EAGER_GRAPH.learning_phase_is_set = True\n             _internal_set_learning_phase(_DUMMY_EAGER_GRAPH.key, value)\n \n@@ -495,7 +502,8 @@ def deprecated_internal_set_learning_phase(value):\n def learning_phase_scope(value):\n     \"\"\"Provides a scope within which the learning phase is equal to `value`.\n \n-    The learning phase gets restored to its original value upon exiting the scope.\n+    The learning phase gets restored to its original value upon exiting the\n+    scope.\n \n     Args:\n        value: Learning phase value, either 0 or 1 (integers).\n@@ -535,7 +543,8 @@ def deprecated_internal_learning_phase_scope(value):\n     removed.\n \n     Args:\n-       value: Learning phase value, either 0 or 1 (integers). 0 = test, 1 = train\n+        value: Learning phase value, either 0 or 1 (integers).\n+            0 = test, 1 = train\n \n     Yields:\n         None.\n@@ -543,7 +552,7 @@ def deprecated_internal_learning_phase_scope(value):\n     Raises:\n         ValueError: if `value` is neither `0` nor `1`.\n     \"\"\"\n-    global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n+    global _GRAPH_LEARNING_PHASES\n     if value not in {0, 1}:\n         raise ValueError(\"Expected learning phase to be 0 or 1.\")\n \n@@ -592,7 +601,7 @@ def eager_learning_phase_scope(value):\n     Raises:\n        ValueError: if `value` is neither `0` nor `1`.\n     \"\"\"\n-    global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n+    global _GRAPH_LEARNING_PHASES\n     assert value in {0, 1}\n     assert tf.compat.v1.executing_eagerly_outside_functions()\n     global_learning_phase_was_set = global_learning_phase_is_set()\n@@ -652,8 +661,8 @@ def _current_graph(op_input_list, graph=None):\n \n     1. If the default graph is being used to construct a function, we\n        use the default graph.\n-    2. If the \"graph\" is specified explicitly, we validate that all of the inputs\n-       in \"op_input_list\" are compatible with that graph.\n+    2. If the \"graph\" is specified explicitly, we validate that all of the\n+       inputs in \"op_input_list\" are compatible with that graph.\n     3. Otherwise, we attempt to select a graph from the first Operation-\n        or Tensor-valued input in \"op_input_list\", and validate that all other\n        such inputs are in the same graph.\n@@ -661,16 +670,17 @@ def _current_graph(op_input_list, graph=None):\n        \"op_input_list\", we attempt to use the default graph.\n \n     Args:\n-      op_input_list: A list of inputs to an operation, which may include `Tensor`,\n-        `Operation`, and other objects that may be converted to a graph element.\n+      op_input_list: A list of inputs to an operation, which may include\n+        `Tensor`, `Operation`, and other objects that may be converted to a\n+        graph element.\n       graph: (Optional) The explicit graph to use.\n \n     Raises:\n       TypeError: If op_input_list is not a list or tuple, or if graph is not a\n         Graph.\n-      ValueError: If a graph is explicitly passed and not all inputs are from it,\n-        or if the inputs are from multiple graphs, or we could not find a graph\n-        and there was no default graph.\n+      ValueError: If a graph is explicitly passed and not all inputs are from\n+        it, or if the inputs are from multiple graphs, or we could not find a\n+        graph and there was no default graph.\n \n     Returns:\n       The appropriate graph to use for the given inputs.\n@@ -692,8 +702,8 @@ def _current_graph(op_input_list, graph=None):\n     original_graph_element = None\n     for op_input in op_input_list:\n         # Determine if this is a valid graph_element.\n-        # TODO(joshl): Note that we exclude subclasses of Tensor. Need to clean this\n-        # up.\n+        # TODO(joshl): Note that we exclude subclasses of Tensor. Need to clean\n+        # this up.\n         if isinstance(\n             op_input, (tf.Operation, tf.Tensor, tf.__internal__.CompositeTensor)\n         ) and (\n@@ -736,8 +746,8 @@ def _get_session(op_input_list=()):\n         ) is None or _SESSION.session.graph is not _current_graph(\n             op_input_list\n         ):\n-            # If we are creating the Session inside a tf.distribute.Strategy scope,\n-            # we ask the strategy for the right session options to use.\n+            # If we are creating the Session inside a tf.distribute.Strategy\n+            # scope, we ask the strategy for the right session options to use.\n             if tf.distribute.has_strategy():\n                 configure_and_create_distributed_session(\n                     tf.distribute.get_strategy()\n@@ -914,8 +924,8 @@ def _is_current_explicit_device(device_type):\n         device_type: A string containing `GPU` or `CPU` (case-insensitive).\n \n     Returns:\n-        A boolean indicating if the current device scope is explicitly set on the\n-        device type.\n+        A boolean indicating if the current device scope is explicitly set on\n+        the device type.\n \n     Raises:\n         ValueError: If the `device_type` string indicates an unsupported device.\n@@ -1178,12 +1188,12 @@ def unique_object_name(\n       name: String name to make unique.\n       name_uid_map: An optional defaultdict(int) to use when creating unique\n         names. If None (default), uses a per-Graph dictionary.\n-      avoid_names: An optional set or dict with names which should not be used. If\n-        None (default), don't avoid any names unless `avoid_observed_names` is\n-        True.\n-      namespace: Gets a name which is unique within the (graph, namespace). Layers\n-        which are not Networks use a blank namespace and so get graph-global\n-        names.\n+      avoid_names: An optional set or dict with names which should not be used.\n+        If None (default), don't avoid any names unless `avoid_observed_names`\n+        is True.\n+      namespace: Gets a name which is unique within the (graph, namespace).\n+        Layers which are not Networks use a blank namespace and so get\n+        graph-global names.\n       zero_based: If True, name sequences start with no suffix (e.g. \"dense\",\n         \"dense_1\"). If False, naming is one-based (\"dense_1\", \"dense_2\").\n       avoid_observed_names: If True, avoid any names that have been observed by\n@@ -1304,7 +1314,8 @@ def is_keras_tensor(x):\n     >>> tf.keras.backend.is_keras_tensor(np_var)\n     Traceback (most recent call last):\n     ...\n-    ValueError: Unexpectedly found an instance of type `<class 'numpy.ndarray'>`.\n+    ValueError: Unexpectedly found an instance of type\n+    `<class 'numpy.ndarray'>`.\n     Expected a symbolic tensor instance.\n     >>> keras_var = tf.keras.backend.variable(np_var)\n     >>> # A variable created with the keras backend is not a Keras tensor.\n@@ -1362,8 +1373,8 @@ def placeholder(\n         name: Optional name string for the placeholder.\n         ragged: Boolean, whether the placeholder should have a ragged type.\n             In this case, values of 'None' in the 'shape' argument represent\n-            ragged dimensions. For more information about RaggedTensors, see this\n-            [guide](https://www.tensorflow.org/guide/ragged_tensors).\n+            ragged dimensions. For more information about RaggedTensors, see\n+            this [guide](https://www.tensorflow.org/guide/ragged_tensors).\n \n     Raises:\n         ValueError: If called with sparse = True and ragged = True.\n@@ -1381,7 +1392,8 @@ def placeholder(\n     \"\"\"\n     if sparse and ragged:\n         raise ValueError(\n-            \"Cannot set both sparse and ragged to True when creating a placeholder.\"\n+            \"Cannot set both sparse and ragged to \"\n+            \"True when creating a placeholder.\"\n         )\n     if dtype is None:\n         dtype = floatx()\n@@ -1835,18 +1847,18 @@ def is_tf_random_generator_enabled():\n     usage of `tf.random.Generator`, please use\n     `tf.keras.backend.experimental.disable_random_generator`.\n \n-    We expect the `tf.random.Generator` code path to become the default, and will\n-    remove the legacy stateful random ops such as `tf.random.uniform` in the\n-    future (see the\n-    [TF RNG guide](https://www.tensorflow.org/guide/random_numbers)).\n+    We expect the `tf.random.Generator` code path to become the default, and\n+    will remove the legacy stateful random ops such as `tf.random.uniform` in\n+    the future (see the [TF RNG guide](\n+    https://www.tensorflow.org/guide/random_numbers)).\n \n     This API will also be removed in a future release as well, together with\n     `tf.keras.backend.experimental.enable_tf_random_generator()` and\n     `tf.keras.backend.experimental.disable_tf_random_generator()`\n \n     Returns:\n-      boolean: whether `tf.random.Generator` is used for random number generation\n-        in Keras.\n+      boolean: whether `tf.random.Generator` is used for random number\n+        generation in Keras.\n     \"\"\"\n     return _USE_GENERATOR_FOR_RNG\n \n@@ -1914,8 +1926,8 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n         self._built = False\n \n     def _set_rng_type(self, rng_type, **kwargs):\n-        # Only supported kwargs is \"force_generator\", which we will remove once we\n-        # clean up all the caller.\n+        # Only supported kwargs is \"force_generator\", which we will remove once\n+        # we clean up all the caller.\n         # TODO(scottzhu): Remove the kwargs for force_generator.\n         if kwargs.get(\"force_generator\", False):\n             rng_type = self.RNG_STATEFUL\n@@ -1932,7 +1944,8 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n             ]:\n                 raise ValueError(\n                     \"Invalid `rng_type` received. \"\n-                    'Valid `rng_type` are [\"stateless\", \"stateful\", \"legacy_stateful\"].'\n+                    'Valid `rng_type` are [\"stateless\", '\n+                    '\"stateful\", \"legacy_stateful\"].'\n                     f\" Got: {rng_type}\"\n                 )\n             self._rng_type = rng_type\n@@ -1940,13 +1953,14 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n     def _maybe_init(self):\n         \"\"\"Lazily init the RandomGenerator.\n \n-        The TF API executing_eagerly_outside_functions() has some side effect, and\n-        couldn't be used before API like tf.enable_eager_execution(). Some of the\n-        client side code was creating the initializer at the code load time, which\n-        triggers the creation of RandomGenerator. Lazy init this class to walkaround\n-        this issue until it is resolved on TF side.\n+        The TF API executing_eagerly_outside_functions() has some side effect,\n+        and couldn't be used before API like tf.enable_eager_execution(). Some\n+        of the client side code was creating the initializer at the code load\n+        time, which triggers the creation of RandomGenerator. Lazy init this\n+        class to walkaround this issue until it is resolved on TF side.\n         \"\"\"\n-        # TODO(b/167482354): Change this back to normal init when the bug is fixed.\n+        # TODO(b/167482354): Change this back to normal init when the bug is\n+        # fixed.\n         if self._built:\n             return\n \n@@ -1954,7 +1968,8 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n             self._rng_type == self.RNG_STATEFUL\n             and not tf.compat.v1.executing_eagerly_outside_functions()\n         ):\n-            # Fall back to legacy stateful since the generator need to work in tf2.\n+            # Fall back to legacy stateful since the generator need to work in\n+            # tf2.\n             self._rng_type = self.RNG_LEGACY_STATEFUL\n \n         if self._rng_type == self.RNG_STATELESS:\n@@ -1969,17 +1984,18 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n                 seed = self._create_seed(self._seed)\n                 self._generator = tf.random.Generator.from_seed(seed)\n         else:\n-            # In legacy stateful, we use stateful op, regardless whether user provide\n-            # seed or not. Seeded stateful op will ensure generating same sequences.\n+            # In legacy stateful, we use stateful op, regardless whether user\n+            # provide seed or not. Seeded stateful op will ensure generating\n+            # same sequences.\n             self._generator = None\n         self._built = True\n \n     def make_seed_for_stateless_op(self):\n         \"\"\"Generate a new seed based on the init config.\n \n-        Note that this will not return python ints which will be frozen in the graph\n-        and cause stateless op to return the same value. It will only return value\n-        when generator is used, otherwise it will return None.\n+        Note that this will not return python ints which will be frozen in the\n+        graph and cause stateless op to return the same value. It will only\n+        return value when generator is used, otherwise it will return None.\n \n         Returns:\n           A tensor with shape [2,].\n@@ -1994,12 +2010,13 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n     def make_legacy_seed(self):\n         \"\"\"Create a new seed for the legacy stateful ops to use.\n \n-        When user didn't provide any original seed, this method will return None.\n-        Otherwise it will increment the counter and return as the new seed.\n+        When user didn't provide any original seed, this method will return\n+        None.  Otherwise it will increment the counter and return as the new\n+        seed.\n \n         Note that it is important to generate different seed for stateful ops in\n-        the `tf.function`. The random ops will return same value when same seed is\n-        provided in the `tf.function`.\n+        the `tf.function`. The random ops will return same value when same seed\n+        is provided in the `tf.function`.\n \n         Returns:\n           int as new seed, or None.\n@@ -2026,14 +2043,14 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n         Args:\n           shape: The shape of the random values to generate.\n           mean: Floats, default to 0. Mean of the random values to generate.\n-          stddev: Floats, default to 1. Standard deviation of the random values to\n-            generate.\n+          stddev: Floats, default to 1. Standard deviation of the random values\n+            to generate.\n           dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `tf.keras.backend.floatx()` is used, which\n-            default to `float32` unless you configured it otherwise (via\n+            supported. If not specified, `tf.keras.backend.floatx()` is used,\n+            which default to `float32` unless you configured it otherwise (via\n             `tf.keras.backend.set_floatx(float_dtype)`)\n-          nonce: Optional integer scalar, that will be folded into the seed in the\n-            stateless mode.\n+          nonce: Optional integer scalar, that will be folded into the seed in\n+            the stateless mode.\n         \"\"\"\n         self._maybe_init()\n         dtype = dtype or floatx()\n@@ -2068,11 +2085,11 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n           minval: Floats, default to None. Upper bound of the range of\n             random values to generate (exclusive).\n           dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `tf.keras.backend.floatx()` is used, which\n-            default to `float32` unless you configured it otherwise (via\n+            supported. If not specified, `tf.keras.backend.floatx()` is used,\n+            which default to `float32` unless you configured it otherwise (via\n             `tf.keras.backend.set_floatx(float_dtype)`)\n-          nonce: Optional integer scalar, that will be folded into the seed in the\n-            stateless mode.\n+          nonce: Optional integer scalar, that will be folded into the seed in\n+            the stateless mode.\n         \"\"\"\n         self._maybe_init()\n         dtype = dtype or floatx()\n@@ -2107,14 +2124,14 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n         Args:\n           shape: The shape of the random values to generate.\n           mean: Floats, default to 0. Mean of the random values to generate.\n-          stddev: Floats, default to 1. Standard deviation of the random values to\n-            generate.\n+          stddev: Floats, default to 1. Standard deviation of the random values\n+            to generate.\n           dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `tf.keras.backend.floatx()` is used, which\n-            default to `float32` unless you configured it otherwise (via\n+            supported. If not specified, `tf.keras.backend.floatx()` is used,\n+            which default to `float32` unless you configured it otherwise (via\n             `tf.keras.backend.set_floatx(float_dtype)`)\n-          nonce: Optional integer scalar, that will be folded into the seed in the\n-            stateless mode.\n+          nonce: Optional integer scalar, that will be folded into the seed in\n+            the stateless mode.\n         \"\"\"\n         self._maybe_init()\n         dtype = dtype or floatx()\n@@ -2868,8 +2885,8 @@ def std(x, axis=None, keepdims=False):\n           `[-rank(x), rank(x))`.\n         keepdims: A boolean, whether to keep the dimensions or not.\n             If `keepdims` is `False`, the rank of the tensor is reduced\n-            by 1. If `keepdims` is `True`, the reduced dimension is retained with\n-            length 1.\n+            by 1. If `keepdims` is `True`, the reduced dimension is retained\n+            with length 1.\n \n     Returns:\n         A tensor with the standard deviation of elements of `x` with same dtype.\n@@ -4259,10 +4276,11 @@ def set_value(x, value):\n                 assign_placeholder = x._assign_placeholder\n                 assign_op = x._assign_op\n             else:\n-                # In order to support assigning weights to resizable variables in\n-                # Keras, we make a placeholder with the correct number of dimensions\n-                # but with None in each dimension. This way, we can assign weights\n-                # of any size (as long as they have the correct dimensionality).\n+                # In order to support assigning weights to resizable variables\n+                # in Keras, we make a placeholder with the correct number of\n+                # dimensions but with None in each dimension. This way, we can\n+                # assign weights of any size (as long as they have the correct\n+                # dimensionality).\n                 placeholder_shape = tf.TensorShape([None] * value.ndim)\n                 assign_placeholder = tf.compat.v1.placeholder(\n                     tf_dtype, shape=placeholder_shape\n@@ -4298,10 +4316,11 @@ def batch_set_value(tuples):\n                         assign_placeholder = x._assign_placeholder\n                         assign_op = x._assign_op\n                     else:\n-                        # In order to support assigning weights to resizable variables in\n-                        # Keras, we make a placeholder with the correct number of dimensions\n-                        # but with None in each dimension. This way, we can assign weights\n-                        # of any size (as long as they have the correct dimensionality).\n+                        # In order to support assigning weights to resizable\n+                        # variables in Keras, we make a placeholder with the\n+                        # correct number of dimensions but with None in each\n+                        # dimension. This way, we can assign weights of any size\n+                        # (as long as they have the correct dimensionality).\n                         placeholder_shape = tf.TensorShape([None] * value.ndim)\n                         assign_placeholder = tf.compat.v1.placeholder(\n                             tf_dtype, shape=placeholder_shape\n@@ -4340,9 +4359,9 @@ def print_tensor(x, message=\"\", summarize=3):\n         x: Tensor to print.\n         message: Message to print jointly with the tensor.\n         summarize: The first and last `summarize` elements within each dimension\n-            are recursively printed per Tensor. If None, then the first 3 and last\n-            3 elements of each dimension are printed for each tensor. If set to\n-            -1, it will print all elements of every tensor.\n+            are recursively printed per Tensor. If None, then the first 3 and\n+            last 3 elements of each dimension are printed for each tensor. If\n+            set to -1, it will print all elements of every tensor.\n \n     Returns:\n         The same tensor `x`, unchanged.\n@@ -4449,7 +4468,8 @@ class GraphExecutionFunction:\n \n         Args:\n           feed_arrays: List of input tensors to be fed Numpy arrays at runtime.\n-          feed_symbols: List of input tensors to be fed symbolic tensors at runtime.\n+          feed_symbols: List of input tensors to be fed symbolic tensors at\n+            runtime.\n           symbol_vals: List of symbolic tensors to be fed to `feed_symbols`.\n           session: Session to use to generate the callable.\n \n@@ -4501,11 +4521,11 @@ class GraphExecutionFunction:\n     def _eval_if_composite(self, tensor):\n         \"\"\"Helper method which evaluates any CompositeTensors passed to it.\"\"\"\n         # We need to evaluate any composite tensor objects that have been\n-        # reconstructed in 'pack_sequence_as', since otherwise they'll be output as\n-        # actual CompositeTensor objects instead of the value(s) contained in the\n-        # CompositeTensors. E.g., if output_structure contains a SparseTensor, then\n-        # this ensures that we return its value as a SparseTensorValue rather than\n-        # a SparseTensor.\n+        # reconstructed in 'pack_sequence_as', since otherwise they'll be output\n+        # as actual CompositeTensor objects instead of the value(s) contained in\n+        # the CompositeTensors. E.g., if output_structure contains a\n+        # SparseTensor, then this ensures that we return its value as a\n+        # SparseTensorValue rather than a SparseTensor.\n         from keras.utils import tf_utils  # pylint: disable=g-import-not-at-top\n \n         if tf_utils.is_extension_type(tensor):\n@@ -4532,8 +4552,8 @@ class GraphExecutionFunction:\n             else:\n                 # Case: feeding Numpy array.\n                 feed_arrays.append(tensor)\n-                # We need to do array conversion and type casting at this level, since\n-                # `callable_fn` only supports exact matches.\n+                # We need to do array conversion and type casting at this level,\n+                # since `callable_fn` only supports exact matches.\n                 tensor_type = tf.as_dtype(tensor.dtype)\n                 array_vals.append(\n                     np.asarray(value, dtype=tensor_type.as_numpy_dtype)\n@@ -4566,11 +4586,11 @@ class GraphExecutionFunction:\n             expand_composites=True,\n         )\n         # We need to evaluate any composite tensor objects that have been\n-        # reconstructed in 'pack_sequence_as', since otherwise they'll be output as\n-        # actual CompositeTensor objects instead of the value(s) contained in the\n-        # CompositeTensors. E.g., if output_structure contains a SparseTensor, then\n-        # this ensures that we return its value as a SparseTensorValue rather than\n-        # a SparseTensor.\n+        # reconstructed in 'pack_sequence_as', since otherwise they'll be output\n+        # as actual CompositeTensor objects instead of the value(s) contained in\n+        # the CompositeTensors. E.g., if output_structure contains a\n+        # SparseTensor, then this ensures that we return its value as a\n+        # SparseTensorValue rather than a SparseTensor.\n         return tf.nest.map_structure(self._eval_if_composite, output_structure)\n \n \n@@ -4624,8 +4644,8 @@ def function(inputs, outputs, updates=None, name=None, **kwargs):\n                 0\n             ] and key not in [\"inputs\", \"outputs\", \"updates\", \"name\"]:\n                 msg = (\n-                    'Invalid argument \"%s\" passed to K.function with TensorFlow '\n-                    \"backend\"\n+                    'Invalid argument \"%s\" passed to K.function with '\n+                    \"TensorFlow backend\"\n                 ) % key\n                 raise ValueError(msg)\n     return GraphExecutionFunction(\n@@ -4707,10 +4727,10 @@ def rnn(\n             (at least 3D), or nested tensors, and each of which has shape\n             `(samples, time, ...)`.\n         initial_states: Tensor with shape `(samples, state_size)`\n-            (no time dimension), containing the initial values for the states used\n-            in the step function. In the case that state_size is in a nested\n-            shape, the shape of initial_states will also follow the nested\n-            structure.\n+            (no time dimension), containing the initial values for the states\n+            used in the step function. In the case that state_size is in a\n+            nested shape, the shape of initial_states will also follow the\n+            nested structure.\n         go_backwards: Boolean. If True, do the iteration over the time\n             dimension in reverse order and return the reversed sequence.\n         mask: Binary tensor with shape `(samples, time, 1)`,\n@@ -4718,21 +4738,22 @@ def rnn(\n         constants: List of constant values passed at each step.\n         unroll: Whether to unroll the RNN or to use a symbolic `while_loop`.\n         input_length: An integer or a 1-D Tensor, depending on whether\n-            the time dimension is fixed-length or not. In case of variable length\n-            input, it is used for masking in case there's no mask specified.\n+            the time dimension is fixed-length or not. In case of variable\n+            length input, it is used for masking in case there's no mask\n+            specified.\n         time_major: Boolean. If true, the inputs and outputs will be in shape\n             `(timesteps, batch, ...)`, whereas in the False case, it will be\n             `(batch, timesteps, ...)`. Using `time_major = True` is a bit more\n-            efficient because it avoids transposes at the beginning and end of the\n-            RNN calculation. However, most TensorFlow data is batch-major, so by\n-            default this function accepts input and emits output in batch-major\n-            form.\n+            efficient because it avoids transposes at the beginning and end of\n+            the RNN calculation. However, most TensorFlow data is batch-major,\n+            so by default this function accepts input and emits output in\n+            batch-major form.\n         zero_output_for_mask: Boolean. If True, the output for masked timestep\n             will be zeros, whereas in the False case, output from previous\n             timestep is returned.\n-        return_all_outputs: Boolean. If True, return the recurrent outputs for all\n-            timesteps in the sequence. If False, only return the output for the\n-            last timestep (which consumes less memory).\n+        return_all_outputs: Boolean. If True, return the recurrent outputs for\n+            all timesteps in the sequence. If False, only return the output for\n+            the last timestep (which consumes less memory).\n \n     Returns:\n         A tuple, `(last_output, outputs, new_states)`.\n@@ -4750,8 +4771,8 @@ def rnn(\n         ValueError: if input dimension is less than 3.\n         ValueError: if `unroll` is `True` but input timestep is not a fixed\n             number.\n-        ValueError: if `mask` is provided (not `None`) but states is not provided\n-            (`len(states)` == 0).\n+        ValueError: if `mask` is provided (not `None`) but states is not\n+            provided (`len(states)` == 0).\n     \"\"\"\n     if not tf.__internal__.tf2.enabled():\n         return_all_outputs = True  # Not supported in TF1.\n@@ -4813,10 +4834,10 @@ def rnn(\n         successive_outputs = []\n \n         # Process the input tensors. The input tensor need to be split on the\n-        # time_step dim, and reverse if go_backwards is True. In the case of nested\n-        # input, the input is flattened and then transformed individually.\n-        # The result of this will be a tuple of lists, each of the item in tuple is\n-        # list of the tensor with shape (batch, feature)\n+        # time_step dim, and reverse if go_backwards is True. In the case of\n+        # nested input, the input is flattened and then transformed\n+        # individually.  The result of this will be a tuple of lists, each of\n+        # the item in tuple is list of the tensor with shape (batch, feature)\n         def _process_single_input_t(input_t):\n             input_t = tf.unstack(input_t)  # unstack for time_step dim\n             if go_backwards:\n@@ -4908,9 +4929,9 @@ def rnn(\n     else:  # Unroll == False\n         states = tuple(initial_states)\n \n-        # Create input tensor array, if the inputs is nested tensors, then it will\n-        # be flattened first, and tensor array will be created one per flattened\n-        # tensor.\n+        # Create input tensor array, if the inputs is nested tensors, then it\n+        # will be flattened first, and tensor array will be created one per\n+        # flattened tensor.\n         input_ta = tuple(\n             tf.TensorArray(\n                 dtype=inp.dtype,\n@@ -4926,14 +4947,14 @@ def rnn(\n             for ta, input_ in zip(input_ta, flatted_inputs)\n         )\n \n-        # Get the time(0) input and compute the output for that, the output will be\n-        # used to determine the dtype of output tensor array. Don't read from\n+        # Get the time(0) input and compute the output for that, the output will\n+        # be used to determine the dtype of output tensor array. Don't read from\n         # input_ta due to TensorArray clear_after_read default to True.\n         input_time_zero = tf.nest.pack_sequence_as(\n             inputs, [inp[0] for inp in flatted_inputs]\n         )\n-        # output_time_zero is used to determine the cell output shape and its dtype.\n-        # the value is discarded.\n+        # output_time_zero is used to determine the cell output shape and its\n+        # dtype.  the value is discarded.\n         output_time_zero, _ = step_function(\n             input_time_zero, tuple(initial_states) + tuple(constants)\n         )\n@@ -4951,8 +4972,8 @@ def rnn(\n \n         time = tf.constant(0, dtype=\"int32\", name=\"time\")\n \n-        # We only specify the 'maximum_iterations' when building for XLA since that\n-        # causes slowdowns on GPU in TF.\n+        # We only specify the 'maximum_iterations' when building for XLA since\n+        # that causes slowdowns on GPU in TF.\n         if (\n             not tf.executing_eagerly()\n             and control_flow_util.GraphOrParentsInXlaContext(\n@@ -5014,8 +5035,8 @@ def rnn(\n             masking_fn = None\n \n         if masking_fn is not None:\n-            # Mask for the T output will be base on the output of T - 1. In the case\n-            # T = 0, a zero filled tensor will be used.\n+            # Mask for the T output will be base on the output of T - 1. In the\n+            # case T = 0, a zero filled tensor will be used.\n             flat_zero_output = tuple(\n                 tf.zeros_like(o) for o in tf.nest.flatten(output_time_zero)\n             )\n@@ -5454,7 +5475,8 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n       [[1. 0. 0.]\n        [0. 1. 0.]\n        [0. 0. 1.]], shape=(3, 3), dtype=float32)\n-    >>> b = tf.constant([.9, .05, .05, .05, .89, .06, .05, .01, .94], shape=[3,3])\n+    >>> b = tf.constant([.9, .05, .05, .05, .89, .06, .05, .01, .94],\n+    ...                 shape=[3, 3])\n     >>> print(b)\n     tf.Tensor(\n       [[0.9  0.05 0.05]\n@@ -5480,7 +5502,8 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n             warnings.warn(\n                 '\"`categorical_crossentropy` received `from_logits=True`, but '\n                 \"the `output` argument was produced by a sigmoid or softmax \"\n-                'activation and thus does not represent logits. Was this intended?\"',\n+                \"activation and thus does not represent logits. \"\n+                \"Was this intended?\",\n                 stacklevel=2,\n             )\n         from_logits = True\n@@ -5544,9 +5567,10 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         output = output._keras_logits  # pylint: disable=protected-access\n         if from_logits:\n             warnings.warn(\n-                '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n-                \"the `output` argument was produced by a sigmoid or softmax \"\n-                'activation and thus does not represent logits. Was this intended?\"',\n+                '\"`sparse_categorical_crossentropy` received '\n+                \"`from_logits=True`, but the `output` argument \"\n+                \"was produced by a sigmoid or softmax activation \"\n+                'and thus does not represent logits. Was this intended?\"',\n                 stacklevel=2,\n             )\n         from_logits = True\n@@ -5582,8 +5606,8 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n             output = tf.compat.v1.transpose(output, perm=permutation)\n     elif axis != -1:\n         raise ValueError(\n-            \"Cannot compute sparse categorical crossentropy with `axis={}` on an \"\n-            \"output tensor with unknown rank\".format(axis)\n+            \"Cannot compute sparse categorical crossentropy with `axis={}` \"\n+            \"on an output tensor with unknown rank\".format(axis)\n         )\n \n     target = cast(target, \"int64\")\n@@ -5612,7 +5636,8 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         )\n \n     if update_shape and output_rank >= 3:\n-        # If our output includes timesteps or spatial dimensions we need to reshape\n+        # If our output includes timesteps or spatial dimensions we need to\n+        # reshape\n         return tf.reshape(res, output_shape[:-1])\n     else:\n         return res\n@@ -5643,8 +5668,9 @@ def binary_crossentropy(target, output, from_logits=False):\n         output = output._keras_logits  # pylint: disable=protected-access\n         if from_logits:\n             warnings.warn(\n-                '\"`binary_crossentropy` received `from_logits=True`, but the `output`'\n-                \" argument was produced by a sigmoid or softmax activation and thus \"\n+                '\"`binary_crossentropy` received `from_logits=True`, '\n+                \"but the `output` argument was produced by a sigmoid \"\n+                \"or softmax activation and thus \"\n                 'does not represent logits. Was this intended?\"',\n                 stacklevel=2,\n             )\n@@ -5711,11 +5737,12 @@ def binary_focal_crossentropy(\n       output: A tensor.\n       apply_class_balancing: A bool, whether to apply weight balancing on the\n         binary classes 0 and 1.\n-      alpha: A weight balancing factor for class 1, default is `0.25` as mentioned\n-        in the reference. The weight for class 0 is `1.0 - alpha`.\n-      gamma: A focusing parameter, default is `2.0` as mentioned in the reference.\n-      from_logits: Whether `output` is expected to be a logits tensor. By default,\n-        we consider that `output` encodes a probability distribution.\n+      alpha: A weight balancing factor for class 1, default is `0.25` as\n+        mentioned in the reference. The weight for class 0 is `1.0 - alpha`.\n+      gamma: A focusing parameter, default is `2.0` as mentioned in the\n+        reference.\n+      from_logits: Whether `output` is expected to be a logits tensor. By\n+        default, we consider that `output` encodes a probability distribution.\n \n     Returns:\n       A tensor.\n@@ -5842,7 +5869,8 @@ def in_top_k(predictions, targets, k):\n     \"\"\"Returns whether the `targets` are in the top `k` `predictions`.\n \n     Args:\n-        predictions: A tensor of shape `(batch_size, classes)` and type `float32`.\n+        predictions: A tensor of shape `(batch_size, classes)` and type\n+          `float32`.\n         targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.\n         k: An `int`, number of top elements to consider.\n \n@@ -6758,8 +6786,8 @@ def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n     Args:\n         shape: A tuple of integers, the shape of tensor to create.\n-        mean: A float, the mean value of the normal distribution to draw samples.\n-          Default to 0.0.\n+        mean: A float, the mean value of the normal distribution to draw\n+          samples. Default to 0.0.\n         stddev: A float, the standard deviation of the normal distribution\n           to draw samples. Default to 1.0.\n         dtype: `tf.dtypes.DType`, dtype of returned tensor. Default to use Keras\n@@ -7181,16 +7209,16 @@ def configure_and_create_distributed_session(distribution_strategy):\n         \"\"\"Create the Distributed Strategy session.\"\"\"\n         session_config = get_default_session_config()\n \n-        # If a session already exists, merge in its config; in the case there is a\n-        # conflict, take values of the existing config.\n+        # If a session already exists, merge in its config; in the case there is\n+        # a conflict, take values of the existing config.\n         global _SESSION\n         if getattr(_SESSION, \"session\", None) and _SESSION.session._config:\n             session_config.MergeFrom(_SESSION.session._config)\n \n         if is_tpu_strategy(distribution_strategy):\n             # TODO(priyag, yuefengz): Remove this workaround when Distribute\n-            # Coordinator is integrated with keras and we can create a session from\n-            # there.\n+            # Coordinator is integrated with keras and we can create a session\n+            # from there.\n             distribution_strategy.configure(session_config)\n             master = (\n                 distribution_strategy.extended._tpu_cluster_resolver.master()\n@@ -7278,8 +7306,8 @@ def maybe_convert_to_ragged(\n         return output\n \n     if go_backwards:\n-        # Reverse based on the timestep dim, so that nested_row_lengths will mask\n-        # from the correct direction. Return the reverse ragged tensor.\n+        # Reverse based on the timestep dim, so that nested_row_lengths will\n+        # mask from the correct direction. Return the reverse ragged tensor.\n         output = reverse(output, [1])\n         ragged = tf.RaggedTensor.from_tensor(output, nested_row_lengths)\n         return reverse(ragged, [1])\n@@ -7291,14 +7319,16 @@ class ContextValueCache(weakref.WeakKeyDictionary):\n     \"\"\"Container that caches (possibly tensor) values based on the context.\n \n     This class is similar to defaultdict, where values may be produced by the\n-    default factory specified during initialization. This class also has a default\n-    value for the key (when key is `None`) -- the key is set to the current graph\n-    or eager context. The default factories for key and value are only used in\n-    `__getitem__` and `setdefault`. The `.get()` behavior remains the same.\n+    default factory specified during initialization. This class also has a\n+    default value for the key (when key is `None`) -- the key is set to the\n+    current graph or eager context. The default factories for key and value are\n+    only used in `__getitem__` and `setdefault`. The `.get()` behavior remains\n+    the same.\n \n-    This object will return the value of the current graph or closest parent graph\n-    if the current graph is a function. This is to reflect the fact that if a\n-    tensor is created in eager/graph, child functions may capture that tensor.\n+    This object will return the value of the current graph or closest parent\n+    graph if the current graph is a function. This is to reflect the fact that\n+    if a tensor is created in eager/graph, child functions may capture that\n+    tensor.\n \n     The default factory method may accept keyword arguments (unlike defaultdict,\n     which only accepts callables with 0 arguments). To pass keyword arguments to\n@@ -7345,11 +7375,11 @@ class ContextValueCache(weakref.WeakKeyDictionary):\n \n     def _get_parent_graph(self, graph):\n         \"\"\"Returns the parent graph or dummy eager object.\"\"\"\n-        # TODO(b/149317164): Currently FuncGraphs use ops.get_default_graph() as the\n-        # outer graph. This results in outer_graph always being a Graph,\n+        # TODO(b/149317164): Currently FuncGraphs use ops.get_default_graph() as\n+        # the outer graph. This results in outer_graph always being a Graph,\n         # even in eager mode (get_default_graph will create a new Graph if there\n-        # isn't a default graph). Because of this bug, we have to specially set the\n-        # key when eager execution is enabled.\n+        # isn't a default graph). Because of this bug, we have to specially set\n+        # the key when eager execution is enabled.\n         parent_graph = graph.outer_graph\n         if (\n             not isinstance(parent_graph, tf.__internal__.FuncGraph)\n@@ -7365,8 +7395,8 @@ class ContextValueCache(weakref.WeakKeyDictionary):\n             return value\n \n         # Since FuncGraphs are able to capture tensors and variables from their\n-        # parent graphs, recursively search to see if there is a value stored for\n-        # one of the parent graphs.\n+        # parent graphs, recursively search to see if there is a value stored\n+        # for one of the parent graphs.\n         if isinstance(key, tf.__internal__.FuncGraph):\n             return self._get_recursive(self._get_parent_graph(key))\n         return None\n@@ -7375,8 +7405,8 @@ class ContextValueCache(weakref.WeakKeyDictionary):\n         \"\"\"Gets the value at key (or current context), or sets default value.\n \n         Args:\n-          key: May be `None` or `Graph`object. When `None`, the key is set to the\n-            current context.\n+          key: May be `None` or `Graph`object. When `None`, the key is set to\n+            the current context.\n \n         Returns:\n           Either the cached or default value.\n@@ -7392,7 +7422,8 @@ class ContextValueCache(weakref.WeakKeyDictionary):\n         return value\n \n     def setdefault(self, key=None, default=None, kwargs=None):\n-        \"\"\"Sets the default value if key is not in dict, and returns the value.\"\"\"\n+        \"\"\"Sets the default value if key is not in dict, and returns the\n+        value.\"\"\"\n         if key is None:\n             key = self._key()\n         kwargs = kwargs or {}\n\n@@ -81,9 +81,9 @@ def floatx():\n def set_floatx(value):\n     \"\"\"Sets the default float type.\n \n-    Note: It is not recommended to set this to float16 for training, as this will\n-    likely cause numeric stability issues. Instead, mixed precision, which is\n-    using a mix of float16 and float32, can be used by calling\n+    Note: It is not recommended to set this to float16 for training, as this\n+    will likely cause numeric stability issues. Instead, mixed precision, which\n+    is using a mix of float16 and float32, can be used by calling\n     `tf.keras.mixed_precision.set_global_policy('mixed_float16')`. See the\n     [mixed precision guide](\n       https://www.tensorflow.org/guide/keras/mixed_precision) for details.\n@@ -106,7 +106,8 @@ def set_floatx(value):\n     accepted_dtypes = {\"float16\", \"float32\", \"float64\"}\n     if value not in accepted_dtypes:\n         raise ValueError(\n-            f\"Unknown `floatx` value: {value}. Expected one of {accepted_dtypes}\"\n+            f\"Unknown `floatx` value: {value}. \"\n+            f\"Expected one of {accepted_dtypes}\"\n         )\n     _FLOATX = str(value)\n \n\n@@ -178,7 +178,8 @@ class BackendUtilsTest(tf.test.TestCase):\n \n     def test_learning_phase_name(self):\n         with backend.name_scope(\"test_scope\"):\n-            # Test that outer name scopes do not affect the learning phase's name.\n+            # Test that outer name scopes do not affect the learning phase's\n+            # name.\n             lp = backend.symbolic_learning_phase()\n         self.assertEqual(lp.name, \"keras_learning_phase:0\")\n \n@@ -1614,8 +1615,8 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n \n         # outputs expected to be same as inputs for the first sample\n         expected_outputs = inputs_vals.copy()\n-        # but for the second sample all outputs in masked region should be the same\n-        # as last output before masked region\n+        # but for the second sample all outputs in masked region should be the\n+        # same as last output before masked region\n         expected_outputs[1, -mask_last_num_timesteps:] = expected_outputs[\n             1, -(mask_last_num_timesteps + 1)\n         ]\n@@ -1654,8 +1655,9 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n             outputs = backend.tile(backend.expand_dims(inputs), [1, 1, 2])\n             return outputs, [backend.identity(s) for s in states]\n             # Note: cannot just return states (which can be a problem) ->\n-            # tensorflow/python/ops/resource_variable_ops.py\", line 824, in set_shape\n-            # NotImplementedError: ResourceVariable does not implement set_shape()\n+            # tensorflow/python/ops/resource_variable_ops.py\", line 824, in\n+            # set_shape NotImplementedError: ResourceVariable does not implement\n+            # set_shape()\n \n         inputs_vals = np.random.random(\n             (num_samples, num_timesteps, num_features)\n@@ -1665,8 +1667,8 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n         mask_vals[-1, -1] = 0  # final timestep masked for last sample\n \n         expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n-        # for the last sample, the final timestep (in masked region) should be the\n-        # same as the second to final output (before masked region)\n+        # for the last sample, the final timestep (in masked region) should be\n+        # the same as the second to final output (before masked region)\n         expected_outputs[-1, -1] = expected_outputs[-1, -2]\n \n         inputs = backend.variable(inputs_vals)\n@@ -1969,8 +1971,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(\n         self,\n     ):\n-        # This test only runs in graph because the TF op layer is not supported yet\n-        # for sparse ops.\n+        # This test only runs in graph because the TF op layer is not supported\n+        # yet for sparse ops.\n         t = backend.placeholder()\n         p = backend.placeholder()\n         o = backend.sparse_categorical_crossentropy(t, p)\n@@ -2552,11 +2554,11 @@ class BackendGraphTests(tf.test.TestCase, parameterized.TestCase):\n             self.assertEqual(outs, [11.0, 2.0])\n \n     def test_function_tf_fetches(self):\n-        # Additional operations can be passed to tf.compat.v1.Session().run() via\n-        # its `fetches` arguments. In contrast to `updates` argument of\n+        # Additional operations can be passed to tf.compat.v1.Session().run()\n+        # via its `fetches` arguments. In contrast to `updates` argument of\n         # backend.function() these do not have control dependency on `outputs`\n-        # so they can run in parallel. Also they should not contribute to output of\n-        # backend.function().\n+        # so they can run in parallel. Also they should not contribute to output\n+        # of backend.function().\n         with tf.Graph().as_default(), self.cached_session():\n             x = backend.variable(0.0)\n             y = backend.variable(0.0)\n@@ -2576,11 +2578,11 @@ class BackendGraphTests(tf.test.TestCase, parameterized.TestCase):\n             )\n \n     def test_function_tf_feed_dict(self):\n-        # Additional substitutions can be passed to `tf.compat.v1.Session().run()`\n-        # via its `feed_dict` arguments. Note that the feed_dict is passed once in\n-        # the constructor but we can modify the values in the dictionary. Through\n-        # this feed_dict we can provide additional substitutions besides Keras\n-        # inputs.\n+        # Additional substitutions can be passed to\n+        # `tf.compat.v1.Session().run()` via its `feed_dict` arguments. Note\n+        # that the feed_dict is passed once in the constructor but we can modify\n+        # the values in the dictionary. Through this feed_dict we can provide\n+        # additional substitutions besides Keras inputs.\n         with tf.Graph().as_default(), self.cached_session():\n             x = backend.variable(0.0)\n             y = backend.variable(0.0)\n@@ -2602,7 +2604,8 @@ class BackendGraphTests(tf.test.TestCase, parameterized.TestCase):\n                 backend.get_session().run(fetches=[x, y]), [20.0, 30.0]\n             )\n \n-            # updated value in feed_dict will be modified within the K.function()\n+            # updated value in feed_dict will be modified within the\n+            # K.function()\n             feed_dict[y_placeholder] = 4.0\n             output = f([20.0])\n             self.assertEqual(output, [21.0])\n@@ -2746,8 +2749,8 @@ class ContextValueCacheTest(tf.test.TestCase):\n         cache.setdefault(None, backend.constant(5))\n \n         with tf.Graph().as_default() as g:\n-            # g is not a child graph of the default test context, so the recursive\n-            # lookup will create a new default value.\n+            # g is not a child graph of the default test context, so the\n+            # recursive lookup will create a new default value.\n             self.assertAllEqual(cache[g], 0)\n \n         @tf.function\n@@ -2799,8 +2802,8 @@ class RandomGeneratorTest(tf.test.TestCase, parameterized.TestCase):\n             self.assertIsNotNone(seeded._generator)\n             self.assertIsNotNone(unseeded._generator)\n         else:\n-            # In v1, we can't use tf.random.Generator since it is not compatible with\n-            # graph mode.\n+            # In v1, we can't use tf.random.Generator since it is not compatible\n+            # with graph mode.\n             self.assertIsNone(seeded._generator)\n             self.assertIsNone(unseeded._generator)\n \n@@ -2815,8 +2818,8 @@ class RandomGeneratorTest(tf.test.TestCase, parameterized.TestCase):\n \n         # Make sure even with unseeded backend generator, as long as we set the\n         # keras random seed, it will make the generator to produce the same\n-        # sequence. This will ensure all the client are in sync in the multi-client\n-        # setting, when they all set the keras seed.\n+        # sequence. This will ensure all the client are in sync in the\n+        # multi-client setting, when they all set the keras seed.\n         tf_utils.set_random_seed(keras_seed)\n         gen2 = backend.RandomGenerator(seed=None, rng_type=\"stateful\")\n         output3 = gen2.random_normal(shape=[2, 3])\n\n@@ -211,13 +211,13 @@ class CallbackList:\n \n         Args:\n           callbacks: List of `Callback` instances.\n-          add_history: Whether a `History` callback should be added, if one does not\n-            already exist in the `callbacks` list.\n-          add_progbar: Whether a `ProgbarLogger` callback should be added, if one\n-            does not already exist in the `callbacks` list.\n+          add_history: Whether a `History` callback should be added, if one does\n+            not already exist in the `callbacks` list.\n+          add_progbar: Whether a `ProgbarLogger` callback should be added, if\n+            one does not already exist in the `callbacks` list.\n           model: The `Model` these callbacks are used with.\n-          **params: If provided, parameters will be passed to each `Callback` via\n-            `Callback.set_params`.\n+          **params: If provided, parameters will be passed to each `Callback`\n+            via `Callback.set_params`.\n         \"\"\"\n         self.callbacks = tf.nest.flatten(callbacks) if callbacks else []\n         self._add_default_callbacks(add_history, add_progbar)\n@@ -253,8 +253,9 @@ class CallbackList:\n \n         self._disallow_batch_hooks_in_ps_strategy()\n \n-        # Performance check: Check batch hooks for slowness compared to batch time.\n-        # Only run check for custom callbacks (i.e. not present in this file).\n+        # Performance check: Check batch hooks for slowness compared to batch\n+        # time.  Only run check for custom callbacks (i.e. not present in this\n+        # file).\n         self._check_timing = any(\n             cbk.__class__.__name__ not in globals() for cbk in self.callbacks\n         )\n@@ -318,7 +319,8 @@ class CallbackList:\n             self._call_batch_end_hook(mode, batch, logs)\n         else:\n             raise ValueError(\n-                f'Unrecognized hook: {hook}. Expected values are [\"begin\", \"end\"]'\n+                f\"Unrecognized hook: {hook}. \"\n+                'Expected values are [\"begin\", \"end\"]'\n             )\n \n     def _call_batch_begin_hook(self, mode, batch, logs):\n@@ -425,8 +427,8 @@ class CallbackList:\n \n         Args:\n             epoch: Integer, index of epoch.\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+               method but that may change in the future.\n         \"\"\"\n         logs = self._process_logs(logs)\n         for callback in self.callbacks:\n@@ -440,8 +442,8 @@ class CallbackList:\n         Args:\n             epoch: Integer, index of epoch.\n             logs: Dict, metric results for this training epoch, and for the\n-              validation epoch if validation is performed. Validation result keys\n-              are prefixed with `val_`.\n+              validation epoch if validation is performed. Validation result\n+              keys are prefixed with `val_`.\n         \"\"\"\n         logs = self._process_logs(logs)\n         for callback in self.callbacks:\n@@ -452,9 +454,9 @@ class CallbackList:\n \n         Args:\n             batch: Integer, index of batch within the current epoch.\n-            logs: Dict, contains the return value of `model.train_step`. Typically,\n-              the values of the `Model`'s metrics are returned.  Example:\n-              `{'loss': 0.2, 'accuracy': 0.7}`.\n+            logs: Dict, contains the return value of `model.train_step`.\n+              Typically, the values of the `Model`'s metrics are returned.\n+              Example: `{'loss': 0.2, 'accuracy': 0.7}`.\n         \"\"\"\n         if self._should_call_train_batch_hooks:\n             self._call_batch_hook(ModeKeys.TRAIN, \"begin\", batch, logs=logs)\n@@ -474,9 +476,9 @@ class CallbackList:\n \n         Args:\n             batch: Integer, index of batch within the current epoch.\n-            logs: Dict, contains the return value of `model.test_step`. Typically,\n-              the values of the `Model`'s metrics are returned.  Example:\n-              `{'loss': 0.2, 'accuracy': 0.7}`.\n+            logs: Dict, contains the return value of `model.test_step`.\n+              Typically, the values of the `Model`'s metrics are returned.\n+              Example: `{'loss': 0.2, 'accuracy': 0.7}`.\n         \"\"\"\n         if self._should_call_test_batch_hooks:\n             self._call_batch_hook(ModeKeys.TEST, \"begin\", batch, logs=logs)\n@@ -539,8 +541,8 @@ class CallbackList:\n         \"\"\"Calls the `on_test_begin` methods of its callbacks.\n \n         Args:\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n         \"\"\"\n         logs = self._process_logs(logs)\n         for callback in self.callbacks:\n@@ -561,8 +563,8 @@ class CallbackList:\n         \"\"\"Calls the 'on_predict_begin` methods of its callbacks.\n \n         Args:\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n         \"\"\"\n         logs = self._process_logs(logs)\n         for callback in self.callbacks:\n@@ -615,8 +617,8 @@ class Callback:\n     `predict` in order to hook into the various stages of the model training and\n     inference lifecycle.\n \n-    To create a custom callback, subclass `keras.callbacks.Callback` and override\n-    the method associated with the stage of interest. See\n+    To create a custom callback, subclass `keras.callbacks.Callback` and\n+    override the method associated with the stage of interest. See\n     https://www.tensorflow.org/guide/keras/custom_callback for more information.\n \n     Example:\n@@ -626,7 +628,8 @@ class Callback:\n     ...   def on_train_end(self, logs=None):\n     ...     global training_finished\n     ...     training_finished = True\n-    >>> model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\n+    >>> model = tf.keras.Sequential([\n+    ...     tf.keras.layers.Dense(1, input_shape=(1,))])\n     >>> model.compile(loss='mean_squared_error')\n     >>> model.fit(tf.constant([[1.0]]), tf.constant([[1.0]]),\n     ...           callbacks=[MyCallback()])\n@@ -696,29 +699,29 @@ class Callback:\n     def on_epoch_begin(self, epoch, logs=None):\n         \"\"\"Called at the start of an epoch.\n \n-        Subclasses should override for any actions to run. This function should only\n-        be called during TRAIN mode.\n+        Subclasses should override for any actions to run. This function should\n+        only be called during TRAIN mode.\n \n         Args:\n             epoch: Integer, index of epoch.\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n         \"\"\"\n \n     @doc_controls.for_subclass_implementers\n     def on_epoch_end(self, epoch, logs=None):\n         \"\"\"Called at the end of an epoch.\n \n-        Subclasses should override for any actions to run. This function should only\n-        be called during TRAIN mode.\n+        Subclasses should override for any actions to run. This function should\n+        only be called during TRAIN mode.\n \n         Args:\n             epoch: Integer, index of epoch.\n             logs: Dict, metric results for this training epoch, and for the\n-              validation epoch if validation is performed. Validation result keys\n-              are prefixed with `val_`. For training epoch, the values of the\n-             `Model`'s metrics are returned. Example : `{'loss': 0.2, 'accuracy':\n-               0.7}`.\n+              validation epoch if validation is performed. Validation result\n+              keys are prefixed with `val_`. For training epoch, the values of\n+              the `Model`'s metrics are returned. Example:\n+              `{'loss': 0.2, 'accuracy': 0.7}`.\n         \"\"\"\n \n     @doc_controls.for_subclass_implementers\n@@ -729,13 +732,13 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Note that if the `steps_per_execution` argument to `compile` in\n-        `tf.keras.Model` is set to `N`, this method will only be called every `N`\n-        batches.\n+        `tf.keras.Model` is set to `N`, this method will only be called every\n+        `N` batches.\n \n         Args:\n             batch: Integer, index of batch within the current epoch.\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n         \"\"\"\n         # For backwards compatibility.\n         self.on_batch_begin(batch, logs=logs)\n@@ -748,8 +751,8 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Note that if the `steps_per_execution` argument to `compile` in\n-        `tf.keras.Model` is set to `N`, this method will only be called every `N`\n-        batches.\n+        `tf.keras.Model` is set to `N`, this method will only be called every\n+        `N` batches.\n \n         Args:\n             batch: Integer, index of batch within the current epoch.\n@@ -769,13 +772,13 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Note that if the `steps_per_execution` argument to `compile` in\n-        `tf.keras.Model` is set to `N`, this method will only be called every `N`\n-        batches.\n+        `tf.keras.Model` is set to `N`, this method will only be called every\n+        `N` batches.\n \n         Args:\n             batch: Integer, index of batch within the current epoch.\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n         \"\"\"\n \n     @doc_controls.for_subclass_implementers\n@@ -789,8 +792,8 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Note that if the `steps_per_execution` argument to `compile` in\n-        `tf.keras.Model` is set to `N`, this method will only be called every `N`\n-        batches.\n+        `tf.keras.Model` is set to `N`, this method will only be called every\n+        `N` batches.\n \n         Args:\n             batch: Integer, index of batch within the current epoch.\n@@ -805,13 +808,13 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Note that if the `steps_per_execution` argument to `compile` in\n-        `tf.keras.Model` is set to `N`, this method will only be called every `N`\n-        batches.\n+        `tf.keras.Model` is set to `N`, this method will only be called every\n+        `N` batches.\n \n         Args:\n             batch: Integer, index of batch within the current epoch.\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n         \"\"\"\n \n     @doc_controls.for_subclass_implementers\n@@ -822,8 +825,8 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Note that if the `steps_per_execution` argument to `compile` in\n-        `tf.keras.Model` is set to `N`, this method will only be called every `N`\n-        batches.\n+        `tf.keras.Model` is set to `N`, this method will only be called every\n+        `N` batches.\n \n         Args:\n             batch: Integer, index of batch within the current epoch.\n@@ -837,8 +840,8 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Args:\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n         \"\"\"\n \n     @doc_controls.for_subclass_implementers\n@@ -848,9 +851,9 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Args:\n-            logs: Dict. Currently the output of the last call to `on_epoch_end()`\n-              is passed to this argument for this method but that may change in\n-              the future.\n+            logs: Dict. Currently the output of the last call to\n+              `on_epoch_end()` is passed to this argument for this method but\n+              that may change in the future.\n         \"\"\"\n \n     @doc_controls.for_subclass_implementers\n@@ -860,8 +863,8 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Args:\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n         \"\"\"\n \n     @doc_controls.for_subclass_implementers\n@@ -883,8 +886,8 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Args:\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n         \"\"\"\n \n     @doc_controls.for_subclass_implementers\n@@ -894,8 +897,8 @@ class Callback:\n         Subclasses should override for any actions to run.\n \n         Args:\n-            logs: Dict. Currently no data is passed to this argument for this method\n-              but that may change in the future.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n         \"\"\"\n \n     def _implements_train_batch_hooks(self):\n@@ -914,7 +917,8 @@ class Callback:\n         ) or not generic_utils.is_default(self.on_test_batch_end)\n \n     def _implements_predict_batch_hooks(self):\n-        \"\"\"Determines if this Callback should be called for each predict batch.\"\"\"\n+        \"\"\"Determines if this Callback should be called for each predict\n+        batch.\"\"\"\n         return not generic_utils.is_default(\n             self.on_predict_batch_begin\n         ) or not generic_utils.is_default(self.on_predict_batch_end)\n@@ -945,7 +949,8 @@ class BaseLogger(Callback):\n         logs = logs or {}\n         batch_size = logs.get(\"size\", 0)\n         # In case of distribution strategy we can potentially run multiple steps\n-        # at the same time, we should account for that in the `seen` calculation.\n+        # at the same time, we should account for that in the `seen`\n+        # calculation.\n         num_steps = logs.get(\"num_steps\", 1)\n         self.seen += batch_size * num_steps\n \n@@ -1107,15 +1112,17 @@ class ProgbarLogger(Callback):\n         self.progbar = None\n \n     def _maybe_init_progbar(self):\n-        \"\"\"Instantiate a `Progbar` if not yet, and update the stateful metrics.\"\"\"\n+        \"\"\"Instantiate a `Progbar` if not yet, and update the stateful\n+        metrics.\"\"\"\n         # TODO(rchao): Legacy TF1 code path may use list for\n-        # `self.stateful_metrics`. Remove \"cast to set\" when TF1 support is dropped.\n+        # `self.stateful_metrics`. Remove \"cast to set\" when TF1 support is\n+        # dropped.\n         self.stateful_metrics = set(self.stateful_metrics)\n \n         if self.model:\n-            # Update the existing stateful metrics as `self.model.metrics` may contain\n-            # updated metrics after `MetricsContainer` is built in the first train\n-            # step.\n+            # Update the existing stateful metrics as `self.model.metrics` may\n+            # contain updated metrics after `MetricsContainer` is built in the\n+            # first train step.\n             self.stateful_metrics = self.stateful_metrics.union(\n                 set(m.name for m in self.model.metrics)\n             )\n@@ -1219,8 +1226,8 @@ class ModelCheckpoint(Callback):\n \n     `ModelCheckpoint` callback is used in conjunction with training using\n     `model.fit()` to save a model or weights (in a checkpoint file) at some\n-    interval, so the model or weights can be loaded later to continue the training\n-    from the state saved.\n+    interval, so the model or weights can be loaded later to continue the\n+    training from the state saved.\n \n     A few options this callback provides include:\n \n@@ -1229,8 +1236,8 @@ class ModelCheckpoint(Callback):\n       performance.\n     - Definition of 'best'; which quantity to monitor and whether it should be\n       maximized or minimized.\n-    - The frequency it should save at. Currently, the callback supports saving at\n-      the end of every epoch, or after a fixed number of training batches.\n+    - The frequency it should save at. Currently, the callback supports saving\n+      at the end of every epoch, or after a fixed number of training batches.\n     - Whether only weights are saved, or the whole model is saved.\n \n     Note: If you get `WARNING:tensorflow:Can save best model only with <name>\n@@ -1256,21 +1263,22 @@ class ModelCheckpoint(Callback):\n     # so far.\n     model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n \n-    # The model weights (that are considered the best) are loaded into the model.\n+    # The model weights (that are considered the best) are loaded into the\n+    # model.\n     model.load_weights(checkpoint_filepath)\n     ```\n \n     Args:\n         filepath: string or `PathLike`, path to save the model file. e.g.\n           filepath = os.path.join(working_dir, 'ckpt', file_name). `filepath`\n-          can contain named formatting options, which will be filled the value of\n-          `epoch` and keys in `logs` (passed in `on_epoch_end`). For example: if\n-          `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the model\n-          checkpoints will be saved with the epoch number and the validation loss\n-          in the filename. The directory of the filepath should not be reused by\n-          any other callbacks to avoid conflicts.\n-        monitor: The metric name to monitor. Typically the metrics are set by the\n-          `Model.compile` method. Note:\n+          can contain named formatting options, which will be filled the value\n+          of `epoch` and keys in `logs` (passed in `on_epoch_end`). For example:\n+          if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the\n+          model checkpoints will be saved with the epoch number and the\n+          validation loss in the filename. The directory of the filepath should\n+          not be reused by any other callbacks to avoid conflicts.\n+        monitor: The metric name to monitor. Typically the metrics are set by\n+          the `Model.compile` method. Note:\n \n           * Prefix the name with `\"val_`\" to monitor validation metrics.\n           * Use `\"loss\"` or \"`val_loss`\" to monitor the model's total loss.\n@@ -1300,21 +1308,21 @@ class ModelCheckpoint(Callback):\n         save_weights_only: if True, then only the model's weights will be saved\n           (`model.save_weights(filepath)`), else the full model is saved\n           (`model.save(filepath)`).\n-        save_freq: `'epoch'` or integer. When using `'epoch'`, the callback saves\n-          the model after each epoch. When using integer, the callback saves the\n-          model at end of this many batches. If the `Model` is compiled with\n-          `steps_per_execution=N`, then the saving criteria will be\n-          checked every Nth batch. Note that if the saving isn't aligned to\n+        save_freq: `'epoch'` or integer. When using `'epoch'`, the callback\n+          saves the model after each epoch. When using integer, the callback\n+          saves the model at end of this many batches. If the `Model` is\n+          compiled with `steps_per_execution=N`, then the saving criteria will\n+          be checked every Nth batch. Note that if the saving isn't aligned to\n           epochs, the monitored metric may potentially be less reliable (it\n           could reflect as little as 1 batch, since the metrics get reset every\n           epoch). Defaults to `'epoch'`.\n         options: Optional `tf.train.CheckpointOptions` object if\n           `save_weights_only` is true or optional `tf.saved_model.SaveOptions`\n           object if `save_weights_only` is false.\n-        initial_value_threshold: Floating point initial \"best\" value of the metric\n-          to be monitored. Only applies if `save_best_value=True`. Only overwrites\n-          the model weights already saved if the performance of current\n-          model is better than this value.\n+        initial_value_threshold: Floating point initial \"best\" value of the\n+          metric to be monitored. Only applies if `save_best_value=True`. Only\n+          overwrites the model weights already saved if the performance of\n+          current model is better than this value.\n         **kwargs: Additional arguments for backwards compatibility. Possible key\n           is `period`.\n     \"\"\"\n@@ -1353,7 +1361,8 @@ class ModelCheckpoint(Callback):\n             else:\n                 raise TypeError(\n                     \"If save_weights_only is True, then `options` must be \"\n-                    f\"either None or a tf.train.CheckpointOptions. Got {options}.\"\n+                    f\"either None or a tf.train.CheckpointOptions. \"\n+                    f\"Got {options}.\"\n                 )\n         else:\n             if options is None or isinstance(\n@@ -1363,11 +1372,12 @@ class ModelCheckpoint(Callback):\n             else:\n                 raise TypeError(\n                     \"If save_weights_only is False, then `options` must be \"\n-                    f\"either None or a tf.saved_model.SaveOptions. Got {options}.\"\n+                    f\"either None or a tf.saved_model.SaveOptions. \"\n+                    f\"Got {options}.\"\n                 )\n \n-        # Deprecated field `load_weights_on_restart` is for loading the checkpoint\n-        # file from `filepath` at the start of `model.fit()`\n+        # Deprecated field `load_weights_on_restart` is for loading the\n+        # checkpoint file from `filepath` at the start of `model.fit()`\n         # TODO(rchao): Remove the arg during next breaking release.\n         if \"load_weights_on_restart\" in kwargs:\n             self.load_weights_on_restart = kwargs[\"load_weights_on_restart\"]\n@@ -1436,13 +1446,14 @@ class ModelCheckpoint(Callback):\n                 filepath_to_load\n             ):\n                 try:\n-                    # `filepath` may contain placeholders such as `{epoch:02d}`, and\n-                    # thus it attempts to load the most recently modified file with file\n-                    # name matching the pattern.\n+                    # `filepath` may contain placeholders such as `{epoch:02d}`,\n+                    # and thus it attempts to load the most recently modified\n+                    # file with file name matching the pattern.\n                     self.model.load_weights(filepath_to_load)\n                 except (IOError, ValueError) as e:\n                     raise ValueError(\n-                        f\"Error loading file from {filepath_to_load}. Reason: {e}\"\n+                        f\"Error loading file from {filepath_to_load}. \"\n+                        f\"Reason: {e}\"\n                     )\n \n     def _implements_train_batch_hooks(self):\n@@ -1512,7 +1523,8 @@ class ModelCheckpoint(Callback):\n                         if self.monitor_op(current, self.best):\n                             if self.verbose > 0:\n                                 io_utils.print_msg(\n-                                    f\"\\nEpoch {epoch + 1}: {self.monitor} improved \"\n+                                    f\"\\nEpoch {epoch + 1}: {self.monitor} \"\n+                                    \"improved \"\n                                     f\"from {self.best:.5f} to {current:.5f}, \"\n                                     f\"saving model to {filepath}\"\n                                 )\n@@ -1533,7 +1545,8 @@ class ModelCheckpoint(Callback):\n                             if self.verbose > 0:\n                                 io_utils.print_msg(\n                                     f\"\\nEpoch {epoch + 1}: \"\n-                                    f\"{self.monitor} did not improve from {self.best:.5f}\"\n+                                    f\"{self.monitor} did not improve \"\n+                                    f\"from {self.best:.5f}\"\n                                 )\n                 else:\n                     if self.verbose > 0:\n@@ -1557,7 +1570,8 @@ class ModelCheckpoint(Callback):\n                     f\"directory: {filepath}\"\n                 )\n             except IOError as e:  # h5py 2.x\n-                # `e.errno` appears to be `None` so checking the content of `e.args[0]`.\n+                # `e.errno` appears to be `None` so checking the content of\n+                # `e.args[0]`.\n                 if \"is a directory\" in str(e.args[0]).lower():\n                     raise IOError(\n                         \"Please specify a non-directory filepath for \"\n@@ -1571,9 +1585,10 @@ class ModelCheckpoint(Callback):\n         \"\"\"Returns the file path for checkpoint.\"\"\"\n         # pylint: disable=protected-access\n         try:\n-            # `filepath` may contain placeholders such as `{epoch:02d}`,`{batch:02d}`\n-            # and `{mape:.2f}`. A mismatch between logged metrics and the path's\n-            # placeholders can cause formatting to fail.\n+            # `filepath` may contain placeholders such as\n+            # `{epoch:02d}`,`{batch:02d}` and `{mape:.2f}`. A mismatch between\n+            # logged metrics and the path's placeholders can cause formatting to\n+            # fail.\n             if batch is None or \"batch\" in logs:\n                 file_path = self.filepath.format(epoch=epoch + 1, **logs)\n             else:\n@@ -1591,9 +1606,9 @@ class ModelCheckpoint(Callback):\n         return self._write_filepath\n \n     def _maybe_remove_file(self):\n-        # Remove the checkpoint directory in multi-worker training where this worker\n-        # should not checkpoint. It is a dummy directory previously saved for sync\n-        # distributed training.\n+        # Remove the checkpoint directory in multi-worker training where this\n+        # worker should not checkpoint. It is a dummy directory previously saved\n+        # for sync distributed training.\n         distributed_file_utils.remove_temp_dir_with_filepath(\n             self._write_filepath, self.model.distribute_strategy\n         )\n@@ -1612,18 +1627,19 @@ class ModelCheckpoint(Callback):\n         \"\"\"Returns the most recently modified filepath matching pattern.\n \n         Pattern may contain python formatting placeholder. If\n-        `tf.train.latest_checkpoint()` does not return None, use that; otherwise,\n-        check for most recently modified one that matches the pattern.\n+        `tf.train.latest_checkpoint()` does not return None, use that;\n+        otherwise, check for most recently modified one that matches the\n+        pattern.\n \n-        In the rare case where there are more than one pattern-matching file having\n-        the same modified time that is most recent among all, return the filepath\n-        that is largest (by `>` operator, lexicographically using the numeric\n-        equivalents). This provides a tie-breaker when multiple files are most\n-        recent. Note that a larger `filepath` can sometimes indicate a later time of\n-        modification (for instance, when epoch/batch is used as formatting option),\n-        but not necessarily (when accuracy or loss is used). The tie-breaker is\n-        put in the logic as best effort to return the most recent, and to avoid\n-        undeterministic result.\n+        In the rare case where there are more than one pattern-matching file\n+        having the same modified time that is most recent among all, return the\n+        filepath that is largest (by `>` operator, lexicographically using the\n+        numeric equivalents). This provides a tie-breaker when multiple files\n+        are most recent. Note that a larger `filepath` can sometimes indicate a\n+        later time of modification (for instance, when epoch/batch is used as\n+        formatting option), but not necessarily (when accuracy or loss is used).\n+        The tie-breaker is put in the logic as best effort to return the most\n+        recent, and to avoid undeterministic result.\n \n         Modified time of a file is obtained with `os.path.getmtime()`.\n \n@@ -1635,7 +1651,8 @@ class ModelCheckpoint(Callback):\n         path_pattern = os.path.join(test_dir, file_pattern)\n         file_paths = [\n             os.path.join(test_dir, file_name) for file_name in\n-            ['f.batch03epoch02.h5', 'f.batch02epoch02.h5', 'f.batch01epoch01.h5']\n+            ['f.batch03epoch02.h5',\n+             'f.batch02epoch02.h5', 'f.batch01epoch01.h5']\n         ]\n         for file_path in file_paths:\n           # Write something to each of the files\n@@ -1645,21 +1662,21 @@ class ModelCheckpoint(Callback):\n         ```\n \n         Args:\n-            pattern: The file pattern that may optionally contain python placeholder\n-                such as `{epoch:02d}`.\n+            pattern: The file pattern that may optionally contain python\n+                placeholder such as `{epoch:02d}`.\n \n         Returns:\n-            The most recently modified file's full filepath matching `pattern`. If\n-            `pattern` does not contain any placeholder, this returns the filepath\n-            that\n-            exactly matches `pattern`. Returns `None` if no match is found.\n+            The most recently modified file's full filepath matching `pattern`.\n+            If `pattern` does not contain any placeholder, this returns the\n+            filepath that exactly matches `pattern`. Returns `None` if no match\n+            is found.\n         \"\"\"\n         dir_name = os.path.dirname(pattern)\n         base_name = os.path.basename(pattern)\n         base_name_regex = \"^\" + re.sub(r\"{.*}\", r\".*\", base_name) + \"$\"\n \n-        # If tf.train.latest_checkpoint tells us there exists a latest checkpoint,\n-        # use that as it is more robust than `os.path.getmtime()`.\n+        # If tf.train.latest_checkpoint tells us there exists a latest\n+        # checkpoint, use that as it is more robust than `os.path.getmtime()`.\n         latest_tf_checkpoint = tf.train.latest_checkpoint(dir_name)\n         if latest_tf_checkpoint is not None and re.match(\n             base_name_regex, os.path.basename(latest_tf_checkpoint)\n@@ -1685,21 +1702,22 @@ class ModelCheckpoint(Callback):\n                     if mod_time > latest_mod_time:\n                         latest_mod_time = mod_time\n                         file_path_with_latest_mod_time = file_path\n-                        # In the case a file with later modified time is found, reset\n-                        # the counter for the number of files with latest modified time.\n+                        # In the case a file with later modified time is found,\n+                        # reset the counter for the number of files with latest\n+                        # modified time.\n                         n_file_with_latest_mod_time = 1\n                     elif mod_time == latest_mod_time:\n-                        # In the case a file has modified time tied with the most recent,\n-                        # increment the counter for the number of files with latest modified\n-                        # time by 1.\n+                        # In the case a file has modified time tied with the\n+                        # most recent, increment the counter for the number of\n+                        # files with latest modified time by 1.\n                         n_file_with_latest_mod_time += 1\n \n         if n_file_with_latest_mod_time == 1:\n             # Return the sole file that has most recent modified time.\n             return file_path_with_latest_mod_time\n         else:\n-            # If there are more than one file having latest modified time, return\n-            # the file path with the largest file name.\n+            # If there are more than one file having latest modified time,\n+            # return the file path with the largest file name.\n             return file_path_with_largest_file_name\n \n \n@@ -1711,20 +1729,20 @@ class BackupAndRestore(Callback):\n     interruption that has happened in the middle of a `Model.fit` execution, by\n     backing up the training states in a temporary checkpoint file (with the help\n     of a `tf.train.CheckpointManager`), at the end of each epoch. Each backup\n-    overwrites the previously written checkpoint file, so at any given time there\n-    is at most one such checkpoint file for backup/restoring purpose.\n+    overwrites the previously written checkpoint file, so at any given time\n+    there is at most one such checkpoint file for backup/restoring purpose.\n \n-    If training restarts before completion, the training state (which includes the\n-    `Model` weights and epoch number) is restored to the most recently saved state\n-    at the beginning of a new `Model.fit` run. At the completion of a `Model.fit`\n-    run, the temporary checkpoint file is deleted.\n+    If training restarts before completion, the training state (which includes\n+    the `Model` weights and epoch number) is restored to the most recently saved\n+    state at the beginning of a new `Model.fit` run. At the completion of a\n+    `Model.fit` run, the temporary checkpoint file is deleted.\n \n     Note that the user is responsible to bring jobs back after the interruption.\n     This callback is important for the backup and restore mechanism for fault\n-    tolerance purpose, and the model to be restored from an previous checkpoint is\n-    expected to be the same as the one used to back up. If user changes arguments\n-    passed to compile or fit, the checkpoint saved for fault tolerance can become\n-    invalid.\n+    tolerance purpose, and the model to be restored from an previous checkpoint\n+    is expected to be the same as the one used to back up. If user changes\n+    arguments passed to compile or fit, the checkpoint saved for fault tolerance\n+    can become invalid.\n \n     Note:\n \n@@ -1733,10 +1751,11 @@ class BackupAndRestore(Callback):\n     `Model.fit` redoes any partial work during the unfinished epoch in which the\n     training got restarted (so the work done before the interruption doesn't\n     affect the final model state).\n-    3. This works for both single worker and multi-worker modes. When `Model.fit`\n-    is used with `tf.distribute`, it supports `tf.distribute.MirroredStrategy`,\n-    `tf.distribute.MultiWorkerMirroredStrategy`, `tf.distribute.TPUStrategy`, and\n-    `tf.distribute.experimental.ParameterServerStrategy`.\n+    3. This works for both single worker and multi-worker modes. When\n+    `Model.fit` is used with `tf.distribute`, it supports\n+    `tf.distribute.MirroredStrategy`,\n+    `tf.distribute.MultiWorkerMirroredStrategy`, `tf.distribute.TPUStrategy`,\n+    and `tf.distribute.experimental.ParameterServerStrategy`.\n \n     Example:\n \n@@ -1753,8 +1772,9 @@ class BackupAndRestore(Callback):\n     ...             verbose=0)\n     ... except:\n     ...   pass\n-    >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,\n-    ...             batch_size=1, callbacks=[callback], verbose=0)\n+    >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n+    ...                     epochs=10, batch_size=1, callbacks=[callback],\n+    ...                     verbose=0)\n     >>> # Only 6 more epochs are run, since first trainning got interrupted at\n     >>> # zero-indexed epoch 4, second training will continue from 4 to 9.\n     >>> len(history.history['loss'])\n@@ -1793,7 +1813,8 @@ class BackupAndRestore(Callback):\n                     \"BackupAndRestore only supports eager mode. In graph \"\n                     \"mode, consider using ModelCheckpoint to manually save \"\n                     \"and restore weights with `model.load_weights()` and by \"\n-                    \"providing `initial_epoch` in `model.fit()` for fault tolerance.\"\n+                    \"providing `initial_epoch` in `model.fit()` for fault \"\n+                    \"tolerance.\"\n                 )\n \n         # Only the chief worker writes model checkpoints, but all workers\n@@ -1810,7 +1831,8 @@ class BackupAndRestore(Callback):\n         ):\n             raise NotImplementedError(\n                 f\"{type(self.model.distribute_strategy)} is not supported yet. \"\n-                \"Currently BackupAndRestore callback only supports empty strategy, \"\n+                \"Currently BackupAndRestore callback \"\n+                \"only supports empty strategy, \"\n                 \"MirroredStrategy, MultiWorkerMirroredStrategy and TPUStrategy.\"\n             )\n         self.model._training_state = worker_training_state.WorkerTrainingState(\n@@ -1821,8 +1843,8 @@ class BackupAndRestore(Callback):\n \n     def on_train_end(self, logs=None):\n         # pylint: disable=protected-access\n-        # On exit of training, delete the training state backup file that was saved\n-        # for the purpose of worker recovery.\n+        # On exit of training, delete the training state backup file that was\n+        # saved for the purpose of worker recovery.\n         self._training_state.delete_backup()\n \n         # Clean up the training state.\n@@ -1981,7 +2003,8 @@ class EarlyStopping(Callback):\n             self.best_epoch = epoch\n             if self.restore_best_weights:\n                 self.best_weights = self.model.get_weights()\n-            # Only restart wait if we beat both the baseline and our previous best.\n+            # Only restart wait if we beat both the baseline and our previous\n+            # best.\n             if self.baseline is None or self._is_improvement(\n                 current, self.baseline\n             ):\n@@ -1994,7 +2017,8 @@ class EarlyStopping(Callback):\n             if self.restore_best_weights and self.best_weights is not None:\n                 if self.verbose > 0:\n                     io_utils.print_msg(\n-                        \"Restoring model weights from the end of the best epoch: \"\n+                        \"Restoring model weights from \"\n+                        \"the end of the best epoch: \"\n                         f\"{self.best_epoch + 1}.\"\n                     )\n                 self.model.set_weights(self.best_weights)\n@@ -2096,10 +2120,10 @@ class RemoteMonitor(Callback):\n class LearningRateScheduler(Callback):\n     \"\"\"Learning rate scheduler.\n \n-    At the beginning of every epoch, this callback gets the updated learning rate\n-    value from `schedule` function provided at `__init__`, with the current epoch\n-    and current learning rate, and applies the updated learning rate\n-    on the optimizer.\n+    At the beginning of every epoch, this callback gets the updated learning\n+    rate value from `schedule` function provided at `__init__`, with the current\n+    epoch and current learning rate, and applies the updated learning rate on\n+    the optimizer.\n \n     Args:\n       schedule: a function that takes an epoch index (integer, indexed from 0)\n@@ -2168,16 +2192,16 @@ def keras_model_summary(name, data, step=None):\n     \"\"\"Writes a Keras model as JSON to as a Summary.\n \n     Writing the Keras model configuration allows the TensorBoard graph plugin to\n-    render a conceptual graph, as opposed to graph of ops. In case the model fails\n-    to serialize as JSON, it ignores and returns False.\n+    render a conceptual graph, as opposed to graph of ops. In case the model\n+    fails to serialize as JSON, it ignores and returns False.\n \n     Args:\n-      name: A name for this summary. The summary tag used for TensorBoard will be\n-        this name prefixed by any active name scopes.\n+      name: A name for this summary. The summary tag used for TensorBoard will\n+        be this name prefixed by any active name scopes.\n       data: A Keras Model to write.\n       step: Explicit `int64`-castable monotonic step value for this summary. If\n-        omitted, this defaults to `tf.summary.experimental.get_step()`, which must\n-        not be None.\n+        omitted, this defaults to `tf.summary.experimental.get_step()`, which\n+        must not be None.\n \n     Returns:\n       True on success, or False if no summary was written because no default\n@@ -2244,8 +2268,8 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n \n     Args:\n         log_dir: the path of the directory where to save the log files to be\n-          parsed by TensorBoard. e.g. log_dir = os.path.join(working_dir, 'logs')\n-          This directory should not be reused by any other callbacks.\n+          parsed by TensorBoard. e.g. log_dir = os.path.join(working_dir,\n+          'logs') This directory should not be reused by any other callbacks.\n         histogram_freq: frequency (in epochs) at which to compute\n           weight histograms for the layers of the model. If set to 0, histograms\n           won't be computed. Validation data (or split) must be specified for\n@@ -2254,11 +2278,12 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n           can become quite large when write_graph is set to True.\n         write_images: whether to write model weights to visualize as image in\n           TensorBoard.\n-        write_steps_per_second: whether to log the training steps per second into\n-          Tensorboard. This supports both epoch and batch frequency logging.\n+        write_steps_per_second: whether to log the training steps per second\n+          into Tensorboard. This supports both epoch and batch frequency\n+          logging.\n         update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n-          writes the losses and metrics to TensorBoard after each batch. The same\n-          applies for `'epoch'`. If using an integer, let's say `1000`, the\n+          writes the losses and metrics to TensorBoard after each batch. The\n+          same applies for `'epoch'`. If using an integer, let's say `1000`, the\n           callback will write the metrics and losses to TensorBoard every 1000\n           batches. Note that writing too frequently to TensorBoard can slow down\n           your training.\n@@ -2299,9 +2324,9 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     model = MyModel()\n     model.compile('sgd', 'mse')\n \n-    # Make sure to set `update_freq=N` to log a batch-level summary every N batches.\n-    # In addition to any `tf.summary` contained in `Model.call`, metrics added in\n-    # `Model.compile` will be logged every N batches.\n+    # Make sure to set `update_freq=N` to log a batch-level summary every N\n+    # batches.  In addition to any `tf.summary` contained in `Model.call`,\n+    # metrics added in `Model.compile` will be logged every N batches.\n     tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n     model.fit(x_train, y_train, callbacks=[tb_callback])\n     ```\n@@ -2319,9 +2344,9 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     model = tf.keras.Model(inputs, outputs)\n     model.compile('sgd', 'mse')\n \n-    # Make sure to set `update_freq=N` to log a batch-level summary every N batches.\n-    # In addition to any `tf.summary` contained in `Model.call`, metrics added in\n-    # `Model.compile` will be logged every N batches.\n+    # Make sure to set `update_freq=N` to log a batch-level summary every N\n+    # batches. In addition to any `tf.summary` contained in `Model.call`,\n+    # metrics added in `Model.compile` will be logged every N batches.\n     tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n     model.fit(x_train, y_train, callbacks=[tb_callback])\n     ```\n@@ -2419,7 +2444,8 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         if unrecognized_kwargs:\n             raise ValueError(\n                 \"Unrecognized arguments in `TensorBoard` Callback: \"\n-                f\"{unrecognized_kwargs}. Supported kwargs are: {supported_kwargs}\"\n+                f\"{unrecognized_kwargs}. \"\n+                f\"Supported kwargs are: {supported_kwargs}\"\n             )\n \n     def set_model(self, model):\n@@ -2477,7 +2503,8 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         with self._train_writer.as_default():\n             with tf.summary.record_if(True):\n                 train_fn = self.model.train_tf_function\n-                # If the train_function is a `tf.function`, we can write out a graph\n+                # If the train_function is a `tf.function`, we can write out a\n+                # graph\n                 if hasattr(train_fn, \"function_spec\"):\n                     tf.summary.graph(\n                         train_fn._concrete_stateful_fn.graph\n@@ -2489,8 +2516,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n             with tf.summary.record_if(True):\n                 summary_writable = (\n                     self.model._is_graph_network\n-                    or self.model.__class__.__name__  # pylint: disable=protected-access\n-                    == \"Sequential\"\n+                    or self.model.__class__.__name__ == \"Sequential\"\n                 )  # pylint: disable=protected-access\n                 if summary_writable:\n                     keras_model_summary(\"keras\", self.model, step=0)\n@@ -2506,8 +2532,8 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         for layer in self.model.layers:\n             if isinstance(layer, core.Embedding):\n                 embedding = config.embeddings.add()\n-                # Embeddings are always the first layer, so this naming should be\n-                # consistent in any keras models checkpoints.\n+                # Embeddings are always the first layer, so this naming should\n+                # be consistent in any keras models checkpoints.\n                 name = (\n                     \"layer_with_weights-0/embeddings/.ATTRIBUTES/VARIABLE_VALUE\"\n                 )\n@@ -2556,8 +2582,8 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         if self.update_freq == \"epoch\":\n             return\n \n-        # See _push_writer for the content of the previous_context, which is pair\n-        # of context.\n+        # See _push_writer for the content of the previous_context, which is\n+        # pair of context.\n         previous_context = self._prev_summary_state.pop()\n         previous_context[1].__exit__(*sys.exc_info())\n         previous_context[0].__exit__(*sys.exc_info())\n@@ -2574,18 +2600,21 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         Setting `profile_batch=0` disables profiling.\n \n         Args:\n-          profile_batch: The range of batches to profile. Should be a non-negative\n-            integer or a comma separated string of pair of positive integers. A pair\n-            of positive integers signify a range of batches to profile.\n+          profile_batch: The range of batches to profile. Should be a\n+            non-negative integer or a comma separated string of pair of positive\n+            integers. A pair of positive integers signify a range of batches to\n+            profile.\n \n         Raises:\n-          ValueError: If profile_batch is not an integer or a comma separated pair\n-                      of positive integers.\n+          ValueError: If profile_batch is not an integer or a comma separated\n+            pair of positive integers.\n \n         \"\"\"\n         profile_batch_error_message = (\n-            \"profile_batch must be a non-negative integer or 2-tuple of positive \"\n-            \"integers. A pair of positive integers signifies a range of batches \"\n+            \"profile_batch must be a non-negative integer or \"\n+            \"2-tuple of positive \"\n+            \"integers. A pair of positive integers \"\n+            \"signifies a range of batches \"\n             f\"to profile. Found: {profile_batch}\"\n         )\n \n@@ -2652,7 +2681,8 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         self._pop_writer()\n \n     def _implements_train_batch_hooks(self):\n-        # Only call batch hooks when tracing or write_steps_per_second are enabled\n+        # Only call batch hooks when tracing or write_steps_per_second are\n+        # enabled\n         return self._should_trace or self.write_steps_per_second\n \n     def on_train_batch_begin(self, batch, logs=None):\n@@ -2770,7 +2800,8 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n                             histogram_weight_name, weight, step=epoch\n                         )\n                         if self.write_images:\n-                            # Add a suffix to prevent summary tag name collision.\n+                            # Add a suffix to prevent summary tag name\n+                            # collision.\n                             image_weight_name = weight_name + \"/image\"\n                             self._log_weight_as_image(\n                                 weight, image_weight_name, epoch\n@@ -2868,13 +2899,13 @@ class ReduceLROnPlateau(Callback):\n         mode: one of `{'auto', 'min', 'max'}`. In `'min'` mode,\n           the learning rate will be reduced when the\n           quantity monitored has stopped decreasing; in `'max'` mode it will be\n-          reduced when the quantity monitored has stopped increasing; in `'auto'`\n-          mode, the direction is automatically inferred from the name of the\n-          monitored quantity.\n+          reduced when the quantity monitored has stopped increasing; in\n+          `'auto'` mode, the direction is automatically inferred from the name\n+          of the monitored quantity.\n         min_delta: threshold for measuring the new optimum, to only focus on\n           significant changes.\n-        cooldown: number of epochs to wait before resuming normal operation after\n-          lr has been reduced.\n+        cooldown: number of epochs to wait before resuming normal operation\n+          after lr has been reduced.\n         min_lr: lower bound on the learning rate.\n     \"\"\"\n \n@@ -2895,7 +2926,8 @@ class ReduceLROnPlateau(Callback):\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError(\n-                f\"ReduceLROnPlateau does not support a factor >= 1.0. Got {factor}\"\n+                f\"ReduceLROnPlateau does not support \"\n+                f\"a factor >= 1.0. Got {factor}\"\n             )\n         if \"epsilon\" in kwargs:\n             min_delta = kwargs.pop(\"epsilon\")\n@@ -2970,7 +3002,8 @@ class ReduceLROnPlateau(Callback):\n                         if self.verbose > 0:\n                             io_utils.print_msg(\n                                 f\"\\nEpoch {epoch +1}: \"\n-                                f\"ReduceLROnPlateau reducing learning rate to {new_lr}.\"\n+                                f\"ReduceLROnPlateau reducing \"\n+                                f\"learning rate to {new_lr}.\"\n                             )\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n\n@@ -374,7 +374,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         cbk = BackupAndRestore(self.get_temp_dir())\n \n         class InterruptingCallback(keras.callbacks.Callback):\n-            \"\"\"A callback to intentionally introduce interruption to training.\"\"\"\n+            \"\"\"A callback to intentionally introduce interruption to\n+            training.\"\"\"\n \n             def on_epoch_end(self, epoch, log=None):\n                 logging.info(f\"counter: {model._train_counter}\")\n@@ -413,7 +414,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n             )\n \n         class InterruptingCallback(keras.callbacks.Callback):\n-            \"\"\"A callback to intentionally introduce interruption to training.\"\"\"\n+            \"\"\"A callback to intentionally introduce interruption to\n+            training.\"\"\"\n \n             def on_epoch_end(self, epoch, log=None):\n                 if epoch == 15:\n@@ -666,7 +668,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n \n         model_type = test_utils.get_model_type()\n         if model_type == \"subclass\":\n-            return  # Skip test since subclassed models cannot be saved in .h5 format.\n+            # Skip test since subclassed models cannot be saved in .h5 format.\n+            return\n         if not tf.__internal__.tf2.enabled():\n             self.skipTest(\"Checkpoint callback only available in v2.\")\n \n@@ -845,8 +848,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n             mode=\"unknown\",\n         )\n \n-        # Case 7: `ModelCheckpoint` with a combination of `save_freq` and `period`.\n-        # Though `period` is deprecated, we're testing it for\n+        # Case 7: `ModelCheckpoint` with a combination of `save_freq` and\n+        # `period`.  Though `period` is deprecated, we're testing it for\n         # backward-compatibility.\n         filepath = os.path.join(temp_dir, \"checkpoint.epoch{epoch:02d}.h5\")\n         cbks = [\n@@ -1026,7 +1029,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         os.remove(filepath.format(epoch=5, batch=1))\n         os.remove(filepath.format(epoch=5, batch=2))\n \n-        # Case 12: ModelCheckpoint saves model with initial_value_threshold param\n+        # Case 12: ModelCheckpoint saves model with initial_value_threshold\n+        # param\n         mode = \"max\"\n         monitor = \"val_acc\"\n         initial_value_threshold = 0\n@@ -1053,7 +1057,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         assert os.path.exists(filepath)\n         os.remove(filepath)\n \n-        # Case 13: ModelCheckpoint saves model with initial_value_threshold param\n+        # Case 13: ModelCheckpoint saves model with initial_value_threshold\n+        # param\n         mode = \"auto\"\n         monitor = \"val_loss\"\n         initial_value_threshold = None\n@@ -1104,8 +1109,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         )\n         assert not os.path.exists(filepath)\n \n-        # Case 15: ModelCheckpoint doesnt save model if loss was min earlier in auto\n-        # mode\n+        # Case 15: ModelCheckpoint doesnt save model if loss was min earlier in\n+        # auto mode\n         mode = \"auto\"\n         monitor = \"val_loss\"\n         initial_value_threshold = 0\n@@ -1228,8 +1233,9 @@ class KerasCallbacksTest(test_combinations.TestCase):\n                 weights_after_one_more_epoch,\n             ) = self._run_load_weights_on_restart_test_common_iterations()\n \n-            # Sleep for some short time period ensuring the files are created with\n-            # a different time (in MacOS OSS the granularity is only 1 second).\n+            # Sleep for some short time period ensuring the files are created\n+            # with a different time (in MacOS OSS the granularity is only 1\n+            # second).\n             time.sleep(2)\n             callback = keras.callbacks.ModelCheckpoint(\n                 filepath=filepath,\n@@ -1261,10 +1267,10 @@ class KerasCallbacksTest(test_combinations.TestCase):\n             )\n             weights_with_one_final_extra_epoch = model.get_weights()\n \n-            # Asserting the weights one epoch after initial fitting and another epoch\n-            # after that are closed, if a ModelCheckpoint with\n-            # load_weights_on_restart=True is given (so the model is restored at the\n-            # beginning of training).\n+            # Asserting the weights one epoch after initial fitting and another\n+            # epoch after that are closed, if a ModelCheckpoint with\n+            # load_weights_on_restart=True is given (so the model is restored at\n+            # the beginning of training).\n             self.assertAllClose(\n                 weights_after_one_more_epoch,\n                 weights_after_model_restoring_and_one_more_epoch,\n@@ -1301,10 +1307,10 @@ class KerasCallbacksTest(test_combinations.TestCase):\n                 model.get_weights()\n             )\n \n-            # Asserting the weights one epoch after initial fitting and another epoch\n-            # after that are different, if a ModelCheckpoint with\n-            # load_weights_on_restart=False is given (so the model is not restored at\n-            # the beginning of training).\n+            # Asserting the weights one epoch after initial fitting and another\n+            # epoch after that are different, if a ModelCheckpoint with\n+            # load_weights_on_restart=False is given (so the model is not\n+            # restored at the beginning of training).\n             self.assertNotAllClose(\n                 weights_after_one_more_epoch,\n                 weights_after_model_restoring_and_one_more_epoch,\n@@ -1671,8 +1677,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n             early_stop.on_epoch_end(epoch, logs={\"val_loss\": losses[epoch]})\n             if early_stop.model.stop_training:\n                 break\n-        # No epoch improves on the baseline, so we should train for only 5 epochs,\n-        # and restore the second model.\n+        # No epoch improves on the baseline, so we should train for only 5\n+        # epochs, and restore the second model.\n         self.assertEqual(epochs_trained, 5)\n         self.assertEqual(early_stop.model.get_weights(), 2)\n \n@@ -1805,9 +1811,9 @@ class KerasCallbacksTest(test_combinations.TestCase):\n                 )\n                 return model\n \n-            # TODO(psv): Make sure the callback works correctly when min_delta is\n-            # set as 0. Test fails when the order of this callback and assertion is\n-            # interchanged.\n+            # TODO(psv): Make sure the callback works correctly when min_delta\n+            # is set as 0. Test fails when the order of this callback and\n+            # assertion is interchanged.\n             model = make_model()\n             cbks = [\n                 keras.callbacks.ReduceLROnPlateau(\n@@ -1834,7 +1840,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n             )\n \n             model = make_model()\n-            # This should reduce the LR after the first epoch (due to high epsilon).\n+            # This should reduce the LR after the first epoch (due to high\n+            # epsilon).\n             cbks = [\n                 keras.callbacks.ReduceLROnPlateau(\n                     monitor=\"val_loss\",\n@@ -1989,8 +1996,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n             os.remove(filepath)\n \n     def test_stop_training_csv(self):\n-        # Test that using the CSVLogger callback with the TerminateOnNaN callback\n-        # does not result in invalid CSVs.\n+        # Test that using the CSVLogger callback with the TerminateOnNaN\n+        # callback does not result in invalid CSVs.\n         np.random.seed(1337)\n         tmpdir = self.get_temp_dir()\n         self.addCleanup(shutil.rmtree, tmpdir, ignore_errors=True)\n@@ -2052,8 +2059,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n \n             values = []\n             with open(fp) as f:\n-                # On Windows, due to \\r\\n line ends, we may end up reading empty lines\n-                # after each line. Skip empty lines.\n+                # On Windows, due to \\r\\n line ends, we may end up reading empty\n+                # lines after each line. Skip empty lines.\n                 values = [x for x in csv.reader(f) if x]\n \n             assert \"nan\" in values[-1], \"The last epoch was not logged.\"\n@@ -2583,9 +2590,9 @@ def list_summaries(logdir):\n                     continue\n                 for value in event.summary.value:\n                     tag = value.tag\n-                    # Case on the `value` rather than the summary metadata because\n-                    # the Keras callback uses `summary_ops_v2` to emit old-style\n-                    # summaries. See b/124535134.\n+                    # Case on the `value` rather than the summary metadata\n+                    # because the Keras callback uses `summary_ops_v2` to emit\n+                    # old-style summaries. See b/124535134.\n                     kind = value.WhichOneof(\"value\")\n                     container = {\n                         \"simple_value\": result.scalars,\n@@ -2599,7 +2606,8 @@ def list_summaries(logdir):\n                             % (kind, path, event)\n                         )\n                     elif kind == \"tensor\" and tag != \"keras\":\n-                        # Convert the tf2 summary proto to old style for type checking.\n+                        # Convert the tf2 summary proto to old style for type\n+                        # checking.\n                         plugin_name = value.metadata.plugin_data.plugin_name\n                         container = {\n                             \"images\": result.images,\n@@ -2962,7 +2970,8 @@ class TestTensorBoardV2(test_combinations.TestCase):\n                     \"embeddings {\\n\",\n                     (\n                         \"  tensor_name: \"\n-                        '\"layer_with_weights-0/embeddings/.ATTRIBUTES/VARIABLE_VALUE\"\\n'\n+                        '\"layer_with_weights-0/embeddings/.ATTRIBUTES/'\n+                        'VARIABLE_VALUE\"\\n'\n                     ),\n                     '  metadata_path: \"metadata.tsv\"\\n',\n                     \"}\\n\",\n@@ -2974,7 +2983,8 @@ class TestTensorBoardV2(test_combinations.TestCase):\n             self.skipTest(\"Custom summaries only supported in V2 code path.\")\n \n         def scalar_v2_mock(name, data, step=None):\n-            \"\"\"A reimplementation of the scalar plugin to avoid circular deps.\"\"\"\n+            \"\"\"A reimplementation of the scalar plugin to avoid circular\n+            deps.\"\"\"\n             metadata = tf.compat.v1.SummaryMetadata()\n             # Should match value in tensorboard/plugins/scalar/metadata.py.\n             metadata.plugin_data.plugin_name = \"scalars\"\n@@ -3210,8 +3220,8 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n     def test_TensorBoard_autoTrace_outerProfiler(self):\n         \"\"\"Runs a profiler session that interferes with the one from the callback.\n \n-        The callback will not generate a profile but execution will proceed without\n-        crashing due to unhandled exceptions.\n+        The callback will not generate a profile but execution will proceed\n+        without crashing due to unhandled exceptions.\n         \"\"\"\n         tf.profiler.experimental.start(logdir=\"\")\n         model = self._get_seq_model()\n@@ -3546,7 +3556,8 @@ class MostRecentlyModifiedFileMatchingPatternTest(tf.test.TestCase):\n                 f.write(\"foo bar\")\n \n         # The result returned from checkpoint_management.latest_checkpoint takes\n-        # priority, so even if it was written earlier, we should still return that.\n+        # priority, so even if it was written earlier, we should still return\n+        # that.\n         self.assertEqual(\n             keras.callbacks.ModelCheckpoint(\n                 None\n@@ -3567,8 +3578,8 @@ class SummaryOpsTest(tf.test.TestCase):\n             keras.callbacks.keras_model_summary(*args, **kwargs)\n         writer.close()\n         events = events_from_logdir(logdir)\n-        # The first event contains no summary values. The written content goes to\n-        # the second event.\n+        # The first event contains no summary values. The written content goes\n+        # to the second event.\n         return events[1]\n \n     @test_utils.run_v2_only\n@@ -3607,7 +3618,8 @@ class SummaryOpsTest(tf.test.TestCase):\n                 x = self.dense(inputs)\n                 return self.activation(x)\n \n-            # Intentionally erroring out at json serialization to test the warning.\n+            # Intentionally erroring out at json serialization to test the\n+            # warning.\n             def get_config(self):\n                 raise NotImplementedError\n \n\n@@ -60,30 +60,30 @@ class TensorBoard(callbacks.TensorBoard):\n           can become quite large when write_graph is set to True.\n         write_grads: whether to visualize gradient histograms in TensorBoard.\n           `histogram_freq` must be greater than 0.\n-        batch_size: size of batch of inputs to feed to the network for histograms\n-          computation.\n+        batch_size: size of batch of inputs to feed to the network for\n+          histograms computation.\n         write_images: whether to write model weights to visualize as image in\n           TensorBoard.\n-        embeddings_freq: frequency (in epochs) at which selected embedding layers\n-          will be saved. If set to 0, embeddings won't be computed. Data to be\n-          visualized in TensorBoard's Embedding tab must be passed as\n+        embeddings_freq: frequency (in epochs) at which selected embedding\n+          layers will be saved. If set to 0, embeddings won't be computed. Data\n+          to be visualized in TensorBoard's Embedding tab must be passed as\n           `embeddings_data`.\n-        embeddings_layer_names: a list of names of layers to keep eye on. If None\n-          or empty list all the embedding layer will be watched.\n-        embeddings_metadata: a dictionary which maps layer name to a file name in\n-          which metadata for this embedding layer is saved.\n+        embeddings_layer_names: a list of names of layers to keep eye on. If\n+          None or empty list all the embedding layer will be watched.\n+        embeddings_metadata: a dictionary which maps layer name to a file name\n+          in which metadata for this embedding layer is saved.\n             [Here are details](\n               https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional)\n               about metadata files format. In case if the same metadata file is\n               used for all embedding layers, string can be passed.\n         embeddings_data: data to be embedded at layers specified in\n-          `embeddings_layer_names`. Numpy array (if the model has a single input)\n-          or list of Numpy arrays (if the model has multiple inputs). Learn more\n-          about embeddings [in this guide](\n+          `embeddings_layer_names`. Numpy array (if the model has a single\n+          input) or list of Numpy arrays (if the model has multiple inputs).\n+          Learn more about embeddings [in this guide](\n           https://www.tensorflow.org/programmers_guide/embedding).\n         update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n-          writes the losses and metrics to TensorBoard after each batch. The same\n-          applies for `'epoch'`. If using an integer, let's say `1000`, the\n+          writes the losses and metrics to TensorBoard after each batch. The\n+          same applies for `'epoch'`. If using an integer, let's say `1000`, the\n           callback will write the metrics and losses to TensorBoard every 1000\n           samples. Note that writing too frequently to TensorBoard can slow down\n           your training.\n@@ -263,8 +263,8 @@ class TensorBoard(callbacks.TensorBoard):\n                 self.embeddings_data, model.input_names\n             )\n \n-            # If embedding_layer_names are not provided, get all of the embedding\n-            # layers from the model.\n+            # If embedding_layer_names are not provided, get all of the\n+            # embedding layers from the model.\n             embeddings_layer_names = self.embeddings_layer_names\n             if not embeddings_layer_names:\n                 embeddings_layer_names = [\n@@ -322,8 +322,9 @@ class TensorBoard(callbacks.TensorBoard):\n                 )\n \n             # TODO(psv): Add integration tests to test embedding visualization\n-            # with TensorBoard callback. We are unable to write a unit test for this\n-            # because TensorBoard dependency assumes TensorFlow package is installed.\n+            # with TensorBoard callback. We are unable to write a unit test for\n+            # this because TensorBoard dependency assumes TensorFlow package is\n+            # installed.\n             config = projector.ProjectorConfig()\n             for layer_name, tensor in embeddings_vars.items():\n                 embedding = config.embeddings.add()\n@@ -412,7 +413,8 @@ class TensorBoard(callbacks.TensorBoard):\n         pass\n \n     def on_epoch_begin(self, epoch, logs=None):\n-        \"\"\"Add histogram op to Model eval_function callbacks, reset batch count.\"\"\"\n+        \"\"\"Add histogram op to Model eval_function callbacks, reset batch\n+        count.\"\"\"\n \n         # check if histogram summary should be run for this epoch\n         if self.histogram_freq and epoch % self.histogram_freq == 0:\n@@ -427,7 +429,8 @@ class TensorBoard(callbacks.TensorBoard):\n             # pylint: enable=protected-access\n \n     def on_epoch_end(self, epoch, logs=None):\n-        \"\"\"Checks if summary ops should run next epoch, logs scalar summaries.\"\"\"\n+        \"\"\"Checks if summary ops should run next epoch, logs scalar\n+        summaries.\"\"\"\n \n         # don't output batch_size and\n         # batch number as TensorBoard summaries\n\n@@ -215,7 +215,8 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n                 metrics=[\"accuracy\"],\n             )\n \n-            # we must generate new callbacks for each test, as they aren't stateless\n+            # we must generate new callbacks for each test, as they aren't\n+            # stateless\n             def callbacks_factory(histogram_freq):\n                 return [\n                     callbacks_v1.TensorBoard(\n\n@@ -44,7 +44,8 @@ class Constraint:\n \n     >>> weight = tf.constant((-1.0, 1.0))\n     >>> NonNegative()(weight)\n-    <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.,  1.], dtype=float32)>\n+    <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.,  1.],\n+    dtype=float32)>\n \n     >>> tf.keras.layers.Dense(4, kernel_constraint=NonNegative())\n     \"\"\"\n@@ -173,7 +174,8 @@ class MinMaxNorm(Constraint):\n     Constrains the weights incident to each hidden unit\n     to have the norm between a lower bound and an upper bound.\n \n-    Also available via the shortcut function `tf.keras.constraints.min_max_norm`.\n+    Also available via the shortcut function\n+    `tf.keras.constraints.min_max_norm`.\n \n     Args:\n       min_value: the minimum norm for the incoming weights.\n@@ -253,9 +255,9 @@ class RadialConstraint(Constraint):\n     ```\n \n     This constraint can be applied to any `Conv2D` layer version, including\n-    `Conv2DTranspose` and `SeparableConv2D`, and with either `\"channels_last\"` or\n-    `\"channels_first\"` data format. The method assumes the weight tensor is of\n-    shape `(rows, cols, input_depth, output_depth)`.\n+    `Conv2DTranspose` and `SeparableConv2D`, and with either `\"channels_last\"`\n+    or `\"channels_first\"` data format. The method assumes the weight tensor is\n+    of shape `(rows, cols, input_depth, output_depth)`.\n     \"\"\"\n \n     @doc_controls.do_not_generate_docs\n@@ -281,7 +283,8 @@ class RadialConstraint(Constraint):\n         )\n \n     def _kernel_constraint(self, kernel):\n-        \"\"\"Radially constraints a kernel with shape (height, width, channels).\"\"\"\n+        \"\"\"Radially constraints a kernel with shape (height, width,\n+        channels).\"\"\"\n         padding = backend.constant([[1, 1], [1, 1]], dtype=\"int32\")\n \n         kernel_shape = backend.shape(kernel)[0]\n\n@@ -39,7 +39,8 @@ class Loss:\n     \"\"\"Loss base class.\n \n     To be implemented by subclasses:\n-    * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`.\n+    * `call()`: Contains the logic for loss calculation using `y_true`,\n+      `y_pred`.\n \n     Example subclass implementation:\n \n@@ -80,10 +81,11 @@ class Loss:\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE`\n             will raise an error. Please see this custom training [tutorial](\n-              https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-                more details.\n+              https://www.tensorflow.org/tutorials/distribute/custom_training)\n+              for more details.\n           name: Optional name for the instance.\n         \"\"\"\n         losses_utils.ReductionV2.validate(reduction)\n@@ -114,24 +116,26 @@ class Loss:\n           y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n           sample_weight: Optional `sample_weight` acts as a coefficient for the\n             loss. If a scalar is provided, then the loss is simply scaled by the\n-            given value. If `sample_weight` is a tensor of size `[batch_size]`, then\n-            the total loss for each sample of the batch is rescaled by the\n+            given value. If `sample_weight` is a tensor of size `[batch_size]`,\n+            then the total loss for each sample of the batch is rescaled by the\n             corresponding element in the `sample_weight` vector. If the shape of\n-            `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to\n-            this shape), then each loss element of `y_pred` is scaled\n-            by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss\n-              functions reduce by 1 dimension, usually axis=-1.)\n+            `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n+            broadcasted to this shape), then each loss element of `y_pred` is\n+            scaled by the corresponding value of `sample_weight`. (Note\n+            on`dN-1`: all loss functions reduce by 1 dimension, usually\n+            axis=-1.)\n \n         Returns:\n           Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n-            shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1`\n-            because all loss functions reduce by 1 dimension, usually axis=-1.)\n+            shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n+            `dN-1` because all loss functions reduce by 1 dimension, usually\n+            axis=-1.)\n \n         Raises:\n           ValueError: If the shape of `sample_weight` is invalid.\n         \"\"\"\n-        # If we are wrapping a lambda function strip '<>' from the name as it is not\n-        # accepted in scope name.\n+        # If we are wrapping a lambda function strip '<>' from the name as it is\n+        # not accepted in scope name.\n         graph_ctx = tf_utils.graph_context_for_symbolic_tensors(\n             y_true, y_pred, sample_weight\n         )\n@@ -192,16 +196,17 @@ class Loss:\n         ):\n             raise ValueError(\n                 \"Please use `tf.keras.losses.Reduction.SUM` or \"\n-                \"`tf.keras.losses.Reduction.NONE` for loss reduction when losses are \"\n-                \"used with `tf.distribute.Strategy` outside of the built-in training \"\n-                \"loops. You can implement \"\n-                \"`tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE` using global batch \"\n-                \"size like:\\n```\\nwith strategy.scope():\\n\"\n+                \"`tf.keras.losses.Reduction.NONE` for loss reduction when \"\n+                \"losses are used with `tf.distribute.Strategy` outside \"\n+                \"of the built-in training loops. You can implement \"\n+                \"`tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE` using \"\n+                \"global batch size like:\\n```\\nwith strategy.scope():\\n\"\n                 \"    loss_obj = tf.keras.losses.CategoricalCrossentropy(\"\n                 \"reduction=tf.keras.losses.Reduction.NONE)\\n....\\n\"\n                 \"    loss = tf.reduce_sum(loss_obj(labels, predictions)) * \"\n                 \"(1. / global_batch_size)\\n```\\nPlease see \"\n-                \"https://www.tensorflow.org/tutorials/distribute/custom_training\"\n+                \"https://www.tensorflow.org/tutorials\"\n+                \"/distribute/custom_training\"\n                 \" for more details.\"\n             )\n \n@@ -226,8 +231,9 @@ class LossFunctionWrapper(Loss):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance.\n@@ -336,11 +342,13 @@ class MeanSquaredError(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n-          name: Optional name for the instance. Defaults to 'mean_squared_error'.\n+          name: Optional name for the instance. Defaults to\n+            'mean_squared_error'.\n         \"\"\"\n         super().__init__(mean_squared_error, name=name, reduction=reduction)\n \n@@ -396,11 +404,13 @@ class MeanAbsoluteError(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n-          name: Optional name for the instance. Defaults to 'mean_absolute_error'.\n+          name: Optional name for the instance. Defaults to\n+            'mean_absolute_error'.\n         \"\"\"\n         super().__init__(mean_absolute_error, name=name, reduction=reduction)\n \n@@ -462,8 +472,9 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance. Defaults to\n@@ -526,8 +537,9 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance. Defaults to\n@@ -615,20 +627,23 @@ class BinaryCrossentropy(LossFunctionWrapper):\n         Args:\n           from_logits: Whether to interpret `y_pred` as a tensor of\n             [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n-              assume that `y_pred` contains probabilities (i.e., values in [0, 1]).\n-          label_smoothing: Float in [0, 1]. When 0, no smoothing occurs. When > 0,\n-            we compute the loss between the predicted labels and a smoothed version\n-            of the true labels, where the smoothing squeezes the labels towards 0.5.\n-            Larger values of `label_smoothing` correspond to heavier smoothing.\n-          axis: The axis along which to compute crossentropy (the features axis).\n-            Defaults to -1.\n+            assume that `y_pred` contains probabilities (i.e., values in [0,\n+            1]).\n+          label_smoothing: Float in [0, 1]. When 0, no smoothing occurs. When >\n+            0, we compute the loss between the predicted labels and a smoothed\n+            version of the true labels, where the smoothing squeezes the labels\n+            towards 0.5.  Larger values of `label_smoothing` correspond to\n+            heavier smoothing.\n+          axis: The axis along which to compute crossentropy (the features\n+            axis).  Defaults to -1.\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Name for the op. Defaults to 'binary_crossentropy'.\n@@ -659,8 +674,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n       `from_logits=False`).\n \n     According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n-    helps to apply a \"focal factor\" to down-weight easy examples and focus more on\n-    hard examples. By default, the focal tensor is computed as follows:\n+    helps to apply a \"focal factor\" to down-weight easy examples and focus more\n+    on hard examples. By default, the focal tensor is computed as follows:\n \n     `focal_factor = (1 - output) ** gamma` for class 1\n     `focal_factor = output ** gamma` for class 0\n@@ -681,7 +696,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     >>> # Example 1: (batch_size = 1, number of samples = 4)\n     >>> y_true = [0, 1, 0, 0]\n     >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n-    >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=2, from_logits=True)\n+    >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=2,\n+    ...                                                from_logits=True)\n     >>> loss(y_true, y_pred).numpy()\n     0.691\n \n@@ -695,7 +711,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     >>> y_true = [[0, 1], [0, 0]]\n     >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n     >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n-    >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=3, from_logits=True)\n+    >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=3,\n+    ...                                                from_logits=True)\n     >>> loss(y_true, y_pred).numpy()\n     0.647\n \n@@ -706,7 +723,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     0.482\n \n     >>> # Using 'sample_weight' attribute with focal effect\n-    >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=3, from_logits=True)\n+    >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=3,\n+    ...                                                from_logits=True)\n     >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n     0.133\n \n@@ -717,7 +735,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     0.097\n \n     >>> # Using 'sum' reduction` type.\n-    >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=4, from_logits=True,\n+    >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=4,\n+    ...                                                from_logits=True,\n     ...     reduction=tf.keras.losses.Reduction.SUM)\n     >>> loss(y_true, y_pred).numpy()\n     1.222\n@@ -730,7 +749,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     0.914\n \n     >>> # Using 'none' reduction type.\n-    >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=5, from_logits=True,\n+    >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n+    ...     gamma=5, from_logits=True,\n     ...     reduction=tf.keras.losses.Reduction.NONE)\n     >>> loss(y_true, y_pred).numpy()\n     array([0.0017 1.1561], dtype=float32)\n@@ -746,9 +766,10 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     Args:\n       apply_class_balancing: A bool, whether to apply weight balancing on the\n         binary classes 0 and 1.\n-      alpha: A weight balancing factor for class 1, default is `0.25` as mentioned\n-        in reference [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n-        The weight for class 0 is `1.0 - alpha`.\n+      alpha: A weight balancing factor for class 1, default is `0.25` as\n+        mentioned in reference [Lin et al., 2018](\n+        https://arxiv.org/pdf/1708.02002.pdf).  The weight for class 0 is\n+        `1.0 - alpha`.\n       gamma: A focusing parameter used to compute the focal factor, default is\n         `2.0` as mentioned in the reference\n         [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n@@ -816,10 +837,11 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n class CategoricalCrossentropy(LossFunctionWrapper):\n     \"\"\"Computes the crossentropy loss between the labels and predictions.\n \n-    Use this crossentropy loss function when there are two or more label classes.\n-    We expect labels to be provided in a `one_hot` representation. If you want to\n-    provide labels as integers, please use `SparseCategoricalCrossentropy` loss.\n-    There should be `# classes` floating point values per feature.\n+    Use this crossentropy loss function when there are two or more label\n+    classes. We expect labels to be provided in a `one_hot` representation. If\n+    you want to provide labels as integers, please use\n+    `SparseCategoricalCrossentropy` loss.  There should be `# classes` floating\n+    point values per feature.\n \n     In the snippet below, there is `# classes` floating pointing values per\n     example. The shape of both `y_pred` and `y_true` are\n@@ -853,7 +875,8 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n     Usage with the `compile()` API:\n \n     ```python\n-    model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalCrossentropy())\n+    model.compile(optimizer='sgd',\n+                  loss=tf.keras.losses.CategoricalCrossentropy())\n     ```\n     \"\"\"\n \n@@ -874,15 +897,16 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n             meaning the confidence on label values are relaxed. For example, if\n             `0.1`, use `0.1 / num_classes` for non-target labels and\n             `0.9 + 0.1 / num_classes` for target labels.\n-          axis: The axis along which to compute crossentropy (the features axis).\n-            Defaults to -1.\n+          axis: The axis along which to compute crossentropy (the features\n+            axis). Defaults to -1.\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance.\n@@ -902,11 +926,12 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     \"\"\"Computes the crossentropy loss between the labels and predictions.\n \n-    Use this crossentropy loss function when there are two or more label classes.\n-    We expect labels to be provided as integers. If you want to provide labels\n-    using `one-hot` representation, please use `CategoricalCrossentropy` loss.\n-    There should be `# classes` floating point values per feature for `y_pred`\n-    and a single floating point value per feature for `y_true`.\n+    Use this crossentropy loss function when there are two or more label\n+    classes.  We expect labels to be provided as integers. If you want to\n+    provide labels using `one-hot` representation, please use\n+    `CategoricalCrossentropy` loss.  There should be `# classes` floating point\n+    values per feature for `y_pred` and a single floating point value per\n+    feature for `y_true`.\n \n     In the snippet below, there is a single floating point value per example for\n     `y_true` and `# classes` floating pointing values per example for `y_pred`.\n@@ -962,8 +987,9 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance. Defaults to\n@@ -1027,8 +1053,9 @@ class Hinge(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance. Defaults to 'hinge'.\n@@ -1088,8 +1115,9 @@ class SquaredHinge(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance. Defaults to 'squared_hinge'.\n@@ -1147,8 +1175,9 @@ class CategoricalHinge(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance. Defaults to 'categorical_hinge'.\n@@ -1203,8 +1232,9 @@ class Poisson(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance. Defaults to 'poisson'.\n@@ -1262,8 +1292,9 @@ class LogCosh(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance. Defaults to 'log_cosh'.\n@@ -1322,8 +1353,9 @@ class KLDivergence(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance. Defaults to 'kl_divergence'.\n@@ -1391,8 +1423,9 @@ class Huber(LossFunctionWrapper):\n             option will be determined by the usage context. For almost all cases\n             this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n             `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n-            will raise an error. Please see this custom training [tutorial](\n+            `tf.keras` `compile` and `fit`, using `AUTO` or\n+            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n+            training [tutorial](\n             https://www.tensorflow.org/tutorials/distribute/custom_training) for\n             more details.\n           name: Optional name for the instance. Defaults to 'huber_loss'.\n@@ -1615,7 +1648,8 @@ def mean_absolute_percentage_error(y_true, y_pred):\n       y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n \n     Returns:\n-      Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`.\n+      Mean absolute percentage error values. shape = `[batch_size, d0, ..\n+      dN-1]`.\n     \"\"\"\n     y_pred = tf.convert_to_tensor(y_pred)\n     y_true = tf.cast(y_true, y_pred.dtype)\n@@ -1665,7 +1699,8 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n       y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n \n     Returns:\n-      Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`.\n+      Mean squared logarithmic error values. shape = `[batch_size, d0, ..\n+      dN-1]`.\n     \"\"\"\n     y_pred = tf.convert_to_tensor(y_pred)\n     y_true = tf.cast(y_true, y_pred.dtype)\n@@ -1718,9 +1753,9 @@ def squared_hinge(y_true, y_pred):\n     ...     np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))\n \n     Args:\n-      y_true: The ground truth values. `y_true` values are expected to be -1 or 1.\n-        If binary (0 or 1) labels are provided we will convert them to -1 or 1.\n-        shape = `[batch_size, d0, .. dN]`.\n+      y_true: The ground truth values. `y_true` values are expected to be -1 or\n+        1. If binary (0 or 1) labels are provided we will convert them to -1 or\n+        1. shape = `[batch_size, d0, .. dN]`.\n       y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n \n     Returns:\n@@ -1752,9 +1787,9 @@ def hinge(y_true, y_pred):\n     ...     np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))\n \n     Args:\n-      y_true: The ground truth values. `y_true` values are expected to be -1 or 1.\n-        If binary (0 or 1) labels are provided they will be converted to -1 or 1.\n-        shape = `[batch_size, d0, .. dN]`.\n+      y_true: The ground truth values. `y_true` values are expected to be -1 or\n+        1. If binary (0 or 1) labels are provided they will be converted to -1\n+        or 1. shape = `[batch_size, d0, .. dN]`.\n       y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n \n     Returns:\n@@ -1863,7 +1898,8 @@ def log_cosh(y_true, y_pred):\n     >>> x = y_pred - y_true\n     >>> assert np.allclose(\n     ...     loss.numpy(),\n-    ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.), axis=-1),\n+    ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),\n+    ...             axis=-1),\n     ...     atol=1e-5)\n \n     Args:\n@@ -1906,8 +1942,8 @@ def categorical_crossentropy(\n     Args:\n       y_true: Tensor of one-hot true targets.\n       y_pred: Tensor of predicted targets.\n-      from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n-        we assume that `y_pred` encodes a probability distribution.\n+      from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+        default, we assume that `y_pred` encodes a probability distribution.\n       label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n         example, if `0.1`, use `0.1 / num_classes` for non-target labels\n         and `0.9 + 0.1 / num_classes` for target labels.\n@@ -1919,7 +1955,8 @@ def categorical_crossentropy(\n     \"\"\"\n     if isinstance(axis, bool):\n         raise ValueError(\n-            f\"`axis` must be of type `int`. Received: axis={axis} of type {type(axis)}\"\n+            f\"`axis` must be of type `int`. \"\n+            f\"Received: axis={axis} of type {type(axis)}\"\n         )\n     y_pred = tf.convert_to_tensor(y_pred)\n     y_true = tf.cast(y_true, y_pred.dtype)\n@@ -1949,8 +1986,8 @@ def _ragged_tensor_categorical_crossentropy(\n     Args:\n       y_true: Tensor of one-hot true targets.\n       y_pred: Tensor of predicted targets.\n-      from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n-        we assume that `y_pred` encodes a probability distribution.\n+      from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+        default, we assume that `y_pred` encodes a probability distribution.\n       label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n         example, if `0.1`, use `0.1 / num_classes` for non-target labels\n         and `0.9 + 0.1 / num_classes` for target labels.\n@@ -1999,8 +2036,8 @@ def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n     Args:\n       y_true: Ground truth values.\n       y_pred: The predicted values.\n-      from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n-        we assume that `y_pred` encodes a probability distribution.\n+      from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+        default, we assume that `y_pred` encodes a probability distribution.\n       axis: Defaults to -1. The dimension along which the entropy is\n         computed.\n \n@@ -2057,11 +2094,12 @@ def binary_crossentropy(\n     Args:\n       y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n       y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n-      from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n-        we assume that `y_pred` encodes a probability distribution.\n+      from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+        default, we assume that `y_pred` encodes a probability distribution.\n       label_smoothing: Float in [0, 1]. If > `0` then smooth the labels by\n         squeezing them towards 0.5 That is, using `1. - 0.5 * label_smoothing`\n-        for the target class and `0.5 * label_smoothing` for the non-target class.\n+        for the target class and `0.5 * label_smoothing` for the non-target\n+        class.\n       axis: The axis along which the mean is computed. Defaults to -1.\n \n     Returns:\n@@ -2093,8 +2131,8 @@ def _ragged_tensor_binary_crossentropy(\n     Args:\n       y_true: Tensor of one-hot true targets.\n       y_pred: Tensor of predicted targets.\n-      from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n-        we assume that `y_pred` encodes a probability distribution.\n+      from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+        default, we assume that `y_pred` encodes a probability distribution.\n       label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n         example, if `0.1`, use `0.1 / num_classes` for non-target labels\n         and `0.9 + 0.1 / num_classes` for target labels.\n@@ -2157,7 +2195,8 @@ def binary_focal_crossentropy(\n \n     >>> y_true = [[0, 1], [0, 0]]\n     >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n-    >>> loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred, gamma=2)\n+    >>> loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,\n+    ...                                                  gamma=2)\n     >>> assert loss.shape == (2,)\n     >>> loss.numpy()\n     array([0.330, 0.206], dtype=float32)\n@@ -2167,14 +2206,16 @@ def binary_focal_crossentropy(\n       y_pred: The predicted values, of shape `(batch_size, d0, .. dN)`.\n       apply_class_balancing: A bool, whether to apply weight balancing on the\n         binary classes 0 and 1.\n-      alpha: A weight balancing factor for class 1, default is `0.25` as mentioned\n-      in the reference. The weight for class 0 is `1.0 - alpha`.\n-      gamma: A focusing parameter, default is `2.0` as mentioned in the reference.\n-      from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n-        we assume that `y_pred` encodes a probability distribution.\n-      label_smoothing: Float in `[0, 1]`. If higher than 0 then smooth the labels\n-        by squeezing them towards `0.5`, i.e., using `1. - 0.5 * label_smoothing`\n-        for the target class and `0.5 * label_smoothing` for the non-target class.\n+      alpha: A weight balancing factor for class 1, default is `0.25` as\n+        mentioned in the reference. The weight for class 0 is `1.0 - alpha`.\n+      gamma: A focusing parameter, default is `2.0` as mentioned in the\n+        reference.\n+      from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+        default, we assume that `y_pred` encodes a probability distribution.\n+      label_smoothing: Float in `[0, 1]`. If higher than 0 then smooth the\n+        labels by squeezing them towards `0.5`, i.e., using `1. - 0.5 *\n+        label_smoothing` for the target class and `0.5 * label_smoothing` for\n+        the non-target class.\n       axis: The axis along which the mean is computed. Defaults to `-1`.\n \n     Returns:\n@@ -2230,12 +2271,14 @@ def _ragged_tensor_binary_focal_crossentropy(\n       y_pred: Tensor of predicted targets.\n       apply_class_balancing: A bool, whether to apply weight balancing on the\n         binary classes 0 and 1.\n-      alpha: A weight balancing factor for class 1, default is `0.25` as mentioned\n-        in the reference [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n-        The weight for class 0 is `1.0 - alpha`.\n-      gamma: A focusing parameter, default is `2.0` as mentioned in the reference.\n-      from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n-        we assume that `y_pred` encodes a probability distribution.\n+      alpha: A weight balancing factor for class 1, default is `0.25` as\n+        mentioned in the reference [Lin et al., 2018](\n+        https://arxiv.org/pdf/1708.02002.pdf). The weight for class 0 is\n+        `1.0 - alpha`.\n+      gamma: A focusing parameter, default is `2.0` as mentioned in the\n+        reference.\n+      from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+        default, we assume that `y_pred` encodes a probability distribution.\n       label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels. For\n         example, if `0.1`, use `0.1 / num_classes` for non-target labels\n         and `0.9 + 0.1 / num_classes` for target labels.\n@@ -2431,7 +2474,8 @@ class CosineSimilarity(LossFunctionWrapper):\n     Usage with the `compile()` API:\n \n     ```python\n-    model.compile(optimizer='sgd', loss=tf.keras.losses.CosineSimilarity(axis=1))\n+    model.compile(optimizer='sgd',\n+                  loss=tf.keras.losses.CosineSimilarity(axis=1))\n     ```\n \n     Args:\n@@ -2439,13 +2483,13 @@ class CosineSimilarity(LossFunctionWrapper):\n         (the features axis). Defaults to -1.\n       reduction: Type of `tf.keras.losses.Reduction` to apply to loss.\n         Default value is `AUTO`. `AUTO` indicates that the reduction option will\n-        be determined by the usage context. For almost all cases this defaults to\n-        `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of\n-        built-in training loops such as `tf.keras` `compile` and `fit`, using\n-        `AUTO` or `SUM_OVER_BATCH_SIZE` will raise an error. Please see this\n-        custom training [tutorial]\n-        (https://www.tensorflow.org/tutorials/distribute/custom_training) for more\n-          details.\n+        be determined by the usage context. For almost all cases this defaults\n+        to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`,\n+        outside of built-in training loops such as `tf.keras` `compile` and\n+        `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE` will raise an error. Please\n+        see this custom training [tutorial](\n+        https://www.tensorflow.org/tutorials/distribute/custom_training) for\n+        more details.\n       name: Optional name for the instance.\n     \"\"\"\n \n@@ -2508,7 +2552,8 @@ def deserialize(name, custom_objects=None):\n     Args:\n         name: Loss configuration.\n         custom_objects: Optional dictionary mapping names (strings) to custom\n-          objects (classes and functions) to be considered during deserialization.\n+          objects (classes and functions) to be considered during\n+          deserialization.\n \n     Returns:\n         A Keras `Loss` instance or a loss function.\n\n@@ -163,8 +163,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(\n         self,\n     ):\n-        # This test only runs in graph because the TF op layer is not supported yet\n-        # for sparse ops.\n+        # This test only runs in graph because the TF op layer is not supported\n+        # yet for sparse ops.\n         t = backend.placeholder()\n         p = backend.placeholder()\n         o = losses.sparse_categorical_crossentropy(t, p)\n@@ -191,12 +191,12 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def test_sparse_categorical_crossentropy_with_float16(self):\n         # See https://github.com/keras-team/keras/issues/15012 for more details.\n-        # we don't cast y_true to have same dtype as y_pred, since y_pred could be\n-        # float16 which has a small upbound, and the casting could cause an\n+        # we don't cast y_true to have same dtype as y_pred, since y_pred could\n+        # be float16 which has a small upbound, and the casting could cause an\n         # underflow. The y_true will be used as int64 anyway.\n \n-        # create 2 observations with 2049 labels, since 2048 is the largest number\n-        # for float16\n+        # create 2 observations with 2049 labels, since 2048 is the largest\n+        # number for float16\n         y_true = [0, 2049]\n         # should result in a loss close to 0 since predicting y_true perfectly\n         y_pred = np.zeros((2, 2050))\n@@ -204,8 +204,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n         y_pred[1][2049] = 1\n         y_pred_16 = tf.convert_to_tensor(y_pred, dtype=tf.float16)\n \n-        # If we did a cast for y_true to float16 in SparseCategoricalCrossentropy,\n-        # then the loss will not be zero.\n+        # If we did a cast for y_true to float16 in\n+        # SparseCategoricalCrossentropy, then the loss will not be zero.\n         scce = losses.SparseCategoricalCrossentropy()\n         self.assertAllClose(scce(y_true, y_pred_16).numpy(), 0.0, atol=1e-3)\n \n@@ -310,7 +310,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n \n     def test_loss_wrapper_dtype(self):\n         # Make sure the loss wrapper doesn't cause any numerical precision loss\n-        # during calculation. See https://github.com/keras-team/keras/issues/15791\n+        # during calculation. See\n+        # https://github.com/keras-team/keras/issues/15791\n         x = tf.convert_to_tensor([[2.1]], dtype=tf.float64)\n         y_true = tf.square(x)\n         y_pred = tf.convert_to_tensor([[3.68]], dtype=tf.float64)\n@@ -1128,7 +1129,8 @@ class BinaryFocalCrossentropyTest(tf.test.TestCase):\n         obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n         loss = obj(y_true, y_pred)\n \n-        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2],\n+        #                                                    [0.7, 0.8]]\n         # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n \n         # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]]\n@@ -1167,7 +1169,8 @@ class BinaryFocalCrossentropyTest(tf.test.TestCase):\n         obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n         loss = obj(y_true, y_pred, sample_weight=1.23)\n \n-        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2],\n+        #                                                    [0.7, 0.8]]\n         # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n \n         # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]] * sample_weight\n@@ -1191,10 +1194,12 @@ class BinaryFocalCrossentropyTest(tf.test.TestCase):\n         #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]\n \n         # bceLoss = -log(p_t) * sample_weight\n-        # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight\n+        # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] *\n+        # sample_weight\n \n         # focalLoss = focal * bceLoss =\n-        # [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]] * sample_weight\n+        # [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]] *\n+        # sample_weight\n         # Reduced loss = 0.799 * 3.21 = 2.565\n \n         self.assertAlmostEqual(self.evaluate(loss), 2.565, 3)\n@@ -1208,7 +1213,8 @@ class BinaryFocalCrossentropyTest(tf.test.TestCase):\n         obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n         loss = obj(y_true, y_pred, sample_weight=sample_weight)\n \n-        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7,\n+        # 0.8]]\n         # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n \n         # bceLoss = -log(p_t) * sample_weight\n@@ -1234,10 +1240,12 @@ class BinaryFocalCrossentropyTest(tf.test.TestCase):\n         #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]\n \n         # bceLoss = -log(p_t) * sample_weight\n-        # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight\n+        # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] *\n+        # sample_weight\n \n         # focalLoss = focal * bceLoss =\n-        # [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]] * sample_weight\n+        # [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]] *\n+        # sample_weight\n         # focalLoss = [[0.00144, 2.72916, 3.0168], [6.8e-7, 0.01122, 3.4e-8]]\n         # Reduced loss = 0.799\n \n@@ -1254,7 +1262,8 @@ class BinaryFocalCrossentropyTest(tf.test.TestCase):\n         )\n         loss = obj(y_true, y_pred)\n \n-        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7,\n+        # 0.8]]\n         # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n \n         # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]]\n@@ -1269,7 +1278,8 @@ class BinaryFocalCrossentropyTest(tf.test.TestCase):\n         obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n         loss = obj(y_true, y_pred)\n \n-        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2, 0.7], [0.8]]\n+        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2, 0.7],\n+        # [0.8]]\n         # focal = (1 - p_t) ** gamma = [[0.01, 0.64, 0.09], [0.04]]\n \n         # bceLoss = -log(p_t) = [[0.105, 1.609, 0.357], [0.223]]\n@@ -1343,7 +1353,8 @@ class BinaryWeightedFocalCrossentropyTest(tf.test.TestCase):\n         )\n         loss = obj(y_true, y_pred)\n \n-        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7,\n+        # 0.8]]\n         # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n         #              = [[0.4, 0.6], [0.4, 0.6]]\n         # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n@@ -1398,7 +1409,8 @@ class BinaryWeightedFocalCrossentropyTest(tf.test.TestCase):\n \n         # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n         #              = [[0.6, 0.4], [0.6, 0.4]]\n-        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7,\n+        # 0.8]]\n         # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n \n         # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]] * sample_weight\n@@ -1429,10 +1441,12 @@ class BinaryWeightedFocalCrossentropyTest(tf.test.TestCase):\n         #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]\n \n         # bceLoss = -log(p_t) * sample_weight\n-        # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight\n+        # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] *\n+        # sample_weight\n \n         # weightedfocalLoss = alpha_weight * focal * bceLoss =\n-        # [[0.00024, 0.45486, 2.0112], [0.00000016, 0.00066, 0.000000008]] * 3.21\n+        # [[0.00024, 0.45486, 2.0112], [0.00000016, 0.00066, 0.000000008]] *\n+        # 3.21\n         # Reduced loss = 0.41116 * 3.21 = 1.32\n \n         self.assertAlmostEqual(self.evaluate(loss), 1.32, 3)\n@@ -1452,7 +1466,8 @@ class BinaryWeightedFocalCrossentropyTest(tf.test.TestCase):\n \n         # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n         #              = [[0.1, 0.9], [0.1, 0.9]]\n-        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7,\n+        # 0.8]]\n         # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n \n         # bceLoss = -log(p_t) * sample_weight\n@@ -1486,11 +1501,13 @@ class BinaryWeightedFocalCrossentropyTest(tf.test.TestCase):\n         #              = [[0.2, 0.2, 0.8], [0.8, 0.2, 0.8]]\n \n         # bceLoss = -log(p_t) * sample_weight\n-        # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight\n+        # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] *\n+        # sample_weight\n \n         # focalLoss = alpha_weight * focal * bceLoss =\n         # [[0.00024, 0.45486, 2.0112], [1.6e-7, 6.6e-4, 8e-9]] * sample_weight\n-        # focalLoss = [[0.000288, 0.5458, 2.41344], [5.44e-7, 2.444e-3, 2.72e-8]]\n+        # focalLoss = [[0.000288, 0.5458, 2.41344], [5.44e-7, 2.444e-3,\n+        # 2.72e-8]]\n         # Reduced loss = 0.49366\n \n         self.assertAlmostEqual(self.evaluate(loss), 0.49366, 3)\n@@ -1511,7 +1528,8 @@ class BinaryWeightedFocalCrossentropyTest(tf.test.TestCase):\n         # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n         #              = [[0.6, 0.4], [0.6, 0.4]]\n \n-        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7,\n+        # 0.8]]\n         # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n \n         # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]]\n@@ -1533,7 +1551,8 @@ class BinaryWeightedFocalCrossentropyTest(tf.test.TestCase):\n \n         # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n         #              = [[0.1, 0.9, 0.1], [0.9]]\n-        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2, 0.7], [0.8]]\n+        # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2, 0.7],\n+        # [0.8]]\n         # focal = (1 - p_t) ** gamma = [[0.01, 0.64, 0.09], [0.04]]\n \n         # bceLoss = -log(p_t) = [[0.105, 1.609, 0.357], [0.223]]\n@@ -2021,7 +2040,8 @@ class SquaredHingeTest(tf.test.TestCase):\n         # y_true = [[-1, 1, -1, 1], [-1, -1, 1, 1]]\n         # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n         # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n-        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5, 0.4]]\n+        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5,\n+        # 0.4]]\n         # squared(max(0, 1 - y_true * y_pred)) = [[0.49, 0.64, 0.81, 0],\n         #                                         [0.5625, 0, 0.25, 0.16]]\n         # loss = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) / 4]\n@@ -2041,7 +2061,8 @@ class SquaredHingeTest(tf.test.TestCase):\n         # y_true = [[-1, 1, -1, 1], [-1, -1, 1, 1]]\n         # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n         # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n-        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5, 0.4]]\n+        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5,\n+        # 0.4]]\n         # squared(max(0, 1 - y_true * y_pred)) = [[0.49, 0.64, 0.81, 0],\n         #                                         [0.5625, 0, 0.25, 0.16]]\n         # loss = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) / 4]\n@@ -2066,7 +2087,8 @@ class SquaredHingeTest(tf.test.TestCase):\n         # y_true = [[-1, 1, -1, 1], [-1, -1, 1, 1]]\n         # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n         # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n-        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5, 0.4]]\n+        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5,\n+        # 0.4]]\n         # squared(max(0, 1 - y_true * y_pred)) = [[0.49, 0.64, 0.81, 0],\n         #                                         [0.5625, 0, 0.25, 0.16]]\n         # loss = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) / 4]\n@@ -2124,7 +2146,8 @@ class CategoricalHingeTest(tf.test.TestCase):\n         loss = cat_hinge_obj(y_true, y_pred)\n \n         # pos = reduce_sum(y_true * y_pred) = [1*4+8*9, 12*2+8*-5] = [76, -16]\n-        # neg = reduce_max((1. - y_true) * y_pred) = [[0, -64], [-12, 48]] = [0, 48]\n+        # neg = reduce_max((1. - y_true) * y_pred) = [[0, -64], [-12, 48]] = [0,\n+        # 48]\n         # cat_hinge = max(0., neg - pos + 1.) = [0, 65]\n         # reduced_loss = (0 + 65)/2 = 32.5\n         self.assertAlmostEqual(self.evaluate(loss), 32.5, 3)\n\n@@ -53,22 +53,23 @@ class Regularizer:\n     activity during optimization. These penalties are summed into the loss\n     function that the network optimizes.\n \n-    Regularization penalties are applied on a per-layer basis. The exact API will\n-    depend on the layer, but many layers (e.g. `Dense`, `Conv1D`, `Conv2D` and\n-    `Conv3D`) have a unified API.\n+    Regularization penalties are applied on a per-layer basis. The exact API\n+    will depend on the layer, but many layers (e.g. `Dense`, `Conv1D`, `Conv2D`\n+    and `Conv3D`) have a unified API.\n \n     These layers expose 3 keyword arguments:\n \n     - `kernel_regularizer`: Regularizer to apply a penalty on the layer's kernel\n     - `bias_regularizer`: Regularizer to apply a penalty on the layer's bias\n-    - `activity_regularizer`: Regularizer to apply a penalty on the layer's output\n+    - `activity_regularizer`: Regularizer to apply a penalty on the layer's\n+    output\n \n     All layers (including custom layers) expose `activity_regularizer` as a\n     settable property, whether or not it is in the constructor arguments.\n \n     The value returned by the `activity_regularizer` is divided by the input\n-    batch size so that the relative weighting between the weight regularizers and\n-    the activity regularizers does not change with the batch size.\n+    batch size so that the relative weighting between the weight regularizers\n+    and the activity regularizers does not change with the batch size.\n \n     You can access a layer's regularization penalties by calling `layer.losses`\n     after calling the layer on inputs.\n@@ -84,7 +85,8 @@ class Regularizer:\n     >>> out = layer(tensor)\n \n     >>> # The kernel regularization term is 0.25\n-    >>> # The activity regularization term (after dividing by the batch size) is 5\n+    >>> # The activity regularization term (after dividing by the batch size)\n+    >>> # is 5\n     >>> tf.math.reduce_sum(layer.losses)\n     <tf.Tensor: shape=(), dtype=float32, numpy=5.25>\n \n@@ -155,9 +157,9 @@ class Regularizer:\n \n     Registration is required for saving and\n     loading models to HDF5 format, Keras model cloning, some visualization\n-    utilities, and exporting models to and from JSON. If using this functionality,\n-    you must make sure any python process running your model has also defined\n-    and registered your custom regularizer.\n+    utilities, and exporting models to and from JSON. If using this\n+    functionality, you must make sure any python process running your model has\n+    also defined and registered your custom regularizer.\n     \"\"\"\n \n     def __call__(self, x):\n@@ -338,9 +340,9 @@ class OrthogonalRegularizer(Regularizer):\n     Arguments:\n       factor: Float. The regularization factor. The regularization penalty will\n         be proportional to `factor` times the mean of the dot products between\n-        the L2-normalized rows (if `mode=\"rows\"`, or columns if `mode=\"columns\"`)\n-        of the inputs, excluding the product of each row/column with itself.\n-        Defaults to 0.01.\n+        the L2-normalized rows (if `mode=\"rows\"`, or columns if\n+        `mode=\"columns\"`) of the inputs, excluding the product of each\n+        row/column with itself.  Defaults to 0.01.\n       mode: String, one of `{\"rows\", \"columns\"}`. Defaults to `\"rows\"`. In rows\n         mode, the regularization effect seeks to make the rows of the input\n         orthogonal to each other. In columns mode, it seeks to make the columns\n\n@@ -281,8 +281,8 @@ class KerasRegularizersTest(test_combinations.TestCase, parameterized.TestCase):\n \n         # We expect to see 9 losses on the model:\n         # - 2 from the 2 add_loss calls on the outer model.\n-        # - 3 from the weight regularizers on the shared_dense layer, unshared_dense\n-        # in inner model 1, unshared_dense in inner model 2.\n+        # - 3 from the weight regularizers on the shared_dense layer,\n+        # unshared_dense in inner model 1, unshared_dense in inner model 2.\n         # - 4 from activity regularizers on the shared_dense layer.\n         self.assertLen(model.losses, 9)\n \n@@ -344,7 +344,8 @@ class KerasRegularizersTest(test_combinations.TestCase, parameterized.TestCase):\n         self.assertAllClose(\n             reg_rows(inputs), factor * sum(rows_pairs) / num_row_pairs\n         )\n-        # Expected: factor * sum(pairwise_dot_products_of_columns) / num_col_pairs\n+        # Expected: factor * sum(pairwise_dot_products_of_columns) /\n+        # num_col_pairs\n         self.assertAllClose(\n             reg_cols(inputs), factor * sum(col_pairs) / num_col_pairs\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
