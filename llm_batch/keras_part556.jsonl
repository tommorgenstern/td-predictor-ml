{"custom_id": "keras#6034134d95c4f724443af4d05e4dace5b7b6b930", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 150 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -106,7 +106,6 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n     seed = tf_draw_seed(seed)\n-    with tf.init_scope():\n     return tf.nn.experimental.stateless_dropout(\n         inputs,\n         rate=rate,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9753e1a467ddc59ba92698ea21c8927f27fae7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 55 | Churn Cumulative: 1358 | Contributors (this commit): 6 | Commits (past 90d): 72 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ class KerasVariable:\n     def __repr__(self):\n         return (\n             f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, \"\n-            \"name={self.name}>\"\n+            f\"name={self.name}>\"\n         )\n \n \n\n@@ -59,6 +59,13 @@ class Variable(KerasVariable):\n     def __init__(self, value, dtype=None, trainable=True, name=None):\n         self.name = name or auto_name(self.__class__.__name__)\n         dtype = standardize_dtype(dtype)\n+        if in_stateless_scope():\n+            raise ValueError(\n+                \"You are attempting to create a variable \"\n+                \"while in a stateless scope. This is disallowed. \"\n+                \"Make sure that all variables are created \"\n+                \"before you start using your layer/model objects. \" \n+            )\n         self._value = jnp.array(value, dtype=dtype)\n         self._dtype = dtype\n         self._shape = tuple(self._value.shape)\n@@ -299,3 +306,18 @@ def execute(op_name, *args, **kwargs):\n         op = getattr(jnp, op_name)\n         return op(*args, **kwargs)\n     raise AttributeError(f\"The JAX backend does not support op '{op_name}'\")\n+\n+\n+def traceable_tensor(shape, dtype=None):\n+    \"\"\"Create a \"traceable tensor\".\n+    \n+    That's a tensor that can be passed as input\n+    to a stateful backend-native function to\n+    create state during the trace.\n+    \"\"\"\n+    shape = list(shape)\n+    dtype = dtype or \"float32\"\n+    for i, x in enumerate(shape):\n+        if x is None:\n+            shape[i] = 1\n+    return jnp.ones(shape, dtype=dtype)\n\\ No newline at end of file\n\n@@ -274,3 +274,18 @@ def execute(op_name, *args, **kwargs):\n     raise AttributeError(\n         f\"The TensorFlow backend does not support op '{op_name}'\"\n     )\n+\n+\n+def traceable_tensor(shape, dtype=None):\n+    \"\"\"Create a \"traceable tensor\".\n+    \n+    That's a tensor that can be passed as input\n+    to a stateful backend-native function to\n+    create state during the trace.\n+    \"\"\"\n+    shape = list(shape)\n+    dtype = dtype or \"float32\"\n+    for i, x in enumerate(shape):\n+        if x is None:\n+            shape[i] = 1\n+    return tf.ones(shape, dtype=dtype)\n\n@@ -221,19 +221,19 @@ class Model(Trainer, Layer):\n                     input_shape = tuple(input_shape)\n                 if isinstance(input_shape, list):\n                     input_tensors = [\n-                        backend.KerasTensor(shape) for shape in input_shape\n+                        backend.traceable_tensor(shape) for shape in input_shape\n                     ]\n                 elif isinstance(input_shape, dict):\n                     input_tensors = {\n-                        k: backend.KerasTensor(shape)\n+                        k: backend.traceable_tensor(shape)\n                         for k, shape in input_shape.items()\n                     }\n                 else:\n-                    input_tensors = backend.KerasTensor(input_shape)\n+                    input_tensors = backend.traceable_tensor(input_shape)\n                 try:\n                     self(input_tensors)\n                     self._build_shapes_dict = config\n-                except:\n+                except Exception as e:\n                     failure = True\n             elif \"shapes_dict\" in config:\n                 # Case: inputs were recorded as multiple keyword arguments.\n@@ -242,7 +242,7 @@ class Model(Trainer, Layer):\n                 ):\n                     # Case: all input keyword arguments were plain tensors.\n                     input_tensors = {\n-                        k: backend.KerasTensor(v)\n+                        k: backend.traceable_tensor(v)\n                         for k, v in config[\"shapes_dict\"].items()\n                     }\n                     try:\n\n@@ -42,7 +42,6 @@ def deserialize(config, custom_objects=None):\n     if config[\"class_name\"].lower() in ALL_OBJECTS_DICT:\n         config[\"class_name\"] = config[\"class_name\"].lower()\n \n-    print(\"deserialize:\", config)\n     return serialization_lib.deserialize_keras_object(\n         config,\n         module_objects=ALL_OBJECTS_DICT,\n@@ -63,16 +62,13 @@ def get(identifier):\n     Returns:\n         A Keras Optimizer instance.\n     \"\"\"\n-    print(\"call get with\", identifier)\n     if isinstance(identifier, Optimizer):\n         return identifier\n     elif isinstance(identifier, dict):\n         return deserialize(identifier)\n     elif isinstance(identifier, str):\n         config = {\"class_name\": identifier, \"config\": {}}\n-        opt = deserialize(config)\n-        print(opt)\n-        return opt\n+        return deserialize(config)\n     else:\n         raise ValueError(\n             f\"Could not interpret optimizer identifier: {identifier}\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#add7a3564fef81073f63ff99336faae709aac7d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 2015 | Contributors (this commit): 7 | Commits (past 90d): 78 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -65,6 +65,10 @@ class Variable(KerasVariable):\n                 \"while in a stateless scope. This is disallowed. \"\n                 \"Make sure that all variables are created \"\n                 \"before you start using your layer/model objects. \"\n+                \"Most of this time, this means you need to \"\n+                \"implement a `def build(self, input_shape)` method \"\n+                \"on your layer/model, which will \"\n+                \"create its variables.\"\n             )\n         self._value = jnp.array(value, dtype=dtype)\n         self._dtype = dtype\n\n@@ -129,6 +129,7 @@ class Layer(Operation):\n             elif \"shapes_dict\" in config:\n                 self.build(**config[\"shapes_dict\"])\n                 self._build_shapes_dict = config[\"shapes_dict\"]\n+            self.built = True\n \n     def add_variable(\n         self,\n@@ -525,8 +526,8 @@ class Layer(Operation):\n         return summary_utils.count_params(self.weights)\n \n     def _maybe_build(self, *args, **kwargs):\n-        arguments_dict = get_arguments_dict(self.call, *args, **kwargs)\n         if not self.built:\n+            arguments_dict = get_arguments_dict(self.call, *args, **kwargs)\n             shapes_dict = get_shapes_dict(arguments_dict)\n             self._build_shapes_dict = shapes_dict\n             if len(shapes_dict) == 1:\n\n@@ -233,7 +233,7 @@ class Model(Trainer, Layer):\n                 try:\n                     self(input_tensors)\n                     self._build_shapes_dict = config\n-                except Exception as e:\n+                except Exception:\n                     failure = True\n             elif \"shapes_dict\" in config:\n                 # Case: inputs were recorded as multiple keyword arguments.\n\n@@ -661,7 +661,7 @@ def deserialize_keras_object(\n     with custom_obj_scope, safe_mode_scope:\n         instance = cls.from_config(inner_config)\n         build_config = config.get(\"build_config\", None)\n-        if build_config:\n+        if build_config and not instance.built:\n             instance.build_from_config(build_config)\n         compile_config = config.get(\"compile_config\", None)\n         if compile_config:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5420ee062f97008023aee40c1fa4757752ab47c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 82 | Churn Cumulative: 1020 | Contributors (this commit): 7 | Commits (past 90d): 55 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,6 +13,8 @@ from keras_core.backend.tensorflow import random\n from keras_core.utils.naming import auto_name\n \n DYNAMIC_SHAPES_OK = True\n+# Disable autograph\n+tf.__internal__.autograph.control_status_ctx().status = 2\n \n \n class Variable(KerasVariable, tf.__internal__.types.Tensor):\n\n@@ -37,6 +37,12 @@ ALL_OBJECTS = {\n }\n \n ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n+ALL_OBJECTS_DICT.update({\n+    \"mae\": mean_absolute_error,\n+    \"MAE\": mean_absolute_error,\n+    \"mse\": mean_squared_error,\n+    \"MSE\": mean_squared_error,\n+})\n \n \n @keras_core_export(\"keras_core.losses.serialize\")\n\n@@ -1,14 +1,69 @@\n from keras_core.api_export import keras_core_export\n+from keras_core.saving import serialization_lib\n+from keras_core.utils import naming\n from keras_core.metrics.confusion_metrics import FalsePositives\n from keras_core.metrics.metric import Metric\n from keras_core.metrics.reduction_metrics import Mean\n from keras_core.metrics.reduction_metrics import MeanMetricWrapper\n from keras_core.metrics.reduction_metrics import Sum\n from keras_core.metrics.regression_metrics import MeanSquaredError\n+from keras_core.metrics.regression_metrics import mean_squared_error\n+from keras_core.metrics.hinge_metrics import Hinge\n+from keras_core.metrics.hinge_metrics import SquaredHinge\n+from keras_core.metrics.hinge_metrics import CategoricalHinge\n \n \n-def deserialize(obj):\n-    raise NotImplementedError\n+ALL_OBJECTS = {\n+    Metric,\n+    Mean,\n+    Sum,\n+    MeanSquaredError,\n+    MeanMetricWrapper,\n+    FalsePositives,\n+    Hinge,\n+    SquaredHinge,\n+    CategoricalHinge,\n+}\n+ALL_OBJECTS_DICT = {\n+    cls.__name__: cls for cls in ALL_OBJECTS\n+}\n+ALL_OBJECTS_DICT.update({\n+    \"mse\": mean_squared_error,\n+    \"MSE\": mean_squared_error,\n+})\n+\n+\n+@keras_core_export(\"keras_core.metrics.serialize\")\n+def serialize(metric):\n+    \"\"\"Serializes metric function or `Metric` instance.\n+\n+    Args:\n+        metric: A Keras `Metric` instance or a metric function.\n+\n+    Returns:\n+        Metric configuration dictionary.\n+    \"\"\"\n+    return serialization_lib.serialize_keras_object(metric)\n+\n+\n+@keras_core_export(\"keras_core.metrics.deserialize\")\n+def deserialize(config, custom_objects=None):\n+    \"\"\"Deserializes a serialized metric class/function instance.\n+\n+    Args:\n+        config: Metric configuration.\n+        custom_objects: Optional dictionary mapping names (strings)\n+            to custom objects (classes and functions) to be\n+            considered during deserialization.\n+\n+    Returns:\n+        A Keras `Metric` instance or a metric function.\n+    \"\"\"\n+    return serialization_lib.deserialize_keras_object(\n+        config,\n+        module_objects=ALL_OBJECTS_DICT,\n+        custom_objects=custom_objects,\n+    )\n \n \n @keras_core_export(\"keras_core.metrics.get\")\n@@ -45,9 +100,7 @@ def get(identifier):\n     if isinstance(identifier, dict):\n         return deserialize(identifier)\n     elif isinstance(identifier, str):\n-        # TODO\n-        # return deserialize(str(identifier))\n-        return globals()[identifier]\n+        return deserialize(identifier)\n     elif callable(identifier):\n         return identifier\n     else:\n\n@@ -4,6 +4,7 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.losses import loss\n from keras_core.metrics.metric import Metric\n+from keras_core.saving import serialization_lib\n \n \n def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n@@ -182,11 +183,13 @@ class MeanMetricWrapper(Mean):\n         return super().update_state(values, sample_weight=sample_weight)\n \n     def get_config(self):\n-        config = {k: v for k, v in self._fn_kwargs.items()}\n-        config[\"fn\"] = self._fn\n         base_config = super().get_config()\n-        return {**base_config.items(), **config.items()}\n+        config = {\"fn\": serialization_lib.serialize_keras_object(self.fn)}\n+        config.update(serialization_lib.serialize_keras_object(self._fn_kwargs))\n+        return {**base_config, **config}\n \n     @classmethod\n     def from_config(cls, config):\n-        raise NotImplementedError\n+        if \"fn\" in config:\n+            config = serialization_lib.deserialize_keras_object(config)\n+        return cls(**config)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1e4ddaf5de8aa5294d0dba73df1866e35ed4ac68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 10 | Files Changed: 5 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 2500 | Contributors (this commit): 9 | Commits (past 90d): 83 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,8 +13,6 @@ from keras_core.backend.tensorflow import random\n from keras_core.utils.naming import auto_name\n \n DYNAMIC_SHAPES_OK = True\n-# Disable autograph\n-tf.__internal__.autograph.control_status_ctx().status = 2\n \n \n class Variable(KerasVariable, tf.__internal__.types.Tensor):\n\n@@ -27,10 +27,8 @@ def max(x, axis=None, keepdims=False):\n \n \n def ones(shape, dtype=\"float32\"):\n-    with tf.init_scope():\n     return tf.ones(shape, dtype=dtype)\n \n \n def zeros(shape, dtype=\"float32\"):\n-    with tf.init_scope():\n     return tf.zeros(shape, dtype=dtype)\n\n@@ -8,7 +8,6 @@ from keras_core.backend.config import floatx\n \n def tf_draw_seed(seed):\n     # TF ops only accept int32/64 seeds but our base seed is uint32.\n-    with tf.init_scope():\n     return tf.cast(draw_seed(seed), dtype=\"int32\")\n \n \n@@ -35,7 +34,6 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n-    with tf.init_scope():\n     return tf.random.stateless_normal(\n         shape=shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed\n     )\n@@ -65,7 +63,6 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n-    with tf.init_scope():\n     return tf.random.stateless_uniform(\n         shape=shape,\n         minval=minval,\n@@ -98,7 +95,6 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n-    with tf.init_scope():\n     return tf.random.stateless_truncated_normal(\n         shape=shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed\n     )\n\n@@ -449,7 +449,7 @@ def functional_like_constructor(cls):\n \n \n def unpack_singleton(x):\n-    if len(x) == 1:\n+    if isinstance(x, (list, tuple)) and len(x) == 1:\n         return x[0]\n     return x\n \n\n@@ -15,7 +15,7 @@ class TestCase(unittest.TestCase):\n         self.addCleanup(lambda: shutil.rmtree(temp_dir))\n         return temp_dir\n \n-    def assertAllClose(self, x1, x2, atol=1e-7, rtol=1e-7):\n+    def assertAllClose(self, x1, x2, atol=1e-6, rtol=1e-6):\n         np.testing.assert_allclose(x1, x2, atol=atol, rtol=rtol)\n \n     def assertAlmostEqual(self, x1, x2, decimal=3):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1fef5cc5eb0a5ca0ccebc77417762478117bd3ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 333 | Contributors (this commit): 4 | Commits (past 90d): 24 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -37,12 +37,14 @@ ALL_OBJECTS = {\n }\n \n ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n-ALL_OBJECTS_DICT.update({\n+ALL_OBJECTS_DICT.update(\n+    {\n         \"mae\": mean_absolute_error,\n         \"MAE\": mean_absolute_error,\n         \"mse\": mean_squared_error,\n         \"MSE\": mean_squared_error,\n-})\n+    }\n+)\n \n \n @keras_core_export(\"keras_core.losses.serialize\")\n\n@@ -1,17 +1,16 @@\n from keras_core.api_export import keras_core_export\n-from keras_core.saving import serialization_lib\n-from keras_core.utils import naming\n from keras_core.metrics.confusion_metrics import FalsePositives\n+from keras_core.metrics.hinge_metrics import CategoricalHinge\n+from keras_core.metrics.hinge_metrics import Hinge\n+from keras_core.metrics.hinge_metrics import SquaredHinge\n from keras_core.metrics.metric import Metric\n from keras_core.metrics.reduction_metrics import Mean\n from keras_core.metrics.reduction_metrics import MeanMetricWrapper\n from keras_core.metrics.reduction_metrics import Sum\n from keras_core.metrics.regression_metrics import MeanSquaredError\n from keras_core.metrics.regression_metrics import mean_squared_error\n-from keras_core.metrics.hinge_metrics import Hinge\n-from keras_core.metrics.hinge_metrics import SquaredHinge\n-from keras_core.metrics.hinge_metrics import CategoricalHinge\n-\n+from keras_core.saving import serialization_lib\n+from keras_core.utils import naming\n \n ALL_OBJECTS = {\n     Metric,\n@@ -24,13 +23,13 @@ ALL_OBJECTS = {\n     SquaredHinge,\n     CategoricalHinge,\n }\n-ALL_OBJECTS_DICT = {\n-    cls.__name__: cls for cls in ALL_OBJECTS\n-}\n-ALL_OBJECTS_DICT.update({\n+ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n+ALL_OBJECTS_DICT.update(\n+    {\n         \"mse\": mean_squared_error,\n         \"MSE\": mean_squared_error,\n-})\n+    }\n+)\n \n \n @keras_core_export(\"keras_core.metrics.serialize\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#387492c78642dfe68b5bfa6f8eb7cdd0418da60e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 43 | Files Changed: 6 | Hunks: 16 | Methods Changed: 12 | Complexity Δ (Sum/Max): 10/20 | Churn Δ: 104 | Churn Cumulative: 2941 | Contributors (this commit): 6 | Commits (past 90d): 116 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ from keras_core.utils.naming import auto_name\n \n \n class KerasTensor:\n-    def __init__(self, shape, dtype=\"float32\", name=None):\n+    def __init__(self, shape, dtype=\"float32\", record_history=True, name=None):\n         from keras_core import backend\n \n         if backend.DYNAMIC_SHAPES_OK:\n@@ -14,6 +14,7 @@ class KerasTensor:\n         self.shape = shape\n         self.dtype = backend.standardize_dtype(dtype)\n         self.name = name or auto_name(self.__class__.__name__)\n+        self.record_history = record_history\n \n     @property\n     def ndim(self):\n\n@@ -535,12 +535,18 @@ class Layer(Operation):\n                 # Single arg: pass it positionally\n                 input_shape = tuple(shapes_dict.values())[0]\n                 with backend.name_scope(self.name):\n+                    if utils.is_default(self.build) and might_have_state(self):\n+                        self._build_for_single_pos_arg(input_shape)\n+                    else:\n                         self.build(input_shape)\n             else:\n                 # More than one shape: pass them by name,\n                 # and check that build() expects the right args.\n                 check_build_signature(self.build, shapes_dict)\n                 with backend.name_scope(self.name):\n+                    if utils.is_default(self.build) and might_have_state(self):\n+                        self._build_for_kwargs(shapes_dict)\n+                    else:\n                         self.build(**shapes_dict)\n             self.built = True\n \n@@ -548,6 +554,42 @@ class Layer(Operation):\n             # may have been updated\n             self._assert_input_compatibility(*args)\n \n+    def _build_for_single_pos_arg(self, input_shape):\n+        # Case: all inputs are in the first arg (possibly nested).\n+        if is_shape_tuple(input_shape):\n+            input_shape = tuple(input_shape)\n+        if isinstance(input_shape, list):\n+            input_tensors = [\n+                backend.KerasTensor(shape, record_history=False) for shape in input_shape\n+            ]\n+        elif isinstance(input_shape, dict):\n+            input_tensors = {\n+                k: backend.KerasTensor(shape, record_history=False)\n+                for k, shape in input_shape.items()\n+            }\n+        else:\n+            input_tensors = backend.KerasTensor(input_shape, record_history=False)\n+        try:\n+            self(input_tensors)\n+            return True\n+        except:\n+            return False\n+\n+    def _build_for_kwargs(self, shapes_dict):\n+        # Case: inputs were recorded as multiple keyword arguments.\n+        if all(is_shape_tuple(s) for s in shapes_dict.values()):\n+            # Case: all input keyword arguments were plain tensors.\n+            input_tensors = {\n+                k: backend.KerasTensor(v, record_history=False) for k, v in shapes_dict.items()\n+            }\n+            try:\n+                self(**input_tensors)\n+            except:\n+                return False\n+        else:\n+            # Not supported: nested input keyword arguments.\n+            return False\n+\n     def __repr__(self):\n         # TODO: improve\n         return f\"<{self.__class__.__name__} name={self.name}>\"\n@@ -743,3 +785,7 @@ def is_shape_tuple(s):\n     return isinstance(s, (list, tuple)) and all(\n         d is None or isinstance(d, int) for d in s\n     )\n+\n+\n+def might_have_state(layer):\n+    return layer._variables or layer._layers or layer._metrics or layer._seed_generators\n\\ No newline at end of file\n\n@@ -109,6 +109,9 @@ class Functional(Function, Model):\n         # From Function\n         return super().compute_output_spec(inputs)\n \n+    def build(self, input_shape):\n+        self.built = True\n+\n     def _assert_input_compatibility(self, *args):\n         return super(Model, self)._assert_input_compatibility(*args)\n \n\n@@ -207,53 +207,17 @@ class Model(Trainer, Layer):\n             )\n \n     def build_from_config(self, config):\n-        def is_shape_tuple(s):\n-            return isinstance(s, (list, tuple)) and all(\n-                d is None or isinstance(d, int) for d in s\n-            )\n-\n-        if config:\n-            failure = False\n+        if not config:\n+            return\n         if \"input_shape\" in config:\n             # Case: all inputs are in the first arg (possibly nested).\n-                input_shape = config[\"input_shape\"]\n-                if is_shape_tuple(input_shape):\n-                    input_shape = tuple(input_shape)\n-                if isinstance(input_shape, list):\n-                    input_tensors = [\n-                        backend.KerasTensor(shape) for shape in input_shape\n-                    ]\n-                elif isinstance(input_shape, dict):\n-                    input_tensors = {\n-                        k: backend.KerasTensor(shape)\n-                        for k, shape in input_shape.items()\n-                    }\n-                else:\n-                    input_tensors = backend.KerasTensor(input_shape)\n-                try:\n-                    self(input_tensors)\n+            status = self._build_for_single_pos_arg(config[\"input_shape\"])\n             self._build_shapes_dict = config\n-                except Exception:\n-                    failure = True\n         elif \"shapes_dict\" in config:\n             # Case: inputs were recorded as multiple keyword arguments.\n-                if all(\n-                    is_shape_tuple(s) for s in config[\"shapes_dict\"].values()\n-                ):\n-                    # Case: all input keyword arguments were plain tensors.\n-                    input_tensors = {\n-                        k: backend.KerasTensor(v)\n-                        for k, v in config[\"shapes_dict\"].items()\n-                    }\n-                    try:\n-                        self(**input_tensors)\n+            status = self._build_for_kwargs(config[\"shapes_dict\"])\n             self._build_shapes_dict = config[\"shapes_dict\"]\n-                    except:\n-                        failure = True\n-                else:\n-                    # Not supported: nested input keyword arguments.\n-                    failure = True\n-            if failure:\n+        if not status:\n             warnings.warn(\n                 f\"Model '{self.name}' had a build config, but the model \"\n                 \"cannot be built automatically in \"\n\n@@ -52,7 +52,10 @@ class Node:\n                     f\"Non-tensor received: {x}\"\n                 )\n         \n+        zero_history = any(not x.record_history for x in self.arguments.keras_tensors)\n+\n         # If inputs don't have metadata yet, add it.\n+        if not zero_history:\n             for tensor in self.arguments.keras_tensors:\n                 if not hasattr(tensor, \"_keras_history\"):\n                     tensor._keras_history = KerasHistory(\n@@ -67,6 +70,7 @@ class Node:\n                 inbound_op._outbound_nodes.append(self)\n \n         # Set metadata on outputs.\n+        if not zero_history:\n             node_index = len(self.operation._inbound_nodes) - 1\n             for i, tensor in enumerate(self.outputs):\n                 tensor._keras_history = KerasHistory(\n\n@@ -174,7 +174,7 @@ class TestTrainer(testing.TestCase):\n         self.assertAllClose(outputs[\"y_one\"], 4 * np.ones((100, 3)))\n         self.assertAllClose(outputs[\"y_two\"], 4 * np.ones((100, 3)))\n \n-    def test_predicte_flow_eager(self):\n+    def test_predict_flow_eager(self):\n         self._test_predict_flow(run_eagerly=True, jit_compile=False)\n \n     def test_predict_flow_graph_fn(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd40e7ec481d0621f2d7ea38e4fc3471dcaffc22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 44 | Churn Cumulative: 1512 | Contributors (this commit): 7 | Commits (past 90d): 95 | Contributors (cumulative): 18 | DMM Complexity: 0.16666666666666666\n\nDIFF:\n@@ -4,9 +4,9 @@ from tensorflow.experimental import numpy as tfnp\n from keras_core.backend.common import KerasVariable\n from keras_core.backend.common import standardize_dtype\n from keras_core.backend.keras_tensor import KerasTensor\n+from keras_core.backend.stateless_scope import StatelessScope\n from keras_core.backend.stateless_scope import get_stateless_scope\n from keras_core.backend.stateless_scope import in_stateless_scope\n-from keras_core.backend.stateless_scope import StatelessScope\n from keras_core.backend.tensorflow import math\n from keras_core.backend.tensorflow import nn\n from keras_core.backend.tensorflow import numpy\n@@ -258,9 +258,12 @@ def compute_output_spec(fn, *args, **kwargs):\n     graph_name = auto_name(\"scratch_graph\")\n     with tf.__internal__.FuncGraph(graph_name).as_default():\n         with StatelessScope():\n+\n             def convert_keras_tensor_to_tf(x):\n                 if isinstance(x, KerasTensor):\n-                    return tf.compat.v1.placeholder(shape=x.shape, dtype=x.dtype)\n+                    return tf.compat.v1.placeholder(\n+                        shape=x.shape, dtype=x.dtype\n+                    )\n                 return x\n \n             args, kwargs = tf.nest.map_structure(\n\n@@ -536,8 +536,12 @@ class Layer(Operation):\n                 # Single arg: pass it positionally\n                 input_shape = tuple(shapes_dict.values())[0]\n                 with backend.name_scope(self.name):\n-                    if utils.is_default(self.build) and might_have_unbuilt_state(self):\n-                        status = self._build_by_run_for_single_pos_arg(input_shape)\n+                    if utils.is_default(\n+                        self.build\n+                    ) and might_have_unbuilt_state(self):\n+                        status = self._build_by_run_for_single_pos_arg(\n+                            input_shape\n+                        )\n                         if not status:\n                             failure = True\n                     else:\n@@ -547,13 +551,16 @@ class Layer(Operation):\n                 # and check that build() expects the right args.\n                 check_build_signature(self.build, shapes_dict)\n                 with backend.name_scope(self.name):\n-                    if utils.is_default(self.build) and might_have_unbuilt_state(self):\n+                    if utils.is_default(\n+                        self.build\n+                    ) and might_have_unbuilt_state(self):\n                         status = self._build_by_run_for_kwargs(shapes_dict)\n                         if not status:\n                             failure = True\n                     else:\n                         self.build(**shapes_dict)\n-            # if failure: # TODO: warn or raise\n+            if failure:  # TODO: warn or raise\n+                pass\n             self.built = True\n \n             # Check input spec again (after build, since self.input_spec\n@@ -566,7 +573,8 @@ class Layer(Operation):\n             input_shape = tuple(input_shape)\n         if isinstance(input_shape, list):\n             input_tensors = [\n-                backend.KerasTensor(shape, record_history=False) for shape in input_shape\n+                backend.KerasTensor(shape, record_history=False)\n+                for shape in input_shape\n             ]\n         elif isinstance(input_shape, dict):\n             input_tensors = {\n@@ -574,11 +582,13 @@ class Layer(Operation):\n                 for k, shape in input_shape.items()\n             }\n         else:\n-            input_tensors = backend.KerasTensor(input_shape, record_history=False)\n+            input_tensors = backend.KerasTensor(\n+                input_shape, record_history=False\n+            )\n         try:\n             self.compute_output_spec(input_tensors)\n             return True\n-        except Exception as e:\n+        except:\n             return False\n \n     def _build_by_run_for_kwargs(self, shapes_dict):\n@@ -586,7 +596,8 @@ class Layer(Operation):\n         if all(is_shape_tuple(s) for s in shapes_dict.values()):\n             # Case: all input keyword arguments were plain tensors.\n             input_tensors = {\n-                k: backend.KerasTensor(v, record_history=False) for k, v in shapes_dict.items()\n+                k: backend.KerasTensor(v, record_history=False)\n+                for k, v in shapes_dict.items()\n             }\n             try:\n                 self.compute_output_spec(**input_tensors)\n\n@@ -2,12 +2,12 @@ import os\n import warnings\n \n from keras_core import backend\n+from keras_core import utils\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n from keras_core.saving import saving_lib\n from keras_core.utils import io_utils\n from keras_core.utils import summary_utils\n-from keras_core import utils\n \n if backend.backend() == \"tensorflow\":\n     from keras_core.backend.tensorflow.trainer import (\n@@ -213,7 +213,9 @@ class Model(Trainer, Layer):\n         if \"input_shape\" in config:\n             # Case: all inputs are in the first arg (possibly nested).\n             if utils.is_default(self.build):\n-                status = self._build_by_run_for_single_pos_arg(config[\"input_shape\"])\n+                status = self._build_by_run_for_single_pos_arg(\n+                    config[\"input_shape\"]\n+                )\n             else:\n                 try:\n                     self.build(config[\"input_shape\"])\n\n@@ -52,7 +52,9 @@ class Node:\n                     f\"Non-tensor received: {x}\"\n                 )\n \n-        zero_history = any(not x.record_history for x in self.arguments.keras_tensors)\n+        zero_history = any(\n+            not x.record_history for x in self.arguments.keras_tensors\n+        )\n \n         # If inputs don't have metadata yet, add it.\n         if not zero_history:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6aaa38c61cb593e2d82cefc88737a7f4f89a8b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 47 | Churn Cumulative: 1398 | Contributors (this commit): 7 | Commits (past 90d): 78 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -303,3 +303,18 @@ def execute(op_name, *args, **kwargs):\n         op = getattr(jnp, op_name)\n         return op(*args, **kwargs)\n     raise AttributeError(f\"The JAX backend does not support op '{op_name}'\")\n+\n+\n+def traceable_tensor(shape, dtype=None):\n+    \"\"\"Create a \"traceable tensor\".\n+\n+    That's a tensor that can be passed as input\n+    to a stateful backend-native function to\n+    create state during the trace.\n+    \"\"\"\n+    shape = list(shape)\n+    dtype = dtype or \"float32\"\n+    for i, x in enumerate(shape):\n+        if x is None:\n+            shape[i] = 1\n+    return jnp.ones(shape, dtype=dtype)\n\n@@ -286,3 +286,18 @@ def execute(op_name, *args, **kwargs):\n     raise AttributeError(\n         f\"The TensorFlow backend does not support op '{op_name}'\"\n     )\n+\n+\n+def traceable_tensor(shape, dtype=None):\n+    \"\"\"Create a \"traceable tensor\".\n+\n+    That's a tensor that can be passed as input\n+    to a stateful backend-native function to\n+    create state during the trace.\n+    \"\"\"\n+    shape = list(shape)\n+    dtype = dtype or \"float32\"\n+    for i, x in enumerate(shape):\n+        if x is None:\n+            shape[i] = 1\n+    return tf.ones(shape, dtype=dtype)\n\n@@ -573,20 +573,17 @@ class Layer(Operation):\n             input_shape = tuple(input_shape)\n         if isinstance(input_shape, list):\n             input_tensors = [\n-                backend.KerasTensor(shape, record_history=False)\n-                for shape in input_shape\n+                backend.traceable_tensor(shape) for shape in input_shape\n             ]\n         elif isinstance(input_shape, dict):\n             input_tensors = {\n-                k: backend.KerasTensor(shape, record_history=False)\n+                k: backend.traceable_tensor(shape)\n                 for k, shape in input_shape.items()\n             }\n         else:\n-            input_tensors = backend.KerasTensor(\n-                input_shape, record_history=False\n-            )\n+            input_tensors = backend.traceable_tensor(input_shape)\n         try:\n-            self.compute_output_spec(input_tensors)\n+            self.call(input_tensors)\n             return True\n         except:\n             return False\n@@ -596,11 +593,11 @@ class Layer(Operation):\n         if all(is_shape_tuple(s) for s in shapes_dict.values()):\n             # Case: all input keyword arguments were plain tensors.\n             input_tensors = {\n-                k: backend.KerasTensor(v, record_history=False)\n-                for k, v in shapes_dict.items()\n+                k: backend.traceable_tensor(shape)\n+                for k, shape in shapes_dict.items()\n             }\n             try:\n-                self.compute_output_spec(**input_tensors)\n+                self.call(**input_tensors)\n             except:\n                 return False\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3a4b682b00a39eb163477941b38dcc0f7974a43e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 470 | Contributors (this commit): 5 | Commits (past 90d): 32 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -559,8 +559,17 @@ class Layer(Operation):\n                             failure = True\n                     else:\n                         self.build(**shapes_dict)\n-            if failure:  # TODO: warn or raise\n-                pass\n+            if failure:\n+                raise ValueError(\n+                    f\"Layer '{self.name}' looks like it has \"\n+                    \"unbuilt state, but Keras is not able to \"\n+                    \"trace the layer `call()` in order to \"\n+                    \"build it automatically. You must implement \"\n+                    \"the `def build(self, input_shape)` method on your \"\n+                    \"layer. It should create all variables used by the \"\n+                    \"layer (e.g. by calling `layer.build()` on all its \"\n+                    \"children layers).\"\n+                )\n             self.built = True\n \n             # Check input spec again (after build, since self.input_spec\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b804a5b6089b277ef6785cbb71cb9479e2c24a75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 3665 | Contributors (this commit): 4 | Commits (past 90d): 22 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2778,6 +2778,7 @@ def square(x):\n \n class Sqrt(Operation):\n     def call(self, x):\n+        x = backend.convert_to_tensor(x)\n         return backend.execute(\"sqrt\", x)\n \n     def compute_output_spec(self, x):\n@@ -2787,6 +2788,7 @@ class Sqrt(Operation):\n def sqrt(x):\n     if any_symbolic_tensors((x,)):\n         return Sqrt().symbolic_call(x)\n+    x = backend.convert_to_tensor(x)\n     return backend.execute(\"sqrt\", x)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#08b67d8f1ed6160e12fc16c01bba6cc7f8acec02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 13 | Methods Changed: 17 | Complexity Δ (Sum/Max): 11/10 | Churn Δ: 95 | Churn Cumulative: 1424 | Contributors (this commit): 5 | Commits (past 90d): 65 | Contributors (cumulative): 11 | DMM Complexity: 0.9444444444444444\n\nDIFF:\n@@ -35,11 +35,15 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n         # Compute gradients\n         # TODO: move value conversion to TF\n+        if self.trainable_weights:\n             trainable_weights = [v.value for v in self.trainable_weights]\n             gradients = tape.gradient(loss, trainable_weights)\n \n             # Update weights\n             self.optimizer.apply_gradients(zip(gradients, trainable_weights))\n+        else:\n+            warnings.warn(\"The model does not have any trainable weights.\")\n+\n         return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n \n     def test_step(self, data):\n\n@@ -42,7 +42,12 @@ from keras_core.utils.tracking import Tracker\n @keras_core_export([\"keras_core.Layer\", \"keras_core.layers.Layer\"])\n class Layer(Operation):\n     def __init__(\n-        self, activity_regularizer=None, trainable=True, dtype=None, name=None\n+        self,\n+        *,\n+        activity_regularizer=None,\n+        trainable=True,\n+        dtype=None,\n+        name=None,\n     ):\n         super().__init__(name=name)\n         self.activity_regularizer = regularizers.get(activity_regularizer)\n@@ -594,7 +599,11 @@ class Layer(Operation):\n         try:\n             self.call(input_tensors)\n             return True\n-        except:\n+        except Exception as e:\n+            warnings.warn(\n+                \"Error when attempting to automatically build \"\n+                f\"the layer by tracing it: {e}\"\n+            )\n             return False\n \n     def _build_by_run_for_kwargs(self, shapes_dict):\n@@ -602,12 +611,18 @@ class Layer(Operation):\n         if all(is_shape_tuple(s) for s in shapes_dict.values()):\n             # Case: all input keyword arguments were plain tensors.\n             input_tensors = {\n-                k: backend.traceable_tensor(shape)\n+                # We strip the `_shape` suffix to recover kwarg names.\n+                k[:-6]: backend.traceable_tensor(shape)\n                 for k, shape in shapes_dict.items()\n             }\n             try:\n                 self.call(**input_tensors)\n-            except:\n+                return True\n+            except Exception as e:\n+                warnings.warn(\n+                    \"Error when attempting to automatically build \"\n+                    f\"the layer by tracing it: {e}\"\n+                )\n                 return False\n         else:\n             # Not supported: nested input keyword arguments.\n\n@@ -1,13 +1,13 @@\n import numpy as np\n \n from keras_core import backend\n+from keras_core import layers\n from keras_core import testing\n-from keras_core.layers.layer import Layer\n \n \n class LayerTest(testing.TestCase):\n     def test_positional_arg_error(self):\n-        class SomeLayer(Layer):\n+        class SomeLayer(layers.Layer):\n             def call(self, x, bool_arg):\n                 if bool_arg:\n                     return x\n@@ -23,7 +23,7 @@ class LayerTest(testing.TestCase):\n         SomeLayer()(x, bool_arg=True)\n \n     def test_rng_seed_tracking(self):\n-        class RNGLayer(Layer):\n+        class RNGLayer(layers.Layer):\n             def __init__(self):\n                 super().__init__()\n                 self.seed_gen = backend.random.SeedGenerator(seed=1337)\n@@ -38,7 +38,7 @@ class LayerTest(testing.TestCase):\n         self.assertAllClose(layer.variables[0], [1337, 1])\n \n         # Test tracking in list attributes.\n-        class RNGListLayer(Layer):\n+        class RNGListLayer(layers.Layer):\n             def __init__(self):\n                 super().__init__()\n                 self.seed_gens = []\n@@ -61,5 +61,65 @@ class LayerTest(testing.TestCase):\n         self.assertAllClose(layer.variables[0], [1, 1])\n         self.assertAllClose(layer.variables[1], [10, 1])\n \n+    def test_layer_tracking(self):\n+        class NestedLayer(layers.Layer):\n+            def __init__(self, units):\n+                super().__init__()\n+                self.dense1 = layers.Dense(units)\n+                self.layer_dict = {\n+                    \"dense2\": layers.Dense(units),\n+                }\n+                self.layer_list = [layers.Dense(units)]\n+                self.units = units\n+\n+            def build(self, input_shape):\n+                self.layer_list.append(layers.Dense(self.units))\n+\n+            def call(self, x):\n+                x = self.dense1(x)\n+                x = self.layer_dict[\"dense2\"](x)\n+                x = self.layer_list[0](x)\n+                x = self.layer_list[1](x)\n+                return x\n+\n+        layer = NestedLayer(3)\n+        layer.build((1, 3))\n+        self.assertLen(layer._layers, 4)\n+        layer(np.zeros((1, 3)))\n+        self.assertLen(layer.weights, 8)\n+\n     def test_build_on_call(self):\n+        class LayerWithUnbuiltState(layers.Layer):\n+            def __init__(self, units):\n+                super().__init__()\n+                self.dense1 = layers.Dense(units)\n+\n+            def call(self, x):\n+                return self.dense1(x)\n+\n+        layer = LayerWithUnbuiltState(2)\n+        layer(backend.KerasTensor((3, 4)))\n+        self.assertLen(layer.weights, 2)\n+\n+        class KwargsLayerWithUnbuiltState(layers.Layer):\n+            def __init__(self, units):\n+                super().__init__()\n+                self.dense1 = layers.Dense(units)\n+                self.dense2 = layers.Dense(units)\n+\n+            def call(self, x1, x2):\n+                return self.dense1(x1) + self.dense2(x2)\n+\n+        layer = KwargsLayerWithUnbuiltState(2)\n+        layer(backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4)))\n+        self.assertLen(layer.weights, 4)\n+\n+        layer = KwargsLayerWithUnbuiltState(2)\n+        layer(x1=backend.KerasTensor((3, 4)), x2=backend.KerasTensor((3, 4)))\n+        self.assertLen(layer.weights, 4)\n+\n+    def test_activity_regularization(self):\n+        pass\n+\n+    def test_add_loss(self):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a5e227cfd618cddb3ef7bf7aa117fcf928303ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 99 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 99 | Churn Cumulative: 750 | Contributors (this commit): 7 | Commits (past 90d): 17 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+# flake8: noqa\n+\n import numpy as np\n \n from keras_core import backend\n@@ -46,6 +48,30 @@ class AdamTest(testing.TestCase):\n         self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n         self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n \n+    def test_correctness_with_golden(self):\n+        optimizer = Adam(amsgrad=True)\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.999, 0.999, 0.999, 0.999, 0.999, 0.999, 0.999, 0.999, 0.999,\n+            0.999], [0.9982, 0.9982, 0.9982, 0.9982, 0.9982, 0.9982, 0.9982,\n+            0.9982, 0.9982, 0.9982], [0.9973, 0.9973, 0.9974, 0.9974, 0.9974,\n+            0.9974, 0.9974, 0.9974, 0.9974, 0.9974], [0.9964, 0.9964, 0.9964,\n+            0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965], [0.9954,\n+            0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955, 0.9955,\n+            0.9955]]\n+        )\n+        # fmt: on\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n+\n     def test_clip_norm(self):\n         # TODO: implement clip_gradients, then uncomment\n         pass\n\n@@ -93,6 +93,7 @@ class SGD(optimizer.Optimizer):\n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n         learning_rate = ops.cast(learning_rate, variable.dtype)\n+        gradient = ops.cast(gradient, variable.dtype)\n         m = None\n         momentum = ops.cast(self.momentum, variable.dtype)\n         m = self.momentums[self._get_variable_index(variable)]\n\n@@ -0,0 +1,72 @@\n+# flake8: noqa\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core import testing\n+from keras_core.optimizers.sgd import SGD\n+\n+\n+class SGDTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = SGD(\n+            learning_rate=0.5,\n+            momentum=0.06,\n+            nesterov=True,\n+            weight_decay=0.004,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        optimizer = SGD(learning_rate=0.5)\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(vars, [0.5, -1.0, -0.5, 3.0], rtol=1e-4, atol=1e-4)\n+\n+    def test_weight_decay(self):\n+        grads, var1, var2, var3 = (\n+            np.zeros(()),\n+            backend.Variable(2.0),\n+            backend.Variable(2.0, name=\"exclude\"),\n+            backend.Variable(2.0),\n+        )\n+        optimizer_1 = SGD(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_1.apply_gradients(zip([grads], [var1]))\n+\n+        optimizer_2 = SGD(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n+        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n+\n+        optimizer_3 = SGD(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n+        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n+\n+        self.assertAlmostEqual(var1.numpy(), 1.9760959, decimal=6)\n+        self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n+        self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n+\n+    def test_correctness_with_golden(self):\n+        optimizer = SGD(nesterov=True)\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n+            0.9999, 0.9999], [0.9989, 0.9979, 0.9969, 0.9959, 0.9949, 0.9939,\n+            0.9929, 0.9919, 0.9909, 0.9899], [0.9979, 0.9959, 0.9939, 0.9919,\n+            0.9899, 0.9879, 0.9859, 0.9839, 0.9819, 0.9799], [0.9969, 0.9939,\n+            0.9909, 0.9879, 0.9849, 0.9819, 0.9789, 0.9759, 0.9729, 0.9699],\n+            [0.9959, 0.9919, 0.9879, 0.9839, 0.9799, 0.9759, 0.9719, 0.9679,\n+            0.9639, 0.9599]]\n+        )\n+        # fmt: on\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
