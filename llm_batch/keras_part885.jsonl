{"custom_id": "keras#6c3dd68c1b2e7783d15244279959e89fe88ee346", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 26 | Churn Cumulative: 2724 | Contributors (this commit): 17 | Commits (past 90d): 22 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -272,15 +272,13 @@ def arctan(x):\n def arctan2(x1, x2):\n     x1 = get_ov_output(x1)\n     x2 = get_ov_output(x2)\n-    x1_type = x1.get_element_type()\n-    x2_type = x2.get_element_type()\n \n-    if x1_type.is_integral():\n-        ov_type = OPENVINO_DTYPES[config.floatx()]\n-        x1 = ov_opset.convert(x1, ov_type)\n-    if x2_type.is_integral():\n-        ov_type = OPENVINO_DTYPES[config.floatx()]\n-        x2 = ov_opset.convert(x2, ov_type)\n+    x1_type = ov_to_keras_type(x1.get_element_type())\n+    x2_type = ov_to_keras_type(x2.get_element_type())\n+    result_type = dtypes.result_type(x1_type, x2_type, float)\n+    result_type = OPENVINO_DTYPES[result_type]\n+    x1 = ov_opset.convert(x1, result_type)\n+    x2 = ov_opset.convert(x2, result_type)\n \n     x = ov_opset.divide(x1, x2)\n     y = ov_opset.atan(x)\n\n@@ -2449,6 +2449,18 @@ class NumpyTwoInputOpsCorrectnessTest(testing.TestCase):\n \n         self.assertAllClose(knp.Arctan2()(x, y), np.arctan2(x, y))\n \n+        a = np.array([0.0, 0.0, 0.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0])\n+        b = np.array([0.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 0.0, 0.0])\n+\n+        self.assertAllClose(knp.arctan2(a, b), np.arctan2(a, b))\n+        self.assertAllClose(knp.Arctan2()(a, b), np.arctan2(a, b))\n+\n+        m = np.array([[3, 4], [7, 8]], dtype=np.int8)\n+        n = np.array([[1, 2], [3, 4]], dtype=float)\n+\n+        self.assertAllClose(knp.arctan2(m, n), np.arctan2(m, n))\n+        self.assertAllClose(knp.Arctan2()(m, n), np.arctan2(m, n))\n+\n     def test_bitwise_and(self):\n         x = np.array([2, 5, 255])\n         y = np.array([3, 14, 16])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d1fb58123b245a451c6699195c6fea45a62fddb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 46 | Files Changed: 2 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 53 | Churn Cumulative: 948 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 0.03125\n\nDIFF:\n@@ -680,17 +680,16 @@ def gaussian_blur(\n             return kernel1d / jnp.sum(kernel1d)\n \n         def _get_gaussian_kernel2d(size, sigma):\n-            size = size.astype(dtype)\n             kernel1d_x = _get_gaussian_kernel1d(size[0], sigma[0])\n             kernel1d_y = _get_gaussian_kernel1d(size[1], sigma[1])\n             return jnp.outer(kernel1d_y, kernel1d_x)\n \n-        kernel = _get_gaussian_kernel2d(kernel_size, sigma)\n-        kernel = kernel[jnp.newaxis, jnp.newaxis, :, :]\n+        kernel = _get_gaussian_kernel2d(kernel_size, sigma)[\n+            jnp.newaxis, jnp.newaxis, :, :\n+        ]\n         return kernel\n \n     images = convert_to_tensor(images)\n-    kernel_size = convert_to_tensor(kernel_size)\n     sigma = convert_to_tensor(sigma)\n     dtype = images.dtype\n \n\n@@ -106,31 +106,6 @@ class RandomGaussianBlur(BaseImagePreprocessingLayer):\n             raise ValueError(error_msg)\n         return lower, upper\n \n-    def create_gaussian_kernel(self, kernel_size, sigma, num_channels):\n-        def get_gaussian_kernel1d(size, sigma):\n-            x = (\n-                self.backend.numpy.arange(size, dtype=self.compute_dtype)\n-                - (size - 1) / 2\n-            )\n-            kernel1d = self.backend.numpy.exp(-0.5 * (x / sigma) ** 2)\n-            return kernel1d / self.backend.numpy.sum(kernel1d)\n-\n-        def get_gaussian_kernel2d(size, sigma):\n-            kernel1d_x = get_gaussian_kernel1d(size[0], sigma[0])\n-            kernel1d_y = get_gaussian_kernel1d(size[1], sigma[1])\n-            return self.backend.numpy.tensordot(kernel1d_y, kernel1d_x, axes=0)\n-\n-        kernel = get_gaussian_kernel2d(kernel_size, sigma)\n-\n-        kernel = self.backend.numpy.reshape(\n-            kernel, (kernel_size[0], kernel_size[1], 1, 1)\n-        )\n-        kernel = self.backend.numpy.tile(kernel, [1, 1, num_channels, 1])\n-\n-        kernel = self.backend.cast(kernel, self.compute_dtype)\n-\n-        return kernel\n-\n     def get_random_transformation(self, data, training=True, seed=None):\n         if not training:\n             return None\n@@ -188,24 +163,14 @@ class RandomGaussianBlur(BaseImagePreprocessingLayer):\n     def transform_images(self, images, transformation=None, training=True):\n         images = self.backend.cast(images, self.compute_dtype)\n         if training and transformation is not None:\n-            if self.data_format == \"channels_first\":\n-                images = self.backend.numpy.swapaxes(images, -3, -1)\n-\n             blur_factor = transformation[\"blur_factor\"]\n             should_apply_blur = transformation[\"should_apply_blur\"]\n \n-            kernel = self.create_gaussian_kernel(\n-                self.kernel_size,\n-                blur_factor,\n-                self.backend.shape(images)[-1],\n-            )\n-\n-            blur_images = self.backend.nn.depthwise_conv(\n+            blur_images = self.backend.image.gaussian_blur(\n                 images,\n-                kernel,\n-                strides=1,\n-                padding=\"same\",\n-                data_format=\"channels_last\",\n+                kernel_size=self.kernel_size,\n+                sigma=blur_factor,\n+                data_format=self.data_format,\n             )\n \n             images = self.backend.numpy.where(\n@@ -218,9 +183,6 @@ class RandomGaussianBlur(BaseImagePreprocessingLayer):\n                 images, self.value_range[0], self.value_range[1]\n             )\n \n-            if self.data_format == \"channels_first\":\n-                images = self.backend.numpy.swapaxes(images, -3, -1)\n-\n             images = self.backend.cast(images, dtype=self.compute_dtype)\n \n         return images\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86dce6f1c32bbe466d0f927e7ab7ca4d76c03024", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 62 | Files Changed: 2 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 97 | Churn Cumulative: 401 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -79,6 +79,7 @@ class Resizing(BaseImagePreprocessingLayer):\n         pad_to_aspect_ratio=False,\n         fill_mode=\"constant\",\n         fill_value=0.0,\n+        antialias=False,\n         data_format=None,\n         **kwargs,\n     ):\n@@ -91,6 +92,7 @@ class Resizing(BaseImagePreprocessingLayer):\n         self.pad_to_aspect_ratio = pad_to_aspect_ratio\n         self.fill_mode = fill_mode\n         self.fill_value = fill_value\n+        self.antialias = bool(antialias)\n         if self.data_format == \"channels_first\":\n             self.height_axis = -2\n             self.width_axis = -1\n@@ -104,6 +106,7 @@ class Resizing(BaseImagePreprocessingLayer):\n             images,\n             size=size,\n             interpolation=self.interpolation,\n+            antialias=self.antialias,\n             data_format=self.data_format,\n             crop_to_aspect_ratio=self.crop_to_aspect_ratio,\n             pad_to_aspect_ratio=self.pad_to_aspect_ratio,\n@@ -299,6 +302,7 @@ class Resizing(BaseImagePreprocessingLayer):\n             \"pad_to_aspect_ratio\": self.pad_to_aspect_ratio,\n             \"fill_mode\": self.fill_mode,\n             \"fill_value\": self.fill_value,\n+            \"antialias\": self.antialias,\n             \"data_format\": self.data_format,\n         }\n         return {**base_config, **config}\n\n@@ -7,80 +7,49 @@ from keras.src import Sequential\n from keras.src import backend\n from keras.src import layers\n from keras.src import testing\n+from keras.src.testing.test_utils import named_product\n \n \n class ResizingTest(testing.TestCase):\n-    def test_resizing_basics(self):\n-        self.run_layer_test(\n-            layers.Resizing,\n-            init_kwargs={\n-                \"height\": 6,\n-                \"width\": 6,\n-                \"data_format\": \"channels_last\",\n-                \"interpolation\": \"bicubic\",\n-                \"crop_to_aspect_ratio\": True,\n-            },\n-            input_shape=(2, 12, 12, 3),\n-            expected_output_shape=(2, 6, 6, 3),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=False,\n-            run_training_check=False,\n+    @parameterized.named_parameters(\n+        named_product(\n+            interpolation=[\"nearest\", \"bilinear\", \"bicubic\", \"lanczos5\"],\n+            crop_pad=[(False, False), (True, False), (False, True)],\n+            antialias=[False, True],\n+            data_format=[\"channels_last\", \"channels_first\"],\n         )\n-        self.run_layer_test(\n-            layers.Resizing,\n-            init_kwargs={\n-                \"height\": 6,\n-                \"width\": 6,\n-                \"data_format\": \"channels_first\",\n-                \"interpolation\": \"bilinear\",\n-                \"crop_to_aspect_ratio\": True,\n-            },\n-            input_shape=(2, 3, 12, 12),\n-            expected_output_shape=(2, 3, 6, 6),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=False,\n-            run_training_check=False,\n-        )\n-        self.run_layer_test(\n-            layers.Resizing,\n-            init_kwargs={\n-                \"height\": 6,\n-                \"width\": 6,\n-                \"data_format\": \"channels_last\",\n-                \"interpolation\": \"nearest\",\n-                \"crop_to_aspect_ratio\": False,\n-            },\n-            input_shape=(2, 12, 12, 3),\n-            expected_output_shape=(2, 6, 6, 3),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=False,\n-            run_training_check=False,\n     )\n+    def test_resizing_basics(\n+        self,\n+        interpolation,\n+        crop_pad,\n+        antialias,\n+        data_format,\n+    ):\n+        if interpolation == \"lanczos5\" and backend.backend() == \"torch\":\n+            self.skipTest(\"Torch does not support lanczos.\")\n+\n+        crop_to_aspect_ratio, pad_to_aspect_ratio = crop_pad\n+        if data_format == \"channels_last\":\n+            input_shape = (2, 12, 12, 3)\n+            expected_output_shape = (2, 6, 6, 3)\n+        else:\n+            input_shape = (2, 3, 12, 12)\n+            expected_output_shape = (2, 3, 6, 6)\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\", reason=\"Torch does not support lanczos.\"\n-    )\n-    def test_resizing_basics_lanczos5(self):\n         self.run_layer_test(\n             layers.Resizing,\n             init_kwargs={\n                 \"height\": 6,\n                 \"width\": 6,\n-                \"data_format\": \"channels_first\",\n-                \"interpolation\": \"lanczos5\",\n-                \"crop_to_aspect_ratio\": False,\n+                \"interpolation\": interpolation,\n+                \"crop_to_aspect_ratio\": crop_to_aspect_ratio,\n+                \"pad_to_aspect_ratio\": pad_to_aspect_ratio,\n+                \"antialias\": antialias,\n+                \"data_format\": data_format,\n             },\n-            input_shape=(2, 3, 12, 12),\n-            expected_output_shape=(2, 3, 6, 6),\n+            input_shape=input_shape,\n+            expected_output_shape=expected_output_shape,\n             expected_num_trainable_weights=0,\n             expected_num_non_trainable_weights=0,\n             expected_num_seed_generators=0,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7efaff8e4699f6db2b7e692c7593e00e3a187e34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 20 | Churn Cumulative: 141 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.8235294117647058\n\nDIFF:\n@@ -279,6 +279,21 @@ class LegacyH5WholeModelTest(testing.TestCase):\n             self.assertIsInstance(loaded_layer.sublayers[1], RegisteredSubLayer)\n             self.assertEqual(loaded_layer.sublayers[1].name, \"MySubLayer\")\n \n+    def test_model_loading_with_axis_arg(self):\n+        input1 = layers.Input(shape=(1, 4), name=\"input1\")\n+        input2 = layers.Input(shape=(1, 4), name=\"input2\")\n+        concat1 = layers.Concatenate(axis=1)([input1, input2])\n+        output = layers.Dense(1, activation=\"sigmoid\")(concat1)\n+        model = models.Model(inputs=[input1, input2], outputs=output)\n+        model.compile(\n+            optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n+        )\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"model_with_axis_arg.h5\"\n+        )\n+        legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n+        legacy_h5_format.load_model_from_hdf5(temp_filepath)\n+\n \n @pytest.mark.requires_trainable_backend\n @pytest.mark.skipif(tf_keras is None, reason=\"Test requires tf_keras\")\n\n@@ -63,8 +63,11 @@ def model_from_config(config, custom_objects=None):\n             config[\"config\"][\"input_shape\"] = batch_input_shape\n \n     axis = config[\"config\"].pop(\"axis\", None)\n-    if axis is not None and isinstance(axis, list) and len(axis) == 1:\n+    if axis is not None:\n+        if isinstance(axis, list) and len(axis) == 1:\n             config[\"config\"][\"axis\"] = int(axis[0])\n+        elif isinstance(axis, (int, float)):\n+            config[\"config\"][\"axis\"] = int(axis)\n \n     # Handle backwards compatibility for Keras lambdas\n     if config[\"class_name\"] == \"Lambda\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#21c8997e3a758cfa9a8dfbf3868a59006faf895e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 24 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 38 | Churn Cumulative: 876 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -850,36 +850,26 @@ def gaussian_blur(\n     if data_format == \"channels_first\":\n         images = np.transpose(images, (0, 2, 3, 1))\n \n-    num_channels = images.shape[-1]\n+    batch_size, height, width, num_channels = images.shape\n+\n     kernel = _create_gaussian_kernel(\n         kernel_size, sigma, num_channels, input_dtype\n     )\n-    batch_size, height, width, _ = images.shape\n-    padded_images = np.pad(\n-        images,\n-        (\n-            (0, 0),\n-            (kernel_size[0] // 2, kernel_size[0] // 2),\n-            (kernel_size[1] // 2, kernel_size[1] // 2),\n-            (0, 0),\n-        ),\n-        mode=\"constant\",\n-    )\n \n-    blurred_images = np.zeros_like(images)\n-    kernel_reshaped = kernel.reshape(\n-        (1, kernel.shape[0], kernel.shape[1], num_channels)\n-    )\n+    pad_h = kernel_size[0] // 2\n+    pad_w = kernel_size[1] // 2\n+\n+    blurred_images = np.empty_like(images)\n \n     for b in range(batch_size):\n-        image_patch = padded_images[b : b + 1, :, :, :]\n-        for i in range(height):\n-            for j in range(width):\n-                patch = image_patch[\n-                    :, i : i + kernel_size[0], j : j + kernel_size[1], :\n-                ]\n-                blurred_images[b, i, j, :] = np.sum(\n-                    patch * kernel_reshaped, axis=(1, 2)\n+        for ch in range(num_channels):\n+            padded = np.pad(\n+                images[b, :, :, ch],\n+                ((pad_h, pad_h), (pad_w, pad_w)),\n+                mode=\"constant\",\n+            )\n+            blurred_images[b, :, :, ch] = scipy.signal.convolve2d(\n+                padded, kernel[:, :, ch], mode=\"valid\"\n             )\n \n     if data_format == \"channels_first\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0902ff4f64f8fc9f48dcefd62144e5619057674a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 18 | Churn Cumulative: 1275 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -444,9 +444,21 @@ def cosh(x):\n \n \n def count_nonzero(x, axis=None):\n-    raise NotImplementedError(\n-        \"`count_nonzero` is not supported with openvino backend\"\n-    )\n+    x = get_ov_output(x)\n+    zero_constant = ov_opset.constant(0, dtype=Type.i32).output(0)\n+    zero_constant = ov_opset.convert_like(zero_constant, x)\n+    x = ov_opset.not_equal(x, zero_constant).output(0)\n+    x = ov_opset.convert(x, Type.i32).output(0)\n+    if axis is None:\n+        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n+        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n+        axis = 0\n+    if isinstance(axis, tuple):\n+        axis = list(axis)\n+    if axis == []:\n+        return OpenVINOKerasTensor(x)\n+    axis = ov_opset.constant(axis, Type.i32).output(0)\n+    return OpenVINOKerasTensor(ov_opset.reduce_sum(x, axis, False).output(0))\n \n \n def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7a7bca6f9530d8eab2625a5ed0e2e3cad224424a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 1284 | Contributors (this commit): 6 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -191,7 +191,14 @@ def amin(x, axis=None, keepdims=False):\n \n \n def append(x1, x2, axis=None):\n-    raise NotImplementedError(\"`append` is not supported with openvino backend\")\n+    x1, x2 = get_ov_output(x1), get_ov_output(x2)\n+    x1, x2 = _align_operand_types(x1, x2, \"append()\")\n+    if axis is None:\n+        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n+        x1 = ov_opset.reshape(x1, flatten_shape, False).output(0)\n+        x2 = ov_opset.reshape(x2, flatten_shape, False).output(0)\n+        axis = 0\n+    return OpenVINOKerasTensor(ov_opset.concat([x1, x2], axis).output(0))\n \n \n def arange(start, stop=None, step=None, dtype=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f7115c2bf126b579d4681f0a0bc36c04d65a7db6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 60 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -331,6 +331,12 @@ class RNN(Layer):\n             cell_kwargs[\"training\"] = training\n \n         def step(inputs, states):\n+            # Create new tensor copies when using PyTorch backend\n+            # with stateful=True. This prevents in-place modifications\n+            # that would otherwise break PyTorch's autograd functionality\n+            # by modifying tensors needed for gradient computation.\n+            if backend.backend() == \"torch\" and self.stateful:\n+                states = tree.map_structure(ops.copy, states)\n             output, new_states = self.cell(inputs, states, **cell_kwargs)\n             if not tree.is_nested(new_states):\n                 new_states = [new_states]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff427e5e40bf9a956cfcbdb64a34d8ec376579c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 260 | Lines Deleted: 1 | Files Changed: 12 | Hunks: 13 | Methods Changed: 14 | Complexity Δ (Sum/Max): 25/11 | Churn Δ: 261 | Churn Cumulative: 4016 | Contributors (this commit): 26 | Commits (past 90d): 80 | Contributors (cumulative): 102 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,6 +83,7 @@ from keras.src.layers.normalization.group_normalization import (\n from keras.src.layers.normalization.layer_normalization import (\n     LayerNormalization,\n )\n+from keras.src.layers.normalization.rms_normalization import RMSNormalization\n from keras.src.layers.normalization.spectral_normalization import (\n     SpectralNormalization,\n )\n\n@@ -91,6 +91,7 @@ from keras.src.ops.nn import polar\n from keras.src.ops.nn import psnr\n from keras.src.ops.nn import relu\n from keras.src.ops.nn import relu6\n+from keras.src.ops.nn import rms_normalization\n from keras.src.ops.nn import selu\n from keras.src.ops.nn import separable_conv\n from keras.src.ops.nn import sigmoid\n\n@@ -35,6 +35,7 @@ from keras.src.ops.nn import polar\n from keras.src.ops.nn import psnr\n from keras.src.ops.nn import relu\n from keras.src.ops.nn import relu6\n+from keras.src.ops.nn import rms_normalization\n from keras.src.ops.nn import selu\n from keras.src.ops.nn import separable_conv\n from keras.src.ops.nn import sigmoid\n\n@@ -83,6 +83,7 @@ from keras.src.layers.normalization.group_normalization import (\n from keras.src.layers.normalization.layer_normalization import (\n     LayerNormalization,\n )\n+from keras.src.layers.normalization.rms_normalization import RMSNormalization\n from keras.src.layers.normalization.spectral_normalization import (\n     SpectralNormalization,\n )\n\n@@ -91,6 +91,7 @@ from keras.src.ops.nn import polar\n from keras.src.ops.nn import psnr\n from keras.src.ops.nn import relu\n from keras.src.ops.nn import relu6\n+from keras.src.ops.nn import rms_normalization\n from keras.src.ops.nn import selu\n from keras.src.ops.nn import separable_conv\n from keras.src.ops.nn import sigmoid\n\n@@ -35,6 +35,7 @@ from keras.src.ops.nn import polar\n from keras.src.ops.nn import psnr\n from keras.src.ops.nn import relu\n from keras.src.ops.nn import relu6\n+from keras.src.ops.nn import rms_normalization\n from keras.src.ops.nn import selu\n from keras.src.ops.nn import separable_conv\n from keras.src.ops.nn import sigmoid\n\n@@ -57,6 +57,7 @@ from keras.src.layers.normalization.group_normalization import (\n from keras.src.layers.normalization.layer_normalization import (\n     LayerNormalization,\n )\n+from keras.src.layers.normalization.rms_normalization import RMSNormalization\n from keras.src.layers.normalization.spectral_normalization import (\n     SpectralNormalization,\n )\n\n@@ -86,7 +86,9 @@ class LayerNormalization(Layer):\n         rms_scaling: If True, `center` and `scale` are ignored, and the\n             inputs are scaled by `gamma` and the inverse square root\n             of the square of all inputs. This is an approximate and faster\n-            approach that avoids ever computing the mean of the input.\n+            approach that avoids ever computing the mean of the input. Note that\n+            this *isn't* equivalent to the computation that the\n+            `keras.layers.RMSNormalization` layer performs.\n         beta_initializer: Initializer for the beta weight. Defaults to zeros.\n         gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n         beta_regularizer: Optional regularizer for the beta weight.\n\n@@ -0,0 +1,98 @@\n+from keras.src import ops\n+from keras.src.api_export import keras_export\n+from keras.src.layers.layer import Layer\n+\n+\n+@keras_export(\"keras.layers.RMSNormalization\")\n+class RMSNormalization(Layer):\n+    \"\"\"Root Mean Square (RMS) Normalization layer.\n+\n+    This layer normalizes the input tensor based on its RMS value.\n+\n+    The Keras layer performs the operation as described in\n+    [Root Mean Square Layer Normalization](https://arxiv.org/pdf/1910.07467)\n+    by Biao Zhang et al.\n+\n+\n+    If `scale` is enabled, the layer will scale the normalized outputs via\n+    a learnable scaling factor.\n+\n+    So, with scaling enabled, the normalization equations\n+    are as follows:\n+\n+    Let the intermediate activations for a mini-batch to be the `inputs`.\n+\n+    ```python\n+    rms_normalization(x) = x * rsqrt(mean(square(x))) * scale\n+    ```\n+\n+    For example:\n+\n+    >>> layer = keras.layers.RMSNormalization()\n+    >>> layer.build([5, 20, 30, 10])\n+    >>> print(layer.scale.shape)\n+    (10,)\n+    >>> layer(np.random.rand(1, 10)).numpy()\n+    array([[0.35098287, 1.0495652 , 1.4645109 , 1.2944688 , 0.31124955,\n+            1.2768592 , 1.184331  , 0.17474432, 0.49955517, 1.2428929 ]],\n+        dtype=float32)\n+\n+    Args:\n+        axis: int. The axis on which to perform the normalization.\n+        epsilon: float. A small number to add to avoid division by zero.\n+    \"\"\"\n+\n+    def __init__(self, axis=-1, epsilon=1e-6, **kwargs):\n+        super().__init__(**kwargs)\n+        self.axis = axis\n+        self.epsilon = epsilon\n+\n+    def build(self, input_shape):\n+        if isinstance(self.axis, list):\n+            shape = tuple([input_shape[dim] for dim in self.axis])\n+        else:\n+            shape = (input_shape[self.axis],)\n+            self.axis = [self.axis]\n+\n+        self.scale = self.add_weight(\n+            name=\"scale\", shape=shape, initializer=\"ones\"\n+        )\n+\n+        self.built = True\n+\n+    def call(self, x):\n+        \"\"\"Applies RMS normalization to the input tensor.\n+\n+        Args:\n+            x: Input tensor of shape (batch_size, input_dim).\n+\n+        Returns:\n+            The RMS-normalized tensor of the same shape (batch_size, input_dim),\n+            scaled by the learned `scale` parameter.\n+        \"\"\"\n+        return ops.rms_normalization(\n+            x, scale=self.scale, axis=self.axis, epsilon=self.epsilon\n+        )\n+\n+    def compute_output_shape(self, input_shape):\n+        if isinstance(self.axis, int):\n+            axes = [self.axis]\n+        else:\n+            axes = self.axis\n+\n+        for axis in axes:\n+            if axis >= len(input_shape) or axis < -len(input_shape):\n+                raise ValueError(\n+                    f\"Axis {axis} is out of bounds for \"\n+                    f\"input shape {input_shape}. \"\n+                    f\"Received: axis={self.axis}\"\n+                )\n+        return input_shape\n+\n+    def get_config(self):\n+        config = {\n+            \"axis\": self.axis,\n+            \"epsilon\": self.epsilon,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,69 @@\n+import numpy as np\n+import pytest\n+\n+from keras.src import layers\n+from keras.src import ops\n+from keras.src import testing\n+\n+\n+class RMSNormalizationTest(testing.TestCase):\n+    @pytest.mark.requires_trainable_backend\n+    def test_ln_basics(self):\n+        self.run_layer_test(\n+            layers.RMSNormalization,\n+            init_kwargs={},\n+            input_shape=(4, 2),\n+            expected_output_shape=(4, 2),\n+            expected_num_trainable_weights=1,\n+            expected_num_seed_generators=0,\n+        )\n+        self.run_layer_test(\n+            layers.RMSNormalization,\n+            init_kwargs={\n+                \"axis\": -1,\n+            },\n+            input_shape=(4, 2),\n+            expected_output_shape=(4, 2),\n+            expected_num_trainable_weights=1,\n+            expected_num_seed_generators=0,\n+        )\n+\n+    def test_correctness(self):\n+        layer = layers.RMSNormalization()\n+        layer.build(input_shape=(2, 2, 2))\n+        inputs = np.random.normal(\n+            loc=5.0, scale=10.0, size=(1000, 2, 2, 2)\n+        ).astype(\"float32\")\n+\n+        inputs = ops.convert_to_tensor(inputs)\n+\n+        out = layer(inputs)\n+        expected = (\n+            inputs\n+            * ops.rsqrt(ops.mean(ops.square(inputs), axis=-1, keepdims=True))\n+            * layer.scale\n+        )\n+\n+        self.assertAllClose(out, expected, atol=1e-1)\n+\n+    def test_output(self):\n+        layer = layers.RMSNormalization()\n+        inputs = np.arange(10).astype(\"float32\")[None, :]\n+        out = layer(inputs)\n+        self.assertAllClose(\n+            out,\n+            [\n+                [\n+                    0.0,\n+                    0.18731716,\n+                    0.37463433,\n+                    0.5619515,\n+                    0.74926865,\n+                    0.9365858,\n+                    1.123903,\n+                    1.3112202,\n+                    1.4985373,\n+                    1.6858544,\n+                ]\n+            ],\n+        )\n\n@@ -10,6 +10,7 @@ from keras.src.backend import standardize_data_format\n from keras.src.backend.common.backend_utils import (\n     compute_conv_transpose_output_shape,\n )\n+from keras.src.backend.common.keras_tensor import is_keras_tensor\n from keras.src.ops import operation_utils\n from keras.src.ops.operation import Operation\n from keras.src.ops.operation_utils import reduce_shape\n@@ -2653,6 +2654,82 @@ def dot_product_attention(\n     )\n \n \n+class RMSNorm(Operation):\n+    def __init__(self, scale, axis=-1, epsilon=None):\n+        super().__init__()\n+        self.axis = axis\n+        self.scale = scale\n+        self.epsilon = epsilon\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape)\n+\n+    def call(self, x):\n+        return _rms_normalization(\n+            x, scale=self.scale, axis=self.axis, epsilon=self.epsilon\n+        )\n+\n+\n+@keras_export(\n+    [\n+        \"keras.ops.rms_normalization\",\n+        \"keras.ops.nn.rms_normalization\",\n+    ]\n+)\n+def rms_normalization(x, scale=1, axis=-1, epsilon=None):\n+    \"\"\"Performs Root Mean Square (RMS) normalization on `x`.\n+\n+    The Keras operation implements the operation as described in\n+    [Root Mean Square Layer Normalization](https://arxiv.org/pdf/1910.07467)\n+    by Biao Zhang et al.\n+\n+    The operation is different from LayerNormalization with RMS scaling.\n+\n+    It is defined as `rms_normalization(x) = x * rsqrt(mean(square(x))) * scale`\n+\n+    Args:\n+        x: Input tensor.\n+        axis: The axis or axes along which to perform normalization.\n+            Default to -1.\n+        scale: Optional scaling factor for the normalization.\n+        epsilon: A lower bound value for the norm.\n+            Defaults to `backend.epsilon()`.\n+\n+    Returns:\n+        The normalized array.\n+\n+    Example:\n+\n+    >>> x = np.random.rand(1, 10)\n+    >>> x_norm = keras.ops.rms_normalization(x, (10,))\n+    >>> print(x_norm)\n+    array([[0.69384296, 0.94444374, 0.16551171, 0.05749961, 1.11008865,\n+        0.52475186, 1.57686807, 1.69893307, 1.27292764, 0.30819128]])\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return RMSNorm(scale=scale, axis=axis, epsilon=epsilon).symbolic_call(x)\n+    return _rms_normalization(x, scale=scale, axis=axis, epsilon=epsilon)\n+\n+\n+def _rms_normalization(x, scale=1, axis=-1, epsilon=None):\n+    x = backend.convert_to_tensor(x)\n+    if len(x.shape) == 0:\n+        x = backend.numpy.expand_dims(x, axis=0)\n+    if epsilon is None:\n+        epsilon = backend.epsilon()\n+\n+    if not is_keras_tensor(scale):\n+        scale = backend.convert_to_tensor(scale, dtype=x.dtype)\n+    if not is_keras_tensor(epsilon):\n+        epsilon = backend.convert_to_tensor(epsilon, dtype=x.dtype)\n+\n+    rrms = backend.math.rsqrt(\n+        backend.numpy.mean(backend.numpy.square(x), axis=axis, keepdims=True)\n+        + epsilon\n+    )\n+    return (x * rrms) * scale\n+\n+\n class Polar(Operation):\n     def __init__(self):\n         super().__init__()\n\n@@ -3142,3 +3142,9 @@ class NNOpsBehaviorTest(testing.TestCase):\n                 beam_width=beam_width,\n                 top_paths=top_paths,\n             )\n+\n+    def test_rms_normalization(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(\n+            knn.rms_normalization(x, (None, 2, 3)).shape, (None, 2, 3)\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2688bfc818f435647ae68c4838d873e8592d3078", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 198 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -156,6 +156,7 @@ def remat(f):\n         pass, the forward computation is recomputed as needed.\n \n     Example:\n+\n     ```python\n     from keras import Model\n     class CustomRematLayer(layers.Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#465a3d2247f6f4deaa702989688de9caeae5c585", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 24 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.src.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.8.0\"\n+__version__ = \"3.9.0\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#19b14183474a065c7d8e15064371281cb26076e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 8 | Churn Cumulative: 146 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -354,7 +354,7 @@ class GRU(RNN):\n \n     1. `activation` == `tanh`\n     2. `recurrent_activation` == `sigmoid`\n-    3. `dropout` == 0 and `recurrent_dropout` == 0\n+    3. `recurrent_dropout` == 0\n     4. `unroll` is `False`\n     5. `use_bias` is `True`\n     6. `reset_after` is `True`\n@@ -553,7 +553,7 @@ class GRU(RNN):\n         if self.use_cudnn in (\"auto\", True):\n             if not self.recurrent_dropout:\n                 try:\n-                    if self.dropout:\n+                    if training and self.dropout:\n                         dp_mask = self.cell.get_dropout_mask(sequences[:, 0, :])\n                         dp_mask = ops.expand_dims(dp_mask, axis=1)\n                         dp_mask = ops.broadcast_to(\n\n@@ -343,7 +343,7 @@ class LSTM(RNN):\n \n     1. `activation` == `tanh`\n     2. `recurrent_activation` == `sigmoid`\n-    3. `dropout` == 0 and `recurrent_dropout` == 0\n+    3. `recurrent_dropout` == 0\n     4. `unroll` is `False`\n     5. `use_bias` is `True`\n     6. Inputs, if use masking, are strictly right-padded.\n@@ -534,7 +534,7 @@ class LSTM(RNN):\n         if self.use_cudnn in (\"auto\", True):\n             if not self.recurrent_dropout:\n                 try:\n-                    if self.dropout:\n+                    if training and self.dropout:\n                         dp_mask = self.cell.get_dropout_mask(sequences[:, 0, :])\n                         dp_mask = ops.expand_dims(dp_mask, axis=1)\n                         dp_mask = ops.broadcast_to(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb1f844472e1018d4db3ca6a9b0b9eab6abcedfd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 88 | Churn Cumulative: 249 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,7 +34,7 @@ class Discretization(TFDataLayer):\n             and `[2., +inf)`.\n             If this option is set, `adapt()` should not be called.\n         num_bins: The integer number of bins to compute.\n-            If this option is set,\n+            If this option is set, `bin_boundaries` should not be set and\n             `adapt()` should be called to learn the bin boundaries.\n         epsilon: Error tolerance, typically a small fraction\n             close to zero (e.g. 0.01). Higher values of epsilon increase\n@@ -130,17 +130,17 @@ class Discretization(TFDataLayer):\n                 f\"Received: `num_bins={num_bins}`\"\n             )\n         if num_bins is not None and bin_boundaries is not None:\n-            if len(bin_boundaries) != num_bins - 1:\n             raise ValueError(\n-                    \"Both `num_bins` and `bin_boundaries` should not be \"\n-                    f\"set. Received: `num_bins={num_bins}` and \"\n+                \"Both `num_bins` and `bin_boundaries` should not be set. \"\n+                f\"Received: `num_bins={num_bins}` and \"\n                 f\"`bin_boundaries={bin_boundaries}`\"\n             )\n-\n-        self.input_bin_boundaries = bin_boundaries\n-        self.bin_boundaries = (\n-            bin_boundaries if bin_boundaries is not None else []\n+        if num_bins is None and bin_boundaries is None:\n+            raise ValueError(\n+                \"You need to set either `num_bins` or `bin_boundaries`.\"\n             )\n+\n+        self.bin_boundaries = bin_boundaries\n         self.num_bins = num_bins\n         self.epsilon = epsilon\n         self.output_mode = output_mode\n@@ -183,7 +183,7 @@ class Discretization(TFDataLayer):\n                 repeating dataset, you must specify the `steps` argument. This\n                 argument is not supported with array inputs or list inputs.\n         \"\"\"\n-        if self.input_bin_boundaries is not None:\n+        if self.num_bins is None:\n             raise ValueError(\n                 \"Cannot adapt a Discretization layer that has been initialized \"\n                 \"with `bin_boundaries`, use `num_bins` instead.\"\n@@ -204,14 +204,14 @@ class Discretization(TFDataLayer):\n         self.summary = merge_summaries(summary, self.summary, self.epsilon)\n \n     def finalize_state(self):\n-        if self.input_bin_boundaries is not None:\n+        if self.num_bins is None:\n             return\n         self.bin_boundaries = get_bin_boundaries(\n             self.summary, self.num_bins\n         ).tolist()\n \n     def reset_state(self):\n-        if self.input_bin_boundaries is not None:\n+        if self.num_bins is None:\n             return\n         self.summary = np.array([[], []], dtype=\"float32\")\n \n@@ -225,6 +225,13 @@ class Discretization(TFDataLayer):\n         return\n \n     def call(self, inputs):\n+        if self.bin_boundaries is None:\n+            raise ValueError(\n+                \"You need to either pass the `bin_boundaries` argument at \"\n+                \"construction time or call `adapt(dataset)` before you can \"\n+                \"start using the `Discretization` layer.\"\n+            )\n+\n         indices = self.backend.numpy.digitize(inputs, self.bin_boundaries)\n         return numerical_utils.encode_categorical_inputs(\n             indices,\n@@ -246,6 +253,23 @@ class Discretization(TFDataLayer):\n             \"dtype\": self.dtype,\n         }\n \n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        if (\n+            config.get(\"bin_boundaries\", None) is not None\n+            and config.get(\"num_bins\", None) is not None\n+        ):\n+            # After `adapt` was called, both `bin_boundaries` and `num_bins` are\n+            # populated, but `__init__` won't let us create a new layer with\n+            # both `bin_boundaries` and `num_bins`. We therefore apply\n+            # `bin_boundaries` after creation.\n+            config = config.copy()\n+            bin_boundaries = config.pop(\"bin_boundaries\")\n+            discretization = cls(**config)\n+            discretization.bin_boundaries = bin_boundaries\n+            return discretization\n+        return cls(**config)\n+\n \n def summarize(values, epsilon):\n     \"\"\"Reduce a 1D sequence of values to a summary.\n\n@@ -131,6 +131,32 @@ class DiscretizationTest(testing.TestCase):\n         for output in ds.take(1):\n             output.numpy()\n \n+    def test_serialization(self):\n+        layer = layers.Discretization(num_bins=5)\n+\n+        # Serialization before `adapt` is called.\n+        config = layer.get_config()\n+        revived_layer = layers.Discretization.from_config(config)\n+        self.assertEqual(config, revived_layer.get_config())\n+\n+        # Serialization after `adapt` is called but `num_bins` was not reached.\n+        layer.adapt(np.array([0.0, 1.0, 5.0]))\n+        config = layer.get_config()\n+        revived_layer = layers.Discretization.from_config(config)\n+        self.assertEqual(config, revived_layer.get_config())\n+\n+        # Serialization after `adapt` is called and `num_bins` is reached.\n+        layer.adapt(np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]))\n+        config = layer.get_config()\n+        revived_layer = layers.Discretization.from_config(config)\n+        self.assertEqual(config, revived_layer.get_config())\n+\n+        # Serialization with `bin_boundaries`.\n+        layer = layers.Discretization(bin_boundaries=[0.0, 0.35, 0.5, 1.0])\n+        config = layer.get_config()\n+        revived_layer = layers.Discretization.from_config(config)\n+        self.assertEqual(config, revived_layer.get_config())\n+\n     def test_saving(self):\n         # With fixed bins\n         layer = layers.Discretization(bin_boundaries=[0.0, 0.35, 0.5, 1.0])\n@@ -163,3 +189,19 @@ class DiscretizationTest(testing.TestCase):\n         model.save(fpath)\n         model = saving_api.load_model(fpath)\n         self.assertAllClose(layer(ref_input), ref_output)\n+\n+    def test_init_num_bins_and_bin_boundaries_raises(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Both `num_bins` and `bin_boundaries`\"\n+        ):\n+            layers.Discretization(num_bins=3, bin_boundaries=[0.0, 1.0])\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"either `num_bins` or `bin_boundaries`\"\n+        ):\n+            layers.Discretization()\n+\n+    def test_call_before_adapt_raises(self):\n+        layer = layers.Discretization(num_bins=3)\n+        with self.assertRaisesRegex(ValueError, \"You need .* call .*adapt\"):\n+            layer([[0.1, 0.8, 0.9]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#335d317a64a531b7025791686d4f995fd1040322", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 26 | Files Changed: 6 | Hunks: 23 | Methods Changed: 25 | Complexity Δ (Sum/Max): 17/13 | Churn Δ: 62 | Churn Cumulative: 1053 | Contributors (this commit): 17 | Commits (past 90d): 18 | Contributors (cumulative): 45 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,9 +27,7 @@ class Variable(KerasVariable):\n         # due to circular dependency.\n         distribution = global_state.get_global_attribute(\"distribution\")\n         if distribution is not None:\n-            self._layout = distribution_lib._to_jax_layout(\n-                distribution.get_variable_layout(self)\n-            )\n+            self._layout = distribution.get_variable_layout(self).backend_layout\n         else:\n             self._layout = None\n         self._direct_assign(value)\n@@ -410,7 +408,7 @@ def device_scope(device_name):\n     if isinstance(device_name, str):\n         # We support string value like \"cpu:0\", \"gpu:1\", etc.\n         device_name = device_name.lower()\n-        jax_device = distribution_lib._to_jax_device(device_name)\n+        jax_device = distribution_lib._to_backend_device(device_name)\n     elif not isinstance(device_name, jax.Device):\n         raise ValueError(\n             \"Invalid value for argument `device_name`. \"\n\n@@ -42,7 +42,7 @@ def distribute_variable(value, layout):\n         jax.Array which is the distributed variable.\n     \"\"\"\n     if not isinstance(layout, jax.sharding.Sharding):\n-        layout = _to_jax_layout(layout)\n+        layout = layout.backend_layout\n     if isinstance(\n         value, (jax.Array, jax.numpy.ndarray)\n     ) and value.sharding.is_equivalent_to(layout, ndim=len(value.shape)):\n@@ -79,7 +79,7 @@ def distribute_tensor(tensor, layout):\n         Distributed value.\n     \"\"\"\n     if not isinstance(layout, jax.sharding.Sharding):\n-        layout = _to_jax_layout(layout)\n+        layout = layout.backend_layout\n     # TODO(scottzhu): This might not be a cheap check, we should consider\n     # have some proper JAX API for doing this check.\n     if jax_utils.is_in_jax_tracing_scope():\n@@ -115,7 +115,7 @@ def distribute_data_input(per_process_batch, layout, batch_dim_name):\n         A global batch distributed according to `layout`.\n     \"\"\"\n     if not isinstance(layout, jax.sharding.Sharding):\n-        layout = _to_jax_layout(layout)\n+        layout = layout.backend_layout\n \n     num_model_replicas_total = layout.mesh.shape[batch_dim_name]\n \n@@ -219,7 +219,7 @@ def process_id():\n     return jax.process_index()\n \n \n-def _to_jax_device(device_name):\n+def _to_backend_device(device_name):\n     if isinstance(device_name, jax.Device):\n         return device_name\n     device_type, device_id = device_name.split(\":\")\n@@ -231,7 +231,7 @@ def _to_jax_device(device_name):\n     raise ValueError(f\"Device not found: {device_name}\")\n \n \n-def _to_jax_mesh(device_mesh):\n+def _to_backend_mesh(device_mesh):\n     \"\"\"Convert the DeviceMesh to JAX backend specific Mesh.\n \n     Args:\n@@ -241,12 +241,12 @@ def _to_jax_mesh(device_mesh):\n         A `jax.sharding.Mesh` instance.\n     \"\"\"\n     shape = device_mesh.devices.shape\n-    devices = [_to_jax_device(d) for d in device_mesh.devices.flatten()]\n+    devices = [_to_backend_device(d) for d in device_mesh.devices.flatten()]\n     devices = np.array(devices).reshape(shape)\n     return jax.sharding.Mesh(devices, device_mesh.axis_names)\n \n \n-def _to_jax_layout(tensor_layout):\n+def _to_backend_layout(tensor_layout):\n     \"\"\"Convert the TensorLayout to JAX backend specific Sharding.\n \n     Args:\n@@ -261,5 +261,5 @@ def _to_jax_layout(tensor_layout):\n             \"for TensorLayout.\"\n         )\n     partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n-    jax_mesh = _to_jax_mesh(tensor_layout.device_mesh)\n+    jax_mesh = tensor_layout.device_mesh.backend_mesh\n     return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n\n@@ -42,7 +42,7 @@ class JaxDistributionLibTest(testing.TestCase):\n         jax_devices = jax.devices(\"cpu\")\n \n         for d, jax_d in zip(devices, jax_devices):\n-            converted_jax_device = backend_dlib._to_jax_device(d)\n+            converted_jax_device = backend_dlib._to_backend_device(d)\n             self.assertIsInstance(converted_jax_device, jax.Device)\n             self.assertEqual(jax_d, converted_jax_device)\n \n@@ -130,26 +130,26 @@ class JaxDistributionLibTest(testing.TestCase):\n         self.assertEqual(backend_dlib.process_id(), 0)\n         self.assertEqual(backend_dlib.num_processes(), 1)\n \n-    def test_to_jax_mesh(self):\n+    def test_to_backend_mesh(self):\n         devices = [f\"cpu:{i}\" for i in range(8)]\n         shape = (4, 2)\n         axis_names = [\"batch\", \"model\"]\n \n         mesh = distribution_lib.DeviceMesh(shape, axis_names, devices)\n-        jax_mesh = backend_dlib._to_jax_mesh(mesh)\n+        jax_mesh = backend_dlib._to_backend_mesh(mesh)\n \n         self.assertIsInstance(jax_mesh, jax.sharding.Mesh)\n         self.assertEqual(jax_mesh.devices.shape, shape)\n         self.assertEqual(jax_mesh.axis_names, (\"batch\", \"model\"))\n \n-    def test_to_jax_layout(self):\n+    def test_to_backend_layout(self):\n         axes = [\"data\", None]\n         mesh = distribution_lib.DeviceMesh(\n             (4, 2), [\"data\", \"model\"], [f\"cpu:{i}\" for i in range(8)]\n         )\n         layout = distribution_lib.TensorLayout(axes, mesh)\n-        jax_sharding = backend_dlib._to_jax_layout(layout)\n-        jax_mesh = backend_dlib._to_jax_mesh(mesh)\n+        jax_sharding = backend_dlib._to_backend_layout(layout)\n+        jax_mesh = backend_dlib._to_backend_mesh(mesh)\n         self.assertEqual(\n             jax_sharding,\n             jax.sharding.NamedSharding(\n@@ -164,7 +164,7 @@ class JaxDistributionLibTest(testing.TestCase):\n         with self.assertRaisesRegex(\n             ValueError, \"Cannot create sharding when device mesh is not set\"\n         ):\n-            backend_dlib._to_jax_layout(layout)\n+            backend_dlib._to_backend_layout(layout)\n \n     def test_variable_assignment_reuse_layout(self):\n         shape = (4, 2)\n@@ -314,7 +314,7 @@ class JaxDistributionLibTest(testing.TestCase):\n         # Note that the intermediate_tensor_layout is only captured during the\n         # actual training, and not at the model building time.\n         intermediate_tensor_layout = jax.sharding.NamedSharding(\n-            backend_dlib._to_jax_mesh(distribution.device_mesh),\n+            backend_dlib._to_backend_mesh(distribution.device_mesh),\n             jax.sharding.PartitionSpec(\"batch\", None),\n         )\n         self.assertTrue(\n\n@@ -1026,9 +1026,9 @@ class JAXEpochIterator(EpochIterator):\n         for data in self.data_adapter.get_jax_iterator():\n             if layouts is None:\n                 layouts = tree.map_structure(\n-                    lambda d: jax_distribution_lib._to_jax_layout(\n-                        distribution.get_data_layout(d.shape)\n-                    ),\n+                    lambda d: distribution.get_data_layout(\n+                        d.shape\n+                    ).backend_layout,\n                     data,\n                 )\n             yield _distribute_data(data, layouts)\n\n@@ -50,7 +50,7 @@ def distribute_value(value, tensor_layout):\n     pass\n \n \n-def _to_dtensor_mesh(device_mesh):\n+def _to_backend_mesh(device_mesh):\n     \"\"\"Convert the DeviceMesh to Tensorflow backend specific Mesh.\n \n     Args:\n@@ -65,7 +65,7 @@ def _to_dtensor_mesh(device_mesh):\n     )\n \n \n-def _to_dtensor_layout(tensor_layout):\n+def _to_backend_layout(tensor_layout):\n     \"\"\"Convert the TensorLayout to Tensorflow backend specific Sharding.\n \n     Args:\n@@ -83,5 +83,5 @@ def _to_dtensor_layout(tensor_layout):\n     sharding_specs = [\n         axis if axis else dtensor.UNSHARDED for axis in tensor_layout.axes\n     ]\n-    dtensor_mesh = _to_dtensor_mesh(tensor_layout.device_mesh)\n+    dtensor_mesh = tensor_layout.device_mesh.backend_mesh\n     return dtensor.Layout(sharding_specs=sharding_specs, mesh=dtensor_mesh)\n\n@@ -197,6 +197,12 @@ class DeviceMesh:\n     def devices(self):\n         return self._devices\n \n+    @property\n+    def backend_mesh(self):\n+        if not hasattr(self, \"_backend_mesh\"):\n+            self._backend_mesh = distribution_lib._to_backend_mesh(self)\n+        return self._backend_mesh\n+\n     def __repr__(self):\n         return (\n             f\"<{self.__class__.__name__} \"\n@@ -251,6 +257,12 @@ class TensorLayout:\n         self._device_mesh = device_mesh\n         self._validate_axes()\n \n+    @property\n+    def backend_layout(self):\n+        if not hasattr(self, \"_backend_layout\"):\n+            self._backend_layout = distribution_lib._to_backend_layout(self)\n+        return self._backend_layout\n+\n     def _validate_axes(self):\n         if self._device_mesh:\n             valid_axis_names = set(self._device_mesh.axis_names)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
