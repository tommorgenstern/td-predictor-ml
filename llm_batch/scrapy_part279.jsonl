{"custom_id": "scrapy#3cfa73b8b12dfe2dd1364af856d2ec46a486b48d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 1952 | Contributors (this commit): 22 | Commits (past 90d): 23 | Contributors (cumulative): 28 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,7 +21,7 @@ from scrapy.utils.log import (\n     log_scrapy_info,\n     LogCounterHandler,\n )\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n \n \n@@ -305,7 +305,8 @@ class CrawlerProcess(CrawlerRunner):\n             d.addBoth(self._stop_reactor)\n \n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n-        resolver_class.install_on_reactor(reactor, crawler=self)\n+        resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n+        resolver.install_on_reactor(reactor)\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n\n@@ -4,7 +4,6 @@ from twisted.internet.interfaces import IHostnameResolver, IResolutionReceiver,\n from zope.interface.declarations import implementer, provider\n \n from scrapy.utils.datatypes import LocalCache\n-from scrapy.utils.misc import create_instance\n \n \n # TODO: cache misses\n@@ -30,10 +29,8 @@ class CachingThreadedResolver(ThreadedResolver):\n             cache_size = 0\n         return cls(reactor, cache_size, crawler.settings.getfloat('DNS_TIMEOUT'))\n \n-    @classmethod\n-    def install_on_reactor(cls, reactor, crawler):\n-        resolver = create_instance(cls, None, crawler, reactor=reactor)\n-        reactor.installResolver(resolver)\n+    def install_on_reactor(self, reactor):\n+        reactor.installResolver(self)\n \n     def getHostByName(self, name, timeout=None):\n         if name in dnscache:\n@@ -72,10 +69,8 @@ class CachingHostnameResolver:\n             cache_size = 0\n         return cls(reactor, cache_size)\n \n-    @classmethod\n-    def install_on_reactor(cls, reactor, crawler):\n-        resolver = create_instance(cls, None, crawler, reactor=reactor)\n-        reactor.installNameResolver(resolver)\n+    def install_on_reactor(self, reactor):\n+        reactor.installNameResolver(self)\n \n     def resolveHostName(self, resolutionReceiver, hostName, portNumber=0,\n                         addressTypes=None, transportSemantics='TCP'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1040f581ec1ba3bcf8f08edb824d59c0e4a93700", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 1968 | Contributors (this commit): 22 | Commits (past 90d): 25 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -306,7 +306,7 @@ class CrawlerProcess(CrawlerRunner):\n \n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n         resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n-        resolver.install_on_reactor(reactor)\n+        resolver.install_on_reactor()\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n\n@@ -19,6 +19,7 @@ class CachingThreadedResolver(ThreadedResolver):\n     def __init__(self, reactor, cache_size, timeout):\n         super(CachingThreadedResolver, self).__init__(reactor)\n         dnscache.limit = cache_size\n+        self.reactor = reactor\n         self.timeout = timeout\n \n     @classmethod\n@@ -29,8 +30,8 @@ class CachingThreadedResolver(ThreadedResolver):\n             cache_size = 0\n         return cls(reactor, cache_size, crawler.settings.getfloat('DNS_TIMEOUT'))\n \n-    def install_on_reactor(self, reactor):\n-        reactor.installResolver(self)\n+    def install_on_reactor(self,):\n+        self.reactor.installResolver(self)\n \n     def getHostByName(self, name, timeout=None):\n         if name in dnscache:\n@@ -58,7 +59,8 @@ class CachingHostnameResolver:\n     \"\"\"\n \n     def __init__(self, reactor, cache_size):\n-        self.resolver = reactor.nameResolver\n+        self.reactor = reactor\n+        self.original_resolver = reactor.nameResolver\n         dnscache.limit = cache_size\n \n     @classmethod\n@@ -69,8 +71,8 @@ class CachingHostnameResolver:\n             cache_size = 0\n         return cls(reactor, cache_size)\n \n-    def install_on_reactor(self, reactor):\n-        reactor.installNameResolver(self)\n+    def install_on_reactor(self):\n+        self.reactor.installNameResolver(self)\n \n     def resolveHostName(self, resolutionReceiver, hostName, portNumber=0,\n                         addressTypes=None, transportSemantics='TCP'):\n@@ -95,7 +97,7 @@ class CachingHostnameResolver:\n         try:\n             result = dnscache[hostName]\n         except KeyError:\n-            result = self.resolver.resolveHostName(\n+            result = self.original_resolver.resolveHostName(\n                 CachingResolutionReceiver(),\n                 hostName,\n                 portNumber,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90e3bd8715701aeea9187837bc32925e3225288f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 39 | Churn Cumulative: 642 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,15 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class IPv6Spider(scrapy.Spider):\n+    name = \"ipv6_spider\"\n+    start_urls = [\"http://[::1]\"]\n+\n+\n+process = CrawlerProcess(settings={\n+    \"RETRY_ENABLED\": False,\n+    \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n+})\n+process.crawl(IPv6Spider)\n+process.start()\n\n@@ -0,0 +1,12 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class IPv6Spider(scrapy.Spider):\n+    name = \"ipv6_spider\"\n+    start_urls = [\"http://[::1]\"]\n+\n+\n+process = CrawlerProcess(settings={\"RETRY_ENABLED\": False})\n+process.crawl(IPv6Spider)\n+process.start()\n\n@@ -305,3 +305,15 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n         log = self.run_script('asyncio_enabled_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n         self.assertIn(\"DEBUG: Asyncio reactor is installed\", log)\n+\n+    def test_default_name_resolver(self):\n+        log = self.run_script('default_name_resolver.py')\n+        self.assertIn('Spider closed (finished)', log)\n+        self.assertIn(\"twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\", log)\n+        self.assertIn(\"'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,\", log)\n+\n+    def test_alternative_name_resolver(self):\n+        log = self.run_script('alternative_name_resolver.py')\n+        self.assertIn('Spider closed (finished)', log)\n+        self.assertIn(\"twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.\", log)\n+        self.assertIn(\"'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d487498cff893d2c507ef5ed0957f101dc3ab4a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 629 | Contributors (this commit): 10 | Commits (past 90d): 8 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -306,14 +306,20 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n         self.assertIn('Spider closed (finished)', log)\n         self.assertIn(\"DEBUG: Asyncio reactor is installed\", log)\n \n-    def test_default_name_resolver(self):\n+    def test_ipv6_default_name_resolver(self):\n         log = self.run_script('default_name_resolver.py')\n         self.assertIn('Spider closed (finished)', log)\n         self.assertIn(\"twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\", log)\n         self.assertIn(\"'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,\", log)\n \n-    def test_alternative_name_resolver(self):\n+    def test_ipv6_alternative_name_resolver(self):\n         log = self.run_script('alternative_name_resolver.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertIn(\"twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.\", log)\n-        self.assertIn(\"'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,\", log)\n+        self.assertTrue(any(\n+            \"twisted.internet.error.ConnectionRefusedError\" in log,\n+            \"twisted.internet.error.ConnectError\" in log,\n+        ))\n+        self.assertTrue(any(\n+            \"'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,\" in log,\n+            \"'downloader/exception_type_count/twisted.internet.error.ConnectError': 1,\" in log,\n+        ))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dee420a69cb5c8319a7df99a5765c5ca90a7063e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 637 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -315,11 +315,11 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n     def test_ipv6_alternative_name_resolver(self):\n         log = self.run_script('alternative_name_resolver.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertTrue(any(\n+        self.assertTrue(any([\n             \"twisted.internet.error.ConnectionRefusedError\" in log,\n             \"twisted.internet.error.ConnectError\" in log,\n-        ))\n-        self.assertTrue(any(\n+        ]))\n+        self.assertTrue(any([\n             \"'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,\" in log,\n             \"'downloader/exception_type_count/twisted.internet.error.ConnectError': 1,\" in log,\n-        ))\n+        ]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#41f7ebf3add25b2251082b206b062c00d1daba58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 269 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,7 +19,6 @@ class CachingThreadedResolver(ThreadedResolver):\n     def __init__(self, reactor, cache_size, timeout):\n         super(CachingThreadedResolver, self).__init__(reactor)\n         dnscache.limit = cache_size\n-        self.reactor = reactor\n         self.timeout = timeout\n \n     @classmethod\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eaa8ed02d05d0cc5e7be1f773a22755676c7fe56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1648 | Contributors (this commit): 60 | Commits (past 90d): 36 | Contributors (cumulative): 69 | DMM Complexity: None\n\nDIFF:\n@@ -78,7 +78,7 @@ class Scraper(object):\n     @defer.inlineCallbacks\n     def open_spider(self, spider):\n         \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n-        self.slot = Slot()\n+        self.slot = Slot(self.crawler.settings.getint('SCRAPER_SLOT_MAX_ACTIVE_SIZE'))\n         yield self.itemproc.open_spider(spider)\n \n     def close_spider(self, spider):\n\n@@ -254,6 +254,8 @@ SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'\n SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'\n SCHEDULER_PRIORITY_QUEUE = 'scrapy.pqueues.ScrapyPriorityQueue'\n \n+SCRAPER_SLOT_MAX_ACTIVE_SIZE = 5000000\n+\n SPIDER_LOADER_CLASS = 'scrapy.spiderloader.SpiderLoader'\n SPIDER_LOADER_WARN_ONLY = False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#06ab668ec7f880f9992dc669a374a6111cef5d04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 9/7 | Churn Δ: 14 | Churn Cumulative: 919 | Contributors (this commit): 29 | Commits (past 90d): 7 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -175,9 +175,9 @@ class FTPFeedStorage(BlockingFeedStorage):\n \n     def _store_in_thread(self, file):\n         ftp_store_file(\n-            self.path, file, self.host,\n-            self.port, self.username, \n-            self.password, self.use_active_mode\n+            path=self.path, file=file, host=self.host,\n+            port=self.port, username=self.username, \n+            password=self.password, use_active_mode=self.use_active_mode\n         )\n \n \n\n@@ -269,9 +269,9 @@ class FTPFilesStore(object):\n     def persist_file(self, path, buf, info, meta=None, headers=None):     \n         path = '%s/%s' % (self.basedir, path)\n         return threads.deferToThread(\n-            ftp_store_file, path,buf,\n-            self.host, self.port,self.username,\n-            self.password, self.USE_ACTIVE_MODE\n+            ftp_store_file, path=path, file=buf,\n+            host=self.host, port=self.port, username=self.username,\n+            password=self.password, use_active_mode=self.USE_ACTIVE_MODE\n         )\n             \n     def stat_file(self, path, info):\n\n@@ -17,7 +17,7 @@ def ftp_makedirs_cwd(ftp, path, first_call=True):\n             ftp.cwd(path)\n \n def ftp_store_file( \n-        path, file, host, port,\n+        *, path, file, host, port,\n         username, password, use_active_mode=False):\n     \"\"\"Opens a FTP connection with passed credentials,sets current directory\n     to the directory extracted from given path, then uploads the file to server\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9899414300b4a6491ea17418c3403aed08e0faf6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 275 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -94,14 +94,12 @@ class CachingHostnameResolver:\n                     dnscache[hostName] = self.resolution\n \n         try:\n-            result = dnscache[hostName]\n+            return dnscache[hostName]\n         except KeyError:\n-            result = self.original_resolver.resolveHostName(\n+            return self.original_resolver.resolveHostName(\n                 CachingResolutionReceiver(),\n                 hostName,\n                 portNumber,\n                 addressTypes,\n                 transportSemantics\n             )\n-        finally:\n-            return result\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c544c0d2b8356125d1a5465b44617aaaaeab0ea1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 44 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,7 +22,7 @@ def ftp_store_file(\n     \"\"\"Opens a FTP connection with passed credentials,sets current directory\n     to the directory extracted from given path, then uploads the file to server\n     \"\"\"\n-    ftp = FTP()\n+    with FTP() as ftp:\n         ftp.connect(host, port)\n         ftp.login(username, password)\n         if use_active_mode:\n@@ -31,4 +31,3 @@ def ftp_store_file(\n         dirname, filename = posixpath.split(path)\n         ftp_makedirs_cwd(ftp, dirname)\n         ftp.storbinary('STOR %s' % filename, file)\n-    ftp.quit()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f5d9eb15f8b50c64c44a7f859a953b92d7a33e6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 4 | Churn Cumulative: 324 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -2,10 +2,10 @@\n This module contains some assorted functions used in tests\n \"\"\"\n \n-import asyncio\n-import os\n from __future__ import absolute_import\n from posixpath import split\n+import asyncio\n+import os\n \n from importlib import import_module\n from twisted.trial.unittest import SkipTest\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40e0a11aa8dd499f725aaa206643aa36411fd514", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 20 | Churn Cumulative: 1696 | Contributors (this commit): 34 | Commits (past 90d): 20 | Contributors (cumulative): 68 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,18 +7,16 @@ See documentation in docs/topics/feed-exports.rst\n import os\n import sys\n import logging\n-import posixpath\n from tempfile import NamedTemporaryFile\n from datetime import datetime\n from urllib.parse import urlparse, unquote\n-from ftplib import FTP\n \n from zope.interface import Interface, implementer\n from twisted.internet import defer, threads\n from w3lib.url import file_uri_to_path\n \n from scrapy import signals\n-from scrapy.utils.ftp import ftp_makedirs_cwd, ftp_store_file\n+from scrapy.utils.ftp import ftp_store_file\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.log import failure_to_exc_info\n\n@@ -28,7 +28,7 @@ from scrapy.utils.python import to_bytes\n from scrapy.utils.request import referer_str\n from scrapy.utils.boto import is_botocore\n from scrapy.utils.datatypes import CaselessDict\n-from scrapy.utils.ftp import ftp_makedirs_cwd, ftp_store_file\n+from scrapy.utils.ftp import ftp_store_file\n \n \n logger = logging.getLogger(__name__)\n@@ -298,7 +298,7 @@ class FTPFilesStore(object):\n                 ftp.retrbinary('RETR %s' % file_path, m.update)\n                 return {'last_modified': last_modified, 'checksum': m.hexdigest()}\n             # The file doesn't exist\n-            except Exception as e :\n+            except Exception:\n                 return {}\n         return threads.deferToThread(_stat_file, path)\n \n\n@@ -17,6 +17,7 @@ def ftp_makedirs_cwd(ftp, path, first_call=True):\n         if first_call:\n             ftp.cwd(path)\n \n+\n def ftp_store_file(\n         *, path, file, host, port,\n         username, password, use_active_mode=False):\n\n@@ -66,8 +66,9 @@ def get_gcs_content_and_delete(bucket, path):\n     return content, acl, blob\n \n \n-def get_ftp_content_and_delete(path, host ,port,\n-        username, password, use_active_mode=False):\n+def get_ftp_content_and_delete(\n+        path, host, port,username,\n+        password, use_active_mode=False):\n     from ftplib import FTP\n     ftp = FTP()\n     ftp.connect(host, port)\n@@ -75,6 +76,7 @@ def get_ftp_content_and_delete(path, host ,port,\n     if use_active_mode:\n         ftp.set_pasv(False)\n     ftp_data = []\n+\n     def buffer_data(data):\n         ftp_data.append(data)\n     ftp.retrbinary('RETR %s' % path, buffer_data)\n\n@@ -367,6 +367,7 @@ class TestGCSFilesStore(unittest.TestCase):\n         self.assertEqual(blob.content_type, 'application/octet-stream')\n         self.assertIn(expected_policy, acl)\n \n+\n class TestFTPFileStore(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_persist(self):\n@@ -386,10 +387,12 @@ class TestFTPFileStore(unittest.TestCase):\n         self.assertIn('checksum', stat)\n         self.assertEqual(stat['checksum'], 'd113d66b2ec7258724a268bd88eef6b6')\n         path = '%s/%s' % (store.basedir, path)\n-        content = get_ftp_content_and_delete(path, store.host, store.port,\n+        content = get_ftp_content_and_delete(\n+            path, store.host, store.port,\n             store.username, store.password, store.USE_ACTIVE_MODE)\n         self.assertEqual(data.decode(), content)\n \n+\n class ItemWithFiles(Item):\n     file_urls = Field()\n     files = Field()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9e6d5573f1180bc70d7eca9f381204c238b3a550", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 578 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,6 @@ from collections import defaultdict\n from email.utils import parsedate_tz, mktime_tz\n from ftplib import FTP\n from io import BytesIO\n-from six.moves.urllib.parse import urlparse\n from urllib.parse import urlparse\n \n from twisted.internet import defer, threads\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80925ab845b7f55be97d9bb91015ceee90efc333", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 13 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 50 | Churn Cumulative: 2854 | Contributors (this commit): 28 | Commits (past 90d): 34 | Contributors (cumulative): 45 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import logging\n import re\n import warnings\n from io import BytesIO\n+from ipaddress import ip_address\n from time import time\n from urllib.parse import urldefrag\n \n@@ -382,7 +383,7 @@ class ScrapyAgent(object):\n     def _cb_bodyready(self, txresponse, request):\n         # deliverBody hangs for responses without body\n         if txresponse.length == 0:\n-            return txresponse, b'', None\n+            return txresponse, b'', None, None\n \n         maxsize = request.meta.get('download_maxsize', self._maxsize)\n         warnsize = request.meta.get('download_warnsize', self._warnsize)\n@@ -418,11 +419,11 @@ class ScrapyAgent(object):\n         return d\n \n     def _cb_bodydone(self, result, request, url):\n-        txresponse, body, flags = result\n+        txresponse, body, flags, ip_address = result\n         status = int(txresponse.code)\n         headers = Headers(txresponse.headers.getAllRawHeaders())\n         respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n-        return respcls(url=url, status=status, headers=headers, body=body, flags=flags)\n+        return respcls(url=url, status=status, headers=headers, body=body, flags=flags, ip_address=ip_address)\n \n \n @implementer(IBodyProducer)\n@@ -456,6 +457,11 @@ class _ResponseReader(protocol.Protocol):\n         self._fail_on_dataloss_warned = False\n         self._reached_warnsize = False\n         self._bytes_received = 0\n+        self._ip_address = None\n+\n+    def connectionMade(self):\n+        if self._ip_address is None:\n+            self._ip_address = ip_address(self.transport._producer.getPeer().host)\n \n     def dataReceived(self, bodyBytes):\n         # This maybe called several times after cancel was called with buffered data.\n@@ -488,16 +494,16 @@ class _ResponseReader(protocol.Protocol):\n \n         body = self._bodybuf.getvalue()\n         if reason.check(ResponseDone):\n-            self._finished.callback((self._txresponse, body, None))\n+            self._finished.callback((self._txresponse, body, None, self._ip_address))\n             return\n \n         if reason.check(PotentialDataLoss):\n-            self._finished.callback((self._txresponse, body, ['partial']))\n+            self._finished.callback((self._txresponse, body, ['partial'], self._ip_address))\n             return\n \n         if reason.check(ResponseFailed) and any(r.check(_DataLoss) for r in reason.value.reasons):\n             if not self._fail_on_dataloss:\n-                self._finished.callback((self._txresponse, body, ['dataloss']))\n+                self._finished.callback((self._txresponse, body, ['dataloss'], self._ip_address))\n                 return\n \n             elif not self._fail_on_dataloss_warned:\n\n@@ -17,13 +17,14 @@ from scrapy.utils.trackref import object_ref\n \n class Response(object_ref):\n \n-    def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None):\n+    def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None, ip_address=None):\n         self.headers = Headers(headers or {})\n         self.status = int(status)\n         self._set_body(body)\n         self._set_url(url)\n         self.request = request\n         self.flags = [] if flags is None else list(flags)\n+        self.ip_address = ip_address\n \n     @property\n     def meta(self):\n@@ -76,7 +77,7 @@ class Response(object_ref):\n         \"\"\"Create a new Response with the same attributes except for those\n         given new values.\n         \"\"\"\n-        for x in ['url', 'status', 'headers', 'body', 'request', 'flags']:\n+        for x in ['url', 'status', 'headers', 'body', 'request', 'flags', 'ip_address']:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n\n@@ -1,5 +1,7 @@\n import json\n import logging\n+from ipaddress import IPv4Address\n+from urllib.parse import urlparse\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -308,3 +310,28 @@ with multiples lines\n         self.assertIn(\"[callback] status 201\", str(log))\n         self.assertIn(\"[errback] status 404\", str(log))\n         self.assertIn(\"[errback] status 500\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_dns_server_ip_address(self):\n+        from socket import gethostbyname\n+\n+        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        url = 'https://example.org'\n+        yield crawler.crawl(seed=url)\n+        ip_address = crawler.spider.meta['responses'][0].ip_address\n+        self.assertIsInstance(ip_address, IPv4Address)\n+        self.assertEqual(str(ip_address), gethostbyname(urlparse(url).netloc))\n+\n+        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        url = self.mockserver.url('/status?n=200')\n+        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+        ip_address = crawler.spider.meta['responses'][0].ip_address\n+        self.assertIsNone(ip_address)\n+\n+        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        url = self.mockserver.url('/echo?body=test')\n+        expected_netloc, _ = urlparse(url).netloc.split(':')\n+        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+        ip_address = crawler.spider.meta['responses'][0].ip_address\n+        self.assertIsInstance(ip_address, IPv4Address)\n+        self.assertEqual(str(ip_address), gethostbyname(expected_netloc))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8da7e296691d2b4eb63e2a442bb600e03e5766f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 69 | Churn Cumulative: 1339 | Contributors (this commit): 13 | Commits (past 90d): 18 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,51 @@\n+from urllib.parse import urlparse\n+\n+from twisted.internet import defer\n+from twisted.internet.base import ThreadedResolver\n+from twisted.internet.interfaces import IResolverSimple\n+from zope.interface.declarations import implementer\n+\n+from scrapy import Spider, Request\n+from scrapy.crawler import CrawlerProcess\n+\n+from tests.mockserver import MockServer\n+\n+\n+@implementer(IResolverSimple)\n+class MockThreadedResolver(ThreadedResolver):\n+    \"\"\"\n+    Resolves all names to localhost\n+    \"\"\"\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, reactor):\n+        return cls(reactor)\n+\n+    def install_on_reactor(self,):\n+        self.reactor.installResolver(self)\n+\n+    def getHostByName(self, name, timeout=None):\n+        return defer.succeed(\"127.0.0.1\")\n+\n+\n+class LocalhostSpider(Spider):\n+    name = \"localhost_spider\"\n+\n+    def start_requests(self):\n+        yield Request(self.url)\n+\n+    def parse(self, response):\n+        netloc = urlparse(response.url).netloc\n+        self.logger.info(\"Host: %s\" % netloc.split(\":\")[0])\n+        self.logger.info(\"Type: %s\" % type(response.ip_address))\n+        self.logger.info(\"IP address: %s\" % response.ip_address)\n+\n+\n+with MockServer() as mockserver:\n+    settings = {\"DNS_RESOLVER\": __name__ + \".MockThreadedResolver\"}\n+    process = CrawlerProcess(settings)\n+\n+    port = urlparse(mockserver.http_address).port\n+    url = \"http://not.a.real.domain:{port}/echo?body=test\".format(port=port)\n+    process.crawl(LocalhostSpider, url=url)\n+    process.start()\n\n@@ -1,6 +1,7 @@\n import json\n import logging\n from ipaddress import IPv4Address\n+from socket import gethostbyname\n from urllib.parse import urlparse\n \n from testfixtures import LogCapture\n@@ -313,15 +314,6 @@ with multiples lines\n \n     @defer.inlineCallbacks\n     def test_dns_server_ip_address(self):\n-        from socket import gethostbyname\n-\n-        crawler = self.runner.create_crawler(SingleRequestSpider)\n-        url = 'https://example.org'\n-        yield crawler.crawl(seed=url)\n-        ip_address = crawler.spider.meta['responses'][0].ip_address\n-        self.assertIsInstance(ip_address, IPv4Address)\n-        self.assertEqual(str(ip_address), gethostbyname(urlparse(url).netloc))\n-\n         crawler = self.runner.create_crawler(SingleRequestSpider)\n         url = self.mockserver.url('/status?n=200')\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n\n@@ -107,6 +107,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n \n     def test_spider_custom_settings_log_level(self):\n         log_file = self.mktemp()\n+\n         class MySpider(scrapy.Spider):\n             name = 'spider'\n             custom_settings = {\n@@ -323,3 +324,10 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n             \"'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,\" in log,\n             \"'downloader/exception_type_count/twisted.internet.error.ConnectError': 1,\" in log,\n         ]))\n+\n+    def test_response_ip_address(self):\n+        log = self.run_script(\"ip_address.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"Host: not.a.real.domain\", log)\n+        self.assertIn(\"Type: <class 'ipaddress.IPv4Address'>\", log)\n+        self.assertIn(\"IP address: 127.0.0.1\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#72b8613ee9827af031862bd84f1bea9acefcbebe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 26 | Churn Cumulative: 1936 | Contributors (this commit): 26 | Commits (past 90d): 8 | Contributors (cumulative): 29 | DMM Complexity: 0.23529411764705882\n\nDIFF:\n@@ -16,6 +16,7 @@ from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n from zope.interface import implementer\n \n+from scrapy import signals\n from scrapy.core.downloader.tls import openssl_methods\n from scrapy.core.downloader.webclient import _parse\n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -32,6 +33,7 @@ class HTTP11DownloadHandler:\n     lazy = False\n \n     def __init__(self, settings, crawler=None):\n+        self.crawler = crawler\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n         self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self._pool._factory.noisy = False\n@@ -76,6 +78,7 @@ class HTTP11DownloadHandler:\n             maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),\n             warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),\n             fail_on_dataloss=self._fail_on_dataloss,\n+            crawler=self.crawler,\n         )\n         return agent.download_request(request)\n \n@@ -272,7 +275,7 @@ class ScrapyAgent(object):\n     _TunnelingAgent = TunnelingAgent\n \n     def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,\n-                 maxsize=0, warnsize=0, fail_on_dataloss=True):\n+                 maxsize=0, warnsize=0, fail_on_dataloss=True, crawler=None):\n         self._contextFactory = contextFactory\n         self._connectTimeout = connectTimeout\n         self._bindAddress = bindAddress\n@@ -281,6 +284,7 @@ class ScrapyAgent(object):\n         self._warnsize = warnsize\n         self._fail_on_dataloss = fail_on_dataloss\n         self._txresponse = None\n+        self._crawler = crawler\n \n     def _get_agent(self, request, timeout):\n         bindaddress = request.meta.get('bindaddress') or self._bindAddress\n@@ -409,7 +413,15 @@ class ScrapyAgent(object):\n \n         d = defer.Deferred(_cancel)\n         txresponse.deliverBody(\n-            _ResponseReader(d, txresponse, request, maxsize, warnsize, fail_on_dataloss)\n+            _ResponseReader(\n+                d,\n+                txresponse,\n+                request,\n+                maxsize,\n+                warnsize,\n+                fail_on_dataloss,\n+                self._crawler,\n+            )\n         )\n \n         # save response for timeouts\n@@ -445,7 +457,7 @@ class _RequestBodyProducer(object):\n \n class _ResponseReader(protocol.Protocol):\n \n-    def __init__(self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss):\n+    def __init__(self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss, crawler):\n         self._finished = finished\n         self._txresponse = txresponse\n         self._request = request\n@@ -456,6 +468,7 @@ class _ResponseReader(protocol.Protocol):\n         self._fail_on_dataloss_warned = False\n         self._reached_warnsize = False\n         self._bytes_received = 0\n+        self._crawler = crawler\n \n     def dataReceived(self, bodyBytes):\n         # This maybe called several times after cancel was called with buffered data.\n@@ -465,6 +478,12 @@ class _ResponseReader(protocol.Protocol):\n         self._bodybuf.write(bodyBytes)\n         self._bytes_received += len(bodyBytes)\n \n+        self._crawler.signals.send_catch_log(\n+            signal=signals.bytes_received,\n+            data=bodyBytes,\n+            request=self._request,\n+        )\n+\n         if self._maxsize and self._bytes_received > self._maxsize:\n             logger.error(\"Received (%(bytes)s) bytes larger than download \"\n                          \"max size (%(maxsize)s) in request %(request)s.\",\n\n@@ -16,6 +16,7 @@ request_dropped = object()\n request_reached_downloader = object()\n response_received = object()\n response_downloaded = object()\n+bytes_received = object()\n item_scraped = object()\n item_dropped = object()\n item_error = object()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cab449b1952020b86fbe2915a537150fc885c567", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 212 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -107,7 +107,7 @@ class CrawlerRun(object):\n         self.reqreached = []\n         self.itemerror = []\n         self.itemresp = []\n-        self.signals_catched = {}\n+        self.signals_caught = {}\n         self.spider_class = spider_class\n \n     def run(self):\n@@ -172,7 +172,7 @@ class CrawlerRun(object):\n         signalargs = kwargs.copy()\n         sig = signalargs.pop('signal')\n         signalargs.pop('sender', None)\n-        self.signals_catched[sig] = signalargs\n+        self.signals_caught[sig] = signalargs\n \n \n class EngineTest(unittest.TestCase):\n@@ -186,7 +186,7 @@ class EngineTest(unittest.TestCase):\n             self._assert_scheduled_requests(urls_to_visit=8)\n             self._assert_downloaded_responses()\n             self._assert_scraped_items()\n-            self._assert_signals_catched()\n+            self._assert_signals_caught()\n \n     @defer.inlineCallbacks\n     def test_crawler_dupefilter(self):\n@@ -263,19 +263,19 @@ class EngineTest(unittest.TestCase):\n                 self.assertEqual('Item 2 name', item['name'])\n                 self.assertEqual('200', item['price'])\n \n-    def _assert_signals_catched(self):\n-        assert signals.engine_started in self.run.signals_catched\n-        assert signals.engine_stopped in self.run.signals_catched\n-        assert signals.spider_opened in self.run.signals_catched\n-        assert signals.spider_idle in self.run.signals_catched\n-        assert signals.spider_closed in self.run.signals_catched\n+    def _assert_signals_caught(self):\n+        assert signals.engine_started in self.run.signals_caught\n+        assert signals.engine_stopped in self.run.signals_caught\n+        assert signals.spider_opened in self.run.signals_caught\n+        assert signals.spider_idle in self.run.signals_caught\n+        assert signals.spider_closed in self.run.signals_caught\n \n         self.assertEqual({'spider': self.run.spider},\n-                         self.run.signals_catched[signals.spider_opened])\n+                         self.run.signals_caught[signals.spider_opened])\n         self.assertEqual({'spider': self.run.spider},\n-                         self.run.signals_catched[signals.spider_idle])\n+                         self.run.signals_caught[signals.spider_idle])\n         self.assertEqual({'spider': self.run.spider, 'reason': 'finished'},\n-                         self.run.signals_catched[signals.spider_closed])\n+                         self.run.signals_caught[signals.spider_closed])\n \n     @defer.inlineCallbacks\n     def test_close_downloader(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bda37e38bd53d5aae691b56d4136fbff99f78158", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 53 | Churn Cumulative: 265 | Contributors (this commit): 12 | Commits (past 90d): 7 | Contributors (cumulative): 12 | DMM Complexity: 0.13513513513513514\n\nDIFF:\n@@ -13,22 +13,24 @@ module with the ``runserver`` argument::\n import os\n import re\n import sys\n+from collections import defaultdict\n from urllib.parse import urlparse\n \n from twisted.internet import reactor, defer\n-from twisted.web import server, static, util\n from twisted.trial import unittest\n+from twisted.web import server, static, util\n+from pydispatch import dispatcher\n \n from scrapy import signals\n from scrapy.core.engine import ExecutionEngine\n-from scrapy.utils.test import get_crawler\n-from pydispatch import dispatcher\n-from tests import tests_datadir\n-from scrapy.spiders import Spider\n+from scrapy.http import Request\n from scrapy.item import Item, Field\n from scrapy.linkextractors import LinkExtractor\n-from scrapy.http import Request\n+from scrapy.spiders import Spider\n from scrapy.utils.signal import disconnect_all\n+from scrapy.utils.test import get_crawler\n+\n+from tests import tests_datadir, get_testdata\n \n \n class TestItem(Item):\n@@ -107,6 +109,7 @@ class CrawlerRun(object):\n         self.reqreached = []\n         self.itemerror = []\n         self.itemresp = []\n+        self.bytes = defaultdict(lambda: b\"\")\n         self.signals_caught = {}\n         self.spider_class = spider_class\n \n@@ -124,6 +127,7 @@ class CrawlerRun(object):\n         self.crawler = get_crawler(self.spider_class)\n         self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n         self.crawler.signals.connect(self.item_error, signals.item_error)\n+        self.crawler.signals.connect(self.bytes_received, signals.bytes_received)\n         self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n         self.crawler.signals.connect(self.request_dropped, signals.request_dropped)\n         self.crawler.signals.connect(self.request_reached, signals.request_reached_downloader)\n@@ -155,6 +159,9 @@ class CrawlerRun(object):\n     def item_scraped(self, item, spider, response):\n         self.itemresp.append((item, response))\n \n+    def bytes_received(self, data, request):\n+        self.bytes[request] += data\n+\n     def request_scheduled(self, request, spider):\n         self.reqplug.append((request, spider))\n \n@@ -187,6 +194,7 @@ class EngineTest(unittest.TestCase):\n             self._assert_downloaded_responses()\n             self._assert_scraped_items()\n             self._assert_signals_caught()\n+            self._assert_bytes_received()\n \n     @defer.inlineCallbacks\n     def test_crawler_dupefilter(self):\n@@ -263,6 +271,39 @@ class EngineTest(unittest.TestCase):\n                 self.assertEqual('Item 2 name', item['name'])\n                 self.assertEqual('200', item['price'])\n \n+    def _assert_bytes_received(self):\n+        self.assertEqual(8, len(self.run.bytes))\n+        for request, data in self.run.bytes.items():\n+            if self.run.getpath(request.url) == \"/\":\n+                self.assertEqual(data, get_testdata(\"test_site\", \"index.html\"))\n+            elif self.run.getpath(request.url) == \"/item1.html\":\n+                self.assertEqual(data, get_testdata(\"test_site\", \"item1.html\"))\n+            elif self.run.getpath(request.url) == \"/item2.html\":\n+                self.assertEqual(data, get_testdata(\"test_site\", \"item2.html\"))\n+            elif self.run.getpath(request.url) == \"/redirected\":\n+                self.assertEqual(data, b\"Redirected here\")\n+            elif self.run.getpath(request.url) == '/redirect':\n+                self.assertEqual(data,\n+                    b\"\\n<html>\\n\"\n+                    b\"    <head>\\n\"\n+                    b\"        <meta http-equiv=\\\"refresh\\\" content=\\\"0;URL=/redirected\\\">\\n\"\n+                    b\"    </head>\\n\"\n+                    b\"    <body bgcolor=\\\"#FFFFFF\\\" text=\\\"#000000\\\">\\n\"\n+                    b\"    <a href=\\\"/redirected\\\">click here</a>\\n\"\n+                    b\"    </body>\\n\"\n+                    b\"</html>\\n\"\n+                )\n+            elif self.run.getpath(request.url) == \"/tem999.html\":\n+                self.assertEqual(data,\n+                    b\"\\n<html>\\n\"\n+                    b\"  <head><title>404 - No Such Resource</title></head>\\n\"\n+                    b\"  <body>\\n\"\n+                    b\"    <h1>No Such Resource</h1>\\n\"\n+                    b\"    <p>File not found.</p>\\n\"\n+                    b\"  </body>\\n\"\n+                    b\"</html>\\n\"\n+                )\n+\n     def _assert_signals_caught(self):\n         assert signals.engine_started in self.run.signals_caught\n         assert signals.engine_stopped in self.run.signals_caught\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#89483ce9f709e230ee5ff9050d206430d2d17c9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 271 | Contributors (this commit): 12 | Commits (past 90d): 8 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -283,7 +283,8 @@ class EngineTest(unittest.TestCase):\n             elif self.run.getpath(request.url) == \"/redirected\":\n                 self.assertEqual(data, b\"Redirected here\")\n             elif self.run.getpath(request.url) == '/redirect':\n-                self.assertEqual(data,\n+                self.assertEqual(\n+                    data,\n                     b\"\\n<html>\\n\"\n                     b\"    <head>\\n\"\n                     b\"        <meta http-equiv=\\\"refresh\\\" content=\\\"0;URL=/redirected\\\">\\n\"\n@@ -294,7 +295,8 @@ class EngineTest(unittest.TestCase):\n                     b\"</html>\\n\"\n                 )\n             elif self.run.getpath(request.url) == \"/tem999.html\":\n-                self.assertEqual(data,\n+                self.assertEqual(\n+                    data,\n                     b\"\\n<html>\\n\"\n                     b\"  <head><title>404 - No Such Resource</title></head>\\n\"\n                     b\"  <body>\\n\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dbe20a863ff63dce937b2d3b159782d8268e6838", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2161 | Contributors (this commit): 27 | Commits (past 90d): 17 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -482,6 +482,7 @@ class _ResponseReader(protocol.Protocol):\n             signal=signals.bytes_received,\n             data=bodyBytes,\n             request=self._request,\n+            spider=self._crawler.spider,\n         )\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n\n@@ -159,7 +159,7 @@ class CrawlerRun(object):\n     def item_scraped(self, item, spider, response):\n         self.itemresp.append((item, response))\n \n-    def bytes_received(self, data, request):\n+    def bytes_received(self, data, request, spider):\n         self.bytes[request] += data\n \n     def request_scheduled(self, request, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
