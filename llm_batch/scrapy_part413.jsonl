{"custom_id": "scrapy#1ddcb568e27cda10db4d0640aa42d020d4624a30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 37 | Files Changed: 6 | Hunks: 23 | Methods Changed: 14 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 125 | Churn Cumulative: 9447 | Contributors (this commit): 74 | Commits (past 90d): 19 | Contributors (cumulative): 140 | DMM Complexity: 1.0\n\nDIFF:\n@@ -127,9 +127,7 @@ class ExecutionEngine:\n         if self.running:\n             raise RuntimeError(\"Engine already running\")\n         self.start_time = time()\n-        await maybe_deferred_to_future(\n-            self.signals.send_catch_log_deferred(signal=signals.engine_started)\n-        )\n+        await self.signals.send_catch_log_async(signal=signals.engine_started)\n         self.running = True\n         self._closewait: Deferred[None] = Deferred()\n         if _start_request_processing:\n@@ -141,9 +139,7 @@ class ExecutionEngine:\n \n         @deferred_f_from_coro_f\n         async def _finish_stopping_engine(_: Any) -> None:\n-            await maybe_deferred_to_future(\n-                self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n-            )\n+            await self.signals.send_catch_log_async(signal=signals.engine_stopped)\n             self._closewait.callback(None)\n \n         if not self.running:\n@@ -415,9 +411,7 @@ class ExecutionEngine:\n         await maybe_deferred_to_future(self.scraper.open_spider(spider))\n         assert self.crawler.stats\n         self.crawler.stats.open_spider(spider)\n-        await maybe_deferred_to_future(\n-            self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n-        )\n+        await self.signals.send_catch_log_async(signals.spider_opened, spider=spider)\n \n     def _spider_idle(self) -> None:\n         \"\"\"\n\n@@ -392,15 +392,13 @@ class Scraper:\n                 logger.log(\n                     *logformatter_adapter(logkws), extra={\"spider\": self.crawler.spider}\n                 )\n-            await maybe_deferred_to_future(\n-                self.signals.send_catch_log_deferred(\n+            await self.signals.send_catch_log_async(\n                 signal=signals.item_dropped,\n                 item=item,\n                 response=response,\n                 spider=self.crawler.spider,\n                 exception=ex,\n             )\n-            )\n         except Exception as ex:\n             logkws = self.logformatter.item_error(\n                 item, ex, response, self.crawler.spider\n@@ -410,28 +408,24 @@ class Scraper:\n                 extra={\"spider\": self.crawler.spider},\n                 exc_info=True,\n             )\n-            await maybe_deferred_to_future(\n-                self.signals.send_catch_log_deferred(\n+            await self.signals.send_catch_log_async(\n                 signal=signals.item_error,\n                 item=item,\n                 response=response,\n                 spider=self.crawler.spider,\n                 failure=Failure(),\n             )\n-            )\n         else:\n             logkws = self.logformatter.scraped(output, response, self.crawler.spider)\n             if logkws is not None:\n                 logger.log(\n                     *logformatter_adapter(logkws), extra={\"spider\": self.crawler.spider}\n                 )\n-            await maybe_deferred_to_future(\n-                self.signals.send_catch_log_deferred(\n+            await self.signals.send_catch_log_async(\n                 signal=signals.item_scraped,\n                 item=output,\n                 response=response,\n                 spider=self.crawler.spider,\n             )\n-            )\n         finally:\n             self.slot.itemproc_size -= 1\n\n@@ -531,9 +531,7 @@ class FeedExporter:\n             await maybe_deferred_to_future(DeferredList(self._pending_deferreds))\n \n         # Send FEED_EXPORTER_CLOSED signal\n-        await maybe_deferred_to_future(\n-            self.crawler.signals.send_catch_log_deferred(signals.feed_exporter_closed)\n-        )\n+        await self.crawler.signals.send_catch_log_async(signals.feed_exporter_closed)\n \n     def _close_slot(self, slot: FeedSlot, spider: Spider) -> Deferred[None] | None:\n         def get_file(slot_: FeedSlot) -> IO[bytes]:\n\n@@ -53,11 +53,10 @@ class SignalManager:\n         self, signal: Any, **kwargs: Any\n     ) -> Deferred[list[tuple[Any, Any]]]:\n         \"\"\"\n-        Like :meth:`send_catch_log` but supports returning\n-        :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n+        Like :meth:`send_catch_log` but supports asynchronous signal handlers.\n \n         Returns a Deferred that gets fired once all signal handlers\n-        deferreds were fired. Send a signal, catch exceptions and log them.\n+        have finished. Send a signal, catch exceptions and log them.\n \n         The keyword arguments are passed to the signal handlers (connected\n         through the :meth:`connect` method).\n@@ -65,6 +64,21 @@ class SignalManager:\n         kwargs.setdefault(\"sender\", self.sender)\n         return _signal.send_catch_log_deferred(signal, **kwargs)\n \n+    async def send_catch_log_async(\n+        self, signal: Any, **kwargs: Any\n+    ) -> list[tuple[Any, Any]]:\n+        \"\"\"\n+        Like :meth:`send_catch_log` but supports asynchronous signal handlers.\n+\n+        Returns a coroutine that completes once all signal handlers\n+        have finished. Send a signal, catch exceptions and log them.\n+\n+        The keyword arguments are passed to the signal handlers (connected\n+        through the :meth:`connect` method).\n+        \"\"\"\n+        kwargs.setdefault(\"sender\", self.sender)\n+        return await _signal.send_catch_log_async(signal, **kwargs)\n+\n     def disconnect_all(self, signal: Any, **kwargs: Any) -> None:\n         \"\"\"\n         Disconnect all receivers from the given signal.\n\n@@ -3,7 +3,7 @@\n from __future__ import annotations\n \n import logging\n-from collections.abc import Sequence\n+from collections.abc import Generator, Sequence\n from typing import Any as TypingAny\n \n from pydispatch.dispatcher import (\n@@ -14,11 +14,11 @@ from pydispatch.dispatcher import (\n     liveReceivers,\n )\n from pydispatch.robustapply import robustApply\n-from twisted.internet.defer import Deferred, DeferredList\n+from twisted.internet.defer import Deferred, DeferredList, inlineCallbacks\n from twisted.python.failure import Failure\n \n from scrapy.exceptions import StopDownload\n-from scrapy.utils.defer import maybeDeferred_coro\n+from scrapy.utils.defer import maybe_deferred_to_future, maybeDeferred_coro\n from scrapy.utils.log import failure_to_exc_info\n \n logger = logging.getLogger(__name__)\n@@ -66,18 +66,19 @@ def send_catch_log(\n     return responses\n \n \n+@inlineCallbacks\n def send_catch_log_deferred(\n     signal: TypingAny = Any,\n     sender: TypingAny = Anonymous,\n     *arguments: TypingAny,\n     **named: TypingAny,\n-) -> Deferred[list[tuple[TypingAny, TypingAny]]]:\n-    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n-    Returns a deferred that gets fired once all signal handlers deferreds were\n-    fired.\n+) -> Generator[Deferred[TypingAny], TypingAny, list[tuple[TypingAny, TypingAny]]]:\n+    \"\"\"Like send_catch_log but supports asynchronous signal handlers.\n+\n+    Returns a deferred that gets fired once all signal handlers have finished.\n     \"\"\"\n \n-    def logerror(failure: Failure, recv: Any) -> Failure:\n+    def logerror(failure: Failure, recv: TypingAny) -> Failure:\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\n                 \"Error caught on signal handler: %(receiver)s\",\n@@ -103,11 +104,24 @@ def send_catch_log_deferred(\n             )\n         )\n         dfds.append(d2)\n-    dl = DeferredList(dfds)\n-    d3: Deferred[list[tuple[TypingAny, TypingAny]]] = dl.addCallback(\n-        lambda out: [x[1] for x in out]\n+\n+    results = yield DeferredList(dfds)\n+    return [result[1] for result in results]\n+\n+\n+async def send_catch_log_async(\n+    signal: TypingAny = Any,\n+    sender: TypingAny = Anonymous,\n+    *arguments: TypingAny,\n+    **named: TypingAny,\n+) -> list[tuple[TypingAny, TypingAny]]:\n+    \"\"\"Like send_catch_log but supports asynchronous signal handlers.\n+\n+    Returns a coroutine that completes once all signal handlers have finished.\n+    \"\"\"\n+    return await maybe_deferred_to_future(\n+        send_catch_log_deferred(signal, sender, *arguments, **named)\n     )\n-    return d3\n \n \n def disconnect_all(signal: TypingAny = Any, sender: TypingAny = Any) -> None:\n\n@@ -7,7 +7,12 @@ from twisted.internet import defer, reactor\n from twisted.python.failure import Failure\n from twisted.trial import unittest\n \n-from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n+from scrapy.utils.defer import deferred_from_coro\n+from scrapy.utils.signal import (\n+    send_catch_log,\n+    send_catch_log_async,\n+    send_catch_log_deferred,\n+)\n from scrapy.utils.test import get_from_asyncio_queue\n \n \n@@ -85,6 +90,38 @@ class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n         return await get_from_asyncio_queue(\"OK\")\n \n \n+class SendCatchLogAsyncTest(TestSendCatchLog):\n+    def _get_result(self, signal, *a, **kw):\n+        return deferred_from_coro(send_catch_log_async(signal, *a, **kw))\n+\n+\n+class SendCatchLogAsyncTest2(SendCatchLogAsyncTest):\n+    def ok_handler(self, arg, handlers_called):\n+        handlers_called.add(self.ok_handler)\n+        assert arg == \"test\"\n+        d = defer.Deferred()\n+        reactor.callLater(0, d.callback, \"OK\")\n+        return d\n+\n+\n+@pytest.mark.usefixtures(\"reactor_pytest\")\n+class SendCatchLogAsyncAsyncDefTest(SendCatchLogAsyncTest):\n+    async def ok_handler(self, arg, handlers_called):\n+        handlers_called.add(self.ok_handler)\n+        assert arg == \"test\"\n+        await defer.succeed(42)\n+        return \"OK\"\n+\n+\n+@pytest.mark.only_asyncio\n+class SendCatchLogAsyncAsyncioTest(SendCatchLogAsyncTest):\n+    async def ok_handler(self, arg, handlers_called):\n+        handlers_called.add(self.ok_handler)\n+        assert arg == \"test\"\n+        await asyncio.sleep(0.2)\n+        return await get_from_asyncio_queue(\"OK\")\n+\n+\n class TestSendCatchLog2:\n     def test_error_logged_if_deferred_not_supported(self):\n         def test_handler():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bf1bfaaa3e584b085f78e9f89ab03c22cfbb3e59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 610 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,7 +53,8 @@ class SignalManager:\n         self, signal: Any, **kwargs: Any\n     ) -> Deferred[list[tuple[Any, Any]]]:\n         \"\"\"\n-        Like :meth:`send_catch_log` but supports asynchronous signal handlers.\n+        Like :meth:`send_catch_log` but supports :ref:`asynchronous signal\n+        handlers <signal-deferred>`.\n \n         Returns a Deferred that gets fired once all signal handlers\n         have finished. Send a signal, catch exceptions and log them.\n@@ -68,7 +69,8 @@ class SignalManager:\n         self, signal: Any, **kwargs: Any\n     ) -> list[tuple[Any, Any]]:\n         \"\"\"\n-        Like :meth:`send_catch_log` but supports asynchronous signal handlers.\n+        Like :meth:`send_catch_log` but supports :ref:`asynchronous signal\n+        handlers <signal-deferred>`.\n \n         Returns a coroutine that completes once all signal handlers\n         have finished. Send a signal, catch exceptions and log them.\n\n@@ -30,7 +30,7 @@ def send_catch_log(\n     *arguments: TypingAny,\n     **named: TypingAny,\n ) -> list[tuple[TypingAny, TypingAny]]:\n-    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n+    \"\"\"Like ``pydispatcher.robust.sendRobust()`` but it also logs errors and returns\n     Failures instead of exceptions.\n     \"\"\"\n     dont_log = named.pop(\"dont_log\", ())\n@@ -73,7 +73,8 @@ def send_catch_log_deferred(\n     *arguments: TypingAny,\n     **named: TypingAny,\n ) -> Generator[Deferred[TypingAny], TypingAny, list[tuple[TypingAny, TypingAny]]]:\n-    \"\"\"Like send_catch_log but supports asynchronous signal handlers.\n+    \"\"\"Like :func:`send_catch_log` but supports :ref:`asynchronous signal handlers\n+    <signal-deferred>`.\n \n     Returns a deferred that gets fired once all signal handlers have finished.\n     \"\"\"\n@@ -115,7 +116,8 @@ async def send_catch_log_async(\n     *arguments: TypingAny,\n     **named: TypingAny,\n ) -> list[tuple[TypingAny, TypingAny]]:\n-    \"\"\"Like send_catch_log but supports asynchronous signal handlers.\n+    \"\"\"Like :func:`send_catch_log` but supports :ref:`asynchronous signal handlers\n+    <signal-deferred>`.\n \n     Returns a coroutine that completes once all signal handlers have finished.\n     \"\"\"\n@@ -126,7 +128,7 @@ async def send_catch_log_async(\n \n def disconnect_all(signal: TypingAny = Any, sender: TypingAny = Any) -> None:\n     \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n-    tests\n+    tests.\n     \"\"\"\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n         disconnect(receiver, signal=signal, sender=sender)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b9caaf8a63bc3280645dca2788a8c4ed1a556769", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 212 | Lines Deleted: 41 | Files Changed: 2 | Hunks: 39 | Methods Changed: 41 | Complexity Δ (Sum/Max): 24/25 | Churn Δ: 253 | Churn Cumulative: 1830 | Contributors (this commit): 30 | Commits (past 90d): 6 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,14 +10,13 @@ import warnings\n from asyncio import Future\n from collections.abc import Awaitable, Coroutine, Iterable, Iterator\n from functools import wraps\n-from types import CoroutineType\n-from typing import TYPE_CHECKING, Any, Generic, TypeVar, Union, cast, overload\n+from typing import TYPE_CHECKING, Any, Generic, TypeVar, cast, overload\n \n-from twisted.internet import defer\n from twisted.internet.defer import (\n     Deferred,\n     DeferredList,\n-    ensureDeferred,\n+    fail,\n+    succeed,\n )\n from twisted.internet.task import Cooperator\n from twisted.python import failure\n@@ -315,7 +314,7 @@ def process_parallel(\n     \"\"\"Return a Deferred with the output of all successful calls to the given\n     callbacks\n     \"\"\"\n-    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n+    dfds = [succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n     d: Deferred[list[tuple[bool, _T2]]] = DeferredList(\n         dfds, fireOnOneErrback=True, consumeErrors=True\n     )\n@@ -366,27 +365,24 @@ async def aiter_errback(\n             errback(failure.Failure(), *a, **kw)\n \n \n-_CT = TypeVar(\"_CT\", bound=Union[Awaitable, CoroutineType, Future])\n+@overload\n+def deferred_from_coro(o: Awaitable[_T]) -> Deferred[_T]: ...\n \n \n @overload\n-def deferred_from_coro(o: _CT) -> Deferred: ...\n+def deferred_from_coro(o: _T2) -> _T2: ...\n \n \n-@overload\n-def deferred_from_coro(o: _T) -> _T: ...\n-\n-\n-def deferred_from_coro(o: _T) -> Deferred | _T:\n+def deferred_from_coro(o: Awaitable[_T] | _T2) -> Deferred[_T] | _T2:\n     \"\"\"Converts a coroutine or other awaitable object into a Deferred,\n     or returns the object as is if it isn't a coroutine.\"\"\"\n     if isinstance(o, Deferred):\n         return o\n-    if asyncio.isfuture(o) or inspect.isawaitable(o):\n+    if inspect.isawaitable(o):\n         if not is_asyncio_reactor_installed():\n             # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n             # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n-            return ensureDeferred(cast(Coroutine[Deferred, Any, Any], o))\n+            return Deferred.fromCoroutine(cast(Coroutine[Deferred[Any], Any, _T], o))\n         # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n         event_loop = _get_asyncio_event_loop()\n         return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n@@ -394,7 +390,7 @@ def deferred_from_coro(o: _T) -> Deferred | _T:\n \n \n def deferred_f_from_coro_f(\n-    coro_f: Callable[_P, Coroutine[Any, Any, _T]],\n+    coro_f: Callable[_P, Awaitable[_T]],\n ) -> Callable[_P, Deferred[_T]]:\n     \"\"\"Converts a coroutine function into a function that returns a Deferred.\n \n@@ -403,7 +399,7 @@ def deferred_f_from_coro_f(\n     \"\"\"\n \n     @wraps(coro_f)\n-    def f(*coro_args: _P.args, **coro_kwargs: _P.kwargs) -> Any:\n+    def f(*coro_args: _P.args, **coro_kwargs: _P.kwargs) -> Deferred[_T]:\n         return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\n \n     return f\n@@ -416,15 +412,15 @@ def maybeDeferred_coro(\n     try:\n         result = f(*args, **kw)\n     except:  # noqa: E722  # pylint: disable=bare-except\n-        return defer.fail(failure.Failure(captureVars=Deferred.debug))\n+        return fail(failure.Failure(captureVars=Deferred.debug))\n \n     if isinstance(result, Deferred):\n         return result\n     if asyncio.isfuture(result) or inspect.isawaitable(result):\n         return deferred_from_coro(result)\n     if isinstance(result, failure.Failure):\n-        return defer.fail(result)\n-    return defer.succeed(result)\n+        return fail(result)\n+    return succeed(result)\n \n \n def deferred_to_future(d: Deferred[_T]) -> Future[_T]:\n\n@@ -1,7 +1,12 @@\n+from __future__ import annotations\n+\n+import asyncio\n import random\n+from asyncio import Future\n+from typing import TYPE_CHECKING, Any\n \n import pytest\n-from twisted.internet import defer, reactor\n+from twisted.internet.defer import Deferred, inlineCallbacks, succeed\n from twisted.python.failure import Failure\n from twisted.trial import unittest\n \n@@ -9,6 +14,8 @@ from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.defer import (\n     aiter_errback,\n     deferred_f_from_coro_f,\n+    deferred_from_coro,\n+    deferred_to_future,\n     iter_errback,\n     maybe_deferred_to_future,\n     mustbe_deferred,\n@@ -17,12 +24,15 @@ from scrapy.utils.defer import (\n     process_parallel,\n )\n \n+if TYPE_CHECKING:\n+    from collections.abc import AsyncGenerator, Awaitable, Callable, Generator\n+\n \n class TestMustbeDeferred(unittest.TestCase):\n-    def test_success_function(self):\n-        steps = []\n+    def test_success_function(self) -> Deferred[list[int]]:\n+        steps: list[int] = []\n \n-        def _append(v):\n+        def _append(v: int) -> list[int]:\n             steps.append(v)\n             return steps\n \n@@ -31,12 +41,14 @@ class TestMustbeDeferred(unittest.TestCase):\n         steps.append(2)  # add another value, that should be caught by assertEqual\n         return dfd\n \n-    def test_unfired_deferred(self):\n-        steps = []\n+    def test_unfired_deferred(self) -> Deferred[list[int]]:\n+        steps: list[int] = []\n+\n+        def _append(v: int) -> Deferred[list[int]]:\n+            from twisted.internet import reactor\n \n-        def _append(v):\n             steps.append(v)\n-            dfd = defer.Deferred()\n+            dfd: Deferred[list[int]] = Deferred()\n             reactor.callLater(0, dfd.callback, steps)\n             return dfd\n \n@@ -51,7 +63,7 @@ def cb1(value, arg1, arg2):\n \n \n def cb2(value, arg1, arg2):\n-    return defer.succeed(f\"(cb2 {value} {arg1} {arg2})\")\n+    return succeed(f\"(cb2 {value} {arg1} {arg2})\")\n \n \n def cb3(value, arg1, arg2):\n@@ -67,7 +79,7 @@ def eb1(failure, arg1, arg2):\n \n \n class TestDeferUtils(unittest.TestCase):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_process_chain(self):\n         x = yield process_chain([cb1, cb2, cb3], \"res\", \"v1\", \"v2\")\n         assert x == \"(cb3 (cb2 (cb1 res v1 v2) v1 v2) v1 v2)\"\n@@ -75,7 +87,7 @@ class TestDeferUtils(unittest.TestCase):\n         with pytest.raises(TypeError):\n             yield process_chain([cb1, cb_fail, cb3], \"res\", \"v1\", \"v2\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_process_parallel(self):\n         x = yield process_parallel([cb1, cb2, cb3], \"res\", \"v1\", \"v2\")\n         assert x == [\"(cb1 res v1 v2)\", \"(cb2 res v1 v2)\", \"(cb3 res v1 v2)\"]\n@@ -88,7 +100,7 @@ class TestDeferUtils(unittest.TestCase):\n \n class TestIterErrback:\n     def test_iter_errback_good(self):\n-        def itergood():\n+        def itergood() -> Generator[int, None, None]:\n             yield from range(10)\n \n         errors = []\n@@ -97,7 +109,7 @@ class TestIterErrback:\n         assert not errors\n \n     def test_iter_errback_bad(self):\n-        def iterbad():\n+        def iterbad() -> Generator[int, None, None]:\n             for x in range(10):\n                 if x == 5:\n                     1 / 0\n@@ -113,7 +125,7 @@ class TestIterErrback:\n class TestAiterErrback(unittest.TestCase):\n     @deferred_f_from_coro_f\n     async def test_aiter_errback_good(self):\n-        async def itergood():\n+        async def itergood() -> AsyncGenerator[int, None]:\n             for x in range(10):\n                 yield x\n \n@@ -124,7 +136,7 @@ class TestAiterErrback(unittest.TestCase):\n \n     @deferred_f_from_coro_f\n     async def test_iter_errback_bad(self):\n-        async def iterbad():\n+        async def iterbad() -> AsyncGenerator[int, None]:\n             for x in range(10):\n                 if x == 5:\n                     1 / 0\n@@ -168,10 +180,12 @@ class TestAsyncCooperator(unittest.TestCase):\n     CONCURRENT_ITEMS = 50\n \n     @staticmethod\n-    def callable(o, results):\n+    def callable(o: int, results: list[int]) -> Deferred[None] | None:\n+        from twisted.internet import reactor\n+\n         if random.random() < 0.4:\n             # simulate async processing\n-            dfd = defer.Deferred()\n+            dfd: Deferred[None] = Deferred()\n             dfd.addCallback(lambda _: results.append(o))\n             delay = random.random() / 8\n             reactor.callLater(delay, dfd.callback, None)\n@@ -181,22 +195,24 @@ class TestAsyncCooperator(unittest.TestCase):\n         return None\n \n     @staticmethod\n-    def get_async_iterable(length):\n+    def get_async_iterable(length: int) -> AsyncGenerator[int, None]:\n         # simulate a simple callback without delays between results\n         return as_async_generator(range(length))\n \n     @staticmethod\n-    async def get_async_iterable_with_delays(length):\n+    async def get_async_iterable_with_delays(length: int) -> AsyncGenerator[int, None]:\n         # simulate a callback with delays between some of the results\n+        from twisted.internet import reactor\n+\n         for i in range(length):\n             if random.random() < 0.1:\n-                dfd = defer.Deferred()\n+                dfd: Deferred[None] = Deferred()\n                 delay = random.random() / 20\n                 reactor.callLater(delay, dfd.callback, None)\n                 await maybe_deferred_to_future(dfd)\n             yield i\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_simple(self):\n         for length in [20, 50, 100]:\n             results = []\n@@ -205,7 +221,7 @@ class TestAsyncCooperator(unittest.TestCase):\n             yield dl\n             assert list(range(length)) == sorted(results)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_delays(self):\n         for length in [20, 50, 100]:\n             results = []\n@@ -213,3 +229,162 @@ class TestAsyncCooperator(unittest.TestCase):\n             dl = parallel_async(ait, self.CONCURRENT_ITEMS, self.callable, results)\n             yield dl\n             assert list(range(length)) == sorted(results)\n+\n+\n+class TestDeferredFromCoro(unittest.TestCase):\n+    def test_deferred(self):\n+        d = Deferred()\n+        result = deferred_from_coro(d)\n+        assert isinstance(result, Deferred)\n+        assert result is d\n+\n+    def test_object(self):\n+        result = deferred_from_coro(42)\n+        assert result == 42\n+\n+    @inlineCallbacks\n+    def test_coroutine(self):\n+        async def coroutine() -> int:\n+            return 42\n+\n+        result = deferred_from_coro(coroutine())\n+        assert isinstance(result, Deferred)\n+        coro_result = yield result\n+        assert coro_result == 42\n+\n+    @pytest.mark.only_asyncio\n+    @inlineCallbacks\n+    def test_coroutine_asyncio(self):\n+        async def coroutine() -> int:\n+            await asyncio.sleep(0)\n+            return 42\n+\n+        result = deferred_from_coro(coroutine())\n+        assert isinstance(result, Deferred)\n+        coro_result = yield result\n+        assert coro_result == 42\n+\n+    @pytest.mark.only_asyncio\n+    @inlineCallbacks\n+    def test_future(self):\n+        future = Future()\n+        result = deferred_from_coro(future)\n+        assert isinstance(result, Deferred)\n+        future.set_result(42)\n+        future_result = yield result\n+        assert future_result == 42\n+\n+\n+class TestDeferredFFromCoroF(unittest.TestCase):\n+    @inlineCallbacks\n+    def _assert_result(\n+        self, c_f: Callable[[], Awaitable[int]]\n+    ) -> Generator[Deferred[Any], Any, None]:\n+        d_f = deferred_f_from_coro_f(c_f)\n+        d = d_f()\n+        assert isinstance(d, Deferred)\n+        result = yield d\n+        assert result == 42\n+\n+    @inlineCallbacks\n+    def test_coroutine(self):\n+        async def c_f() -> int:\n+            return 42\n+\n+        yield self._assert_result(c_f)\n+\n+    @inlineCallbacks\n+    def test_coroutine_asyncio(self):\n+        async def c_f() -> int:\n+            return 42\n+\n+        yield self._assert_result(c_f)\n+\n+    @pytest.mark.only_asyncio\n+    @inlineCallbacks\n+    def test_future(self):\n+        def c_f() -> Future[int]:\n+            f: Future[int] = Future()\n+            f.set_result(42)\n+            return f\n+\n+        yield self._assert_result(c_f)\n+\n+\n+class TestDeferredToFuture(unittest.TestCase):\n+    @deferred_f_from_coro_f\n+    async def test_deferred(self):\n+        d = Deferred()\n+        result = deferred_to_future(d)\n+        assert isinstance(result, Future)\n+        d.callback(42)\n+        future_result = await result\n+        assert future_result == 42\n+\n+    @deferred_f_from_coro_f\n+    async def test_wrapped_coroutine(self):\n+        async def c_f() -> int:\n+            return 42\n+\n+        d = deferred_from_coro(c_f())\n+        result = deferred_to_future(d)\n+        assert isinstance(result, Future)\n+        future_result = await result\n+        assert future_result == 42\n+\n+    @pytest.mark.only_asyncio\n+    @deferred_f_from_coro_f\n+    async def test_wrapped_coroutine_asyncio(self):\n+        async def c_f() -> int:\n+            await asyncio.sleep(0)\n+            return 42\n+\n+        d = deferred_from_coro(c_f())\n+        result = maybe_deferred_to_future(d)\n+        assert isinstance(result, Future)\n+        future_result = await result\n+        assert future_result == 42\n+\n+\n+@pytest.mark.only_asyncio\n+class TestMaybeDeferredToFutureAsyncio(unittest.TestCase):\n+    @deferred_f_from_coro_f\n+    async def test_deferred(self):\n+        d = Deferred()\n+        result = maybe_deferred_to_future(d)\n+        assert isinstance(result, Future)\n+        d.callback(42)\n+        future_result = await result\n+        assert future_result == 42\n+\n+    @deferred_f_from_coro_f\n+    async def test_wrapped_coroutine(self):\n+        async def c_f() -> int:\n+            return 42\n+\n+        d = deferred_from_coro(c_f())\n+        result = maybe_deferred_to_future(d)\n+        assert isinstance(result, Future)\n+        future_result = await result\n+        assert future_result == 42\n+\n+    @deferred_f_from_coro_f\n+    async def test_wrapped_coroutine_asyncio(self):\n+        async def c_f() -> int:\n+            await asyncio.sleep(0)\n+            return 42\n+\n+        d = deferred_from_coro(c_f())\n+        result = maybe_deferred_to_future(d)\n+        assert isinstance(result, Future)\n+        future_result = await result\n+        assert future_result == 42\n+\n+\n+@pytest.mark.only_not_asyncio\n+class TestMaybeDeferredToFutureNotAsyncio:\n+    def test_deferred(self):\n+        d = Deferred()\n+        result = maybe_deferred_to_future(d)\n+        assert isinstance(result, Deferred)\n+        assert result is d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d92d16510b8c82d529abf1662bce3e16ee293ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 17 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 38 | Churn Cumulative: 3230 | Contributors (this commit): 26 | Commits (past 90d): 6 | Contributors (cumulative): 37 | DMM Complexity: 0.0\n\nDIFF:\n@@ -173,8 +173,8 @@ class Scheduler(BaseScheduler):\n \n     :ref:`Start requests <start-requests>` are sent in the order they are\n     yielded from :meth:`~scrapy.Spider.start`, and given the same\n-    :attr:`~scrapy.http.Request.priority`, start requests take precedence over\n-    other requests.\n+    :attr:`~scrapy.http.Request.priority`, other requests take precedence over\n+    start requests.\n \n     You can set :setting:`SCHEDULER_START_MEMORY_QUEUE` and\n     :setting:`SCHEDULER_START_DISK_QUEUE` to ``None`` to handle start requests\n\n@@ -160,28 +160,32 @@ class ScrapyPriorityQueue:\n \n     def pop(self) -> Request | None:\n         while self.curprio is not None:\n-            if self._start_queues:\n             try:\n-                    q = self._start_queues[self.curprio]\n+                q = self.queues[self.curprio]\n             except KeyError:\n                 pass\n             else:\n                 m = q.pop()\n                 if not q:\n-                        del self._start_queues[self.curprio]\n+                    del self.queues[self.curprio]\n                     q.close()\n+                    if not self._start_queues:\n+                        self._update_curprio()\n                 return m\n+            if self._start_queues:\n                 try:\n-                q = self.queues[self.curprio]\n+                    q = self._start_queues[self.curprio]\n                 except KeyError:\n                     self._update_curprio()\n                 else:\n                     m = q.pop()\n                     if not q:\n-                    del self.queues[self.curprio]\n+                        del self._start_queues[self.curprio]\n                         q.close()\n                         self._update_curprio()\n                     return m\n+            else:\n+                self._update_curprio()\n         return None\n \n     def _update_curprio(self) -> None:\n\n@@ -189,9 +189,9 @@ class RequestSendOrderTestCase(TestCase):\n \n     @deferred_f_from_coro_f\n     async def test_default(self):\n-        \"\"\"By default, start requests take priority over callback requests and\n+        \"\"\"By default, callback requests take priority over start requests and\n         are sent in order. Priority matters, but given the same priority, a\n-        start request takes precedence.\"\"\"\n+        callback request takes precedence.\"\"\"\n         nums = [1, 2, 3, 4, 5, 6]\n         response_seconds = 0\n         download_slots = 1\n@@ -207,13 +207,13 @@ class RequestSendOrderTestCase(TestCase):\n             yield _request(1)\n \n             for request in (\n-                _request(4, priority=1),\n-                _request(6),\n+                _request(2, priority=1),\n+                _request(5),\n             ):\n                 spider.crawler.engine._slot.scheduler.enqueue_request(request)\n-            yield _request(5)\n-            yield _request(2, priority=1)\n+            yield _request(6)\n             yield _request(3, priority=1)\n+            yield _request(4, priority=1)\n \n         def parse(spider, response):\n             return\n@@ -249,13 +249,13 @@ class RequestSendOrderTestCase(TestCase):\n             yield _request(1)\n \n             for request in (\n-                _request(4, priority=1),\n-                _request(6),\n+                _request(2, priority=1),\n+                _request(5),\n             ):\n                 spider.crawler.engine._slot.scheduler.enqueue_request(request)\n-            yield _request(5)\n+            yield _request(6)\n+            yield _request(4, priority=1)\n             yield _request(3, priority=1)\n-            yield _request(2, priority=1)\n \n         def parse(spider, response):\n             return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e3f82afaf1ab12ac8f5915dfb0b926391bc81f52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 54 | Churn Cumulative: 349 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,7 @@ from scrapy.http.request import Request\n from scrapy.pqueues import DownloaderAwarePriorityQueue, ScrapyPriorityQueue\n from scrapy.spiders import Spider\n from scrapy.squeues import FifoMemoryQueue\n+from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.test import get_crawler\n from tests.test_scheduler import MockDownloader, MockEngine\n \n@@ -155,3 +156,56 @@ class TestDownloaderAwarePriorityQueue:\n         assert self.queue.peek().url == req3.url\n         assert self.queue.pop().url == req3.url\n         assert self.queue.peek() is None\n+\n+\n+@pytest.mark.parametrize(\n+    (\"input\", \"output\"),\n+    [\n+        # By default, start requests are FIFO, other requests are LIFO.\n+        ([{}, {}], [2, 1]),\n+        ([{\"start\": True}, {\"start\": True}], [1, 2]),\n+        # Priority matters.\n+        ([{\"priority\": 1}, {\"start\": True}], [1, 2]),\n+        ([{}, {\"start\": True, \"priority\": 1}], [2, 1]),\n+        # For the same priority, start requests pop last.\n+        ([{}, {\"start\": True}], [1, 2]),\n+        ([{\"start\": True}, {}], [2, 1]),\n+    ],\n+)\n+def test_pop_order(input, output):\n+    def make_url(index):\n+        return f\"https://toscrape.com/{index}\"\n+\n+    def make_request(index, data):\n+        meta = {}\n+        if data.get(\"start\", False):\n+            meta[\"is_start_request\"] = True\n+        return Request(\n+            url=make_url(index),\n+            priority=data.get(\"priority\", 0),\n+            meta=meta,\n+        )\n+\n+    input_requests = [\n+        make_request(index, data) for index, data in enumerate(input, start=1)\n+    ]\n+    expected_output_urls = [make_url(index) for index in output]\n+\n+    crawler = get_crawler(Spider)\n+    settings = crawler.settings\n+    queue = build_from_crawler(\n+        ScrapyPriorityQueue,\n+        crawler,\n+        downstream_queue_cls=load_object(settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n+        key=\"\",\n+        start_queue_cls=load_object(settings[\"SCHEDULER_START_MEMORY_QUEUE\"]),\n+    )\n+\n+    for request in input_requests:\n+        queue.push(request)\n+\n+    actual_output_urls = []\n+    while request := queue.pop():\n+        actual_output_urls.append(request.url)\n+\n+    assert actual_output_urls == expected_output_urls\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b41aea4873319df15ccb9145a4940ff1702d123a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 855 | Lines Deleted: 771 | Files Changed: 5 | Hunks: 32 | Methods Changed: 128 | Complexity Δ (Sum/Max): 22/97 | Churn Δ: 1626 | Churn Cumulative: 5126 | Contributors (this commit): 37 | Commits (past 90d): 7 | Contributors (cumulative): 41 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,46 @@\n+\"\"\"Tests for scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING\n+\n+import pytest\n+\n+from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n+from scrapy.http import Request\n+from scrapy.spiders import Spider\n+from tests.test_downloader_handlers_http_base import TestHttpBase, TestHttpProxyBase\n+\n+if TYPE_CHECKING:\n+    from scrapy.core.downloader.handlers import DownloadHandlerProtocol\n+\n+\n+class HTTP10DownloadHandlerMixin:\n+    @property\n+    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n+        return HTTP10DownloadHandler\n+\n+\n+@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n+class TestHttp10(HTTP10DownloadHandlerMixin, TestHttpBase):\n+    \"\"\"HTTP 1.0 test case\"\"\"\n+\n+    def test_protocol(self):\n+        request = Request(self.getURL(\"host\"), method=\"GET\")\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.protocol)\n+        d.addCallback(self.assertEqual, \"HTTP/1.0\")\n+        return d\n+\n+\n+class TestHttps10(TestHttp10):\n+    scheme = \"https\"\n+\n+\n+@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n+class TestHttp10Proxy(HTTP10DownloadHandlerMixin, TestHttpProxyBase):\n+    def test_download_with_proxy_https_timeout(self):\n+        pytest.skip(\"Not implemented\")\n+\n+    def test_download_with_proxy_without_http_scheme(self):\n+        pytest.skip(\"Not implemented\")\n\n@@ -0,0 +1,69 @@\n+\"\"\"Tests for scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any\n+\n+from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n+from tests.test_downloader_handlers_http_base import (\n+    TestHttp11Base,\n+    TestHttpMockServerBase,\n+    TestHttpProxyBase,\n+    TestHttps11Base,\n+    TestHttpsCustomCiphersBase,\n+    TestHttpsInvalidDNSIdBase,\n+    TestHttpsInvalidDNSPatternBase,\n+    TestHttpsWrongHostnameBase,\n+    TestSimpleHttpsBase,\n+)\n+\n+if TYPE_CHECKING:\n+    from scrapy.core.downloader.handlers import DownloadHandlerProtocol\n+\n+\n+class HTTP11DownloadHandlerMixin:\n+    @property\n+    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n+        return HTTP11DownloadHandler\n+\n+\n+class TestHttp11(HTTP11DownloadHandlerMixin, TestHttp11Base):\n+    pass\n+\n+\n+class TestHttps11(HTTP11DownloadHandlerMixin, TestHttps11Base):\n+    pass\n+\n+\n+class TestSimpleHttps(HTTP11DownloadHandlerMixin, TestSimpleHttpsBase):\n+    pass\n+\n+\n+class Https11WrongHostnameTestCase(\n+    HTTP11DownloadHandlerMixin, TestHttpsWrongHostnameBase\n+):\n+    pass\n+\n+\n+class Https11InvalidDNSId(HTTP11DownloadHandlerMixin, TestHttpsInvalidDNSIdBase):\n+    pass\n+\n+\n+class Https11InvalidDNSPattern(\n+    HTTP11DownloadHandlerMixin, TestHttpsInvalidDNSPatternBase\n+):\n+    pass\n+\n+\n+class Https11CustomCiphers(HTTP11DownloadHandlerMixin, TestHttpsCustomCiphersBase):\n+    pass\n+\n+\n+class TestHttp11MockServer(TestHttpMockServerBase):\n+    @property\n+    def settings_dict(self) -> dict[str, Any] | None:\n+        return None  # default handler settings\n+\n+\n+class TestHttp11Proxy(HTTP11DownloadHandlerMixin, TestHttpProxyBase):\n+    pass\n\n@@ -1,4 +1,9 @@\n+\"\"\"Tests for scrapy.core.downloader.handlers.http2.H2DownloadHandler.\"\"\"\n+\n+from __future__ import annotations\n+\n import json\n+from typing import TYPE_CHECKING, Any\n from unittest import mock\n \n import pytest\n@@ -8,61 +13,43 @@ from twisted.web import server\n from twisted.web.error import SchemeNotSupported\n from twisted.web.http import H2_ENABLED\n \n-from scrapy.core.downloader.handlers import DownloadHandlerProtocol\n from scrapy.http import Request\n from scrapy.spiders import Spider\n from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.test import get_crawler\n from tests.mockserver import ssl_context_factory\n-from tests.test_downloader_handlers import (\n+from tests.test_downloader_handlers_http_base import (\n+    TestHttpMockServerBase,\n+    TestHttpProxyBase,\n+    TestHttps11Base,\n+    TestHttpsCustomCiphersBase,\n+    TestHttpsInvalidDNSIdBase,\n+    TestHttpsInvalidDNSPatternBase,\n+    TestHttpsWrongHostnameBase,\n     UriResource,\n )\n \n+if TYPE_CHECKING:\n+    from scrapy.core.downloader.handlers import DownloadHandlerProtocol\n+\n+\n pytestmark = pytest.mark.skipif(\n     not H2_ENABLED, reason=\"HTTP/2 support in Twisted is not enabled\"\n )\n \n \n-class BaseTestClasses:\n-    # A hack to prevent tests from the imported classes to run here too.\n-    # See https://stackoverflow.com/q/1323455/113586 for other ways.\n-    from tests.test_downloader_handlers import (\n-        TestHttp11MockServer as TestHttp11MockServer,\n-    )\n-    from tests.test_downloader_handlers import (\n-        TestHttp11Proxy as TestHttp11Proxy,\n-    )\n-    from tests.test_downloader_handlers import (\n-        TestHttps11 as TestHttps11,\n-    )\n-    from tests.test_downloader_handlers import (\n-        TestHttps11CustomCiphers as TestHttps11CustomCiphers,\n-    )\n-    from tests.test_downloader_handlers import (\n-        TestHttps11InvalidDNSId as TestHttps11InvalidDNSId,\n-    )\n-    from tests.test_downloader_handlers import (\n-        TestHttps11InvalidDNSPattern as TestHttps11InvalidDNSPattern,\n-    )\n-    from tests.test_downloader_handlers import (\n-        TestHttps11WrongHostname as TestHttps11WrongHostname,\n-    )\n-\n-\n-def _get_dh() -> type[DownloadHandlerProtocol]:\n+class H2DownloadHandlerMixin:\n+    @property\n+    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n+        # the import can fail when H2_ENABLED is False\n         from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n \n         return H2DownloadHandler\n \n \n-class TestHttps2(BaseTestClasses.TestHttps11):\n-    scheme = \"https\"\n+class TestHttps2(H2DownloadHandlerMixin, TestHttps11Base):\n     HTTP2_DATALOSS_SKIP_REASON = \"Content-Length mismatch raises InvalidBodyLengthError\"\n \n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return _get_dh()\n-\n     def test_protocol(self):\n         request = Request(self.getURL(\"host\"), method=\"GET\")\n         d = self.download_request(request, Spider(\"foo\"))\n@@ -179,42 +166,37 @@ class TestHttps2(BaseTestClasses.TestHttps11):\n         return d\n \n \n-class Https2WrongHostnameTestCase(BaseTestClasses.TestHttps11WrongHostname):\n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return _get_dh()\n+class Https2WrongHostnameTestCase(H2DownloadHandlerMixin, TestHttpsWrongHostnameBase):\n+    pass\n \n \n-class Https2InvalidDNSId(BaseTestClasses.TestHttps11InvalidDNSId):\n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return _get_dh()\n+class Https2InvalidDNSId(H2DownloadHandlerMixin, TestHttpsInvalidDNSIdBase):\n+    pass\n \n \n-class Https2InvalidDNSPattern(BaseTestClasses.TestHttps11InvalidDNSPattern):\n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return _get_dh()\n+class Https2InvalidDNSPattern(H2DownloadHandlerMixin, TestHttpsInvalidDNSPatternBase):\n+    pass\n \n \n-class Https2CustomCiphers(BaseTestClasses.TestHttps11CustomCiphers):\n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return _get_dh()\n+class Https2CustomCiphers(H2DownloadHandlerMixin, TestHttpsCustomCiphersBase):\n+    pass\n \n \n-class Http2MockServerTestCase(BaseTestClasses.TestHttp11MockServer):\n+class Http2MockServerTestCase(TestHttpMockServerBase):\n     \"\"\"HTTP 2.0 test case with MockServer\"\"\"\n \n-    settings_dict = {\n+    @property\n+    def settings_dict(self) -> dict[str, Any] | None:\n+        return {\n             \"DOWNLOAD_HANDLERS\": {\n                 \"https\": \"scrapy.core.downloader.handlers.http2.H2DownloadHandler\"\n             }\n         }\n+\n     is_secure = True\n \n \n-class Https2ProxyTestCase(BaseTestClasses.TestHttp11Proxy):\n+class Https2ProxyTestCase(H2DownloadHandlerMixin, TestHttpProxyBase):\n     # only used for HTTPS tests\n     keyfile = \"keys/localhost.key\"\n     certfile = \"keys/localhost.crt\"\n@@ -224,10 +206,6 @@ class Https2ProxyTestCase(BaseTestClasses.TestHttp11Proxy):\n \n     expected_http_proxy_request_body = b\"/\"\n \n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return _get_dh()\n-\n     def setUp(self):\n         site = server.Site(UriResource(), timeout=None)\n         self.port = reactor.listenSSL(\n\n@@ -1,52 +1,35 @@\n+\"\"\"Tests for DownloadHandlers and for specific non-HTTP download handlers.\"\"\"\n+\n from __future__ import annotations\n \n import contextlib\n import os\n import shutil\n import sys\n-from abc import ABC, abstractmethod\n from pathlib import Path\n from tempfile import mkdtemp, mkstemp\n-from unittest import SkipTest, mock\n+from unittest import mock\n \n import pytest\n-from testfixtures import LogCapture\n from twisted.cred import checkers, credentials, portal\n-from twisted.internet import defer, error, reactor\n+from twisted.internet import reactor\n from twisted.protocols.ftp import FTPFactory, FTPRealm\n-from twisted.protocols.policies import WrappingFactory\n from twisted.trial import unittest\n-from twisted.web import resource, server, static, util\n-from twisted.web.client import ResponseFailed\n-from twisted.web.http import _DataLoss\n from w3lib.url import path_to_file_uri\n \n-from scrapy.core.downloader.handlers import DownloadHandlerProtocol, DownloadHandlers\n+from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.handlers.datauri import DataURIDownloadHandler\n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n-from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n-from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import Headers, HtmlResponse, Request\n+from scrapy.http import HtmlResponse, Request\n from scrapy.http.response.text import TextResponse\n from scrapy.responsetypes import responsetypes\n from scrapy.spiders import Spider\n from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n-from tests import NON_EXISTING_RESOLVABLE\n-from tests.mockserver import (\n-    Echo,\n-    ForeverTakingResource,\n-    HostHeaderResource,\n-    MockServer,\n-    NoLengthResource,\n-    PayloadResource,\n-    ssl_context_factory,\n-)\n-from tests.spiders import SingleRequestSpider\n \n \n class DummyDH:\n@@ -137,696 +120,6 @@ class TestFile(unittest.TestCase):\n         return self.assertFailure(d, OSError)\n \n \n-class ContentLengthHeaderResource(resource.Resource):\n-    \"\"\"\n-    A testing resource which renders itself as the value of the Content-Length\n-    header from the request.\n-    \"\"\"\n-\n-    def render(self, request):\n-        return request.requestHeaders.getRawHeaders(b\"content-length\")[0]\n-\n-\n-class ChunkedResource(resource.Resource):\n-    def render(self, request):\n-        def response():\n-            request.write(b\"chunked \")\n-            request.write(b\"content\\n\")\n-            request.finish()\n-\n-        reactor.callLater(0, response)\n-        return server.NOT_DONE_YET\n-\n-\n-class BrokenChunkedResource(resource.Resource):\n-    def render(self, request):\n-        def response():\n-            request.write(b\"chunked \")\n-            request.write(b\"content\\n\")\n-            # Disable terminating chunk on finish.\n-            request.chunked = False\n-            closeConnection(request)\n-\n-        reactor.callLater(0, response)\n-        return server.NOT_DONE_YET\n-\n-\n-class BrokenDownloadResource(resource.Resource):\n-    def render(self, request):\n-        def response():\n-            request.setHeader(b\"Content-Length\", b\"20\")\n-            request.write(b\"partial\")\n-            closeConnection(request)\n-\n-        reactor.callLater(0, response)\n-        return server.NOT_DONE_YET\n-\n-\n-def closeConnection(request):\n-    # We have to force a disconnection for HTTP/1.1 clients. Otherwise\n-    # client keeps the connection open waiting for more data.\n-    request.channel.loseConnection()\n-    request.finish()\n-\n-\n-class EmptyContentTypeHeaderResource(resource.Resource):\n-    \"\"\"\n-    A testing resource which renders itself as the value of request body\n-    without content-type header in response.\n-    \"\"\"\n-\n-    def render(self, request):\n-        request.setHeader(\"content-type\", \"\")\n-        return request.content.read()\n-\n-\n-class LargeChunkedFileResource(resource.Resource):\n-    def render(self, request):\n-        def response():\n-            for i in range(1024):\n-                request.write(b\"x\" * 1024)\n-            request.finish()\n-\n-        reactor.callLater(0, response)\n-        return server.NOT_DONE_YET\n-\n-\n-class DuplicateHeaderResource(resource.Resource):\n-    def render(self, request):\n-        request.responseHeaders.setRawHeaders(b\"Set-Cookie\", [b\"a=b\", b\"c=d\"])\n-        return b\"\"\n-\n-\n-class TestHttp(unittest.TestCase, ABC):\n-    scheme = \"http\"\n-\n-    # only used for HTTPS tests\n-    keyfile = \"keys/localhost.key\"\n-    certfile = \"keys/localhost.crt\"\n-\n-    @property\n-    @abstractmethod\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        raise NotImplementedError\n-\n-    def setUp(self):\n-        self.tmpname = Path(mkdtemp())\n-        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n-        r = static.File(str(self.tmpname))\n-        r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n-        r.putChild(b\"wait\", ForeverTakingResource())\n-        r.putChild(b\"hang-after-headers\", ForeverTakingResource(write=True))\n-        r.putChild(b\"nolength\", NoLengthResource())\n-        r.putChild(b\"host\", HostHeaderResource())\n-        r.putChild(b\"payload\", PayloadResource())\n-        r.putChild(b\"broken\", BrokenDownloadResource())\n-        r.putChild(b\"chunked\", ChunkedResource())\n-        r.putChild(b\"broken-chunked\", BrokenChunkedResource())\n-        r.putChild(b\"contentlength\", ContentLengthHeaderResource())\n-        r.putChild(b\"nocontenttype\", EmptyContentTypeHeaderResource())\n-        r.putChild(b\"largechunkedfile\", LargeChunkedFileResource())\n-        r.putChild(b\"duplicate-header\", DuplicateHeaderResource())\n-        r.putChild(b\"echo\", Echo())\n-        self.site = server.Site(r, timeout=None)\n-        self.wrapper = WrappingFactory(self.site)\n-        self.host = \"localhost\"\n-        if self.scheme == \"https\":\n-            # Using WrappingFactory do not enable HTTP/2 failing all the\n-            # tests with H2DownloadHandler\n-            self.port = reactor.listenSSL(\n-                0,\n-                self.site,\n-                ssl_context_factory(self.keyfile, self.certfile),\n-                interface=self.host,\n-            )\n-        else:\n-            self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n-        self.portno = self.port.getHost().port\n-        self.download_handler = build_from_crawler(\n-            self.download_handler_cls, get_crawler()\n-        )\n-        self.download_request = self.download_handler.download_request\n-\n-    @defer.inlineCallbacks\n-    def tearDown(self):\n-        yield self.port.stopListening()\n-        if hasattr(self.download_handler, \"close\"):\n-            yield self.download_handler.close()\n-        shutil.rmtree(self.tmpname)\n-\n-    def getURL(self, path):\n-        return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n-\n-    def test_download(self):\n-        request = Request(self.getURL(\"file\"))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        return d\n-\n-    def test_download_head(self):\n-        request = Request(self.getURL(\"file\"), method=\"HEAD\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"\")\n-        return d\n-\n-    def test_redirect_status(self):\n-        request = Request(self.getURL(\"redirect\"))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.status)\n-        d.addCallback(self.assertEqual, 302)\n-        return d\n-\n-    def test_redirect_status_head(self):\n-        request = Request(self.getURL(\"redirect\"), method=\"HEAD\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.status)\n-        d.addCallback(self.assertEqual, 302)\n-        return d\n-\n-    @defer.inlineCallbacks\n-    def test_timeout_download_from_spider_nodata_rcvd(self):\n-        if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n-            # https://twistedmatrix.com/trac/ticket/10279\n-            raise unittest.SkipTest(\n-                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n-            )\n-\n-        # client connects but no data is received\n-        spider = Spider(\"foo\")\n-        meta = {\"download_timeout\": 0.5}\n-        request = Request(self.getURL(\"wait\"), meta=meta)\n-        d = self.download_request(request, spider)\n-        yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n-\n-    @defer.inlineCallbacks\n-    def test_timeout_download_from_spider_server_hangs(self):\n-        if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n-            # https://twistedmatrix.com/trac/ticket/10279\n-            raise unittest.SkipTest(\n-                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n-            )\n-        # client connects, server send headers and some body bytes but hangs\n-        spider = Spider(\"foo\")\n-        meta = {\"download_timeout\": 0.5}\n-        request = Request(self.getURL(\"hang-after-headers\"), meta=meta)\n-        d = self.download_request(request, spider)\n-        yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n-\n-    def test_host_header_not_in_request_headers(self):\n-        def _test(response):\n-            assert response.body == to_bytes(f\"{self.host}:{self.portno}\")\n-            assert not request.headers\n-\n-        request = Request(self.getURL(\"host\"))\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_host_header_seted_in_request_headers(self):\n-        host = self.host + \":\" + str(self.portno)\n-\n-        def _test(response):\n-            assert response.body == host.encode()\n-            assert request.headers.get(\"Host\") == host.encode()\n-\n-        request = Request(self.getURL(\"host\"), headers={\"Host\": host})\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_content_length_zero_bodyless_post_request_headers(self):\n-        \"\"\"Tests if \"Content-Length: 0\" is sent for bodyless POST requests.\n-\n-        This is not strictly required by HTTP RFCs but can cause trouble\n-        for some web servers.\n-        See:\n-        https://github.com/scrapy/scrapy/issues/823\n-        https://issues.apache.org/jira/browse/TS-2902\n-        https://github.com/kennethreitz/requests/issues/405\n-        https://bugs.python.org/issue14721\n-        \"\"\"\n-\n-        def _test(response):\n-            assert response.body == b\"0\"\n-\n-        request = Request(self.getURL(\"contentlength\"), method=\"POST\")\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_content_length_zero_bodyless_post_only_one(self):\n-        def _test(response):\n-            import json\n-\n-            headers = Headers(json.loads(response.text)[\"headers\"])\n-            contentlengths = headers.getlist(\"Content-Length\")\n-            assert len(contentlengths) == 1\n-            assert contentlengths == [b\"0\"]\n-\n-        request = Request(self.getURL(\"echo\"), method=\"POST\")\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_payload(self):\n-        body = b\"1\" * 100  # PayloadResource requires body length to be 100\n-        request = Request(self.getURL(\"payload\"), method=\"POST\", body=body)\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, body)\n-        return d\n-\n-    def test_response_header_content_length(self):\n-        request = Request(self.getURL(\"file\"), method=b\"GET\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.headers[b\"content-length\"])\n-        d.addCallback(self.assertEqual, b\"159\")\n-        return d\n-\n-    def _test_response_class(self, filename, body, response_class):\n-        def _test(response):\n-            assert type(response) is response_class  # pylint: disable=unidiomatic-typecheck\n-\n-        request = Request(self.getURL(filename), body=body)\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_response_class_from_url(self):\n-        return self._test_response_class(\"foo.html\", b\"\", HtmlResponse)\n-\n-    def test_response_class_from_body(self):\n-        return self._test_response_class(\n-            \"foo\",\n-            b\"<!DOCTYPE html>\\n<title>.</title>\",\n-            HtmlResponse,\n-        )\n-\n-    def test_get_duplicate_header(self):\n-        def _test(response):\n-            assert response.headers.getlist(b\"Set-Cookie\") == [b\"a=b\", b\"c=d\"]\n-\n-        request = Request(self.getURL(\"duplicate-header\"))\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-\n-@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n-class TestHttp10(TestHttp):\n-    \"\"\"HTTP 1.0 test case\"\"\"\n-\n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return HTTP10DownloadHandler\n-\n-    def test_protocol(self):\n-        request = Request(self.getURL(\"host\"), method=\"GET\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.protocol)\n-        d.addCallback(self.assertEqual, \"HTTP/1.0\")\n-        return d\n-\n-\n-class TestHttps10(TestHttp10):\n-    scheme = \"https\"\n-\n-\n-class TestHttp11(TestHttp):\n-    \"\"\"HTTP 1.1 test case\"\"\"\n-\n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return HTTP11DownloadHandler\n-\n-    def test_download_without_maxsize_limit(self):\n-        request = Request(self.getURL(\"file\"))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        return d\n-\n-    def test_response_class_choosing_request(self):\n-        \"\"\"Tests choosing of correct response type\n-        in case of Content-Type is empty but body contains text.\n-        \"\"\"\n-        body = b\"Some plain text\\ndata with tabs\\t and null bytes\\0\"\n-\n-        def _test_type(response):\n-            assert type(response) is TextResponse  # pylint: disable=unidiomatic-typecheck\n-\n-        request = Request(self.getURL(\"nocontenttype\"), body=body)\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(_test_type)\n-        return d\n-\n-    @defer.inlineCallbacks\n-    def test_download_with_maxsize(self):\n-        request = Request(self.getURL(\"file\"))\n-\n-        # 10 is minimal size for this request and the limit is only counted on\n-        # response body. (regardless of headers)\n-        d = self.download_request(request, Spider(\"foo\", download_maxsize=10))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        yield d\n-\n-        d = self.download_request(request, Spider(\"foo\", download_maxsize=9))\n-        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n-\n-    @defer.inlineCallbacks\n-    def test_download_with_maxsize_very_large_file(self):\n-        with mock.patch(\"scrapy.core.downloader.handlers.http11.logger\") as logger:\n-            request = Request(self.getURL(\"largechunkedfile\"))\n-\n-            def check(logger):\n-                logger.warning.assert_called_once_with(mock.ANY, mock.ANY)\n-\n-            d = self.download_request(request, Spider(\"foo\", download_maxsize=1500))\n-            yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n-\n-            # As the error message is logged in the dataReceived callback, we\n-            # have to give a bit of time to the reactor to process the queue\n-            # after closing the connection.\n-            d = defer.Deferred()\n-            d.addCallback(check)\n-            reactor.callLater(0.1, d.callback, logger)\n-            yield d\n-\n-    @defer.inlineCallbacks\n-    def test_download_with_maxsize_per_req(self):\n-        meta = {\"download_maxsize\": 2}\n-        request = Request(self.getURL(\"file\"), meta=meta)\n-        d = self.download_request(request, Spider(\"foo\"))\n-        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n-\n-    @defer.inlineCallbacks\n-    def test_download_with_small_maxsize_per_spider(self):\n-        request = Request(self.getURL(\"file\"))\n-        d = self.download_request(request, Spider(\"foo\", download_maxsize=2))\n-        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n-\n-    def test_download_with_large_maxsize_per_spider(self):\n-        request = Request(self.getURL(\"file\"))\n-        d = self.download_request(request, Spider(\"foo\", download_maxsize=100))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        return d\n-\n-    def test_download_chunked_content(self):\n-        request = Request(self.getURL(\"chunked\"))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"chunked content\\n\")\n-        return d\n-\n-    def test_download_broken_content_cause_data_loss(self, url=\"broken\"):\n-        request = Request(self.getURL(url))\n-        d = self.download_request(request, Spider(\"foo\"))\n-\n-        def checkDataLoss(failure):\n-            if failure.check(ResponseFailed) and any(\n-                r.check(_DataLoss) for r in failure.value.reasons\n-            ):\n-                return None\n-            return failure\n-\n-        d.addCallback(lambda _: self.fail(\"No DataLoss exception\"))\n-        d.addErrback(checkDataLoss)\n-        return d\n-\n-    def test_download_broken_chunked_content_cause_data_loss(self):\n-        return self.test_download_broken_content_cause_data_loss(\"broken-chunked\")\n-\n-    def test_download_broken_content_allow_data_loss(self, url=\"broken\"):\n-        request = Request(self.getURL(url), meta={\"download_fail_on_dataloss\": False})\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.flags)\n-        d.addCallback(self.assertEqual, [\"dataloss\"])\n-        return d\n-\n-    def test_download_broken_chunked_content_allow_data_loss(self):\n-        return self.test_download_broken_content_allow_data_loss(\"broken-chunked\")\n-\n-    def test_download_broken_content_allow_data_loss_via_setting(self, url=\"broken\"):\n-        crawler = get_crawler(settings_dict={\"DOWNLOAD_FAIL_ON_DATALOSS\": False})\n-        download_handler = build_from_crawler(self.download_handler_cls, crawler)\n-        request = Request(self.getURL(url))\n-        d = download_handler.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.flags)\n-        d.addCallback(self.assertEqual, [\"dataloss\"])\n-        return d\n-\n-    def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n-        return self.test_download_broken_content_allow_data_loss_via_setting(\n-            \"broken-chunked\"\n-        )\n-\n-    def test_protocol(self):\n-        request = Request(self.getURL(\"host\"), method=\"GET\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.protocol)\n-        d.addCallback(self.assertEqual, \"HTTP/1.1\")\n-        return d\n-\n-\n-class TestHttps11(TestHttp11):\n-    scheme = \"https\"\n-\n-    tls_log_message = (\n-        'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=localhost\", '\n-        'subject \"/C=IE/O=Scrapy/CN=localhost\"'\n-    )\n-\n-    @defer.inlineCallbacks\n-    def test_tls_logging(self):\n-        crawler = get_crawler(\n-            settings_dict={\"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\": True}\n-        )\n-        download_handler = build_from_crawler(self.download_handler_cls, crawler)\n-        try:\n-            with LogCapture() as log_capture:\n-                request = Request(self.getURL(\"file\"))\n-                d = download_handler.download_request(request, Spider(\"foo\"))\n-                d.addCallback(lambda r: r.body)\n-                d.addCallback(self.assertEqual, b\"0123456789\")\n-                yield d\n-                log_capture.check_present(\n-                    (\"scrapy.core.downloader.tls\", \"DEBUG\", self.tls_log_message)\n-                )\n-        finally:\n-            yield download_handler.close()\n-\n-\n-class TestSimpleHttps(unittest.TestCase):\n-    \"\"\"Base class for special cases tested with just one simple request\"\"\"\n-\n-    keyfile = \"keys/localhost.key\"\n-    certfile = \"keys/localhost.crt\"\n-    cipher_string: str | None = None\n-\n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return HTTP11DownloadHandler\n-\n-    def setUp(self):\n-        self.tmpname = Path(mkdtemp())\n-        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n-        r = static.File(str(self.tmpname))\n-        self.site = server.Site(r, timeout=None)\n-        self.host = \"localhost\"\n-        self.port = reactor.listenSSL(\n-            0,\n-            self.site,\n-            ssl_context_factory(\n-                self.keyfile, self.certfile, cipher_string=self.cipher_string\n-            ),\n-            interface=self.host,\n-        )\n-        self.portno = self.port.getHost().port\n-        if self.cipher_string is not None:\n-            settings_dict = {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.cipher_string}\n-        else:\n-            settings_dict = None\n-        crawler = get_crawler(settings_dict=settings_dict)\n-        self.download_handler = build_from_crawler(self.download_handler_cls, crawler)\n-        self.download_request = self.download_handler.download_request\n-\n-    @defer.inlineCallbacks\n-    def tearDown(self):\n-        yield self.port.stopListening()\n-        if hasattr(self.download_handler, \"close\"):\n-            yield self.download_handler.close()\n-        shutil.rmtree(self.tmpname)\n-\n-    def getURL(self, path):\n-        return f\"https://{self.host}:{self.portno}/{path}\"\n-\n-    def test_download(self):\n-        request = Request(self.getURL(\"file\"))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        return d\n-\n-\n-class TestHttps11WrongHostname(TestSimpleHttps):\n-    # above tests use a server certificate for \"localhost\",\n-    # client connection to \"localhost\" too.\n-    # here we test that even if the server certificate is for another domain,\n-    # \"www.example.com\" in this case,\n-    # the tests still pass\n-    keyfile = \"keys/example-com.key.pem\"\n-    certfile = \"keys/example-com.cert.pem\"\n-\n-\n-class TestHttps11InvalidDNSId(TestSimpleHttps):\n-    \"\"\"Connect to HTTPS hosts with IP while certificate uses domain names IDs.\"\"\"\n-\n-    def setUp(self):\n-        super().setUp()\n-        self.host = \"127.0.0.1\"\n-\n-\n-class TestHttps11InvalidDNSPattern(TestSimpleHttps):\n-    \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n-\n-    keyfile = \"keys/localhost.ip.key\"\n-    certfile = \"keys/localhost.ip.crt\"\n-\n-\n-class TestHttps11CustomCiphers(TestSimpleHttps):\n-    cipher_string = \"CAMELLIA256-SHA\"\n-\n-\n-class TestHttp11MockServer(unittest.TestCase):\n-    \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n-\n-    settings_dict: dict | None = None\n-    is_secure = False\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        cls.mockserver = MockServer()\n-        cls.mockserver.__enter__()\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-        cls.mockserver.__exit__(None, None, None)\n-\n-    @defer.inlineCallbacks\n-    def test_download_with_content_length(self):\n-        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n-        # http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid\n-        # download it\n-        yield crawler.crawl(\n-            seed=Request(\n-                url=self.mockserver.url(\"/partial\", is_secure=self.is_secure),\n-                meta={\"download_maxsize\": 1000},\n-            )\n-        )\n-        failure = crawler.spider.meta[\"failure\"]\n-        assert isinstance(failure.value, defer.CancelledError)\n-\n-    @defer.inlineCallbacks\n-    def test_download(self):\n-        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n-        yield crawler.crawl(\n-            seed=Request(url=self.mockserver.url(\"\", is_secure=self.is_secure))\n-        )\n-        failure = crawler.spider.meta.get(\"failure\")\n-        assert failure is None\n-        reason = crawler.spider.meta[\"close_reason\"]\n-        assert reason == \"finished\"\n-\n-\n-class UriResource(resource.Resource):\n-    \"\"\"Return the full uri that was requested\"\"\"\n-\n-    def getChild(self, path, request):\n-        return self\n-\n-    def render(self, request):\n-        # Note: this is an ugly hack for CONNECT request timeout test.\n-        #       Returning some data here fail SSL/TLS handshake\n-        # ToDo: implement proper HTTPS proxy tests, not faking them.\n-        if request.method != b\"CONNECT\":\n-            return request.uri\n-        return b\"\"\n-\n-\n-class TestHttpProxy(unittest.TestCase, ABC):\n-    expected_http_proxy_request_body = b\"http://example.com\"\n-\n-    @property\n-    @abstractmethod\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        raise NotImplementedError\n-\n-    def setUp(self):\n-        site = server.Site(UriResource(), timeout=None)\n-        wrapper = WrappingFactory(site)\n-        self.port = reactor.listenTCP(0, wrapper, interface=\"127.0.0.1\")\n-        self.portno = self.port.getHost().port\n-        self.download_handler = build_from_crawler(\n-            self.download_handler_cls, get_crawler()\n-        )\n-        self.download_request = self.download_handler.download_request\n-\n-    @defer.inlineCallbacks\n-    def tearDown(self):\n-        yield self.port.stopListening()\n-        if hasattr(self.download_handler, \"close\"):\n-            yield self.download_handler.close()\n-\n-    def getURL(self, path):\n-        return f\"http://127.0.0.1:{self.portno}/{path}\"\n-\n-    def test_download_with_proxy(self):\n-        def _test(response):\n-            assert response.status == 200\n-            assert response.url == request.url\n-            assert response.body == self.expected_http_proxy_request_body\n-\n-        http_proxy = self.getURL(\"\")\n-        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_download_without_proxy(self):\n-        def _test(response):\n-            assert response.status == 200\n-            assert response.url == request.url\n-            assert response.body == b\"/path/to/resource\"\n-\n-        request = Request(self.getURL(\"path/to/resource\"))\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-\n-@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n-class TestHttp10Proxy(TestHttpProxy):\n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return HTTP10DownloadHandler\n-\n-\n-class TestHttp11Proxy(TestHttpProxy):\n-    @property\n-    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n-        return HTTP11DownloadHandler\n-\n-    @defer.inlineCallbacks\n-    def test_download_with_proxy_https_timeout(self):\n-        \"\"\"Test TunnelingTCP4ClientEndpoint\"\"\"\n-        if NON_EXISTING_RESOLVABLE:\n-            raise SkipTest(\"Non-existing hosts are resolvable\")\n-        http_proxy = self.getURL(\"\")\n-        domain = \"https://no-such-domain.nosuch\"\n-        request = Request(domain, meta={\"proxy\": http_proxy, \"download_timeout\": 0.2})\n-        d = self.download_request(request, Spider(\"foo\"))\n-        timeout = yield self.assertFailure(d, error.TimeoutError)\n-        assert domain in timeout.osError\n-\n-    def test_download_with_proxy_without_http_scheme(self):\n-        def _test(response):\n-            assert response.status == 200\n-            assert response.url == request.url\n-            assert response.body == self.expected_http_proxy_request_body\n-\n-        http_proxy = self.getURL(\"\").replace(\"http://\", \"\")\n-        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-\n class HttpDownloadHandlerMock:\n     def __init__(self, *args, **kwargs):\n         pass\n\n@@ -0,0 +1,698 @@\n+\"\"\"Base classes for HTTP download handler tests.\"\"\"\n+\n+from __future__ import annotations\n+\n+import shutil\n+import sys\n+from abc import ABC, abstractmethod\n+from pathlib import Path\n+from tempfile import mkdtemp\n+from typing import TYPE_CHECKING, Any\n+from unittest import mock\n+\n+import pytest\n+from testfixtures import LogCapture\n+from twisted.internet import defer, error, reactor\n+from twisted.protocols.policies import WrappingFactory\n+from twisted.trial import unittest\n+from twisted.web import resource, server, static, util\n+from twisted.web._newclient import ResponseFailed\n+from twisted.web.http import _DataLoss\n+\n+from scrapy.http import Headers, HtmlResponse, Request, TextResponse\n+from scrapy.spiders import Spider\n+from scrapy.utils.misc import build_from_crawler\n+from scrapy.utils.python import to_bytes\n+from scrapy.utils.test import get_crawler\n+from tests import NON_EXISTING_RESOLVABLE\n+from tests.mockserver import (\n+    Echo,\n+    ForeverTakingResource,\n+    HostHeaderResource,\n+    MockServer,\n+    NoLengthResource,\n+    PayloadResource,\n+    ssl_context_factory,\n+)\n+from tests.spiders import SingleRequestSpider\n+\n+if TYPE_CHECKING:\n+    from scrapy.core.downloader.handlers import DownloadHandlerProtocol\n+\n+\n+class ContentLengthHeaderResource(resource.Resource):\n+    \"\"\"\n+    A testing resource which renders itself as the value of the Content-Length\n+    header from the request.\n+    \"\"\"\n+\n+    def render(self, request):\n+        return request.requestHeaders.getRawHeaders(b\"content-length\")[0]\n+\n+\n+class ChunkedResource(resource.Resource):\n+    def render(self, request):\n+        def response():\n+            request.write(b\"chunked \")\n+            request.write(b\"content\\n\")\n+            request.finish()\n+\n+        reactor.callLater(0, response)\n+        return server.NOT_DONE_YET\n+\n+\n+class BrokenChunkedResource(resource.Resource):\n+    def render(self, request):\n+        def response():\n+            request.write(b\"chunked \")\n+            request.write(b\"content\\n\")\n+            # Disable terminating chunk on finish.\n+            request.chunked = False\n+            closeConnection(request)\n+\n+        reactor.callLater(0, response)\n+        return server.NOT_DONE_YET\n+\n+\n+class BrokenDownloadResource(resource.Resource):\n+    def render(self, request):\n+        def response():\n+            request.setHeader(b\"Content-Length\", b\"20\")\n+            request.write(b\"partial\")\n+            closeConnection(request)\n+\n+        reactor.callLater(0, response)\n+        return server.NOT_DONE_YET\n+\n+\n+def closeConnection(request):\n+    # We have to force a disconnection for HTTP/1.1 clients. Otherwise\n+    # client keeps the connection open waiting for more data.\n+    request.channel.loseConnection()\n+    request.finish()\n+\n+\n+class EmptyContentTypeHeaderResource(resource.Resource):\n+    \"\"\"\n+    A testing resource which renders itself as the value of request body\n+    without content-type header in response.\n+    \"\"\"\n+\n+    def render(self, request):\n+        request.setHeader(\"content-type\", \"\")\n+        return request.content.read()\n+\n+\n+class LargeChunkedFileResource(resource.Resource):\n+    def render(self, request):\n+        def response():\n+            for i in range(1024):\n+                request.write(b\"x\" * 1024)\n+            request.finish()\n+\n+        reactor.callLater(0, response)\n+        return server.NOT_DONE_YET\n+\n+\n+class DuplicateHeaderResource(resource.Resource):\n+    def render(self, request):\n+        request.responseHeaders.setRawHeaders(b\"Set-Cookie\", [b\"a=b\", b\"c=d\"])\n+        return b\"\"\n+\n+\n+class TestHttpBase(unittest.TestCase, ABC):\n+    scheme = \"http\"\n+\n+    # only used for HTTPS tests\n+    keyfile = \"keys/localhost.key\"\n+    certfile = \"keys/localhost.crt\"\n+\n+    @property\n+    @abstractmethod\n+    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n+        raise NotImplementedError\n+\n+    def setUp(self):\n+        self.tmpname = Path(mkdtemp())\n+        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n+        r = static.File(str(self.tmpname))\n+        r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n+        r.putChild(b\"wait\", ForeverTakingResource())\n+        r.putChild(b\"hang-after-headers\", ForeverTakingResource(write=True))\n+        r.putChild(b\"nolength\", NoLengthResource())\n+        r.putChild(b\"host\", HostHeaderResource())\n+        r.putChild(b\"payload\", PayloadResource())\n+        r.putChild(b\"broken\", BrokenDownloadResource())\n+        r.putChild(b\"chunked\", ChunkedResource())\n+        r.putChild(b\"broken-chunked\", BrokenChunkedResource())\n+        r.putChild(b\"contentlength\", ContentLengthHeaderResource())\n+        r.putChild(b\"nocontenttype\", EmptyContentTypeHeaderResource())\n+        r.putChild(b\"largechunkedfile\", LargeChunkedFileResource())\n+        r.putChild(b\"duplicate-header\", DuplicateHeaderResource())\n+        r.putChild(b\"echo\", Echo())\n+        self.site = server.Site(r, timeout=None)\n+        self.wrapper = WrappingFactory(self.site)\n+        self.host = \"localhost\"\n+        if self.scheme == \"https\":\n+            # Using WrappingFactory do not enable HTTP/2 failing all the\n+            # tests with H2DownloadHandler\n+            self.port = reactor.listenSSL(\n+                0,\n+                self.site,\n+                ssl_context_factory(self.keyfile, self.certfile),\n+                interface=self.host,\n+            )\n+        else:\n+            self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n+        self.portno = self.port.getHost().port\n+        self.download_handler = build_from_crawler(\n+            self.download_handler_cls, get_crawler()\n+        )\n+        self.download_request = self.download_handler.download_request\n+\n+    @defer.inlineCallbacks\n+    def tearDown(self):\n+        yield self.port.stopListening()\n+        if hasattr(self.download_handler, \"close\"):\n+            yield self.download_handler.close()\n+        shutil.rmtree(self.tmpname)\n+\n+    def getURL(self, path):\n+        return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n+\n+    def test_download(self):\n+        request = Request(self.getURL(\"file\"))\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n+        return d\n+\n+    def test_download_head(self):\n+        request = Request(self.getURL(\"file\"), method=\"HEAD\")\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEqual, b\"\")\n+        return d\n+\n+    def test_redirect_status(self):\n+        request = Request(self.getURL(\"redirect\"))\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.status)\n+        d.addCallback(self.assertEqual, 302)\n+        return d\n+\n+    def test_redirect_status_head(self):\n+        request = Request(self.getURL(\"redirect\"), method=\"HEAD\")\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.status)\n+        d.addCallback(self.assertEqual, 302)\n+        return d\n+\n+    @defer.inlineCallbacks\n+    def test_timeout_download_from_spider_nodata_rcvd(self):\n+        if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n+            # https://twistedmatrix.com/trac/ticket/10279\n+            raise unittest.SkipTest(\n+                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n+            )\n+\n+        # client connects but no data is received\n+        spider = Spider(\"foo\")\n+        meta = {\"download_timeout\": 0.5}\n+        request = Request(self.getURL(\"wait\"), meta=meta)\n+        d = self.download_request(request, spider)\n+        yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n+\n+    @defer.inlineCallbacks\n+    def test_timeout_download_from_spider_server_hangs(self):\n+        if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n+            # https://twistedmatrix.com/trac/ticket/10279\n+            raise unittest.SkipTest(\n+                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n+            )\n+        # client connects, server send headers and some body bytes but hangs\n+        spider = Spider(\"foo\")\n+        meta = {\"download_timeout\": 0.5}\n+        request = Request(self.getURL(\"hang-after-headers\"), meta=meta)\n+        d = self.download_request(request, spider)\n+        yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n+\n+    def test_host_header_not_in_request_headers(self):\n+        def _test(response):\n+            assert response.body == to_bytes(f\"{self.host}:{self.portno}\")\n+            assert not request.headers\n+\n+        request = Request(self.getURL(\"host\"))\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+\n+    def test_host_header_seted_in_request_headers(self):\n+        host = self.host + \":\" + str(self.portno)\n+\n+        def _test(response):\n+            assert response.body == host.encode()\n+            assert request.headers.get(\"Host\") == host.encode()\n+\n+        request = Request(self.getURL(\"host\"), headers={\"Host\": host})\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+\n+    def test_content_length_zero_bodyless_post_request_headers(self):\n+        \"\"\"Tests if \"Content-Length: 0\" is sent for bodyless POST requests.\n+\n+        This is not strictly required by HTTP RFCs but can cause trouble\n+        for some web servers.\n+        See:\n+        https://github.com/scrapy/scrapy/issues/823\n+        https://issues.apache.org/jira/browse/TS-2902\n+        https://github.com/kennethreitz/requests/issues/405\n+        https://bugs.python.org/issue14721\n+        \"\"\"\n+\n+        def _test(response):\n+            assert response.body == b\"0\"\n+\n+        request = Request(self.getURL(\"contentlength\"), method=\"POST\")\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+\n+    def test_content_length_zero_bodyless_post_only_one(self):\n+        def _test(response):\n+            import json\n+\n+            headers = Headers(json.loads(response.text)[\"headers\"])\n+            contentlengths = headers.getlist(\"Content-Length\")\n+            assert len(contentlengths) == 1\n+            assert contentlengths == [b\"0\"]\n+\n+        request = Request(self.getURL(\"echo\"), method=\"POST\")\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+\n+    def test_payload(self):\n+        body = b\"1\" * 100  # PayloadResource requires body length to be 100\n+        request = Request(self.getURL(\"payload\"), method=\"POST\", body=body)\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEqual, body)\n+        return d\n+\n+    def test_response_header_content_length(self):\n+        request = Request(self.getURL(\"file\"), method=b\"GET\")\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.headers[b\"content-length\"])\n+        d.addCallback(self.assertEqual, b\"159\")\n+        return d\n+\n+    def _test_response_class(self, filename, body, response_class):\n+        def _test(response):\n+            assert type(response) is response_class  # pylint: disable=unidiomatic-typecheck\n+\n+        request = Request(self.getURL(filename), body=body)\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+\n+    def test_response_class_from_url(self):\n+        return self._test_response_class(\"foo.html\", b\"\", HtmlResponse)\n+\n+    def test_response_class_from_body(self):\n+        return self._test_response_class(\n+            \"foo\",\n+            b\"<!DOCTYPE html>\\n<title>.</title>\",\n+            HtmlResponse,\n+        )\n+\n+    def test_get_duplicate_header(self):\n+        def _test(response):\n+            assert response.headers.getlist(b\"Set-Cookie\") == [b\"a=b\", b\"c=d\"]\n+\n+        request = Request(self.getURL(\"duplicate-header\"))\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+\n+\n+class TestHttp11Base(TestHttpBase):\n+    \"\"\"HTTP 1.1 test case\"\"\"\n+\n+    def test_download_without_maxsize_limit(self):\n+        request = Request(self.getURL(\"file\"))\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n+        return d\n+\n+    def test_response_class_choosing_request(self):\n+        \"\"\"Tests choosing of correct response type\n+        in case of Content-Type is empty but body contains text.\n+        \"\"\"\n+        body = b\"Some plain text\\ndata with tabs\\t and null bytes\\0\"\n+\n+        def _test_type(response):\n+            assert type(response) is TextResponse  # pylint: disable=unidiomatic-typecheck\n+\n+        request = Request(self.getURL(\"nocontenttype\"), body=body)\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(_test_type)\n+        return d\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_maxsize(self):\n+        request = Request(self.getURL(\"file\"))\n+\n+        # 10 is minimal size for this request and the limit is only counted on\n+        # response body. (regardless of headers)\n+        d = self.download_request(request, Spider(\"foo\", download_maxsize=10))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n+        yield d\n+\n+        d = self.download_request(request, Spider(\"foo\", download_maxsize=9))\n+        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_maxsize_very_large_file(self):\n+        with mock.patch(\"scrapy.core.downloader.handlers.http11.logger\") as logger:\n+            request = Request(self.getURL(\"largechunkedfile\"))\n+\n+            def check(logger):\n+                logger.warning.assert_called_once_with(mock.ANY, mock.ANY)\n+\n+            d = self.download_request(request, Spider(\"foo\", download_maxsize=1500))\n+            yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+\n+            # As the error message is logged in the dataReceived callback, we\n+            # have to give a bit of time to the reactor to process the queue\n+            # after closing the connection.\n+            d = defer.Deferred()\n+            d.addCallback(check)\n+            reactor.callLater(0.1, d.callback, logger)\n+            yield d\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_maxsize_per_req(self):\n+        meta = {\"download_maxsize\": 2}\n+        request = Request(self.getURL(\"file\"), meta=meta)\n+        d = self.download_request(request, Spider(\"foo\"))\n+        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_small_maxsize_per_spider(self):\n+        request = Request(self.getURL(\"file\"))\n+        d = self.download_request(request, Spider(\"foo\", download_maxsize=2))\n+        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+\n+    def test_download_with_large_maxsize_per_spider(self):\n+        request = Request(self.getURL(\"file\"))\n+        d = self.download_request(request, Spider(\"foo\", download_maxsize=100))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n+        return d\n+\n+    def test_download_chunked_content(self):\n+        request = Request(self.getURL(\"chunked\"))\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEqual, b\"chunked content\\n\")\n+        return d\n+\n+    def test_download_broken_content_cause_data_loss(self, url=\"broken\"):\n+        # TODO: this one checks for Twisted-specific exceptions\n+        request = Request(self.getURL(url))\n+        d = self.download_request(request, Spider(\"foo\"))\n+\n+        def checkDataLoss(failure):\n+            if failure.check(ResponseFailed) and any(\n+                r.check(_DataLoss) for r in failure.value.reasons\n+            ):\n+                return None\n+            return failure\n+\n+        d.addCallback(lambda _: self.fail(\"No DataLoss exception\"))\n+        d.addErrback(checkDataLoss)\n+        return d\n+\n+    def test_download_broken_chunked_content_cause_data_loss(self):\n+        return self.test_download_broken_content_cause_data_loss(\"broken-chunked\")\n+\n+    def test_download_broken_content_allow_data_loss(self, url=\"broken\"):\n+        request = Request(self.getURL(url), meta={\"download_fail_on_dataloss\": False})\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.flags)\n+        d.addCallback(self.assertEqual, [\"dataloss\"])\n+        return d\n+\n+    def test_download_broken_chunked_content_allow_data_loss(self):\n+        return self.test_download_broken_content_allow_data_loss(\"broken-chunked\")\n+\n+    def test_download_broken_content_allow_data_loss_via_setting(self, url=\"broken\"):\n+        crawler = get_crawler(settings_dict={\"DOWNLOAD_FAIL_ON_DATALOSS\": False})\n+        download_handler = build_from_crawler(self.download_handler_cls, crawler)\n+        request = Request(self.getURL(url))\n+        d = download_handler.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.flags)\n+        d.addCallback(self.assertEqual, [\"dataloss\"])\n+        return d\n+\n+    def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n+        return self.test_download_broken_content_allow_data_loss_via_setting(\n+            \"broken-chunked\"\n+        )\n+\n+    def test_protocol(self):\n+        request = Request(self.getURL(\"host\"), method=\"GET\")\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.protocol)\n+        d.addCallback(self.assertEqual, \"HTTP/1.1\")\n+        return d\n+\n+\n+class TestHttps11Base(TestHttp11Base):\n+    scheme = \"https\"\n+\n+    tls_log_message = (\n+        'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=localhost\", '\n+        'subject \"/C=IE/O=Scrapy/CN=localhost\"'\n+    )\n+\n+    @defer.inlineCallbacks\n+    def test_tls_logging(self):\n+        crawler = get_crawler(\n+            settings_dict={\"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\": True}\n+        )\n+        download_handler = build_from_crawler(self.download_handler_cls, crawler)\n+        try:\n+            with LogCapture() as log_capture:\n+                request = Request(self.getURL(\"file\"))\n+                d = download_handler.download_request(request, Spider(\"foo\"))\n+                d.addCallback(lambda r: r.body)\n+                d.addCallback(self.assertEqual, b\"0123456789\")\n+                yield d\n+                log_capture.check_present(\n+                    (\"scrapy.core.downloader.tls\", \"DEBUG\", self.tls_log_message)\n+                )\n+        finally:\n+            yield download_handler.close()\n+\n+\n+class TestSimpleHttpsBase(unittest.TestCase, ABC):\n+    \"\"\"Base class for special cases tested with just one simple request\"\"\"\n+\n+    keyfile = \"keys/localhost.key\"\n+    certfile = \"keys/localhost.crt\"\n+    cipher_string: str | None = None\n+\n+    @property\n+    @abstractmethod\n+    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n+        raise NotImplementedError\n+\n+    def setUp(self):\n+        self.tmpname = Path(mkdtemp())\n+        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n+        r = static.File(str(self.tmpname))\n+        self.site = server.Site(r, timeout=None)\n+        self.host = \"localhost\"\n+        self.port = reactor.listenSSL(\n+            0,\n+            self.site,\n+            ssl_context_factory(\n+                self.keyfile, self.certfile, cipher_string=self.cipher_string\n+            ),\n+            interface=self.host,\n+        )\n+        self.portno = self.port.getHost().port\n+        if self.cipher_string is not None:\n+            settings_dict = {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.cipher_string}\n+        else:\n+            settings_dict = None\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.download_handler = build_from_crawler(self.download_handler_cls, crawler)\n+        self.download_request = self.download_handler.download_request\n+\n+    @defer.inlineCallbacks\n+    def tearDown(self):\n+        yield self.port.stopListening()\n+        if hasattr(self.download_handler, \"close\"):\n+            yield self.download_handler.close()\n+        shutil.rmtree(self.tmpname)\n+\n+    def getURL(self, path):\n+        return f\"https://{self.host}:{self.portno}/{path}\"\n+\n+    def test_download(self):\n+        request = Request(self.getURL(\"file\"))\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n+        return d\n+\n+\n+class TestHttpsWrongHostnameBase(TestSimpleHttpsBase):\n+    # above tests use a server certificate for \"localhost\",\n+    # client connection to \"localhost\" too.\n+    # here we test that even if the server certificate is for another domain,\n+    # \"www.example.com\" in this case,\n+    # the tests still pass\n+    keyfile = \"keys/example-com.key.pem\"\n+    certfile = \"keys/example-com.cert.pem\"\n+\n+\n+class TestHttpsInvalidDNSIdBase(TestSimpleHttpsBase):\n+    \"\"\"Connect to HTTPS hosts with IP while certificate uses domain names IDs.\"\"\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.host = \"127.0.0.1\"\n+\n+\n+class TestHttpsInvalidDNSPatternBase(TestSimpleHttpsBase):\n+    \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n+\n+    keyfile = \"keys/localhost.ip.key\"\n+    certfile = \"keys/localhost.ip.crt\"\n+\n+\n+class TestHttpsCustomCiphersBase(TestSimpleHttpsBase):\n+    cipher_string = \"CAMELLIA256-SHA\"\n+\n+\n+class TestHttpMockServerBase(unittest.TestCase, ABC):\n+    \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n+\n+    @property\n+    @abstractmethod\n+    def settings_dict(self) -> dict[str, Any] | None:\n+        raise NotImplementedError\n+\n+    is_secure = False\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_content_length(self):\n+        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n+        # http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid\n+        # download it\n+        yield crawler.crawl(\n+            seed=Request(\n+                url=self.mockserver.url(\"/partial\", is_secure=self.is_secure),\n+                meta={\"download_maxsize\": 1000},\n+            )\n+        )\n+        failure = crawler.spider.meta[\"failure\"]\n+        assert isinstance(failure.value, defer.CancelledError)\n+\n+    @defer.inlineCallbacks\n+    def test_download(self):\n+        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n+        yield crawler.crawl(\n+            seed=Request(url=self.mockserver.url(\"\", is_secure=self.is_secure))\n+        )\n+        failure = crawler.spider.meta.get(\"failure\")\n+        assert failure is None\n+        reason = crawler.spider.meta[\"close_reason\"]\n+        assert reason == \"finished\"\n+\n+\n+class UriResource(resource.Resource):\n+    \"\"\"Return the full uri that was requested\"\"\"\n+\n+    def getChild(self, path, request):\n+        return self\n+\n+    def render(self, request):\n+        # Note: this is an ugly hack for CONNECT request timeout test.\n+        #       Returning some data here fail SSL/TLS handshake\n+        # ToDo: implement proper HTTPS proxy tests, not faking them.\n+        if request.method != b\"CONNECT\":\n+            return request.uri\n+        return b\"\"\n+\n+\n+class TestHttpProxyBase(unittest.TestCase, ABC):\n+    expected_http_proxy_request_body = b\"http://example.com\"\n+\n+    @property\n+    @abstractmethod\n+    def download_handler_cls(self) -> type[DownloadHandlerProtocol]:\n+        raise NotImplementedError\n+\n+    def setUp(self):\n+        site = server.Site(UriResource(), timeout=None)\n+        wrapper = WrappingFactory(site)\n+        self.port = reactor.listenTCP(0, wrapper, interface=\"127.0.0.1\")\n+        self.portno = self.port.getHost().port\n+        self.download_handler = build_from_crawler(\n+            self.download_handler_cls, get_crawler()\n+        )\n+        self.download_request = self.download_handler.download_request\n+\n+    @defer.inlineCallbacks\n+    def tearDown(self):\n+        yield self.port.stopListening()\n+        if hasattr(self.download_handler, \"close\"):\n+            yield self.download_handler.close()\n+\n+    def getURL(self, path):\n+        return f\"http://127.0.0.1:{self.portno}/{path}\"\n+\n+    def test_download_with_proxy(self):\n+        def _test(response):\n+            assert response.status == 200\n+            assert response.url == request.url\n+            assert response.body == self.expected_http_proxy_request_body\n+\n+        http_proxy = self.getURL(\"\")\n+        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+\n+    def test_download_without_proxy(self):\n+        def _test(response):\n+            assert response.status == 200\n+            assert response.url == request.url\n+            assert response.body == b\"/path/to/resource\"\n+\n+        request = Request(self.getURL(\"path/to/resource\"))\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_proxy_https_timeout(self):\n+        if NON_EXISTING_RESOLVABLE:\n+            pytest.skip(\"Non-existing hosts are resolvable\")\n+        http_proxy = self.getURL(\"\")\n+        domain = \"https://no-such-domain.nosuch\"\n+        request = Request(domain, meta={\"proxy\": http_proxy, \"download_timeout\": 0.2})\n+        d = self.download_request(request, Spider(\"foo\"))\n+        timeout = yield self.assertFailure(d, error.TimeoutError)\n+        assert domain in timeout.osError\n+\n+    def test_download_with_proxy_without_http_scheme(self):\n+        def _test(response):\n+            assert response.status == 200\n+            assert response.url == request.url\n+            assert response.body == self.expected_http_proxy_request_body\n+\n+        http_proxy = self.getURL(\"\").replace(\"http://\", \"\")\n+        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
