{"custom_id": "keras#d9f1874e1f595121c0c83c499b9a60989a58e594", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 91 | Files Changed: 4 | Hunks: 49 | Methods Changed: 21 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 140 | Churn Cumulative: 1646 | Contributors (this commit): 14 | Commits (past 90d): 15 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -107,15 +107,15 @@ class Dense(Layer):\n         self.supports_masking = True\n \n     def build(self, input_shape):\n-        input_dim = input_shape[-1]\n+        kernel_shape = (input_shape[-1], self.units)\n         if self.quantization_mode:\n-            self.quantized_build(input_shape, mode=self.quantization_mode)\n+            self.quantized_build(kernel_shape, mode=self.quantization_mode)\n         if self.quantization_mode != \"int8\":\n             # If the layer is quantized to int8, `self._kernel` will be added\n             # in `self._int8_build`. Therefore, we skip it here.\n             self._kernel = self.add_weight(\n                 name=\"kernel\",\n-                shape=(input_dim, self.units),\n+                shape=kernel_shape,\n                 initializer=self.kernel_initializer,\n                 regularizer=self.kernel_regularizer,\n                 constraint=self.kernel_constraint,\n@@ -130,7 +130,7 @@ class Dense(Layer):\n             )\n         else:\n             self.bias = None\n-        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n+        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_shape[-1]})\n         self.built = True\n         if self.lora_rank:\n             self.enable_lora(self.lora_rank)\n@@ -312,37 +312,30 @@ class Dense(Layer):\n \n     # Quantization-related (int8 and float8) methods\n \n-    def quantized_build(self, input_shape, mode):\n+    def quantized_build(self, kernel_shape, mode):\n         if mode == \"int8\":\n-            input_dim = input_shape[-1]\n-            kernel_shape = (input_dim, self.units)\n             self._int8_build(kernel_shape)\n         elif mode == \"float8\":\n             self._float8_build()\n         else:\n             raise self._quantization_mode_error(mode)\n+        self._is_quantized = True\n \n-    def _int8_build(\n-        self,\n-        kernel_shape,\n-        kernel_initializer=\"zeros\",\n-        kernel_scale_initializer=\"ones\",\n-    ):\n+    def _int8_build(self, kernel_shape):\n         self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n         self._kernel = self.add_weight(\n             name=\"kernel\",\n             shape=kernel_shape,\n-            initializer=kernel_initializer,\n+            initializer=\"zeros\",\n             dtype=\"int8\",\n             trainable=False,\n         )\n         self.kernel_scale = self.add_weight(\n             name=\"kernel_scale\",\n             shape=(self.units,),\n-            initializer=kernel_scale_initializer,\n+            initializer=\"ones\",\n             trainable=False,\n         )\n-        self._is_quantized = True\n \n     def _float8_build(self):\n         from keras.src.dtype_policies import QuantizedFloat8DTypePolicy\n@@ -362,6 +355,7 @@ class Dense(Layer):\n             \"dtype\": \"float32\",  # Always be float32\n             \"trainable\": True,\n             \"autocast\": False,\n+            \"overwrite_with_gradient\": True,\n         }\n         amax_history_kwargs = {\n             \"shape\": (amax_history_length,),\n@@ -369,6 +363,7 @@ class Dense(Layer):\n             \"dtype\": \"float32\",  # Always be float32\n             \"trainable\": True,\n             \"autocast\": False,\n+            \"overwrite_with_gradient\": True,\n         }\n         self.inputs_scale = self.add_weight(name=\"inputs_scale\", **scale_kwargs)\n         self.inputs_amax_history = self.add_weight(\n@@ -384,16 +379,6 @@ class Dense(Layer):\n         self.outputs_grad_amax_history = self.add_weight(\n             name=\"outputs_grad_amax_history\", **amax_history_kwargs\n         )\n-        # We need to set `overwrite_with_gradient=True` to instruct the\n-        # optimizer to directly overwrite these variables with their computed\n-        # gradients during training\n-        self.inputs_scale.overwrite_with_gradient = True\n-        self.inputs_amax_history.overwrite_with_gradient = True\n-        self.kernel_scale.overwrite_with_gradient = True\n-        self.kernel_amax_history.overwrite_with_gradient = True\n-        self.outputs_grad_scale.overwrite_with_gradient = True\n-        self.outputs_grad_amax_history.overwrite_with_gradient = True\n-        self._is_quantized = True\n \n     def _int8_call(self, inputs, training=None):\n         @ops.custom_gradient\n@@ -526,21 +511,17 @@ class Dense(Layer):\n         if type_check and (type(self) is not Dense):\n             raise self._not_implemented_error(self.quantize)\n \n+        kernel_shape = self._kernel.shape\n         if mode == \"int8\":\n-            # Quantize `self._kernel` to int8 and compute corresponding scale\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                 self._kernel, axis=0, to_numpy=True\n             )\n             kernel_scale = ops.squeeze(kernel_scale, axis=0)\n-            kernel_shape = tuple(self._kernel.shape)\n             del self._kernel\n-            # Utilize a lambda expression as an initializer to prevent adding a\n-            # large constant to the computation graph.\n-            self._int8_build(kernel_shape, kernel_value, kernel_scale)\n-        elif mode == \"float8\":\n-            self._float8_build()\n-        else:\n-            raise self._quantization_mode_error(mode)\n+        self.quantized_build(kernel_shape, mode)\n+        if mode == \"int8\":\n+            self._kernel.assign(kernel_value)\n+            self.kernel_scale.assign(kernel_scale)\n \n         # Set new dtype policy\n         if self.dtype_policy.quantization_mode is None:\n\n@@ -160,11 +160,9 @@ class EinsumDense(Layer):\n         )\n         kernel_shape, bias_shape, full_output_shape = shape_data\n         self.full_output_shape = tuple(full_output_shape)\n-        # `self._int8_build` needs `self.input_spec`\n         self.input_spec = InputSpec(ndim=len(input_shape))\n-        # We use `self._dtype_policy` to check to avoid issues in torch dynamo\n         if self.quantization_mode is not None:\n-            self.quantized_build(input_shape, mode=self.quantization_mode)\n+            self.quantized_build(kernel_shape, mode=self.quantization_mode)\n         if self.quantization_mode != \"int8\":\n             # If the layer is quantized to int8, `self._kernel` will be added\n             # in `self._int8_build`. Therefore, we skip it here.\n@@ -372,27 +370,16 @@ class EinsumDense(Layer):\n \n     # Quantization-related (int8 and float8) methods\n \n-    def quantized_build(self, input_shape, mode):\n+    def quantized_build(self, kernel_shape, mode):\n         if mode == \"int8\":\n-            shape_data = _analyze_einsum_string(\n-                self.equation,\n-                self.bias_axes,\n-                input_shape,\n-                self.partial_output_shape,\n-            )\n-            kernel_shape, _, _ = shape_data\n             self._int8_build(kernel_shape)\n         elif mode == \"float8\":\n             self._float8_build()\n         else:\n             raise self._quantization_mode_error(mode)\n+        self._is_quantized = True\n \n-    def _int8_build(\n-        self,\n-        kernel_shape,\n-        kernel_initializer=\"zeros\",\n-        kernel_scale_initializer=\"ones\",\n-    ):\n+    def _int8_build(self, kernel_shape):\n         (\n             self._input_reduced_axes,\n             self._kernel_reduced_axes,\n@@ -411,7 +398,7 @@ class EinsumDense(Layer):\n         self._kernel = self.add_weight(\n             name=\"kernel\",\n             shape=kernel_shape,\n-            initializer=kernel_initializer,\n+            initializer=\"zeros\",\n             dtype=\"int8\",\n             trainable=False,\n         )\n@@ -426,10 +413,9 @@ class EinsumDense(Layer):\n         self.kernel_scale = self.add_weight(\n             name=\"kernel_scale\",\n             shape=kernel_scale_shape,\n-            initializer=kernel_scale_initializer,\n+            initializer=\"ones\",\n             trainable=False,\n         )\n-        self._is_quantized = True\n \n     def _float8_build(self):\n         from keras.src.dtype_policies import QuantizedFloat8DTypePolicy\n@@ -449,6 +435,7 @@ class EinsumDense(Layer):\n             \"dtype\": \"float32\",  # Always be float32\n             \"trainable\": True,\n             \"autocast\": False,\n+            \"overwrite_with_gradient\": True,\n         }\n         amax_history_kwargs = {\n             \"shape\": (amax_history_length,),\n@@ -456,6 +443,7 @@ class EinsumDense(Layer):\n             \"dtype\": \"float32\",  # Always be float32\n             \"trainable\": True,\n             \"autocast\": False,\n+            \"overwrite_with_gradient\": True,\n         }\n         self.inputs_scale = self.add_weight(name=\"inputs_scale\", **scale_kwargs)\n         self.inputs_amax_history = self.add_weight(\n@@ -471,16 +459,6 @@ class EinsumDense(Layer):\n         self.outputs_grad_amax_history = self.add_weight(\n             name=\"outputs_grad_amax_history\", **amax_history_kwargs\n         )\n-        # We need to set `overwrite_with_gradient=True` to instruct the\n-        # optimizer to directly overwrite these variables with their computed\n-        # gradients during training\n-        self.inputs_scale.overwrite_with_gradient = True\n-        self.inputs_amax_history.overwrite_with_gradient = True\n-        self.kernel_scale.overwrite_with_gradient = True\n-        self.kernel_amax_history.overwrite_with_gradient = True\n-        self.outputs_grad_scale.overwrite_with_gradient = True\n-        self.outputs_grad_amax_history.overwrite_with_gradient = True\n-        self._is_quantized = True\n \n     def _int8_call(self, inputs, training=None):\n         @ops.custom_gradient\n@@ -642,6 +620,7 @@ class EinsumDense(Layer):\n         if type_check and (type(self) is not EinsumDense):\n             raise self._not_implemented_error(self.quantize)\n \n+        kernel_shape = self._kernel.shape\n         if mode == \"int8\":\n             (\n                 self._input_reduced_axes,\n@@ -670,15 +649,11 @@ class EinsumDense(Layer):\n                 kernel_scale = ops.squeeze(\n                     kernel_scale, axis=self._kernel_squeeze_axes\n                 )\n-            kernel_shape = tuple(self._kernel.shape)\n             del self._kernel\n-            # Utilize a lambda expression as an initializer to prevent adding a\n-            # large constant to the computation graph.\n-            self._int8_build(kernel_shape, kernel_value, kernel_scale)\n-        elif mode == \"float8\":\n-            self._float8_build()\n-        else:\n-            raise self._quantization_mode_error(mode)\n+        self.quantized_build(kernel_shape, mode)\n+        if mode == \"int8\":\n+            self._kernel.assign(kernel_value)\n+            self.kernel_scale.assign(kernel_scale)\n \n         # Set new dtype policy\n         if self.dtype_policy.quantization_mode is None:\n\n@@ -119,11 +119,12 @@ class Embedding(Layer):\n     def build(self, input_shape=None):\n         if self.built:\n             return\n+        embeddings_shape = (self.input_dim, self.output_dim)\n         if self.quantization_mode is not None:\n-            self.quantized_build(input_shape, mode=self.quantization_mode)\n+            self.quantized_build(embeddings_shape, mode=self.quantization_mode)\n         if self.quantization_mode != \"int8\":\n             self._embeddings = self.add_weight(\n-                shape=(self.input_dim, self.output_dim),\n+                shape=embeddings_shape,\n                 initializer=self.embeddings_initializer,\n                 name=\"embeddings\",\n                 regularizer=self.embeddings_regularizer,\n@@ -312,21 +313,18 @@ class Embedding(Layer):\n             f\"Received: quantization_mode={mode}\"\n         )\n \n-    def quantized_build(self, input_shape, mode):\n+    def quantized_build(self, embeddings_shape, mode):\n         if mode == \"int8\":\n-            self._int8_build()\n+            self._int8_build(embeddings_shape)\n         else:\n             raise self._quantization_mode_error(mode)\n+        self._is_quantized = True\n \n-    def _int8_build(\n-        self,\n-        embeddings_initializer=\"zeros\",\n-        embeddings_scale_initializer=\"ones\",\n-    ):\n+    def _int8_build(self, embeddings_shape):\n         self._embeddings = self.add_weight(\n             name=\"embeddings\",\n-            shape=(self.input_dim, self.output_dim),\n-            initializer=embeddings_initializer,\n+            shape=embeddings_shape,\n+            initializer=\"zeros\",\n             dtype=\"int8\",\n             trainable=False,\n         )\n@@ -336,10 +334,9 @@ class Embedding(Layer):\n         self.embeddings_scale = self.add_weight(\n             name=\"embeddings_scale\",\n             shape=(self.input_dim,),\n-            initializer=embeddings_scale_initializer,\n+            initializer=\"ones\",\n             trainable=False,\n         )\n-        self._is_quantized = True\n \n     def quantized_call(self, *args, **kwargs):\n         if self.quantization_mode != \"int8\":\n@@ -371,6 +368,7 @@ class Embedding(Layer):\n         if type_check and (type(self) is not Embedding):\n             raise self._not_implemented_error(self.quantize)\n \n+        embeddings_shape = (self.input_dim, self.output_dim)\n         if mode == \"int8\":\n             # Quantize `self._embeddings` to int8 and compute corresponding\n             # scale\n@@ -379,11 +377,10 @@ class Embedding(Layer):\n             )\n             embeddings_scale = ops.squeeze(embeddings_scale, axis=-1)\n             del self._embeddings\n-            # Utilize a lambda expression as an initializer to prevent adding a\n-            # large constant to the computation graph.\n-            self._int8_build(embeddings_value, embeddings_scale)\n-        else:\n-            raise self._quantization_mode_error(mode)\n+        self.quantized_build(embeddings_shape, mode)\n+        if mode == \"int8\":\n+            self._embeddings.assign(embeddings_value)\n+            self.embeddings_scale.assign(embeddings_scale)\n \n         # Set new dtype policy\n         if self.dtype_policy.quantization_mode is None:\n\n@@ -521,6 +521,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         regularizer=None,\n         constraint=None,\n         aggregation=\"none\",\n+        overwrite_with_gradient=False,\n         name=None,\n     ):\n         \"\"\"Add a weight variable to the layer.\n@@ -552,6 +553,9 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n                 the type of multi-replica aggregation to be used for this\n                 variable when writing custom data parallel training loops.\n                 Defaults to `\"none\"`.\n+            overwrite_with_gradient: Boolean, whether to overwrite the variable\n+                with the computed gradient. This is useful for float8 training.\n+                Defaults to `False`.\n             name: String name of the variable. Useful for debugging purposes.\n         \"\"\"\n         self._check_super_called()\n@@ -580,6 +584,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         # Will be added to layer.losses\n         variable.regularizer = regularizers.get(regularizer)\n         variable.constraint = constraints.get(constraint)\n+        variable.overwrite_with_gradient = overwrite_with_gradient\n         self._track_variable(variable)\n         return variable\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f2793ec63ede88dd6ef8b4296c4b837572fd095", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 67 | Churn Cumulative: 258 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,10 @@\n import jax\n import numpy as np\n \n+from keras.src.backend.common import global_state\n+from keras.src.random import seed_generator\n from keras.src.utils import jax_utils\n+from keras.src.utils import rng_utils\n \n \n def list_devices(device_type=None):\n@@ -185,6 +188,52 @@ def distribute_data_input(per_process_batch, layout, batch_dim_name):\n     return global_batch_array\n \n \n+def initialize_rng():\n+    \"\"\"Initializes the global random number generator across processes.\n+\n+    This is required for consistent initialization in multi-host settings.\n+    \"\"\"\n+    global_seed = rng_utils.get_random_seed()\n+    # Only set a random seed if not already set\n+    # via keras.config.set_random_seed()\n+    if global_seed is None:\n+        # Generate a random seed on each CPU host and psum them to get a single\n+        # consistent seed across all processes.\n+        cpu_devices = jax.devices(\"cpu\")\n+        num_local_cpu_devices = jax.local_device_count(\"cpu\")\n+        # Seed must be in range [0, 2^32 - 1], so to ensure proper range and\n+        # avoid signed integer overflow, we use uint32.\n+        local_seed = jax.numpy.asarray(\n+            [seed_generator.make_default_seed()] * num_local_cpu_devices,\n+            dtype=jax.numpy.uint32,\n+        )\n+        # Sum across processes and pull out the first item.\n+        global_seed = jax.pmap(\n+            lambda x: jax.lax.psum(x, \"all\"),\n+            axis_name=\"all\",\n+            devices=cpu_devices,\n+        )(local_seed).item(0)\n+        # Set the global seed.\n+        rng_utils.set_random_seed(global_seed)\n+\n+    # Check if the global seed generator is set and ensure it has an initialized\n+    # seed.  Otherwise, reset the seed to the global seed.\n+    global_seed_generator = global_state.get_global_attribute(\n+        \"global_seed_generator\"\n+    )\n+    if global_seed_generator is not None:\n+        seed = global_seed_generator.get_config()[\"seed\"]\n+        if seed is None:\n+            global_state.set_global_attribute(\n+                \"global_seed_generator\",\n+                seed_generator.SeedGenerator(\n+                    seed=global_seed,\n+                    name=global_seed_generator.name,\n+                    backend=global_seed_generator.backend,\n+                ),\n+            )\n+\n+\n def initialize(job_addresses, num_processes, process_id):\n     if job_addresses and \",\" in job_addresses:\n         # When user provide all the job addresses, we will split and get the\n@@ -208,6 +257,9 @@ def initialize(job_addresses, num_processes, process_id):\n         process_id=process_id,\n     )\n \n+    # Ensure the random number generator is initialized across processes.\n+    initialize_rng()\n+\n \n def num_processes():\n     \"\"\"Return the number of processes for the current distribution setting.\"\"\"\n\n@@ -4,8 +4,11 @@ import numpy as np\n \n from keras.src import backend\n from keras.src.api_export import keras_export\n+from keras.src.backend.common import global_state\n from keras.src.utils.module_utils import tensorflow as tf\n \n+GLOBAL_RANDOM_SEED = \"global_random_seed\"\n+\n \n @keras_export(\"keras.utils.set_random_seed\")\n def set_random_seed(seed):\n@@ -46,6 +49,9 @@ def set_random_seed(seed):\n             \"Expected `seed` argument to be an integer. \"\n             f\"Received: seed={seed} (of type {type(seed)})\"\n         )\n+\n+    # Store seed in global state so we can query it if set.\n+    global_state.set_global_attribute(GLOBAL_RANDOM_SEED, seed)\n     random.seed(seed)\n     np.random.seed(seed)\n     if tf.available:\n@@ -54,3 +60,12 @@ def set_random_seed(seed):\n         import torch\n \n         torch.manual_seed(seed)\n+\n+\n+def get_random_seed():\n+    \"\"\"Returns the explicit integer random seed if set.\n+\n+    If the seed has been explicitly set via `set_random_seed`, then\n+    returns the seed.  Otherwise, returns `None`.\n+    \"\"\"\n+    return global_state.get_global_attribute(GLOBAL_RANDOM_SEED)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d22f21367712474392ee1cefe1a1c0be63f208c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 14 | Churn Cumulative: 1739 | Contributors (this commit): 21 | Commits (past 90d): 29 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,6 +12,7 @@ from keras.src.backend.openvino.core import OpenVINOKerasTensor\n from keras.src.backend.openvino.core import (\n     align_operand_types as _align_operand_types,\n )\n+from keras.src.backend.openvino.core import convert_to_tensor\n from keras.src.backend.openvino.core import get_ov_output\n from keras.src.backend.openvino.core import ov_to_keras_type\n \n@@ -846,7 +847,18 @@ def greater_equal(x1, x2):\n \n \n def hstack(xs):\n-    raise NotImplementedError(\"`hstack` is not supported with openvino backend\")\n+    if not isinstance(xs, (list, tuple)):\n+        xs = (xs,)\n+    elems = [convert_to_tensor(elem) for elem in xs]\n+    element_type = elems[0].output.get_element_type()\n+    elems = [get_ov_output(elem, element_type) for elem in elems]\n+    is_1d = elems and len(elems[0].get_partial_shape().to_shape()) == 1\n+    axis = 0 if is_1d else 1\n+    for i in range(1, len(elems)):\n+        elems[0], elems[i] = _align_operand_types(\n+            elems[0], elems[i], \"hstack()\"\n+        )\n+    return OpenVINOKerasTensor(ov_opset.concat(elems, axis).output(0))\n \n \n def identity(n, dtype=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44c2f831daed1a2f3413c8886e105f9ad8fcc9d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 427 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: 0.5\n\nDIFF:\n@@ -108,5 +108,3 @@ class JaxOptimizer(base_optimizer.BaseOptimizer):\n                         average_var * should_overwrite_model_vars_int\n                         + var.value * should_not_overwrite_model_vars_int\n                     )\n-\n-        self._iterations.assign_add(1)\n\n@@ -513,9 +513,7 @@ class BaseOptimizer(KerasSaveable):\n                 )\n             )\n \n-            if len(list(grads)) == 0:\n-                return\n-\n+            if len(list(grads)) > 0:\n                 # Unscale gradients.\n                 scale = self.loss_scale_factor\n                 if scale is not None:\n@@ -528,6 +526,9 @@ class BaseOptimizer(KerasSaveable):\n                     if variable.constraint is not None:\n                         variable.assign(variable.constraint(variable))\n \n+        # Update iteration counter.\n+        self._iterations.assign_add(1)\n+\n     def _backend_apply_gradients(self, grads, trainable_variables):\n         \"\"\"Apply method that can be overridden by different backends.\n \n@@ -606,8 +607,6 @@ class BaseOptimizer(KerasSaveable):\n                     ),\n                     lambda: None,\n                 )\n-        # Update iteration counter.\n-        self._iterations.assign_add(1)\n \n     def _backend_update_step(self, grads, trainable_variables, learning_rate):\n         \"\"\"Collective update_step that can be overridden by the backend.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#583306d25b632fad608d625723d9d7426698daa0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 102 | Lines Deleted: 106 | Files Changed: 2 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): -7/4 | Churn Δ: 208 | Churn Cumulative: 522 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@\n \n import jax\n import numpy as np\n+from jax.experimental import layout as jax_layout\n \n from keras.src.backend.common import global_state\n from keras.src.random import seed_generator\n@@ -39,32 +40,13 @@ def distribute_variable(value, layout):\n     Args:\n         value: the initial value of the variable.\n         layout: `TensorLayout` for the created variable, or a\n-            `jax.sharding.Sharding` instance.\n+            JAX-supported layout instance\n+            (e.g. `jax.experimental.layout.Layout`, `jax.sharding.Sharding`).\n \n     Returns:\n         jax.Array which is the distributed variable.\n     \"\"\"\n-    if not isinstance(layout, jax.sharding.Sharding):\n-        layout = layout.backend_layout\n-    if isinstance(\n-        value, (jax.Array, jax.numpy.ndarray)\n-    ) and value.sharding.is_equivalent_to(layout, ndim=len(value.shape)):\n-        # Skip the relayout if the value is already having the proper sharding\n-        return value\n-\n-    if layout.is_fully_addressable:\n-        return jax.device_put(value, layout)\n-    else:\n-        # Need to only distribute the value to local addressable devices, and\n-        # repack them back into global format.\n-        mapping = layout.addressable_devices_indices_map(value.shape)\n-        local_values = jax.device_put(\n-            [value[i] for i in mapping.values()], list(mapping.keys())\n-        )\n-        global_value = jax.make_array_from_single_device_arrays(\n-            value.shape, layout, local_values\n-        )\n-        return global_value\n+    return distribute_tensor(value, layout)\n \n \n def distribute_tensor(tensor, layout):\n@@ -75,39 +57,43 @@ def distribute_tensor(tensor, layout):\n \n     Args:\n         tensor: `jax.Array` that need to be distributed.\n-        layout: `TensorLayout` for the distribution information, or a\n-            `jax.sharding.Sharding` instance.\n+        layout: `TensorLayout` for the created variable, or a\n+            JAX-supported layout instance\n+            (e.g. `jax.experimental.layout.Layout`, `jax.sharding.Sharding`).\n \n     Returns:\n         Distributed value.\n     \"\"\"\n-    if not isinstance(layout, jax.sharding.Sharding):\n+    # Avoid circular imports.\n+    from keras.src.distribution import TensorLayout\n+\n+    if isinstance(layout, TensorLayout):\n         layout = layout.backend_layout\n+\n     # TODO(scottzhu): This might not be a cheap check, we should consider\n     # have some proper JAX API for doing this check.\n     if jax_utils.is_in_jax_tracing_scope():\n         return jax.lax.with_sharding_constraint(tensor, layout)\n \n-    if layout.is_fully_addressable:\n+    # Skip relayout if unnecessary.\n+    if isinstance(tensor, jax.Array):\n+        if isinstance(\n+            layout, jax.sharding.Sharding\n+        ) and tensor.sharding.is_equivalent_to(layout, ndim=len(tensor.shape)):\n+            return tensor\n+        elif isinstance(layout, jax_layout.Layout):\n+            current_layout = getattr(tensor, \"layout\", None)\n+            if current_layout == layout:\n+                return tensor\n+\n     return jax.device_put(tensor, layout)\n-    else:\n-        # Need to only distribute the value to local addressable devices, and\n-        # repack them back into global format.\n-        mapping = layout.addressable_devices_indices_map(tensor.shape)\n-        local_values = jax.device_put(\n-            [tensor[i] for i in mapping.values()], list(mapping.keys())\n-        )\n-        global_value = jax.make_array_from_single_device_arrays(\n-            tensor.shape, layout, local_values\n-        )\n-        return global_value\n \n \n def distribute_data_input(per_process_batch, layout, batch_dim_name):\n     \"\"\"Distribute the input data with the corresponding layout.\n \n     Note that the inputs here is a local worker batch. Within the local worker,\n-    the data need to be further partitioned to map to the each of the devices.\n+    the data need to be further partitioned to map to each of the devices.\n \n     Args:\n         inputs: `jax.Array` that is already sharded to a local process size.\n@@ -117,75 +103,13 @@ def distribute_data_input(per_process_batch, layout, batch_dim_name):\n     Returns:\n         A global batch distributed according to `layout`.\n     \"\"\"\n-    if not isinstance(layout, jax.sharding.Sharding):\n+    # Avoid circular imports.\n+    from keras.src.distribution import TensorLayout\n+\n+    if isinstance(layout, TensorLayout):\n         layout = layout.backend_layout\n \n-    num_model_replicas_total = layout.mesh.shape[batch_dim_name]\n-\n-    mesh_model_dim_size = 1\n-    for name, dim_size in layout.mesh.shape.items():\n-        if not name == batch_dim_name:\n-            mesh_model_dim_size *= dim_size\n-\n-    num_model_replicas_per_process = num_model_replicas_total / num_processes()\n-    per_process_batch_size = per_process_batch.shape[0]\n-\n-    if num_model_replicas_per_process >= 1:\n-        # If there is more than one model replica per process, we need to\n-        # further shard the data to each of the model replicas.\n-        if num_model_replicas_total % num_processes() != 0:\n-            raise ValueError(\n-                \"If there is more than one replica per process, the batch \"\n-                \"dimension of the mesh should be divisible \"\n-                \"by the number of processes. Here, \"\n-                f\"batch dimension = {num_model_replicas_total}, while \"\n-                f\"number of processes = {num_processes()}\"\n-            )\n-\n-        per_replica_batch_size = int(\n-            per_process_batch_size // num_model_replicas_per_process\n-        )\n-        if per_process_batch_size % per_replica_batch_size != 0:\n-            raise ValueError(\n-                \"`per_process_batch_size` should be divisible by `\"\n-                \"per_replica_batch_size`. \"\n-                f\"per_process_batch_size={per_process_batch_size} and \"\n-                f\"per_replica_batch_size = {per_replica_batch_size}\"\n-            )\n-        per_replica_batches = np.split(\n-            per_process_batch, num_model_replicas_per_process\n-        )\n-        # Replicate data along the model_dim.\n-        per_device_batches = [\n-            per_replica_batch\n-            for per_replica_batch in per_replica_batches\n-            for _ in range(mesh_model_dim_size)\n-        ]\n-        batches_on_devices = [\n-            jax.device_put(batch, device)\n-            for batch, device in zip(\n-                per_device_batches, layout.addressable_devices\n-            )\n-        ]\n-    else:\n-        # If there are less than one model replicas per process, we need to\n-        # replicate the data to each of the model replicas. No further data\n-        # sharding is needed.\n-        per_replica_batch_size = per_process_batch_size\n-        batches_on_devices = [\n-            jax.device_put(per_process_batch, device)\n-            for device in layout.addressable_devices\n-        ]\n-\n-    global_batch_size = per_replica_batch_size * num_model_replicas_total\n-    global_batch_shape = (global_batch_size,) + per_process_batch.shape[1:]\n-    global_batch_array = jax.make_array_from_single_device_arrays(\n-        shape=global_batch_shape,\n-        sharding=layout,\n-        arrays=batches_on_devices,\n-    )\n-\n-    return global_batch_array\n+    return jax.make_array_from_process_local_data(layout, per_process_batch)\n \n \n def initialize_rng():\n\n@@ -7,6 +7,7 @@ from unittest import mock\n import jax\n import numpy as np\n import pytest\n+from jax.experimental import layout as jax_layout\n \n from keras.src import backend\n from keras.src import layers\n@@ -92,7 +93,6 @@ class JaxDistributionLibTest(testing.TestCase):\n \n     def test_distribute_variable(self):\n         # This test only verify the single worker/process behavior.\n-        # The multi-process test lives in g3.\n         jax_mesh = jax.sharding.Mesh(\n             np.array(jax.devices()).reshape(2, 4), (\"batch\", \"model\")\n         )\n@@ -126,6 +126,78 @@ class JaxDistributionLibTest(testing.TestCase):\n         # layout specified.\n         self.assertTrue(result.sharding.is_equivalent_to(target_layout, ndim=2))\n \n+    def test_distribute_tensor_with_jax_layout(self):\n+        jax_mesh = jax.sharding.Mesh(\n+            np.array(jax.devices()).reshape(2, 4), (\"batch\", \"model\")\n+        )\n+\n+        inputs = jax.numpy.array(np.random.normal(size=(16, 8)))\n+        target_layout = jax_layout.Layout(\n+            sharding=jax.sharding.NamedSharding(\n+                jax_mesh, jax.sharding.PartitionSpec(\"batch\", None)\n+            )\n+        )\n+\n+        @functools.partial(jax.jit, static_argnames=\"target_layout\")\n+        def test_function(inputs, target_layout):\n+            return distribution_lib.distribute_tensor(inputs, target_layout)\n+\n+        result = test_function(inputs, target_layout)\n+        # Note that the returned tensor has a different sharding implementation\n+        # which is GSPMDSharding, but it should be equivalent as the target\n+        # layout specified.\n+        self.assertTrue(\n+            result.sharding.is_equivalent_to(target_layout.sharding, ndim=2)\n+        )\n+\n+        # Test without jit.\n+        result = distribution_lib.distribute_tensor(inputs, target_layout)\n+        self.assertTrue(\n+            result.sharding.is_equivalent_to(target_layout.sharding, ndim=2)\n+        )\n+\n+    def test_distribute_variable_with_jax_layout(self):\n+        # This test only verify the single worker/process behavior.\n+        jax_mesh = jax.sharding.Mesh(\n+            np.array(jax.devices()).reshape(2, 4), (\"batch\", \"model\")\n+        )\n+\n+        variable = jax.numpy.array(np.random.normal(size=(16, 8)))\n+        target_layout = jax_layout.Layout(\n+            sharding=jax.sharding.NamedSharding(\n+                jax_mesh, jax.sharding.PartitionSpec(\"model\", None)\n+            )\n+        )\n+\n+        result = backend_dlib.distribute_variable(variable, target_layout)\n+        # Note that the returned tensor has a different sharding implementation\n+        # which is GSPMDSharding, but it should be equivalent as the target\n+        # layout specified.\n+        self.assertTrue(\n+            result.sharding.is_equivalent_to(target_layout.sharding, ndim=2)\n+        )\n+\n+    def test_distribute_input_data_with_jax_layout(self):\n+        # This test only verify the single worker/process behavior.\n+        jax_mesh = jax.sharding.Mesh(\n+            np.array(jax.devices()).reshape(2, 4), (\"batch\", \"model\")\n+        )\n+\n+        input_data = jax.numpy.array(np.random.normal(size=(16, 8)))\n+        target_layout = jax_layout.Layout(\n+            sharding=jax.sharding.NamedSharding(\n+                jax_mesh, jax.sharding.PartitionSpec(\"batch\", None)\n+            )\n+        )\n+\n+        result = backend_dlib.distribute_variable(input_data, target_layout)\n+        # Note that the returned tensor has a different sharding implementation\n+        # which is GSPMDSharding, but it should be equivalent as the target\n+        # layout specified.\n+        self.assertTrue(\n+            result.sharding.is_equivalent_to(target_layout.sharding, ndim=2)\n+        )\n+\n     def test_processes(self):\n         self.assertEqual(backend_dlib.process_id(), 0)\n         self.assertEqual(backend_dlib.num_processes(), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cbb3682cab33ae3f97e1a3b7da3e6cbeae07b83b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 25 | Churn Cumulative: 836 | Contributors (this commit): 16 | Commits (past 90d): 10 | Contributors (cumulative): 25 | DMM Complexity: 0.8181818181818182\n\nDIFF:\n@@ -51,6 +51,7 @@ class Variable:\n         value: The current value of the variable (NumPy array or tensor).\n         name: The name of the variable (string).\n         path: The path of the variable within the Keras model or layer (string).\n+        kwargs: Additional backend-specific keyword arguments.\n \n     Examples:\n \n@@ -98,7 +99,9 @@ class Variable:\n         aggregation=\"none\",\n         synchronization=\"auto\",\n         name=None,\n+        **kwargs,\n     ):\n+        del kwargs\n         name = name or auto_name(self.__class__.__name__)\n         if not isinstance(name, str) or \"/\" in name:\n             raise ValueError(\n\n@@ -20,20 +20,30 @@ IS_THREAD_SAFE = True\n \n \n class Variable(KerasVariable):\n+    def __init__(self, *args, layout=None, **kwargs):\n+        # Intercept layout parameter so that it is available\n+        # during initialization.\n+        self._layout = layout\n+        super().__init__(*args, **kwargs)\n+\n     def _initialize(self, value):\n         # Note that variable.shape is needed by distribution_lib\n         self._shape = self._validate_shape(value.shape)\n         # We can't import the keras/distribution/distribution_lib\n         # due to circular dependency.\n         distribution = global_state.get_global_attribute(\"distribution\")\n-        if distribution is not None:\n-            self._layout = distribution.get_variable_layout(self).backend_layout\n+        if self._layout is None and distribution is not None:\n+            tensor_layout = distribution.get_variable_layout(self)\n+            from keras.src.distribution import TensorLayout\n+\n+            if isinstance(tensor_layout, TensorLayout):\n+                self._layout = tensor_layout.backend_layout\n             else:\n-            self._layout = None\n+                self._layout = tensor_layout\n         self._direct_assign(value)\n \n     def _direct_assign(self, value):\n-        if getattr(self, \"_layout\", None) is not None:\n+        if self._layout is not None:\n             value = distribution_lib.distribute_variable(value, self._layout)\n         self._value = value\n \n\n@@ -244,6 +244,7 @@ class BaseOptimizer(KerasSaveable):\n         initializer=\"zeros\",\n         dtype=None,\n         aggregation=\"none\",\n+        layout=None,\n         name=None,\n     ):\n         \"\"\"Add a variable to the optimizer.\n@@ -261,6 +262,7 @@ class BaseOptimizer(KerasSaveable):\n                 the type of multi-replica aggregation to be used for this\n                 variable when writing custom data parallel training loops.\n                 Defaults to `\"none\"`.\n+            layout: Optional tensor layout.  Defaults to `None`.\n             name: String name of the variable. Useful for debugging purposes.\n \n         Returns:\n@@ -275,6 +277,7 @@ class BaseOptimizer(KerasSaveable):\n                 dtype=dtype,\n                 trainable=False,\n                 aggregation=aggregation,\n+                layout=layout,\n                 name=name,\n             )\n         self._track_variable(variable)\n@@ -319,6 +322,7 @@ class BaseOptimizer(KerasSaveable):\n             initializer=initializer,\n             dtype=reference_variable.dtype,\n             name=name,\n+            layout=getattr(reference_variable, \"_layout\", None),\n         )\n \n     def add_optimizer_variables(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24d226b4d5ef2d3b508b3fde94d91d1e902c622e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 0 | Files Changed: 11 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 12/4 | Churn Δ: 87 | Churn Cumulative: 9760 | Contributors (this commit): 46 | Commits (past 90d): 104 | Contributors (cumulative): 141 | DMM Complexity: 1.0\n\nDIFF:\n@@ -185,6 +185,7 @@ from keras.src.ops.numpy import full_like as full_like\n from keras.src.ops.numpy import get_item as get_item\n from keras.src.ops.numpy import greater as greater\n from keras.src.ops.numpy import greater_equal as greater_equal\n+from keras.src.ops.numpy import hamming as hamming\n from keras.src.ops.numpy import histogram as histogram\n from keras.src.ops.numpy import hstack as hstack\n from keras.src.ops.numpy import identity as identity\n\n@@ -75,6 +75,7 @@ from keras.src.ops.numpy import full_like as full_like\n from keras.src.ops.numpy import get_item as get_item\n from keras.src.ops.numpy import greater as greater\n from keras.src.ops.numpy import greater_equal as greater_equal\n+from keras.src.ops.numpy import hamming as hamming\n from keras.src.ops.numpy import histogram as histogram\n from keras.src.ops.numpy import hstack as hstack\n from keras.src.ops.numpy import identity as identity\n\n@@ -185,6 +185,7 @@ from keras.src.ops.numpy import full_like as full_like\n from keras.src.ops.numpy import get_item as get_item\n from keras.src.ops.numpy import greater as greater\n from keras.src.ops.numpy import greater_equal as greater_equal\n+from keras.src.ops.numpy import hamming as hamming\n from keras.src.ops.numpy import histogram as histogram\n from keras.src.ops.numpy import hstack as hstack\n from keras.src.ops.numpy import identity as identity\n\n@@ -75,6 +75,7 @@ from keras.src.ops.numpy import full_like as full_like\n from keras.src.ops.numpy import get_item as get_item\n from keras.src.ops.numpy import greater as greater\n from keras.src.ops.numpy import greater_equal as greater_equal\n+from keras.src.ops.numpy import hamming as hamming\n from keras.src.ops.numpy import histogram as histogram\n from keras.src.ops.numpy import hstack as hstack\n from keras.src.ops.numpy import identity as identity\n\n@@ -42,6 +42,11 @@ def bartlett(x):\n     return jnp.bartlett(x)\n \n \n+def hamming(x):\n+    x = convert_to_tensor(x)\n+    return jnp.hamming(x)\n+\n+\n def bincount(x, weights=None, minlength=0, sparse=False):\n     # Note: bincount is never tracable / jittable because the output shape\n     # depends on the values in x.\n\n@@ -310,6 +310,11 @@ def bartlett(x):\n     return np.bartlett(x).astype(config.floatx())\n \n \n+def hamming(x):\n+    x = convert_to_tensor(x)\n+    return np.hamming(x).astype(config.floatx())\n+\n+\n def bincount(x, weights=None, minlength=0, sparse=False):\n     if sparse:\n         raise ValueError(\"Unsupported value `sparse=True` with numpy backend\")\n\n@@ -475,6 +475,12 @@ def bartlett(x):\n     )\n \n \n+def hamming(x):\n+    raise NotImplementedError(\n+        \"`hamming` is not supported with openvino backend\"\n+    )\n+\n+\n def bincount(x, weights=None, minlength=0, sparse=False):\n     if x is None:\n         raise ValueError(\"input x is None\")\n\n@@ -146,6 +146,11 @@ def bartlett(x):\n     return window\n \n \n+def hamming(x):\n+    x = convert_to_tensor(x, dtype=tf.int32)\n+    return tf.signal.hamming_window(x, periodic=False)\n+\n+\n def bincount(x, weights=None, minlength=0, sparse=False):\n     x = convert_to_tensor(x)\n     dtypes_to_resolve = [x.dtype]\n\n@@ -435,6 +435,11 @@ def bartlett(x):\n     return torch.signal.windows.bartlett(x)\n \n \n+def hamming(x):\n+    x = convert_to_tensor(x)\n+    return torch.signal.windows.hamming(x)\n+\n+\n def bincount(x, weights=None, minlength=0, sparse=False):\n     if sparse:\n         raise ValueError(\"Unsupported value `sparse=True` with torch backend\")\n\n@@ -1239,6 +1239,37 @@ def bartlett(x):\n     return backend.numpy.bartlett(x)\n \n \n+class Hamming(Operation):\n+    def call(self, x):\n+        return backend.numpy.hamming(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=backend.floatx())\n+\n+\n+@keras_export([\"keras.ops.hamming\", \"keras.ops.numpy.hamming\"])\n+def hamming(x):\n+    \"\"\"Hamming window function.\n+\n+    The Hamming window is defined as:\n+    `w[n] = 0.54 - 0.46 * cos(2 * pi * n / (N - 1))` for `0 <= n <= N - 1`.\n+\n+    Args:\n+        x: Scalar or 1D Tensor. The window length.\n+\n+    Returns:\n+        A 1D tensor containing the Hamming window values.\n+\n+    Example:\n+    >>> x = keras.ops.convert_to_tensor(5)\n+    >>> keras.ops.hamming(x)\n+    array([0.08, 0.54, 1.  , 0.54, 0.08], dtype=float32)\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Hamming().symbolic_call(x)\n+    return backend.numpy.hamming(x)\n+\n+\n class Bincount(Operation):\n     def __init__(self, weights=None, minlength=0, sparse=False):\n         super().__init__()\n\n@@ -1228,6 +1228,10 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = np.random.randint(1, 100 + 1)\n         self.assertEqual(knp.blackman(x).shape[0], x)\n \n+    def test_hamming(self):\n+        x = np.random.randint(1, 100 + 1)\n+        self.assertEqual(knp.hamming(x).shape[0], x)\n+\n     def test_bitwise_invert(self):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.bitwise_invert(x).shape, (None, 3))\n@@ -3610,6 +3614,12 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n \n         self.assertAllClose(knp.Blackman()(x), np.blackman(x))\n \n+    def test_hamming(self):\n+        x = np.random.randint(1, 100 + 1)\n+        self.assertAllClose(knp.hamming(x), np.hamming(x))\n+\n+        self.assertAllClose(knp.Hamming()(x), np.hamming(x))\n+\n     @parameterized.named_parameters(\n         named_product(sparse_input=(False, True), sparse_arg=(False, True))\n     )\n@@ -5605,6 +5615,22 @@ class NumpyDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_hamming(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((), dtype=dtype)\n+        x_jax = jnp.ones((), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.hamming(x_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.hamming(x).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.Hamming().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=INT_DTYPES))\n     def test_bincount(self, dtype):\n         import jax.numpy as jnp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aeac0119b5a2401a57268a163269e66065f3779f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1752 | Contributors (this commit): 21 | Commits (past 90d): 30 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1252,9 +1252,10 @@ def real(x):\n \n \n def reciprocal(x):\n-    raise NotImplementedError(\n-        \"`reciprocal` is not supported with openvino backend\"\n-    )\n+    x = get_ov_output(x)\n+    one_constant = ov_opset.constant(1, dtype=x.get_element_type()).output(0)\n+    x = ov_opset.divide(one_constant, x).output(0)\n+    return OpenVINOKerasTensor(x)\n \n \n def repeat(x, repeats, axis=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3000d813efb160a326b498e8ace636c47e62fd74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 36 | Churn Cumulative: 259 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,8 +59,14 @@ class RandomGrayscale(BaseImagePreprocessingLayer):\n     def get_random_transformation(self, images, training=True, seed=None):\n         if seed is None:\n             seed = self._get_seed_generator(self.backend._backend)\n+        # Base case: Unbatched data\n+        batch_size = 1\n+        if len(images.shape) == 4:\n+            # This is a batch of images (4D input)\n+            batch_size = self.backend.core.shape(images)[0]\n+\n         random_values = self.backend.random.uniform(\n-            shape=(self.backend.core.shape(images)[0],),\n+            shape=(batch_size,),\n             minval=0,\n             maxval=1,\n             seed=seed,\n\n@@ -78,17 +78,37 @@ class RandomGrayscaleTest(testing.TestCase):\n \n     def test_grayscale_with_single_color_image(self):\n         test_cases = [\n+            # batched inputs\n             (np.full((1, 4, 4, 3), 128, dtype=np.float32), \"channels_last\"),\n             (np.full((1, 3, 4, 4), 128, dtype=np.float32), \"channels_first\"),\n+            # unbatched inputs\n+            (np.full((4, 4, 3), 128, dtype=np.float32), \"channels_last\"),\n+            (np.full((3, 4, 4), 128, dtype=np.float32), \"channels_first\"),\n         ]\n \n         for xs, data_format in test_cases:\n             layer = layers.RandomGrayscale(factor=1.0, data_format=data_format)\n             transformed = ops.convert_to_numpy(layer(xs))\n \n-            if data_format == \"channels_last\":\n-                unique_vals = np.unique(transformed[0, :, :, 0])\n-                self.assertEqual(len(unique_vals), 1)\n+            # Determine if the input was batched\n+            is_batched = len(xs.shape) == 4\n+\n+            # If batched, select the first image from the batch for inspection.\n+            # Otherwise, use the transformed image directly.\n+            # `image_to_inspect` will always be a 3D tensor.\n+            if is_batched:\n+                image_to_inspect = transformed[0]\n             else:\n-                unique_vals = np.unique(transformed[0, 0, :, :])\n+                image_to_inspect = transformed\n+\n+            if data_format == \"channels_last\":\n+                # image_to_inspect has shape (H, W, C),\n+                # get the first channel [:, :, 0]\n+                channel_data = image_to_inspect[:, :, 0]\n+            else:  # data_format == \"channels_first\"\n+                # image_to_inspect has shape (C, H, W),\n+                # get the first channel [0, :, :]\n+                channel_data = image_to_inspect[0, :, :]\n+\n+            unique_vals = np.unique(channel_data)\n             self.assertEqual(len(unique_vals), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
