{"custom_id": "scrapy#6fc78270427c41e401a01a46551d27dd4ddf846c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -15/0 | Churn Δ: 5 | Churn Cumulative: 7734 | Contributors (this commit): 46 | Commits (past 90d): 5 | Contributors (cumulative): 49 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,7 +42,6 @@ class GzipPlugin:\n \n     def close(self) -> None:\n         self.gzipfile.close()\n-        self.file.close()\n \n \n class Bz2Plugin:\n@@ -69,7 +68,6 @@ class Bz2Plugin:\n \n     def close(self) -> None:\n         self.bz2file.close()\n-        self.file.close()\n \n \n class LZMAPlugin:\n@@ -111,7 +109,6 @@ class LZMAPlugin:\n \n     def close(self) -> None:\n         self.lzmafile.close()\n-        self.file.close()\n \n \n # io.IOBase is subclassed here, so that exporters can use the PostProcessingManager\n\n@@ -1731,6 +1731,7 @@ class FeedExportTest(FeedExportTestBase):\n \n             def store(self, file):\n                 Storage.store_file = file\n+                Storage.file_was_closed = file.closed\n                 file.close()\n \n         settings = {\n@@ -1746,6 +1747,7 @@ class FeedExportTest(FeedExportTestBase):\n         }\n         yield self.exported_no_data(settings)\n         self.assertIs(Storage.open_file, Storage.store_file)\n+        self.assertFalse(Storage.file_was_closed)\n \n \n class FeedPostProcessedExportsTest(FeedExportTestBase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f19045403a44011a62162d73fedacf5038af098f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 34 | Churn Cumulative: 1850 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,7 +2,6 @@ import pickle\n import re\n import unittest\n from typing import Optional\n-from unittest.mock import Mock\n \n from packaging.version import Version\n from pytest import mark\n@@ -854,34 +853,13 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         )\n \n     def test_link_allowed_is_false_with_empty_url(self):\n-        mock_link = Mock()\n-        mock_link.url = \"\"\n-        expected = False\n-\n-        actual = LxmlLinkExtractor()._link_allowed(mock_link)\n-\n-        self.assertEqual(expected, actual)\n+        bad_link = Link(\"\")\n+        self.assertFalse(LxmlLinkExtractor()._link_allowed(bad_link))\n \n     def test_link_allowed_is_false_with_bad_url_prefix(self):\n-        mock_link = Mock()\n-        mock_link.url = \"htp://should_be_http.com\"\n-        expected = False\n-\n-        actual = LxmlLinkExtractor()._link_allowed(mock_link)\n-\n-        self.assertEqual(expected, actual)\n+        bad_link = Link(\"htp://should_be_http.example\")\n+        self.assertFalse(LxmlLinkExtractor()._link_allowed(bad_link))\n \n     def test_link_allowed_is_false_with_missing_url_prefix(self):\n-        mock_link = Mock()\n-        mock_link.url = \"should_have_prefix.com\"\n-        expected = False\n-\n-        actual = LxmlLinkExtractor()._link_allowed(mock_link)\n-\n-        self.assertEqual(expected, actual)\n-\n-    def test_link_allowed_raises_with_none_url(self):\n-        mock_link = Mock()\n-        mock_link.url = None\n-\n-        self.assertRaises(AttributeError, LxmlLinkExtractor()._link_allowed, mock_link)\n+        bad_link = Link(\"should_have_prefix.example\")\n+        self.assertFalse(LxmlLinkExtractor()._link_allowed(bad_link))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b7a7ae7dbbaddd9d14b50c1257370af51e1ac1b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 9 | Churn Cumulative: 2836 | Contributors (this commit): 31 | Commits (past 90d): 6 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1643,7 +1643,7 @@ class JsonRequestTest(RequestTest):\n             self.assertEqual(kwargs[\"allow_nan\"], True)\n \n     def test_replacement_both_body_and_data_warns(self):\n-        \"\"\"Test that we can get a warning if both body and data are passed for branch coverage\"\"\"\n+        \"\"\"Test that we get a warning if both body and data are passed\"\"\"\n         body1 = None\n         body2 = b\"body\"\n         data1 = {\n@@ -1654,12 +1654,11 @@ class JsonRequestTest(RequestTest):\n         }\n         r1 = self.request_class(url=\"http://www.example.com/\", data=data1, body=body1)\n \n-        with mock.patch(\"warnings.warn\") as mock_warn:\n+        with warnings.catch_warnings(record=True) as _warnings:\n             r1.replace(data=data2, body=body2)\n-            mock_warn.assert_called_once()\n-            (warning_message,), _ = mock_warn.call_args\n             self.assertIn(\n-                \"Both body and data passed. data will be ignored\", warning_message\n+                \"Both body and data passed. data will be ignored\",\n+                str(_warnings[0].message),\n             )\n \n     def tearDown(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#877398a3dee8e92300ef52177b986be16a55f277", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 2842 | Contributors (this commit): 31 | Commits (past 90d): 7 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1452,12 +1452,8 @@ class FormRequestTest(RequestTest):\n             </body></html>\"\"\"\n         )\n \n-        class CustomFormdata:\n-            def __iter__(self):\n-                raise ValueError(\"Custom iteration error for testing\")\n-\n         with self.assertRaises(ValueError) as context:\n-            FormRequest.from_response(response, formdata=CustomFormdata())\n+            FormRequest.from_response(response, formdata=(\"a\",))\n \n         self.assertIn(\n             \"formdata should be a dict or iterable of tuples\", str(context.exception)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c513e7d6e58efc99c5dfc8ad135dc23df528c5fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 52 | Churn Cumulative: 267 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,6 @@\n+from unittest.mock import Mock, call, patch\n+\n+from scrapy.commands.check import TextTestResult\n from tests.test_commands import CommandTest\n \n \n@@ -94,3 +97,52 @@ class CheckSpider(scrapy.Spider):\n             raise Exception('SCRAPY_CHECK not set')\n         \"\"\"\n         self._test_contract(parse_def=parse_def)\n+\n+    def test_printSummary_with_unsuccessful_test_result_without_errors_and_without_failures(\n+        self,\n+    ):\n+        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n+        start_time = 1.0\n+        stop_time = 2.0\n+        result.testsRun = 5\n+        result.failures = []\n+        result.errors = []\n+        result.unexpectedSuccesses = [\"a\", \"b\"]\n+        with patch.object(result.stream, \"write\") as mock_write:\n+            result.printSummary(start_time, stop_time)\n+            mock_write.assert_has_calls([call(\"FAILED\"), call(\"\\n\")])\n+\n+    def test_printSummary_with_unsuccessful_test_result_with_only_failures(self):\n+        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n+        start_time = 1.0\n+        stop_time = 2.0\n+        result.testsRun = 5\n+        result.failures = [(self, \"failure\")]\n+        result.errors = []\n+        with patch.object(result.stream, \"writeln\") as mock_write:\n+            result.printSummary(start_time, stop_time)\n+            mock_write.assert_called_with(\" (failures=1)\")\n+\n+    def test_printSummary_with_unsuccessful_test_result_with_only_errors(self):\n+        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n+        start_time = 1.0\n+        stop_time = 2.0\n+        result.testsRun = 5\n+        result.failures = []\n+        result.errors = [(self, \"error\")]\n+        with patch.object(result.stream, \"writeln\") as mock_write:\n+            result.printSummary(start_time, stop_time)\n+            mock_write.assert_called_with(\" (errors=1)\")\n+\n+    def test_printSummary_with_unsuccessful_test_result_with_both_failures_and_errors(\n+        self,\n+    ):\n+        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n+        start_time = 1.0\n+        stop_time = 2.0\n+        result.testsRun = 5\n+        result.failures = [(self, \"failure\")]\n+        result.errors = [(self, \"error\")]\n+        with patch.object(result.stream, \"writeln\") as mock_write:\n+            result.printSummary(start_time, stop_time)\n+            mock_write.assert_called_with(\" (failures=1, errors=1)\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e208f82076182e71a4eb8470eb527363158f4b0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 13/7 | Churn Δ: 92 | Churn Cumulative: 1467 | Contributors (this commit): 22 | Commits (past 90d): 4 | Contributors (cumulative): 35 | DMM Complexity: 0.9859154929577465\n\nDIFF:\n@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n import warnings\n+from itertools import chain\n from logging import getLogger\n from typing import TYPE_CHECKING, List, Optional, Union\n \n@@ -102,18 +103,18 @@ class HttpCompressionMiddleware:\n         if isinstance(response, Response):\n             content_encoding = response.headers.getlist(\"Content-Encoding\")\n             if content_encoding:\n-                encoding = content_encoding.pop()\n                 max_size = request.meta.get(\"download_maxsize\", self._max_size)\n                 warn_size = request.meta.get(\"download_warnsize\", self._warn_size)\n                 try:\n-                    decoded_body = self._decode(\n-                        response.body, encoding.lower(), max_size\n+                    decoded_body, content_encoding = self._handle_encoding(\n+                        response.body, content_encoding, max_size\n                     )\n                 except _DecompressionMaxSizeExceeded:\n                     raise IgnoreRequest(\n                         f\"Ignored response {response} because its body \"\n-                        f\"({len(response.body)} B) exceeded DOWNLOAD_MAXSIZE \"\n-                        f\"({max_size} B) during decompression.\"\n+                        f\"({len(response.body)} B compressed) exceeded \"\n+                        f\"DOWNLOAD_MAXSIZE ({max_size} B) during \"\n+                        f\"decompression.\"\n                     )\n                 if len(response.body) < warn_size <= len(decoded_body):\n                     logger.warning(\n@@ -121,6 +122,7 @@ class HttpCompressionMiddleware:\n                         f\"({len(decoded_body)} B) is larger than the \"\n                         f\"download warning size ({warn_size} B).\"\n                     )\n+                response.headers[\"Content-Encoding\"] = content_encoding\n                 if self.stats:\n                     self.stats.inc_value(\n                         \"httpcompression/response_bytes\",\n@@ -144,6 +146,28 @@ class HttpCompressionMiddleware:\n \n         return response\n \n+    def _handle_encoding(self, body, content_encoding, max_size):\n+        to_decode, to_keep = self._split_encodings(content_encoding)\n+        for encoding in to_decode:\n+            body = self._decode(body, encoding, max_size)\n+        return body, to_keep\n+\n+    def _split_encodings(self, content_encoding):\n+        to_keep = [\n+            encoding.strip().lower()\n+            for encoding in chain.from_iterable(\n+                encodings.split(b\",\") for encodings in content_encoding\n+            )\n+        ]\n+        to_decode = []\n+        while to_keep:\n+            encoding = to_keep.pop()\n+            if encoding not in ACCEPTED_ENCODINGS:\n+                to_keep.append(encoding)\n+                return to_decode, to_keep\n+            to_decode.append(encoding)\n+        return to_decode, to_keep\n+\n     def _decode(self, body: bytes, encoding: bytes, max_size: int) -> bytes:\n         if encoding == b\"gzip\" or encoding == b\"x-gzip\":\n             return gunzip(body, max_size=max_size)\n\n@@ -27,6 +27,8 @@ FORMAT = {\n     \"x-gzip\": (\"html-gzip.bin\", \"gzip\"),\n     \"rawdeflate\": (\"html-rawdeflate.bin\", \"deflate\"),\n     \"zlibdeflate\": (\"html-zlibdeflate.bin\", \"deflate\"),\n+    \"gzip-deflate\": (\"html-gzip-deflate.bin\", \"gzip, deflate\"),\n+    \"gzip-deflate-gzip\": (\"html-gzip-deflate-gzip.bin\", \"gzip, deflate, gzip\"),\n     \"br\": (\"html-br.bin\", \"br\"),\n     # $ zstd raw.html --content-size -o html-zstd-static-content-size.bin\n     \"zstd-static-content-size\": (\"html-zstd-static-content-size.bin\", \"zstd\"),\n@@ -205,6 +207,62 @@ class HttpCompressionTest(TestCase):\n         assert newresponse is not response\n         self.assertEqual(newresponse.headers.getlist(\"Content-Encoding\"), [b\"uuencode\"])\n \n+    def test_multi_compression_single_header(self):\n+        response = self._getresponse(\"gzip-deflate\")\n+        request = response.request\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        assert newresponse is not response\n+        assert \"Content-Encoding\" not in newresponse.headers\n+        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n+\n+    def test_multi_compression_single_header_invalid_compression(self):\n+        response = self._getresponse(\"gzip-deflate\")\n+        response.headers[\"Content-Encoding\"] = [b\"gzip, foo, deflate\"]\n+        request = response.request\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        assert newresponse is not response\n+        self.assertEqual(\n+            newresponse.headers.getlist(\"Content-Encoding\"), [b\"gzip\", b\"foo\"]\n+        )\n+\n+    def test_multi_compression_multiple_header(self):\n+        response = self._getresponse(\"gzip-deflate\")\n+        response.headers[\"Content-Encoding\"] = [\"gzip\", \"deflate\"]\n+        request = response.request\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        assert newresponse is not response\n+        assert \"Content-Encoding\" not in newresponse.headers\n+        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n+\n+    def test_multi_compression_multiple_header_invalid_compression(self):\n+        response = self._getresponse(\"gzip-deflate\")\n+        response.headers[\"Content-Encoding\"] = [\"gzip\", \"foo\", \"deflate\"]\n+        request = response.request\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        assert newresponse is not response\n+        self.assertEqual(\n+            newresponse.headers.getlist(\"Content-Encoding\"), [b\"gzip\", b\"foo\"]\n+        )\n+\n+    def test_multi_compression_single_and_multiple_header(self):\n+        response = self._getresponse(\"gzip-deflate-gzip\")\n+        response.headers[\"Content-Encoding\"] = [\"gzip\", \"deflate, gzip\"]\n+        request = response.request\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        assert newresponse is not response\n+        assert \"Content-Encoding\" not in newresponse.headers\n+        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n+\n+    def test_multi_compression_single_and_multiple_header_invalid_compression(self):\n+        response = self._getresponse(\"gzip-deflate\")\n+        response.headers[\"Content-Encoding\"] = [\"gzip\", \"foo,deflate\"]\n+        request = response.request\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        assert newresponse is not response\n+        self.assertEqual(\n+            newresponse.headers.getlist(\"Content-Encoding\"), [b\"gzip\", b\"foo\"]\n+        )\n+\n     def test_process_response_encoding_inside_body(self):\n         headers = {\n             \"Content-Type\": \"text/html\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0ef9a89a1a27caefa357b8f147a59b0959d58ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 33 | Files Changed: 8 | Hunks: 27 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 59 | Churn Cumulative: 9154 | Contributors (this commit): 51 | Commits (past 90d): 11 | Contributors (cumulative): 134 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,13 +6,12 @@ import platform\n import re\n import subprocess\n import sys\n-import tempfile\n from contextlib import contextmanager\n from itertools import chain\n from pathlib import Path\n from shutil import copytree, rmtree\n from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n-from tempfile import mkdtemp\n+from tempfile import TemporaryFile, mkdtemp\n from threading import Timer\n from typing import Dict, Generator, Optional, Union\n from unittest import skipIf\n@@ -82,7 +81,7 @@ class ProjectTest(unittest.TestCase):\n         rmtree(self.temp_path)\n \n     def call(self, *new_args, **kwargs):\n-        with tempfile.TemporaryFile() as out:\n+        with TemporaryFile() as out:\n             args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n             return subprocess.call(\n                 args, stdout=out, stderr=out, cwd=self.cwd, env=self.env, **kwargs\n\n@@ -2,8 +2,8 @@ import contextlib\n import os\n import shutil\n import sys\n-import tempfile\n from pathlib import Path\n+from tempfile import mkdtemp, mkstemp\n from typing import Optional, Type\n from unittest import SkipTest, mock\n \n@@ -107,13 +107,14 @@ class LoadTestCase(unittest.TestCase):\n class FileTestCase(unittest.TestCase):\n     def setUp(self):\n         # add a special char to check that they are handled correctly\n-        self.tmpname = Path(self.mktemp() + \"^\")\n+        self.fd, self.tmpname = mkstemp(suffix=\"^\")\n         Path(self.tmpname).write_text(\"0123456789\", encoding=\"utf-8\")\n         handler = build_from_crawler(FileDownloadHandler, get_crawler())\n         self.download_request = handler.download_request\n \n     def tearDown(self):\n-        self.tmpname.unlink()\n+        os.close(self.fd)\n+        os.remove(self.tmpname)\n \n     def test_download(self):\n         def _test(response):\n@@ -122,12 +123,12 @@ class FileTestCase(unittest.TestCase):\n             self.assertEqual(response.body, b\"0123456789\")\n             self.assertEqual(response.protocol, None)\n \n-        request = Request(path_to_file_uri(str(self.tmpname)))\n+        request = Request(path_to_file_uri(self.tmpname))\n         assert request.url.upper().endswith(\"%5E\")\n         return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n     def test_non_existent(self):\n-        request = Request(path_to_file_uri(self.mktemp()))\n+        request = Request(path_to_file_uri(mkdtemp()))\n         d = self.download_request(request, Spider(\"foo\"))\n         return self.assertFailure(d, OSError)\n \n@@ -224,8 +225,7 @@ class HttpTestCase(unittest.TestCase):\n     certfile = \"keys/localhost.crt\"\n \n     def setUp(self):\n-        self.tmpname = Path(self.mktemp())\n-        self.tmpname.mkdir()\n+        self.tmpname = Path(mkdtemp())\n         (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n         r = static.File(str(self.tmpname))\n         r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n@@ -651,8 +651,7 @@ class Https11CustomCiphers(unittest.TestCase):\n     certfile = \"keys/localhost.crt\"\n \n     def setUp(self):\n-        self.tmpname = Path(self.mktemp())\n-        self.tmpname.mkdir()\n+        self.tmpname = Path(mkdtemp())\n         (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n         r = static.File(str(self.tmpname))\n         self.site = server.Site(r, timeout=None)\n@@ -1015,8 +1014,7 @@ class BaseFTPTestCase(unittest.TestCase):\n         from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n \n         # setup dirs and test file\n-        self.directory = Path(self.mktemp())\n-        self.directory.mkdir()\n+        self.directory = Path(mkdtemp())\n         userdir = self.directory / self.username\n         userdir.mkdir()\n         for filename, content in self.test_files:\n@@ -1092,7 +1090,7 @@ class BaseFTPTestCase(unittest.TestCase):\n         return self._add_test_callbacks(d, _test)\n \n     def test_ftp_local_filename(self):\n-        f, local_fname = tempfile.mkstemp()\n+        f, local_fname = mkstemp()\n         fname_bytes = to_bytes(local_fname)\n         local_fname = Path(local_fname)\n         os.close(f)\n@@ -1113,7 +1111,7 @@ class BaseFTPTestCase(unittest.TestCase):\n         return self._add_test_callbacks(d, _test)\n \n     def _test_response_class(self, filename, response_class):\n-        f, local_fname = tempfile.mkstemp()\n+        f, local_fname = mkstemp()\n         local_fname = Path(local_fname)\n         os.close(f)\n         meta = {}\n@@ -1163,9 +1161,7 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n         from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n \n         # setup dir and test file\n-        self.directory = Path(self.mktemp())\n-        self.directory.mkdir()\n-\n+        self.directory = Path(mkdtemp())\n         for filename, content in self.test_files:\n             (self.directory / filename).write_bytes(content)\n \n\n@@ -5,6 +5,7 @@ import shutil\n import string\n from ipaddress import IPv4Address\n from pathlib import Path\n+from tempfile import mkdtemp\n from typing import Dict\n from unittest import mock, skipIf\n from urllib.parse import urlencode\n@@ -185,8 +186,7 @@ class Https2ClientProtocolTestCase(TestCase):\n     certificate_file = Path(__file__).parent / \"keys\" / \"localhost.crt\"\n \n     def _init_resource(self):\n-        self.temp_directory = self.mktemp()\n-        Path(self.temp_directory).mkdir()\n+        self.temp_directory = mkdtemp()\n         r = File(self.temp_directory)\n         r.putChild(b\"get-data-html-small\", GetDataHtmlSmall())\n         r.putChild(b\"get-data-html-large\", GetDataHtmlLarge())\n\n@@ -1,5 +1,6 @@\n import shutil\n from pathlib import Path\n+from tempfile import mkdtemp\n from typing import Optional, Set\n \n from testfixtures import LogCapture\n@@ -67,8 +68,7 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.mockserver.__enter__()\n \n         # prepare a directory for storing files\n-        self.tmpmediastore = Path(self.mktemp())\n-        self.tmpmediastore.mkdir()\n+        self.tmpmediastore = Path(mkdtemp())\n         self.settings = {\n             \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             \"ITEM_PIPELINES\": {self.pipeline_class: 1},\n\n@@ -3,6 +3,7 @@ import sys\n import tempfile\n import warnings\n from pathlib import Path\n+from tempfile import mkdtemp\n \n from twisted.trial import unittest\n from zope.interface.verify import verifyObject\n@@ -139,8 +140,7 @@ class SpiderLoaderTest(unittest.TestCase):\n class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n     def setUp(self):\n         orig_spiders_dir = module_dir / \"test_spiders\"\n-        self.tmpdir = Path(self.mktemp())\n-        self.tmpdir.mkdir()\n+        self.tmpdir = Path(mkdtemp())\n         self.spiders_dir = self.tmpdir / \"test_spiders_xxx\"\n         _copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(str(self.tmpdir))\n\n@@ -1,6 +1,6 @@\n import shutil\n from datetime import datetime, timezone\n-from pathlib import Path\n+from tempfile import mkdtemp\n \n from twisted.trial import unittest\n \n@@ -12,8 +12,7 @@ from scrapy.utils.test import get_crawler\n \n class SpiderStateTest(unittest.TestCase):\n     def test_store_load(self):\n-        jobdir = self.mktemp()\n-        Path(jobdir).mkdir()\n+        jobdir = mkdtemp()\n         try:\n             spider = Spider(name=\"default\")\n             dt = datetime.now(tz=timezone.utc)\n\n@@ -25,7 +25,7 @@ class BaseQueueTestCase(unittest.TestCase):\n     def setUp(self):\n         self.tmpdir = tempfile.mkdtemp(prefix=\"scrapy-queue-tests-\")\n         self.qpath = self.tempfilename()\n-        self.qdir = self.mkdtemp()\n+        self.qdir = tempfile.mkdtemp()\n         self.crawler = get_crawler(Spider)\n \n     def tearDown(self):\n\n@@ -4,6 +4,7 @@ Tests borrowed from the twisted.web.client tests.\n \"\"\"\n import shutil\n from pathlib import Path\n+from tempfile import mkdtemp\n \n import OpenSSL.SSL\n from twisted.internet import defer, reactor\n@@ -274,8 +275,7 @@ class WebClientTestCase(unittest.TestCase):\n         return reactor.listenTCP(0, site, interface=\"127.0.0.1\")\n \n     def setUp(self):\n-        self.tmpname = Path(self.mktemp())\n-        self.tmpname.mkdir()\n+        self.tmpname = Path(mkdtemp())\n         (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n         r = static.File(str(self.tmpname))\n         r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n@@ -440,8 +440,7 @@ class WebClientSSLTestCase(unittest.TestCase):\n         return f\"https://127.0.0.1:{self.portno}/{path}\"\n \n     def setUp(self):\n-        self.tmpname = Path(self.mktemp())\n-        self.tmpname.mkdir()\n+        self.tmpname = Path(mkdtemp())\n         (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n         r = static.File(str(self.tmpname))\n         r.putChild(b\"payload\", PayloadResource())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2d46b4acf5855faaf2d6baa36615f08bd8aefccf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 345 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 13 | Complexity Δ (Sum/Max): 23/22 | Churn Δ: 345 | Churn Cumulative: 424 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,6 +16,11 @@ class AutoThrottle:\n         self.target_concurrency = crawler.settings.getfloat(\n             \"AUTOTHROTTLE_TARGET_CONCURRENCY\"\n         )\n+        if self.target_concurrency <= 0.0:\n+            raise NotConfigured(\n+                f\"AUTOTHROTTLE_TARGET_CONCURRENCY \"\n+                f\"({self.target_concurrency!r}) must be higher than 0.\"\n+            )\n         crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(\n             self._response_downloaded, signal=signals.response_downloaded\n\n@@ -0,0 +1,340 @@\n+from logging import INFO\n+from unittest.mock import Mock\n+\n+import pytest\n+\n+from scrapy import Request, Spider\n+from scrapy.exceptions import NotConfigured\n+from scrapy.extensions.throttle import AutoThrottle\n+from scrapy.http.response import Response\n+from scrapy.settings.default_settings import (\n+    AUTOTHROTTLE_MAX_DELAY,\n+    AUTOTHROTTLE_START_DELAY,\n+    DOWNLOAD_DELAY,\n+)\n+from scrapy.utils.misc import build_from_crawler\n+from scrapy.utils.test import get_crawler as _get_crawler\n+\n+UNSET = object()\n+\n+\n+class TestSpider(Spider):\n+    name = \"test\"\n+\n+\n+def get_crawler(settings=None, spidercls=None):\n+    settings = settings or {}\n+    settings[\"AUTOTHROTTLE_ENABLED\"] = True\n+    return _get_crawler(settings_dict=settings, spidercls=spidercls)\n+\n+\n+@pytest.mark.parametrize(\n+    (\"value\", \"expected\"),\n+    (\n+        (UNSET, False),\n+        (False, False),\n+        (True, True),\n+    ),\n+)\n+def test_enabled(value, expected):\n+    settings = {}\n+    if value is not UNSET:\n+        settings[\"AUTOTHROTTLE_ENABLED\"] = value\n+    crawler = _get_crawler(settings_dict=settings)\n+    if expected:\n+        build_from_crawler(AutoThrottle, crawler)\n+    else:\n+        with pytest.raises(NotConfigured):\n+            build_from_crawler(AutoThrottle, crawler)\n+\n+\n+@pytest.mark.parametrize(\n+    \"value\",\n+    (\n+        0.0,\n+        -1.0,\n+    ),\n+)\n+def test_target_concurrency_invalid(value):\n+    settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": value}\n+    crawler = get_crawler(settings)\n+    with pytest.raises(NotConfigured):\n+        build_from_crawler(AutoThrottle, crawler)\n+\n+\n+@pytest.mark.parametrize(\n+    (\"spider\", \"setting\", \"expected\"),\n+    (\n+        (UNSET, UNSET, DOWNLOAD_DELAY),\n+        (1.0, UNSET, 1.0),\n+        (UNSET, 1.0, 1.0),\n+        (1.0, 2.0, 1.0),\n+        (3.0, 2.0, 3.0),\n+    ),\n+)\n+def test_mindelay_definition(spider, setting, expected):\n+    settings = {}\n+    if setting is not UNSET:\n+        settings[\"DOWNLOAD_DELAY\"] = setting\n+\n+    class _TestSpider(Spider):\n+        name = \"test\"\n+\n+    if spider is not UNSET:\n+        _TestSpider.download_delay = spider\n+\n+    crawler = get_crawler(settings, _TestSpider)\n+    at = build_from_crawler(AutoThrottle, crawler)\n+    at._spider_opened(_TestSpider())\n+    assert at.mindelay == expected\n+\n+\n+@pytest.mark.parametrize(\n+    (\"value\", \"expected\"),\n+    (\n+        (UNSET, AUTOTHROTTLE_MAX_DELAY),\n+        (1.0, 1.0),\n+    ),\n+)\n+def test_maxdelay_definition(value, expected):\n+    settings = {}\n+    if value is not UNSET:\n+        settings[\"AUTOTHROTTLE_MAX_DELAY\"] = value\n+    crawler = get_crawler(settings)\n+    at = build_from_crawler(AutoThrottle, crawler)\n+    at._spider_opened(TestSpider())\n+    assert at.maxdelay == expected\n+\n+\n+@pytest.mark.parametrize(\n+    (\"min_spider\", \"min_setting\", \"start_setting\", \"expected\"),\n+    (\n+        (UNSET, UNSET, UNSET, AUTOTHROTTLE_START_DELAY),\n+        (AUTOTHROTTLE_START_DELAY - 1.0, UNSET, UNSET, AUTOTHROTTLE_START_DELAY),\n+        (AUTOTHROTTLE_START_DELAY + 1.0, UNSET, UNSET, AUTOTHROTTLE_START_DELAY + 1.0),\n+        (UNSET, AUTOTHROTTLE_START_DELAY - 1.0, UNSET, AUTOTHROTTLE_START_DELAY),\n+        (UNSET, AUTOTHROTTLE_START_DELAY + 1.0, UNSET, AUTOTHROTTLE_START_DELAY + 1.0),\n+        (UNSET, UNSET, AUTOTHROTTLE_START_DELAY - 1.0, AUTOTHROTTLE_START_DELAY - 1.0),\n+        (UNSET, UNSET, AUTOTHROTTLE_START_DELAY + 1.0, AUTOTHROTTLE_START_DELAY + 1.0),\n+        (\n+            AUTOTHROTTLE_START_DELAY + 1.0,\n+            AUTOTHROTTLE_START_DELAY + 2.0,\n+            UNSET,\n+            AUTOTHROTTLE_START_DELAY + 1.0,\n+        ),\n+        (\n+            AUTOTHROTTLE_START_DELAY + 2.0,\n+            UNSET,\n+            AUTOTHROTTLE_START_DELAY + 1.0,\n+            AUTOTHROTTLE_START_DELAY + 2.0,\n+        ),\n+        (\n+            AUTOTHROTTLE_START_DELAY + 1.0,\n+            UNSET,\n+            AUTOTHROTTLE_START_DELAY + 2.0,\n+            AUTOTHROTTLE_START_DELAY + 2.0,\n+        ),\n+    ),\n+)\n+def test_startdelay_definition(min_spider, min_setting, start_setting, expected):\n+    settings = {}\n+    if min_setting is not UNSET:\n+        settings[\"DOWNLOAD_DELAY\"] = min_setting\n+    if start_setting is not UNSET:\n+        settings[\"AUTOTHROTTLE_START_DELAY\"] = start_setting\n+\n+    class _TestSpider(Spider):\n+        name = \"test\"\n+\n+    if min_spider is not UNSET:\n+        _TestSpider.download_delay = min_spider\n+\n+    crawler = get_crawler(settings, _TestSpider)\n+    at = build_from_crawler(AutoThrottle, crawler)\n+    spider = _TestSpider()\n+    at._spider_opened(spider)\n+    assert spider.download_delay == expected\n+\n+\n+@pytest.mark.parametrize(\n+    (\"meta\", \"slot\"),\n+    (\n+        ({}, None),\n+        ({\"download_latency\": 1.0}, None),\n+        ({\"download_slot\": \"foo\"}, None),\n+        ({\"download_slot\": \"foo\"}, \"foo\"),\n+        ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, None),\n+    ),\n+)\n+def test_skipped(meta, slot):\n+    crawler = get_crawler()\n+    at = build_from_crawler(AutoThrottle, crawler)\n+    spider = TestSpider()\n+    at._spider_opened(spider)\n+    request = Request(\"https://example.com\", meta=meta)\n+\n+    crawler.engine = Mock()\n+    crawler.engine.downloader = Mock()\n+    crawler.engine.downloader.slots = {}\n+    if slot is not None:\n+        crawler.engine.downloader.slots[slot] = object()\n+    at._adjust_delay = None  # Raise exception if called.\n+\n+    at._response_downloaded(None, request, spider)\n+\n+\n+@pytest.mark.parametrize(\n+    (\"download_latency\", \"target_concurrency\", \"slot_delay\", \"expected\"),\n+    (\n+        (2.0, 2.0, 1.0, 1.0),\n+        (1.0, 2.0, 1.0, 0.75),\n+        (4.0, 2.0, 1.0, 2.0),\n+        (2.0, 1.0, 1.0, 2.0),\n+        (2.0, 4.0, 1.0, 0.75),\n+        (2.0, 2.0, 0.5, 1.0),\n+        (2.0, 2.0, 2.0, 1.5),\n+    ),\n+)\n+def test_adjustment(download_latency, target_concurrency, slot_delay, expected):\n+    settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency}\n+    crawler = get_crawler(settings)\n+    at = build_from_crawler(AutoThrottle, crawler)\n+    spider = TestSpider()\n+    at._spider_opened(spider)\n+    meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n+    request = Request(\"https://example.com\", meta=meta)\n+    response = Response(request.url)\n+\n+    crawler.engine = Mock()\n+    crawler.engine.downloader = Mock()\n+    crawler.engine.downloader.slots = {}\n+    slot = Mock()\n+    slot.delay = slot_delay\n+    crawler.engine.downloader.slots[\"foo\"] = slot\n+\n+    at._response_downloaded(response, request, spider)\n+\n+    assert slot.delay == expected, f\"{slot.delay} != {expected}\"\n+\n+\n+@pytest.mark.parametrize(\n+    (\"mindelay\", \"maxdelay\", \"expected\"),\n+    (\n+        (0.5, 2.0, 1.0),\n+        (0.25, 0.5, 0.5),\n+        (2.0, 4.0, 2.0),\n+    ),\n+)\n+def test_adjustment_limits(mindelay, maxdelay, expected):\n+    download_latency, target_concurrency, slot_delay = (2.0, 2.0, 1.0)\n+    # expected adjustment without limits with these values: 1.0\n+    settings = {\n+        \"AUTOTHROTTLE_MAX_DELAY\": maxdelay,\n+        \"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency,\n+        \"DOWNLOAD_DELAY\": mindelay,\n+    }\n+    crawler = get_crawler(settings)\n+    at = build_from_crawler(AutoThrottle, crawler)\n+    spider = TestSpider()\n+    at._spider_opened(spider)\n+    meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n+    request = Request(\"https://example.com\", meta=meta)\n+    response = Response(request.url)\n+\n+    crawler.engine = Mock()\n+    crawler.engine.downloader = Mock()\n+    crawler.engine.downloader.slots = {}\n+    slot = Mock()\n+    slot.delay = slot_delay\n+    crawler.engine.downloader.slots[\"foo\"] = slot\n+\n+    at._response_downloaded(response, request, spider)\n+\n+    assert slot.delay == expected, f\"{slot.delay} != {expected}\"\n+\n+\n+@pytest.mark.parametrize(\n+    (\"download_latency\", \"target_concurrency\", \"slot_delay\", \"expected\"),\n+    (\n+        (2.0, 2.0, 1.0, 1.0),\n+        (1.0, 2.0, 1.0, 1.0),  # Instead of 0.75\n+        (4.0, 2.0, 1.0, 2.0),\n+    ),\n+)\n+def test_adjustment_bad_response(\n+    download_latency, target_concurrency, slot_delay, expected\n+):\n+    settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency}\n+    crawler = get_crawler(settings)\n+    at = build_from_crawler(AutoThrottle, crawler)\n+    spider = TestSpider()\n+    at._spider_opened(spider)\n+    meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n+    request = Request(\"https://example.com\", meta=meta)\n+    response = Response(request.url, status=400)\n+\n+    crawler.engine = Mock()\n+    crawler.engine.downloader = Mock()\n+    crawler.engine.downloader.slots = {}\n+    slot = Mock()\n+    slot.delay = slot_delay\n+    crawler.engine.downloader.slots[\"foo\"] = slot\n+\n+    at._response_downloaded(response, request, spider)\n+\n+    assert slot.delay == expected, f\"{slot.delay} != {expected}\"\n+\n+\n+def test_debug(caplog):\n+    settings = {\"AUTOTHROTTLE_DEBUG\": True}\n+    crawler = get_crawler(settings)\n+    at = build_from_crawler(AutoThrottle, crawler)\n+    spider = TestSpider()\n+    at._spider_opened(spider)\n+    meta = {\"download_latency\": 1.0, \"download_slot\": \"foo\"}\n+    request = Request(\"https://example.com\", meta=meta)\n+    response = Response(request.url, body=b\"foo\")\n+\n+    crawler.engine = Mock()\n+    crawler.engine.downloader = Mock()\n+    crawler.engine.downloader.slots = {}\n+    slot = Mock()\n+    slot.delay = 2.0\n+    slot.transferring = (None, None)\n+    crawler.engine.downloader.slots[\"foo\"] = slot\n+\n+    caplog.clear()\n+    with caplog.at_level(INFO):\n+        at._response_downloaded(response, request, spider)\n+\n+    assert caplog.record_tuples == [\n+        (\n+            \"scrapy.extensions.throttle\",\n+            INFO,\n+            \"slot: foo | conc: 2 | delay: 1500 ms (-500) | latency: 1000 ms | size:     3 bytes\",\n+        ),\n+    ]\n+\n+\n+def test_debug_disabled(caplog):\n+    crawler = get_crawler()\n+    at = build_from_crawler(AutoThrottle, crawler)\n+    spider = TestSpider()\n+    at._spider_opened(spider)\n+    meta = {\"download_latency\": 1.0, \"download_slot\": \"foo\"}\n+    request = Request(\"https://example.com\", meta=meta)\n+    response = Response(request.url, body=b\"foo\")\n+\n+    crawler.engine = Mock()\n+    crawler.engine.downloader = Mock()\n+    crawler.engine.downloader.slots = {}\n+    slot = Mock()\n+    slot.delay = 2.0\n+    slot.transferring = (None, None)\n+    crawler.engine.downloader.slots[\"foo\"] = slot\n+\n+    caplog.clear()\n+    with caplog.at_level(INFO):\n+        at._response_downloaded(response, request, spider)\n+\n+    assert caplog.record_tuples == []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d87f949526470fc4847c99f58e1dcc40d4e9ed00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 900 | Contributors (this commit): 39 | Commits (past 90d): 3 | Contributors (cumulative): 46 | DMM Complexity: None\n\nDIFF:\n@@ -7,9 +7,13 @@ See documentation in docs/topics/request-response.rst\n import xmlrpc.client as xmlrpclib\n from typing import Any, Optional\n \n+from defusedxml import xmlrpc\n+\n from scrapy.http.request import Request\n from scrapy.utils.python import get_func_args\n \n+xmlrpc.monkey_patch()\n+\n DUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n \n \n\n@@ -22,6 +22,7 @@ install_requires = [\n     \"packaging\",\n     \"tldextract\",\n     \"lxml>=4.4.1\",\n+    \"defusedxml>=0.7.1\",\n ]\n extras_require = {\n     ':platform_python_implementation == \"CPython\"': [\"PyDispatcher>=2.0.5\"],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f945ad6db728234987629b2299750abee5c1781", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 63 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -7,12 +7,12 @@ See documentation in docs/topics/request-response.rst\n import xmlrpc.client as xmlrpclib\n from typing import Any, Optional\n \n-from defusedxml import xmlrpc\n+import defusedxml.xmlrpc as xml_rpc\n \n from scrapy.http.request import Request\n from scrapy.utils.python import get_func_args\n \n-xmlrpc.monkey_patch()\n+xml_rpc.monkey_patch()\n \n DUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#008ebb65fc2f0e7cfe9fa43d7ac938a94b3098fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 67 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -7,12 +7,12 @@ See documentation in docs/topics/request-response.rst\n import xmlrpc.client as xmlrpclib\n from typing import Any, Optional\n \n-import defusedxml.xmlrpc as xml_rpc\n+import defusedxml.xmlrpc\n \n from scrapy.http.request import Request\n from scrapy.utils.python import get_func_args\n \n-xml_rpc.monkey_patch()\n+defusedxml.xmlrpc.monkey_patch()\n \n DUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#282767f23b2e71969bea3bd5492abde88d2054c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 53 | Files Changed: 56 | Hunks: 73 | Methods Changed: 20 | Complexity Δ (Sum/Max): -23/0 | Churn Δ: 149 | Churn Cumulative: 33929 | Contributors (this commit): 200 | Commits (past 90d): 93 | Contributors (cumulative): 907 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n \"\"\"\n Base class for Scrapy commands\n \"\"\"\n+\n import argparse\n import os\n from pathlib import Path\n\n@@ -3,6 +3,7 @@ Scrapy Shell\n \n See documentation in docs/topics/shell.rst\n \"\"\"\n+\n from argparse import Namespace\n from threading import Thread\n from typing import List, Type\n\n@@ -21,9 +21,9 @@ logger = logging.getLogger(__name__)\n class DownloadHandlers:\n     def __init__(self, crawler: \"Crawler\"):\n         self._crawler: \"Crawler\" = crawler\n-        self._schemes: Dict[\n-            str, Union[str, Callable]\n-        ] = {}  # stores acceptable schemes on instancing\n+        self._schemes: Dict[str, Union[str, Callable]] = (\n+            {}\n+        )  # stores acceptable schemes on instancing\n         self._handlers: Dict[str, Any] = {}  # stores instanced handlers for schemes\n         self._notconfigured: Dict[str, str] = {}  # remembers failed handlers\n         handlers: Dict[str, Union[str, Callable]] = without_none_values(\n\n@@ -1,5 +1,6 @@\n \"\"\"Download handlers for http and https schemes\n \"\"\"\n+\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import to_unicode\n \n\n@@ -3,6 +3,7 @@ Downloader Middleware manager\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n+\n from typing import Any, Callable, Generator, List, Union, cast\n \n from twisted.internet.defer import Deferred, inlineCallbacks\n\n@@ -4,6 +4,7 @@ This is the Scrapy engine which controls the Scheduler, Downloader and Spider.\n For more information see docs/topics/architecture.rst\n \n \"\"\"\n+\n import logging\n from time import time\n from typing import (\n\n@@ -111,17 +111,17 @@ class Stream:\n         # Metadata of an HTTP/2 connection stream\n         # initialized when stream is instantiated\n         self.metadata: Dict = {\n-            \"request_content_length\": 0\n-            if self._request.body is None\n-            else len(self._request.body),\n+            \"request_content_length\": (\n+                0 if self._request.body is None else len(self._request.body)\n+            ),\n             # Flag to keep track whether the stream has initiated the request\n             \"request_sent\": False,\n             # Flag to track whether we have logged about exceeding download warnsize\n             \"reached_warnsize\": False,\n             # Each time we send a data frame, we will decrease value by the amount send.\n-            \"remaining_content_length\": 0\n-            if self._request.body is None\n-            else len(self._request.body),\n+            \"remaining_content_length\": (\n+                0 if self._request.body is None else len(self._request.body)\n+            ),\n             # Flag to keep track whether client (self) have closed this stream\n             \"stream_closed_local\": False,\n             # Flag to keep track whether the server has closed the stream\n\n@@ -1,5 +1,6 @@\n \"\"\"This module implements the Scraper component which parses responses and\n extracts information from them\"\"\"\n+\n from __future__ import annotations\n \n import logging\n\n@@ -3,6 +3,7 @@ Spider Middleware manager\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+\n import logging\n from inspect import isasyncgenfunction, iscoroutine\n from itertools import islice\n\n@@ -3,6 +3,7 @@ DefaultHeaders downloader middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n+\n from __future__ import annotations\n \n from typing import TYPE_CHECKING, Iterable, Tuple, Union\n\n@@ -3,6 +3,7 @@ Download timeout middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n+\n from __future__ import annotations\n \n from typing import TYPE_CHECKING, Union\n\n@@ -9,6 +9,7 @@ RETRY_HTTP_CODES - which HTTP response codes to retry\n Failed pages are collected on the scraping process and rescheduled at the end,\n once the spider has finished crawling all regular (non failed) pages.\n \"\"\"\n+\n from __future__ import annotations\n \n import warnings\n\n@@ -4,6 +4,7 @@ Scrapy core exceptions\n These exceptions are documented in docs/topics/exceptions.rst. Please don't add\n new exceptions here without documenting them there.\n \"\"\"\n+\n from typing import Any\n \n # Internal\n\n@@ -3,6 +3,7 @@ The Extension Manager\n \n See documentation in docs/topics/extensions.rst\n \"\"\"\n+\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.conf import build_component_list\n \n\n@@ -1,6 +1,7 @@\n \"\"\"\n Extension for collecting core stats like items scraped and start/finish times\n \"\"\"\n+\n from datetime import datetime, timezone\n \n from scrapy import signals\n\n@@ -3,6 +3,7 @@ MemoryUsage extension\n \n See documentation in docs/topics/extensions.rst\n \"\"\"\n+\n import logging\n import socket\n import sys\n\n@@ -1,6 +1,7 @@\n \"\"\"\n Extension for processing data before they are exported to feeds.\n \"\"\"\n+\n from bz2 import BZ2File\n from gzip import GzipFile\n from io import IOBase\n\n@@ -4,6 +4,7 @@ requests in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n+\n import inspect\n from typing import (\n     Any,\n@@ -231,12 +232,16 @@ class Request(object_ref):\n         \"\"\"\n         d = {\n             \"url\": self.url,  # urls are safe (safe_string_url)\n-            \"callback\": _find_method(spider, self.callback)\n+            \"callback\": (\n+                _find_method(spider, self.callback)\n                 if callable(self.callback)\n-            else self.callback,\n-            \"errback\": _find_method(spider, self.errback)\n+                else self.callback\n+            ),\n+            \"errback\": (\n+                _find_method(spider, self.errback)\n                 if callable(self.errback)\n-            else self.errback,\n+                else self.errback\n+            ),\n             \"headers\": dict(self.headers),\n         }\n         for attr in self.attributes:\n\n@@ -4,6 +4,7 @@ This module implements the XmlRpcRequest class which is a more convenient class\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n+\n import xmlrpc.client as xmlrpclib\n from typing import Any, Optional\n \n\n@@ -4,6 +4,7 @@ responses in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n+\n from __future__ import annotations\n \n from ipaddress import IPv4Address, IPv6Address\n\n@@ -4,6 +4,7 @@ discovering (through HTTP headers) to base Response class.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n+\n from __future__ import annotations\n \n import json\n\n@@ -4,6 +4,7 @@ This module defines the Link object used in Link extractors.\n For actual link extractors implementation see scrapy.linkextractors, or\n its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n+\n from typing import Any\n \n \n\n@@ -5,6 +5,7 @@ This package contains a collection of Link Extractors.\n \n For more info see docs/topics/link-extractors.rst\n \"\"\"\n+\n import re\n \n # common file extensions that are not followed if they occur in links\n\n@@ -1,6 +1,7 @@\n \"\"\"\n Link extractor based on lxml.html\n \"\"\"\n+\n import logging\n import operator\n from functools import partial\n\n@@ -3,6 +3,7 @@ Item Loader\n \n See documentation in docs/topics/loaders.rst\n \"\"\"\n+\n import itemloaders\n \n from scrapy.item import Item\n\n@@ -3,6 +3,7 @@ Mail sending helpers\n \n See documentation in docs/topics/email.rst\n \"\"\"\n+\n import logging\n from email import encoders as Encoders\n from email.mime.base import MIMEBase\n\n@@ -3,6 +3,7 @@ Item pipeline\n \n See documentation in docs/item-pipeline.rst\n \"\"\"\n+\n from typing import Any, List\n \n from twisted.internet.defer import Deferred\n\n@@ -3,6 +3,7 @@ Files Pipeline\n \n See documentation in topics/media-pipeline.rst\n \"\"\"\n+\n import base64\n import functools\n import hashlib\n\n@@ -3,6 +3,7 @@ Images Pipeline\n \n See documentation in topics/media-pipeline.rst\n \"\"\"\n+\n import functools\n import hashlib\n import warnings\n\n@@ -2,6 +2,7 @@\n This module implements a class which returns the appropriate Response class\n based on different criteria.\n \"\"\"\n+\n from io import StringIO\n from mimetypes import MimeTypes\n from pkgutil import get_data\n\n@@ -1,6 +1,7 @@\n \"\"\"\n XPath selectors based on lxml\n \"\"\"\n+\n from typing import Any, Optional, Type, Union\n \n from parsel import Selector as _ParselSelector\n\n@@ -58,7 +58,6 @@ def get_settings_priority(priority: Union[int, str]) -> int:\n \n \n class SettingsAttribute:\n-\n     \"\"\"Class for storing data related to settings attributes.\n \n     This class is intended for internal usage, you should try Settings class\n\n@@ -3,6 +3,7 @@\n See documentation in docs/topics/shell.rst\n \n \"\"\"\n+\n import os\n import signal\n \n\n@@ -3,6 +3,7 @@ HttpError Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+\n from __future__ import annotations\n \n import logging\n\n@@ -3,6 +3,7 @@ Offsite Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+\n from __future__ import annotations\n \n import logging\n\n@@ -2,6 +2,7 @@\n RefererMiddleware: populates Request referer field, based on the Response which\n originated it.\n \"\"\"\n+\n from __future__ import annotations\n \n import warnings\n\n@@ -3,6 +3,7 @@ Base class for Scrapy spiders\n \n See documentation in docs/topics/spiders.rst\n \"\"\"\n+\n from __future__ import annotations\n \n import logging\n\n@@ -4,6 +4,7 @@ for scraping from an XML feed.\n \n See documentation in docs/topics/spiders.rst\n \"\"\"\n+\n from scrapy.exceptions import NotConfigured, NotSupported\n from scrapy.selector import Selector\n from scrapy.spiders import Spider\n\n@@ -1,6 +1,7 @@\n \"\"\"\n Scrapy extension for collecting scraping stats\n \"\"\"\n+\n import logging\n import pprint\n from typing import TYPE_CHECKING, Any, Dict, Optional\n\n@@ -1,6 +1,7 @@\n \"\"\"\n Helper functions for dealing with Twisted deferreds\n \"\"\"\n+\n import asyncio\n import inspect\n from asyncio import Future\n@@ -304,13 +305,11 @@ _T = TypeVar(\"_T\")\n \n \n @overload\n-def deferred_from_coro(o: _CT) -> Deferred:\n-    ...\n+def deferred_from_coro(o: _CT) -> Deferred: ...\n \n \n @overload\n-def deferred_from_coro(o: _T) -> _T:\n-    ...\n+def deferred_from_coro(o: _T) -> _T: ...\n \n \n def deferred_from_coro(o: _T) -> Union[Deferred, _T]:\n\n@@ -138,13 +138,11 @@ DEPRECATION_RULES: List[Tuple[str, str]] = []\n \n \n @overload\n-def update_classpath(path: str) -> str:\n-    ...\n+def update_classpath(path: str) -> str: ...\n \n \n @overload\n-def update_classpath(path: Any) -> Any:\n-    ...\n+def update_classpath(path: Any) -> Any: ...\n \n \n def update_classpath(path: Any) -> Any:\n\n@@ -225,18 +225,17 @@ def csviter(\n \n \n @overload\n-def _body_or_str(obj: Union[Response, str, bytes]) -> str:\n-    ...\n+def _body_or_str(obj: Union[Response, str, bytes]) -> str: ...\n \n \n @overload\n-def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[True]) -> str:\n-    ...\n+def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[True]) -> str: ...\n \n \n @overload\n-def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[False]) -> bytes:\n-    ...\n+def _body_or_str(\n+    obj: Union[Response, str, bytes], unicode: Literal[False]\n+) -> bytes: ...\n \n \n def _body_or_str(\n\n@@ -1,4 +1,5 @@\n \"\"\"Helper functions which don't fit anywhere else\"\"\"\n+\n import ast\n import hashlib\n import inspect\n\n@@ -1,6 +1,7 @@\n \"\"\"\n This module contains essential stuff that should've come with Python itself ;)\n \"\"\"\n+\n import collections.abc\n import gc\n import inspect\n@@ -285,13 +286,11 @@ def equal_attributes(\n \n \n @overload\n-def without_none_values(iterable: Mapping) -> dict:\n-    ...\n+def without_none_values(iterable: Mapping) -> dict: ...\n \n \n @overload\n-def without_none_values(iterable: Iterable) -> Iterable:\n-    ...\n+def without_none_values(iterable: Iterable) -> Iterable: ...\n \n \n def without_none_values(iterable: Union[Mapping, Iterable]) -> Union[dict, Iterable]:\n\n@@ -44,7 +44,9 @@ def _serialize_headers(\n             yield from request.headers.getlist(header)\n \n \n-_fingerprint_cache: \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]\"\n+_fingerprint_cache: (\n+    \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]\"\n+)\n _fingerprint_cache = WeakKeyDictionary()\n \n \n@@ -114,8 +116,7 @@ def fingerprint(\n \n \n class RequestFingerprinterProtocol(Protocol):\n-    def fingerprint(self, request: Request) -> bytes:\n-        ...\n+    def fingerprint(self, request: Request) -> bytes: ...\n \n \n class RequestFingerprinter:\n\n@@ -2,6 +2,7 @@\n This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n+\n import os\n import re\n import tempfile\n@@ -29,9 +30,9 @@ def get_base_url(response: \"scrapy.http.response.text.TextResponse\") -> str:\n     return _baseurl_cache[response]\n \n \n-_metaref_cache: \"WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]\" = (\n-    WeakKeyDictionary()\n-)\n+_metaref_cache: (\n+    \"WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]\"\n+) = WeakKeyDictionary()\n \n \n def get_meta_refresh(\n\n@@ -1,4 +1,5 @@\n \"\"\"Helper functions for working with signals\"\"\"\n+\n import collections.abc\n import logging\n from typing import Any as TypingAny\n\n@@ -4,6 +4,7 @@ Module for processing Sitemaps.\n Note: The main purpose of this module is to provide support for the\n SitemapSpider, its API is subject to change without notice.\n \"\"\"\n+\n from typing import Any, Dict, Generator, Iterator, Optional\n from urllib.parse import urljoin\n \n\n@@ -39,13 +39,11 @@ def iterate_spider_output(result: AsyncGenerator) -> AsyncGenerator:  # type: ig\n \n \n @overload\n-def iterate_spider_output(result: CoroutineType) -> Deferred:\n-    ...\n+def iterate_spider_output(result: CoroutineType) -> Deferred: ...\n \n \n @overload\n-def iterate_spider_output(result: _T) -> Iterable:\n-    ...\n+def iterate_spider_output(result: _T) -> Iterable: ...\n \n \n def iterate_spider_output(result: Any) -> Union[Iterable, AsyncGenerator, Deferred]:\n@@ -83,8 +81,7 @@ def spidercls_for_request(\n     default_spidercls: Type[Spider],\n     log_none: bool = ...,\n     log_multiple: bool = ...,\n-) -> Type[Spider]:\n-    ...\n+) -> Type[Spider]: ...\n \n \n @overload\n@@ -94,8 +91,7 @@ def spidercls_for_request(\n     default_spidercls: Literal[None],\n     log_none: bool = ...,\n     log_multiple: bool = ...,\n-) -> Optional[Type[Spider]]:\n-    ...\n+) -> Optional[Type[Spider]]: ...\n \n \n @overload\n@@ -105,8 +101,7 @@ def spidercls_for_request(\n     *,\n     log_none: bool = ...,\n     log_multiple: bool = ...,\n-) -> Optional[Type[Spider]]:\n-    ...\n+) -> Optional[Type[Spider]]: ...\n \n \n def spidercls_for_request(\n\n@@ -5,6 +5,7 @@ library.\n Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n+\n import re\n from typing import TYPE_CHECKING, Iterable, Optional, Type, Union, cast\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n\n@@ -1,4 +1,5 @@\n \"\"\"DBM-like dummy module\"\"\"\n+\n import collections\n from typing import Any, DefaultDict\n \n\n@@ -1,6 +1,7 @@\n \"\"\"\n Some spiders used for testing and benchmarking\n \"\"\"\n+\n import asyncio\n import time\n from urllib.parse import urlencode\n\n@@ -121,7 +121,9 @@ class BaseItemExporterTest(unittest.TestCase):\n         self.assertEqual(name, \"John\\xa3\")\n \n         ie = self._get_exporter(fields_to_export={\"name\": \"名稱\"})\n-        self.assertEqual(list(ie._get_serialized_fields(self.i)), [(\"名稱\", \"John\\xa3\")])\n+        self.assertEqual(\n+            list(ie._get_serialized_fields(self.i)), [(\"名稱\", \"John\\xa3\")]\n+        )\n \n     def test_field_custom_serializer(self):\n         i = self.custom_field_item_class(name=\"John\\xa3\", age=\"22\")\n\n@@ -22,9 +22,9 @@ from scrapy.utils.test import get_crawler\n try:\n     from PIL import Image  # noqa: imported just to check for the import error\n except ImportError:\n-    skip_pillow: Optional[\n-        str\n-    ] = \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n+    skip_pillow: Optional[str] = (\n+        \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n+    )\n else:\n     skip_pillow = None\n \n\n@@ -33,7 +33,10 @@ class ResponseTypesTest(unittest.TestCase):\n             (\"attachment;filename=dataµ.tar.gz\".encode(\"latin-1\"), Response),\n             (\"attachment;filename=data高.doc\".encode(\"gbk\"), Response),\n             (\"attachment;filename=دورهdata.html\".encode(\"cp720\"), HtmlResponse),\n-            (\"attachment;filename=日本語版Wikipedia.xml\".encode(\"iso2022_jp\"), XmlResponse),\n+            (\n+                \"attachment;filename=日本語版Wikipedia.xml\".encode(\"iso2022_jp\"),\n+                XmlResponse,\n+            ),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_disposition(source)\n\n@@ -2,6 +2,7 @@\n from twisted.internet import defer\n Tests borrowed from the twisted.web.client tests.\n \"\"\"\n+\n import shutil\n from pathlib import Path\n from tempfile import mkdtemp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
