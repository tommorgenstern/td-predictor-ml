{"custom_id": "keras#10c76237abf9913dfcba9a0ac73e8db50ea21a44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 31 | Files Changed: 5 | Hunks: 25 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 67 | Churn Cumulative: 14035 | Contributors (this commit): 67 | Commits (past 90d): 56 | Contributors (cumulative): 80 | DMM Complexity: 0.0\n\nDIFF:\n@@ -398,8 +398,10 @@ class ConvLSTM2D(ConvRecurrent2D):\n         initial_state = K.zeros_like(inputs)\n         # (samples, rows, cols, filters)\n         initial_state = K.sum(initial_state, axis=1)\n+        shape = list(self.kernel_shape)\n+        shape[-1] = self.filters\n         initial_state = self.input_conv(initial_state,\n-                                        K.zeros(self.kernel_shape),\n+                                        K.zeros(tuple(shape)),\n                                         padding=self.padding)\n \n         initial_states = [initial_state for _ in range(2)]\n@@ -408,7 +410,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n     def reset_states(self):\n         if not self.stateful:\n             raise RuntimeError('Layer must be stateful.')\n-        input_shape = self.input_spec[0].shape\n+        input_shape = self.input_spec.shape\n         output_shape = self.compute_output_shape(input_shape)\n         if not input_shape[0]:\n             raise ValueError('If a RNN is stateful, a complete ' +\n@@ -470,15 +472,14 @@ class ConvLSTM2D(ConvRecurrent2D):\n                             padding=padding,\n                             data_format=self.data_format,\n                             dilation_rate=self.dilation_rate)\n-        if self.use_bias:\n-            conv_out = K.bias_add(conv_out,\n-                                  self.bias,\n+        if b is not None:\n+            conv_out = K.bias_add(conv_out, b,\n                                   data_format=self.data_format)\n         return conv_out\n \n-    def reccurent_conv(self, x, w, padding='valid'):\n+    def reccurent_conv(self, x, w):\n         conv_out = K.conv2d(x, w, strides=(1, 1),\n-                            padding=padding,\n+                            padding='same',\n                             data_format=self.data_format)\n         return conv_out\n \n@@ -498,22 +499,18 @@ class ConvLSTM2D(ConvRecurrent2D):\n         x_o = self.input_conv(inputs * dp_mask[3], self.kernel_o, self.bias_o,\n                               padding=self.padding)\n         h_i = self.reccurent_conv(h_tm1 * rec_dp_mask[0],\n-                                  self.recurrent_kernel_i,\n-                                  padding='same')\n+                                  self.recurrent_kernel_i)\n         h_f = self.reccurent_conv(h_tm1 * rec_dp_mask[1],\n-                                  self.recurrent_kernel_f,\n-                                  padding='same')\n+                                  self.recurrent_kernel_f)\n         h_c = self.reccurent_conv(h_tm1 * rec_dp_mask[2],\n-                                  self.recurrent_kernel_c,\n-                                  padding='same')\n+                                  self.recurrent_kernel_c)\n         h_o = self.reccurent_conv(h_tm1 * rec_dp_mask[3],\n-                                  self.recurrent_kernel_o,\n-                                  padding='same')\n+                                  self.recurrent_kernel_o)\n \n-        i = self.inner_activation(x_i + h_i)\n-        f = self.inner_activation(x_f + h_f)\n+        i = self.recurrent_activation(x_i + h_i)\n+        f = self.recurrent_activation(x_f + h_f)\n         c = f * c_tm1 + i * self.activation(x_c + h_c)\n-        o = self.inner_activation(x_o + h_o)\n+        o = self.recurrent_activation(x_o + h_o)\n         h = o * self.activation(c)\n         return h, [h, c]\n \n\n@@ -18,6 +18,7 @@ from ..engine import InputSpec\n from ..engine import Layer\n from ..utils.generic_utils import func_dump\n from ..utils.generic_utils import func_load\n+from ..utils.generic_utils import deserialize_keras_object\n \n \n class Masking(Layer):\n@@ -678,25 +679,31 @@ class Lambda(Layer):\n \n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n-        # Insert custom objects into globals.\n-        if custom_objects:\n-            globs = globals().copy()\n-            globs.update(custom_objects)\n-        else:\n         globs = globals()\n-\n+        if custom_objects:\n+            globs = dict(globs.items() + custom_objects.items())\n         function_type = config.pop('function_type')\n         if function_type == 'function':\n-            function = get_from_module(config['function'], globs, 'core')\n+            # Simple lookup in custom objects\n+            function = deserialize_keras_object(\n+                config['function'],\n+                custom_objects=custom_objects,\n+                printable_module_name='function in Lambda layer')\n         elif function_type == 'lambda':\n+            # Unsafe deserialization from bytecode\n             function = func_load(config['function'], globs=globs)\n         else:\n             raise TypeError('Unknown function type:', function_type)\n \n         output_shape_type = config.pop('output_shape_type')\n         if output_shape_type == 'function':\n-            output_shape = get_from_module(config['output_shape'], globs, 'core')\n+            # Simple lookup in custom objects\n+            output_shape = deserialize_keras_object(\n+                config['output_shape'],\n+                custom_objects=custom_objects,\n+                printable_module_name='output_shape function in Lambda layer')\n         elif output_shape_type == 'lambda':\n+            # Unsafe deserialization from bytecode\n             output_shape = func_load(config['output_shape'], globs=globs)\n         else:\n             output_shape = config['output_shape']\n\n@@ -147,7 +147,7 @@ def deserialize_keras_object(identifier, module_objects=None,\n             fn = module_objects.get(function_name)\n             if fn is None:\n                 raise ValueError('Unknown ' + printable_module_name,\n-                                 ':', class_name)\n+                                 ':' + class_name)\n         return fn\n     else:\n         raise ValueError('Could not interpret serialized ' +\n\n@@ -105,6 +105,7 @@ def test_lambda():\n     config = ld.get_config()\n     ld = layer_from_config({'class_name': 'Lambda', 'config': config})\n \n+    # test with lambda\n     ld = layers.Lambda(\n         lambda x: K.concatenate([K.square(x), x]),\n         output_shape=lambda s: tuple(list(s)[:-1] + [2 * s[-1]]))\n\n@@ -9,7 +9,7 @@ from keras.utils.test_utils import layer_test\n from keras import regularizers\n \n \n-def test_recurrent_convolutional():\n+def test_convolutional_recurrent():\n     num_row = 3\n     num_col = 3\n     filters = 5\n@@ -24,13 +24,13 @@ def test_recurrent_convolutional():\n             inputs = np.random.rand(num_samples, sequence_len,\n                                     input_channel,\n                                     input_num_row, input_num_col)\n-        else:  # tf\n+        else:\n             inputs = np.random.rand(num_samples, sequence_len,\n                                     input_num_row, input_num_col,\n                                     input_channel)\n \n         for return_sequences in [True, False]:\n-            # test for ouptput shape:\n+            # test for output shape:\n             output = layer_test(convolutional_recurrent.ConvLSTM2D,\n                                 kwargs={'data_format': data_format,\n                                         'return_sequences': return_sequences,\n@@ -51,7 +51,7 @@ def test_recurrent_convolutional():\n \n             assert output.shape == tuple(output_shape)\n \n-            # No need to check statefulness for both\n+            # No need to check following tests for both data formats\n             if data_format == 'channels_first' or return_sequences:\n                 continue\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#45a10bc6d708fade197a37bfbc62312caf70e6a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 39 | Churn Cumulative: 1186 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -438,10 +438,9 @@ class ConvLSTM2D(ConvRecurrent2D):\n     def get_constants(self, inputs, training=None):\n         constants = []\n         if self.implementation == 0 and 0 < self.dropout < 1:\n-            input_shape = K.int_shape(inputs)\n-            input_dim = input_shape[-1]\n-            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, int(input_dim)))\n+            ones = K.zeros_like(inputs)\n+            ones = K.sum(ones, axis=1)\n+            ones += 1\n \n             def dropped_inputs():\n                 return K.dropout(ones, self.dropout)\n@@ -454,8 +453,13 @@ class ConvLSTM2D(ConvRecurrent2D):\n             constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n \n         if 0 < self.recurrent_dropout < 1:\n-            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, self.filters))\n+            shape = list(self.kernel_shape)\n+            shape[-1] = self.filters\n+            ones = K.zeros_like(inputs)\n+            ones = K.sum(ones, axis=1)\n+            ones = self.input_conv(ones, K.zeros(shape),\n+                                   padding=self.padding)\n+            ones += 1.\n \n             def dropped_inputs():\n                 return K.dropout(ones, self.recurrent_dropout)\n\n@@ -39,18 +39,6 @@ def test_convolutional_recurrent():\n                                         'padding': 'valid'},\n                                 input_shape=inputs.shape)\n \n-            output_shape = [num_samples, input_num_row, input_num_col]\n-\n-            if data_format == 'channels_first':\n-                output_shape.insert(1, filters)\n-            else:\n-                output_shape.insert(3, filters)\n-\n-            if return_sequences:\n-                output_shape.insert(1, sequence_len)\n-\n-            assert output.shape == tuple(output_shape)\n-\n             # No need to check following tests for both data formats\n             if data_format == 'channels_first' or return_sequences:\n                 continue\n@@ -69,11 +57,10 @@ def test_convolutional_recurrent():\n             model.add(layer)\n             model.compile(optimizer='sgd', loss='mse')\n             out1 = model.predict(np.ones_like(inputs))\n-            assert(out1.shape == tuple(output_shape))\n \n             # train once so that the states change\n             model.train_on_batch(np.ones_like(inputs),\n-                                 np.ones_like(output))\n+                                 np.random.random(out1.shape))\n             out2 = model.predict(np.ones_like(inputs))\n \n             # if the state is not reset, output should be different\n@@ -99,10 +86,14 @@ def test_convolutional_recurrent():\n                       'return_sequences': return_sequences,\n                       'kernel_size': (num_row, num_col),\n                       'stateful': True,\n+                      'filters': filters,\n                       'batch_input_shape': inputs.shape,\n-                      'kernel_regularizer': regularizers.WeightRegularizer(l1=0.01),\n-                      'recurrent_regularizer': regularizers.WeightRegularizer(l1=0.01),\n+                      'kernel_regularizer': regularizers.L1L2(l1=0.01),\n+                      'recurrent_regularizer': regularizers.L1L2(l1=0.01),\n                       'bias_regularizer': 'l2',\n+                      'kernel_constraint': 'max_norm',\n+                      'recurrent_constraint': 'max_norm',\n+                      'bias_constraint': 'max_norm',\n                       'padding': 'same'}\n \n             layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1a4aea4be03dd6902e50458b0a3c291bfdcf00e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 355 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -64,7 +64,7 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n     # instantiation\n     layer = layer_cls(**kwargs)\n \n-    # test get_weights , set_weights\n+    # test get_weights , set_weights at layer level\n     weights = layer.get_weights()\n     layer.set_weights(weights)\n \n@@ -83,8 +83,6 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n \n     # check shape inference\n     model = Model(x, y)\n-    model.compile('rmsprop', 'mse')\n-\n     expected_output_shape = layer.compute_output_shape(input_shape)\n     actual_output = model.predict(input_data)\n     actual_output_shape = actual_output.shape\n@@ -104,6 +102,10 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n         _output = recovered_model.predict(input_data)\n         assert_allclose(_output, actual_output, rtol=1e-3)\n \n+    # test training mode (e.g. useful for dropout tests)\n+    model.compile('rmsprop', 'mse')\n+    model.train_on_batch(input_data, actual_output)\n+\n     # test as first layer in Sequential API\n     layer_config = layer.get_config()\n     layer_config['batch_input_shape'] = input_shape\n@@ -111,7 +113,6 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n \n     model = Sequential()\n     model.add(layer)\n-    model.compile('rmsprop', 'mse')\n     actual_output = model.predict(input_data)\n     actual_output_shape = actual_output.shape\n     for expected_dim, actual_dim in zip(expected_output_shape,\n@@ -121,9 +122,18 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n     if expected_output is not None:\n         assert_allclose(actual_output, expected_output, rtol=1e-3)\n \n-    # test JSON serialization\n-    json_model = model.to_json()\n-    model = model_from_json(json_model)\n+    # test serialization, weight setting at model level\n+    model_config = model.get_config()\n+    recovered_model = Sequential.from_config(model_config)\n+    if model.weights:\n+        weights = model.get_weights()\n+        recovered_model.set_weights(weights)\n+        _output = recovered_model.predict(input_data)\n+        assert_allclose(_output, actual_output, rtol=1e-3)\n+\n+    # test training mode (e.g. useful for dropout tests)\n+    model.compile('rmsprop', 'mse')\n+    model.train_on_batch(input_data, actual_output)\n \n     # for further checks in the caller function\n     return actual_output\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#82f9e2358c35c5bbff8714280174617ba29cf92b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 239 | Lines Deleted: 526 | Files Changed: 14 | Hunks: 119 | Methods Changed: 69 | Complexity Δ (Sum/Max): 11/9 | Churn Δ: 765 | Churn Cumulative: 45461 | Contributors (this commit): 183 | Commits (past 90d): 233 | Contributors (cumulative): 325 | DMM Complexity: 0.0\n\nDIFF:\n@@ -236,6 +236,9 @@ def to_dense(tensor):\n         return tensor\n \n \n+name_scope = tf.name_scope\n+\n+\n def variable(value, dtype=None, name=None):\n     \"\"\"Instantiates a variable and returns it.\n \n@@ -275,7 +278,6 @@ def variable(value, dtype=None, name=None):\n             v = tf.SparseTensor(indices=indices,\n                                 values=sparse_coo.data,\n                                 shape=sparse_coo.shape)\n-        v._dims = len(sparse_coo.shape)\n         v._keras_shape = sparse_coo.shape\n         v._uses_learning_phase = False\n         return v\n@@ -344,8 +346,7 @@ def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n         if ndim:\n             shape = tuple([None for _ in range(ndim)])\n     if sparse:\n-        x = tf.sparse_placeholder(dtype, name=name)\n-        x._dims = len(shape)\n+        x = tf.sparse_placeholder(dtype, shape=shape, name=name)\n     else:\n         x = tf.placeholder(dtype, shape=shape, name=name)\n     x._keras_shape = shape\n@@ -406,8 +407,13 @@ def int_shape(x):\n         (2, 2)\n     ```\n     \"\"\"\n+    if hasattr(x, '_keras_shape'):\n+        return x._keras_shape\n     shape = x.get_shape()\n+    try:\n         return tuple([i.__int__() for i in shape])\n+    except ValueError:\n+        return None\n \n \n def ndim(x):\n@@ -431,9 +437,6 @@ def ndim(x):\n         2\n     ```\n     \"\"\"\n-    if is_sparse(x):\n-        return x._dims\n-\n     dims = x.get_shape()._dims\n     if dims is not None:\n         return len(dims)\n\n@@ -1,4 +1,5 @@\n from collections import defaultdict\n+from contextlib import contextmanager\n import theano\n from theano import tensor as T\n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n@@ -84,7 +85,7 @@ def to_dense(tensor):\n         return tensor\n \n \n-def is_explicit_shape(shape):\n+def _is_explicit_shape(shape):\n     if hasattr(shape, '__iter__'):\n         for x in shape:\n             if x is not None:\n@@ -94,6 +95,24 @@ def is_explicit_shape(shape):\n     return False\n \n \n+NAME_SCOPE_STACK = []\n+\n+\n+@contextmanager\n+def name_scope(name):\n+    global NAME_SCOPE_STACK\n+    NAME_SCOPE_STACK.append(name)\n+    yield\n+    NAME_SCOPE_STACK.pop()\n+\n+\n+def _prepare_name(name, default):\n+    prefix = '/'.join(NAME_SCOPE_STACK)\n+    if name is None:\n+        return prefix + '/' + default\n+    return prefix + '/' + name\n+\n+\n def variable(value, dtype=None, name=None):\n     \"\"\"Instantiates a variable and returns it.\n \n@@ -109,7 +128,8 @@ def variable(value, dtype=None, name=None):\n         dtype = floatx()\n     if hasattr(value, 'tocoo'):\n         _assert_sparse_module()\n-        variable = th_sparse_module.as_sparse_variable(value)\n+        variable = th_sparse_module.as_sparse_variable(\n+            value, name=_prepare_name(name, 'variable'))\n     else:\n         if isinstance(value, (theano.tensor.TensorVariable,\n                               theano.tensor.sharedvar.TensorSharedVariable,\n@@ -117,7 +137,9 @@ def variable(value, dtype=None, name=None):\n             # Support for RandomStreams().normal(), .uniform().\n             value = value.eval()\n         value = np.asarray(value, dtype=dtype)\n-        variable = theano.shared(value=value, name=name, strict=False)\n+        variable = theano.shared(value=value,\n+                                 name=_prepare_name(name, 'variable'),\n+                                 strict=False)\n     variable._keras_shape = value.shape\n     variable._uses_learning_phase = False\n     return variable\n@@ -129,7 +151,9 @@ def constant(value, dtype=None, shape=None, name=None):\n     if shape is None:\n         shape = ()\n     np_value = value * np.ones(shape)\n-    const = T.constant(np_value, dtype=dtype, name=name)\n+    const = T.constant(np_value,\n+                       dtype=dtype,\n+                       name=_prepare_name(name, 'constant'))\n     const._keras_shape = shape\n     const._uses_learning_phase = False\n     return const\n@@ -147,6 +171,7 @@ def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n     else:\n         shape = tuple([None for _ in range(ndim)])\n \n+    name = _prepare_name(name, 'placeholder')\n     broadcast = (False,) * ndim\n     if sparse:\n         _assert_sparse_module()\n@@ -630,7 +655,7 @@ def concatenate(tensors, axis=-1):\n \n def reshape(x, shape):\n     y = T.reshape(x, shape)\n-    if is_explicit_shape(shape):\n+    if _is_explicit_shape(shape):\n         y._keras_shape = shape\n         if hasattr(x, '_uses_learning_phase'):\n             y._uses_learning_phase = x._uses_learning_phase\n\n@@ -138,6 +138,9 @@ class CallbackList(object):\n         for callback in self.callbacks:\n             callback.on_train_end(logs)\n \n+    def __iter__(self):\n+        return iter(self.callbacks)\n+\n \n class Callback(object):\n     \"\"\"Abstract base class used to build new callbacks.\n@@ -167,7 +170,7 @@ class Callback(object):\n     \"\"\"\n \n     def __init__(self):\n-        pass\n+        self.validation_data = None\n \n     def set_params(self, params):\n         self.params = params\n@@ -638,16 +641,16 @@ class TensorBoard(Callback):\n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}\n \n-        if self.model.validation_data and self.histogram_freq:\n+        if self.validation_data and self.histogram_freq:\n             if epoch % self.histogram_freq == 0:\n                 # TODO: implement batched calls to sess.run\n                 # (current call will likely go OOM on GPU)\n                 if self.model.uses_learning_phase:\n                     cut_v_data = len(self.model.inputs)\n-                    val_data = self.model.validation_data[:cut_v_data] + [0]\n+                    val_data = self.validation_data[:cut_v_data] + [0]\n                     tensors = self.model.inputs + [K.learning_phase()]\n                 else:\n-                    val_data = self.model.validation_data\n+                    val_data = self.validation_data\n                     tensors = self.model.inputs\n                 feed_dict = dict(zip(tensors, val_data))\n                 result = self.sess.run([self.merged], feed_dict=feed_dict)\n\n@@ -357,18 +357,20 @@ class Layer(object):\n                                      str(spec.ndim) + ', found ndim=' +\n                                      str(K.ndim(x)))\n             if spec.max_ndim is not None:\n-                if K.ndim(x) > spec.max_ndim:\n+                ndim = K.ndim(x)\n+                if ndim is not None and ndim > spec.max_ndim:\n                     raise ValueError('Input ' + str(input_index) +\n                                      ' is incompatible with layer ' +\n                                      self.name + ': expected max_ndim=' +\n                                      str(spec.max_ndim) + ', found ndim=' +\n                                      str(K.ndim(x)))\n             if spec.min_ndim is not None:\n-                if K.ndim(x) < spec.min_ndim:\n+                ndim = K.ndim(x)\n+                if ndim is not None and ndim < spec.min_ndim:\n                     raise ValueError('Input ' + str(input_index) +\n                                      ' is incompatible with layer ' +\n                                      self.name + ': expected min_ndim=' +\n-                                     str(spec.max_ndim) + ', found ndim=' +\n+                                     str(spec.min_ndim) + ', found ndim=' +\n                                      str(K.ndim(x)))\n             # Check dtype.\n             if spec.dtype is not None:\n@@ -383,7 +385,8 @@ class Layer(object):\n                 try:\n                     x_shape = K.int_shape(x)\n                 except TypeError:\n-                    continue\n+                    x_shape = None\n+                if x_shape is not None:\n                     for axis, value in spec.axes.items():\n                         if value is not None and x_shape[int(axis)] not in {value, None}:\n                             raise ValueError('Input ' + str(input_index) +\n@@ -397,7 +400,8 @@ class Layer(object):\n                 try:\n                     x_shape = K.int_shape(x)\n                 except TypeError:\n-                    continue\n+                    x_shape = None\n+                if x_shape is not None:\n                     for spec_dim, dim in zip(spec.shape, x_shape):\n                         if spec_dim is not None and dim is not None:\n                             if spec_dim != dim:\n@@ -438,6 +442,7 @@ class Layer(object):\n             x: Can be a tensor or list/tuple of tensors.\n             **kwargs: Additional keyword arguments to be passed to `call()`.\n         \"\"\"\n+        with K.name_scope(self.name):\n             # Handle laying building (weight creating, input spec locking).\n             if not self.built:\n                 # Raise exceptions in case the input is not compatible\n@@ -487,8 +492,15 @@ class Layer(object):\n             # Actually call the layer, collecting output(s), mask(s), and shape(s).\n             output = self.call(x, **kwargs)\n             output_mask = self.compute_mask(x, previous_mask)\n+\n             # Infering the output shape is only relevant for Theano.\n+            if all([s is not None for s in _to_list(input_shape)]):\n                 output_shape = self.compute_output_shape(input_shape)\n+            else:\n+                if isinstance(input_shape, list):\n+                    output_shape = [None for _ in input_shape]\n+                else:\n+                    output_shape = None\n \n             # Add an inbound node to the layer, so that it keeps track\n             # of the call and of all new variables created during the call.\n@@ -516,7 +528,7 @@ class Layer(object):\n         output_tensors = _to_list(output_tensors)\n         input_masks = _to_list(input_masks)\n         output_masks = _to_list(output_masks)\n-        input_shapes = _to_list(output_shapes)\n+        input_shapes = _to_list(input_shapes)\n         output_shapes = _to_list(output_shapes)\n \n         # Collect input tensor(s) coordinates.\n@@ -530,11 +542,9 @@ class Layer(object):\n                 node_indices.append(node_index)\n                 tensor_indices.append(tensor_index)\n             else:\n-                assert len(input_tensors) == 1\n-                inbound_layers = []\n-                node_indices = []\n-                tensor_indices = []\n-                break\n+                inbound_layers.append(None)\n+                node_indices.append(None)\n+                tensor_indices.append(None)\n \n         # Create node, add it to inbound nodes.\n         Node(\n@@ -553,7 +563,8 @@ class Layer(object):\n         # Update tensor history, _keras_shape and _uses_learning_phase.\n         for i in range(len(output_tensors)):\n             output_tensors[i]._keras_shape = output_shapes[i]\n-            uses_lp = any([x._uses_learning_phase for x in input_tensors])\n+            uses_lp = any([getattr(x, '_uses_learning_phase', False) for x in input_tensors])\n+            uses_lp = getattr(self, 'uses_learning_phase', False) or uses_lp\n             output_tensors[i]._uses_learning_phase = getattr(output_tensors[i], '_uses_learning_phase', False) or uses_lp\n             output_tensors[i]._keras_history = (self,\n                                                 len(self.inbound_nodes) - 1,\n@@ -572,12 +583,12 @@ class Layer(object):\n         \"\"\"\n         return input_shape\n \n-    def compute_mask(self, input, mask=None):\n+    def compute_mask(self, inputs, mask=None):\n         \"\"\"Computes an output masking tensor, given an input tensor\n         (or list thereof) and an input mask (or list thereof).\n \n         # Arguments\n-            input: Tensor or list of tensors.\n+            inputs: Tensor or list of tensors.\n             input_mask: Tensor or list of tensors.\n \n         # Returns\n@@ -1720,8 +1731,8 @@ class Container(Layer):\n             output_tensors, output_masks, output_shapes = self.run_internal_graph(inputs, masks)\n             return output_tensors\n \n-    def compute_mask(self, input, mask):\n-        inputs = _to_list(input)\n+    def compute_mask(self, inputs, mask):\n+        inputs = _to_list(inputs)\n         if mask is None:\n             masks = [None for _ in range(len(inputs))]\n         else:\n@@ -1857,6 +1868,7 @@ class Container(Layer):\n                         computed_data.append(tensor_map[str(id(x))])\n                 if len(computed_data) == len(reference_input_tensors):\n                     # call layer\n+                    with K.name_scope(layer.name):\n                         if len(computed_data) == 1:\n                             computed_tensor, computed_mask = computed_data[0]\n                             if 'mask' in inspect.getargspec(layer.call).args:\n@@ -2420,10 +2432,10 @@ def _collect_input_shape(input_tensors):\n     input_tensors = _to_list(input_tensors)\n     shapes = []\n     for x in input_tensors:\n-        if hasattr(x, '_keras_shape'):\n-            shapes.append(x._keras_shape)\n-        else:\n-            raise ValueError('Input tensor is not a Keras tensor:', x)\n+        try:\n+            shapes.append(K.int_shape(x))\n+        except TypeError:\n+            shapes.append(None)\n     if len(shapes) == 1:\n         return shapes[0]\n     return shapes\n\n@@ -861,6 +861,8 @@ class Model(Container):\n         })\n         callbacks.on_train_begin()\n         callback_model.stop_training = False\n+        for cbk in callbacks:\n+            cbk.validation_data = val_ins\n \n         for epoch in range(initial_epoch, epochs):\n             callbacks.on_epoch_begin(epoch)\n@@ -1500,7 +1502,8 @@ class Model(Container):\n                                  str(validation_data))\n             val_x, val_y, val_sample_weights = self._standardize_user_data(\n                 val_x, val_y, val_sample_weight)\n-\n+            for cbk in callbacks:\n+                cbk.validation_data = val_x + [val_y, val_sample_weights]\n         enqueuer = None\n \n         try:\n\n@@ -922,7 +922,7 @@ class SeparableConv2D(Conv2D):\n         self.depthwise_initializer = initializers.get(depthwise_initializer)\n         self.pointwise_initializer = initializers.get(pointwise_initializer)\n         self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n-        self.pointwise_regularizer = regularizers.get(pointwise_constraint)\n+        self.pointwise_regularizer = regularizers.get(pointwise_regularizer)\n         self.depthwise_constraint = constraints.get(depthwise_constraint)\n         self.pointwise_constraint = constraints.get(pointwise_constraint)\n \n\n@@ -24,6 +24,8 @@ class _Merge(Layer):\n                              'Got ' + str(len(input_shape)) + ' inputs.')\n         # TODO: handle shapes with None entries.\n         input_shapes_set = set(input_shape)\n+        if None in input_shapes_set:\n+            input_shapes_set.remove(None)\n         if len(input_shapes_set) > 1:\n             raise ValueError('Only tensors of same shape can '\n                              'be merged by layer' + self.name +\n@@ -33,7 +35,8 @@ class _Merge(Layer):\n         return self._merge_function(inputs)\n \n     def compute_mask(self, inputs, mask=None):\n-        if mask is not None:\n+        if mask is None:\n+            return None\n         if not isinstance(mask, list):\n             raise ValueError('`mask` should be a list.')\n         if not isinstance(inputs, list):\n@@ -41,6 +44,8 @@ class _Merge(Layer):\n         if len(mask) != len(inputs):\n             raise ValueError('The lists `inputs` and `mask` '\n                              'should have the same length.')\n+        if all([m is None for m in mask]):\n+            return None\n         masks = [K.expand_dims(m, 0) for m in mask if m is not None]\n         return K.all(K.concatenate(masks, axis=0), axis=0, keepdims=False)\n \n@@ -103,6 +108,8 @@ class Concatenate(_Merge):\n         if not isinstance(input_shape, list) or len(input_shape) != 2:\n             raise ValueError('`Concatenate` layer should be called '\n                              'on a list of 2 inputs.')\n+        if input_shape[0] is None or input_shape[1] is None:\n+            return\n         reduced_inputs_shapes = [list(shape) for shape in input_shape]\n         shape_set = set()\n         for i in range(len(reduced_inputs_shapes)):\n@@ -136,13 +143,15 @@ class Concatenate(_Merge):\n     def compute_mask(self, inputs, mask=None):\n         if mask is None:\n             return None\n-        if not isinstance(inputs, list):\n-            raise ValueError('A `Concatenate` layer should be called '\n-                             'on a list of 2 inputs.')\n         if not isinstance(mask, list):\n-            raise ValueError('TODO')\n-        if len(inputs) != len(mask):\n-            raise ValueError('TODO')\n+            raise ValueError('`mask` should be a list.')\n+        if not isinstance(inputs, list):\n+            raise ValueError('`inputs` should be a list.')\n+        if len(mask) != len(inputs):\n+            raise ValueError('The lists `inputs` and `mask` '\n+                             'should have the same length.')\n+        if all([m is None for m in mask]):\n+            return None\n         # Make a list of masks while making sure\n         # the dimensionality of each mask\n         # is the same as the corresponding input.\n@@ -164,7 +173,7 @@ class Concatenate(_Merge):\n         config = {\n             'axis': self.axis,\n         }\n-        base_config = super(Dot, self).get_config()\n+        base_config = super(Concatenate, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -195,6 +204,8 @@ class Dot(_Merge):\n                              'on a list of 2 inputs.')\n         shape1 = input_shape[0]\n         shape2 = input_shape[1]\n+        if shape1 is None or shape2 is None:\n+            return\n         if isinstance(self.axes, int):\n             if self.axes < 0:\n                 axes = [self.axes % len(shape1), self.axes % len(shape2)]\n@@ -250,37 +261,37 @@ class Dot(_Merge):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-def sum(inputs):\n+def sum(inputs, **kwargs):\n     \"\"\"TODO\n     \"\"\"\n-    return Sum()(inputs)\n+    return Sum(**kwargs)(inputs)\n \n \n-def multiply(inputs):\n+def multiply(inputs, **kwargs):\n     \"\"\"TODO\n     \"\"\"\n-    return Multiply()(inputs)\n+    return Multiply(**kwargs)(inputs)\n \n \n-def average(inputs):\n+def average(inputs, **kwargs):\n     \"\"\"TODO\n     \"\"\"\n-    return Average()(inputs)\n+    return Average(**kwargs)(inputs)\n \n \n-def maximum(inputs):\n+def maximum(inputs, **kwargs):\n     \"\"\"TODO\n     \"\"\"\n-    return Maximum()(inputs)\n+    return Maximum(**kwargs)(inputs)\n \n \n-def concatenate(inputs, axis=-1):\n+def concatenate(inputs, axis=-1, **kwargs):\n     \"\"\"TODO\n     \"\"\"\n-    return Concatenate(axis=axis)(inputs)\n+    return Concatenate(axis=axis, **kwargs)(inputs)\n \n \n-def dot(inputs, axes, normalize=False):\n+def dot(inputs, axes, normalize=False, **kwargs):\n     \"\"\"TODO\n     \"\"\"\n-    return Dot(axes=axes, normalize=normalize)(inputs)\n+    return Dot(axes=axes, normalize=normalize, **kwargs)(inputs)\n\n@@ -259,6 +259,7 @@ class Sequential(Model):\n         self.inputs = []  # tensors\n         self.outputs = []  # tensors (length 1)\n         self._trainable = True\n+        self._initial_weights = None\n \n         # model attributes\n         self.inbound_nodes = []\n\n@@ -3,7 +3,8 @@ import json\n import numpy as np\n \n from keras.layers import Dense, Dropout, InputLayer\n-from keras.engine import merge, Input, get_source_inputs\n+from keras import layers\n+from keras.engine import Input, get_source_inputs\n from keras.models import Model, Sequential\n from keras import backend as K\n from keras.models import model_from_json, model_from_yaml\n@@ -84,13 +85,11 @@ def test_learning_phase():\n     dp = Dropout(0.5, name='dropout')\n     b_2 = dp(b)\n \n-    assert dp.uses_learning_phase\n-\n     assert not a_2._uses_learning_phase\n     assert b_2._uses_learning_phase\n \n     # test merge\n-    m = merge([a_2, b_2], mode='concat')\n+    m = layers.concatenate([a_2, b_2])\n     assert m._uses_learning_phase\n \n     # Test recursion\n@@ -166,6 +165,7 @@ def test_node_construction():\n     # test layer properties\n     test_layer = Dense(16, name='test_layer')\n     a_test = test_layer(a)\n+    assert K.int_shape(test_layer.kernel) == (32, 16)\n     assert test_layer.input == a\n     assert test_layer.output == a_test\n     assert test_layer.input_mask is None\n@@ -207,7 +207,7 @@ def test_multi_input_layer():\n     a_2 = dense(a)\n     b_2 = dense(b)\n \n-    merged = merge([a_2, b_2], mode='concat', name='merge')\n+    merged = layers.concatenate([a_2, b_2], name='merge')\n     assert merged._keras_shape == (None, 16 * 2)\n     merge_layer, merge_node_index, merge_tensor_index = merged._keras_history\n \n@@ -223,7 +223,7 @@ def test_multi_input_layer():\n     c = Dense(64, name='dense_2')(merged)\n     d = Dense(5, name='dense_3')(c)\n \n-    model = Model(input=[a, b], output=[c, d], name='model')\n+    model = Model(inputs=[a, b], outputs=[c, d], name='model')\n     assert len(model.layers) == 6\n     print('model.input_layers:', model.input_layers)\n     print('model.input_layers_node_indices:', model.input_layers_node_indices)\n@@ -233,6 +233,7 @@ def test_multi_input_layer():\n     print('output_shape:', model.compute_output_shape([(None, 32), (None, 32)]))\n     assert model.compute_output_shape([(None, 32), (None, 32)]) == [(None, 64), (None, 5)]\n \n+    print('mask:', model.compute_mask([a, b], [None, None]))\n     assert model.compute_mask([a, b], [None, None]) == [None, None]\n \n     print('output_shape:', model.compute_output_shape([(None, 32), (None, 32)]))\n@@ -289,11 +290,11 @@ def test_recursion():\n     dense = Dense(16, name='dense_1')\n     a_2 = dense(a)\n     b_2 = dense(b)\n-    merged = merge([a_2, b_2], mode='concat', name='merge')\n+    merged = layers.concatenate([a_2, b_2], name='merge')\n     c = Dense(64, name='dense_2')(merged)\n     d = Dense(5, name='dense_3')(c)\n \n-    model = Model(input=[a, b], output=[c, d], name='model')\n+    model = Model(inputs=[a, b], outputs=[c, d], name='model')\n \n     e = Input(shape=(32,), name='input_e')\n     f = Input(shape=(32,), name='input_f')\n@@ -307,7 +308,7 @@ def test_recursion():\n     # test separate manipulation of different layer outputs\n     i = Dense(7, name='dense_4')(h)\n \n-    final_model = Model(input=[e, f], output=[i, g], name='final')\n+    final_model = Model(inputs=[e, f], outputs=[i, g], name='final')\n     assert len(final_model.inputs) == 2\n     assert len(final_model.outputs) == 2\n     assert len(final_model.layers) == 4\n@@ -354,7 +355,7 @@ def test_recursion():\n \n     assert n._keras_shape == (None, 5)\n     assert q._keras_shape == (None, 64)\n-    s = merge([n, q], mode='concat', name='merge_nq')\n+    s = layers.concatenate([n, q], name='merge_nq')\n     assert s._keras_shape == (None, 64 + 5)\n \n     # test with single output as 1-elem list\n@@ -390,14 +391,14 @@ def test_recursion():\n     assert [x.shape for x in fn_outputs] == [(10, 69)]\n \n     config = model.get_config()\n-    new_model = Model.from_config(config)\n+    Model.from_config(config)\n \n     model.summary()\n     json_str = model.to_json()\n-    new_model = model_from_json(json_str)\n+    model_from_json(json_str)\n \n     yaml_str = model.to_yaml()\n-    new_model = model_from_yaml(yaml_str)\n+    model_from_yaml(yaml_str)\n \n     ####################################################\n     # test invalid graphs\n@@ -409,14 +410,14 @@ def test_recursion():\n     m, n = model([j, k])\n \n     with pytest.raises(Exception):\n-        invalid_model = Model([j, k], [m, n])\n+        Model([j, k], [m, n])\n \n     # disconnected graph\n     j = Input(shape=(32,), name='input_j')\n     k = Input(shape=(32,), name='input_k')\n     m, n = model([j, k])\n     with pytest.raises(Exception) as e:\n-        invalid_model = Model([j], [m, n])\n+        Model([j], [m, n])\n \n     # redudant outputs\n     j = Input(shape=(32,), name='input_j')\n@@ -424,21 +425,21 @@ def test_recursion():\n     m, n = model([j, k])\n     # this should work lol\n     # TODO: raise a warning\n-    invalid_model = Model([j, k], [m, n, n])\n+    Model([j, k], [m, n, n])\n \n     # redundant inputs\n     j = Input(shape=(32,), name='input_j')\n     k = Input(shape=(32,), name='input_k')\n     m, n = model([j, k])\n     with pytest.raises(Exception):\n-        invalid_model = Model([j, k, j], [m, n])\n+        Model([j, k, j], [m, n])\n \n     # i have not idea what I'm doing: garbage as inputs/outputs\n     j = Input(shape=(32,), name='input_j')\n     k = Input(shape=(32,), name='input_k')\n     m, n = model([j, k])\n     with pytest.raises(Exception):\n-        invalid_model = Model([j, k], [m, n, 0])\n+        Model([j, k], [m, n, 0])\n \n     ####################################################\n     # test calling layers/models on TF tensors\n@@ -450,196 +451,23 @@ def test_recursion():\n         m, n = model([j, k])\n         tf_model = Model([j, k], [m, n])\n \n-        # magic\n         j_tf = tf.placeholder(dtype=K.floatx())\n         k_tf = tf.placeholder(dtype=K.floatx())\n         m_tf, n_tf = tf_model([j_tf, k_tf])\n-        assert not hasattr(m_tf, '_keras_shape')\n-        assert not hasattr(n_tf, '_keras_shape')\n         assert K.int_shape(m_tf) == (None, 64)\n         assert K.int_shape(n_tf) == (None, 5)\n \n         # test merge\n-        o_tf = merge([j_tf, k_tf], mode='concat', concat_axis=1)\n+        layers.concatenate([j_tf, k_tf], axis=1)\n+        layers.sum([j_tf, k_tf])\n \n         # test tensor input\n         x = tf.placeholder(shape=(None, 2), dtype=K.floatx())\n-        input_layer = InputLayer(input_tensor=x)\n+        InputLayer(input_tensor=x)\n \n         x = Input(tensor=x)\n-        y = Dense(2)(x)\n+        Dense(2)(x)\n \n \n-# @keras_test\n-# def test_functional_guide():\n-#     # MNIST\n-#     from keras.layers import Input, Dense, LSTM\n-#     from keras.models import Model\n-#     from keras.utils import np_utils\n-\n-#     # this returns a tensor\n-#     inputs = Input(shape=(784,))\n-\n-#     # a layer instance is callable on a tensor, and returns a tensor\n-#     x = Dense(64, activation='relu')(inputs)\n-#     x = Dense(64, activation='relu')(x)\n-#     predictions = Dense(10, activation='softmax')(x)\n-\n-#     # this creates a model that includes\n-#     # the Input layer and three Dense layers\n-#     model = Model(input=inputs, output=predictions)\n-#     model.compile(optimizer='rmsprop',\n-#                   loss='categorical_crossentropy',\n-#                   metrics=['accuracy'])\n-\n-#     # the data, shuffled and split between tran and test sets\n-#     X_train = np.random.random((100, 784))\n-#     Y_train = np.random.random((100, 10))\n-\n-#     model.fit(X_train, Y_train, epochs=2, batch_size=128)\n-\n-#     assert model.inputs == [inputs]\n-#     assert model.outputs == [predictions]\n-#     assert model.input == inputs\n-#     assert model.output == predictions\n-#     assert model.input_shape == (None, 784)\n-#     assert model.output_shape == (None, 10)\n-\n-#     # try calling the sequential model\n-#     inputs = Input(shape=(784,))\n-#     new_outputs = model(inputs)\n-#     new_model = Model(input=inputs, output=new_outputs)\n-#     new_model.compile(optimizer='rmsprop',\n-#                       loss='categorical_crossentropy',\n-#                       metrics=['accuracy'])\n-\n-#     ##################################################\n-#     # multi-io\n-#     ##################################################\n-#     tweet_a = Input(shape=(4, 25))\n-#     tweet_b = Input(shape=(4, 25))\n-#     # this layer can take as input a matrix\n-#     # and will return a vector of size 64\n-#     shared_lstm = LSTM(64)\n-\n-#     # when we reuse the same layer instance\n-#     # multiple times, the weights of the layer\n-#     # are also being reused\n-#     # (it is effectively *the same* layer)\n-#     encoded_a = shared_lstm(tweet_a)\n-#     encoded_b = shared_lstm(tweet_b)\n-\n-#     # we can then concatenate the two vectors:\n-#     merged_vector = merge([encoded_a, encoded_b],\n-#                           mode='concat', concat_axis=-1)\n-\n-#     # and add a logistic regression on top\n-#     predictions = Dense(1, activation='sigmoid')(merged_vector)\n-\n-#     # we define a trainable model linking the\n-#     # tweet inputs to the predictions\n-#     model = Model(input=[tweet_a, tweet_b], output=predictions)\n-\n-#     model.compile(optimizer='rmsprop',\n-#                   loss='binary_crossentropy',\n-#                   metrics=['accuracy'])\n-#     data_a = np.random.random((1000, 4, 25))\n-#     data_b = np.random.random((1000, 4, 25))\n-#     labels = np.random.random((1000,))\n-#     model.fit([data_a, data_b], labels, epochs=1)\n-\n-#     model.summary()\n-#     assert model.inputs == [tweet_a, tweet_b]\n-#     assert model.outputs == [predictions]\n-#     assert model.input == [tweet_a, tweet_b]\n-#     assert model.output == predictions\n-\n-#     assert model.output == predictions\n-#     assert model.input_shape == [(None, 4, 25), (None, 4, 25)]\n-#     assert model.output_shape == (None, 1)\n-\n-#     assert shared_lstm.get_output_at(0) == encoded_a\n-#     assert shared_lstm.get_output_at(1) == encoded_b\n-#     assert shared_lstm.input_shape == (None, 4, 25)\n-\n-\n-@keras_test\n-def test_sequential_regression():\n-    from keras.models import Sequential, Model\n-    from keras.layers import Merge, Embedding, BatchNormalization, LSTM, InputLayer, Input\n-\n-    # start with a basic example of using a Sequential model\n-    # inside the functional API\n-    seq = Sequential()\n-    seq.add(Dense(input_dim=10, output_dim=10))\n-\n-    x = Input(shape=(10,))\n-    y = seq(x)\n-    model = Model(x, y)\n-    model.compile('rmsprop', 'mse')\n-    weights = model.get_weights()\n-\n-    # test serialization\n-    config = model.get_config()\n-    model = Model.from_config(config)\n-    model.compile('rmsprop', 'mse')\n-    model.set_weights(weights)\n-\n-    # more advanced model with multiple branches\n-\n-    branch_1 = Sequential(name='branch_1')\n-    branch_1.add(Embedding(input_dim=100,\n-                           output_dim=10,\n-                           input_length=2,\n-                           name='embed_1'))\n-    branch_1.add(LSTM(32, name='lstm_1'))\n-\n-    branch_2 = Sequential(name='branch_2')\n-    branch_2.add(Dense(32, input_shape=(8,), name='dense_2'))\n-\n-    branch_3 = Sequential(name='branch_3')\n-    branch_3.add(Dense(32, input_shape=(6,), name='dense_3'))\n-\n-    branch_1_2 = Sequential([Merge([branch_1, branch_2], mode='concat')], name='branch_1_2')\n-    branch_1_2.add(Dense(16, name='dense_1_2-0'))\n-    # test whether impromtu input_shape breaks the model\n-    branch_1_2.add(Dense(16, input_shape=(16,), name='dense_1_2-1'))\n-\n-    model = Sequential([Merge([branch_1_2, branch_3], mode='concat')], name='final')\n-    model.add(Dense(16, name='dense_final'))\n-    model.compile(optimizer='rmsprop',\n-                  loss='categorical_crossentropy',\n-                  metrics=['accuracy'])\n-\n-    model.summary()\n-\n-    x = (100 * np.random.random((100, 2))).astype('int32')\n-    y = np.random.random((100, 8))\n-    z = np.random.random((100, 6))\n-    labels = np.random.random((100, 16))\n-    model.fit([x, y, z], labels, epochs=1)\n-\n-    # test if Sequential can be called in the functional API\n-\n-    a = Input(shape=(2,), dtype='int32')\n-    b = Input(shape=(8,))\n-    c = Input(shape=(6,))\n-    o = model([a, b, c])\n-\n-    outer_model = Model([a, b, c], o)\n-    outer_model.compile(optimizer='rmsprop',\n-                        loss='categorical_crossentropy',\n-                        metrics=['accuracy'])\n-    outer_model.fit([x, y, z], labels, epochs=1)\n-\n-    # test serialization\n-    config = outer_model.get_config()\n-    outer_model = Model.from_config(config)\n-    outer_model.compile(optimizer='rmsprop',\n-                        loss='categorical_crossentropy',\n-                        metrics=['accuracy'])\n-    outer_model.fit([x, y, z], labels, epochs=1)\n-\n-\n-if __name__ == \"__main__\":\n+if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -3,7 +3,7 @@ import numpy as np\n from numpy.testing import assert_allclose\n \n from keras.layers import Dense, Dropout\n-from keras.engine.topology import merge, Input\n+from keras.engine.topology import Input\n from keras.engine.training import Model, check_loss_and_target_compatibility\n from keras.models import Sequential\n from keras import backend as K\n\n@@ -186,12 +186,12 @@ def test_separable_conv_2d():\n                                    'pointwise_regularizer': 'l2',\n                                    'bias_regularizer': 'l2',\n                                    'activity_regularizer': 'l2',\n-                                   'pointwise_constraint': 'unitnorm',\n-                                   'depthwise_constraint': 'unitnorm',\n+                                   'pointwise_constraint': 'unit_norm',\n+                                   'depthwise_constraint': 'unit_norm',\n                                    'strides': strides,\n                                    'depth_multiplier': multiplier},\n                            input_shape=(num_samples, num_row, num_col, stack_size))\n-\n+    # Test dilation\n     layer_test(convolutional.SeparableConv2D,\n                kwargs={'filters': filters,\n                        'kernel_size': (3, 3),\n\n@@ -1,169 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras.utils.test_utils import layer_test\n-from keras.utils.test_utils import keras_test\n-from keras.layers import recurrent\n-from keras.layers import embeddings\n-from keras.models import Sequential\n-from keras.layers.core import Masking\n-from keras import regularizers\n-from keras import backend as K\n-\n-num_samples, timesteps, embedding_dim, units = 2, 5, 4, 3\n-embedding_num = 12\n-\n-\n-@keras_test\n-def rnn_test(f):\n-    \"\"\"\n-    All the recurrent layers share the same interface,\n-    so we can run through them with a single function.\n-    \"\"\"\n-    f = keras_test(f)\n-    return pytest.mark.parametrize('layer_class', [\n-        recurrent.SimpleRNN,\n-        recurrent.GRU,\n-        recurrent.LSTM\n-    ])(f)\n-\n-\n-@rnn_test\n-def test_return_sequences(layer_class):\n-    layer_test(layer_class,\n-               kwargs={'units': units,\n-                       'return_sequences': True},\n-               input_shape=(num_samples, timesteps, embedding_dim))\n-\n-\n-@rnn_test\n-def test_dynamic_behavior(layer_class):\n-    layer = layer_class(units, input_shape=(None, embedding_dim))\n-    model = Sequential()\n-    model.add(layer)\n-    model.compile('sgd', 'mse')\n-    x = np.random.random((num_samples, timesteps, embedding_dim))\n-    y = np.random.random((num_samples, units))\n-    model.train_on_batch(x, y)\n-\n-\n-@rnn_test\n-def test_dropout(layer_class):\n-    layer_test(layer_class,\n-               kwargs={'units': units,\n-                       'dropout': 0.1,\n-                       'recurrent_dropout': 0.1},\n-               input_shape=(num_samples, timesteps, embedding_dim))\n-\n-\n-@rnn_test\n-def test_implementation_mode(layer_class):\n-    for mode in [0, 1, 2]:\n-        layer_test(layer_class,\n-                   kwargs={'units': units,\n-                           'implementation': mode},\n-                   input_shape=(num_samples, timesteps, embedding_dim))\n-\n-\n-@rnn_test\n-def test_statefulness(layer_class):\n-    model = Sequential()\n-    model.add(embeddings.Embedding(embedding_num, embedding_dim,\n-                                   mask_zero=True,\n-                                   input_length=timesteps,\n-                                   batch_input_shape=(num_samples, timesteps)))\n-    layer = layer_class(units, return_sequences=False,\n-                        stateful=True,\n-                        weights=None)\n-    model.add(layer)\n-    model.compile(optimizer='sgd', loss='mse')\n-    out1 = model.predict(np.ones((num_samples, timesteps)))\n-    assert(out1.shape == (num_samples, units))\n-\n-    # train once so that the states change\n-    model.train_on_batch(np.ones((num_samples, timesteps)),\n-                         np.ones((num_samples, units)))\n-    out2 = model.predict(np.ones((num_samples, timesteps)))\n-\n-    # if the state is not reset, output should be different\n-    assert(out1.max() != out2.max())\n-\n-    # check that output changes after states are reset\n-    # (even though the model itself didn't change)\n-    layer.reset_states()\n-    out3 = model.predict(np.ones((num_samples, timesteps)))\n-    assert(out2.max() != out3.max())\n-\n-    # check that container-level reset_states() works\n-    model.reset_states()\n-    out4 = model.predict(np.ones((num_samples, timesteps)))\n-    assert_allclose(out3, out4, atol=1e-5)\n-\n-    # check that the call to `predict` updated the states\n-    out5 = model.predict(np.ones((num_samples, timesteps)))\n-    assert(out4.max() != out5.max())\n-\n-    # Check masking\n-    layer.reset_states()\n-\n-    left_padded_input = np.ones((num_samples, timesteps))\n-    left_padded_input[0, :1] = 0\n-    left_padded_input[1, :2] = 0\n-    out6 = model.predict(left_padded_input)\n-\n-    layer.reset_states()\n-\n-    right_padded_input = np.ones((num_samples, timesteps))\n-    right_padded_input[0, -1:] = 0\n-    right_padded_input[1, -2:] = 0\n-    out7 = model.predict(right_padded_input)\n-\n-    assert_allclose(out7, out6, atol=1e-5)\n-\n-\n-@rnn_test\n-def test_regularizer(layer_class):\n-    layer = layer_class(units, return_sequences=False, weights=None,\n-                        batch_input_shape=(num_samples, timesteps, embedding_dim),\n-                        kernel_regularizer=regularizers.l1(0.01),\n-                        recurrent_regularizer=regularizers.l1(0.01),\n-                        bias_regularizer='l2')\n-    layer.build((None, None, 2))\n-    assert len(layer.losses) == 3\n-    layer(K.variable(np.ones((2, 3, 2))))\n-    assert len(layer.losses) == 3\n-\n-\n-@keras_test\n-def test_masking_layer():\n-    ''' This test based on a previously failing issue here:\n-    https://github.com/fchollet/keras/issues/1567\n-    '''\n-    inputs = np.random.random((6, 3, 4))\n-    targets = np.abs(np.random.random((6, 3, 5)))\n-    targets /= targets.sum(axis=-1, keepdims=True)\n-\n-    model = Sequential()\n-    model.add(Masking(input_shape=(3, 4)))\n-    model.add(recurrent.LSTM(units=5, return_sequences=True, unroll=False))\n-    model.compile(loss='categorical_crossentropy', optimizer='adam')\n-    model.fit(inputs, targets, epochs=1, batch_size=100, verbose=1)\n-\n-    model = Sequential()\n-    model.add(Masking(input_shape=(3, 4)))\n-    model.add(recurrent.LSTM(units=5, return_sequences=True, unroll=True))\n-    model.compile(loss='categorical_crossentropy', optimizer='adam')\n-    model.fit(inputs, targets, epochs=1, batch_size=100, verbose=1)\n-\n-\n-@rnn_test\n-def test_from_config(layer_class):\n-    for stateful in (False, True):\n-        l1 = layer_class(units=1, stateful=stateful)\n-        l2 = layer_class.from_config(l1.get_config())\n-        assert l1.get_config() == l2.get_config()\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -2,11 +2,10 @@ from __future__ import absolute_import\n from __future__ import print_function\n import pytest\n import numpy as np\n-np.random.seed(1337)\n \n from keras.utils.test_utils import get_test_data\n from keras.models import Sequential\n-from keras.layers import Dense, Activation, RepeatVector, TimeDistributedDense, GRU\n+from keras.layers import Dense, Activation, GRU, TimeDistributed\n from keras.utils import np_utils\n from keras.utils.test_utils import keras_test\n \n@@ -14,43 +13,37 @@ num_classes = 10\n batch_size = 128\n epochs = 15\n weighted_class = 5\n-standard_weight = 1\n high_weight = 10\n train_samples = 5000\n test_samples = 1000\n timesteps = 3\n input_dim = 10\n loss = 'mse'\n+standard_weight = 1\n+standard_score_sequential = 0.5\n \n-(X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+\n+def _get_test_data():\n+    np.random.seed(1337)\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=train_samples,\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n                                                          num_classes=num_classes)\n-\n+    int_y_test = y_test.copy()\n+    int_y_train = y_train.copy()\n     # convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, num_classes)\n-Y_test = np_utils.to_categorical(y_test, num_classes)\n-test_ids = np.where(y_test == np.array(weighted_class))[0]\n+    y_train = np_utils.to_categorical(y_train, num_classes)\n+    y_test = np_utils.to_categorical(y_test, num_classes)\n+    test_ids = np.where(int_y_test == np.array(weighted_class))[0]\n \n     class_weight = dict([(i, standard_weight) for i in range(num_classes)])\n     class_weight[weighted_class] = high_weight\n \n     sample_weight = np.ones((y_train.shape[0])) * standard_weight\n-sample_weight[y_train == weighted_class] = high_weight\n+    sample_weight[int_y_train == weighted_class] = high_weight\n \n-temporal_X_train = np.reshape(X_train, (len(X_train), 1, X_train.shape[1]))\n-temporal_X_train = np.repeat(temporal_X_train, timesteps, axis=1)\n-temporal_X_test = np.reshape(X_test, (len(X_test), 1, X_test.shape[1]))\n-temporal_X_test = np.repeat(temporal_X_test, timesteps, axis=1)\n-\n-temporal_Y_train = np.reshape(Y_train, (len(Y_train), 1, Y_train.shape[1]))\n-temporal_Y_train = np.repeat(temporal_Y_train, timesteps, axis=1)\n-temporal_Y_test = np.reshape(Y_test, (len(Y_test), 1, Y_test.shape[1]))\n-temporal_Y_test = np.repeat(temporal_Y_test, timesteps, axis=1)\n-\n-temporal_sample_weight = np.reshape(sample_weight, (len(sample_weight), 1))\n-temporal_sample_weight = np.repeat(temporal_sample_weight, timesteps, axis=1)\n+    return (x_train, y_train), (x_test, y_test), (sample_weight, class_weight, test_ids)\n \n \n def create_sequential_model():\n@@ -65,56 +58,33 @@ def create_sequential_model():\n def create_temporal_sequential_model():\n     model = Sequential()\n     model.add(GRU(32, input_shape=(timesteps, input_dim), return_sequences=True))\n-    model.add(TimeDistributedDense(num_classes))\n+    model.add(TimeDistributed(Dense(num_classes)))\n     model.add(Activation('softmax'))\n     return model\n \n \n-@keras_test\n-def _test_weights_sequential(model, class_weight=None, sample_weight=None,\n-                             X_train=X_train, Y_train=Y_train,\n-                             X_test=X_test, Y_test=Y_test):\n-    if sample_weight is not None:\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  epochs=epochs // 3, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight)\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  epochs=epochs // 3, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight,\n-                  validation_split=0.1)\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  epochs=epochs // 3, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight,\n-                  validation_data=(X_train, Y_train, sample_weight))\n-    else:\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  epochs=epochs // 2, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight)\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  epochs=epochs // 2, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight,\n-                  validation_split=0.1)\n-\n-    model.train_on_batch(X_train[:32], Y_train[:32],\n-                         class_weight=class_weight,\n-                         sample_weight=sample_weight[:32] if sample_weight is not None else None)\n-    model.test_on_batch(X_train[:32], Y_train[:32],\n-                        sample_weight=sample_weight[:32] if sample_weight is not None else None)\n-    score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n-    return score\n-\n-\n-# no weights: reference point\n-model = create_sequential_model()\n-model.compile(loss=loss, optimizer='rmsprop')\n-standard_score_sequential = _test_weights_sequential(model)\n-\n-\n @keras_test\n def test_sequential_class_weights():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop')\n-    score = _test_weights_sequential(model, class_weight=class_weight)\n+\n+    (x_train, y_train), (x_test, y_test), (sample_weight, class_weight, test_ids) = _get_test_data()\n+\n+    model.fit(x_train, y_train, batch_size=batch_size,\n+              epochs=epochs // 3, verbose=0,\n+              class_weight=class_weight,\n+              validation_data=(x_train, y_train, sample_weight))\n+    model.fit(x_train, y_train, batch_size=batch_size,\n+              epochs=epochs // 2, verbose=0,\n+              class_weight=class_weight)\n+    model.fit(x_train, y_train, batch_size=batch_size,\n+              epochs=epochs // 2, verbose=0,\n+              class_weight=class_weight,\n+              validation_split=0.1)\n+\n+    model.train_on_batch(x_train[:32], y_train[:32],\n+                         class_weight=class_weight)\n+    score = model.evaluate(x_test[test_ids, :], y_test[test_ids, :], verbose=0)\n     assert(score < standard_score_sequential)\n \n \n@@ -122,33 +92,59 @@ def test_sequential_class_weights():\n def test_sequential_sample_weights():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop')\n-    score = _test_weights_sequential(model, sample_weight=sample_weight)\n+\n+    (x_train, y_train), (x_test, y_test), (sample_weight, class_weight, test_ids) = _get_test_data()\n+\n+    model.fit(x_train, y_train, batch_size=batch_size,\n+              epochs=epochs // 3, verbose=0,\n+              sample_weight=sample_weight)\n+    model.fit(x_train, y_train, batch_size=batch_size,\n+              epochs=epochs // 3, verbose=0,\n+              sample_weight=sample_weight,\n+              validation_split=0.1)\n+\n+    model.train_on_batch(x_train[:32], y_train[:32],\n+                         sample_weight=sample_weight[:32])\n+    model.test_on_batch(x_train[:32], y_train[:32],\n+                        sample_weight=sample_weight[:32])\n+    score = model.evaluate(x_test[test_ids, :], y_test[test_ids, :], verbose=0)\n     assert(score < standard_score_sequential)\n \n \n @keras_test\n def test_sequential_temporal_sample_weights():\n+    (x_train, y_train), (x_test, y_test), (sample_weight, class_weight, test_ids) = _get_test_data()\n+\n+    temporal_x_train = np.reshape(x_train, (len(x_train), 1, x_train.shape[1]))\n+    temporal_x_train = np.repeat(temporal_x_train, timesteps, axis=1)\n+    temporal_x_test = np.reshape(x_test, (len(x_test), 1, x_test.shape[1]))\n+    temporal_x_test = np.repeat(temporal_x_test, timesteps, axis=1)\n+\n+    temporal_y_train = np.reshape(y_train, (len(y_train), 1, y_train.shape[1]))\n+    temporal_y_train = np.repeat(temporal_y_train, timesteps, axis=1)\n+    temporal_y_test = np.reshape(y_test, (len(y_test), 1, y_test.shape[1]))\n+    temporal_y_test = np.repeat(temporal_y_test, timesteps, axis=1)\n+\n+    temporal_sample_weight = np.reshape(sample_weight, (len(sample_weight), 1))\n+    temporal_sample_weight = np.repeat(temporal_sample_weight, timesteps, axis=1)\n+\n     model = create_temporal_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop',\n                   sample_weight_mode='temporal')\n-    score = _test_weights_sequential(model,\n-                                     sample_weight=temporal_sample_weight,\n-                                     X_train=temporal_X_train,\n-                                     X_test=temporal_X_test,\n-                                     Y_train=temporal_Y_train,\n-                                     Y_test=temporal_Y_test)\n-    assert(score < standard_score_sequential)\n \n-    # a twist: sample-wise weights with temporal output\n-    model = create_temporal_sequential_model()\n-    model.compile(loss=loss, optimizer='rmsprop',\n-                  sample_weight_mode=None)\n-    score = _test_weights_sequential(model,\n-                                     sample_weight=sample_weight,\n-                                     X_train=temporal_X_train,\n-                                     X_test=temporal_X_test,\n-                                     Y_train=temporal_Y_train,\n-                                     Y_test=temporal_Y_test)\n+    model.fit(temporal_x_train, temporal_y_train, batch_size=batch_size,\n+              epochs=epochs // 3, verbose=0,\n+              sample_weight=temporal_sample_weight)\n+    model.fit(temporal_x_train, temporal_y_train, batch_size=batch_size,\n+              epochs=epochs // 3, verbose=0,\n+              sample_weight=temporal_sample_weight,\n+              validation_split=0.1)\n+\n+    model.train_on_batch(temporal_x_train[:32], temporal_y_train[:32],\n+                         sample_weight=temporal_sample_weight[:32])\n+    model.test_on_batch(temporal_x_train[:32], temporal_y_train[:32],\n+                        sample_weight=temporal_sample_weight[:32])\n+    score = model.evaluate(temporal_x_test[test_ids], temporal_y_test[test_ids], verbose=0)\n     assert(score < standard_score_sequential)\n \n \n\n@@ -5,10 +5,10 @@ import numpy as np\n from numpy.testing import assert_allclose\n \n from keras.models import Model, Sequential\n-from keras.layers import Dense, Dropout, Lambda, RepeatVector, TimeDistributed\n+from keras.layers import Dense, Lambda, RepeatVector, TimeDistributed\n from keras.layers import Input\n from keras import optimizers\n-from keras import objectives\n+from keras import losses\n from keras import metrics\n from keras.utils.test_utils import keras_test\n from keras.models import save_model, load_model\n@@ -20,7 +20,7 @@ def test_sequential_model_saving():\n     model.add(Dense(2, input_shape=(3,)))\n     model.add(RepeatVector(3))\n     model.add(TimeDistributed(Dense(3)))\n-    model.compile(loss=objectives.MSE,\n+    model.compile(loss=losses.MSE,\n                   optimizer=optimizers.RMSprop(lr=0.0001),\n                   metrics=[metrics.categorical_accuracy],\n                   sample_weight_mode='temporal')\n@@ -52,7 +52,7 @@ def test_sequential_model_saving():\n def test_sequential_model_saving_2():\n     # test with custom optimizer, loss\n     custom_opt = optimizers.rmsprop\n-    custom_loss = objectives.mse\n+    custom_loss = losses.mse\n     model = Sequential()\n     model.add(Dense(2, input_shape=(3,)))\n     model.add(Dense(3))\n@@ -82,7 +82,7 @@ def test_fuctional_model_saving():\n     output = Dense(3)(x)\n \n     model = Model(input, output)\n-    model.compile(loss=objectives.MSE,\n+    model.compile(loss=losses.MSE,\n                   optimizer=optimizers.RMSprop(lr=0.0001),\n                   metrics=[metrics.categorical_accuracy])\n     x = np.random.random((1, 3))\n@@ -136,12 +136,12 @@ def test_loading_weights_by_name():\n \n     # test with custom optimizer, loss\n     custom_opt = optimizers.rmsprop\n-    custom_loss = objectives.mse\n+    custom_loss = losses.mse\n \n     # sequential model\n     model = Sequential()\n-    model.add(Dense(2, input_shape=(3,), name=\"rick\"))\n-    model.add(Dense(3, name=\"morty\"))\n+    model.add(Dense(2, input_shape=(3,), name='rick'))\n+    model.add(Dense(3, name='morty'))\n     model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n \n     x = np.random.random((1, 3))\n@@ -157,8 +157,8 @@ def test_loading_weights_by_name():\n     # delete and recreate model\n     del(model)\n     model = Sequential()\n-    model.add(Dense(2, input_shape=(3,), name=\"rick\"))\n-    model.add(Dense(3, name=\"morty\"))\n+    model.add(Dense(2, input_shape=(3,), name='rick'))\n+    model.add(Dense(3, name='morty'))\n     model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n \n     # load weights from first model\n@@ -183,12 +183,12 @@ def test_loading_weights_by_name_2():\n \n     # test with custom optimizer, loss\n     custom_opt = optimizers.rmsprop\n-    custom_loss = objectives.mse\n+    custom_loss = losses.mse\n \n     # sequential model\n     model = Sequential()\n-    model.add(Dense(2, input_shape=(3,), name=\"rick\"))\n-    model.add(Dense(3, name=\"morty\"))\n+    model.add(Dense(2, input_shape=(3,), name='rick'))\n+    model.add(Dense(3, name='morty'))\n     model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n \n     x = np.random.random((1, 3))\n@@ -204,12 +204,12 @@ def test_loading_weights_by_name_2():\n     # delete and recreate model using Functional API\n     del(model)\n     data = Input(shape=(3,))\n-    rick = Dense(2, name=\"rick\")(data)\n-    jerry = Dense(3, name=\"jerry\")(rick)  # add 2 layers (but maintain shapes)\n-    jessica = Dense(2, name=\"jessica\")(jerry)\n-    morty = Dense(3, name=\"morty\")(jessica)\n+    rick = Dense(2, name='rick')(data)\n+    jerry = Dense(3, name='jerry')(rick)  # add 2 layers (but maintain shapes)\n+    jessica = Dense(2, name='jessica')(jerry)\n+    morty = Dense(3, name='morty')(jessica)\n \n-    model = Model(input=[data], output=[morty])\n+    model = Model(inputs=[data], outputs=[morty])\n     model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n \n     # load weights from first model\n@@ -244,7 +244,7 @@ def test_saving_lambda_custom_objects():\n     output = Dense(3)(x)\n \n     model = Model(input, output)\n-    model.compile(loss=objectives.MSE,\n+    model.compile(loss=losses.MSE,\n                   optimizer=optimizers.RMSprop(lr=0.0001),\n                   metrics=[metrics.categorical_accuracy])\n     x = np.random.random((1, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
