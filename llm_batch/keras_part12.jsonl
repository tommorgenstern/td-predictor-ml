{"custom_id": "keras#31a6ee342b8d609d0765c679a0759acf5c9d1611", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 20 | Churn Cumulative: 2288 | Contributors (this commit): 19 | Commits (past 90d): 100 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,8 +38,8 @@ class Sequential(Layer):\n         self.regularizers += regularizers\n         self.constraints += constraints\n \n-        if hasattr(layer,'activity_regularizer'):\n-            self.cost_updates.append(layer.activity_regularizer)\n+        if hasattr(layer, 'cost_update'):\n+            self.cost_updates.append(layer.cost_update)\n \n     def get_output(self, train=False):\n         return self.layers[-1].get_output(train)\n\n@@ -283,7 +283,7 @@ class ActivityRegularization(Layer):\n     '''\n     def __init__(self, activity_regularizer):\n         super(ActivityRegularization, self).__init__()\n-        self.activity_regularizer = activity_regularizer(self.get_output())\n+        self.cost_update = activity_regularizer(self)\n \n     def get_output(self, train):\n         return self.get_input(train)\n\n@@ -74,7 +74,7 @@ class Model(object):\n \n         if hasattr(self, 'cost_updates'):\n             for u in self.cost_updates:\n-                train_loss += u\n+                train_loss += u()\n \n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints,  train_loss)\n \n@@ -306,6 +306,7 @@ class Sequential(Model, containers.Sequential):\n         self.params = [] # learnable\n         self.regularizers = [] # same size as params\n         self.constraints = [] # same size as params\n+        self.cost_updates = [] # NOT the same size as params\n \n \n     def get_config(self, verbose=0):\n@@ -357,4 +358,3 @@ class Sequential(Model, containers.Sequential):\n             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n             self.layers[k].set_weights(weights)\n         f.close()\n-        \n\n@@ -24,3 +24,13 @@ def l1l2(l1=.01, l2=.01):\n \n def identity(g, p):\n     return g\n+\n+\n+def activity_l1(l=.01):\n+    # activity dependent l1 norm\n+    def l1wrap(layer):\n+        # needs to be wrapped twice because input is not present during instantiation\n+        def l1wrap_wrap():\n+            return l * T.sum(T.mean(layer.get_output(True)**2, axis=0))\n+        return l1wrap_wrap\n+    return l1wrap\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cf49d59aff3766c2cf78d17a78e2b821db902228", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 53 | Churn Cumulative: 53 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,53 @@\n+__author__ = 'mccolgan'\n+\n+\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers.convolutional import Convolution2D\n+from keras.layers.core import Dense, Flatten, ActivityRegularization\n+from keras.utils import np_utils\n+from keras.regularizers import l2, activity_l1\n+\n+import numpy as np\n+\n+np.random.seed(1337)\n+\n+max_train_samples = 128*8\n+max_test_samples = 1000\n+\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape((-1,28*28))\n+X_test = X_test.reshape((-1,28*28))\n+\n+nb_classes = len(np.unique(y_train))\n+Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n+Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n+\n+model_noreg = Sequential()\n+model_noreg.add(Flatten())\n+model_noreg.add(Dense(28*28, 20, activation='sigmoid'))\n+model_noreg.add(Dense(20, 10, activation='sigmoid'))\n+\n+model_noreg.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+model_noreg.fit(X_train, Y_train)\n+\n+score_noreg = model_noreg.evaluate(X_test, Y_test)\n+score_train_noreg = model_noreg.evaluate(X_train, Y_train)\n+\n+model_reg = Sequential()\n+model_reg.add(Flatten())\n+model_reg.add(Dense(28*28, 20, activation='sigmoid'))\n+model_reg.add(ActivityRegularization(activity_l1(0.015)))\n+model_reg.add(Dense(20, 10, activation='sigmoid'))\n+\n+model_reg.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+model_reg.fit(X_train, Y_train)\n+\n+score_reg = model_reg.evaluate(X_test, Y_test)\n+score_train_reg = model_reg.evaluate(X_train, Y_train)\n+\n+print\n+print\n+print \"Overfitting without regularisation: %f - %f = %f\" % ( score_noreg , score_train_noreg , score_noreg-score_train_noreg)\n+print \"Overfitting with regularisation: %f - %f = %f\" % ( score_reg , score_train_reg , score_reg-score_train_reg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bd361595be9c17d8c7dde4f4aaac820db2ae5ab3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 50 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -31,6 +31,6 @@ def activity_l1(l=.01):\n     def l1wrap(layer):\n         # needs to be wrapped twice because input is not present during instantiation\n         def l1wrap_wrap():\n-            return l * T.sum(T.mean(layer.get_output(True)**2, axis=0))\n+            return l * T.sum(T.mean(T.abs(layer.get_output(True)), axis=0))\n         return l1wrap_wrap\n     return l1wrap\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a51d7ae1457f64ce0865421120260aeef5cab68e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 11 | Churn Cumulative: 61 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,6 +31,15 @@ def activity_l1(l=.01):\n     def l1wrap(layer):\n         # needs to be wrapped twice because input is not present during instantiation\n         def l1wrap_wrap():\n-            return l * T.sum(T.mean(T.abs(layer.get_output(True)), axis=0))\n+            return l * T.sum(T.mean(abs(layer.get_output(True)), axis=0))\n         return l1wrap_wrap\n     return l1wrap\n+\n+def activity_l2(l=.01):\n+    # activity dependent l1 norm\n+    def l2wrap(layer):\n+        # needs to be wrapped twice because input is not present during instantiation\n+        def l2wrap_wrap():\n+            return l * T.sum(T.mean(layer.get_output(True)**2, axis=0))\n+        return l2wrap_wrap\n+    return l2wrap\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4cfb5d7971694d9aeb7a03d1c7c279d60ff1f1f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 42 | Churn Cumulative: 1303 | Contributors (this commit): 16 | Commits (past 90d): 57 | Contributors (cumulative): 16 | DMM Complexity: 0.6875\n\nDIFF:\n@@ -41,8 +41,19 @@ def slice_X(X, start=None, stop=None):\n         else:\n             return X[start:stop]\n \n-def calculate_class_weights(Y, class_weight):\n-    if isinstance(class_weight, dict):\n+def standardize_instance_weights(instance_weight):\n+    if isinstance(instance_weight, list):\n+        w = np.array(instance_weight)\n+    elif isinstance(instance_weight, np.ndarray):\n+        w = instance_weight\n+    else:\n+        w = None\n+    return w\n+\n+def calculate_loss_weights(Y, instance_weight=None, class_weight=None):\n+    if instance_weight is not None:\n+        w = instance_weight\n+    elif isinstance(class_weight, dict):\n         if Y.shape[1] > 1:\n             y_classes = Y.argmax(axis=1)\n         elif Y.shape[1] == 1:\n@@ -71,9 +82,9 @@ class Model(object):\n         self.y = T.zeros_like(self.y_train)\n \n         # parameter for rescaling the objective function\n-        self.class_weights = T.vector()\n+        self.loss_weights = T.vector()\n \n-        train_loss = self.loss(self.y, self.y_train, self.class_weights)\n+        train_loss = self.loss(self.y, self.y_train, self.loss_weights)\n         test_score = self.loss(self.y, self.y_test)\n \n         if class_mode == \"categorical\":\n@@ -90,11 +101,11 @@ class Model(object):\n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints, train_loss)\n \n         if type(self.X_train) == list:\n-            train_ins = self.X_train + [self.y, self.class_weights]\n+            train_ins = self.X_train + [self.y, self.loss_weights]\n             test_ins = self.X_test + [self.y]\n             predict_ins = self.X_test\n         else:\n-            train_ins = [self.X_train, self.y, self.class_weights]\n+            train_ins = [self.X_train, self.y, self.loss_weights]\n             test_ins = [self.X_test, self.y]\n             predict_ins = [self.X_test]\n \n@@ -110,11 +121,13 @@ class Model(object):\n             allow_input_downcast=True, mode=theano_mode)\n \n \n-    def train(self, X, y, accuracy=False, class_weight=None):\n+    def train(self, X, y, accuracy=False, instance_weight=None, class_weight=None):\n         X = standardize_X(X)\n         y = standardize_y(y)\n-        # calculate the weight vector for the loss function\n-        w = calculate_class_weights(y, class_weight)\n+\n+        instance_weight = standardize_instance_weights(instance_weight)\n+        w = calculate_loss_weights(y, instance_weight=instance_weight, class_weight=class_weight)\n+\n         ins = X + [y, w]\n         if accuracy:\n             return self._train_with_acc(*ins)\n@@ -133,11 +146,14 @@ class Model(object):\n \n \n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n-            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False, class_weight=None):\n+            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False,\n+            instance_weight=None, class_weight=None):\n \n         X = standardize_X(X)\n         y = standardize_y(y)\n \n+        instance_weight = standardize_instance_weights(instance_weight)\n+\n         do_validation = False\n         if validation_data:\n             try:\n@@ -191,8 +207,10 @@ class Model(object):\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n \n-                # calculate weight vector for current batch\n-                w = calculate_class_weights(y_batch, class_weight)\n+                if instance_weight is not None:\n+                    w = calculate_loss_weights(y_batch, instance_weight=instance_weight[batch_ids])\n+                else:\n+                    w = calculate_loss_weights(y_batch, class_weight=class_weight)\n \n                 batch_logs = {}\n                 batch_logs['batch'] = batch_index\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b3402f50117f462c66b9d4ce7be699da7900b107", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6329378ca3514ad18a3e1dfb4512e8d04c99bdf5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 124 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,13 +4,15 @@ import numpy as np\n import random\n from six.moves import range\n \n-def pad_sequences(sequences, maxlen=None, dtype='int32'):\n+def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre'):\n     \"\"\"\n         Pad each sequence to the same length: \n         the length of the longuest sequence.\n \n         If maxlen is provided, any sequence longer\n         than maxlen is truncated to maxlen.\n+\n+        Support post-padding and pre-padding (default).\n     \"\"\"\n     lengths = [len(s) for s in sequences]\n \n@@ -20,7 +22,10 @@ def pad_sequences(sequences, maxlen=None, dtype='int32'):\n \n     x = np.zeros((nb_samples, maxlen)).astype(dtype)\n     for idx, s in enumerate(sequences):\n+        if padding == 'post':\n             x[idx, :lengths[idx]] = s[:maxlen]\n+        else:\n+            x[idx, -min(maxlen, lengths[idx]):] = s[:maxlen]\n     return x\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a75a31381e1e7acfbc08c22a3b10954b5823d143", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 95 | Churn Cumulative: 1212 | Contributors (this commit): 12 | Commits (past 90d): 60 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,8 @@ from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n srng = RandomStreams()\n \n+default_mask_val = -999.\n+\n class Layer(object):\n     def __init__(self):\n         self.params = []\n\n@@ -3,7 +3,7 @@ import theano\n import theano.tensor as T\n \n from .. import activations, initializations\n-from ..layers.core import Layer\n+from ..layers.core import Layer, default_mask_val\n from ..constraints import unitnorm\n \n \n@@ -15,7 +15,7 @@ class Embedding(Layer):\n         @input_dim: size of vocabulary (highest input integer + 1)\n         @out_dim: size of dense representation\n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', weights=None, W_regularizer=None, W_constraint=None):\n+    def __init__(self, input_dim, output_dim, init='uniform', weights=None, W_regularizer=None, W_constraint=None, mask_val=default_mask_val):\n         super(Embedding,self).__init__()\n         self.init = initializations.get(init)\n         self.input_dim = input_dim\n@@ -23,6 +23,7 @@ class Embedding(Layer):\n \n         self.input = T.imatrix()\n         self.W = self.init((self.input_dim, self.output_dim))\n+        T.set_subtensor(self.W[0,:], mask_val)\n         self.params = [self.W]\n         self.constraints = [W_constraint]\n         self.regularizers = [W_regularizer]\n\n@@ -5,8 +5,8 @@ import theano.tensor as T\n import numpy as np\n \n from .. import activations, initializations\n-from ..utils.theano_utils import shared_zeros, alloc_zeros_matrix\n-from ..layers.core import Layer\n+from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix\n+from ..layers.core import Layer, default_mask_val\n from six.moves import range\n \n class SimpleRNN(Layer):\n@@ -19,7 +19,7 @@ class SimpleRNN(Layer):\n     '''\n     def __init__(self, input_dim, output_dim, \n         init='glorot_uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n-        truncate_gradient=-1, return_sequences=False):\n+        truncate_gradient=-1, return_sequences=False, mask_val=default_mask_val):\n         super(SimpleRNN,self).__init__()\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -28,6 +28,7 @@ class SimpleRNN(Layer):\n         self.truncate_gradient = truncate_gradient\n         self.activation = activations.get(activation)\n         self.return_sequences = return_sequences\n+        self.mask_val = shared_scalar(mask_val)\n         self.input = T.tensor3()\n \n         self.W = self.init((self.input_dim, self.output_dim))\n@@ -38,13 +39,13 @@ class SimpleRNN(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def _step(self, x_t, h_tm1, u):\n+    def _step(self, x_t, mask_t, h_tm1, u):\n         '''\n             Variable names follow the conventions from: \n             http://deeplearning.net/software/theano/library/scan.html\n \n         '''\n-        return self.activation(x_t + T.dot(h_tm1, u))\n+        return mask_t*self.activation(x_t + T.dot(h_tm1, u))\n \n     def get_output(self, train):\n         X = self.get_input(train) # shape: (nb_samples, time (padded with zeros at the end), input_dim)\n@@ -53,12 +54,15 @@ class SimpleRNN(Layer):\n \n         x = T.dot(X, self.W) + self.b\n \n+        mask = T.neq(x,self.mask_val).sum(axis=2)>0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n+        mask = T.addbroadcast(mask[:,:,np.newaxis], 2)\n+        \n         # scan = theano symbolic loop.\n         # See: http://deeplearning.net/software/theano/library/scan.html\n         # Iterate over the first dimension of the x array (=time).\n         outputs, updates = theano.scan(\n             self._step, # this will be called with arguments (sequences[i], outputs[i-1], non_sequences[i])\n-            sequences=x, # tensors to iterate over, inputs to _step\n+            sequences=[x, mask], # tensors to iterate over, inputs to _step\n             # initialization of the output. Input to _step with default tap=-1.\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=self.U, # static inputs to _step\n\n\n@@ -0,0 +1,72 @@\n+# Dummy test data as input to RNN. This input is 3 timesteps long where the third timestep always matches the\n+# first. Without masking it should be able to learn it, with masking it should fail.\n+\n+import numpy as np\n+from keras.utils.theano_utils import sharedX\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation, Merge\n+from keras.layers.embeddings import Embedding\n+from keras.layers.recurrent import SimpleRNN\n+import theano\n+\n+theano.config.exception_verbosity='high' \n+\n+# (nb_samples, timesteps, dimensions)\n+X = np.random.random_integers(1, 4, size=(400000,3))\n+\n+model = Sequential()\n+model.add(Embedding(5, 2))\n+model.add(SimpleRNN(2,3, activation='relu', return_sequences=True))\n+model.add(SimpleRNN(3,3, activation='relu'))\n+model.add(Dense(3,4, activation='softmax'))\n+model.compile(loss='categorical_crossentropy',\n+        optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n+print(\"Compiled model\")\n+\n+W = model.get_weights() # We'll save these so we can reset it later\n+\n+\n+Xmask0 = X.copy()\n+Xmask0[:,0] = 0\n+\n+X0_onehot = np.zeros((X.shape[0], 4))\n+X1_onehot = np.zeros((X.shape[0], 4))\n+for i, row in enumerate(X):\n+    X0_onehot[i, row[0]-1] = 1\n+    X1_onehot[i, row[1]-1] = 1\n+\n+# Uniform score: 4 options = ln(4) nats (2 bits)\n+# we should not do better than this when we mask out the part of the input\n+# that gives us the correct answer\n+uniform_score = np.log(4)\n+\n+# Train it to guess 0th dim\n+model.fit(X, X0_onehot, nb_epoch=1)\n+score = model.evaluate(X, X0_onehot)\n+if score > uniform_score*0.9:\n+    raise Exception('Failed to learn to copy timestep 0, score %f' % score)\n+\n+model.set_weights(W)\n+\n+# Train without showing it the 0th dim to learn 1st dim\n+model.fit(X[:,1:], X1_onehot, nb_epoch=1)\n+score = model.evaluate(X[:,1:], X1_onehot)\n+if score > uniform_score*0.9:\n+    raise Exception('Failed to learn to copy timestep 1, score %f' % score)\n+\n+model.set_weights(W)\n+\n+# Train to guess 0th dim when 0th dim has been masked (should fail)\n+model.fit(Xmask0, X0_onehot, nb_epoch=1)\n+score = model.evaluate(Xmask0, X0_onehot)\n+if score < uniform_score*0.9:\n+   raise Exception('Somehow learned to copy timestep 0 despite mask, score %f' % score)\n+\n+model.set_weights(W)\n+\n+# Train to guess 1st dim when 0th dim has been masked (should succeed)\n+model.fit(Xmask0, X1_onehot, nb_epoch=1)\n+score = model.evaluate(Xmask0, X1_onehot)\n+if score > uniform_score*0.9:\n+    raise Exception('Failed to learn to copy timestep 1 in masked model, score %f' % score)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f445d9a21185c26beaf56fa8599f2cb5d7d05f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 123 | Contributors (this commit): 5 | Commits (past 90d): 14 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,13 +39,13 @@ class SimpleRNN(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def _step(self, x_t, mask_t, h_tm1, u):\n+    def _step(self, x_t, mask_t, mask_tm1, h_tm1, u):\n         '''\n             Variable names follow the conventions from: \n             http://deeplearning.net/software/theano/library/scan.html\n \n         '''\n-        return mask_t * self.activation(x_t + T.dot(h_tm1, u))\n+        return mask_t * self.activation(x_t + mask_tm1 * T.dot(h_tm1, u))\n \n     def get_output(self, train):\n         X = self.get_input(train) # shape: (nb_samples, time (padded with zeros at the end), input_dim)\n@@ -57,12 +57,15 @@ class SimpleRNN(Layer):\n         mask = T.neq(x, self.mask_val).sum(axis=2) > 0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n         mask = T.addbroadcast(mask[:, :, np.newaxis], 2)\n \n+        mask_tm1 = alloc_zeros_matrix(*mask.shape)\n+        mask_tm1 = T.addbroadcast(T.set_subtensor(mask[1:, :, :], mask[:-1, :, :]), 2)\n+        \n         # scan = theano symbolic loop.\n         # See: http://deeplearning.net/software/theano/library/scan.html\n         # Iterate over the first dimension of the x array (=time).\n         outputs, updates = theano.scan(\n             self._step, # this will be called with arguments (sequences[i], outputs[i-1], non_sequences[i])\n-            sequences=[x, mask], # tensors to iterate over, inputs to _step\n+            sequences=[x, mask, mask_tm1], # tensors to iterate over, inputs to _step\n             # initialization of the output. Input to _step with default tap=-1.\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=self.U, # static inputs to _step\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3695591704a21c6d0d6236c11c1bad7de4e314c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 125 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -58,7 +58,7 @@ class SimpleRNN(Layer):\n         mask = T.addbroadcast(mask[:, :, np.newaxis], 2)\n \n         mask_tm1 = alloc_zeros_matrix(*mask.shape)\n-        mask_tm1 = T.addbroadcast(T.set_subtensor(mask[1:, :, :], mask[:-1, :, :]), 2)\n+        mask_tm1 = T.addbroadcast(T.set_subtensor(mask_tm1[1:, :, :], mask[:-1, :, :]), 2)\n         \n         # scan = theano symbolic loop.\n         # See: http://deeplearning.net/software/theano/library/scan.html\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36676451c951a91dd07b272808cb21b32e931e75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 131 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -52,14 +52,14 @@ class SimpleRNN(Layer):\n         # new shape: (time, nb_samples, input_dim) -> because theano.scan iterates over main dimension\n         X = X.dimshuffle((1,0,2)) \n \n-        x = T.dot(X, self.W) + self.b\n-\n-        mask = T.neq(x, self.mask_val).sum(axis=2) > 0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n+        mask = T.neq(X, self.mask_val).sum(axis=2) > 0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n         mask = T.addbroadcast(mask[:, :, np.newaxis], 2)\n \n         mask_tm1 = alloc_zeros_matrix(*mask.shape)\n         mask_tm1 = T.addbroadcast(T.set_subtensor(mask_tm1[1:, :, :], mask[:-1, :, :]), 2)\n \n+        x = T.dot(X, self.W) + self.b\n+        \n         # scan = theano symbolic loop.\n         # See: http://deeplearning.net/software/theano/library/scan.html\n         # Iterate over the first dimension of the x array (=time).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#32c507eaf81063cee449e152459b85a7037614bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 133 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,7 @@ class SimpleRNN(Layer):\n         mask = T.neq(X, self.mask_val).sum(axis=2) > 0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n         mask = T.addbroadcast(mask[:, :, np.newaxis], 2)\n \n-        mask_tm1 = alloc_zeros_matrix(*mask.shape)\n+        mask_tm1 = alloc_zeros_matrix(*mask.shape).astype('int8')\n         mask_tm1 = T.addbroadcast(T.set_subtensor(mask_tm1[1:, :, :], mask[:-1, :, :]), 2)\n \n         x = T.dot(X, self.W) + self.b\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bde9ff82320e3a6cd55eb3f6068865c94d1d27f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 203 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,7 +15,7 @@ class Embedding(Layer):\n         @input_dim: size of vocabulary (highest input integer + 1)\n         @out_dim: size of dense representation\n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', weights=None, W_regularizer=None, W_constraint=None, mask_val=default_mask_val):\n+    def __init__(self, input_dim, output_dim, init='uniform', weights=None, W_regularizer=None, W_constraint=None, zero_is_mask=False, mask_val=default_mask_val):\n         super(Embedding,self).__init__()\n         self.init = initializations.get(init)\n         self.input_dim = input_dim\n@@ -23,7 +23,11 @@ class Embedding(Layer):\n \n         self.input = T.imatrix()\n         self.W = self.init((self.input_dim, self.output_dim))\n+        self.zero_is_mask = zero_is_mask\n+\n+        if zero_is_mask:\n             T.set_subtensor(self.W[0, :], mask_val)\n+\n         self.params = [self.W]\n         self.constraints = [W_constraint]\n         self.regularizers = [W_regularizer]\n\n@@ -15,7 +15,7 @@ theano.config.exception_verbosity='high'\n X = np.random.random_integers(1, 4, size=(400000,3))\n \n model = Sequential()\n-model.add(Embedding(5, 2))\n+model.add(Embedding(5, 2, zero_is_mask=True))\n model.add(SimpleRNN(2,3, activation='relu', return_sequences=True))\n model.add(SimpleRNN(3,3, activation='relu'))\n model.add(Dense(3,4, activation='softmax'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8f1a6a11b325375041562e77a7dd59f91d13b22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 84 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,7 @@ from keras.models import Sequential\n from keras.layers.core import Dense, Activation, Merge\n from keras.layers.embeddings import Embedding\n from keras.layers.recurrent import SimpleRNN\n+from keras.layers.core import default_mask_val\n import theano\n \n theano.config.exception_verbosity='high' \n@@ -23,6 +24,10 @@ model.compile(loss='categorical_crossentropy',\n         optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n print(\"Compiled model\")\n \n+if model.layers[0].W.get_value()[0,:] != default_mask_val:\n+    raise Exception(\"Did not set the mask val properly into the Embedding W matrix, got: \",\n+            model.layers[0].W.get_value()[0,:]\n+\n W = model.get_weights() # We'll save these so we can reset it later\n \n \n@@ -46,6 +51,11 @@ score = model.evaluate(X, X0_onehot)\n if score > uniform_score*0.9:\n     raise Exception('Failed to learn to copy timestep 0, score %f' % score)\n \n+if model.layers[0].W.get_value()[0,:] != default_mask_val:\n+    raise Exception(\"After training, the W of the Embedding's mask value changed to: \",\n+            model.layers[0].W.get_value()[0,:]\n+    \n+\n model.set_weights(W)\n \n # Train without showing it the 0th dim to learn 1st dim\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7a612bb8c88bbd38421fe076b6f296ccd4e599f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 13 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 57 | Churn Cumulative: 403 | Contributors (this commit): 6 | Commits (past 90d): 33 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,8 @@ import theano.tensor as T\n \n from .. import activations, initializations\n from ..layers.core import Layer, default_mask_val\n+from ..utils.theano_utils import sharedX\n+\n from ..constraints import unitnorm\n \n \n@@ -26,7 +28,8 @@ class Embedding(Layer):\n         self.zero_is_mask = zero_is_mask\n \n         if zero_is_mask:\n-            T.set_subtensor(self.W[0, :], mask_val)\n+            # This doesn't seem particularly elegant\n+            self.W = sharedX(T.set_subtensor(self.W[0, :], mask_val).eval())\n \n         self.params = [self.W]\n         self.constraints = [W_constraint]\n\n@@ -45,7 +45,8 @@ class SimpleRNN(Layer):\n             http://deeplearning.net/software/theano/library/scan.html\n \n         '''\n-        return mask_t * self.activation(x_t + mask_tm1 * T.dot(h_tm1, u))\n+        normal_output = self.activation(x_t + mask_tm1 * T.dot(h_tm1, u))\n+        return mask_t * normal_output + (1 - mask_t) * self.mask_val\n \n     def get_output(self, train):\n         X = self.get_input(train) # shape: (nb_samples, time (padded with zeros at the end), input_dim)\n\n@@ -24,9 +24,10 @@ model.compile(loss='categorical_crossentropy',\n         optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n print(\"Compiled model\")\n \n-if model.layers[0].W.get_value()[0,:] != default_mask_val:\n+W0 = model.layers[0].W.get_value()[0,:]\n+if (W0 != default_mask_val).any():\n     raise Exception(\"Did not set the mask val properly into the Embedding W matrix, got: \",\n-            model.layers[0].W.get_value()[0,:]\n+            W0)\n \n W = model.get_weights() # We'll save these so we can reset it later\n \n@@ -34,6 +35,9 @@ W = model.get_weights() # We'll save these so we can reset it later\n Xmask0 = X.copy()\n Xmask0[:,0] = 0\n \n+Xmask1 = X.copy()\n+Xmask1[:,1] = 0\n+\n X0_onehot = np.zeros((X.shape[0], 4))\n X1_onehot = np.zeros((X.shape[0], 4))\n for i, row in enumerate(X):\n@@ -44,39 +48,58 @@ for i, row in enumerate(X):\n # we should not do better than this when we mask out the part of the input\n # that gives us the correct answer\n uniform_score = np.log(4)\n+batch_size=512\n \n # Train it to guess 0th dim\n-model.fit(X, X0_onehot, nb_epoch=1)\n-score = model.evaluate(X, X0_onehot)\n+model.fit(X, X0_onehot, nb_epoch=1, batch_size=batch_size)\n+score = model.evaluate(X, X0_onehot, batch_size=batch_size)\n if score > uniform_score*0.9:\n     raise Exception('Failed to learn to copy timestep 0, score %f' % score)\n \n-if model.layers[0].W.get_value()[0,:] != default_mask_val:\n-    raise Exception(\"After training, the W of the Embedding's mask value changed to: \",\n-            model.layers[0].W.get_value()[0,:]\n+W0 = model.layers[0].W.get_value()[0,:]\n+if (W0 != default_mask_val).any():\n+    raise Exception(\"After training, the W0 of the Embedding's mask value changed to: \",\n+            W0)\n     \n \n model.set_weights(W)\n \n # Train without showing it the 0th dim to learn 1st dim\n-model.fit(X[:,1:], X1_onehot, nb_epoch=1)\n-score = model.evaluate(X[:,1:], X1_onehot)\n+model.fit(X[:,1:], X1_onehot, nb_epoch=1, batch_size=batch_size)\n+score = model.evaluate(X[:,1:], X1_onehot, batch_size=batch_size)\n if score > uniform_score*0.9:\n     raise Exception('Failed to learn to copy timestep 1, score %f' % score)\n \n model.set_weights(W)\n \n # Train to guess 0th dim when 0th dim has been masked (should fail)\n-model.fit(Xmask0, X0_onehot, nb_epoch=1)\n-score = model.evaluate(Xmask0, X0_onehot)\n+model.fit(Xmask0, X0_onehot, nb_epoch=1, batch_size=batch_size)\n+score = model.evaluate(Xmask0, X0_onehot, batch_size=batch_size)\n if score < uniform_score*0.9:\n    raise Exception('Somehow learned to copy timestep 0 despite mask, score %f' % score)\n \n model.set_weights(W)\n \n # Train to guess 1st dim when 0th dim has been masked (should succeed)\n-model.fit(Xmask0, X1_onehot, nb_epoch=1)\n-score = model.evaluate(Xmask0, X1_onehot)\n+model.fit(Xmask0, X1_onehot, nb_epoch=1, batch_size=batch_size)\n+score = model.evaluate(Xmask0, X1_onehot, batch_size=batch_size)\n if score > uniform_score*0.9:\n     raise Exception('Failed to learn to copy timestep 1 in masked model, score %f' % score)\n \n+\n+\n+model.set_weights(W)\n+\n+# Finally, make sure the mask is actually blocking input, mask out timestep 1, and see if\n+# it can learn timestep 0 (should fail)\n+model.fit(Xmask1, X0_onehot, nb_epoch=1, batch_size=batch_size)\n+\n+W0 = model.layers[0].W.get_value()[0,:]\n+if (W0 != default_mask_val).any():\n+    raise Exception(\"After masked training, the W0 of the Embedding's mask value changed to: \",\n+            W0)\n+\n+score = model.evaluate(Xmask1, X0_onehot, batch_size=batch_size)\n+if score < uniform_score*0.9:\n+    raise Exception('Somehow learned to copy timestep 0 despite masking 1, score %f' % score)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4f0f8b3948eb6e994dda8253df959857a0681f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 1333 | Contributors (this commit): 16 | Commits (past 90d): 58 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -41,18 +41,18 @@ def slice_X(X, start=None, stop=None):\n         else:\n             return X[start:stop]\n \n-def standardize_instance_weights(instance_weight):\n-    if isinstance(instance_weight, list):\n-        w = np.array(instance_weight)\n-    elif isinstance(instance_weight, np.ndarray):\n-        w = instance_weight\n+def standardize_sample_weights(sample_weight):\n+    if isinstance(sample_weight, list):\n+        w = np.array(sample_weight)\n+    elif isinstance(sample_weight, np.ndarray):\n+        w = sample_weight\n     else:\n         w = None\n     return w\n \n-def calculate_loss_weights(Y, instance_weight=None, class_weight=None):\n-    if instance_weight is not None:\n-        w = instance_weight\n+def calculate_loss_weights(Y, sample_weight=None, class_weight=None):\n+    if sample_weight is not None:\n+        w = sample_weight\n     elif isinstance(class_weight, dict):\n         if Y.shape[1] > 1:\n             y_classes = Y.argmax(axis=1)\n@@ -121,12 +121,12 @@ class Model(object):\n             allow_input_downcast=True, mode=theano_mode)\n \n \n-    def train(self, X, y, accuracy=False, instance_weight=None, class_weight=None):\n+    def train(self, X, y, accuracy=False, sample_weight=None, class_weight=None):\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n-        instance_weight = standardize_instance_weights(instance_weight)\n-        w = calculate_loss_weights(y, instance_weight=instance_weight, class_weight=class_weight)\n+        sample_weight = standardize_sample_weights(sample_weight)\n+        w = calculate_loss_weights(y, sample_weight=sample_weight, class_weight=class_weight)\n \n         ins = X + [y, w]\n         if accuracy:\n@@ -147,12 +147,12 @@ class Model(object):\n \n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True, show_accuracy=False,\n-            instance_weight=None, class_weight=None):\n+            sample_weight=None, class_weight=None):\n \n         X = standardize_X(X)\n         y = standardize_y(y)\n \n-        instance_weight = standardize_instance_weights(instance_weight)\n+        sample_weight = standardize_sample_weights(sample_weight)\n \n         do_validation = False\n         if validation_data:\n@@ -207,8 +207,8 @@ class Model(object):\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n \n-                if instance_weight is not None:\n-                    w = calculate_loss_weights(y_batch, instance_weight=instance_weight[batch_ids])\n+                if sample_weight is not None:\n+                    w = calculate_loss_weights(y_batch, sample_weight=sample_weight[batch_ids])\n                 else:\n                     w = calculate_loss_weights(y_batch, class_weight=class_weight)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1b9f35f535364c12eea2fe50ba937f1a22d38a70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 117 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 119 | Churn Cumulative: 119 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -85,8 +85,6 @@ for nb in range(nb_classes):\n batch_size = 64\n nb_epoch = 10\n \n-np.random.seed(1337) # for reproducibility\n-\n def generateData(n_samples, n_dim):\n     A_feats = np.random.randn(n_samples, n_dim)\n     B_feats = np.random.randn(n_samples, n_dim)\n@@ -162,3 +160,120 @@ neg_preds, pos_preds = (1.0*np.sum(res == -1)/res.shape[0], 1.0*np.sum(res == 1)\n assert(neg_preds > pos_preds)\n print(\"sqr hinge: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n \n+\n+############################\n+# sample weight test cases #\n+############################\n+\n+batch_size = 128\n+nb_epoch = 15\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape(60000,784)[:max_train_samples]\n+X_test = X_test.reshape(10000,784)[:max_test_samples]\n+X_train = X_train.astype(\"float32\")\n+X_test = X_test.astype(\"float32\")\n+X_train /= 255\n+X_test /= 255\n+\n+# convert class vectors to binary class matrices\n+y_train = y_train[:max_train_samples]\n+y_test = y_test[:max_test_samples]\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+\n+print(\"Sample weight test cases\")\n+\n+# categorical crossentropy\n+model_sampleweights_fit = createMNISTModel()\n+model_sampleweights_train = createMNISTModel()\n+model_fit = createMNISTModel()\n+model_train = createMNISTModel()\n+\n+sample_weight = np.ones((Y_train.shape[0]))\n+sample_weight[y_train == 9] = high_weight\n+\n+model_sampleweights_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test), sample_weight=sample_weight)\n+model_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test))\n+model_sampleweights_train.train(X_train, Y_train, sample_weight=sample_weight)\n+model_train.train(X_train, Y_train)\n+\n+print('MNIST Classification accuracies on test set for fitted models:')\n+for nb in range(nb_classes):\n+    testIdcs = np.where(y_test == np.array(nb))[0]\n+    X_temp = X_test[testIdcs, :]\n+    Y_temp = Y_test[testIdcs,:]\n+    # eval model which was trained with fit()\n+    score_sw = model_sampleweights_fit.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n+    score = model_fit.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n+    # eval model which was trained with train()\n+    score_sw_train = model_sampleweights_train.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n+    score_train = model_train.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n+    # print test accuracies for class weighted model vs. uniform weights\n+    print(\"Digit %d: sample_weight = %d -> %.3f \\t sample_weight = %d -> %.3f\" % (nb, 100 if nb == 9 else 1, score_sw[1], 1, score[1]))\n+    if nb == 9 and (score_sw[1] <= score[1] or score_sw_train[1] <= score_train[1]):\n+        raise Exception('Sample weights are not implemented correctly')\n+\n+\n+n_dim = 100\n+X_train, y_train = generateData(1000, n_dim)\n+X_test, y_test = generateData(5000, n_dim)\n+\n+y_train[y_train == -1] = 0\n+y_test[y_test == -1] = 0\n+\n+sample_weight = np.ones((y_train.shape[0]))\n+sample_weight[y_train == 0] = 1.5\n+\n+# binary crossentropy\n+model = createModel('binary_crossentropy', n_dim)\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), sample_weight=sample_weight)\n+res = model.predict(X_test, verbose=verbosity).round()\n+neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n+assert(neg_preds > pos_preds)\n+print(\"binary crossentropy: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n+\n+# MAE\n+model = createModel('mean_absolute_error', n_dim)\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), sample_weight=sample_weight)\n+res = model.predict(X_test, verbose=verbosity).round()\n+neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n+assert(neg_preds > pos_preds)\n+print(\"MAE: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n+\n+# MSE\n+model = createModel('mean_squared_error', n_dim)\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), sample_weight=sample_weight)\n+res = model.predict(X_test, verbose=verbosity).round()\n+neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n+assert(neg_preds > pos_preds)\n+print(\"MSE: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n+\n+# hinge losses, map labels\n+y_train[y_train == 0] = -1\n+y_test[y_test == 0] = -1\n+sample_weight = np.ones((y_train.shape[0]))\n+sample_weight[y_train == -1] = 1.5\n+\n+# hinge\n+model = createModel('hinge', n_dim, \"tanh\")\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), sample_weight=sample_weight)\n+res = model.predict(X_test, verbose=verbosity)\n+res[res < 0] = -1\n+res[res >= 0] = 1\n+neg_preds, pos_preds = (1.0*np.sum(res == -1)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n+assert(neg_preds > pos_preds)\n+print(\"hinge: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n+\n+# squared hinge\n+model = createModel('squared_hinge', n_dim, \"tanh\")\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), sample_weight=sample_weight)\n+res = model.predict(X_test, verbose=verbosity)\n+res[res < 0] = -1\n+res[res >= 0] = 1\n+neg_preds, pos_preds = (1.0*np.sum(res == -1)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n+assert(neg_preds > pos_preds)\n+print(\"sqr hinge: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7278db105dc3ae1d4c1e38b497e54c863a7836e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 26 | Churn Cumulative: 162 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,24 @@ from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix\n from ..layers.core import Layer, default_mask_val\n from six.moves import range\n \n+def get_mask(X, mask_val, steps_back=0):\n+    '''\n+        Given X, a (timesteps, nb_samples, n_dimensions) tensor, returns a mask\n+        tensor  with dimension (timesteps + steps_back, nb_samples, 1).  This\n+        matrix is left-padded with `steps_back` zeros in the time dimension, and\n+        elsewhere has a 1 for every entry except for those corresponding to a\n+        vector in X that has every entry equal to mask_val.\n+    '''\n+    mask = T.neq(X, mask_val).sum(axis=2) > 0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n+    mask = T.addbroadcast(mask[:, :, np.newaxis], 2) # (time, nb_samples, 1) matrix.\n+    if steps_back > 0:\n+        # left-pad in time with 0\n+        pad = alloc_zeros_matrix(steps_back, mask.shape[1], 1).astype('uint8')\n+        mask = T.concatenate([pad, mask], axis=0)\n+    return mask\n+\n+    \n+\n class SimpleRNN(Layer):\n     '''\n         Fully connected RNN where output is to fed back to input.\n@@ -53,11 +71,7 @@ class SimpleRNN(Layer):\n         # new shape: (time, nb_samples, input_dim) -> because theano.scan iterates over main dimension\n         X = X.dimshuffle((1,0,2)) \n \n-        mask = T.neq(X, self.mask_val).sum(axis=2) > 0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n-        mask = T.addbroadcast(mask[:, :, np.newaxis], 2)\n-\n-        mask_tm1 = alloc_zeros_matrix(*mask.shape).astype('int8')\n-        mask_tm1 = T.addbroadcast(T.set_subtensor(mask_tm1[1:, :, :], mask[:-1, :, :]), 2)\n+        mask = get_mask(X, self.mask_val, steps_back=1)\n \n         x = T.dot(X, self.W) + self.b\n         \n@@ -66,7 +80,7 @@ class SimpleRNN(Layer):\n         # Iterate over the first dimension of the x array (=time).\n         outputs, updates = theano.scan(\n             self._step, # this will be called with arguments (sequences[i], outputs[i-1], non_sequences[i])\n-            sequences=[x, mask, mask_tm1], # tensors to iterate over, inputs to _step\n+            sequences=[x, dict(input=mask,taps=[0, -1])], # tensors to iterate over, inputs to _step\n             # initialization of the output. Input to _step with default tap=-1.\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=self.U, # static inputs to _step\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
