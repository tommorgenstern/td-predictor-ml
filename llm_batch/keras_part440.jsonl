{"custom_id": "keras#231d90f9a4903b4bfebc8dd356ace341121b8054", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 125 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -73,10 +73,11 @@ def split_dataset(dataset,\n   if shuffle: \n     if seed:\n       np.random.seed(seed)\n-    np.random.shuffle(dataset)\n+    np.random.shuffle(dataset_as_list)\n     \n   total_size = len(dataset_as_list)\n    \n+  # if both left_size and right_size are None, raise error\n   if right_size is None and left_size is None:\n     raise ValueError('Both `left_size` and `right_size`cannot be `None`'\n                      ' either one of them must specified')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#68fef62d2788cea69f75101447af2bcb3405186c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 865 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -331,7 +331,7 @@ def EfficientNet(\n     # normalize the input, we need to divide another sqrt(var) to match the\n     # original implementation.\n     # See https://github.com/tensorflow/tensorflow/issues/49930 for more details\n-    x = x / tf.math.sqrt(IMAGENET_STDDEV_RGB)\n+    x = layers.Rescaling(1. / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x)\n \n   x = layers.ZeroPadding2D(\n       padding=imagenet_utils.correct_pad(x, 3),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#59b6c70efc09e3119795b321fa46470b4f64c8f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 15 | Churn Cumulative: 8620 | Contributors (this commit): 14 | Commits (past 90d): 11 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4023,7 +4023,7 @@ def batch_set_value(tuples):\n       tuples: a list of tuples `(tensor, value)`.\n           `value` should be a Numpy array.\n   \"\"\"\n-  if tf.compat.v1.executing_eagerly_outside_functions():\n+  if tf.executing_eagerly() or tf.inside_function():\n     for x, value in tuples:\n       x.assign(np.asarray(value, dtype=dtype_numpy(x)))\n   else:\n\n@@ -2028,6 +2028,19 @@ class ResetStatesTest(test_combinations.TestCase):\n     finally:\n       backend.set_floatx('float32')\n \n+  def test_function_wrapped_reset_state(self):\n+    m = metrics.Mean(name='my_mean')\n+\n+    # check reset_state in function.\n+    @tf.function\n+    def reset_in_fn():\n+      m.reset_state()\n+      return m.update_state(100)\n+\n+    for _ in range(5):\n+      self.evaluate(reset_in_fn())\n+    self.assertEqual(self.evaluate(m.count), 1)\n+\n \n @test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MergeStateTest(test_combinations.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00568ab1a275c02c9770b6510d16b2a89ef74f17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 969 | Contributors (this commit): 1 | Commits (past 90d): 17 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,7 +20,8 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n \n-from keras.utils import dataset_utils\n+# from keras.utils import dataset_utils\n+import dataset_utils\n from tensorflow.python.util.tf_export import keras_export\n \n try:\n@@ -101,6 +102,7 @@ def audio_dataset_from_directory(\n       sampling_rate: Number of samples taken each second.\n       output_sequence_length: Maximum length of a audio sequence. audio files longer than this will \n         be truncated to `output_sequence_length`\n+        If set to `None` then shorter sequences will be padded by the longest sequence in the current batch\n       ragged: Whether to return a ragged dataset if the sequence length is not the same. Default: False.\n           If set to False, `output_sequence_length` should have a value.\n       shuffle: Whether to shuffle the data. Default: True.\n@@ -152,11 +154,7 @@ def audio_dataset_from_directory(\n             f'`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", or None. Received: label_mode={label_mode}'\n         )\n \n-    if not ragged and output_sequence_length is None:\n-        raise ValueError(\n-            f\"The dataset should be ragged dataset or fixed sequence length dataset. Received: ragged={ragged} and output_sequence_length={output_sequence_length}\"\n-        )\n-    elif ragged and output_sequence_length is not None:\n+    if ragged and output_sequence_length is not None:\n         raise ValueError(\"Cannot set both `ragged` and `output_sequence_length`\")\n \n     if sampling_rate is not None:\n@@ -172,7 +170,7 @@ def audio_dataset_from_directory(\n \n         if tfio is None:\n             raise ImportError(\n-                \"To use the argument `sampling_rate`, you should install tensorflow_io. You can install it via pip install tensorflow-io.\"\n+                \"To use the argument `sampling_rate`, you should install tensorflow_io. You can install it via `pip install tensorflow-io`.\"\n             )\n \n     if labels is None or label_mode is None:\n@@ -219,6 +217,8 @@ def audio_dataset_from_directory(\n             shuffle=shuffle,\n             seed=seed,\n             class_names=class_names,\n+            output_sequence_length=output_sequence_length,\n+            ragged=ragged,\n         )\n         val_dataset = prepare_dataset(\n             dataset=val_dataset,\n@@ -226,6 +226,8 @@ def audio_dataset_from_directory(\n             shuffle=False,\n             seed=seed,\n             class_names=class_names,\n+            output_sequence_length=output_sequence_length,\n+            ragged=ragged,\n         )\n         return train_dataset, val_dataset\n \n@@ -249,15 +251,23 @@ def audio_dataset_from_directory(\n             shuffle=shuffle,\n             seed=seed,\n             class_names=class_names,\n+            output_sequence_length=output_sequence_length,\n+            ragged=ragged,\n         )\n         return dataset\n \n \n-def prepare_dataset(dataset, batch_size, shuffle, seed, class_names):\n+def prepare_dataset(\n+    dataset, batch_size, shuffle, seed, class_names, output_sequence_length, ragged\n+):\n     dataset = dataset.prefetch(tf.data.AUTOTUNE)\n     if batch_size is not None:\n         if shuffle:\n             dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n+\n+        if output_sequence_length is None and not ragged:\n+            dataset = dataset.padded_batch(batch_size, padded_shapes=([None, None], []))\n+        else:\n             dataset = dataset.batch(batch_size)\n     else:\n         if shuffle:\n@@ -399,3 +409,9 @@ def paths_and_labels_to_dataset(\n         label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n         audio_ds = tf.data.Dataset.zip((audio_ds, label_ds))\n     return audio_ds\n+\n+\n+ds = audio_dataset_from_directory(\"./mini_speech_commands\", ragged=True)\n+\n+for s, l in ds.take(1000):\n+    print(s.shape, l.shape)\n\n@@ -1,4 +1,4 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#15b01be02f91f73aaa685365c3fd7ee315fc9d40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 617 | Contributors (this commit): 1 | Commits (past 90d): 15 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -409,9 +409,3 @@ def paths_and_labels_to_dataset(\n         label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n         audio_ds = tf.data.Dataset.zip((audio_ds, label_ds))\n     return audio_ds\n\\ No newline at end of file\n-\n-\n-ds = audio_dataset_from_directory(\"./mini_speech_commands\", ragged=True)\n-\n-for s, l in ds.take(1000):\n-    print(s.shape, l.shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2344cb344af18e290619ac980376ea6272111c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 620 | Contributors (this commit): 1 | Commits (past 90d): 16 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -20,8 +20,7 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n \n-# from keras.utils import dataset_utils\n-import dataset_utils\n+from keras.utils import dataset_utils\n from tensorflow.python.util.tf_export import keras_export\n \n try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7fbf7d348f23dfdd87a5451e2c1ca826aacc4d32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 621 | Contributors (this commit): 1 | Commits (past 90d): 17 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -103,7 +103,6 @@ def audio_dataset_from_directory(\n         be truncated to `output_sequence_length`\n         If set to `None` then shorter sequences will be padded by the longest sequence in the current batch\n       ragged: Whether to return a ragged dataset if the sequence length is not the same. Default: False.\n-          If set to False, `output_sequence_length` should have a value.\n       shuffle: Whether to shuffle the data. Default: True.\n           If set to False, sorts the data in alphanumeric order.\n       seed: Optional random seed for shuffling and transformations.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ace1d5b0ae37264e3b5e5c6db7f6356de0c43a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 625 | Contributors (this commit): 1 | Commits (past 90d): 18 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -100,8 +100,8 @@ def audio_dataset_from_directory(\n         (the dataset will yield individual samples).\n       sampling_rate: Number of samples taken each second.\n       output_sequence_length: Maximum length of a audio sequence. audio files longer than this will \n-        be truncated to `output_sequence_length`\n-        If set to `None` then shorter sequences will be padded by the longest sequence in the current batch\n+        be truncated to `output_sequence_length`.\n+        If set to `None` then shorter sequences will be padded by the longest sequence in the current batch.\n       ragged: Whether to return a ragged dataset if the sequence length is not the same. Default: False.\n       shuffle: Whether to shuffle the data. Default: True.\n           If set to False, sorts the data in alphanumeric order.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#677726b9d967bbdc354bb3bf9c350d360e59ab36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 16918 | Contributors (this commit): 127 | Commits (past 90d): 28 | Contributors (cumulative): 127 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1851,9 +1851,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         outputs = step_function(self, iterator)\n         for _ in tf.range(self._steps_per_execution - 1):\n           tf.autograph.experimental.set_loop_options(\n-              shape_invariants=[(\n-                  t, tf_utils.get_tensor_spec(t, dynamic_batch=True).shape)\n-                                for t in tf.nest.flatten(outputs)])\n+              shape_invariants=[(outputs, tf.nest.map_structure(\n+                  lambda t: tf_utils.get_tensor_spec(\n+                      t, dynamic_batch=True).shape,\n+                  outputs))])\n           step_outputs = step_function(self, iterator)\n           outputs = tf.nest.map_structure(lambda t1, t2: concat([t1, t2]),\n                                           outputs, step_outputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04a11f365fb9c479e235b8b57280fc80892cd28c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 249 | Lines Deleted: 254 | Files Changed: 13 | Hunks: 49 | Methods Changed: 58 | Complexity Δ (Sum/Max): -16/36 | Churn Δ: 503 | Churn Cumulative: 44691 | Contributors (this commit): 164 | Commits (past 90d): 85 | Contributors (cumulative): 267 | DMM Complexity: 0.23255813953488372\n\nDIFF:\n@@ -21,7 +21,6 @@ import tensorflow.compat.v2 as tf\n \n import collections\n import contextlib\n-import copy\n import functools\n import itertools\n import textwrap\n@@ -932,7 +931,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # - input_spec compatibility is only checked against `inputs`\n     # - mixed precision casting (autocast) is only applied to `inputs`,\n     #   not to any other argument.\n-    inputs, args, kwargs = self._split_out_first_arg(args, kwargs)\n+    inputs, args, kwargs = self._call_spec.split_out_first_arg(args, kwargs)\n     input_list = tf.nest.flatten(inputs)\n \n     # Functional Model construction mode is invoked when `Layer`s are called on\n@@ -2279,8 +2278,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     training_value = None\n     training_arg_passed_by_framework = False\n     # Priority 1: `training` was explicitly passed a non-None value.\n-    if self._call_arg_was_passed('training', args, kwargs):\n-      training_value = self._get_call_arg_value('training', args, kwargs)\n+    if self._call_spec.arg_was_passed('training', args, kwargs):\n+      training_value = self._call_spec.get_arg_value('training', args, kwargs)\n       if not self._expects_training_arg:\n         kwargs.pop('training')\n \n@@ -2301,11 +2300,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       # in the `call` signature (or in inference mode if the `call` signature\n       # specifies no non-None default).\n       else:\n-        training_value = self._default_training_arg\n+        training_value = self._call_spec.default_training_arg\n       # In cases (2), (3), (4) the training argument is passed automatically\n       # by the framework, and will not be hard-coded into the model.\n       if self._expects_training_arg:\n-        args, kwargs = self._set_call_arg_value('training', training_value,\n+        args, kwargs = self._call_spec.set_arg_value('training', training_value,\n                                                      args, kwargs)\n         training_arg_passed_by_framework = True\n \n@@ -2320,7 +2319,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                          'Tensor or a list of Tensors, not None '\n                          '(layer: ' + self.name + ').')\n       if training_arg_passed_by_framework:\n-        args, kwargs = self._set_call_arg_value(\n+        args, kwargs = self._call_spec.set_arg_value(\n             'training', None, args, kwargs, pop_kwarg_if_none=True)\n       if mask_arg_passed_by_framework:\n         kwargs.pop('mask')\n@@ -2333,8 +2332,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     training_mode = None\n     if self._expects_training_arg:\n       # (1) `training` was passed to this `Layer.call`.\n-      if self._call_arg_was_passed('training', args, kwargs):\n-        training_mode = self._get_call_arg_value('training', args, kwargs)\n+      if self._call_spec.arg_was_passed('training', args, kwargs):\n+        training_mode = self._call_spec.get_arg_value('training', args, kwargs)\n       # If no `training` arg was passed, or `None` was explicitly passed,\n       # the framework will make a decision about the training mode is.\n       if training_mode is None:\n@@ -2356,11 +2355,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         # or treating the layer as if it is in inference if no non-None default\n         # is specified in the `call` signature.\n         else:\n-          training_mode = self._default_training_arg\n+          training_mode = self._call_spec.default_training_arg\n \n         # For case (2), (3), (4) `training` arg is passed by framework.\n-        args, kwargs = self._set_call_arg_value('training', training_mode, args,\n-                                                kwargs)\n+        args, kwargs = self._call_spec.set_arg_value('training', training_mode,\n+                                                     args, kwargs)\n     else:\n       if 'training' in kwargs:\n         # `training` was passed to this `Layer` but is not needed for\n@@ -2614,8 +2613,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       # or `compute_mask`.\n       input_masks = None\n       implicit_mask = False\n-    elif self._call_arg_was_passed('mask', args, kwargs):\n-      input_masks = self._get_call_arg_value('mask', args, kwargs)\n+    elif self._call_spec.arg_was_passed('mask', args, kwargs):\n+      input_masks = self._call_spec.get_arg_value('mask', args, kwargs)\n       implicit_mask = False\n     else:\n       input_masks = [getattr(t, '_keras_mask', None) for t in input_list]\n@@ -2628,47 +2627,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         implicit_mask = True\n     return input_masks, implicit_mask\n \n-  def _call_arg_was_passed(self, arg_name, args, kwargs, inputs_in_args=False):\n-    # Performance optimization: do no work in most common case.\n-    if not args and not kwargs:\n-      return False\n-\n-    if arg_name in kwargs:\n-      return True\n-    call_fn_args = self._call_fn_args\n-    if not inputs_in_args:\n-      # Ignore `inputs` arg.\n-      call_fn_args = call_fn_args[1:]\n-    return arg_name in dict(zip(call_fn_args, args))\n-\n-  def _get_call_arg_value(self, arg_name, args, kwargs, inputs_in_args=False):\n-    if arg_name in kwargs:\n-      return kwargs[arg_name]\n-    call_fn_args = self._call_fn_args\n-    if not inputs_in_args:\n-      # Ignore `inputs` arg.\n-      call_fn_args = call_fn_args[1:]\n-    args_dict = dict(zip(call_fn_args, args))\n-    return args_dict[arg_name]\n-\n-  def _set_call_arg_value(\n-      self, arg_name, new_value, args,\n-      kwargs, inputs_in_args=False, pop_kwarg_if_none=False):\n-    arg_pos = self._call_fn_arg_positions.get(arg_name, None)\n-    if arg_pos is not None:\n-      if not inputs_in_args:\n-        # Ignore `inputs` arg.\n-        arg_pos = arg_pos - 1\n-      if len(args) > arg_pos:\n-        args = list(args)\n-        args[arg_pos] = new_value\n-        return tuple(args), kwargs\n-    if new_value is None and pop_kwarg_if_none:\n-      kwargs.pop(arg_name, None)\n-    else:\n-      kwargs[arg_name] = new_value\n-    return args, kwargs\n-\n   def _set_connectivity_metadata(self, args, kwargs, outputs):\n     # If the layer returns tensors from its inputs unmodified,\n     # we copy them to avoid loss of KerasHistory metadata.\n@@ -2995,69 +2953,18 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     return True\n \n   def _init_call_fn_args(self, expects_training_arg=None):\n-    # Clear cached call function arguments.\n-    self.__class__._call_full_argspec.fget.cache.pop(self, None)\n-    self.__class__._call_fn_args.fget.cache.pop(self, None)\n-    self.__class__._call_accepts_kwargs.fget.cache.pop(self, None)\n-\n-    call_fn_args = self._call_fn_args\n-    call_fn_args += self._call_full_argspec.kwonlyargs or []\n-    if expects_training_arg is None:\n-      self._expects_training_arg = ('training' in call_fn_args or\n-                                    self._call_accepts_kwargs)\n-    else:\n-      # Use value encoded into the metadata when loading from the SavedModel.\n-      self._expects_training_arg = expects_training_arg\n-    # The default training arg will be any (non-None) default specified in the\n-    # method signature, or None if no value is specified.\n-    call_fn_arg_defaults = self._call_fn_arg_defaults.copy()\n-    call_fn_arg_defaults.update(self._call_full_argspec.kwonlydefaults or {})\n-    self._default_training_arg = call_fn_arg_defaults.get('training')\n-\n-    self._expects_mask_arg = ('mask' in call_fn_args or\n-                              self._call_accepts_kwargs)\n+    self._call_spec = layer_utils.CallFunctionSpec(self.call)\n+    if expects_training_arg is not None:\n+      self._call_spec.expects_training_arg = expects_training_arg\n \n   @property\n-  @layer_utils.cached_per_instance\n-  def _call_full_argspec(self):\n-    # Argspec inspection is expensive and the call spec is used often, so it\n-    # makes sense to cache the result.\n-    return tf_inspect.getfullargspec(self.call)\n+  def _expects_training_arg(self):\n+    \"\"\"Whether the call function uses 'training' as a parameter.\"\"\"\n+    return self._call_spec.expects_training_arg\n \n   @property\n-  @layer_utils.cached_per_instance\n-  def _call_fn_args(self):\n-    all_args = self._call_full_argspec.args\n-    # Scrub `self` that appears if a decorator was applied.\n-    if all_args and all_args[0] == 'self':\n-      return all_args[1:]\n-    return all_args\n-\n-  @property\n-  @layer_utils.cached_per_instance\n-  def _call_fn_arg_defaults(self):\n-    call_fn_args = self._call_fn_args\n-    call_fn_defaults = self._call_full_argspec.defaults or []\n-    defaults = dict()\n-\n-    # The call arg defaults are an n-tuple of the last n elements of the args\n-    # list. (n = # of elements that have a default argument)\n-    for i in range(-1 * len(call_fn_defaults), 0):\n-      defaults[call_fn_args[i]] = call_fn_defaults[i]\n-    return defaults\n-\n-  @property\n-  @layer_utils.cached_per_instance\n-  def _call_fn_arg_positions(self):\n-    call_fn_arg_positions = dict()\n-    for pos, arg in enumerate(self._call_fn_args):\n-      call_fn_arg_positions[arg] = pos\n-    return call_fn_arg_positions\n-\n-  @property\n-  @layer_utils.cached_per_instance\n-  def _call_accepts_kwargs(self):\n-    return self._call_full_argspec.varkw is not None\n+  def _expects_mask_arg(self):\n+    return self._call_spec.expects_mask_arg\n \n   @property\n   def _eager_losses(self):\n@@ -3086,21 +2993,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     return output\n \n-  def _split_out_first_arg(self, args, kwargs):\n-    # Grab the argument corresponding to the first argument in the\n-    # layer's `call` method spec. This will either be the first positional\n-    # argument, or it will be provided as a keyword argument.\n-    if args:\n-      inputs = args[0]\n-      args = args[1:]\n-    elif self._call_fn_args[0] in kwargs:\n-      kwargs = copy.copy(kwargs)\n-      inputs = kwargs.pop(self._call_fn_args[0])\n-    else:\n-      raise ValueError(\n-          'The first argument to `Layer.call` must always be passed.')\n-    return inputs, args, kwargs\n-\n   # SavedModel properties. Please see keras/saving/saved_model for details.\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n\n@@ -1285,10 +1285,10 @@ class NestedTrackingTest(tf.test.TestCase):\n         return x\n \n     nondefun_layer = NonDefunLayer()\n-    self.assertEqual(nondefun_layer._call_fn_args,\n+    self.assertEqual(nondefun_layer._call_spec.arg_names,\n                      ['inputs', 'a', 'mask', 'b', 'training'])\n     defun_layer = DefunLayer()\n-    self.assertEqual(defun_layer._call_fn_args,\n+    self.assertEqual(defun_layer._call_spec.arg_names,\n                      ['x', 'mask', 'a', 'training', 'b'])\n \n   def test_sequential_model(self):\n\n@@ -652,8 +652,8 @@ class Layer(base_layer.Layer):\n     if args:\n       inputs = args[0]\n       args = args[1:]\n-    elif self._call_fn_args[0] in kwargs:\n-      inputs = kwargs.pop(self._call_fn_args[0])\n+    elif self._call_spec.arg_names[0] in kwargs:\n+      inputs = kwargs.pop(self._call_spec.arg_names[0])\n     else:\n       raise ValueError(\n           'The first argument to `Layer.call` must always be passed.')\n@@ -685,7 +685,7 @@ class Layer(base_layer.Layer):\n     mask_arg_passed_by_framework = False\n     input_masks = self._collect_input_masks(inputs, args, kwargs)\n     if (self._expects_mask_arg and input_masks is not None and\n-        not self._call_arg_was_passed('mask', args, kwargs)):\n+        not self._call_spec.arg_was_passed('mask', args, kwargs)):\n       mask_arg_passed_by_framework = True\n       kwargs['mask'] = input_masks\n \n@@ -694,8 +694,8 @@ class Layer(base_layer.Layer):\n     training_value = None\n     training_arg_passed_by_framework = False\n     # Priority 1: `training` was explicitly passed.\n-    if self._call_arg_was_passed('training', args, kwargs):\n-      training_value = self._get_call_arg_value('training', args, kwargs)\n+    if self._call_spec.arg_was_passed('training', args, kwargs):\n+      training_value = self._call_spec.get_arg_value('training', args, kwargs)\n       if not self._expects_training_arg:\n         kwargs.pop('training')\n \n@@ -719,8 +719,8 @@ class Layer(base_layer.Layer):\n           training_value = tf.cast(training_value, tf.bool)\n         else:\n           training_value = bool(training_value)\n-        args, kwargs = self._set_call_arg_value(\n-            'training', training_value, args, kwargs)\n+        args, kwargs = self._call_spec.set_arg_value('training', training_value,\n+                                                     args, kwargs)\n         training_arg_passed_by_framework = True\n \n     # Only create Keras history if at least one tensor originates from a\n@@ -783,7 +783,7 @@ class Layer(base_layer.Layer):\n                              '(layer: ' + self.name + ').')\n           if base_layer_utils.have_all_keras_metadata(inputs):\n             if training_arg_passed_by_framework:\n-              args, kwargs = self._set_call_arg_value(\n+              args, kwargs = self._call_spec.set_arg_value(\n                   'training', None, args, kwargs, pop_kwarg_if_none=True)\n             if mask_arg_passed_by_framework:\n               kwargs.pop('mask')\n@@ -1921,8 +1921,8 @@ class Layer(base_layer.Layer):\n \n   def _collect_input_masks(self, inputs, args, kwargs):\n     \"\"\"Checks if `mask` argument was passed, else gathers mask from inputs.\"\"\"\n-    if self._call_arg_was_passed('mask', args, kwargs):\n-      return self._get_call_arg_value('mask', args, kwargs)\n+    if self._call_spec.arg_was_passed('mask', args, kwargs):\n+      return self._call_spec.get_arg_value('mask', args, kwargs)\n \n     if not self._should_compute_mask:\n       return None\n@@ -1933,45 +1933,6 @@ class Layer(base_layer.Layer):\n       return None\n     return input_masks\n \n-  def _call_arg_was_passed(self, arg_name, args, kwargs, inputs_in_args=False):\n-    if arg_name in kwargs:\n-      return True\n-    call_fn_args = self._call_fn_args\n-    if not inputs_in_args:\n-      # Ignore `inputs` arg.\n-      call_fn_args = call_fn_args[1:]\n-    if arg_name in dict(zip(call_fn_args, args)):\n-      return True\n-    return False\n-\n-  def _get_call_arg_value(self, arg_name, args, kwargs, inputs_in_args=False):\n-    if arg_name in kwargs:\n-      return kwargs[arg_name]\n-    call_fn_args = self._call_fn_args\n-    if not inputs_in_args:\n-      # Ignore `inputs` arg.\n-      call_fn_args = call_fn_args[1:]\n-    args_dict = dict(zip(call_fn_args, args))\n-    return args_dict[arg_name]\n-\n-  def _set_call_arg_value(\n-      self, arg_name, new_value, args,\n-      kwargs, inputs_in_args=False, pop_kwarg_if_none=False):\n-    arg_pos = self._call_fn_arg_positions.get(arg_name, None)\n-    if arg_pos is not None:\n-      if not inputs_in_args:\n-        # Ignore `inputs` arg.\n-        arg_pos = arg_pos - 1\n-      if len(args) > arg_pos:\n-        args = list(args)\n-        args[arg_pos] = new_value\n-        return args, kwargs\n-    if new_value is None and pop_kwarg_if_none:\n-      kwargs.pop(arg_name, None)\n-    else:\n-      kwargs[arg_name] = new_value\n-    return args, kwargs\n-\n   def _get_node_attribute_at_index(self, node_index, attr, attr_name):\n     \"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\n \n@@ -2223,55 +2184,10 @@ class Layer(base_layer.Layer):\n   def _is_layer(self):\n     return True\n \n-  def _init_call_fn_args(self, expects_training_arg=None):\n-    # Clear cached call function arguments.\n-    self.__class__._call_full_argspec.fget.cache.pop(self, None)\n-    self.__class__._call_fn_args.fget.cache.pop(self, None)\n-    self.__class__._call_accepts_kwargs.fget.cache.pop(self, None)\n-\n-    call_fn_args = self._call_fn_args\n-    if expects_training_arg is None:\n-      self._expects_training_arg = ('training' in call_fn_args or\n-                                    self._call_accepts_kwargs)\n-    else:\n-      # Use value encoded into the metadata when loading from the SavedModel.\n-      self._expects_training_arg = expects_training_arg\n-    self._expects_mask_arg = ('mask' in call_fn_args or\n-                              self._call_accepts_kwargs)\n-\n-  @property\n-  @layer_utils.cached_per_instance\n-  def _call_full_argspec(self):\n-    # Argspec inspection is expensive and the call spec is used often, so it\n-    # makes sense to cache the result.\n-    return tf_inspect.getfullargspec(self.call)\n-\n-  @property\n-  @layer_utils.cached_per_instance\n-  def _call_fn_args(self):\n-    all_args = self._call_full_argspec.args\n-    # Scrub `self` that appears if a decorator was applied.\n-    if all_args and all_args[0] == 'self':\n-      return all_args[1:]\n-    return all_args\n-\n-  @property\n-  @layer_utils.cached_per_instance\n-  def _call_fn_arg_positions(self):\n-    call_fn_arg_positions = dict()\n-    for pos, arg in enumerate(self._call_fn_args):\n-      call_fn_arg_positions[arg] = pos\n-    return call_fn_arg_positions\n-\n-  @property\n-  @layer_utils.cached_per_instance\n-  def _call_accepts_kwargs(self):\n-    return self._call_full_argspec.varkw is not None\n-\n   @property\n   @layer_utils.cached_per_instance\n   def _should_compute_mask(self):\n-    return ('mask' in self._call_fn_args or\n+    return ('mask' in self._call_spec.arg_names or\n             getattr(self, 'compute_mask', None) is not None)\n \n   def _dedup_weights(self, weights):\n\n@@ -192,8 +192,8 @@ class Functional(training_lib.Model):\n     self._compute_output_and_mask_jointly = True\n     # `_expects_training_arg` is True since the `training` argument is always\n     # present in the signature of the `call` method of a graph network.\n-    self._expects_training_arg = True\n-    self._expects_mask_arg = True\n+    self._call_spec.expects_training_arg = True\n+    self._call_spec.expects_mask_arg = True\n     # A graph network does not autocast inputs, as its layers will cast them\n     # instead.\n     self._autocast = False\n@@ -1463,9 +1463,10 @@ class ModuleWrapper(base_layer.Layer):\n     # Check if module.__call__ has a `training` arg or accepts `**kwargs`.\n     method = getattr(module, method_name)\n     method_arg_spec = tf_inspect.getfullargspec(method)\n-    self._expects_training_arg = ('training' in method_arg_spec.args or\n+    self._call_spec.expects_training_arg = ('training' in method_arg_spec.args\n+                                            or\n                                             method_arg_spec.varkw is not None)\n-    self._expects_mask_arg = ('mask' in method_arg_spec.args or\n+    self._call_spec.expects_mask_arg = ('mask' in method_arg_spec.args or\n                                         method_arg_spec.varkw is not None)\n \n   def call(self, *args, **kwargs):\n\n@@ -174,10 +174,11 @@ class Node:\n     \"\"\"Serializes `Node` for Functional API's `get_config`.\"\"\"\n     # Serialization still special-cases first argument.\n     args, kwargs = self.call_args, self.call_kwargs\n-    inputs, args, kwargs = self.layer._split_out_first_arg(args, kwargs)\n+    inputs, args, kwargs = self.layer._call_spec.split_out_first_arg(\n+        args, kwargs)\n \n     # Treat everything other than first argument as a kwarg.\n-    arguments = dict(zip(self.layer._call_fn_args[1:], args))\n+    arguments = dict(zip(self.layer._call_spec.arg_names[1:], args))\n     arguments.update(kwargs)\n     kwargs = arguments\n \n\n@@ -417,7 +417,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           x = base_layer_utils.generate_placeholders_from_shape(input_shape)\n \n         kwargs = {}\n-        call_signature = self._call_full_argspec\n+        call_signature = self._call_spec.full_argspec\n         call_args = call_signature.args\n         # Exclude `self`, `inputs`, and any argument with a default value.\n         if len(call_args) > 2:\n@@ -466,7 +466,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       copied_args = copy.copy(args)\n       copied_kwargs = copy.copy(kwargs)\n \n-      inputs, copied_args, copied_kwargs = self._split_out_first_arg(\n+      inputs, copied_args, copied_kwargs = self._call_spec.split_out_first_arg(\n           copied_args, copied_kwargs)\n \n       def _convert_to_graph_inputs(x):\n@@ -3032,7 +3032,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   def _check_call_args(self, method_name):\n     \"\"\"Check that `call()` has only one positional arg.\"\"\"\n     # Always allow first arg, regardless of arg name.\n-    fullargspec = self._call_full_argspec\n+    fullargspec = self._call_spec.full_argspec\n     if fullargspec.defaults:\n       positional_args = fullargspec.args[:-len(fullargspec.defaults)]\n     else:\n\n@@ -1306,7 +1306,7 @@ class Model(training_lib.Model):\n   def _check_call_args(self, method_name):\n     \"\"\"Check that `call` has only one positional arg.\"\"\"\n     # Always allow first arg, regardless of arg name.\n-    fullargspec = self._call_full_argspec\n+    fullargspec = self._call_spec.full_argspec\n     if fullargspec.defaults:\n       positional_args = fullargspec.args[:-len(fullargspec.defaults)]\n     else:\n\n@@ -63,8 +63,8 @@ class ClassMethod(Layer):\n     # (e.g., don't unnest lists that contain one element)\n     self._preserve_input_structure_in_config = True\n \n-    self._expects_training_arg = False\n-    self._expects_mask_arg = False\n+    self._call_spec.expects_training_arg = False\n+    self._call_spec.expects_mask_arg = False\n \n   def call(self, args, kwargs):\n     return getattr(self.cls_ref, self.method_name)(*args, **kwargs)\n@@ -240,8 +240,8 @@ class TFOpLambda(Layer):\n     # Warning on every invocation will be quite irksome in Eager mode.\n     self._already_warned = False\n \n-    self._expects_training_arg = False\n-    self._expects_mask_arg = False\n+    self._call_spec.expects_training_arg = False\n+    self._call_spec.expects_mask_arg = False\n \n   def _call_wrapper(self, *args, **kwargs):\n     created_variables = []\n\n@@ -48,9 +48,9 @@ class _RNNCellWrapper(AbstractRNNCell):\n     super(_RNNCellWrapper, self).__init__(*args, **kwargs)\n     self.cell = cell\n     cell_call_spec = tf_inspect.getfullargspec(cell.call)\n-    self._expects_training_arg = (\"training\" in cell_call_spec.args) or (\n-        cell_call_spec.varkw is not None\n-    )\n+    self._call_spec.expects_training_arg = ((\"training\"\n+                                             in cell_call_spec.args) or\n+                                            (cell_call_spec.varkw is not None))\n \n   def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n     \"\"\"Calls the wrapped cell and performs the wrapping logic.\n\n@@ -228,7 +228,7 @@ class Layer(base_layer.Layer):\n     else:\n       self._keras_style = False\n \n-    self._call_has_scope_arg = 'scope' in self._call_fn_args\n+    self._call_has_scope_arg = 'scope' in self._call_spec.arg_names\n     if scope:\n       with tf.compat.v1.variable_scope(scope) as captured_scope:\n         self._scope = captured_scope\n@@ -559,8 +559,8 @@ class Layer(base_layer.Layer):\n       try:\n         call_has_scope_arg = self._call_has_scope_arg\n       except AttributeError:\n-        self._call_fn_args = variable_scope_shim.fn_args(self.call)\n-        self._call_has_scope_arg = 'scope' in self._call_fn_args\n+        self._call_spec.arg_names = variable_scope_shim.fn_args(self.call)\n+        self._call_has_scope_arg = 'scope' in self._call_spec.arg_names\n         call_has_scope_arg = self._call_has_scope_arg\n       if call_has_scope_arg:\n         kwargs['scope'] = scope\n\n@@ -1018,7 +1018,8 @@ class RevivedLayer:\n \n     with utils.no_automatic_dependency_tracking_scope(revived_obj):\n       # pylint:disable=protected-access\n-      revived_obj._expects_training_arg = metadata['expects_training_arg']\n+      revived_obj._call_spec.expects_training_arg = metadata[\n+          'expects_training_arg']\n       config = metadata.get('config')\n       if generic_utils.validate_config(config):\n         revived_obj._config = config\n@@ -1165,7 +1166,8 @@ class RevivedNetwork(RevivedLayer):\n     # \"keras_api\" for keras-specific attributes.\n     with utils.no_automatic_dependency_tracking_scope(revived_obj):\n       # pylint:disable=protected-access\n-      revived_obj._expects_training_arg = metadata['expects_training_arg']\n+      revived_obj._call_spec.expects_training_arg = metadata[\n+          'expects_training_arg']\n       config = metadata.get('config')\n       if generic_utils.validate_config(config):\n         revived_obj._config = config\n\n@@ -480,7 +480,7 @@ class LayerCallCollection:\n       return (utils.get_training_arg(self._training_arg_index, args, kwargs)\n               is not None)\n     else:\n-      return self.layer._call_arg_was_passed(  # pylint: disable=protected-access\n+      return self.layer._call_spec.arg_was_passed(  # pylint: disable=protected-access\n           'training',\n           args,\n           kwargs,\n@@ -490,14 +490,14 @@ class LayerCallCollection:\n     if not self.layer._expects_training_arg and self._expects_training_arg:  # pylint: disable=protected-access\n       return utils.get_training_arg(self._training_arg_index, args, kwargs)\n     else:\n-      return self.layer._get_call_arg_value(  # pylint: disable=protected-access\n+      return self.layer._call_spec.get_arg_value(  # pylint: disable=protected-access\n           'training',\n           args,\n           kwargs,\n           inputs_in_args=True)\n \n   def get_input_arg_value(self, args, kwargs):\n-    return self.layer._get_call_arg_value(  # pylint: disable=protected-access\n+    return self.layer._call_spec.get_arg_value(  # pylint: disable=protected-access\n         self._input_arg_name,\n         args,\n         kwargs,\n\n@@ -15,14 +15,16 @@\n # pylint: disable=protected-access\n \"\"\"Utilities related to layer/model functionality.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n+import copy\n import functools\n import weakref\n \n from keras.utils import io_utils\n+from keras.utils import tf_inspect\n import numpy as np\n-from tensorflow.python.util.tf_export import keras_export\n+\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n \n \n @keras_export('keras.utils.get_source_inputs')\n@@ -511,3 +513,184 @@ def filter_empty_layer_containers(layer_list):\n       # Trackable data structures will not show up in \".layers\" lists, but\n       # the layers they contain will.\n       to_visit.extend(sub_layers[::-1])\n+\n+\n+class CallFunctionSpec:\n+  \"\"\"Caches the spec and provides utilities for handling call function args.\"\"\"\n+\n+  def __init__(self, fn):\n+    \"\"\"Initialies a `CallFunctionSpec`.\n+\n+    Args:\n+      fn: the call function of a layer.\n+    \"\"\"\n+    self._full_argspec = tf_inspect.getfullargspec(fn)\n+\n+    self._arg_names = self._full_argspec.args\n+    # Scrub `self` that appears if a decorator was applied.\n+    if self._arg_names and self._arg_names[0] == 'self':\n+      self._arg_names = self._arg_names[1:]\n+    self._arg_names += self._full_argspec.kwonlyargs or []\n+\n+    call_accepts_kwargs = self._full_argspec.varkw is not None\n+    self._expects_training_arg = ('training' in self._arg_names or\n+                                  call_accepts_kwargs)\n+    self._expects_mask_arg = 'mask' in self._arg_names or call_accepts_kwargs\n+\n+    call_fn_defaults = self._full_argspec.defaults or []\n+    defaults = dict()\n+    # The call arg defaults are an n-tuple of the last n elements of the args\n+    # list. (n = # of elements that have a default argument)\n+    for i in range(-1 * len(call_fn_defaults), 0):\n+      defaults[self._arg_names[i]] = call_fn_defaults[i]\n+    # The default training arg will be any (non-None) default specified in the\n+    # method signature, or None if no value is specified.\n+    defaults.update(self._full_argspec.kwonlydefaults or {})\n+    self._default_training_arg = defaults.get('training')\n+\n+  @property\n+  def full_argspec(self):\n+    \"\"\"Returns the FullArgSpec of the call function.\"\"\"\n+    return self._full_argspec\n+\n+  @property\n+  def arg_names(self):\n+    \"\"\"List of names of args and kwonlyargs.\"\"\"\n+    return self._arg_names\n+\n+  @arg_names.setter\n+  def arg_names(self, value):\n+    self._arg_names = value\n+\n+  @property\n+  @cached_per_instance\n+  def arg_positions(self):\n+    \"\"\"Returns a dict mapping arg names to their index positions.\"\"\"\n+    call_fn_arg_positions = dict()\n+    for pos, arg in enumerate(self._arg_names):\n+      call_fn_arg_positions[arg] = pos\n+    return call_fn_arg_positions\n+\n+  @property\n+  def expects_training_arg(self):\n+    \"\"\"Whether the call function uses 'training' as a parameter.\"\"\"\n+    return self._expects_training_arg\n+\n+  @expects_training_arg.setter\n+  def expects_training_arg(self, value):\n+    self._expects_training_arg = value\n+\n+  @property\n+  def expects_mask_arg(self):\n+    \"\"\"Whether the call function uses `mask` as a parameter.\"\"\"\n+    return self._expects_mask_arg\n+\n+  @expects_mask_arg.setter\n+  def expects_mask_arg(self, value):\n+    self._expects_mask_arg = value\n+\n+  @property\n+  def default_training_arg(self):\n+    \"\"\"The default value given to the \"training\" argument.\"\"\"\n+    return self._default_training_arg\n+\n+  def arg_was_passed(self, arg_name, args, kwargs, inputs_in_args=False):\n+    \"\"\"Returns true if argument is present in `args` or `kwargs`.\n+\n+    Args:\n+      arg_name: String name of the argument to find.\n+      args: Tuple of args passed to the call function.\n+      kwargs: Dictionary of kwargs  passed to the call function.\n+      inputs_in_args: Whether the input argument (the first argument in the call\n+        function) is included in `args`. Defaults to `False`.\n+\n+    Returns:\n+      True if argument with `arg_name` is present in `args` or `kwargs`.\n+    \"\"\"\n+    # Performance optimization: do no work in most common case.\n+    if not args and not kwargs:\n+      return False\n+\n+    if arg_name in kwargs:\n+      return True\n+    call_fn_args = self._arg_names\n+    if not inputs_in_args:\n+      # Ignore `inputs` arg.\n+      call_fn_args = call_fn_args[1:]\n+    return arg_name in dict(zip(call_fn_args, args))\n+\n+  def get_arg_value(self, arg_name, args, kwargs, inputs_in_args=False):\n+    \"\"\"Retrieves the value for the argument with name `arg_name`.\n+\n+    Args:\n+      arg_name: String name of the argument to find.\n+      args: Tuple of args passed to the call function.\n+      kwargs: Dictionary of kwargs  passed to the call function.\n+      inputs_in_args: Whether the input argument (the first argument in the call\n+        function) is included in `args`. Defaults to `False`.\n+\n+    Returns:\n+      The value of the argument with name `arg_name`, extracted from `args` or\n+      `kwargs`.\n+    \"\"\"\n+    if arg_name in kwargs:\n+      return kwargs[arg_name]\n+    call_fn_args = self._arg_names\n+    if not inputs_in_args:\n+      # Ignore `inputs` arg.\n+      call_fn_args = call_fn_args[1:]\n+    args_dict = dict(zip(call_fn_args, args))\n+    return args_dict[arg_name]\n+\n+  def set_arg_value(self,\n+                    arg_name,\n+                    new_value,\n+                    args,\n+                    kwargs,\n+                    inputs_in_args=False,\n+                    pop_kwarg_if_none=False):\n+    \"\"\"Sets the value of an argument into the given args/kwargs.\n+\n+    Args:\n+      arg_name: String name of the argument to find.\n+      new_value: New value to give to the argument.\n+      args: Tuple of args passed to the call function.\n+      kwargs: Dictionary of kwargs  passed to the call function.\n+      inputs_in_args: Whether the input argument (the first argument in the call\n+        function) is included in `args`. Defaults to `False`.\n+      pop_kwarg_if_none: If the new value is `None`, and this is `True`, then\n+        the argument is deleted from `kwargs`.\n+\n+    Returns:\n+      The updated `(args, kwargs)`.\n+    \"\"\"\n+    arg_pos = self.arg_positions.get(arg_name, None)\n+    if arg_pos is not None:\n+      if not inputs_in_args:\n+        # Ignore `inputs` arg.\n+        arg_pos = arg_pos - 1\n+      if len(args) > arg_pos:\n+        args = list(args)\n+        args[arg_pos] = new_value\n+        return tuple(args), kwargs\n+    if new_value is None and pop_kwarg_if_none:\n+      kwargs.pop(arg_name, None)\n+    else:\n+      kwargs[arg_name] = new_value\n+    return args, kwargs\n+\n+  def split_out_first_arg(self, args, kwargs):\n+    \"\"\"Splits (args, kwargs) into (inputs, args, kwargs).\"\"\"\n+    # Grab the argument corresponding to the first argument in the\n+    # layer's `call` method spec. This will either be the first positional\n+    # argument, or it will be provided as a keyword argument.\n+    if args:\n+      inputs = args[0]\n+      args = args[1:]\n+    elif self._arg_names[0] in kwargs:\n+      kwargs = copy.copy(kwargs)\n+      inputs = kwargs.pop(self._arg_names[0])\n+    else:\n+      raise ValueError(\n+          'The first argument to `Layer.call` must always be passed.')\n+    return inputs, args, kwargs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93cef86d705e6718ff0d914e303e29cec23195ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 8563 | Contributors (this commit): 27 | Commits (past 90d): 15 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -553,7 +553,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         Note that `trainable` cannot be `True` if `synchronization`\n         is set to `ON_READ`.\n       constraint: Constraint instance (callable).\n-      use_resource: Whether to use `ResourceVariable`.\n+      use_resource: Whether to use [ResourceVariable](\n+      https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables).\n       synchronization: Indicates when a distributed a variable will be\n         aggregated. Accepted values are constants defined in the class\n         `tf.VariableSynchronization`. By default the synchronization is set to\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
