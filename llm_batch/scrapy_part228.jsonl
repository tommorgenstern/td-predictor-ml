{"custom_id": "scrapy#d09eed7674b5df2a1883a7c7abad40fdfd062c74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 22 | Files Changed: 4 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 32 | Churn Cumulative: 1110 | Contributors (this commit): 22 | Commits (past 90d): 8 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,10 +7,10 @@ from six.moves.html_parser import HTMLParser\n from six.moves.urllib.parse import urljoin\n \n from w3lib.url import safe_url_string\n+from w3lib.html import strip_html5_whitespace\n \n from scrapy.link import Link\n from scrapy.utils.python import unique as unique_list\n-from scrapy.utils.url import trim_href_attribute\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n@@ -71,9 +71,9 @@ class HtmlParserLinkExtractor(HTMLParser):\n         if self.scan_tag(tag):\n             for attr, value in attrs:\n                 if self.scan_attr(attr):\n-                    url = self.process_attr(value)\n                     if self.strip:\n-                        url = trim_href_attribute(url)\n+                        value = strip_html5_whitespace(value)\n+                    url = self.process_attr(value)\n                     link = Link(url=url)\n                     self.links.append(link)\n                     self.current_link = link\n\n@@ -2,15 +2,15 @@\n Link extractor based on lxml.html\n \"\"\"\n import six\n-from six.moves.urllib.parse import urlparse, urljoin\n+from six.moves.urllib.parse import urljoin\n \n import lxml.etree as etree\n+from w3lib.html import strip_html5_whitespace\n \n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list, to_native_str\n from scrapy.utils.response import get_base_url\n-from scrapy.utils.url import trim_href_attribute\n from scrapy.linkextractors import FilteringLinkExtractor\n \n \n@@ -53,7 +53,7 @@ class LxmlParserLinkExtractor(object):\n             # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n             try:\n                 if self.strip:\n-                    attr_val = trim_href_attribute(attr_val)\n+                    attr_val = strip_html5_whitespace(attr_val)\n                 attr_val = urljoin(base_url, attr_val)\n             except ValueError:\n                 continue  # skipping bogus links\n\n@@ -7,12 +7,13 @@ import warnings\n from sgmllib import SGMLParser\n \n from w3lib.url import safe_url_string\n+from w3lib.html import strip_html5_whitespace\n+\n from scrapy.link import Link\n from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list, to_unicode\n from scrapy.utils.response import get_base_url\n-from scrapy.utils.url import trim_href_attribute\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n@@ -81,10 +82,10 @@ class BaseSgmlLinkExtractor(SGMLParser):\n         if self.scan_tag(tag):\n             for attr, value in attrs:\n                 if self.scan_attr(attr):\n+                    if self.strip and value is not None:\n+                        value = strip_html5_whitespace(value)\n                     url = self.process_value(value)\n                     if url is not None:\n-                        if self.strip:\n-                            url = trim_href_attribute(url)\n                         link = Link(url=url, nofollow=rel_has_nofollow(dict(attrs).get('rel')))\n                         self.links.append(link)\n                         self.current_link = link\n\n@@ -103,16 +103,3 @@ def guess_scheme(url):\n         return any_to_uri(url)\n     else:\n         return add_http_if_no_scheme(url)\n-\n-\n-def trim_href_attribute(href):\n-    \"\"\"\n-    Process href attribute of ``a`` or ``area`` elements according to HTML5\n-    standards (strip all leading and trailing whitespaces). References:\n-\n-    * https://www.w3.org/TR/html5/links.html#links-created-by-a-and-area-elements\n-    * https://www.w3.org/TR/html5/infrastructure.html#valid-url-potentially-surrounded-by-spaces\n-    * https://www.w3.org/TR/html5/infrastructure.html#strip-leading-and-trailing-whitespace\n-    * https://www.w3.org/TR/html5/infrastructure.html#space-character\n-    \"\"\"\n-    return href.strip(' \\t\\n\\r\\x0c')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#692975acb40c6394424dfb728b1ffa46b3b3c55d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 19 | Churn Cumulative: 169 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,10 +66,20 @@ class Spider(object_ref):\n         crawler.signals.connect(self.close, signals.spider_closed)\n \n     def start_requests(self):\n+        if self.make_requests_from_url is not Spider.make_requests_from_url:\n+            warnings.warn(\n+                \"Spider.make_requests_from_url method is deprecated; \"\n+                \"it won't be called in future Scrapy releases. \"\n+                \"Please override start_requests method instead.\"\n+            )\n             for url in self.start_urls:\n                 yield self.make_requests_from_url(url)\n+        else:\n+            for url in self.start_urls:\n+                yield Request(url, dont_filter=True)\n \n     def make_requests_from_url(self, url):\n+        \"\"\" This method is deprecated. \"\"\"\n         return Request(url, dont_filter=True)\n \n     def parse(self, response):\n\n@@ -170,10 +170,7 @@ class DuplicateStartRequestsSpider(Spider):\n         for i in range(0, self.distinct_urls):\n             for j in range(0, self.dupe_factor):\n                 url = \"http://localhost:8998/echo?headers=1&body=test%d\" % i\n-                yield self.make_requests_from_url(url)\n-\n-    def make_requests_from_url(self, url):\n-        return Request(url, dont_filter=self.dont_filter)\n+                yield Request(url, dont_filter=self.dont_filter)\n \n     def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n         super(DuplicateStartRequestsSpider, self).__init__(*args, **kwargs)\n\n@@ -66,8 +66,8 @@ class TestSpider(Spider):\n \n \n class TestDupeFilterSpider(TestSpider):\n-    def make_requests_from_url(self, url):\n-        return Request(url)  # dont_filter=False\n+    def start_requests(self):\n+        return (Request(url) for url in self.start_urls)  # no dont_filter=True\n \n \n class DictItemsSpider(TestSpider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f0c4e5dfd587a2b522a48c141c5af0a35ea8e56e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 94 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,5 +31,8 @@ class ProjectUtilsTest(unittest.TestCase):\n     def test_data_path_inside_project(self):\n         with inside_a_project() as proj_path:\n             expected = os.path.join(proj_path, '.scrapy', 'somepath')\n-            self.assertEquals(expected, data_path('somepath'))\n+            self.assertEquals(\n+                os.path.realpath(expected),\n+                os.path.realpath(data_path('somepath'))\n+            )\n             self.assertEquals('/absolute/path', data_path('/absolute/path'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b9928558b293be71b02afc4da7ba4e42896b6a87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 17 | Files Changed: 4 | Hunks: 22 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/2 | Churn Δ: 50 | Churn Cumulative: 1212 | Contributors (this commit): 14 | Commits (past 90d): 5 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import os\n import six\n import contextlib\n+import shutil\n try:\n     from unittest import mock\n except ImportError:\n@@ -84,11 +85,13 @@ class FileTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tmpname = self.mktemp()\n-        fd = open(self.tmpname + '^', 'w')\n-        fd.write('0123456789')\n-        fd.close()\n+        with open(self.tmpname + '^', 'w') as f:\n+            f.write('0123456789')\n         self.download_request = FileDownloadHandler(Settings()).download_request\n \n+    def tearDown(self):\n+        os.unlink(self.tmpname + '^')\n+\n     def test_download(self):\n         def _test(response):\n             self.assertEquals(response.url, request.url)\n@@ -134,10 +137,10 @@ class HttpTestCase(unittest.TestCase):\n     certfile = 'keys/cert.pem'\n \n     def setUp(self):\n-        name = self.mktemp()\n-        os.mkdir(name)\n-        FilePath(name).child(\"file\").setContent(b\"0123456789\")\n-        r = static.File(name)\n+        self.tmpname = self.mktemp()\n+        os.mkdir(self.tmpname)\n+        FilePath(self.tmpname).child(\"file\").setContent(b\"0123456789\")\n+        r = static.File(self.tmpname)\n         r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n         r.putChild(b\"wait\", ForeverTakingResource())\n         r.putChild(b\"hang-after-headers\", ForeverTakingResource(write=True))\n@@ -165,6 +168,7 @@ class HttpTestCase(unittest.TestCase):\n         yield self.port.stopListening()\n         if hasattr(self.download_handler, 'close'):\n             yield self.download_handler.close()\n+        shutil.rmtree(self.tmpname)\n \n     def getURL(self, path):\n         return \"%s://%s:%d/%s\" % (self.scheme, self.host, self.portno, path)\n@@ -709,6 +713,9 @@ class FTPTestCase(unittest.TestCase):\n         self.download_handler = FTPDownloadHandler(Settings())\n         self.addCleanup(self.port.stopListening)\n \n+    def tearDown(self):\n+        shutil.rmtree(self.directory)\n+\n     def _add_test_callbacks(self, deferred, callback=None, errback=None):\n         def _clean(data):\n             self.download_handler.client.transport.loseConnection()\n\n@@ -57,8 +57,11 @@ class FileFeedStorageTest(unittest.TestCase):\n         file.write(b\"content\")\n         yield storage.store(file)\n         self.assertTrue(os.path.exists(path))\n+        try:\n             with open(path, 'rb') as fp:\n                 self.assertEqual(fp.read(), b\"content\")\n+        finally:\n+            os.unlink(path)\n \n \n class FTPFeedStorageTest(unittest.TestCase):\n@@ -79,12 +82,15 @@ class FTPFeedStorageTest(unittest.TestCase):\n         file.write(b\"content\")\n         yield storage.store(file)\n         self.assertTrue(os.path.exists(path))\n+        try:\n             with open(path, 'rb') as fp:\n                 self.assertEqual(fp.read(), b\"content\")\n             # again, to check s3 objects are overwritten\n             yield storage.store(BytesIO(b\"new content\"))\n             with open(path, 'rb') as fp:\n                 self.assertEqual(fp.read(), b\"new content\")\n+        finally:\n+            os.unlink(path)\n \n \n class BlockingFeedStorageTest(unittest.TestCase):\n\n@@ -1,5 +1,6 @@\n import os\n from datetime import datetime\n+import shutil\n from twisted.trial import unittest\n \n from scrapy.extensions.spiderstate import SpiderState\n@@ -13,6 +14,7 @@ class SpiderStateTest(unittest.TestCase):\n     def test_store_load(self):\n         jobdir = self.mktemp()\n         os.mkdir(jobdir)\n+        try:\n             spider = Spider(name='default')\n             dt = datetime.now()\n \n@@ -27,6 +29,8 @@ class SpiderStateTest(unittest.TestCase):\n             ss2.spider_opened(spider2)\n             self.assertEqual(spider.state, {'one': 1, 'dt': dt})\n             ss2.spider_closed(spider2)\n+        finally:\n+            shutil.rmtree(jobdir)\n \n     def test_state_attribute(self):\n         # state attribute must be present if jobdir is not set, to provide a\n\n@@ -4,7 +4,7 @@ Tests borrowed from the twisted.web.client tests.\n \"\"\"\n import os\n import six\n-from six.moves.urllib.parse import urlparse\n+import shutil\n \n from twisted.trial import unittest\n from twisted.web import server, static, util, resource\n@@ -12,6 +12,7 @@ from twisted.internet import reactor, defer\n from twisted.test.proto_helpers import StringTransport\n from twisted.python.filepath import FilePath\n from twisted.protocols.policies import WrappingFactory\n+from twisted.internet.defer import inlineCallbacks\n \n from scrapy.core.downloader import webclient as client\n from scrapy.http import Request, Headers\n@@ -229,10 +230,10 @@ class WebClientTestCase(unittest.TestCase):\n         return reactor.listenTCP(0, site, interface=\"127.0.0.1\")\n \n     def setUp(self):\n-        name = self.mktemp()\n-        os.mkdir(name)\n-        FilePath(name).child(\"file\").setContent(b\"0123456789\")\n-        r = static.File(name)\n+        self.tmpname = self.mktemp()\n+        os.mkdir(self.tmpname)\n+        FilePath(self.tmpname).child(\"file\").setContent(b\"0123456789\")\n+        r = static.File(self.tmpname)\n         r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n         r.putChild(b\"wait\", ForeverTakingResource())\n         r.putChild(b\"error\", ErrorResource())\n@@ -246,8 +247,10 @@ class WebClientTestCase(unittest.TestCase):\n         self.port = self._listen(self.wrapper)\n         self.portno = self.port.getHost().port\n \n+    @inlineCallbacks\n     def tearDown(self):\n-        return self.port.stopListening()\n+        yield self.port.stopListening()\n+        shutil.rmtree(self.tmpname)\n \n     def getURL(self, path):\n         return \"http://127.0.0.1:%d/%s\" % (self.portno, path)\n@@ -266,7 +269,6 @@ class WebClientTestCase(unittest.TestCase):\n             getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}).addCallback(\n                 self.assertEquals, to_bytes(\"www.example.com\"))])\n \n-\n     def test_getPage(self):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n@@ -276,7 +278,6 @@ class WebClientTestCase(unittest.TestCase):\n         d.addCallback(self.assertEquals, b\"0123456789\")\n         return d\n \n-\n     def test_getPageHead(self):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n@@ -289,7 +290,6 @@ class WebClientTestCase(unittest.TestCase):\n             _getPage(\"head\").addCallback(self.assertEqual, b\"\"),\n             _getPage(\"HEAD\").addCallback(self.assertEqual, b\"\")])\n \n-\n     def test_timeoutNotTriggering(self):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and the page is\n@@ -301,7 +301,6 @@ class WebClientTestCase(unittest.TestCase):\n             self.assertEquals, to_bytes(\"127.0.0.1:%d\" % self.portno))\n         return d\n \n-\n     def test_timeoutTriggering(self):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and that many\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#78cb46a0797e836c2a7c7366bffd3663b75b0afe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 251 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -350,7 +350,7 @@ class WebClientTestCase(unittest.TestCase):\n                 b'    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n                 b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n')\n \n-    def test_Encoding(self):\n+    def test_encoding(self):\n         \"\"\" Test that non-standart body encoding matches\n         Content-Encoding header \"\"\"\n         body = b'\\xd0\\x81\\xd1\\x8e\\xd0\\xaf'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a1e8a8525d2312842c7e1cca8ba6e4e1a83084b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 40 | Churn Cumulative: 240 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,11 +66,14 @@ class Spider(object_ref):\n         crawler.signals.connect(self.close, signals.spider_closed)\n \n     def start_requests(self):\n-        if self.make_requests_from_url is not Spider.make_requests_from_url:\n+        cls = self.__class__\n+        if cls.make_requests_from_url is not Spider.make_requests_from_url:\n             warnings.warn(\n-                \"Spider.make_requests_from_url method is deprecated; \"\n-                \"it won't be called in future Scrapy releases. \"\n-                \"Please override start_requests method instead.\"\n+                \"Spider.make_requests_from_url method is deprecated; it \"\n+                \"won't be called in future Scrapy releases. Please \"\n+                \"override Spider.start_requests method instead (see %s.%s).\" % (\n+                    cls.__module__, cls.__name__\n+                ),\n             )\n             for url in self.start_urls:\n                 yield self.make_requests_from_url(url)\n\n@@ -345,7 +345,7 @@ Sitemap: /sitemap-relative-url.xml\n                           'http://www.example.com/sitemap-relative-url.xml'])\n \n \n-class BaseSpiderDeprecationTest(unittest.TestCase):\n+class DeprecationTest(unittest.TestCase):\n \n     def test_basespider_is_deprecated(self):\n         with warnings.catch_warnings(record=True) as w:\n@@ -399,6 +399,29 @@ class BaseSpiderDeprecationTest(unittest.TestCase):\n         assert isinstance(CrawlSpider(name='foo'), Spider)\n         assert isinstance(CrawlSpider(name='foo'), BaseSpider)\n \n+    def test_make_requests_from_url_deprecated(self):\n+        class MySpider4(Spider):\n+            name = 'spider1'\n+            start_urls = ['http://example.com']\n \n-if __name__ == '__main__':\n-    unittest.main()\n+        class MySpider5(Spider):\n+            name = 'spider2'\n+            start_urls = ['http://example.com']\n+\n+            def make_requests_from_url(self, url):\n+                return Request(url + \"/foo\", dont_filter=True)\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            # spider without overridden make_requests_from_url method\n+            # doesn't issue a warning\n+            spider1 = MySpider4()\n+            self.assertEqual(len(list(spider1.start_requests())), 1)\n+            self.assertEqual(len(w), 0)\n+\n+            # spider with overridden make_requests_from_url issues a warning,\n+            # but the method still works\n+            spider2 = MySpider5()\n+            requests = list(spider2.start_requests())\n+            self.assertEqual(len(requests), 1)\n+            self.assertEqual(requests[0].url, 'http://example.com/foo')\n+            self.assertEqual(len(w), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5adacc4dde1ae0dff866803c6d437ebcb4db2730", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 35 | Churn Cumulative: 388 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,7 @@ from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.deprecate import method_is_overridden\n \n \n class Spider(object_ref):\n@@ -67,7 +68,7 @@ class Spider(object_ref):\n \n     def start_requests(self):\n         cls = self.__class__\n-        if cls.make_requests_from_url is not Spider.make_requests_from_url:\n+        if method_is_overridden(cls, Spider, 'make_requests_from_url'):\n             warnings.warn(\n                 \"Spider.make_requests_from_url method is deprecated; it \"\n                 \"won't be called in future Scrapy releases. Please \"\n\n@@ -156,3 +156,35 @@ def update_classpath(path):\n                           ScrapyDeprecationWarning)\n             return new_path\n     return path\n+\n+\n+def method_is_overridden(subclass, base_class, method_name):\n+    \"\"\" \n+    Return True if a method named ``method_name`` of a ``base_class`` \n+    is overridden in a ``subclass``. \n+    \n+    >>> class Base(object):\n+    ...     def foo(self):\n+    ...         pass\n+    >>> class Sub1(Base):\n+    ...     pass\n+    >>> class Sub2(Base):\n+    ...     def foo(self):\n+    ...         pass\n+    >>> class Sub3(Sub1):\n+    ...     def foo(self):\n+    ...         pass\n+    >>> class Sub4(Sub2):\n+    ...     pass\n+    >>> method_is_overridden(Sub1, Base, 'foo')\n+    False\n+    >>> method_is_overridden(Sub2, Base, 'foo')\n+    True\n+    >>> method_is_overridden(Sub3, Base, 'foo')\n+    True\n+    >>> method_is_overridden(Sub4, Base, 'foo')\n+    True\n+    \"\"\"\n+    base_method = getattr(base_class, method_name)\n+    sub_method = getattr(subclass, method_name)\n+    return base_method.__code__ is not sub_method.__code__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ffa2c5759d068e298d94be406fc26faffac6034", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 16 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 73 | Churn Cumulative: 1339 | Contributors (this commit): 36 | Commits (past 90d): 8 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ In case of status 200 request, response.headers will come with two keys:\n \n import re\n from io import BytesIO\n-from six.moves.urllib.parse import urlparse, unquote\n+from six.moves.urllib.parse import unquote\n \n from twisted.internet import reactor\n from twisted.protocols.ftp import FTPClient, CommandFailed\n@@ -38,6 +38,8 @@ from twisted.internet.protocol import Protocol, ClientCreator\n \n from scrapy.http import Response\n from scrapy.responsetypes import responsetypes\n+from scrapy.utils.httpobj import urlparse_cached\n+\n \n class ReceivedDataProtocol(Protocol):\n     def __init__(self, filename=None):\n@@ -64,13 +66,17 @@ class FTPDownloadHandler(object):\n         \"default\": 503,\n     }\n \n-    def __init__(self, setting):\n-        pass\n+    def __init__(self, settings):\n+        self.anonymous_user = settings['FTP_ANONYMOUS_USER']\n+        self.anonymous_password = settings['FTP_ANONYMOUS_PASSWORD']\n \n     def download_request(self, request, spider):\n-        parsed_url = urlparse(request.url)\n-        creator = ClientCreator(reactor, FTPClient, request.meta[\"ftp_user\"],\n-                                    request.meta[\"ftp_password\"],\n+        parsed_url = urlparse_cached(request)\n+        user = request.meta.get(\"ftp_user\", self.anonymous_user)\n+        password = request.meta.get(\"ftp_password\")\n+        if user == self.anonymous_user and password is None:\n+            password = self.anonymous_password\n+        creator = ClientCreator(reactor, FTPClient, user, password,\n             passive=request.meta.get(\"ftp_passive\", 1))\n         return creator.connectTCP(parsed_url.hostname, parsed_url.port or 21).addCallback(self.gotClient,\n                                 request, unquote(parsed_url.path))\n\n@@ -161,6 +161,9 @@ FEED_EXPORTERS_BASE = {\n \n FILES_STORE_S3_ACL = 'private'\n \n+FTP_ANONYMOUS_USER = 'anonymous'\n+FTP_ANONYMOUS_PASSWORD = 'anonymous@example.com'\n+\n HTTPCACHE_ENABLED = False\n HTTPCACHE_DIR = 'httpcache'\n HTTPCACHE_IGNORE_MISSING = False\n\n@@ -676,10 +676,11 @@ class S3TestCase(unittest.TestCase):\n             b'AWS 0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=')\n \n \n-class FTPTestCase(unittest.TestCase):\n+class BaseFTPTestCase(unittest.TestCase):\n \n     username = \"scrapy\"\n     password = \"passwd\"\n+    req_meta = {\"ftp_user\": username, \"ftp_password\": password}\n \n     if six.PY3:\n         skip = \"Twisted missing ftp support for PY3\"\n@@ -722,7 +723,7 @@ class FTPTestCase(unittest.TestCase):\n \n     def test_ftp_download_success(self):\n         request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n-                meta={\"ftp_user\": self.username, \"ftp_password\": self.password})\n+                          meta=self.req_meta)\n         d = self.download_handler.download_request(request, None)\n \n         def _test(r):\n@@ -734,7 +735,7 @@ class FTPTestCase(unittest.TestCase):\n     def test_ftp_download_path_with_spaces(self):\n         request = Request(\n             url=\"ftp://127.0.0.1:%s/file with spaces.txt\" % self.portNum,\n-            meta={\"ftp_user\": self.username, \"ftp_password\": self.password}\n+            meta=self.req_meta\n         )\n         d = self.download_handler.download_request(request, None)\n \n@@ -746,7 +747,7 @@ class FTPTestCase(unittest.TestCase):\n \n     def test_ftp_download_notexist(self):\n         request = Request(url=\"ftp://127.0.0.1:%s/notexist.txt\" % self.portNum,\n-                meta={\"ftp_user\": self.username, \"ftp_password\": self.password})\n+                          meta=self.req_meta)\n         d = self.download_handler.download_request(request, None)\n \n         def _test(r):\n@@ -755,8 +756,10 @@ class FTPTestCase(unittest.TestCase):\n \n     def test_ftp_local_filename(self):\n         local_fname = \"/tmp/file.txt\"\n+        meta = {\"ftp_local_filename\": local_fname}\n+        meta.update(self.req_meta)\n         request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n-                meta={\"ftp_user\": self.username, \"ftp_password\": self.password, \"ftp_local_filename\": local_fname})\n+                          meta=meta)\n         d = self.download_handler.download_request(request, None)\n \n         def _test(r):\n@@ -768,13 +771,50 @@ class FTPTestCase(unittest.TestCase):\n             os.remove(local_fname)\n         return self._add_test_callbacks(d, _test)\n \n+\n+class FTPTestCase(BaseFTPTestCase):\n+\n     def test_invalid_credentials(self):\n         from twisted.protocols.ftp import ConnectionLost\n \n+        meta = dict(self.req_meta)\n+        meta.update({\"ftp_password\": 'invalid'})\n         request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n-                meta={\"ftp_user\": self.username, \"ftp_password\": 'invalid'})\n+                          meta=meta)\n         d = self.download_handler.download_request(request, None)\n \n         def _test(r):\n             self.assertEqual(r.type, ConnectionLost)\n         return self._add_test_callbacks(d, errback=_test)\n+\n+\n+class AnonymousFTPTestCase(BaseFTPTestCase):\n+\n+    username = \"anonymous\"\n+    req_meta = {}\n+\n+    def setUp(self):\n+        from twisted.protocols.ftp import FTPRealm, FTPFactory\n+        from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n+\n+        # setup dir and test file\n+        self.directory = self.mktemp()\n+        os.mkdir(self.directory)\n+\n+        fp = FilePath(self.directory)\n+        fp.child('file.txt').setContent(\"I have the power!\")\n+        fp.child('file with spaces.txt').setContent(\"Moooooooooo power!\")\n+\n+        # setup server for anonymous access\n+        realm = FTPRealm(anonymousRoot=self.directory)\n+        p = portal.Portal(realm)\n+        p.registerChecker(checkers.AllowAnonymousAccess(),\n+                          credentials.IAnonymous)\n+\n+        self.factory = FTPFactory(portal=p,\n+                                  userAnonymous=self.username)\n+        print(\"self.factory.allowAnonymous=%r\" % self.factory.allowAnonymous)\n+        self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n+        self.portNum = self.port.getHost().port\n+        self.download_handler = FTPDownloadHandler(Settings())\n+        self.addCleanup(self.port.stopListening)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26b7c039a9ba3a975b94472dbd54d6fa7bbc358a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 708 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -813,7 +813,6 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n \n         self.factory = FTPFactory(portal=p,\n                                   userAnonymous=self.username)\n-        print(\"self.factory.allowAnonymous=%r\" % self.factory.allowAnonymous)\n         self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n         self.portNum = self.port.getHost().port\n         self.download_handler = FTPDownloadHandler(Settings())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#565baae1e2b5f4b805d8f5354d164d61423f59ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 20 | Churn Cumulative: 652 | Contributors (this commit): 34 | Commits (past 90d): 7 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -67,17 +67,18 @@ class FTPDownloadHandler(object):\n     }\n \n     def __init__(self, settings):\n-        self.anonymous_user = settings['FTP_ANONYMOUS_USER']\n-        self.anonymous_password = settings['FTP_ANONYMOUS_PASSWORD']\n+        self.default_user = settings['FTP_USER']\n+        self.default_password = settings['FTP_PASSWORD']\n+        self.passive_mode = settings['FTP_PASSIVE_MODE']\n \n     def download_request(self, request, spider):\n         parsed_url = urlparse_cached(request)\n-        user = request.meta.get(\"ftp_user\", self.anonymous_user)\n-        password = request.meta.get(\"ftp_password\")\n-        if user == self.anonymous_user and password is None:\n-            password = self.anonymous_password\n+        user = request.meta.get(\"ftp_user\", self.default_user)\n+        password = request.meta.get(\"ftp_password\", self.default_password)\n+        passive_mode = 1 if bool(request.meta.get(\"ftp_passive\",\n+                                                  self.passive_mode)) else 0\n         creator = ClientCreator(reactor, FTPClient, user, password,\n-            passive=request.meta.get(\"ftp_passive\", 1))\n+            passive=passive_mode)\n         return creator.connectTCP(parsed_url.hostname, parsed_url.port or 21).addCallback(self.gotClient,\n                                 request, unquote(parsed_url.path))\n \n\n@@ -161,8 +161,9 @@ FEED_EXPORTERS_BASE = {\n \n FILES_STORE_S3_ACL = 'private'\n \n-FTP_ANONYMOUS_USER = 'anonymous'\n-FTP_ANONYMOUS_PASSWORD = 'anonymous@example.com'\n+FTP_USER = 'anonymous'\n+FTP_PASSWORD = 'anonymous@example.com'\n+FTP_PASSIVE_MODE = True\n \n HTTPCACHE_ENABLED = False\n HTTPCACHE_DIR = 'httpcache'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d35a01a1036c94b13a3828b34bd1752000b5c0fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 501 | Contributors (this commit): 33 | Commits (past 90d): 4 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -162,7 +162,7 @@ FEED_EXPORTERS_BASE = {\n FILES_STORE_S3_ACL = 'private'\n \n FTP_USER = 'anonymous'\n-FTP_PASSWORD = 'anonymous@example.com'\n+FTP_PASSWORD = 'guest'\n FTP_PASSIVE_MODE = True\n \n HTTPCACHE_ENABLED = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f2e200120bc20b6d4792ade2b295395296340ee0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 715 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ try:\n     from unittest import mock\n except ImportError:\n     import mock\n+import shutil\n \n from twisted.trial import unittest\n from twisted.protocols.policies import WrappingFactory\n@@ -710,6 +711,9 @@ class BaseFTPTestCase(unittest.TestCase):\n         self.download_handler = FTPDownloadHandler(Settings())\n         self.addCleanup(self.port.stopListening)\n \n+    def tearDown(self):\n+        shutil.rmtree(self.directory)\n+\n     def _add_test_callbacks(self, deferred, callback=None, errback=None):\n         def _clean(data):\n             self.download_handler.client.transport.loseConnection()\n@@ -817,3 +821,6 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n         self.portNum = self.port.getHost().port\n         self.download_handler = FTPDownloadHandler(Settings())\n         self.addCleanup(self.port.stopListening)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.directory)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ff3e299eb0c6fe486ecdaafaae340bb55a0c226e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 6 | Files Changed: 5 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 27 | Churn Cumulative: 540 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,7 +18,7 @@ class Request(object_ref):\n \n     def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                  cookies=None, meta=None, encoding='utf-8', priority=0,\n-                 dont_filter=False, errback=None):\n+                 dont_filter=False, errback=None, flags=None):\n \n         self._encoding = encoding  # this one has to be set first\n         self.method = str(method).upper()\n@@ -36,6 +36,7 @@ class Request(object_ref):\n         self.dont_filter = dont_filter\n \n         self._meta = dict(meta) if meta else None\n+        self.flags = [] if flags is None else list(flags)\n \n     @property\n     def meta(self):\n\n@@ -7,7 +7,7 @@ from scrapy.utils.request import referer_str\n \n SCRAPEDMSG = u\"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n DROPPEDMSG = u\"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n-CRAWLEDMSG = u\"Crawled (%(status)s) %(request)s (referer: %(referer)s)%(flags)s\"\n+CRAWLEDMSG = u\"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\n \n \n class LogFormatter(object):\n@@ -32,15 +32,17 @@ class LogFormatter(object):\n     \"\"\"\n \n     def crawled(self, request, response, spider):\n-        flags = ' %s' % str(response.flags) if response.flags else ''\n+        request_flags = ' %s' % str(request.flags) if request.flags else ''\n+        response_flags = ' %s' % str(response.flags) if response.flags else ''\n         return {\n             'level': logging.DEBUG,\n             'msg': CRAWLEDMSG,\n             'args': {\n                 'status': response.status,\n                 'request': request,\n+                'request_flags' : request_flags,\n                 'referer': referer_str(request),\n-                'flags': flags,\n+                'response_flags': response_flags,\n             }\n         }\n \n\n@@ -32,6 +32,7 @@ def request_to_dict(request, spider=None):\n         '_encoding': request._encoding,\n         'priority': request.priority,\n         'dont_filter': request.dont_filter,\n+        'flags': request.flags\n     }\n     if type(request) is not Request:\n         d['_class'] = request.__module__ + '.' + request.__class__.__name__\n@@ -62,7 +63,8 @@ def request_from_dict(d, spider=None):\n         meta=d['meta'],\n         encoding=d['_encoding'],\n         priority=d['priority'],\n-        dont_filter=d['dont_filter'])\n+        dont_filter=d['dont_filter'],\n+        flags=d.get('flags'))\n \n \n def _find_method(obj, func):\n\n@@ -36,6 +36,14 @@ class LoggingContribTest(unittest.TestCase):\n         self.assertEqual(logline,\n             \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n \n+    def test_flags_in_request(self):\n+        req = Request(\"http://www.example.com\", flags=['test','flag'])\n+        res = Response(\"http://www.example.com\")\n+        logkws = self.formatter.crawled(req, res, self.spider)\n+        logline = logkws['msg'] % logkws['args']\n+        self.assertEqual(logline,\n+        \"Crawled (200) <GET http://www.example.com> ['test', 'flag'] (referer: None)\")\n+\n     def test_dropped(self):\n         item = {}\n         exception = Exception(u\"\\u2018\")\n\n@@ -25,7 +25,8 @@ class RequestSerializationTest(unittest.TestCase):\n             cookies={'currency': u'руб'},\n             encoding='latin-1',\n             priority=20,\n-            meta={'a': 'b'})\n+            meta={'a': 'b'},\n+            flags=['testFlag'])\n         self._assert_serializes_ok(r)\n \n     def test_latin1_body(self):\n@@ -54,6 +55,7 @@ class RequestSerializationTest(unittest.TestCase):\n         self.assertEqual(r1._encoding, r2._encoding)\n         self.assertEqual(r1.priority, r2.priority)\n         self.assertEqual(r1.dont_filter, r2.dont_filter)\n+        self.assertEqual(r1.flags, r2.flags)\n \n     def test_request_class(self):\n         r = FormRequest(\"http://www.example.com\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#adb180fbaf7de9f720e76b33c742f6cdaa083cdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1445 | Contributors (this commit): 16 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -105,7 +105,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         self._tunneledHost = host\n         self._tunneledPort = port\n         self._contextFactory = contextFactory\n-        self._connectBuffer = b''\n+        self._connectBuffer = bytearray()\n \n     def requestTunnel(self, protocol):\n         \"\"\"Asks the proxy to open a tunnel.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#301847d862a1412f3aa8302b242e3f6b9f1442e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 235 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,15 +1,15 @@\n from OpenSSL import SSL\n from twisted.internet.ssl import ClientContextFactory\n \n-try:\n+from scrapy import twisted_version\n+\n+if twisted_version >= (14, 0, 0):\n \n     from zope.interface.declarations import implementer\n \n-    # the following should be available from Twisted 14.0.0\n     from twisted.internet.ssl import (optionsForClientTLS,\n                                       CertificateOptions,\n                                       platformTrust)\n-\n     from twisted.web.client import BrowserLikePolicyForHTTPS\n     from twisted.web.iweb import IPolicyForHTTPS\n \n@@ -86,7 +86,7 @@ try:\n                                             'method': self._ssl_method,\n                                        })\n \n-except ImportError:\n+else:\n \n     class ScrapyClientContextFactory(ClientContextFactory):\n         \"A SSL context factory which is more permissive against SSL bugs.\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb7d99ed81e70a5a59d47e69995ac5c025a6a8c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 57 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -101,7 +101,7 @@ class FilteringLinkExtractor(object):\n         links = [x for x in links if self._link_allowed(x)]\n         if self.canonicalize:\n             for link in links:\n-                link.url = canonicalize_url(urlparse(link.url))\n+                link.url = canonicalize_url(link.url)\n         links = self.link_extractor._process_links(links)\n         return links\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#47f7da8724b5a453979c2b0d10114c23ac1bd170", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 12 | Methods Changed: 11 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 49 | Churn Cumulative: 511 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ from six.moves.urllib.parse import urljoin\n \n import lxml.etree as etree\n from w3lib.html import strip_html5_whitespace\n+from w3lib.url import canonicalize_url\n \n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n@@ -29,12 +30,17 @@ def _nons(tag):\n \n class LxmlParserLinkExtractor(object):\n     def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False,\n-                 strip=True):\n+                 strip=True, canonicalized=False):\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n         self.process_attr = process if callable(process) else lambda v: v\n         self.unique = unique\n         self.strip = strip\n+        if canonicalized:\n+            self.link_key = lambda link: link.url\n+        else:\n+            self.link_key = lambda link: canonicalize_url(link.url,\n+                                                          keep_fragments=True)\n \n     def _iter_links(self, document):\n         for el in document.iter(etree.Element):\n@@ -82,21 +88,27 @@ class LxmlParserLinkExtractor(object):\n \n     def _deduplicate_if_needed(self, links):\n         if self.unique:\n-            return unique_list(links, key=lambda link: link.url)\n+            return unique_list(links, key=self.link_key)\n         return links\n \n \n class LxmlLinkExtractor(FilteringLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n-                 tags=('a', 'area'), attrs=('href',), canonicalize=True,\n+                 tags=('a', 'area'), attrs=('href',), canonicalize=False,\n                  unique=True, process_value=None, deny_extensions=None, restrict_css=(),\n                  strip=True):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n-        lx = LxmlParserLinkExtractor(tag=tag_func, attr=attr_func,\n-            unique=unique, process=process_value, strip=strip)\n+        lx = LxmlParserLinkExtractor(\n+            tag=tag_func,\n+            attr=attr_func,\n+            unique=unique,\n+            process=process_value,\n+            strip=strip,\n+            canonicalized=canonicalize\n+        )\n \n         super(LxmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n             allow_domains=allow_domains, deny_domains=deny_domains,\n\n@@ -30,6 +30,7 @@ class Base:\n                 Link(url='http://example.com/sample1.html', text=u''),\n                 Link(url='http://example.com/sample2.html', text=u'sample 2'),\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+                Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment'),\n                 Link(url='http://www.google.com/something', text=u''),\n                 Link(url='http://example.com/innertag.html', text=u'inner tag'),\n                 Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n@@ -41,6 +42,7 @@ class Base:\n                 Link(url='http://example.com/sample1.html', text=u''),\n                 Link(url='http://example.com/sample2.html', text=u'sample 2'),\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+                Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment')\n             ])\n \n         def test_extract_filter_allow_with_duplicates(self):\n@@ -50,6 +52,27 @@ class Base:\n                 Link(url='http://example.com/sample2.html', text=u'sample 2'),\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n+                Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment')\n+            ])\n+\n+        def test_extract_filter_allow_with_duplicates_canonicalize(self):\n+            lx = self.extractor_cls(allow=('sample', ), unique=False,\n+                                    canonicalize=True)\n+            self.assertEqual([link for link in lx.extract_links(self.response)], [\n+                Link(url='http://example.com/sample1.html', text=u''),\n+                Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+                Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+                Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n+                Link(url='http://example.com/sample3.html', text='sample 3 repetition with fragment')\n+            ])\n+\n+        def test_extract_filter_allow_no_duplicates_canonicalize(self):\n+            lx = self.extractor_cls(allow=('sample',), unique=True,\n+                                    canonicalize=True)\n+            self.assertEqual([link for link in lx.extract_links(self.response)], [\n+                Link(url='http://example.com/sample1.html', text=u''),\n+                Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+                Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n             ])\n \n         def test_extract_filter_allow_and_deny(self):\n@@ -73,6 +96,8 @@ class Base:\n                 Link(url='http://example.com/sample1.html', text=u''),\n                 Link(url='http://example.com/sample2.html', text=u'sample 2'),\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+                Link(url='http://example.com/sample3.html#foo',\n+                     text='sample 3 repetition with fragment')\n             ])\n \n             lx = self.extractor_cls(allow='sample', deny='3')\n@@ -280,6 +305,7 @@ class Base:\n                 Link(url='http://example.com/sample1.html', text=u''),\n                 Link(url='http://example.com/sample2.html', text=u'sample 2'),\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+                Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment'),\n                 Link(url='http://www.google.com/something', text=u''),\n                 Link(url='http://example.com/innertag.html', text=u'inner tag'),\n                 Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n@@ -291,6 +317,7 @@ class Base:\n                 Link(url='http://example.com/sample2.html', text=u'sample 2'),\n                 Link(url='http://example.com/sample2.jpg', text=u''),\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+                Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment'),\n                 Link(url='http://www.google.com/something', text=u''),\n                 Link(url='http://example.com/innertag.html', text=u'inner tag'),\n                 Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
