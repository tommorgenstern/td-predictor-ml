{"custom_id": "keras#57f644c2a5201db668db439465391cd3d5ab6766", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 92 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,7 @@ class DenseTest(testing.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n-            supports_masking=False,\n+            supports_masking=True,\n         )\n         # 3D case, some regularizers.\n         self.run_layer_test(\n@@ -40,7 +40,7 @@ class DenseTest(testing.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_seed_generators=0,\n             expected_num_losses=2,  # we have 2 regularizers.\n-            supports_masking=False,\n+            supports_masking=True,\n         )\n \n     def test_dense_correctness(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6769821b1c9ff2b52c7cad2460f481e72f9c9837", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 337 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 337 | Churn Cumulative: 337 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,337 @@\n+# To run this demo, you will need to spin up a \"TPU VM\" on Google Cloud.\n+# Please follow instructions here: https://cloud.google.com/tpu/docs/run-calculation-jax\n+\n+# Force a JAX backend\n+import os, pprint, collections\n+\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+\n+pp = pprint.PrettyPrinter()\n+\n+import jax\n+import jax.numpy as jnp\n+import tensorflow as tf  # just for tf.data\n+import keras_core as keras  # Keras multi-backend\n+\n+import numpy as np\n+from tqdm import tqdm\n+\n+from jax.experimental import mesh_utils\n+from jax.sharding import Mesh\n+from jax.sharding import NamedSharding\n+from jax.sharding import PartitionSpec as P\n+\n+\"\"\" Dataset\n+Classic MNIST, loaded using tf.data\n+\"\"\"\n+\n+BATCH_SIZE = 192\n+\n+(x_train, train_labels), (\n+    x_eval,\n+    eval_labels,\n+) = keras.datasets.mnist.load_data()\n+x_train = np.expand_dims(x_train, axis=-1).astype(\n+    np.float32\n+)  # from 28x28 to 28x28 x 1 color channel (B&W)\n+x_eval = np.expand_dims(x_eval, axis=-1).astype(np.float32)\n+\n+train_data = tf.data.Dataset.from_tensor_slices((x_train, train_labels))\n+train_data = train_data.shuffle(5000, reshuffle_each_iteration=True)\n+train_data = train_data.batch(BATCH_SIZE, drop_remainder=True)\n+train_data = train_data.repeat()\n+\n+eval_data = tf.data.Dataset.from_tensor_slices((x_eval, eval_labels))\n+eval_data = eval_data.batch(10000)  # everything as one batch\n+\n+STEPS_PER_EPOCH = len(train_labels) // BATCH_SIZE\n+\n+\"\"\" Keras model\n+Simple but non-trivial model with:\n+* Batch Normalization (non-trainable state updated during trainig, different training-time and inference behavior)\n+* Dropout (randomness, different training time and inference behavior)\n+\"\"\"\n+\n+\n+# Keras \"sequential\" model building style\n+def make_backbone():\n+    return keras.Sequential(\n+        [\n+            keras.layers.Rescaling(\n+                1.0 / 255.0\n+            ),  # input images are in the range [0, 255]\n+            keras.layers.Conv2D(\n+                filters=12, kernel_size=3, padding=\"same\", use_bias=False\n+            ),\n+            keras.layers.BatchNormalization(scale=False, center=True),\n+            keras.layers.Activation(\"relu\"),\n+            keras.layers.Conv2D(\n+                filters=24,\n+                kernel_size=6,\n+                padding=\"same\",\n+                use_bias=False,\n+                strides=2,\n+            ),\n+            keras.layers.BatchNormalization(scale=False, center=True),\n+            keras.layers.Activation(\"relu\"),\n+            keras.layers.Conv2D(\n+                filters=32,\n+                kernel_size=6,\n+                padding=\"same\",\n+                use_bias=False,\n+                strides=2,\n+                name=\"large_k\",\n+            ),\n+            keras.layers.BatchNormalization(scale=False, center=True),\n+            keras.layers.Activation(\"relu\"),\n+        ],\n+        name=\"backbone\",\n+    )\n+\n+\n+def make_model():\n+    input = keras.Input(shape=[28, 28, 1])\n+    y = make_backbone()(input)\n+    y = keras.layers.Flatten()(y)\n+    y = keras.layers.Dense(200, activation=\"relu\")(y)\n+    y = keras.layers.Dropout(0.4)(y)\n+    y = keras.layers.Dense(10, activation=\"softmax\")(y)\n+    model = keras.Model(inputs=input, outputs=y)\n+    return model\n+\n+\n+\"\"\" JAX-native distribution with a Keras model\n+For now, you have to write a custom training loop for this\n+Note: The features required by jax.sharding are not supported by the Colab TPU\n+runtime at this time, but are available on Cloud TPU VMs and Kaggle TPU VMs.\n+\"\"\"\n+\n+if len(jax.local_devices()) < 8:\n+    raise Exception(\"This part requires 8 devices to run\")\n+else:\n+    print(\"\\nIdentified local devices:\")\n+    pp.pprint(jax.local_devices())\n+\n+# ----------------- Keras ---------------------\n+\n+# instantiate the model\n+model = make_model()\n+\n+# learning rate\n+lr = keras.optimizers.schedules.ExponentialDecay(0.01, STEPS_PER_EPOCH, 0.6)\n+\n+# optimizer\n+optimizer = keras.optimizers.Adam(lr)\n+\n+# initialize all state with .build()\n+(one_batch, one_batch_labels) = next(iter(train_data))\n+model.build(one_batch)\n+optimizer.build(model.trainable_variables)\n+\n+\"\"\" Distribution settings\n+\n+* Sharding the data on the batch axis\n+* Replicating all model variables\n+\n+Note: this implements standard \"data parallel\" distributed training\n+\n+* Just for show, sharding the largest convolutional kernel along the\n+  \"channels\" axis 4-ways and replicating 2-ways\n+\n+Note: this does not reflect a best practice but is intended to show\n+      that you can split a very large kernel across multiple devices\n+      if you have to\n+\"\"\"\n+\n+print(\n+    \"\\nMostly data-parallel distribution. \"\n+    \"Data is sharded across devices while the model is replicated. \"\n+    \"For demo purposes, we split the largest kernel 4-ways \"\n+    \"(and replicate 2-ways since we have 8 devices).\"\n+)\n+\n+# ------------------ Jax ----------------------\n+\n+devices = mesh_utils.create_device_mesh((8,))\n+\n+# data will be split along the batch axis\n+data_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh\n+# naming axes of the sharded partition\n+data_sharding = NamedSharding(data_mesh,P(\"batch\",),)\n+\n+# all variables will be replicated on all devices\n+var_mesh = Mesh(devices, axis_names=(\"_\"))\n+# in NamedSharding, axes that are not mentioned are replicated (all axes here)\n+var_replication = NamedSharding(var_mesh, P())\n+\n+# for the demo, we will split the largest kernel 4-ways (and replicate 2-ways since we have 8 devices)\n+large_kernel_mesh = Mesh(\n+    devices.reshape((-1, 4)), axis_names=(None, \"out_chan\")\n+)  # naming axes of the mesh\n+large_kernel_sharding = NamedSharding(\n+    large_kernel_mesh, P(None, None, None, \"out_chan\")\n+)  # naming axes of the sharded partition\n+\n+# ----------------- Keras ---------------------\n+\n+# Use Keras APIs to find the variable of a specific layer (we will be sharding this one in a special way)\n+# In a Conv2D or Dense layer, the variables are 'kernel' and 'bias'\n+special_layer_var = model.get_layer(\"backbone\").get_layer(\"large_k\").kernel\n+\n+# ------------------ Jax ----------------------\n+# - accessing variables in Keras lists model.trainable_variables,\n+# - model.non_trainable_variables and optimizer.variables\n+\n+# Apply the distribution settings to the model variables\n+non_trainable_variables = jax.device_put(\n+    model.non_trainable_variables, var_replication\n+)\n+optimizer_variables = jax.device_put(optimizer.variables, var_replication)\n+# this is what you would do replicate all trainable variables:\n+# trainable_variables = jax.device_put(model.trainable_variables, var_replication)\n+\n+# For the demo, we split the largest kernel 4-ways instead of replicating it.\n+# We still replicate all other trainable variables as in standard \"data-parallel\"\n+# distributed training.\n+print_once = True\n+trainable_variables = model.trainable_variables\n+for i, v in enumerate(trainable_variables):\n+    if v is special_layer_var:\n+        # Apply distribution settings: sharding\n+        sharded_v = jax.device_put(v, large_kernel_sharding)\n+        trainable_variables[i] = sharded_v\n+\n+        print(\"Sharding of convolutional\", v.name, v.shape)\n+        jax.debug.visualize_array_sharding(\n+            jnp.reshape(sharded_v, [-1, v.shape[-1]])\n+        )\n+    else:\n+        # Apply distribution settings: replication\n+        replicated_v = jax.device_put(v, var_replication)\n+        trainable_variables[i] = replicated_v\n+\n+        if print_once:\n+            print_once = False\n+            print(\n+                \"\\nSharding of all other model variables (they are replicated)\"\n+            )\n+            jax.debug.visualize_array_sharding(\n+                jnp.reshape(replicated_v, [-1, v.shape[-1]])\n+            )\n+\n+# collect state in a handy named tuple\n+TrainingState = collections.namedtuple(\n+    \"TrainingState\",\n+    [\"trainable_variables\", \"non_trainable_variables\", \"optimizer_variables\"],\n+)\n+device_train_state = TrainingState(\n+    trainable_variables=trainable_variables,\n+    non_trainable_variables=non_trainable_variables,\n+    optimizer_variables=optimizer_variables,\n+)\n+# display data sharding\n+x, y = next(iter(train_data))\n+sharded_x = jax.device_put(x.numpy(), data_sharding)\n+print(\"Data sharding\")\n+jax.debug.visualize_array_sharding(jnp.reshape(sharded_x, [-1, 28 * 28]))\n+\n+# ------------------ Jax ----------------------\n+# - Using Keras-provided stateless APIs\n+# - model.stateless_call\n+# - optimizer.stateless_apply\n+# These functions also work on other backends.\n+\n+# define loss\n+loss = keras.losses.SparseCategoricalCrossentropy()\n+\n+\n+# This is the loss function that will be differentiated.\n+# Keras provides a pure functional forward pass: model.stateless_call\n+def compute_loss(trainable_variables, non_trainable_variables, x, y):\n+    y_pred, updated_non_trainable_variables = model.stateless_call(\n+        trainable_variables, non_trainable_variables, x\n+    )\n+    loss_value = loss(y, y_pred)\n+    return loss_value, updated_non_trainable_variables\n+\n+\n+# function to compute gradients\n+compute_gradients = jax.value_and_grad(compute_loss, has_aux=True)\n+\n+\n+# Trainig step: Keras provides a pure functional optimizer.stateless_apply\n+@jax.jit\n+def train_step(train_state, x, y):\n+    (loss_value, non_trainable_variables), grads = compute_gradients(\n+        train_state.trainable_variables,\n+        train_state.non_trainable_variables,\n+        x,\n+        y,\n+    )\n+\n+    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n+        grads, train_state.trainable_variables, train_state.optimizer_variables\n+    )\n+\n+    return loss_value, TrainingState(\n+        trainable_variables, non_trainable_variables, optimizer_variables\n+    )\n+\n+\n+# training loop\n+EPOCHS = 5\n+print(\"\\nTrainig:\")\n+data_iter = iter(train_data)\n+for epoch in range(EPOCHS):\n+    for i in tqdm(range(STEPS_PER_EPOCH)):\n+        x, y = next(data_iter)\n+        sharded_x = jax.device_put(x.numpy(), data_sharding)\n+        loss_value, device_train_state = train_step(\n+            device_train_state, sharded_x, y.numpy()\n+        )\n+    print(\"Epoch\", epoch, \"loss:\", loss_value)\n+\n+# The output of the model is still sharded. Sharding follows the data.\n+\n+data, labels = next(iter(eval_data))\n+sharded_data = jax.device_put(data.numpy(), data_sharding)\n+\n+\n+@jax.jit\n+def predict(data):\n+    predictions, updated_non_trainable_variables = model.stateless_call(\n+        device_train_state.trainable_variables,\n+        device_train_state.non_trainable_variables,\n+        data,\n+    )\n+    return predictions\n+\n+\n+predictions = predict(sharded_data)\n+print(\"\\nModel output sharding follows data sharding:\")\n+jax.debug.visualize_array_sharding(predictions)\n+\n+# Post-processing model state update to write them back into the model\n+update = lambda variable, value: variable.assign(value)\n+\n+jax.tree_map(\n+    update, model.trainable_variables, device_train_state.trainable_variables\n+)\n+jax.tree_map(\n+    update,\n+    model.non_trainable_variables,\n+    device_train_state.non_trainable_variables,\n+)\n+jax.tree_map(\n+    update, optimizer.variables, device_train_state.optimizer_variables\n+)\n+\n+# check that the model has the new state by running an eval\n+# known issue: the optimizer should not be required here\n+model.compile(\n+    loss=keras.losses.SparseCategoricalCrossentropy(),\n+    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n+)\n+print(\"\\nUpdating model and running an eval:\")\n+loss, accuracy = model.evaluate(eval_data)\n+print(\"The model achieved an evaluation accuracy of:\", accuracy)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ea8e1e752121dbab6761e79d8889a275caef6aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 29 | Files Changed: 4 | Hunks: 18 | Methods Changed: 13 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 63 | Churn Cumulative: 4220 | Contributors (this commit): 8 | Commits (past 90d): 92 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -254,7 +254,7 @@ def vectorized_map(function, elements):\n def scatter(indices, values, shape):\n     indices = convert_to_tensor(indices)\n     values = convert_to_tensor(values)\n-    zeros = torch.zeros(shape, dtype=values.dtype).to(get_device())\n+    zeros = torch.zeros(shape, dtype=values.dtype, device=get_device())\n \n     index_length = indices.shape[-1]\n     value_shape = shape[index_length:]\n\n@@ -7,9 +7,9 @@ from keras_core.backend.torch.core import get_device\n def segment_sum(data, segment_ids, num_segments=None, **kwargs):\n     data = convert_to_tensor(data)\n     segment_ids = convert_to_tensor(segment_ids)\n-    num_repeats = (\n-        torch.prod(torch.tensor(data.shape[1:])).long().to(get_device())\n-    )\n+    num_repeats = torch.prod(\n+        torch.tensor(data.shape[1:], device=get_device())\n+    ).long()\n     # To use `scatter_add` in torch, we need to replicate `segment_ids` into the\n     # shape of `data`.\n     segment_ids = (\n@@ -32,10 +32,8 @@ def segment_sum(data, segment_ids, num_segments=None, **kwargs):\n     # Add one more dimension to the result shape with the \"+1\".\n     shape = (num_segments + 1,) + tuple(data.shape[1:])\n \n-    result = (\n-        torch.zeros(*shape)\n-        .to(get_device())\n-        .scatter_add(0, segment_ids, data.float())\n+    result = torch.zeros(*shape, device=get_device()).scatter_add(\n+        0, segment_ids, data.float()\n     )\n \n     # Removing the extra dimension.\n\n@@ -69,14 +69,14 @@ def ones(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n     if isinstance(shape, int):\n         shape = (shape,)\n-    return torch.ones(size=shape, dtype=dtype).to(get_device())\n+    return torch.ones(size=shape, dtype=dtype, device=get_device())\n \n \n def zeros(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n     if isinstance(shape, int):\n         shape = (shape,)\n-    return torch.zeros(size=shape, dtype=dtype).to(get_device())\n+    return torch.zeros(size=shape, dtype=dtype, device=get_device())\n \n \n def zeros_like(x, dtype=None):\n@@ -144,8 +144,10 @@ def append(\n def arange(start, stop=None, step=1, dtype=None):\n     dtype = to_torch_dtype(dtype)\n     if stop is None:\n-        return torch.arange(end=start, dtype=dtype)\n-    return torch.arange(start, stop, step=step, dtype=dtype)\n+        return torch.arange(end=start, dtype=dtype, device=get_device())\n+    return torch.arange(\n+        start, stop, step=step, dtype=dtype, device=get_device()\n+    )\n \n \n def arccos(x):\n@@ -310,7 +312,7 @@ def dot(x, y):\n \n def empty(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n-    return torch.empty(size=shape, dtype=dtype)\n+    return torch.empty(size=shape, dtype=dtype, device=get_device())\n \n \n def equal(x1, x2):\n@@ -355,7 +357,9 @@ def full(shape, fill_value, dtype=None):\n         expand_size = len(shape) - len(fill_value.shape)\n         tile_shape = tuple(shape[:expand_size]) + (1,) * len(fill_value.shape)\n         return torch.tile(fill_value, tile_shape)\n-    return torch.full(size=shape, fill_value=fill_value, dtype=dtype)\n+    return torch.full(\n+        size=shape, fill_value=fill_value, dtype=dtype, device=get_device()\n+    )\n \n \n def full_like(x, fill_value, dtype=None):\n@@ -436,7 +440,7 @@ def linspace(\n     if hasattr(start, \"__len__\") and hasattr(stop, \"__len__\"):\n         start, stop = convert_to_tensor(start), convert_to_tensor(stop)\n         stop = cast(stop, dtype) if endpoint is False and dtype else stop\n-        steps = torch.arange(num, dtype=dtype).to(get_device()) / (num - 1)\n+        steps = torch.arange(num, dtype=dtype, device=get_device()) / (num - 1)\n \n         # reshape `steps` to allow for broadcasting\n         for i in range(start.ndim):\n@@ -510,7 +514,7 @@ def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n     if hasattr(start, \"__len__\") and hasattr(stop, \"__len__\"):\n         start, stop = convert_to_tensor(start), convert_to_tensor(stop)\n         stop = cast(stop, dtype) if endpoint is False and dtype else stop\n-        steps = torch.arange(num, dtype=dtype).to(get_device()) / (num - 1)\n+        steps = torch.arange(num, dtype=dtype, device=get_device()) / (num - 1)\n \n         # reshape `steps` to allow for broadcasting\n         for i in range(start.ndim):\n@@ -787,7 +791,7 @@ def trace(x, offset=None, axis1=None, axis2=None):\n def tri(N, M=None, k=0, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n     M = M or N\n-    x = torch.ones((N, M), dtype=dtype)\n+    x = torch.ones((N, M), dtype=dtype, device=get_device())\n     return torch.tril(x, diagonal=k)\n \n \n@@ -883,7 +887,7 @@ def eye(N, M=None, k=None, dtype=\"float32\"):\n     M = N if M is None else M\n     k = 0 if k is None else k\n     if k == 0:\n-        return torch.eye(N, M, dtype=dtype).to(get_device())\n+        return torch.eye(N, M, dtype=dtype, device=get_device())\n     diag_length = np.maximum(N, M)\n-    diag = torch.ones(diag_length, dtype=dtype)\n-    return torch.diag(diag, diagonal=k)[:N, :M].to(get_device())\n+    diag = torch.ones(diag_length, dtype=dtype, device=get_device())\n+    return torch.diag(diag, diagonal=k)[:N, :M]\n\n@@ -102,10 +102,10 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     x = normal(shape + (4,), mean=0, stddev=1, dtype=dtype, seed=seed)\n     valid = (x > -2) & (x < 2)\n     indexes = valid.max(-1, keepdim=True)[1]\n-    trunc_x = torch.empty(shape)\n+    trunc_x = torch.empty(shape, device=get_device())\n     trunc_x.data.copy_(x.gather(-1, indexes).squeeze(-1))\n     trunc_x.data.mul_(stddev).add_(mean)\n-    return trunc_x.to(get_device())\n+    return trunc_x\n \n \n def _get_concrete_noise_shape(inputs, noise_shape):\n@@ -122,16 +122,19 @@ def _get_concrete_noise_shape(inputs, noise_shape):\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n-    seed, _ = draw_seed(seed)\n-    generator = torch.Generator()\n-    generator.manual_seed(int(seed))\n-\n     keep_prob = 1.0 - rate\n     noise_shape = _get_concrete_noise_shape(inputs, noise_shape)\n-    keep_prob_matrix = torch.full(noise_shape, keep_prob)\n-    mask = torch.bernoulli(keep_prob_matrix, generator=generator).bool()\n+    keep_prob_matrix = torch.full(noise_shape, keep_prob, device=get_device())\n+    generator = torch_seed_generator(seed)\n+\n+    # Do not use generator during symbolic execution.\n+    if get_device() == \"meta\":\n+        mask = torch.bernoulli(keep_prob_matrix)\n+    else:\n+        mask = torch.bernoulli(keep_prob_matrix, generator=generator)\n+\n+    mask = mask.bool()\n     mask = torch.broadcast_to(mask, inputs.shape)\n-    mask = mask.to(get_device())\n     return torch.where(\n         mask, inputs / keep_prob, torch.zeros_like(inputs, dtype=inputs.dtype)\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c373b6e1da6b6c900a4fe01e12553726eadc9781", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 105 | Lines Deleted: 63 | Files Changed: 2 | Hunks: 33 | Methods Changed: 15 | Complexity Δ (Sum/Max): -2/4 | Churn Δ: 168 | Churn Cumulative: 2613 | Contributors (this commit): 10 | Commits (past 90d): 124 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -780,15 +780,13 @@ class Layer(BackendLayer, Operation):\n         else:\n             # Use compute_output_shape() to return the right output spec\n             call_spec = CallSpec(self.call, args, kwargs)\n-            shapes_dict = get_shapes_dict(\n-                self.compute_output_shape, call_spec, self.__class__\n+            shapes_dict = get_shapes_dict(call_spec)\n+            shapes_dict = update_shapes_dict_for_target_fn(\n+                self.compute_output_shape,\n+                shapes_dict=shapes_dict,\n+                call_spec=call_spec,\n+                class_name=self.__class__.__name__,\n             )\n-            if len(shapes_dict) == 1:\n-                # Single arg: pass it positionally\n-                input_shape = tuple(shapes_dict.values())[0]\n-                output_shape = self.compute_output_shape(input_shape)\n-            else:\n-                # More than one shape: pass them by name.\n             output_shape = self.compute_output_shape(**shapes_dict)\n \n             if (\n@@ -950,36 +948,29 @@ class Layer(BackendLayer, Operation):\n \n     def _maybe_build(self, call_spec):\n         if not self.built:\n-            shapes_dict = get_shapes_dict(self.build, call_spec, self.__class__)\n+            shapes_dict = get_shapes_dict(call_spec)\n             self._build_shapes_dict = shapes_dict\n-            failure = False\n \n+            with backend.name_scope(self.name):\n+                if not utils.is_default(self.build):\n+                    shapes_dict = update_shapes_dict_for_target_fn(\n+                        self.build,\n+                        shapes_dict=shapes_dict,\n+                        call_spec=call_spec,\n+                        class_name=self.__class__.__name__,\n+                    )\n+                    self.build(**shapes_dict)\n+                elif might_have_unbuilt_state(self):\n                     if len(shapes_dict) == 1:\n                         # Single arg: pass it positionally\n-                input_shape = tuple(shapes_dict.values())[0]\n-                with backend.name_scope(self.name):\n-                    if utils.is_default(\n-                        self.build\n-                    ) and might_have_unbuilt_state(self):\n-                        status = self._build_by_run_for_single_pos_arg(\n-                            input_shape\n+                        success = self._build_by_run_for_single_pos_arg(\n+                            tuple(shapes_dict.values())[0]\n                         )\n-                        if not status:\n-                            failure = True\n                     else:\n-                        self.build(input_shape)\n-            else:\n-                with backend.name_scope(self.name):\n-                    if utils.is_default(self.build):\n-                        if might_have_unbuilt_state(self):\n-                            status = self._build_by_run_for_kwargs(shapes_dict)\n-                            if not status:\n-                                failure = True\n-                    else:\n-                        self.build(**shapes_dict)\n-            if failure:\n+                        success = self._build_by_run_for_kwargs(shapes_dict)\n+                    if not success:\n                         if call_spec.eager:\n-                    # Will let the actual eager call do the state-building\n+                            # Will let the actual eager call do state-building\n                             return\n                         raise ValueError(\n                             f\"Layer '{self.name}' looks like it has \"\n@@ -1218,17 +1209,16 @@ def get_arguments_dict(fn, args, kwargs):\n     return arg_dict\n \n \n-def get_shapes_dict(target_fn, call_spec, cls):\n+def get_shapes_dict(call_spec):\n     \"\"\"Convert the call() arguments dict into a dict of input shape arguments.\n \n     Example:\n \n     ```\n-    >>> get_shapes_dict(self.build, call_spec, cls)\n+    >>> get_shapes_dict(call_spec)\n     {\"input_a_shape\": (2, 3)}\n     ```\n     \"\"\"\n-    expected_names = check_shapes_signature(target_fn, call_spec, cls)\n     shapes_dict = {}\n     for k, v in call_spec.tensor_arguments_dict.items():\n         if k == \"mask\" or k.startswith(\"mask_\"):\n@@ -1237,8 +1227,6 @@ def get_shapes_dict(target_fn, call_spec, cls):\n         if k == \"kwargs\" or k == \"args\":\n             # Do not include catch-alls in shapes dict\n             continue\n-        if expected_names is not None and f\"{k}_shape\" not in expected_names:\n-            continue\n         if k in call_spec.nested_tensor_argument_names:\n             shapes_dict[f\"{k}_shape\"] = nest.map_structure(\n                 lambda x: backend.standardize_shape(x.shape), v\n@@ -1248,22 +1236,30 @@ def get_shapes_dict(target_fn, call_spec, cls):\n     return shapes_dict\n \n \n-def check_shapes_signature(target_fn, call_spec, cls):\n-    \"\"\"Asserts that the argument names in `target_fn` match arguments in `call`.\n+def update_shapes_dict_for_target_fn(\n+    target_fn,\n+    shapes_dict,\n+    call_spec,\n+    class_name,\n+):\n+    \"\"\"Updates a `shapes_dict` for `build()` or `compute_output_shape()`.\n \n-    We use this to check that `build()` and `compute_output_shape()` arguments\n-    align with `call()` arguments.\n+    This function will align a dictionary of the shapes of all tensor\n+    passed to `call`, with the signatures of `build()` or\n+    `compute_output_shape()`.\n \n-    For instance if `build()` has the signature\n-    `def build(self, a_shape, b_shape)` we expect `call()` to accept the\n-    arguments `a` and `b`.\n+    The alignment is a follows:\n \n-    When there is a single argument accepted by `target_fn`, we do allow any\n-    name and do not check the call signature.\n+    - If `build()` or `compute_output_shape()` accept only one argument,\n+        forward the shape of the first positional argument from call without\n+        checking any argument names.\n+    - If `build()` or `compute_output_shape()` accept multiple arguments,\n+        enforce that all argument names match a call argument name, e.g.\n+        `foo_shape` would match call argument `foo`.\n \n     Returns:\n-        The list of arguments names expected by the `target_fn` or\n-        `None` if any passed name is acceptable.\n+        An updated `shapes_dict` that can be used to invoke\n+        `target_fn(**shapes_dict)`.\n     \"\"\"\n     if utils.is_default(target_fn):\n         return None\n@@ -1276,31 +1272,40 @@ def check_shapes_signature(target_fn, call_spec, cls):\n             param.KEYWORD_ONLY,\n         ):\n             expected_names.append(name)\n+\n+    # Single arg: don't check names, pass first shape.\n     if len(expected_names) == 1:\n-        return None\n+        key = expected_names[0]\n+        input_shape = tuple(shapes_dict.values())[0]\n+        return {key: input_shape}\n+\n+    # Multiple args: check that all names line up.\n+    kwargs = {}\n     for name in expected_names:\n         method_name = target_fn.__name__\n         error_preamble = (\n             f\"For a `{method_name}()` method with more than one argument, all \"\n             \"arguments should have a `_shape` suffix and match an argument \"\n             f\"from `call()`. E.g. `{method_name}(self, foo_shape, bar_shape)` \"\n-            \"would match `call(self, foo, bar)`.\"\n         )\n         if not name.endswith(\"_shape\"):\n             raise ValueError(\n-                f\"{error_preamble} For layer '{cls.__name__}', \"\n+                f\"{error_preamble} For layer '{class_name}', \"\n                 f\"Received `{method_name}()` argument \"\n                 f\"`{name}`, which does not end in `_shape`.\"\n             )\n         expected_call_arg = utils.removesuffix(name, \"_shape\")\n         if expected_call_arg not in call_spec.arguments_dict:\n             raise ValueError(\n-                f\"{error_preamble} For layer '{cls.__name__}', \"\n+                f\"{error_preamble} For layer '{class_name}', \"\n                 f\"received `{method_name}()` argument \"\n                 f\"`{name}`, but `call()` does not have argument \"\n                 f\"`{expected_call_arg}`.\"\n             )\n-    return expected_names\n+        if name in shapes_dict:\n+            kwargs[name] = shapes_dict[name]\n+\n+    return kwargs\n \n \n class CallContext:\n\n@@ -598,40 +598,77 @@ class LayerTest(testing.TestCase):\n     def test_build_signature_errors(self):\n         class NoShapeSuffix(layers.Layer):\n             def build(self, foo_shape, bar):\n-                self._built = True\n+                self.built = True\n \n             def call(self, foo, bar):\n                 return foo + bar\n \n         class NonMatchingArgument(layers.Layer):\n             def build(self, foo_shape, baz_shape):\n-                self._built = True\n+                self.built = True\n \n             def call(self, foo, bar):\n-                return foo + bar\n+                return foo[:, 0] + bar[:, 0]\n \n         class MatchingArguments(layers.Layer):\n-            def build(self, foo_shape, bar_shape):\n-                self._built = True\n+            def build(self, bar_shape, foo_shape):\n+                self.foo_shape = foo_shape\n+                self.bar_shape = bar_shape\n+                self.built = True\n \n             def call(self, foo, bar):\n-                return foo + bar\n+                return foo[:, 0] + bar[:, 0]\n \n-        foo = backend.numpy.ones((4, 4))\n-        bar = backend.numpy.ones((4, 4))\n+        class SubsetArguments(layers.Layer):\n+            def build(self, baz_shape, foo_shape):\n+                self.foo_shape = foo_shape\n+                self.baz_shape = baz_shape\n+                self.built = True\n+\n+            def call(self, foo, bar=None, baz=None):\n+                return foo[:, 0] + bar[:, 0] + baz[:, 0]\n+\n+        class SingleArgument(layers.Layer):\n+            def build(self, anything_whatsoever):\n+                self.foo_shape = anything_whatsoever\n+                self.built = True\n+\n+            def call(self, foo, bar):\n+                return foo[:, 0] + bar[:, 0]\n+\n+        foo = backend.numpy.ones((4, 1))\n+        bar = backend.numpy.ones((4, 2))\n+        baz = backend.numpy.ones((4, 3))\n         with self.assertRaisesRegex(\n             ValueError,\n             r\"argument `bar`, which does not end in `_shape`\",\n         ):\n-            NoShapeSuffix()(foo, bar)\n+            layer = NoShapeSuffix()\n+            layer(foo, bar)\n \n         with self.assertRaisesRegex(\n             ValueError,\n             r\"`baz_shape`, but `call\\(\\)` does not have argument `baz`\",\n         ):\n-            NonMatchingArgument()(foo, bar)\n+            layer = NonMatchingArgument()\n+            layer(foo, bar)\n \n-        MatchingArguments()(foo, bar)\n+        # Align by name when build and call arguments match.\n+        layer = MatchingArguments()\n+        layer(foo, bar)\n+        self.assertEqual(layer.foo_shape, foo.shape)\n+        self.assertEqual(layer.bar_shape, bar.shape)\n+\n+        # Align by name when build supports a subset of call arguments.\n+        layer = SubsetArguments()\n+        layer(foo, bar, baz)\n+        self.assertEqual(layer.foo_shape, foo.shape)\n+        self.assertEqual(layer.baz_shape, baz.shape)\n+\n+        # When build has only one argument, match the first call argument.\n+        layer = SingleArgument()\n+        layer(foo, bar)\n+        self.assertEqual(layer.foo_shape, foo.shape)\n \n     def test_training_arg_not_specified(self):\n         class NoTrainingSpecified(layers.Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1fe5d5e27f16eb55d1bdc3acc2fd5688e0be0c38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1060 | Contributors (this commit): 7 | Commits (past 90d): 35 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -99,7 +99,7 @@ def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     # Take a larger standard normal dist, discard values outside 2 * stddev\n     # Offset by mean and stddev\n-    x = normal(shape + (4,), mean=0, stddev=1, dtype=dtype, seed=seed)\n+    x = normal(tuple(shape) + (4,), mean=0, stddev=1, dtype=dtype, seed=seed)\n     valid = (x > -2) & (x < 2)\n     indexes = valid.max(-1, keepdim=True)[1]\n     trunc_x = torch.empty(shape, device=get_device())\n\n@@ -91,6 +91,8 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         {\"seed\": 10, \"shape\": (2, 3, 4), \"mean\": 0, \"stddev\": 1},\n         {\"seed\": 10, \"shape\": (2, 3), \"mean\": 10, \"stddev\": 1},\n         {\"seed\": 10, \"shape\": (2, 3), \"mean\": 10, \"stddev\": 3},\n+        # Test list shapes.\n+        {\"seed\": 10, \"shape\": [2, 3], \"mean\": 10, \"stddev\": 3},\n     )\n     def test_truncated_normal(self, seed, shape, mean, stddev):\n         np.random.seed(seed)\n@@ -98,7 +100,7 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         res = random.truncated_normal(\n             shape, mean=mean, stddev=stddev, seed=seed\n         )\n-        self.assertEqual(res.shape, shape)\n+        self.assertEqual(res.shape, tuple(shape))\n         self.assertEqual(res.shape, np_res.shape)\n         self.assertLessEqual(knp.max(res), mean + 2 * stddev)\n         self.assertGreaterEqual(knp.max(res), mean - 2 * stddev)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a5aea0236299132c1497c77e1255bc1710bfffca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 609 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ from keras_core.applications import resnet\n \n @keras_core_export(\n     [\n-        \"keras_core.applications.resnet_v2.ResNet50V2\",\n+        \"keras_core.applications.ResNet50V2\",\n         \"keras_core.applications.resnet_v2.ResNet50V2\",\n     ]\n )\n@@ -45,7 +45,7 @@ def ResNet50V2(\n \n @keras_core_export(\n     [\n-        \"keras_core.applications.resnet_v2.ResNet101V2\",\n+        \"keras_core.applications.ResNet101V2\",\n         \"keras_core.applications.resnet_v2.ResNet101V2\",\n     ]\n )\n@@ -85,7 +85,7 @@ def ResNet101V2(\n \n @keras_core_export(\n     [\n-        \"keras_core.applications.resnet_v2.ResNet152V2\",\n+        \"keras_core.applications.ResNet152V2\",\n         \"keras_core.applications.resnet_v2.ResNet152V2\",\n     ]\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f42f8e28fd1e106c9392e8ba8fbb8e556af10e71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 70 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ class Add(Merge):\n     def _merge_function(self, inputs):\n         output = inputs[0]\n         for i in range(1, len(inputs)):\n-            output += inputs[i]\n+            output = output + inputs[i]\n         return output\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6810a0870e7c7d082c7774409f3d8ecaeecbf88c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 71 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ class Average(Merge):\n     def _merge_function(self, inputs):\n         output = inputs[0]\n         for i in range(1, len(inputs)):\n-            output += inputs[i]\n+            output = output + inputs[i]\n         return output / len(inputs)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd6692a9fdc8b4969e49788c783e1c0f2be847a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1271 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -598,7 +598,7 @@ class R2Score(base_metric.Metric):\n \n     def reset_state(self):\n         for v in self.variables:\n-            v.assign(tf.zeros(v.shape))\n+            v.assign(tf.zeros(v.shape, dtype=v.dtype))\n \n     def get_config(self):\n         config = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4548ae5ca6533a04b06f883ad31b9d32a64eaf6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 103 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 13/8 | Churn Δ: 105 | Churn Cumulative: 4173 | Contributors (this commit): 11 | Commits (past 90d): 157 | Contributors (cumulative): 26 | DMM Complexity: 0.9375\n\nDIFF:\n@@ -152,13 +152,18 @@ def assert_input_compatibility(input_spec, inputs, layer_name):\n             inputs = list_inputs\n \n     inputs = nest.flatten(inputs)\n+    if len(input_spec) != len(inputs):\n+        raise ValueError(\n+            f\"Layer '{layer_name}' expected {len(input_spec)} input(s). \"\n+            f\"Received {len(inputs)} instead.\"\n+        )\n     for x in inputs:\n         # Having a shape/dtype is the only commonality of the various\n         # tensor-like objects that may be passed. The most common kind of\n         # invalid type we are guarding for is a Layer instance (Functional API),\n         # which does not have a `shape` attribute.\n         if not hasattr(x, \"shape\"):\n-            raise TypeError(\n+            raise ValueError(\n                 f\"Inputs to a layer should be tensors. Got '{x}' \"\n                 f\"(of type {type(x)}) as input for layer '{layer_name}'.\"\n             )\n\n@@ -242,7 +242,7 @@ class Layer(BackendLayer, Operation):\n         self.built = False\n         self.dtype_policy = mixed_precision.resolve_policy(dtype)\n         self.autocast = autocast\n-        self.input_spec = None\n+        self._input_spec = None\n         self.supports_jit = True\n \n         self._trainable = trainable\n@@ -299,6 +299,14 @@ class Layer(BackendLayer, Operation):\n         self._metrics = metrics\n         self._seed_generators = seed_generators\n \n+    @property\n+    def input_spec(self):\n+        return self._input_spec\n+\n+    @input_spec.setter\n+    def input_spec(self, value):\n+        self._input_spec = value\n+\n     @utils.default\n     def build(self, input_shape):\n         if utils.is_default(self.build) and might_have_unbuilt_state(self):\n\n@@ -6,6 +6,7 @@ from tensorflow import nest\n \n from keras_core import backend\n from keras_core import operations as ops\n+from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n from keras_core.models.model import Model\n from keras_core.operations.function import Function\n@@ -296,6 +297,46 @@ class Functional(Function, Model):\n         # Symbolic only. TODO\n         raise NotImplementedError\n \n+    @property\n+    def input_spec(self):\n+        if hasattr(self, \"_manual_input_spec\"):\n+            return self._manual_input_spec\n+\n+        def shape_with_no_batch_size(x):\n+            x = list(x)\n+            if x:\n+                x[0] = None\n+            return tuple(x)\n+\n+        if isinstance(self._inputs_struct, dict):\n+            # Case where `_nested_inputs` is a plain dict of Inputs.\n+            names = sorted(self._inputs_struct.keys())\n+            return [\n+                InputSpec(\n+                    shape=shape_with_no_batch_size(\n+                        self._inputs_struct[name].shape\n+                    ),\n+                    allow_last_axis_squeeze=True,\n+                    name=name,\n+                )\n+                for name in names\n+            ]\n+        else:\n+            # Single input, or list/tuple of inputs.\n+            # The data may be passed as a dict keyed by input name.\n+            return [\n+                InputSpec(\n+                    shape=shape_with_no_batch_size(x.shape),\n+                    allow_last_axis_squeeze=True,\n+                    name=x._keras_history[0].name,\n+                )\n+                for x in self._inputs\n+            ]\n+\n+    @input_spec.setter\n+    def input_spec(self, value):\n+        self._manual_input_spec = value\n+\n     def get_config(self):\n         if not functional_like_constructor(self.__class__):\n             # Subclassed networks are not serializable\n\n@@ -4,6 +4,7 @@ from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n from keras_core.layers.core.input_layer import Input\n+from keras_core.layers.input_spec import InputSpec\n from keras_core.models import Functional\n from keras_core.models import Model\n \n@@ -222,6 +223,52 @@ class FunctionalTest(testing.TestCase):\n         model = Functional({\"a\": input_a, \"b\": input_b}, outputs)\n         self.run_class_serialization_test(model)\n \n+    def test_bad_input_spec(self):\n+        # Single input\n+        inputs = Input(shape=(4,))\n+        outputs = layers.Dense(2)(inputs)\n+        model = Functional(inputs, outputs)\n+        with self.assertRaisesRegex(\n+            ValueError, r\"expected shape=\\(None, 4\\), found shape=\\(2, 3\\)\"\n+        ):\n+            model(np.zeros((2, 3)))\n+        with self.assertRaisesRegex(ValueError, \"expected 1 input\"):\n+            model([np.zeros((2, 4)), np.zeros((2, 4))])\n+\n+        # List input\n+        input_a = Input(shape=(4,), name=\"a\")\n+        input_b = Input(shape=(4,), name=\"b\")\n+        x = input_a + input_b\n+        outputs = layers.Dense(2)(x)\n+        model = Functional([input_a, input_b], outputs)\n+        with self.assertRaisesRegex(ValueError, \"expected 2 input\"):\n+            model(np.zeros((2, 3)))\n+        with self.assertRaisesRegex(\n+            ValueError, r\"expected shape=\\(None, 4\\), found shape=\\(2, 3\\)\"\n+        ):\n+            model([np.zeros((2, 3)), np.zeros((2, 4))])\n+\n+        # Dict input\n+        model = Functional({\"a\": input_a, \"b\": input_b}, outputs)\n+        with self.assertRaisesRegex(ValueError, \"expected 2 input\"):\n+            model(np.zeros((2, 3)))\n+        with self.assertRaisesRegex(\n+            ValueError, r\"expected shape=\\(None, 4\\), found shape=\\(2, 3\\)\"\n+        ):\n+            model({\"a\": np.zeros((2, 3)), \"b\": np.zeros((2, 4))})\n+\n+    def test_manual_input_spec(self):\n+        inputs = Input(shape=(None, 3))\n+        outputs = layers.Dense(2)(inputs)\n+        model = Functional(inputs, outputs)\n+        model.input_spec = InputSpec(shape=(None, 4, 3))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"expected shape=\\(None, 4, 3\\), found shape=\\(2, 3, 3\\)\",\n+        ):\n+            model(np.zeros((2, 3, 3)))\n+        model(np.zeros((2, 4, 3)))\n+\n     def test_add_loss(self):\n         # TODO\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0edec40b6974935de0dc0f1880af115c696bd70a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 483 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 483 | Churn Cumulative: 483 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,483 @@\n+\"\"\"\n+Title: Involutional neural networks\n+Author: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498)\n+Date created: 2021/07/25\n+Last modified: 2021/07/25\n+Description: Deep dive into location-specific and channel-agnostic \"involution\" kernels.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+Convolution has been the basis of most modern neural\n+networks for computer vision. A convolution kernel is\n+spatial-agnostic and channel-specific. Because of this, it isn't able\n+to adapt to different visual patterns with respect to\n+different spatial locations. Along with location-related problems, the\n+receptive field of convolution creates challenges with regard to capturing\n+long-range spatial interactions.\n+\n+To address the above issues, Li et. al. rethink the properties\n+of convolution in\n+[Involution: Inverting the Inherence of Convolution for VisualRecognition](https://arxiv.org/abs/2103.06255).\n+The authors propose the \"involution kernel\", that is location-specific and\n+channel-agnostic. Due to the location-specific nature of the operation,\n+the authors say that self-attention falls under the design paradigm of\n+involution.\n+\n+This example describes the involution kernel, compares two image\n+classification models, one with convolution and the other with\n+involution, and also tries drawing a parallel with the self-attention\n+layer.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import tensorflow as tf\n+import keras_core as keras\n+import matplotlib.pyplot as plt\n+\n+# Set seed for reproducibility.\n+tf.random.set_seed(42)\n+\n+\"\"\"\n+## Convolution\n+\n+Convolution remains the mainstay of deep neural networks for computer vision.\n+To understand Involution, it is necessary to talk about the\n+convolution operation.\n+\n+![Imgur](https://i.imgur.com/MSKLsm5.png)\n+\n+Consider an input tensor **X** with dimensions **H**, **W** and\n+**C_in**. We take a collection of **C_out** convolution kernels each of\n+shape **K**, **K**, **C_in**. With the multiply-add operation between\n+the input tensor and the kernels we obtain an output tensor **Y** with\n+dimensions **H**, **W**, **C_out**.\n+\n+In the diagram above `C_out=3`. This makes the output tensor of shape H,\n+W and 3. One can notice that the convoltuion kernel does not depend on\n+the spatial position of the input tensor which makes it\n+**location-agnostic**. On the other hand, each channel in the output\n+tensor is based on a specific convolution filter which makes is\n+**channel-specific**.\n+\"\"\"\n+\n+\"\"\"\n+## Involution\n+\n+The idea is to have an operation that is both **location-specific**\n+and **channel-agnostic**. Trying to implement these specific properties poses\n+a challenge. With a fixed number of involution kernels (for each\n+spatial position) we will **not** be able to process variable-resolution\n+input tensors.\n+\n+To solve this problem, the authors have considered *generating* each\n+kernel conditioned on specific spatial positions. With this method, we\n+should be able to process variable-resolution input tensors with ease.\n+The diagram below provides an intuition on this kernel generation\n+method.\n+\n+![Imgur](https://i.imgur.com/jtrGGQg.png)\n+\"\"\"\n+\n+\n+class Involution(keras.layers.Layer):\n+    def __init__(\n+        self, channel, group_number, kernel_size, stride, reduction_ratio, name\n+    ):\n+        super().__init__(name=name)\n+\n+        # Initialize the parameters.\n+        self.channel = channel\n+        self.group_number = group_number\n+        self.kernel_size = kernel_size\n+        self.stride = stride\n+        self.reduction_ratio = reduction_ratio\n+\n+    def build(self, input_shape):\n+        # Get the shape of the input.\n+        (_, height, width, num_channels) = input_shape\n+\n+        # Scale the height and width with respect to the strides.\n+        height = height // self.stride\n+        width = width // self.stride\n+\n+        # Define a layer that average pools the input tensor\n+        # if stride is more than 1.\n+        self.stride_layer = (\n+            keras.layers.AveragePooling2D(\n+                pool_size=self.stride, strides=self.stride, padding=\"same\"\n+            )\n+            if self.stride > 1\n+            else tf.identity\n+        )\n+        # Define the kernel generation layer.\n+        self.kernel_gen = keras.Sequential(\n+            [\n+                keras.layers.Conv2D(\n+                    filters=self.channel // self.reduction_ratio, kernel_size=1\n+                ),\n+                keras.layers.BatchNormalization(),\n+                keras.layers.ReLU(),\n+                keras.layers.Conv2D(\n+                    filters=self.kernel_size * self.kernel_size * self.group_number,\n+                    kernel_size=1,\n+                ),\n+            ]\n+        )\n+        # Define reshape layers\n+        self.kernel_reshape = keras.layers.Reshape(\n+            target_shape=(\n+                height,\n+                width,\n+                self.kernel_size * self.kernel_size,\n+                1,\n+                self.group_number,\n+            )\n+        )\n+        self.input_patches_reshape = keras.layers.Reshape(\n+            target_shape=(\n+                height,\n+                width,\n+                self.kernel_size * self.kernel_size,\n+                num_channels // self.group_number,\n+                self.group_number,\n+            )\n+        )\n+        self.output_reshape = keras.layers.Reshape(\n+            target_shape=(height, width, num_channels)\n+        )\n+\n+    def call(self, x):\n+        # Generate the kernel with respect to the input tensor.\n+        # B, H, W, K*K*G\n+        kernel_input = self.stride_layer(x)\n+        kernel = self.kernel_gen(kernel_input)\n+\n+        # reshape the kerenl\n+        # B, H, W, K*K, 1, G\n+        kernel = self.kernel_reshape(kernel)\n+\n+        # Extract input patches.\n+        # B, H, W, K*K*C\n+        input_patches = tf.image.extract_patches(\n+            images=x,\n+            sizes=[1, self.kernel_size, self.kernel_size, 1],\n+            strides=[1, self.stride, self.stride, 1],\n+            rates=[1, 1, 1, 1],\n+            padding=\"SAME\",\n+        )\n+\n+        # Reshape the input patches to align with later operations.\n+        # B, H, W, K*K, C//G, G\n+        input_patches = self.input_patches_reshape(input_patches)\n+\n+        # Compute the multiply-add operation of kernels and patches.\n+        # B, H, W, K*K, C//G, G\n+        output = tf.multiply(kernel, input_patches)\n+        # B, H, W, C//G, G\n+        output = tf.reduce_sum(output, axis=3)\n+\n+        # Reshape the output kernel.\n+        # B, H, W, C\n+        output = self.output_reshape(output)\n+\n+        # Return the output tensor and the kernel.\n+        return output, kernel\n+\n+\n+\"\"\"\n+## Testing the Involution layer\n+\"\"\"\n+\n+# Define the input tensor.\n+input_tensor = tf.random.normal((32, 256, 256, 3))\n+\n+# Compute involution with stride 1.\n+output_tensor, _ = Involution(\n+    channel=3, group_number=1, kernel_size=5, stride=1, reduction_ratio=1, name=\"inv_1\"\n+)(input_tensor)\n+print(f\"with stride 1 ouput shape: {output_tensor.shape}\")\n+\n+# Compute involution with stride 2.\n+output_tensor, _ = Involution(\n+    channel=3, group_number=1, kernel_size=5, stride=2, reduction_ratio=1, name=\"inv_2\"\n+)(input_tensor)\n+print(f\"with stride 2 ouput shape: {output_tensor.shape}\")\n+\n+# Compute involution with stride 1, channel 16 and reduction ratio 2.\n+output_tensor, _ = Involution(\n+    channel=16, group_number=1, kernel_size=5, stride=1, reduction_ratio=2, name=\"inv_3\"\n+)(input_tensor)\n+print(\n+    \"with channel 16 and reduction ratio 2 ouput shape: {}\".format(output_tensor.shape)\n+)\n+\n+\"\"\"\n+## Image Classification\n+\n+In this section, we will build an image-classifier model. There will\n+be two models one with convolutions and the other with involutions.\n+\n+The image-classification model is heavily inspired by this\n+[Convolutional Neural Network (CNN)](https://www.tensorflow.org/tutorials/images/cnn)\n+tutorial from Google.\n+\"\"\"\n+\n+\"\"\"\n+## Get the CIFAR10 Dataset\n+\"\"\"\n+\n+# Load the CIFAR10 dataset.\n+print(\"loading the CIFAR10 dataset...\")\n+(\n+    (train_images, train_labels),\n+    (\n+        test_images,\n+        test_labels,\n+    ),\n+) = keras.datasets.cifar10.load_data()\n+\n+# Normalize pixel values to be between 0 and 1.\n+(train_images, test_images) = (train_images / 255.0, test_images / 255.0)\n+\n+# Shuffle and batch the dataset.\n+train_ds = (\n+    tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n+    .shuffle(256)\n+    .batch(256)\n+)\n+test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(256)\n+\n+\"\"\"\n+## Visualise the data\n+\"\"\"\n+\n+class_names = [\n+    \"airplane\",\n+    \"automobile\",\n+    \"bird\",\n+    \"cat\",\n+    \"deer\",\n+    \"dog\",\n+    \"frog\",\n+    \"horse\",\n+    \"ship\",\n+    \"truck\",\n+]\n+\n+plt.figure(figsize=(10, 10))\n+for i in range(25):\n+    plt.subplot(5, 5, i + 1)\n+    plt.xticks([])\n+    plt.yticks([])\n+    plt.grid(False)\n+    plt.imshow(train_images[i])\n+    plt.xlabel(class_names[train_labels[i][0]])\n+plt.show()\n+\n+\"\"\"\n+## Convolutional Neural Network\n+\"\"\"\n+\n+# Build the conv model.\n+print(\"building the convolution model...\")\n+conv_model = keras.Sequential(\n+    [\n+        keras.layers.Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding=\"same\"),\n+        keras.layers.ReLU(name=\"relu1\"),\n+        keras.layers.MaxPooling2D((2, 2)),\n+        keras.layers.Conv2D(64, (3, 3), padding=\"same\"),\n+        keras.layers.ReLU(name=\"relu2\"),\n+        keras.layers.MaxPooling2D((2, 2)),\n+        keras.layers.Conv2D(64, (3, 3), padding=\"same\"),\n+        keras.layers.ReLU(name=\"relu3\"),\n+        keras.layers.Flatten(),\n+        keras.layers.Dense(64, activation=\"relu\"),\n+        keras.layers.Dense(10),\n+    ]\n+)\n+\n+# Compile the mode with the necessary loss function and optimizer.\n+print(\"compiling the convolution model...\")\n+conv_model.compile(\n+    optimizer=\"adam\",\n+    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+    metrics=[\"accuracy\"],\n+    jit_compile=False,\n+)\n+\n+# Train the model.\n+print(\"conv model training...\")\n+conv_hist = conv_model.fit(train_ds, epochs=20, validation_data=test_ds)\n+\n+\"\"\"\n+## Involutional Neural Network\n+\"\"\"\n+\n+# Build the involution model.\n+print(\"building the involution model...\")\n+\n+inputs = keras.Input(shape=(32, 32, 3))\n+x, _ = Involution(\n+    channel=3, group_number=1, kernel_size=3, stride=1, reduction_ratio=2, name=\"inv_1\"\n+)(inputs)\n+x = keras.layers.ReLU()(x)\n+x = keras.layers.MaxPooling2D((2, 2))(x)\n+x, _ = Involution(\n+    channel=3, group_number=1, kernel_size=3, stride=1, reduction_ratio=2, name=\"inv_2\"\n+)(x)\n+x = keras.layers.ReLU()(x)\n+x = keras.layers.MaxPooling2D((2, 2))(x)\n+x, _ = Involution(\n+    channel=3, group_number=1, kernel_size=3, stride=1, reduction_ratio=2, name=\"inv_3\"\n+)(x)\n+x = keras.layers.ReLU()(x)\n+x = keras.layers.Flatten()(x)\n+x = keras.layers.Dense(64, activation=\"relu\")(x)\n+outputs = keras.layers.Dense(10)(x)\n+\n+inv_model = keras.Model(inputs=[inputs], outputs=[outputs], name=\"inv_model\")\n+\n+# Compile the mode with the necessary loss function and optimizer.\n+print(\"compiling the involution model...\")\n+inv_model.compile(\n+    optimizer=\"adam\",\n+    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+    metrics=[\"accuracy\"],\n+    jit_compile=False,\n+)\n+\n+# train the model\n+print(\"inv model training...\")\n+inv_hist = inv_model.fit(train_ds, epochs=20, validation_data=test_ds)\n+\n+\"\"\"\n+## Comparisons\n+\n+In this section, we will be looking at both the models and compare a\n+few pointers.\n+\"\"\"\n+\n+\"\"\"\n+### Parameters\n+\n+One can see that with a similar architecture the parameters in a CNN\n+is much larger than that of an INN (Involutional Neural Network).\n+\"\"\"\n+\n+conv_model.summary()\n+\n+inv_model.summary()\n+\n+\"\"\"\n+### Loss and Accuracy Plots\n+\n+Here, the loss and the accuracy plots demonstrate that INNs are slow\n+learners (with lower parameters).\n+\"\"\"\n+\n+plt.figure(figsize=(20, 5))\n+\n+plt.subplot(1, 2, 1)\n+plt.title(\"Convolution Loss\")\n+plt.plot(conv_hist.history[\"loss\"], label=\"loss\")\n+plt.plot(conv_hist.history[\"val_loss\"], label=\"val_loss\")\n+plt.legend()\n+\n+plt.subplot(1, 2, 2)\n+plt.title(\"Involution Loss\")\n+plt.plot(inv_hist.history[\"loss\"], label=\"loss\")\n+plt.plot(inv_hist.history[\"val_loss\"], label=\"val_loss\")\n+plt.legend()\n+\n+plt.show()\n+\n+plt.figure(figsize=(20, 5))\n+\n+plt.subplot(1, 2, 1)\n+plt.title(\"Convolution Accuracy\")\n+plt.plot(conv_hist.history[\"accuracy\"], label=\"accuracy\")\n+plt.plot(conv_hist.history[\"val_accuracy\"], label=\"val_accuracy\")\n+plt.legend()\n+\n+plt.subplot(1, 2, 2)\n+plt.title(\"Involution Accuracy\")\n+plt.plot(inv_hist.history[\"accuracy\"], label=\"accuracy\")\n+plt.plot(inv_hist.history[\"val_accuracy\"], label=\"val_accuracy\")\n+plt.legend()\n+\n+plt.show()\n+\n+\"\"\"\n+## Visualizing Involution Kernels\n+\n+To visualize the kernels, we take the sum of **K×K** values from each\n+involution kernel. **All the representatives at different spatial\n+locations frame the corresponding heat map.**\n+\n+The authors mention:\n+\n+\"Our proposed involution is reminiscent of self-attention and\n+essentially could become a generalized version of it.\"\n+\n+With the visualization of the kernel we can indeed obtain an attention\n+map of the image. The learned involution kernels provides attention to\n+individual spatial positions of the input tensor. The\n+**location-specific** property makes involution a generic space of models\n+in which self-attention belongs.\n+\"\"\"\n+\n+layer_names = [\"inv_1\", \"inv_2\", \"inv_3\"]\n+# The kernel is the second output of the layer\n+outputs = [inv_model.get_layer(name).output[1] for name in layer_names]\n+vis_model = keras.Model(inv_model.input, outputs)\n+\n+fig, axes = plt.subplots(nrows=10, ncols=4, figsize=(10, 30))\n+\n+for ax, test_image in zip(axes, test_images[:10]):\n+    inv_out = vis_model.predict(test_image[None, ...])\n+    inv1_kernel, inv2_kernel, inv3_kernel = inv_out\n+\n+    inv1_kernel = tf.reduce_sum(inv1_kernel, axis=[-1, -2, -3])\n+    inv2_kernel = tf.reduce_sum(inv2_kernel, axis=[-1, -2, -3])\n+    inv3_kernel = tf.reduce_sum(inv3_kernel, axis=[-1, -2, -3])\n+\n+    ax[0].imshow(keras.utils.array_to_img(test_image))\n+    ax[0].set_title(\"Input Image\")\n+\n+    ax[1].imshow(keras.utils.array_to_img(inv1_kernel[0, ..., None]))\n+    ax[1].set_title(\"Involution Kernel 1\")\n+\n+    ax[2].imshow(keras.utils.array_to_img(inv2_kernel[0, ..., None]))\n+    ax[2].set_title(\"Involution Kernel 2\")\n+\n+    ax[3].imshow(keras.utils.array_to_img(inv3_kernel[0, ..., None]))\n+    ax[3].set_title(\"Involution Kernel 3\")\n+\n+\"\"\"\n+## Conclusions\n+\n+In this example, the main focus was to build an `Involution` layer which\n+can be easily reused. While our comparisons were based on a specific\n+task, feel free to use the layer for different tasks and report your\n+results.\n+\n+According to me, the key take-away of involution is its\n+relationship with self-attention. The intuition behind location-specific\n+and channel-spefic processing makes sense in a lot of tasks.\n+\n+Moving forward one can:\n+\n+- Look at [Yannick's video](https://youtu.be/pH2jZun8MoY) on\n+    involution for a better understanding.\n+- Experiment with the various hyperparameters of the involution layer.\n+- Build different models with the involution layer.\n+- Try building a different kernel generation method altogether.\n+\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/involution)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/involution).\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
