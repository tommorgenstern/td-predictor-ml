{"custom_id": "keras#4eaf56e59b5c49b1d6b709c64961afa31222e6e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1337 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,8 @@ class LocallyConnected1D(Layer):\n             specifying the stride length of the convolution.\n             Specifying any stride value != 1 is incompatible with specifying\n             any `dilation_rate` value != 1.\n-        padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+        padding: Currently only supports `\"valid\"` (case-insensitive).\n+            `\"same\"` may be supported in the future.\n         activation: Activation function to use\n             (see [activations](../activations.md)).\n             If you don't specify anything, no activation is applied\n@@ -221,7 +222,8 @@ class LocallyConnected2D(Layer):\n             all spatial dimensions.\n             Specifying any stride value != 1 is incompatible with specifying\n             any `dilation_rate` value != 1.\n-        padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+        padding: Currently only support `\"valid\"` (case-insensitive).\n+            `\"same\"` will be supported in future.\n         data_format: A string,\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e21c1fa7d39df780c44ff53247c68cc3892a4d2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9067 | Contributors (this commit): 58 | Commits (past 90d): 28 | Contributors (cumulative): 58 | DMM Complexity: None\n\nDIFF:\n@@ -186,7 +186,7 @@ def load_model(filepath, custom_objects=None):\n         ValueError: In case of an invalid savefile.\n     \"\"\"\n     if h5py is None:\n-        raise ImportError('`save_model` requires h5py.')\n+        raise ImportError('`load_model` requires h5py.')\n \n     if not custom_objects:\n         custom_objects = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4dc2a3d6bd7bd186bb79ad9d9cbcde60bf44b72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 6 | Churn Cumulative: 1218 | Contributors (this commit): 13 | Commits (past 90d): 12 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -192,11 +192,13 @@ class KerasClassifier(BaseWrapper):\n                 details about the training history at each epoch.\n         \"\"\"\n         y = np.array(y)\n-        if len(y.shape) != 1:\n+        if len(y.shape) == 2 and y.shape[1] > 1:\n             self.classes_ = np.arange(y.shape[1])\n-        else:\n+        elif (len(y.shape) == 2 and y.shape[1] == 1) or len(y.shape) == 1:\n             self.classes_ = np.unique(y)\n             y = np.searchsorted(self.classes_, y)\n+        else:\n+            raise ValueError('Invalid shape for y')\n         self.n_classes_ = len(self.classes_)\n         return super(KerasClassifier, self).fit(x, y, **kwargs)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ae4a145ea4d25ffcb5611c5f2f4755ee282f1959", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4208 | Contributors (this commit): 39 | Commits (past 90d): 38 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -1205,7 +1205,7 @@ class Model(Container):\n             if batch_index == 0:\n                 for batch_out in batch_outs:\n                     shape = (samples,) + batch_out.shape[1:]\n-                    outs.append(np.zeros(shape, dtype=K.floatx()))\n+                    outs.append(np.zeros(shape, dtype=batch_out.dtype))\n \n             for i, batch_out in enumerate(batch_outs):\n                 outs[i][batch_start:batch_end] = batch_out\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5dc734f4e08b997ae50c4e29a5c4b589595b188", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 704 | Contributors (this commit): 8 | Commits (past 90d): 17 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -229,7 +229,9 @@ class Bidirectional(Wrapper):\n         self.backward_layer.reset_states()\n \n     def build(self, input_shape):\n+        with K.name_scope(self.forward_layer.name):\n             self.forward_layer.build(input_shape)\n+        with K.name_scope(self.backward_layer.name):\n             self.backward_layer.build(input_shape)\n         self.built = True\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#31ecfb28c3202b4e551027bb1f2aef39a9eee936", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 60 | Churn Cumulative: 12385 | Contributors (this commit): 85 | Commits (past 90d): 110 | Contributors (cumulative): 130 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1062,6 +1062,34 @@ def prod(x, axis=None, keepdims=False):\n     return tf.reduce_prod(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n+def cumsum(x, axis=0):\n+    \"\"\"Cumulative sum of the values in a tensor, alongside the specified axis.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: An integer, the axis to compute the sum.\n+\n+    # Returns\n+        A tensor of the cumulative sum of values of `x` along `axis`.\n+    \"\"\"\n+    axis = _normalize_axis(axis, ndim(x))\n+    return tf.cumsum(x, axis=axis)\n+\n+\n+def cumprod(x, axis=0):\n+    \"\"\"Cumulative product of the values in a tensor, alongside the specified axis.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: An integer, the axis to compute the product.\n+\n+    # Returns\n+        A tensor of the cumulative product of values of `x` along `axis`.\n+    \"\"\"\n+    axis = _normalize_axis(axis, ndim(x))\n+    return tf.cumprod(x, axis=axis)\n+\n+\n def var(x, axis=None, keepdims=False):\n     \"\"\"Variance of a tensor, alongside the specified axis.\n \n\n@@ -430,6 +430,32 @@ def prod(x, axis=None, keepdims=False):\n     return T.prod(x, axis=axis, keepdims=keepdims)\n \n \n+def cumsum(x, axis=0):\n+    \"\"\"Cumulative sum of the values in a tensor, alongside the specified axis.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: An integer, the axis to compute the sum.\n+\n+    # Returns\n+        A tensor of the cumulative sum of values of `x` along `axis`.\n+    \"\"\"\n+    return T.extra_ops.cumsum(x, axis=axis)\n+\n+\n+def cumprod(x, axis=0):\n+    \"\"\"Cumulative product of the values in a tensor, alongside the specified axis.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: An integer, the axis to compute the product.\n+\n+    # Returns\n+        A tensor of the cumulative product of values of `x` along `axis`.\n+    \"\"\"\n+    return T.extra_ops.cumprod(x, axis=axis)\n+\n+\n def mean(x, axis=None, keepdims=False):\n     \"\"\"Mean of a tensor, alongside the specified axis.\n     \"\"\"\n\n@@ -283,6 +283,12 @@ class TestBackend(object):\n         check_single_tensor_operation('prod', (4, 2), axis=1, keepdims=True)\n         check_single_tensor_operation('prod', (4, 2, 3), axis=[1, -1])\n \n+        check_single_tensor_operation('cumsum', (4, 2))\n+        check_single_tensor_operation('cumsum', (4, 2), axis=1)\n+\n+        check_single_tensor_operation('cumprod', (4, 2))\n+        check_single_tensor_operation('cumprod', (4, 2), axis=1)\n+\n         # does not work yet, wait for bool <-> int casting in TF (coming soon)\n         # check_single_tensor_operation('any', (4, 2))\n         # check_single_tensor_operation('any', (4, 2), axis=1, keepdims=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9217effdb445d9fd55853f716065579e2ffb9283", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 9639 | Contributors (this commit): 39 | Commits (past 90d): 42 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -1259,7 +1259,7 @@ class ZeroPadding2D(Layer):\n             - If tuple of 2 ints:\n                 interpreted as two different\n                 symmetric padding values for height and width:\n-                `(symmetric_height_pad, symmetrc_width_pad)`.\n+                `(symmetric_height_pad, symmetric_width_pad)`.\n             - If tuple of 2 tuples of 2 ints:\n                 interpreted as\n                 `((top_pad, bottom_pad), (left_pad, right_pad))`\n@@ -1501,7 +1501,7 @@ class Cropping2D(Layer):\n             - If tuple of 2 ints:\n                 interpreted as two different\n                 symmetric cropping values for height and width:\n-                `(symmetric_height_crop, symmetrc_width_crop)`.\n+                `(symmetric_height_crop, symmetric_width_crop)`.\n             - If tuple of 2 tuples of 2 ints:\n                 interpreted as\n                 `((top_crop, bottom_crop), (left_crop, right_crop))`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1a353f06ec83fd7cc8b9b09af7d7463852f7e4b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9641 | Contributors (this commit): 40 | Commits (past 90d): 43 | Contributors (cumulative): 40 | DMM Complexity: None\n\nDIFF:\n@@ -677,7 +677,7 @@ class Conv2DTranspose(Conv2D):\n                  kernel_size,\n                  strides=(1, 1),\n                  padding='valid',\n-                 data_format='channels_last',\n+                 data_format=None,\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9405be8f838f44cb9cd8924bbf7604de880ada6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 22 | Churn Cumulative: 97 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,17 +12,8 @@ def test_locallyconnected_1d():\n     input_dim = 5\n     filter_length = 3\n     filters = 4\n-\n-    for padding in ['valid']:\n-        for strides in [1]:\n-            if padding == 'same' and strides != 1:\n-                continue\n-            layer_test(local.LocallyConnected1D,\n-                       kwargs={'filters': filters,\n-                               'kernel_size': filter_length,\n-                               'padding': padding,\n-                               'strides': strides},\n-                       input_shape=(num_samples, num_steps, input_dim))\n+    padding = 'valid'\n+    strides = 1\n \n     layer_test(local.LocallyConnected1D,\n                kwargs={'filters': filters,\n@@ -37,17 +28,14 @@ def test_locallyconnected_1d():\n \n @keras_test\n def test_locallyconnected_2d():\n-    num_samples = 8\n+    num_samples = 5\n     filters = 3\n     stack_size = 4\n     num_row = 6\n-    num_col = 10\n+    num_col = 8\n+    padding = 'valid'\n \n-    for padding in ['valid']:\n     for strides in [(1, 1), (2, 2)]:\n-            if padding == 'same' and strides != (1, 1):\n-                continue\n-\n         layer_test(local.LocallyConnected2D,\n                    kwargs={'filters': filters,\n                            'kernel_size': 3,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#befbdaa076eedb2787ce340b1a5b4accf93d123d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 30 | Files Changed: 16 | Hunks: 21 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 88 | Churn Cumulative: 4389 | Contributors (this commit): 38 | Commits (past 90d): 84 | Contributors (cumulative): 89 | DMM Complexity: 1.0\n\nDIFF:\n@@ -179,7 +179,9 @@ for iteration in range(1, 200):\n     print()\n     print('-' * 50)\n     print('Iteration', iteration)\n-    model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1,\n+    model.fit(x_train, y_train,\n+              batch_size=BATCH_SIZE,\n+              epochs=1,\n               validation_data=(x_val, y_val))\n     # Select 10 samples from the validation set at random so we can visualize\n     # errors.\n\n@@ -98,8 +98,10 @@ model.compile(loss='categorical_crossentropy',\n \n # train the model\n model.fit(x_train, y_train,\n-          batch_size=batch_size, epochs=epochs,\n-          verbose=1, validation_data=(x_test, y_test))\n+          batch_size=batch_size,\n+          epochs=epochs,\n+          verbose=1,\n+          validation_data=(x_test, y_test))\n \n # next, compare with an equivalent network\n # with2x bigger Dense layers and ReLU\n\n@@ -67,7 +67,9 @@ model.compile(loss='binary_crossentropy',\n               metrics=['accuracy'])\n \n print('Train...')\n-model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n+model.fit(x_train, y_train,\n+          batch_size=batch_size,\n+          epochs=epochs,\n           validation_data=(x_test, y_test))\n score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n print('Test score:', score)\n\n@@ -45,7 +45,9 @@ model.compile(loss='binary_crossentropy',\n               metrics=['accuracy'])\n \n print('Train...')\n-model.fit(x_train, y_train, batch_size=batch_size, epochs=15,\n+model.fit(x_train, y_train,\n+          batch_size=batch_size,\n+          epochs=15,\n           validation_data=(x_test, y_test))\n score, acc = model.evaluate(x_test, y_test,\n                             batch_size=batch_size)\n\n@@ -73,7 +73,9 @@ for iteration in range(1, 60):\n     print()\n     print('-' * 50)\n     print('Iteration', iteration)\n-    model.fit(X, y, batch_size=128, epochs=1)\n+    model.fit(X, y,\n+              batch_size=128,\n+              epochs=1)\n \n     start_index = random.randint(0, len(text) - maxlen - 1)\n \n\n@@ -60,8 +60,11 @@ model.compile(loss=keras.losses.categorical_crossentropy,\n               optimizer=keras.optimizers.Adadelta(),\n               metrics=['accuracy'])\n \n-model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n-          verbose=1, validation_data=(x_test, y_test))\n+model.fit(x_train, y_train,\n+          batch_size=batch_size,\n+          epochs=epochs,\n+          verbose=1,\n+          validation_data=(x_test, y_test))\n score = model.evaluate(x_test, y_test, verbose=0)\n print('Test loss:', score[0])\n print('Test accuracy:', score[1])\n\n@@ -79,8 +79,10 @@ model.compile(loss='categorical_crossentropy',\n \n # Training.\n model.fit(x_train, y_train,\n-          batch_size=batch_size, epochs=epochs,\n-          verbose=1, validation_data=(x_test, y_test))\n+          batch_size=batch_size,\n+          epochs=epochs,\n+          verbose=1,\n+          validation_data=(x_test, y_test))\n \n # Evaluation.\n scores = model.evaluate(x_test, y_test, verbose=0)\n\n@@ -62,8 +62,11 @@ model.compile(loss='categorical_crossentropy',\n               optimizer=rmsprop,\n               metrics=['accuracy'])\n \n-model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n-          verbose=1, validation_data=(x_test, y_test))\n+model.fit(x_train, y_train,\n+          batch_size=batch_size,\n+          epochs=epochs,\n+          verbose=1,\n+          validation_data=(x_test, y_test))\n \n scores = model.evaluate(x_test, y_test, verbose=0)\n print('IRNN test score:', scores[0])\n\n@@ -48,8 +48,10 @@ model.compile(loss='categorical_crossentropy',\n               metrics=['accuracy'])\n \n history = model.fit(x_train, y_train,\n-                    batch_size=batch_size, epochs=epochs,\n-                    verbose=1, validation_data=(x_test, y_test))\n+                    batch_size=batch_size,\n+                    epochs=epochs,\n+                    verbose=1,\n+                    validation_data=(x_test, y_test))\n score = model.evaluate(x_test, y_test, verbose=0)\n print('Test loss:', score[0])\n print('Test accuracy:', score[1])\n\n@@ -231,7 +231,8 @@ def make_teacher_model(train_data, validation_data, epochs=3):\n                   metrics=['accuracy'])\n \n     train_x, train_y = train_data\n-    history = model.fit(train_x, train_y, epochs=epochs,\n+    history = model.fit(train_x, train_y,\n+                        epochs=epochs,\n                         validation_data=validation_data)\n     return model, history\n \n@@ -280,7 +281,8 @@ def make_wider_student_model(teacher_model, train_data,\n                   metrics=['accuracy'])\n \n     train_x, train_y = train_data\n-    history = model.fit(train_x, train_y, epochs=epochs,\n+    history = model.fit(train_x, train_y,\n+                        epochs=epochs,\n                         validation_data=validation_data)\n     return model, history\n \n@@ -328,7 +330,8 @@ def make_deeper_student_model(teacher_model, train_data,\n                   metrics=['accuracy'])\n \n     train_x, train_y = train_data\n-    history = model.fit(train_x, train_y, epochs=epochs,\n+    history = model.fit(train_x, train_y,\n+                        epochs=epochs,\n                         validation_data=validation_data)\n     return model, history\n \n\n@@ -117,9 +117,9 @@ model = Model([input_a, input_b], distance)\n rms = RMSprop()\n model.compile(loss=contrastive_loss, optimizer=rms)\n model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n-          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y),\n           batch_size=128,\n-          epochs=epochs)\n+          epochs=epochs,\n+          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n \n # compute final accuracy on training and test sets\n pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n\n@@ -186,8 +186,10 @@ model = Model(img_input, y)\n model.compile('adam', 'mse')\r\n \r\n # Fit the model\r\n-model.fit(x_train, x_train, validation_data=(x_test, x_test),\r\n-          batch_size=batch_size, epochs=epochs)\r\n+model.fit(x_train, x_train,\r\n+          batch_size=batch_size,\r\n+          epochs=epochs,\r\n+          validation_data=(x_test, x_test))\r\n \r\n # Plot\r\n x_recon = model.predict(x_test[:25])\r\n\n@@ -63,7 +63,8 @@ def train_model(model, train, test, num_classes):\n \n     t = now()\n     model.fit(x_train, y_train,\n-              batch_size=batch_size, epochs=epochs,\n+              batch_size=batch_size,\n+              epochs=epochs,\n               verbose=1,\n               validation_data=(x_test, y_test))\n     print('Training time: %s' % (now() - t))\n\n@@ -143,6 +143,7 @@ model.compile(loss='categorical_crossentropy',\n               optimizer='rmsprop',\n               metrics=['acc'])\n \n-# happy learning!\n-model.fit(x_train, y_train, validation_data=(x_val, y_val),\n-          epochs=10, batch_size=128)\n+model.fit(x_train, y_train,\n+          batch_size=128,\n+          epochs=10,\n+          validation_data=(x_val, y_val))\n\n@@ -50,8 +50,10 @@ model.compile(loss='categorical_crossentropy',\n               metrics=['accuracy'])\n \n history = model.fit(x_train, y_train,\n-                    epochs=epochs, batch_size=batch_size,\n-                    verbose=1, validation_split=0.1)\n+                    batch_size=batch_size,\n+                    epochs=epochs,\n+                    verbose=1,\n+                    validation_split=0.1)\n score = model.evaluate(x_test, y_test,\n                        batch_size=batch_size, verbose=1)\n print('Test score:', score[0])\n\n@@ -70,11 +70,10 @@ for i in range(epochs):\n     # Each of these series are offset by one step and can be\n     # extracted with cos[i::batch_size].\n \n-    model.fit(cos,\n-              expected_output,\n+    model.fit(cos, expected_output,\n               batch_size=batch_size,\n-              verbose=1,\n               epochs=1,\n+              verbose=1,\n               shuffle=False)\n     model.reset_states()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1732555403aeff4f8756637508e2aa43a1be6e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 12172 | Contributors (this commit): 64 | Commits (past 90d): 30 | Contributors (cumulative): 64 | DMM Complexity: None\n\nDIFF:\n@@ -129,7 +129,7 @@ class SpatialDropout1D(Dropout):\n     between feature maps and should be used instead.\n \n     # Arguments\n-        p: float between 0 and 1. Fraction of the input units to drop.\n+        rate: float between 0 and 1. Fraction of the input units to drop.\n \n     # Input shape\n         3D tensor with shape:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0be8040e7988996232da210d04002f170a5d3f98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 40 | Churn Cumulative: 807 | Contributors (this commit): 10 | Commits (past 90d): 21 | Contributors (cumulative): 12 | DMM Complexity: 0.25\n\nDIFF:\n@@ -2,6 +2,7 @@\n from __future__ import absolute_import\n \n import copy\n+import inspect\n from ..engine import Layer\n from ..engine import InputSpec\n from .. import backend as K\n@@ -208,21 +209,36 @@ class Bidirectional(Wrapper):\n         elif self.merge_mode is None:\n             return [self.forward_layer.compute_output_shape(input_shape)] * 2\n \n-    def call(self, inputs, mask=None):\n-        y = self.forward_layer.call(inputs, mask)\n-        y_rev = self.backward_layer.call(inputs, mask)\n+    def call(self, inputs, training=None, mask=None):\n+        func_args = inspect.getargspec(self.layer.call).args\n+        kwargs = {}\n+        for arg in ('training', 'mask'):\n+            if arg in func_args:\n+                kwargs[arg] = eval(arg)\n+\n+        y = self.forward_layer.call(inputs, **kwargs)\n+        y_rev = self.backward_layer.call(inputs, **kwargs)\n         if self.return_sequences:\n             y_rev = K.reverse(y_rev, 1)\n         if self.merge_mode == 'concat':\n-            return K.concatenate([y, y_rev])\n+            output = K.concatenate([y, y_rev])\n         elif self.merge_mode == 'sum':\n-            return y + y_rev\n+            output = y + y_rev\n         elif self.merge_mode == 'ave':\n-            return (y + y_rev) / 2\n+            output = (y + y_rev) / 2\n         elif self.merge_mode == 'mul':\n-            return y * y_rev\n+            output = y * y_rev\n         elif self.merge_mode is None:\n-            return [y, y_rev]\n+            output = [y, y_rev]\n+\n+        # Properly set learning phase\n+        if 0 < self.layer.dropout + self.layer.recurrent_dropout:\n+            if self.merge_mode is None:\n+                for out in output:\n+                    out._uses_learning_phase = True\n+            else:\n+                output._uses_learning_phase = True\n+        return output\n \n     def reset_states(self):\n         self.forward_layer.reset_states()\n\n@@ -103,6 +103,7 @@ def test_Bidirectional():\n     dim = 2\n     timesteps = 2\n     output_dim = 2\n+    dropout_rate = 0.2\n     for mode in ['sum', 'concat']:\n         x = np.random.random((samples, timesteps, dim))\n         target_dim = 2 * output_dim if mode == 'concat' else output_dim\n@@ -110,7 +111,8 @@ def test_Bidirectional():\n \n         # test with Sequential model\n         model = Sequential()\n-        model.add(wrappers.Bidirectional(rnn(output_dim),\n+        model.add(wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n+                                             recurrent_dropout=dropout_rate),\n                                          merge_mode=mode, input_shape=(timesteps, dim)))\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, epochs=1, batch_size=1)\n@@ -130,7 +132,9 @@ def test_Bidirectional():\n \n         # test with functional API\n         input = Input((timesteps, dim))\n-        output = wrappers.Bidirectional(rnn(output_dim), merge_mode=mode)(input)\n+        output = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n+                                            recurrent_dropout=dropout_rate),\n+                                        merge_mode=mode)(input)\n         model = Model(input, output)\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, epochs=1, batch_size=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#57ff6e99cada8eed046f68c99faf6dad799b96c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 9071 | Contributors (this commit): 59 | Commits (past 90d): 29 | Contributors (cumulative): 59 | DMM Complexity: None\n\nDIFF:\n@@ -1180,13 +1180,13 @@ class Sequential(Model):\n         return copy.deepcopy(config)\n \n     @classmethod\n-    def from_config(cls, config):\n+    def from_config(cls, config, custom_objects=None):\n         if 'class_name' not in config[0] or config[0]['class_name'] == 'Merge':\n             return cls.legacy_from_config(config)\n \n         model = cls()\n         for conf in config:\n-            layer = layer_module.deserialize(conf)\n+            layer = layer_module.deserialize(conf, custom_objects=custom_objects)\n             model.add(layer)\n         return model\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50057d8fe23de4e88c1b0b45eafc5bffd922a941", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 185 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 36/31 | Churn Δ: 195 | Churn Cumulative: 858 | Contributors (this commit): 5 | Commits (past 90d): 18 | Contributors (cumulative): 8 | DMM Complexity: 0.26277372262773724\n\nDIFF:\n@@ -18,6 +18,44 @@ class _Merge(Layer):\n     def _merge_function(self, inputs):\n         raise NotImplementedError\n \n+    def _compute_elemwise_op_output_shape(self, shape1, shape2):\n+        \"\"\"Computes the shape of the resultant of an elementwise operation.\n+\n+        # Arguments\n+            shape1: tuple or None. Shape of the first tensor\n+            shape2: tuple or None. Shape of the second tensor\n+\n+        # Returns\n+            expected output shape when an element-wise operation is\n+            carried out on 2 tensors with shapes shape1 and shape2.\n+            tuple or None.\n+\n+        # Raises\n+            ValueError if shape1 and shape2 are not compaible for\n+            element-wise operations\n+        \"\"\"\n+        if None in [shape1, shape2]:\n+            return None\n+        elif len(shape1) < len(shape2):\n+            return _compute_elemwise_op_output_shape(shape2, shape1)\n+        elif len(shape2) == 0:\n+            return shape1\n+        output_shape = list(shape1[:-len(shape2)])\n+        for i, j in zip(shape1[-len(shape2):], shape2):\n+            if i is None or j is None:\n+                output_shape.append(None)\n+            elif i == 1:\n+                output_shape.append(j)\n+            elif j == 1:\n+                output_shape.append(i)\n+            else:\n+                if i != j:\n+                    raise ValueError('Operands could not be broadcast '\n+                                     'together with shapes ' +\n+                                     str(shape1) + ' ' + str(shape2))\n+                output_shape.append(i)\n+        return tuple(output_shape)\n+\n     def build(self, input_shape):\n         # Used purely for shape validation.\n         if not isinstance(input_shape, list):\n@@ -27,20 +65,106 @@ class _Merge(Layer):\n             raise ValueError('A merge layer should be called '\n                              'on a list of at least 2 inputs. '\n                              'Got ' + str(len(input_shape)) + ' inputs.')\n-        if all([shape is None for shape in input_shape]):\n-            return\n-        # TODO: handle shapes with None entries.\n-        input_shapes_set = set(input_shape)\n-        if None in input_shapes_set:\n-            input_shapes_set.remove(None)\n-        if len(input_shapes_set) > 1:\n-            raise ValueError('Only tensors of same shape can '\n-                             'be merged by layer' + self.name +\n-                             ' Got input shapes: %s' % input_shape)\n+        batch_sizes = [s[0] for s in input_shape if s is not None]\n+        batch_sizes = set(batch_sizes)\n+        batch_sizes -= set([None])\n+        if len(batch_sizes) > 1:\n+            raise ValueError('Can not merge tensors with different '\n+                             'batch sizes. Got tensors with shapes : ' +\n+                             str(input_shape))\n+        if input_shape[0] is None:\n+            output_shape = None\n+        else:\n+            output_shape = input_shape[0][1:]\n+        for i in range(1, len(input_shape)):\n+            if input_shape[i] is None:\n+                shape = None\n+            else:\n+                shape = input_shape[i][1:]\n+            output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n+        # If the inputs have different ranks, we have to reshape them\n+        # to make them broadcastable.\n+        if None not in input_shape and len(set(map(len, input_shape))) == 1:\n+            self._reshape_required = False\n+        else:\n+            self._reshape_required = True\n \n     def call(self, inputs):\n+        if self._reshape_required:\n+            reshaped_inputs = []\n+            input_ndims = list(map(K.ndim, inputs))\n+            if None not in input_ndims:\n+                # If ranks of all inputs are available,\n+                # we simply expand each of them at axis=1\n+                # until all of them have the same rank.\n+                max_ndim = max(input_ndims)\n+                for x in inputs:\n+                    x_ndim = K.ndim(x)\n+                    for _ in range(max_ndim - x_ndim):\n+                        x = K.expand_dims(x, 1)\n+                    reshaped_inputs.append(x)\n+                return self._merge_function(reshaped_inputs)\n+            else:\n+                # Transpose all inputs so that batch size is the last dimension.\n+                # (batch_size, dim1, dim2, ... ) -> (dim1, dim2, ... , batch_size)\n+                transposed = False\n+                for x in inputs:\n+                    x_ndim = K.ndim(x)\n+                    if x_ndim is None:\n+                        x_shape = K.shape(x)\n+                        batch_size = x_shape[0]\n+                        new_shape = K.concatenate([x_shape[1:], K.expand_dims(batch_size)])\n+                        x_transposed = K.reshape(x, K.stack([batch_size, K.prod(x_shape[1:])]))\n+                        x_transposed = K.permute_dimensions(x_transposed, (1, 0))\n+                        x_transposed = K.reshape(x_transposed, new_shape)\n+                        reshaped_inputs.append(x_transposed)\n+                        transposed = True\n+                    elif x_ndim > 1:\n+                        dims = list(range(1, x_ndim)) + [0]\n+                        reshaped_inputs.append(K.permute_dimensions(x, dims))\n+                        transposed = True\n+                    else:\n+                        # We don't transpose inputs if they are 1D vectors or scalars.\n+                        reshaped_inputs.append(x)\n+                y = self._merge_function(reshaped_inputs)\n+                y_ndim = K.ndim(y)\n+                if transposed:\n+                    # If inputs have been transposed, we have to transpose the output too.\n+                    if y_ndim is None:\n+                        y_shape = K.shape(y)\n+                        y_ndim = K.shape(y_shape)[0]\n+                        batch_size = y_shape[y_ndim - 1]\n+                        new_shape = K.concatenate([K.expand_dims(batch_size), y_shape[:y_ndim - 1]])\n+                        y = K.reshape(y, (-1, batch_size))\n+                        y = K.permute_dimensions(y, (1, 0))\n+                        y = K.reshape(y, new_shape)\n+                    elif y_ndim > 1:\n+                        dims = [y_ndim - 1] + list(range(y_ndim - 1))\n+                        y = K.permute_dimensions(y, dims)\n+                return y\n+        else:\n             return self._merge_function(inputs)\n \n+    def compute_output_shape(self, input_shape):\n+        if input_shape[0] is None:\n+            output_shape = None\n+        else:\n+            output_shape = input_shape[0][1:]\n+        for i in range(1, len(input_shape)):\n+            if input_shape[i] is None:\n+                shape = None\n+            else:\n+                shape = input_shape[i][1:]\n+            output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n+        batch_sizes = [s[0] for s in input_shape if s is not None]\n+        batch_sizes = set(batch_sizes)\n+        batch_sizes -= set([None])\n+        if len(batch_sizes) == 1:\n+            output_shape = (batch_sizes[0],) + output_shape\n+        else:\n+            output_shape = (None,) + output_shape\n+        return output_shape\n+\n     def compute_output_shape(self, input_shape):\n         # Layers that change the shape should already implement\n         # compute_output_shape anyway\n\n@@ -3,6 +3,7 @@ import numpy as np\n from numpy.testing import assert_allclose\n from keras import layers\n from keras import models\n+from keras import backend as K\n from keras.utils.test_utils import layer_test\n from keras.utils.test_utils import keras_test\n from keras.layers import merge\n@@ -137,5 +138,55 @@ def test_merge_dot():\n     assert_allclose(out, expected, atol=1e-4)\n \n \n+@keras_test\n+def test_merge_broadcast():\n+    # shapes provided\n+    i1 = layers.Input(shape=(4, 5))\n+    i2 = layers.Input(shape=(5,))\n+    ops = [layers.add, layers.maximum]\n+    for op in ops:\n+        o = op([i1, i2])\n+        assert o._keras_shape == (None, 4, 5)\n+        model = models.Model([i1, i2], o)\n+\n+        x1 = np.random.random((2, 4, 5))\n+        x2 = np.random.random((2, 5))\n+        out = model.predict([x1, x2])\n+        assert out.shape == (2, 4, 5)\n+\n+    # shapes not provided\n+    i1 = layers.Input(shape=(None, None))\n+    i2 = layers.Input(shape=(None,))\n+    ops = [layers.add, layers.maximum]\n+    for op in ops:\n+        o = op([i1, i2])\n+        assert o._keras_shape == (None, None, None)\n+        model = models.Model([i1, i2], o)\n+\n+        x1 = np.random.random((2, 4, 5))\n+        x2 = np.random.random((2, 5))\n+        out = model.predict([x1, x2])\n+        assert out.shape == (2, 4, 5)\n+\n+    # ndim not provided\n+    if K.backend() == 'tensorflow':\n+        k_ndim = K.ndim\n+        K.ndim = lambda _: None\n+\n+        i1 = layers.Input(shape=(None, None))\n+        i2 = layers.Input(shape=(None,))\n+        ops = [layers.add, layers.maximum]\n+        for op in ops:\n+            o = op([i1, i2])\n+            assert o._keras_shape == (None, None, None)\n+            model = models.Model([i1, i2], o)\n+\n+            x1 = np.random.random((2, 4, 5))\n+            x2 = np.random.random((2, 5))\n+            out = model.predict([x1, x2])\n+            assert out.shape == (2, 4, 5)\n+        K.ndim = k_ndim\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#568d1a5b8a9b592fe988f304fea90826862d416f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 12410 | Contributors (this commit): 85 | Commits (past 90d): 113 | Contributors (cumulative): 131 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3341,7 +3341,7 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n \n # HIGH ORDER FUNCTIONS\n \n-def map_fn(fn, elems, name=None):\n+def map_fn(fn, elems, name=None, dtype=None):\n     \"\"\"Map the function fn over the elements elems and return the outputs.\n \n     # Arguments\n@@ -3353,7 +3353,7 @@ def map_fn(fn, elems, name=None):\n         Tensor with first dimension equal to the elems and second depending on\n         fn\n     \"\"\"\n-    return tf.map_fn(fn, elems, name=name)\n+    return tf.map_fn(fn, elems, name=name, dtype=dtype)\n \n \n def foldl(fn, elems, initializer=None, name=None):\n\n@@ -2127,7 +2127,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n \n # HIGH ORDER FUNCTIONS\n \n-def map_fn(fn, elems, name=None):\n+def map_fn(fn, elems, name=None, dtype=None):\n     \"\"\"Map the function fn over the elements elems and return the outputs.\n \n     # Arguments\n\n@@ -987,15 +987,25 @@ class TestBackend(object):\n     def test_map(self):\n         x = np.random.rand(10, 3).astype(np.float32)\n         for K in [KTF, KTH]:\n-            kx = K.eval(K.map_fn(K.sum, x))\n+            vx = K.variable(x)\n+            kx = K.eval(K.map_fn(K.sum, vx))\n+            # make sure we can also walk the indexes in tensorflow which we\n+            # can't without specifying dtype\n+            kx2 = K.eval(K.map_fn(\n+                lambda i: K.sum(vx[i]),\n+                K.arange(10),\n+                dtype=K.floatx()\n+            ))\n \n             assert (10,) == kx.shape\n+            assert (10,) == kx2.shape\n             assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n+            assert_allclose(kx, kx2, atol=1e-05)\n \n     def test_foldl(self):\n         x = np.random.rand(10, 3).astype(np.float32)\n         for K in [KTF, KTH]:\n-            kx = K.eval(K.foldl(lambda a, b: a + b, x))\n+            kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n \n             assert (3,) == kx.shape\n             assert_allclose(x.sum(axis=0), kx, atol=1e-05)\n@@ -1007,8 +1017,9 @@ class TestBackend(object):\n         # right to left we have no such problem and the result is larger\n         x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n         for K in [KTF, KTH]:\n-            p1 = K.eval(K.foldl(lambda a, b: a * b, x))\n-            p2 = K.eval(K.foldr(lambda a, b: a * b, x))\n+            vx = K.variable(x)\n+            p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n+            p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n \n             assert p1 < p2\n             assert 9e-38 < p2 <= 1e-37\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f6fb452a231dc83468b9c3be29cd160af53ea9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 445 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -101,7 +101,7 @@ def build_discriminator():\n     cnn.add(LeakyReLU())\n     cnn.add(Dropout(0.3))\n \n-    cnn.add(Conv2D(64, 3, padding='same', strides=2))\n+    cnn.add(Conv2D(64, 3, padding='same', strides=1))\n     cnn.add(LeakyReLU())\n     cnn.add(Dropout(0.3))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#466f0b91f114a2152886e685969c404bca8de34b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 662 | Contributors (this commit): 6 | Commits (past 90d): 14 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,7 @@ class _Merge(Layer):\n         if None in [shape1, shape2]:\n             return None\n         elif len(shape1) < len(shape2):\n-            return _compute_elemwise_op_output_shape(shape2, shape1)\n+            return self._compute_elemwise_op_output_shape(shape2, shape1)\n         elif len(shape2) == 0:\n             return shape1\n         output_shape = list(shape1[:-len(shape2)])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3dd5fc88f78f0bad58e5e10032132d36b019604b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 669 | Contributors (this commit): 6 | Commits (past 90d): 15 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -165,13 +165,6 @@ class _Merge(Layer):\n             output_shape = (None,) + output_shape\n         return output_shape\n \n-    def compute_output_shape(self, input_shape):\n-        # Layers that change the shape should already implement\n-        # compute_output_shape anyway\n-        # TODO: If the merge layer in the future accepts broadcastable inputs\n-        #       then both this function and build should be changed\n-        return input_shape[0]\n-\n     def compute_mask(self, inputs, mask=None):\n         if mask is None:\n             return None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa4c747b7e9ef8deca9f806dd8e77315f805f2ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1744 | Contributors (this commit): 16 | Commits (past 90d): 24 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -91,7 +91,7 @@ else:\n         horizontal_flip=True,  # randomly flip images\n         vertical_flip=False)  # randomly flip images\n \n-    # Compute quantities required for featurewise normalization\n+    # Compute quantities required for feature-wise normalization\n     # (std, mean, and principal components if ZCA whitening is applied).\n     datagen.fit(x_train)\n \n\n@@ -222,7 +222,7 @@ if __name__ == '__main__':\n             noise = np.random.uniform(-1, 1, (2 * batch_size, latent_size))\n             sampled_labels = np.random.randint(0, 10, 2 * batch_size)\n \n-            # we want to train the genrator to trick the discriminator\n+            # we want to train the generator to trick the discriminator\n             # For the generator, we want all the {fake, not-fake} labels to say\n             # not-fake\n             trick = np.ones(2 * batch_size)\n\n@@ -26,7 +26,7 @@ Notes\n \n Experiments\n - Teacher model: a basic CNN model trained on MNIST for 3 epochs.\n-- Net2WiderNet exepriment:\n+- Net2WiderNet experiment:\n   + Student model has a wider Conv2D layer and a wider FC layer.\n   + Comparison of 'random-padding' vs 'net2wider' weight initialization.\n   + With both methods, student model should immediately perform as well as\n\n@@ -35,12 +35,12 @@ applied as a bias because we know the MNIST digits are mapped to [0,1].\n References:\r\n [3]\r\n 'Deep Residual Learning for Image Recognition'\r\n-Kaiming He, xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n+Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n https://arxiv.org/abs/1512.03385v1\r\n \r\n [4]\r\n 'Identity Mappings in Deep Residual Networks'\r\n-Kaiming He, xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n+Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n https://arxiv.org/abs/1603.05027v3\r\n \r\n '''\r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80b72fa7b3ac69f821ea63a1e2a92412da188117", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 7451 | Contributors (this commit): 66 | Commits (past 90d): 56 | Contributors (cumulative): 66 | DMM Complexity: 1.0\n\nDIFF:\n@@ -67,6 +67,7 @@ def clear_session():\n     reset_uids()\n     _SESSION = None\n     phase = tf.placeholder(dtype='bool', name='keras_learning_phase')\n+    _GRAPH_LEARNING_PHASES = {}\n     _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = phase\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2321e61e1732f7a27841eca36cdaf34ed5d26dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 7658 | Contributors (this commit): 30 | Commits (past 90d): 50 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -503,6 +503,8 @@ class Layer(object):\n             ValueError: in case the layer is missing shape information\n                 for its `build` call.\n         \"\"\"\n+        if type(inputs) is list:\n+            inputs = inputs[:]\n         with K.name_scope(self.name):\n             # Handle laying building (weight creating, input spec locking).\n             if not self.built:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4eff36910b86d6b5618cafdb4ae17bb932a5265b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 245 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n from __future__ import print_function\n+import os\n import pytest\n import numpy as np\n from keras.models import Sequential\n@@ -85,6 +86,8 @@ def test_multiprocessing_training_fromfile():\n                         max_q_size=10,\n                         pickle_safe=False)\n \n+    os.remove('data.npz')\n+\n \n @keras_test\n def test_multiprocessing_predicting():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
