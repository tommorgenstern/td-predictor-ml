{"custom_id": "scrapy#ce03ccd4ecea6e3d341da23202024053428529e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 11 | Churn Cumulative: 122 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,17 +23,6 @@ class DepthMiddleware(object):\n         maxdepth = settings.getint('DEPTH_LIMIT')\n         usestats = settings.getbool('DEPTH_STATS')\n         verbose = settings.getbool('DEPTH_STATS_VERBOSE')\n-        sorder = settings['SCHEDULER_ORDER']\n-        if sorder:\n-            # XXX: backwards compatibility with old SCHEDULER_ORDER setting\n-            # will be removed on Scrapy 0.15\n-            warnings.warn(\"SCHEDULER_ORDER setting is deprecated, \" \\\n-                \"use DEPTH_PRIORITY instead\", ScrapyDeprecationWarning)\n-            if sorder == 'BFO':\n-                prio = 1\n-            elif sorder == 'DFO':\n-                prio = -1\n-        else:\n         prio = settings.getint('DEPTH_PRIORITY')\n         if usestats:\n             from scrapy.stats import stats\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#431441cb525829712db8e12014f19577cbd008c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 32 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,7 @@ def setup(app):\n     app.add_role('source', source_role)\n \n def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n-    url = 'http://dev.scrapy.org/browser/' + text\n+    url = 'https://github.com/scrapy/scrapy/blob/master/' + text\n     set_classes(options)\n     node = nodes.reference(rawtext, text, refuri=ref, **options)\n     return [node], []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#43b79afc9cdee1cb9eeba9c6bd56666db91f3011", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 244 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -221,4 +221,4 @@ class LifoDiskQueueTest(LifoMemoryQueueTest):\n         q = self.queue()\n         q.pop()\n         q.close()\n-        self.assertLess(os.path.getsize(self.path), size)\n+        assert os.path.getsize(self.path), size\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#13cd9a1b0fda2ffaa475db2d913cfcefd448bffb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 22 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n \"\"\"Set User-Agent header per spider or use a default value from settings\"\"\"\n \n from scrapy.utils.python import WeakKeyCache\n-from scrapy.utils import deprecate\n \n \n class UserAgentMiddleware(object):\n@@ -12,7 +11,6 @@ class UserAgentMiddleware(object):\n \n     def _user_agent(self, spider):\n         if hasattr(spider, 'user_agent'):\n-            deprecate.attribute(spider, 'user_agent', 'USER_AGENT')\n             return spider.user_agent\n         return spider.settings['USER_AGENT']\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ade5efdc617a3f6c33b1c968862e0cbdc277d0d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 130 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,6 +16,10 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n             help=\"set spider argument (may be repeated)\")\n+        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", \\\n+            help=\"store scraped items into FILE (using feed exports)\")\n+        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\", default=\"jsonlines\", \\\n+            help=\"format to use in feed exports (default: %default)\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -23,6 +27,9 @@ class Command(ScrapyCommand):\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n             raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n+        if opts.output:\n+            self.settings.overrides['FEED_URI'] = opts.output\n+            self.settings.overrides['FEED_FORMAT'] = opts.output_format\n \n     def run(self, args, opts):\n         if len(args) < 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#10ced29e18a4408c989c0ec39741fa3051cb3d37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 43 | Files Changed: 2 | Hunks: 24 | Methods Changed: 14 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 111 | Churn Cumulative: 449 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,6 @@ from tempfile import TemporaryFile\n from datetime import datetime\n from urlparse import urlparse\n from ftplib import FTP\n-from shutil import copyfileobj\n \n from zope.interface import Interface, implements\n from twisted.internet import defer, threads\n@@ -29,7 +28,11 @@ class IFeedStorage(Interface):\n     def __init__(uri):\n         \"\"\"Initialize the storage with the parameters given in the URI\"\"\"\n \n-    def store(file, spider):\n+    def open(spider):\n+        \"\"\"Open the storage for the given spider. It must return a file-like\n+        object that will be used for the exporters\"\"\"\n+\n+    def store(file):\n         \"\"\"Store the given file stream\"\"\"\n \n \n@@ -37,10 +40,13 @@ class BlockingFeedStorage(object):\n \n     implements(IFeedStorage)\n \n-    def store(self, file, spider):\n-        return threads.deferToThread(self._store_in_thread, file, spider)\n+    def open(self, spider):\n+        return TemporaryFile(prefix='feed-')\n \n-    def _store_in_thread(self, file, spider):\n+    def store(self, file):\n+        return threads.deferToThread(self._store_in_thread, file)\n+\n+    def _store_in_thread(self, file):\n         raise NotImplementedError\n \n \n@@ -51,23 +57,27 @@ class StdoutFeedStorage(object):\n     def __init__(self, uri, _stdout=sys.stdout):\n         self._stdout = _stdout\n \n-    def store(self, file, spider):\n-        copyfileobj(file, self._stdout)\n+    def open(self, spider):\n+        return self._stdout\n \n+    def store(self, file):\n+        pass\n \n-class FileFeedStorage(BlockingFeedStorage):\n+class FileFeedStorage(object):\n+\n+    implements(IFeedStorage)\n \n     def __init__(self, uri):\n         self.path = file_uri_to_path(uri)\n \n-    def _store_in_thread(self, file, spider):\n+    def open(self, spider):\n         dirname = os.path.dirname(self.path)\n         if dirname and not os.path.exists(dirname):\n             os.makedirs(dirname)\n-        f = open(self.path, 'wb')\n-        copyfileobj(file, f)\n-        f.close()\n+        return open(self.path, 'ab')\n \n+    def store(self, file):\n+        file.close()\n \n class S3FeedStorage(BlockingFeedStorage):\n \n@@ -83,7 +93,8 @@ class S3FeedStorage(BlockingFeedStorage):\n         self.secret_key = u.password or settings['AWS_SECRET_ACCESS_KEY']\n         self.keyname = u.path\n \n-    def _store_in_thread(self, file, spider):\n+    def _store_in_thread(self, file):\n+        file.seek(0)\n         conn = self.connect_s3(self.access_key, self.secret_key)\n         bucket = conn.get_bucket(self.bucketname, validate=False)\n         key = bucket.new_key(self.keyname)\n@@ -101,7 +112,8 @@ class FTPFeedStorage(BlockingFeedStorage):\n         self.password = u.password\n         self.path = u.path\n \n-    def _store_in_thread(self, file, spider):\n+    def _store_in_thread(self, file):\n+        file.seek(0)\n         ftp = FTP()\n         ftp.connect(self.host, self.port)\n         ftp.login(self.username, self.password)\n@@ -112,9 +124,11 @@ class FTPFeedStorage(BlockingFeedStorage):\n \n \n class SpiderSlot(object):\n-    def __init__(self, file, exp):\n+    def __init__(self, file, exporter, storage, uri):\n         self.file = file\n-        self.exporter = exp\n+        self.exporter = exporter\n+        self.storage = storage\n+        self.uri = uri\n         self.itemcount = 0\n \n class FeedExporter(object):\n@@ -139,10 +153,12 @@ class FeedExporter(object):\n         dispatcher.connect(self.item_scraped, signals.item_scraped)\n \n     def open_spider(self, spider):\n-        file = TemporaryFile(prefix='feed-')\n-        exp = self._get_exporter(file)\n-        exp.start_exporting()\n-        self.slots[spider] = SpiderSlot(file, exp)\n+        uri = self.urifmt % self._get_uri_params(spider)\n+        storage = self._get_storage(uri)\n+        file = storage.open(spider)\n+        exporter = self._get_exporter(file)\n+        exporter.start_exporting()\n+        self.slots[spider] = SpiderSlot(file, exporter, storage, uri)\n \n     def close_spider(self, spider):\n         slot = self.slots.pop(spider)\n@@ -150,14 +166,9 @@ class FeedExporter(object):\n             return\n         slot.exporter.finish_exporting()\n         nbytes = slot.file.tell()\n-        slot.file.seek(0)\n-        uri = self.urifmt % self._get_uri_params(spider)\n-        storage = self._get_storage(uri)\n-        if not storage:\n-            return\n         logfmt = \"%%s %s feed (%d items, %d bytes) in: %s\" % (self.format, \\\n-            slot.itemcount, nbytes, uri)\n-        d = defer.maybeDeferred(storage.store, slot.file, spider)\n+            slot.itemcount, nbytes, slot.uri)\n+        d = defer.maybeDeferred(slot.storage.store, slot.file)\n         d.addCallback(lambda _: log.msg(logfmt % \"Stored\", spider=spider))\n         d.addErrback(log.err, logfmt % \"Error storing\", spider=spider)\n         d.addBoth(lambda _: slot.file.close())\n\n@@ -10,18 +10,7 @@ from scrapy.spider import BaseSpider\n from scrapy.contrib.feedexport import IFeedStorage, FileFeedStorage, FTPFeedStorage, S3FeedStorage, StdoutFeedStorage\n from scrapy.utils.test import assert_aws_environ\n \n-class FeedStorageTest(unittest.TestCase):\n-\n-    @defer.inlineCallbacks\n-    def _assert_stores(self, storage, path):\n-        yield storage.store(StringIO(\"content\"), BaseSpider(\"default\"))\n-        self.failUnless(os.path.exists(path))\n-        self.failUnlessEqual(open(path).read(), \"content\")\n-        # again, to check files are overwritten properly\n-        yield storage.store(StringIO(\"new content\"), BaseSpider(\"default\"))\n-        self.failUnlessEqual(open(path).read(), \"new content\")\n-\n-class FileFeedStorageTest(FeedStorageTest):\n+class FileFeedStorageTest(unittest.TestCase):\n \n     def test_store_file_uri(self):\n         path = os.path.abspath(self.mktemp())\n@@ -47,8 +36,17 @@ class FileFeedStorageTest(FeedStorageTest):\n         st = FileFeedStorage(path)\n         verifyObject(IFeedStorage, st)\n \n+    @defer.inlineCallbacks\n+    def _assert_stores(self, storage, path):\n+        spider = BaseSpider(\"default\")\n+        file = storage.open(spider)\n+        file.write(\"content\")\n+        yield storage.store(file)\n+        self.failUnless(os.path.exists(path))\n+        self.failUnlessEqual(open(path).read(), \"content\")\n \n-class FTPFeedStorageTest(FeedStorageTest):\n+\n+class FTPFeedStorageTest(unittest.TestCase):\n \n     def test_store(self):\n         uri = os.environ.get('FEEDTEST_FTP_URI')\n@@ -59,6 +57,18 @@ class FTPFeedStorageTest(FeedStorageTest):\n         verifyObject(IFeedStorage, st)\n         return self._assert_stores(st, path)\n \n+    @defer.inlineCallbacks\n+    def _assert_stores(self, storage, path):\n+        spider = BaseSpider(\"default\")\n+        file = storage.open(spider)\n+        file.write(\"content\")\n+        yield storage.store(file)\n+        self.failUnless(os.path.exists(path))\n+        self.failUnlessEqual(open(path).read(), \"content\")\n+        # again, to check s3 objects are overwritten\n+        yield storage.store(StringIO(\"new content\"))\n+        self.failUnlessEqual(open(path).read(), \"new content\")\n+\n \n class S3FeedStorageTest(unittest.TestCase):\n \n@@ -71,16 +81,20 @@ class S3FeedStorageTest(unittest.TestCase):\n         from boto import connect_s3\n         storage = S3FeedStorage(uri)\n         verifyObject(IFeedStorage, storage)\n-        yield storage.store(StringIO(\"content\"), BaseSpider(\"default\"))\n+        file = storage.open(BaseSpider(\"default\"))\n+        file.write(\"content\")\n+        yield storage.store(file)\n         u = urlparse.urlparse(uri)\n         key = connect_s3().get_bucket(u.hostname, validate=False).get_key(u.path)\n         self.failUnlessEqual(key.get_contents_as_string(), \"content\")\n \n-class StdoutFeedStorageTest(FeedStorageTest):\n+class StdoutFeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_store(self):\n         out = StringIO()\n         storage = StdoutFeedStorage('stdout:', _stdout=out)\n-        yield storage.store(StringIO(\"content\"), BaseSpider(\"default\"))\n+        file = storage.open(BaseSpider(\"default\"))\n+        file.write(\"content\")\n+        yield storage.store(file)\n         self.assertEqual(out.getvalue(), \"content\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#028bf3386de8dc524b40f3d7b951581a932e896e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 301 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -165,13 +165,11 @@ class FeedExporter(object):\n         if not slot.itemcount and not self.store_empty:\n             return\n         slot.exporter.finish_exporting()\n-        nbytes = slot.file.tell()\n-        logfmt = \"%%s %s feed (%d items, %d bytes) in: %s\" % (self.format, \\\n-            slot.itemcount, nbytes, slot.uri)\n+        logfmt = \"%%s %s feed (%d items) in: %s\" % (self.format, \\\n+            slot.itemcount, slot.uri)\n         d = defer.maybeDeferred(slot.storage.store, slot.file)\n         d.addCallback(lambda _: log.msg(logfmt % \"Stored\", spider=spider))\n         d.addErrback(log.err, logfmt % \"Error storing\", spider=spider)\n-        d.addBoth(lambda _: slot.file.close())\n         return d\n \n     def item_scraped(self, item, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64b8e2648eb3bfa8e9111708f702ad689ff41598", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 137 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,9 +17,9 @@ class Command(ScrapyCommand):\n         parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n             help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", \\\n-            help=\"store scraped items into FILE (using feed exports)\")\n+            help=\"dump scraped items into FILE (use - for stdout)\")\n         parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\", default=\"jsonlines\", \\\n-            help=\"format to use in feed exports (default: %default)\")\n+            help=\"format to use for dumping items with -o (default: %default)\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -28,6 +28,9 @@ class Command(ScrapyCommand):\n         except ValueError:\n             raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n         if opts.output:\n+            if opts.output == '-':\n+                self.settings.overrides['FEED_URI'] = 'stdout:'\n+            else:\n                 self.settings.overrides['FEED_URI'] = opts.output\n             self.settings.overrides['FEED_FORMAT'] = opts.output_format\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c38c49d56a7e904315c68ca16a27a66a440f205e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 1110 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ Item Exporters are used to export/serialize items into different formats.\n import csv\n import pprint\n import marshal\n-from cPickle import Pickler\n+import cPickle as pickle\n from xml.sax.saxutils import XMLGenerator\n \n from scrapy.utils.py26 import json\n@@ -178,12 +178,14 @@ class CsvItemExporter(BaseItemExporter):\n \n class PickleItemExporter(BaseItemExporter):\n \n-    def __init__(self, file, protocol=0, **kwargs):\n+    def __init__(self, file, protocol=2, **kwargs):\n         self._configure(kwargs)\n-        self.pickler = Pickler(file, protocol)\n+        self.file =file\n+        self.protocol = protocol\n \n     def export_item(self, item):\n-        self.pickler.dump(dict(self._get_serialized_fields(item)))\n+        d = dict(self._get_serialized_fields(item))\n+        pickle.dump(d, self.file, self.protocol)\n \n \n class MarshalItemExporter(BaseItemExporter):\n\n@@ -164,6 +164,7 @@ FEED_EXPORTERS_BASE = {\n     'csv': 'scrapy.contrib.exporter.CsvItemExporter',\n     'xml': 'scrapy.contrib.exporter.XmlItemExporter',\n     'marshal': 'scrapy.contrib.exporter.MarshalItemExporter',\n+    'pickle': 'scrapy.contrib.exporter.PickleItemExporter',\n }\n \n HTTPCACHE_ENABLED = False\n\n@@ -88,6 +88,19 @@ class PickleItemExporterTest(BaseItemExporterTest):\n     def _check_output(self):\n         self._assert_expected_item(pickle.loads(self.output.getvalue()))\n \n+    def test_export_multiple_items(self):\n+        i1 = TestItem(name='hello', age='world')\n+        i2 = TestItem(name='bye', age='world')\n+        f = StringIO()\n+        ie = PickleItemExporter(f)\n+        ie.start_exporting()\n+        ie.export_item(i1)\n+        ie.export_item(i2)\n+        ie.finish_exporting()\n+        f.reset()\n+        self.assertEqual(pickle.load(f), i1)\n+        self.assertEqual(pickle.load(f), i2)\n+\n \n class CsvItemExporterTest(BaseItemExporterTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c085f816412a4ddbfb527ee92d8624d4afeb07f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 28 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,6 @@ Download timeout middleware\n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n from scrapy.utils.python import WeakKeyCache\n-from scrapy.utils import deprecate\n \n \n class DownloadTimeoutMiddleware(object):\n@@ -14,7 +13,6 @@ class DownloadTimeoutMiddleware(object):\n \n     def _download_timeout(self, spider):\n         if hasattr(spider, 'download_timeout'):\n-            deprecate.attribute(spider, 'download_timeout', 'DOWNLOAD_TIMEOUT')\n             return spider.download_timeout\n         return spider.settings.getint('DOWNLOAD_TIMEOUT')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4821a123d17e90686ea0b1eb9447dedcb604431", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 21 | Churn Cumulative: 737 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,7 +22,6 @@ from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookup\n                                    ConnectionRefusedError, ConnectionDone, ConnectError, \\\n                                    ConnectionLost, TCPTimedOutError\n from twisted.internet.defer import TimeoutError as UserTimeoutError\n-from twisted.web.client import PartialDownloadError\n \n from scrapy import log\n from scrapy.exceptions import NotConfigured\n@@ -35,7 +34,7 @@ class RetryMiddleware(object):\n     # decompress an empty response\n     EXCEPTIONS_TO_RETRY = (ServerTimeoutError, UserTimeoutError, DNSLookupError,\n                            ConnectionRefusedError, ConnectionDone, ConnectError,\n-                           ConnectionLost, PartialDownloadError, TCPTimedOutError,\n+                           ConnectionLost, TCPTimedOutError,\n                            IOError)\n \n     def __init__(self):\n\n@@ -2,7 +2,7 @@ from time import time\n from urlparse import urlparse, urlunparse, urldefrag\n \n from twisted.python import failure\n-from twisted.web.client import PartialDownloadError, HTTPClientFactory\n+from twisted.web.client import HTTPClientFactory\n from twisted.web.http import HTTPClient\n from twisted.internet import defer\n \n@@ -65,9 +65,6 @@ class ScrapyHTTPPageGetter(HTTPClient):\n     def handleResponse(self, response):\n         if self.factory.method.upper() == 'HEAD':\n             self.factory.page('')\n-        elif self.length != None and self.length != 0:\n-            self.factory.noPage(failure.Failure(\n-                PartialDownloadError(self.factory.status, None, response)))\n         else:\n             self.factory.page(response)\n         self.transport.loseConnection()\n\n@@ -10,7 +10,6 @@ from twisted.web.test.test_webclient import ForeverTakingResource, \\\n         PayloadResource, BrokenDownloadResource\n from w3lib.url import path_to_file_uri\n \n-from scrapy.core.downloader.webclient import PartialDownloadError\n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.http import HttpDownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n@@ -131,11 +130,6 @@ class HttpTestCase(unittest.TestCase):\n         d.addCallback(self.assertEquals, body)\n         return d\n \n-    def test_broken_download(self):\n-        request = Request(self.getURL('broken'))\n-        d = self.download_request(request, BaseSpider('foo'))\n-        return self.assertFailure(d, PartialDownloadError)\n-\n \n class UriResource(resource.Resource):\n     \"\"\"Return the full uri that was requested\"\"\"\n\n@@ -222,13 +222,6 @@ class WebClientTestCase(unittest.TestCase):\n         s = \"0123456789\" * 10\n         return getPage(self.getURL(\"payload\"), body=s).addCallback(self.assertEquals, s)\n \n-    def testBrokenDownload(self):\n-        # test what happens when download gets disconnected in the middle\n-        d = getPage(self.getURL(\"broken\"))\n-        d = self.assertFailure(d, client.PartialDownloadError)\n-        d.addCallback(lambda exc: self.assertEquals(exc.response, \"abc\"))\n-        return d\n-\n     def testHostHeader(self):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#37ad4f879141679c0a90d0c9abc4e36ec9e102db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 21 | Churn Cumulative: 1254 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,7 @@ from w3lib.url import safe_url_string\n from scrapy.http.headers import Headers\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.decorator import deprecated\n+from scrapy.utils.url import escape_ajax\n from scrapy.http.common import deprecated_setter\n \n class Request(object_ref):\n@@ -48,7 +49,7 @@ class Request(object_ref):\n \n     def _set_url(self, url):\n         if isinstance(url, str):\n-            self._url = safe_url_string(url)\n+            self._url = escape_ajax(safe_url_string(url))\n         elif isinstance(url, unicode):\n             if self.encoding is None:\n                 raise TypeError('Cannot convert unicode url - %s has no encoding' %\n\n@@ -107,6 +107,10 @@ class RequestTest(unittest.TestCase):\n         assert isinstance(r4.body, str)\n         self.assertEqual(r4.body, \"Price: \\xa3100\")\n \n+    def test_ajax_url(self):\n+        r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n+        self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key=value\")\n+\n     def test_copy(self):\n         \"\"\"Test Request copy\"\"\"\n         \n\n@@ -2,6 +2,8 @@ import unittest\n from scrapy.spider import BaseSpider\n from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url\n \n+__doctests__ = ['scrapy.utils.url']\n+\n class UrlUtilsTest(unittest.TestCase):\n \n     def test_url_is_from_any_domain(self):\n\n@@ -62,3 +62,15 @@ def parse_url(url, encoding=None):\n     \"\"\"\n     return url if isinstance(url, urlparse.ParseResult) else \\\n         urlparse.urlparse(unicode_to_str(url, encoding))\n+\n+def escape_ajax(url):\n+    \"\"\"\n+    Return the crawleable url according to:\n+    http://code.google.com/web/ajaxcrawling/docs/getting-started.html\n+\n+    TODO: add support for urls with query arguments\n+\n+    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n+    'www.example.com/ajax.html?_escaped_fragment_=key=value'\n+    \"\"\"\n+    return url.replace('#!', '?_escaped_fragment_=')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6cc40dc0623e6d19b56d4ef942d34fbcc80dfdd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 173 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class MemoryUsage(object):\n \n         self.crawler = crawler\n         self.warned = False\n-        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY')\n+        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY_MAIL')\n         self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n         self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n         self.report = crawler.settings.getbool('MEMUSAGE_REPORT')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7232c31f78358741f3b7c532bdc268d43ff51622", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 75 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,7 @@ class Environment(object):\n             message['_spider'])\n         if not os.path.exists(logsdir):\n             os.makedirs(logsdir)\n-        to_delete = sorted(os.listdir(logsdir), reverse=True)[:-self.logs_to_keep]\n+        to_delete = sorted((os.path.join(logsdir, x) for x in os.listdir(logsdir)), key=os.path.getmtime)[:-self.logs_to_keep]\n         for x in to_delete:\n-            os.remove(os.path.join(logsdir, x))\n+            os.remove(x)\n         return os.path.join(logsdir, \"%s.log\" % message['_job'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec1ef0235f9deee0c263c9b31652d3e74a754acc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 555 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,8 @@ import unittest\n import urlparse\n \n from scrapy.http import Response, TextResponse, HtmlResponse\n-from scrapy.utils.response import body_or_str, response_httprepr, open_in_browser\n+from scrapy.utils.response import body_or_str, response_httprepr, open_in_browser, \\\n+    get_meta_refresh\n \n __doctests__ = ['scrapy.utils.response']\n \n@@ -55,5 +56,21 @@ class ResponseUtilsTest(unittest.TestCase):\n         self.assertRaises(TypeError, open_in_browser, Response(url, body=body), \\\n             debug=True)\n \n+    def test_get_meta_refresh(self):\n+        r1 = HtmlResponse(\"http://www.example.com\", body=\"\"\"\n+        <html>\n+        <head><title>Dummy</title><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n+        <body>blahablsdfsal&amp;</body>\n+        </html>\"\"\")\n+        r2 = HtmlResponse(\"http://www.example.com\", body=\"\"\"\n+        <html>\n+        <head><title>Dummy</title><noScript>\n+        <meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n+        </noSCRIPT>\n+        <body>blahablsdfsal&amp;</body>\n+        </html>\"\"\")\n+        self.assertEqual(get_meta_refresh(r1), (5.0, 'http://example.org/newpage'))\n+        self.assertEqual(get_meta_refresh(r2), (None, None))\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -4,6 +4,7 @@ scrapy.http.Response objects\n \"\"\"\n \n import os\n+import re\n import weakref\n import webbrowser\n import tempfile\n@@ -12,7 +13,6 @@ from twisted.web import http\n from twisted.web.http import RESPONSES\n from w3lib import html\n \n-from scrapy.xlib.BeautifulSoup import BeautifulSoup\n from scrapy.http import Response, HtmlResponse\n \n def body_or_str(obj, unicode=True):\n@@ -34,11 +34,13 @@ def get_base_url(response):\n             response.encoding)\n     return _baseurl_cache[response]\n \n+_noscript_re = re.compile(u'<noscript>.*?</noscript>', re.IGNORECASE | re.DOTALL)\n _metaref_cache = weakref.WeakKeyDictionary()\n def get_meta_refresh(response):\n     \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.body_as_unicode()[0:4096]\n+        text = _noscript_re.sub(u'', text)\n         _metaref_cache[response] = html.get_meta_refresh(text, response.url, \\\n             response.encoding)\n     return _metaref_cache[response]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36df87b4de49702ccd159270b974ed64d4a23260", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 566 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,8 +69,17 @@ class ResponseUtilsTest(unittest.TestCase):\n         </noSCRIPT>\n         <body>blahablsdfsal&amp;</body>\n         </html>\"\"\")\n+        r3 = HtmlResponse(\"http://www.example.com\", body=\"\"\"\n+    <noscript><meta http-equiv=\"REFRESH\" content=\"0;url=http://www.example.com/newpage</noscript>\n+    <script type=\"text/javascript\">\n+    if(!checkCookies()){\n+        document.write('<meta http-equiv=\"REFRESH\" content=\"0;url=http://www.example.com/newpage\">');\n+    }\n+    </script>\n+        \"\"\")\n         self.assertEqual(get_meta_refresh(r1), (5.0, 'http://example.org/newpage'))\n         self.assertEqual(get_meta_refresh(r2), (None, None))\n+        self.assertEqual(get_meta_refresh(r3), (None, None))\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -35,12 +35,14 @@ def get_base_url(response):\n     return _baseurl_cache[response]\n \n _noscript_re = re.compile(u'<noscript>.*?</noscript>', re.IGNORECASE | re.DOTALL)\n+_script_re = re.compile(u'<script.*?>.*?</script>', re.IGNORECASE | re.DOTALL)\n _metaref_cache = weakref.WeakKeyDictionary()\n def get_meta_refresh(response):\n     \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.body_as_unicode()[0:4096]\n         text = _noscript_re.sub(u'', text)\n+        text = _script_re.sub(u'', text)\n         _metaref_cache[response] = html.get_meta_refresh(text, response.url, \\\n             response.encoding)\n     return _metaref_cache[response]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d37a788d22d9f3061a08b00119c8c0ee6462e8cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 93 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,9 +38,10 @@ class SpiderManager(object):\n \n     def create(self, spider_name, **spider_kwargs):\n         try:\n-            return self._spiders[spider_name](**spider_kwargs)\n+            spcls = self._spiders[spider_name]\n         except KeyError:\n             raise KeyError(\"Spider not found: %s\" % spider_name)\n+        return spcls(**spider_kwargs)\n \n     def find_by_request(self, request):\n         return [name for name, cls in self._spiders.iteritems()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d13de4366636be7af1bc74aa8925d5e81ed8022", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 548 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,6 +29,7 @@ class Shell(object):\n         self.crawler = crawler\n         self.update_vars = update_vars or (lambda x: None)\n         self.item_class = load_object(crawler.settings['DEFAULT_ITEM_CLASS'])\n+        self.spider = None\n         self.inthread = inthread\n         self.code = code\n         self.vars = {}\n@@ -57,15 +58,22 @@ class Shell(object):\n             start_python_console(self.vars)\n \n     def _schedule(self, request, spider):\n+        spider = self._open_spider(request, spider)\n+        d = request_deferred(request)\n+        d.addCallback(lambda x: (x, spider))\n+        self.crawler.engine.crawl(request, spider)\n+        return d\n+\n+    def _open_spider(self, request, spider):\n+        if self.spider:\n+            return self.spider\n         if spider is None:\n             spider = create_spider_for_request(self.crawler.spiders, request, \\\n                 BaseSpider('default'), log_multiple=True)\n         spider.set_crawler(self.crawler)\n         self.crawler.engine.open_spider(spider, close_if_idle=False)\n-        d = request_deferred(request)\n-        d.addCallback(lambda x: (x, spider))\n-        self.crawler.engine.crawl(request, spider)\n-        return d\n+        self.spider = spider\n+        return spider\n \n     def fetch(self, request_or_url, spider=None):\n         if isinstance(request_or_url, Request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f649f0e30137501a897f7977e3b3a4b7a9191b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 138 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 13, 0)\n-__version__ = \"0.13.0\"\n+version_info = (0, 14, 0)\n+__version__ = \"0.14.0\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e29f9e5b249eb40e0213a8cb099deaf350ed7202", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 142 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 14, 0)\n-__version__ = \"0.14.0\"\n+version_info = (0, 15, 0)\n+__version__ = \"0.15.0\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b0184eb594ea6b6df1caa36428d7b23111f3859", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 242 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,6 +76,9 @@ class AutoThrottle(object):\n \n     def spider_opened(self, spider):\n         spider.download_delay = self.START_DELAY\n+        if hasattr(spider, \"max_concurrent_requests\"):\n+            self.MAX_CONCURRENCY = spider.max_concurrent_requests\n+        # override in order to avoid to initialize slot with concurrency > 1\n         spider.max_concurrent_requests = 1\n         self.last_latencies = [self.START_DELAY]\n         self.last_lat = self.START_DELAY, 0.0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#175a4b5957c82c814ef386f0e3276225ad86f3f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 245 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,6 +76,9 @@ class AutoThrottle(object):\n \n     def spider_opened(self, spider):\n         spider.download_delay = self.START_DELAY\n+        if hasattr(spider, \"max_concurrent_requests\"):\n+            self.MAX_CONCURRENCY = spider.max_concurrent_requests\n+        # override in order to avoid to initialize slot with concurrency > 1\n         spider.max_concurrent_requests = 1\n         self.last_latencies = [self.START_DELAY]\n         self.last_lat = self.START_DELAY, 0.0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c0494578547117f4be01afb3f54717f52319b36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 164 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -98,6 +98,14 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         self.start_time = time()\n         self.deferred = defer.Deferred().addCallback(self._build_response, request)\n \n+        # Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected\n+        # to have _disconnectedDeferred. See Twisted r32329.\n+        # As Scrapy implements it's own logic to handle redirects is not\n+        # needed to add the callback _waitForDisconnect.\n+        # Specifically this avoids the AttributeError exception when\n+        # clientConnectionFailed method is called.\n+        self._disconnectedDeferred = defer.Deferred()\n+\n         self._set_connection_attributes(request)\n \n         # set Host header based on url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98f3f87530034941a9f8313b2c2c2578f869ffa3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 172 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -98,6 +98,14 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         self.start_time = time()\n         self.deferred = defer.Deferred().addCallback(self._build_response, request)\n \n+        # Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected\n+        # to have _disconnectedDeferred. See Twisted r32329.\n+        # As Scrapy implements it's own logic to handle redirects is not\n+        # needed to add the callback _waitForDisconnect.\n+        # Specifically this avoids the AttributeError exception when\n+        # clientConnectionFailed method is called.\n+        self._disconnectedDeferred = defer.Deferred()\n+\n         self._set_connection_attributes(request)\n \n         # set Host header based on url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#63d583d9be12ae52dfcb60d960f4657c7241a1f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 631 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -102,7 +102,7 @@ class S3ImagesStore(object):\n \n     def _get_boto_bucket(self):\n         from boto.s3.connection import S3Connection\n-        c = S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY)\n+        c = S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)\n         return c.get_bucket(self.bucket, validate=False)\n \n     def _get_boto_key(self, key):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64ba6e7982fcf7f4ee9f7582de47c72db8fe0cc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 28 | Churn Cumulative: 63 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,8 +5,10 @@ See documentation in docs/topics/extensions.rst\n \"\"\"\n \n import os\n+import sys\n import signal\n import traceback\n+import threading\n from pdb import Pdb\n \n from scrapy.utils.engine import format_engine_status\n@@ -14,7 +16,9 @@ from scrapy import log\n \n \n class StackTraceDump(object):\n-    def __init__(self):\n+\n+    def __init__(self, crawler=None):\n+        self.crawler = crawler\n         try:\n             signal.signal(signal.SIGUSR2, self.dump_stacktrace)\n             signal.signal(signal.SIGQUIT, self.dump_stacktrace)\n@@ -22,13 +26,27 @@ class StackTraceDump(object):\n             # win32 platforms don't support SIGUSR signals\n             pass\n \n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n     def dump_stacktrace(self, signum, frame):\n-        msg = \"Dumping stack trace and engine status\" + os.linesep\n-        msg += \"\".join(traceback.format_stack(frame))\n-        msg += os.linesep\n-        msg += format_engine_status()\n+        stackdumps = self._thread_stacks()\n+        enginestatus = format_engine_status(self.crawler.engine)\n+        msg = \"Dumping stack trace and engine status\" \\\n+            \"\\n{0}\\n{1}\".format(enginestatus, stackdumps)\n         log.msg(msg)\n \n+    def _thread_stacks(self):\n+        id2name = dict((th.ident, th.name) for th in threading.enumerate())\n+        dumps = ''\n+        for id_, frame in sys._current_frames().items():\n+            name = id2name.get(id_, '')\n+            dump = ''.join(traceback.format_stack(frame))\n+            dumps += \"# Thread: {0}({1})\\n{2}\\n\".format(name, id_, dump)\n+        return dumps\n+\n \n class Debugger(object):\n     def __init__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f214c94912c358de2641d4297b5d1d13a16380f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 129 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,7 +7,7 @@ See documentation in docs/topics/spiders.rst\n \n import copy\n \n-from scrapy.http import Request\n+from scrapy.http import Request, HtmlResponse\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.spider import BaseSpider\n from scrapy.conf import settings\n@@ -46,6 +46,8 @@ class CrawlSpider(BaseSpider):\n         return results\n \n     def _requests_to_follow(self, response):\n+        if not isinstance(response, HtmlResponse):\n+            return\n         seen = set()\n         for n, rule in enumerate(self._rules):\n             links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
