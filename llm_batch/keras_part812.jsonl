{"custom_id": "keras#a3fe639d4554e31fb916f4a77b9ea2ee3a9ce29c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 277 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -50,7 +50,6 @@ class EarlyStopping(Callback):\n             improvement is expected and thus training will not be stopped.\n             Defaults to `0`.\n \n-\n     Example:\n \n     >>> callback = keras.callbacks.EarlyStopping(monitor='loss',\n\n@@ -2077,12 +2077,13 @@ def psnr(\n     x2,\n     max_val,\n ):\n-    \"\"\"Peak Signal-to-Noise Ratio (PSNR) calculation.\n+    \"\"\"Peak Signal-to-Noise Ratio (PSNR) function.\n \n-    This function calculates the Peak Signal-to-Noise Ratio between two signals,\n+    This function computes the Peak Signal-to-Noise Ratio between two signals,\n     `x1` and `x2`. PSNR is a measure of the quality of a reconstructed signal.\n     The higher the PSNR, the closer the reconstructed signal is to the original\n-    signal.\n+    signal. Note that it can become negative when the signal power is\n+    smaller that the noise power.\n \n     Args:\n         x1: The first input signal.\n@@ -2093,14 +2094,12 @@ def psnr(\n         float: The PSNR value between `x1` and `x2`.\n \n     Examples:\n-        >>> import numpy as np\n-        >>> from keras import ops\n-        >>> x = np.random.random((2, 4, 4, 3))\n-        >>> y = np.random.random((2, 4, 4, 3))\n+\n+    >>> x1 = keras.random.normal((2, 4, 4, 3))\n+    >>> x2 = keras.random.normal((2, 4, 4, 3))\n     >>> max_val = 1.0\n-        >>> psnr_value = ops.nn.psnr(x, y, max_val)\n-        >>> psnr_value\n-        20.0\n+    >>> keras.ops.nn.psnr(x1, x2, max_val)\n+    -3.1697404\n     \"\"\"\n     if any_symbolic_tensors(\n         (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d0ca6bc65db4fb84c5838e7b7e7e8bb4044b1d7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 22 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import contextlib\n-import os\n \n import ml_dtypes\n import numpy as np\n@@ -19,10 +18,7 @@ SUPPORTS_SPARSE_TENSORS = False\n # Some operators such as 'aten::_foreach_mul_.Scalar'\n # are not currently implemented for the MPS device.\n # check https://github.com/pytorch/pytorch/issues/77764.\n-if (\n-    torch.backends.mps.is_available()\n-    and os.getenv(\"PYTORCH_ENABLE_MPS_FALLBACK\") == \"1\"\n-):\n+if torch.backends.mps.is_available():\n     DEFAULT_DEVICE = \"mps\"\n elif torch.cuda.is_available():\n     DEFAULT_DEVICE = \"cuda\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#90e0eccbfa832fbff5fd67012608d210f6cd861e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 247 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 27 | Methods Changed: 23 | Complexity Δ (Sum/Max): 23/15 | Churn Δ: 279 | Churn Cumulative: 319 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.7096774193548387\n\nDIFF:\n@@ -11,7 +11,14 @@ from keras.src.saving import serialization_lib\n \n \n @keras_export(\"keras.models.clone_model\")\n-def clone_model(model, input_tensors=None, clone_function=None):\n+def clone_model(\n+    model,\n+    input_tensors=None,\n+    clone_function=None,\n+    call_function=None,\n+    recursive=False,\n+    **kwargs,\n+):\n     \"\"\"Clone a Functional or Sequential `Model` instance.\n \n     Model cloning is similar to calling a model on new inputs,\n@@ -29,24 +36,44 @@ def clone_model(model, input_tensors=None, clone_function=None):\n         input_tensors: optional list of input tensors or InputLayer objects\n             to build the model upon. If not provided,\n             new `Input` objects will be created.\n-        clone_function: Callable to be used to clone each layer in the target\n+        clone_function: Callable with signature `fn(layer)`\n+            to be used to clone each layer in the target\n             model (except `Input` instances). It takes as argument the\n             layer instance to be cloned, and returns the corresponding layer\n             instance to be used in the model copy. If unspecified, this callable\n-            becomes the following serialization/deserialization function:\n+            defaults to the following serialization/deserialization function:\n             `lambda layer: layer.__class__.from_config(layer.get_config())`.\n             By passing a custom callable, you can customize your copy of the\n             model, e.g. by wrapping certain layers of interest (you might want\n             to replace all `LSTM` instances with equivalent\n             `Bidirectional(LSTM(...))` instances, for example).\n             Defaults to `None`.\n+        call_function: Callable with signature\n+            `fn(layer, *args, **kwargs)` to be used to call each\n+            cloned layer and a set of inputs. It takes the layer instance,\n+            the call arguments and keyword arguments, and returns the\n+            call outputs. If unspecified, this callable defaults to\n+            the regular `__call__()` method:\n+            `def fn(layer, *args, **kwargs): return layer(*args, **kwargs)`.\n+            By passing a custom callable, you can insert new layers before or\n+            after a given layer. Note: this argument can only be used with\n+            Functional models.\n+        recursive: Boolean. Whether to recursively clone any Sequential\n+            or Functional models encountered in the original\n+            Sequential/Functional model. If `False`,\n+            then inner models are cloned by calling `clone_function()`.\n+            If `True`, then inner models are cloned by calling `clone_model()`\n+            with the same `clone_function`, `call_function`, and `recursive`\n+            arguments. Note that in this case, `call_function`\n+            will not be propagated to any Sequential model\n+            (since it is not applicable to Sequential models).\n \n     Returns:\n         An instance of `Model` reproducing the behavior\n         of the original model, on top of new inputs tensors,\n         using newly instantiated weights. The cloned model may behave\n         differently from the original model if a custom `clone_function`\n-        modifies the layer.\n+        or `call_function` modifies a layer or layer call.\n \n     Example:\n \n@@ -74,6 +101,23 @@ def clone_model(model, input_tensors=None, clone_function=None):\n     new_model = clone_model(model)\n     ```\n \n+    Using a `call_function` to add a `Dropout` layer after each `Dense` layer\n+    (without recreating new layers):\n+\n+    ```python\n+    def call_function(layer, *args, **kwargs):\n+        out = layer(*args, **kwargs)\n+        if isinstance(layer, keras.layers.Dense):\n+            out = keras.layers.Dropout(0.5)(out)\n+        return out\n+\n+    new_model = clone_model(\n+        model,\n+        clone_function=lambda x: x,  # Reuse the same layers.\n+        call_function=call_function,\n+    )\n+    ```\n+\n     Note that subclassed models cannot be cloned by default,\n     since their internal layer structure is not known.\n     To achieve equivalent functionality\n@@ -88,11 +132,44 @@ def clone_model(model, input_tensors=None, clone_function=None):\n     In the case of a subclassed model, you cannot using a custom\n     `clone_function`.\n     \"\"\"\n+    cache = kwargs.pop(\"cache\", None)\n+    if kwargs:\n+        raise ValueError(\n+            f\"Unexpected keyword argument(s): {tuple(kwargs.keys())}\"\n+        )\n+\n     if isinstance(model, Sequential):\n+        # Wrap clone_function to handle recursiveness and layer sharing.\n+        clone_function = _wrap_clone_function(\n+            clone_function,\n+            call_function=call_function,\n+            recursive=recursive,\n+            cache=cache,\n+        )\n+        if call_function is not None:\n+            raise ValueError(\n+                \"`call_function` argument is not supported with Sequential \"\n+                \"models.  In a Sequential model, layers aren't called \"\n+                \"at model-construction time (they're merely listed). \"\n+                \"Use `call_function` with Functional models only. \"\n+                \"Received model of \"\n+                f\"type '{model.__class__.__name__}', with \"\n+                f\"call_function={clone_function}\"\n+            )\n         return _clone_sequential_model(\n-            model, input_tensors=input_tensors, clone_function=clone_function\n+            model,\n+            clone_function=clone_function,\n+            input_tensors=input_tensors,\n         )\n     if isinstance(model, Functional):\n+        # Wrap clone_function to handle recursiveness and layer sharing.\n+        clone_function = _wrap_clone_function(\n+            clone_function,\n+            call_function=call_function,\n+            recursive=recursive,\n+            cache=cache,\n+        )\n+\n         # If the get_config() method is the same as a regular Functional\n         # model, we're safe to use _clone_functional_model (which relies\n         # on a Functional constructor). In the case where the get_config\n@@ -104,27 +181,78 @@ def clone_model(model, input_tensors=None, clone_function=None):\n         ):\n             return _clone_functional_model(\n                 model,\n-                input_tensors=input_tensors,\n                 clone_function=clone_function,\n+                call_function=call_function,\n+                input_tensors=input_tensors,\n             )\n \n     # Case of a custom model class\n     if clone_function or input_tensors:\n         raise ValueError(\n-            \"Arguments clone_function and input_tensors \"\n+            \"Arguments `clone_function` and `input_tensors` \"\n             \"are only supported for Sequential models \"\n             \"or Functional models. Received model of \"\n             f\"type '{model.__class__.__name__}', with \"\n             f\"clone_function={clone_function} and \"\n             f\"input_tensors={input_tensors}\"\n         )\n+    if call_function is not None:\n+        raise ValueError(\n+            \"Argument `call_function` is only supported \"\n+            \"for Functional models. Received model of \"\n+            f\"type '{model.__class__.__name__}', with \"\n+            f\"call_function={clone_function}\"\n+        )\n     config = serialization_lib.serialize_keras_object(model)\n     return serialization_lib.deserialize_keras_object(\n         config, custom_objects={model.__class__.__name__: model.__class__}\n     )\n \n \n-def _clone_sequential_model(model, input_tensors=None, clone_function=None):\n+def _wrap_clone_function(\n+    clone_function, call_function=None, recursive=False, cache=None\n+):\n+    \"\"\"Wrapper to handle recursiveness and layer sharing.\"\"\"\n+    if clone_function is None:\n+\n+        def _clone_layer(layer):\n+            return layer.__class__.from_config(layer.get_config())\n+\n+        clone_function = _clone_layer\n+\n+    if cache is None:\n+        cache = {}\n+\n+    def wrapped_clone_function(layer):\n+        if id(layer) in cache:\n+            return cache[id(layer)]\n+        if recursive:\n+            if isinstance(layer, Sequential):\n+                # Note: Sequential doens't support call_function.\n+                clone = clone_model(\n+                    layer,\n+                    clone_function=clone_function,\n+                    cache=cache,\n+                )\n+                cache[id(layer)] = clone\n+                return clone\n+            elif isinstance(layer, Functional):\n+                clone = clone_model(\n+                    layer,\n+                    clone_function=clone_function,\n+                    call_function=call_function,\n+                    cache=cache,\n+                )\n+                cache[id(layer)] = clone\n+                return clone\n+        clone = clone_function(layer)\n+        cache[id(layer)] = clone\n+        return clone\n+\n+    return wrapped_clone_function\n+\n+\n+def _clone_sequential_model(model, clone_function, input_tensors=None):\n     \"\"\"Clone a `Sequential` model instance.\n \n     Model cloning is similar to calling a model on new inputs,\n@@ -144,12 +272,6 @@ def _clone_sequential_model(model, input_tensors=None, clone_function=None):\n         of the original model, on top of new inputs tensors,\n         using newly instantiated weights.\n     \"\"\"\n-    if clone_function is None:\n-\n-        def _clone_layer(layer):\n-            return layer.__class__.from_config(layer.get_config())\n-\n-        clone_function = _clone_layer\n \n     if not isinstance(model, Sequential):\n         raise ValueError(\n@@ -202,7 +324,9 @@ def _clone_sequential_model(model, input_tensors=None, clone_function=None):\n     return Sequential(new_layers, name=model.name, trainable=model.trainable)\n \n \n-def _clone_functional_model(model, input_tensors=None, clone_function=None):\n+def _clone_functional_model(\n+    model, clone_function, input_tensors=None, call_function=None\n+):\n     \"\"\"Clone a `Functional` model instance.\n \n     Model cloning is similar to calling a model on new inputs,\n@@ -224,17 +348,6 @@ def _clone_functional_model(model, input_tensors=None, clone_function=None):\n         of the original model, on top of new inputs tensors,\n         using newly instantiated weights.\n     \"\"\"\n-    if clone_function is None:\n-        seen = {}\n-\n-        def _clone_layer(layer):\n-            if layer in seen:\n-                return seen[layer]\n-            new_layer = layer.__class__.from_config(layer.get_config())\n-            seen[layer] = new_layer\n-            return new_layer\n-\n-        clone_function = _clone_layer\n \n     if not callable(clone_function):\n         raise ValueError(\n@@ -276,7 +389,9 @@ def _clone_functional_model(model, input_tensors=None, clone_function=None):\n         return new_layer\n \n     output_tensors = model._run_through_graph(\n-        input_tensors, operation_fn=operation_fn\n+        input_tensors,\n+        operation_fn=operation_fn,\n+        call_fn=call_function,\n     )\n \n     if functional_like_constructor(model.__class__):\n\n@@ -4,6 +4,7 @@ from absl.testing import parameterized\n \n from keras.src import layers\n from keras.src import models\n+from keras.src import ops\n from keras.src import testing\n from keras.src import tree\n from keras.src.models.cloning import clone_model\n@@ -21,6 +22,24 @@ def get_mlp_functional_model(shared_layers=False):\n     return model\n \n \n+def get_nested_functional_model():\n+    inputs = layers.Input(shape=(4,))\n+    x = layers.Dense(3)(inputs)\n+    mlp = get_mlp_functional_model()\n+    x = mlp(x)\n+    outputs = layers.Dense(2)(x)\n+    model = models.Model(inputs, outputs)\n+    return model\n+\n+\n+def get_nested_sequential_model():\n+    model = models.Sequential()\n+    model.add(layers.Dense(2))\n+    model.add(get_sequential_model(explicit_input=False))\n+    model.add(layers.Dense(2))\n+    return model\n+\n+\n def get_cnn_functional_model(shared_layers=False):\n     inputs = layers.Input(shape=(7, 3))\n     x = layers.Conv1D(2, 2, padding=\"same\")(inputs)\n@@ -57,6 +76,19 @@ def get_subclassed_model():\n \n @pytest.mark.requires_trainable_backend\n class CloneModelTest(testing.TestCase, parameterized.TestCase):\n+\n+    def assert_models_equal(self, model1, model2, ref_input):\n+        result1 = model1(ref_input)\n+        result2 = model2(ref_input)\n+        for r1, r2 in zip(tree.flatten(result1), tree.flatten(result2)):\n+            self.assertAllClose(\n+                ops.convert_to_numpy(r1), ops.convert_to_numpy(r2)\n+            )\n+\n+    def assert_weights_equal(self, model1, model2):\n+        for a, b in zip(model1.weights, model2.weights):\n+            self.assertAllClose(a.numpy(), b.numpy())\n+\n     @parameterized.named_parameters(\n         (\"mlp_functional\", get_mlp_functional_model),\n         (\"cnn_functional\", get_cnn_functional_model, True),\n@@ -71,11 +103,10 @@ class CloneModelTest(testing.TestCase, parameterized.TestCase):\n         ref_input = np.random.random((2, 7, 3) if is_conv else (2, 3))\n         model = model_fn()\n         new_model = clone_model(model)\n-        ref_output = model(ref_input)\n+        model(ref_input)  # Maybe needed to build the model\n         new_model(ref_input)  # Maybe needed to build the model\n         new_model.set_weights(model.get_weights())\n-        output = new_model(ref_input)\n-        self.assertAllClose(ref_output, output)\n+        self.assert_models_equal(model, new_model, ref_input)\n \n     @parameterized.named_parameters(\n         (\"mlp_functional\", get_mlp_functional_model),\n@@ -121,3 +152,68 @@ class CloneModelTest(testing.TestCase, parameterized.TestCase):\n             \"`input_tensors` must have the same structure as model.input\",\n         ):\n             model = clone_model(model0, input_tensors=(x, y))\n+\n+    def test_call_fn(self):\n+        model = get_mlp_functional_model(shared_layers=False)\n+\n+        def call_function(layer, *args, **kwargs):\n+            out = layer(*args, **kwargs)\n+            if isinstance(layer, layers.Dense):\n+                out = layers.Dropout(0.5)(out)\n+            return out\n+\n+        new_model = clone_model(\n+            model,\n+            clone_function=lambda x: x,  # Reuse the same layers.\n+            call_function=call_function,\n+        )\n+        self.assertLen(model.layers, 3)\n+        self.assertLen(new_model.layers, 5)\n+        self.assertIsInstance(new_model.layers[2], layers.Dropout)\n+        self.assertIsInstance(new_model.layers[4], layers.Dropout)\n+        ref_input = np.random.random((2, 3))\n+        self.assert_models_equal(model, new_model, ref_input)\n+\n+    def test_recursive(self):\n+        model = get_nested_functional_model()\n+\n+        def call_function(layer, *args, **kwargs):\n+            out = layer(*args, **kwargs)\n+            if isinstance(layer, layers.Dense):\n+                out = layers.Dropout(0.5)(out)\n+            return out\n+\n+        new_model = clone_model(\n+            model,\n+            clone_function=lambda x: x,  # Reuse the same layers.\n+            call_function=call_function,\n+            recursive=True,\n+        )\n+        self.assertLen(model._flatten_layers(), 8)\n+        self.assertLen(new_model._flatten_layers(), 12)\n+        self.assertIsInstance(new_model.layers[3].layers[2], layers.Dropout)\n+        self.assertIsInstance(new_model.layers[3].layers[4], layers.Dropout)\n+        ref_input = np.random.random((2, 4))\n+        self.assert_models_equal(model, new_model, ref_input)\n+\n+        # Sequential.\n+        def clone_function(layer):\n+            layer = layer.__class__.from_config(layer.get_config())\n+            layer.flag = True\n+            return layer\n+\n+        model = get_nested_sequential_model()\n+        new_model = clone_model(\n+            model,\n+            clone_function=clone_function,\n+            recursive=True,\n+        )\n+        ref_input = np.random.random((2, 3))\n+        model(ref_input)  # Maybe needed to build the model\n+        new_model(ref_input)  # Maybe needed to build the model\n+        new_model.set_weights(model.get_weights())\n+        self.assert_models_equal(model, new_model, ref_input)\n+        for l1, l2 in zip(model._flatten_layers(), new_model._flatten_layers()):\n+            if isinstance(l2, layers.Dense):\n+                self.assertFalse(hasattr(l1, \"flag\"))\n+                self.assertTrue(hasattr(l2, \"flag\"))\n\n@@ -121,7 +121,7 @@ class Function(Operation):\n         self._assert_input_compatibility(inputs)\n         return self._run_through_graph(inputs, operation_fn=lambda op: op)\n \n-    def _run_through_graph(self, inputs, operation_fn):\n+    def _run_through_graph(self, inputs, operation_fn, call_fn=None):\n         \"\"\"Execute the graph.\n \n         At each node we compute outputs via\n@@ -148,7 +148,11 @@ class Function(Operation):\n                     continue  # Node is not computable, try skipping.\n \n                 args, kwargs = node.arguments.fill_in(tensor_dict)\n-                outputs = operation_fn(node.operation)(*args, **kwargs)\n+                op = operation_fn(node.operation)\n+                if call_fn is not None:\n+                    outputs = call_fn(op, *args, **kwargs)\n+                else:\n+                    outputs = op(*args, **kwargs)\n \n                 # Update tensor_dict.\n                 for x, y in zip(node.outputs, tree.flatten(outputs)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8200a78d85a9371afaf78563c5f78744720be4e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 16 | Churn Cumulative: 36 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -272,7 +272,11 @@ class Bidirectional(Wrapper):\n         return None\n \n     def build(self, sequences_shape, initial_state_shape=None):\n+        if not self.layer.built:\n+            self.layer.build(sequences_shape)\n+        if not self.forward_layer.built:\n             self.forward_layer.build(sequences_shape)\n+        if not self.backward_layer.built:\n             self.backward_layer.build(sequences_shape)\n         self.built = True\n \n\n@@ -906,3 +906,15 @@ class SavingBattleTest(testing.TestCase):\n         out = new_model(x)\n         self.assertAllClose(ref_out[0], out[0])\n         self.assertAllClose(ref_out[1], out[1])\n+\n+    def test_bidirectional_lstm_saving(self):\n+        inputs = keras.Input((3, 2))\n+        outputs = keras.layers.Bidirectional(keras.layers.LSTM(64))(inputs)\n+        model = keras.Model(inputs, outputs)\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"bidir_lstm.keras\")\n+        model.save(temp_filepath)\n+        new_model = keras.saving.load_model(temp_filepath)\n+        x = np.random.random((1, 3, 2))\n+        ref_out = model(x)\n+        out = new_model(x)\n+        self.assertAllClose(ref_out, out)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b3d96057ff45541dc5ca7dd6ef5c8062536bce3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ class SimpleRNNTest(testing.TestCase):\n             init_kwargs={\"layer\": layers.SimpleRNN(4)},\n             input_shape=(3, 2, 4),\n             expected_output_shape=(3, 8),\n-            expected_num_trainable_weights=6,\n+            expected_num_trainable_weights=9,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n         )\n@@ -27,7 +27,7 @@ class SimpleRNNTest(testing.TestCase):\n             },\n             input_shape=(3, 2, 4),\n             expected_output_shape=(3, 4),\n-            expected_num_trainable_weights=6,\n+            expected_num_trainable_weights=9,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#504bb958903afc84b2810c27b88775e8f0d40bfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 23 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,11 +11,18 @@ from keras.src.backend.jax.core import convert_to_tensor\n \n def cholesky(a):\n     out = jnp.linalg.cholesky(a)\n+    try:\n+        # In eager mode, raise for nan to\n+        # achieve behavior consistency with numpy\n         if jnp.any(jnp.isnan(out)):\n             raise ValueError(\n                 \"Cholesky decomposition failed. \"\n-            \"The input might not be a valid positive definite matrix.\"\n+                \"The input might not be a valid \"\n+                \"positive definite matrix.\"\n             )\n+    except jax.errors.TracerBoolConversionError:\n+        # Cannot raise for nan in tracing mode\n+        pass\n     return out\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7824ac5f476acea4145d6ebe7256c24d5e463c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 52 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -548,7 +548,7 @@ def _extract_patches(\n     if not strides:\n         strides = size\n     out_dim = patch_h * patch_w * channels_in\n-    kernel = backend.numpy.eye(out_dim)\n+    kernel = backend.numpy.eye(out_dim, dtype=image.dtype)\n     kernel = backend.numpy.reshape(\n         kernel, (patch_h, patch_w, channels_in, out_dim)\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67419fb2986ebc11110d20f8e555298d26cb106d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 11 | Churn Cumulative: 37 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,13 +3,12 @@ import copy\n from keras.src import ops\n from keras.src import utils\n from keras.src.api_export import keras_export\n-from keras.src.layers.core.wrapper import Wrapper\n from keras.src.layers.layer import Layer\n from keras.src.saving import serialization_lib\n \n \n @keras_export(\"keras.layers.Bidirectional\")\n-class Bidirectional(Wrapper):\n+class Bidirectional(Layer):\n     \"\"\"Bidirectional wrapper for RNNs.\n \n     Args:\n@@ -105,7 +104,7 @@ class Bidirectional(Wrapper):\n                 \"Merge mode should be one of \"\n                 '{\"sum\", \"mul\", \"ave\", \"concat\", None}'\n             )\n-        super().__init__(layer, **kwargs)\n+        super().__init__(**kwargs)\n \n         # Recreate the forward layer from the original layer config, so that it\n         # will not carry over any state from the layer.\n@@ -272,8 +271,6 @@ class Bidirectional(Wrapper):\n         return None\n \n     def build(self, sequences_shape, initial_state_shape=None):\n-        if not self.layer.built:\n-            self.layer.build(sequences_shape)\n         if not self.forward_layer.built:\n             self.forward_layer.build(sequences_shape)\n         if not self.backward_layer.built:\n\n@@ -14,7 +14,7 @@ class SimpleRNNTest(testing.TestCase):\n             init_kwargs={\"layer\": layers.SimpleRNN(4)},\n             input_shape=(3, 2, 4),\n             expected_output_shape=(3, 8),\n-            expected_num_trainable_weights=9,\n+            expected_num_trainable_weights=6,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n         )\n@@ -27,7 +27,7 @@ class SimpleRNNTest(testing.TestCase):\n             },\n             input_shape=(3, 2, 4),\n             expected_output_shape=(3, 4),\n-            expected_num_trainable_weights=9,\n+            expected_num_trainable_weights=6,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5883a25f1b7c6eacc3f21f1821751a4109700796", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 12 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 94 | Churn Cumulative: 120 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -409,12 +409,34 @@ def paths_and_labels_to_dataset(\n ):\n     \"\"\"Constructs a fixed-size dataset of audio and labels.\"\"\"\n     path_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n+    if label_mode:\n+        label_ds = dataset_utils.labels_to_dataset(\n+            labels, label_mode, num_classes\n+        )\n+        ds = tf.data.Dataset.zip((path_ds, label_ds))\n+    else:\n+        ds = path_ds\n+\n     if shuffle:\n-        path_ds = path_ds.shuffle(\n-            buffer_size=shuffle_buffer_size or 1024, seed=seed\n+        ds = ds.shuffle(buffer_size=shuffle_buffer_size or 1024, seed=seed)\n+\n+    if label_mode:\n+        ds = ds.map(\n+            lambda x, y: (\n+                read_and_decode_audio(x, sampling_rate, output_sequence_length),\n+                y,\n+            ),\n+            num_parallel_calls=tf.data.AUTOTUNE,\n         )\n \n-    audio_ds = path_ds.map(\n+        if ragged:\n+            ds = ds.map(\n+                lambda x, y: (tf.RaggedTensor.from_tensor(x), y),\n+                num_parallel_calls=tf.data.AUTOTUNE,\n+            )\n+\n+    else:\n+        ds = ds.map(\n             lambda x: read_and_decode_audio(\n                 x, sampling_rate, output_sequence_length\n             ),\n@@ -422,14 +444,9 @@ def paths_and_labels_to_dataset(\n         )\n \n         if ragged:\n-        audio_ds = audio_ds.map(\n+            ds = ds.map(\n                 lambda x: tf.RaggedTensor.from_tensor(x),\n                 num_parallel_calls=tf.data.AUTOTUNE,\n             )\n \n-    if label_mode:\n-        label_ds = dataset_utils.labels_to_dataset(\n-            labels, label_mode, num_classes\n-        )\n-        audio_ds = tf.data.Dataset.zip((audio_ds, label_ds))\n-    return audio_ds\n+    return ds\n\n@@ -367,12 +367,17 @@ def paths_and_labels_to_dataset(\n     seed=None,\n ):\n     \"\"\"Constructs a dataset of images and labels.\"\"\"\n-    # TODO(fchollet): consider making num_parallel_calls settable\n     path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n-    if shuffle:\n-        path_ds = path_ds.shuffle(\n-            buffer_size=shuffle_buffer_size or 1024, seed=seed\n+    if label_mode:\n+        label_ds = dataset_utils.labels_to_dataset(\n+            labels, label_mode, num_classes\n         )\n+        ds = tf.data.Dataset.zip((path_ds, label_ds))\n+    else:\n+        ds = path_ds\n+\n+    if shuffle:\n+        ds = ds.shuffle(buffer_size=shuffle_buffer_size or 1024, seed=seed)\n \n     args = (\n         image_size,\n@@ -382,15 +387,16 @@ def paths_and_labels_to_dataset(\n         crop_to_aspect_ratio,\n         pad_to_aspect_ratio,\n     )\n-    img_ds = path_ds.map(\n+    if label_mode:\n+        ds = ds.map(\n+            lambda x, y: (load_image(x, *args), y),\n+            num_parallel_calls=tf.data.AUTOTUNE,\n+        )\n+    else:\n+        ds = ds.map(\n             lambda x: load_image(x, *args), num_parallel_calls=tf.data.AUTOTUNE\n         )\n-    if label_mode:\n-        label_ds = dataset_utils.labels_to_dataset(\n-            labels, label_mode, num_classes\n-        )\n-        img_ds = tf.data.Dataset.zip((img_ds, label_ds))\n-    return img_ds\n+    return ds\n \n \n def load_image(\n\n@@ -258,21 +258,28 @@ def paths_and_labels_to_dataset(\n ):\n     \"\"\"Constructs a dataset of text strings and labels.\"\"\"\n     path_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n-    if shuffle:\n-        path_ds = path_ds.shuffle(\n-            buffer_size=shuffle_buffer_size or 1024, seed=seed\n-        )\n-\n-    string_ds = path_ds.map(\n-        lambda x: path_to_string_content(x, max_length),\n-        num_parallel_calls=tf.data.AUTOTUNE,\n-    )\n     if label_mode:\n         label_ds = dataset_utils.labels_to_dataset(\n             labels, label_mode, num_classes\n         )\n-        string_ds = tf.data.Dataset.zip((string_ds, label_ds))\n-    return string_ds\n+        ds = tf.data.Dataset.zip((path_ds, label_ds))\n+    else:\n+        ds = path_ds\n+\n+    if shuffle:\n+        ds = ds.shuffle(buffer_size=shuffle_buffer_size or 1024, seed=seed)\n+\n+    if label_mode:\n+        ds = ds.map(\n+            lambda x, y: (path_to_string_content(x, max_length), y),\n+            num_parallel_calls=tf.data.AUTOTUNE,\n+        )\n+    else:\n+        ds = ds.map(\n+            lambda x: path_to_string_content(x, max_length),\n+            num_parallel_calls=tf.data.AUTOTUNE,\n+        )\n+    return ds\n \n \n def path_to_string_content(path, max_length):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#61e37bd18bc37163702eeb0d39a1cac1115bdfc6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.src.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.3.2\"\n+__version__ = \"3.3.3\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#533bd4342a3f35e162797ba6d62cfdc5477ef109", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 24 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -164,7 +164,7 @@ class CompileMetrics(metrics_module.Metric):\n         if not self.built:\n             return []\n         vars = []\n-        for m in self._flat_metrics + self._flat_weighted_metrics:\n+        for m in self.metrics:\n             if m is not None:\n                 vars.extend(m.variables)\n         return vars\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6c4ac55692c132cd16211f4877fac6dbeead749", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1612 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -86,9 +86,8 @@ def build(root_path, is_nightly=False, rc_index=None):\n         move_tf_keras_directory()\n         print(os.getcwd())\n \n-        # from keras.src.version import __version__  # noqa: E402\n+        from keras.src.version import __version__  # noqa: E402\n \n-        __version__ = \"3.3.2\"\n         export_version_string(__version__, is_nightly, rc_index)\n         return build_and_save_output(root_path, __version__)\n     finally:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe03ca5974c970627b9289ec95689c44c7176540", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 113 | Files Changed: 6 | Hunks: 80 | Methods Changed: 29 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 208 | Churn Cumulative: 428 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 12 | DMM Complexity: 0.16666666666666666\n\nDIFF:\n@@ -52,11 +52,10 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n                 \"scipy_sparse\",\n             ],\n             array_dtype=[\"float32\", \"float64\"],\n-            iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n             shuffle=[False, \"batch\", True],\n         )\n     )\n-    def test_basic_flow(self, array_type, array_dtype, iterator_type, shuffle):\n+    def test_basic_flow(self, array_type, array_dtype, shuffle):\n         x = self.make_array(array_type, (34, 4), array_dtype)\n         y = self.make_array(array_type, (34, 2), \"int32\")\n         xdim1 = 1 if array_type == \"pandas_series\" else 4\n@@ -75,10 +74,10 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(adapter.has_partial_batch, True)\n         self.assertEqual(adapter.partial_batch_size, 2)\n \n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n             expected_class = np.ndarray\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n             if array_type == \"tf_ragged\":\n                 expected_class = tf.RaggedTensor\n@@ -88,13 +87,13 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n                 expected_class = tf.SparseTensor\n             else:\n                 expected_class = tf.Tensor\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n             if array_type in (\"tf_sparse\", \"jax_sparse\", \"scipy_sparse\"):\n                 expected_class = jax_sparse.JAXSparse\n             else:\n                 expected_class = jax.Array\n-        elif iterator_type == \"torch\":\n+        elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n \n\n@@ -3,12 +3,14 @@ import math\n import jax\n import jax.experimental.sparse as jax_sparse\n import numpy as np\n+import pytest\n import scipy\n import tensorflow as tf\n import torch\n from absl.testing import parameterized\n from jax import numpy as jnp\n \n+from keras.src import backend\n from keras.src import testing\n from keras.src.testing.test_utils import named_product\n from keras.src.trainers.data_adapters import generator_data_adapter\n@@ -37,10 +39,9 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n                 {\"testcase_name\": \"no_weight\", \"use_sample_weight\": False},\n             ],\n             generator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n-            iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n         )\n     )\n-    def test_basic_flow(self, use_sample_weight, generator_type, iterator_type):\n+    def test_basic_flow(self, use_sample_weight, generator_type):\n         x = np.random.random((34, 4)).astype(\"float32\")\n         y = np.array([[i, i] for i in range(34)], dtype=\"float32\")\n         sw = np.random.random((34,)).astype(\"float32\")\n@@ -64,16 +65,16 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n         )\n \n         adapter = generator_data_adapter.GeneratorDataAdapter(make_generator())\n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n             expected_class = np.ndarray\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n             expected_class = tf.Tensor\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n             expected_class = jax.Array\n-        elif iterator_type == \"torch\":\n+        elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n \n@@ -101,10 +102,7 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n                 sample_order.append(by[i, 0])\n         self.assertAllClose(sample_order, list(range(34)))\n \n-    @parameterized.named_parameters(\n-        named_product(iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"])\n-    )\n-    def test_with_different_shapes(self, iterator_type):\n+    def test_with_different_shapes(self):\n         def generator():\n             yield np.ones([16, 4], \"float32\"), np.ones([16, 2], \"float32\")\n             yield np.ones([16, 5], \"float32\"), np.ones([16, 2], \"float32\")\n@@ -112,13 +110,13 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n \n         adapter = generator_data_adapter.GeneratorDataAdapter(generator())\n \n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n-        elif iterator_type == \"torch\":\n+        elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n \n         for i, batch in enumerate(it):\n@@ -137,11 +135,13 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n                 self.assertEqual(by.shape, (2, 2))\n \n     @parameterized.named_parameters(\n-        named_product(\n-            generator_type=[\"tf\", \"jax\", \"scipy\"], iterator_type=[\"tf\", \"jax\"]\n+        named_product(generator_type=[\"tf\", \"jax\", \"scipy\"])\n     )\n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS,\n+        reason=\"Backend does not support sparse tensors\",\n     )\n-    def test_scipy_sparse_tensors(self, generator_type, iterator_type):\n+    def test_scipy_sparse_tensors(self, generator_type):\n         if generator_type == \"tf\":\n             x = tf.SparseTensor([[0, 0], [1, 2]], [1.0, 2.0], (2, 4))\n             y = tf.SparseTensor([[0, 0], [1, 1]], [3.0, 4.0], (2, 2))\n@@ -158,10 +158,10 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n \n         adapter = generator_data_adapter.GeneratorDataAdapter(generate())\n \n-        if iterator_type == \"tf\":\n+        if backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n             expected_class = tf.SparseTensor\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n             expected_class = jax_sparse.BCOO\n \n\n@@ -342,11 +342,6 @@ def get_worker_id_queue():\n     return _WORKER_ID_QUEUE\n \n \n-def init_pool(seqs):\n-    global _SHARED_SEQUENCES\n-    _SHARED_SEQUENCES = seqs\n-\n-\n def get_index(uid, i):\n     \"\"\"Get the value from the PyDataset `uid` at index `i`.\n \n\n@@ -3,10 +3,12 @@ import time\n \n import jax\n import numpy as np\n+import pytest\n import tensorflow as tf\n import torch\n from absl.testing import parameterized\n \n+from keras.src import backend\n from keras.src import testing\n from keras.src.testing.test_utils import named_product\n from keras.src.trainers.data_adapters import py_dataset_adapter\n@@ -77,6 +79,21 @@ class DictPyDataset(py_dataset_adapter.PyDataset):\n         return batch\n \n \n+class ExceptionPyDataset(py_dataset_adapter.PyDataset):\n+\n+    @property\n+    def num_batches(self):\n+        return 4\n+\n+    def __getitem__(self, index):\n+        if index < 2:\n+            return (\n+                np.random.random((64, 4)).astype(\"float32\"),\n+                np.random.random((64, 2)).astype(\"float32\"),\n+            )\n+        raise ValueError(\"Expected exception\")\n+\n+\n class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.named_parameters(\n         named_product(\n@@ -113,7 +130,6 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n                 },\n             ],\n             infinite=[True, False],\n-            iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n             shuffle=[True, False],\n         )\n     )\n@@ -122,11 +138,13 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n         shuffle,\n         dataset_type,\n         infinite,\n-        iterator_type,\n         workers=0,\n         use_multiprocessing=False,\n         max_queue_size=0,\n     ):\n+        if use_multiprocessing and (infinite or shuffle):\n+            pytest.skip(\"Starting processes is slow, only test one variant\")\n+\n         set_random_seed(1337)\n         x = np.random.random((64, 4)).astype(\"float32\")\n         y = np.array([[i, i] for i in range(64)], dtype=\"float32\")\n@@ -149,16 +167,16 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n             py_dataset, shuffle=shuffle\n         )\n \n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n             expected_class = np.ndarray\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n             expected_class = tf.Tensor\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n             expected_class = jax.Array\n-        elif iterator_type == \"torch\":\n+        elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n \n@@ -257,10 +275,7 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(tuple(bx.shape), (4, 4))\n             self.assertEqual(tuple(by.shape), (4, 2))\n \n-    @parameterized.named_parameters(\n-        named_product(iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"])\n-    )\n-    def test_with_different_shapes(self, iterator_type):\n+    def test_with_different_shapes(self):\n \n         class TestPyDataset(py_dataset_adapter.PyDataset):\n             @property\n@@ -285,13 +300,13 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n             TestPyDataset(), shuffle=False\n         )\n \n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n-        elif iterator_type == \"torch\":\n+        elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n \n         for i, batch in enumerate(it):\n@@ -310,7 +325,6 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n                 self.assertEqual(by.shape, (2, 2))\n \n     @parameterized.named_parameters(\n-        named_product(\n         [\n             {\n                 \"testcase_name\": \"multiprocessing\",\n@@ -326,51 +340,37 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n             {\n                 \"testcase_name\": \"single\",\n             },\n-            ],\n-            iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n-        )\n+        ]\n     )\n     def test_exception_reported(\n         self,\n-        iterator_type,\n         workers=0,\n         use_multiprocessing=False,\n         max_queue_size=0,\n     ):\n-        class ExceptionPyDataset(py_dataset_adapter.PyDataset):\n-\n-            @property\n-            def num_batches(self):\n-                return 4\n-\n-            def __getitem__(self, index):\n-                if index < 2:\n-                    return (\n-                        np.random.random((64, 4)).astype(\"float32\"),\n-                        np.random.random((64, 2)).astype(\"float32\"),\n-                    )\n-                raise ValueError(\"Excepted exception\")\n-\n-        adapter = py_dataset_adapter.PyDatasetAdapter(\n-            ExceptionPyDataset(), shuffle=False\n+        dataset = ExceptionPyDataset(\n+            workers=workers,\n+            use_multiprocessing=use_multiprocessing,\n+            max_queue_size=max_queue_size,\n         )\n+        adapter = py_dataset_adapter.PyDatasetAdapter(dataset, shuffle=False)\n \n         expected_exception_class = ValueError\n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n             # tf.data wraps the exception\n             expected_exception_class = tf.errors.InvalidArgumentError\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n-        elif iterator_type == \"torch\":\n+        elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n \n         it = iter(it)\n         next(it)\n         next(it)\n         with self.assertRaisesRegex(\n-            expected_exception_class, \"Excepted exception\"\n+            expected_exception_class, \"Expected exception\"\n         ):\n             next(it)\n\n@@ -2,20 +2,18 @@ from unittest import mock\n \n import jax\n import numpy as np\n+import pytest\n import tensorflow as tf\n import torch\n from absl.testing import parameterized\n \n+from keras.src import backend\n from keras.src import testing\n-from keras.src.testing.test_utils import named_product\n from keras.src.trainers.data_adapters import tf_dataset_adapter\n \n \n class TestTFDatasetAdapter(testing.TestCase, parameterized.TestCase):\n-    @parameterized.named_parameters(\n-        named_product(iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"])\n-    )\n-    def test_basic_flow(self, iterator_type):\n+    def test_basic_flow(self):\n         x = tf.random.normal((34, 4))\n         y = tf.random.normal((34, 2))\n         base_ds = tf.data.Dataset.from_tensor_slices((x, y)).batch(16)\n@@ -26,16 +24,16 @@ class TestTFDatasetAdapter(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(adapter.has_partial_batch, None)\n         self.assertEqual(adapter.partial_batch_size, None)\n \n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n             expected_class = np.ndarray\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n             expected_class = tf.Tensor\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n             expected_class = jax.Array\n-        elif iterator_type == \"torch\":\n+        elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n \n@@ -258,10 +256,11 @@ class TestTFDatasetAdapter(testing.TestCase, parameterized.TestCase):\n                 self.assertEqual(tuple(bx.shape), (2, 4))\n                 self.assertEqual(tuple(by.shape), (2, 2))\n \n-    @parameterized.named_parameters(\n-        named_product(iterator_type=[\"np\", \"tf\", \"jax\"])\n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS and backend.backend() != \"numpy\",\n+        reason=\"Backend does not support sparse tensors\",\n     )\n-    def test_tf_sparse_tensors(self, iterator_type):\n+    def test_tf_sparse_tensors(self):\n         x = tf.SparseTensor(\n             indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 4)\n         )\n@@ -271,13 +270,13 @@ class TestTFDatasetAdapter(testing.TestCase, parameterized.TestCase):\n         base_ds = tf.data.Dataset.from_tensors((x, y))\n         adapter = tf_dataset_adapter.TFDatasetAdapter(base_ds)\n \n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n             expected_class = np.ndarray\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n             expected_class = tf.SparseTensor\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n             expected_class = jax.experimental.sparse.BCOO\n \n\n@@ -6,6 +6,7 @@ import tensorflow as tf\n import torch\n from absl.testing import parameterized\n \n+from keras.src import backend\n from keras.src import testing\n from keras.src.testing.test_utils import named_product\n from keras.src.trainers.data_adapters.torch_data_loader_adapter import (\n@@ -14,10 +15,7 @@ from keras.src.trainers.data_adapters.torch_data_loader_adapter import (\n \n \n class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n-    @parameterized.named_parameters(\n-        named_product(iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"])\n-    )\n-    def test_basic_dataloader(self, iterator_type):\n+    def test_basic_dataloader(self):\n         x = torch.normal(2, 3, size=(34, 4))\n         y = torch.normal(1, 3, size=(34, 2))\n         ds = torch.utils.data.TensorDataset(x, y)\n@@ -29,16 +27,16 @@ class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(adapter.has_partial_batch, True)\n         self.assertEqual(adapter.partial_batch_size, 2)\n \n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n             expected_class = np.ndarray\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n             expected_class = tf.Tensor\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n             expected_class = jax.Array\n-        elif iterator_type == \"torch\":\n+        elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n \n@@ -57,15 +55,9 @@ class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n                 self.assertEqual(by.shape, (2, 2))\n \n     @parameterized.named_parameters(\n-        named_product(\n-            batch_size=[None, 3],\n-            implements_len=[True, False],\n-            iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n+        named_product(batch_size=[None, 3], implements_len=[True, False])\n     )\n-    )\n-    def test_dataloader_iterable_dataset(\n-        self, batch_size, implements_len, iterator_type\n-    ):\n+    def test_dataloader_iterable_dataset(self, batch_size, implements_len):\n \n         class TestIterableDataset(torch.utils.data.IterableDataset):\n             def __init__(self):\n@@ -104,16 +96,16 @@ class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n             self.assertIsNone(adapter.has_partial_batch)\n             self.assertIsNone(adapter.partial_batch_size)\n \n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n             expected_class = np.ndarray\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n             expected_class = tf.Tensor\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n             expected_class = jax.Array\n-        elif iterator_type == \"torch\":\n+        elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n \n@@ -142,10 +134,7 @@ class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n         else:\n             self.assertEqual(batch_count, 10)\n \n-    @parameterized.named_parameters(\n-        named_product(iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"])\n-    )\n-    def test_with_different_shapes(self, iterator_type):\n+    def test_with_different_shapes(self):\n         x = (\n             [np.ones([4], \"float32\")] * 16\n             + [np.ones([5], \"float32\")] * 16\n@@ -161,13 +150,13 @@ class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(adapter.has_partial_batch, True)\n         self.assertEqual(adapter.partial_batch_size, 2)\n \n-        if iterator_type == \"np\":\n+        if backend.backend() == \"numpy\":\n             it = adapter.get_numpy_iterator()\n-        elif iterator_type == \"tf\":\n+        elif backend.backend() == \"tensorflow\":\n             it = adapter.get_tf_dataset()\n-        elif iterator_type == \"jax\":\n+        elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n-        elif iterator_type == \"torch\":\n+        elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n \n         for i, batch in enumerate(it):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
