{"custom_id": "scrapy#782f866572d8bf8b1673ea962ae57d7631d1d9de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 34 | Churn Cumulative: 1510 | Contributors (this commit): 26 | Commits (past 90d): 7 | Contributors (cumulative): 36 | DMM Complexity: 0.84\n\nDIFF:\n@@ -114,10 +114,12 @@ def _get_form(response, formname, formid, formnumber, formxpath):\n \n def _get_inputs(form, formdata, dont_click, clickdata, response):\n     try:\n-        formdata = dict(formdata or ())\n+        formdata_keys = dict(formdata or ()).keys()\n     except (ValueError, TypeError):\n         raise ValueError('formdata should be a dict or iterable of tuples')\n \n+    if not formdata:\n+        formdata = ()\n     inputs = form.xpath('descendant::textarea'\n                         '|descendant::select'\n                         '|descendant::input[not(@type) or @type['\n@@ -128,14 +130,17 @@ def _get_inputs(form, formdata, dont_click, clickdata, response):\n                             \"re\": \"http://exslt.org/regular-expressions\"})\n     values = [(k, u'' if v is None else v)\n               for k, v in (_value(e) for e in inputs)\n-              if k and k not in formdata]\n+              if k and k not in formdata_keys]\n \n     if not dont_click:\n         clickable = _get_clickable(clickdata, form)\n         if clickable and clickable[0] not in formdata and not clickable[0] is None:\n             values.append(clickable)\n \n-    values.extend((k, v) for k, v in formdata.items() if v is not None)\n+    if isinstance(formdata, dict):\n+        formdata = formdata.items()\n+\n+    values.extend((k, v) for k, v in formdata if v is not None)\n     return values\n \n \n\n@@ -401,6 +401,29 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs[u'test2'], [u'xxx µ'])\n         self.assertEqual(fs[u'six'], [u'seven'])\n \n+    def test_from_response_duplicate_form_key(self):\n+        response = _buildresponse(\n+                '<form></form>',\n+                url='http://www.example.com')\n+        req = self.request_class.from_response(response,\n+                method='GET',\n+                formdata=(('foo', 'bar'), ('foo', 'baz')))\n+        self.assertEqual(urlparse(req.url).hostname, 'www.example.com')\n+        self.assertEqual(urlparse(req.url).query, 'foo=bar&foo=baz')\n+    \n+    def test_from_response_override_duplicate_form_key(self):\n+        response = _buildresponse(\n+            \"\"\"<form action=\"get.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"one\" value=\"1\">\n+            <input type=\"hidden\" name=\"two\" value=\"3\">\n+            </form>\"\"\")\n+        req = self.request_class.from_response(\n+            response,\n+            formdata=(('two', '2'), ('two', '4')))\n+        fs = _qs(req)\n+        self.assertEqual(fs[b'one'], [b'1'])\n+        self.assertEqual(fs[b'two'], [b'2', b'4'])\n+\n     def test_from_response_extra_headers(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"post.php\" method=\"POST\">\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#701cd2ff9d4be34fcf63a7410d09f425410635f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 28 | Churn Cumulative: 455 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,12 @@ class Scheduler(object):\n     def from_crawler(cls, crawler):\n         settings = crawler.settings\n         dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n-        dupefilter = dupefilter_cls.from_settings(settings)\n+        if hasattr(dupefilter_cls, 'from_crawler'):\n+            dupefilter = dupefilter_cls.from_crawler(crawler)\n+        elif hasattr(dupefilter_cls, 'from_settings'):\n+            dupefilter = dupefilter_cls.from_settings(crawler.settings)\n+        else:\n+            dupefilter = dupefilter_cls()\n         pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n         dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n         mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n\n@@ -6,10 +6,31 @@ import shutil\n from scrapy.dupefilters import RFPDupeFilter\n from scrapy.http import Request\n from scrapy.utils.python import to_bytes\n+from scrapy.utils.job import job_dir\n+from scrapy.utils.test import get_crawler\n \n \n class RFPDupeFilterTest(unittest.TestCase):\n \n+    def test_dupefilter_from_crawler(self):\n+\n+        class FromCrawlerRFPDupeFilter(RFPDupeFilter):\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                debug = crawler.settings.getbool('DUPEFILTER_DEBUG')\n+                df = cls(job_dir(crawler.settings), debug)\n+                df.user_agent = crawler.settings.get('USER_AGENT')\n+                return df\n+\n+        crawler = get_crawler(settings_dict={'DUPEFILTER_DEBUG': True, 'USER_AGENT': 'test ua'})\n+        dupefilter = FromCrawlerRFPDupeFilter.from_crawler(crawler)\n+\n+        self.assertTrue(dupefilter.debug)\n+        self.assertEqual(dupefilter.user_agent, 'test ua')\n+\n+        dupefilter.close('finished')\n+\n     def test_filter(self):\n         dupefilter = RFPDupeFilter()\n         dupefilter.open()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d306fe30ac08401e74fd20cf90e5164a4125d8e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 38 | Churn Cumulative: 87 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,31 +5,49 @@ import shutil\n \n from scrapy.dupefilters import RFPDupeFilter\n from scrapy.http import Request\n+from scrapy.core.scheduler import Scheduler\n from scrapy.utils.python import to_bytes\n from scrapy.utils.job import job_dir\n from scrapy.utils.test import get_crawler\n \n \n-class RFPDupeFilterTest(unittest.TestCase):\n-\n-    def test_dupefilter_from_crawler(self):\n-\n class FromCrawlerRFPDupeFilter(RFPDupeFilter):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         debug = crawler.settings.getbool('DUPEFILTER_DEBUG')\n         df = cls(job_dir(crawler.settings), debug)\n-                df.user_agent = crawler.settings.get('USER_AGENT')\n+        df.method = crawler.settings.get('METHOD')\n         return df\n \n-        crawler = get_crawler(settings_dict={'DUPEFILTER_DEBUG': True, 'USER_AGENT': 'test ua'})\n-        dupefilter = FromCrawlerRFPDupeFilter.from_crawler(crawler)\n \n-        self.assertTrue(dupefilter.debug)\n-        self.assertEqual(dupefilter.user_agent, 'test ua')\n+class FromSettingsRFPDupeFilter(RFPDupeFilter):\n \n-        dupefilter.close('finished')\n+    @classmethod\n+    def from_settings(cls, settings):\n+        debug = settings.getbool('DUPEFILTER_DEBUG')\n+        df = cls(job_dir(settings), debug)\n+        df.method = settings.get('METHOD')\n+        return df\n+\n+\n+class RFPDupeFilterTest(unittest.TestCase):\n+\n+    def test_from_crawler_scheduler(self):\n+        settings = {'DUPEFILTER_DEBUG': True, 'METHOD': 'from_crawler',\n+                    'DUPEFILTER_CLASS': __name__  + '.FromCrawlerRFPDupeFilter'}\n+        crawler = get_crawler(settings_dict=settings)\n+        scheduler = Scheduler.from_crawler(crawler)\n+        self.assertTrue(scheduler.df.debug)\n+        self.assertEqual(scheduler.df.method, 'from_crawler')\n+\n+    def test_from_settings_scheduler(self):\n+        settings = {'DUPEFILTER_DEBUG': True, 'METHOD': 'from_settings',\n+                    'DUPEFILTER_CLASS': __name__  + '.FromSettingsRFPDupeFilter'}\n+        crawler = get_crawler(settings_dict=settings)\n+        scheduler = Scheduler.from_crawler(crawler)\n+        self.assertTrue(scheduler.df.debug)\n+        self.assertEqual(scheduler.df.method, 'from_settings')\n \n     def test_filter(self):\n         dupefilter = RFPDupeFilter()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0089a4ab31d1764dd38c30c4448a4c62efd9b9c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 111 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,7 +17,7 @@ class FromCrawlerRFPDupeFilter(RFPDupeFilter):\n     def from_crawler(cls, crawler):\n         debug = crawler.settings.getbool('DUPEFILTER_DEBUG')\n         df = cls(job_dir(crawler.settings), debug)\n-        df.method = crawler.settings.get('METHOD')\n+        df.method = 'from_crawler'\n         return df\n \n \n@@ -27,28 +27,40 @@ class FromSettingsRFPDupeFilter(RFPDupeFilter):\n     def from_settings(cls, settings):\n         debug = settings.getbool('DUPEFILTER_DEBUG')\n         df = cls(job_dir(settings), debug)\n-        df.method = settings.get('METHOD')\n+        df.method = 'from_settings'\n         return df\n \n \n+class DirectRFPDupeFilter(RFPDupeFilter):\n+    method = 'n/a'\n+\n+\n class RFPDupeFilterTest(unittest.TestCase):\n \n-    def test_from_crawler_scheduler(self):\n-        settings = {'DUPEFILTER_DEBUG': True, 'METHOD': 'from_crawler',\n+    def test_df_from_crawler_scheduler(self):\n+        settings = {'DUPEFILTER_DEBUG': True,\n                     'DUPEFILTER_CLASS': __name__  + '.FromCrawlerRFPDupeFilter'}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, 'from_crawler')\n \n-    def test_from_settings_scheduler(self):\n-        settings = {'DUPEFILTER_DEBUG': True, 'METHOD': 'from_settings',\n+    def test_df_from_settings_scheduler(self):\n+        settings = {'DUPEFILTER_DEBUG': True,\n                     'DUPEFILTER_CLASS': __name__  + '.FromSettingsRFPDupeFilter'}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, 'from_settings')\n \n+    def test_df_direct_scheduler(self):\n+        settings = {'DUPEFILTER_DEBUG': True,\n+                    'DUPEFILTER_CLASS': __name__  + '.DirectRFPDupeFilter'}\n+        crawler = get_crawler(settings_dict=settings)\n+        scheduler = Scheduler.from_crawler(crawler)\n+        self.assertTrue(scheduler.df.debug)\n+        self.assertEqual(scheduler.df.method, 'n/a')\n+\n     def test_filter(self):\n         dupefilter = RFPDupeFilter()\n         dupefilter.open()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9e14f8c7e4141fd5216efc378c9d003ea981a4d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 117 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,7 +31,7 @@ class FromSettingsRFPDupeFilter(RFPDupeFilter):\n         return df\n \n \n-class DirectRFPDupeFilter(RFPDupeFilter):\n+class DirectDupeFilter(object):\n     method = 'n/a'\n \n \n@@ -54,11 +54,9 @@ class RFPDupeFilterTest(unittest.TestCase):\n         self.assertEqual(scheduler.df.method, 'from_settings')\n \n     def test_df_direct_scheduler(self):\n-        settings = {'DUPEFILTER_DEBUG': True,\n-                    'DUPEFILTER_CLASS': __name__  + '.DirectRFPDupeFilter'}\n+        settings = {'DUPEFILTER_CLASS': __name__  + '.DirectDupeFilter'}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n-        self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, 'n/a')\n \n     def test_filter(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#999341b60bd5289ca97c8187d0ff380a8555ba5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 9 | Churn Cumulative: 415 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,7 @@ import logging\n from os.path import join, exists\n \n from scrapy.utils.reqser import request_to_dict, request_from_dict\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import load_object, create_instance\n from scrapy.utils.job import job_dir\n \n logger = logging.getLogger(__name__)\n@@ -26,12 +26,7 @@ class Scheduler(object):\n     def from_crawler(cls, crawler):\n         settings = crawler.settings\n         dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n-        if hasattr(dupefilter_cls, 'from_crawler'):\n-            dupefilter = dupefilter_cls.from_crawler(crawler)\n-        elif hasattr(dupefilter_cls, 'from_settings'):\n-            dupefilter = dupefilter_cls.from_settings(crawler.settings)\n-        else:\n-            dupefilter = dupefilter_cls()\n+        dupefilter = create_instance(dupefilter_cls, settings, crawler)\n         pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n         dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n         mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6d3e87e3a4fd306829a84f934a971fd4e337a26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#801d3c07b4b7e57d50429e714a8255e7747568f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 11 | Complexity Δ (Sum/Max): 10/9 | Churn Δ: 84 | Churn Cumulative: 446 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,6 +66,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     if result is not None:\n                         raise _InvalidOutput('Middleware {} must return None or raise an exception, got {}' \\\n                                              .format(fname(method), type(result)))\n+                except _InvalidOutput:\n+                    raise\n                 except:\n                     return scrape_func(Failure(), request, spider)\n             return scrape_func(response, request, spider)\n\n@@ -0,0 +1,82 @@\n+from twisted.trial.unittest import TestCase\n+from twisted.python.failure import Failure\n+\n+from scrapy.spiders import Spider\n+from scrapy.http import Request, Response\n+from scrapy.exceptions import _InvalidOutput\n+from scrapy.utils.test import get_crawler\n+from scrapy.core.spidermw import SpiderMiddlewareManager\n+from tests import mock\n+\n+\n+class SpiderMiddlewareTestCase(TestCase):\n+\n+    def setUp(self):\n+        self.request = Request('http://example.com/index.html')\n+        self.response = Response(self.request.url, request=self.request)\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider('foo')\n+        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n+\n+    def _scrape_response(self):\n+        \"\"\"Execute spider mw manager's scrape_response method and return the result.\n+        Raise exception in case of failure.\n+        \"\"\"\n+        scrape_func = mock.MagicMock()\n+        dfd = self.mwman.scrape_response(scrape_func, self.response, self.request, self.spider)\n+        # catch deferred result and return the value\n+        results = []\n+        dfd.addBoth(results.append)\n+        self._wait(dfd)\n+        ret = results[0]\n+        return ret\n+\n+\n+class ProcessSpiderInputInvalidOutput(SpiderMiddlewareTestCase):\n+    \"\"\"Invalid return value for process_spider_input method\"\"\"\n+\n+    def test_invalid_process_spider_input(self):\n+\n+        class InvalidProcessSpiderInputMiddleware:\n+            def process_spider_input(self, response, spider):\n+                return 1\n+\n+        self.mwman._add_middleware(InvalidProcessSpiderInputMiddleware())\n+        result = self._scrape_response()\n+        self.assertIsInstance(result, Failure)\n+        self.assertIsInstance(result.value, _InvalidOutput)\n+\n+\n+class ProcessSpiderOutputInvalidOutput(SpiderMiddlewareTestCase):\n+    \"\"\"Invalid return value for process_spider_output method\"\"\"\n+\n+    def test_invalid_process_spider_output(self):\n+\n+        class InvalidProcessSpiderOutputMiddleware:\n+            def process_spider_output(self, response, result, spider):\n+                return 1\n+\n+        self.mwman._add_middleware(InvalidProcessSpiderOutputMiddleware())\n+        result = self._scrape_response()\n+        self.assertIsInstance(result, Failure)\n+        self.assertIsInstance(result.value, _InvalidOutput)\n+\n+\n+class ProcessSpiderExceptionInvalidOutput(SpiderMiddlewareTestCase):\n+    \"\"\"Invalid return value for process_spider_exception method\"\"\"\n+\n+    def test_invalid_process_spider_exception(self):\n+\n+        class InvalidProcessSpiderOutputExceptionMiddleware:\n+            def process_spider_exception(self, response, exception, spider):\n+                return 1\n+\n+        class RaiseExceptionProcessSpiderOutputMiddleware:\n+            def process_spider_output(self, response, result, spider):\n+                raise Exception()\n+\n+        self.mwman._add_middleware(InvalidProcessSpiderOutputExceptionMiddleware())\n+        self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n+        result = self._scrape_response()\n+        self.assertIsInstance(result, Failure)\n+        self.assertIsInstance(result.value, _InvalidOutput)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#980be4cb4b70c71e5502c8f31c85882540f3f078", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 142 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -269,9 +269,10 @@ class FeedExporter(object):\n             try:\n                 self._get_storage(uri)\n                 return True\n-            except NotConfigured:\n-                logger.error(\"Disabled feed storage scheme: %(scheme)s\",\n-                             {'scheme': scheme})\n+            except NotConfigured as e:\n+                logger.error(\"Disabled feed storage scheme: %(scheme)s. \"\n+                             \"Reason: %(reason)s\",\n+                             {'scheme': scheme, 'reason': str(e)})\n         else:\n             logger.error(\"Unknown feed storage scheme: %(scheme)s\",\n                          {'scheme': scheme})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c87a4f5c6fd97d22345bb444bc90b56ef0624aa5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 829 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,13 +1,10 @@\n import six\n import json\n import copy\n-import warnings\n from collections import MutableMapping\n from importlib import import_module\n from pprint import pformat\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n-\n from . import default_settings\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8c55f5eb159ae85d468a89b74ffaff3e824144ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): -34/0 | Churn Δ: 24 | Churn Cumulative: 1569 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -103,7 +103,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                         yield r\n                 except Exception as ex:\n                     exception_result = process_spider_exception(Failure(ex), index+1)\n-                    if exception_result is None or isinstance(exception_result, Failure):\n+                    if isinstance(exception_result, Failure):\n                         raise\n                     recovered.extend(exception_result)\n \n@@ -115,7 +115,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     result = method(response=response, result=result, spider=spider)\n                 except Exception as ex:\n                     exception_result = process_spider_exception(Failure(ex), method_index+1)\n-                    if exception_result is None or isinstance(exception_result, Failure):\n+                    if isinstance(exception_result, Failure):\n                         raise\n                     return exception_result\n                 if _isiterable(result):\n\n@@ -80,3 +80,23 @@ class ProcessSpiderExceptionInvalidOutput(SpiderMiddlewareTestCase):\n         result = self._scrape_response()\n         self.assertIsInstance(result, Failure)\n         self.assertIsInstance(result.value, _InvalidOutput)\n+\n+\n+class ProcessSpiderExceptionReRaise(SpiderMiddlewareTestCase):\n+    \"\"\"Re raise the exception by returning None\"\"\"\n+\n+    def test_process_spider_exception_return_none(self):\n+\n+        class ProcessSpiderOutputExceptionReturnNoneMiddleware:\n+            def process_spider_exception(self, response, exception, spider):\n+                return None\n+\n+        class RaiseExceptionProcessSpiderOutputMiddleware:\n+            def process_spider_output(self, response, result, spider):\n+                1/0\n+\n+        self.mwman._add_middleware(ProcessSpiderOutputExceptionReturnNoneMiddleware())\n+        self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n+        result = self._scrape_response()\n+        self.assertIsInstance(result, Failure)\n+        self.assertIsInstance(result.value, ZeroDivisionError)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40449fa0eb707bac1ae2b78f0f812372e90f17b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 46/46 | Churn Δ: 48 | Churn Cumulative: 2231 | Contributors (this commit): 15 | Commits (past 90d): 14 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -135,7 +135,6 @@ class Scraper(object):\n             return self.spidermw.scrape_response(\n                 self.call_spider, request_result, request, spider)\n         else:\n-            # FIXME: don't ignore errors in spider middleware\n             dfd = self.call_spider(request_result, request, spider)\n             return dfd.addErrback(\n                 self._log_download_errors, request_result, request, spider)\n\n@@ -87,7 +87,7 @@ class ProcessSpiderExceptionReRaise(SpiderMiddlewareTestCase):\n \n     def test_process_spider_exception_return_none(self):\n \n-        class ProcessSpiderOutputExceptionReturnNoneMiddleware:\n+        class ProcessSpiderExceptionReturnNoneMiddleware:\n             def process_spider_exception(self, response, exception, spider):\n                 return None\n \n@@ -95,7 +95,7 @@ class ProcessSpiderExceptionReRaise(SpiderMiddlewareTestCase):\n             def process_spider_output(self, response, result, spider):\n                 1/0\n \n-        self.mwman._add_middleware(ProcessSpiderOutputExceptionReturnNoneMiddleware())\n+        self.mwman._add_middleware(ProcessSpiderExceptionReturnNoneMiddleware())\n         self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n         result = self._scrape_response()\n         self.assertIsInstance(result, Failure)\n\n@@ -45,8 +45,13 @@ class RecoveryMiddleware:\n \n # ================================================================================\n # (1) exceptions from a spider middleware's process_spider_input method\n-class ProcessSpiderInputSpider(Spider):\n-    name = 'ProcessSpiderInputSpider'\n+class FailProcessSpiderInputMiddleware:\n+    def process_spider_input(self, response, spider):\n+        spider.logger.info('Middleware: will raise IndexError')\n+        raise IndexError()\n+\n+class ProcessSpiderInputSpiderWithoutErrback(Spider):\n+    name = 'ProcessSpiderInputSpiderWithoutErrback'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n             # spider\n@@ -58,23 +63,23 @@ class ProcessSpiderInputSpider(Spider):\n     }\n \n     def start_requests(self):\n-        yield Request(url=self.mockserver.url('/status?n=200'),\n-                      callback=self.parse, errback=self.errback)\n+        yield Request(url=self.mockserver.url('/status?n=200'), callback=self.parse)\n \n     def parse(self, response):\n         return {'from': 'callback'}\n \n+\n+class ProcessSpiderInputSpiderWithErrback(ProcessSpiderInputSpiderWithoutErrback):\n+    name = 'ProcessSpiderInputSpiderWithErrback'\n+\n+    def start_requests(self):\n+        yield Request(url=self.mockserver.url('/status?n=200'), callback=self.parse, errback=self.errback)\n+\n     def errback(self, failure):\n         self.logger.info('Got a Failure on the Request errback')\n         return {'from': 'errback'}\n \n \n-class FailProcessSpiderInputMiddleware:\n-    def process_spider_input(self, response, spider):\n-        spider.logger.info('Middleware: will raise IndexError')\n-        raise IndexError()\n-\n-\n # ================================================================================\n # (2) exceptions from a spider callback (generator)\n class GeneratorCallbackSpider(Spider):\n@@ -278,12 +283,22 @@ class TestSpiderMiddleware(TestCase):\n         self.assertIn(\"'item_scraped_count': 3\", str(log))\n \n     @defer.inlineCallbacks\n-    def test_process_spider_input_errback(self):\n+    def test_process_spider_input_without_errback(self):\n         \"\"\"\n-        (1) An exception from the process_spider_input chain should not be caught by the\n-        process_spider_exception chain, it should go directly to the Request errback\n+        (1.1) An exception from the process_spider_input chain should be caught by the\n+        process_spider_exception chain from the start if the Request has no errback\n         \"\"\"\n-        log1 = yield self.crawl_log(ProcessSpiderInputSpider)\n+        log1 = yield self.crawl_log(ProcessSpiderInputSpiderWithoutErrback)\n+        self.assertIn(\"Middleware: will raise IndexError\", str(log1))\n+        self.assertIn(\"Middleware: IndexError exception caught\", str(log1))\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_input_with_errback(self):\n+        \"\"\"\n+        (1.2) An exception from the process_spider_input chain should not be caught by the\n+        process_spider_exception chain if the Request has an errback\n+        \"\"\"\n+        log1 = yield self.crawl_log(ProcessSpiderInputSpiderWithErrback)\n         self.assertNotIn(\"Middleware: IndexError exception caught\", str(log1))\n         self.assertIn(\"Middleware: will raise IndexError\", str(log1))\n         self.assertIn(\"Got a Failure on the Request errback\", str(log1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#16dad81715d3970149c0cf7a318e73a0d84be1ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 19 | Churn Cumulative: 402 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,7 +84,7 @@ class ContractsManager(object):\n \n         def eb_wrapper(failure):\n             case = _create_testcase(method, 'errback')\n-            exc_info = failure.value, failure.type, failure.getTracebackObject()\n+            exc_info = failure.type, failure.value, failure.getTracebackObject()\n             results.addError(case, exc_info)\n \n         request.callback = cb_wrapper\n\n@@ -1,7 +1,9 @@\n from unittest import TextTestResult\n \n+from twisted.python import failure\n from twisted.trial import unittest\n \n+from scrapy.spidermiddlewares.httperror import HttpError\n from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.item import Item, Field\n@@ -185,3 +187,18 @@ class ContractsManagerTest(unittest.TestCase):\n                 self.results)\n         request.callback(response)\n         self.should_fail()\n+\n+    def test_errback(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+\n+        try:\n+            raise HttpError(response, 'Ignoring non-200 response')\n+        except HttpError:\n+            failure_mock = failure.Failure()\n+\n+        request = self.conman.from_method(spider.returns_request, self.results)\n+        request.errback(failure_mock)\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertTrue(self.results.errors)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb7d4cbce379c4e3fac2ca89b6e0772d0c690935", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 336 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,7 +41,11 @@ class ContractsManager(object):\n         requests = []\n         for method in self.tested_methods_from_spidercls(type(spider)):\n             bound_method = spider.__getattribute__(method)\n+            try:\n                 requests.append(self.from_method(bound_method, results))\n+            except:\n+                case = _create_testcase(bound_method, 'contract')\n+                results.addError(case, sys.exc_info())\n \n         return requests\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ebbde57eca310d3c5c0f530a2e668572dde4d952", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 55 | Churn Cumulative: 125 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ from twisted.trial import unittest\n from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.item import Item, Field\n-from scrapy.contracts import ContractsManager\n+from scrapy.contracts import ContractsManager, Contract\n from scrapy.contracts.default import (\n     UrlContract,\n     ReturnsContract,\n@@ -22,6 +22,21 @@ class ResponseMock(object):\n     url = 'http://scrapy.org'\n \n \n+class CustomSuccessContract(Contract):\n+    name = 'custom_success_contract'\n+\n+    def adjust_request_args(self, args):\n+        args['url'] = 'http://scrapy.org'\n+        return args\n+\n+\n+class CustomFailContract(Contract):\n+    name = 'custom_fail_contract'\n+\n+    def adjust_request_args(self, args):\n+        raise TypeError('Error in adjust_request_args')\n+\n+\n class TestSpider(Spider):\n     name = 'demo_spider'\n \n@@ -99,8 +114,34 @@ class TestSpider(Spider):\n         pass\n \n \n+class CustomContractSuccessSpider(Spider):\n+    name = 'custom_contract_success_spider'\n+\n+    def parse(self, response):\n+        \"\"\"\n+        @custom_success_contract\n+        \"\"\"\n+        pass\n+\n+\n+class CustomContractFailSpider(Spider):\n+    name = 'custom_contract_fail_spider'\n+\n+    def parse(self, response):\n+        \"\"\"\n+        @custom_fail_contract\n+        \"\"\"\n+        pass\n+\n+\n class ContractsManagerTest(unittest.TestCase):\n-    contracts = [UrlContract, ReturnsContract, ScrapesContract]\n+    contracts = [\n+        UrlContract,\n+        ReturnsContract,\n+        ScrapesContract,\n+        CustomSuccessContract,\n+        CustomFailContract\n+    ]\n \n     def setUp(self):\n         self.conman = ContractsManager(self.contracts)\n@@ -114,6 +155,9 @@ class ContractsManagerTest(unittest.TestCase):\n         self.assertTrue(self.results.failures)\n         self.assertFalse(self.results.errors)\n \n+    def should_error(self):\n+        self.assertTrue(self.results.errors)\n+\n     def test_contracts(self):\n         spider = TestSpider()\n \n@@ -185,3 +229,10 @@ class ContractsManagerTest(unittest.TestCase):\n                 self.results)\n         request.callback(response)\n         self.should_fail()\n+\n+    def test_custom_contracts(self):\n+        self.conman.from_spider(CustomContractSuccessSpider(), self.results)\n+        self.should_succeed()\n+\n+        self.conman.from_spider(CustomContractFailSpider(), self.results)\n+        self.should_error()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#76220e8733b1462ecaa5db1cdd4a58ad4e93ceb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 13 | Churn Cumulative: 474 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import sys\n import re\n from functools import wraps\n+from inspect import getmembers\n from unittest import TestCase\n \n from scrapy.http import Request\n@@ -17,7 +18,7 @@ class ContractsManager(object):\n \n     def tested_methods_from_spidercls(self, spidercls):\n         methods = []\n-        for key, value in vars(spidercls).items():\n+        for key, value in getmembers(spidercls):\n             if (callable(value) and value.__doc__ and\n                     re.search(r'^\\s*@', value.__doc__, re.MULTILINE)):\n                 methods.append(key)\n\n@@ -101,6 +101,10 @@ class TestSpider(Spider):\n         pass\n \n \n+class InheritsTestSpider(TestSpider):\n+    name = 'inherits_demo_spider'\n+\n+\n class ContractsManagerTest(unittest.TestCase):\n     contracts = [UrlContract, ReturnsContract, ScrapesContract]\n \n@@ -202,3 +206,9 @@ class ContractsManagerTest(unittest.TestCase):\n \n         self.assertFalse(self.results.failures)\n         self.assertTrue(self.results.errors)\n+\n+    def test_inherited_contracts(self):\n+        spider = InheritsTestSpider()\n+\n+        requests = self.conman.from_spider(spider, self.results)\n+        self.assertTrue(requests)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8fc017d345af1e6fa53334fc9df92ebd8f9dc32e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 344 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -50,7 +50,12 @@ class ContractsManager(object):\n         if contracts:\n             # calculate request args\n             args, kwargs = get_spec(Request.__init__)\n+\n+            # Don't filter requests to allow\n+            # testing different callbacks on the same URL.\n+            kwargs['dont_filter'] = True\n             kwargs['callback'] = method\n+\n             for contract in contracts:\n                 kwargs = contract.adjust_request_args(kwargs)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b4b1e4834376e8565d727a1a5087cae31450f931", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 31 | Churn Cumulative: 510 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -3,6 +3,7 @@ import re\n from functools import wraps\n from unittest import TestCase\n \n+from scrapy import FormRequest\n from scrapy.http import Request\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.python import get_spec\n@@ -57,6 +58,10 @@ class ContractsManager(object):\n             # create and prepare request\n             args.remove('self')\n             if set(args).issubset(set(kwargs)):\n+                if 'formdata' in kwargs:\n+                    kwargs['method'] = 'POST'\n+                    request = FormRequest(**kwargs)\n+                else:\n                     request = Request(**kwargs)\n \n                 # execute pre and post hooks in order\n\n@@ -3,11 +3,12 @@ from unittest import TextTestResult\n from twisted.python import failure\n from twisted.trial import unittest\n \n+from scrapy import FormRequest\n from scrapy.spidermiddlewares.httperror import HttpError\n from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.item import Item, Field\n-from scrapy.contracts import ContractsManager\n+from scrapy.contracts import ContractsManager, Contract\n from scrapy.contracts.default import (\n     UrlContract,\n     ReturnsContract,\n@@ -24,6 +25,14 @@ class ResponseMock(object):\n     url = 'http://scrapy.org'\n \n \n+class CustomFormContract(Contract):\n+    name = 'custom_form'\n+\n+    def adjust_request_args(self, args):\n+        args['formdata'] = {'name': 'scrapy'}\n+        return args\n+\n+\n class TestSpider(Spider):\n     name = 'demo_spider'\n \n@@ -100,9 +109,16 @@ class TestSpider(Spider):\n         \"\"\"\n         pass\n \n+    def custom_form(self, response):\n+        \"\"\"\n+        @url http://scrapy.org\n+        @custom_form\n+        \"\"\"\n+        pass\n+\n \n class ContractsManagerTest(unittest.TestCase):\n-    contracts = [UrlContract, ReturnsContract, ScrapesContract]\n+    contracts = [UrlContract, ReturnsContract, ScrapesContract, CustomFormContract]\n \n     def setUp(self):\n         self.conman = ContractsManager(self.contracts)\n@@ -202,3 +218,9 @@ class ContractsManagerTest(unittest.TestCase):\n \n         self.assertFalse(self.results.failures)\n         self.assertTrue(self.results.errors)\n+\n+    def test_form_contract(self):\n+        spider = TestSpider()\n+        request = self.conman.from_method(spider.custom_form, self.results)\n+        self.assertEqual(request.method, 'POST')\n+        self.assertIsInstance(request, FormRequest)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#152fde70b12f3c6e0e73230370f79f4b4a7ea906", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1080 | Contributors (this commit): 15 | Commits (past 90d): 50 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,8 @@\n import os\n import six\n-import contextlib\n import shutil\n+import tempfile\n+import contextlib\n try:\n     from unittest import mock\n except ImportError:\n@@ -913,7 +914,8 @@ class BaseFTPTestCase(unittest.TestCase):\n         return self._add_test_callbacks(d, _test)\n \n     def test_ftp_local_filename(self):\n-        local_fname = b\"/tmp/file.txt\"\n+        f, local_fname = tempfile.mkstemp()\n+        os.close(f)\n         meta = {\"ftp_local_filename\": local_fname}\n         meta.update(self.req_meta)\n         request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n@@ -922,7 +924,8 @@ class BaseFTPTestCase(unittest.TestCase):\n \n         def _test(r):\n             self.assertEqual(r.body, local_fname)\n-            self.assertEqual(r.headers, {b'Local Filename': [b'/tmp/file.txt'], b'Size': [b'17']})\n+            self.assertEqual(r.headers, {b'Local Filename': [local_fname],\n+                                         b'Size': [b'17']})\n             self.assertTrue(os.path.exists(local_fname))\n             with open(local_fname, \"rb\") as f:\n                 self.assertEqual(f.read(), b\"I have the power!\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#57a1d66c61423c85703a4008d351e9d566580810", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 615 | Contributors (this commit): 11 | Commits (past 90d): 19 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,18 +3,17 @@ import os\n import sys\n import subprocess\n import tempfile\n-from time import sleep\n from os.path import exists, join, abspath\n from shutil import rmtree, copytree\n from tempfile import mkdtemp\n from contextlib import contextmanager\n+from threading import Timer\n \n from twisted.trial import unittest\n from twisted.internet import defer\n \n import scrapy\n from scrapy.utils.python import to_native_str\n-from scrapy.utils.python import retry_on_eintr\n from scrapy.utils.test import get_testenv\n from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n@@ -46,16 +45,18 @@ class ProjectTest(unittest.TestCase):\n                              stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                              **popen_kwargs)\n \n-        waited = 0\n-        interval = 0.2\n-        while p.poll() is None:\n-            sleep(interval)\n-            waited += interval\n-            if waited > 15:\n+        def kill_proc():\n             p.kill()\n             assert False, 'Command took too much time to complete'\n \n-        return p\n+        timer = Timer(15, kill_proc)\n+        try:\n+            timer.start()\n+            stdout, stderr = p.communicate()\n+        finally:\n+            timer.cancel()\n+\n+        return to_native_str(stdout), to_native_str(stderr)\n \n \n class StartprojectTest(ProjectTest):\n@@ -111,8 +112,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         assert exists(join(self.tmpl_proj, 'root_template'))\n \n         args = ['--set', 'TEMPLATES_DIR=%s' % self.tmpl]\n-        p = self.proc('startproject', self.project_name, *args)\n-        out = to_native_str(retry_on_eintr(p.stdout.read))\n+        out, err = self.proc('startproject', self.project_name, *args)\n         self.assertIn(\"New Scrapy project %r, using template directory\" % self.project_name, out)\n         self.assertIn(self.tmpl_proj, out)\n         assert exists(join(self.proj_path, 'root_template'))\n@@ -140,12 +140,10 @@ class GenspiderCommandTest(CommandTest):\n     def test_template(self, tplname='crawl'):\n         args = ['--template=%s' % tplname] if tplname else []\n         spname = 'test_spider'\n-        p = self.proc('genspider', spname, 'test.com', *args)\n-        out = to_native_str(retry_on_eintr(p.stdout.read))\n+        out, err = self.proc('genspider', spname, 'test.com', *args)\n         self.assertIn(\"Created spider %r using template %r in module\" % (spname, tplname), out)\n         self.assertTrue(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n-        p = self.proc('genspider', spname, 'test.com', *args)\n-        out = to_native_str(retry_on_eintr(p.stdout.read))\n+        out, err = self.proc('genspider', spname, 'test.com', *args)\n         self.assertIn(\"Spider %r already exists in module\" % spname, out)\n \n     def test_template_basic(self):\n@@ -212,8 +210,8 @@ class MySpider(scrapy.Spider):\n             return self.proc('runspider', fname, *args)\n \n     def get_log(self, code, name='myspider.py', args=()):\n-        p = self.runspider(code, name=name, args=args)\n-        return to_native_str(p.stderr.read())\n+        stdout, stderr = self.runspider(code, name=name, args=args)\n+        return stderr\n \n     def test_runspider(self):\n         log = self.get_log(self.debug_log_spider)\n@@ -279,14 +277,17 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"No spider found in file\", log)\n \n     def test_runspider_file_not_found(self):\n-        p = self.proc('runspider', 'some_non_existent_file')\n-        log = to_native_str(p.stderr.read())\n+        _, log = self.proc('runspider', 'some_non_existent_file')\n         self.assertIn(\"File not found: some_non_existent_file\", log)\n \n     def test_runspider_unable_to_load(self):\n         log = self.get_log('', name='myspider.txt')\n         self.assertIn('Unable to load', log)\n \n+\n+class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n+    command = 'parse'\n+\n     def test_start_requests_errors(self):\n         log = self.get_log(\"\"\"\n import scrapy\n@@ -304,8 +305,7 @@ class BadSpider(scrapy.Spider):\n class BenchCommandTest(CommandTest):\n \n     def test_run(self):\n-        p = self.proc('bench', '-s', 'LOGSTATS_INTERVAL=0.001',\n+        _, log = self.proc('bench', '-s', 'LOGSTATS_INTERVAL=0.001',\n                            '-s', 'CLOSESPIDER_TIMEOUT=0.01')\n-        log = to_native_str(p.stderr.read())\n         self.assertIn('INFO: Crawled', log)\n         self.assertNotIn('Unhandled Error', log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ed8255bde04d7e07747792022858f161c2296096", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 633 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -56,7 +56,7 @@ class ProjectTest(unittest.TestCase):\n         finally:\n             timer.cancel()\n \n-        return to_native_str(stdout), to_native_str(stderr)\n+        return p, to_native_str(stdout), to_native_str(stderr)\n \n \n class StartprojectTest(ProjectTest):\n@@ -112,7 +112,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         assert exists(join(self.tmpl_proj, 'root_template'))\n \n         args = ['--set', 'TEMPLATES_DIR=%s' % self.tmpl]\n-        out, err = self.proc('startproject', self.project_name, *args)\n+        p, out, err = self.proc('startproject', self.project_name, *args)\n         self.assertIn(\"New Scrapy project %r, using template directory\" % self.project_name, out)\n         self.assertIn(self.tmpl_proj, out)\n         assert exists(join(self.proj_path, 'root_template'))\n@@ -140,10 +140,10 @@ class GenspiderCommandTest(CommandTest):\n     def test_template(self, tplname='crawl'):\n         args = ['--template=%s' % tplname] if tplname else []\n         spname = 'test_spider'\n-        out, err = self.proc('genspider', spname, 'test.com', *args)\n+        p, out, err = self.proc('genspider', spname, 'test.com', *args)\n         self.assertIn(\"Created spider %r using template %r in module\" % (spname, tplname), out)\n         self.assertTrue(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n-        out, err = self.proc('genspider', spname, 'test.com', *args)\n+        p, out, err = self.proc('genspider', spname, 'test.com', *args)\n         self.assertIn(\"Spider %r already exists in module\" % spname, out)\n \n     def test_template_basic(self):\n@@ -210,7 +210,7 @@ class MySpider(scrapy.Spider):\n             return self.proc('runspider', fname, *args)\n \n     def get_log(self, code, name='myspider.py', args=()):\n-        stdout, stderr = self.runspider(code, name=name, args=args)\n+        p, stdout, stderr = self.runspider(code, name=name, args=args)\n         return stderr\n \n     def test_runspider(self):\n@@ -221,12 +221,12 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"INFO: Spider closed (finished)\", log)\n \n     def test_run_fail_spider(self):\n-        proc = self.runspider(\"import scrapy\\n\" + inspect.getsource(ExceptionSpider))\n+        proc, _, _ = self.runspider(\"import scrapy\\n\" + inspect.getsource(ExceptionSpider))\n         ret = proc.returncode\n         self.assertNotEqual(ret, 0)\n \n     def test_run_good_spider(self):\n-        proc = self.runspider(\"import scrapy\\n\" + inspect.getsource(NoRequestsSpider))\n+        proc, _, _ = self.runspider(\"import scrapy\\n\" + inspect.getsource(NoRequestsSpider))\n         ret = proc.returncode\n         self.assertEqual(ret, 0)\n \n@@ -277,7 +277,7 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"No spider found in file\", log)\n \n     def test_runspider_file_not_found(self):\n-        _, log = self.proc('runspider', 'some_non_existent_file')\n+        _, _, log = self.proc('runspider', 'some_non_existent_file')\n         self.assertIn(\"File not found: some_non_existent_file\", log)\n \n     def test_runspider_unable_to_load(self):\n@@ -305,7 +305,7 @@ class BadSpider(scrapy.Spider):\n class BenchCommandTest(CommandTest):\n \n     def test_run(self):\n-        _, log = self.proc('bench', '-s', 'LOGSTATS_INTERVAL=0.001',\n+        _, _, log = self.proc('bench', '-s', 'LOGSTATS_INTERVAL=0.001',\n                            '-s', 'CLOSESPIDER_TIMEOUT=0.01')\n         self.assertIn('INFO: Crawled', log)\n         self.assertNotIn('Unhandled Error', log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
