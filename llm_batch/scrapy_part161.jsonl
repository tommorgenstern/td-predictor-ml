{"custom_id": "scrapy#2519b3031e25970501fe8fa42373a0e5431c563d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 390 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,8 @@\n-import sys, time, random, urllib\n+import sys, time, random, urllib, os\n from subprocess import Popen, PIPE\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n-from twisted.internet import reactor, defer\n+from twisted.internet import reactor, defer, ssl\n from scrapy import twisted_version\n \n \n@@ -180,10 +180,17 @@ class MockServer():\n if __name__ == \"__main__\":\n     root = Root()\n     factory = Site(root)\n-    port = reactor.listenTCP(8998, factory)\n+    httpPort = reactor.listenTCP(8998, factory)\n+    contextFactory = ssl.DefaultOpenSSLContextFactory(\n+         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),\n+         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),\n+         )\n+    httpsPort = reactor.listenSSL(8999, factory, contextFactory)\n \n     def print_listening():\n-        h = port.getHost()\n-        print \"Mock server running at http://%s:%d\" % (h.host, h.port)\n+        httpHost = httpPort.getHost()\n+        httpsHost = httpsPort.getHost()\n+        print \"Mock server running at http://%s:%d and https://%s:%d\" % (\n+            httpHost.host, httpHost.port, httpsHost.host, httpsHost.port)\n     reactor.callWhenRunning(print_listening)\n     reactor.run()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#479b4942bd88e72485dafda853dedfd494c57414", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 657 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import signal\n+from itertools import chain\n \n from twisted.internet import reactor, defer\n \n@@ -21,8 +22,7 @@ class Crawler(object):\n \n         spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n         self.spiders = spman_cls.from_crawler(self)\n-\n-        self.scheduled = {}\n+        self._scheduled = {}\n \n     def install(self):\n         import scrapy.project\n@@ -46,20 +46,17 @@ class Crawler(object):\n \n     def crawl(self, spider, requests=None):\n         spider.set_crawler(self)\n-\n         if self.configured and self.engine.running:\n-            assert not self.scheduled\n-            return self.schedule(spider, requests)\n+            assert not self._scheduled\n+            return self._schedule(spider, requests)\n+        elif requests is None:\n+            self._scheduled[spider] = None\n         else:\n-            self.scheduled.setdefault(spider, []).append(requests)\n-\n-    def schedule(self, spider, batches=[]):\n-        requests = []\n-        for batch in batches:\n-            if batch is None:\n-                batch = spider.start_requests()\n-            requests.extend(batch)\n+            self._scheduled.setdefault(spider, []).append(requests)\n \n+    def _schedule(self, spider, batches=()):\n+        requests = chain.from_iterable(batches) \\\n+            if batches else spider.start_requests()\n         return self.engine.open_spider(spider, requests)\n \n     def _spider_closed(self, spider=None):\n@@ -70,8 +67,8 @@ class Crawler(object):\n     def start(self):\n         yield defer.maybeDeferred(self.configure)\n \n-        for spider, batches in self.scheduled.iteritems():\n-            yield self.schedule(spider, batches)\n+        for spider, batches in self._scheduled.iteritems():\n+            yield self._schedule(spider, batches)\n \n         yield defer.maybeDeferred(self.engine.start)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d806184bb6c13b0d0b121a8a5a29260508d4cd43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 16 | Churn Cumulative: 901 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -77,6 +77,10 @@ class XPathSelector(object_ref):\n         for el in self._root.iter('*'):\n             if el.tag.startswith('{'):\n                 el.tag = el.tag.split('}', 1)[1]\n+            # loop on element attributes also\n+            for an in el.attrib.keys():\n+                if an.startswith('{'):\n+                    el.attrib[an.split('}', 1)[1]] = el.attrib.pop(an)\n \n     def __nonzero__(self):\n         return bool(self.extract())\n\n@@ -27,6 +27,18 @@ class LxmlXPathSelectorTestCase(test_selector.XPathSelectorTestCase):\n         xxs.remove_namespaces()\n         self.assertEqual(len(xxs.select(\"//link\")), 2)\n \n+    def test_remove_attributes_namespaces(self):\n+        xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<feed xmlns:atom=\"http://www.w3.org/2005/Atom\" xml:lang=\"en-US\" xmlns:media=\"http://search.yahoo.com/mrss/\">\n+  <link atom:type=\"text/html\">\n+  <link atom:type=\"application/atom+xml\">\n+</feed>\n+\"\"\"\n+        xxs = XmlXPathSelector(XmlResponse(\"http://example.com/feed.atom\", body=xml))\n+        self.assertEqual(len(xxs.select(\"//link/@type\")), 0)\n+        xxs.remove_namespaces()\n+        self.assertEqual(len(xxs.select(\"//link/@type\")), 2)\n+\n class Libxml2DocumentTest(unittest.TestCase):\n \n     def test_caching(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#10646f17689908201dc32680e5ba7df3b36f5a59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 21 | Churn Cumulative: 157 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,6 +11,7 @@ from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n \n+\n class RobotsTxtMiddleware(object):\n     DOWNLOAD_PRIORITY = 1000\n \n@@ -19,18 +20,16 @@ class RobotsTxtMiddleware(object):\n             raise NotConfigured\n \n         self.crawler = crawler\n+        self._useragent = crawler.settings.get('USER_AGENT')\n         self._parsers = {}\n-        self._spider_netlocs = {}\n-        self._useragents = {}\n-        crawler.signals.connect(self.spider_opened, signals.spider_opened)\n-        crawler.signals.connect(self.spider_closed, signals.spider_closed)\n+        self._spider_netlocs = set()\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler)\n \n     def process_request(self, request, spider):\n-        useragent = self._useragents[spider]\n+        useragent = self._useragent\n         rp = self.robot_parser(request, spider)\n         if rp and not rp.can_fetch(useragent, request.url):\n             log.msg(format=\"Forbidden by robots.txt: %(request)s\",\n@@ -46,20 +45,10 @@ class RobotsTxtMiddleware(object):\n             robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY)\n             dfd = self.crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n-            self._spider_netlocs[spider].add(netloc)\n+            self._spider_netlocs.add(netloc)\n         return self._parsers[netloc]\n \n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)\n         rp.parse(response.body.splitlines())\n         self._parsers[urlparse_cached(response).netloc] = rp\n-\n-    def spider_opened(self, spider):\n-        self._spider_netlocs[spider] = set()\n-        self._useragents[spider] = spider.settings['USER_AGENT']\n-\n-    def spider_closed(self, spider):\n-        for netloc in self._spider_netlocs[spider]:\n-            del self._parsers[netloc]\n-        del self._spider_netlocs[spider]\n-        del self._useragents[spider]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f56f547738d2ab6afdb63b1e1f293728647b18c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 83 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,23 +5,27 @@ See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n from w3lib.http import basic_auth_header\n-from scrapy.utils.python import WeakKeyCache\n+\n+from scrapy import signals\n \n \n class HttpAuthMiddleware(object):\n     \"\"\"Set Basic HTTP Authorization header\n     (http_user and http_pass spider class attributes)\"\"\"\n \n-    def __init__(self):\n-        self._cache = WeakKeyCache(self._authorization)\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        o = cls()\n+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n+        return o\n \n-    def _authorization(self, spider):\n+    def spider_opened(self, spider):\n         usr = getattr(spider, 'http_user', '')\n         pwd = getattr(spider, 'http_pass', '')\n         if usr or pwd:\n-            return basic_auth_header(usr, pwd)\n+            self.auth = basic_auth_header(usr, pwd)\n \n     def process_request(self, request, spider):\n-        auth = self._cache[spider]\n+        auth = getattr(self, 'auth', None)\n         if auth and 'Authorization' not in request.headers:\n             request.headers['Authorization'] = auth\n\n@@ -13,6 +13,7 @@ class HttpAuthMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n         self.mw = HttpAuthMiddleware()\n         self.spider = TestSpider('foo')\n+        self.mw.spider_opened(self.spider)\n \n     def tearDown(self):\n         del self.mw\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fbb0ad6419201fef5555cce02d7fc5ad6682c13c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 10 | Methods Changed: 9 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 23 | Churn Cumulative: 119 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,25 +1,23 @@\n \"\"\"Set User-Agent header per spider or use a default value from settings\"\"\"\n \n-from scrapy.utils.python import WeakKeyCache\n+from scrapy import signals\n \n \n class UserAgentMiddleware(object):\n     \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n \n     def __init__(self, user_agent='Scrapy'):\n-        self.cache = WeakKeyCache(self._user_agent)\n         self.user_agent = user_agent\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler.settings['USER_AGENT'])\n+        o = cls(crawler.settings['USER_AGENT'])\n+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n+        return o\n \n-    def _user_agent(self, spider):\n-        if hasattr(spider, 'user_agent'):\n-            return spider.user_agent\n-        return self.user_agent\n+    def spider_opened(self, spider):\n+        self.user_agent = getattr(spider, 'user_agent', self.user_agent)\n \n     def process_request(self, request, spider):\n-        ua = self.cache[spider]\n-        if ua:\n-            request.headers.setdefault('User-Agent', ua)\n+        if self.user_agent:\n+            request.headers.setdefault('User-Agent', self.user_agent)\n\n@@ -24,6 +24,7 @@ class UserAgentMiddlewareTest(TestCase):\n         # settings UESR_AGENT to None should remove the user agent\n         spider, mw = self.get_spider_and_mw('default_useragent')\n         spider.user_agent = None\n+        mw.spider_opened(spider)\n         req = Request('http://scrapytest.org/')\n         assert mw.process_request(req, spider) is None\n         assert req.headers.get('User-Agent') is None\n@@ -31,6 +32,7 @@ class UserAgentMiddlewareTest(TestCase):\n     def test_spider_agent(self):\n         spider, mw = self.get_spider_and_mw('default_useragent')\n         spider.user_agent = 'spider_useragent'\n+        mw.spider_opened(spider)\n         req = Request('http://scrapytest.org/')\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.headers['User-Agent'], 'spider_useragent')\n@@ -38,6 +40,7 @@ class UserAgentMiddlewareTest(TestCase):\n     def test_header_agent(self):\n         spider, mw = self.get_spider_and_mw('default_useragent')\n         spider.user_agent = 'spider_useragent'\n+        mw.spider_opened(spider)\n         req = Request('http://scrapytest.org/', headers={'User-Agent': 'header_useragent'})\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.headers['User-Agent'], 'header_useragent')\n@@ -45,7 +48,7 @@ class UserAgentMiddlewareTest(TestCase):\n     def test_no_agent(self):\n         spider, mw = self.get_spider_and_mw(None)\n         spider.user_agent = None\n+        mw.spider_opened(spider)\n         req = Request('http://scrapytest.org/')\n         assert mw.process_request(req, spider) is None\n         assert 'User-Agent' not in req.headers\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d09b6e1f2f559d149ef75eb8a50bce41084b4dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 22 | Churn Cumulative: 137 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,25 +3,24 @@ Download timeout middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n-from scrapy.utils.python import WeakKeyCache\n+\n+from scrapy import signals\n \n \n class DownloadTimeoutMiddleware(object):\n \n     def __init__(self, timeout=180):\n-        self._cache = WeakKeyCache(self._download_timeout)\n         self._timeout = timeout\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler.settings['DOWNLOAD_TIMEOUT'])\n+        o = cls(crawler.settings['DOWNLOAD_TIMEOUT'])\n+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n+        return o\n \n-    def _download_timeout(self, spider):\n-        if hasattr(spider, 'download_timeout'):\n-            return spider.download_timeout\n-        return self._timeout\n+    def spider_opened(self, spider):\n+        self._timeout = getattr(spider, 'download_timeout', self._timeout)\n \n     def process_request(self, request, spider):\n-        timeout = self._cache[spider]\n-        if timeout:\n-            request.meta.setdefault('download_timeout', timeout)\n+        if self._timeout:\n+            request.meta.setdefault('download_timeout', self._timeout)\n\n@@ -17,18 +17,21 @@ class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n \n     def test_default_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n+        mw.spider_opened(spider)\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.meta.get('download_timeout'), 180)\n \n     def test_spider_has_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n         spider.download_timeout = 2\n+        mw.spider_opened(spider)\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.meta.get('download_timeout'), 2)\n \n     def test_request_has_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n         spider.download_timeout = 2\n+        mw.spider_opened(spider)\n         req.meta['download_timeout'] = 1\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.meta.get('download_timeout'), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#332bf3b6b7bd238df5e8be601ee0686231a6776c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 105 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,22 +3,17 @@ DefaultHeaders downloader middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n-from scrapy.utils.python import WeakKeyCache\n \n \n class DefaultHeadersMiddleware(object):\n \n-    def __init__(self, settings):\n-        self._headers = WeakKeyCache(self._default_headers)\n-        self._settings = settings\n+    def __init__(self, headers):\n+        self._headers = headers\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler.settings)\n-\n-    def _default_headers(self, spider):\n-        return self._settings.get('DEFAULT_REQUEST_HEADERS').items()\n+        return cls(crawler.settings.get('DEFAULT_REQUEST_HEADERS').items())\n \n     def process_request(self, request, spider):\n-        for k, v in self._headers[spider]:\n+        for k, v in self._headers:\n             request.headers.setdefault(k, v)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a84e5f806c7f909b2b497eea90c22f807baeff0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 18 | Methods Changed: 15 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 49 | Churn Cumulative: 2287 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,7 +55,8 @@ class ExecutionEngine(object):\n         self.settings = crawler.settings\n         self.signals = crawler.signals\n         self.logformatter = crawler.logformatter\n-        self.slots = {}\n+        self.slot = None\n+        self.spider = None\n         self.running = False\n         self.paused = False\n         self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n@@ -93,9 +94,8 @@ class ExecutionEngine(object):\n         self.paused = False\n \n     def _next_request(self, spider):\n-        try:\n-            slot = self.slots[spider]\n-        except KeyError:\n+        slot = self.slot\n+        if not slot:\n             return\n \n         if self.paused:\n@@ -121,14 +121,14 @@ class ExecutionEngine(object):\n             self._spider_idle(spider)\n \n     def _needs_backout(self, spider):\n-        slot = self.slots[spider]\n+        slot = self.slot\n         return not self.running \\\n             or slot.closing \\\n             or self.downloader.needs_backout() \\\n             or self.scraper.slot.needs_backout()\n \n     def _next_request_from_scheduler(self, spider):\n-        slot = self.slots[spider]\n+        slot = self.slot\n         request = slot.scheduler.next_request()\n         if not request:\n             return\n@@ -154,32 +154,32 @@ class ExecutionEngine(object):\n \n     def spider_is_idle(self, spider):\n         scraper_idle = self.scraper.slot.is_idle()\n-        pending = self.slots[spider].scheduler.has_pending_requests()\n+        pending = self.slot.scheduler.has_pending_requests()\n         downloading = bool(self.downloader.active)\n         idle = scraper_idle and not (pending or downloading)\n         return idle\n \n     @property\n     def open_spiders(self):\n-        return self.slots.keys()\n+        return [self.spider] if self.spider else []\n \n     def has_capacity(self):\n         \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n-        return len(self.slots) < self._concurrent_spiders\n+        return not bool(self.slot)\n \n     def crawl(self, request, spider):\n         assert spider in self.open_spiders, \\\n             \"Spider %r not opened when crawling: %s\" % (spider.name, request)\n         self.schedule(request, spider)\n-        self.slots[spider].nextcall.schedule()\n+        self.slot.nextcall.schedule()\n \n     def schedule(self, request, spider):\n         self.signals.send_catch_log(signal=signals.request_scheduled,\n                 request=request, spider=spider)\n-        return self.slots[spider].scheduler.enqueue_request(request)\n+        return self.slot.scheduler.enqueue_request(request)\n \n     def download(self, request, spider):\n-        slot = self.slots[spider]\n+        slot = self.slot\n         slot.add_request(request)\n         d = self._download(request, spider)\n         d.addBoth(self._downloaded, slot, request, spider)\n@@ -191,7 +191,7 @@ class ExecutionEngine(object):\n                 if isinstance(response, Request) else response\n \n     def _download(self, request, spider):\n-        slot = self.slots[spider]\n+        slot = self.slot\n         slot.add_request(request)\n         def _on_success(response):\n             assert isinstance(response, (Response, Request))\n@@ -214,14 +214,15 @@ class ExecutionEngine(object):\n \n     @defer.inlineCallbacks\n     def open_spider(self, spider, start_requests=(), close_if_idle=True):\n-        assert self.has_capacity(), \"No free spider slots when opening %r\" % \\\n+        assert self.has_capacity(), \"No free spider slot when opening %r\" % \\\n             spider.name\n         log.msg(\"Spider opened\", spider=spider)\n         nextcall = CallLaterOnce(self._next_request, spider)\n         scheduler = self.scheduler_cls.from_crawler(self.crawler)\n         start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n         slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n-        self.slots[spider] = slot\n+        self.slot = slot\n+        self.spider = spider\n         yield scheduler.open(spider)\n         yield self.scraper.open_spider(spider)\n         self.crawler.stats.open_spider(spider)\n@@ -240,7 +241,7 @@ class ExecutionEngine(object):\n             spider=spider, dont_log=DontCloseSpider)\n         if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n                 for _, x in res):\n-            self.slots[spider].nextcall.schedule(5)\n+            self.slot.nextcall.schedule(5)\n             return\n \n         if self.spider_is_idle(spider):\n@@ -249,7 +250,7 @@ class ExecutionEngine(object):\n     def close_spider(self, spider, reason='cancelled'):\n         \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n \n-        slot = self.slots[spider]\n+        slot = self.slot\n         if slot.closing:\n             return slot.closing\n         log.msg(format=\"Closing spider (%(reason)s)\", reason=reason, spider=spider)\n@@ -276,7 +277,10 @@ class ExecutionEngine(object):\n \n         dfd.addBoth(lambda _: log.msg(format=\"Spider closed (%(reason)s)\", reason=reason, spider=spider))\n \n-        dfd.addBoth(lambda _: self.slots.pop(spider))\n+        dfd.addBoth(lambda _: setattr(self, 'slot', None))\n+        dfd.addErrback(log.err, spider=spider)\n+\n+        dfd.addBoth(lambda _: setattr(self, 'spider', None))\n         dfd.addErrback(log.err, spider=spider)\n \n         dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n@@ -7,16 +7,15 @@ def get_engine_status(engine):\n     global_tests = [\n         \"time()-engine.start_time\",\n         \"engine.has_capacity()\",\n-        \"len(engine.downloader.slots)\",\n         \"len(engine.downloader.active)\",\n         \"engine.scraper.is_idle()\",\n     ]\n     spider_tests = [\n         \"engine.spider_is_idle(spider)\",\n-        \"engine.slots[spider].closing\",\n-        \"len(engine.slots[spider].inprogress)\",\n-        \"len(engine.slots[spider].scheduler.dqs or [])\",\n-        \"len(engine.slots[spider].scheduler.mqs)\",\n+        \"engine.slot.closing\",\n+        \"len(engine.slot.inprogress)\",\n+        \"len(engine.slot.scheduler.dqs or [])\",\n+        \"len(engine.slot.scheduler.mqs)\",\n         \"len(engine.scraper.slot.queue)\",\n         \"len(engine.scraper.slot.active)\",\n         \"engine.scraper.slot.active_size\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b1288ba54fc957598f8658154a154dc07b28f54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 57 | Churn Cumulative: 760 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ See documentation in docs/topics/shell.rst\n \"\"\"\n import signal\n \n-from twisted.internet import reactor, threads\n+from twisted.internet import reactor, threads, defer\n from twisted.python import threadable\n from w3lib.url import any_to_uri\n \n@@ -14,7 +14,6 @@ from scrapy.spider import BaseSpider\n from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n from scrapy.utils.spider import create_spider_for_request\n from scrapy.utils.misc import load_object\n-from scrapy.utils.request import request_deferred\n from scrapy.utils.response import open_in_browser\n from scrapy.utils.console import start_python_console\n from scrapy.settings import Settings\n@@ -55,7 +54,7 @@ class Shell(object):\n \n     def _schedule(self, request, spider):\n         spider = self._open_spider(request, spider)\n-        d = request_deferred(request)\n+        d = _request_deferred(request)\n         d.addCallback(lambda x: (x, spider))\n         self.crawler.engine.crawl(request, spider)\n         return d\n@@ -63,9 +62,12 @@ class Shell(object):\n     def _open_spider(self, request, spider):\n         if self.spider:\n             return self.spider\n+\n         if spider is None:\n-            spider = create_spider_for_request(self.crawler.spiders, request,\n-                BaseSpider('default'), log_multiple=True)\n+            spider = create_spider_for_request(self.crawler.spiders,\n+                                               request,\n+                                               BaseSpider('default'),\n+                                               log_multiple=True)\n         spider.set_crawler(self.crawler)\n         self.crawler.engine.open_spider(spider, close_if_idle=False)\n         self.spider = spider\n@@ -127,3 +129,30 @@ def inspect_response(response, spider=None):\n     \"\"\"Open a shell to inspect the given response\"\"\"\n     from scrapy.project import crawler\n     Shell(crawler).start(response=response, spider=spider)\n+\n+\n+def _request_deferred(request):\n+    \"\"\"Wrap a request inside a Deferred.\n+\n+    This function is harmful, do not use it until you know what you are doing.\n+\n+    This returns a Deferred whose first pair of callbacks are the request\n+    callback and errback. The Deferred also triggers when the request\n+    callback/errback is executed (ie. when the request is downloaded)\n+\n+    WARNING: Do not call request.replace() until after the deferred is called.\n+    \"\"\"\n+    request_callback = request.callback\n+    request_errback = request.errback\n+    def _restore_callbacks(result):\n+        request.callback = request_callback\n+        request.errback = request_errback\n+        return result\n+\n+    d = defer.Deferred()\n+    d.addBoth(_restore_callbacks)\n+    if request.callback:\n+        d.addCallbacks(request.callback, request.errback)\n+\n+    request.callback, request.errback = d.callback, d.errback\n+    return d\n\n@@ -45,3 +45,9 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         _, out, _ = yield self.execute([self.url('/redirect'), '-c', 'response.url'])\n         assert out.strip().endswith('/redirected')\n \n+    @defer.inlineCallbacks\n+    def test_request_replace(self):\n+        url = self.url('/text')\n+        code = \"fetch('{0}') or fetch(response.request.replace(method='POST'))\"\n+        errcode, out, _ = yield self.execute(['-c', code.format(url)])\n+        self.assertEqual(errcode, 0, out)\n\n@@ -81,15 +81,3 @@ def request_httprepr(request):\n     s += request.body\n     return s\n \n-def request_deferred(request):\n-    \"\"\"Wrap a request inside a Deferred.\n-\n-    This returns a Deferred whose first pair of callbacks are the request\n-    callback and errback. The Deferred also triggers when the request\n-    callback/errback is executed (ie. when the request is downloaded)\n-    \"\"\"\n-    d = Deferred()\n-    if request.callback:\n-        d.addCallbacks(request.callback, request.errback)\n-    request.callback, request.errback = d.callback, d.errback\n-    return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aa6fb7daaacdb349cdd5b7fe89658e2d056c4722", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 54 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,21 +1,25 @@\n \n-def start_python_console(namespace=None, noipython=False):\n+def start_python_console(namespace=None, noipython=False, banner=''):\n     \"\"\"Start Python console binded to the given namespace. If IPython is\n     available, an IPython console will be started instead, unless `noipython`\n     is True. Also, tab completion will be used on Unix systems.\n     \"\"\"\n     if namespace is None:\n         namespace = {}\n+\n     try:\n         try: # use IPython if available\n             if noipython:\n-                raise ImportError\n-            import IPython\n+                raise ImportError()\n+\n             try:\n-                IPython.embed(user_ns=namespace)\n-            except AttributeError:\n-                shell = IPython.Shell.IPShellEmbed(argv=[], user_ns=namespace)\n-                shell()\n+                from IPython.frontend.terminal.embed import InteractiveShellEmbed\n+                sh = InteractiveShellEmbed(banner1=banner)\n+            except ImportError:\n+                from IPython.Shell import IPShellEmbed\n+                sh = IPShellEmbed(banner=banner)\n+\n+            sh(global_ns={}, local_ns=namespace)\n         except ImportError:\n             import code\n             try: # readline module is only available on unix systems\n@@ -25,6 +29,6 @@ def start_python_console(namespace=None, noipython=False):\n             else:\n                 import rlcompleter\n                 readline.parse_and_bind(\"tab:complete\")\n-            code.interact(banner='', local=namespace)\n+            code.interact(banner=banner, local=namespace)\n     except SystemExit: # raised when using exit() in python code.interact\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc17e9d41242784bbf9b145de80a9483e3c7844f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 13/11 | Churn Δ: 42 | Churn Cumulative: 1244 | Contributors (this commit): 7 | Commits (past 90d): 12 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -24,3 +24,5 @@ else:\n         from scrapy.selector.libxml2sel import *\n     else:\n         from scrapy.selector.lxmlsel import *\n+\n+from scrapy.selector.csssel import *\n\n@@ -0,0 +1,32 @@\n+from cssselect import GenericTranslator, HTMLTranslator\n+from scrapy.utils.python import flatten\n+from scrapy.selector import HtmlXPathSelector, XmlXPathSelector, XPathSelectorList\n+\n+class CSSSelectorList(XPathSelectorList):\n+    def xpath(self, xpath):\n+        return self.__class__(flatten([x.xpath(xpath) for x in self]))\n+\n+    def get(self, attr):\n+        return self.__class__(flatten([x.get(attr) for x in self]))\n+\n+    def text(self, all=False):\n+        return self.__class__(flatten([x.text(all) for x in self]))\n+\n+class CSSSelectorMixin(object):\n+    def select(self, css):\n+        return CSSSelectorList(super(CSSSelectorMixin, self).select(self.translator.css_to_xpath(css)))\n+\n+    def xpath(self, xpath):\n+        return CSSSelectorList(super(CSSSelectorMixin, self).select(xpath))\n+\n+    def text(self, all=False):\n+        return self.xpath('string()') if all else self.xpath('text()')\n+\n+    def get(self, attr):\n+        return self.xpath('@' + attr)\n+\n+class XmlCSSSelector(CSSSelectorMixin, XmlXPathSelector):\n+    translator = GenericTranslator()\n+\n+class HtmlCSSSelector(CSSSelectorMixin, HtmlXPathSelector):\n+    translator = HTMLTranslator()\n\n@@ -11,7 +11,7 @@ from w3lib.url import any_to_uri\n \n from scrapy.item import BaseItem\n from scrapy.spider import BaseSpider\n-from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n+from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector, XmlCSSSelector, HtmlCSSSelector\n from scrapy.utils.spider import create_spider_for_request\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n@@ -97,8 +97,12 @@ class Shell(object):\n         self.vars['response'] = response\n         self.vars['xxs'] = XmlXPathSelector(response) \\\n             if isinstance(response, XmlResponse) else None\n+        self.vars['xcs'] = XmlCSSSelector(response) \\\n+            if isinstance(response, XmlResponse) else None\n         self.vars['hxs'] = HtmlXPathSelector(response) \\\n             if isinstance(response, HtmlResponse) else None\n+        self.vars['hcs'] = HtmlCSSSelector(response) \\\n+            if isinstance(response, HtmlResponse) else None\n         if self.inthread:\n             self.vars['fetch'] = self.fetch\n         self.vars['view'] = open_in_browser\n\n@@ -122,6 +122,6 @@ try:\n except ImportError:\n     from distutils.core import setup\n else:\n-    setup_args['install_requires'] = ['Twisted>=10.0.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL']\n+    setup_args['install_requires'] = ['Twisted>=10.0.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL', 'cssselect>0.8']\n \n setup(**setup_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b38ac27eee70b41a4d6d2545b5bf78139ff23f24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 21 | Files Changed: 5 | Hunks: 23 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 57 | Churn Cumulative: 946 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,11 +7,10 @@ variable.\n Two backends are currently available: lxml (default) and libxml2.\n \n \"\"\"\n-\n import os\n \n-backend = os.environ.get('SCRAPY_SELECTORS_BACKEND')\n \n+backend = os.environ.get('SCRAPY_SELECTORS_BACKEND')\n if backend == 'libxml2':\n     from scrapy.selector.libxml2sel import *\n elif backend == 'lxml':\n@@ -26,3 +25,15 @@ else:\n         from scrapy.selector.lxmlsel import *\n \n from scrapy.selector.csssel import *\n+from scrapy.selector.list import SelectorList\n+\n+\n+class XPathSelectorList(SelectorList):\n+\n+    def __init__(self, *a, **kw):\n+        import warnings\n+        from scrapy.exceptions import ScrapyDeprecationWarning\n+        warnings.warn('XPathSelectorList is deprecated, use '\n+                      'scrapy.selector.SelectorList instead',\n+                      category=ScrapyDeprecationWarning, stacklevel=1)\n+        super(XPathSelectorList, self).__init__(*a, **kw)\n\n@@ -1,8 +1,10 @@\n from cssselect import GenericTranslator, HTMLTranslator\n from scrapy.utils.python import flatten\n-from scrapy.selector import HtmlXPathSelector, XmlXPathSelector, XPathSelectorList\n+from scrapy.selector import HtmlXPathSelector, XmlXPathSelector\n+from .list import SelectorList\n \n-class CSSSelectorList(XPathSelectorList):\n+\n+class CSSSelectorList(SelectorList):\n     def xpath(self, xpath):\n         return self.__class__(flatten([x.xpath(xpath) for x in self]))\n \n@@ -12,6 +14,7 @@ class CSSSelectorList(XPathSelectorList):\n     def text(self, all=False):\n         return self.__class__(flatten([x.text(all) for x in self]))\n \n+\n class CSSSelectorMixin(object):\n     def select(self, css):\n         return CSSSelectorList(super(CSSSelectorMixin, self).select(self.translator.css_to_xpath(css)))\n@@ -25,8 +28,10 @@ class CSSSelectorMixin(object):\n     def get(self, attr):\n         return self.xpath('@' + attr)\n \n+\n class XmlCSSSelector(CSSSelectorMixin, XmlXPathSelector):\n     translator = GenericTranslator()\n \n+\n class HtmlCSSSelector(CSSSelectorMixin, HtmlXPathSelector):\n     translator = HTMLTranslator()\n\n@@ -12,10 +12,11 @@ from scrapy.utils.misc import extract_regex\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.decorator import deprecated\n from .libxml2document import Libxml2Document, xmlDoc_from_html, xmlDoc_from_xml\n-from .list import XPathSelectorList\n+from .list import SelectorList\n+\n+\n+__all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector']\n \n-__all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector', \\\n-    'XPathSelectorList']\n \n class XPathSelector(object_ref):\n \n@@ -44,13 +45,13 @@ class XPathSelector(object_ref):\n             except libxml2.xpathError:\n                 raise ValueError(\"Invalid XPath: %s\" % xpath)\n             if hasattr(xpath_result, '__iter__'):\n-                return XPathSelectorList([self.__class__(node=node, parent=self, \\\n+                return SelectorList([self.__class__(node=node, parent=self, \\\n                     expr=xpath) for node in xpath_result])\n             else:\n-                return XPathSelectorList([self.__class__(node=xpath_result, \\\n+                return SelectorList([self.__class__(node=xpath_result, \\\n                     parent=self, expr=xpath)])\n         else:\n-            return XPathSelectorList([])\n+            return SelectorList([])\n \n     def re(self, regex):\n         return extract_regex(regex, self.extract())\n\n@@ -1,7 +1,8 @@\n from scrapy.utils.python import flatten\n from scrapy.utils.decorator import deprecated\n \n-class XPathSelectorList(list):\n+\n+class SelectorList(list):\n \n     def __getslice__(self, i, j):\n         return self.__class__(list.__getslice__(self, i, j))\n@@ -18,6 +19,6 @@ class XPathSelectorList(list):\n     def extract_unquoted(self):\n         return [x.extract_unquoted() for x in self]\n \n-    @deprecated(use_instead='XPathSelectorList.select')\n+    @deprecated(use_instead='SelectorList.select')\n     def x(self, xpath):\n         return self.select(xpath)\n\n@@ -10,11 +10,10 @@ from scrapy.utils.python import unicode_to_str\n from scrapy.utils.decorator import deprecated\n from scrapy.http import TextResponse\n from .lxmldocument import LxmlDocument\n-from .list import XPathSelectorList\n+from .list import SelectorList\n \n \n-__all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector', \\\n-           'XPathSelectorList']\n+__all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector']\n \n \n class XPathSelector(object_ref):\n@@ -25,8 +24,8 @@ class XPathSelector(object_ref):\n \n     def __init__(self, response=None, text=None, namespaces=None, _root=None, _expr=None):\n         if text is not None:\n-            response = TextResponse(url='about:blank', \\\n-                body=unicode_to_str(text, 'utf-8'), encoding='utf-8')\n+            response = TextResponse(url='about:blank', encoding='utf-8',\n+                                    body=unicode_to_str(text, 'utf-8'))\n         if response is not None:\n             _root = LxmlDocument(response, self._parser)\n \n@@ -39,7 +38,7 @@ class XPathSelector(object_ref):\n         try:\n             xpathev = self._root.xpath\n         except AttributeError:\n-            return XPathSelectorList([])\n+            return SelectorList([])\n \n         try:\n             result = xpathev(xpath, namespaces=self.namespaces)\n@@ -51,7 +50,7 @@ class XPathSelector(object_ref):\n \n         result = [self.__class__(_root=x, _expr=xpath, namespaces=self.namespaces)\n                   for x in result]\n-        return XPathSelectorList(result)\n+        return SelectorList(result)\n \n     def re(self, regex):\n         return extract_regex(regex, self.extract())\n@@ -88,10 +87,8 @@ class XPathSelector(object_ref):\n     def __str__(self):\n         data = repr(self.extract()[:40])\n         return \"<%s xpath=%r data=%s>\" % (type(self).__name__, self._expr, data)\n-\n     __repr__ = __str__\n \n-\n     @deprecated(use_instead='XPathSelector.extract')\n     def extract_unquoted(self):\n         return self.extract()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4e6967b854644d527c0eb5ace61e80a590fbd520", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 190 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 10 | Methods Changed: 17 | Complexity Δ (Sum/Max): 18/15 | Churn Δ: 212 | Churn Cumulative: 253 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.8313253012048193\n\nDIFF:\n@@ -1,37 +1,88 @@\n from cssselect import GenericTranslator, HTMLTranslator\n-from scrapy.utils.python import flatten\n-from scrapy.selector import HtmlXPathSelector, XmlXPathSelector\n-from .list import SelectorList\n+from cssselect.xpath import XPathExpr, ExpressionError\n+from scrapy.selector import XPathSelector, HtmlXPathSelector, XmlXPathSelector\n \n \n-class CSSSelectorList(SelectorList):\n-    def xpath(self, xpath):\n-        return self.__class__(flatten([x.xpath(xpath) for x in self]))\n+class ScrapyXPathExpr(XPathExpr):\n \n-    def get(self, attr):\n-        return self.__class__(flatten([x.get(attr) for x in self]))\n+    textnode = False\n+    attribute = None\n \n-    def text(self, all=False):\n-        return self.__class__(flatten([x.text(all) for x in self]))\n+    @classmethod\n+    def from_xpath(cls, xpath, textnode=False, attribute=None):\n+        x = cls(path=xpath.path, element=xpath.element, condition=xpath.condition)\n+        x.textnode = textnode\n+        x.attribute = attribute\n+        return x\n+\n+    def __str__(self):\n+        path = super(ScrapyXPathExpr, self).__str__()\n+        if self.textnode:\n+            if path == '*':\n+                path = 'text()'\n+            elif path.endswith('::*/*'):\n+                path = path[:-3] + 'text()'\n+            else:\n+                path += '/text()'\n+\n+        if self.attribute is not None:\n+            if path.endswith('::*/*'):\n+                path = path[:-2]\n+            path += '/@%s' % self.attribute\n+\n+        return path\n+\n+    def join(self, combiner, other):\n+        super(ScrapyXPathExpr, self).join(combiner, other)\n+        self.textnode = other.textnode\n+        self.attribute = other.attribute\n+        return self\n+\n+\n+class TranslatorMixin(object):\n+\n+    def xpath_element(self, selector):\n+        xpath = super(TranslatorMixin, self).xpath_element(selector)\n+        return ScrapyXPathExpr.from_xpath(xpath)\n+\n+    def xpath_text_pseudo(self, xpath):\n+        \"\"\"Support selecting text nodes using :text pseudo-element\"\"\"\n+        return ScrapyXPathExpr.from_xpath(xpath, textnode=True)\n+\n+    def xpath_attribute_function(self, xpath, function):\n+        if function.argument_types() not in (['STRING'], ['IDENT']):\n+            raise ExpressionError(\n+                \"Expected a single string or ident for :contains(), got %r\"\n+                % function.arguments)\n+        value = function.arguments[0].value\n+        return ScrapyXPathExpr.from_xpath(xpath, attribute=value)\n+\n+\n+class ScrapyGenericTranslator(TranslatorMixin, GenericTranslator):\n+    pass\n+\n+\n+class ScrapyHTMLTranslator(TranslatorMixin, HTMLTranslator):\n+    pass\n \n \n class CSSSelectorMixin(object):\n+\n     def select(self, css):\n-        return CSSSelectorList(super(CSSSelectorMixin, self).select(self.translator.css_to_xpath(css)))\n+        xpath = self._css2xpath(css)\n+        return super(CSSSelectorMixin, self).select(xpath)\n \n-    def xpath(self, xpath):\n-        return CSSSelectorList(super(CSSSelectorMixin, self).select(xpath))\n-\n-    def text(self, all=False):\n-        return self.xpath('string()') if all else self.xpath('text()')\n-\n-    def get(self, attr):\n-        return self.xpath('@' + attr)\n+    def _css2xpath(self, css):\n+        return self.translator.css_to_xpath(css)\n \n \n-class XmlCSSSelector(CSSSelectorMixin, XmlXPathSelector):\n-    translator = GenericTranslator()\n+class CSSSelector(CSSSelectorMixin, XPathSelector):\n+    translator = ScrapyHTMLTranslator()\n \n \n class HtmlCSSSelector(CSSSelectorMixin, HtmlXPathSelector):\n-    translator = HTMLTranslator()\n+    translator = ScrapyHTMLTranslator()\n+\n+\n+class XmlCSSSelector(CSSSelectorMixin, XmlXPathSelector):\n+    translator = ScrapyGenericTranslator()\n\n@@ -0,0 +1,117 @@\n+\"\"\"\n+Selector tests for cssselect backend\n+\"\"\"\n+from twisted.trial import unittest\n+from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n+from scrapy.selector import CSSSelector, XmlCSSSelector, HtmlCSSSelector\n+from scrapy.selector.csssel import ScrapyHTMLTranslator\n+\n+HTMLBODY = '''\n+<html>\n+<body>\n+<div>\n+ <a id=\"name-anchor\" name=\"foo\"></a>\n+ <a id=\"tag-anchor\" rel=\"tag\" href=\"http://localhost/foo\">link</a>\n+ <a id=\"nofollow-anchor\" rel=\"nofollow\" href=\"https://example.org\"> link</a>\n+ <p id=\"paragraph\">\n+   lorem ipsum text\n+   <b id=\"p-b\">hi</b> <em id=\"p-em\">there</em>\n+   <b id=\"p-b2\">guy</b>\n+   <input type=\"checkbox\" id=\"checkbox-unchecked\" />\n+   <input type=\"checkbox\" id=\"checkbox-disabled\" disabled=\"\" />\n+   <input type=\"text\" id=\"text-checked\" checked=\"checked\" />\n+   <input type=\"hidden\" />\n+   <input type=\"hidden\" disabled=\"disabled\" />\n+   <input type=\"checkbox\" id=\"checkbox-checked\" checked=\"checked\" />\n+   <input type=\"checkbox\" id=\"checkbox-disabled-checked\"\n+          disabled=\"disabled\" checked=\"checked\" />\n+   <fieldset id=\"fieldset\" disabled=\"disabled\">\n+     <input type=\"checkbox\" id=\"checkbox-fieldset-disabled\" />\n+     <input type=\"hidden\" />\n+   </fieldset>\n+ </p>\n+ <map name=\"dummymap\">\n+   <area shape=\"circle\" coords=\"200,250,25\" href=\"foo.html\" id=\"area-href\" />\n+   <area shape=\"default\" id=\"area-nohref\" />\n+ </map>\n+</div>\n+<div class=\"cool-footer\" id=\"foobar-div\" foobar=\"ab bc cde\">\n+    <span id=\"foobar-span\">foo ter</span>\n+</div>\n+</body></html>\n+'''\n+\n+\n+class TranslatorMixinTest(unittest.TestCase):\n+\n+    tr_cls = ScrapyHTMLTranslator\n+\n+    def setUp(self):\n+        self.tr = self.tr_cls()\n+        self.c2x = self.tr.css_to_xpath\n+\n+    def test_attribute_function(self):\n+        cases = [\n+            (':attribute(name)', u'descendant-or-self::*/@name'),\n+            ('a:attribute(name)', u'descendant-or-self::a/@name'),\n+            ('a :attribute(name)', u'descendant-or-self::a/descendant-or-self::*/@name'),\n+            ('a > :attribute(name)', u'descendant-or-self::a/*/@name'),\n+        ]\n+        for css, xpath in cases:\n+            self.assertEqual(self.c2x(css), xpath, css)\n+\n+    def test_text_pseudo_element(self):\n+        cases = [\n+            (':text', u'descendant-or-self::text()'),\n+            ('p:text', u'descendant-or-self::p/text()'),\n+            ('p :text', u'descendant-or-self::p/descendant-or-self::text()'),\n+            ('#id:text', u\"descendant-or-self::*[@id = 'id']/text()\"),\n+            ('p#id:text', u\"descendant-or-self::p[@id = 'id']/text()\"),\n+            ('p#id :text', u\"descendant-or-self::p[@id = 'id']/descendant-or-self::text()\"),\n+            ('p#id > :text', u\"descendant-or-self::p[@id = 'id']/*/text()\"),\n+            ('p#id ~ :text', u\"descendant-or-self::p[@id = 'id']/following-sibling::*/text()\"),\n+            ('a[href]:text', u'descendant-or-self::a[@href]/text()'),\n+            ('a[href] :text', u'descendant-or-self::a[@href]/descendant-or-self::text()'),\n+            ('p:text, a:text', u\"descendant-or-self::p/text() | descendant-or-self::a/text()\"),\n+        ]\n+        for css, xpath in cases:\n+            self.assertEqual(self.c2x(css), xpath, css)\n+\n+\n+class HTMLCSSSelectorTest(unittest.TestCase):\n+\n+    hcs_cls = HtmlCSSSelector\n+\n+    def setUp(self):\n+        self.htmlresponse = HtmlResponse('http://example.com', body=HTMLBODY)\n+        self.hcs = self.hcs_cls(self.htmlresponse)\n+\n+    def x(self, *a, **kw):\n+        return [v.strip() for v in self.hcs.select(*a, **kw).extract() if v.strip()]\n+\n+    def test_selector_simple(self):\n+        for x in self.hcs.select('input'):\n+            self.assertTrue(isinstance(x, self.hcs.__class__), x)\n+        self.assertEqual(self.hcs.select('input').extract(),\n+                         [x.extract() for x in self.hcs.select('input')])\n+\n+    def test_text_pseudo_element(self):\n+        self.assertEqual(self.x('#p-b2'), [u'<b id=\"p-b2\">guy</b>'])\n+        self.assertEqual(self.x('#p-b2:text'), [u'guy'])\n+        self.assertEqual(self.x('#p-b2 :text'), [u'guy'])\n+        self.assertEqual(self.x('#paragraph:text'), [u'lorem ipsum text'])\n+        self.assertEqual(self.x('#paragraph :text'), [u'lorem ipsum text', u'hi', u'there', u'guy'])\n+        self.assertEqual(self.x('p:text'), [u'lorem ipsum text'])\n+        self.assertEqual(self.x('p :text'), [u'lorem ipsum text', u'hi', u'there', u'guy'])\n+\n+    def test_attribute_function(self):\n+        self.assertEqual(self.x('#p-b2:attribute(id)'), [u'p-b2'])\n+        self.assertEqual(self.x('.cool-footer:attribute(class)'), [u'cool-footer'])\n+        self.assertEqual(self.x('.cool-footer :attribute(id)'), [u'foobar-div', u'foobar-span'])\n+        self.assertEqual(self.x('map[name=\"dummymap\"] :attribute(shape)'), [u'circle', u'default'])\n+\n+    def test_nested_selector(self):\n+        self.assertEqual(self.hcs.select('p').select('b:text').extract(),\n+                         [u'hi', u'guy'])\n+        self.assertEqual(self.hcs.select('div').select('area:last-child').extract(),\n+                         [u'<area shape=\"default\" id=\"area-nohref\">'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
