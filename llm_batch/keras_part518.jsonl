{"custom_id": "keras#5ce6017623a61e7f34daa11569e462ef8fc3f660", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 42 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ _ASSETS_DIRNAME = \"assets\"\n \n # A temporary flag to enable the new idempotent saving framework.\n _SAVING_V3_ENABLED = threading.local()\n-_SAVING_V3_ENABLED.value = False\n+_SAVING_V3_ENABLED.value = True\n \n ATTR_SKIPLIST = frozenset(\n     {\n@@ -689,7 +689,7 @@ def _is_keras_trackable(obj):\n \n \n def saving_v3_enabled():\n-    return getattr(_SAVING_V3_ENABLED, \"value\", False)\n+    return getattr(_SAVING_V3_ENABLED, \"value\", True)\n \n \n # Some debugging utilities.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ede9c711ecdc731d644bc913944fe24d1b45c8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 218 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -31,7 +31,11 @@ class Lion(optimizer.Optimizer):\n     optimizers such as Adam that also rely on second-order moments. This make\n     Lion more memory-efficient as it only keeps track of the momentum. According\n     to the authors (see reference), its performance gain over Adam grows with\n-    the batch size.\n+    the batch size. Because the update of Lion is produced through the sign\n+    operation, resulting in a larger norm, a suitable learning rate for Lion is\n+    typically 3-10x smaller than that for AdamW. The weight decay for Lion\n+    should be in turn 3-10x larger than that for AdamW to maintain a\n+    similar strength (lr * wd).\n \n     Args:\n       learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2a8e49939b37791f39b8f537bbe54e1f7187a376", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 21 | Churn Cumulative: 239 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -88,6 +88,8 @@ class Lion(optimizer.Optimizer):\n         self._learning_rate = self._build_learning_rate(learning_rate)\n         self.beta_1 = beta_1\n         self.beta_2 = beta_2\n+        if isinstance(beta_1, (int, float)) and (beta_1 < 0 or beta_1 > 1):\n+            raise ValueError(\"`beta_1` must be between [0, 1].\")\n \n     def build(self, var_list):\n         \"\"\"Initialize optimizer variables.\n@@ -100,6 +102,7 @@ class Lion(optimizer.Optimizer):\n         super().build(var_list)\n         if hasattr(self, \"_built\") and self._built:\n             return\n+        if self.beta_1 != 0.0:\n             self.momentums = []\n             for var in var_list:\n                 self.momentums.append(\n@@ -115,10 +118,17 @@ class Lion(optimizer.Optimizer):\n         beta_1 = tf.cast(self.beta_1, variable.dtype)\n         beta_2 = tf.cast(self.beta_2, variable.dtype)\n         var_key = self._var_key(variable)\n-        m = self.momentums[self._index_dict[var_key]]\n \n         if isinstance(gradient, tf.IndexedSlices):\n-            # Sparse gradients (use m as a buffer)\n+            # Sparse gradients\n+            if self.beta_1 == 0.0:\n+                variable.scatter_sub(\n+                    tf.IndexedSlices(\n+                        lr * tf.math.sign(gradient.values), gradient.indices\n+                    )\n+                )\n+            else:  # use m as a buffer\n+                m = self.momentums[self._index_dict[var_key]]\n                 m.assign(m * beta_1)\n                 m.scatter_add(\n                     tf.IndexedSlices(\n@@ -130,11 +140,16 @@ class Lion(optimizer.Optimizer):\n                 m.assign(m * beta_2 / beta_1)\n                 m.scatter_add(\n                     tf.IndexedSlices(\n-                    gradient.values * (1.0 - beta_2 / beta_1), gradient.indices\n+                        gradient.values * (1.0 - beta_2 / beta_1),\n+                        gradient.indices,\n                     )\n                 )\n         else:\n             # Dense gradients\n+            if self.beta_1 == 0.0:\n+                variable.assign_sub(lr * tf.math.sign(gradient))\n+            else:\n+                m = self.momentums[self._index_dict[var_key]]\n                 variable.assign_sub(\n                     lr * tf.math.sign(m * beta_1 + gradient * (1.0 - beta_1))\n                 )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#61224483219906ce4889cc9e94408ffa12d29b19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 157 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/7 | Churn Δ: 175 | Churn Cumulative: 414 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -88,8 +88,11 @@ class Lion(optimizer.Optimizer):\n         self._learning_rate = self._build_learning_rate(learning_rate)\n         self.beta_1 = beta_1\n         self.beta_2 = beta_2\n-        if isinstance(beta_1, (int, float)) and (beta_1 < 0 or beta_1 > 1):\n-            raise ValueError(\"`beta_1` must be between [0, 1].\")\n+        if self.beta_1 == 0.0:\n+            raise ValueError(\n+                \"`beta_1` must be between [0, 1] otherwise the optimizer \"\n+                \"degenerate to SignSGD.\"\n+            )\n \n     def build(self, var_list):\n         \"\"\"Initialize optimizer variables.\n@@ -102,7 +105,6 @@ class Lion(optimizer.Optimizer):\n         super().build(var_list)\n         if hasattr(self, \"_built\") and self._built:\n             return\n-        if self.beta_1 != 0.0:\n         self.momentums = []\n         for var in var_list:\n             self.momentums.append(\n@@ -118,17 +120,10 @@ class Lion(optimizer.Optimizer):\n         beta_1 = tf.cast(self.beta_1, variable.dtype)\n         beta_2 = tf.cast(self.beta_2, variable.dtype)\n         var_key = self._var_key(variable)\n+        m = self.momentums[self._index_dict[var_key]]\n \n         if isinstance(gradient, tf.IndexedSlices):\n-            # Sparse gradients\n-            if self.beta_1 == 0.0:\n-                variable.scatter_sub(\n-                    tf.IndexedSlices(\n-                        lr * tf.math.sign(gradient.values), gradient.indices\n-                    )\n-                )\n-            else:  # use m as a buffer\n-                m = self.momentums[self._index_dict[var_key]]\n+            # Sparse gradients (use m as a buffer)\n             m.assign(m * beta_1)\n             m.scatter_add(\n                 tf.IndexedSlices(\n@@ -140,16 +135,11 @@ class Lion(optimizer.Optimizer):\n             m.assign(m * beta_2 / beta_1)\n             m.scatter_add(\n                 tf.IndexedSlices(\n-                        gradient.values * (1.0 - beta_2 / beta_1),\n-                        gradient.indices,\n+                    gradient.values * (1.0 - beta_2 / beta_1), gradient.indices\n                 )\n             )\n         else:\n             # Dense gradients\n-            if self.beta_1 == 0.0:\n-                variable.assign_sub(lr * tf.math.sign(gradient))\n-            else:\n-                m = self.momentums[self._index_dict[var_key]]\n             variable.assign_sub(\n                 lr * tf.math.sign(m * beta_1 + gradient * (1.0 - beta_1))\n             )\n\n@@ -0,0 +1,149 @@\n+# Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Lion.\"\"\"\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.framework import dtypes\n+\n+from keras.optimizers.lion import Lion\n+\n+\n+def lion_update_numpy(\n+    params,\n+    grads,\n+    momentums,\n+    learning_rate=0.0001,\n+    beta_1=0.9,\n+    beta_2=0.99,\n+):\n+    params = params - learning_rate * np.sign(\n+        beta_1 * momentums + (1 - beta_1) * grads\n+    )\n+    momentums = beta_2 * momentums + (1 - beta_2) * grads\n+    return params, momentums\n+\n+\n+class LionOptimizerTest(tf.test.TestCase):\n+    def testDense(self):\n+        for dtype in [dtypes.half, dtypes.float32, dtypes.float64]:\n+            learning_rate = 0.0001\n+            beta_1 = 0.9\n+            beta_2 = 0.99\n+            with self.cached_session():\n+                m0_np = np.array([0.0, 0.0], dtype=dtype.as_numpy_dtype)\n+                m1_np = np.array([0.0, 0.0], dtype=dtype.as_numpy_dtype)\n+                var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n+                var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n+                grads0_np = np.array([0.9, 0.0], dtype=dtype.as_numpy_dtype)\n+                grads1_np = np.array([0.1, 0.0], dtype=dtype.as_numpy_dtype)\n+\n+                var0 = tf.Variable(var0_np)\n+                var1 = tf.Variable(var1_np)\n+                grads0 = tf.constant(grads0_np)\n+                grads1 = tf.constant(grads1_np)\n+                optimizer = Lion(\n+                    learning_rate=learning_rate,\n+                    beta_1=beta_1,\n+                    beta_2=beta_2,\n+                )\n+\n+                # Run 3 steps of Lion\n+                for _ in range(3):\n+                    optimizer.apply_gradients(\n+                        zip([grads0, grads1], [var0, var1])\n+                    )\n+                    var0_np, m0_np = lion_update_numpy(\n+                        var0_np,\n+                        grads0_np,\n+                        m0_np,\n+                        learning_rate=learning_rate,\n+                        beta_1=beta_1,\n+                        beta_2=beta_2,\n+                    )\n+                    var1_np, m1_np = lion_update_numpy(\n+                        var1_np,\n+                        grads1_np,\n+                        m1_np,\n+                        learning_rate=learning_rate,\n+                        beta_1=beta_1,\n+                        beta_2=beta_2,\n+                    )\n+                    # Validate updated params\n+                    self.assertAllCloseAccordingToType(var0_np, var0)\n+                    self.assertAllCloseAccordingToType(var1_np, var1)\n+\n+    def testSparse(self):\n+        for dtype in [dtypes.half, dtypes.float32, dtypes.float64]:\n+            learning_rate = 0.0001\n+            beta_1 = 0.9\n+            beta_2 = 0.99\n+            with self.cached_session():\n+                m0_np = np.array([0.0, 0.0], dtype=dtype.as_numpy_dtype)\n+                m1_np = np.array([0.0, 0.0], dtype=dtype.as_numpy_dtype)\n+                var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n+                var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n+                grads0_np = np.array([0.9, 0.0], dtype=dtype.as_numpy_dtype)\n+                grads1_np = np.array([0.1, 0.0], dtype=dtype.as_numpy_dtype)\n+\n+                var0 = tf.Variable(var0_np)\n+                var1 = tf.Variable(var1_np)\n+                grads0_np_indices = np.array([0], dtype=np.int32)\n+                grads0 = tf.IndexedSlices(\n+                    tf.constant(grads0_np[grads0_np_indices]),\n+                    tf.constant(grads0_np_indices),\n+                    tf.constant([2]),\n+                )\n+                grads1_np_indices = np.array([0], dtype=np.int32)\n+                grads1 = tf.IndexedSlices(\n+                    tf.constant(grads1_np[grads1_np_indices]),\n+                    tf.constant(grads1_np_indices),\n+                    tf.constant([2]),\n+                )\n+\n+                optimizer = Lion(\n+                    learning_rate=learning_rate,\n+                    beta_1=beta_1,\n+                    beta_2=beta_2,\n+                )\n+\n+                # Run 3 steps of Lion\n+                for _ in range(3):\n+                    optimizer.apply_gradients(\n+                        zip([grads0, grads1], [var0, var1])\n+                    )\n+                    var0_np, m0_np = lion_update_numpy(\n+                        var0_np,\n+                        grads0_np,\n+                        m0_np,\n+                        learning_rate=learning_rate,\n+                        beta_1=beta_1,\n+                        beta_2=beta_2,\n+                    )\n+                    var1_np, m1_np = lion_update_numpy(\n+                        var1_np,\n+                        grads1_np,\n+                        m1_np,\n+                        learning_rate=learning_rate,\n+                        beta_1=beta_1,\n+                        beta_2=beta_2,\n+                    )\n+                    # Validate updated params\n+                    self.assertAllCloseAccordingToType(var0_np, var0)\n+                    self.assertAllCloseAccordingToType(var1_np, var1)\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8b9f81df42d672a675e6519f4e8558f8bb5462c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9553 | Contributors (this commit): 117 | Commits (past 90d): 6 | Contributors (cumulative): 117 | DMM Complexity: None\n\nDIFF:\n@@ -1351,7 +1351,7 @@ class ModelCheckpoint(Callback):\n         self.save_freq = save_freq\n         self.epochs_since_last_save = 0\n         self._batches_seen_since_last_saving = 0\n-        self._last_batch_seen = 0\n+        self._last_batch_seen = -1\n         self.best = initial_value_threshold\n \n         if save_weights_only:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c2ac2d318111498e4437bb2b77120350c491449", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 861 | Contributors (this commit): 23 | Commits (past 90d): 1 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -164,6 +164,12 @@ def load_data(\n @keras_export(\"keras.datasets.reuters.get_word_index\")\n def get_word_index(path=\"reuters_word_index.json\"):\n     \"\"\"Retrieves a dict mapping words to their index in the Reuters dataset.\n+    Actual word indices starts from 3, with 3 indices reserved for:\n+    0 (padding), 1 (start), 2 (oof).\n+\n+    E.g. word index of 'the' is 1, but the in the actual training data, the\n+    index of 'the' will be 1+3 = 4. Vice versa, to translate word indices in\n+    training data back to words using this mapping, indices need to substract 3.\n \n     Args:\n         path: where to cache the data (relative to `~/.keras/dataset`).\n@@ -182,3 +188,21 @@ def get_word_index(path=\"reuters_word_index.json\"):\n     )\n     with open(path) as f:\n         return json.load(f)\n+\n+\n+@keras_export(\"keras.datasets.reuters.get_ylabels\")\n+def get_ylabels():\n+    \"\"\"Returns the y label as a list of strings with indices matching\n+    training data.\n+\n+    See references from:\n+    - https://github.com/keras-team/keras/issues/12072#issuecomment-458154097\n+    - https://martin-thoma.com/nlp-reuters/\n+    \"\"\"\n+    return ('cocoa','grain', 'veg-oil', 'earn', 'acq', 'wheat', 'copper',\n+        'housing', 'money-supply', 'coffee', 'sugar', 'trade', 'reserves',\n+        'ship', 'cotton', 'carcass', 'crude', 'nat-gas', 'cpi', 'money-fx',\n+        'interest', 'gnp', 'meal-feed', 'alum', 'oilseed', 'gold', 'tin',\n+        'strategic-metal', 'livestock', 'retail', 'ipi', 'iron-steel', 'rubber',\n+        'heat', 'jobs', 'lei', 'bop', 'zinc', 'orange', 'pet-chem', 'dlr',\n+        'gas', 'silver', 'wpi', 'hog', 'lead')\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4901592dd143f682c5ca43861dc5917977ee41dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 273 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -17,11 +17,13 @@\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers import optimizer\n+from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n+@register_keras_serializable()\n @keras_export(\"keras.optimizers.Lion\", v1=[])\n class Lion(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Lion algorithm.\n@@ -88,10 +90,10 @@ class Lion(optimizer.Optimizer):\n         self._learning_rate = self._build_learning_rate(learning_rate)\n         self.beta_1 = beta_1\n         self.beta_2 = beta_2\n-        if self.beta_1 == 0.0:\n+        if beta_1 <= 0 or beta_1 > 1:\n             raise ValueError(\n-                \"`beta_1` must be between [0, 1] otherwise the optimizer \"\n-                \"degenerate to SignSGD.\"\n+                f\"`beta_1`={beta_1} but it must be between ]0, 1], otherwise, \"\n+                \" the optimizer degenerate to SignSGD.\"\n             )\n \n     def build(self, var_list):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fee5345c724ff53c056055a151c2f39b1445797e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 277 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -92,8 +92,8 @@ class Lion(optimizer.Optimizer):\n         self.beta_2 = beta_2\n         if beta_1 <= 0 or beta_1 > 1:\n             raise ValueError(\n-                f\"`beta_1`={beta_1} but it must be between ]0, 1], otherwise, \"\n-                \" the optimizer degenerate to SignSGD.\"\n+                f\"`beta_1`={beta_1} must be between ]0, 1]. Otherwise, \"\n+                \"the optimizer degenerates to SignSGD.\"\n             )\n \n     def build(self, var_list):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f003f9f11e4b8efad33d4078ddae8533fd19f51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 66 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -464,7 +464,7 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n             vhat = self.get_slot(var, \"vhat\")\n             vhat.assign(tf.maximum(vhat, v))\n             v = vhat\n-        var.assign_sub((m * alpha) / (tf.sqrt(v) - coefficients[\"epsilon\"]))\n+        var.assign_sub((m * alpha) / (tf.sqrt(v) + coefficients[\"epsilon\"]))\n \n     @tf.function(jit_compile=True)\n     def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4cd944ddf2ede19954b6a767ef244ca0e36f3aea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 130 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -19,8 +19,8 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n from keras.dtensor import test_util\n-from keras.testing_infra import test_utils\n from keras.layers.normalization import batch_normalization\n+from keras.testing_infra import test_utils\n \n # isort: off\n # Import the MirroredStrategy that is backed by DTensor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0468c5cfd09a4d4bed7b810b58c14c3278231120", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 47 | Churn Cumulative: 545 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: 0.8846153846153846\n\nDIFF:\n@@ -35,13 +35,9 @@ from tensorflow.python.util.tf_export import keras_export\n PLAIN_TYPES = (str, int, float, bool)\n SHARED_OBJECTS = threading.local()\n SAFE_MODE = threading.local()\n-# TODO(nkovela): Create custom `__internal__` namespace serialization support.\n # TODO(nkovela): Debug serialization of decorated functions inside lambdas\n # to allow for serialization of custom_gradient.\n-NON_SERIALIZABLE_CLASS_MODULES = (\n-    \"tensorflow.python.ops.custom_gradient\",\n-    \"keras.__internal__\",\n-)\n+NON_SERIALIZABLE_CLASS_MODULES = (\"tensorflow.python.ops.custom_gradient\",)\n BUILTIN_MODULES = (\n     \"activations\",\n     \"constraints\",\n@@ -712,7 +708,14 @@ def _retrieve_class_or_fn(\n         # module name might not match the package structure\n         # (e.g. experimental symbols).\n         if module == \"keras\" or module.startswith(\"keras.\"):\n-            obj = tf_export.get_symbol_from_name(module + \".\" + name)\n+            api_name = module + \".\" + name\n+\n+            # Legacy internal APIs are stored in TF API naming dict\n+            # with `compat.v1` prefix\n+            if \"__internal__.legacy\" in api_name:\n+                api_name = \"compat.v1.\" + api_name\n+\n+            obj = tf_export.get_symbol_from_name(api_name)\n             if obj is not None:\n                 return obj\n \n\n@@ -312,6 +312,38 @@ class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertIs(layers[0].activation, layers[1].activation)\n         self.assertIs(new_layers[0].activation, new_layers[1].activation)\n \n+    def test_legacy_internal_object(self):\n+        from keras.layers.rnn.legacy_cells import (\n+            LSTMCell,  # pylint: disable=C6204\n+        )\n+\n+        # tf.nn.rnn_cell.LSTMCell belongs to keras.__internal__.legacy namespace\n+        cell = LSTMCell(32)\n+        x = keras.Input((None, 5))\n+        layer = keras.layers.RNN(cell)\n+        y = layer(x)\n+        model = keras.models.Model(x, y)\n+        model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n+\n+        x_in = np.random.random((3, 5, 5))\n+        y_out_1 = model.predict(x_in)\n+        weights = model.get_weights()\n+\n+        # serialize and deserialize\n+        config = serialization_lib.serialize_keras_object(layer)\n+        layer = serialization_lib.deserialize_keras_object(\n+            config,\n+            custom_objects={\"LSTMCell\": LSTMCell},\n+        )\n+\n+        # Restore RNN cell into model with weights\n+        y = layer(x)\n+        model = keras.models.Model(x, y)\n+        model.set_weights(weights)\n+        y_out_2 = model.predict(x_in)\n+\n+        self.assertAllClose(y_out_1, y_out_2, atol=1e-5)\n+\n \n @test_utils.run_v2_only\n class BackwardsCompatibilityTest(tf.test.TestCase, parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d6006954c5c27dce69c006b120eb3d8da51d792e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 3861 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,8 +49,12 @@ class _UnwrapPreventer:\n def _is_all_finite(grads):\n     \"\"\"Returns a scalar boolean tensor indicating if all gradients are\n     finite.\"\"\"\n+    def raw_values(g):\n+        return g.values if isinstance(g, tf.IndexedSlices) else g\n+\n     is_finite_per_grad = [\n-        tf.reduce_all(tf.math.is_finite(g)) for g in grads if g is not None\n+        tf.reduce_all(tf.math.is_finite(raw_values(g)))\n+        for g in grads if g is not None\n     ]\n     return tf.reduce_all(is_finite_per_grad)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d08241c28ffe9a6371f1d096568108d79b8d71a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 55 | Churn Cumulative: 916 | Contributors (this commit): 23 | Commits (past 90d): 2 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -199,10 +199,51 @@ def get_ylabels():\n     - https://github.com/keras-team/keras/issues/12072#issuecomment-458154097\n     - https://martin-thoma.com/nlp-reuters/\n     \"\"\"\n-    return ('cocoa','grain', 'veg-oil', 'earn', 'acq', 'wheat', 'copper',\n-        'housing', 'money-supply', 'coffee', 'sugar', 'trade', 'reserves',\n-        'ship', 'cotton', 'carcass', 'crude', 'nat-gas', 'cpi', 'money-fx',\n-        'interest', 'gnp', 'meal-feed', 'alum', 'oilseed', 'gold', 'tin',\n-        'strategic-metal', 'livestock', 'retail', 'ipi', 'iron-steel', 'rubber',\n-        'heat', 'jobs', 'lei', 'bop', 'zinc', 'orange', 'pet-chem', 'dlr',\n-        'gas', 'silver', 'wpi', 'hog', 'lead')\n\\ No newline at end of file\n+    return (\n+        \"cocoa\",\n+        \"grain\",\n+        \"veg-oil\",\n+        \"earn\",\n+        \"acq\",\n+        \"wheat\",\n+        \"copper\",\n+        \"housing\",\n+        \"money-supply\",\n+        \"coffee\",\n+        \"sugar\",\n+        \"trade\",\n+        \"reserves\",\n+        \"ship\",\n+        \"cotton\",\n+        \"carcass\",\n+        \"crude\",\n+        \"nat-gas\",\n+        \"cpi\",\n+        \"money-fx\",\n+        \"interest\",\n+        \"gnp\",\n+        \"meal-feed\",\n+        \"alum\",\n+        \"oilseed\",\n+        \"gold\",\n+        \"tin\",\n+        \"strategic-metal\",\n+        \"livestock\",\n+        \"retail\",\n+        \"ipi\",\n+        \"iron-steel\",\n+        \"rubber\",\n+        \"heat\",\n+        \"jobs\",\n+        \"lei\",\n+        \"bop\",\n+        \"zinc\",\n+        \"orange\",\n+        \"pet-chem\",\n+        \"dlr\",\n+        \"gas\",\n+        \"silver\",\n+        \"wpi\",\n+        \"hog\",\n+        \"lead\",\n+    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5696b5ab7dd401ca15b7e6ffba17ef05b1bb012a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 122 | Churn Cumulative: 17154 | Contributors (this commit): 46 | Commits (past 90d): 12 | Contributors (cumulative): 58 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5574,6 +5574,42 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     return -tf.reduce_sum(target * tf.math.log(output), axis)\n \n \n+@keras_export(\"keras.backend.categorical_focal_crossentropy\")\n+@tf.__internal__.dispatch.add_dispatch_support\n+@doc_controls.do_not_generate_docs\n+def categorical_focal_crossentropy(\n+        target,\n+        output,\n+        alpha=0.25,\n+        gamma=2.0,\n+        from_logits=False,\n+        axis=-1,\n+):\n+    \n+    output, from_logits = _get_logits(\n+        output, from_logits, \"Softmax\", \"categorical_focal_crossentropy\"\n+    )\n+\n+    output = tf.__internal__.smart_cond.smart_cond(\n+        from_logits,\n+        lambda: softmax(output),\n+        lambda: output,\n+    )\n+\n+    epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n+    output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)\n+\n+    cce = -target * tf.math.log(output)\n+\n+    # Calculate factors\n+    modulating_factor = tf.pow(1.0 - output, gamma)\n+    weighting_factor = tf.multiply(modulating_factor, alpha)\n+\n+    # Apply weighting factor\n+    focal_cce = tf.multiply(weighting_factor, cce)\n+    focal_cce = tf.reduce_sum(focal_cce, axis=axis)\n+    return focal_cce\n+\n @keras_export(\"keras.backend.sparse_categorical_crossentropy\")\n @tf.__internal__.dispatch.add_dispatch_support\n @doc_controls.do_not_generate_docs\n\n@@ -921,6 +921,40 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n             axis=axis,\n         )\n \n+@keras_export(\"keras.losses.CategoricalFocalCrossentropy\")\n+class CategoricalFocalCrossentropy(LossFunctionWrapper):\n+    def __init__(\n+            self,\n+            alpha=0.25,\n+            gamma=2.0,\n+            from_logits=False,\n+            label_smoothing=0.0,\n+            axis=-1,\n+            reduction=losses_utils.ReductionV2.AUTO,\n+            name=\"categorical_focal_crossentropy\",\n+    ):\n+        \"\"\"Initializes `CategoricalFocalCrossentropy` instance.\"\"\"\n+        super().__init__(\n+            categorical_focal_crossentropy,\n+            alpha=alpha,\n+            gamma=gamma,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+        self.from_logits = from_logits\n+        self.alpha = alpha\n+        self.gamma = gamma\n+\n+    def get_config(self):\n+        config = {\n+            \"alpha\": self.alpha,\n+            \"gamma\": self.gamma,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n \n @keras_export(\"keras.losses.SparseCategoricalCrossentropy\")\n class SparseCategoricalCrossentropy(LossFunctionWrapper):\n@@ -2025,6 +2059,58 @@ def _ragged_tensor_categorical_crossentropy(\n     return _ragged_tensor_apply_loss(fn, y_true, y_pred)\n \n \n+@keras_export(\n+    \"keras.metrics.categorical_focal_crossentropy\",\n+    \"keras.losses.categorical_focal_crossentropy\",\n+)\n+@tf.__internal__.dispatch.add_dispatch_support\n+def categorical_focal_crossentropy(\n+        y_true,\n+        y_pred,\n+        alpha=0.25,\n+        gamma=2.0,\n+        from_logits=False,\n+        label_smoothing=0.0,\n+        axis=-1,\n+):\n+\n+    if isinstance(axis, bool):\n+        raise ValueError(\n+            \"`axis` must be of type `int`. \"\n+            f\"Received: axis={axis} of type {type(axis)}\"\n+        )\n+    y_pred = tf.convert_to_tensor(y_pred)\n+    y_true = tf.cast(y_true, y_pred.dtype)\n+    label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n+\n+    if y_pred.shape[-1] == 1:\n+        warnings.warn(\n+            \"In loss categorical_focal_crossentropy, expected \"\n+            \"y_pred.shape to be (batch_size, num_classes) \"\n+            f\"with num_classes > 1. Received: y_pred.shape={y_pred.shape}. \"\n+            \"Consider using 'binary_crossentropy' if you only have 2 classes.\",\n+            SyntaxWarning,\n+            stacklevel=2,\n+        )\n+\n+    def _smooth_labels():\n+        num_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n+        return y_true * (1.0 - label_smoothing) + (\n+                label_smoothing / num_classes\n+        )\n+\n+    y_true = tf.__internal__.smart_cond.smart_cond(\n+        label_smoothing, _smooth_labels, lambda: y_true\n+    )\n+\n+    return backend.categorical_focal_crossentropy(\n+            target=y_true,\n+            output=y_pred,\n+            alpha=alpha,\n+            gamma=gamma,\n+            from_logits=from_logits,\n+            axis=axis, )\n+\n @keras_export(\n     \"keras.metrics.sparse_categorical_crossentropy\",\n     \"keras.losses.sparse_categorical_crossentropy\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#40e547feaf8a505e559fac6818780c985963f8cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 6274 | Contributors (this commit): 33 | Commits (past 90d): 8 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2111,6 +2111,26 @@ def categorical_focal_crossentropy(\n             from_logits=from_logits,\n             axis=axis, )\n \n+@dispatch.dispatch_for_types(categorical_focal_crossentropy, tf.RaggedTensor)\n+def _ragged_tensor_categorical_focal_crossentropy(\n+    y_true,\n+    y_pred,\n+    alpha=0.25,\n+    gamma=2.0,\n+    from_logits=False,\n+    label_smoothing=0.0,\n+    axis=-1,\n+):\n+    fn = functools.partial(\n+        categorical_focal_crossentropy,\n+        alpha=alpha,\n+        gamma=gamma,\n+        from_logits=from_logits,\n+        label_smoothing=label_smoothing,\n+        axis=axis,\n+    )\n+    return _ragged_tensor_apply_loss(fn, y_true, y_pred)\n+\n @keras_export(\n     \"keras.metrics.sparse_categorical_crossentropy\",\n     \"keras.losses.sparse_categorical_crossentropy\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b1fcf1b34c988a315c6279b321c8f7195e70bb32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 929 | Contributors (this commit): 23 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -164,8 +164,9 @@ def load_data(\n @keras_export(\"keras.datasets.reuters.get_word_index\")\n def get_word_index(path=\"reuters_word_index.json\"):\n     \"\"\"Retrieves a dict mapping words to their index in the Reuters dataset.\n+\n     Actual word indices starts from 3, with 3 indices reserved for:\n-    0 (padding), 1 (start), 2 (oof).\n+    0 (padding), 1 (start), 2 (oov).\n \n     E.g. word index of 'the' is 1, but the in the actual training data, the\n     index of 'the' will be 1 + 3 = 4. Vice versa, to translate word indices in\n@@ -191,13 +192,13 @@ def get_word_index(path=\"reuters_word_index.json\"):\n \n \n @keras_export(\"keras.datasets.reuters.get_ylabels\")\n-def get_ylabels():\n-    \"\"\"Returns the y label as a list of strings with indices matching\n-    training data.\n+def get_label_names():\n+    \"\"\"Returns labels as a list of strings with indices matching training data.\n \n     See references from:\n-    - https://github.com/keras-team/keras/issues/12072#issuecomment-458154097\n-    - https://martin-thoma.com/nlp-reuters/\n+\n+    - [Github Discussion](https://github.com/keras-team/keras/issues/12072#issuecomment-458154097)\n+    - [Blog Introduction to Reuters Dataset](https://martin-thoma.com/nlp-reuters/)\n     \"\"\"\n     return (\n         \"cocoa\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d85556ed9304a4b677f137f9e99b68eb408a08c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 933 | Contributors (this commit): 23 | Commits (past 90d): 4 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -197,8 +197,8 @@ def get_label_names():\n \n     See references from:\n \n-    - [Github Discussion](https://github.com/keras-team/keras/issues/12072#issuecomment-458154097)\n-    - [Blog Introduction to Reuters Dataset](https://martin-thoma.com/nlp-reuters/)\n+    - [Github Discussion](https://github.com/keras-team/keras/issues/12072)\n+    - [Reuters Dataset](https://martin-thoma.com/nlp-reuters/)\n     \"\"\"\n     return (\n         \"cocoa\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d29df56478a93f68dd75384bdcab88d871c8681e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 938 | Contributors (this commit): 23 | Commits (past 90d): 5 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -191,13 +191,12 @@ def get_word_index(path=\"reuters_word_index.json\"):\n         return json.load(f)\n \n \n-@keras_export(\"keras.datasets.reuters.get_ylabels\")\n+@keras_export(\"keras.datasets.reuters.get_label_names\")\n def get_label_names():\n     \"\"\"Returns labels as a list of strings with indices matching training data.\n \n-    See references from:\n+    Reference:\n \n-    - [Github Discussion](https://github.com/keras-team/keras/issues/12072)\n     - [Reuters Dataset](https://martin-thoma.com/nlp-reuters/)\n     \"\"\"\n     return (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d271cf4b182355da9fc83cd2d3d4d7574472f67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 25 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 27 | Churn Cumulative: 1939 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -44,35 +44,10 @@ def get_var(val, dtype, name=None):\n     return tf.Variable(val, dtype=dtype, name=name)\n \n \n-def set_cpu_logical_devices_to_at_least(num):\n-    \"\"\"Create cpu logical devices of at least a given number.\"\"\"\n-    physical_devices = tf.config.list_physical_devices(\"CPU\")\n-    if not physical_devices:\n-        raise RuntimeError(\"No CPU found\")\n-    if len(physical_devices) >= num:\n-        return\n-    # By default each physical device corresponds to one logical device. We\n-    # create multiple logical devices for the last physical device so that we\n-    # have `num` logical devices.\n-    num = num - len(physical_devices) + 1\n-    logical_devices = []\n-    for _ in range(num):\n-        logical_devices.append(tf.config.LogicalDeviceConfiguration())\n-    # Create logical devices from the last device since sometimes the first GPU\n-    # is the primary graphic card and may have less memory available.\n-    tf.config.set_logical_device_configuration(\n-        physical_devices[-1], logical_devices\n-    )\n-\n-\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(mode=[\"graph\", \"eager\"])\n )\n class AutoCastVariableTest(tf.test.TestCase, parameterized.TestCase):\n-    def setUp(self):\n-        set_cpu_logical_devices_to_at_least(3)\n-        super().setUp()\n-\n     @tf.__internal__.distribute.combinations.generate(maybe_distribute)\n     def test_read(self, distribution):\n         with distribution.scope():\n\n@@ -49,6 +49,8 @@ from keras.testing_infra import test_utils\n def create_mirrored_strategy():\n     # The test creates two virtual CPUs, and we use both of them to test with\n     # multiple devices.\n+    # pylint: disable=protected-access\n+    tf.distribute.MirroredStrategy._collective_key_base += 1\n     return tf.distribute.MirroredStrategy([\"cpu:0\", \"cpu:1\"])\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ae1ae70024b548fc2aee47c976ca4c30530157f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 302 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 15 | Complexity Δ (Sum/Max): 24/13 | Churn Δ: 302 | Churn Cumulative: 302 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,133 @@\n+# Copyright 2023 The Keras Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras.layers.rnn import Wrapper\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+# Adapted from TF-Addons implementation\n+@keras_export(\"keras.layers.SpectralNormalization\", v1=[])\n+class SpectralNormalization(Wrapper):\n+    \"\"\"Performs spectral normalization on weights.\n+    This wrapper controls the Lipschitz constant of the layer by\n+    constraining its spectral norm, which can stabilize the training of GANs.\n+    See [Spectral Normalization for GAN](https://arxiv.org/abs/1802.05957).\n+    Wrap `tf.keras.layers.Conv2D`:\n+    >>> x = np.random.rand(1, 10, 10, 1)\n+    >>> conv2d = SpectralNormalization(tf.keras.layers.Conv2D(2, 2))\n+    >>> y = conv2d(x)\n+    >>> y.shape\n+    TensorShape([1, 9, 9, 2])\n+    Wrap `tf.keras.layers.Dense`:\n+    >>> x = np.random.rand(1, 10, 10, 1)\n+    >>> dense = SpectralNormalization(tf.keras.layers.Dense(10))\n+    >>> y = dense(x)\n+    >>> y.shape\n+    TensorShape([1, 10, 10, 10])\n+    Args:\n+      layer: A `tf.keras.layers.Layer` instance that\n+        has either `kernel` or `embeddings` attribute.\n+      power_iterations: `int`, the number of iterations during normalization.\n+    Raises:\n+      AssertionError: If not initialized with a `Layer` instance.\n+      ValueError: If initialized with negative `power_iterations`.\n+      AttributeError: If `layer` does not has `kernel` or `embeddings`\n+        attribute.\n+    \"\"\"\n+\n+    def __init__(self, layer, power_iterations=1, **kwargs):\n+        super().__init__(layer, **kwargs)\n+        if power_iterations <= 0:\n+            raise ValueError(\n+                \"`power_iterations` should be greater than zero, got \"\n+                \"`power_iterations={}`\".format(power_iterations)\n+            )\n+        self.power_iterations = power_iterations\n+        self._initialized = False\n+\n+    def build(self, input_shape):\n+        \"\"\"Build `Layer`\"\"\"\n+        super().build(input_shape)\n+        input_shape = tf.TensorShape(input_shape)\n+        self.input_spec = tf.keras.layers.InputSpec(\n+            shape=[None] + input_shape[1:]\n+        )\n+\n+        if hasattr(self.layer, \"kernel\"):\n+            self.w = self.layer.kernel\n+        elif hasattr(self.layer, \"embeddings\"):\n+            self.w = self.layer.embeddings\n+        else:\n+            raise AttributeError(\n+                \"{} object has no attribute 'kernel' nor \"\n+                \"'embeddings'\".format(type(self.layer).__name__)\n+            )\n+\n+        self.w_shape = self.w.shape.as_list()\n+\n+        self.u = self.add_weight(\n+            shape=(1, self.w_shape[-1]),\n+            initializer=tf.initializers.TruncatedNormal(stddev=0.02),\n+            trainable=False,\n+            name=\"sn_u\",\n+            dtype=self.w.dtype,\n+        )\n+\n+    def call(self, inputs, training=None):\n+        \"\"\"Call `Layer`\"\"\"\n+        if training is None:\n+            training = tf.keras.backend.learning_phase()\n+\n+        if training:\n+            self.normalize_weights()\n+\n+        output = self.layer(inputs)\n+        return output\n+\n+    def compute_output_shape(self, input_shape):\n+        return tf.TensorShape(\n+            self.layer.compute_output_shape(input_shape).as_list()\n+        )\n+\n+    def normalize_weights(self):\n+        \"\"\"Generate spectral normalized weights.\n+        This method will update the value of `self.w` with the\n+        spectral normalized value, so that the layer is ready for `call()`.\n+        \"\"\"\n+\n+        w = tf.reshape(self.w, [-1, self.w_shape[-1]])\n+        u = self.u\n+\n+        # check for zeroes weights\n+        if not tf.reduce_all(tf.equal(w, 0.0)):\n+            for _ in range(self.power_iterations):\n+                v = tf.math.l2_normalize(tf.matmul(u, w, transpose_b=True))\n+                u = tf.math.l2_normalize(tf.matmul(v, w))\n+            u = tf.stop_gradient(u)\n+            v = tf.stop_gradient(v)\n+            sigma = tf.matmul(tf.matmul(v, w), u, transpose_b=True)\n+            self.u.assign(tf.cast(u, self.u.dtype))\n+            self.w.assign(\n+                tf.cast(tf.reshape(self.w / sigma, self.w_shape), self.w.dtype)\n+            )\n+\n+    def get_config(self):\n+        config = {\"power_iterations\": self.power_iterations}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,169 @@\n+# Copyright 2023 The Keras Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+from absl.testing import parameterized\n+\n+import keras\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n+\n+\n+class SpectralNormalizationTest(test_combinations.TestCase):\n+    @test_combinations.run_all_keras_modes\n+    def test_basic_spectralnorm(self):\n+        test_utils.layer_test(\n+            keras.layers.SpectralNormalization,\n+            kwargs={\"layer\": tf.keras.layers.Dense(2), \"input_shape\": (3, 4)},\n+            input_data=tf.random.uniform((10, 3, 4)),\n+        )\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_from_to_config(self):\n+        base_layer = tf.keras.layers.Dense(1)\n+        sn = keras.layers.SpectralNormalization(base_layer)\n+        config = sn.get_config()\n+\n+        new_sn = keras.layers.SpectralNormalization.from_config(config)\n+        self.assertEqual(sn.power_iterations, new_sn.power_iterations)\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_save_load_model(self):\n+        base_layer = tf.keras.layers.Dense(1)\n+        input_shape = [1]\n+\n+        inputs = tf.keras.layers.Input(shape=input_shape)\n+        sn_layer = keras.layers.SpectralNormalization(base_layer)\n+        model = tf.keras.models.Sequential(layers=[inputs, sn_layer])\n+\n+        # initialize model\n+        model.predict(tf.random.uniform((2, 1)))\n+\n+        model.save(\"test.h5\")\n+        new_model = tf.keras.models.load_model(\"test.h5\")\n+\n+        self.assertEqual(\n+            model.layers[0].get_config(), new_model.layers[0].get_config()\n+        )\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_normalization(self):\n+        inputs = tf.keras.layers.Input(shape=[2, 2, 1])\n+\n+        base_layer = tf.keras.layers.Conv2D(\n+            1, (2, 2), kernel_initializer=tf.constant_initializer(value=2)\n+        )\n+        sn_layer = keras.layers.SpectralNormalization(base_layer)\n+        model = tf.keras.models.Sequential(layers=[inputs, sn_layer])\n+\n+        weights = tf.squeeze(model.layers[0].w.numpy())\n+        # This wrapper normalizes weights by the maximum eigen value\n+        eigen_val, _ = tf.linalg.eig(weights)\n+        weights_normalized = weights / tf.reduce_max(eigen_val)\n+\n+        for training in [False, True]:\n+            _ = model(\n+                tf.constant(tf.ones((1, 2, 2, 1), dtype=tf.float32)),\n+                training=training,\n+            )\n+            if training:\n+                w = weights_normalized\n+            else:\n+                w = weights\n+            self.assertAllClose(w, tf.squeeze(model.layers[0].w.numpy()))\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_apply_layer(self):\n+        images = tf.ones((1, 2, 2, 1))\n+        sn_wrapper = keras.layers.SpectralNormalization(\n+            tf.keras.layers.Conv2D(\n+                1, [2, 2], kernel_initializer=tf.constant_initializer(value=1)\n+            ),\n+            input_shape=(2, 2, 1),\n+        )\n+\n+        result = sn_wrapper(images, training=False)\n+        result_train = sn_wrapper(images, training=True)\n+        expected_output = tf.constant([[[[4.0]]]], dtype=tf.float32)\n+\n+        self.assertAllClose(result, expected_output)\n+        # max eigen value of 2x2 matrix of ones is 2\n+        self.assertAllClose(result_train, expected_output / 2)\n+        self.assertTrue(hasattr(sn_wrapper, \"u\"))\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_no_layer(self):\n+        images = tf.random.uniform((2, 4, 43))\n+        with self.assertRaises(AssertionError):\n+            keras.layers.SpectralNormalization(images)\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_no_kernel(self):\n+        with self.assertRaises(AttributeError):\n+            keras.layers.SpectralNormalization(\n+                tf.keras.layers.MaxPooling2D(2, 2)\n+            ).build((2, 2))\n+\n+    @test_combinations.run_all_keras_modes\n+    @parameterized.parameters(\n+        [\n+            (lambda: tf.keras.layers.Dense(2), [3, 2]),\n+            (\n+                lambda: tf.keras.layers.Conv2D(3, (2, 2), padding=\"same\"),\n+                [4, 4, 3],\n+            ),\n+            (lambda: tf.keras.layers.Embedding(2, 10), [2]),\n+        ],\n+    )\n+    def test_model_build(self, base_layer_fn, input_shape):\n+        inputs = tf.keras.layers.Input(shape=input_shape)\n+        base_layer = base_layer_fn()\n+        sn_layer = keras.layers.SpectralNormalization(base_layer)\n+        model = tf.keras.models.Sequential(layers=[inputs, sn_layer])\n+        model.build()\n+        self.assertTrue(hasattr(model.layers[0], \"u\"))\n+\n+    @test_combinations.run_all_keras_modes\n+    @parameterized.parameters(\n+        [\n+            (lambda: tf.keras.layers.Dense(2), [3, 2], [3, 2]),\n+            (\n+                lambda: tf.keras.layers.Conv2D(3, (2, 2), padding=\"same\"),\n+                [4, 4, 3],\n+                [4, 4, 3],\n+            ),\n+            (lambda: tf.keras.layers.Embedding(2, 10), [2], [2, 10]),\n+        ],\n+    )\n+    def test_model_fit(self, base_layer_fn, input_shape, output_shape):\n+        inputs = tf.keras.layers.Input(shape=input_shape)\n+        base_layer = base_layer_fn()\n+\n+        sn_layer = keras.layers.SpectralNormalization(base_layer)\n+        model = tf.keras.models.Sequential(layers=[inputs, sn_layer])\n+        model.add(tf.keras.layers.Activation(\"relu\"))\n+\n+        model.compile(\n+            optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n+            loss=\"mse\",\n+        )\n+        model.fit(\n+            tf.random.uniform((2, *input_shape)),\n+            tf.random.uniform((2, *output_shape)),\n+            epochs=3,\n+            batch_size=10,\n+            verbose=0,\n+        )\n+        self.assertTrue(hasattr(model.layers[0], \"u\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
