{"custom_id": "scrapy#036f3e562716aaf67a4d0ff1c8011281394ef240", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2316 | Lines Deleted: 656 | Files Changed: 74 | Hunks: 429 | Methods Changed: 384 | Complexity Δ (Sum/Max): 247/51 | Churn Δ: 2972 | Churn Cumulative: 48545 | Contributors (this commit): 179 | Commits (past 90d): 154 | Contributors (cumulative): 944 | DMM Complexity: 0.75\n\nDIFF:\n@@ -34,6 +34,10 @@ class QPSSpider(Spider):\n         elif self.download_delay is not None:\n             self.download_delay = float(self.download_delay)\n \n+    async def start(self):\n+        for item_or_request in self.start_requests():\n+            yield item_or_request\n+\n     def start_requests(self):\n         url = self.benchurl\n         if self.latency is not None:\n\n@@ -13,9 +13,7 @@ from scrapy.linkextractors import LinkExtractor\n \n if TYPE_CHECKING:\n     import argparse\n-    from collections.abc import Iterable\n-\n-    from scrapy import Request\n+    from collections.abc import AsyncIterator\n \n \n class Command(ScrapyCommand):\n@@ -61,10 +59,10 @@ class _BenchSpider(scrapy.Spider):\n     baseurl = \"http://localhost:8998\"\n     link_extractor = LinkExtractor()\n \n-    def start_requests(self) -> Iterable[Request]:\n+    async def start(self) -> AsyncIterator[Any]:\n         qargs = {\"total\": self.total, \"show\": self.show}\n         url = f\"{self.baseurl}?{urlencode(qargs, doseq=True)}\"\n-        return [scrapy.Request(url, dont_filter=True)]\n+        yield scrapy.Request(url, dont_filter=True)\n \n     def parse(self, response: Response) -> Any:\n         assert isinstance(response, TextResponse)\n\n@@ -80,10 +80,14 @@ class Command(ScrapyCommand):\n         assert self.crawler_process\n         spider_loader = self.crawler_process.spider_loader\n \n+        async def start(self):\n+            for request in conman.from_spider(self, result):\n+                yield request\n+\n         with set_environ(SCRAPY_CHECK=\"true\"):\n             for spidername in args or spider_loader.list():\n                 spidercls = spider_loader.load(spidername)\n-                spidercls.start_requests = lambda s: conman.from_spider(s, result)  # type: ignore[assignment,method-assign,return-value]\n+                spidercls.start = start  # type: ignore[assignment,method-assign,return-value]\n \n                 tested_methods = conman.tested_methods_from_spidercls(spidercls)\n                 if opts.list:\n@@ -101,10 +105,10 @@ class Command(ScrapyCommand):\n                     for method in sorted(methods):\n                         print(f\"  * {method}\")\n             else:\n-                start = time.time()\n+                start_time = time.time()\n                 self.crawler_process.start()\n                 stop = time.time()\n \n                 result.printErrors()\n-                result.printSummary(start, stop)\n+                result.printSummary(start_time, stop)\n                 self.exitcode = int(not result.wasSuccessful())\n\n@@ -89,5 +89,11 @@ class Command(ScrapyCommand):\n             spidercls = spider_loader.load(opts.spider)\n         else:\n             spidercls = spidercls_for_request(spider_loader, request, spidercls)\n-        self.crawler_process.crawl(spidercls, start_requests=lambda: [request])\n+\n+        async def start(self):\n+            yield request\n+\n+        spidercls.start = start  # type: ignore[method-assign,attr-defined]\n+\n+        self.crawler_process.crawl(spidercls)\n         self.crawler_process.start()\n\n@@ -22,7 +22,7 @@ from scrapy.utils.spider import spidercls_for_request\n \n if TYPE_CHECKING:\n     import argparse\n-    from collections.abc import AsyncGenerator, Coroutine, Iterable\n+    from collections.abc import AsyncGenerator, AsyncIterator, Coroutine, Iterable\n \n     from twisted.python.failure import Failure\n \n@@ -258,11 +258,11 @@ class Command(BaseRunSpiderCommand):\n             if not self.spidercls:\n                 logger.error(\"Unable to find spider for: %(url)s\", {\"url\": url})\n \n-        def _start_requests(spider: Spider) -> Iterable[Request]:\n+        async def start(spider: Spider) -> AsyncIterator[Any]:\n             yield self.prepare_request(spider, Request(url), opts)\n \n         if self.spidercls:\n-            self.spidercls.start_requests = _start_requests  # type: ignore[assignment,method-assign]\n+            self.spidercls.start = start  # type: ignore[assignment,method-assign]\n \n     def start_parsing(self, url: str, opts: argparse.Namespace) -> None:\n         assert self.crawler_process\n\n@@ -24,9 +24,9 @@ if TYPE_CHECKING:\n class Command(ScrapyCommand):\n     requires_project = False\n     default_settings = {\n+        \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n         \"KEEP_ALIVE\": True,\n         \"LOGSTATS_INTERVAL\": 0,\n-        \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n     }\n \n     def syntax(self) -> str:\n@@ -85,7 +85,7 @@ class Command(ScrapyCommand):\n         crawler._apply_settings()\n         # The Shell class needs a persistent engine in the crawler\n         crawler.engine = crawler._create_engine()\n-        crawler.engine.start()\n+        crawler.engine.start(_start_request_processing=False)\n \n         self._start_crawler_thread()\n \n\n@@ -9,6 +9,7 @@ from __future__ import annotations\n \n import logging\n from time import time\n+from traceback import format_exc\n from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n from twisted.internet.defer import Deferred, inlineCallbacks, succeed\n@@ -16,15 +17,19 @@ from twisted.internet.task import LoopingCall\n from twisted.python.failure import Failure\n \n from scrapy import signals\n-from scrapy.core.scraper import Scraper, _HandleOutputDeferred\n+from scrapy.core.scraper import Scraper\n from scrapy.exceptions import CloseSpider, DontCloseSpider, IgnoreRequest\n from scrapy.http import Request, Response\n+from scrapy.utils.defer import (\n+    deferred_f_from_coro_f,\n+    maybe_deferred_to_future,\n+)\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.reactor import CallLaterOnce\n \n if TYPE_CHECKING:\n-    from collections.abc import Callable, Generator, Iterable, Iterator\n+    from collections.abc import AsyncIterator, Callable, Generator\n \n     from scrapy.core.downloader import Downloader\n     from scrapy.core.scheduler import BaseScheduler\n@@ -40,17 +45,15 @@ logger = logging.getLogger(__name__)\n _T = TypeVar(\"_T\")\n \n \n-class Slot:\n+class _Slot:\n     def __init__(\n         self,\n-        start_requests: Iterable[Request],\n         close_if_idle: bool,\n         nextcall: CallLaterOnce[None],\n         scheduler: BaseScheduler,\n     ) -> None:\n         self.closing: Deferred[None] | None = None\n         self.inprogress: set[Request] = set()\n-        self.start_requests: Iterator[Request] | None = iter(start_requests)\n         self.close_if_idle: bool = close_if_idle\n         self.nextcall: CallLaterOnce[None] = nextcall\n         self.scheduler: BaseScheduler = scheduler\n@@ -78,6 +81,8 @@ class Slot:\n \n \n class ExecutionEngine:\n+    _SLOT_HEARTBEAT_INTERVAL: float = 5.0\n+\n     def __init__(\n         self,\n         crawler: Crawler,\n@@ -88,20 +93,25 @@ class ExecutionEngine:\n         self.signals: SignalManager = crawler.signals\n         assert crawler.logformatter\n         self.logformatter: LogFormatter = crawler.logformatter\n-        self.slot: Slot | None = None\n+        self._slot: _Slot | None = None\n         self.spider: Spider | None = None\n         self.running: bool = False\n         self.paused: bool = False\n-        self.scheduler_cls: type[BaseScheduler] = self._get_scheduler_class(\n-            crawler.settings\n-        )\n-        downloader_cls: type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n-        self.downloader: Downloader = downloader_cls(crawler)\n-        self.scraper: Scraper = Scraper(crawler)\n         self._spider_closed_callback: Callable[[Spider], Deferred[None] | None] = (\n             spider_closed_callback\n         )\n         self.start_time: float | None = None\n+        self._start: AsyncIterator[Any] | None = None\n+        downloader_cls: type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n+        try:\n+            self.scheduler_cls: type[BaseScheduler] = self._get_scheduler_class(\n+                crawler.settings\n+            )\n+            self.downloader: Downloader = downloader_cls(crawler)\n+            self.scraper: Scraper = Scraper(crawler)\n+        except Exception:\n+            self.close()\n+            raise\n \n     def _get_scheduler_class(self, settings: BaseSettings) -> type[BaseScheduler]:\n         from scrapy.core.scheduler import BaseScheduler\n@@ -114,22 +124,28 @@ class ExecutionEngine:\n             )\n         return scheduler_cls\n \n-    @inlineCallbacks\n-    def start(self) -> Generator[Deferred[Any], Any, None]:\n+    @deferred_f_from_coro_f\n+    async def start(self, _start_request_processing=True) -> None:\n         if self.running:\n             raise RuntimeError(\"Engine already running\")\n         self.start_time = time()\n-        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n+        await maybe_deferred_to_future(\n+            self.signals.send_catch_log_deferred(signal=signals.engine_started)\n+        )\n         self.running = True\n         self._closewait: Deferred[None] = Deferred()\n-        yield self._closewait\n+        if _start_request_processing:\n+            self._start_request_processing()\n+        await maybe_deferred_to_future(self._closewait)\n \n     def stop(self) -> Deferred[None]:\n         \"\"\"Gracefully stop the execution engine\"\"\"\n \n-        @inlineCallbacks\n-        def _finish_stopping_engine(_: Any) -> Generator[Deferred[Any], Any, None]:\n-            yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n+        @deferred_f_from_coro_f\n+        async def _finish_stopping_engine(_: Any) -> None:\n+            await maybe_deferred_to_future(\n+                self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n+            )\n             self._closewait.callback(None)\n \n         if not self.running:\n@@ -163,59 +179,85 @@ class ExecutionEngine:\n     def unpause(self) -> None:\n         self.paused = False\n \n-    def _next_request(self) -> None:\n-        if self.slot is None:\n-            return\n+    async def _process_start_next(self):\n+        \"\"\"Processes the next item or request from Spider.start().\n \n-        assert self.spider is not None  # typing\n-\n-        if self.paused:\n-            return\n-\n-        while (\n-            not self._needs_backout()\n-            and self._next_request_from_scheduler() is not None\n-        ):\n-            pass\n-\n-        if self.slot.start_requests is not None and not self._needs_backout():\n+        If a request, it is scheduled. If an item, it is sent to item\n+        pipelines.\n+        \"\"\"\n         try:\n-                request_or_item = next(self.slot.start_requests)\n-            except StopIteration:\n-                self.slot.start_requests = None\n-            except Exception:\n-                self.slot.start_requests = None\n+            item_or_request = await self._start.__anext__()\n+        except StopAsyncIteration:\n+            self._start = None\n+        except Exception as exception:\n+            self._start = None\n+            exception_traceback = format_exc()\n             logger.error(\n-                    \"Error while obtaining start requests\",\n+                f\"Error while reading start items and requests: {exception}.\\n{exception_traceback}\",\n                 exc_info=True,\n-                    extra={\"spider\": self.spider},\n             )\n         else:\n-                if isinstance(request_or_item, Request):\n-                    self.crawl(request_or_item)\n+            if not self.spider:\n+                return  # spider already closed\n+            if isinstance(item_or_request, Request):\n+                self.crawl(item_or_request)\n             else:\n-                    self.scraper.start_itemproc(request_or_item, response=None)\n+                self.scraper.start_itemproc(item_or_request, response=None)\n+                self._slot.nextcall.schedule()\n \n-        if self.spider_is_idle() and self.slot.close_if_idle:\n+    @deferred_f_from_coro_f\n+    async def _start_request_processing(self) -> None:\n+        \"\"\"Starts consuming Spider.start() output and sending scheduled\n+        requests.\"\"\"\n+        # Starts the processing of scheduled requests, as well as a periodic\n+        # call to that processing method for scenarios where the scheduler\n+        # reports having pending requests but returns none.\n+        assert self._slot is not None  # typing\n+        self._slot.nextcall.schedule()\n+        self._slot.heartbeat.start(self._SLOT_HEARTBEAT_INTERVAL)\n+\n+        while self._start and self.spider:\n+            await self._process_start_next()\n+            if not self.needs_backout():\n+                # Give room for the outcome of self._process_start_next() to be\n+                # processed before continuing with the next iteration.\n+                self._slot.nextcall.schedule()\n+                await self._slot.nextcall.wait()\n+\n+    def _start_scheduled_requests(self) -> None:\n+        if self._slot is None or self._slot.closing is not None or self.paused:\n+            return\n+\n+        while not self.needs_backout():\n+            if not self._start_scheduled_request():\n+                break\n+\n+        if self.spider_is_idle() and self._slot.close_if_idle:\n             self._spider_idle()\n \n-    def _needs_backout(self) -> bool:\n-        assert self.slot is not None  # typing\n+    def needs_backout(self) -> bool:\n+        \"\"\"Returns ``True`` if no more requests can be sent at the moment, or\n+        ``False`` otherwise.\n+\n+        See :ref:`start-requests-lazy` for an example.\n+        \"\"\"\n+        assert self._slot is not None  # typing\n         assert self.scraper.slot is not None  # typing\n         return (\n             not self.running\n-            or bool(self.slot.closing)\n+            or bool(self._slot.closing)\n             or self.downloader.needs_backout()\n             or self.scraper.slot.needs_backout()\n         )\n \n-    def _next_request_from_scheduler(self) -> Deferred[None] | None:\n-        assert self.slot is not None  # typing\n+    def _start_scheduled_request(self) -> bool:\n+        assert self._slot is not None  # typing\n         assert self.spider is not None  # typing\n \n-        request = self.slot.scheduler.next_request()\n+        request = self._slot.scheduler.next_request()\n         if request is None:\n-            return None\n+            self.signals.send_catch_log(signals.scheduler_empty)\n+            return False\n \n         d: Deferred[Response | Request] = self._download(request)\n         d.addBoth(self._handle_downloader_output, request)\n@@ -228,8 +270,8 @@ class ExecutionEngine:\n         )\n \n         def _remove_request(_: Any) -> None:\n-            assert self.slot\n-            self.slot.remove_request(request)\n+            assert self._slot\n+            self._slot.remove_request(request)\n \n         d2: Deferred[None] = d.addBoth(_remove_request)\n         d2.addErrback(\n@@ -239,7 +281,7 @@ class ExecutionEngine:\n                 extra={\"spider\": self.spider},\n             )\n         )\n-        slot = self.slot\n+        slot = self._slot\n         d2.addBoth(lambda _: slot.nextcall.schedule())\n         d2.addErrback(\n             lambda f: logger.info(\n@@ -248,13 +290,12 @@ class ExecutionEngine:\n                 extra={\"spider\": self.spider},\n             )\n         )\n-        return d2\n+        return True\n \n+    @inlineCallbacks\n     def _handle_downloader_output(\n         self, result: Request | Response | Failure, request: Request\n-    ) -> _HandleOutputDeferred | None:\n-        assert self.spider is not None  # typing\n-\n+    ) -> Generator[Deferred[Any], Any, None]:\n         if not isinstance(result, (Request, Response, Failure)):\n             raise TypeError(\n                 f\"Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}\"\n@@ -263,35 +304,35 @@ class ExecutionEngine:\n         # downloader middleware can return requests (for example, redirects)\n         if isinstance(result, Request):\n             self.crawl(result)\n-            return None\n+            return\n \n-        d = self.scraper.enqueue_scrape(result, request)\n-        d.addErrback(\n-            lambda f: logger.error(\n-                \"Error while enqueuing downloader output\",\n-                exc_info=failure_to_exc_info(f),\n+        try:\n+            yield self.scraper.enqueue_scrape(result, request)\n+        except Exception:\n+            assert self.spider is not None\n+            logger.error(\n+                \"Error while enqueuing scrape\",\n+                exc_info=True,\n                 extra={\"spider\": self.spider},\n             )\n-        )\n-        return d\n \n     def spider_is_idle(self) -> bool:\n-        if self.slot is None:\n+        if self._slot is None:\n             raise RuntimeError(\"Engine slot not assigned\")\n         if not self.scraper.slot.is_idle():  # type: ignore[union-attr]\n             return False\n         if self.downloader.active:  # downloader has pending requests\n             return False\n-        if self.slot.start_requests is not None:  # not all start requests are handled\n+        if self._start is not None:  # not all start requests are handled\n             return False\n-        return not self.slot.scheduler.has_pending_requests()\n+        return not self._slot.scheduler.has_pending_requests()\n \n     def crawl(self, request: Request) -> None:\n         \"\"\"Inject the request into the spider <-> downloader pipeline\"\"\"\n         if self.spider is None:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n         self._schedule_request(request)\n-        self.slot.nextcall.schedule()  # type: ignore[union-attr]\n+        self._slot.nextcall.schedule()  # type: ignore[union-attr]\n \n     def _schedule_request(self, request: Request) -> None:\n         request_scheduled_result = self.signals.send_catch_log(\n@@ -303,7 +344,7 @@ class ExecutionEngine:\n         for handler, result in request_scheduled_result:\n             if isinstance(result, Failure) and isinstance(result.value, IgnoreRequest):\n                 return\n-        if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n+        if not self._slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n             self.signals.send_catch_log(\n                 signals.request_dropped, request=request, spider=self.spider\n             )\n@@ -320,14 +361,14 @@ class ExecutionEngine:\n     def _downloaded(\n         self, result: Response | Request | Failure, request: Request\n     ) -> Deferred[Response] | Response | Failure:\n-        assert self.slot is not None  # typing\n-        self.slot.remove_request(request)\n+        assert self._slot is not None  # typing\n+        self._slot.remove_request(request)\n         return self.download(result) if isinstance(result, Request) else result\n \n     def _download(self, request: Request) -> Deferred[Response | Request]:\n-        assert self.slot is not None  # typing\n+        assert self._slot is not None  # typing\n \n-        self.slot.add_request(request)\n+        self._slot.add_request(request)\n \n         def _on_success(result: Response | Request) -> Response | Request:\n             if not isinstance(result, (Response, Request)):\n@@ -352,8 +393,8 @@ class ExecutionEngine:\n             return result\n \n         def _on_complete(_: _T) -> _T:\n-            assert self.slot is not None\n-            self.slot.nextcall.schedule()\n+            assert self._slot is not None\n+            self._slot.nextcall.schedule()\n             return _\n \n         assert self.spider is not None\n@@ -362,31 +403,28 @@ class ExecutionEngine:\n         dwld.addBoth(_on_complete)\n         return dwld\n \n-    @inlineCallbacks\n-    def open_spider(\n+    @deferred_f_from_coro_f\n+    async def open_spider(\n         self,\n         spider: Spider,\n-        start_requests: Iterable[Request] = (),\n         close_if_idle: bool = True,\n-    ) -> Generator[Deferred[Any], Any, None]:\n-        if self.slot is not None:\n+    ) -> None:\n+        if self._slot is not None:\n             raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n         logger.info(\"Spider opened\", extra={\"spider\": spider})\n-        nextcall = CallLaterOnce(self._next_request)\n-        scheduler = build_from_crawler(self.scheduler_cls, self.crawler)\n-        start_requests = yield self.scraper.spidermw.process_start_requests(\n-            start_requests, spider\n-        )\n-        self.slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n         self.spider = spider\n+        nextcall = CallLaterOnce(self._start_scheduled_requests)\n+        scheduler = build_from_crawler(self.scheduler_cls, self.crawler)\n+        self._slot = _Slot(close_if_idle, nextcall, scheduler)\n+        self._start = await self.scraper.spidermw.process_start(spider)\n         if hasattr(scheduler, \"open\") and (d := scheduler.open(spider)):\n-            yield d\n-        yield self.scraper.open_spider(spider)\n+            await maybe_deferred_to_future(d)\n+        await maybe_deferred_to_future(self.scraper.open_spider(spider))\n         assert self.crawler.stats\n         self.crawler.stats.open_spider(spider)\n-        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n-        self.slot.nextcall.schedule()\n-        self.slot.heartbeat.start(5)\n+        await maybe_deferred_to_future(\n+            self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n+        )\n \n     def _spider_idle(self) -> None:\n         \"\"\"\n@@ -415,17 +453,17 @@ class ExecutionEngine:\n \n     def close_spider(self, spider: Spider, reason: str = \"cancelled\") -> Deferred[None]:\n         \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n-        if self.slot is None:\n+        if self._slot is None:\n             raise RuntimeError(\"Engine slot not assigned\")\n \n-        if self.slot.closing is not None:\n-            return self.slot.closing\n+        if self._slot.closing is not None:\n+            return self._slot.closing\n \n         logger.info(\n             \"Closing spider (%(reason)s)\", {\"reason\": reason}, extra={\"spider\": spider}\n         )\n \n-        dfd = self.slot.close()\n+        dfd = self._slot.close()\n \n         def log_failure(msg: str) -> Callable[[Failure], None]:\n             def errback(failure: Failure) -> None:\n@@ -441,8 +479,8 @@ class ExecutionEngine:\n         dfd.addBoth(lambda _: self.scraper.close_spider())\n         dfd.addErrback(log_failure(\"Scraper close failure\"))\n \n-        if hasattr(self.slot.scheduler, \"close\"):\n-            dfd.addBoth(lambda _: cast(Slot, self.slot).scheduler.close(reason))\n+        if hasattr(self._slot.scheduler, \"close\"):\n+            dfd.addBoth(lambda _: cast(_Slot, self._slot).scheduler.close(reason))\n             dfd.addErrback(log_failure(\"Scheduler close failure\"))\n \n         dfd.addBoth(\n\n@@ -5,13 +5,16 @@ import logging\n from abc import abstractmethod\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, cast\n+from warnings import warn\n \n # working around https://github.com/sphinx-doc/sphinx/issues/10400\n from twisted.internet.defer import Deferred  # noqa: TC002\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.spiders import Spider  # noqa: TC001\n from scrapy.utils.job import job_dir\n from scrapy.utils.misc import build_from_crawler, load_object\n+from scrapy.utils.python import global_object_name\n \n if TYPE_CHECKING:\n     # requires queuelib >= 1.6.2\n@@ -50,18 +53,17 @@ class BaseSchedulerMeta(type):\n \n \n class BaseScheduler(metaclass=BaseSchedulerMeta):\n-    \"\"\"\n-    The scheduler component is responsible for storing requests received from\n-    the engine, and feeding them back upon request (also to the engine).\n+    \"\"\"The scheduler component is responsible for storing requests received\n+    from the engine, and feeding them back upon request (also to the engine).\n \n     The original sources of said requests are:\n \n-    * Spider: ``start_requests`` method, requests created for URLs in the ``start_urls`` attribute, request callbacks\n+    * Spider: ``start`` method, requests created for URLs in the ``start_urls`` attribute, request callbacks\n     * Spider middleware: ``process_spider_output`` and ``process_spider_exception`` methods\n     * Downloader middleware: ``process_request``, ``process_response`` and ``process_exception`` methods\n \n     The order in which the scheduler returns its stored requests (via the ``next_request`` method)\n-    plays a great part in determining the order in which those requests are downloaded.\n+    plays a great part in determining the order in which those requests are downloaded. See :ref:`request-order`.\n \n     The methods defined in this class constitute the minimal interface that the Scrapy engine will interact with.\n     \"\"\"\n@@ -126,20 +128,125 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n \n \n class Scheduler(BaseScheduler):\n+    \"\"\"Default scheduler.\n+\n+    Requests are stored into priority queues\n+    (:setting:`SCHEDULER_PRIORITY_QUEUE`) that sort requests by\n+    :attr:`~scrapy.http.Request.priority`.\n+\n+    By default, a single, memory-based priority queue is used for all requests.\n+    When using :setting:`JOBDIR`, a disk-based priority queue is also created,\n+    and only unserializable requests are stored in the memory-based priority\n+    queue. For a given priority value, requests in memory take precedence over\n+    requests in disk.\n+\n+    Each priority queue stores requests in separate internal queues, one per\n+    priority value. The memory priority queue uses\n+    :setting:`SCHEDULER_MEMORY_QUEUE` queues, while the disk priority queue\n+    uses :setting:`SCHEDULER_DISK_QUEUE` queues. The internal queues determine\n+    :ref:`request order <request-order>` when requests have the same priority.\n+    :ref:`Start requests <start-requests>` are stored into separate internal\n+    queues by default, and :ref:`ordered differently <start-request-order>`.\n+\n+    Duplicate requests are filtered out with an instance of\n+    :setting:`DUPEFILTER_CLASS`.\n+\n+    .. _request-order:\n+\n+    Request order\n+    =============\n+\n+    With default settings, pending requests are stored in a LIFO_ queue\n+    (:ref:`except for start requests <start-request-order>`). As a result,\n+    crawling happens in `DFO order`_, which is usually the most convenient\n+    crawl order. However, you can enforce :ref:`BFO <bfo>` or :ref:`a custom\n+    order <custom-request-order>` (:ref:`except for the first few requests\n+    <concurrency-v-order>`).\n+\n+    .. _LIFO: https://en.wikipedia.org/wiki/Stack_(abstract_data_type)\n+    .. _DFO order: https://en.wikipedia.org/wiki/Depth-first_search\n+\n+    .. _start-request-order:\n+\n+    Start request order\n+    -------------------\n+\n+    :ref:`Start requests <start-requests>` are sent in the order they are\n+    yielded from :meth:`~scrapy.Spider.start`, and given the same\n+    :attr:`~scrapy.http.Request.priority`, start requests take precedence over\n+    other requests.\n+\n+    You can set :setting:`SCHEDULER_START_MEMORY_QUEUE` and\n+    :setting:`SCHEDULER_START_DISK_QUEUE` to ``None`` to handle start requests\n+    the same as other requests when it comes to order and priority.\n+\n+\n+    .. _bfo:\n+\n+    Crawling in BFO order\n+    ---------------------\n+\n+    If you do want to crawl in `BFO order`_, you can do it by setting the\n+    following :ref:`settings <topics-settings>`:\n+\n+    | :setting:`DEPTH_PRIORITY` = ``1``\n+    | :setting:`SCHEDULER_DISK_QUEUE` = ``\"scrapy.squeues.PickleFifoDiskQueue\"``\n+    | :setting:`SCHEDULER_MEMORY_QUEUE` = ``\"scrapy.squeues.FifoMemoryQueue\"``\n+\n+    .. _BFO order: https://en.wikipedia.org/wiki/Breadth-first_search\n+\n+\n+    .. _custom-request-order:\n+\n+    Crawling in a custom order\n+    --------------------------\n+\n+    You can manually set :attr:`~scrapy.http.Request.priority` on requests to\n+    force a specific request order.\n+\n+\n+    .. _concurrency-v-order:\n+\n+    Concurrency affects order\n+    -------------------------\n+\n+    While pending requests are below the configured values of\n+    :setting:`CONCURRENT_REQUESTS`, :setting:`CONCURRENT_REQUESTS_PER_DOMAIN`\n+    or :setting:`CONCURRENT_REQUESTS_PER_IP`, those requests are sent\n+    concurrently.\n+\n+    As a result, the first few requests of a crawl may not follow the desired\n+    order. Lowering those settings to ``1`` enforces the desired order except\n+    for the very first request, but it significantly slows down the crawl as a\n+    whole.\n     \"\"\"\n-    Default Scrapy scheduler. This implementation also handles duplication\n-    filtering via the :setting:`dupefilter <DUPEFILTER_CLASS>`.\n \n-    This scheduler stores requests into several priority queues (defined by the\n-    :setting:`SCHEDULER_PRIORITY_QUEUE` setting). In turn, said priority queues\n-    are backed by either memory or disk based queues (respectively defined by the\n-    :setting:`SCHEDULER_MEMORY_QUEUE` and :setting:`SCHEDULER_DISK_QUEUE` settings).\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        dupefilter_cls = load_object(crawler.settings[\"DUPEFILTER_CLASS\"])\n+        return cls(\n+            dupefilter=build_from_crawler(dupefilter_cls, crawler),\n+            jobdir=job_dir(crawler.settings),\n+            dqclass=load_object(crawler.settings[\"SCHEDULER_DISK_QUEUE\"]),\n+            mqclass=load_object(crawler.settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n+            logunser=crawler.settings.getbool(\"SCHEDULER_DEBUG\"),\n+            stats=crawler.stats,\n+            pqclass=load_object(crawler.settings[\"SCHEDULER_PRIORITY_QUEUE\"]),\n+            crawler=crawler,\n+        )\n \n-    Request prioritization is almost entirely delegated to the priority queue. The only\n-    prioritization performed by this scheduler is using the disk-based queue if present\n-    (i.e. if the :setting:`JOBDIR` setting is defined) and falling back to the memory-based\n-    queue if a serialization error occurs. If the disk queue is not present, the memory one\n-    is used directly.\n+    def __init__(\n+        self,\n+        dupefilter: BaseDupeFilter,\n+        jobdir: str | None = None,\n+        dqclass: type[BaseQueue] | None = None,\n+        mqclass: type[BaseQueue] | None = None,\n+        logunser: bool = False,\n+        stats: StatsCollector | None = None,\n+        pqclass: type[ScrapyPriorityQueue] | None = None,\n+        crawler: Crawler | None = None,\n+    ):\n+        \"\"\"Initialize the scheduler.\n \n         :param dupefilter: An object responsible for checking and filtering duplicate requests.\n                         The value for the :setting:`DUPEFILTER_CLASS` setting is used by default.\n@@ -175,18 +282,6 @@ class Scheduler(BaseScheduler):\n         :param crawler: The crawler object corresponding to the current crawl.\n         :type crawler: :class:`scrapy.crawler.Crawler`\n         \"\"\"\n-\n-    def __init__(\n-        self,\n-        dupefilter: BaseDupeFilter,\n-        jobdir: str | None = None,\n-        dqclass: type[BaseQueue] | None = None,\n-        mqclass: type[BaseQueue] | None = None,\n-        logunser: bool = False,\n-        stats: StatsCollector | None = None,\n-        pqclass: type[ScrapyPriorityQueue] | None = None,\n-        crawler: Crawler | None = None,\n-    ):\n         self.df: BaseDupeFilter = dupefilter\n         self.dqdir: str | None = self._dqdir(jobdir)\n         self.pqclass: type[ScrapyPriorityQueue] | None = pqclass\n@@ -195,23 +290,22 @@ class Scheduler(BaseScheduler):\n         self.logunser: bool = logunser\n         self.stats: StatsCollector | None = stats\n         self.crawler: Crawler | None = crawler\n-\n-    @classmethod\n-    def from_crawler(cls, crawler: Crawler) -> Self:\n-        \"\"\"\n-        Factory method, initializes the scheduler with arguments taken from the crawl settings\n-        \"\"\"\n-        dupefilter_cls = load_object(crawler.settings[\"DUPEFILTER_CLASS\"])\n-        return cls(\n-            dupefilter=build_from_crawler(dupefilter_cls, crawler),\n-            jobdir=job_dir(crawler.settings),\n-            dqclass=load_object(crawler.settings[\"SCHEDULER_DISK_QUEUE\"]),\n-            mqclass=load_object(crawler.settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n-            logunser=crawler.settings.getbool(\"SCHEDULER_DEBUG\"),\n-            stats=crawler.stats,\n-            pqclass=load_object(crawler.settings[\"SCHEDULER_PRIORITY_QUEUE\"]),\n-            crawler=crawler,\n+        self._sdqclass: type[BaseQueue] | None = self._get_start_queue_cls(\n+            crawler, \"DISK\"\n         )\n+        self._smqclass: type[BaseQueue] | None = self._get_start_queue_cls(\n+            crawler, \"MEMORY\"\n+        )\n+\n+    def _get_start_queue_cls(\n+        self, crawler: Crawler | None, queue: str\n+    ) -> type[BaseQueue] | None:\n+        if crawler is None:\n+            return None\n+        cls = crawler.settings[f\"SCHEDULER_START_{queue}_QUEUE\"]\n+        if not cls:\n+            return None\n+        return load_object(cls)\n \n     def has_pending_requests(self) -> bool:\n         return len(self) > 0\n@@ -324,6 +418,21 @@ class Scheduler(BaseScheduler):\n         \"\"\"Create a new priority queue instance, with in-memory storage\"\"\"\n         assert self.crawler\n         assert self.pqclass\n+        try:\n+            return build_from_crawler(\n+                self.pqclass,\n+                self.crawler,\n+                downstream_queue_cls=self.mqclass,\n+                key=\"\",\n+                start_queue_cls=self._smqclass,\n+            )\n+        except TypeError:\n+            warn(\n+                f\"The __init__ method of {global_object_name(self.pqclass)} \"\n+                f\"does not support a `start_queue_cls` keyword-only \"\n+                f\"parameter.\",\n+                ScrapyDeprecationWarning,\n+            )\n             return build_from_crawler(\n                 self.pqclass,\n                 self.crawler,\n@@ -337,6 +446,22 @@ class Scheduler(BaseScheduler):\n         assert self.dqdir\n         assert self.pqclass\n         state = self._read_dqs_state(self.dqdir)\n+        try:\n+            q = build_from_crawler(\n+                self.pqclass,\n+                self.crawler,\n+                downstream_queue_cls=self.dqclass,\n+                key=self.dqdir,\n+                startprios=state,\n+                start_queue_cls=self._sdqclass,\n+            )\n+        except TypeError:\n+            warn(\n+                f\"The __init__ method of {global_object_name(self.pqclass)} \"\n+                f\"does not support a `start_queue_cls` keyword-only \"\n+                f\"parameter.\",\n+                ScrapyDeprecationWarning,\n+            )\n             q = build_from_crawler(\n                 self.pqclass,\n                 self.crawler,\n\n@@ -6,10 +6,10 @@ from __future__ import annotations\n import logging\n import warnings\n from collections import deque\n-from collections.abc import AsyncIterable, Iterator\n-from typing import TYPE_CHECKING, Any, TypeVar, Union, cast\n+from collections.abc import AsyncIterator\n+from typing import TYPE_CHECKING, Any, TypeVar, Union\n \n-from twisted.internet.defer import Deferred, inlineCallbacks\n+from twisted.internet.defer import Deferred, inlineCallbacks, maybeDeferred\n from twisted.python.failure import Failure\n \n from scrapy import Spider, signals\n@@ -22,10 +22,12 @@ from scrapy.exceptions import (\n )\n from scrapy.http import Request, Response\n from scrapy.utils.defer import (\n+    _defer_sleep,\n     aiter_errback,\n-    defer_fail,\n-    defer_succeed,\n+    deferred_f_from_coro_f,\n+    deferred_from_coro,\n     iter_errback,\n+    maybe_deferred_to_future,\n     parallel,\n     parallel_async,\n )\n@@ -46,9 +48,7 @@ logger = logging.getLogger(__name__)\n \n \n _T = TypeVar(\"_T\")\n-_ParallelResult = list[tuple[bool, Iterator[Any]]]\n-_HandleOutputDeferred = Deferred[Union[_ParallelResult, None]]\n-QueueTuple = tuple[Union[Response, Failure], Request, _HandleOutputDeferred]\n+QueueTuple = tuple[Union[Response, Failure], Request, Deferred[None]]\n \n \n class Slot:\n@@ -66,8 +66,9 @@ class Slot:\n \n     def add_response_request(\n         self, result: Response | Failure, request: Request\n-    ) -> _HandleOutputDeferred:\n-        deferred: _HandleOutputDeferred = Deferred()\n+    ) -> Deferred[None]:\n+        # this Deferred will be awaited in enqueue_scrape()\n+        deferred: Deferred[None] = Deferred()\n         self.queue.append((result, request, deferred))\n         if isinstance(result, Response):\n             self.active_size += max(len(result.body), self.MIN_RESPONSE_SIZE)\n@@ -76,9 +77,9 @@ class Slot:\n         return deferred\n \n     def next_response_request_deferred(self) -> QueueTuple:\n-        response, request, deferred = self.queue.popleft()\n+        result, request, deferred = self.queue.popleft()\n         self.active.add(request)\n-        return response, request, deferred\n+        return result, request, deferred\n \n     def finish_response(self, result: Response | Failure, request: Request) -> None:\n         self.active.remove(request)\n@@ -143,9 +144,10 @@ class Scraper:\n             assert self.crawler.spider\n             self.slot.closing.callback(self.crawler.spider)\n \n+    @inlineCallbacks\n     def enqueue_scrape(\n         self, result: Response | Failure, request: Request, spider: Spider | None = None\n-    ) -> _HandleOutputDeferred:\n+    ) -> Generator[Deferred[Any], Any, None]:\n         if spider is not None:\n             warnings.warn(\n                 \"Passing a 'spider' argument to Scraper.enqueue_scrape() is deprecated.\",\n@@ -156,103 +158,106 @@ class Scraper:\n         if self.slot is None:\n             raise RuntimeError(\"Scraper slot not assigned\")\n         dfd = self.slot.add_response_request(result, request)\n-\n-        def finish_scraping(_: _T) -> _T:\n-            assert self.slot is not None\n+        self._scrape_next()\n+        try:\n+            yield dfd\n+        except Exception:\n+            logger.error(\n+                \"Scraper bug processing %(request)s\",\n+                {\"request\": request},\n+                exc_info=True,\n+                extra={\"spider\": self.crawler.spider},\n+            )\n+        finally:\n             self.slot.finish_response(result, request)\n             self._check_if_closing()\n             self._scrape_next()\n-            return _\n-\n-        dfd.addBoth(finish_scraping)\n-        dfd.addErrback(\n-            lambda f: logger.error(\n-                \"Scraper bug processing %(request)s\",\n-                {\"request\": request},\n-                exc_info=failure_to_exc_info(f),\n-                extra={\"spider\": self.crawler.spider},\n-            )\n-        )\n-        self._scrape_next()\n-        return dfd\n \n     def _scrape_next(self) -> None:\n         assert self.slot is not None  # typing\n         while self.slot.queue:\n-            response, request, deferred = self.slot.next_response_request_deferred()\n-            self._scrape(response, request).chainDeferred(deferred)\n+            result, request, deferred = self.slot.next_response_request_deferred()\n+            self._scrape(result, request).chainDeferred(deferred)\n \n-    def _scrape(\n-        self, result: Response | Failure, request: Request\n-    ) -> _HandleOutputDeferred:\n-        \"\"\"\n-        Handle the downloaded response or failure through the spider callback/errback\n-        \"\"\"\n+    @deferred_f_from_coro_f\n+    async def _scrape(self, result: Response | Failure, request: Request) -> None:\n+        \"\"\"Handle the downloaded response or failure through the spider callback/errback.\"\"\"\n         if not isinstance(result, (Response, Failure)):\n             raise TypeError(\n                 f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\"\n             )\n-        dfd: Deferred[Iterable[Any] | AsyncIterable[Any]] = self._scrape2(\n-            result, request\n-        )  # returns spider's processed output\n-        dfd.addErrback(self.handle_spider_error, request, result)\n-        dfd2: _HandleOutputDeferred = dfd.addCallback(\n-            self.handle_spider_output, request, cast(Response, result)\n-        )\n-        return dfd2\n \n-    def _scrape2(\n-        self, result: Response | Failure, request: Request\n-    ) -> Deferred[Iterable[Any] | AsyncIterable[Any]]:\n-        \"\"\"\n-        Handle the different cases of request's result been a Response or a Failure\n-        \"\"\"\n-        if isinstance(result, Response):\n-            # Deferreds are invariant so Mutable*Chain isn't matched to *Iterable\n         assert self.crawler.spider\n-            return self.spidermw.scrape_response(  # type: ignore[return-value]\n+        if isinstance(result, Response):\n+            try:\n+                # call the spider middlewares and the request callback with the response\n+                output = await maybe_deferred_to_future(\n+                    self.spidermw.scrape_response(\n                         self.call_spider, result, request, self.crawler.spider\n                     )\n-        # else result is a Failure\n-        dfd = self.call_spider(result, request)\n-        dfd.addErrback(self._log_download_errors, result, request)\n-        return dfd\n+                )\n+            except Exception:\n+                self.handle_spider_error(Failure(), request, result)\n+            else:\n+                await self.handle_spider_output_async(output, request, result)\n+            return\n+\n+        try:\n+            # call the request errback with the downloader error\n+            await self.call_spider_async(result, request)\n+        except Exception as spider_exc:\n+            # the errback didn't silence the exception\n+            if not result.check(IgnoreRequest):\n+                logkws = self.logformatter.download_error(\n+                    result, request, self.crawler.spider\n+                )\n+                logger.log(\n+                    *logformatter_adapter(logkws),\n+                    extra={\"spider\": self.crawler.spider},\n+                    exc_info=failure_to_exc_info(result),\n+                )\n+            if spider_exc is not result.value:\n+                # the errback raised a different exception, handle it\n+                self.handle_spider_error(Failure(), request, result)\n \n     def call_spider(\n         self, result: Response | Failure, request: Request, spider: Spider | None = None\n-    ) -> Deferred[Iterable[Any] | AsyncIterable[Any]]:\n+    ) -> Deferred[Iterable[Any] | AsyncIterator[Any]]:\n         if spider is not None:\n             warnings.warn(\n                 \"Passing a 'spider' argument to Scraper.call_spider() is deprecated.\",\n                 category=ScrapyDeprecationWarning,\n                 stacklevel=2,\n             )\n+        return deferred_from_coro(self.call_spider_async(result, request))\n \n+    async def call_spider_async(\n+        self, result: Response | Failure, request: Request\n+    ) -> Iterable[Any] | AsyncIterator[Any]:\n+        \"\"\"Call the request callback or errback with the response or failure.\"\"\"\n+        await maybe_deferred_to_future(_defer_sleep())\n         assert self.crawler.spider\n-        dfd: Deferred[Any]\n         if isinstance(result, Response):\n             if getattr(result, \"request\", None) is None:\n                 result.request = request\n             assert result.request\n             callback = result.request.callback or self.crawler.spider._parse\n             warn_on_generator_with_return_value(self.crawler.spider, callback)\n-            dfd = defer_succeed(result)\n-            dfd.addCallbacks(\n-                callback=callback, callbackKeywords=result.request.cb_kwargs\n-            )\n+            output = callback(result, **result.request.cb_kwargs)\n         else:  # result is a Failure\n             # TODO: properly type adding this attribute to a Failure\n             result.request = request  # type: ignore[attr-defined]\n-            dfd = defer_fail(result)\n-            if request.errback:\n-                warn_on_generator_with_return_value(\n-                    self.crawler.spider, request.errback\n+            if not request.errback:\n+                result.raiseException()\n+            warn_on_generator_with_return_value(self.crawler.spider, request.errback)\n+            output = request.errback(result)\n+            if isinstance(output, Failure):\n+                output.raiseException()\n+            # else the errback returned actual output (like a callback),\n+            # which needs to be passed to iterate_spider_output()\n+        return await maybe_deferred_to_future(\n+            maybeDeferred(iterate_spider_output, output)\n         )\n-                dfd.addErrback(request.errback)\n-        dfd2: Deferred[Iterable[Any] | AsyncIterable[Any]] = dfd.addCallback(\n-            iterate_spider_output\n-        )\n-        return dfd2\n \n     def handle_spider_error(\n         self,\n@@ -261,6 +266,7 @@ class Scraper:\n         response: Response | Failure,\n         spider: Spider | None = None,\n     ) -> None:\n+        \"\"\"Handle an exception raised by a spider callback or errback.\"\"\"\n         if spider is not None:\n             warnings.warn(\n                 \"Passing a 'spider' argument to Scraper.handle_spider_error() is deprecated.\",\n@@ -301,57 +307,68 @@ class Scraper:\n \n     def handle_spider_output(\n         self,\n-        result: Iterable[_T] | AsyncIterable[_T],\n+        result: Iterable[_T] | AsyncIterator[_T],\n         request: Request,\n         response: Response,\n         spider: Spider | None = None,\n-    ) -> _HandleOutputDeferred:\n+    ) -> Deferred[None]:\n+        \"\"\"Pass items/requests produced by a callback to ``_process_spidermw_output()`` in parallel.\"\"\"\n         if spider is not None:\n             warnings.warn(\n                 \"Passing a 'spider' argument to Scraper.handle_spider_output() is deprecated.\",\n                 category=ScrapyDeprecationWarning,\n                 stacklevel=2,\n             )\n+        return deferred_from_coro(\n+            self.handle_spider_output_async(result, request, response)\n+        )\n \n-        if not result:\n-            return defer_succeed(None)\n-        it: Iterable[_T] | AsyncIterable[_T]\n-        dfd: Deferred[_ParallelResult]\n-        if isinstance(result, AsyncIterable):\n-            it = aiter_errback(result, self.handle_spider_error, request, response)\n-            dfd = parallel_async(\n-                it,\n+    async def handle_spider_output_async(\n+        self,\n+        result: Iterable[_T] | AsyncIterator[_T],\n+        request: Request,\n+        response: Response,\n+    ) -> None:\n+        \"\"\"Pass items/requests produced by a callback to ``_process_spidermw_output()`` in parallel.\"\"\"\n+        if isinstance(result, AsyncIterator):\n+            ait = aiter_errback(result, self.handle_spider_error, request, response)\n+            await maybe_deferred_to_future(\n+                parallel_async(\n+                    ait,\n                     self.concurrent_items,\n                     self._process_spidermw_output,\n                     response,\n                 )\n-        else:\n+            )\n+            return\n         it = iter_errback(result, self.handle_spider_error, request, response)\n-            dfd = parallel(\n+        await maybe_deferred_to_future(\n+            parallel(\n                 it,\n                 self.concurrent_items,\n                 self._process_spidermw_output,\n                 response,\n             )\n-        # returning Deferred[_ParallelResult] instead of Deferred[Union[_ParallelResult, None]]\n-        return dfd  # type: ignore[return-value]\n+        )\n \n-    def _process_spidermw_output(\n-        self, output: Any, response: Response\n-    ) -> Deferred[Any] | None:\n+    @deferred_f_from_coro_f\n+    async def _process_spidermw_output(self, output: Any, response: Response) -> None:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n-        from the given spider\n+        from the given spider.\n+\n+        Items are sent to the item pipelines, requests are scheduled.\n         \"\"\"\n         if isinstance(output, Request):\n             assert self.crawler.engine is not None  # typing\n             self.crawler.engine.crawl(request=output)\n-        elif output is None:\n-            pass\n-        else:\n-            return self.start_itemproc(output, response=response)\n-        return None\n+            return\n+        if output is not None:\n+            await maybe_deferred_to_future(\n+                self.start_itemproc(output, response=response)\n+            )\n \n-    def start_itemproc(self, item: Any, *, response: Response | None) -> Deferred[Any]:\n+    @deferred_f_from_coro_f\n+    async def start_itemproc(self, item: Any, *, response: Response | None) -> None:\n         \"\"\"Send *item* to the item pipelines for processing.\n \n         *response* is the source of the item data. If the item does not come\n@@ -360,90 +377,56 @@ class Scraper:\n         assert self.slot is not None  # typing\n         assert self.crawler.spider is not None  # typing\n         self.slot.itemproc_size += 1\n-        dfd = self.itemproc.process_item(item, self.crawler.spider)\n-        dfd.addBoth(self._itemproc_finished, item, response)\n-        return dfd\n-\n-    def _log_download_errors(\n-        self,\n-        spider_failure: Failure,\n-        download_failure: Failure,\n-        request: Request,\n-    ) -> Failure | None:\n-        \"\"\"Log and silence errors that come from the engine (typically download\n-        errors that got propagated thru here).\n-\n-        spider_failure: the value passed into the errback of self.call_spider()\n-        (likely raised in the request errback)\n-\n-        download_failure: the value passed into _scrape2() from\n-        ExecutionEngine._handle_downloader_output() as \"result\"\n-        (likely raised in the download handler or a downloader middleware)\n-        \"\"\"\n-        if not download_failure.check(IgnoreRequest):\n-            assert self.crawler.spider\n-            logkws = self.logformatter.download_error(\n-                download_failure, request, self.crawler.spider\n-            )\n-            logger.log(\n-                *logformatter_adapter(logkws),\n-                extra={\"spider\": self.crawler.spider},\n-                exc_info=failure_to_exc_info(download_failure),\n-            )\n-        if spider_failure is not download_failure:\n-            # a request errback raised a different exception, it needs to be handled later\n-            return spider_failure\n-        return None\n-\n-    def _itemproc_finished(\n-        self, output: Any, item: Any, response: Response | None\n-    ) -> Deferred[Any]:\n-        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\"\"\"\n-        assert self.slot is not None  # typing\n-        assert self.crawler.spider\n-        self.slot.itemproc_size -= 1\n-        if isinstance(output, Failure):\n-            ex = output.value\n-            if isinstance(ex, DropItem):\n-                logkws = self.logformatter.dropped(\n-                    item, ex, response, self.crawler.spider\n+        try:\n+            output = await maybe_deferred_to_future(\n+                self.itemproc.process_item(item, self.crawler.spider)\n             )\n+        except DropItem as ex:\n+            logkws = self.logformatter.dropped(item, ex, response, self.crawler.spider)\n             if logkws is not None:\n                 logger.log(\n-                        *logformatter_adapter(logkws),\n-                        extra={\"spider\": self.crawler.spider},\n+                    *logformatter_adapter(logkws), extra={\"spider\": self.crawler.spider}\n                 )\n-                return self.signals.send_catch_log_deferred(\n+            await maybe_deferred_to_future(\n+                self.signals.send_catch_log_deferred(\n                     signal=signals.item_dropped,\n                     item=item,\n                     response=response,\n                     spider=self.crawler.spider,\n-                    exception=output.value,\n+                    exception=ex,\n                 )\n-            assert ex\n+            )\n+        except Exception as ex:\n             logkws = self.logformatter.item_error(\n                 item, ex, response, self.crawler.spider\n             )\n             logger.log(\n                 *logformatter_adapter(logkws),\n                 extra={\"spider\": self.crawler.spider},\n-                exc_info=failure_to_exc_info(output),\n+                exc_info=True,\n             )\n-            return self.signals.send_catch_log_deferred(\n+            await maybe_deferred_to_future(\n+                self.signals.send_catch_log_deferred(\n                     signal=signals.item_error,\n                     item=item,\n                     response=response,\n                     spider=self.crawler.spider,\n-                failure=output,\n+                    failure=Failure(),\n                 )\n+            )\n+        else:\n             logkws = self.logformatter.scraped(output, response, self.crawler.spider)\n             if logkws is not None:\n                 logger.log(\n                     *logformatter_adapter(logkws), extra={\"spider\": self.crawler.spider}\n                 )\n-        return self.signals.send_catch_log_deferred(\n+            await maybe_deferred_to_future(\n+                self.signals.send_catch_log_deferred(\n                     signal=signals.item_scraped,\n                     item=output,\n                     response=response,\n                     spider=self.crawler.spider,\n                 )\n+            )\n+        finally:\n+            self.slot.itemproc_size -= 1\n\n@@ -7,16 +7,17 @@ See documentation in docs/topics/spider-middleware.rst\n from __future__ import annotations\n \n import logging\n-from collections.abc import AsyncIterable, Callable, Iterable\n+from collections.abc import AsyncIterator, Callable, Iterable\n from inspect import isasyncgenfunction, iscoroutine\n from itertools import islice\n from typing import TYPE_CHECKING, Any, TypeVar, Union, cast\n+from warnings import warn\n \n from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.python.failure import Failure\n \n from scrapy import Request, Spider\n-from scrapy.exceptions import _InvalidOutput\n+from scrapy.exceptions import ScrapyDeprecationWarning, _InvalidOutput\n from scrapy.http import Response\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n@@ -40,12 +41,13 @@ logger = logging.getLogger(__name__)\n \n _T = TypeVar(\"_T\")\n ScrapeFunc = Callable[\n-    [Union[Response, Failure], Request], Union[Iterable[_T], AsyncIterable[_T]]\n+    [Union[Response, Failure], Request],\n+    Deferred[Union[Iterable[_T], AsyncIterator[_T]]],\n ]\n \n \n def _isiterable(o: Any) -> bool:\n-    return isinstance(o, (Iterable, AsyncIterable))\n+    return isinstance(o, (Iterable, AsyncIterator))\n \n \n class SpiderMiddlewareManager(MiddlewareManager):\n@@ -55,12 +57,75 @@ class SpiderMiddlewareManager(MiddlewareManager):\n     def _get_mwlist_from_settings(cls, settings: BaseSettings) -> list[Any]:\n         return build_component_list(settings.getwithbase(\"SPIDER_MIDDLEWARES\"))\n \n+    def __init__(self, *middlewares: Any) -> None:\n+        self._check_deprecated_process_start_requests_use(middlewares)\n+        super().__init__(*middlewares)\n+\n+    def _check_deprecated_process_start_requests_use(\n+        self, middlewares: tuple[Any]\n+    ) -> None:\n+        deprecated_middlewares = [\n+            middleware\n+            for middleware in middlewares\n+            if hasattr(middleware, \"process_start_requests\")\n+            and not hasattr(middleware, \"process_start\")\n+        ]\n+        modern_middlewares = [\n+            middleware\n+            for middleware in middlewares\n+            if not hasattr(middleware, \"process_start_requests\")\n+            and hasattr(middleware, \"process_start\")\n+        ]\n+        if deprecated_middlewares and modern_middlewares:\n+            raise ValueError(\n+                \"You are trying to combine spider middlewares that only \"\n+                \"define the deprecated process_start_requests() method () \"\n+                \"with spider middlewares that only define the \"\n+                \"process_start() method (). This is not possible. You must \"\n+                \"either disable or make universal 1 of those 2 sets of \"\n+                \"spider middlewares. Making a spider middleware universal \"\n+                \"means having it define both methods. See the release notes \"\n+                \"of Scrapy VERSION for details: \"\n+                \"https://docs.scrapy.org/en/VERSION/news.html\"\n+            )\n+\n+        self._use_start_requests = bool(deprecated_middlewares)\n+        if self._use_start_requests:\n+            deprecated_middleware_list = \", \".join(\n+                global_object_name(middleware.__class__)\n+                for middleware in deprecated_middlewares\n+            )\n+            warn(\n+                f\"The following enabled spider middlewares, directly or \"\n+                f\"through their parent classes, define the deprecated \"\n+                f\"process_start_requests() method: \"\n+                f\"{deprecated_middleware_list}. process_start_requests() has \"\n+                f\"been deprecated in favor of a new method, process_start(), \"\n+                f\"to support asynchronous code execution. \"\n+                f\"process_start_requests() will stop being called in a future \"\n+                f\"version of Scrapy. If you use Scrapy VERSION or higher \"\n+                f\"only, replace process_start_requests() with \"\n+                f\"process_start(); note that process_start() is a coroutine \"\n+                f\"(async def). If you need to maintain compatibility with \"\n+                f\"lower Scrapy versions, when defining \"\n+                f\"process_start_requests() in a spider middleware class, \"\n+                f\"define process_start() as well. See the release notes of \"\n+                f\"Scrapy VERSION for details: \"\n+                f\"https://docs.scrapy.org/en/VERSION/news.html\",\n+                ScrapyDeprecationWarning,\n+            )\n+\n     def _add_middleware(self, mw: Any) -> None:\n         super()._add_middleware(mw)\n         if hasattr(mw, \"process_spider_input\"):\n             self.methods[\"process_spider_input\"].append(mw.process_spider_input)\n+        if self._use_start_requests:\n             if hasattr(mw, \"process_start_requests\"):\n-            self.methods[\"process_start_requests\"].appendleft(mw.process_start_requests)\n+                self.methods[\"process_start_requests\"].appendleft(\n+                    mw.process_start_requests\n+                )\n+        elif hasattr(mw, \"process_start\"):\n+            self.methods[\"process_start\"].appendleft(mw.process_start)\n         process_spider_output = self._get_async_method_pair(mw, \"process_spider_output\")\n         self.methods[\"process_spider_output\"].appendleft(process_spider_output)\n         process_spider_exception = getattr(mw, \"process_spider_exception\", None)\n@@ -72,7 +137,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         response: Response,\n         request: Request,\n         spider: Spider,\n-    ) -> Iterable[_T] | AsyncIterable[_T]:\n+    ) -> Deferred[Iterable[_T] | AsyncIterator[_T]]:\n         for method in self.methods[\"process_spider_input\"]:\n             method = cast(Callable, method)\n             try:\n@@ -93,10 +158,10 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         self,\n         response: Response,\n         spider: Spider,\n-        iterable: Iterable[_T] | AsyncIterable[_T],\n+        iterable: Iterable[_T] | AsyncIterator[_T],\n         exception_processor_index: int,\n         recover_to: MutableChain[_T] | MutableAsyncChain[_T],\n-    ) -> Iterable[_T] | AsyncIterable[_T]:\n+    ) -> Iterable[_T] | AsyncIterator[_T]:\n         def process_sync(iterable: Iterable[_T]) -> Iterable[_T]:\n             try:\n                 yield from iterable\n@@ -112,7 +177,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 assert isinstance(recover_to, MutableChain)\n                 recover_to.extend(exception_result)\n \n-        async def process_async(iterable: AsyncIterable[_T]) -> AsyncIterable[_T]:\n+        async def process_async(iterable: AsyncIterator[_T]) -> AsyncIterator[_T]:\n             try:\n                 async for r in iterable:\n                     yield r\n@@ -128,7 +193,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 assert isinstance(recover_to, MutableAsyncChain)\n                 recover_to.extend(exception_result)\n \n-        if isinstance(iterable, AsyncIterable):\n+        if isinstance(iterable, AsyncIterator):\n             return process_async(iterable)\n         return process_sync(iterable)\n \n@@ -187,13 +252,13 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         self,\n         response: Response,\n         spider: Spider,\n-        result: Iterable[_T] | AsyncIterable[_T],\n+        result: Iterable[_T] | AsyncIterator[_T],\n         start_index: int = 0,\n     ) -> Generator[Deferred[Any], Any, MutableChain[_T] | MutableAsyncChain[_T]]:\n         # items in this iterable do not need to go through the process_spider_output\n         # chain, they went through it already from the process_spider_exception method\n         recovered: MutableChain[_T] | MutableAsyncChain[_T]\n-        last_result_is_async = isinstance(result, AsyncIterable)\n+        last_result_is_async = isinstance(result, AsyncIterator)\n         recovered = MutableAsyncChain() if last_result_is_async else MutableChain()\n \n         # There are three cases for the middleware: def foo, async def foo, def foo + async def foo_async.\n@@ -220,7 +285,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     need_downgrade = True\n             try:\n                 if need_upgrade:\n-                    # Iterable -> AsyncIterable\n+                    # Iterable -> AsyncIterator\n                     result = as_async_generator(result)\n                 elif need_downgrade:\n                     logger.warning(\n@@ -230,10 +295,10 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                         f\" https://docs.scrapy.org/en/latest/topics/coroutines.html#for-middleware-users\"\n                         f\" for more information.\"\n                     )\n-                    assert isinstance(result, AsyncIterable)\n-                    # AsyncIterable -> Iterable\n+                    assert isinstance(result, AsyncIterator)\n+                    # AsyncIterator -> Iterable\n                     result = yield deferred_from_coro(collect_asyncgen(result))\n-                    if isinstance(recovered, AsyncIterable):\n+                    if isinstance(recovered, AsyncIterator):\n                         recovered_collected = yield deferred_from_coro(\n                             collect_asyncgen(recovered)\n                         )\n@@ -266,7 +331,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                         f\"{type(result)}\"\n                     )\n                 raise _InvalidOutput(msg)\n-            last_result_is_async = isinstance(result, AsyncIterable)\n+            last_result_is_async = isinstance(result, AsyncIterator)\n \n         if last_result_is_async:\n             return MutableAsyncChain(result, recovered)\n@@ -276,23 +341,23 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         self,\n         response: Response,\n         spider: Spider,\n-        result: Iterable[_T] | AsyncIterable[_T],\n+        result: Iterable[_T] | AsyncIterator[_T],\n     ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n         recovered: MutableChain[_T] | MutableAsyncChain[_T]\n-        if isinstance(result, AsyncIterable):\n+        if isinstance(result, AsyncIterator):\n             recovered = MutableAsyncChain()\n         else:\n             recovered = MutableChain()\n         result = self._evaluate_iterable(response, spider, result, 0, recovered)\n         result = await maybe_deferred_to_future(\n             cast(\n-                \"Deferred[Iterable[_T] | AsyncIterable[_T]]\",\n+                \"Deferred[Iterable[_T] | AsyncIterator[_T]]\",\n                 self._process_spider_output(response, spider, result),\n             )\n         )\n-        if isinstance(result, AsyncIterable):\n+        if isinstance(result, AsyncIterator):\n             return MutableAsyncChain(result, recovered)\n-        if isinstance(recovered, AsyncIterable):\n+        if isinstance(recovered, AsyncIterator):\n             recovered_collected = await collect_asyncgen(recovered)\n             recovered = MutableChain(recovered_collected)\n         return MutableChain(result, recovered)\n@@ -305,7 +370,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         spider: Spider,\n     ) -> Deferred[MutableChain[_T] | MutableAsyncChain[_T]]:\n         async def process_callback_output(\n-            result: Iterable[_T] | AsyncIterable[_T],\n+            result: Iterable[_T] | AsyncIterator[_T],\n         ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n             return await self._process_callback_output(response, spider, result)\n \n@@ -314,7 +379,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         ) -> Failure | MutableChain[_T] | MutableAsyncChain[_T]:\n             return self._process_spider_exception(response, spider, _failure)\n \n-        dfd: Deferred[Iterable[_T] | AsyncIterable[_T]] = mustbe_deferred(\n+        dfd: Deferred[Iterable[_T] | AsyncIterator[_T]] = mustbe_deferred(\n             self._process_spider_input, scrape_func, response, request, spider\n         )\n         dfd2: Deferred[MutableChain[_T] | MutableAsyncChain[_T]] = dfd.addCallback(\n@@ -323,10 +388,90 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         dfd2.addErrback(process_spider_exception)\n         return dfd2\n \n-    def process_start_requests(\n-        self, start_requests: Iterable[Request], spider: Spider\n-    ) -> Deferred[Iterable[Request]]:\n-        return self._process_chain(\"process_start_requests\", start_requests, spider)\n+    async def process_start(self, spider: Spider) -> AsyncIterator[Any] | None:\n+        self._check_deprecated_start_requests_use(spider)\n+        if self._use_start_requests:\n+            sync_start = iter(spider.start_requests())\n+            sync_start = await maybe_deferred_to_future(\n+                self._process_chain(\"process_start_requests\", sync_start, spider)\n+            )\n+            start: AsyncIterator[Any] = as_async_generator(sync_start)\n+        else:\n+            start = spider.start()\n+            start = await maybe_deferred_to_future(\n+                self._process_chain(\"process_start\", start)\n+            )\n+        return start\n+\n+    def _check_deprecated_start_requests_use(self, spider: Spider):\n+        start_requests_cls = None\n+        start_cls = None\n+        spidercls = spider.__class__\n+        mro = spidercls.__mro__\n+\n+        for cls in mro:\n+            cls_dict = cls.__dict__\n+            if start_requests_cls is None and \"start_requests\" in cls_dict:\n+                start_requests_cls = cls\n+            if start_cls is None and \"start\" in cls_dict:\n+                start_cls = cls\n+            if start_requests_cls is not None and start_cls is not None:\n+                break\n+\n+        # Spider defines both, start_requests and start.\n+        assert start_requests_cls is not None\n+        assert start_cls is not None\n+\n+        if (\n+            start_requests_cls is not Spider\n+            and start_cls is not start_requests_cls\n+            and mro.index(start_requests_cls) < mro.index(start_cls)\n+        ):\n+            src = global_object_name(start_requests_cls)\n+            if start_requests_cls is not spidercls:\n+                src += f\" (inherited by {global_object_name(spidercls)})\"\n+            warn(\n+                f\"{src} defines the deprecated start_requests() method. \"\n+                f\"start_requests() has been deprecated in favor of a new \"\n+                f\"method, start(), to support asynchronous code \"\n+                f\"execution. start_requests() will stop being called in a \"\n+                f\"future version of Scrapy. If you use Scrapy VERSION or \"\n+                f\"higher only, replace start_requests() with start(); \"\n+                f\"note that start() is a coroutine (async def). If you \"\n+                f\"need to maintain compatibility with lower Scrapy versions, \"\n+                f\"when overriding start_requests() in a spider class, \"\n+                f\"override start() as well; you can use super() to \"\n+                f\"reuse the inherited start() implementation without \"\n+                f\"copy-pasting. See the release notes of Scrapy VERSION for \"\n+                f\"details: https://docs.scrapy.org/en/VERSION/news.html\",\n+                ScrapyDeprecationWarning,\n+            )\n+\n+        if (\n+            self._use_start_requests\n+            and start_cls is not Spider\n+            and start_requests_cls is not start_cls\n+            and mro.index(start_cls) < mro.index(start_requests_cls)\n+        ):\n+            src = global_object_name(start_cls)\n+            if start_cls is not spidercls:\n+                src += f\" (inherited by {global_object_name(spidercls)})\"\n+            raise ValueError(\n+                f\"{src} does not define the deprecated start_requests() \"\n+                f\"method. However, one or more of your enabled spider \"\n+                f\"middlewares (reported in an earlier deprecation warning) \"\n+                f\"define the process_start_requests() method, and not the \"\n+                f\"process_start() method, making them only compatible with \"\n+                f\"(deprecated) spiders that define the start_requests() \"\n+                f\"method. To solve this issue, disable the offending spider \"\n+                f\"middlewares, upgrade them as described in that earlier \"\n+                f\"deprecation warning, or make your spider compatible with \"\n+                f\"deprecated spider middlewares (and earlier Scrapy versions) \"\n+                f\"by defining a sync start_requests() method that works \"\n+                f\"similarly to its existing start() method. See the \"\n+                f\"release notes of Scrapy VERSION for details: \"\n+                f\"https://docs.scrapy.org/en/VERSION/news.html\"\n+            )\n \n     # This method is only needed until _async compatibility methods are removed.\n     @staticmethod\n\n@@ -136,6 +136,9 @@ class Crawler:\n             \"Overridden settings:\\n%(settings)s\", {\"settings\": pprint.pformat(d)}\n         )\n \n+    # Cannot use @deferred_f_from_coro_f because that relies on the reactor\n+    # being installed already, which is done within _apply_settings(), inside\n+    # this method.\n     @inlineCallbacks\n     def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred[Any], Any, None]:\n         if self.crawling:\n@@ -151,9 +154,8 @@ class Crawler:\n             self._apply_settings()\n             self._update_root_log_handler()\n             self.engine = self._create_engine()\n-            start_requests = iter(self.spider.start_requests())\n-            yield self.engine.open_spider(self.spider, start_requests)\n-            yield maybeDeferred(self.engine.start)\n+            yield self.engine.open_spider(self.spider)\n+            yield self.engine.start()\n         except Exception:\n             self.crawling = False\n             if self.engine is not None:\n\n@@ -104,7 +104,6 @@ class TelnetConsole(protocol.ServerFactory):\n         telnet_vars: dict[str, Any] = {\n             \"engine\": self.crawler.engine,\n             \"spider\": self.crawler.engine.spider,\n-            \"slot\": self.crawler.engine.slot,\n             \"crawler\": self.crawler,\n             \"extensions\": self.crawler.extensions,\n             \"stats\": self.crawler.stats,\n\n@@ -130,6 +130,16 @@ class Request(object_ref):\n         self._set_body(body)\n         if not isinstance(priority, int):\n             raise TypeError(f\"Request priority not an integer: {priority!r}\")\n+\n+        #: Default: ``0``\n+        #:\n+        #: Value that the :ref:`scheduler <topics-scheduler>` may use for\n+        #: request prioritization.\n+        #:\n+        #: Built-in schedulers prioritize requests with a higher priority\n+        #: value.\n+        #:\n+        #: Negative values are allowed.\n         self.priority: int = priority\n \n         if not (callable(callback) or callback is None):\n@@ -191,7 +201,7 @@ class Request(object_ref):\n         #:\n         #: When defining the start URLs of a spider through\n         #: :attr:`~scrapy.Spider.start_urls`, this attribute is enabled by\n-        #: default. See :meth:`~scrapy.Spider.start_requests`.\n+        #: default. See :meth:`~scrapy.Spider.start`.\n         self.dont_filter: bool = dont_filter\n \n         self._meta: dict[str, Any] | None = dict(meta) if meta else None\n\n@@ -98,7 +98,7 @@ class LogFormatter:\n         \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n         src: Any\n         if response is None:\n-            src = f\"{global_object_name(spider.__class__)}.start_requests\"\n+            src = f\"{global_object_name(spider.__class__)}.start\"\n         elif isinstance(response, Failure):\n             src = response.getErrorMessage()\n         else:\n\n@@ -72,7 +72,6 @@ class ScrapyPriorityQueue:\n     startprios is a sequence of priorities to start with. If the queue was\n     previously closed leaving some priority buckets non-empty, those priorities\n     should be passed in startprios.\n-\n     \"\"\"\n \n     @classmethod\n@@ -82,8 +81,16 @@ class ScrapyPriorityQueue:\n         downstream_queue_cls: type[QueueProtocol],\n         key: str,\n         startprios: Iterable[int] = (),\n+        *,\n+        start_queue_cls: type[QueueProtocol] | None = None,\n     ) -> Self:\n-        return cls(crawler, downstream_queue_cls, key, startprios)\n+        return cls(\n+            crawler,\n+            downstream_queue_cls,\n+            key,\n+            startprios,\n+            start_queue_cls=start_queue_cls,\n+        )\n \n     def __init__(\n         self,\n@@ -91,11 +98,15 @@ class ScrapyPriorityQueue:\n         downstream_queue_cls: type[QueueProtocol],\n         key: str,\n         startprios: Iterable[int] = (),\n+        *,\n+        start_queue_cls: type[QueueProtocol] | None = None,\n     ):\n         self.crawler: Crawler = crawler\n         self.downstream_queue_cls: type[QueueProtocol] = downstream_queue_cls\n+        self._start_queue_cls: type[QueueProtocol] | None = start_queue_cls\n         self.key: str = key\n         self.queues: dict[int, QueueProtocol] = {}\n+        self._start_queues: dict[int, QueueProtocol] = {}\n         self.curprio: int | None = None\n         self.init_prios(startprios)\n \n@@ -104,7 +115,13 @@ class ScrapyPriorityQueue:\n             return\n \n         for priority in startprios:\n-            self.queues[priority] = self.qfactory(priority)\n+            q = self.qfactory(priority)\n+            if q:\n+                self.queues[priority] = q\n+            if self._start_queue_cls:\n+                q = self._sqfactory(priority)\n+                if q:\n+                    self._start_queues[priority] = q\n \n         self.curprio = min(startprios)\n \n@@ -115,11 +132,25 @@ class ScrapyPriorityQueue:\n             self.key + \"/\" + str(key),\n         )\n \n+    def _sqfactory(self, key: int) -> QueueProtocol:\n+        assert self._start_queue_cls is not None\n+        return build_from_crawler(\n+            self._start_queue_cls,\n+            self.crawler,\n+            f\"{self.key}/{key}s\",\n+        )\n+\n     def priority(self, request: Request) -> int:\n         return -request.priority\n \n     def push(self, request: Request) -> None:\n         priority = self.priority(request)\n+        is_start_request = request.meta.get(\"is_start_request\", False)\n+        if is_start_request and self._start_queue_cls:\n+            if priority not in self._start_queues:\n+                self._start_queues[priority] = self._sqfactory(priority)\n+            q = self._start_queues[priority]\n+        else:\n             if priority not in self.queues:\n                 self.queues[priority] = self.qfactory(priority)\n             q = self.queues[priority]\n@@ -128,16 +159,39 @@ class ScrapyPriorityQueue:\n             self.curprio = priority\n \n     def pop(self) -> Request | None:\n-        if self.curprio is None:\n-            return None\n+        while self.curprio is not None:\n+            if self._start_queues:\n+                try:\n+                    q = self._start_queues[self.curprio]\n+                except KeyError:\n+                    pass\n+                else:\n+                    m = q.pop()\n+                    if not q:\n+                        del self._start_queues[self.curprio]\n+                        q.close()\n+                    return m\n+            try:\n                 q = self.queues[self.curprio]\n+            except KeyError:\n+                self._update_curprio()\n+            else:\n                 m = q.pop()\n                 if not q:\n                     del self.queues[self.curprio]\n                     q.close()\n-            prios = [p for p, q in self.queues.items() if q]\n-            self.curprio = min(prios) if prios else None\n+                    self._update_curprio()\n                 return m\n+        return None\n+\n+    def _update_curprio(self) -> None:\n+        prios = {\n+            p\n+            for queues in (self.queues, self._start_queues)\n+            for p, q in queues.items()\n+            if q\n+        }\n+        self.curprio = min(prios) if prios else None\n \n     def peek(self) -> Request | None:\n         \"\"\"Returns the next object to be returned by :meth:`pop`,\n@@ -148,19 +202,31 @@ class ScrapyPriorityQueue:\n         \"\"\"\n         if self.curprio is None:\n             return None\n+        try:\n+            queue = self._start_queues[self.curprio]\n+        except KeyError:\n             queue = self.queues[self.curprio]\n         # Protocols can't declare optional members\n         return cast(Request, queue.peek())  # type: ignore[attr-defined]\n \n     def close(self) -> list[int]:\n-        active: list[int] = []\n-        for p, q in self.queues.items():\n-            active.append(p)\n+        active: set[int] = set()\n+        for queues in (self.queues, self._start_queues):\n+            for p, q in queues.items():\n+                active.add(p)\n                 q.close()\n-        return active\n+        return list(active)\n \n     def __len__(self) -> int:\n-        return sum(len(x) for x in self.queues.values()) if self.queues else 0\n+        return (\n+            sum(\n+                len(x)\n+                for queues in (self.queues, self._start_queues)\n+                for x in queues.values()\n+            )\n+            if self.queues or self._start_queues\n+            else 0\n+        )\n \n \n class DownloaderInterface:\n@@ -194,8 +260,16 @@ class DownloaderAwarePriorityQueue:\n         downstream_queue_cls: type[QueueProtocol],\n         key: str,\n         startprios: dict[str, Iterable[int]] | None = None,\n+        *,\n+        start_queue_cls: type[QueueProtocol] | None = None,\n     ) -> Self:\n-        return cls(crawler, downstream_queue_cls, key, startprios)\n+        return cls(\n+            crawler,\n+            downstream_queue_cls,\n+            key,\n+            startprios,\n+            start_queue_cls=start_queue_cls,\n+        )\n \n     def __init__(\n         self,\n@@ -203,6 +277,8 @@ class DownloaderAwarePriorityQueue:\n         downstream_queue_cls: type[QueueProtocol],\n         key: str,\n         slot_startprios: dict[str, Iterable[int]] | None = None,\n+        *,\n+        start_queue_cls: type[QueueProtocol] | None = None,\n     ):\n         if crawler.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\") != 0:\n             raise ValueError(\n@@ -222,6 +298,7 @@ class DownloaderAwarePriorityQueue:\n \n         self._downloader_interface: DownloaderInterface = DownloaderInterface(crawler)\n         self.downstream_queue_cls: type[QueueProtocol] = downstream_queue_cls\n+        self._start_queue_cls: type[QueueProtocol] | None = start_queue_cls\n         self.key: str = key\n         self.crawler: Crawler = crawler\n \n@@ -237,6 +314,7 @@ class DownloaderAwarePriorityQueue:\n             self.downstream_queue_cls,\n             self.key + \"/\" + _path_safe(slot),\n             startprios,\n+            start_queue_cls=self._start_queue_cls,\n         )\n \n     def pop(self) -> Request | None:\n\n@@ -305,6 +305,8 @@ SCHEDULER = \"scrapy.core.scheduler.Scheduler\"\n SCHEDULER_DISK_QUEUE = \"scrapy.squeues.PickleLifoDiskQueue\"\n SCHEDULER_MEMORY_QUEUE = \"scrapy.squeues.LifoMemoryQueue\"\n SCHEDULER_PRIORITY_QUEUE = \"scrapy.pqueues.ScrapyPriorityQueue\"\n+SCHEDULER_START_DISK_QUEUE = \"scrapy.squeues.PickleFifoDiskQueue\"\n+SCHEDULER_START_MEMORY_QUEUE = \"scrapy.squeues.FifoMemoryQueue\"\n \n SCRAPER_SLOT_MAX_ACTIVE_SIZE = 5000000\n \n@@ -315,6 +317,7 @@ SPIDER_MIDDLEWARES = {}\n \n SPIDER_MIDDLEWARES_BASE = {\n     # Engine side\n+    \"scrapy.spidermiddlewares.start.StartSpiderMiddleware\": 25,\n     \"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\": 50,\n     \"scrapy.spidermiddlewares.referer.RefererMiddleware\": 700,\n     \"scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\": 800,\n\n@@ -24,6 +24,7 @@ from scrapy.spiders import Spider\n from scrapy.utils.conf import get_config\n from scrapy.utils.console import DEFAULT_PYTHON_SHELLS, start_python_console\n from scrapy.utils.datatypes import SequenceExclude\n+from scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import is_asyncio_reactor_installed, set_asyncio_event_loop\n from scrapy.utils.response import open_in_browser\n@@ -102,25 +103,33 @@ class Shell:\n             # set the asyncio event loop for the current thread\n             event_loop_path = self.crawler.settings[\"ASYNCIO_EVENT_LOOP\"]\n             set_asyncio_event_loop(event_loop_path)\n-        spider = self._open_spider(request, spider)\n+\n+        def crawl_request(_):\n+            assert self.crawler.engine is not None\n+            self.crawler.engine.crawl(request)\n+\n+        d2 = self._open_spider(request, spider)\n+        d2.addCallback(crawl_request)\n+\n         d = _request_deferred(request)\n         d.addCallback(lambda x: (x, spider))\n-        assert self.crawler.engine\n-        self.crawler.engine.crawl(request)\n         return d\n \n-    def _open_spider(self, request: Request, spider: Spider | None) -> Spider:\n+    @deferred_f_from_coro_f\n+    async def _open_spider(self, request: Request, spider: Spider | None) -> None:\n         if self.spider:\n-            return self.spider\n+            return\n \n         if spider is None:\n             spider = self.crawler.spider or self.crawler._create_spider()\n \n         self.crawler.spider = spider\n         assert self.crawler.engine\n+        await maybe_deferred_to_future(\n             self.crawler.engine.open_spider(spider, close_if_idle=False)\n+        )\n+        self.crawler.engine._start_request_processing()\n         self.spider = spider\n-        return spider\n \n     def fetch(\n         self,\n\n@@ -1,13 +1,12 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any\n+from typing import Any\n \n from pydispatch import dispatcher\n+from twisted.internet.defer import Deferred\n \n from scrapy.utils import signal as _signal\n-\n-if TYPE_CHECKING:\n-    from twisted.internet.defer import Deferred\n+from scrapy.utils.defer import maybe_deferred_to_future\n \n \n class SignalManager:\n@@ -75,3 +74,17 @@ class SignalManager:\n         \"\"\"\n         kwargs.setdefault(\"sender\", self.sender)\n         _signal.disconnect_all(signal, **kwargs)\n+\n+    async def wait_for(self, signal):\n+        \"\"\"Await the next *signal*.\n+\n+        See :ref:`start-requests-lazy` for an example.\n+        \"\"\"\n+        d = Deferred()\n+\n+        def handle():\n+            self.disconnect(handle, signal)\n+            d.callback(None)\n+\n+        self.connect(handle, signal)\n+        await maybe_deferred_to_future(d)\n\n@@ -7,6 +7,7 @@ signals here without documenting them there.\n \n engine_started = object()\n engine_stopped = object()\n+scheduler_empty = object()\n spider_opened = object()\n spider_idle = object()\n spider_closed = object()\n\n@@ -5,7 +5,7 @@ from typing import TYPE_CHECKING, Any\n from scrapy import Request, Spider\n \n if TYPE_CHECKING:\n-    from collections.abc import AsyncIterable, Iterable\n+    from collections.abc import AsyncIterator, Iterable\n \n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n@@ -17,9 +17,9 @@ if TYPE_CHECKING:\n class BaseSpiderMiddleware:\n     \"\"\"Optional base class for spider middlewares.\n \n-    This class provides helper methods for asynchronous ``process_spider_output``\n-    methods. Middlewares that don't have a ``process_spider_output`` method don't need\n-    to use it.\n+    This class provides helper methods for asynchronous\n+    ``process_spider_output()`` and ``process_start()`` methods. Middlewares\n+    that don't have either of these methods don't need to use this class.\n \n     You can override the\n     :meth:`~scrapy.spidermiddlewares.base.BaseSpiderMiddleware.get_processed_request`\n@@ -38,59 +38,70 @@ class BaseSpiderMiddleware:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n+    def process_start_requests(\n+        self, start: Iterable[Any], spider: Spider\n+    ) -> Iterable[Any]:\n+        for o in start:\n+            if (o := self._get_processed(o, None)) is not None:\n+                yield o\n+\n+    async def process_start(self, start: AsyncIterator[Any]) -> AsyncIterator[Any]:\n+        async for o in start:\n+            if (o := self._get_processed(o, None)) is not None:\n+                yield o\n+\n     def process_spider_output(\n         self, response: Response, result: Iterable[Any], spider: Spider\n     ) -> Iterable[Any]:\n         for o in result:\n-            if isinstance(o, Request):\n-                o = self.get_processed_request(o, response)\n-            else:\n-                o = self.get_processed_item(o, response)\n-            if o is not None:\n+            if (o := self._get_processed(o, response)) is not None:\n                 yield o\n \n     async def process_spider_output_async(\n-        self, response: Response, result: AsyncIterable[Any], spider: Spider\n-    ) -> AsyncIterable[Any]:\n+        self, response: Response, result: AsyncIterator[Any], spider: Spider\n+    ) -> AsyncIterator[Any]:\n         async for o in result:\n-            if isinstance(o, Request):\n-                o = self.get_processed_request(o, response)\n-            else:\n-                o = self.get_processed_item(o, response)\n-            if o is not None:\n+            if (o := self._get_processed(o, response)) is not None:\n                 yield o\n \n+    def _get_processed(self, o: Any, response: Response | None) -> Any:\n+        if isinstance(o, Request):\n+            return self.get_processed_request(o, response)\n+        return self.get_processed_item(o, response)\n+\n     def get_processed_request(\n-        self, request: Request, response: Response\n+        self, request: Request, response: Response | None\n     ) -> Request | None:\n         \"\"\"Return a processed request from the spider output.\n \n-        This method is called with a single request from the spider output.\n-        It should return the same or a different request, or ``None`` to\n-        ignore it.\n+        This method is called with a single request from the start seeds or the\n+        spider output. It should return the same or a different request, or\n+        ``None`` to ignore it.\n \n         :param request: the input request\n         :type request: :class:`~scrapy.Request` object\n \n         :param response: the response being processed\n-        :type response: :class:`~scrapy.http.Response` object\n+        :type response: :class:`~scrapy.http.Response` object or ``None`` for\n+            start seeds\n \n         :return: the processed request or ``None``\n         \"\"\"\n         return request\n \n-    def get_processed_item(self, item: Any, response: Response) -> Any:\n+    def get_processed_item(self, item: Any, response: Response | None) -> Any:\n         \"\"\"Return a processed item from the spider output.\n \n-        This method is called with a single item from the spider output.\n-        It should return the same or a different item, or ``None`` to\n-        ignore it.\n+        This method is called with a single item from the start seeds or the\n+        spider output. It should return the same or a different item, or\n+        ``None`` to ignore it.\n \n         :param item: the input item\n         :type item: item object\n \n         :param response: the response being processed\n-        :type response: :class:`~scrapy.http.Response` object\n+        :type response: :class:`~scrapy.http.Response` object or ``None`` for\n+            start seeds\n \n         :return: the processed item or ``None``\n         \"\"\"\n\n@@ -12,7 +12,7 @@ from typing import TYPE_CHECKING, Any\n from scrapy.spidermiddlewares.base import BaseSpiderMiddleware\n \n if TYPE_CHECKING:\n-    from collections.abc import AsyncIterable, Iterable\n+    from collections.abc import AsyncIterator, Iterable\n \n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n@@ -59,8 +59,8 @@ class DepthMiddleware(BaseSpiderMiddleware):\n         yield from super().process_spider_output(response, result, spider)\n \n     async def process_spider_output_async(\n-        self, response: Response, result: AsyncIterable[Any], spider: Spider\n-    ) -> AsyncIterable[Any]:\n+        self, response: Response, result: AsyncIterator[Any], spider: Spider\n+    ) -> AsyncIterator[Any]:\n         self._init_depth(response, spider)\n         async for o in super().process_spider_output_async(response, result, spider):\n             yield o\n@@ -73,8 +73,11 @@ class DepthMiddleware(BaseSpiderMiddleware):\n                 self.stats.inc_value(\"request_depth_count/0\", spider=spider)\n \n     def get_processed_request(\n-        self, request: Request, response: Response\n+        self, request: Request, response: Response | None\n     ) -> Request | None:\n+        if response is None:\n+            # start requests\n+            return request\n         depth = response.meta[\"depth\"] + 1\n         request.meta[\"depth\"] = depth\n         if self.prio:\n\n@@ -49,8 +49,11 @@ class OffsiteMiddleware(BaseSpiderMiddleware):\n         return o\n \n     def get_processed_request(\n-        self, request: Request, response: Response\n+        self, request: Request, response: Response | None\n     ) -> Request | None:\n+        if response is None:\n+            # skip start requests for backward compatibility\n+            return request\n         assert self.crawler.spider\n         if (\n             request.dont_filter\n\n@@ -370,8 +370,11 @@ class RefererMiddleware(BaseSpiderMiddleware):\n         return cls() if cls else self.default_policy()\n \n     def get_processed_request(\n-        self, request: Request, response: Response\n+        self, request: Request, response: Response | None\n     ) -> Request | None:\n+        if response is None:\n+            # start requests\n+            return request\n         referrer = self.policy(response, request).referrer(response.url, request.url)\n         if referrer is not None:\n             request.headers.setdefault(\"Referer\", referrer)\n\n@@ -0,0 +1,31 @@\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING\n+\n+from .base import BaseSpiderMiddleware\n+\n+if TYPE_CHECKING:\n+    from scrapy.http import Request\n+    from scrapy.http.response import Response\n+\n+\n+class StartSpiderMiddleware(BaseSpiderMiddleware):\n+    \"\"\"Set :reqmeta:`is_start_request`.\n+\n+    .. reqmeta:: is_start_request\n+\n+    is_start_request\n+    ----------------\n+\n+    :attr:`~scrapy.Request.meta` key that is set to ``True`` in :ref:`start\n+    requests <start-requests>`, allowing you to tell start requests apart from\n+    other requests, e.g. in :ref:`downloader middlewares\n+    <topics-downloader-middleware>`.\n+    \"\"\"\n+\n+    def get_processed_request(\n+        self, request: Request, response: Response | None\n+    ) -> Request | None:\n+        if response is None:\n+            request.meta.setdefault(\"is_start_request\", True)\n+        return request\n\n@@ -39,7 +39,7 @@ class UrlLengthMiddleware(BaseSpiderMiddleware):\n         return o\n \n     def get_processed_request(\n-        self, request: Request, response: Response\n+        self, request: Request, response: Response | None\n     ) -> Request | None:\n         if len(request.url) <= self.maxlength:\n             return request\n\n@@ -7,15 +7,17 @@ See documentation in docs/topics/spiders.rst\n from __future__ import annotations\n \n import logging\n+import warnings\n from typing import TYPE_CHECKING, Any, cast\n \n from scrapy import signals\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n \n if TYPE_CHECKING:\n-    from collections.abc import Iterable\n+    from collections.abc import AsyncIterator, Iterable\n \n     from twisted.internet.defer import Deferred\n \n@@ -29,13 +31,19 @@ if TYPE_CHECKING:\n \n \n class Spider(object_ref):\n-    \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n-    class.\n+    \"\"\"Base class that any spider must subclass.\n+\n+    It provides a default :meth:`start` implementation that sends\n+    requests based on the :attr:`start_urls` class attribute and calls the\n+    :meth:`parse` method for each response.\n     \"\"\"\n \n     name: str\n     custom_settings: dict[_SettingsKeyT, Any] | None = None\n \n+    #: Start URLs. See :meth:`start`.\n+    start_urls: list[str]\n+\n     def __init__(self, name: str | None = None, **kwargs: Any):\n         if name is not None:\n             self.name: str = name\n@@ -72,7 +80,70 @@ class Spider(object_ref):\n         self.settings: BaseSettings = crawler.settings\n         crawler.signals.connect(self.close, signals.spider_closed)\n \n-    def start_requests(self) -> Iterable[Request]:\n+    async def start(self) -> AsyncIterator[Any]:\n+        \"\"\"Yield the initial :class:`~scrapy.Request` objects to send.\n+\n+        .. versionadded:: VERSION\n+\n+        For example:\n+\n+        .. code-block:: python\n+\n+            from scrapy import Request, Spider\n+\n+\n+            class MySpider(Spider):\n+                name = \"myspider\"\n+\n+                async def start(self):\n+                    yield Request(\"https://toscrape.com/\")\n+\n+        The default implementation reads URLs from :attr:`start_urls` and\n+        yields a request for each with :attr:`~scrapy.Request.dont_filter`\n+        enabled. It is functionally equivalent to:\n+\n+        .. code-block:: python\n+\n+            async def start(self):\n+                for url in self.start_urls:\n+                    yield Request(url, dont_filter=True)\n+\n+        You can also yield :ref:`items <topics-items>`. For example:\n+\n+        .. code-block:: python\n+\n+            async def start(self):\n+                yield {\"foo\": \"bar\"}\n+\n+        To write spiders that work on Scrapy versions lower than VERSION,\n+        define also a synchronous ``start_requests()`` method that returns an\n+        iterable. For example:\n+\n+        .. code-block:: python\n+\n+            def start_requests(self):\n+                yield Request(\"https://toscrape.com/\")\n+\n+        .. seealso:: :ref:`start-requests`\n+        \"\"\"\n+        with warnings.catch_warnings():\n+            warnings.filterwarnings(\n+                \"ignore\", category=ScrapyDeprecationWarning, module=r\"^scrapy\\.spiders$\"\n+            )\n+            for item_or_request in self.start_requests():\n+                yield item_or_request\n+\n+    def start_requests(self) -> Iterable[Any]:\n+        warnings.warn(\n+            (\n+                \"The Spider.start_requests() method is deprecated, use \"\n+                \"Spider.start() instead. If you are calling \"\n+                \"super().start_requests() from a Spider.start() override, \"\n+                \"iterate super().start() instead.\"\n+            ),\n+            ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         if not self.start_urls and hasattr(self, \"start_url\"):\n             raise AttributeError(\n                 \"Crawling could not start: 'start_urls' not found \"\n\n@@ -8,7 +8,7 @@ See documentation in docs/topics/spiders.rst\n from __future__ import annotations\n \n import copy\n-from collections.abc import AsyncIterable, Awaitable, Callable\n+from collections.abc import AsyncIterator, Awaitable, Callable\n from typing import TYPE_CHECKING, Any, Optional, TypeVar, cast\n \n from twisted.python.failure import Failure\n@@ -156,10 +156,10 @@ class CrawlSpider(Spider):\n         callback: CallbackT | None,\n         cb_kwargs: dict[str, Any],\n         follow: bool = True,\n-    ) -> AsyncIterable[Any]:\n+    ) -> AsyncIterator[Any]:\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n-            if isinstance(cb_res, AsyncIterable):\n+            if isinstance(cb_res, AsyncIterator):\n                 cb_res = await collect_asyncgen(cb_res)\n             elif isinstance(cb_res, Awaitable):\n                 cb_res = await cb_res\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import warnings\n-from collections.abc import Iterable\n+from collections.abc import AsyncIterator, Iterable\n from typing import TYPE_CHECKING, Any, cast\n \n from scrapy import Request\n@@ -29,6 +29,14 @@ class InitSpider(Spider):\n             stacklevel=2,\n         )\n \n+    async def start(self) -> AsyncIterator[Any]:\n+        with warnings.catch_warnings():\n+            warnings.filterwarnings(\n+                \"ignore\", category=ScrapyDeprecationWarning, module=r\"^scrapy\\.spiders$\"\n+            )\n+            for item_or_request in self.start_requests():\n+                yield item_or_request\n+\n     def start_requests(self) -> Iterable[Request]:\n         self._postinit_reqs: Iterable[Request] = super().start_requests()\n         return cast(Iterable[Request], iterate_spider_output(self.init_request()))\n\n@@ -4,7 +4,7 @@ import logging\n import re\n \n # Iterable is needed at the run time for the SitemapSpider._parse_sitemap() annotation\n-from collections.abc import Iterable, Sequence  # noqa: TC003\n+from collections.abc import AsyncIterator, Iterable, Sequence  # noqa: TC003\n from typing import TYPE_CHECKING, Any, cast\n \n from scrapy.http import Request, Response, XmlResponse\n@@ -53,6 +53,10 @@ class SitemapSpider(Spider):\n             self._cbs.append((regex(r), c))\n         self._follow: list[re.Pattern[str]] = [regex(x) for x in self.sitemap_follow]\n \n+    async def start(self) -> AsyncIterator[Any]:\n+        for item_or_request in self.start_requests():\n+            yield item_or_request\n+\n     def start_requests(self) -> Iterable[Request]:\n         for url in self.sitemap_urls:\n             yield Request(url, self._parse_sitemap)\n\n@@ -1,20 +1,20 @@\n from __future__ import annotations\n \n-from collections.abc import AsyncGenerator, AsyncIterable, Iterable\n+from collections.abc import AsyncGenerator, AsyncIterator, Iterable\n from typing import TypeVar\n \n _T = TypeVar(\"_T\")\n \n \n-async def collect_asyncgen(result: AsyncIterable[_T]) -> list[_T]:\n+async def collect_asyncgen(result: AsyncIterator[_T]) -> list[_T]:\n     return [x async for x in result]\n \n \n async def as_async_generator(\n-    it: Iterable[_T] | AsyncIterable[_T],\n+    it: Iterable[_T] | AsyncIterator[_T],\n ) -> AsyncGenerator[_T]:\n     \"\"\"Wraps an iterable (sync or async) into an async generator.\"\"\"\n-    if isinstance(it, AsyncIterable):\n+    if isinstance(it, AsyncIterator):\n         async for r in it:\n             yield r\n     else:\n\n@@ -14,7 +14,11 @@ from types import CoroutineType\n from typing import TYPE_CHECKING, Any, Generic, TypeVar, Union, cast, overload\n \n from twisted.internet import defer\n-from twisted.internet.defer import Deferred, DeferredList, ensureDeferred\n+from twisted.internet.defer import (\n+    Deferred,\n+    DeferredList,\n+    ensureDeferred,\n+)\n from twisted.internet.task import Cooperator\n from twisted.python import failure\n \n@@ -22,7 +26,7 @@ from scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\n from scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed\n \n if TYPE_CHECKING:\n-    from collections.abc import AsyncIterable, AsyncIterator, Callable\n+    from collections.abc import AsyncIterator, Callable\n \n     from twisted.python.failure import Failure\n \n@@ -36,6 +40,9 @@ _T = TypeVar(\"_T\")\n _T2 = TypeVar(\"_T2\")\n \n \n+_DEFER_DELAY = 0.1\n+\n+\n def defer_fail(_failure: Failure) -> Deferred[Any]:\n     \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n     next reactor loop\n@@ -46,7 +53,7 @@ def defer_fail(_failure: Failure) -> Deferred[Any]:\n     from twisted.internet import reactor\n \n     d: Deferred[Any] = Deferred()\n-    reactor.callLater(0.1, d.errback, _failure)\n+    reactor.callLater(_DEFER_DELAY, d.errback, _failure)\n     return d\n \n \n@@ -60,7 +67,16 @@ def defer_succeed(result: _T) -> Deferred[_T]:\n     from twisted.internet import reactor\n \n     d: Deferred[_T] = Deferred()\n-    reactor.callLater(0.1, d.callback, result)\n+    reactor.callLater(_DEFER_DELAY, d.callback, result)\n+    return d\n+\n+\n+def _defer_sleep() -> Deferred[None]:\n+    \"\"\"Like ``defer_succeed`` and ``defer_fail`` but doesn't call any real callbacks.\"\"\"\n+    from twisted.internet import reactor\n+\n+    d: Deferred[None] = Deferred()\n+    reactor.callLater(_DEFER_DELAY, d.callback, None)\n     return d\n \n \n@@ -177,7 +193,7 @@ class _AsyncCooperatorAdapter(Iterator, Generic[_T]):\n \n     def __init__(\n         self,\n-        aiterable: AsyncIterable[_T],\n+        aiterable: AsyncIterator[_T],\n         callable: Callable[Concatenate[_T, _P], Deferred[Any] | None],\n         *callable_args: _P.args,\n         **callable_kwargs: _P.kwargs,\n@@ -234,7 +250,7 @@ class _AsyncCooperatorAdapter(Iterator, Generic[_T]):\n \n \n def parallel_async(\n-    async_iterable: AsyncIterable[_T],\n+    async_iterable: AsyncIterator[_T],\n     count: int,\n     callable: Callable[Concatenate[_T, _P], Deferred[Any] | None],\n     *args: _P.args,\n@@ -332,13 +348,13 @@ def iter_errback(\n \n \n async def aiter_errback(\n-    aiterable: AsyncIterable[_T],\n+    aiterable: AsyncIterator[_T],\n     errback: Callable[Concatenate[Failure, _P], Any],\n     *a: _P.args,\n     **kw: _P.kwargs,\n-) -> AsyncIterable[_T]:\n+) -> AsyncIterator[_T]:\n     \"\"\"Wraps an async iterable calling an errback if an error is caught while\n-    iterating it. Similar to scrapy.utils.defer.iter_errback()\n+    iterating it. Similar to :func:`scrapy.utils.defer.iter_errback`.\n     \"\"\"\n     it = aiterable.__aiter__()\n     while True:\n\n@@ -18,10 +18,10 @@ def get_engine_status(engine: ExecutionEngine) -> list[tuple[str, Any]]:\n         \"engine.scraper.is_idle()\",\n         \"engine.spider.name\",\n         \"engine.spider_is_idle()\",\n-        \"engine.slot.closing\",\n-        \"len(engine.slot.inprogress)\",\n-        \"len(engine.slot.scheduler.dqs or [])\",\n-        \"len(engine.slot.scheduler.mqs)\",\n+        \"engine._slot.closing\",\n+        \"len(engine._slot.inprogress)\",\n+        \"len(engine._slot.scheduler.dqs or [])\",\n+        \"len(engine._slot.scheduler.mqs)\",\n         \"len(engine.scraper.slot.queue)\",\n         \"len(engine.scraper.slot.active)\",\n         \"engine.scraper.slot.active_size\",\n\n@@ -10,7 +10,7 @@ import re\n import sys\n import warnings\n import weakref\n-from collections.abc import AsyncIterable, Iterable, Mapping\n+from collections.abc import AsyncIterator, Iterable, Mapping\n from functools import partial, wraps\n from itertools import chain\n from typing import TYPE_CHECKING, Any, TypeVar, overload\n@@ -19,11 +19,12 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.asyncgen import as_async_generator\n \n if TYPE_CHECKING:\n-    from collections.abc import AsyncIterator, Callable, Iterator\n+    from collections.abc import Callable, Iterator\n     from re import Pattern\n \n     # typing.Concatenate and typing.ParamSpec require Python 3.10\n-    from typing_extensions import Concatenate, ParamSpec\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Concatenate, ParamSpec, Self\n \n     _P = ParamSpec(\"_P\")\n \n@@ -369,25 +370,25 @@ class MutableChain(Iterable[_T]):\n \n \n async def _async_chain(\n-    *iterables: Iterable[_T] | AsyncIterable[_T],\n+    *iterables: Iterable[_T] | AsyncIterator[_T],\n ) -> AsyncIterator[_T]:\n     for it in iterables:\n         async for o in as_async_generator(it):\n             yield o\n \n \n-class MutableAsyncChain(AsyncIterable[_T]):\n+class MutableAsyncChain(AsyncIterator[_T]):\n     \"\"\"\n     Similar to MutableChain but for async iterables\n     \"\"\"\n \n-    def __init__(self, *args: Iterable[_T] | AsyncIterable[_T]):\n+    def __init__(self, *args: Iterable[_T] | AsyncIterator[_T]):\n         self.data: AsyncIterator[_T] = _async_chain(*args)\n \n-    def extend(self, *iterables: Iterable[_T] | AsyncIterable[_T]) -> None:\n+    def extend(self, *iterables: Iterable[_T] | AsyncIterator[_T]) -> None:\n         self.data = _async_chain(self.data, _async_chain(*iterables))\n \n-    def __aiter__(self) -> AsyncIterator[_T]:\n+    def __aiter__(self) -> Self:\n         return self\n \n     async def __anext__(self) -> _T:\n\n@@ -7,6 +7,7 @@ from typing import TYPE_CHECKING, Any, Generic, TypeVar\n from warnings import catch_warnings, filterwarnings\n \n from twisted.internet import asyncioreactor, error\n+from twisted.internet.defer import Deferred\n \n from scrapy.utils.misc import load_object\n \n@@ -54,6 +55,7 @@ class CallLaterOnce(Generic[_T]):\n         self._a: tuple[Any, ...] = a\n         self._kw: dict[str, Any] = kw\n         self._call: DelayedCall | None = None\n+        self._deferreds: list[Deferred] = []\n \n     def schedule(self, delay: float = 0) -> None:\n         from twisted.internet import reactor\n@@ -66,8 +68,23 @@ class CallLaterOnce(Generic[_T]):\n             self._call.cancel()\n \n     def __call__(self) -> _T:\n+        from twisted.internet import reactor\n+\n         self._call = None\n-        return self._func(*self._a, **self._kw)\n+        result = self._func(*self._a, **self._kw)\n+\n+        for d in self._deferreds:\n+            reactor.callLater(0, d.callback, None)\n+        self._deferreds = []\n+\n+        return result\n+\n+    async def wait(self):\n+        from scrapy.utils.defer import maybe_deferred_to_future\n+\n+        d = Deferred()\n+        self._deferreds.append(d)\n+        await maybe_deferred_to_future(d)\n \n \n def set_asyncio_event_loop_policy() -> None:\n@@ -114,6 +131,8 @@ def set_asyncio_event_loop(event_loop_path: str | None) -> AbstractEventLoop:\n     \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n     if event_loop_path is not None:\n         event_loop_class: type[AbstractEventLoop] = load_object(event_loop_path)\n+        event_loop = _get_asyncio_event_loop()\n+        if not isinstance(event_loop, event_loop_class):\n             event_loop = event_loop_class()\n             asyncio.set_event_loop(event_loop)\n     else:\n\n@@ -13,9 +13,10 @@ class NoRequestsSpider(scrapy.Spider):\n         spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n         return spider\n \n-    def start_requests(self):\n+    async def start(self):\n         self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n-        return []\n+        return\n+        yield\n \n \n process = CrawlerProcess(settings={})\n\n@@ -5,8 +5,9 @@ from scrapy.crawler import CrawlerProcess\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(\n\n@@ -12,8 +12,9 @@ class ReactorCheckExtension:\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(\n\n@@ -38,8 +38,9 @@ class ReactorCheckExtension:\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(\n\n@@ -15,8 +15,9 @@ from scrapy.crawler import CrawlerProcess  # noqa: E402\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(\n\n@@ -16,8 +16,9 @@ from scrapy.crawler import CrawlerProcess  # noqa: E402\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(\n\n@@ -11,7 +11,7 @@ class CachingHostnameResolverSpider(scrapy.Spider):\n \n     name = \"caching_hostname_resolver_spider\"\n \n-    def start_requests(self):\n+    async def start(self):\n         yield scrapy.Request(self.url)\n \n     def parse(self, response):\n\n@@ -5,8 +5,9 @@ from scrapy.crawler import CrawlerProcess\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(settings={})\n\n@@ -8,8 +8,9 @@ from scrapy.crawler import CrawlerProcess\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(settings={})\n\n@@ -8,8 +8,9 @@ from scrapy.crawler import CrawlerProcess\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(\n\n@@ -10,8 +10,9 @@ selectreactor.install()\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(settings={})\n\n@@ -17,8 +17,9 @@ installReactor(reactor)\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(\n\n@@ -9,8 +9,9 @@ selectreactor.install()\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(\n\n@@ -5,8 +5,9 @@ from scrapy.crawler import CrawlerProcess\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n process = CrawlerProcess(settings={})\n\n@@ -10,8 +10,9 @@ class NoRequestsSpider(Spider):\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n     }\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\", \"LOG_LEVEL\": \"DEBUG\"})\n\n@@ -32,7 +32,7 @@ def createResolver(servers=None, resolvconf=None, hosts=None):\n class LocalhostSpider(Spider):\n     name = \"localhost_spider\"\n \n-    def start_requests(self):\n+    async def start(self):\n         yield Request(self.url)\n \n     def parse(self, response):\n\n@@ -8,6 +8,9 @@ import os\n import socket\n from pathlib import Path\n \n+from twisted import version as TWISTED_VERSION\n+from twisted.python.versions import Version\n+\n # ignore system-wide proxies for tests\n # which would send requests to a totally unsuspecting server\n # (e.g. because urllib does not fully understand the proxy spec)\n@@ -30,3 +33,6 @@ except socket.gaierror:\n def get_testdata(*paths: str) -> bytes:\n     \"\"\"Return test data\"\"\"\n     return Path(tests_datadir, *paths).read_bytes()\n+\n+\n+TWISTED_KEEPS_TRACEBACKS = TWISTED_VERSION >= Version(\"twisted\", 24, 10, 0)\n\n@@ -68,7 +68,7 @@ class DelaySpider(MetaSpider):\n         self.b = b\n         self.t1 = self.t2 = self.t2_err = 0\n \n-    def start_requests(self):\n+    async def start(self):\n         self.t1 = time.time()\n         url = self.mockserver.url(f\"/delay?n={self.n}&b={self.b}\")\n         yield Request(url, callback=self.parse, errback=self.errback)\n@@ -105,7 +105,7 @@ class LogSpider(MetaSpider):\n class SlowSpider(DelaySpider):\n     name = \"slow\"\n \n-    def start_requests(self):\n+    async def start(self):\n         # 1st response is fast\n         url = self.mockserver.url(\"/delay?n=0&b=0\")\n         yield Request(url, callback=self.parse, errback=self.errback)\n@@ -255,7 +255,7 @@ class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n             callback=cb,\n         )\n \n-    def start_requests(self):\n+    async def start(self):\n         for i in range(1, self.initial_reqs + 1):\n             yield self._get_req(i)\n \n@@ -319,7 +319,7 @@ class ErrorSpider(FollowAllSpider):\n             self.raise_exception()\n \n \n-class BrokenStartRequestsSpider(FollowAllSpider):\n+class BrokenStartSpider(FollowAllSpider):\n     fail_before_yield = False\n     fail_yielding = False\n \n@@ -327,7 +327,7 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n         super().__init__(*a, **kw)\n         self.seedsseen = []\n \n-    def start_requests(self):\n+    async def start(self):\n         if self.fail_before_yield:\n             1 / 0\n \n@@ -338,22 +338,20 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n             if self.fail_yielding:\n                 2 / 0\n \n-        assert self.seedsseen, (\n-            \"All start requests consumed before any download happened\"\n-        )\n+        assert self.seedsseen, \"All seeds consumed before any download happened\"\n \n     def parse(self, response):\n         self.seedsseen.append(response.meta.get(\"seed\"))\n         yield from super().parse(response)\n \n \n-class StartRequestsItemSpider(FollowAllSpider):\n-    def start_requests(self):\n+class StartItemSpider(FollowAllSpider):\n+    async def start(self):\n         yield {\"name\": \"test item\"}\n \n \n-class StartRequestsGoodAndBadOutput(FollowAllSpider):\n-    def start_requests(self):\n+class StartGoodAndBadOutput(FollowAllSpider):\n+    async def start(self):\n         yield {\"a\": \"a\"}\n         yield Request(\"data:,a\")\n         yield \"data:,b\"\n@@ -365,7 +363,7 @@ class SingleRequestSpider(MetaSpider):\n     callback_func = None\n     errback_func = None\n \n-    def start_requests(self):\n+    async def start(self):\n         if isinstance(self.seed, Request):\n             yield self.seed.replace(callback=self.parse, errback=self.on_error)\n         else:\n@@ -386,13 +384,13 @@ class SingleRequestSpider(MetaSpider):\n         return None\n \n \n-class DuplicateStartRequestsSpider(MockServerSpider):\n+class DuplicateStartSpider(MockServerSpider):\n     dont_filter = True\n     name = \"duplicatestartrequests\"\n     distinct_urls = 2\n     dupe_factor = 3\n \n-    def start_requests(self):\n+    async def start(self):\n         for i in range(self.distinct_urls):\n             for j in range(self.dupe_factor):\n                 url = self.mockserver.url(f\"/echo?headers=1&body=test{i}\")\n@@ -417,7 +415,7 @@ class CrawlSpiderWithParseMethod(MockServerSpider, CrawlSpider):\n     }\n     rules = (Rule(LinkExtractor(), callback=\"parse\", follow=True),)\n \n-    def start_requests(self):\n+    async def start(self):\n         test_body = b\"\"\"\n         <html>\n             <head><title>Page title<title></head>\n@@ -471,7 +469,7 @@ class CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):\n     name = \"crawl_spider_with_errback\"\n     rules = (Rule(LinkExtractor(), callback=\"parse\", errback=\"errback\", follow=True),)\n \n-    def start_requests(self):\n+    async def start(self):\n         test_body = b\"\"\"\n         <html>\n             <head><title>Page title<title></head>\n@@ -516,7 +514,7 @@ class BytesReceivedCallbackSpider(MetaSpider):\n         crawler.signals.connect(spider.bytes_received, signals.bytes_received)\n         return spider\n \n-    def start_requests(self):\n+    async def start(self):\n         body = b\"a\" * self.full_response_length\n         url = self.mockserver.url(\"/alpayload\")\n         yield Request(url, method=\"POST\", body=body, errback=self.errback)\n@@ -545,7 +543,7 @@ class HeadersReceivedCallbackSpider(MetaSpider):\n         crawler.signals.connect(spider.headers_received, signals.headers_received)\n         return spider\n \n-    def start_requests(self):\n+    async def start(self):\n         yield Request(self.mockserver.url(\"/status\"), errback=self.errback)\n \n     def parse(self, response):\n\n@@ -2,17 +2,26 @@ import sys\n from pathlib import Path\n from subprocess import PIPE, Popen\n \n+from .. import TWISTED_KEEPS_TRACEBACKS\n+\n \n class TestCmdlineCrawlPipeline:\n     def _execute(self, spname):\n         args = (sys.executable, \"-m\", \"scrapy.cmdline\", \"crawl\", spname)\n         cwd = Path(__file__).resolve().parent\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, cwd=cwd)\n-        proc.communicate()\n-        return proc.returncode\n+        _, stderr = proc.communicate()\n+        return proc.returncode, stderr\n \n     def test_open_spider_normally_in_pipeline(self):\n-        assert self._execute(\"normal\") == 0\n+        returncode, stderr = self._execute(\"normal\")\n+        assert returncode == 0\n \n     def test_exception_at_open_spider_in_pipeline(self):\n-        assert self._execute(\"exception\") == 1\n+        returncode, stderr = self._execute(\"exception\")\n+        # An unhandled exception in a pipeline should not stop the crawl\n+        assert returncode == 0\n+        if TWISTED_KEEPS_TRACEBACKS:\n+            assert b'RuntimeError(\"exception\")' in stderr\n+        else:\n+            assert b\"RuntimeError: exception\" in stderr\n\n@@ -670,9 +670,10 @@ import scrapy\n class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n-    def start_requests(self):\n+    async def start(self):\n         self.logger.debug(\"It Works!\")\n-        return []\n+        return\n+        yield\n \"\"\"\n \n     badspider = \"\"\"\n@@ -680,8 +681,9 @@ import scrapy\n \n class BadSpider(scrapy.Spider):\n     name = \"bad\"\n-    def start_requests(self):\n+    async def start(self):\n         raise Exception(\"oops!\")\n+        yield\n         \"\"\"\n \n     @contextmanager\n@@ -771,10 +773,10 @@ class MySpider(scrapy.Spider):\n         log = self.get_log(\"\", name=\"myspider.txt\")\n         assert \"Unable to load\" in log\n \n-    def test_start_requests_errors(self):\n+    def test_start_errors(self):\n         log = self.get_log(self.badspider, name=\"badspider.py\")\n-        assert \"start_requests\" in log\n-        assert \"badspider.py\" in log\n+        assert \"start\" in log\n+        assert \"badspider.py\" in log, log\n \n     def test_asyncio_enabled_true(self):\n         log = self.get_log(\n@@ -846,9 +848,10 @@ import scrapy\n class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n-    def start_requests(self):\n+    async def start(self):\n         self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n-        return []\n+        return\n+        yield\n \"\"\"\n         args = [\"-o\", \"example.json\"]\n         log = self.get_log(spider_code, args=args)\n@@ -862,13 +865,14 @@ import scrapy\n class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n-    def start_requests(self):\n+    async def start(self):\n         self.logger.debug(\n             'FEEDS: {}'.format(\n                 json.dumps(self.settings.getdict('FEEDS'), sort_keys=True)\n             )\n         )\n-        return []\n+        return\n+        yield\n \"\"\"\n         Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n         args = [\"-O\", \"example.json\"]\n@@ -888,8 +892,9 @@ import scrapy\n class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \"\"\"\n         args = [\"-o\", \"example1.json\", \"-O\", \"example2.json\"]\n         log = self.get_log(spider_code, args=args)\n@@ -904,9 +909,10 @@ import scrapy\n class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n-    def start_requests(self):\n+    async def start(self):\n         self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n-        return []\n+        return\n+        yield\n \"\"\"\n         args = [\"-o\", \"-:json\"]\n         log = self.get_log(spider_code, args=args)\n@@ -983,9 +989,10 @@ class MySpider(scrapy.Spider):\n         spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n         return spider\n \n-    def start_requests(self):\n+    async def start(self):\n         self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n-        return []\n+        return\n+        yield\n \"\"\"\n         args = [\"-a\", \"foo=42\"]\n         log = self.get_log(spider_code, args=args)\n@@ -1001,9 +1008,9 @@ class TestWindowsRunSpiderCommand(TestRunSpiderCommand):\n             raise unittest.SkipTest(\"Windows required for .pyw files\")\n         return super().setUp()\n \n-    def test_start_requests_errors(self):\n+    def test_start_errors(self):\n         log = self.get_log(self.badspider, name=\"badspider.pyw\")\n-        assert \"start_requests\" in log\n+        assert \"start\" in log\n         assert \"badspider.pyw\" in log\n \n     def test_runspider_unable_to_load(self):\n@@ -1053,9 +1060,10 @@ import scrapy\n class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n-    def start_requests(self):\n+    async def start(self):\n         self.logger.debug('It works!')\n-        return []\n+        return\n+        yield\n \"\"\"\n         log = self.get_log(spider_code)\n         assert \"[myspider] DEBUG: It works!\" in log\n@@ -1067,9 +1075,10 @@ import scrapy\n class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n-    def start_requests(self):\n+    async def start(self):\n         self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n-        return []\n+        return\n+        yield\n \"\"\"\n         args = [\"-o\", \"example.json\"]\n         log = self.get_log(spider_code, args=args)\n@@ -1083,13 +1092,14 @@ import scrapy\n class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n-    def start_requests(self):\n+    async def start(self):\n         self.logger.debug(\n             'FEEDS: {}'.format(\n                 json.dumps(self.settings.getdict('FEEDS'), sort_keys=True)\n             )\n         )\n-        return []\n+        return\n+        yield\n \"\"\"\n         Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n         args = [\"-O\", \"example.json\"]\n@@ -1109,8 +1119,9 @@ import scrapy\n class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \"\"\"\n         args = [\"-o\", \"example1.json\", \"-O\", \"example2.json\"]\n         log = self.get_log(spider_code, args=args)\n\n@@ -511,8 +511,9 @@ class TestContractsManager(unittest.TestCase):\n                 super().__init__(*args, **kwargs)\n                 self.visited = 0\n \n-            def start_requests(self_):  # pylint: disable=no-self-argument\n-                return self.conman.from_spider(self_, self.results)\n+            async def start(self_):  # pylint: disable=no-self-argument\n+                for item_or_request in self.conman.from_spider(self_, self.results):\n+                    yield item_or_request\n \n             def parse_first(self, response):\n                 self.visited += 1\n\n@@ -36,7 +36,7 @@ from tests.spiders import (\n     AsyncDefDeferredMaybeWrappedSpider,\n     AsyncDefDeferredWrappedSpider,\n     AsyncDefSpider,\n-    BrokenStartRequestsSpider,\n+    BrokenStartSpider,\n     BytesReceivedCallbackSpider,\n     BytesReceivedErrbackSpider,\n     CrawlSpiderWithAsyncCallback,\n@@ -45,14 +45,14 @@ from tests.spiders import (\n     CrawlSpiderWithParseMethod,\n     CrawlSpiderWithProcessRequestCallbackKeywordArguments,\n     DelaySpider,\n-    DuplicateStartRequestsSpider,\n+    DuplicateStartSpider,\n     FollowAllSpider,\n     HeadersReceivedCallbackSpider,\n     HeadersReceivedErrbackSpider,\n     SimpleSpider,\n     SingleRequestSpider,\n-    StartRequestsGoodAndBadOutput,\n-    StartRequestsItemSpider,\n+    StartGoodAndBadOutput,\n+    StartItemSpider,\n )\n \n \n@@ -165,9 +165,9 @@ class TestCrawl(TestCase):\n         self._assert_retried(log)\n \n     @defer.inlineCallbacks\n-    def test_start_requests_bug_before_yield(self):\n+    def test_start_bug_before_yield(self):\n         with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n-            crawler = get_crawler(BrokenStartRequestsSpider)\n+            crawler = get_crawler(BrokenStartSpider)\n             yield crawler.crawl(fail_before_yield=1, mockserver=self.mockserver)\n \n         assert len(log.records) == 1\n@@ -176,9 +176,9 @@ class TestCrawl(TestCase):\n         assert record.exc_info[0] is ZeroDivisionError\n \n     @defer.inlineCallbacks\n-    def test_start_requests_bug_yielding(self):\n+    def test_start_bug_yielding(self):\n         with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n-            crawler = get_crawler(BrokenStartRequestsSpider)\n+            crawler = get_crawler(BrokenStartSpider)\n             yield crawler.crawl(fail_yielding=1, mockserver=self.mockserver)\n \n         assert len(log.records) == 1\n@@ -187,14 +187,14 @@ class TestCrawl(TestCase):\n         assert record.exc_info[0] is ZeroDivisionError\n \n     @defer.inlineCallbacks\n-    def test_start_requests_items(self):\n+    def test_start_items(self):\n         items = []\n \n         def _on_item_scraped(item):\n             items.append(item)\n \n         with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n-            crawler = get_crawler(StartRequestsItemSpider)\n+            crawler = get_crawler(StartItemSpider)\n             crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n             yield crawler.crawl(mockserver=self.mockserver)\n \n@@ -202,11 +202,11 @@ class TestCrawl(TestCase):\n         assert items == [{\"name\": \"test item\"}]\n \n     @defer.inlineCallbacks\n-    def test_start_requests_unsupported_output(self):\n+    def test_start_unsupported_output(self):\n         \"\"\"Anything that is not a request is assumed to be an item, avoiding a\n-        potentially expensive call to itemadapter.is_item, and letting instead\n-        things fail when ItemAdapter is actually used on the corresponding\n-        non-item object.\"\"\"\n+        potentially expensive call to itemadapter.is_item(), and letting\n+        instead things fail when ItemAdapter is actually used on the\n+        corresponding non-item object.\"\"\"\n \n         items = []\n \n@@ -214,7 +214,7 @@ class TestCrawl(TestCase):\n             items.append(item)\n \n         with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n-            crawler = get_crawler(StartRequestsGoodAndBadOutput)\n+            crawler = get_crawler(StartGoodAndBadOutput)\n             crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n             yield crawler.crawl(mockserver=self.mockserver)\n \n@@ -223,24 +223,15 @@ class TestCrawl(TestCase):\n         assert not any(isinstance(item, Request) for item in items)\n \n     @defer.inlineCallbacks\n-    def test_start_requests_laziness(self):\n+    def test_start_dupes(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n-        crawler = get_crawler(BrokenStartRequestsSpider, settings)\n-        yield crawler.crawl(mockserver=self.mockserver)\n-        assert crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(\n-            99\n-        ), crawler.spider.seedsseen\n-\n-    @defer.inlineCallbacks\n-    def test_start_requests_dupes(self):\n-        settings = {\"CONCURRENT_REQUESTS\": 1}\n-        crawler = get_crawler(DuplicateStartRequestsSpider, settings)\n+        crawler = get_crawler(DuplicateStartSpider, settings)\n         yield crawler.crawl(\n             dont_filter=True, distinct_urls=2, dupe_factor=3, mockserver=self.mockserver\n         )\n         assert crawler.spider.visited == 6\n \n-        crawler = get_crawler(DuplicateStartRequestsSpider, settings)\n+        crawler = get_crawler(DuplicateStartSpider, settings)\n         yield crawler.crawl(\n             dont_filter=False,\n             distinct_urls=3,\n@@ -322,10 +313,10 @@ with multiples lines\n         # basic asserts in case of weird communication errors\n         assert \"responses\" in crawler.spider.meta\n         assert \"failures\" not in crawler.spider.meta\n-        # start requests doesn't set Referer header\n+        # start() doesn't set Referer header\n         echo0 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][2].body))\n         assert \"Referer\" not in echo0[\"headers\"]\n-        # following request sets Referer to start request url\n+        # following request sets Referer to the source request url\n         echo1 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][1].body))\n         assert echo1[\"headers\"].get(\"Referer\") == [req0.url]\n         # next request avoids Referer header\n@@ -378,27 +369,6 @@ with multiples lines\n         assert s[\"engine.spider.name\"] == crawler.spider.name\n         assert s[\"len(engine.scraper.slot.active)\"] == \"1\"\n \n-    @defer.inlineCallbacks\n-    def test_graceful_crawl_error_handling(self):\n-        \"\"\"\n-        Test whether errors happening anywhere in Crawler.crawl() are properly\n-        reported (and not somehow swallowed) after a graceful engine shutdown.\n-        The errors should not come from within Scrapy's core but from within\n-        spiders/middlewares/etc., e.g. raised in Spider.start_requests(),\n-        SpiderMiddleware.process_start_requests(), etc.\n-        \"\"\"\n-\n-        class TestError(Exception):\n-            pass\n-\n-        class FaultySpider(SimpleSpider):\n-            def start_requests(self):\n-                raise TestError\n-\n-        crawler = get_crawler(FaultySpider)\n-        yield self.assertFailure(crawler.crawl(mockserver=self.mockserver), TestError)\n-        assert not crawler.crawling\n-\n     @defer.inlineCallbacks\n     def test_open_spider_error_on_faulty_pipeline(self):\n         settings = {\n\n@@ -153,7 +153,7 @@ class TestCrawler(TestBaseCrawler):\n                 super().__init__(**kwargs)\n                 self.crawler = crawler\n \n-            def start_requests(self):\n+            async def start(self):\n                 MySpider.result = crawler.get_downloader_middleware(MySpider.cls)\n                 return\n                 yield\n@@ -233,7 +233,7 @@ class TestCrawler(TestBaseCrawler):\n                 super().__init__(**kwargs)\n                 self.crawler = crawler\n \n-            def start_requests(self):\n+            async def start(self):\n                 MySpider.result = crawler.get_extension(MySpider.cls)\n                 return\n                 yield\n@@ -313,7 +313,7 @@ class TestCrawler(TestBaseCrawler):\n                 super().__init__(**kwargs)\n                 self.crawler = crawler\n \n-            def start_requests(self):\n+            async def start(self):\n                 MySpider.result = crawler.get_item_pipeline(MySpider.cls)\n                 return\n                 yield\n@@ -393,7 +393,7 @@ class TestCrawler(TestBaseCrawler):\n                 super().__init__(**kwargs)\n                 self.crawler = crawler\n \n-            def start_requests(self):\n+            async def start(self):\n                 MySpider.result = crawler.get_spider_middleware(MySpider.cls)\n                 return\n                 yield\n@@ -580,8 +580,9 @@ class ExceptionSpider(scrapy.Spider):\n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n \n-    def start_requests(self):\n-        return []\n+    async def start(self):\n+        return\n+        yield\n \n \n @pytest.mark.usefixtures(\"reactor_pytest\")\n\n@@ -25,7 +25,7 @@ class TestManagerBase(TestCase):\n         self.spider = self.crawler._create_spider(\"foo\")\n         self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n         self.crawler.engine = self.crawler._create_engine()\n-        return self.crawler.engine.open_spider(self.spider, start_requests=())\n+        return self.crawler.engine.open_spider(self.spider)\n \n     def tearDown(self):\n         return self.crawler.engine.close_spider(self.spider)\n\n@@ -28,7 +28,7 @@ class DownloaderSlotsSettingsTestSpider(MetaSpider):\n         },\n     }\n \n-    def start_requests(self):\n+    async def start(self):\n         self.times = {None: []}\n \n         slots = [*self.custom_settings.get(\"DOWNLOAD_SLOTS\", {}), None]\n\n@@ -29,7 +29,7 @@ from twisted.trial import unittest\n from twisted.web import server, static, util\n \n from scrapy import signals\n-from scrapy.core.engine import ExecutionEngine, Slot\n+from scrapy.core.engine import ExecutionEngine, _Slot\n from scrapy.core.scheduler import BaseScheduler\n from scrapy.exceptions import CloseSpider, IgnoreRequest\n from scrapy.http import Request\n@@ -92,8 +92,9 @@ class MySpider(Spider):\n \n \n class DupeFilterSpider(MySpider):\n-    def start_requests(self):\n-        return (Request(url) for url in self.start_urls)  # no dont_filter=True\n+    async def start(self):\n+        for url in self.start_urls:\n+            yield Request(url)  # no dont_filter=True\n \n \n class DictItemsSpider(MySpider):\n@@ -149,7 +150,6 @@ class CrawlerRun:\n     \"\"\"A class to run the crawler and keep track of events occurred\"\"\"\n \n     def __init__(self, spider_class):\n-        self.spider = None\n         self.respplug = []\n         self.reqplug = []\n         self.reqdropped = []\n@@ -190,7 +190,6 @@ class CrawlerRun:\n             self.response_downloaded, signals.response_downloaded\n         )\n         self.crawler.crawl(start_urls=start_urls)\n-        self.spider = self.crawler.spider\n \n         self.deferred = defer.Deferred()\n         dispatcher.connect(self.stop, signals.engine_stopped)\n@@ -296,7 +295,7 @@ class TestEngineBase(unittest.TestCase):\n         assert len(run.itemerror) == 2\n         for item, response, spider, failure in run.itemerror:\n             assert failure.value.__class__ is ZeroDivisionError\n-            assert spider == run.spider\n+            assert spider == run.crawler.spider\n \n             assert item[\"url\"] == response.url\n             if \"item1.html\" in item[\"url\"]:\n@@ -377,11 +376,14 @@ class TestEngineBase(unittest.TestCase):\n         assert signals.spider_closed in run.signals_caught\n         assert signals.headers_received in run.signals_caught\n \n-        assert {\"spider\": run.spider} == run.signals_caught[signals.spider_opened]\n-        assert {\"spider\": run.spider} == run.signals_caught[signals.spider_idle]\n-        assert {\"spider\": run.spider, \"reason\": \"finished\"} == run.signals_caught[\n-            signals.spider_closed\n+        assert {\"spider\": run.crawler.spider} == run.signals_caught[\n+            signals.spider_opened\n         ]\n+        assert {\"spider\": run.crawler.spider} == run.signals_caught[signals.spider_idle]\n+        assert {\n+            \"spider\": run.crawler.spider,\n+            \"reason\": \"finished\",\n+        } == run.signals_caught[signals.spider_closed]\n \n \n class TestEngine(TestEngineBase):\n@@ -419,9 +421,10 @@ class TestEngine(TestEngineBase):\n     def test_crawler_change_close_reason_on_idle(self):\n         run = CrawlerRun(ChangeCloseReasonSpider)\n         yield run.run()\n-        assert {\"spider\": run.spider, \"reason\": \"custom_reason\"} == run.signals_caught[\n-            signals.spider_closed\n-        ]\n+        assert {\n+            \"spider\": run.crawler.spider,\n+            \"reason\": \"custom_reason\",\n+        } == run.signals_caught[signals.spider_closed]\n \n     @defer.inlineCallbacks\n     def test_close_downloader(self):\n@@ -471,7 +474,7 @@ class TestEngine(TestEngineBase):\n         finally:\n             timer.cancel()\n \n-        assert b\"Traceback\" not in stderr\n+        assert b\"Traceback\" not in stderr, stderr\n \n \n def test_request_scheduled_signal(caplog):\n@@ -491,7 +494,13 @@ def test_request_scheduled_signal(caplog):\n     engine = ExecutionEngine(crawler, lambda _: None)\n     engine.downloader._slot_gc_loop.stop()\n     scheduler = TestScheduler()\n-    engine.slot = Slot((), None, Mock(), scheduler)\n+\n+    async def start():\n+        return\n+        yield\n+\n+    engine._start = start()\n+    engine._slot = _Slot(False, Mock(), scheduler)\n     crawler.signals.connect(signal_handler, request_scheduled)\n     keep_request = Request(\"https://keep.example\")\n     engine._schedule_request(keep_request)\n\n@@ -0,0 +1,364 @@\n+from __future__ import annotations\n+\n+from collections import deque\n+from logging import ERROR\n+from typing import TYPE_CHECKING\n+\n+from testfixtures import LogCapture\n+from twisted.internet.defer import Deferred\n+from twisted.trial.unittest import TestCase\n+\n+from scrapy import Request, Spider, signals\n+from scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\n+from scrapy.utils.test import get_crawler\n+\n+from .mockserver import MockServer\n+from .test_scheduler import MemoryScheduler\n+\n+if TYPE_CHECKING:\n+    from scrapy.http import Response\n+\n+\n+async def sleep(seconds: float = 0.001) -> None:\n+    from twisted.internet import reactor\n+\n+    deferred: Deferred[None] = Deferred()\n+    reactor.callLater(seconds, deferred.callback, None)\n+    await maybe_deferred_to_future(deferred)\n+\n+\n+class MainTestCase(TestCase):\n+    @deferred_f_from_coro_f\n+    async def test_sleep(self):\n+        \"\"\"Neither asynchronous sleeps on Spider.start() nor the equivalent on\n+        the scheduler (returning no requests while also returning True from\n+        the has_pending_requests() method) should cause the spider to miss the\n+        processing of any later requests.\"\"\"\n+        seconds = 2\n+\n+        class TestSpider(Spider):\n+            name = \"test\"\n+\n+            async def start(self):\n+                from twisted.internet import reactor\n+\n+                yield Request(\"data:,a\")\n+\n+                await sleep(seconds)\n+\n+                self.crawler.engine._slot.scheduler.pause()\n+                self.crawler.engine._slot.scheduler.enqueue_request(Request(\"data:,b\"))\n+\n+                # During this time, the scheduler reports having requests but\n+                # returns None.\n+                await sleep(seconds)\n+\n+                self.crawler.engine._slot.scheduler.unpause()\n+\n+                # The scheduler request is processed.\n+                await sleep(seconds)\n+\n+                yield Request(\"data:,c\")\n+\n+                await sleep(seconds)\n+\n+                self.crawler.engine._slot.scheduler.pause()\n+                self.crawler.engine._slot.scheduler.enqueue_request(Request(\"data:,d\"))\n+\n+                # The last start request is processed during the time until the\n+                # delayed call below, proving that the start iteration can\n+                # finish before a scheduler “sleep” without causing the\n+                # scheduler to finish.\n+                reactor.callLater(seconds, self.crawler.engine._slot.scheduler.unpause)\n+\n+            def parse(self, response):\n+                pass\n+\n+        actual_urls = []\n+\n+        def track_url(request, spider):\n+            actual_urls.append(request.url)\n+\n+        settings = {\"SCHEDULER\": MemoryScheduler}\n+        crawler = get_crawler(TestSpider, settings_dict=settings)\n+        crawler.signals.connect(track_url, signals.request_reached_downloader)\n+        await maybe_deferred_to_future(crawler.crawl())\n+        assert crawler.stats.get_value(\"finish_reason\") == \"finished\"\n+        expected_urls = [\"data:,a\", \"data:,b\", \"data:,c\", \"data:,d\"]\n+        assert actual_urls == expected_urls, f\"{actual_urls=} != {expected_urls=}\"\n+\n+    @deferred_f_from_coro_f\n+    async def test_close_during_start_iteration(self):\n+        class TestSpider(Spider):\n+            name = \"test\"\n+\n+            async def start(self):\n+                assert self.crawler.engine is not None\n+                await maybe_deferred_to_future(self.crawler.engine.close())\n+                yield Request(\"data:,a\")\n+\n+            def parse(self, response):\n+                pass\n+\n+        actual_urls = []\n+\n+        def track_url(request, spider):\n+            actual_urls.append(request.url)\n+\n+        settings = {\"SCHEDULER\": MemoryScheduler}\n+        crawler = get_crawler(TestSpider, settings_dict=settings)\n+        crawler.signals.connect(track_url, signals.request_reached_downloader)\n+\n+        with LogCapture(level=ERROR) as log:\n+            await maybe_deferred_to_future(crawler.crawl())\n+\n+        assert not log.records, f\"{log.records=}\"\n+        finish_reason = crawler.stats.get_value(\"finish_reason\")\n+        assert finish_reason == \"shutdown\", f\"{finish_reason=}\"\n+        expected_urls = []\n+        assert actual_urls == expected_urls, f\"{actual_urls=} != {expected_urls=}\"\n+\n+\n+class RequestSendOrderTestCase(TestCase):\n+    seconds = 0.1  # increase if flaky\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)  # increase if flaky\n+\n+    def request(self, num, response_seconds, download_slots, priority=0):\n+        url = self.mockserver.url(f\"/delay?n={response_seconds}&{num}\")\n+        meta = {\"download_slot\": str(num % download_slots)}\n+        return Request(url, meta=meta, priority=priority)\n+\n+    def get_num(self, request_or_response: Request | Response):\n+        return int(request_or_response.url.rsplit(\"&\", maxsplit=1)[1])\n+\n+    @deferred_f_from_coro_f\n+    async def _test_request_order(\n+        self,\n+        start_nums,\n+        cb_nums=None,\n+        settings=None,\n+        response_seconds=None,\n+        download_slots=1,\n+        start_fn=None,\n+        parse_fn=None,\n+    ):\n+        cb_nums = cb_nums or []\n+        settings = settings or {}\n+        response_seconds = response_seconds or self.seconds\n+\n+        cb_requests = deque(\n+            [self.request(num, response_seconds, download_slots) for num in cb_nums]\n+        )\n+\n+        if start_fn is None:\n+\n+            async def start_fn(spider):\n+                for num in start_nums:\n+                    yield self.request(num, response_seconds, download_slots)\n+\n+        if parse_fn is None:\n+\n+            def parse_fn(spider, response):\n+                while cb_requests:\n+                    yield cb_requests.popleft()\n+\n+        class TestSpider(Spider):\n+            name = \"test\"\n+            start = start_fn\n+            parse = parse_fn\n+\n+        actual_nums = []\n+\n+        def track_num(request, spider):\n+            actual_nums.append(self.get_num(request))\n+\n+        crawler = get_crawler(TestSpider, settings_dict=settings)\n+        crawler.signals.connect(track_num, signals.request_reached_downloader)\n+        await maybe_deferred_to_future(crawler.crawl())\n+        assert crawler.stats.get_value(\"finish_reason\") == \"finished\"\n+        expected_nums = sorted(start_nums + cb_nums)\n+        assert actual_nums == expected_nums, f\"{actual_nums=} != {expected_nums=}\"\n+\n+    @deferred_f_from_coro_f\n+    async def test_default(self):\n+        \"\"\"By default, start requests take priority over callback requests and\n+        are sent in order. Priority matters, but given the same priority, a\n+        start request takes precedence.\"\"\"\n+        nums = [1, 2, 3, 4, 5, 6]\n+        response_seconds = 0\n+        download_slots = 1\n+\n+        def _request(num, priority=0):\n+            return self.request(\n+                num, response_seconds, download_slots, priority=priority\n+            )\n+\n+        async def start(spider):\n+            # The first CONCURRENT_REQUESTS start requests are sent\n+            # immediately.\n+            yield _request(1)\n+\n+            for request in (\n+                _request(4, priority=1),\n+                _request(6),\n+            ):\n+                spider.crawler.engine._slot.scheduler.enqueue_request(request)\n+            yield _request(5)\n+            yield _request(2, priority=1)\n+            yield _request(3, priority=1)\n+\n+        def parse(spider, response):\n+            return\n+            yield\n+\n+        await maybe_deferred_to_future(\n+            self._test_request_order(\n+                start_nums=nums,\n+                settings={\"CONCURRENT_REQUESTS\": 1},\n+                response_seconds=response_seconds,\n+                start_fn=start,\n+                parse_fn=parse,\n+            )\n+        )\n+\n+    @deferred_f_from_coro_f\n+    async def test_lifo_start(self):\n+        \"\"\"Changing the queues of start requests to LIFO, matching the queues\n+        of non-start requests, does not cause all requests to be stored in the\n+        same queue objects, it only affects the order of start requests.\"\"\"\n+        nums = [1, 2, 3, 4, 5, 6]\n+        response_seconds = 0\n+        download_slots = 1\n+\n+        def _request(num, priority=0):\n+            return self.request(\n+                num, response_seconds, download_slots, priority=priority\n+            )\n+\n+        async def start(spider):\n+            # The first CONCURRENT_REQUESTS start requests are sent\n+            # immediately.\n+            yield _request(1)\n+\n+            for request in (\n+                _request(4, priority=1),\n+                _request(6),\n+            ):\n+                spider.crawler.engine._slot.scheduler.enqueue_request(request)\n+            yield _request(5)\n+            yield _request(3, priority=1)\n+            yield _request(2, priority=1)\n+\n+        def parse(spider, response):\n+            return\n+            yield\n+\n+        await maybe_deferred_to_future(\n+            self._test_request_order(\n+                start_nums=nums,\n+                settings={\n+                    \"CONCURRENT_REQUESTS\": 1,\n+                    \"SCHEDULER_START_MEMORY_QUEUE\": \"scrapy.squeues.LifoMemoryQueue\",\n+                },\n+                response_seconds=response_seconds,\n+                start_fn=start,\n+                parse_fn=parse,\n+            )\n+        )\n+\n+    @deferred_f_from_coro_f\n+    async def test_shared_queues(self):\n+        \"\"\"If SCHEDULER_START_*_QUEUE is falsy, start requests and other\n+        requests share the same queue, i.e. start requests are not priorized\n+        over other requests if their priority matches.\"\"\"\n+        nums = list(range(1, 14))\n+        response_seconds = 0\n+        download_slots = 1\n+\n+        def _request(num, priority=0):\n+            return self.request(\n+                num, response_seconds, download_slots, priority=priority\n+            )\n+\n+        async def start(spider):\n+            # The first CONCURRENT_REQUESTS start requests are sent\n+            # immediately.\n+            yield _request(1)\n+\n+            # Below, priority 1 requests are sent first, and requests are sent\n+            # in LIFO order.\n+\n+            for request in (\n+                _request(7, priority=1),\n+                _request(6, priority=1),\n+                _request(13),\n+                _request(12),\n+            ):\n+                spider.crawler.engine._slot.scheduler.enqueue_request(request)\n+\n+            yield _request(11)\n+            yield _request(10)\n+            yield _request(5, priority=1)\n+            yield _request(4, priority=1)\n+\n+            for request in (\n+                _request(3, priority=1),\n+                _request(2, priority=1),\n+                _request(9),\n+                _request(8),\n+            ):\n+                spider.crawler.engine._slot.scheduler.enqueue_request(request)\n+\n+        def parse(spider, response):\n+            return\n+            yield\n+\n+        await maybe_deferred_to_future(\n+            self._test_request_order(\n+                start_nums=nums,\n+                settings={\n+                    \"CONCURRENT_REQUESTS\": 1,\n+                    \"SCHEDULER_START_MEMORY_QUEUE\": None,\n+                },\n+                response_seconds=response_seconds,\n+                start_fn=start,\n+                parse_fn=parse,\n+            )\n+        )\n+\n+    # Examples from the “Start requests” section of the documentation about\n+    # spiders.\n+\n+    @deferred_f_from_coro_f\n+    async def test_lazy(self):\n+        start_nums = [1, 2, 4]\n+        cb_nums = [3]\n+        response_seconds = self.seconds * 2**1  # increase if flaky\n+        download_slots = 1\n+\n+        async def start(spider):\n+            for num in start_nums:\n+                if spider.crawler.engine.needs_backout():\n+                    await spider.crawler.signals.wait_for(signals.scheduler_empty)\n+                request = self.request(num, response_seconds, download_slots)\n+                yield request\n+\n+        await maybe_deferred_to_future(\n+            self._test_request_order(\n+                start_nums=start_nums,\n+                cb_nums=cb_nums,\n+                settings={\n+                    \"CONCURRENT_REQUESTS\": 1,\n+                },\n+                response_seconds=response_seconds,\n+                start_fn=start,\n+            )\n+        )\n\n@@ -69,7 +69,7 @@ class AsyncDefNotAsyncioPipeline:\n class ItemSpider(Spider):\n     name = \"itemspider\"\n \n-    def start_requests(self):\n+    async def start(self):\n         yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n\n@@ -28,10 +28,10 @@ class InjectArgumentsSpiderMiddleware:\n     Make sure spider middlewares are able to update the keyword arguments\n     \"\"\"\n \n-    def process_start_requests(self, start_requests, spider):\n-        for request in start_requests:\n+    async def process_start(self, start):\n+        async for request in start:\n             if request.callback.__name__ == \"parse_spider_mw\":\n-                request.cb_kwargs[\"from_process_start_requests\"] = True\n+                request.cb_kwargs[\"from_process_start\"] = True\n             yield request\n \n     def process_spider_input(self, response, spider):\n@@ -62,7 +62,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n \n     checks: list[bool] = []\n \n-    def start_requests(self):\n+    async def start(self):\n         data = {\"key\": \"value\", \"number\": 123, \"callback\": \"some_callback\"}\n         yield Request(self.mockserver.url(\"/first\"), self.parse_first, cb_kwargs=data)\n         yield Request(\n@@ -138,11 +138,9 @@ class KeywordArgumentsSpider(MockServerSpider):\n         self.checks.append(bool(from_process_response))\n         self.crawler.stats.inc_value(\"boolean_checks\", 2)\n \n-    def parse_spider_mw(\n-        self, response, from_process_spider_input, from_process_start_requests\n-    ):\n+    def parse_spider_mw(self, response, from_process_spider_input, from_process_start):\n         self.checks.append(bool(from_process_spider_input))\n-        self.checks.append(bool(from_process_start_requests))\n+        self.checks.append(bool(from_process_start))\n         self.crawler.stats.inc_value(\"boolean_checks\", 2)\n         return Request(self.mockserver.url(\"/spider_mw_2\"), self.parse_spider_mw_2)\n \n\n@@ -3,6 +3,7 @@ from __future__ import annotations\n import shutil\n import tempfile\n from abc import ABC, abstractmethod\n+from collections import deque\n from typing import Any, NamedTuple\n \n import pytest\n@@ -10,7 +11,7 @@ from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n from scrapy.core.downloader import Downloader\n-from scrapy.core.scheduler import Scheduler\n+from scrapy.core.scheduler import BaseScheduler, Scheduler\n from scrapy.crawler import Crawler\n from scrapy.http import Request\n from scrapy.spiders import Spider\n@@ -20,6 +21,38 @@ from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n \n \n+class MemoryScheduler(BaseScheduler):\n+    paused = False\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.queue = deque(\n+            Request(value) if isinstance(value, str) else value\n+            for value in getattr(self, \"queue\", [])\n+        )\n+\n+    def enqueue_request(self, request: Request) -> bool:\n+        self.queue.append(request)\n+        return True\n+\n+    def has_pending_requests(self) -> bool:\n+        return self.paused or bool(self.queue)\n+\n+    def next_request(self) -> Request | None:\n+        if self.paused:\n+            return None\n+        try:\n+            return self.queue.pop()\n+        except IndexError:\n+            return None\n+\n+    def pause(self) -> None:\n+        self.paused = True\n+\n+    def unpause(self) -> None:\n+        self.paused = False\n+\n+\n class MockEngine(NamedTuple):\n     downloader: MockDownloader\n \n\n@@ -1,8 +1,9 @@\n import pytest\n from twisted.internet import defer\n-from twisted.trial import unittest\n+from twisted.trial.unittest import TestCase\n \n from scrapy import Request, Spider, signals\n+from scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\n from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n from tests.mockserver import MockServer\n \n@@ -10,7 +11,7 @@ from tests.mockserver import MockServer\n class ItemSpider(Spider):\n     name = \"itemspider\"\n \n-    def start_requests(self):\n+    async def start(self):\n         for index in range(10):\n             yield Request(\n                 self.mockserver.url(f\"/status?n=200&id={index}\"), meta={\"index\": index}\n@@ -20,7 +21,21 @@ class ItemSpider(Spider):\n         return {\"index\": response.meta[\"index\"]}\n \n \n-class TestAsyncSignal(unittest.TestCase):\n+class MainTestCase(TestCase):\n+    @deferred_f_from_coro_f\n+    async def test_scheduler_empty(self):\n+        crawler = get_crawler()\n+        calls = []\n+\n+        def track_call():\n+            calls.append(object())\n+\n+        crawler.signals.connect(track_call, signals.scheduler_empty)\n+        await maybe_deferred_to_future(crawler.crawl())\n+        assert len(calls) >= 1\n+\n+\n+class MockServerTestCase(TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n\n@@ -1,8 +1,7 @@\n import gzip\n-import inspect\n import warnings\n from io import BytesIO\n-from logging import WARNING\n+from logging import ERROR, WARNING\n from pathlib import Path\n from typing import Any\n from unittest import mock\n@@ -27,6 +26,7 @@ from scrapy.spiders import (\n     XMLFeedSpider,\n )\n from scrapy.spiders.init import InitSpider\n+from scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\n from scrapy.utils.test import get_crawler, get_reactor_settings\n from tests import get_testdata, tests_datadir\n \n@@ -45,12 +45,6 @@ class TestSpider(unittest.TestCase):\n         assert spider.name == \"example.com\"\n         assert spider.start_urls == []  # pylint: disable=use-implicit-booleaness-not-comparison\n \n-    def test_start_requests(self):\n-        spider = self.spider_class(\"example.com\")\n-        start_requests = spider.start_requests()\n-        assert inspect.isgenerator(start_requests)\n-        assert not list(start_requests)\n-\n     def test_spider_args(self):\n         \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n         spider = self.spider_class(\"example.com\", foo=\"bar\")\n@@ -152,6 +146,22 @@ class TestSpider(unittest.TestCase):\n class TestInitSpider(TestSpider):\n     spider_class = InitSpider\n \n+    @deferred_f_from_coro_f\n+    async def test_start_urls(self):\n+        responses = []\n+\n+        class TestSpider(self.spider_class):\n+            name = \"test\"\n+            start_urls = [\"data:,\"]\n+\n+            async def parse(self, response):\n+                responses.append(response)\n+\n+        crawler = get_crawler(TestSpider)\n+        await maybe_deferred_to_future(crawler.crawl())\n+        assert len(responses) == 1\n+        assert responses[0].url == \"data:,\"\n+\n \n class TestXMLFeedSpider(TestSpider):\n     spider_class = XMLFeedSpider\n@@ -454,12 +464,17 @@ class TestCrawlSpider(TestSpider):\n         assert hasattr(spider, \"_follow_links\")\n         assert not spider._follow_links\n \n+    @inlineCallbacks\n     def test_start_url(self):\n-        spider = self.spider_class(\"example.com\")\n-        spider.start_url = \"https://www.example.com\"\n+        class TestSpider(self.spider_class):\n+            name = \"test\"\n+            start_url = \"https://www.example.com\"\n \n-        with pytest.raises(AttributeError, match=r\"^Crawling could not start.*$\"):\n-            list(spider.start_requests())\n+        crawler = get_crawler(TestSpider)\n+        with LogCapture(\"scrapy.core.engine\", propagate=False, level=ERROR) as log:\n+            yield crawler.crawl()\n+        assert \"Error while reading start items and requests\" in str(log)\n+        assert \"did you miss an 's'?\" in str(log)\n \n \n class TestSitemapSpider(TestSpider):\n@@ -776,6 +791,24 @@ Sitemap: /sitemap-relative-url.xml\n             ),\n         )\n \n+    @deferred_f_from_coro_f\n+    async def test_sitemap_urls(self):\n+        class TestSpider(self.spider_class):\n+            name = \"test\"\n+            sitemap_urls = [\"https://toscrape.com/sitemap.xml\"]\n+\n+        crawler = get_crawler(TestSpider)\n+        spider = TestSpider.from_crawler(crawler)\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            requests = [request async for request in spider.start()]\n+\n+        assert len(requests) == 1\n+        request = requests[0]\n+        assert request.url == \"https://toscrape.com/sitemap.xml\"\n+        assert request.dont_filter is False\n+        assert request.callback == spider._parse_sitemap\n+\n \n class TestDeprecation:\n     def test_crawl_spider(self):\n\n@@ -0,0 +1,186 @@\n+import warnings\n+from asyncio import sleep\n+\n+import pytest\n+from testfixtures import LogCapture\n+from twisted.trial.unittest import TestCase\n+\n+from scrapy import Spider, signals\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\n+from scrapy.utils.test import get_crawler\n+\n+from .utils import twisted_sleep\n+\n+SLEEP_SECONDS = 0.1\n+\n+ITEM_A = {\"id\": \"a\"}\n+ITEM_B = {\"id\": \"b\"}\n+\n+\n+class MainTestCase(TestCase):\n+    async def _test_spider(self, spider, expected_items=None):\n+        actual_items = []\n+        expected_items = [] if expected_items is None else expected_items\n+\n+        def track_item(item, response, spider):\n+            actual_items.append(item)\n+\n+        crawler = get_crawler(spider)\n+        crawler.signals.connect(track_item, signals.item_scraped)\n+        await maybe_deferred_to_future(crawler.crawl())\n+        assert crawler.stats.get_value(\"finish_reason\") == \"finished\"\n+        assert actual_items == expected_items\n+\n+    @deferred_f_from_coro_f\n+    async def test_start_urls(self):\n+        class TestSpider(Spider):\n+            name = \"test\"\n+            start_urls = [\"data:,\"]\n+\n+            async def parse(self, response):\n+                yield ITEM_A\n+\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_spider(TestSpider, [ITEM_A])\n+\n+    @deferred_f_from_coro_f\n+    async def test_start(self):\n+        class TestSpider(Spider):\n+            name = \"test\"\n+\n+            async def start(self):\n+                yield ITEM_A\n+\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_spider(TestSpider, [ITEM_A])\n+\n+    @deferred_f_from_coro_f\n+    async def test_start_subclass(self):\n+        class BaseSpider(Spider):\n+            async def start(self):\n+                yield ITEM_A\n+\n+        class TestSpider(BaseSpider):\n+            name = \"test\"\n+\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_spider(TestSpider, [ITEM_A])\n+\n+    @deferred_f_from_coro_f\n+    async def test_deprecated(self):\n+        class TestSpider(Spider):\n+            name = \"test\"\n+\n+            def start_requests(self):\n+                yield ITEM_A\n+\n+        with pytest.warns(ScrapyDeprecationWarning):\n+            await self._test_spider(TestSpider, [ITEM_A])\n+\n+    @deferred_f_from_coro_f\n+    async def test_deprecated_subclass(self):\n+        class BaseSpider(Spider):\n+            def start_requests(self):\n+                yield ITEM_A\n+\n+        class TestSpider(BaseSpider):\n+            name = \"test\"\n+\n+        # The warning must be about the base class and not the subclass.\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"BaseSpider\"):\n+            await self._test_spider(TestSpider, [ITEM_A])\n+\n+    @deferred_f_from_coro_f\n+    async def test_universal(self):\n+        class TestSpider(Spider):\n+            name = \"test\"\n+\n+            async def start(self):\n+                yield ITEM_A\n+\n+            def start_requests(self):\n+                yield ITEM_B\n+\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_spider(TestSpider, [ITEM_A])\n+\n+    @deferred_f_from_coro_f\n+    async def test_universal_subclass(self):\n+        class BaseSpider(Spider):\n+            async def start(self):\n+                yield ITEM_A\n+\n+            def start_requests(self):\n+                yield ITEM_B\n+\n+        class TestSpider(BaseSpider):\n+            name = \"test\"\n+\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_spider(TestSpider, [ITEM_A])\n+\n+    @deferred_f_from_coro_f\n+    async def test_start_deprecated_super(self):\n+        class TestSpider(Spider):\n+            name = \"test\"\n+\n+            async def start(self):\n+                for item_or_request in super().start_requests():\n+                    yield item_or_request\n+\n+        with pytest.warns(\n+            ScrapyDeprecationWarning, match=r\"use Spider\\.start\\(\\) instead\"\n+        ) as messages:\n+            await self._test_spider(TestSpider, [])\n+        assert messages[0].filename.endswith(\"test_spider_start.py\")\n+\n+    async def _test_start(self, start_, expected_items=None):\n+        class TestSpider(Spider):\n+            name = \"test\"\n+            start = start_\n+\n+        await self._test_spider(TestSpider, expected_items)\n+\n+    @pytest.mark.only_asyncio\n+    @deferred_f_from_coro_f\n+    async def test_asyncio_delayed(self):\n+        async def start(spider):\n+            await sleep(SLEEP_SECONDS)\n+            yield ITEM_A\n+\n+        await self._test_start(start, [ITEM_A])\n+\n+    @deferred_f_from_coro_f\n+    async def test_twisted_delayed(self):\n+        async def start(spider):\n+            await maybe_deferred_to_future(twisted_sleep(SLEEP_SECONDS))\n+            yield ITEM_A\n+\n+        await self._test_start(start, [ITEM_A])\n+\n+    # Exceptions\n+\n+    @deferred_f_from_coro_f\n+    async def test_deprecated_non_generator_exception(self):\n+        class TestSpider(Spider):\n+            name = \"test\"\n+\n+            def start_requests(self):\n+                raise RuntimeError\n+\n+        with (\n+            LogCapture() as log,\n+            pytest.warns(\n+                ScrapyDeprecationWarning,\n+                match=r\"defines the deprecated start_requests\\(\\) method\",\n+            ),\n+        ):\n+            await self._test_spider(TestSpider, [])\n+\n+        assert \"in start_requests\\n    raise RuntimeError\" in str(log)\n\n@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n from collections.abc import AsyncIterator, Iterable\n+from inspect import isasyncgen\n from typing import Any\n from unittest import mock\n \n@@ -111,7 +112,7 @@ class TestProcessSpiderExceptionReRaise(TestSpiderMiddleware):\n class TestBaseAsyncSpiderMiddleware(TestSpiderMiddleware):\n     \"\"\"Helpers for testing sync, async and mixed middlewares.\n \n-    Should work for process_spider_output and, when it's supported, process_start_requests.\n+    Should work for process_spider_output and, when it's supported, process_start.\n     \"\"\"\n \n     ITEM_TYPE: type | tuple\n@@ -200,7 +201,7 @@ class ProcessSpiderExceptionSimpleIterableMiddleware:\n         yield {\"foo\": 3}\n \n \n-class ProcessSpiderExceptionAsyncIterableMiddleware:\n+class ProcessSpiderExceptionAsyncIteratorMiddleware:\n     async def process_spider_exception(self, response, exception, spider):\n         yield {\"foo\": 1}\n         d = defer.Deferred()\n@@ -319,37 +320,43 @@ class TestProcessSpiderOutputInvalidResult(TestBaseAsyncSpiderMiddleware):\n             )\n \n \n-class ProcessStartRequestsSimpleMiddleware:\n-    def process_start_requests(self, start_requests, spider):\n-        yield from start_requests\n+class ProcessStartSimpleMiddleware:\n+    async def process_start(self, start):\n+        async for item_or_request in start:\n+            yield item_or_request\n \n \n-class TestProcessStartRequestsSimple(TestBaseAsyncSpiderMiddleware):\n-    \"\"\"process_start_requests tests for simple start_requests\"\"\"\n+class TestProcessStartSimple(TestBaseAsyncSpiderMiddleware):\n+    \"\"\"process_start tests for simple start\"\"\"\n \n     ITEM_TYPE = (Request, dict)\n-    MW_SIMPLE = ProcessStartRequestsSimpleMiddleware\n+    MW_SIMPLE = ProcessStartSimpleMiddleware\n \n-    def _start_requests(self):\n+    async def _get_processed_start(self, *mw_classes):\n+        class TestSpider(Spider):\n+            name = \"test\"\n+\n+            async def start(self):\n                 for i in range(2):\n                     yield Request(f\"https://example.com/{i}\", dont_filter=True)\n                 yield {\"name\": \"test item\"}\n \n-    @defer.inlineCallbacks\n-    def _get_middleware_result(self, *mw_classes, start_index: int | None = None):\n-        setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n+        setting = self._construct_mw_setting(*mw_classes)\n         self.crawler = get_crawler(\n-            Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}, \"SPIDER_MIDDLEWARES\": setting}\n+            TestSpider, {\"SPIDER_MIDDLEWARES_BASE\": {}, \"SPIDER_MIDDLEWARES\": setting}\n         )\n-        self.spider = self.crawler._create_spider(\"foo\")\n+        self.spider = self.crawler._create_spider()\n         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n-        start_requests = iter(self._start_requests())\n-        results = yield self.mwman.process_start_requests(start_requests, self.spider)\n-        return results\n+        return await self.mwman.process_start(self.spider)\n \n-    def test_simple(self):\n+    @deferred_f_from_coro_f\n+    async def test_simple(self):\n         \"\"\"Simple mw\"\"\"\n-        return self._test_simple_base(self.MW_SIMPLE)\n+        start = await self._get_processed_start(self.MW_SIMPLE)\n+        assert isasyncgen(start)\n+        start_list = await collect_asyncgen(start)\n+        assert len(start_list) == self.RESULT_COUNT\n+        assert isinstance(start_list[0], self.ITEM_TYPE)\n \n \n class UniversalMiddlewareNoSync:\n@@ -507,7 +514,7 @@ class TestProcessSpiderException(TestBaseAsyncSpiderMiddleware):\n     MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n     MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n     MW_EXC_SIMPLE = ProcessSpiderExceptionSimpleIterableMiddleware\n-    MW_EXC_ASYNCGEN = ProcessSpiderExceptionAsyncIterableMiddleware\n+    MW_EXC_ASYNCGEN = ProcessSpiderExceptionAsyncIteratorMiddleware\n \n     def _scrape_func(self, *args, **kwargs):\n         1 / 0\n\n@@ -27,16 +27,19 @@ def test_trivial(crawler):\n     assert mw.crawler is crawler\n     test_req = Request(\"data:,\")\n     spider_output = [test_req, {\"foo\": \"bar\"}]\n-    processed = list(\n+    for processed in [\n+        list(\n             mw.process_spider_output(Response(\"data:,\"), spider_output, crawler.spider)\n-    )\n+        ),\n+        list(mw.process_start_requests(spider_output, crawler.spider)),\n+    ]:\n         assert processed == [test_req, {\"foo\": \"bar\"}]\n \n \n def test_processed_request(crawler):\n     class ProcessReqSpiderMiddleware(BaseSpiderMiddleware):\n         def get_processed_request(\n-            self, request: Request, response: Response\n+            self, request: Request, response: Response | None\n         ) -> Request | None:\n             if request.url == \"data:2,\":\n                 return None\n@@ -49,9 +52,12 @@ def test_processed_request(crawler):\n     test_req2 = Request(\"data:2,\")\n     test_req3 = Request(\"data:3,\")\n     spider_output = [test_req1, {\"foo\": \"bar\"}, test_req2, test_req3]\n-    processed = list(\n+    for processed in [\n+        list(\n             mw.process_spider_output(Response(\"data:,\"), spider_output, crawler.spider)\n-    )\n+        ),\n+        list(mw.process_start_requests(spider_output, crawler.spider)),\n+    ]:\n         assert len(processed) == 3\n         assert isinstance(processed[0], Request)\n         assert processed[0].url == \"data:1,\"\n@@ -62,7 +68,7 @@ def test_processed_request(crawler):\n \n def test_processed_item(crawler):\n     class ProcessItemSpiderMiddleware(BaseSpiderMiddleware):\n-        def get_processed_item(self, item: Any, response: Response) -> Any:\n+        def get_processed_item(self, item: Any, response: Response | None) -> Any:\n             if item[\"foo\"] == 2:\n                 return None\n             if item[\"foo\"] == 3:\n@@ -72,16 +78,19 @@ def test_processed_item(crawler):\n     mw = ProcessItemSpiderMiddleware.from_crawler(crawler)\n     test_req = Request(\"data:,\")\n     spider_output = [{\"foo\": 1}, {\"foo\": 2}, test_req, {\"foo\": 3}]\n-    processed = list(\n+    for processed in [\n+        list(\n             mw.process_spider_output(Response(\"data:,\"), spider_output, crawler.spider)\n-    )\n+        ),\n+        list(mw.process_start_requests(spider_output, crawler.spider)),\n+    ]:\n         assert processed == [{\"foo\": 1}, test_req, {\"foo\": 30}]\n \n \n def test_processed_both(crawler):\n     class ProcessBothSpiderMiddleware(BaseSpiderMiddleware):\n         def get_processed_request(\n-            self, request: Request, response: Response\n+            self, request: Request, response: Response | None\n         ) -> Request | None:\n             if request.url == \"data:2,\":\n                 return None\n@@ -89,7 +98,7 @@ def test_processed_both(crawler):\n                 return Request(\"data:30,\")\n             return request\n \n-        def get_processed_item(self, item: Any, response: Response) -> Any:\n+        def get_processed_item(self, item: Any, response: Response | None) -> Any:\n             if item[\"foo\"] == 2:\n                 return None\n             if item[\"foo\"] == 3:\n@@ -108,9 +117,12 @@ def test_processed_both(crawler):\n         {\"foo\": 3},\n         test_req3,\n     ]\n-    processed = list(\n+    for processed in [\n+        list(\n             mw.process_spider_output(Response(\"data:,\"), spider_output, crawler.spider)\n-    )\n+        ),\n+        list(mw.process_start_requests(spider_output, crawler.spider)),\n+    ]:\n         assert len(processed) == 4\n         assert isinstance(processed[0], Request)\n         assert processed[0].url == \"data:1,\"\n\n@@ -30,7 +30,7 @@ class _HttpErrorSpider(MockServerSpider):\n         self.skipped = set()\n         self.parsed = set()\n \n-    def start_requests(self):\n+    async def start(self):\n         for url in self.start_urls:\n             yield Request(url, self.parse, errback=self.on_error)\n \n\n@@ -36,7 +36,7 @@ class RecoverySpider(Spider):\n         },\n     }\n \n-    def start_requests(self):\n+    async def start(self):\n         yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n@@ -73,7 +73,7 @@ class ProcessSpiderInputSpiderWithoutErrback(Spider):\n         }\n     }\n \n-    def start_requests(self):\n+    async def start(self):\n         yield Request(url=self.mockserver.url(\"/status?n=200\"), callback=self.parse)\n \n     def parse(self, response):\n@@ -83,7 +83,7 @@ class ProcessSpiderInputSpiderWithoutErrback(Spider):\n class ProcessSpiderInputSpiderWithErrback(ProcessSpiderInputSpiderWithoutErrback):\n     name = \"ProcessSpiderInputSpiderWithErrback\"\n \n-    def start_requests(self):\n+    async def start(self):\n         yield Request(\n             self.mockserver.url(\"/status?n=200\"), self.parse, errback=self.errback\n         )\n@@ -103,7 +103,7 @@ class GeneratorCallbackSpider(Spider):\n         },\n     }\n \n-    def start_requests(self):\n+    async def start(self):\n         yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n@@ -140,7 +140,7 @@ class NotGeneratorCallbackSpider(Spider):\n         },\n     }\n \n-    def start_requests(self):\n+    async def start(self):\n         yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n@@ -215,7 +215,7 @@ class GeneratorOutputChainSpider(Spider):\n         },\n     }\n \n-    def start_requests(self):\n+    async def start(self):\n         yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n@@ -287,8 +287,8 @@ class NotGeneratorOutputChainSpider(Spider):\n         },\n     }\n \n-    def start_requests(self):\n-        return [Request(self.mockserver.url(\"/status?n=200\"))]\n+    async def start(self):\n+        yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n         return [\n\n@@ -0,0 +1,352 @@\n+import warnings\n+from asyncio import sleep\n+\n+import pytest\n+from twisted.trial.unittest import TestCase\n+\n+from scrapy import Spider, signals\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\n+from scrapy.utils.test import get_crawler\n+from tests.test_spider_start import SLEEP_SECONDS\n+\n+from .utils import twisted_sleep\n+\n+ITEM_A = {\"id\": \"a\"}\n+ITEM_B = {\"id\": \"b\"}\n+ITEM_C = {\"id\": \"c\"}\n+ITEM_D = {\"id\": \"d\"}\n+\n+\n+class AsyncioSleepSpiderMiddleware:\n+    async def process_start(self, start):\n+        await sleep(SLEEP_SECONDS)\n+        async for item_or_request in start:\n+            yield item_or_request\n+\n+\n+class NoOpSpiderMiddleware:\n+    async def process_start(self, start):\n+        async for item_or_request in start:\n+            yield item_or_request\n+\n+\n+class TwistedSleepSpiderMiddleware:\n+    async def process_start(self, start):\n+        await maybe_deferred_to_future(twisted_sleep(SLEEP_SECONDS))\n+        async for item_or_request in start:\n+            yield item_or_request\n+\n+\n+class UniversalSpiderMiddleware:\n+    async def process_start(self, start):\n+        async for item_or_request in start:\n+            yield item_or_request\n+\n+    def process_start_requests(self, start_requests, spider):\n+        raise NotImplementedError\n+\n+\n+# Spiders and spider middlewares for MainTestCase._test_wrap\n+\n+\n+class ModernWrapSpider(Spider):\n+    name = \"test\"\n+\n+    async def start(self):\n+        yield ITEM_B\n+\n+\n+class ModernWrapSpiderSubclass(ModernWrapSpider):\n+    name = \"test\"\n+\n+\n+class UniversalWrapSpider(Spider):\n+    name = \"test\"\n+\n+    async def start(self):\n+        yield ITEM_B\n+\n+    def start_requests(self):\n+        yield ITEM_D\n+\n+\n+class DeprecatedWrapSpider(Spider):\n+    name = \"test\"\n+\n+    def start_requests(self):\n+        yield ITEM_B\n+\n+\n+class ModernWrapSpiderMiddleware:\n+    async def process_start(self, start):\n+        yield ITEM_A\n+        async for item_or_request in start:\n+            yield item_or_request\n+        yield ITEM_C\n+\n+\n+class UniversalWrapSpiderMiddleware:\n+    async def process_start(self, start):\n+        yield ITEM_A\n+        async for item_or_request in start:\n+            yield item_or_request\n+        yield ITEM_C\n+\n+    def process_start_requests(self, start, spider):\n+        yield ITEM_A\n+        yield from start\n+        yield ITEM_C\n+\n+\n+class DeprecatedWrapSpiderMiddleware:\n+    def process_start_requests(self, start, spider):\n+        yield ITEM_A\n+        yield from start\n+        yield ITEM_C\n+\n+\n+class MainTestCase(TestCase):\n+    async def _test(self, spider_middlewares, spider_cls, expected_items):\n+        actual_items = []\n+\n+        def track_item(item, response, spider):\n+            actual_items.append(item)\n+\n+        settings = {\n+            \"SPIDER_MIDDLEWARES\": {cls: n for n, cls in enumerate(spider_middlewares)},\n+        }\n+        crawler = get_crawler(spider_cls, settings_dict=settings)\n+        crawler.signals.connect(track_item, signals.item_scraped)\n+        await maybe_deferred_to_future(crawler.crawl())\n+        assert crawler.stats.get_value(\"finish_reason\") == \"finished\"\n+        assert actual_items == expected_items, f\"{actual_items=} != {expected_items=}\"\n+\n+    async def _test_wrap(self, spider_middleware, spider_cls, expected_items=None):\n+        expected_items = expected_items or [ITEM_A, ITEM_B, ITEM_C]\n+        await self._test([spider_middleware], spider_cls, expected_items)\n+\n+    async def _test_douple_wrap(self, smw1, smw2, spider_cls, expected_items=None):\n+        expected_items = expected_items or [ITEM_A, ITEM_A, ITEM_B, ITEM_C, ITEM_C]\n+        await self._test([smw1, smw2], spider_cls, expected_items)\n+\n+    @deferred_f_from_coro_f\n+    async def test_modern_mw_modern_spider(self):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_wrap(ModernWrapSpiderMiddleware, ModernWrapSpider)\n+\n+    @deferred_f_from_coro_f\n+    async def test_modern_mw_universal_spider(self):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_wrap(ModernWrapSpiderMiddleware, UniversalWrapSpider)\n+\n+    @deferred_f_from_coro_f\n+    async def test_modern_mw_deprecated_spider(self):\n+        with pytest.warns(\n+            ScrapyDeprecationWarning, match=r\"deprecated start_requests\\(\\)\"\n+        ):\n+            await self._test_wrap(ModernWrapSpiderMiddleware, DeprecatedWrapSpider)\n+\n+    @deferred_f_from_coro_f\n+    async def test_universal_mw_modern_spider(self):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_wrap(UniversalWrapSpiderMiddleware, ModernWrapSpider)\n+\n+    @deferred_f_from_coro_f\n+    async def test_universal_mw_universal_spider(self):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_wrap(UniversalWrapSpiderMiddleware, UniversalWrapSpider)\n+\n+    @deferred_f_from_coro_f\n+    async def test_universal_mw_deprecated_spider(self):\n+        with pytest.warns(\n+            ScrapyDeprecationWarning, match=r\"deprecated start_requests\\(\\)\"\n+        ):\n+            await self._test_wrap(UniversalWrapSpiderMiddleware, DeprecatedWrapSpider)\n+\n+    @deferred_f_from_coro_f\n+    async def test_deprecated_mw_modern_spider(self):\n+        with (\n+            pytest.warns(\n+                ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n+            ),\n+            pytest.raises(\n+                ValueError, match=r\"only compatible with \\(deprecated\\) spiders\"\n+            ),\n+        ):\n+            await self._test_wrap(DeprecatedWrapSpiderMiddleware, ModernWrapSpider)\n+\n+    @deferred_f_from_coro_f\n+    async def test_deprecated_mw_modern_spider_subclass(self):\n+        with (\n+            pytest.warns(\n+                ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n+            ),\n+            pytest.raises(\n+                ValueError,\n+                match=r\"^\\S+?\\.ModernWrapSpider \\(inherited by \\S+?.ModernWrapSpiderSubclass\\) .*? only compatible with \\(deprecated\\) spiders\",\n+            ),\n+        ):\n+            await self._test_wrap(\n+                DeprecatedWrapSpiderMiddleware, ModernWrapSpiderSubclass\n+            )\n+\n+    @deferred_f_from_coro_f\n+    async def test_deprecated_mw_universal_spider(self):\n+        with pytest.warns(\n+            ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n+        ):\n+            await self._test_wrap(\n+                DeprecatedWrapSpiderMiddleware,\n+                UniversalWrapSpider,\n+                [ITEM_A, ITEM_D, ITEM_C],\n+            )\n+\n+    @deferred_f_from_coro_f\n+    async def test_deprecated_mw_deprecated_spider(self):\n+        with (\n+            pytest.warns(\n+                ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n+            ),\n+            pytest.warns(\n+                ScrapyDeprecationWarning, match=r\"deprecated start_requests\\(\\)\"\n+            ),\n+        ):\n+            await self._test_wrap(DeprecatedWrapSpiderMiddleware, DeprecatedWrapSpider)\n+\n+    @deferred_f_from_coro_f\n+    async def test_modern_mw_universal_mw_modern_spider(self):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_douple_wrap(\n+                ModernWrapSpiderMiddleware,\n+                UniversalWrapSpiderMiddleware,\n+                ModernWrapSpider,\n+            )\n+\n+    @deferred_f_from_coro_f\n+    async def test_modern_mw_deprecated_mw_modern_spider(self):\n+        with pytest.raises(ValueError, match=r\"trying to combine spider middlewares\"):\n+            await self._test_douple_wrap(\n+                ModernWrapSpiderMiddleware,\n+                DeprecatedWrapSpiderMiddleware,\n+                ModernWrapSpider,\n+            )\n+\n+    @deferred_f_from_coro_f\n+    async def test_universal_mw_deprecated_mw_modern_spider(self):\n+        with (\n+            pytest.warns(\n+                ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n+            ),\n+            pytest.raises(\n+                ValueError, match=r\"only compatible with \\(deprecated\\) spiders\"\n+            ),\n+        ):\n+            await self._test_douple_wrap(\n+                UniversalWrapSpiderMiddleware,\n+                DeprecatedWrapSpiderMiddleware,\n+                ModernWrapSpider,\n+            )\n+\n+    @deferred_f_from_coro_f\n+    async def test_modern_mw_universal_mw_universal_spider(self):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            await self._test_douple_wrap(\n+                ModernWrapSpiderMiddleware,\n+                UniversalWrapSpiderMiddleware,\n+                UniversalWrapSpider,\n+            )\n+\n+    @deferred_f_from_coro_f\n+    async def test_modern_mw_deprecated_mw_universal_spider(self):\n+        with pytest.raises(ValueError, match=r\"trying to combine spider middlewares\"):\n+            await self._test_douple_wrap(\n+                ModernWrapSpiderMiddleware,\n+                DeprecatedWrapSpiderMiddleware,\n+                UniversalWrapSpider,\n+            )\n+\n+    @deferred_f_from_coro_f\n+    async def test_universal_mw_deprecated_mw_universal_spider(self):\n+        with pytest.warns(\n+            ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n+        ):\n+            await self._test_douple_wrap(\n+                UniversalWrapSpiderMiddleware,\n+                DeprecatedWrapSpiderMiddleware,\n+                UniversalWrapSpider,\n+                [ITEM_A, ITEM_A, ITEM_D, ITEM_C, ITEM_C],\n+            )\n+\n+    @deferred_f_from_coro_f\n+    async def test_modern_mw_universal_mw_deprecated_spider(self):\n+        with pytest.warns(\n+            ScrapyDeprecationWarning, match=r\"deprecated start_requests\\(\\)\"\n+        ):\n+            await self._test_douple_wrap(\n+                ModernWrapSpiderMiddleware,\n+                UniversalWrapSpiderMiddleware,\n+                DeprecatedWrapSpider,\n+            )\n+\n+    @deferred_f_from_coro_f\n+    async def test_modern_mw_deprecated_mw_deprecated_spider(self):\n+        with pytest.raises(ValueError, match=r\"trying to combine spider middlewares\"):\n+            await self._test_douple_wrap(\n+                ModernWrapSpiderMiddleware,\n+                DeprecatedWrapSpiderMiddleware,\n+                DeprecatedWrapSpider,\n+            )\n+\n+    @deferred_f_from_coro_f\n+    async def test_universal_mw_deprecated_mw_deprecated_spider(self):\n+        with (\n+            pytest.warns(\n+                ScrapyDeprecationWarning, match=r\"deprecated process_start_requests\\(\\)\"\n+            ),\n+            pytest.warns(\n+                ScrapyDeprecationWarning, match=r\"deprecated start_requests\\(\\)\"\n+            ),\n+        ):\n+            await self._test_douple_wrap(\n+                UniversalWrapSpiderMiddleware,\n+                DeprecatedWrapSpiderMiddleware,\n+                DeprecatedWrapSpider,\n+            )\n+\n+    async def _test_sleep(self, spider_middlewares):\n+        class TestSpider(Spider):\n+            name = \"test\"\n+\n+            async def start(self):\n+                yield ITEM_A\n+\n+        await self._test(spider_middlewares, TestSpider, [ITEM_A])\n+\n+    @pytest.mark.only_asyncio\n+    @deferred_f_from_coro_f\n+    async def test_asyncio_sleep_single(self):\n+        await self._test_sleep([AsyncioSleepSpiderMiddleware])\n+\n+    @pytest.mark.only_asyncio\n+    @deferred_f_from_coro_f\n+    async def test_asyncio_sleep_multiple(self):\n+        await self._test_sleep(\n+            [NoOpSpiderMiddleware, AsyncioSleepSpiderMiddleware, NoOpSpiderMiddleware]\n+        )\n+\n+    @deferred_f_from_coro_f\n+    async def test_twisted_sleep_single(self):\n+        await self._test_sleep([TwistedSleepSpiderMiddleware])\n+\n+    @deferred_f_from_coro_f\n+    async def test_twisted_sleep_multiple(self):\n+        await self._test_sleep(\n+            [NoOpSpiderMiddleware, TwistedSleepSpiderMiddleware, NoOpSpiderMiddleware]\n+        )\n\n@@ -0,0 +1,44 @@\n+from twisted.trial.unittest import TestCase\n+\n+from scrapy.http import Request\n+from scrapy.spidermiddlewares.start import StartSpiderMiddleware\n+from scrapy.spiders import Spider\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+from scrapy.utils.misc import build_from_crawler\n+from scrapy.utils.test import get_crawler\n+\n+\n+class TestMiddleware(TestCase):\n+    @deferred_f_from_coro_f\n+    async def test_async(self):\n+        crawler = get_crawler(Spider)\n+        mw = build_from_crawler(StartSpiderMiddleware, crawler)\n+\n+        async def start():\n+            yield Request(\"data:,1\")\n+            yield Request(\"data:,2\", meta={\"is_start_request\": True})\n+            yield Request(\"data:,2\", meta={\"is_start_request\": False})\n+            yield Request(\"data:,2\", meta={\"is_start_request\": \"foo\"})\n+\n+        result = [\n+            request.meta[\"is_start_request\"]\n+            async for request in mw.process_start(start())\n+        ]\n+        assert result == [True, True, False, \"foo\"]\n+\n+    @deferred_f_from_coro_f\n+    async def test_sync(self):\n+        crawler = get_crawler(Spider)\n+        mw = build_from_crawler(StartSpiderMiddleware, crawler)\n+\n+        def start():\n+            yield Request(\"data:,1\")\n+            yield Request(\"data:,2\", meta={\"is_start_request\": True})\n+            yield Request(\"data:,2\", meta={\"is_start_request\": False})\n+            yield Request(\"data:,2\", meta={\"is_start_request\": \"foo\"})\n+\n+        result = [\n+            request.meta[\"is_start_request\"]\n+            for request in mw.process_start_requests(start(), Spider(\"test\"))\n+        ]\n+        assert result == [True, True, False, \"foo\"]\n\n@@ -0,0 +1,9 @@\n+from twisted.internet.defer import Deferred\n+\n+\n+def twisted_sleep(seconds):\n+    from twisted.internet import reactor\n+\n+    d = Deferred()\n+    reactor.callLater(seconds, d.callback, None)\n+    return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
