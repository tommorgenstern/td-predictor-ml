{"custom_id": "keras#87bda1850183ac62d0ce8805790d3eb3d245bf8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 10/4 | Churn Δ: 61 | Churn Cumulative: 1429 | Contributors (this commit): 13 | Commits (past 90d): 37 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -94,3 +94,7 @@ def fft2(a):\n     complex_input = _get_complex_tensor_from_tuple(a)\n     complex_output = jax.numpy.fft.fft2(complex_input)\n     return jax.numpy.real(complex_output), jax.numpy.imag(complex_output)\n+\n+\n+def rsqrt(x):\n+    return jax.lax.rsqrt(x)\n\n@@ -85,3 +85,7 @@ def fft2(a):\n     complex_input = _get_complex_tensor_from_tuple(a)\n     complex_output = tf.signal.fft2d(complex_input)\n     return tf.math.real(complex_output), tf.math.imag(complex_output)\n+\n+\n+def rsqrt(x):\n+    return tf.math.rsqrt(x)\n\n@@ -160,3 +160,8 @@ def fft2(a):\n     complex_input = _get_complex_tensor_from_tuple(a)\n     complex_output = torch.fft.fft2(complex_input)\n     return complex_output.real, complex_output.imag\n+\n+\n+def rsqrt(x):\n+    x = convert_to_tensor(x)\n+    return torch.rsqrt(x)\n\n@@ -450,3 +450,34 @@ def fft2(a):\n     if any_symbolic_tensors(a):\n         return FFT2().symbolic_call(a)\n     return backend.math.fft2(a)\n+\n+\n+class Rsqrt(Operation):\n+    \"\"\"Computes reciprocal of square root of x element-wise.\n+\n+    Args:\n+        x: input tensor\n+\n+    Returns:\n+        A tensor with the same type as `x`.\n+\n+    Example:\n+\n+    >>> x = keras_core.ops.convert_to_tensor([2., 3., -2.])\n+    >>> rsqrt(x)\n+    \"\"\"\n+\n+    def call(self, x):\n+        x = backend.convert_to_tensor(x)\n+        return backend.math.rsqrt(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_core_export(\"keras_core.ops.rsqrt\")\n+def rsqrt(x):\n+    if any_symbolic_tensors((x,)):\n+        return Rsqrt().symbolic_call(x)\n+    x = backend.convert_to_tensor(x)\n+    return backend.math.rsqrt(x)\n\n@@ -82,6 +82,10 @@ class MathOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(real_output.shape, ref_shape)\n         self.assertEqual(imag_output.shape, ref_shape)\n \n+    def test_rsqrt(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(kmath.rsqrt(x).shape, (None, 3))\n+\n \n class MathOpsStaticShapeTest(testing.TestCase):\n     @pytest.mark.skipif(\n@@ -160,6 +164,10 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(real_output.shape, ref.shape)\n         self.assertEqual(imag_output.shape, ref.shape)\n \n+    def test_rsqrt(self):\n+        x = KerasTensor([4, 3], dtype=\"float32\")\n+        self.assertEqual(kmath.rsqrt(x).shape, (4, 3))\n+\n \n class MathOpsCorrectnessTest(testing.TestCase):\n     @pytest.mark.skipif(\n@@ -394,3 +402,12 @@ class MathOpsCorrectnessTest(testing.TestCase):\n         imag_ref = np.imag(ref)\n         self.assertAllClose(real_ref, real_output)\n         self.assertAllClose(imag_ref, imag_output)\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\",\n+        reason=\"Numpy does not support rsqrt.\",\n+    )\n+    def test_rsqrt(self):\n+        x = np.array([[1, 4, 9], [16, 25, 36]], dtype=\"float32\")\n+        self.assertAllClose(kmath.rsqrt(x), 1 / np.sqrt(x))\n+        self.assertAllClose(kmath.Rsqrt()(x), 1 / np.sqrt(x))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#43ffbcea7d13a40d6921beb9e936b83a48f75a2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 34 | Churn Cumulative: 565 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ Title: Learning to tokenize in Vision Transformers\n Authors: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Sayak Paul](https://twitter.com/RisingSayak) (equal contribution)\n Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2021/12/10\n-Last modified: 2023/07/18\n+Last modified: 2023/08/14\n Description: Adaptively generating a smaller number of tokens for Vision Transformers.\n Accelerator: GPU\n \"\"\"\n@@ -165,19 +165,25 @@ tokens.\n \"\"\"\n \n \n-def position_embedding(\n-    projected_patches, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n-):\n-    # Build the positions.\n-    positions = ops.arange(start=0, stop=num_patches, step=1)\n-\n-    # Encode the positions with an Embedding layer.\n-    encoded_positions = layers.Embedding(\n+class PatchEncoder(layers.Layer):\n+    def __init__(self, num_patches, projection_dim):\n+        super().__init__()\n+        self.num_patches = num_patches\n+        self.position_embedding = layers.Embedding(\n             input_dim=num_patches, output_dim=projection_dim\n-    )(positions)\n+        )\n \n-    # Add encoded positions to the projected patches.\n-    return projected_patches + encoded_positions\n+    def call(self, patch):\n+        positions = ops.expand_dims(\n+            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n+        )\n+        encoded = patch + self.position_embedding(positions)\n+        return encoded\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"num_patches\": self.num_patches})\n+        return config\n \n \n \"\"\"\n@@ -337,7 +343,9 @@ def create_vit_classifier(\n     )  # (B, number_patches, projection_dim)\n \n     # Add positional embeddings to the projected patches.\n-    encoded_patches = position_embedding(\n+    encoded_patches = PatchEncoder(\n+        num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n+    )(\n         projected_patches\n     )  # (B, number_patches, projection_dim)\n     encoded_patches = layers.Dropout(0.1)(encoded_patches)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#57227e0d3525b8821409d67b22243d1e169e8485", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 108 | Files Changed: 6 | Hunks: 27 | Methods Changed: 17 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 147 | Churn Cumulative: 2141 | Contributors (this commit): 15 | Commits (past 90d): 46 | Contributors (cumulative): 36 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,7 +3,7 @@ Title: Learning to tokenize in Vision Transformers\n Authors: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Sayak Paul](https://twitter.com/RisingSayak) (equal contribution)\n Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2021/12/10\n-Last modified: 2023/08/14\n+Last modified: 2023/07/18\n Description: Adaptively generating a smaller number of tokens for Vision Transformers.\n Accelerator: GPU\n \"\"\"\n@@ -165,25 +165,19 @@ tokens.\n \"\"\"\n \n \n-class PatchEncoder(layers.Layer):\n-    def __init__(self, num_patches, projection_dim):\n-        super().__init__()\n-        self.num_patches = num_patches\n-        self.position_embedding = layers.Embedding(\n+def position_embedding(\n+    projected_patches, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n+):\n+    # Build the positions.\n+    positions = ops.arange(start=0, stop=num_patches, step=1)\n+\n+    # Encode the positions with an Embedding layer.\n+    encoded_positions = layers.Embedding(\n         input_dim=num_patches, output_dim=projection_dim\n-        )\n+    )(positions)\n \n-    def call(self, patch):\n-        positions = ops.expand_dims(\n-            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n-        )\n-        encoded = patch + self.position_embedding(positions)\n-        return encoded\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update({\"num_patches\": self.num_patches})\n-        return config\n+    # Add encoded positions to the projected patches.\n+    return projected_patches + encoded_positions\n \n \n \"\"\"\n@@ -343,9 +337,7 @@ def create_vit_classifier(\n     )  # (B, number_patches, projection_dim)\n \n     # Add positional embeddings to the projected patches.\n-    encoded_patches = PatchEncoder(\n-        num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n-    )(\n+    encoded_patches = position_embedding(\n         projected_patches\n     )  # (B, number_patches, projection_dim)\n     encoded_patches = layers.Dropout(0.1)(encoded_patches)\n\n@@ -94,7 +94,3 @@ def fft2(a):\n     complex_input = _get_complex_tensor_from_tuple(a)\n     complex_output = jax.numpy.fft.fft2(complex_input)\n     return jax.numpy.real(complex_output), jax.numpy.imag(complex_output)\n-\n-\n-def rsqrt(x):\n-    return jax.lax.rsqrt(x)\n\n@@ -85,7 +85,3 @@ def fft2(a):\n     complex_input = _get_complex_tensor_from_tuple(a)\n     complex_output = tf.signal.fft2d(complex_input)\n     return tf.math.real(complex_output), tf.math.imag(complex_output)\n-\n-\n-def rsqrt(x):\n-    return tf.math.rsqrt(x)\n\n@@ -160,8 +160,3 @@ def fft2(a):\n     complex_input = _get_complex_tensor_from_tuple(a)\n     complex_output = torch.fft.fft2(complex_input)\n     return complex_output.real, complex_output.imag\n-\n-\n-def rsqrt(x):\n-    x = convert_to_tensor(x)\n-    return torch.rsqrt(x)\n\n@@ -315,21 +315,21 @@ def qr(x, mode=\"reduced\"):\n \n \n class FFT(Operation):\n-    def compute_output_spec(self, a):\n-        if not isinstance(a, (tuple, list)) or len(a) != 2:\n+    def compute_output_spec(self, x):\n+        if not isinstance(x, (tuple, list)) or len(x) != 2:\n             raise ValueError(\n-                \"Input `a` should be a tuple of two tensors - real and \"\n-                f\"imaginary. Received: a={a}\"\n+                \"Input `x` should be a tuple of two tensors - real and \"\n+                f\"imaginary. Received: x={x}\"\n             )\n \n-        real, imag = a\n+        real, imag = x\n         # Both real and imaginary parts should have the same shape.\n         if real.shape != imag.shape:\n             raise ValueError(\n                 \"Input `a` should be a tuple of two tensors - real and \"\n                 \"imaginary. Both the real and imaginary parts should have the \"\n-                f\"same shape. Received: a[0].shape = {real.shape}, \"\n-                f\"a[1].shape = {imag.shape}\"\n+                f\"same shape. Received: x[0].shape = {real.shape}, \"\n+                f\"x[1].shape = {imag.shape}\"\n             )\n \n         # We are calculating 1D FFT. Hence, rank >= 1.\n@@ -357,21 +357,21 @@ class FFT(Operation):\n \n \n class FFT2(Operation):\n-    def compute_output_spec(self, a):\n-        if not isinstance(a, (tuple, list)) or len(a) != 2:\n+    def compute_output_spec(self, x):\n+        if not isinstance(x, (tuple, list)) or len(x) != 2:\n             raise ValueError(\n-                \"Input `a` should be a tuple of two tensors - real and \"\n-                f\"imaginary. Received: a={a}\"\n+                \"Input `x` should be a tuple of two tensors - real and \"\n+                f\"imaginary. Received: x={x}\"\n             )\n \n-        real, imag = a\n+        real, imag = x\n         # Both real and imaginary parts should have the same shape.\n         if real.shape != imag.shape:\n             raise ValueError(\n-                \"Input `a` should be a tuple of two tensors - real and \"\n+                \"Input `x` should be a tuple of two tensors - real and \"\n                 \"imaginary. Both the real and imaginary parts should have the \"\n-                f\"same shape. Received: a[0].shape = {real.shape}, \"\n-                f\"a[1].shape = {imag.shape}\"\n+                f\"same shape. Received: x[0].shape = {real.shape}, \"\n+                f\"x[1].shape = {imag.shape}\"\n             )\n         # We are calculating 2D FFT. Hence, rank >= 2.\n         if len(real.shape) < 2:\n@@ -399,11 +399,11 @@ class FFT2(Operation):\n \n \n @keras_core_export(\"keras_core.ops.fft\")\n-def fft(a):\n+def fft(x):\n     \"\"\"Computes the Fast Fourier Transform along last axis of input.\n \n     Args:\n-        a: Tuple of the real and imaginary parts of the input tensor. Both\n+        x: Tuple of the real and imaginary parts of the input tensor. Both\n             tensors in the tuple should be of floating type.\n \n     Returns:\n@@ -412,24 +412,24 @@ def fft(a):\n \n     Example:\n \n-    >>> a = (\n+    >>> x = (\n     ...     keras_core.ops.convert_to_tensor([1., 2.]),\n     ...     keras_core.ops.convert_to_tensor([0., 1.]),\n     ... )\n     >>> fft(x)\n     (array([ 3., -1.], dtype=float32), array([ 1., -1.], dtype=float32))\n     \"\"\"\n-    if any_symbolic_tensors(a):\n-        return FFT().symbolic_call(a)\n-    return backend.math.fft(a)\n+    if any_symbolic_tensors(x):\n+        return FFT().symbolic_call(x)\n+    return backend.math.fft(x)\n \n \n @keras_core_export(\"keras_core.ops.fft2\")\n-def fft2(a):\n+def fft2(x):\n     \"\"\"Computes the 2D Fast Fourier Transform along the last two axes of input.\n \n     Args:\n-        a: Tuple of the real and imaginary parts of the input tensor. Both\n+        x: Tuple of the real and imaginary parts of the input tensor. Both\n             tensors in the tuple should be of floating type.\n \n     Returns:\n@@ -447,37 +447,6 @@ def fft2(a):\n         [ 0., -2.]], dtype=float32), array([[ 2.,  0.],\n         [ 0., -2.]], dtype=float32))\n     \"\"\"\n-    if any_symbolic_tensors(a):\n-        return FFT2().symbolic_call(a)\n-    return backend.math.fft2(a)\n-\n-\n-class Rsqrt(Operation):\n-    \"\"\"Computes reciprocal of square root of x element-wise.\n-\n-    Args:\n-        x: input tensor\n-\n-    Returns:\n-        A tensor with the same type as `x`.\n-\n-    Example:\n-\n-    >>> x = keras_core.ops.convert_to_tensor([2., 3., -2.])\n-    >>> rsqrt(x)\n-    \"\"\"\n-\n-    def call(self, x):\n-        x = backend.convert_to_tensor(x)\n-        return backend.math.rsqrt(x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, dtype=x.dtype)\n-\n-\n-@keras_core_export(\"keras_core.ops.rsqrt\")\n-def rsqrt(x):\n-    if any_symbolic_tensors((x,)):\n-        return Rsqrt().symbolic_call(x)\n-    x = backend.convert_to_tensor(x)\n-    return backend.math.rsqrt(x)\n+    if any_symbolic_tensors(x):\n+        return FFT2().symbolic_call(x)\n+    return backend.math.fft2(x)\n\n@@ -82,10 +82,6 @@ class MathOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(real_output.shape, ref_shape)\n         self.assertEqual(imag_output.shape, ref_shape)\n \n-    def test_rsqrt(self):\n-        x = KerasTensor([None, 3])\n-        self.assertEqual(kmath.rsqrt(x).shape, (None, 3))\n-\n \n class MathOpsStaticShapeTest(testing.TestCase):\n     @pytest.mark.skipif(\n@@ -164,10 +160,6 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(real_output.shape, ref.shape)\n         self.assertEqual(imag_output.shape, ref.shape)\n \n-    def test_rsqrt(self):\n-        x = KerasTensor([4, 3], dtype=\"float32\")\n-        self.assertEqual(kmath.rsqrt(x).shape, (4, 3))\n-\n \n class MathOpsCorrectnessTest(testing.TestCase):\n     @pytest.mark.skipif(\n@@ -402,12 +394,3 @@ class MathOpsCorrectnessTest(testing.TestCase):\n         imag_ref = np.imag(ref)\n         self.assertAllClose(real_ref, real_output)\n         self.assertAllClose(imag_ref, imag_output)\n-\n-    @pytest.mark.skipif(\n-        backend.backend() == \"numpy\",\n-        reason=\"Numpy does not support rsqrt.\",\n-    )\n-    def test_rsqrt(self):\n-        x = np.array([[1, 4, 9], [16, 25, 36]], dtype=\"float32\")\n-        self.assertAllClose(kmath.rsqrt(x), 1 / np.sqrt(x))\n-        self.assertAllClose(kmath.Rsqrt()(x), 1 / np.sqrt(x))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9f0a5ab9e0a2b104b7bfb3903be70c081a25324", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 13 | Files Changed: 6 | Hunks: 12 | Methods Changed: 12 | Complexity Δ (Sum/Max): 16/8 | Churn Δ: 126 | Churn Cumulative: 2267 | Contributors (this commit): 15 | Commits (past 90d): 52 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ Title: Learning to tokenize in Vision Transformers\n Authors: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Sayak Paul](https://twitter.com/RisingSayak) (equal contribution)\n Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2021/12/10\n-Last modified: 2023/07/18\n+Last modified: 2023/08/14\n Description: Adaptively generating a smaller number of tokens for Vision Transformers.\n Accelerator: GPU\n \"\"\"\n@@ -165,19 +165,25 @@ tokens.\n \"\"\"\n \n \n-def position_embedding(\n-    projected_patches, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n-):\n-    # Build the positions.\n-    positions = ops.arange(start=0, stop=num_patches, step=1)\n-\n-    # Encode the positions with an Embedding layer.\n-    encoded_positions = layers.Embedding(\n+class PatchEncoder(layers.Layer):\n+    def __init__(self, num_patches, projection_dim):\n+        super().__init__()\n+        self.num_patches = num_patches\n+        self.position_embedding = layers.Embedding(\n             input_dim=num_patches, output_dim=projection_dim\n-    )(positions)\n+        )\n \n-    # Add encoded positions to the projected patches.\n-    return projected_patches + encoded_positions\n+    def call(self, patch):\n+        positions = ops.expand_dims(\n+            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n+        )\n+        encoded = patch + self.position_embedding(positions)\n+        return encoded\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"num_patches\": self.num_patches})\n+        return config\n \n \n \"\"\"\n@@ -337,7 +343,9 @@ def create_vit_classifier(\n     )  # (B, number_patches, projection_dim)\n \n     # Add positional embeddings to the projected patches.\n-    encoded_patches = position_embedding(\n+    encoded_patches = PatchEncoder(\n+        num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n+    )(\n         projected_patches\n     )  # (B, number_patches, projection_dim)\n     encoded_patches = layers.Dropout(0.1)(encoded_patches)\n\n@@ -94,3 +94,7 @@ def fft2(a):\n     complex_input = _get_complex_tensor_from_tuple(a)\n     complex_output = jax.numpy.fft.fft2(complex_input)\n     return jax.numpy.real(complex_output), jax.numpy.imag(complex_output)\n+\n+\n+def rsqrt(x):\n+    return jax.lax.rsqrt(x)\n\n@@ -85,3 +85,7 @@ def fft2(a):\n     complex_input = _get_complex_tensor_from_tuple(a)\n     complex_output = tf.signal.fft2d(complex_input)\n     return tf.math.real(complex_output), tf.math.imag(complex_output)\n+\n+\n+def rsqrt(x):\n+    return tf.math.rsqrt(x)\n\n@@ -160,3 +160,8 @@ def fft2(a):\n     complex_input = _get_complex_tensor_from_tuple(a)\n     complex_output = torch.fft.fft2(complex_input)\n     return complex_output.real, complex_output.imag\n+\n+\n+def rsqrt(x):\n+    x = convert_to_tensor(x)\n+    return torch.rsqrt(x)\n\n@@ -450,3 +450,65 @@ def fft2(x):\n     if any_symbolic_tensors(x):\n         return FFT2().symbolic_call(x)\n     return backend.math.fft2(x)\n+\n+\n+class Rsqrt(Operation):\n+    \"\"\"Computes reciprocal of square root of x element-wise.\n+\n+    Args:\n+        x: input tensor\n+\n+    Returns:\n+        A tensor with the same type as `x`.\n+\n+    Example:\n+\n+    >>> x = keras_core.ops.convert_to_tensor([2., 3., -2.])\n+    >>> rsqrt(x)\n+    \"\"\"\n+\n+    def call(self, x):\n+        x = backend.convert_to_tensor(x)\n+        return backend.math.rsqrt(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_core_export(\"keras_core.ops.rsqrt\")\n+def rsqrt(x):\n+    if any_symbolic_tensors((x,)):\n+        return Rsqrt().symbolic_call(x)\n+    x = backend.convert_to_tensor(x)\n+    return backend.math.rsqrt(x)\n+\n+\n+class Rsqrt(Operation):\n+    \"\"\"Computes reciprocal of square root of x element-wise.\n+\n+    Args:\n+        x: input tensor\n+\n+    Returns:\n+        A tensor with the same type as `x`.\n+\n+    Example:\n+\n+    >>> x = keras_core.ops.convert_to_tensor([2., 3., -2.])\n+    >>> rsqrt(x)\n+    \"\"\"\n+\n+    def call(self, x):\n+        x = backend.convert_to_tensor(x)\n+        return backend.math.rsqrt(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_core_export(\"keras_core.ops.rsqrt\")\n+def rsqrt(x):\n+    if any_symbolic_tensors((x,)):\n+        return Rsqrt().symbolic_call(x)\n+    x = backend.convert_to_tensor(x)\n+    return backend.math.rsqrt(x)\n\n@@ -82,6 +82,10 @@ class MathOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(real_output.shape, ref_shape)\n         self.assertEqual(imag_output.shape, ref_shape)\n \n+    def test_rsqrt(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(kmath.rsqrt(x).shape, (None, 3))\n+\n \n class MathOpsStaticShapeTest(testing.TestCase):\n     @pytest.mark.skipif(\n@@ -160,6 +164,10 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(real_output.shape, ref.shape)\n         self.assertEqual(imag_output.shape, ref.shape)\n \n+    def test_rsqrt(self):\n+        x = KerasTensor([4, 3], dtype=\"float32\")\n+        self.assertEqual(kmath.rsqrt(x).shape, (4, 3))\n+\n \n class MathOpsCorrectnessTest(testing.TestCase):\n     @pytest.mark.skipif(\n@@ -394,3 +402,12 @@ class MathOpsCorrectnessTest(testing.TestCase):\n         imag_ref = np.imag(ref)\n         self.assertAllClose(real_ref, real_output)\n         self.assertAllClose(imag_ref, imag_output)\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\",\n+        reason=\"Numpy does not support rsqrt.\",\n+    )\n+    def test_rsqrt(self):\n+        x = np.array([[1, 4, 9], [16, 25, 36]], dtype=\"float32\")\n+        self.assertAllClose(kmath.rsqrt(x), 1 / np.sqrt(x))\n+        self.assertAllClose(kmath.Rsqrt()(x), 1 / np.sqrt(x))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4513bf7ba017d0c81c5b75cc1ba601dd4f11558e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 34 | Files Changed: 4 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): -3/1 | Churn Δ: 44 | Churn Cumulative: 1126 | Contributors (this commit): 11 | Commits (past 90d): 25 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -88,8 +88,10 @@ class LayerNormalizationTest(testing.TestCase):\n \n     def test_output(self):\n         layer = layers.LayerNormalization(\n-            dtype=\"float32\", beta_initializer=\"ones\", gamma_initializer=\"ones\",\n+            dtype=\"float32\",\n+            beta_initializer=\"ones\",\n+            gamma_initializer=\"ones\",\n         )\n         inputs = np.arange(5).astype(\"float32\")[None, :]\n         out = layer(inputs)\n-        self.assertAllClose(out, [[-0.41386, 0.29307, 1., 1.70693, 2.41386]])\n+        self.assertAllClose(out, [[-0.41386, 0.29307, 1.0, 1.70693, 2.41386]])\n\n@@ -481,34 +481,3 @@ def rsqrt(x):\n         return Rsqrt().symbolic_call(x)\n     x = backend.convert_to_tensor(x)\n     return backend.math.rsqrt(x)\n-\n-\n-class Rsqrt(Operation):\n-    \"\"\"Computes reciprocal of square root of x element-wise.\n-\n-    Args:\n-        x: input tensor\n-\n-    Returns:\n-        A tensor with the same type as `x`.\n-\n-    Example:\n-\n-    >>> x = keras_core.ops.convert_to_tensor([2., 3., -2.])\n-    >>> rsqrt(x)\n-    \"\"\"\n-\n-    def call(self, x):\n-        x = backend.convert_to_tensor(x)\n-        return backend.math.rsqrt(x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, dtype=x.dtype)\n-\n-\n-@keras_core_export(\"keras_core.ops.rsqrt\")\n-def rsqrt(x):\n-    if any_symbolic_tensors((x,)):\n-        return Rsqrt().symbolic_call(x)\n-    x = backend.convert_to_tensor(x)\n-    return backend.math.rsqrt(x)\n\n@@ -261,7 +261,7 @@ def get_source_inputs(tensor):\n         node = operation._inbound_nodes[node_index]\n         if node.is_input:\n             # Reached input node, stop recursion.\n-            return tree.flatten(node.input_tensors)\n+            return tree.flatten(node.output_tensors)\n         else:\n             source_tensors = []\n             for tensor in node.input_tensors:\n\n@@ -1,6 +1,7 @@\n from keras_core import backend\n from keras_core import ops\n from keras_core import testing\n+from keras_core.layers.core import input_layer\n from keras_core.ops import operation_utils\n \n \n@@ -12,3 +13,7 @@ class OperationUtilsTest(testing.TestCase):\n         x += 2\n         x = ops.square(x)\n         self.assertEqual(operation_utils.get_source_inputs(x), [x1, x2])\n+\n+    def test_get_source_inputs_return_input_tensor(self):\n+        inputs = input_layer.Input(shape=(10,))\n+        self.assertIs(operation_utils.get_source_inputs(inputs)[0], inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#53ce721319d954ebcf804555e35373380672ea16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 429 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -12,6 +12,9 @@ from keras_core import testing\n from keras_core.saving import saving_api\n \n \n+@pytest.mark.skipif(\n+    backend.backend() == \"numpy\", reason=\"Failing for numpy backend.\"\n+)\n class IndexLookupLayerTest(testing.TestCase, parameterized.TestCase):\n     def test_basics_string_vocab(self):\n         # Case: adapt + list inputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#998f69f88941f6033fba175c67362f81b45d7858", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 694 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 18 | Complexity Δ (Sum/Max): 26/26 | Churn Δ: 694 | Churn Cumulative: 694 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,694 @@\n+\"\"\"\n+Title: CycleGAN\n+Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n+Date created: 2020/08/12\n+Last modified: 2020/08/12\n+Description: Implementation of CycleGAN.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## CycleGAN\n+\n+CycleGAN is a model that aims to solve the image-to-image translation\n+problem. The goal of the image-to-image translation problem is to learn the\n+mapping between an input image and an output image using a training set of\n+aligned image pairs. However, obtaining paired examples isn't always feasible.\n+CycleGAN tries to learn this mapping without requiring paired input-output images,\n+using cycle-consistent adversarial networks.\n+\n+- [Paper](https://arxiv.org/pdf/1703.10593.pdf)\n+- [Original implementation](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import matplotlib.pyplot as plt\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import ops\n+\n+import tensorflow as tf\n+import tensorflow_datasets as tfds\n+\n+tfds.disable_progress_bar()\n+autotune = tf.data.AUTOTUNE\n+\n+\"\"\"\n+## Prepare the dataset\n+\n+In this example, we will be using the\n+[horse to zebra](https://www.tensorflow.org/datasets/catalog/cycle_gan#cycle_ganhorse2zebra)\n+dataset.\n+\"\"\"\n+\n+# Load the horse-zebra dataset using tensorflow-datasets.\n+dataset, _ = tfds.load(\n+    \"cycle_gan/horse2zebra\", with_info=True, as_supervised=True\n+)\n+train_horses, train_zebras = dataset[\"trainA\"], dataset[\"trainB\"]\n+test_horses, test_zebras = dataset[\"testA\"], dataset[\"testB\"]\n+\n+# Define the standard image size.\n+orig_img_size = (286, 286)\n+# Size of the random crops to be used during training.\n+input_img_size = (256, 256, 3)\n+# Weights initializer for the layers.\n+kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n+# Gamma initializer for instance normalization.\n+gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n+\n+buffer_size = 256\n+batch_size = 1\n+\n+\n+def normalize_img(img):\n+    img = ops.cast(img, dtype=\"float32\")\n+    # Map values in the range [-1, 1]\n+    return (img / 127.5) - 1.0\n+\n+\n+def preprocess_train_image(img, label):\n+    # Random flip\n+    img = tf.image.random_flip_left_right(img)\n+    # Resize to the original size first\n+    img = tf.image.resize(img, [*orig_img_size])\n+    # Random crop to 256X256\n+    img = tf.image.random_crop(img, size=[*input_img_size])\n+    # Normalize the pixel values in the range [-1, 1]\n+    img = normalize_img(img)\n+    return img\n+\n+\n+def preprocess_test_image(img, label):\n+    # Only resizing and normalization for the test images.\n+    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n+    img = normalize_img(img)\n+    return img\n+\n+\n+\"\"\"\n+## Create `Dataset` objects\n+\"\"\"\n+\n+\n+# Apply the preprocessing operations to the training data\n+train_horses = (\n+    train_horses.map(preprocess_train_image, num_parallel_calls=autotune)\n+    .cache()\n+    .shuffle(buffer_size)\n+    .batch(batch_size)\n+)\n+train_zebras = (\n+    train_zebras.map(preprocess_train_image, num_parallel_calls=autotune)\n+    .cache()\n+    .shuffle(buffer_size)\n+    .batch(batch_size)\n+)\n+\n+# Apply the preprocessing operations to the test data\n+test_horses = (\n+    test_horses.map(preprocess_test_image, num_parallel_calls=autotune)\n+    .cache()\n+    .shuffle(buffer_size)\n+    .batch(batch_size)\n+)\n+test_zebras = (\n+    test_zebras.map(preprocess_test_image, num_parallel_calls=autotune)\n+    .cache()\n+    .shuffle(buffer_size)\n+    .batch(batch_size)\n+)\n+\n+\n+\"\"\"\n+## Visualize some samples\n+\"\"\"\n+\n+\n+_, ax = plt.subplots(4, 2, figsize=(10, 15))\n+for i, samples in enumerate(zip(train_horses.take(4), train_zebras.take(4))):\n+    horse = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n+    zebra = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n+    ax[i, 0].imshow(horse)\n+    ax[i, 1].imshow(zebra)\n+plt.show()\n+\n+\n+\"\"\"\n+## Building blocks used in the CycleGAN generators and discriminators\n+\"\"\"\n+\n+\n+class ReflectionPadding2D(layers.Layer):\n+    \"\"\"Implements Reflection Padding as a layer.\n+\n+    Args:\n+        padding(tuple): Amount of padding for the\n+        spatial dimensions.\n+\n+    Returns:\n+        A padded tensor with the same type as the input tensor.\n+    \"\"\"\n+\n+    def __init__(self, padding=(1, 1), **kwargs):\n+        self.padding = tuple(padding)\n+        super().__init__(**kwargs)\n+\n+    def call(self, input_tensor, mask=None):\n+        padding_width, padding_height = self.padding\n+        padding_tensor = [\n+            [0, 0],\n+            [padding_height, padding_height],\n+            [padding_width, padding_width],\n+            [0, 0],\n+        ]\n+        return ops.pad(\n+            input_tensor, ops.convert_to_tensor(padding_tensor), mode=\"reflect\"\n+        )\n+\n+\n+def residual_block(\n+    x,\n+    activation,\n+    kernel_initializer=kernel_init,\n+    kernel_size=(3, 3),\n+    strides=(1, 1),\n+    padding=\"valid\",\n+    gamma_initializer=gamma_init,\n+    use_bias=False,\n+):\n+    dim = x.shape[-1]\n+    input_tensor = x\n+\n+    x = ReflectionPadding2D()(input_tensor)\n+    x = layers.Conv2D(\n+        dim,\n+        kernel_size,\n+        strides=strides,\n+        kernel_initializer=kernel_initializer,\n+        padding=padding,\n+        use_bias=use_bias,\n+    )(x)\n+    x = layers.GroupNormalization(\n+        groups=x.shape[-1], gamma_initializer=gamma_initializer\n+    )(x)\n+    x = activation(x)\n+\n+    x = ReflectionPadding2D()(x)\n+    x = layers.Conv2D(\n+        dim,\n+        kernel_size,\n+        strides=strides,\n+        kernel_initializer=kernel_initializer,\n+        padding=padding,\n+        use_bias=use_bias,\n+    )(x)\n+    x = layers.GroupNormalization(\n+        groups=x.shape[-1], gamma_initializer=gamma_initializer\n+    )(x)\n+    x = layers.add([input_tensor, x])\n+    return x\n+\n+\n+def downsample(\n+    x,\n+    filters,\n+    activation,\n+    kernel_initializer=kernel_init,\n+    kernel_size=(3, 3),\n+    strides=(2, 2),\n+    padding=\"same\",\n+    gamma_initializer=gamma_init,\n+    use_bias=False,\n+):\n+    x = layers.Conv2D(\n+        filters,\n+        kernel_size,\n+        strides=strides,\n+        kernel_initializer=kernel_initializer,\n+        padding=padding,\n+        use_bias=use_bias,\n+    )(x)\n+    x = layers.GroupNormalization(\n+        groups=x.shape[-1], gamma_initializer=gamma_initializer\n+    )(x)\n+    if activation:\n+        x = activation(x)\n+    return x\n+\n+\n+def upsample(\n+    x,\n+    filters,\n+    activation,\n+    kernel_size=(3, 3),\n+    strides=(2, 2),\n+    padding=\"same\",\n+    kernel_initializer=kernel_init,\n+    gamma_initializer=gamma_init,\n+    use_bias=False,\n+):\n+    x = layers.Conv2DTranspose(\n+        filters,\n+        kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        kernel_initializer=kernel_initializer,\n+        use_bias=use_bias,\n+    )(x)\n+    x = layers.GroupNormalization(\n+        groups=x.shape[-1], gamma_initializer=gamma_initializer\n+    )(x)\n+    if activation:\n+        x = activation(x)\n+    return x\n+\n+\n+\"\"\"\n+## Build the generators\n+\n+The generator consists of downsampling blocks: nine residual blocks\n+and upsampling blocks. The structure of the generator is the following:\n+\n+```\n+c7s1-64 ==> Conv block with `relu` activation, filter size of 7\n+d128 ====|\n+         |-> 2 downsampling blocks\n+d256 ====|\n+R256 ====|\n+R256     |\n+R256     |\n+R256     |\n+R256     |-> 9 residual blocks\n+R256     |\n+R256     |\n+R256     |\n+R256 ====|\n+u128 ====|\n+         |-> 2 upsampling blocks\n+u64  ====|\n+c7s1-3 => Last conv block with `tanh` activation, filter size of 7.\n+```\n+\"\"\"\n+\n+\n+def get_resnet_generator(\n+    filters=64,\n+    num_downsampling_blocks=2,\n+    num_residual_blocks=9,\n+    num_upsample_blocks=2,\n+    gamma_initializer=gamma_init,\n+    name=None,\n+):\n+    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n+    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n+    x = layers.Conv2D(\n+        filters, (7, 7), kernel_initializer=kernel_init, use_bias=False\n+    )(x)\n+    x = layers.GroupNormalization(\n+        groups=x.shape[-1], gamma_initializer=gamma_initializer\n+    )(x)\n+    x = layers.Activation(\"relu\")(x)\n+\n+    # Downsampling\n+    for _ in range(num_downsampling_blocks):\n+        filters *= 2\n+        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n+\n+    # Residual blocks\n+    for _ in range(num_residual_blocks):\n+        x = residual_block(x, activation=layers.Activation(\"relu\"))\n+\n+    # Upsampling\n+    for _ in range(num_upsample_blocks):\n+        filters //= 2\n+        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n+\n+    # Final block\n+    x = ReflectionPadding2D(padding=(3, 3))(x)\n+    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n+    x = layers.Activation(\"tanh\")(x)\n+\n+    model = keras.models.Model(img_input, x, name=name)\n+    return model\n+\n+\n+\"\"\"\n+## Build the discriminators\n+\n+The discriminators implement the following architecture:\n+`C64->C128->C256->C512`\n+\"\"\"\n+\n+\n+def get_discriminator(\n+    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n+):\n+    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n+    x = layers.Conv2D(\n+        filters,\n+        (4, 4),\n+        strides=(2, 2),\n+        padding=\"same\",\n+        kernel_initializer=kernel_initializer,\n+    )(img_input)\n+    x = layers.LeakyReLU(0.2)(x)\n+\n+    num_filters = filters\n+    for num_downsample_block in range(3):\n+        num_filters *= 2\n+        if num_downsample_block < 2:\n+            x = downsample(\n+                x,\n+                filters=num_filters,\n+                activation=layers.LeakyReLU(0.2),\n+                kernel_size=(4, 4),\n+                strides=(2, 2),\n+            )\n+        else:\n+            x = downsample(\n+                x,\n+                filters=num_filters,\n+                activation=layers.LeakyReLU(0.2),\n+                kernel_size=(4, 4),\n+                strides=(1, 1),\n+            )\n+\n+    x = layers.Conv2D(\n+        1,\n+        (4, 4),\n+        strides=(1, 1),\n+        padding=\"same\",\n+        kernel_initializer=kernel_initializer,\n+    )(x)\n+\n+    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n+    return model\n+\n+\n+# Get the generators\n+gen_G = get_resnet_generator(name=\"generator_G\")\n+gen_F = get_resnet_generator(name=\"generator_F\")\n+\n+# Get the discriminators\n+disc_X = get_discriminator(name=\"discriminator_X\")\n+disc_Y = get_discriminator(name=\"discriminator_Y\")\n+\n+\n+\"\"\"\n+## Build the CycleGAN model\n+\n+We will override the `train_step()` method of the `Model` class\n+for training via `fit()`.\n+\"\"\"\n+\n+\n+class CycleGan(keras.Model):\n+    def __init__(\n+        self,\n+        generator_G,\n+        generator_F,\n+        discriminator_X,\n+        discriminator_Y,\n+        lambda_cycle=10.0,\n+        lambda_identity=0.5,\n+    ):\n+        super().__init__()\n+        self.gen_G = generator_G\n+        self.gen_F = generator_F\n+        self.disc_X = discriminator_X\n+        self.disc_Y = discriminator_Y\n+        self.lambda_cycle = lambda_cycle\n+        self.lambda_identity = lambda_identity\n+\n+    def call(self, inputs):\n+        return (\n+            self.disc_X(inputs),\n+            self.disc_Y(inputs),\n+            self.gen_G(inputs),\n+            self.gen_F(inputs),\n+        )\n+\n+    def compile(\n+        self,\n+        gen_G_optimizer,\n+        gen_F_optimizer,\n+        disc_X_optimizer,\n+        disc_Y_optimizer,\n+        gen_loss_fn,\n+        disc_loss_fn,\n+    ):\n+        super().compile()\n+        self.gen_G_optimizer = gen_G_optimizer\n+        self.gen_F_optimizer = gen_F_optimizer\n+        self.disc_X_optimizer = disc_X_optimizer\n+        self.disc_Y_optimizer = disc_Y_optimizer\n+        self.generator_loss_fn = gen_loss_fn\n+        self.discriminator_loss_fn = disc_loss_fn\n+        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n+        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n+\n+    def train_step(self, batch_data):\n+        # x is Horse and y is zebra\n+        real_x, real_y = batch_data\n+\n+        # For CycleGAN, we need to calculate different\n+        # kinds of losses for the generators and discriminators.\n+        # We will perform the following steps here:\n+        #\n+        # 1. Pass real images through the generators and get the generated images\n+        # 2. Pass the generated images back to the generators to check if we\n+        #    can predict the original image from the generated image.\n+        # 3. Do an identity mapping of the real images using the generators.\n+        # 4. Pass the generated images in 1) to the corresponding discriminators.\n+        # 5. Calculate the generators total loss (adversarial + cycle + identity)\n+        # 6. Calculate the discriminators loss\n+        # 7. Update the weights of the generators\n+        # 8. Update the weights of the discriminators\n+        # 9. Return the losses in a dictionary\n+\n+        with tf.GradientTape(persistent=True) as tape:\n+            # Horse to fake zebra\n+            fake_y = self.gen_G(real_x, training=True)\n+            # Zebra to fake horse -> y2x\n+            fake_x = self.gen_F(real_y, training=True)\n+\n+            # Cycle (Horse to fake zebra to fake horse): x -> y -> x\n+            cycled_x = self.gen_F(fake_y, training=True)\n+            # Cycle (Zebra to fake horse to fake zebra) y -> x -> y\n+            cycled_y = self.gen_G(fake_x, training=True)\n+\n+            # Identity mapping\n+            same_x = self.gen_F(real_x, training=True)\n+            same_y = self.gen_G(real_y, training=True)\n+\n+            # Discriminator output\n+            disc_real_x = self.disc_X(real_x, training=True)\n+            disc_fake_x = self.disc_X(fake_x, training=True)\n+\n+            disc_real_y = self.disc_Y(real_y, training=True)\n+            disc_fake_y = self.disc_Y(fake_y, training=True)\n+\n+            # Generator adversarial loss\n+            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n+            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n+\n+            # Generator cycle loss\n+            cycle_loss_G = (\n+                self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n+            )\n+            cycle_loss_F = (\n+                self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n+            )\n+\n+            # Generator identity loss\n+            id_loss_G = (\n+                self.identity_loss_fn(real_y, same_y)\n+                * self.lambda_cycle\n+                * self.lambda_identity\n+            )\n+            id_loss_F = (\n+                self.identity_loss_fn(real_x, same_x)\n+                * self.lambda_cycle\n+                * self.lambda_identity\n+            )\n+\n+            # Total generator loss\n+            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n+            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n+\n+            # Discriminator loss\n+            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n+            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n+\n+        # Get the gradients for the generators\n+        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n+        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n+\n+        # Get the gradients for the discriminators\n+        disc_X_grads = tape.gradient(\n+            disc_X_loss, self.disc_X.trainable_variables\n+        )\n+        disc_Y_grads = tape.gradient(\n+            disc_Y_loss, self.disc_Y.trainable_variables\n+        )\n+\n+        # Update the weights of the generators\n+        self.gen_G_optimizer.apply_gradients(\n+            zip(grads_G, self.gen_G.trainable_variables)\n+        )\n+        self.gen_F_optimizer.apply_gradients(\n+            zip(grads_F, self.gen_F.trainable_variables)\n+        )\n+\n+        # Update the weights of the discriminators\n+        self.disc_X_optimizer.apply_gradients(\n+            zip(disc_X_grads, self.disc_X.trainable_variables)\n+        )\n+        self.disc_Y_optimizer.apply_gradients(\n+            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n+        )\n+\n+        return {\n+            \"G_loss\": total_loss_G,\n+            \"F_loss\": total_loss_F,\n+            \"D_X_loss\": disc_X_loss,\n+            \"D_Y_loss\": disc_Y_loss,\n+        }\n+\n+\n+\"\"\"\n+## Create a callback that periodically saves generated images\n+\"\"\"\n+\n+\n+class GANMonitor(keras.callbacks.Callback):\n+    \"\"\"A callback to generate and save images after each epoch\"\"\"\n+\n+    def __init__(self, num_img=4):\n+        self.num_img = num_img\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n+        for i, img in enumerate(test_horses.take(self.num_img)):\n+            prediction = self.model.gen_G(img)[0].numpy()\n+            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n+            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n+\n+            ax[i, 0].imshow(img)\n+            ax[i, 1].imshow(prediction)\n+            ax[i, 0].set_title(\"Input image\")\n+            ax[i, 1].set_title(\"Translated image\")\n+            ax[i, 0].axis(\"off\")\n+            ax[i, 1].axis(\"off\")\n+\n+            prediction = keras.utils.array_to_img(prediction)\n+            prediction.save(\n+                \"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1)\n+            )\n+        plt.show()\n+        plt.close()\n+\n+\n+\"\"\"\n+## Train the end-to-end model\n+\"\"\"\n+\n+\n+# Loss function for evaluating adversarial loss\n+adv_loss_fn = keras.losses.MeanSquaredError()\n+\n+# Define the loss function for the generators\n+\n+\n+def generator_loss_fn(fake):\n+    fake_loss = adv_loss_fn(ops.ones_like(fake), fake)\n+    return fake_loss\n+\n+\n+# Define the loss function for the discriminators\n+def discriminator_loss_fn(real, fake):\n+    real_loss = adv_loss_fn(ops.ones_like(real), real)\n+    fake_loss = adv_loss_fn(ops.zeros_like(fake), fake)\n+    return (real_loss + fake_loss) * 0.5\n+\n+\n+# Create cycle gan model\n+cycle_gan_model = CycleGan(\n+    generator_G=gen_G,\n+    generator_F=gen_F,\n+    discriminator_X=disc_X,\n+    discriminator_Y=disc_Y,\n+)\n+\n+# Compile the model\n+cycle_gan_model.compile(\n+    gen_G_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n+    gen_F_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n+    disc_X_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n+    disc_Y_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n+    gen_loss_fn=generator_loss_fn,\n+    disc_loss_fn=discriminator_loss_fn,\n+)\n+# Callbacks\n+plotter = GANMonitor()\n+checkpoint_filepath = (\n+    \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}.weights.h5\"\n+)\n+model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n+    filepath=checkpoint_filepath, save_weights_only=True\n+)\n+\n+# Here we will train the model for just one epoch as each epoch takes around\n+# 7 minutes on a single P100 backed machine.\n+cycle_gan_model.fit(\n+    tf.data.Dataset.zip((train_horses, train_zebras)),\n+    epochs=1,\n+    callbacks=[plotter, model_checkpoint_callback],\n+)\n+\n+\"\"\"\n+Test the performance of the model.\n+\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/CycleGAN)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/CycleGAN).\n+\"\"\"\n+\n+\n+# This model was trained for 90 epochs. We will be loading those weights\n+# here. Once the weights are loaded, we will take a few samples from the test\n+# data and check the model's performance.\n+\n+\"\"\"shell\n+curl -LO https://github.com/freedomtan/cyclegan-keras-core/archive/refs/tags/2.0.zip\n+unzip -qq 2.0.zip\n+\"\"\"\n+\n+\n+# Load the checkpoints\n+weight_file = \"./cyclegan-keras-core-2.0/model_checkpoints/cyclegan_checkpoints.090.weights.h5\"\n+cycle_gan_model.load_weights(weight_file)\n+print(\"Weights loaded successfully\")\n+\n+_, ax = plt.subplots(4, 2, figsize=(10, 15))\n+for i, img in enumerate(test_horses.take(4)):\n+    prediction = cycle_gan_model.gen_G(img, training=False)[0].numpy()\n+    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n+    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n+\n+    ax[i, 0].imshow(img)\n+    ax[i, 1].imshow(prediction)\n+    ax[i, 0].set_title(\"Input image\")\n+    ax[i, 0].set_title(\"Input image\")\n+    ax[i, 1].set_title(\"Translated image\")\n+    ax[i, 0].axis(\"off\")\n+    ax[i, 1].axis(\"off\")\n+\n+    prediction = keras.utils.array_to_img(prediction)\n+    prediction.save(\"predicted_img_{i}.png\".format(i=i))\n+plt.tight_layout()\n+plt.show()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#558c244df5897bd80d7c8824835f41798aa4caa0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 51 | Churn Cumulative: 1113 | Contributors (this commit): 8 | Commits (past 90d): 19 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -82,6 +82,10 @@ class LayerNormalization(Layer):\n             When the next layer is linear (also e.g. `nn.relu`), this can be\n             disabled since the scaling will be done by the next layer.\n             Defaults to `True`.\n+        rms_scaling: If True, `center` and `scale` are ignored, and the\n+            inputs are scaled by `gamma` and the inverse square root\n+            of the square of all inputs. This is an approximate and faster\n+            approach that avoids ever computing the mean of the input.\n         beta_initializer: Initializer for the beta weight. Defaults to zeros.\n         gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n         beta_regularizer: Optional regularizer for the beta weight.\n@@ -106,6 +110,7 @@ class LayerNormalization(Layer):\n         epsilon=1e-3,\n         center=True,\n         scale=True,\n+        rms_scaling=False,\n         beta_initializer=\"zeros\",\n         gamma_initializer=\"ones\",\n         beta_regularizer=None,\n@@ -128,6 +133,7 @@ class LayerNormalization(Layer):\n         self.epsilon = epsilon\n         self.center = center\n         self.scale = scale\n+        self.rms_scaling = rms_scaling\n         self.beta_initializer = initializers.get(beta_initializer)\n         self.gamma_initializer = initializers.get(gamma_initializer)\n         self.beta_regularizer = regularizers.get(beta_regularizer)\n@@ -143,7 +149,7 @@ class LayerNormalization(Layer):\n         else:\n             shape = (input_shape[self.axis],)\n             self.axis = [self.axis]\n-        if self.scale:\n+        if self.scale or self.rms_scaling:\n             self.gamma = self.add_weight(\n                 name=\"gamma\",\n                 shape=shape,\n@@ -155,7 +161,7 @@ class LayerNormalization(Layer):\n         else:\n             self.gamma = None\n \n-        if self.center:\n+        if self.center and not self.rms_scaling:\n             self.beta = self.add_weight(\n                 name=\"beta\",\n                 shape=shape,\n@@ -196,17 +202,20 @@ class LayerNormalization(Layer):\n             # this is at least as numerically stable as the fused version.\n             inputs = ops.cast(inputs, \"float32\")\n \n-        # Calculate the mean and variance last axis (layer activations).\n-        mean = ops.mean(inputs, axis=self.axis, keepdims=True)\n-        # Don't use ops.var, as that would re-compute the mean from scratch.\n-        variance = ops.mean(\n-            ops.power(inputs - mean, 2), axis=self.axis, keepdims=True\n-        )\n-\n-        scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n+        # Calculate the variance last axis (layer activations).\n+        variance = ops.var(inputs, axis=self.axis, keepdims=True)\n \n         # Compute the batch normalization.\n         inv = 1 / ops.sqrt(variance + self.epsilon)\n+\n+        if self.rms_scaling:\n+            # Calculate outputs with only variance and gamma if rms scaling\n+            # is enabled\n+            outputs = inputs * ops.cast(inv, inputs.dtype) * self.gamma\n+        else:\n+            # Calculate the mean last axis (layer activations).\n+            mean = ops.mean(inputs, axis=self.axis, keepdims=True)\n+            scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n             if scale is not None:\n                 scale = ops.cast(scale, inputs.dtype)\n                 inv = inv * scale\n@@ -214,6 +223,7 @@ class LayerNormalization(Layer):\n             if offset is not None:\n                 offset = ops.cast(offset, inputs.dtype)\n                 x = offset + x\n+\n             outputs = inputs * ops.cast(inv, inputs.dtype) + ops.cast(\n                 x, inputs.dtype\n             )\n\n@@ -49,6 +49,17 @@ class LayerNormalizationTest(testing.TestCase):\n             expected_num_losses=0,\n             supports_masking=True,\n         )\n+        self.run_layer_test(\n+            layers.LayerNormalization,\n+            init_kwargs={\"rms_scaling\": True},\n+            input_shape=(3, 3),\n+            expected_output_shape=(3, 3),\n+            expected_num_trainable_weights=1,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n         self.run_layer_test(\n             layers.LayerNormalization,\n             init_kwargs={\"axis\": (-3, -2, -1)},\n@@ -95,3 +106,13 @@ class LayerNormalizationTest(testing.TestCase):\n         inputs = np.arange(5).astype(\"float32\")[None, :]\n         out = layer(inputs)\n         self.assertAllClose(out, [[-0.41386, 0.29307, 1.0, 1.70693, 2.41386]])\n+\n+    def test_output_with_rms_scaling(self):\n+        layer = layers.LayerNormalization(\n+            dtype=\"float32\",\n+            rms_scaling=True,\n+            gamma_initializer=\"ones\",\n+        )\n+        inputs = np.arange(5).astype(\"float32\")[None, :]\n+        out = layer(inputs)\n+        self.assertAllClose(out, [[0., 0.70693, 1.41386, 2.12079, 2.82772]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
