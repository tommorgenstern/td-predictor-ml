{"custom_id": "scrapy#443fb98a4776f4196662bb48918f1471758b7ae7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 109 | Files Changed: 2 | Hunks: 34 | Methods Changed: 27 | Complexity Δ (Sum/Max): -4/10 | Churn Δ: 190 | Churn Cumulative: 1386 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: 0.95\n\nDIFF:\n@@ -5,42 +5,11 @@ from collections import namedtuple\n from queuelib import PriorityQueue\n \n from scrapy.utils.reqser import request_to_dict, request_from_dict\n-from scrapy.core.downloader import Downloader\n-from scrapy.http import Request\n-from scrapy.signals import request_reached_downloader, request_left_downloader\n-from scrapy.utils.httpobj import urlparse_cached\n \n \n logger = logging.getLogger(__name__)\n \n \n-SCHEDULER_SLOT_META_KEY = Downloader.DOWNLOAD_SLOT\n-\n-\n-def _scheduler_slot_read(request, default=None):\n-    return request.meta.get(SCHEDULER_SLOT_META_KEY, default)\n-\n-\n-def _scheduler_slot_write(request, slot):\n-    request.meta[SCHEDULER_SLOT_META_KEY] = slot\n-\n-\n-def _set_scheduler_slot(request):\n-    \"\"\"\n-    >>> request = Request('http://example.com')\n-    >>> _set_scheduler_slot(request)\n-    'example.com'\n-    >>> _scheduler_slot_read(request)\n-    'example.com'\n-    \"\"\"\n-    slot = _scheduler_slot_read(request, None)\n-    if slot is not None:\n-        return slot\n-    slot = urlparse_cached(request).hostname or ''\n-    _scheduler_slot_write(request, slot)\n-    return slot\n-\n-\n def _path_safe(text):\n     \"\"\" Return a filesystem-safe version of a string ``text`` \"\"\"\n     pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_'\n@@ -138,6 +107,25 @@ class ScrapyPriorityQueue(PriorityQueue):\n         return request\n \n \n+class DownloaderInterface(object):\n+\n+    def __init__(self, crawler):\n+        self.downloader = crawler.engine.downloader\n+\n+    def stats(self, possible_slots):\n+        return [(self._active_downloads(slot), slot)\n+                for slot in possible_slots]\n+\n+    def get_slot_key(self, request):\n+        return self.downloader._get_slot_key(request, None)\n+\n+    def _active_downloads(self, slot):\n+        \"\"\" Return a number of requests in a Downloader for a given slot \"\"\"\n+        if slot not in self.downloader.slots:\n+            return 0\n+        return len(self.downloader.slots[slot].active)\n+\n+\n class DownloaderAwarePriorityQueue(object):\n     \"\"\" PriorityQueue which takes Downlaoder activity in account:\n     domains (slots) with the least amount of active downloads are dequeued\n@@ -170,68 +158,25 @@ class DownloaderAwarePriorityQueue(object):\n         def pqfactory(startprios=()):\n             return ScrapyPriorityQueue(crawler, qfactory, startprios, serialize)\n         self._slot_pqueues = _SlotPriorityQueues(pqfactory, slot_startprios)\n-\n-        self._active_downloads = {slot: 0 for slot in self._slot_pqueues.pqueues}\n-        crawler.signals.connect(self.on_response_download,\n-                                signal=request_left_downloader)\n-        crawler.signals.connect(self.on_request_reached_downloader,\n-                                signal=request_reached_downloader)\n         self.serialize = serialize\n-\n-    # There are two PriorityQueues at the same time (memory and disk-based),\n-    # and they both listen to Downloader signals. To filter out signals\n-    # coming from the other queue, each queue keeps track of its own\n-    # requests using mark / unmark / check_mark methods.\n-    def mark(self, request):\n-        request.meta[self._DOWNLOADER_AWARE_PQ_ID] = id(self)\n-\n-    def check_mark(self, request):\n-        return request.meta.get(self._DOWNLOADER_AWARE_PQ_ID, None) == id(self)\n-\n-    def unmark(self, request):\n-        del request.meta[self._DOWNLOADER_AWARE_PQ_ID]\n+        self._downloader_interface = DownloaderInterface(crawler)\n \n     def pop(self):\n-        slots = [(active_downloads, slot)\n-                 for slot, active_downloads in self._active_downloads.items()\n-                 if slot in self._slot_pqueues]\n+        stats = self._downloader_interface.stats(self._slot_pqueues.pqueues)\n \n-        if not slots:\n+        if not stats:\n             return\n \n-        slot = min(slots)[1]\n+        slot = min(stats)[1]\n         request = self._slot_pqueues.pop_slot(slot)\n-        self.mark(request)\n         return request\n \n     def push(self, request, priority):\n-        slot = _set_scheduler_slot(request)\n+        slot = self._downloader_interface.get_slot_key(request)\n         priority_slot = _Priority(priority=priority, slot=slot)\n         self._slot_pqueues.push_slot(slot, request, priority_slot)\n-        if slot not in self._active_downloads:\n-            self._active_downloads[slot] = 0\n-\n-    def on_response_download(self, request, spider):\n-        if not self.check_mark(request):\n-            return\n-        self.unmark(request)\n-\n-        slot = _scheduler_slot_read(request)\n-        if slot not in self._active_downloads or self._active_downloads[slot] <= 0:\n-            raise ValueError('Got response for a wrong slot \"%s\"' % (slot, ))\n-        self._active_downloads[slot] -= 1\n-        if self._active_downloads[slot] == 0 and slot not in self._slot_pqueues:\n-            del self._active_downloads[slot]\n-\n-    def on_request_reached_downloader(self, request, spider):\n-        if not self.check_mark(request):\n-            return\n-\n-        slot = _scheduler_slot_read(request)\n-        self._active_downloads[slot] = self._active_downloads.get(slot, 0) + 1\n \n     def close(self):\n-        self._active_downloads.clear()\n         active = self._slot_pqueues.close()\n         return {slot: [p.priority for p in startprios]\n                 for slot, startprios in active.items()}\n\n@@ -1,20 +1,50 @@\n import shutil\n import tempfile\n import unittest\n+import collections\n \n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n from scrapy.crawler import Crawler\n+from scrapy.core.downloader import Downloader\n from scrapy.core.scheduler import Scheduler\n from scrapy.http import Request\n-from scrapy.pqueues import _scheduler_slot_read, _scheduler_slot_write\n-from scrapy.signals import request_reached_downloader, request_left_downloader\n from scrapy.spiders import Spider\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n \n \n+MockEngine = collections.namedtuple('MockEngine', ['downloader'])\n+MockSlot = collections.namedtuple('MockSlot', ['active'])\n+\n+\n+class MockDownloader:\n+    def __init__(self):\n+        self.slots = dict()\n+\n+    def _set_slot_key(self, slot, request, spider):\n+        request.meta[Downloader.DOWNLOAD_SLOT] = slot\n+\n+    def _get_slot_key(self, request, spider):\n+        if Downloader.DOWNLOAD_SLOT in request.meta:\n+            return request.meta[Downloader.DOWNLOAD_SLOT]\n+\n+        return urlparse_cached(request).hostname or ''\n+\n+    def increment(self, slot_key):\n+        slot = self.slots.setdefault(slot_key, MockSlot(active=list()))\n+        slot.active.append(1)\n+\n+    def decrement(self, slot_key):\n+        slot = self.slots.get(slot_key)\n+        slot.active.pop()\n+\n+    def close(self):\n+        pass\n+\n+\n class MockCrawler(Crawler):\n     def __init__(self, priority_queue_cls, jobdir):\n \n@@ -27,6 +57,7 @@ class MockCrawler(Crawler):\n                 DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter'\n                 )\n         super(MockCrawler, self).__init__(Spider, settings)\n+        self.engine = MockEngine(downloader=MockDownloader())\n \n \n class SchedulerHandler:\n@@ -42,6 +73,7 @@ class SchedulerHandler:\n     def close_scheduler(self):\n         self.scheduler.close('finished')\n         self.mock_crawler.stop()\n+        self.mock_crawler.engine.downloader.close()\n \n     def setUp(self):\n         self.create_scheduler()\n@@ -147,11 +179,11 @@ class BaseSchedulerOnDiskTester(SchedulerHandler):\n \n \n class TestSchedulerInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):\n-    priority_queue_cls = 'queuelib.PriorityQueue'\n+    priority_queue_cls = 'scrapy.pqueues.ScrapyPriorityQueue'\n \n \n class TestSchedulerOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):\n-    priority_queue_cls = 'queuelib.PriorityQueue'\n+    priority_queue_cls = 'scrapy.pqueues.ScrapyPriorityQueue'\n \n \n _SLOTS = [(\"http://foo.com/a\", 'a'),\n@@ -172,7 +204,7 @@ class TestMigration(unittest.TestCase):\n \n     def _migration(self, tmp_dir):\n         prev_scheduler_handler = SchedulerHandler()\n-        prev_scheduler_handler.priority_queue_cls = 'queuelib.PriorityQueue'\n+        prev_scheduler_handler.priority_queue_cls = 'scrapy.pqueues.ScrapyPriorityQueue'\n         prev_scheduler_handler.jobdir = tmp_dir\n \n         prev_scheduler_handler.create_scheduler()\n@@ -196,30 +228,25 @@ class TestSchedulerWithDownloaderAwareInMemory(BaseSchedulerInMemoryTester,\n     priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n \n     def test_logic(self):\n+        downloader = self.mock_crawler.engine.downloader\n         for url, slot in _SLOTS:\n             request = Request(url)\n-            _scheduler_slot_write(request, slot)\n+            downloader._set_slot_key(slot, request, None)\n             self.scheduler.enqueue_request(request)\n \n         slots = list()\n         requests = list()\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n-            slots.append(_scheduler_slot_read(request))\n-            self.mock_crawler.signals.send_catch_log(\n-                    signal=request_reached_downloader,\n-                    request=request,\n-                    spider=self.spider\n-                    )\n+            slot = downloader._get_slot_key(request, None)\n+            slots.append(slot)\n+            downloader.increment(slot)\n             requests.append(request)\n         self.assertEqual(len(slots), len(_SLOTS))\n \n         for request in requests:\n-            self.mock_crawler.signals.send_catch_log(\n-                    signal=request_left_downloader,\n-                    request=request,\n-                    spider=self.spider\n-                    )\n+            slot = downloader._get_slot_key(request, None)\n+            self.mock_crawler.engine.downloader.decrement(slot)\n \n         unique_slots = len(set(s for _, s in _SLOTS))\n         for i in range(0, len(_SLOTS), unique_slots):\n@@ -239,9 +266,11 @@ class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n     priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n \n     def test_logic(self):\n+        downloader = self.mock_crawler.engine.downloader\n+\n         for url, slot in _SLOTS:\n             request = Request(url)\n-            _scheduler_slot_write(request, slot)\n+            downloader._set_slot_key(slot, request, None)\n             self.scheduler.enqueue_request(request)\n \n         self.close_scheduler()\n@@ -249,27 +278,22 @@ class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n \n         slots = []\n         requests = []\n+        downloader = self.mock_crawler.engine.downloader\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n-            slots.append(_scheduler_slot_read(request))\n-            self.mock_crawler.signals.send_catch_log(\n-                    signal=request_reached_downloader,\n-                    request=request,\n-                    spider=self.spider\n-                    )\n+            slot = downloader._get_slot_key(request, None)\n+            slots.append(slot)\n+            downloader.increment(slot)\n             requests.append(request)\n \n-        self.assertEqual(self.scheduler.mqs._active_downloads, {})\n         self.assertEqual(len(slots), len(_SLOTS))\n \n         for request in requests:\n-            self.mock_crawler.signals.send_catch_log(\n-                    signal=request_left_downloader,\n-                    request=request,\n-                    spider=self.spider\n-                    )\n+            slot = downloader._get_slot_key(request, None)\n+            downloader.decrement(slot)\n \n         _is_slots_unique(_SLOTS, slots)\n+        self.assertEqual(sum(len(s.active) for s in downloader.slots.values()), 0)\n \n \n class StartUrlsSpider(Spider):\n@@ -277,6 +301,9 @@ class StartUrlsSpider(Spider):\n     def __init__(self, start_urls):\n         self.start_urls = start_urls\n \n+    def parse(self, response):\n+        pass\n+\n \n class TestIntegrationWithDownloaderAwareOnDisk(TestCase):\n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#989bba6cb340fcc1ddb32e75ade567864d8b3884", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 63 | Files Changed: 3 | Hunks: 3 | Methods Changed: 11 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 63 | Churn Cumulative: 939 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -188,9 +188,6 @@ class Downloader(object):\n         def finish_transferring(_):\n             slot.transferring.remove(request)\n             self._process_queue(spider, slot)\n-            self.signals.send_catch_log(signal=signals.request_left_downloader,\n-                                        request=request,\n-                                        spider=spider)\n             return _\n \n         return dfd.addBoth(finish_transferring)\n\n@@ -14,7 +14,6 @@ spider_error = object()\n request_scheduled = object()\n request_dropped = object()\n request_reached_downloader = object()\n-request_left_downloader = object()\n response_received = object()\n response_downloaded = object()\n item_scraped = object()\n\n@@ -1,59 +0,0 @@\n-from twisted.internet import defer\n-from twisted.trial.unittest import TestCase\n-from scrapy.signals import request_left_downloader\n-from scrapy.spiders import Spider\n-from scrapy.utils.test import get_crawler\n-from tests.mockserver import MockServer\n-\n-class SignalCatcherSpider(Spider):\n-    name = 'signal_catcher'\n-\n-    def __init__(self, crawler, url, *args, **kwargs):\n-        super(SignalCatcherSpider, self).__init__(*args, **kwargs)\n-        crawler.signals.connect(self.on_response_download,\n-                                signal=request_left_downloader)\n-        self.catched_times = 0\n-        self.start_urls = [url]\n-\n-    @classmethod\n-    def from_crawler(cls, crawler, *args, **kwargs):\n-        spider = cls(crawler, *args, **kwargs)\n-        return spider\n-\n-    def on_response_download(self, request, spider):\n-        self.catched_times = self.catched_times + 1\n-\n-\n-class TestCatching(TestCase):\n-\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n-\n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n-\n-    @defer.inlineCallbacks\n-    def test_success(self):\n-        crawler = get_crawler(SignalCatcherSpider)\n-        yield crawler.crawl(self.mockserver.url(\"/status?n=200\"))\n-        self.assertEqual(crawler.spider.catched_times, 1)\n-\n-    @defer.inlineCallbacks\n-    def test_timeout(self):\n-        crawler = get_crawler(SignalCatcherSpider,\n-                              {'DOWNLOAD_TIMEOUT': 0.1})\n-        yield crawler.crawl(self.mockserver.url(\"/delay?n=0.2\"))\n-        self.assertEqual(crawler.spider.catched_times, 1)\n-\n-    @defer.inlineCallbacks\n-    def test_disconnect(self):\n-        crawler = get_crawler(SignalCatcherSpider)\n-        yield crawler.crawl(self.mockserver.url(\"/drop\"))\n-        self.assertEqual(crawler.spider.catched_times, 1)\n-\n-    @defer.inlineCallbacks\n-    def test_noconnect(self):\n-        crawler = get_crawler(SignalCatcherSpider)\n-        yield crawler.crawl('http://thereisdefinetelynosuchdomain.com')\n-        self.assertEqual(crawler.spider.catched_times, 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8afffb7234b282dd8bd28eec2e4eb8e3f86b5723", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 31 | Files Changed: 1 | Hunks: 19 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 72 | Churn Cumulative: 663 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -24,9 +24,6 @@ class MockDownloader:\n     def __init__(self):\n         self.slots = dict()\n \n-    def _set_slot_key(self, slot, request, spider):\n-        request.meta[Downloader.DOWNLOAD_SLOT] = slot\n-\n     def _get_slot_key(self, request, spider):\n         if Downloader.DOWNLOAD_SLOT in request.meta:\n             return request.meta[Downloader.DOWNLOAD_SLOT]\n@@ -186,7 +183,7 @@ class TestSchedulerOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):\n     priority_queue_cls = 'scrapy.pqueues.ScrapyPriorityQueue'\n \n \n-_SLOTS = [(\"http://foo.com/a\", 'a'),\n+_URLS_WITH_SLOTS = [(\"http://foo.com/a\", 'a'),\n                     (\"http://foo.com/b\", 'a'),\n                     (\"http://foo.com/c\", 'b'),\n                     (\"http://foo.com/d\", 'b'),\n@@ -228,37 +225,52 @@ class TestSchedulerWithDownloaderAwareInMemory(BaseSchedulerInMemoryTester,\n     priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n \n     def test_logic(self):\n-        downloader = self.mock_crawler.engine.downloader\n-        for url, slot in _SLOTS:\n+        for url, slot in _URLS_WITH_SLOTS:\n             request = Request(url)\n-            downloader._set_slot_key(slot, request, None)\n+            request.meta[Downloader.DOWNLOAD_SLOT] = slot\n             self.scheduler.enqueue_request(request)\n \n-        slots = list()\n+        downloader = self.mock_crawler.engine.downloader\n+        dequeued_slots = list()\n         requests = list()\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n             slot = downloader._get_slot_key(request, None)\n-            slots.append(slot)\n+            dequeued_slots.append(slot)\n             downloader.increment(slot)\n             requests.append(request)\n-        self.assertEqual(len(slots), len(_SLOTS))\n \n         for request in requests:\n             slot = downloader._get_slot_key(request, None)\n             self.mock_crawler.engine.downloader.decrement(slot)\n \n-        unique_slots = len(set(s for _, s in _SLOTS))\n-        for i in range(0, len(_SLOTS), unique_slots):\n-            part = slots[i:i + unique_slots]\n-            self.assertEqual(len(part), len(set(part)))\n+        self.assertTrue(_is_scheduling_fair(list(s for u, s in _URLS_WITH_SLOTS),\n+                                            dequeued_slots))\n \n \n-def _is_slots_unique(base_slots, result_slots):\n-    unique_slots = len(set(s for _, s in base_slots))\n-    for i in range(0, len(result_slots), unique_slots):\n-        part = result_slots[i:i + unique_slots]\n-        assert len(part) == len(set(part))\n+def _is_scheduling_fair(enqueued_slots, dequeued_slots):\n+    \"\"\"\n+    We enqueued same number of requests for every slot.\n+    Assert correct order, e.g.\n+\n+    >>> enqueued = ['a', 'b', 'c'] * 2\n+    >>> correct = ['a', 'c', 'b', 'b', 'a', 'c']\n+    >>> incorrect = ['a', 'a', 'b', 'c', 'c', 'b']\n+    >>> _is_scheduling_fair(enqueued, correct)\n+    True\n+    >>> _is_scheduling_fair(enqueued, incorrect)\n+    False\n+    \"\"\"\n+    if len(dequeued_slots) != len(enqueued_slots):\n+        return False\n+\n+    slots_number = len(set(enqueued_slots))\n+    for i in range(0, len(dequeued_slots), slots_number):\n+        part = dequeued_slots[i:i + slots_number]\n+        if len(part) != len(set(part)):\n+            return False\n+\n+    return True\n \n \n class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n@@ -266,33 +278,31 @@ class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n     priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n \n     def test_logic(self):\n-        downloader = self.mock_crawler.engine.downloader\n \n-        for url, slot in _SLOTS:\n+        for url, slot in _URLS_WITH_SLOTS:\n             request = Request(url)\n-            downloader._set_slot_key(slot, request, None)\n+            request.meta[Downloader.DOWNLOAD_SLOT] = slot\n             self.scheduler.enqueue_request(request)\n \n         self.close_scheduler()\n         self.create_scheduler()\n \n-        slots = []\n+        dequeued_slots = list()\n         requests = []\n         downloader = self.mock_crawler.engine.downloader\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n             slot = downloader._get_slot_key(request, None)\n-            slots.append(slot)\n+            dequeued_slots.append(slot)\n             downloader.increment(slot)\n             requests.append(request)\n \n-        self.assertEqual(len(slots), len(_SLOTS))\n-\n         for request in requests:\n             slot = downloader._get_slot_key(request, None)\n             downloader.decrement(slot)\n \n-        _is_slots_unique(_SLOTS, slots)\n+        self.assertTrue(_is_scheduling_fair(list(s for u, s in _URLS_WITH_SLOTS),\n+                                            dequeued_slots))\n         self.assertEqual(sum(len(s.active) for s in downloader.slots.values()), 0)\n \n \n@@ -305,7 +315,7 @@ class StartUrlsSpider(Spider):\n         pass\n \n \n-class TestIntegrationWithDownloaderAwareOnDisk(TestCase):\n+class TestIntegrationWithDownloaderAwareInMemory(TestCase):\n     def setUp(self):\n         self.crawler = get_crawler(\n                     StartUrlsSpider,\n@@ -322,10 +332,10 @@ class TestIntegrationWithDownloaderAwareOnDisk(TestCase):\n         with MockServer() as mockserver:\n \n             url = mockserver.url(\"/status?n=200\", is_secure=False)\n-            slots = [url] * 6\n-            yield self.crawler.crawl(slots)\n+            start_urls = [url] * 6\n+            yield self.crawler.crawl(start_urls)\n             self.assertEqual(self.crawler.stats.get_value('downloader/response_count'),\n-                             len(slots))\n+                             len(start_urls))\n \n \n class TestIncompatibility(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df574de8cc5c58618f6075ca3afb14059a9e30ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 35 | Files Changed: 2 | Hunks: 13 | Methods Changed: 3 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 65 | Churn Cumulative: 1523 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,7 +11,16 @@ logger = logging.getLogger(__name__)\n \n \n def _path_safe(text):\n-    \"\"\" Return a filesystem-safe version of a string ``text`` \"\"\"\n+    \"\"\"\n+    Return a filesystem-safe version of a string ``text``\n+\n+    >>> _path_safe('simple.org').startswith('simple.org')\n+    True\n+    >>> _path_safe('dash-underscore_.org').startswith('dash-underscore_.org')\n+    True\n+    >>> _path_safe('some@symbol?').startswith('some_symbol_')\n+    True\n+    \"\"\"\n     pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_'\n                              for c in text])\n     # as we replace some letters we can get collision for different slots\n@@ -131,7 +140,6 @@ class DownloaderAwarePriorityQueue(object):\n     domains (slots) with the least amount of active downloads are dequeued\n     first.\n     \"\"\"\n-    _DOWNLOADER_AWARE_PQ_ID = '_DOWNLOADER_AWARE_PQ_ID'\n \n     @classmethod\n     def from_crawler(cls, crawler, qfactory, slot_startprios=None, serialize=False):\n\n@@ -20,7 +20,7 @@ MockEngine = collections.namedtuple('MockEngine', ['downloader'])\n MockSlot = collections.namedtuple('MockSlot', ['active'])\n \n \n-class MockDownloader:\n+class MockDownloader(object):\n     def __init__(self):\n         self.slots = dict()\n \n@@ -57,7 +57,7 @@ class MockCrawler(Crawler):\n         self.engine = MockEngine(downloader=MockDownloader())\n \n \n-class SchedulerHandler:\n+class SchedulerHandler(object):\n     priority_queue_cls = None\n     jobdir = None\n \n@@ -220,34 +220,6 @@ class TestMigration(unittest.TestCase):\n             self._migration(self.tmpdir)\n \n \n-class TestSchedulerWithDownloaderAwareInMemory(BaseSchedulerInMemoryTester,\n-                                               unittest.TestCase):\n-    priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n-\n-    def test_logic(self):\n-        for url, slot in _URLS_WITH_SLOTS:\n-            request = Request(url)\n-            request.meta[Downloader.DOWNLOAD_SLOT] = slot\n-            self.scheduler.enqueue_request(request)\n-\n-        downloader = self.mock_crawler.engine.downloader\n-        dequeued_slots = list()\n-        requests = list()\n-        while self.scheduler.has_pending_requests():\n-            request = self.scheduler.next_request()\n-            slot = downloader._get_slot_key(request, None)\n-            dequeued_slots.append(slot)\n-            downloader.increment(slot)\n-            requests.append(request)\n-\n-        for request in requests:\n-            slot = downloader._get_slot_key(request, None)\n-            self.mock_crawler.engine.downloader.decrement(slot)\n-\n-        self.assertTrue(_is_scheduling_fair(list(s for u, s in _URLS_WITH_SLOTS),\n-                                            dequeued_slots))\n-\n-\n def _is_scheduling_fair(enqueued_slots, dequeued_slots):\n     \"\"\"\n     We enqueued same number of requests for every slot.\n@@ -273,17 +245,17 @@ def _is_scheduling_fair(enqueued_slots, dequeued_slots):\n     return True\n \n \n-class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n-                                             unittest.TestCase):\n+class DownloaderAwareSchedulerTestMixin(object):\n     priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n+    reopen = False\n \n     def test_logic(self):\n-\n         for url, slot in _URLS_WITH_SLOTS:\n             request = Request(url)\n             request.meta[Downloader.DOWNLOAD_SLOT] = slot\n             self.scheduler.enqueue_request(request)\n \n+        if self.reopen:\n             self.close_scheduler()\n             self.create_scheduler()\n \n@@ -292,12 +264,14 @@ class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n         downloader = self.mock_crawler.engine.downloader\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n+            # pylint: disable=protected-access\n             slot = downloader._get_slot_key(request, None)\n             dequeued_slots.append(slot)\n             downloader.increment(slot)\n             requests.append(request)\n \n         for request in requests:\n+            # pylint: disable=protected-access\n             slot = downloader._get_slot_key(request, None)\n             downloader.decrement(slot)\n \n@@ -306,10 +280,23 @@ class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n         self.assertEqual(sum(len(s.active) for s in downloader.slots.values()), 0)\n \n \n+class TestSchedulerWithDownloaderAwareInMemory(DownloaderAwareSchedulerTestMixin,\n+                                               BaseSchedulerInMemoryTester,\n+                                               unittest.TestCase):\n+    pass\n+\n+\n+class TestSchedulerWithDownloaderAwareOnDisk(DownloaderAwareSchedulerTestMixin,\n+                                             BaseSchedulerOnDiskTester,\n+                                             unittest.TestCase):\n+    reopen = True\n+\n+\n class StartUrlsSpider(Spider):\n \n     def __init__(self, start_urls):\n         self.start_urls = start_urls\n+        super(StartUrlsSpider, self).__init__(start_urls)\n \n     def parse(self, response):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31b8a6b33aed9e77a4d37a5c83b1545202207cad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 442 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,4 @@\n import logging\n-import tempfile\n import warnings\n \n from twisted.internet import defer\n@@ -37,7 +36,11 @@ class CrawlerTestCase(BaseCrawlerTest):\n             self.assertIsInstance(spiders, sl_cls)\n \n             self.crawler.spiders\n-            self.assertEqual(len(w), 1, \"Warn deprecated access only once\")\n+            is_one_warning = len(w) == 1\n+            if not is_one_warning:\n+                for warning in w:\n+                    print(warning)\n+            self.assertTrue(is_one_warning, \"Warn deprecated access only once\")\n \n     def test_populate_spidercls_settings(self):\n         spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#73e4ff5304d273404a147d06726a8ae8cae1c925", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 6 | Churn Cumulative: 448 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -182,8 +182,12 @@ class CrawlerRunnerTestCase(BaseCrawlerTest):\n                 'SPIDER_MANAGER_CLASS': 'tests.test_crawler.CustomSpiderLoader'\n             })\n             self.assertIsInstance(runner.spider_loader, CustomSpiderLoader)\n-            self.assertEqual(len(w), 1)\n+            is_one_warning = len(w) == 1\n+            if not is_one_warning:\n+                for warning in w:\n+                    print(warning)\n             self.assertIn('Please use SPIDER_LOADER_CLASS', str(w[0].message))\n+            self.assertTrue(is_one_warning)\n \n     def test_crawl_rejects_spider_objects(self):\n         with raises(ValueError):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dc8310e2929d228a224f12055a26cfef71d751cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1109 | Contributors (this commit): 17 | Commits (past 90d): 15 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -274,12 +274,13 @@ class FormRequestTest(RequestTest):\n         r1 = self.request_class(\"http://www.example.com\", formdata={})\n         self.assertEqual(r1.body, b'')\n         \n-    def test_formdata_overrides_querystring_duplicates(self):\n+    def test_formdata_overrides_querystring(self):\n         data = (('a', 'one'), ('a', 'two'), ('b', '2'))\n         url = self.request_class('http://www.example.com/?a=0&b=1&c=3#fragment', method='GET', formdata=data).url.split('#')[0]\n         fs = _qs(self.request_class(url, method='GET', formdata=data))\n         self.assertEqual(set(fs[b'a']), {b'one', b'two'})\n         self.assertEqual(fs[b'b'], [b'2'])\n+        self.assertNone(fs[b'c'])\n \n         data = {'a' : '1', 'b' : '2'}\n         fs = _qs(self.request_class('http://www.example.com/', method='GET', formdata=data))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#213b9eb879c4c6d8ae60ffa950688a38951b3349", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1111 | Contributors (this commit): 17 | Commits (past 90d): 16 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -280,7 +280,7 @@ class FormRequestTest(RequestTest):\n         fs = _qs(self.request_class(url, method='GET', formdata=data))\n         self.assertEqual(set(fs[b'a']), {b'one', b'two'})\n         self.assertEqual(fs[b'b'], [b'2'])\n-        self.assertNone(fs[b'c'])\n+        self.assertIsNone(fs[b'c'])\n \n         data = {'a' : '1', 'b' : '2'}\n         fs = _qs(self.request_class('http://www.example.com/', method='GET', formdata=data))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ae856e8ba835d1d99510e9509a462b55a757e85f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1113 | Contributors (this commit): 17 | Commits (past 90d): 17 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -280,7 +280,7 @@ class FormRequestTest(RequestTest):\n         fs = _qs(self.request_class(url, method='GET', formdata=data))\n         self.assertEqual(set(fs[b'a']), {b'one', b'two'})\n         self.assertEqual(fs[b'b'], [b'2'])\n-        self.assertIsNone(fs[b'c'])\n+        self.assertIsNone(fs.get([b'c']))\n \n         data = {'a' : '1', 'b' : '2'}\n         fs = _qs(self.request_class('http://www.example.com/', method='GET', formdata=data))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5f2ad5377e54ecdb3059891db4887eb09e56bfc6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1115 | Contributors (this commit): 17 | Commits (past 90d): 18 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -280,7 +280,7 @@ class FormRequestTest(RequestTest):\n         fs = _qs(self.request_class(url, method='GET', formdata=data))\n         self.assertEqual(set(fs[b'a']), {b'one', b'two'})\n         self.assertEqual(fs[b'b'], [b'2'])\n-        self.assertIsNone(fs.get([b'c']))\n+        self.assertIsNone(fs.get(b'c'))\n \n         data = {'a' : '1', 'b' : '2'}\n         fs = _qs(self.request_class('http://www.example.com/', method='GET', formdata=data))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2fd8b7c28c255148f6d7320cb97292ab2d569eff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 232 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,6 +34,8 @@ class BaseRedirectMiddleware(object):\n             redirected.meta['redirect_ttl'] = ttl - 1\n             redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + \\\n                 [request.url]\n+            redirected.meta['redirect_reasons'] = request.meta.get('redirect_reasons', []) + \\\n+                [reason]\n             redirected.dont_filter = request.dont_filter\n             redirected.priority = request.priority + self.priority_adjust\n             logger.debug(\"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n\n@@ -139,6 +139,16 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')\n         self.assertEqual(req3.meta['redirect_urls'], ['http://scrapytest.org/first', 'http://scrapytest.org/redirected'])\n \n+    def test_redirect_reasons(self):\n+        req1 = Request('http://scrapytest.org/first')\n+        rsp1 = Response('http://scrapytest.org/first', headers={'Location': '/redirected1'}, status=301)\n+        req2 = self.mw.process_response(req1, rsp1, self.spider)\n+        rsp2 = Response('http://scrapytest.org/redirected1', headers={'Location': '/redirected2'}, status=301)\n+        req3 = self.mw.process_response(req2, rsp2, self.spider)\n+\n+        self.assertEqual(req2.meta['redirect_reasons'], [301])\n+        self.assertEqual(req3.meta['redirect_reasons'], [301, 301])\n+\n     def test_spider_handling(self):\n         smartspider = self.crawler._create_spider('smarty')\n         smartspider.handle_httpstatus_list = [404, 301, 302]\n@@ -259,6 +269,15 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')\n         self.assertEqual(req3.meta['redirect_urls'], ['http://scrapytest.org/first', 'http://scrapytest.org/redirected'])\n \n+    def test_redirect_reasons(self):\n+        req1 = Request('http://scrapytest.org/first')\n+        rsp1 = HtmlResponse('http://scrapytest.org/first', body=self._body(url='/redirected'))\n+        req2 = self.mw.process_response(req1, rsp1, self.spider)\n+        rsp2 = HtmlResponse('http://scrapytest.org/redirected', body=self._body(url='/redirected1'))\n+        req3 = self.mw.process_response(req2, rsp2, self.spider)\n+\n+        self.assertEqual(req2.meta['redirect_reasons'], ['meta refresh'])\n+        self.assertEqual(req3.meta['redirect_reasons'], ['meta refresh', 'meta refresh'])\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ca882d8d9f094244d9f1d3476fb72fab9c230765", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 498 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -80,9 +80,10 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         # straigh forward case: RGB and JPEG\n         COLOUR = (0, 127, 255)\n         im, buf = _create_image('JPEG', 'RGB', SIZE, COLOUR)\n-        converted, buf = self.pipeline.convert_image(im, buf)\n+        converted, converted_buf = self.pipeline.convert_image(im, buf)\n         self.assertEqual(converted.mode, 'RGB')\n         self.assertEqual(converted.getcolors(), [(10000, COLOUR)])\n+        self.assertEqual(converted_buf.read(), buf.read())\n \n         # check that thumbnail keep image ratio\n         thumbnail, buf = self.pipeline.convert_image(converted, buf, size=(10, 25))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#70a4d93aa324fb276e60d641b37bdc6eb707b1cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 41 | Churn Cumulative: 562 | Contributors (this commit): 10 | Commits (past 90d): 7 | Contributors (cumulative): 16 | DMM Complexity: 0.24\n\nDIFF:\n@@ -39,6 +39,9 @@ class KeywordArgumentsSpider(MockServerSpider):\n         yield Request(self.mockserver.url('/general_with'), self.parse_general, cb_kwargs=data)\n         yield Request(self.mockserver.url('/general_without'), self.parse_general)\n         yield Request(self.mockserver.url('/no_kwargs'), self.parse_no_kwargs)\n+        yield Request(self.mockserver.url('/default'), self.parse_default, cb_kwargs=data)\n+        yield Request(self.mockserver.url('/takes_less'), self.parse_takes_less, cb_kwargs=data)\n+        yield Request(self.mockserver.url('/takes_more'), self.parse_takes_more, cb_kwargs=data)\n \n     def parse_first(self, response, key, number):\n         self.checks.append(key == 'value')\n@@ -66,6 +69,25 @@ class KeywordArgumentsSpider(MockServerSpider):\n         self.checks.append(response.url.endswith('/no_kwargs'))\n         self.crawler.stats.inc_value('boolean_checks')\n \n+    def parse_default(self, response, key, number=None, default=99):\n+        self.checks.append(response.url.endswith('/default'))\n+        self.checks.append(key == 'value')\n+        self.checks.append(number == 123)\n+        self.checks.append(default == 99)\n+        self.crawler.stats.inc_value('boolean_checks', 4)\n+\n+    def parse_takes_less(self, response, key):\n+        \"\"\"\n+        Should raise\n+        TypeError: parse_takes_less() got an unexpected keyword argument 'number'\n+        \"\"\"\n+\n+    def parse_takes_more(self, response, key, number, other):\n+        \"\"\"\n+        Should raise\n+        TypeError: parse_takes_more() missing 1 required positional argument: 'other'\n+        \"\"\"\n+\n \n class FollowAllSpider(MetaSpider):\n \n\n@@ -4,6 +4,7 @@ import logging\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n+import six\n \n from scrapy.http import Request\n from scrapy.crawler import CrawlerRunner\n@@ -15,6 +16,8 @@ from tests.mockserver import MockServer\n \n class CrawlTestCase(TestCase):\n \n+    maxDiff = None\n+\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n@@ -26,9 +29,25 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_callback_kwargs(self):\n         crawler = self.runner.create_crawler(KeywordArgumentsSpider)\n+        with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n         self.assertTrue(all(crawler.spider.checks))\n         self.assertEqual(len(crawler.spider.checks), crawler.stats.get_value('boolean_checks'))\n+        # check exceptions for argument mismatch\n+        exceptions = {}\n+        for line in log.records:\n+            for key in ('takes_less', 'takes_more'):\n+                if key in line.getMessage():\n+                    exceptions[key] = line\n+        self.assertEqual(exceptions['takes_less'].exc_info[0], TypeError)\n+        self.assertEqual(str(exceptions['takes_less'].exc_info[1]), \"parse_takes_less() got an unexpected keyword argument 'number'\")\n+        self.assertEqual(exceptions['takes_more'].exc_info[0], TypeError)\n+        # py2 and py3 messages are different\n+        exc_message = str(exceptions['takes_more'].exc_info[1])\n+        if six.PY2:\n+            self.assertEqual(exc_message, \"parse_takes_more() takes exactly 5 arguments (4 given)\")\n+        elif six.PY3:\n+            self.assertEqual(exc_message, \"parse_takes_more() missing 1 required positional argument: 'other'\")\n \n     @defer.inlineCallbacks\n     def test_follow_all(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8af6331b5ff62d71ff80eddcc52b85c25482c0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 15 | Methods Changed: 13 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 45 | Churn Cumulative: 910 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,12 +51,13 @@ class Command(ScrapyCommand):\n             help=\"use this callback for parsing, instead looking for a callback\")\n         parser.add_option(\"-m\", \"--meta\", dest=\"meta\",\n             help=\"inject extra meta into the Request, it must be a valid raw json string\")\n+        parser.add_option(\"--cb_kwargs\", dest=\"cb_kwargs\",\n+            help=\"inject extra cb_kwargs into the Request, it must be a valid raw json string\")\n         parser.add_option(\"-d\", \"--depth\", dest=\"depth\", type=\"int\", default=1,\n             help=\"maximum depth for parsing requests [default: %default]\")\n         parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n             help=\"print each depth level one by one\")\n \n-\n     @property\n     def max_level(self):\n         levels = list(self.items.keys()) + list(self.requests.keys())\n@@ -111,10 +112,11 @@ class Command(ScrapyCommand):\n             if not opts.nolinks:\n                 self.print_requests(colour=colour)\n \n-    def run_callback(self, response, cb):\n+    def run_callback(self, response, callback, cb_kwargs=None):\n+        cb_kwargs = cb_kwargs or {}\n         items, requests = [], []\n \n-        for x in iterate_spider_output(cb(response)):\n+        for x in iterate_spider_output(callback(response, **cb_kwargs)):\n             if isinstance(x, (BaseItem, dict)):\n                 items.append(x)\n             elif isinstance(x, Request):\n@@ -142,8 +144,7 @@ class Command(ScrapyCommand):\n         else:\n             self.spidercls = spidercls_for_request(spider_loader, Request(url))\n             if not self.spidercls:\n-                logger.error('Unable to find spider for: %(url)s',\n-                             {'url': url})\n+                logger.error('Unable to find spider for: %(url)s', {'url': url})\n \n         # Request requires callback argument as callable or None, not string\n         request = Request(url, None)\n@@ -160,7 +161,7 @@ class Command(ScrapyCommand):\n                          {'url': url})\n \n     def prepare_request(self, spider, request, opts):\n-        def callback(response):\n+        def callback(response, **cb_kwargs):\n             # memorize first request\n             if not self.first_response:\n                 self.first_response = response\n@@ -192,7 +193,7 @@ class Command(ScrapyCommand):\n             # parse items and requests\n             depth = response.meta['_depth']\n \n-            items, requests = self.run_callback(response, cb)\n+            items, requests = self.run_callback(response, cb, cb_kwargs)\n             if opts.pipelines:\n                 itemproc = self.pcrawler.engine.scraper.itemproc\n                 for item in items:\n@@ -211,6 +212,10 @@ class Command(ScrapyCommand):\n         if opts.meta:\n             request.meta.update(opts.meta)\n \n+        # update cb_kwargs if any extra cb_kwargs was passed through the --cb_kwargs option.\n+        if opts.cb_kwargs:\n+            request.cb_kwargs.update(opts.cb_kwargs)\n+\n         request.meta['_depth'] = 1\n         request.meta['_callback'] = request.callback\n         request.callback = callback\n@@ -221,23 +226,29 @@ class Command(ScrapyCommand):\n \n         self.process_spider_arguments(opts)\n         self.process_request_meta(opts)\n+        self.process_request_cb_kwargs(opts)\n \n     def process_spider_arguments(self, opts):\n-\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n             raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n \n     def process_request_meta(self, opts):\n-\n         if opts.meta:\n             try:\n                 opts.meta = json.loads(opts.meta)\n             except ValueError:\n-                raise UsageError(\"Invalid -m/--meta value, pass a valid json string to -m or --meta. \" \\\n+                raise UsageError(\"Invalid -m/--meta value, pass a valid json string to -m or --meta. \"\n                                  \"Example: --meta='{\\\"foo\\\" : \\\"bar\\\"}'\", print_help=False)\n \n+    def process_request_cb_kwargs(self, opts):\n+        if opts.cb_kwargs:\n+            try:\n+                opts.cb_kwargs = json.loads(opts.cb_kwargs)\n+            except ValueError:\n+                raise UsageError(\"Invalid --cb_kwargs value, pass a valid json string to --cb_kwargs. \"\n+                                 \"Example: --cb_kwargs='{\\\"foo\\\" : \\\"bar\\\"}'\", print_help=False)\n \n     def run(self, args, opts):\n         # parse arguments\n\n@@ -43,6 +43,12 @@ class MySpider(scrapy.Spider):\n         else:\n             self.logger.debug('It Works!')\n \n+    def parse_request_with_cb_kwargs(self, response, foo=None, key=None):\n+        if foo == 'bar' and key == 'value':\n+            self.logger.debug('It Works!')\n+        else:\n+            self.logger.debug('It Does Not Work :(')\n+\n     def parse_request_without_meta(self, response):\n         foo = response.meta.get('foo', 'bar')\n \n@@ -120,6 +126,14 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            self.url('/html')])\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n+    @defer.inlineCallbacks\n+    def test_request_with_cb_kwargs(self):\n+        raw_json_string = '{\"foo\" : \"bar\", \"key\": \"value\"}'\n+        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n+                                           '--cb_kwargs', raw_json_string,\n+                                           '-c', 'parse_request_with_cb_kwargs',\n+                                           self.url('/html')])\n+        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_request_without_meta(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
