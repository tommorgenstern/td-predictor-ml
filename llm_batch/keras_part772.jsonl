{"custom_id": "keras#7d431cea42d13f189473f6c0d2d33eaa5228c40f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 9 | Files Changed: 5 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 3050 | Contributors (this commit): 12 | Commits (past 90d): 20 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -27,7 +27,7 @@ class Conv2D(BaseConv):\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape\n-            `(batch_size, channels, height, width)`\n+            `(batch_size, height, width, channels)`\n             while `\"channels_first\"` corresponds to inputs with shape\n             `(batch_size, channels, height, width)`. It defaults to the\n             `image_data_format` value found in your Keras config file at\n\n@@ -29,7 +29,7 @@ class Conv3D(BaseConv):\n             corresponds to inputs with shape\n             `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n             while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`.\n+            `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n             It defaults to the `image_data_format` value found in your Keras\n             config file at `~/.keras/keras.json`. If you never set it, then it\n             will be `\"channels_last\"`.\n\n@@ -34,7 +34,7 @@ class Conv3DTranspose(BaseConvTranspose):\n             corresponds to inputs with shape\n             `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n             while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`.\n+            `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n             It defaults to the `image_data_format` value found in your Keras\n             config file at `~/.keras/keras.json`. If you never set it, then it\n             will be `\"channels_last\"`.\n\n@@ -40,10 +40,11 @@ class DepthwiseConv2D(BaseDepthwiseConv):\n             output channels will be equal to `input_channel * depth_multiplier`.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n-            corresponds to inputs with shape `(batch, steps, features)`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n             while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch, features, steps)`. It defaults to the `image_data_format`\n-            value found in your Keras config file at `~/.keras/keras.json`.\n+            `(batch, channels, height, width)`. It defaults to the\n+            `image_data_format` value found in your Keras config file\n+            at `~/.keras/keras.json`.\n             If you never set it, then it will be `\"channels_last\"`.\n         dilation_rate: int or tuple/list of 2 integers, specifying the dilation\n             rate to use for dilated convolution.\n\n@@ -32,10 +32,11 @@ class SeparableConv2D(BaseSeparableConv):\n             `strides=1`, the output has the same size as the input.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n-            corresponds to inputs with shape `(batch, steps, features)`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n             while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch, features, steps)`. It defaults to the `image_data_format`\n-            value found in your Keras config file at `~/.keras/keras.json`.\n+            `(batch, channels, height, width)`. It defaults to the\n+            `image_data_format` value found in your Keras config file\n+            at `~/.keras/keras.json`.\n             If you never set it, then it will be `\"channels_last\"`.\n         dilation_rate: int or tuple/list of 2 integers, specifying the dilation\n             rate to use for dilated convolution. If only one int is specified,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a41eb5ec7c4be5548abf4ff66f067bbfb46a4f7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 82 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -194,8 +194,8 @@ def shuffle(x, axis=0, seed=None):\n def gamma(shape, alpha, dtype=None, seed=None):\n     dtype = dtype or floatx()\n     dtype = to_torch_dtype(dtype)\n-    alpha = torch.ones(shape) * torch.tensor(alpha)\n-    beta = torch.ones(shape)\n+    alpha = torch.broadcast_to(convert_to_tensor(alpha), shape)\n+    beta = torch.ones(shape, device=get_device())\n     prev_rng_state = torch.random.get_rng_state()\n     first_seed, second_seed = draw_seed(seed)\n     torch.manual_seed(first_seed + second_seed)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#001fce58fca859534da011063491cc303ba6013c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 9 | Files Changed: 6 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 22 | Churn Cumulative: 1040 | Contributors (this commit): 8 | Commits (past 90d): 15 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -93,8 +93,10 @@ def get(identifier):\n     \"\"\"Retrieve a Keras activation function via an identifier.\"\"\"\n     if identifier is None:\n         return linear\n-    if isinstance(identifier, (str, dict)):\n+    if isinstance(identifier, dict):\n         obj = deserialize(identifier)\n+    elif isinstance(identifier, str):\n+        obj = ALL_OBJECTS_DICT.get(identifier, None)\n     else:\n         obj = identifier\n     if callable(obj):\n\n@@ -21,11 +21,11 @@ def register_internal_serializable(path, symbol):\n \n \n def get_symbol_from_name(name):\n-    REGISTERED_NAMES_TO_OBJS.get(name, None)\n+    return REGISTERED_NAMES_TO_OBJS.get(name, None)\n \n \n def get_name_from_symbol(symbol):\n-    REGISTERED_OBJS_TO_NAMES.get(symbol, None)\n+    return REGISTERED_OBJS_TO_NAMES.get(symbol, None)\n \n \n if namex:\n\n@@ -46,8 +46,7 @@ def get(identifier):\n     if isinstance(identifier, dict):\n         obj = deserialize(identifier)\n     elif isinstance(identifier, str):\n-        config = {\"class_name\": str(identifier), \"config\": {}}\n-        obj = deserialize(config)\n+        obj = ALL_OBJECTS_DICT.get(identifier, None)\n     else:\n         obj = identifier\n \n\n@@ -163,7 +163,7 @@ def get(identifier):\n     if isinstance(identifier, dict):\n         obj = deserialize(identifier)\n     elif isinstance(identifier, str):\n-        obj = deserialize(identifier)\n+        obj = ALL_OBJECTS_DICT.get(identifier, None)\n     else:\n         obj = identifier\n \n\n@@ -1,3 +1,5 @@\n+import inspect\n+\n from keras.api_export import keras_export\n from keras.metrics.accuracy_metrics import Accuracy\n from keras.metrics.accuracy_metrics import BinaryAccuracy\n@@ -191,10 +193,12 @@ def get(identifier):\n     if isinstance(identifier, dict):\n         obj = deserialize(identifier)\n     elif isinstance(identifier, str):\n-        obj = deserialize(identifier)\n+        obj = ALL_OBJECTS_DICT.get(identifier, None)\n     else:\n         obj = identifier\n     if callable(obj):\n+        if inspect.isclass(obj):\n+            obj = obj()\n         return obj\n     else:\n         raise ValueError(f\"Could not interpret metric identifier: {identifier}\")\n\n@@ -46,8 +46,7 @@ def get(identifier):\n     if isinstance(identifier, dict):\n         obj = deserialize(identifier)\n     elif isinstance(identifier, str):\n-        config = {\"class_name\": str(identifier), \"config\": {}}\n-        obj = deserialize(config)\n+        obj = ALL_OBJECTS_DICT.get(identifier, None)\n     else:\n         obj = identifier\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b6b1c1a161c7eb7e03504638bdee52426fdeb0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 11 | Churn Cumulative: 126 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -89,6 +89,13 @@ class SeedGenerator:\n             self.state.assign((seed_state + 1) * 5387 % 933199)\n         return new_seed_value\n \n+    def get_config(self):\n+        return {\"seed\": self._initial_seed}\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        return cls(**config)\n+\n \n def global_seed_generator():\n     if jax_utils.is_in_jax_tracing_scope():\n\n@@ -77,3 +77,7 @@ class SeedGeneratorTest(testing.TestCase):\n             \"When tracing a JAX function, you should only use seeded random\",\n         ):\n             traced_function()\n+\n+    def test_seed_generator_serialization(self):\n+        random_generator = seed_generator.SeedGenerator(seed=42)\n+        self.run_class_serialization_test(random_generator)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6fe9aab5f9197e377a385389db3dadeab33d6e6d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 313 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -106,7 +106,7 @@ class Adafactor(optimizer.Optimizer):\n             else:\n                 # Always factor the last 2 dimensions.\n                 r_shape = var.shape[:-1]\n-                c_shape = var.shape[:-2] + var.shape[-1]\n+                c_shape = var.shape[:-2] + (var.shape[-1],)\n                 self._r.append(\n                     self.add_variable(\n                         shape=r_shape,\n@@ -177,7 +177,6 @@ class Adafactor(optimizer.Optimizer):\n                 v, beta_2_t * v + (1 - beta_2_t) * regulated_grad_square\n             )\n \n-        # `convert_to_tensor` unifies the handling of sparse and dense grads.\n         u_t = ops.divide(gradient, ops.sqrt(v))\n         u_t_hat = ops.divide(\n             u_t,\n\n@@ -20,7 +20,7 @@ class AdafactorTest(testing.TestCase):\n         )\n         self.run_class_serialization_test(optimizer)\n \n-    def test_single_step(self):\n+    def test_single_step_1d(self):\n         optimizer = Adafactor(learning_rate=0.5)\n         grads = np.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n@@ -29,6 +29,15 @@ class AdafactorTest(testing.TestCase):\n             vars, [-0.3693, 0.6307, 1.6307, 2.6307], rtol=1e-4, atol=1e-4\n         )\n \n+    def test_single_step_2d(self):\n+        optimizer = Adafactor(learning_rate=0.5)\n+        grads = np.array([[1.0, 6.0], [7.0, 2.0]])\n+        vars = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(\n+            vars, [[0.7007, -0.0081], [1.2492, 3.4407]], rtol=1e-4, atol=1e-4\n+        )\n+\n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n             np.zeros(()),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#08a46d85c6c2bfd1d5ad4153ef166a15bfbc44d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 24 | Files Changed: 4 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 32 | Churn Cumulative: 9246 | Contributors (this commit): 7 | Commits (past 90d): 89 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -807,6 +807,7 @@ def isclose(x1, x2):\n     return tfnp.isclose(x1, x2)\n \n \n+@sparse.densifying_unary(True)\n def isfinite(x):\n     return tfnp.isfinite(x)\n \n@@ -824,7 +825,7 @@ def isnan(x):\n def less(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # tfnp handles the casting internally during comparision, but it lacks\n+    # tfnp handles the casting internally during comparison, but it lacks\n     # support for bfloat16. Therefore we explicitly cast to the same dtype.\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n@@ -1021,7 +1022,6 @@ def minimum(x1, x2):\n     return tfnp.minimum(x1, x2)\n \n \n-@sparse.elementwise_division\n def mod(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n@@ -1640,7 +1640,6 @@ def eye(N, M=None, k=0, dtype=None):\n     return tfnp.eye(N, M=M, k=k, dtype=dtype)\n \n \n-@sparse.elementwise_division\n def floor_divide(x1, x2):\n     return tfnp.floor_divide(x1, x2)\n \n\n@@ -355,7 +355,7 @@ def elementwise_unary(func):\n       `tf.SparseTensor` or `tf.IndexedSlices`, the indices of the result must be\n       the same. Therefore:\n         - Reduction operations are not supported (e.g. `mean`).\n-        - Operation for which the result may be dense (e.g. `reciprocal`), or\n+        - Operations for which the result may be dense (e.g. `reciprocal`), or\n           the sparse indices depend on the inputs are not supported (e.g.\n           `clip`). This implies that `func(0)` must be 0.\n \n@@ -631,8 +631,8 @@ def elementwise_division(func):\n     \"\"\"Decorator to add support for `tf.SparseTensor` and `tf.IndexedSlices` to\n     element-wise binary division and related operators.\n \n-    This decorator is designed for operations related to the division of the\n-    two operands (e.g. `divide`, `mod`). It accepts `tf.SparseTensor` and\n+    This decorator is designed for operations related to the division of two\n+    operands (e.g. `divide`). It accepts `tf.SparseTensor` and\n     `tf.IndexedSlices` for both the dividend and the divisor, but handles them\n     differently based on whether they are the dividend or the divisor.\n \n\n@@ -3831,12 +3831,7 @@ class Mod(Operation):\n         )\n         if output_dtype == \"bool\":\n             output_dtype = \"int32\"\n-        x1_sparse = getattr(x1, \"sparse\", False)\n-        x2_sparse = getattr(x2, \"sparse\", False)\n-        output_sparse = x1_sparse and not x2_sparse\n-        return KerasTensor(\n-            output_shape, dtype=output_dtype, sparse=output_sparse\n-        )\n+        return KerasTensor(output_shape, dtype=output_dtype)\n \n \n @keras_export([\"keras.ops.mod\", \"keras.ops.numpy.mod\"])\n@@ -5972,10 +5967,7 @@ class FloorDivide(Operation):\n         x1_shape = getattr(x1, \"shape\", [])\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(x1_shape, x2_shape)\n-        x1_sparse = getattr(x1, \"sparse\", False)\n-        x2_sparse = getattr(x2, \"sparse\", False)\n-        output_sparse = x1_sparse and not x2_sparse\n-        return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n+        return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n @keras_export([\"keras.ops.floor_divide\", \"keras.ops.numpy.floor_divide\"])\n\n@@ -4082,6 +4082,7 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         \"cos\",\n         \"cosh\",\n         \"exp\",\n+        \"isfinite\",\n         \"log\",\n         \"log10\",\n         \"log2\",\n@@ -4138,10 +4139,8 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         (\"maximum\", union_sparseness),\n         (\"minimum\", union_sparseness),\n         (\"multiply\", intersection_sparseness),\n-        (\"mod\", division_sparseness),\n         (\"divide\", division_sparseness),\n         (\"true_divide\", division_sparseness),\n-        (\"floor_divide\", division_sparseness),\n     ]\n     BINARY_OPS_TESTS = [\n         {\n@@ -4270,9 +4269,6 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n     def test_binary_correctness_sparse_tensor(\n         self, x, y, op_function, op_class, np_op, op_sparseness, dtype\n     ):\n-        if dtype == \"int32\" and op_function.__name__ in (\"floor_divide\", \"mod\"):\n-            self.skipTest(f\"{op_function.__name__} does not support integers\")\n-\n         x = backend.cast(x, dtype)\n         y = backend.cast(y, dtype)\n         expected_result = np_op(\n@@ -4294,9 +4290,6 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n     def test_binary_correctness_indexed_slices(\n         self, x, y, op_function, op_class, np_op, op_sparseness, dtype\n     ):\n-        if dtype == \"int32\" and op_function.__name__ in (\"floor_divide\", \"mod\"):\n-            self.skipTest(f\"{op_function.__name__} does not support integers\")\n-\n         x = backend.cast(x, dtype)\n         y = backend.cast(y, dtype)\n         expected_result = np_op(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e70f28ff3f3b2060b83c23a1aabc85b4ccece991", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 25 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import multiprocessing\n+import multiprocessing.dummy\n import queue\n import random\n import threading\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#92e7171a46c89b899664ac6b05181ee6642aaff1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 171 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 11/7 | Churn Δ: 171 | Churn Cumulative: 2764 | Contributors (this commit): 9 | Commits (past 90d): 41 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -787,3 +787,55 @@ def batch_normalization(\n         scale=scale,\n         variance_epsilon=epsilon,\n     )\n+\n+\n+def ctc_loss(\n+    target,\n+    output,\n+    target_length,\n+    output_length,\n+    mask_index=0,\n+):\n+    \"\"\"Runs CTC (Connectionist Temporal Classification) loss on each\n+    batch element.\n+\n+    Arguments:\n+        target: Tensor `(batch_size, max_target_length)` containing the\n+            target sequences in integer format.\n+        output: Tensor `(batch_size, max_output_length, num_classes)`\n+            containing the output of the softmax.\n+        target_length: Tensor `(batch_size,)` containing the sequence length\n+            for each target sequence in the batch.\n+        output_length: Tensor `(batch_size,)` containing the sequence length\n+            for each output sequence in the batch.\n+        mask_index: The value in `target` and `output` that represents the\n+            blank label.\n+\n+    Returns:\n+        A tensor of shape `(batch_size,)` containing the CTC loss for each\n+        sample in the batch.\n+    \"\"\"\n+    target = tf.convert_to_tensor(target)\n+    target = tf.cast(target, dtype=\"int32\")\n+    output = tf.convert_to_tensor(output)\n+    output = tf.cast(output, dtype=\"float32\")\n+\n+    max_label_len = tf.shape(target)[1]\n+\n+    mask = tf.sequence_mask(target_length, max_label_len)\n+    indices = tf.where(mask)\n+    values = tf.boolean_mask(target, mask)\n+\n+    sparse_target = tf.SparseTensor(\n+        indices=indices,\n+        values=values,\n+        dense_shape=tf.cast(tf.shape(target), dtype=\"int64\"),\n+    )\n+\n+    return tf.nn.ctc_loss(\n+        labels=sparse_target,\n+        logits=output,\n+        label_length=target_length,\n+        logit_length=output_length,\n+        blank_index=mask_index,\n+    )\n\n@@ -745,3 +745,27 @@ def batch_normalization(\n     order.pop(1)\n     order.insert(axis, 1)\n     return x.permute(order)\n+\n+\n+def ctc_loss(\n+    target,\n+    output,\n+    target_length,\n+    output_length,\n+    mask_index=0,\n+):\n+    target = convert_to_tensor(target)\n+    output = convert_to_tensor(output)\n+    target_length = convert_to_tensor(target_length)\n+    output_length = convert_to_tensor(output_length)\n+\n+    logits = tnn.log_softmax(output, dim=-1)\n+\n+    return tnn.ctc_loss(\n+        logits,\n+        target,\n+        output_length,\n+        target_length,\n+        blank=mask_index,\n+        reduction=\"none\",\n+    )\n\n@@ -1777,3 +1777,67 @@ def batch_normalization(\n     return backend.nn.batch_normalization(\n         x, mean, variance, axis, offset, scale, epsilon\n     )\n+\n+\n+class CtcLoss(Operation):\n+    def __init__(self, mask_index):\n+        super().__init__()\n+        self.mask_index = mask_index\n+\n+    def call(self, target, output, target_length, output_length):\n+        return backend.nn.ctc_loss(\n+            target, output, target_length, output_length, self.mask_index\n+        )\n+\n+    def _check_shape_first_dim(self, name1, shape1, name2, shape2):\n+        if shape1[0] != shape2[0]:\n+            raise ValueError(\n+                f\"Arguments `{name1}` and `{name2}` must have the same \"\n+                \"first dimension. \"\n+                f\"Received shapes: `{shape1}` and `{shape2}`.\"\n+            )\n+\n+    def compute_output_spec(self, target, output, target_length, output_length):\n+        self._check_shape_first_dim(\n+            \"target\", target.shape, \"output\", output.shape\n+        )\n+        self._check_shape_first_dim(\n+            \"target_length\", target_length.shape, \"target\", target.shape\n+        )\n+        self._check_shape_first_dim(\n+            \"output_length\", output_length.shape, \"output\", output.shape\n+        )\n+\n+        return KerasTensor((target.shape[0],), dtype=target.dtype)\n+\n+\n+@keras_export(\n+    [\n+        \"keras.ops.ctc_loss\",\n+        \"keras.ops.nn.ctc_loss\",\n+    ]\n+)\n+def ctc_loss(target, output, target_length, output_length, mask_index=0):\n+    \"\"\"CTC (Connectionist Temporal Classification) loss.\n+\n+    Args:\n+        target: A tensor of shape `(batch_size, target_max_length)` containing\n+            the true labels in integer format.\n+        output: A tensor of shape `(batch_size, output_max_length, num_classes)`\n+            containing the output from the network.\n+        target_length: A tensor of shape `(batch_size,)` containing the\n+            true label lengths.\n+        output_length: A tensor of shape `(batch_size,)` containing the\n+            output lengths.\n+        mask_index: The index of the mask character in the vocabulary.\n+            Defaults to `0`.\n+    \"\"\"\n+\n+    if any_symbolic_tensors((target, output, target_length, output_length)):\n+        return CtcLoss(mask_index).symbolic_call(\n+            target, output, target_length, output_length\n+        )\n+\n+    return backend.nn.ctc_loss(\n+        target, output, target_length, output_length, mask_index\n+    )\n\n@@ -974,6 +974,17 @@ class NNOpsStaticShapeTest(testing.TestCase):\n             (10, 3, 4, 5),\n         )\n \n+    @pytest.mark.skipif(\n+        backend.backend() not in [\"tensorflow\", \"torch\"],\n+        reason=\"Only TF and Torch support CTC loss\",\n+    )\n+    def test_ctc_loss(self):\n+        x = KerasTensor([10, 3, 4])\n+        y = KerasTensor([10, 3], dtype=\"int32\")\n+        x_lengths = KerasTensor([10], dtype=\"int32\")\n+        y_lengths = KerasTensor([10], dtype=\"int32\")\n+        self.assertEqual(knn.ctc_loss(x, y, x_lengths, y_lengths).shape, (10,))\n+\n \n class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_relu(self):\n@@ -1750,6 +1761,26 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertEqual(tuple(output.shape), (2, 3, 3, 5))\n \n+    @pytest.mark.skipif(\n+        backend.backend() not in [\"tensorflow\", \"torch\"],\n+        reason=\"Only TF and Torch support CTC loss\",\n+    )\n+    def test_ctc_loss(self):\n+        labels = np.array([[1, 2, 1], [1, 2, 2]])\n+        outputs = np.array(\n+            [\n+                [[0.4, 0.8, 0.4], [0.4, 0.8, 0.4]],\n+                [[0.2, 0.8, 0.3], [0.2, 0.3, 0.3]],\n+                [[0.9, 0.4, 0.5], [0.4, 0.3, 0.2]],\n+            ]\n+        )\n+\n+        label_length = np.array([3, 2])\n+        output_length = np.array([3, 2])\n+\n+        result = knn.ctc_loss(labels, outputs, label_length, output_length)\n+        self.assertAllClose(result, np.array([3.4411672, 1.91680186]))\n+\n \n class TestLogitRecovery(testing.TestCase):\n     def test_logit_recovery_binary_crossentropy(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a14af8538c2ac715e02a947e398a0a2ebf76603a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 158 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 11 | Complexity Δ (Sum/Max): 17/11 | Churn Δ: 158 | Churn Cumulative: 2479 | Contributors (this commit): 24 | Commits (past 90d): 6 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -86,6 +86,7 @@ from keras.layers.preprocessing.text_vectorization import TextVectorization\n from keras.layers.regularization.activity_regularization import (\n     ActivityRegularization,\n )\n+from keras.layers.regularization.alpha_dropout import AlphaDropout\n from keras.layers.regularization.dropout import Dropout\n from keras.layers.regularization.gaussian_dropout import GaussianDropout\n from keras.layers.regularization.gaussian_noise import GaussianNoise\n\n@@ -0,0 +1,97 @@\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+\n+\n+@keras_export(\"keras.layers.AlphaDropout\")\n+class AlphaDropout(Layer):\n+    \"\"\"Applies Alpha Dropout to the input.\n+\n+    Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n+    to their original values, in order to ensure the self-normalizing property\n+    even after this dropout.\n+    Alpha Dropout fits well to Scaled Exponential Linear Units (SELU) by\n+    randomly setting activations to the negative saturation value.\n+\n+    Args:\n+        rate: Float between 0 and 1. The multiplicative noise will have\n+            standard deviation `sqrt(rate / (1 - rate))`.\n+        noise_shape: 1D integer tensor representing the shape of the\n+            binary alpha dropout mask that will be multiplied with the input.\n+            For instance, if your inputs have shape\n+            `(batch_size, timesteps, features)` and\n+            you want the alpha dropout mask to be the same for all timesteps,\n+            you can use `noise_shape=(batch_size, 1, features)`.\n+        seed: A Python integer to use as random seed.\n+\n+    Call arguments:\n+        inputs: Input tensor (of any rank).\n+        training: Python boolean indicating whether the layer should behave in\n+            training mode (adding alpha dropout) or in inference mode\n+            (doing nothing).\n+    \"\"\"\n+\n+    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n+        super().__init__(**kwargs)\n+        if not 0 <= rate <= 1:\n+            raise ValueError(\n+                f\"Invalid value received for argument \"\n+                \"`rate`. Expected a float value between 0 and 1. \"\n+                f\"Received: rate={rate}\"\n+            )\n+        self.rate = rate\n+        self.seed = seed\n+        self.noise_shape = noise_shape\n+        self.seed_generator = backend.random.SeedGenerator(seed)\n+        self.supports_masking = True\n+        self.built = True\n+\n+    def call(self, inputs, training=False):\n+        if training and self.rate > 0:\n+            noise_shape = self._get_concrete_noise_shape(\n+                inputs, self.noise_shape\n+            )\n+            alpha = 1.6732632423543772848170429916717\n+            scale = 1.0507009873554804934193349852946\n+            alpha_p = -alpha * scale\n+\n+            kept_idx = ops.greater_equal(\n+                ops.random.uniform(noise_shape, seed=self.seed_generator),\n+                self.rate,\n+            )\n+            kept_idx = ops.cast(kept_idx, inputs.dtype)\n+\n+            # Compute affine transformation parameters\n+            a = ((1 - self.rate) * (1 + self.rate * alpha_p**2)) ** -0.5\n+            b = -a * alpha_p * self.rate\n+\n+            # Apply mask\n+            x = inputs * kept_idx + alpha_p * (1 - kept_idx)\n+            return a * x + b\n+\n+        return inputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def _get_concrete_noise_shape(self, inputs, noise_shape):\n+        if noise_shape is None:\n+            return inputs.shape\n+\n+        concrete_inputs_shape = inputs.shape\n+        concrete_noise_shape = []\n+        for i, value in enumerate(noise_shape):\n+            concrete_noise_shape.append(\n+                concrete_inputs_shape[i] if value is None else value\n+            )\n+        return concrete_noise_shape\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"rate\": self.rate,\n+            \"seed\": self.seed,\n+            \"noise_shape\": self.noise_shape,\n+        }\n+        return {**base_config, **config}\n\n@@ -0,0 +1,60 @@\n+import numpy as np\n+import pytest\n+\n+from keras import backend\n+from keras import layers\n+from keras import testing\n+\n+\n+class AlphaDropoutTest(testing.TestCase):\n+    @pytest.mark.requires_trainable_backend\n+    def test_alpha_dropout_basics(self):\n+        self.run_layer_test(\n+            layers.AlphaDropout,\n+            init_kwargs={\n+                \"rate\": 0.2,\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=1,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    def test_alpha_dropout_correctness(self):\n+        inputs = np.ones((20, 500)).astype(\"float32\")\n+        layer = layers.AlphaDropout(0.3, seed=1337)\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(\n+            np.std(backend.convert_to_numpy(outputs)), 1.0, atol=1e-1\n+        )\n+\n+    def test_alpha_dropout_partial_noise_shape_dynamic(self):\n+        inputs = np.ones((20, 5, 10))\n+        layer = layers.AlphaDropout(0.5, noise_shape=(None, 1, None))\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(outputs[:, 0, :], outputs[:, 1, :])\n+\n+    def test_alpha_dropout_partial_noise_shape_static(self):\n+        inputs = np.ones((20, 5, 10))\n+        layer = layers.AlphaDropout(0.5, noise_shape=(20, 1, 10))\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(outputs[:, 0, :], outputs[:, 1, :])\n+\n+    def test_alpha_dropout_negative_rate(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value received for argument `rate`. \"\n+            \"Expected a float value between 0 and 1.\",\n+        ):\n+            _ = layers.AlphaDropout(rate=-0.5)\n+\n+    def test_alpha_dropout_rate_greater_than_one(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value received for argument `rate`. \"\n+            \"Expected a float value between 0 and 1.\",\n+        ):\n+            _ = layers.AlphaDropout(rate=1.5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d550552a0fff734b7b3ccdf3c12c02b5cd81605b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 13 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.0.1\"\n+__version__ = \"3.0.2\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#565953a256973527dd6c294641d1e8bd4bcaae07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 4173 | Contributors (this commit): 17 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -384,6 +384,8 @@ def NASNetMobile(\n             \"at this time due to an outstanding bug. \"\n             \"If interested, please open a PR.\"\n         )\n+    if (include_top == False) and (input_shape == None):\n+        input_shape = (224, 224, 3)\n     return NASNet(\n         input_shape,\n         penultimate_filters=1056,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e18d450d9f1510c4bb05c558e86d4104e12c7ac2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4175 | Contributors (this commit): 17 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -384,7 +384,7 @@ def NASNetMobile(\n             \"at this time due to an outstanding bug. \"\n             \"If interested, please open a PR.\"\n         )\n-    if (include_top == False) and (input_shape == None):\n+    if not include_top and input_shape is None:\n         input_shape = (224, 224, 3)\n     return NASNet(\n         input_shape,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2906fb10f9c47a90057209f9e5d98836090ad90d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 21 | Methods Changed: 12 | Complexity Δ (Sum/Max): 16/10 | Churn Δ: 115 | Churn Cumulative: 2389 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: 0.7605633802816901\n\nDIFF:\n@@ -171,16 +171,23 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n         else:\n             asset_store = None\n \n+        failed_trackables = set()\n+        error_msgs = {}\n         _load_state(\n             model,\n             weights_store=weights_store,\n             assets_store=asset_store,\n             inner_path=\"\",\n             visited_trackables=set(),\n+            failed_trackables=failed_trackables,\n+            error_msgs=error_msgs,\n         )\n         weights_store.close()\n         if asset_store:\n             asset_store.close()\n+\n+        if failed_trackables:\n+            _raise_loading_failure(error_msgs)\n     return model\n \n \n@@ -225,6 +232,8 @@ def load_weights_only(model, filepath, skip_mismatch=False):\n             _VARS_FNAME + \".h5\", archive=archive, mode=\"r\"\n         )\n \n+    failed_trackables = set()\n+    error_msgs = {}\n     _load_state(\n         model,\n         weights_store=weights_store,\n@@ -232,6 +241,8 @@ def load_weights_only(model, filepath, skip_mismatch=False):\n         inner_path=\"\",\n         skip_mismatch=skip_mismatch,\n         visited_trackables=set(),\n+        failed_trackables=failed_trackables,\n+        error_msgs=error_msgs,\n     )\n     weights_store.close()\n     if temp_dir and file_utils.exists(temp_dir):\n@@ -239,6 +250,26 @@ def load_weights_only(model, filepath, skip_mismatch=False):\n     if archive:\n         archive.close()\n \n+    if failed_trackables:\n+        _raise_loading_failure(error_msgs, warn_only=skip_mismatch)\n+\n+\n+def _raise_loading_failure(error_msgs, warn_only=False):\n+    first_key = list(error_msgs.keys())[0]\n+    ex_trackable, ex_error = error_msgs[first_key]\n+    msg = (\n+        f\"A total of {len(error_msgs)} objects could not \"\n+        \"be loaded. Example error message for \"\n+        f\"object {ex_trackable}:\\n\\n\"\n+        f\"{ex_error}\\n\\n\"\n+        \"List of objects that could not be loaded:\\n\"\n+        f\"{[x[0] for x in error_msgs.values()]}\"\n+    )\n+    if warn_only:\n+        warnings.warn(msg)\n+    else:\n+        raise ValueError(msg)\n+\n \n def _write_to_zip_recursively(zipfile_to_save, system_path, zip_path):\n     if not file_utils.isdir(system_path):\n@@ -256,6 +287,13 @@ def _write_to_zip_recursively(zipfile_to_save, system_path, zip_path):\n             )\n \n \n+def _name_key(name):\n+    \"\"\"Make sure that private attributes are visited last.\"\"\"\n+    if name.startswith(\"_\"):\n+        return \"~\" + name\n+    return name\n+\n+\n def _walk_trackable(trackable):\n     from keras.models import Functional\n     from keras.models import Sequential\n@@ -276,7 +314,7 @@ def _walk_trackable(trackable):\n         raise ValueError(f\"Invalid obj_type: {obj_type}\")\n     attr_skiplist = get_attr_skiplist(obj_type)\n \n-    for child_attr in sorted(dir(trackable)):\n+    for child_attr in sorted(dir(trackable), key=lambda x: _name_key(x)):\n         if child_attr.startswith(\"__\") or child_attr in attr_skiplist:\n             continue\n         try:\n@@ -336,40 +374,42 @@ def _load_state(\n     inner_path,\n     skip_mismatch=False,\n     visited_trackables=None,\n+    failed_trackables=None,\n+    error_msgs=None,\n ):\n     if visited_trackables and id(trackable) in visited_trackables:\n         return\n \n+    failure = False\n+\n     if hasattr(trackable, \"load_own_variables\") and weights_store:\n-        if skip_mismatch:\n+        if skip_mismatch or failed_trackables is not None:\n             try:\n                 trackable.load_own_variables(weights_store.get(inner_path))\n             except Exception as e:\n-                warnings.warn(\n-                    f\"Could not load weights in object {trackable}. \"\n-                    \"Skipping object. \"\n-                    f\"Exception encountered: {e}\",\n-                    stacklevel=2,\n-                )\n+                failed_trackables.add(id(trackable))\n+                error_msgs[id(trackable)] = trackable, e\n+                failure = True\n         else:\n             trackable.load_own_variables(weights_store.get(inner_path))\n \n     if hasattr(trackable, \"load_assets\") and assets_store:\n-        if skip_mismatch:\n+        if skip_mismatch or failed_trackables is not None:\n             try:\n                 trackable.load_assets(assets_store.get(inner_path))\n             except Exception as e:\n-                warnings.warn(\n-                    f\"Could not load assets in object {trackable}. \"\n-                    \"Skipping object. \"\n-                    f\"Exception encountered: {e}\",\n-                    stacklevel=2,\n-                )\n+                failed_trackables.add(id(trackable))\n+                error_msgs[id(trackable)] = trackable, e\n+                failure = True\n         else:\n             trackable.load_assets(assets_store.get(inner_path))\n \n+    if not failure:\n         if visited_trackables is not None:\n             visited_trackables.add(id(trackable))\n+        if id(trackable) in failed_trackables:\n+            failed_trackables.remove(id(trackable))\n+            error_msgs.pop(id(trackable))\n \n     # Recursively load states for Keras trackables such as layers/optimizers.\n     for child_attr, child_obj in _walk_trackable(trackable):\n@@ -383,6 +423,8 @@ def _load_state(\n                 ),\n                 skip_mismatch=skip_mismatch,\n                 visited_trackables=visited_trackables,\n+                failed_trackables=failed_trackables,\n+                error_msgs=error_msgs,\n             )\n         elif isinstance(child_obj, (list, dict, tuple, set)):\n             _load_container_state(\n@@ -394,6 +436,8 @@ def _load_state(\n                 ),\n                 skip_mismatch=skip_mismatch,\n                 visited_trackables=visited_trackables,\n+                failed_trackables=failed_trackables,\n+                error_msgs=error_msgs,\n             )\n \n \n@@ -431,6 +475,8 @@ def _load_container_state(\n     inner_path,\n     skip_mismatch,\n     visited_trackables,\n+    failed_trackables,\n+    error_msgs,\n ):\n     used_names = {}\n     if isinstance(container, dict):\n@@ -451,6 +497,8 @@ def _load_container_state(\n                 inner_path=file_utils.join(inner_path, name).replace(\"\\\\\", \"/\"),\n                 skip_mismatch=skip_mismatch,\n                 visited_trackables=visited_trackables,\n+                failed_trackables=failed_trackables,\n+                error_msgs=error_msgs,\n             )\n \n \n\n@@ -716,3 +716,40 @@ class SavingBattleTest(testing.TestCase):\n \n         with self.assertRaisesRegex(TypeError, \"are explicitly deserialized\"):\n             _ = saving_lib.load_model(temp_filepath)\n+\n+    def test_redefinition_of_trackable(self):\n+        \"\"\"Test that a trackable can be aliased under a new name.\"\"\"\n+\n+        class NormalModel(keras.Model):\n+            def __init__(self):\n+                super().__init__()\n+                self.dense = keras.layers.Dense(3)\n+\n+            def call(self, x):\n+                return self.dense(x)\n+\n+        class WeirdModel(keras.Model):\n+            def __init__(self):\n+                super().__init__()\n+                # This property will be traversed first,\n+                # but \"_dense\" isn't in the saved file\n+                # generated by NormalModel.\n+                self.a_dense = keras.layers.Dense(3)\n+\n+            @property\n+            def dense(self):\n+                return self.a_dense\n+\n+            def call(self, x):\n+                return self.dense(x)\n+\n+        temp_filepath = \"normal_model.weights.h5\"\n+        model_a = NormalModel()\n+        model_a(np.random.random((2, 2)))\n+        model_a.save_weights(temp_filepath)\n+        model_b = WeirdModel()\n+        model_b(np.random.random((2, 2)))\n+        model_b.load_weights(temp_filepath)\n+        self.assertAllClose(\n+            model_a.dense.kernel.numpy(), model_b.dense.kernel.numpy()\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6a6e4f8ee09711df135b5727aac93bfab008620a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 90 | Churn Cumulative: 2410 | Contributors (this commit): 7 | Commits (past 90d): 41 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -558,3 +558,78 @@ def batch_normalization(\n         res = res + offset\n \n     return x * inv + res\n+\n+\n+def ctc_loss(\n+    target,\n+    output,\n+    target_length,\n+    output_length,\n+    mask_index=0,\n+):\n+    batch_size, _, _ = output.shape\n+    batch_size, max_target_length = target.shape\n+\n+    output = output.transpose((1, 0, 2))\n+    target = target.transpose((1, 0))\n+\n+    logits = jnn.log_softmax(output)\n+    mgrid_t, mgrid_b = jnp.meshgrid(\n+        jnp.arange(max_target_length), jnp.arange(batch_size)\n+    )\n+    logprobs_emit = logits[mgrid_t, mgrid_b, target[:, :, None]]\n+    logprobs_mask = logits[:, :, mask_index]\n+\n+    logit_paddings = jnp.array(\n+        jnp.arange(max_target_length) < output_length[:, None],\n+        dtype=jnp.float32,\n+    )\n+\n+    repeat = jnp.array(target[1:] == target[:-1])\n+    repeat = jnp.pad(repeat, ((0, 1), (0, 0))).transpose((1, 0))\n+\n+    _logepsilon = -100000.0\n+\n+    def _iterate(prev, x):\n+        prev_mask, prev_emit = prev\n+        logprob_mask, logprob_emit, pad = x\n+\n+        prev_mask_orig = prev_mask\n+        prev_mask = prev_mask.at[:, 1:].set(\n+            jnp.logaddexp(prev_mask[:, 1:], prev_emit + _logepsilon * repeat),\n+        )\n+        emit = jnp.logaddexp(\n+            prev_mask[:, :-1] + logprob_emit, prev_emit + logprob_emit\n+        )\n+\n+        mask = prev_mask + logprob_mask[:, None]\n+        mask = mask.at[:, 1:].set(\n+            jnp.logaddexp(\n+                mask[:, 1:],\n+                prev_emit + logprob_mask[:, None] + _logepsilon * (1 - repeat),\n+            )\n+        )\n+\n+        pad = pad[:, None]\n+        emit = emit * pad + prev_emit * (1 - pad)\n+        mask = mask * pad + prev_mask_orig * (1 - pad)\n+\n+        return (mask, emit), (mask, emit)\n+\n+    mask_init = jnp.full((batch_size, max_target_length + 1), _logepsilon)\n+    mask_init = mask_init.at[:, 0].set(0.0)\n+    emit_init = jnp.full((batch_size, max_target_length), _logepsilon)\n+\n+    _, (alphas_mask, alphas_emit) = lax.scan(\n+        _iterate,\n+        (mask_init, emit_init),\n+        (logprobs_mask, logprobs_emit, logit_paddings.transpose()),\n+    )\n+\n+    last_alpha_mask = (\n+        alphas_mask[-1]\n+        .at[:, 1:]\n+        .set(jnp.logaddexp(alphas_mask[-1, :, 1:], alphas_emit[-1]))\n+    )\n+\n+    return -last_alpha_mask[jnp.arange(batch_size), target_length]\n\n@@ -838,4 +838,5 @@ def ctc_loss(\n         label_length=target_length,\n         logit_length=output_length,\n         blank_index=mask_index,\n+        logits_time_major=False,\n     )\n\n@@ -759,6 +759,7 @@ def ctc_loss(\n     target_length = convert_to_tensor(target_length)\n     output_length = convert_to_tensor(output_length)\n \n+    output = torch.transpose(output, 1, 0)\n     logits = tnn.log_softmax(output, dim=-1)\n \n     return tnn.ctc_loss(\n\n@@ -975,8 +975,8 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         )\n \n     @pytest.mark.skipif(\n-        backend.backend() not in [\"tensorflow\", \"torch\"],\n-        reason=\"Only TF and Torch support CTC loss\",\n+        backend.backend() == \"numpy\",\n+        reason=\"Numpy does not support CTC loss\",\n     )\n     def test_ctc_loss(self):\n         x = KerasTensor([10, 3, 4])\n@@ -1762,16 +1762,15 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(tuple(output.shape), (2, 3, 3, 5))\n \n     @pytest.mark.skipif(\n-        backend.backend() not in [\"tensorflow\", \"torch\"],\n-        reason=\"Only TF and Torch support CTC loss\",\n+        backend.backend() == \"numpy\",\n+        reason=\"Numpy does not support CTC loss\",\n     )\n     def test_ctc_loss(self):\n         labels = np.array([[1, 2, 1], [1, 2, 2]])\n         outputs = np.array(\n             [\n-                [[0.4, 0.8, 0.4], [0.4, 0.8, 0.4]],\n-                [[0.2, 0.8, 0.3], [0.2, 0.3, 0.3]],\n-                [[0.9, 0.4, 0.5], [0.4, 0.3, 0.2]],\n+                [[0.4, 0.8, 0.4], [0.2, 0.8, 0.3], [0.9, 0.4, 0.5]],\n+                [[0.4, 0.8, 0.4], [0.2, 0.3, 0.3], [0.4, 0.3, 0.2]],\n             ]\n         )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f6d5a364f6124f7efcb83ec4648adaa01e433b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 28 | Churn Cumulative: 229 | Contributors (this commit): 4 | Commits (past 90d): 15 | Contributors (cumulative): 7 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -503,7 +503,7 @@ class Layer(BackendLayer, Operation):\n                 name=name,\n             )\n         # Will be added to layer.losses\n-        variable.regularizer = regularizer\n+        variable.regularizer = regularizers.get(regularizer)\n         variable.constraint = constraints.get(constraint)\n         self._track_variable(variable)\n         return variable\n@@ -962,10 +962,13 @@ class Layer(BackendLayer, Operation):\n         mapping = list(trainable_mapping) + list(non_trainable_mapping)\n \n         # Call in stateless scope\n+        losses = None\n         with backend.StatelessScope(\n             state_mapping=mapping, collect_losses=return_losses\n         ) as scope:\n             outputs = self.call(*args, **kwargs)\n+            if return_losses:\n+                losses = self.losses\n \n         # Gather updated non-trainable variables\n         non_trainable_variables = []\n@@ -977,7 +980,7 @@ class Layer(BackendLayer, Operation):\n                 non_trainable_variables.append(v)\n \n         if return_losses:\n-            return outputs, non_trainable_variables, scope.losses[:]\n+            return outputs, non_trainable_variables, losses\n         return outputs, non_trainable_variables\n \n     def compute_output_spec(self, *args, **kwargs):\n@@ -1072,19 +1075,24 @@ class Layer(BackendLayer, Operation):\n         else:\n             return self._losses[:]\n \n+    def _get_regularization_losses(self):\n+        weight_regularization_losses = []\n+        for v in self.trainable_weights:\n+            regularizer = getattr(v, \"regularizer\", None)\n+            if regularizer is None:\n+                continue\n+            if backend.in_stateless_scope():\n+                v = backend.get_stateless_scope().get_current_value(v)\n+            weight_regularization_losses.append(regularizer(v))\n+        return weight_regularization_losses\n+\n     @property\n     def losses(self):\n         \"\"\"List of scalar losses from `add_loss`, regularizers and sublayers.\"\"\"\n         losses = self._get_own_losses()\n         for layer in self._flatten_layers(include_self=False):\n             losses.extend(layer._get_own_losses())\n-        weight_regularization_losses = []\n-        for v in self.trainable_weights:\n-            if backend.in_stateless_scope():\n-                v = backend.get_stateless_scope().get_current_value(v)\n-            regularizer = getattr(v, \"regularizer\", None)\n-            if regularizer:\n-                weight_regularization_losses.append(regularizer(v))\n+        weight_regularization_losses = self._get_regularization_losses()\n         losses.extend(weight_regularization_losses)\n         return losses\n \n\n@@ -581,6 +581,7 @@ class LayerTest(testing.TestCase):\n                     shape=(),\n                     initializer=\"zeros\",\n                     trainable=True,\n+                    regularizer=\"l1\",\n                 )\n                 self.built = True\n \n@@ -632,6 +633,7 @@ class LayerTest(testing.TestCase):\n             layer1.non_trainable_variables, non_trainable_variables\n         ):\n             self.assertAllClose(ref_v, v)\n+        self.assertLen(losses, 2)\n         for ref_loss, loss in zip(layer1.losses, losses):\n             self.assertAllClose(ref_loss, loss)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
