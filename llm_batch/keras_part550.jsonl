{"custom_id": "keras#69d93beb0b25b2b42ac2fa79c5eab3c8a6f38427", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 422 | Lines Deleted: 594 | Files Changed: 10 | Hunks: 12 | Methods Changed: 52 | Complexity Δ (Sum/Max): 19/36 | Churn Δ: 1016 | Churn Cumulative: 3845 | Contributors (this commit): 7 | Commits (past 90d): 100 | Contributors (cumulative): 34 | DMM Complexity: 0.0\n\nDIFF:\n@@ -44,7 +44,7 @@ def cast(x, dtype):\n \n \n def cond(pred, true_fn, false_fn):\n-    return jax.lax.cond(pred, true_fun=true_fn, false_fun=false_fn)\n+    return jax.lax.cond(pred, true_fn=true_fn, false_fun=false_fn)\n \n \n def name_scope(name):\n\n@@ -87,10 +87,7 @@ def reduce_values(values, reduction=\"sum_over_batch_size\"):\n         return values\n     loss = ops.sum(values)\n     if reduction == \"sum_over_batch_size\":\n-        loss /= ops.cast(\n-            ops.prod(ops.convert_to_tensor(ops.shape(values), dtype=\"int32\")),\n-            loss.dtype,\n-        )\n+        loss /= ops.cast(ops.shape(values)[0], loss.dtype)\n     return loss\n \n \n\n@@ -125,216 +125,6 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n         )\n \n \n-@keras_core_export(\"keras_core.losses.Hinge\")\n-class Hinge(LossFunctionWrapper):\n-    \"\"\"Computes the hinge loss between `y_true` & `y_pred`.\n-\n-    Formula:\n-\n-    ```python\n-    loss = maximum(1 - y_true * y_pred, 0)\n-    ```\n-\n-    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n-    provided we will convert them to -1 or 1.\n-\n-    Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or None.\n-        name: Optional name for the instance. Defaults to `\"hinge\"`\n-    \"\"\"\n-\n-    def __init__(self, reduction=\"sum_over_batch_size\", name=\"hinge\"):\n-        super().__init__(hinge, reduction=reduction, name=name)\n-\n-\n-@keras_core_export(\"keras_core.losses.SquaredHinge\")\n-class SquaredHinge(LossFunctionWrapper):\n-    \"\"\"Computes the squared hinge loss between `y_true` & `y_pred`.\n-\n-    Formula:\n-\n-    ```python\n-    loss = square(maximum(1 - y_true * y_pred, 0))\n-    ```\n-\n-    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n-    provided we will convert them to -1 or 1.\n-\n-    Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or None.\n-        name: Optional name for the instance. Defaults to `\"squared_hinge\"`\n-    \"\"\"\n-\n-    def __init__(self, reduction=\"sum_over_batch_size\", name=\"squared_hinge\"):\n-        super().__init__(squared_hinge, reduction=reduction, name=name)\n-\n-\n-@keras_core_export(\"keras_core.losses.CategoricalHinge\")\n-class CategoricalHinge(LossFunctionWrapper):\n-    \"\"\"Computes the categorical hinge loss between `y_true` & `y_pred`.\n-\n-    Formula:\n-\n-    ```python\n-    loss = maximum(neg - pos + 1, 0)\n-    ```\n-\n-    where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`\n-\n-    Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or None.\n-        name: Optional name for the instance. Defaults to\n-            `\"categorical_hinge\"`\n-    \"\"\"\n-\n-    def __init__(\n-        self, reduction=\"sum_over_batch_size\", name=\"categorical_hinge\"\n-    ):\n-        super().__init__(categorical_hinge, reduction=reduction, name=name)\n-\n-\n-def convert_binary_labels_to_hinge(y_true):\n-    \"\"\"Converts binary labels into -1/1 for hinge loss/metric calculation.\"\"\"\n-    are_zeros = ops.equal(y_true, 0)\n-    are_ones = ops.equal(y_true, 1)\n-    is_binary = ops.all((ops.logical_or(are_zeros, are_ones)))\n-\n-    def _convert_binary_labels():\n-        # Convert the binary labels to -1 or 1.\n-        return 2.0 * y_true - 1.0\n-\n-    def _return_labels_unconverted():\n-        # Returns the labels unchanged if they are non-binary\n-        return y_true\n-\n-    updated_y_true = ops.cond(\n-        is_binary, _convert_binary_labels, _return_labels_unconverted\n-    )\n-    return updated_y_true\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.metrics.hinge\",\n-        \"keras_core.losses.hinge\",\n-    ]\n-)\n-def hinge(y_true, y_pred):\n-    \"\"\"Computes the hinge loss between `y_true` & `y_pred`.\n-\n-    Formula:\n-\n-    ```python\n-    loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1)\n-    ```\n-\n-    Standalone usage:\n-\n-    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.hinge(y_true, y_pred)\n-\n-    Args:\n-        y_true: The ground truth values. `y_true` values are expected to be -1\n-            or 1. If binary (0 or 1) labels are provided they will be converted\n-            to -1 or 1 with shape = `[batch_size, d0, .. dN]`.\n-        y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n-\n-    Returns:\n-        Hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n-    \"\"\"\n-    y_pred = ops.convert_to_tensor(y_pred)\n-    y_true = ops.cast(y_true, dtype=y_pred.dtype)\n-    y_true = ops.convert_to_tensor(y_true)\n-    y_true = convert_binary_labels_to_hinge(y_true)\n-    return ops.mean(ops.maximum(1.0 - y_true * y_pred, 0.0), axis=-1)\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.metrics.squared_hinge\",\n-        \"keras_core.losses.squared_hinge\",\n-    ]\n-)\n-def squared_hinge(y_true, y_pred):\n-    \"\"\"Computes the squared hinge loss between `y_true` & `y_pred`.\n-\n-    Formula:\n-\n-    ```python\n-    loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1)\n-    ```\n-\n-    Standalone usage:\n-\n-    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.squared_hinge(y_true, y_pred)\n-\n-    Args:\n-        y_true: The ground truth values. `y_true` values are expected to be -1\n-            or 1. If binary (0 or 1) labels are provided we will convert them\n-            to -1 or 1 with shape = `[batch_size, d0, .. dN]`.\n-        y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n-\n-    Returns:\n-        Squared hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n-    \"\"\"\n-    y_pred = ops.convert_to_tensor(y_pred)\n-    y_true = ops.cast(y_true, y_pred.dtype)\n-    y_true = convert_binary_labels_to_hinge(y_true)\n-    return ops.mean(\n-        ops.square(ops.maximum(1.0 - y_true * y_pred, 0.0)), axis=-1\n-    )\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.metrics.categorical_hinge\",\n-        \"keras_core.losses.categorical_hinge\",\n-    ]\n-)\n-def categorical_hinge(y_true, y_pred):\n-    \"\"\"Computes the categorical hinge loss between `y_true` & `y_pred`.\n-\n-    Formula:\n-\n-    ```python\n-    loss = maximum(neg - pos + 1, 0)\n-    ```\n-\n-    where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`\n-\n-    Standalone usage:\n-\n-    >>> y_true = np.random.randint(0, 3, size=(2,))\n-    >>> y_true = np.eye(np.max(y_true) + 1)[y_true]\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.categorical_hinge(y_true, y_pred)\n-\n-    Args:\n-        y_true: The ground truth values. `y_true` values are expected to be\n-            either `{-1, +1}` or `{0, 1}` (i.e. a one-hot-encoded tensor) with\n-            shape = `[batch_size, d0, .. dN]`.\n-        y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n-\n-    Returns:\n-        Categorical hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n-    \"\"\"\n-    y_pred = ops.convert_to_tensor(y_pred)\n-    y_true = ops.cast(y_true, y_pred.dtype)\n-    pos = ops.sum(y_true * y_pred, axis=-1)\n-    neg = ops.max((1.0 - y_true) * y_pred, axis=-1)\n-    zero = ops.cast(0.0, y_pred.dtype)\n-    return ops.maximum(neg - pos + 1.0, zero)\n-\n-\n @keras_core_export(\n     [\n         \"keras_core.metrics.mean_squared_error\",\n\n@@ -220,165 +220,3 @@ class MeanSquaredLogarithmicErrorTest(testing.TestCase):\n         y_pred = np.array([[4, 8, 12], [8, 1, 3]], dtype=\"float32\")\n         loss = msle_obj(y_true, y_pred, sample_weight=0)\n         self.assertAlmostEqual(loss, 0.0, 3)\n-\n-\n-class HingeTest(testing.TestCase):\n-    def test_unweighted(self):\n-        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n-        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n-\n-        # Reduction = \"sum_over_batch_size\"\n-        hinge_obj = losses.Hinge(reduction=\"sum_over_batch_size\")\n-        loss = hinge_obj(y_true, y_pred)\n-        self.assertAlmostEqual(loss, 1.3, 3)\n-\n-        # Reduction = \"sum\"\n-        hinge_obj = losses.Hinge(reduction=\"sum\")\n-        loss = hinge_obj(y_true, y_pred)\n-        self.assertAlmostEqual(loss, 2.6, 3)\n-\n-        # Reduction = None\n-        hinge_obj = losses.Hinge(reduction=None)\n-        loss = hinge_obj(y_true, y_pred)\n-        self.assertAllClose(loss, [1.1, 1.5])\n-\n-        # Bad reduction\n-        with self.assertRaisesRegex(ValueError, \"Invalid value for argument\"):\n-            losses.Hinge(reduction=\"abc\")\n-\n-    def test_weighted(self):\n-        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n-        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n-        sample_weight = [1, 0]\n-\n-        # Reduction = \"sum_over_batch_size\"\n-        hinge_obj = losses.Hinge(reduction=\"sum_over_batch_size\")\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAlmostEqual(loss, 0.55, 3)\n-\n-        # Reduction = \"sum\"\n-        hinge_obj = losses.Hinge(reduction=\"sum\")\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAlmostEqual(loss, 1.1, 3)\n-\n-        # Reduction = None\n-        hinge_obj = losses.Hinge(reduction=None)\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAlmostEqual(loss, [1.1, 0.0])\n-\n-    def test_zero_weighted(self):\n-        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n-        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n-        sample_weight = 0.0\n-\n-        hinge_obj = losses.Hinge()\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss, 0.0)\n-\n-\n-class SquaredHingeTest(testing.TestCase):\n-    def test_unweighted(self):\n-        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n-        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n-\n-        # Reduction = \"sum_over_batch_size\"\n-        hinge_obj = losses.SquaredHinge(reduction=\"sum_over_batch_size\")\n-        loss = hinge_obj(y_true, y_pred)\n-        self.assertAlmostEqual(loss, 1.86, 3)\n-\n-        # Reduction = \"sum\"\n-        hinge_obj = losses.SquaredHinge(reduction=\"sum\")\n-        loss = hinge_obj(y_true, y_pred)\n-        self.assertAlmostEqual(loss, 3.72, 3)\n-\n-        # Reduction = None\n-        hinge_obj = losses.SquaredHinge(reduction=None)\n-        loss = hinge_obj(y_true, y_pred)\n-        self.assertAllClose(loss, [1.46, 2.26])\n-\n-        # Bad reduction\n-        with self.assertRaisesRegex(ValueError, \"Invalid value for argument\"):\n-            losses.SquaredHinge(reduction=\"abc\")\n-\n-    def test_weighted(self):\n-        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n-        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n-        sample_weight = [1, 0]\n-\n-        # Reduction = \"sum_over_batch_size\"\n-        hinge_obj = losses.SquaredHinge(reduction=\"sum_over_batch_size\")\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAlmostEqual(loss, 0.73, 3)\n-\n-        # Reduction = \"sum\"\n-        hinge_obj = losses.SquaredHinge(reduction=\"sum\")\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAlmostEqual(loss, 1.46, 3)\n-\n-        # Reduction = None\n-        hinge_obj = losses.SquaredHinge(reduction=None)\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAlmostEqual(loss, [1.46, 0.0])\n-\n-    def test_zero_weighted(self):\n-        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n-        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n-        sample_weight = 0.0\n-\n-        hinge_obj = losses.SquaredHinge()\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss, 0.0)\n-\n-\n-class CategoricalHingeTest(testing.TestCase):\n-    def test_unweighted(self):\n-        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n-        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n-\n-        # Reduction = \"sum_over_batch_size\"\n-        hinge_obj = losses.CategoricalHinge(reduction=\"sum_over_batch_size\")\n-        loss = hinge_obj(y_true, y_pred)\n-        self.assertAlmostEqual(loss, 1.4, 3)\n-\n-        # Reduction = \"sum\"\n-        hinge_obj = losses.CategoricalHinge(reduction=\"sum\")\n-        loss = hinge_obj(y_true, y_pred)\n-        self.assertAlmostEqual(loss, 2.8, 3)\n-\n-        # Reduction = None\n-        hinge_obj = losses.CategoricalHinge(reduction=None)\n-        loss = hinge_obj(y_true, y_pred)\n-        self.assertAllClose(loss, [1.2, 1.6])\n-\n-        # Bad reduction\n-        with self.assertRaisesRegex(ValueError, \"Invalid value for argument\"):\n-            losses.CategoricalHinge(reduction=\"abc\")\n-\n-    def test_weighted(self):\n-        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n-        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n-        sample_weight = [1, 0]\n-\n-        # Reduction = \"sum_over_batch_size\"\n-        hinge_obj = losses.CategoricalHinge(reduction=\"sum_over_batch_size\")\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAlmostEqual(loss, 0.6, 3)\n-\n-        # Reduction = \"sum\"\n-        hinge_obj = losses.CategoricalHinge(reduction=\"sum\")\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAlmostEqual(loss, 1.2, 3)\n-\n-        # Reduction = None\n-        hinge_obj = losses.CategoricalHinge(reduction=None)\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAlmostEqual(loss, [1.2, 0.0])\n-\n-    def test_zero_weighted(self):\n-        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n-        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n-        sample_weight = 0.0\n-\n-        hinge_obj = losses.CategoricalHinge()\n-        loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss, 0.0)\n\n@@ -1,97 +0,0 @@\n-from keras_core import operations as ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.losses import losses\n-from keras_core.losses.losses import categorical_hinge\n-from keras_core.losses.losses import hinge\n-from keras_core.losses.losses import squared_hinge\n-from keras_core.metrics import reduction_metrics\n-from keras_core.metrics.metric import Metric\n-\n-\n-@keras_core_export(\"keras_core.metrics.Hinge\")\n-class Hinge(reduction_metrics.MeanMetricWrapper):\n-    \"\"\"Computes the hinge metric between `y_true` and `y_pred`.\n-\n-    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n-    provided we will convert them to -1 or 1.\n-\n-    Args:\n-        name: (Optional) string name of the metric instance.\n-        dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = keras_core.metrics.Hinge()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n-    >>> m.result()\n-    1.3\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result()\n-    1.1\n-    \"\"\"\n-\n-    def __init__(self, name=\"hinge\", dtype=None):\n-        super().__init__(fn=hinge, name=name, dtype=dtype)\n-\n-    def get_config(self):\n-        return {\"name\": self.name, \"dtype\": self.dtype}\n-\n-\n-@keras_core_export(\"keras_core.metrics.SquaredHinge\")\n-class SquaredHinge(reduction_metrics.MeanMetricWrapper):\n-    \"\"\"Computes the hinge metric between `y_true` and `y_pred`.\n-\n-    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n-    provided we will convert them to -1 or 1.\n-\n-    Args:\n-        name: (Optional) string name of the metric instance.\n-        dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = keras_core.metrics.SquaredHinge()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n-    >>> m.result()\n-    1.86\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result()\n-    1.46\n-    \"\"\"\n-\n-    def __init__(self, name=\"squared_hinge\", dtype=None):\n-        super().__init__(fn=squared_hinge, name=name, dtype=dtype)\n-\n-    def get_config(self):\n-        return {\"name\": self.name, \"dtype\": self.dtype}\n-\n-\n-@keras_core_export(\"keras_core.metrics.CategoricalHinge\")\n-class CategoricalHinge(reduction_metrics.MeanMetricWrapper):\n-    \"\"\"Computes the categorical hinge metric between `y_true` and `y_pred`.\n-\n-    Args:\n-        name: (Optional) string name of the metric instance.\n-        dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-    >>> m = keras_core.metrics.CategoricalHinge()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n-    >>> m.result().numpy()\n-    1.4000001\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result()\n-    1.2\n-    \"\"\"\n-\n-    def __init__(self, name=\"categorical_hinge\", dtype=None):\n-        super().__init__(fn=categorical_hinge, name=name, dtype=dtype)\n-\n-    def get_config(self):\n-        return {\"name\": self.name, \"dtype\": self.dtype}\n\n@@ -1,113 +0,0 @@\n-import numpy as np\n-\n-from keras_core import operations as ops\n-from keras_core import testing\n-from keras_core.metrics import hinge_metrics\n-\n-\n-class HingeTest(testing.TestCase):\n-    def test_config(self):\n-        hinge_obj = hinge_metrics.Hinge(name=\"hinge\", dtype=\"int32\")\n-        self.assertEqual(hinge_obj.name, \"hinge\")\n-        self.assertEqual(hinge_obj._dtype, \"int32\")\n-        # TODO: Check save and restore config\n-\n-    def test_unweighted(self):\n-        hinge_obj = hinge_metrics.Hinge()\n-        y_true = np.array([[0.0, 1.0, 0.0, 1.0], [0.0, 0.0, 1.0, 1.0]])\n-        y_pred = np.array([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n-        hinge_obj.update_state(y_true, y_pred)\n-        result = hinge_obj.result()\n-        self.assertAllClose(0.506, result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        hinge_obj = hinge_metrics.Hinge()\n-        y_true = np.array([[-1, 1, -1, 1], [-1, -1, 1, 1]])\n-        y_pred = np.array([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n-        sample_weight = np.array([1.5, 2.0])\n-        result = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(0.493, result, atol=1e-3)\n-\n-\n-class SquaredHingeTest(testing.TestCase):\n-    def test_config(self):\n-        sq_hinge_obj = hinge_metrics.SquaredHinge(\n-            name=\"squared_hinge\", dtype=\"int32\"\n-        )\n-        self.assertEqual(sq_hinge_obj.name, \"squared_hinge\")\n-        self.assertEqual(sq_hinge_obj._dtype, \"int32\")\n-        # TODO: Check save and restore config\n-\n-    def test_unweighted(self):\n-        sq_hinge_obj = hinge_metrics.SquaredHinge()\n-        y_true = np.array([[0, 1, 0, 1], [0, 0, 1, 1]], dtype=\"float32\")\n-        y_pred = np.array([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n-        sq_hinge_obj.update_state(y_true, y_pred)\n-        result = sq_hinge_obj.result()\n-        self.assertAllClose(0.364, result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        sq_hinge_obj = hinge_metrics.SquaredHinge()\n-        y_true = np.array([[-1, 1, -1, 1], [-1, -1, 1, 1]], dtype=\"float32\")\n-        y_pred = np.array([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n-        sample_weight = np.array([1.5, 2.0])\n-        result = sq_hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(0.347, result, atol=1e-3)\n-\n-\n-class CategoricalHingeTest(testing.TestCase):\n-    def test_config(self):\n-        cat_hinge_obj = hinge_metrics.CategoricalHinge(\n-            name=\"cat_hinge\", dtype=\"int32\"\n-        )\n-        self.assertEqual(cat_hinge_obj.name, \"cat_hinge\")\n-        self.assertEqual(cat_hinge_obj._dtype, \"int32\")\n-        # TODO: Check save and restore config\n-\n-    def test_unweighted(self):\n-        cat_hinge_obj = hinge_metrics.CategoricalHinge()\n-        y_true = np.array(\n-            (\n-                (0, 1, 0, 1, 0),\n-                (0, 0, 1, 1, 1),\n-                (1, 1, 1, 1, 0),\n-                (0, 0, 0, 0, 1),\n-            ),\n-            dtype=\"float32\",\n-        )\n-        y_pred = np.array(\n-            (\n-                (0, 0, 1, 1, 0),\n-                (1, 1, 1, 1, 1),\n-                (0, 1, 0, 1, 0),\n-                (1, 1, 1, 1, 1),\n-            ),\n-            dtype=\"float32\",\n-        )\n-        cat_hinge_obj.update_state(y_true, y_pred)\n-        result = cat_hinge_obj.result()\n-        self.assertAllClose(0.5, result, atol=1e-5)\n-\n-    def test_weighted(self):\n-        cat_hinge_obj = hinge_metrics.CategoricalHinge()\n-        y_true = np.array(\n-            (\n-                (0, 1, 0, 1, 0),\n-                (0, 0, 1, 1, 1),\n-                (1, 1, 1, 1, 0),\n-                (0, 0, 0, 0, 1),\n-            ),\n-            dtype=\"float32\",\n-        )\n-        y_pred = np.array(\n-            (\n-                (0, 0, 1, 1, 0),\n-                (1, 1, 1, 1, 1),\n-                (0, 1, 0, 1, 0),\n-                (1, 1, 1, 1, 1),\n-            ),\n-            dtype=\"float32\",\n-        )\n-        sample_weight = np.array((1.0, 1.5, 2.0, 2.5))\n-        result = cat_hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(0.5, result, atol=1e-5)\n\n@@ -106,7 +106,7 @@ class Mean(Metric):\n             shape=(), initializer=initializers.Zeros(), dtype=self.dtype\n         )\n         self.count = self.add_variable(\n-            shape=(), initializer=initializers.Zeros(), dtype=self.dtype\n+            shape=(), initializer=initializers.Zeros(), dtype=\"int32\"\n         )\n \n     def update_state(self, values, sample_weight=None):\n@@ -122,7 +122,7 @@ class Mean(Metric):\n             num_samples = ops.sum(\n                 ops.ones(shape=(num_samples,)) * sample_weight\n             )\n-        self.count.assign(self.count + ops.cast(num_samples, dtype=self.dtype))\n+        self.count.assign(self.count + ops.cast(num_samples, dtype=\"int32\"))\n \n     def reset_state(self):\n         self.total.assign(0.0)\n\n@@ -226,8 +226,83 @@ def operation_fn(operation, training):\n \n \n def functional_like_constructor(cls):\n-    raise NotImplementedError\n+    init_args = inspect.getfullargspec(cls.__init__).args[1:]\n+    functional_init_args = inspect.getfullargspec(Functional.__init__).args[\n+        1:\n+    ]\n+    if init_args == functional_init_args:\n+        return True\n+    return False\n \n \n-def get_functional_config(model):\n-    raise NotImplementedError\n+def get_functional_config(network):\n+    config = config or {}\n+    \n+    config[\"name\"] = network.name\n+    node_conversion_map = {}\n+    for layer in network.layers:\n+        kept_nodes = 1 if _should_skip_first_node(layer) else 0\n+        for original_node_index, node in enumerate(layer._inbound_nodes):\n+            node_key = _make_node_key(layer.name, original_node_index)\n+            if node_key in network._network_nodes:\n+                node_conversion_map[node_key] = kept_nodes\n+                kept_nodes += 1\n+    layer_configs = []\n+\n+    with serialization_lib.SharedObjectSavingScope():\n+        for layer in network.layers:  # From the earliest layers on.\n+            filtered_inbound_nodes = []\n+            for original_node_index, node in enumerate(layer._inbound_nodes):\n+                node_key = _make_node_key(layer.name, original_node_index)\n+                if node_key in network._network_nodes and not node.is_input:\n+                    # The node is relevant to the model:\n+                    # add to filtered_inbound_nodes.\n+                    node_data = node.serialize(\n+                        _make_node_key, node_conversion_map\n+                    )\n+                    filtered_inbound_nodes.append(node_data)\n+            layer_config = serialization_lib.serialize_keras_object(layer)\n+            layer_config[\"name\"] = layer.name\n+            layer_config[\"inbound_nodes\"] = filtered_inbound_nodes\n+            layer_configs.append(layer_config)\n+        config[\"layers\"] = layer_configs\n+\n+    # Gather info about inputs and outputs.\n+    model_inputs = []\n+    for i in range(len(network._input_layers)):\n+        layer, node_index, tensor_index = network._input_coordinates[i]\n+        node_key = _make_node_key(layer.name, node_index)\n+        if node_key not in network._network_nodes:\n+            continue\n+        new_node_index = node_conversion_map[node_key]\n+        model_inputs.append(\n+            [layer.name, new_node_index, tensor_index]\n+        )\n+    model_inputs = nest.pack_sequence_as(\n+        network._nested_inputs, model_inputs\n+    )\n+    # Preserve external Keras compat for Models with single input.\n+    if not tf.nest.is_nested(model_inputs):\n+        model_inputs = [model_inputs]\n+    model_inputs = tf_utils.convert_inner_node_data(model_inputs)\n+    config[\"input_layers\"] = model_inputs\n+\n+    model_outputs = []\n+    for i in range(len(network._output_layers)):\n+        layer, node_index, tensor_index = network._output_coordinates[i]\n+        node_key = _make_node_key(layer.name, node_index)\n+        if node_key not in network._network_nodes:\n+            continue\n+        new_node_index = node_conversion_map[node_key]\n+        model_outputs.append(\n+            tf_utils.ListWrapper([layer.name, new_node_index, tensor_index])\n+        )\n+    model_outputs = tf.nest.pack_sequence_as(\n+        network._nested_outputs, model_outputs\n+    )\n+    # Preserve external Keras compat for Models with single output.\n+    if not tf.nest.is_nested(model_outputs):\n+        model_outputs = [model_outputs]\n+    model_outputs = tf_utils.convert_inner_node_data(model_outputs)\n+    config[\"output_layers\"] = model_outputs\n+    return config\n\n@@ -0,0 +1,339 @@\n+\"\"\"Tests for serialization_lib.\"\"\"\n+\n+import json\n+\n+import numpy as np\n+\n+import keras_core\n+from keras_core import operations as ops\n+from keras_core import testing\n+from keras_core.saving import serialization_lib\n+\n+\n+def custom_fn(x):\n+    return x**2\n+\n+\n+class CustomLayer(keras_core.layers.Layer):\n+    def __init__(self, factor):\n+        super().__init__()\n+        self.factor = factor\n+\n+    def call(self, x):\n+        return x * self.factor\n+\n+    def get_config(self):\n+        return {\"factor\": self.factor}\n+\n+\n+class NestedCustomLayer(keras_core.layers.Layer):\n+    def __init__(self, factor, dense=None, activation=None):\n+        super().__init__()\n+        self.factor = factor\n+\n+        if dense is None:\n+            self.dense = keras_core.layers.Dense(1, activation=custom_fn)\n+        else:\n+            self.dense = serialization_lib.deserialize_keras_object(dense)\n+        self.activation = serialization_lib.deserialize_keras_object(activation)\n+\n+    def call(self, x):\n+        return self.dense(x * self.factor)\n+\n+    def get_config(self):\n+        return {\n+            \"factor\": self.factor,\n+            \"dense\": self.dense,\n+            \"activation\": self.activation,\n+        }\n+\n+\n+class WrapperLayer(keras_core.layers.Layer):\n+    def __init__(self, layer, **kwargs):\n+        super().__init__(**kwargs)\n+        self.layer = layer\n+\n+    def call(self, x):\n+        return self.layer(x)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        return {\"layer\": self.layer, **config}\n+\n+\n+class SerializationLibTest(testing.TestCase):\n+    def roundtrip(self, obj, custom_objects=None, safe_mode=True):\n+        serialized = serialization_lib.serialize_keras_object(obj)\n+        json_data = json.dumps(serialized)\n+        json_data = json.loads(json_data)\n+        deserialized = serialization_lib.deserialize_keras_object(\n+            json_data, custom_objects=custom_objects, safe_mode=safe_mode\n+        )\n+        reserialized = serialization_lib.serialize_keras_object(deserialized)\n+        return serialized, deserialized, reserialized\n+\n+    def test_simple_objects(self):\n+        for obj in [\n+            \"hello\",\n+            b\"hello\",\n+            np.array([0, 1]),\n+            np.array([0.0, 1.0]),\n+            np.float32(1.0),\n+            [\"hello\", 0, \"world\", 1.0, True],\n+            {\"1\": \"hello\", \"2\": 0, \"3\": True},\n+            {\"1\": \"hello\", \"2\": [True, False]},\n+        ]:\n+            serialized, _, reserialized = self.roundtrip(obj)\n+            self.assertEqual(serialized, reserialized)\n+\n+    def test_builtin_layers(self):\n+        serialized, _, reserialized = self.roundtrip(keras_core.layers.Dense(3))\n+        self.assertEqual(serialized, reserialized)\n+\n+    def test_tensors_and_shapes(self):\n+        x = ops.random.normal((2, 2), dtype=\"float64\")\n+        obj = {\"x\": x}\n+        _, new_obj, _ = self.roundtrip(obj)\n+        self.assertAllClose(x, new_obj[\"x\"], atol=1e-5)\n+\n+        obj = {\"x.shape\": x.shape}\n+        _, new_obj, _ = self.roundtrip(obj)\n+        self.assertEqual(tuple(x.shape), tuple(new_obj[\"x.shape\"]))\n+\n+    def test_custom_fn(self):\n+        obj = {\"activation\": custom_fn}\n+        serialized, _, reserialized = self.roundtrip(\n+            obj, custom_objects={\"custom_fn\": custom_fn}\n+        )\n+        self.assertEqual(serialized, reserialized)\n+\n+        # Test inside layer\n+        dense = keras_core.layers.Dense(1, activation=custom_fn)\n+        dense.build((None, 2))\n+        _, new_dense, _ = self.roundtrip(\n+            dense, custom_objects={\"custom_fn\": custom_fn}\n+        )\n+        x = ops.random.normal((2, 2))\n+        y1 = dense(x)\n+        _ = new_dense(x)\n+        new_dense.set_weights(dense.get_weights())\n+        y2 = new_dense(x)\n+        self.assertAllClose(y1, y2, atol=1e-5)\n+\n+    def test_custom_layer(self):\n+        layer = CustomLayer(factor=2)\n+        x = ops.random.normal((2, 2))\n+        y1 = layer(x)\n+        _, new_layer, _ = self.roundtrip(\n+            layer, custom_objects={\"CustomLayer\": CustomLayer}\n+        )\n+        y2 = new_layer(x)\n+        self.assertAllClose(y1, y2, atol=1e-5)\n+\n+        layer = NestedCustomLayer(factor=2)\n+        x = ops.random.normal((2, 2))\n+        y1 = layer(x)\n+        _, new_layer, _ = self.roundtrip(\n+            layer,\n+            custom_objects={\n+                \"NestedCustomLayer\": NestedCustomLayer,\n+                \"custom_fn\": custom_fn,\n+            },\n+        )\n+        _ = new_layer(x)\n+        new_layer.set_weights(layer.get_weights())\n+        y2 = new_layer(x)\n+        self.assertAllClose(y1, y2, atol=1e-5)\n+\n+    def test_lambda_fn(self):\n+        obj = {\"activation\": lambda x: x**2}\n+        with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n+            self.roundtrip(obj, safe_mode=True)\n+\n+        _, new_obj, _ = self.roundtrip(obj, safe_mode=False)\n+        self.assertEqual(obj[\"activation\"](3), new_obj[\"activation\"](3))\n+\n+    # TODO\n+    # def test_lambda_layer(self):\n+    #     lmbda = keras_core.layers.Lambda(lambda x: x**2)\n+    #     with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n+    #         self.roundtrip(lmbda, safe_mode=True)\n+\n+    #     _, new_lmbda, _ = self.roundtrip(lmbda, safe_mode=False)\n+    #     x = ops.random.normal((2, 2))\n+    #     y1 = lmbda(x)\n+    #     y2 = new_lmbda(x)\n+    #     self.assertAllClose(y1, y2, atol=1e-5)\n+\n+    # def test_safe_mode_scope(self):\n+    #     lmbda = keras_core.layers.Lambda(lambda x: x**2)\n+    #     with serialization_lib.SafeModeScope(safe_mode=True):\n+    #         with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n+    #             self.roundtrip(lmbda)\n+    #     with serialization_lib.SafeModeScope(safe_mode=False):\n+    #         _, new_lmbda, _ = self.roundtrip(lmbda)\n+    #     x = ops.random.normal((2, 2))\n+    #     y1 = lmbda(x)\n+    #     y2 = new_lmbda(x)\n+    #     self.assertAllClose(y1, y2, atol=1e-5)\n+\n+    def shared_inner_layer(self):\n+        input_1 = keras_core.Input((2,))\n+        input_2 = keras_core.Input((2,))\n+        shared_layer = keras_core.layers.Dense(1)\n+        output_1 = shared_layer(input_1)\n+        wrapper_layer = WrapperLayer(shared_layer)\n+        output_2 = wrapper_layer(input_2)\n+        model = keras_core.Model([input_1, input_2], [output_1, output_2])\n+        _, new_model, _ = self.roundtrip(\n+            model, custom_objects={\"WrapperLayer\": WrapperLayer}\n+        )\n+\n+        self.assertIs(model.layers[2], model.layers[3].layer)\n+        self.assertIs(new_model.layers[2], new_model.layers[3].layer)\n+\n+    # TODO\n+    # def test_functional_subclass(self):\n+    #     class PlainFunctionalSubclass(keras_core.Model):\n+    #         pass\n+\n+    #     inputs = keras_core.Input((2,), batch_size=3)\n+    #     outputs = keras_core.layers.Dense(1)(inputs)\n+    #     model = PlainFunctionalSubclass(inputs, outputs)\n+    #     x = ops.random.normal((2, 2))\n+    #     y1 = model(x)\n+    #     _, new_model, _ = self.roundtrip(\n+    #         model,\n+    #         custom_objects={\"PlainFunctionalSubclass\": PlainFunctionalSubclass},\n+    #     )\n+    #     new_model.set_weights(model.get_weights())\n+    #     y2 = new_model(x)\n+    #     self.assertAllClose(y1, y2, atol=1e-5)\n+    #     self.assertIsInstance(new_model, PlainFunctionalSubclass)\n+\n+    #     class FunctionalSubclassWCustomInit(keras_core.Model):\n+    #         def __init__(self, num_units=1, **kwargs):\n+    #             inputs = keras_core.Input((2,), batch_size=3)\n+    #             outputs = keras_core.layers.Dense(num_units)(inputs)\n+    #             super().__init__(inputs, outputs)\n+\n+    #     model = FunctionalSubclassWCustomInit(num_units=2)\n+    #     x = ops.random.normal((2, 2))\n+    #     y1 = model(x)\n+    #     _, new_model, _ = self.roundtrip(\n+    #         model,\n+    #         custom_objects={\n+    #             \"FunctionalSubclassWCustomInit\": FunctionalSubclassWCustomInit\n+    #         },\n+    #     )\n+    #     new_model.set_weights(model.get_weights())\n+    #     y2 = new_model(x)\n+    #     self.assertAllClose(y1, y2, atol=1e-5)\n+    #     self.assertIsInstance(new_model, FunctionalSubclassWCustomInit)\n+\n+    def test_shared_object(self):\n+        class MyLayer(keras_core.layers.Layer):\n+            def __init__(self, activation, **kwargs):\n+                super().__init__(**kwargs)\n+                if isinstance(activation, dict):\n+                    self.activation = (\n+                        serialization_lib.deserialize_keras_object(activation)\n+                    )\n+                else:\n+                    self.activation = activation\n+\n+            def call(self, x):\n+                return self.activation(x)\n+\n+            def get_config(self):\n+                config = super().get_config()\n+                config[\"activation\"] = self.activation\n+                return config\n+\n+        class SharedActivation:\n+            def __call__(self, x):\n+                return x**2\n+\n+            def get_config(self):\n+                return {}\n+\n+            @classmethod\n+            def from_config(cls, config):\n+                return cls()\n+\n+        shared_act = SharedActivation()\n+        layer_1 = MyLayer(activation=shared_act)\n+        layer_2 = MyLayer(activation=shared_act)\n+        layers = [layer_1, layer_2]\n+\n+        with serialization_lib.ObjectSharingScope():\n+            serialized, new_layers, reserialized = self.roundtrip(\n+                layers,\n+                custom_objects={\n+                    \"MyLayer\": MyLayer,\n+                    \"SharedActivation\": SharedActivation,\n+                },\n+            )\n+        self.assertIn(\"shared_object_id\", serialized[0][\"config\"][\"activation\"])\n+        obj_id = serialized[0][\"config\"][\"activation\"]\n+        self.assertIn(\"shared_object_id\", serialized[1][\"config\"][\"activation\"])\n+        self.assertEqual(obj_id, serialized[1][\"config\"][\"activation\"])\n+        self.assertIs(layers[0].activation, layers[1].activation)\n+        self.assertIs(new_layers[0].activation, new_layers[1].activation)\n+\n+\n+@keras_core.saving.register_keras_serializable()\n+class MyDense(keras_core.layers.Layer):\n+    def __init__(\n+        self,\n+        units,\n+        *,\n+        kernel_regularizer=None,\n+        kernel_initializer=None,\n+        **kwargs\n+    ):\n+        super().__init__(**kwargs)\n+        self._units = units\n+        self._kernel_regularizer = kernel_regularizer\n+        self._kernel_initializer = kernel_initializer\n+\n+    def get_config(self):\n+        return dict(\n+            units=self._units,\n+            kernel_initializer=self._kernel_initializer,\n+            kernel_regularizer=self._kernel_regularizer,\n+            **super().get_config()\n+        )\n+\n+    def build(self, input_shape):\n+        _, input_units = input_shape\n+        self._kernel = self.add_weight(\n+            name=\"kernel\",\n+            shape=[input_units, self._units],\n+            dtype=\"float32\",\n+            regularizer=self._kernel_regularizer,\n+            initializer=self._kernel_initializer,\n+        )\n+\n+    def call(self, inputs):\n+        return ops.matmul(inputs, self._kernel)\n+\n+\n+@keras_core.saving.register_keras_serializable()\n+class MyWrapper(keras_core.layers.Layer):\n+    def __init__(self, wrapped, **kwargs):\n+        super().__init__(**kwargs)\n+        self._wrapped = wrapped\n+\n+    def get_config(self):\n+        return dict(wrapped=self._wrapped, **super().get_config())\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        config[\"wrapped\"] = keras_core.saving.deserialize_keras_object(\n+            config[\"wrapped\"]\n+        )\n+        return cls(**config)\n+\n+    def call(self, inputs):\n+        return self._wrapped(inputs)\n\n@@ -108,8 +108,7 @@ class TestCase(unittest.TestCase):\n             )\n         if expected_mask_shape is not None and supports_masking is not True:\n             raise ValueError(\n-                \"\"\"In order to use expected_mask_shape, supports_masking\n-                must be True.\"\"\"\n+                \"In order to use expected_mask_shape, supports_masking must be True.\"\n             )\n \n         init_kwargs = init_kwargs or {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#96fe6e4b2c5f25ea8d0dd7f8077fa1a334ed87e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 84 | Files Changed: 4 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 103 | Churn Cumulative: 867 | Contributors (this commit): 6 | Commits (past 90d): 57 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,7 +52,8 @@ def serialize(activation):\n             f\"Unknown activation function '{activation}' cannot be \"\n             \"serialized due to invalid function name. Make sure to use \"\n             \"an activation name that matches the references defined in \"\n-            \"activations.py or use `@keras_core.saving.register_keras_serializable()`\"\n+            \"activations.py or use \"\n+            \"`@keras_core.saving.register_keras_serializable()`\"\n             \"to register any custom activations. \"\n             f\"config={fn_config}\"\n         )\n\n@@ -8,6 +8,8 @@ from keras_core import operations as ops\n from keras_core.layers.layer import Layer\n from keras_core.models.model import Model\n from keras_core.operations.function import Function\n+from keras_core.operations.function import make_node_key\n+from keras_core.saving import serialization_lib\n from keras_core.utils import python_utils\n from keras_core.utils import tracking\n \n@@ -212,6 +214,10 @@ class Functional(Function, Model):\n                     config.pop(key, None)\n         return config\n \n+    @classmethod\n+    def from_config(self):\n+        raise NotImplementedError\n+\n \n def operation_fn(operation, training):\n     def call(*args, **kwargs):\n@@ -227,82 +233,11 @@ def operation_fn(operation, training):\n \n def functional_like_constructor(cls):\n     init_args = inspect.getfullargspec(cls.__init__).args[1:]\n-    functional_init_args = inspect.getfullargspec(Functional.__init__).args[\n-        1:\n-    ]\n+    functional_init_args = inspect.getfullargspec(Functional.__init__).args[1:]\n     if init_args == functional_init_args:\n         return True\n     return False\n \n \n def get_functional_config(network):\n-    config = config or {}\n-    \n-    config[\"name\"] = network.name\n-    node_conversion_map = {}\n-    for layer in network.layers:\n-        kept_nodes = 1 if _should_skip_first_node(layer) else 0\n-        for original_node_index, node in enumerate(layer._inbound_nodes):\n-            node_key = _make_node_key(layer.name, original_node_index)\n-            if node_key in network._network_nodes:\n-                node_conversion_map[node_key] = kept_nodes\n-                kept_nodes += 1\n-    layer_configs = []\n-\n-    with serialization_lib.SharedObjectSavingScope():\n-        for layer in network.layers:  # From the earliest layers on.\n-            filtered_inbound_nodes = []\n-            for original_node_index, node in enumerate(layer._inbound_nodes):\n-                node_key = _make_node_key(layer.name, original_node_index)\n-                if node_key in network._network_nodes and not node.is_input:\n-                    # The node is relevant to the model:\n-                    # add to filtered_inbound_nodes.\n-                    node_data = node.serialize(\n-                        _make_node_key, node_conversion_map\n-                    )\n-                    filtered_inbound_nodes.append(node_data)\n-            layer_config = serialization_lib.serialize_keras_object(layer)\n-            layer_config[\"name\"] = layer.name\n-            layer_config[\"inbound_nodes\"] = filtered_inbound_nodes\n-            layer_configs.append(layer_config)\n-        config[\"layers\"] = layer_configs\n-\n-    # Gather info about inputs and outputs.\n-    model_inputs = []\n-    for i in range(len(network._input_layers)):\n-        layer, node_index, tensor_index = network._input_coordinates[i]\n-        node_key = _make_node_key(layer.name, node_index)\n-        if node_key not in network._network_nodes:\n-            continue\n-        new_node_index = node_conversion_map[node_key]\n-        model_inputs.append(\n-            [layer.name, new_node_index, tensor_index]\n-        )\n-    model_inputs = nest.pack_sequence_as(\n-        network._nested_inputs, model_inputs\n-    )\n-    # Preserve external Keras compat for Models with single input.\n-    if not tf.nest.is_nested(model_inputs):\n-        model_inputs = [model_inputs]\n-    model_inputs = tf_utils.convert_inner_node_data(model_inputs)\n-    config[\"input_layers\"] = model_inputs\n-\n-    model_outputs = []\n-    for i in range(len(network._output_layers)):\n-        layer, node_index, tensor_index = network._output_coordinates[i]\n-        node_key = _make_node_key(layer.name, node_index)\n-        if node_key not in network._network_nodes:\n-            continue\n-        new_node_index = node_conversion_map[node_key]\n-        model_outputs.append(\n-            tf_utils.ListWrapper([layer.name, new_node_index, tensor_index])\n-        )\n-    model_outputs = tf.nest.pack_sequence_as(\n-        network._nested_outputs, model_outputs\n-    )\n-    # Preserve external Keras compat for Models with single output.\n-    if not tf.nest.is_nested(model_outputs):\n-        model_outputs = [model_outputs]\n-    model_outputs = tf_utils.convert_inner_node_data(model_outputs)\n-    config[\"output_layers\"] = model_outputs\n-    return config\n+    raise NotImplementedError\n\n@@ -4,7 +4,6 @@ from tensorflow import nest\n \n from keras_core.backend import KerasTensor\n from keras_core.operations.operation import Operation\n-from keras_core.utils.naming import auto_name\n \n \n class Function(Operation):\n@@ -16,7 +15,7 @@ class Function(Operation):\n         self._inputs = nest.flatten(inputs)\n         self._outputs = nest.flatten(outputs)\n \n-        (nodes, nodes_by_depth, operations, operations_by_depth) = _map_graph(\n+        (nodes, nodes_by_depth, operations, operations_by_depth) = map_graph(\n             self._inputs, self._outputs\n         )\n         self._nodes = nodes\n@@ -135,11 +134,11 @@ class Function(Operation):\n                         )\n \n \n-def _make_node_key(op_name, node_index):\n+def make_node_key(op_name, node_index):\n     return op_name + \"_ib-\" + str(node_index)\n \n \n-def _map_graph(inputs, outputs):\n+def map_graph(inputs, outputs):\n     \"\"\"Validates a graph's topology and gather its operations and nodes.\n \n     Args:\n@@ -157,7 +156,7 @@ def _map_graph(inputs, outputs):\n     # Nodes are ordered from inputs -> outputs.\n     nodes_in_decreasing_depth, operation_indices = _build_map(outputs)\n     network_nodes = {\n-        _make_node_key(\n+        make_node_key(\n             str(id(node.operation)), node.operation._inbound_nodes.index(node)\n         )\n         for node in nodes_in_decreasing_depth\n@@ -196,7 +195,7 @@ def _map_graph(inputs, outputs):\n             operations_depths[input_operation] = 0\n             operation_indices[input_operation] = -1\n             nodes_depths[input_operation._inbound_nodes[0]] = 0\n-            network_nodes.add(_make_node_key(input_operation.name, 0))\n+            network_nodes.add(make_node_key(input_operation.name, 0))\n \n     # Build a dict {depth: list of nodes with this depth}\n     nodes_by_depth = collections.defaultdict(list)\n\n@@ -61,10 +61,10 @@ class Operation:\n     def __new__(cls, *args, **kwargs):\n         \"\"\"We override __new__ to saving serializable constructor arguments.\n \n-        These arguments are used to auto-generate an object serialization config,\n-        which enables user-created subclasses to be serializable out of the box\n-        in most cases without forcing the user to manually implement\n-        `get_config()`.\n+        These arguments are used to auto-generate an object serialization\n+        config, which enables user-created subclasses to be serializable\n+        out of the box in most cases without forcing the user\n+        to manually implement `get_config()`.\n         \"\"\"\n         # Generate a config to be returned by default by `get_config()`.\n         arg_names = inspect.getfullargspec(cls.__init__).args\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#108c217078106a22010f5a606de6987aa884f5c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 144 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 144 | Churn Cumulative: 720 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,144 @@\n+from keras_core import operations as ops\n+from keras_core.optimizers import optimizer\n+\n+\n+class Adam(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Adam algorithm.\n+\n+    Adam optimization is a stochastic gradient descent method that is based on\n+    adaptive estimation of first-order and second-order moments.\n+\n+    According to\n+    [Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\n+    the method is \"*computationally\n+    efficient, has little memory requirement, invariant to diagonal rescaling of\n+    gradients, and is well suited for problems that are large in terms of\n+    data/parameters*\".\n+\n+    Args:\n+      learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n+        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        learning rate. Defaults to `0.001`.\n+      beta_1: A float value or a constant float tensor, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n+      beta_2: A float value or a constant float tensor, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        exponential decay rate for the 2nd moment estimates. Defaults to\n+        `0.999`.\n+      epsilon: A small constant for numerical stability. This epsilon is\n+        \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n+        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n+        `1e-7`.\n+      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n+        the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n+      {{base_optimizer_keyword_args}}\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        beta_1=0.9,\n+        beta_2=0.999,\n+        epsilon=1e-7,\n+        amsgrad=False,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"Adam\",\n+        **kwargs\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs\n+        )\n+        self.beta_1 = beta_1\n+        self.beta_2 = beta_2\n+        self.epsilon = epsilon\n+        self.amsgrad = amsgrad\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Adam optimizer has 3 types of variables: momentums, velocities and\n+        velocity_hat (only set when amsgrad is applied),\n+\n+        Args:\n+          var_list: list of model variables to build Adam variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._momentums = []\n+        self._velocities = []\n+        for var in var_list:\n+            self._momentums.append(\n+                self.add_variable_from_reference(\n+                    model_variable=var, variable_name=\"m\"\n+                )\n+            )\n+            self._velocities.append(\n+                self.add_variable_from_reference(\n+                    model_variable=var, variable_name=\"v\"\n+                )\n+            )\n+        if self.amsgrad:\n+            self._velocity_hats = []\n+            for var in var_list:\n+                self._velocity_hats.append(\n+                    self.add_variable_from_reference(\n+                        model_variable=var, variable_name=\"vhat\"\n+                    )\n+                )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+        beta_1_power = None\n+        beta_2_power = None\n+        lr = ops.cast(self.learning_rate, variable.dtype)\n+        local_step = ops.cast(self.iterations + 1, variable.dtype)\n+        beta_1_power = ops.power(ops.cast(self.beta_1, variable.dtype), local_step)\n+        beta_2_power = ops.power(ops.cast(self.beta_2, variable.dtype), local_step)\n+\n+        m = self._momentums[self._get_variable_index(variable)]\n+        v = self._velocities[self._get_variable_index(variable)]\n+\n+        alpha = lr * ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n+\n+        m.assign(m + (gradient - m) * (1 - self.beta_1))\n+        v.assign(v + (ops.square(gradient) - v) * (1 - self.beta_2))\n+        if self.amsgrad:\n+            v_hat = self._velocity_hats[self._get_variable_index(variable)]\n+            v_hat.assign(ops.maximum(v_hat, v))\n+            v = v_hat\n+        variable.assign(m - (m * alpha) / (ops.sqrt(v) + self.epsilon))\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"beta_1\": self.beta_1,\n+                \"beta_2\": self.beta_2,\n+                \"epsilon\": self.epsilon,\n+                \"amsgrad\": self.amsgrad,\n+            }\n+        )\n+        return config\n+\n+\n+Adam.__doc__ = Adam.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
