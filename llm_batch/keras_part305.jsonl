{"custom_id": "keras#de27619e9dc602833a5578e59f034bc49a8d9cd4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 251 | Lines Deleted: 237 | Files Changed: 5 | Hunks: 40 | Methods Changed: 21 | Complexity Δ (Sum/Max): 12/16 | Churn Δ: 488 | Churn Cumulative: 20090 | Contributors (this commit): 145 | Commits (past 90d): 55 | Contributors (cumulative): 168 | DMM Complexity: 0.9923076923076923\n\nDIFF:\n@@ -160,5 +160,7 @@ elif backend() == 'tensorflow':\n     from .load_backend import manual_variable_initialization\r\n     from .load_backend import get_session\r\n     from .load_backend import set_session\r\n+    from .load_backend import top_k\r\n+    from .load_backend import MeanIoU\r\n elif backend() == 'cntk':\r\n     from .load_backend import clear_session\r\n\n@@ -28,6 +28,8 @@ py_any = any\n py_sum = sum\n py_slice = slice\n \n+MeanIoU = tf.keras.metrics.MeanIoU\n+\n # INTERNAL UTILS\n \n # This list holds the available devices.\n@@ -3501,6 +3503,24 @@ def in_top_k(predictions, targets, k):\n                           k=k)\n \n \n+def top_k(input, k=1, sorted=True, name=None):\n+    \"\"\"Finds values and indices of the k largest entries for the last dimension.\n+\n+    # Arguments\n+        input: 1-D or higher Tensor with last dimension at least k.\n+        k: 0-D int32 Tensor. Number of top elements to look for along the last\n+            dimension (along each row for matrices).\n+        sorted: If true the resulting k elements will be sorted by the values\n+            in descending order.\n+        name: Optional name for the operation.\n+\n+    # Returns\n+        values: The k largest elements along each last dimensional slice.\n+        indices: The indices of values within the last dimension of input.\n+    \"\"\"\n+    return tf.math.top_k(input, k=k, sorted=sorted, name=name)\n+\n+\n # CONVOLUTIONS\n \n \n\n@@ -1407,6 +1407,11 @@ class Precision(Metric):\n                  dtype=None):\n         super(Precision, self).__init__(name=name, dtype=dtype)\n         self.init_thresholds = thresholds\n+        if top_k is not None and K.backend() != 'tensorflow':\n+            raise RuntimeError(\n+                '`top_k` argument for `Precision` metric is currently supported only '\n+                'with TensorFlow backend.')\n+\n         self.top_k = top_k\n         self.class_id = class_id\n \n@@ -1512,6 +1517,11 @@ class Recall(Metric):\n                  dtype=None):\n         super(Recall, self).__init__(name=name, dtype=dtype)\n         self.init_thresholds = thresholds\n+        if top_k is not None and K.backend() != 'tensorflow':\n+            raise RuntimeError(\n+                '`top_k` argument for `Recall` metric is currently supported only '\n+                'with TensorFlow backend.')\n+\n         self.top_k = top_k\n         self.class_id = class_id\n \n@@ -1832,111 +1842,6 @@ class AUC(Metric):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class MeanIoU(Metric):\n-    \"\"\"Computes the mean Intersection-Over-Union metric.\n-\n-    Mean Intersection-Over-Union is a common evaluation metric for semantic image\n-    segmentation, which first computes the IOU for each semantic class and then\n-    computes the average over classes. IOU is defined as follows:\n-    IOU = true_positive / (true_positive + false_positive + false_negative).\n-\n-    The predictions are accumulated in a confusion matrix, weighted by\n-    `sample_weight` and the metric is then calculated from it.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Usage with the compile API:\n-    ```python\n-    model = keras.Model(inputs, outputs)\n-    model.compile(\n-        'sgd',\n-        loss='mse',\n-        metrics=[eras.metrics.MeanIoU(num_classes=2)])\n-    ```\n-\n-    # Arguments\n-        num_classes: The possible number of labels the prediction task can have.\n-            This value must be provided, since a confusion matrix of dimension =\n-            [num_classes, num_classes] will be allocated.\n-        name: (Optional) string name of the metric instance.\n-        dtype: (Optional) data type of the metric result.\n-    \"\"\"\n-\n-    def __init__(self, num_classes, name=None, dtype=None):\n-        super(MeanIoU, self).__init__(name=name, dtype=dtype)\n-        self.num_classes = num_classes\n-\n-        # Variable to accumulate the predictions in the confusion matrix. Setting\n-        # the type to be `float64` as required by confusion_matrix_ops.\n-        self.total_cm = self.add_weight(\n-            'total_confusion_matrix',\n-            shape=(num_classes, num_classes),\n-            initializer='zeros',\n-            dtype='float64')\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        y_true = K.cast(y_true, self._dtype)\n-        y_pred = K.cast(y_pred, self._dtype)\n-\n-        # Flatten the input if its rank > 1.\n-        if y_pred.shape.ndims > 1:\n-            y_pred = K.reshape(y_pred, [-1])\n-\n-        if y_true.shape.ndims > 1:\n-            y_true = K.reshape(y_true, [-1])\n-\n-        if sample_weight is not None and sample_weight.shape.ndims > 1:\n-            sample_weight = K.reshape(sample_weight, [-1])\n-\n-        # Accumulate the prediction to current confusion matrix.\n-        current_cm = confusion_matrix.confusion_matrix(\n-            y_true,\n-            y_pred,\n-            self.num_classes,\n-            weights=sample_weight,\n-            dtype=dtypes.float64)\n-\n-        return self.total_cm.assign_add(current_cm)\n-\n-    def result(self):\n-        \"\"\"Compute the mean intersection-over-union via the confusion matrix.\"\"\"\n-        sum_over_row = K.cast(\n-            K.sum(self.total_cm, axis=0), dtype=self._dtype)\n-        sum_over_col = K.cast(\n-            K.sum(self.total_cm, axis=1), dtype=self._dtype)\n-        true_positives = K.cast(\n-            array_ops.diag_part(self.total_cm), dtype=self._dtype)\n-\n-        # sum_over_row + sum_over_col =\n-        #     2 * true_positives + false_positives + false_negatives.\n-        denominator = sum_over_row + sum_over_col - true_positives\n-\n-        # The mean is only computed over classes that appear in the\n-        # label or prediction tensor. If the denominator is 0, we need to\n-        # ignore the class.\n-        num_valid_entries = K.sum(\n-            K.cast(K.not_equal(denominator, 0), dtype=self._dtype))\n-\n-        denominator = K.maximum(denominator, 0)\n-        iou = K.switch(\n-            K.greater(denominator, 0),\n-            true_positives / denominator,\n-            K.zeros_like(true_positives))\n-        return K.switch(\n-            K.greater(num_valid_entries, 0),\n-            K.sum(iou) / num_valid_entries,\n-            K.zeros_like(K.sum(iou)))\n-\n-    def reset_states(self):\n-        K.set_value(self.total_cm, np.zeros((self.num_classes, self.num_classes)))\n-\n-    def get_config(self):\n-        config = {'num_classes': self.num_classes}\n-        base_config = super(MeanIoU, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n def accuracy(y_true, y_pred):\n     if not K.is_tensor(y_pred):\n         y_pred = K.constant(y_pred)\n@@ -2006,6 +1911,7 @@ mae = MAE = mean_absolute_error\n mape = MAPE = mean_absolute_percentage_error\n msle = MSLE = mean_squared_logarithmic_error\n cosine = cosine_similarity = cosine_proximity\n+MeanIoU = K.MeanIoU\n \n \n def serialize(metric):\n\n@@ -72,15 +72,17 @@ def filter_top_k(x, k):\n     \"\"\"Filters top-k values in the last dim of x and set the rest to NEG_INF.\n     Used for computing top-k prediction values in dense labels (which has the same\n     shape as predictions) for recall and precision top-k metrics.\n+\n     # Arguments\n         x: tensor with any dimensions.\n         k: the number of values to keep.\n+\n     # Returns\n         tensor with same shape and dtype as x.\n     \"\"\"\n-    _, top_k_idx = nn_ops.top_k(x, k, sorted=False)\n+    _, top_k_idx = K.top_k(x, k, sorted=False)\n     top_k_mask = K.sum(\n-        K.one_hot(top_k_idx, x.shape[-1], axis=-1), axis=-2)\n+        K.one_hot(top_k_idx, x.shape[-1]), axis=-2)\n     return x * top_k_mask + NEG_INF * (1 - top_k_mask)\n \n \n@@ -174,7 +176,7 @@ def weighted_assign_add(label, pred, weights, var):\n def update_confusion_matrix_variables(variables_to_update,\n                                       y_true,\n                                       y_pred,\n-                                      thresholds,\n+                                      thresholds=0.5,\n                                       top_k=None,\n                                       class_id=None,\n                                       sample_weight=None):\n@@ -277,7 +279,6 @@ def update_confusion_matrix_variables(variables_to_update,\n \n     # Compare predictions and threshold.\n     pred_is_pos = K.greater(preds_tiled, thresh_tiled)\n-    pred_is_neg = K.greater(thresh_tiled, preds_tiled)\n \n     # Tile labels by number of thresholds\n     label_is_pos = K.tile(labels_2d, [num_thresholds, 1])\n@@ -299,6 +300,8 @@ def update_confusion_matrix_variables(variables_to_update,\n     update_fn = ConfusionMatrix.FALSE_NEGATIVES in variables_to_update\n \n     if update_fn or update_tn:\n+        pred_is_neg = K.equal(\n+            pred_is_pos, K.zeros_like(pred_is_pos, dtype=pred_is_pos.dtype))\n         loop_vars[ConfusionMatrix.FALSE_NEGATIVES] = (label_is_pos, pred_is_neg)\n \n     if update_fp or update_tn:\n\n@@ -663,82 +663,86 @@ class TestPrecisionTest(object):\n         expected_precision = weighted_tp / weighted_positives\n         assert np.allclose([expected_precision, 0], K.eval(result), 1e-3)\n \n-    # def test_unweighted_top_k(self):\n-    #     p_obj = metrics.Precision(top_k=3)\n-    #     y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-    #     result = p_obj(y_true, y_pred)\n-    #     assert np.isclose(1. / 3, K.eval(result))\n+    @pytest.mark.skipif(K.backend() != 'tensorflow', reason=\"requires tensorflow\")\n+    def test_unweighted_top_k(self):\n+        p_obj = metrics.Precision(top_k=3)\n+        y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1. / 3, K.eval(result))\n \n-    # def test_weighted_top_k(self):\n-    #     p_obj = metrics.Precision(top_k=3)\n-    #     y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n-    #     y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n-    #     K.eval(\n-    #         p_obj(\n-    #             y_true1,\n-    #             y_pred1,\n-    #             sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n+    @pytest.mark.skipif(K.backend() != 'tensorflow', reason=\"requires tensorflow\")\n+    def test_weighted_top_k(self):\n+        p_obj = metrics.Precision(top_k=3)\n+        y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n+        y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n+        K.eval(\n+            p_obj(\n+                y_true1,\n+                y_pred1,\n+                sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n \n-    #     y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n-    #     y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n-    #     result = p_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n+        y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n+        y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n+        result = p_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n \n-    #     tp = (2 + 5) + (3 + 3)\n-    #     predicted_positives = (1 + 2 + 5) + (3 + 3 + 3)\n-    #     expected_precision = tp / predicted_positives\n-    #     assert np.isclose(expected_precision, K.eval(result))\n+        tp = (2 + 5) + (3 + 3)\n+        predicted_positives = (1 + 2 + 5) + (3 + 3 + 3)\n+        expected_precision = tp / predicted_positives\n+        assert np.isclose(expected_precision, K.eval(result))\n \n-    # def test_unweighted_class_id(self):\n-    #     p_obj = metrics.Precision(class_id=2)\n+    def test_unweighted_class_id(self):\n+        p_obj = metrics.Precision(class_id=2)\n \n-    #     y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-    #     result = p_obj(y_true, y_pred)\n-    #     assert np.isclose(1, K.eval(result))\n-    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n-    #     assert np.isclose(0, K.eval(p_obj.false_positives))\n+        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(0, K.eval(p_obj.false_positives))\n \n-    #     y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-    #     result = p_obj(y_true, y_pred)\n-    #     assert np.isclose(1, K.eval(result))\n-    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n-    #     assert np.isclose(0, K.eval(p_obj.false_positives))\n+        y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(0, K.eval(p_obj.false_positives))\n \n-    #     y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n-    #     result = p_obj(y_true, y_pred)\n-    #     assert np.isclose(0.5, K.eval(result))\n-    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n-    #     assert np.isclose(1, K.eval(p_obj.false_positives))\n+        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(1, K.eval(p_obj.false_positives))\n \n-    # def test_unweighted_top_k_and_class_id(self):\n-    #     p_obj = metrics.Precision(class_id=2, top_k=2)\n+    @pytest.mark.skipif(K.backend() != 'tensorflow', reason=\"requires tensorflow\")\n+    def test_unweighted_top_k_and_class_id(self):\n+        p_obj = metrics.Precision(class_id=2, top_k=2)\n \n-    #     y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-    #     result = p_obj(y_true, y_pred)\n-    #     assert np.isclose(1, K.eval(result))\n-    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n-    #     assert np.isclose(0, K.eval(p_obj.false_positives))\n+        y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(0, K.eval(p_obj.false_positives))\n \n-    #     y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-    #     result = p_obj(y_true, y_pred)\n-    #     assert np.isclose(1, K.eval(result))\n-    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n-    #     assert np.isclose(0, K.eval(p_obj.false_positives))\n+        y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(0, K.eval(p_obj.false_positives))\n \n-    # def test_unweighted_top_k_and_threshold(self):\n-    #     p_obj = metrics.Precision(thresholds=.7, top_k=2)\n+    @pytest.mark.skipif(K.backend() != 'tensorflow', reason=\"requires tensorflow\")\n+    def test_unweighted_top_k_and_threshold(self):\n+        p_obj = metrics.Precision(thresholds=.7, top_k=2)\n \n-    #     y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n-    #     result = p_obj(y_true, y_pred)\n-    #     assert np.isclose(1, K.eval(result))\n-    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n-    #     assert np.isclose(0, K.eval(p_obj.false_positives))\n+        y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(0, K.eval(p_obj.false_positives))\n \n \n class TestRecall(object):\n@@ -812,79 +816,158 @@ class TestRecall(object):\n         expected_recall = weighted_tp / weighted_positives\n         assert np.allclose([expected_recall, 0], K.eval(result), 1e-3)\n \n-    # def test_unweighted_top_k(self):\n-    #     r_obj = metrics.Recall(top_k=3)\n-    #     y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-    #     result = r_obj(y_true, y_pred)\n-    #     assert np.isclose(0.5, K.eval(result))\n+    @pytest.mark.skipif(K.backend() != 'tensorflow', reason=\"requires tensorflow\")\n+    def test_unweighted_top_k(self):\n+        r_obj = metrics.Recall(top_k=3)\n+        y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n \n-    # def test_weighted_top_k(self):\n-    #     r_obj = metrics.Recall(top_k=3)\n-    #     y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n-    #     y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n-    #     K.eval(\n-    #         r_obj(\n-    #             y_true1,\n-    #             y_pred1,\n-    #             sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n+    @pytest.mark.skipif(K.backend() != 'tensorflow', reason=\"requires tensorflow\")\n+    def test_weighted_top_k(self):\n+        r_obj = metrics.Recall(top_k=3)\n+        y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n+        y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n+        K.eval(\n+            r_obj(\n+                y_true1,\n+                y_pred1,\n+                sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n \n-    #     y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n-    #     y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n-    #     result = r_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n+        y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n+        y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n+        result = r_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n \n-    #     tp = (2 + 5) + (3 + 3)\n-    #     positives = (4 + 2 + 5) + (3 + 3 + 3 + 3)\n-    #     expected_recall = tp / positives\n-    #     assert np.isclose(expected_recall, K.eval(result))\n+        tp = (2 + 5) + (3 + 3)\n+        positives = (4 + 2 + 5) + (3 + 3 + 3 + 3)\n+        expected_recall = tp / positives\n+        assert np.isclose(expected_recall, K.eval(result))\n \n-    # def test_unweighted_class_id(self):\n-    #     r_obj = metrics.Recall(class_id=2)\n+    def test_unweighted_class_id(self):\n+        r_obj = metrics.Recall(class_id=2)\n \n-    #     y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-    #     result = r_obj(y_true, y_pred)\n-    #     assert np.isclose(1, K.eval(result))\n-    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n-    #     assert np.isclose(0, K.eval(r_obj.false_negatives))\n+        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(0, K.eval(r_obj.false_negatives))\n \n-    #     y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-    #     result = r_obj(y_true, y_pred)\n-    #     assert np.isclose(0.5, K.eval(result))\n-    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n-    #     assert np.isclose(1, K.eval(r_obj.false_negatives))\n+        y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(1, K.eval(r_obj.false_negatives))\n \n-    #     y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n-    #     result = r_obj(y_true, y_pred)\n-    #     assert np.isclose(0.5, K.eval(result))\n-    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n-    #     assert np.isclose(1, K.eval(r_obj.false_negatives))\n+        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(1, K.eval(r_obj.false_negatives))\n \n-    # def test_unweighted_top_k_and_class_id(self):\n-    #     r_obj = metrics.Recall(class_id=2, top_k=2)\n+    @pytest.mark.skipif(K.backend() != 'tensorflow', reason=\"requires tensorflow\")\n+    def test_unweighted_top_k_and_class_id(self):\n+        r_obj = metrics.Recall(class_id=2, top_k=2)\n \n-    #     y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-    #     result = r_obj(y_true, y_pred)\n-    #     assert np.isclose(1, K.eval(result))\n-    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n-    #     assert np.isclose(0, K.eval(r_obj.false_negatives))\n+        y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(0, K.eval(r_obj.false_negatives))\n \n-    #     y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n-    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-    #     result = r_obj(y_true, y_pred)\n-    #     assert np.isclose(0.5, K.eval(result))\n-    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n-    #     assert np.isclose(1, K.eval(r_obj.false_negatives))\n+        y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(1, K.eval(r_obj.false_negatives))\n \n-    # def test_unweighted_top_k_and_threshold(self):\n-    #     r_obj = metrics.Recall(thresholds=.7, top_k=2)\n+    @pytest.mark.skipif(K.backend() != 'tensorflow', reason=\"requires tensorflow\")\n+    def test_unweighted_top_k_and_threshold(self):\n+        r_obj = metrics.Recall(thresholds=.7, top_k=2)\n \n-    #     y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n-    #     y_true = K.constant([1, 1, 1, 0, 1], shape=(1, 5))\n-    #     result = r_obj(y_true, y_pred)\n-    #     assert np.isclose(0.25, K.eval(result))\n-    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n-    #     assert np.isclose(3, K.eval(r_obj.false_negatives))\n+        y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([1, 1, 1, 0, 1], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.25, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(3, K.eval(r_obj.false_negatives))\n+\n+\n+@pytest.mark.skipif(K.backend() != 'tensorflow', reason=\"requires tensorflow\")\n+class TestMeanIoU(object):\n+\n+    def test_config(self):\n+        m_obj = metrics.MeanIoU(num_classes=2, name='mean_iou')\n+        assert m_obj.name == 'mean_iou'\n+        assert m_obj.num_classes == 2\n+\n+        m_obj2 = metrics.MeanIoU.from_config(m_obj.get_config())\n+        assert m_obj2.name == 'mean_iou'\n+        assert m_obj2.num_classes == 2\n+\n+    def test_unweighted(self):\n+        y_pred = K.constant([0, 1, 0, 1], shape=(1, 4))\n+        y_true = K.constant([0, 0, 1, 1], shape=(1, 4))\n+\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        result = m_obj(y_true, y_pred)\n+\n+        # cm = [[1, 1],\n+        #       [1, 1]]\n+        # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        y_pred = K.constant([0, 1, 0, 1], dtype='float32')\n+        y_true = K.constant([0, 0, 1, 1])\n+        sample_weight = K.constant([0.2, 0.3, 0.4, 0.1])\n+\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # cm = [[0.2, 0.3],\n+        #       [0.4, 0.1]]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)) / 2\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_multi_dim_input(self):\n+        y_pred = K.constant([[0, 1], [0, 1]], dtype='float32')\n+        y_true = K.constant([[0, 0], [1, 1]])\n+        sample_weight = K.constant([[0.2, 0.3], [0.4, 0.1]])\n+\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # cm = [[0.2, 0.3],\n+        #       [0.4, 0.1]]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)) / 2\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_zero_valid_entries(self):\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        assert np.allclose(K.eval(m_obj.result()), 0, atol=1e-3)\n+\n+    def test_zero_and_non_zero_entries(self):\n+        y_pred = K.constant([1], dtype='float32')\n+        y_true = K.constant([1])\n+\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        result = m_obj(y_true, y_pred)\n+\n+        # cm = [[0, 0],\n+        #       [0, 1]]\n+        # sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (0 + 1 / (1 + 1 - 1)) / 1\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4de37bcb4aaf32cd60b9046eb75ecf3598c6cba0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 2961 | Contributors (this commit): 17 | Commits (past 90d): 25 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -1911,6 +1911,7 @@ mae = MAE = mean_absolute_error\n mape = MAPE = mean_absolute_percentage_error\n msle = MSLE = mean_squared_logarithmic_error\n cosine = cosine_similarity = cosine_proximity\n+if K.backend() == 'tensorflow':\n     MeanIoU = K.MeanIoU\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8de897a0f448757f96a5e6417180d3b9ae9227c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 9 | Churn Cumulative: 1413 | Contributors (this commit): 26 | Commits (past 90d): 5 | Contributors (cumulative): 27 | DMM Complexity: 0.0\n\nDIFF:\n@@ -56,7 +56,7 @@ model.add(Dense(num_classes))\n model.add(Activation('softmax'))\n \n # initiate RMSprop optimizer\n-opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n+opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n \n # Let's train the model using RMSprop\n model.compile(loss='categorical_crossentropy',\n\n@@ -211,7 +211,8 @@ def fit_generator(model,\n \n                 outs = model.train_on_batch(x, y,\n                                             sample_weight=sample_weight,\n-                                            class_weight=class_weight)\n+                                            class_weight=class_weight,\n+                                            reset_metrics=False)\n \n                 outs = to_list(outs)\n                 for l, o in zip(out_labels, outs):\n@@ -371,7 +372,9 @@ def evaluate_generator(model, generator,\n \n             batch_logs = {'batch': steps_done, 'size': batch_size}\n             callbacks._call_batch_hook('test', 'begin', steps_done, batch_logs)\n-            outs = model.test_on_batch(x, y, sample_weight=sample_weight)\n+            outs = model.test_on_batch(x, y,\n+                                       sample_weight=sample_weight,\n+                                       reset_metrics=False)\n             outs = to_list(outs)\n             outs_per_batch.append(outs)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2bc43bff1e243f4031690277b67f60592c1edf58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 247 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 247 | Churn Cumulative: 642 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,247 +0,0 @@\n-\"\"\"\n-This example trains a simple CNN-Capsule Network on the CIFAR10 data set.\n-\n-Without Data Augmentation:\n-It gets to 75% validation accuracy in 10 epochs, 79% after 15 epochs,\n-and overfitting after 20 epochs\n-\n-With Data Augmentation:\n-It gets to 75% validation accuracy in 10 epochs, 79% after 15 epochs,\n-and 83% after 30 epochs.\n-\n-The highest achieved validation accuracy is 83.79% after 50 epochs.\n-This is a fast implementation that takes just 20s/epoch on a GTX 1070 GPU.\n-\n-The paper \"Dynamic Routing Between Capsules\": https://arxiv.org/abs/1710.09829\n-\"\"\"\n-from __future__ import print_function\n-\n-from keras import activations\n-from keras import backend as K\n-from keras import layers\n-from keras import utils\n-from keras.datasets import cifar10\n-from keras.models import Model\n-from keras.preprocessing.image import ImageDataGenerator\n-\n-\n-def squash(x, axis=-1):\n-    \"\"\"The Squashing Function.\n-    The nonlinear activation function used in Capsule Network\n-    # Arguments\n-        x: Input Tensor.\n-        axis: Integer axis along which the squashing function is to be applied.\n-\n-    # Returns\n-        Tensor with scaled value of the input tensor\n-    \"\"\"\n-    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n-    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n-    return scale * x\n-\n-\n-def margin_loss(y_true, y_pred):\n-    \"\"\"Margin loss\n-\n-    # Arguments\n-        y_true: tensor of true targets.\n-        y_pred: tensor of predicted targets.\n-\n-    # Returns\n-        Tensor with one scalar loss entry per sample.\n-    \"\"\"\n-    lamb, margin = 0.5, 0.1\n-    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n-        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n-\n-\n-class Capsule(layers.Layer):\n-    \"\"\"Capsule Network\n-\n-    A Capsule Network Layer implementation in Keras\n-    There are two versions of Capsule Networks.\n-    One is similar to dense layer (for the fixed-shape input),\n-    and the other is similar to time distributed dense layer\n-    (for inputs of varied length).\n-\n-    The input shape of Capsule must be (batch_size,\n-                                        input_num_capsule,\n-                                        input_dim_capsule\n-                                       )\n-    and the output shape is (batch_size,\n-                             num_capsule,\n-                             dim_capsule\n-                            )\n-    The Capsule implementation is from https://github.com/bojone/Capsule/\n-\n-\n-    # Arguments\n-        num_capsule: An integer, the number of capsules.\n-        dim_capsule: An integer, the dimensions of the capsule.\n-        routings: An integer, the number of routings.\n-        share_weights: A boolean, sets weight sharing between layers.\n-        activation: A string, the activation function to be applied.\n-    \"\"\"\n-\n-    def __init__(self,\n-                 num_capsule,\n-                 dim_capsule,\n-                 routings=3,\n-                 share_weights=True,\n-                 activation='squash',\n-                 **kwargs):\n-        super(Capsule, self).__init__(**kwargs)\n-        self.num_capsule = num_capsule\n-        self.dim_capsule = dim_capsule\n-        self.routings = routings\n-        self.share_weights = share_weights\n-        if activation == 'squash':\n-            self.activation = squash\n-        else:\n-            self.activation = activations.get(activation)\n-\n-    def build(self, input_shape):\n-        input_dim_capsule = input_shape[-1]\n-        if self.share_weights:\n-            self.kernel = self.add_weight(\n-                name='capsule_kernel',\n-                shape=(1, input_dim_capsule,\n-                       self.num_capsule * self.dim_capsule),\n-                initializer='glorot_uniform',\n-                trainable=True)\n-        else:\n-            input_num_capsule = input_shape[-2]\n-            self.kernel = self.add_weight(\n-                name='capsule_kernel',\n-                shape=(input_num_capsule, input_dim_capsule,\n-                       self.num_capsule * self.dim_capsule),\n-                initializer='glorot_uniform',\n-                trainable=True)\n-\n-    def call(self, inputs, **kwargs):\n-        \"\"\"Following the routing algorithm from Hinton's paper,\n-        but replace b = b + <u,v> with b = <u,v>.\n-\n-        This change can improve the feature representation of the capsule.\n-\n-        However, you can replace\n-            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n-        with\n-            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n-        to get standard routing.\n-        \"\"\"\n-\n-        if self.share_weights:\n-            hat_inputs = K.conv1d(inputs, self.kernel)\n-        else:\n-            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n-\n-        batch_size = K.shape(inputs)[0]\n-        input_num_capsule = K.shape(inputs)[1]\n-        hat_inputs = K.reshape(hat_inputs,\n-                               (batch_size, input_num_capsule,\n-                                self.num_capsule, self.dim_capsule))\n-        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n-\n-        b = K.zeros_like(hat_inputs[:, :, :, 0])\n-        print(self.routings)\n-        for i in range(self.routings):\n-            c = K.softmax(b, 1)\n-            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n-            if i < self.routings - 1:\n-                b = K.batch_dot(o, hat_inputs, [2, 3])\n-                if K.backend() == 'theano':\n-                    o = K.sum(o, axis=1)\n-        return o\n-\n-    def compute_output_shape(self, input_shape):\n-        return None, self.num_capsule, self.dim_capsule\n-\n-\n-batch_size = 128\n-num_classes = 10\n-epochs = 100\n-(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n-\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-y_train = utils.to_categorical(y_train, num_classes)\n-y_test = utils.to_categorical(y_test, num_classes)\n-\n-# A simple Conv2D model\n-input_image = layers.Input(shape=(None, None, 3))\n-x = layers.Conv2D(64, (3, 3), activation='relu')(input_image)\n-x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n-x = layers.AveragePooling2D((2, 2))(x)\n-x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n-x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n-\n-# Now, we reshape it to (batch_size, input_num_capsule, input_dim_capsule)\n-# then connect a capsule layer.\n-# The output of final model is the lengths of 10 capsules, which have 16 dimensions.\n-# The length of the output vector of the capsule expresses the probability of\n-# existence of the entity, so the problem becomes a 10 two-classification problem.\n-\n-x = layers.Reshape((-1, 128))(x)\n-capsule = Capsule(10, 16, 3, True)(x)\n-output = layers.Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n-model = Model(inputs=input_image, outputs=output)\n-\n-# Margin loss is used\n-model.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy'])\n-model.summary()\n-\n-# Compare the performance with and without data augmentation\n-data_augmentation = True\n-\n-if not data_augmentation:\n-    print('Not using data augmentation.')\n-    model.fit(\n-        x_train,\n-        y_train,\n-        batch_size=batch_size,\n-        epochs=epochs,\n-        validation_data=(x_test, y_test),\n-        shuffle=True)\n-else:\n-    print('Using real-time data augmentation.')\n-    # This will do preprocessing and real-time data augmentation:\n-    datagen = ImageDataGenerator(\n-        featurewise_center=False,  # set input mean to 0 over the dataset\n-        samplewise_center=False,  # set each sample mean to 0\n-        featurewise_std_normalization=False,  # divide inputs by dataset std\n-        samplewise_std_normalization=False,  # divide each input by its std\n-        zca_whitening=False,  # apply ZCA whitening\n-        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n-        rotation_range=0,  # randomly rotate images in 0 to 180 degrees\n-        width_shift_range=0.1,  # randomly shift images horizontally\n-        height_shift_range=0.1,  # randomly shift images vertically\n-        shear_range=0.,  # set range for random shear\n-        zoom_range=0.,  # set range for random zoom\n-        channel_shift_range=0.,  # set range for random channel shifts\n-        # set mode for filling points outside the input boundaries\n-        fill_mode='nearest',\n-        cval=0.,  # value used for fill_mode = \"constant\"\n-        horizontal_flip=True,  # randomly flip images\n-        vertical_flip=False,  # randomly flip images\n-        # set rescaling factor (applied before any other transformation)\n-        rescale=None,\n-        # set function that will be applied on each input\n-        preprocessing_function=None,\n-        # image data format, either \"channels_first\" or \"channels_last\"\n-        data_format=None,\n-        # fraction of images reserved for validation (strictly between 0 and 1)\n-        validation_split=0.0)\n-\n-    # Compute quantities required for feature-wise normalization\n-    # (std, mean, and principal components if ZCA whitening is applied).\n-    datagen.fit(x_train)\n-\n-    # Fit the model on the batches generated by datagen.flow().\n-    model.fit_generator(\n-        datagen.flow(x_train, y_train, batch_size=batch_size),\n-        epochs=epochs,\n-        validation_data=(x_test, y_test),\n-        workers=4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8446ef617f5bf90950f3934671fc09d5784ac97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 180 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 182 | Churn Cumulative: 973 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,181 +0,0 @@\n-'''\n-#Train a simple deep CNN on the CIFAR10 small images dataset using augmentation.\n-\n-Using TensorFlow internal augmentation APIs by replacing ImageGenerator with\n-an embedded AugmentLayer using LambdaLayer, which is faster on GPU.\n-\n-** Benchmark of `ImageGenerator`(IG) vs `AugmentLayer`(AL) both using augmentation\n-2D:**\n-\n-(backend = Tensorflow-GPU, Nvidia Tesla P100-SXM2)\n-\n-Epoch no. | IG %Accuracy   | IG Performance | AL %Accuracy  | AL Performance\n----------:|---------------:|---------------:|--------------:|--------------:\n-1         | 44.84          | 15 ms/step     | 45.54         | 358 us/step\n-2         | 52.34          |  8 ms/step     | 50.55         | 285 us/step\n-8         | 65.45          |  8 ms/step     | 65.59         | 281 us/step\n-25        | 76.74          |  8 ms/step     | 76.17         | 280 us/step\n-100       | 78.81          |  8 ms/step     | 78.70         | 285 us/step\n-\n-Settings: horizontal_flip = True\n-\n-\n-Epoch no. | IG %Accuracy   | IG Performance | AL %Accuracy  | AL Performance\n----------:|---------------:|---------------:|--------------:|--------------:\n-1         | 43.46          | 15 ms/step     | 42.21         | 334 us/step\n-2         | 48.95          | 11 ms/step     | 48.06         | 282 us/step\n-8         | 63.59          | 11 ms/step     | 61.35         | 290 us/step\n-25        | 72.25          | 12 ms/step     | 71.08         | 287 us/step\n-100       | 76.35          | 11 ms/step     | 74.62         | 286 us/step\n-\n-Settings: rotation = 30.0\n-\n-\n-(Corner process and rotation precision by `ImageGenerator` and `AugmentLayer`\n-are slightly different.)\n-'''\n-\n-from __future__ import print_function\n-import keras\n-from keras.datasets import cifar10\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation, Flatten\n-from keras.layers import Conv2D, Lambda, MaxPooling2D\n-from keras import backend as K\n-import os\n-\n-if K.backend() != 'tensorflow':\n-    raise RuntimeError('This example can only run with the '\n-                       'TensorFlow backend, '\n-                       'because it requires TF-native augmentation APIs')\n-\n-import tensorflow as tf\n-\n-\n-def augment_2d(inputs, rotation=0, horizontal_flip=False, vertical_flip=False):\n-    \"\"\"Apply additive augmentation on 2D data.\n-\n-    # Arguments\n-      rotation: A float, the degree range for rotation (0 <= rotation < 180),\n-          e.g. 3 for random image rotation between (-3.0, 3.0).\n-      horizontal_flip: A boolean, whether to allow random horizontal flip,\n-          e.g. true for 50% possibility to flip image horizontally.\n-      vertical_flip: A boolean, whether to allow random vertical flip,\n-          e.g. true for 50% possibility to flip image vertically.\n-\n-    # Returns\n-      input data after augmentation, whose shape is the same as its original.\n-    \"\"\"\n-    if inputs.dtype != tf.float32:\n-        inputs = tf.image.convert_image_dtype(inputs, dtype=tf.float32)\n-\n-    with tf.name_scope('augmentation'):\n-        shp = tf.shape(inputs)\n-        batch_size, height, width = shp[0], shp[1], shp[2]\n-        width = tf.cast(width, tf.float32)\n-        height = tf.cast(height, tf.float32)\n-\n-        transforms = []\n-        identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32)\n-\n-        if rotation > 0:\n-            angle_rad = rotation * 3.141592653589793 / 180.0\n-            angles = tf.random_uniform([batch_size], -angle_rad, angle_rad)\n-            f = tf.contrib.image.angles_to_projective_transforms(angles,\n-                                                                 height, width)\n-            transforms.append(f)\n-\n-        if horizontal_flip:\n-            coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5)\n-            shape = [-1., 0., width, 0., 1., 0., 0., 0.]\n-            flip_transform = tf.convert_to_tensor(shape, dtype=tf.float32)\n-            flip = tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1])\n-            noflip = tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])\n-            transforms.append(tf.where(coin, flip, noflip))\n-\n-        if vertical_flip:\n-            coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5)\n-            shape = [1., 0., 0., 0., -1., height, 0., 0.]\n-            flip_transform = tf.convert_to_tensor(shape, dtype=tf.float32)\n-            flip = tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1])\n-            noflip = tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])\n-            transforms.append(tf.where(coin, flip, noflip))\n-\n-    if transforms:\n-        f = tf.contrib.image.compose_transforms(*transforms)\n-        inputs = tf.contrib.image.transform(inputs, f, interpolation='BILINEAR')\n-    return inputs\n-\n-\n-batch_size = 32\n-num_classes = 10\n-epochs = 100\n-num_predictions = 20\n-save_dir = '/tmp/saved_models'\n-model_name = 'keras_cifar10_trained_model.h5'\n-\n-# The data, split between train and test sets:\n-(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n-print('x_train shape:', x_train.shape)\n-print(x_train.shape[0], 'train samples')\n-print(x_test.shape[0], 'test samples')\n-\n-# Convert class vectors to binary class matrices.\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-model = Sequential()\n-model.add(Lambda(augment_2d,\n-                 input_shape=x_train.shape[1:],\n-                 arguments={'rotation': 8.0, 'horizontal_flip': True}))\n-model.add(Conv2D(32, (3, 3), padding='same'))\n-model.add(Activation('relu'))\n-model.add(Conv2D(32, (3, 3)))\n-model.add(Activation('relu'))\n-model.add(MaxPooling2D(pool_size=(2, 2)))\n-model.add(Dropout(0.25))\n-\n-model.add(Conv2D(64, (3, 3), padding='same'))\n-model.add(Activation('relu'))\n-model.add(Conv2D(64, (3, 3)))\n-model.add(Activation('relu'))\n-model.add(MaxPooling2D(pool_size=(2, 2)))\n-model.add(Dropout(0.25))\n-\n-model.add(Flatten())\n-model.add(Dense(512))\n-model.add(Activation('relu'))\n-model.add(Dropout(0.5))\n-model.add(Dense(num_classes))\n-model.add(Activation('softmax'))\n-\n-# initiate RMSprop optimizer\n-opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n-\n-# Let's train the model using RMSprop\n-model.compile(loss='categorical_crossentropy',\n-              optimizer=opt,\n-              metrics=['accuracy'])\n-\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          epochs=epochs,\n-          validation_data=(x_test, y_test),\n-          shuffle=True)\n-\n-# Save model and weights\n-if not os.path.isdir(save_dir):\n-    os.makedirs(save_dir)\n-model_path = os.path.join(save_dir, model_name)\n-model.save(model_path)\n-print('Saved trained model at %s ' % model_path)\n-\n-# Score trained model.\n-scores = model.evaluate(x_test, y_test, verbose=1)\n-print('Test loss:', scores[0])\n-print('Test accuracy:', scores[1])\n\n@@ -164,7 +164,8 @@ def visualize_layer(model,\n             img = deprocess_image(input_img_data[0])\n             img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n                                                            pil_image.BICUBIC))\n-            input_img_data = [process_image(img, input_img_data[0])]\n+            input_img_data = np.expand_dims(\n+                process_image(img, input_img_data[0]), 0)\n \n         # decode the resulting input image\n         img = deprocess_image(input_img_data[0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e6d685b141c5fa19dcbd184a4792df4ee04f2504", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 465 | Files Changed: 13 | Hunks: 28 | Methods Changed: 11 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 497 | Churn Cumulative: 6682 | Contributors (this commit): 67 | Commits (past 90d): 14 | Contributors (cumulative): 113 | DMM Complexity: 0.0\n\nDIFF:\n@@ -56,7 +56,7 @@ model.add(Dense(num_classes))\n model.add(Activation('softmax'))\n \n # initiate RMSprop optimizer\n-opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n+opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n \n # Let's train the model using RMSprop\n model.compile(loss='categorical_crossentropy',\n\n@@ -358,7 +358,7 @@ else:\n     model = resnet_v1(input_shape=input_shape, depth=depth)\n \n model.compile(loss='categorical_crossentropy',\n-              optimizer=Adam(lr=lr_schedule(0)),\n+              optimizer=Adam(learning_rate=lr_schedule(0)),\n               metrics=['accuracy'])\n model.summary()\n print(model_type)\n\n@@ -1,6 +1,7 @@\n # -*- coding: utf-8 -*-\r\n \r\n import numpy as np\r\n+import argparse\r\n import cv2\r\n import matplotlib.pyplot as plt\r\n \r\n@@ -9,9 +10,13 @@ from keras.models import Model\n import keras.applications.resnet50 as resnet\r\n from keras.layers import UpSampling2D, Conv2D\r\n \r\n+# Set an appropriate image file\r\n+parser = argparse.ArgumentParser(description='Class activation maps with Keras.')\r\n+parser.add_argument('input_image', metavar='base', type=str,\r\n+                    help='Path to the image to use.')\r\n+args = parser.parse_args()\r\n+input_image = args.input_image\r\n \r\n-# Please set an appropriate image file\r\n-INPUT_IMG_FILE = \"dog.jpg\"\r\n \r\n ################################################################\r\n # The following parameters can be changed to other models\r\n@@ -20,8 +25,8 @@ INPUT_IMG_FILE = \"dog.jpg\"\n NETWORK_INPUT_SIZE = 224\r\n MODEL_CLASS = resnet.ResNet50\r\n PREPROCESS_FN = resnet.preprocess_input\r\n-LAST_CONV_LAYER = \"activation_49\"\r\n-PRED_LAYER = \"fc1000\"\r\n+LAST_CONV_LAYER = 'activation_49'\r\n+PRED_LAYER = 'fc1000'\r\n ################################################################\r\n \r\n # number of imagenet classes\r\n@@ -38,8 +43,8 @@ def load_img(fname, input_size, preprocess_fn):\n \r\n def get_cam_model(model_class,\r\n                   input_size=224,\r\n-                  last_conv_layer=\"activation_49\",\r\n-                  pred_layer=\"fc1000\"):\r\n+                  last_conv_layer='activation_49',\r\n+                  pred_layer='fc1000'):\r\n     model = model_class(input_shape=(input_size, input_size, 3))\r\n \r\n     final_params = model.get_layer(pred_layer).get_weights()\r\n@@ -47,14 +52,14 @@ def get_cam_model(model_class,\n         1, 1, -1, N_CLASSES), final_params[1])\r\n \r\n     last_conv_output = model.get_layer(last_conv_layer).output\r\n-    x = UpSampling2D(size=(32, 32), interpolation=\"bilinear\")(\r\n+    x = UpSampling2D(size=(32, 32), interpolation='bilinear')(\r\n         last_conv_output)\r\n     x = Conv2D(filters=N_CLASSES, kernel_size=(\r\n-        1, 1), name=\"predictions_2\")(x)\r\n+        1, 1), name='predictions_2')(x)\r\n \r\n     cam_model = Model(inputs=model.input,\r\n                       outputs=[model.output, x])\r\n-    cam_model.get_layer(\"predictions_2\").set_weights(final_params)\r\n+    cam_model.get_layer('predictions_2').set_weights(final_params)\r\n     return cam_model\r\n \r\n \r\n@@ -67,7 +72,7 @@ def postprocess(preds, cams, top_k=1):\n \r\n \r\n # 1. load image\r\n-imgs, original_img, original_size = load_img(INPUT_IMG_FILE,\r\n+imgs, original_img, original_size = load_img(input_image,\r\n                                              input_size=NETWORK_INPUT_SIZE,\r\n                                              preprocess_fn=resnet.preprocess_input)\r\n \r\n\n@@ -91,9 +91,9 @@ for layer_name in settings['features']:\n     # We avoid border artifacts by only involving non-border pixels in the loss.\n     scaling = K.prod(K.cast(K.shape(x), 'float32'))\n     if K.image_data_format() == 'channels_first':\n-        loss += coeff * K.sum(K.square(x[:, :, 2: -2, 2: -2])) / scaling\n+        loss = loss + coeff * K.sum(K.square(x[:, :, 2: -2, 2: -2])) / scaling\n     else:\n-        loss += coeff * K.sum(K.square(x[:, 2: -2, 2: -2, :])) / scaling\n+        loss = loss + coeff * K.sum(K.square(x[:, 2: -2, 2: -2, :])) / scaling\n \n # Compute the gradients of the dream wrt the loss.\n grads = K.gradients(loss, dream)[0]\n\n@@ -516,7 +516,10 @@ def train(run_name, start_epoch, stop_epoch, img_w):\n         name='ctc')([y_pred, labels, input_length, label_length])\n \n     # clipnorm seems to speeds up convergence\n-    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n+    sgd = SGD(learning_rate=0.02,\n+              decay=1e-6,\n+              momentum=0.9,\n+              nesterov=True)\n \n     model = Model(inputs=[input_data, labels, input_length, label_length],\n                   outputs=loss_out)\n\n@@ -60,7 +60,7 @@ model = Sequential()\n model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n model.add(Dense(len(chars), activation='softmax'))\n \n-optimizer = RMSprop(lr=0.01)\n+optimizer = RMSprop(learning_rate=0.01)\n model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n \n \n\n@@ -125,8 +125,8 @@ def build_discriminator():\n \n     return Model(image, [fake, aux])\n \n-if __name__ == '__main__':\n \n+if __name__ == '__main__':\n     # batch and latent size taken from the paper\n     epochs = 100\n     batch_size = 100\n@@ -140,7 +140,7 @@ if __name__ == '__main__':\n     print('Discriminator model:')\n     discriminator = build_discriminator()\n     discriminator.compile(\n-        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n+        optimizer=Adam(learning_rate=adam_lr, beta_1=adam_beta_1),\n         loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n     )\n     discriminator.summary()\n@@ -161,7 +161,7 @@ if __name__ == '__main__':\n \n     print('Combined model:')\n     combined.compile(\n-        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n+        optimizer=Adam(learning_rate=adam_lr, beta_1=adam_beta_1),\n         loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n     )\n     combined.summary()\n\n@@ -1,107 +0,0 @@\n-'''MNIST classification with TensorFlow's Dataset API.\n-\n-Introduced in TensorFlow 1.3, the Dataset API is now the\n-standard method for loading data into TensorFlow models.\n-A Dataset is a sequence of elements, which are themselves\n-composed of tf.Tensor components. For more details, see:\n-https://www.tensorflow.org/programmers_guide/datasets\n-\n-To use this with Keras, we make a dataset out of elements\n-of the form (input batch, output batch). From there, we\n-create a one-shot iterator and a graph node corresponding\n-to its get_next() method. Its components are then provided\n-to the network's Input layer and the Model.compile() method,\n-respectively.\n-\n-This example is intended to closely follow the\n-mnist_tfrecord.py example.\n-'''\n-import numpy as np\n-import os\n-import tempfile\n-\n-import keras\n-from keras import backend as K\n-from keras import layers\n-from keras.datasets import mnist\n-\n-import tensorflow as tf\n-\n-\n-if K.backend() != 'tensorflow':\n-    raise RuntimeError('This example can only run with the TensorFlow backend,'\n-                       ' because it requires the Datset API, which is not'\n-                       ' supported on other platforms.')\n-\n-\n-def cnn_layers(inputs):\n-    x = layers.Conv2D(32, (3, 3),\n-                      activation='relu', padding='valid')(inputs)\n-    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n-    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n-    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n-    x = layers.Flatten()(x)\n-    x = layers.Dense(512, activation='relu')(x)\n-    x = layers.Dropout(0.5)(x)\n-    predictions = layers.Dense(num_classes,\n-                               activation='softmax',\n-                               name='x_train_out')(x)\n-    return predictions\n-\n-\n-batch_size = 128\n-buffer_size = 10000\n-steps_per_epoch = int(np.ceil(60000 / float(batch_size)))  # = 469\n-epochs = 5\n-num_classes = 10\n-\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-x_train = x_train.astype(np.float32) / 255\n-x_train = np.expand_dims(x_train, -1)\n-y_train = tf.one_hot(y_train, num_classes)\n-\n-# Create the dataset and its associated one-shot iterator.\n-dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n-dataset = dataset.repeat()\n-dataset = dataset.shuffle(buffer_size)\n-dataset = dataset.batch(batch_size)\n-iterator = dataset.make_one_shot_iterator()\n-\n-# Model creation using tensors from the get_next() graph node.\n-inputs, targets = iterator.get_next()\n-model_input = layers.Input(tensor=inputs)\n-model_output = cnn_layers(model_input)\n-train_model = keras.models.Model(inputs=model_input, outputs=model_output)\n-\n-train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n-                    loss='categorical_crossentropy',\n-                    metrics=['accuracy'],\n-                    target_tensors=[targets])\n-train_model.summary()\n-\n-train_model.fit(epochs=epochs,\n-                steps_per_epoch=steps_per_epoch)\n-\n-# Save the model weights.\n-weight_path = os.path.join(tempfile.gettempdir(), 'saved_wt.h5')\n-train_model.save_weights(weight_path)\n-\n-# Clean up the TF session.\n-K.clear_session()\n-\n-# Second session to test loading trained model without tensors.\n-x_test = x_test.astype(np.float32)\n-x_test = np.expand_dims(x_test, -1)\n-\n-x_test_inp = layers.Input(shape=x_test.shape[1:])\n-test_out = cnn_layers(x_test_inp)\n-test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out)\n-\n-test_model.load_weights(weight_path)\n-test_model.compile(optimizer='rmsprop',\n-                   loss='sparse_categorical_crossentropy',\n-                   metrics=['accuracy'])\n-test_model.summary()\n-\n-loss, acc = test_model.evaluate(x_test, y_test, num_classes)\n-print('\\nTest accuracy: {0}'.format(acc))\n\n@@ -57,7 +57,7 @@ model.add(SimpleRNN(hidden_units,\n                     input_shape=x_train.shape[1:]))\n model.add(Dense(num_classes))\n model.add(Activation('softmax'))\n-rmsprop = RMSprop(lr=learning_rate)\n+rmsprop = RMSprop(learning_rate=learning_rate)\n model.compile(loss='categorical_crossentropy',\n               optimizer=rmsprop,\n               metrics=['accuracy'])\n\n@@ -240,7 +240,7 @@ def make_teacher_model(x_train, y_train,\n     model.add(Dense(64, activation='relu', name='fc1'))\n     model.add(Dense(num_classes, activation='softmax', name='fc2'))\n     model.compile(loss='categorical_crossentropy',\n-                  optimizer=SGD(lr=0.01, momentum=0.9),\n+                  optimizer=SGD(learning_rate=0.01, momentum=0.9),\n                   metrics=['accuracy'])\n \n     model.fit(x_train, y_train,\n@@ -291,7 +291,7 @@ def make_wider_student_model(teacher_model,\n     model.get_layer('fc2').set_weights([new_w_fc2, b_fc2])\n \n     model.compile(loss='categorical_crossentropy',\n-                  optimizer=SGD(lr=0.001, momentum=0.9),\n+                  optimizer=SGD(learning_rate=0.001, momentum=0.9),\n                   metrics=['accuracy'])\n \n     model.fit(x_train, y_train,\n@@ -340,7 +340,7 @@ def make_deeper_student_model(teacher_model,\n                  'conv1', 'conv2', 'fc1', 'fc2'])\n \n     model.compile(loss='categorical_crossentropy',\n-                  optimizer=SGD(lr=0.001, momentum=0.9),\n+                  optimizer=SGD(learning_rate=0.001, momentum=0.9),\n                   metrics=['accuracy'])\n \n     model.fit(x_train, y_train,\n\n@@ -73,9 +73,9 @@ def make_model(dense_layer_sizes, filters, kernel_size, pool_size):\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='adadelta',\n                   metrics=['accuracy'])\n-\n     return model\n \n+\n dense_size_candidates = [[32], [64], [32, 32], [64, 64]]\n my_classifier = KerasClassifier(make_model, batch_size=32)\n validator = GridSearchCV(my_classifier,\n\n@@ -1,238 +0,0 @@\n-'''MNIST dataset with TFRecords, the standard TensorFlow data format.\n-\n-TFRecord is a data format supported throughout TensorFlow.\n-This example demonstrates how to load TFRecord data using\n-Input Tensors. Input Tensors differ from the normal Keras\n-workflow because instead of fitting to data loaded into a\n-a numpy array, data is supplied via a special tensor that\n-reads data from nodes that are wired directly into model\n-graph with the `Input(tensor=input_tensor)` parameter.\n-\n-There are several advantages to using Input Tensors.\n-First, if a dataset is already in TFRecord format you\n-can load and train on that data directly in Keras.\n-Second, extended backend API capabilities such as TensorFlow\n-data augmentation is easy to integrate directly into your\n-Keras training scripts via input tensors.\n-Third, TensorFlow implements several data APIs for\n-TFRecords, some of which provide significantly faster\n-training performance than numpy arrays can provide because\n-they run via the C++ backend. Please note that this\n-example is tailored for brevity and clarity and not\n-to demonstrate performance or augmentation capabilities.\n-\n-Input Tensors also have important disadvantages. In\n-particular, Input Tensors are fixed at model construction\n-because rewiring networks is not yet supported.\n-For this reason, changing the data input source means\n-model weights must be saved and the model rebuilt\n-from scratch to connect the new input data.\n-validation cannot currently be performed as training\n-progresses, and must be performed after training completes.\n-This example demonstrates how to train with input\n-tensors, save the model weights, and then evaluate the\n-model using the numpy based Keras API.\n-\n-Gets to ~99.1% test accuracy after 5 epochs\n-(high variance from run to run: 98.9-99.3).\n-'''\n-import numpy as np\n-import os\n-import tensorflow as tf\n-import keras\n-from keras import backend as K\n-from keras import layers\n-from keras.callbacks import Callback\n-\n-from tensorflow.contrib.learn.python.learn.datasets import mnist\n-\n-if K.backend() != 'tensorflow':\n-    raise RuntimeError('This example can only run with the '\n-                       'TensorFlow backend, '\n-                       'because it requires TFRecords, which '\n-                       'are not supported on other platforms.')\n-\n-\n-class EvaluateInputTensor(Callback):\n-    \"\"\" Validate a model which does not expect external numpy data during training.\n-\n-    Keras does not expect external numpy data at training time, and thus cannot\n-    accept numpy arrays for validation when all of a Keras Model's\n-    `Input(input_tensor)` layers are provided an  `input_tensor` parameter,\n-    and the call to `Model.compile(target_tensors)` defines all `target_tensors`.\n-    Instead, create a second model for validation which is also configured\n-    with input tensors and add it to the `EvaluateInputTensor` callback\n-    to perform validation.\n-\n-    It is recommended that this callback be the first in the list of callbacks\n-    because it defines the validation variables required by many other callbacks,\n-    and Callbacks are made in order.\n-\n-    # Arguments\n-        model: Keras model on which to call model.evaluate().\n-        steps: Integer or `None`.\n-            Total number of steps (batches of samples)\n-            before declaring the evaluation round finished.\n-            Ignored with the default value of `None`.\n-    \"\"\"\n-\n-    def __init__(self, model, steps, metrics_prefix='val', verbose=1):\n-        # parameter of callbacks passed during initialization\n-        # pass evalation mode directly\n-        super(EvaluateInputTensor, self).__init__()\n-        self.val_model = model\n-        self.num_steps = steps\n-        self.verbose = verbose\n-        self.metrics_prefix = metrics_prefix\n-\n-    def on_epoch_end(self, epoch, logs={}):\n-        self.val_model.set_weights(self.model.get_weights())\n-        results = self.val_model.evaluate(None, None, steps=int(self.num_steps),\n-                                          verbose=self.verbose)\n-        metrics_str = '\\n'\n-        for result, name in zip(results, self.val_model.metrics_names):\n-            metric_name = self.metrics_prefix + '_' + name\n-            logs[metric_name] = result\n-            if self.verbose > 0:\n-                metrics_str = metrics_str + metric_name + ': ' + str(result) + ' '\n-\n-        if self.verbose > 0:\n-            print(metrics_str)\n-\n-\n-def cnn_layers(x_train_input):\n-    x = layers.Conv2D(32, (3, 3),\n-                      activation='relu', padding='valid')(x_train_input)\n-    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n-    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n-    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n-    x = layers.Flatten()(x)\n-    x = layers.Dense(512, activation='relu')(x)\n-    x = layers.Dropout(0.5)(x)\n-    x_train_out = layers.Dense(num_classes,\n-                               activation='softmax',\n-                               name='x_train_out')(x)\n-    return x_train_out\n-\n-sess = K.get_session()\n-\n-batch_size = 100\n-batch_shape = (batch_size, 28, 28, 1)\n-epochs = 5\n-num_classes = 10\n-\n-# The capacity variable controls the maximum queue size\n-# allowed when prefetching data for training.\n-capacity = 10000\n-\n-# min_after_dequeue is the minimum number elements in the queue\n-# after a dequeue, which ensures sufficient mixing of elements.\n-min_after_dequeue = 3000\n-\n-# If `enqueue_many` is `False`, `tensors` is assumed to represent a\n-# single example.  An input tensor with shape `[x, y, z]` will be output\n-# as a tensor with shape `[batch_size, x, y, z]`.\n-#\n-# If `enqueue_many` is `True`, `tensors` is assumed to represent a\n-# batch of examples, where the first dimension is indexed by example,\n-# and all members of `tensors` should have the same size in the\n-# first dimension.  If an input tensor has shape `[*, x, y, z]`, the\n-# output will have shape `[batch_size, x, y, z]`.\n-enqueue_many = True\n-\n-cache_dir = os.path.expanduser(\n-    os.path.join('~', '.keras', 'datasets', 'MNIST-data'))\n-data = mnist.read_data_sets(cache_dir, validation_size=0)\n-\n-x_train_batch, y_train_batch = tf.train.shuffle_batch(\n-    tensors=[data.train.images, data.train.labels.astype(np.int32)],\n-    batch_size=batch_size,\n-    capacity=capacity,\n-    min_after_dequeue=min_after_dequeue,\n-    enqueue_many=enqueue_many,\n-    num_threads=8)\n-\n-x_train_batch = tf.cast(x_train_batch, tf.float32)\n-x_train_batch = tf.reshape(x_train_batch, shape=batch_shape)\n-\n-y_train_batch = tf.cast(y_train_batch, tf.int32)\n-y_train_batch = tf.one_hot(y_train_batch, num_classes)\n-\n-x_batch_shape = x_train_batch.get_shape().as_list()\n-y_batch_shape = y_train_batch.get_shape().as_list()\n-\n-model_input = layers.Input(tensor=x_train_batch)\n-model_output = cnn_layers(model_input)\n-train_model = keras.models.Model(inputs=model_input, outputs=model_output)\n-\n-# Pass the target tensor `y_train_batch` to `compile`\n-# via the `target_tensors` keyword argument:\n-train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n-                    loss='categorical_crossentropy',\n-                    metrics=['accuracy'],\n-                    target_tensors=[y_train_batch])\n-train_model.summary()\n-\n-x_test_batch, y_test_batch = tf.train.batch(\n-    tensors=[data.test.images, data.test.labels.astype(np.int32)],\n-    batch_size=batch_size,\n-    capacity=capacity,\n-    enqueue_many=enqueue_many,\n-    num_threads=8)\n-\n-# Create a separate test model\n-# to perform validation during training\n-x_test_batch = tf.cast(x_test_batch, tf.float32)\n-x_test_batch = tf.reshape(x_test_batch, shape=batch_shape)\n-\n-y_test_batch = tf.cast(y_test_batch, tf.int32)\n-y_test_batch = tf.one_hot(y_test_batch, num_classes)\n-\n-x_test_batch_shape = x_test_batch.get_shape().as_list()\n-y_test_batch_shape = y_test_batch.get_shape().as_list()\n-\n-test_model_input = layers.Input(tensor=x_test_batch)\n-test_model_output = cnn_layers(test_model_input)\n-test_model = keras.models.Model(inputs=test_model_input, outputs=test_model_output)\n-\n-# Pass the target tensor `y_test_batch` to `compile`\n-# via the `target_tensors` keyword argument:\n-test_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n-                   loss='categorical_crossentropy',\n-                   metrics=['accuracy'],\n-                   target_tensors=[y_test_batch])\n-\n-# Fit the model using data from the TFRecord data tensors.\n-coord = tf.train.Coordinator()\n-threads = tf.train.start_queue_runners(sess, coord)\n-\n-train_model.fit(\n-    epochs=epochs,\n-    steps_per_epoch=int(np.ceil(data.train.num_examples / float(batch_size))),\n-    callbacks=[EvaluateInputTensor(test_model, steps=100)])\n-\n-# Save the model weights.\n-train_model.save_weights('saved_wt.h5')\n-\n-# Clean up the TF session.\n-coord.request_stop()\n-coord.join(threads)\n-K.clear_session()\n-\n-# Second Session to test loading trained model without tensors\n-x_test = np.reshape(data.test.images, (data.test.images.shape[0], 28, 28, 1))\n-y_test = data.test.labels\n-x_test_inp = layers.Input(shape=(x_test.shape[1:]))\n-test_out = cnn_layers(x_test_inp)\n-test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out)\n-\n-test_model.load_weights('saved_wt.h5')\n-test_model.compile(optimizer='rmsprop',\n-                   loss='categorical_crossentropy',\n-                   metrics=['accuracy'])\n-test_model.summary()\n-\n-loss, acc = test_model.evaluate(x_test,\n-                                keras.utils.to_categorical(y_test),\n-                                batch_size=batch_size)\n-print('\\nTest accuracy: {0}'.format(acc))\n\n@@ -1,96 +0,0 @@\n-'''Trains a simple convnet on the MNIST dataset and embeds test data.\n-\n-The test data is embedded using the weights of the final dense layer, just\n-before the classification head. This embedding can then be visualized using\n-TensorBoard's Embedding Projector.\n-'''\n-\n-from __future__ import print_function\n-\n-from os import makedirs\n-from os.path import exists, join\n-\n-import keras\n-from keras.callbacks import TensorBoard\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Flatten\n-from keras.layers import Conv2D, MaxPooling2D\n-from keras import backend as K\n-\n-import numpy as np\n-\n-batch_size = 128\n-num_classes = 10\n-epochs = 12\n-log_dir = './logs'\n-\n-if not exists(log_dir):\n-    makedirs(log_dir)\n-\n-# input image dimensions\n-img_rows, img_cols = 28, 28\n-\n-# the data, split between train and test sets\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-\n-if K.image_data_format() == 'channels_first':\n-    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n-    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n-    input_shape = (1, img_rows, img_cols)\n-else:\n-    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n-    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n-    input_shape = (img_rows, img_cols, 1)\n-\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-print('x_train shape:', x_train.shape)\n-print(x_train.shape[0], 'train samples')\n-print(x_test.shape[0], 'test samples')\n-\n-# save class labels to disk to color data points in TensorBoard accordingly\n-with open(join(log_dir, 'metadata.tsv'), 'w') as f:\n-    np.savetxt(f, y_test)\n-\n-# convert class vectors to binary class matrices\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-tensorboard = TensorBoard(batch_size=batch_size,\n-                          embeddings_freq=1,\n-                          embeddings_layer_names=['features'],\n-                          embeddings_metadata='metadata.tsv',\n-                          embeddings_data=x_test)\n-\n-model = Sequential()\n-model.add(Conv2D(32, kernel_size=(3, 3),\n-                 activation='relu',\n-                 input_shape=input_shape))\n-model.add(Conv2D(64, (3, 3), activation='relu'))\n-model.add(MaxPooling2D(pool_size=(2, 2)))\n-model.add(Dropout(0.25))\n-model.add(Flatten())\n-model.add(Dense(128, activation='relu', name='features'))\n-model.add(Dropout(0.5))\n-model.add(Dense(num_classes, activation='softmax'))\n-\n-model.compile(loss=keras.losses.categorical_crossentropy,\n-              optimizer=keras.optimizers.Adadelta(),\n-              metrics=['accuracy'])\n-\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          callbacks=[tensorboard],\n-          epochs=epochs,\n-          verbose=1,\n-          validation_data=(x_test, y_test))\n-score = model.evaluate(x_test, y_test, verbose=0)\n-print('Test loss:', score[0])\n-print('Test accuracy:', score[1])\n-\n-# You can now launch tensorboard with `tensorboard --logdir=./logs` on your\n-# command line and then go to http://localhost:6006/#projector to view the\n-# embeddings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
