{"custom_id": "scrapy#37760eb13fba85bc592f9b5169475f0d23476cb6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 22 | Churn Cumulative: 367 | Contributors (this commit): 3 | Commits (past 90d): 49 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -69,11 +69,11 @@ class ScrapyCommand(object):\n     def process_options(self, args, opts):\n         if opts.logfile:\n             settings.overrides['LOG_ENABLED'] = True\n-            settings.overrides['LOGFILE'] = opts.logfile\n+            settings.overrides['LOG_FILE'] = opts.logfile\n \n         if opts.loglevel:\n             settings.overrides['LOG_ENABLED'] = True\n-            settings.overrides['LOGLEVEL'] = opts.loglevel\n+            settings.overrides['LOG_LEVEL'] = opts.loglevel\n \n         if opts.nolog:\n             settings.overrides['LOG_ENABLED'] = False\n\n@@ -1 +1 @@\n-LOGLEVEL='WARNING'\n+LOG_LEVEL='WARNING'\n\n@@ -103,8 +103,8 @@ ITEM_PIPELINES = []\n \n LOG_ENABLED = True\n LOG_STDOUT = False\n-LOGLEVEL = 'DEBUG'\n-LOGFILE = None\n+LOG_LEVEL = 'DEBUG'\n+LOG_FILE = None\n \n MAIL_HOST = 'localhost'\n MAIL_FROM = 'scrapy@localhost'\n\n@@ -34,12 +34,12 @@ log_level = DEBUG\n \n started = False\n \n-def start(logfile=None, loglevel=None, log_stdout=None):\n+def start(logfile=None, loglevel=None, logstdout=None):\n     \"\"\"Initialize and start logging facility\"\"\"\n     global log_level, started\n \n     # set loglevel\n-    loglevel = loglevel or settings['LOGLEVEL']\n+    loglevel = loglevel or settings['LOG_LEVEL'] or settings['LOGLEVEL']\n     log_level = globals()[loglevel] if loglevel else DEBUG\n     if started or not settings.getbool('LOG_ENABLED'):\n         return\n@@ -47,11 +47,11 @@ def start(logfile=None, loglevel=None, log_stdout=None):\n \n     # set log observer\n     if log.defaultObserver: # check twisted log not already started\n-        logfile = logfile or settings['LOGFILE']\n-        log_stdout = log_stdout or settings.getbool('LOG_STDOUT')\n+        logfile = logfile or settings['LOG_FILE'] or settings['LOGFILE']\n+        logstdout = logstdout or settings.getbool('LOG_STDOUT')\n \n         file = open(logfile, 'a') if logfile else sys.stderr\n-        log.startLogging(file, setStdout=log_stdout)\n+        log.startLogging(file, setStdout=logstdout)\n \n def msg(message, level=INFO, component=BOT_NAME, domain=None):\n     \"\"\"Log message according to the level\"\"\"\n\n@@ -19,7 +19,7 @@ class CoreStats(object):\n     def __init__(self):\n         stats.set_value('envinfo/user', getpass.getuser())\n         stats.set_value('envinfo/host', socket.gethostname())\n-        stats.set_value('envinfo/logfile', settings['LOGFILE'])\n+        stats.set_value('envinfo/logfile', settings['LOG_FILE'])\n         stats.set_value('envinfo/pid', os.getpid())\n \n         dispatcher.connect(self.stats_domain_opened, signal=stats_domain_opened)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#daea3f8a0c3fdfe2566b211cab65448869e8fa08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,18 +4,8 @@ def setup(app):\n         rolename      = \"setting\",\n         indextemplate = \"pair: %s; setting\",\n     )\n-    app.add_crossref_type(\n-        directivename = \"topic\",\n-        rolename      = \"topic\",\n-        indextemplate = \"pair: %s; topic\",\n-    )\n     app.add_crossref_type(\n         directivename = \"signal\",\n         rolename      = \"signal\",\n         indextemplate = \"pair: %s; signal\",\n     )\n-    app.add_crossref_type(\n-        directivename = \"exception\",\n-        rolename      = \"exception\",\n-        indextemplate = \"pair: %s; exception\",\n-    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#505078738088c07d6db5ea1eabf836140cfd0e1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 55 | Churn Cumulative: 158 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.12\n\nDIFF:\n@@ -1,12 +1,18 @@\n import shutil\n import string\n+from os import listdir\n from os.path import join, dirname, abspath, exists\n \n+import scrapy\n from scrapy.spider import spiders\n from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n from scrapy.utils.template import render_templatefile, string_camelcase\n \n+\n+SPIDER_TEMPLATES_PATH = join(scrapy.__path__[0], 'templates', 'spiders')\n+\n+\n def sanitize_module_name(module_name):\n     \"\"\"Sanitize the given module name, by replacing dashes with underscores and\n     prefixing it with a letter if it doesn't start with one\n@@ -22,26 +28,32 @@ class Command(ScrapyCommand):\n         return \"[options] <spider_module_name> <spider_domain_name>\"\n \n     def short_desc(self):\n-        return \"Generate new spider based on template passed with --template\"\n+        return \"Generate new spider based on template passed with -t or --template\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--template\", dest=\"template\", default=\"crawl\",\n+        parser.add_option(\"--list\", dest=\"list\", action=\"store_true\")\n+        parser.add_option(\"--dump\", dest=\"dump\", action=\"store_true\")\n+        parser.add_option(\"-t\", \"--template\", dest=\"template\", default=\"crawl\",\n             help=\"Uses a custom template.\")\n         parser.add_option(\"--force\", dest=\"force\", action=\"store_true\",\n             help=\"If the spider already exists, overwrite it with the template\")\n \n     def run(self, args, opts):\n+        if opts.list:\n+            self._list_templates()\n+            return\n+\n+        if opts.dump:\n+            template_file = self._find_template(opts.template)\n+            if template_file:\n+                template = open(template_file, 'r')\n+                print template.read() \n+            return\n+\n         if len(args) < 2:\n             return False\n \n-        template_file = join(settings['TEMPLATES_DIR'], 'spider_%s.tmpl' % \\\n-            opts.template)\n-        if not exists(template_file):\n-            print \"Unable to create spider: template %r not found.\" % opts.template\n-            print \"Use genspider --list to see all available templates.\"\n-            return\n-\n         module = sanitize_module_name(args[0])\n         domain = args[1]\n         spider = spiders.fromdomain(domain)\n@@ -49,6 +61,9 @@ class Command(ScrapyCommand):\n             print \"Spider '%s' already exists in module:\" % domain\n             print \"  %s\" % spider.__module__\n             return\n+\n+        template_file = self._find_template(opts.template)\n+        if template_file:\n             self._genspider(module, domain, opts.template, template_file)\n \n     def _genspider(self, module, domain, template_name, template_file):\n@@ -71,3 +86,25 @@ class Command(ScrapyCommand):\n         print \"Created spider %r using template %r in module:\" % (domain, \\\n             template_name)\n         print \"  %s.%s\" % (spiders_module.__name__, module)\n+\n+    def _find_template(self, template):\n+        template_file = join(settings['TEMPLATES_DIR'], '%s.tmpl' % template)\n+        if not exists(template_file):\n+            template_file = join(SPIDER_TEMPLATES_PATH, '%s.tmpl' % template)\n+            if not exists(template_file):\n+                print \"Unable to find template %r.\" \\\n+                        % template\n+                print \"Use genspider --list to see all available templates.\"\n+                return None\n+        return template_file\n+\n+    def _list_templates(self):\n+        files = set()\n+        if exists(settings['TEMPLATES_DIR']):\n+            files.update(listdir(settings['TEMPLATES_DIR']))\n+        files.update(listdir(SPIDER_TEMPLATES_PATH))\n+\n+        for filename in sorted(files):\n+            if filename.endswith('.tmpl'):\n+                print filename\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#07c1f3b8d8edbe50d3333c9fb53382637a0e994c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 28 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -55,8 +55,8 @@ class XMLFeedSpider(InitSpider):\n         containing any of them.\n         \"\"\"\n \n-        for xSel in nodes:\n-            ret = self.parse_item(response, xSel)\n+        for selector in nodes:\n+            ret = self.parse_item(response, selector)\n             if isinstance(ret, (BaseItem, Request)):\n                 ret = [ret]\n             if not isinstance(ret, (list, tuple)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9635a7839c980615228599fc11993303e11f6c2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 16 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -38,7 +38,7 @@ source_suffix = '.rst'\n #source_encoding = 'utf-8'\n \n # The master toctree document.\n-master_doc = 'contents'\n+master_doc = 'index'\n \n # General information about the project.\n project = u'Scrapy'\n@@ -130,9 +130,7 @@ html_use_smartypants = True\n \n # Additional templates that should be rendered to pages, maps page names to\n # template names.\n-html_additional_pages = {\n-    'index': 'index.html'\n-}\n+#html_additional_pages = {}\n \n # If false, no module index is generated.\n #html_use_modindex = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#825cec51ff84bcb1e6318f7c4cfeda35860f4d84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 41 | Churn Cumulative: 284 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,20 +53,18 @@ class StatsCollector(object):\n         send_catch_log(stats_domain_opened, domain=domain)\n \n     def close_domain(self, domain, reason):\n-        if self._dump:\n-            log.msg(\"Dumping stats:\\n\" + pprint.pformat(self.get_stats(domain)), \\\n-                domain=domain)\n+        send_catch_log(stats_domain_closing, domain=domain, reason=reason)\n         stats = self._stats.pop(domain)\n         send_catch_log(stats_domain_closed, domain=domain, reason=reason, \\\n             domain_stats=stats)\n+        if self._dump:\n+            log.msg(\"Dumping domain stats:\\n\" + pprint.pformat(stats), \\\n+                domain=domain)\n \n     def engine_stopped(self):\n         if self._dump:\n             log.msg(\"Dumping global stats:\\n\" + pprint.pformat(self.get_stats()))\n \n-    def _start_closing_domain(self, domain, reason):\n-        send_catch_log(stats_domain_closing, domain=domain, reason=reason)\n-        self.close_domain(domain, reason)\n \n class MemoryStatsCollector(StatsCollector):\n \n\n@@ -1,6 +1,9 @@\n import unittest \n \n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy.stats.collector import StatsCollector, DummyStatsCollector\n+from scrapy.stats.signals import stats_domain_opened, stats_domain_closing, \\\n+    stats_domain_closed\n \n class StatsCollectorTest(unittest.TestCase):\n \n@@ -45,5 +48,33 @@ class StatsCollectorTest(unittest.TestCase):\n         self.assertEqual(stats.get_stats(), {})\n         self.assertEqual(stats.get_stats('a'), {})\n \n+    def test_signals(self):\n+        signals_catched = set()\n+\n+        def domain_open(domain):\n+            assert domain == 'example.com'\n+            signals_catched.add(stats_domain_opened)\n+        def domain_closing(domain, reason):\n+            assert domain == 'example.com'\n+            assert reason == 'testing'\n+            signals_catched.add(stats_domain_closing)\n+        def domain_closed(domain, reason, domain_stats):\n+            assert domain == 'example.com'\n+            assert reason == 'testing'\n+            assert domain_stats == {'test': 1}\n+            signals_catched.add(stats_domain_closed)\n+\n+        dispatcher.connect(domain_open, signal=stats_domain_opened)\n+        dispatcher.connect(domain_closing, signal=stats_domain_closing)\n+        dispatcher.connect(domain_closed, signal=stats_domain_closed)\n+\n+        stats = StatsCollector()\n+        stats.open_domain('example.com')\n+        stats.set_value('test', 1, domain='example.com')\n+        stats.close_domain('example.com', 'testing')\n+        assert stats_domain_opened in signals_catched\n+        assert stats_domain_closing in signals_catched\n+        assert stats_domain_closed in signals_catched\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f204da82d16a956d202a4e185780a480519b02fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 138 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,10 +54,12 @@ class StatsCollectorTest(unittest.TestCase):\n         def domain_open(domain):\n             assert domain == 'example.com'\n             signals_catched.add(stats_domain_opened)\n+\n         def domain_closing(domain, reason):\n             assert domain == 'example.com'\n             assert reason == 'testing'\n             signals_catched.add(stats_domain_closing)\n+\n         def domain_closed(domain, reason, domain_stats):\n             assert domain == 'example.com'\n             assert reason == 'testing'\n@@ -76,5 +78,9 @@ class StatsCollectorTest(unittest.TestCase):\n         assert stats_domain_closing in signals_catched\n         assert stats_domain_closed in signals_catched\n \n+        dispatcher.disconnect(domain_open, signal=stats_domain_opened)\n+        dispatcher.disconnect(domain_closing, signal=stats_domain_closing)\n+        dispatcher.disconnect(domain_closed, signal=stats_domain_closed)\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#499dc6677cd53ba176fa14cb691f2d8b35773e0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 55 | Churn Cumulative: 68 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,42 @@\n+import unittest\n+\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.utils.signal import send_catch_log\n+from scrapy import log\n+\n+test_signal = object()\n+\n+class SignalUtilsTest(unittest.TestCase):\n+\n+    def test_send_catch_log(self):\n+        handlers_called = set()\n+\n+        def test_handler_error(arg):\n+            handlers_called.add(test_handler_error)\n+            a = 1/0\n+\n+        def test_handler_check(arg):\n+            handlers_called.add(test_handler_check)\n+            assert arg == 'test'\n+\n+        def log_received(message, level):\n+            handlers_called.add(log_received)\n+            assert \"test_handler_error\" in message\n+            assert level == log.ERROR\n+\n+        dispatcher.connect(log_received, signal=log.logmessage_received)\n+        dispatcher.connect(test_handler_error, signal=test_signal)\n+        dispatcher.connect(test_handler_check, signal=test_signal)\n+        send_catch_log(test_signal, arg='test')\n+\n+        assert test_handler_error in handlers_called\n+        assert test_handler_check in handlers_called\n+        assert log_received in handlers_called\n+\n+        dispatcher.disconnect(log_received, signal=log.logmessage_received)\n+        dispatcher.disconnect(test_handler_error, signal=test_signal)\n+        dispatcher.disconnect(test_handler_check, signal=test_signal)\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -1,13 +1,14 @@\n \"\"\"Helper functinos for working with signals\"\"\"\n \n from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.xlib.pydispatch.robust import sendRobust\n from scrapy import log\n \n def send_catch_log(*args, **kwargs):\n-    \"\"\"Same as dispatcher.send but logs any exceptions raised by the signal\n-    handlers\n+    \"\"\"Same as dispatcher.robust.sendRobust but logs any exceptions raised by\n+    the signal handlers\n     \"\"\"\n-    try:\n-        dispatcher.send(*args, **kwargs)\n-    except:\n-        log.exc(\"Exception catched on signal dispatch\")\n+    for receiver, result in sendRobust(*args, **kwargs):\n+        if isinstance(result, Exception):\n+            log.msg(\"Exception caught on signal dispatch: receiver=%r, \" \\\n+                \" exception=%r\" % (receiver, result), level=log.ERROR)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de899091954686f6b898d784fdaca93428148083", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 82 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,6 +18,7 @@ class SignalUtilsTest(unittest.TestCase):\n         def test_handler_check(arg):\n             handlers_called.add(test_handler_check)\n             assert arg == 'test'\n+            return \"OK\"\n \n         def log_received(message, level):\n             handlers_called.add(log_received)\n@@ -27,11 +28,14 @@ class SignalUtilsTest(unittest.TestCase):\n         dispatcher.connect(log_received, signal=log.logmessage_received)\n         dispatcher.connect(test_handler_error, signal=test_signal)\n         dispatcher.connect(test_handler_check, signal=test_signal)\n-        send_catch_log(test_signal, arg='test')\n+        result = send_catch_log(test_signal, arg='test')\n \n         assert test_handler_error in handlers_called\n         assert test_handler_check in handlers_called\n         assert log_received in handlers_called\n+        self.assertEqual(result[0][0], test_handler_error)\n+        self.assert_(isinstance(result[0][1], Exception))\n+        self.assertEqual(result[1], (test_handler_check, \"OK\"))\n \n         dispatcher.disconnect(log_received, signal=log.logmessage_received)\n         dispatcher.disconnect(test_handler_error, signal=test_signal)\n\n@@ -8,7 +8,9 @@ def send_catch_log(*args, **kwargs):\n     \"\"\"Same as dispatcher.robust.sendRobust but logs any exceptions raised by\n     the signal handlers\n     \"\"\"\n-    for receiver, result in sendRobust(*args, **kwargs):\n-        if isinstance(result, Exception):\n+    results = sendRobust(*args, **kwargs)\n+    for receiver, response in results:\n+        if isinstance(response, Exception):\n             log.msg(\"Exception caught on signal dispatch: receiver=%r, \" \\\n-                \" exception=%r\" % (receiver, result), level=log.ERROR)\n+                \" exception=%r\" % (receiver, response), level=log.ERROR)\n+    return results\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0ed849248ff572eb502853d5d6b8c7fe03cc11b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 149 | Lines Deleted: 126 | Files Changed: 3 | Hunks: 21 | Methods Changed: 24 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 275 | Churn Cumulative: 818 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,20 +11,38 @@ from xml.sax.saxutils import XMLGenerator\n __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter', \\\n     'CsvItemExporter', 'XmlItemExporter']\n \n-identity = lambda x: x\n-\n class BaseItemExporter(object):\n \n-    fields_to_export = None\n-    export_empty_fields = False\n+    def __init__(self, **kwargs):\n+        self._configure(kwargs)\n+\n+    def _configure(self, options, dont_fail=False):\n+        \"\"\"Configure the exporter by poping options from the ``options`` dict.\n+        If dont_fail is set, it won't raise an exception on unexpected options\n+        (useful for using with keyword arguments in subclasses constructors)\n+        \"\"\"\n+        self.fields_to_export = options.pop('fields_to_export', None)\n+        self.export_empty_fields = options.pop('export_empty_fields', False)\n+        self.encoding = options.pop('encoding', 'utf-8')\n+        if not dont_fail and options:\n+            raise TypeError(\"Unexpected options: %s\" % ', '.join(options.keys()))\n \n     def export_item(self, item):\n         raise NotImplementedError\n \n     def serialize_field(self, field, name, value):\n-        serializer = field.get('serializer', identity)\n+        serializer = field.get('serializer', self._to_str_if_unicode)\n         return serializer(value)\n \n+    def start_exporting(self):\n+        pass\n+\n+    def finish_exporting(self):\n+        pass\n+\n+    def _to_str_if_unicode(self, value):\n+        return value.encode(self.encoding) if isinstance(value, unicode) else value\n+\n     def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n         \"\"\"Return the fields to export as an iterable of tuples (name,\n         serialized_value)\n@@ -52,22 +70,14 @@ class BaseItemExporter(object):\n \n             yield field_name, value\n \n-    def start_exporting(self):\n-        pass\n-\n-    def finish_exporting(self):\n-        pass\n-\n-\n \n class XmlItemExporter(BaseItemExporter):\n \n-    item_element = 'item'\n-    root_element = 'items'\n-\n-    def __init__(self, file):\n-        super(XmlItemExporter, self).__init__()\n-        self.xg = XMLGenerator(file)\n+    def __init__(self, file, **kwargs):\n+        self.item_element = kwargs.pop('item_element', 'item')\n+        self.root_element = kwargs.pop('root_element', 'items')\n+        self._configure(kwargs)\n+        self.xg = XMLGenerator(file, encoding=self.encoding)\n \n     def start_exporting(self):\n         self.xg.startDocument()\n@@ -91,17 +101,16 @@ class XmlItemExporter(BaseItemExporter):\n \n class CsvItemExporter(BaseItemExporter):\n \n-    include_headers_line = False\n-\n-    def __init__(self, *args, **kwargs):\n-        super(CsvItemExporter, self).__init__()\n-        self.csv_writer = csv.writer(*args, **kwargs)\n+    def __init__(self, file, include_headers_line=False, **kwargs):\n+        self._configure(kwargs, dont_fail=True)\n+        self.include_headers_line = include_headers_line\n+        self.csv_writer = csv.writer(file, **kwargs)\n \n     def start_exporting(self):\n         if self.include_headers_line:\n             if not self.fields_to_export:\n-                raise RuntimeError(\"You must set fields_to_export in order to\" + \\\n-                                   \" use include_headers_line\")\n+                raise RuntimeError(\"You must set fields_to_export in order\" + \\\n+                                   \" to use include_headers_line\")\n             self.csv_writer.writerow(self.fields_to_export)\n \n     def export_item(self, item):\n@@ -114,9 +123,9 @@ class CsvItemExporter(BaseItemExporter):\n \n class PickleItemExporter(BaseItemExporter):\n \n-    def __init__(self, *args, **kwargs):\n-        super(PickleItemExporter, self).__init__()\n-        self.pickler = Pickler(*args, **kwargs)\n+    def __init__(self, file, protocol=0, **kwargs):\n+        self._configure(kwargs)\n+        self.pickler = Pickler(file, protocol)\n \n     def export_item(self, item):\n         self.pickler.dump(dict(self._get_serialized_fields(item)))\n@@ -124,8 +133,8 @@ class PickleItemExporter(BaseItemExporter):\n \n class PprintItemExporter(BaseItemExporter):\n \n-    def __init__(self, file):\n-        super(PprintItemExporter, self).__init__()\n+    def __init__(self, file, **kwargs):\n+        self._configure(kwargs)\n         self.file = file\n \n     def export_item(self, item):\n\n@@ -7,10 +7,10 @@ except ImportError:\n \n class JsonLinesItemExporter(BaseItemExporter):\n \n-    def __init__(self, file, *args, **kwargs):\n-        super(JsonLinesItemExporter, self).__init__()\n+    def __init__(self, file, **kwargs):\n+        self._configure(kwargs)\n         self.file = file\n-        self.encoder = json.JSONEncoder(*args, **kwargs)\n+        self.encoder = json.JSONEncoder(**kwargs)\n \n     def export_item(self, item):\n         itemdict = dict(self._get_serialized_fields(item))\n\n@@ -4,6 +4,7 @@ from cStringIO import StringIO\n from twisted.trial import unittest\n \n from scrapy.item import Item, Field\n+from scrapy.utils.python import str_to_unicode\n from scrapy.contrib.exporter import BaseItemExporter, PprintItemExporter, \\\n     PickleItemExporter, CsvItemExporter, XmlItemExporter\n \n@@ -15,16 +16,21 @@ class TestItem(Item):\n class BaseItemExporterTest(unittest.TestCase):\n \n     def setUp(self):\n-        self.i = TestItem(name=u'John', age='22')\n+        self.i = TestItem(name=u'John\\xa3', age='22')\n         self.output = StringIO()\n         self.ie = self._get_exporter()\n \n-    def _get_exporter(self):\n-        return BaseItemExporter()\n+    def _get_exporter(self, **kwargs):\n+        return BaseItemExporter(**kwargs)\n \n     def _check_output(self):\n         pass\n \n+    def _assert_expected_item(self, exported_dict):\n+        for k, v in exported_dict.items():\n+            exported_dict[k] = str_to_unicode(v)\n+        self.assertEqual(self.i, exported_dict)\n+\n     def test_export_item(self):\n         self.ie.start_exporting()\n         try:\n@@ -37,10 +43,104 @@ class BaseItemExporterTest(unittest.TestCase):\n \n     def test_serialize_field(self):\n         self.assertEqual(self.ie.serialize_field( \\\n-            self.i.fields['name'], 'name', self.i['name']), 'John')\n+            self.i.fields['name'], 'name', self.i['name']), 'John\\xc2\\xa3')\n         self.assertEqual( \\\n             self.ie.serialize_field(self.i.fields['age'], 'age', self.i['age']), '22')\n \n+    def test_fields_to_export(self):\n+        ie = self._get_exporter(fields_to_export=['name'])\n+        self.assertEqual(list(ie._get_serialized_fields(self.i)), [('name', 'John\\xc2\\xa3')])\n+\n+        ie = self._get_exporter(fields_to_export=['name'], encoding='latin-1')\n+        name = list(ie._get_serialized_fields(self.i))[0][1]\n+        assert isinstance(name, str)\n+        self.assertEqual(name, 'John\\xa3')\n+\n+    def test_field_custom_serializer(self):\n+        def custom_serializer(value):\n+            return str(int(value) + 2)\n+\n+        class CustomFieldItem(Item):\n+            name = Field()\n+            age = Field(serializer=custom_serializer)\n+\n+        i = CustomFieldItem(name=u'John\\xa3', age='22')\n+\n+        ie = self._get_exporter()\n+        self.assertEqual(ie.serialize_field(i.fields['name'], 'name', i['name']), 'John\\xc2\\xa3')\n+        self.assertEqual(ie.serialize_field(i.fields['age'], 'age', i['age']), '24')\n+\n+\n+class PprintItemExporterTest(BaseItemExporterTest):\n+\n+    def _get_exporter(self, **kwargs):\n+        return PprintItemExporter(self.output, **kwargs)\n+\n+    def _check_output(self):\n+        self._assert_expected_item(eval(self.output.getvalue()))\n+\n+\n+class PickleItemExporterTest(BaseItemExporterTest):\n+\n+    def _get_exporter(self, **kwargs):\n+        return PickleItemExporter(self.output, **kwargs)\n+\n+    def _check_output(self):\n+        self._assert_expected_item(pickle.loads(self.output.getvalue()))\n+\n+\n+class CsvItemExporterTest(BaseItemExporterTest):\n+\n+    def _get_exporter(self, **kwargs):\n+        return CsvItemExporter(self.output, **kwargs)\n+\n+    def _check_output(self):\n+        self.assertEqual(self.output.getvalue(), '22,John\\xc2\\xa3\\r\\n')\n+\n+    def test_header(self):\n+        output = StringIO()\n+        ie = CsvItemExporter(output, include_headers_line=True)\n+        self.assertRaises(RuntimeError, ie.start_exporting)\n+\n+        ie = CsvItemExporter(output, include_headers_line=True, \\\n+            fields_to_export=self.i.fields.keys())\n+        ie.start_exporting()\n+        ie.export_item(self.i)\n+        ie.finish_exporting()\n+        self.assertEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n+\n+\n+class XmlItemExporterTest(BaseItemExporterTest):\n+\n+    def _get_exporter(self, **kwargs):\n+        return XmlItemExporter(self.output, **kwargs)\n+\n+    def _check_output(self):\n+        expected_value = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><age>22</age><name>John\\xc2\\xa3</name></item></items>'\n+        self.assertEqual(self.output.getvalue(), expected_value)\n+\n+\n+class JsonLinesItemExporterTest(BaseItemExporterTest):\n+\n+    def _get_exporter(self, **kwargs):\n+        try:\n+            import json\n+        except ImportError:\n+            try:\n+                import simplejson\n+            except ImportError:\n+                raise unittest.SkipTest(\"simplejson module not available\")\n+        from scrapy.contrib.exporter.jsonlines import JsonLinesItemExporter\n+        return JsonLinesItemExporter(self.output, **kwargs)\n+\n+    def _check_output(self):\n+        import simplejson\n+        exported = simplejson.loads(self.output.getvalue().strip())\n+        self.assertEqual(exported, dict(self.i))\n+\n+\n+class CustomItemExporterTest(unittest.TestCase):\n+\n     def test_exporter_custom_serializer(self):\n         class CustomItemExporter(BaseItemExporter):\n             def serialize_field(self, field, name, value):\n@@ -50,99 +150,13 @@ class BaseItemExporterTest(unittest.TestCase):\n                     return super(CustomItemExporter, self).serialize_field(field, \\\n                         name, value)\n \n+        i = TestItem(name=u'John', age='22')\n         ie = CustomItemExporter()\n \n         self.assertEqual( \\\n-            ie.serialize_field(self.i.fields['name'], 'name', self.i['name']), 'John')\n+            ie.serialize_field(i.fields['name'], 'name', i['name']), 'John')\n         self.assertEqual(\n-            ie.serialize_field(self.i.fields['age'], 'age', self.i['age']), '23')\n-\n-    def test_field_custom_serializer(self):\n-        def custom_serializer(value):\n-            return str(int(value) + 2)\n-\n-        class CustomFieldItem(Item):\n-            name = Field()\n-            age = Field(serializer=custom_serializer)\n-\n-        i = CustomFieldItem(name=u'John', age='22')\n-\n-        self.assertEqual( \\\n-            self.ie.serialize_field(i.fields['name'], 'name', i['name']), 'John')\n-        self.assertEqual( \\\n-            self.ie.serialize_field(i.fields['age'], 'age', i['age']), '24')\n-\n-    def test_fields_to_export(self):\n-        ie = BaseItemExporter()\n-        ie.fields_to_export = ['name']\n-\n-        self.assertEqual(list(ie._get_serialized_fields(self.i)), [('name', 'John')])\n-\n-class PprintItemExporterTest(BaseItemExporterTest):\n-\n-    def _get_exporter(self):\n-        return PprintItemExporter(self.output)\n-\n-    def _check_output(self):\n-        self.assertEqual(dict(self.i), eval(self.output.getvalue()))\n-\n-\n-class PickleItemExporterTest(BaseItemExporterTest):\n-        \n-    def _get_exporter(self):\n-        return PickleItemExporter(self.output)\n-\n-    def _check_output(self):\n-        self.assertEqual(dict(self.i), pickle.loads(self.output.getvalue()))\n-\n-class CsvItemExporterTest(BaseItemExporterTest):\n-\n-    def _get_exporter(self):\n-        return CsvItemExporter(self.output)\n-\n-    def _check_output(self):\n-        self.assertEqual(self.output.getvalue(), '22,John\\r\\n')\n-\n-    def test_header(self):\n-        output = StringIO()\n-        ie = CsvItemExporter(output)\n-        ie.include_headers_line = True\n-\n-        self.assertRaises(RuntimeError, ie.start_exporting)\n-\n-        ie.fields_to_export = self.i.fields.keys()\n-        ie.start_exporting()\n-        ie.export_item(self.i)\n-        ie.finish_exporting()\n-\n-        self.assertEqual(output.getvalue(), 'age,name\\r\\n22,John\\r\\n')\n-\n-\n-class XmlItemExporterTest(BaseItemExporterTest):\n-\n-    def _get_exporter(self):\n-        return XmlItemExporter(self.output)\n-\n-    def _check_output(self):\n-        expected_value = '<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\\n<items><item><age>22</age><name>John</name></item></items>'\n-        self.assertEqual(self.output.getvalue(), expected_value)\n-\n-\n-class JsonLinesItemExporterTest(BaseItemExporterTest):\n-\n-    def _get_exporter(self):\n-        try:\n-            import json\n-        except ImportError:\n-            try:\n-                import simplejson\n-            except ImportError:\n-                raise unittest.SkipTest(\"simplejson module not available\") \n-        from scrapy.contrib.exporter.jsonlines import JsonLinesItemExporter\n-        return JsonLinesItemExporter(self.output)\n-\n-    def _check_output(self):\n-        self.assertEqual(self.output.getvalue(), '{\"age\": \"22\", \"name\": \"John\"}\\n')\n+            ie.serialize_field(i.fields['age'], 'age', i['age']), '23')\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e2bd49a81da6917a3e31da8ea4bdcdb174e1214", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 17 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 61 | Churn Cumulative: 101 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,9 +13,10 @@ def _parsed_url_args(parsed):\n     host = parsed.hostname\n     port = parsed.port\n     scheme = parsed.scheme\n+    netloc = parsed.netloc\n     if port is None:\n         port = 443 if scheme == 'https' else 80\n-    return scheme, host, port, path\n+    return scheme, netloc, host, port, path\n \n def _parse(url):\n     url = url.strip()\n@@ -82,16 +83,16 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         self.method = method\n         self.body = body or None\n         if parsedurl:\n-            self.scheme, self.host, self.port, self.path = _parsed_url_args(parsedurl)\n+            self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsedurl)\n         else:\n-            self.scheme, self.host, self.port, self.path = _parse(url)\n+            self.scheme, self.netloc, self.host, self.port, self.path = _parse(url)\n \n         self.timeout = timeout\n         self.headers = Headers(headers or {})\n         self.deferred = defer.Deferred()\n \n         # set Host header based on url\n-        self.headers.setdefault('Host', self.host)\n+        self.headers.setdefault('Host', self.netloc)\n \n         # set Content-Length based len of body\n         if self.body is not None:\n\n@@ -21,36 +21,37 @@ class ParseUrlTestCase(unittest.TestCase):\n \n     def _parse(self, url):\n         f = client.ScrapyHTTPClientFactory(url)\n-        return (f.scheme, f.host, f.port, f.path)\n+        return (f.scheme, f.netloc, f.host, f.port, f.path)\n \n     def testParse(self):\n+        lip = '127.0.0.1'\n         tests = (\n-                (\"http://127.0.0.1?c=v&c2=v2#fragment\",     ('http', '127.0.0.1', 80, '/?c=v&c2=v2')),\n-                (\"http://127.0.0.1/?c=v&c2=v2#fragment\",    ('http', '127.0.0.1', 80, '/?c=v&c2=v2')),\n-                (\"http://127.0.0.1/foo?c=v&c2=v2#frag\",     ('http', '127.0.0.1', 80, '/foo?c=v&c2=v2')),\n-                (\"http://127.0.0.1:100?c=v&c2=v2#fragment\", ('http', '127.0.0.1', 100, '/?c=v&c2=v2')),\n-                (\"http://127.0.0.1:100/?c=v&c2=v2#frag\",    ('http', '127.0.0.1', 100, '/?c=v&c2=v2')),\n-                (\"http://127.0.0.1:100/foo?c=v&c2=v2#frag\", ('http', '127.0.0.1', 100, '/foo?c=v&c2=v2')),\n+    (\"http://127.0.0.1?c=v&c2=v2#fragment\",     ('http', lip, lip, 80, '/?c=v&c2=v2')),\n+    (\"http://127.0.0.1/?c=v&c2=v2#fragment\",    ('http', lip, lip, 80, '/?c=v&c2=v2')),\n+    (\"http://127.0.0.1/foo?c=v&c2=v2#frag\",     ('http', lip, lip, 80, '/foo?c=v&c2=v2')),\n+    (\"http://127.0.0.1:100?c=v&c2=v2#fragment\", ('http', lip+':100', lip, 100, '/?c=v&c2=v2')),\n+    (\"http://127.0.0.1:100/?c=v&c2=v2#frag\",    ('http', lip+':100', lip, 100, '/?c=v&c2=v2')),\n+    (\"http://127.0.0.1:100/foo?c=v&c2=v2#frag\", ('http', lip+':100', lip, 100, '/foo?c=v&c2=v2')),\n \n-                (\"http://127.0.0.1\",                    ('http', '127.0.0.1', 80, '/')),\n-                (\"http://127.0.0.1/\",                   ('http', '127.0.0.1', 80, '/')),\n-                (\"http://127.0.0.1/foo\",                ('http', '127.0.0.1', 80, '/foo')),\n-                (\"http://127.0.0.1?param=value\",        ('http', '127.0.0.1', 80, '/?param=value')),\n-                (\"http://127.0.0.1/?param=value\",       ('http', '127.0.0.1', 80, '/?param=value')),\n-                (\"http://127.0.0.1:12345/foo\",          ('http', '127.0.0.1', 12345, '/foo')),\n-                (\"http://spam:12345/foo\",               ('http', 'spam', 12345, '/foo')),\n-                (\"http://spam.scrapytest.org/foo\",      ('http', 'spam.scrapytest.org', 80, '/foo')),\n+    (\"http://127.0.0.1\",              ('http', lip, lip, 80, '/')),\n+    (\"http://127.0.0.1/\",             ('http', lip, lip, 80, '/')),\n+    (\"http://127.0.0.1/foo\",          ('http', lip, lip, 80, '/foo')),\n+    (\"http://127.0.0.1?param=value\",  ('http', lip, lip, 80, '/?param=value')),\n+    (\"http://127.0.0.1/?param=value\", ('http', lip, lip, 80, '/?param=value')),\n+    (\"http://127.0.0.1:12345/foo\",    ('http', lip+':12345', lip, 12345, '/foo')),\n+    (\"http://spam:12345/foo\",         ('http', 'spam:12345', 'spam', 12345, '/foo')),\n+    (\"http://spam.test.org/foo\",      ('http', 'spam.test.org', 'spam.test.org', 80, '/foo')),\n \n-                (\"https://127.0.0.1/foo\",               ('https', '127.0.0.1', 443, '/foo')),\n-                (\"https://127.0.0.1/?param=value\",      ('https', '127.0.0.1', 443, '/?param=value')),\n-                (\"https://127.0.0.1:12345/\",            ('https', '127.0.0.1', 12345, '/')),\n+    (\"https://127.0.0.1/foo\",         ('https', lip, lip, 443, '/foo')),\n+    (\"https://127.0.0.1/?param=value\", ('https', lip, lip, 443, '/?param=value')),\n+    (\"https://127.0.0.1:12345/\",      ('https', lip+':12345', lip, 12345, '/')),\n \n-                (\"http://scrapytest.org/foo \",          ('http', 'scrapytest.org', 80, '/foo')),\n-                (\"http://egg:7890 \",                     ('http', 'egg', 7890, '/')),\n+    (\"http://scrapytest.org/foo \",    ('http', 'scrapytest.org', 'scrapytest.org', 80, '/foo')),\n+    (\"http://egg:7890 \",              ('http', 'egg:7890', 'egg', 7890, '/')),\n     )\n \n         for url, test in tests:\n-            self.assertEquals(self._parse(url), test, url)\n+            self.assertEquals(client._parse(url), test, url)\n \n     def test_externalUnicodeInterference(self):\n         \"\"\"\n@@ -61,8 +62,9 @@ class ParseUrlTestCase(unittest.TestCase):\n         badInput = u'http://example.com/path'\n         goodInput = badInput.encode('ascii')\n         urlparse(badInput)\n-        scheme, host, port, path = self._parse(goodInput)\n+        scheme, netloc, host, port, path = self._parse(goodInput)\n         self.assertTrue(isinstance(scheme, str))\n+        self.assertTrue(isinstance(netloc, str))\n         self.assertTrue(isinstance(host, str))\n         self.assertTrue(isinstance(path, str))\n         self.assertTrue(isinstance(port, int))\n@@ -204,7 +206,7 @@ class WebClientTestCase(unittest.TestCase):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n         return defer.gatherResults([\n-            client.getPage(self.getURL(\"host\")).addCallback(self.assertEquals, \"127.0.0.1\"),\n+            client.getPage(self.getURL(\"host\")).addCallback(self.assertEquals, \"127.0.0.1:%d\" % self.portno),\n             client.getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}).addCallback(self.assertEquals, \"www.example.com\")])\n \n \n@@ -238,7 +240,7 @@ class WebClientTestCase(unittest.TestCase):\n         called back with the contents of the page.\n         \"\"\"\n         d = client.getPage(self.getURL(\"host\"), timeout=100)\n-        d.addCallback(self.assertEquals, \"127.0.0.1\")\n+        d.addCallback(self.assertEquals, \"127.0.0.1:%d\" % self.portno)\n         return d\n \n \n@@ -271,7 +273,7 @@ class WebClientTestCase(unittest.TestCase):\n \n     def testFactoryInfo(self):\n         url = self.getURL('file')\n-        scheme, host, port, path = client._parse(url)\n+        scheme, netloc, host, port, path = client._parse(url)\n         factory = client.ScrapyHTTPClientFactory(url)\n         reactor.connectTCP(host, port, factory)\n         return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#73e7788ba2de6f133b06552f6a993398edf7db79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 408 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -128,7 +128,13 @@ class Scraper(object):\n \n     def call_spider(self, result, request, spider):\n         defer_result(result).chainDeferred(request.deferred)\n-        return request.deferred.addCallback(arg_to_iter)\n+        return request.deferred.addCallback(self._iterable_spider_output)\n+\n+    def _iterable_spider_output(self, result):\n+        if isinstance(result, (BaseItem, Request)):\n+            return [result]\n+        else:\n+            return arg_to_iter(result)\n \n     def handle_spider_error(self, _failure, request, spider, propagated_failure=None):\n         referer = request.headers.get('Referer', None)\n\n@@ -40,7 +40,7 @@ class TestSpider(BaseSpider):\n         m = self.price_re.search(response.body)\n         if m:\n             item['price'] = m.group(1)\n-        return [item]\n+        return item\n         \n \n SPIDER = TestSpider()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2cb0cfe7ea9acc91fe3da91d20c13d78c3ba1b4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 396 | Contributors (this commit): 3 | Commits (past 90d): 17 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,7 +5,7 @@ from twisted.python.failure import Failure\n from twisted.internet import defer\n \n from scrapy.utils.defer import defer_result, defer_succeed, parallel\n-from scrapy.utils.misc import arg_to_iter, load_object\n+from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log\n from scrapy.core.exceptions import IgnoreRequest, DropItem\n from scrapy.core import signals\n@@ -131,10 +131,7 @@ class Scraper(object):\n         return request.deferred.addCallback(self._iterable_spider_output)\n \n     def _iterable_spider_output(self, result):\n-        if isinstance(result, (BaseItem, Request)):\n-            return [result]\n-        else:\n-            return arg_to_iter(result)\n+        return [result] if isinstance(result, (BaseItem, Request)) else result\n \n     def handle_spider_error(self, _failure, request, spider, propagated_failure=None):\n         referer = request.headers.get('Referer', None)\n@@ -177,7 +174,7 @@ class Scraper(object):\n         elif output is None:\n             pass\n         else:\n-            log.msg(\"Spider must return Request, BaseItem or None, got '%s' in %s\" % \\\n+            log.msg(\"Spider must return Request, BaseItem or None, got %r in %s\" % \\\n                 (type(output).__name__, request), log.ERROR, domain=domain)\n \n     def _check_propagated_failure(self, spider_failure, propagated_failure, request, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7e916407df75a54f8b4e868abb35e852acfb813", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware\n from scrapy.conf import settings\n \n \n-class CookiesMiddlewareTest(TestCase):\n+class UserAgentMiddlewareTest(TestCase):\n \n     def setUp(self):\n         spiders.spider_modules = ['scrapy.tests.test_spiders']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0186c6937aac1fa294f22367822c98a4edbf202f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 28 | Churn Cumulative: 28 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,28 @@\n+import unittest\n+\n+from scrapy.http import Request\n+from scrapy.contrib.downloadermiddleware.httpauth import HttpAuthMiddleware\n+from scrapy.spider import BaseSpider\n+\n+class TestSpider(BaseSpider):\n+    http_user = 'foo'\n+    http_pass = 'bar'\n+\n+class HttpAuthMiddlewareTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.mw = HttpAuthMiddleware()\n+\n+    def tearDown(self):\n+        del self.mw\n+\n+    def test_auth(self):\n+        self.mw.default_useragent = 'default_useragent'\n+        spider = TestSpider()\n+        req = Request('http://scrapytest.org/')\n+        assert self.mw.process_request(req, spider) is None\n+        self.assertEquals(req.headers['Authorization'], 'Basic Zm9vOmJhcg==')\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
