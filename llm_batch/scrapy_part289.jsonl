{"custom_id": "scrapy#dad2ea75222d6240c569440d3221f5fc00925682", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 2948 | Contributors (this commit): 26 | Commits (past 90d): 21 | Contributors (cumulative): 43 | DMM Complexity: None\n\nDIFF:\n@@ -295,7 +295,7 @@ class FeedExporter:\n         :param uri: uri of the new batch to start\n         :param feed: dict with parameters of feed\n         :param spider: user spider\n-        :param template_uri: template uri which contains %(time_id)s or %(batch_id)s to create new uri\n+        :param template_uri: template uri which contains %(batch_time)s or %(batch_id)s to create new uri\n         \"\"\"\n         storage = self._get_storage(uri)\n         file = storage.open(spider)\n@@ -358,12 +358,12 @@ class FeedExporter:\n \n     def _batch_deliveries_supported(self, uri):\n         \"\"\"\n-        If FEED_STORAGE_BATCH_SIZE setting is specified uri has to contain %(time_id)s or %(batch_id)s\n+        If FEED_STORAGE_BATCH_SIZE setting is specified uri has to contain %(batch_time)s or %(batch_id)s\n         to distinguish different files of partial output\n         \"\"\"\n-        if self.storage_batch_size is None or '%(time_id)s' in uri or '%(batch_id)s' in uri:\n+        if self.storage_batch_size is None or '%(batch_time)s' in uri or '%(batch_id)s' in uri:\n             return True\n-        logger.warning('%(time_id)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_SIZE setting is specified')\n+        logger.warning('%(batch_time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_SIZE setting is specified')\n         return False\n \n     def _storage_supported(self, uri):\n@@ -396,7 +396,7 @@ class FeedExporter:\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n         params['time'] = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n-        params['time_id'] = datetime.utcnow().isoformat().replace(':', '-')\n+        params['batch_time'] = datetime.utcnow().isoformat().replace(':', '-')\n         params['batch_id'] = slot.batch_id + 1 if slot is not None else 1\n         uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n         uripar_function(params, spider)\n\n@@ -989,7 +989,7 @@ class FeedExportTest(FeedExportTestBase):\n \n class PartialDeliveriesTest(FeedExportTestBase):\n     __test__ = True\n-    _file_mark = '_%(time_id)s_#%(batch_id)s_'\n+    _file_mark = '_%(batch_time)s_#%(batch_id)s_'\n \n     @defer.inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n@@ -1146,7 +1146,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n         yield self.assertExported(items, header, rows, settings=settings)\n \n     def test_wrong_path(self):\n-        \"\"\" If path is without %(time_id)s or %(batch_id)s an exception must be raised \"\"\"\n+        \"\"\" If path is without %(batch_time)s or %(batch_id)s an exception must be raised \"\"\"\n         settings = {\n             'FEEDS': {\n                 self._random_temp_filename(): {'format': 'xml'},\n@@ -1236,7 +1236,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n     def test_batch_path_differ(self):\n         \"\"\"\n         Test that the name of all batch files differ from each other.\n-        So %(time_id)s replaced with the current date.\n+        So %(batch_time)s replaced with the current date.\n         \"\"\"\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n@@ -1245,7 +1245,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n         ]\n         settings = {\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), '%(time_id)s'): {\n+                os.path.join(self._random_temp_filename(), '%(batch_time)s'): {\n                     'format': 'json',\n                 },\n             },\n@@ -1281,7 +1281,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n         chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n         filename = ''.join(chars)\n         prefix = 'tmp/{filename}'.format(filename=filename)\n-        s3_test_file_uri = 's3://{bucket_name}/{prefix}/%(time_id)s.json'.format(\n+        s3_test_file_uri = 's3://{bucket_name}/{prefix}/%(batch_time)s.json'.format(\n             bucket_name=s3_test_bucket_name, prefix=prefix\n         )\n         storage = S3FeedStorage(s3_test_bucket_name, access_key, secret_key)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f75941f79dfeb931649f9e8880cdbd1b0a8e4681", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 32 | Churn Cumulative: 584 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,7 +28,12 @@ class BaseItem(object_ref):\n     Unlike instances of :class:`dict`, instances of :class:`BaseItem` may be\n     :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n     \"\"\"\n-    pass\n+\n+    def __new__(cls, *args, **kwargs):\n+        if issubclass(cls, BaseItem) and not (issubclass(cls, Item) or issubclass(cls, DictItem)):\n+            warn('scrapy.item.BaseItem is deprecated, please use scrapy.item.Item instead',\n+                 ScrapyDeprecationWarning, stacklevel=2)\n+        return super(BaseItem, cls).__new__(cls, *args, **kwargs)\n \n \n class Field(dict):\n\n@@ -4,7 +4,7 @@ from unittest import mock\n from warnings import catch_warnings\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.item import ABCMeta, DictItem, Field, Item, ItemMeta\n+from scrapy.item import ABCMeta, BaseItem, DictItem, Field, Item, ItemMeta\n \n \n PY36_PLUS = (sys.version_info.major >= 3) and (sys.version_info.minor >= 6)\n@@ -131,12 +131,12 @@ class ItemTest(unittest.TestCase):\n         self.assertSortedEqual(list(item.values()), [u'New'])\n \n     def test_metaclass_inheritance(self):\n-        class BaseItem(Item):\n+        class ParentItem(Item):\n             name = Field()\n             keys = Field()\n             values = Field()\n \n-        class TestItem(BaseItem):\n+        class TestItem(ParentItem):\n             keys = Field()\n \n         i = TestItem()\n@@ -321,13 +321,28 @@ class DictItemTest(unittest.TestCase):\n \n     def test_deprecation_warning(self):\n         with catch_warnings(record=True) as warnings:\n-            dict_item = DictItem()\n+            DictItem()\n             self.assertEqual(len(warnings), 1)\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n         with catch_warnings(record=True) as warnings:\n             class SubclassedDictItem(DictItem):\n                 pass\n-            subclassed_dict_item = SubclassedDictItem()\n+            SubclassedDictItem()\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+\n+\n+class BaseItemTest(unittest.TestCase):\n+\n+    def test_deprecation_warning(self):\n+        with catch_warnings(record=True) as warnings:\n+            BaseItem()\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+        with catch_warnings(record=True) as warnings:\n+            class SubclassedBaseItem(BaseItem):\n+                pass\n+            SubclassedBaseItem()\n             self.assertEqual(len(warnings), 1)\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#622ce860669b8bd7fc581b74d414aef24f4fb041", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 276 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -347,5 +347,18 @@ class BaseItemTest(unittest.TestCase):\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n \n \n+class ItemNoDeprecationWarningTest(unittest.TestCase):\n+\n+    def test_no_deprecation_warning(self):\n+        with catch_warnings(record=True) as warnings:\n+            Item()\n+            self.assertEqual(len(warnings), 0)\n+        with catch_warnings(record=True) as warnings:\n+            class SubclassedItem(Item):\n+                pass\n+            SubclassedItem()\n+            self.assertEqual(len(warnings), 0)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7988c676a956457843416f8586544a58bf121f63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 21 | Files Changed: 4 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 43 | Churn Cumulative: 1020 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,18 +15,8 @@ from scrapy.utils.trackref import object_ref\n \n \n class BaseItem(object_ref):\n-    \"\"\"Base class for all scraped items.\n-\n-    In Scrapy, an object is considered an *item* if it is an instance of either\n-    :class:`BaseItem` or :class:`dict`. For example, when the output of a\n-    spider callback is evaluated, only instances of :class:`BaseItem` or\n-    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n-\n-    If you need instances of a custom class to be considered items by Scrapy,\n-    you must inherit from either :class:`BaseItem` or :class:`dict`.\n-\n-    Unlike instances of :class:`dict`, instances of :class:`BaseItem` may be\n-    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n+    \"\"\"\n+    Deprecated, please use :class:`scrapy.item.Item` instead\n     \"\"\"\n \n     def __new__(cls, *args, **kwargs):\n@@ -91,8 +81,7 @@ class DictItem(MutableMapping, BaseItem):\n         if key in self.fields:\n             self._values[key] = value\n         else:\n-            raise KeyError(\"%s does not support field: %s\" %\n-                (self.__class__.__name__, key))\n+            raise KeyError(\"%s does not support field: %s\" % (self.__class__.__name__, key))\n \n     def __delitem__(self, key):\n         del self._values[key]\n@@ -104,8 +93,7 @@ class DictItem(MutableMapping, BaseItem):\n \n     def __setattr__(self, name, value):\n         if not name.startswith('_'):\n-            raise AttributeError(\"Use item[%r] = %r to set field value\" %\n-                (name, value))\n+            raise AttributeError(\"Use item[%r] = %r to set field value\" % (name, value))\n         super(DictItem, self).__setattr__(name, value)\n \n     def __len__(self):\n@@ -132,4 +120,17 @@ class DictItem(MutableMapping, BaseItem):\n \n \n class Item(DictItem, metaclass=ItemMeta):\n-    pass\n+    \"\"\"\n+    Base class for scraped items.\n+\n+    In Scrapy, an object is considered an *item* if it is an instance of either\n+    :class:`Item` or :class:`dict`. For example, when the output of a\n+    spider callback is evaluated, only instances of :class:`Item` or\n+    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n+\n+    If you need instances of a custom class to be considered items by Scrapy,\n+    you must inherit from either :class:`Item` or :class:`dict`.\n+\n+    Unlike instances of :class:`dict`, instances of :class:`Item` may be\n+    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n+    \"\"\"\n\n@@ -52,7 +52,7 @@ class XMLFeedSpider(Spider):\n         \"\"\"This method is called for the nodes matching the provided tag name\n         (itertag). Receives the response and an Selector for each node.\n         Overriding this method is mandatory. Otherwise, you spider won't work.\n-        This method must return either a BaseItem, a Request, or a list\n+        This method must return either an item, a request, or a list\n         containing any of them.\n         \"\"\"\n \n\n@@ -601,7 +601,7 @@ class NoInputReprocessingItemLoader(BaseNoInputReprocessingLoader):\n \n class NoInputReprocessingFromItemTest(unittest.TestCase):\n     \"\"\"\n-    Loaders initialized from loaded items must not reprocess fields (BaseItem instances)\n+    Loaders initialized from loaded items must not reprocess fields (Item instances)\n     \"\"\"\n     def test_avoid_reprocessing_with_initial_values_single(self):\n         il = NoInputReprocessingItemLoader(item=NoInputReprocessingItem(title='foo'))\n\n@@ -2,7 +2,7 @@ import unittest\n \n from scrapy import Spider\n from scrapy.http import Request\n-from scrapy.item import BaseItem\n+from scrapy.item import Item\n from scrapy.utils.spider import iterate_spider_output, iter_spider_classes\n \n \n@@ -17,7 +17,7 @@ class MySpider2(Spider):\n class UtilsSpidersTestCase(unittest.TestCase):\n \n     def test_iterate_spider_output(self):\n-        i = BaseItem()\n+        i = Item()\n         r = Request('http://scrapytest.org')\n         o = object()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#17c0cf64aee1641e1ad33c5b46a61435c5969f2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 22 | Files Changed: 7 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 1976 | Contributors (this commit): 37 | Commits (past 90d): 15 | Contributors (cumulative): 65 | DMM Complexity: 0.5\n\nDIFF:\n@@ -17,10 +17,10 @@ class ContractsManager:\n             self.contracts[contract.name] = contract\n \n     def tested_methods_from_spidercls(self, spidercls):\n+        is_method = re.compile(r\"^\\s*@\", re.MULTILINE).search\n         methods = []\n         for key, value in getmembers(spidercls):\n-            if (callable(value) and value.__doc__ and\n-                    re.search(r'^\\s*@', value.__doc__, re.MULTILINE)):\n+            if callable(value) and value.__doc__ and is_method(value.__doc__):\n                 methods.append(key)\n \n         return methods\n\n@@ -60,11 +60,14 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n     Handle redirection of requests based on response status\n     and meta-refresh html tag.\n     \"\"\"\n+\n     def process_response(self, request, response, spider):\n-        if (request.meta.get('dont_redirect', False) or\n-                response.status in getattr(spider, 'handle_httpstatus_list', []) or\n-                response.status in request.meta.get('handle_httpstatus_list', []) or\n-                request.meta.get('handle_httpstatus_all', False)):\n+        if (\n+            request.meta.get('dont_redirect', False)\n+            or response.status in getattr(spider, 'handle_httpstatus_list', [])\n+            or response.status in request.meta.get('handle_httpstatus_list', [])\n+            or request.meta.get('handle_httpstatus_all', False)\n+        ):\n             return response\n \n         allowed_status = (301, 302, 303, 307, 308)\n\n@@ -76,8 +76,10 @@ class TelnetConsole(protocol.ServerFactory):\n             \"\"\"An implementation of IPortal\"\"\"\n             @defers\n             def login(self_, credentials, mind, *interfaces):\n-                if not (credentials.username == self.username.encode('utf8') and\n-                        credentials.checkPassword(self.password.encode('utf8'))):\n+                if not (\n+                    credentials.username == self.username.encode('utf8')\n+                    and credentials.checkPassword(self.password.encode('utf8'))\n+                ):\n                     raise ValueError(\"Invalid credentials\")\n \n                 protocol = telnet.TelnetBootstrapProtocol(\n\n@@ -61,8 +61,7 @@ class FilteringLinkExtractor:\n \n     def __new__(cls, *args, **kwargs):\n         from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n-        if (issubclass(cls, FilteringLinkExtractor) and\n-                not issubclass(cls, LxmlLinkExtractor)):\n+        if issubclass(cls, FilteringLinkExtractor) and not issubclass(cls, LxmlLinkExtractor):\n             warn('scrapy.linkextractors.FilteringLinkExtractor is deprecated, '\n                  'please use scrapy.linkextractors.LinkExtractor instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n\n@@ -163,9 +163,10 @@ class StrictOriginPolicy(ReferrerPolicy):\n     name = POLICY_STRICT_ORIGIN\n \n     def referrer(self, response_url, request_url):\n-        if ((self.tls_protected(response_url) and\n-             self.potentially_trustworthy(request_url))\n-            or not self.tls_protected(response_url)):\n+        if (\n+            self.tls_protected(response_url) and self.potentially_trustworthy(request_url)\n+            or not self.tls_protected(response_url)\n+        ):\n             return self.origin_referrer(response_url)\n \n \n@@ -213,9 +214,10 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n         origin = self.origin(response_url)\n         if origin == self.origin(request_url):\n             return self.stripped_referrer(response_url)\n-        elif ((self.tls_protected(response_url) and\n-               self.potentially_trustworthy(request_url))\n-              or not self.tls_protected(response_url)):\n+        elif (\n+            self.tls_protected(response_url) and self.potentially_trustworthy(request_url)\n+            or not self.tls_protected(response_url)\n+        ):\n             return self.origin_referrer(response_url)\n \n \n\n@@ -52,8 +52,7 @@ def is_gzipped(response):\n     \"\"\"Return True if the response is gzipped, or False otherwise\"\"\"\n     ctype = response.headers.get('Content-Type', b'')\n     cenc = response.headers.get('Content-Encoding', b'').lower()\n-    return (_is_gzipped(ctype) or\n-            (_is_octetstream(ctype) and cenc in (b'gzip', b'x-gzip')))\n+    return _is_gzipped(ctype) or _is_octetstream(ctype) and cenc in (b'gzip', b'x-gzip')\n \n \n def gzip_magic_number(response):\n\n@@ -13,7 +13,7 @@ class ChunkedTest(unittest.TestCase):\n         chunked_body += \"8\\r\\n\" + \"sequence\\r\\n\"\n         chunked_body += \"0\\r\\n\\r\\n\"\n         body = decode_chunked_transfer(chunked_body)\n-        self.assertEqual(body,\n-                         \"This is the data in the first chunk\\r\\n\" +\n-                         \"and this is the second one\\r\\n\" +\n-                         \"consequence\")\n+        self.assertEqual(\n+            body,\n+            \"This is the data in the first chunk\\r\\nand this is the second one\\r\\nconsequence\"\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#49e8a337f78ec5e30eacfcd201b66d68deeecb56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 3035 | Contributors (this commit): 34 | Commits (past 90d): 12 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -94,11 +94,12 @@ class FTPDownloadHandler:\n     def gotClient(self, client, request, filepath):\n         self.client = client\n         protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n-        return client.retrieveFile(filepath, protocol)\\\n-                .addCallbacks(callback=self._build_response,\n+        return client.retrieveFile(filepath, protocol).addCallbacks(\n+            callback=self._build_response,\n             callbackArgs=(request, protocol),\n             errback=self._failed,\n-                        errbackArgs=(request,))\n+            errbackArgs=(request,),\n+        )\n \n     def _build_response(self, result, request, protocol):\n         self.result = result\n\n@@ -230,8 +230,7 @@ class ExecutionEngine:\n \n     def _downloaded(self, response, slot, request, spider):\n         slot.remove_request(request)\n-        return self.download(response, spider) \\\n-                if isinstance(response, Request) else response\n+        return self.download(response, spider) if isinstance(response, Request) else response\n \n     def _download(self, request, spider):\n         slot = self.slot\n\n@@ -15,16 +15,17 @@ def attribute(obj, oldattr, newattr, version='0.12'):\n         stacklevel=3)\n \n \n-def create_deprecated_class(name, new_class, clsdict=None,\n+def create_deprecated_class(\n+    name,\n+    new_class,\n+    clsdict=None,\n     warn_category=ScrapyDeprecationWarning,\n     warn_once=True,\n     old_class_path=None,\n     new_class_path=None,\n-                            subclass_warn_message=\"{cls} inherits from \"\n-                                    \"deprecated class {old}, please inherit \"\n-                                    \"from {new}.\",\n-                            instance_warn_message=\"{cls} is deprecated, \"\n-                                    \"instantiate {new} instead.\"):\n+    subclass_warn_message=\"{cls} inherits from deprecated class {old}, please inherit from {new}.\",\n+    instance_warn_message=\"{cls} is deprecated, instantiate {new} instead.\"\n+):\n     \"\"\"\n     Return a \"deprecated\" class that causes its subclasses to issue a warning.\n     Subclasses of ``new_class`` are considered subclasses of this class.\n\n@@ -50,8 +50,7 @@ def request_fingerprint(request, include_headers=None, keep_fragments=False):\n \n     \"\"\"\n     if include_headers:\n-        include_headers = tuple(to_bytes(h.lower())\n-                                 for h in sorted(include_headers))\n+        include_headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers))\n     cache = _fingerprint_cache.setdefault(request, {})\n     cache_key = (include_headers, keep_fragments)\n     if cache_key not in cache:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe0c582ee083ad8085a33443af0ffbc67b44fc16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 56 | Files Changed: 9 | Hunks: 38 | Methods Changed: 40 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 117 | Churn Cumulative: 5549 | Contributors (this commit): 44 | Commits (past 90d): 17 | Contributors (cumulative): 101 | DMM Complexity: 1.0\n\nDIFF:\n@@ -184,8 +184,7 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n             if self.fail_yielding:\n                 2 / 0\n \n-        assert self.seedsseen, \\\n-                'All start requests consumed before any download happened'\n+        assert self.seedsseen, 'All start requests consumed before any download happened'\n \n     def parse(self, response):\n         self.seedsseen.append(response.meta.get('seed'))\n\n@@ -41,8 +41,7 @@ class TestCloseSpider(TestCase):\n         yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n         reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_errorcount')\n-        key = 'spider_exceptions/{name}'\\\n-                .format(name=crawler.spider.exception_cls.__name__)\n+        key = 'spider_exceptions/{name}'.format(name=crawler.spider.exception_cls.__name__)\n         errorcount = crawler.stats.get_value(key)\n         self.assertTrue(errorcount >= close_on)\n \n\n@@ -1090,8 +1090,7 @@ class DataURITestCase(unittest.TestCase):\n     def test_default_mediatype_encoding(self):\n         def _test(response):\n             self.assertEqual(response.text, 'A brief note')\n-            self.assertEqual(type(response),\n-                              responsetypes.from_mimetype(\"text/plain\"))\n+            self.assertEqual(type(response), responsetypes.from_mimetype(\"text/plain\"))\n             self.assertEqual(response.encoding, \"US-ASCII\")\n \n         request = Request(\"data:,A%20brief%20note\")\n@@ -1100,8 +1099,7 @@ class DataURITestCase(unittest.TestCase):\n     def test_default_mediatype(self):\n         def _test(response):\n             self.assertEqual(response.text, u'\\u038e\\u03a3\\u038e')\n-            self.assertEqual(type(response),\n-                              responsetypes.from_mimetype(\"text/plain\"))\n+            self.assertEqual(type(response), responsetypes.from_mimetype(\"text/plain\"))\n             self.assertEqual(response.encoding, \"iso-8859-7\")\n \n         request = Request(\"data:;charset=iso-8859-7,%be%d3%be\")\n@@ -1119,8 +1117,7 @@ class DataURITestCase(unittest.TestCase):\n     def test_mediatype_parameters(self):\n         def _test(response):\n             self.assertEqual(response.text, u'\\u038e\\u03a3\\u038e')\n-            self.assertEqual(type(response),\n-                              responsetypes.from_mimetype(\"text/plain\"))\n+            self.assertEqual(type(response), responsetypes.from_mimetype(\"text/plain\"))\n             self.assertEqual(response.encoding, \"utf-8\")\n \n         request = Request('data:text/plain;foo=%22foo;bar%5C%22%22;'\n\n@@ -28,8 +28,8 @@ class DecompressionMiddlewareTest(TestCase):\n         for fmt in self.test_formats:\n             rsp = self.test_responses[fmt]\n             new = self.mw.process_response(None, rsp, self.spider)\n-            assert isinstance(new, XmlResponse), \\\n-                    'Failed %s, response type %s' % (fmt, type(new).__name__)\n+            error_msg = 'Failed %s, response type %s' % (fmt, type(new).__name__)\n+            assert isinstance(new, XmlResponse), error_msg\n             assert_samelines(self, new.body, self.uncompressed_body, fmt)\n \n     def test_plain_response(self):\n\n@@ -181,8 +181,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n             rsp = Response(url, headers={'Location': url2}, status=301, request=req)\n             r = self.mw.process_response(req, rsp, self.spider)\n             self.assertIs(r, rsp)\n-        _test_passthrough(Request(url, meta={'handle_httpstatus_list':\n-                                                           [404, 301, 302]}))\n+        _test_passthrough(Request(url, meta={'handle_httpstatus_list': [404, 301, 302]}))\n         _test_passthrough(Request(url, meta={'handle_httpstatus_all': True}))\n \n     def test_latin1_location(self):\n\n@@ -399,8 +399,7 @@ class FormRequestTest(RequestTest):\n \n     def test_custom_encoding_bytes(self):\n         data = {b'\\xb5 one': b'two', b'price': b'\\xa3 100'}\n-        r2 = self.request_class(\"http://www.example.com\", formdata=data,\n-                                    encoding='latin1')\n+        r2 = self.request_class(\"http://www.example.com\", formdata=data, encoding='latin1')\n         self.assertEqual(r2.method, 'POST')\n         self.assertEqual(r2.encoding, 'latin1')\n         self.assertQueryEqual(r2.body, b'price=%A3+100&%B5+one=two')\n@@ -408,8 +407,7 @@ class FormRequestTest(RequestTest):\n \n     def test_custom_encoding_textual_data(self):\n         data = {'price': u'£ 100'}\n-        r3 = self.request_class(\"http://www.example.com\", formdata=data,\n-                                    encoding='latin1')\n+        r3 = self.request_class(\"http://www.example.com\", formdata=data, encoding='latin1')\n         self.assertEqual(r3.encoding, 'latin1')\n         self.assertEqual(r3.body, b'price=%A3+100')\n \n\n@@ -67,8 +67,7 @@ class SelectorTestCase(unittest.TestCase):\n         headers = {'Content-Type': ['text/html; charset=utf-8']}\n         response = HtmlResponse(url=\"http://example.com\", headers=headers, body=html_utf8)\n         x = Selector(response)\n-        self.assertEqual(x.xpath(\"//span[@id='blank']/text()\").getall(),\n-                          [u'\\xa3'])\n+        self.assertEqual(x.xpath(\"//span[@id='blank']/text()\").getall(), [u'\\xa3'])\n \n     def test_badly_encoded_body(self):\n         # \\xe9 alone isn't valid utf8 sequence\n\n@@ -111,8 +111,7 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n                 self.mw.process_spider_input(self.res402, self.spider))\n \n     def test_meta_overrides_settings(self):\n-        request = Request('http://scrapytest.org',\n-                              meta={'handle_httpstatus_list': [404]})\n+        request = Request('http://scrapytest.org', meta={'handle_httpstatus_list': [404]})\n         res404 = self.res404.copy()\n         res404.request = request\n         res402 = self.res402.copy()\n@@ -146,8 +145,7 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n                 self.mw.process_spider_input(self.res404, self.spider))\n \n     def test_meta_overrides_settings(self):\n-        request = Request('http://scrapytest.org',\n-                              meta={'handle_httpstatus_list': [404]})\n+        request = Request('http://scrapytest.org', meta={'handle_httpstatus_list': [404]})\n         res404 = self.res404.copy()\n         res404.request = request\n         res402 = self.res402.copy()\n\n@@ -77,108 +77,124 @@ class UrlUtilsTest(unittest.TestCase):\n class AddHttpIfNoScheme(unittest.TestCase):\n \n     def test_add_scheme(self):\n-        self.assertEqual(add_http_if_no_scheme('www.example.com'),\n-                                               'http://www.example.com')\n+        self.assertEqual(add_http_if_no_scheme('www.example.com'), 'http://www.example.com')\n \n     def test_without_subdomain(self):\n-        self.assertEqual(add_http_if_no_scheme('example.com'),\n-                                               'http://example.com')\n+        self.assertEqual(add_http_if_no_scheme('example.com'), 'http://example.com')\n \n     def test_path(self):\n-        self.assertEqual(add_http_if_no_scheme('www.example.com/some/page.html'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('www.example.com/some/page.html'),\n             'http://www.example.com/some/page.html')\n \n     def test_port(self):\n-        self.assertEqual(add_http_if_no_scheme('www.example.com:80'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('www.example.com:80'),\n             'http://www.example.com:80')\n \n     def test_fragment(self):\n-        self.assertEqual(add_http_if_no_scheme('www.example.com/some/page#frag'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('www.example.com/some/page#frag'),\n             'http://www.example.com/some/page#frag')\n \n     def test_query(self):\n-        self.assertEqual(add_http_if_no_scheme('www.example.com/do?a=1&b=2&c=3'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('www.example.com/do?a=1&b=2&c=3'),\n             'http://www.example.com/do?a=1&b=2&c=3')\n \n     def test_username_password(self):\n-        self.assertEqual(add_http_if_no_scheme('username:password@www.example.com'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('username:password@www.example.com'),\n             'http://username:password@www.example.com')\n \n     def test_complete_url(self):\n-        self.assertEqual(add_http_if_no_scheme('username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n             'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')\n \n     def test_preserve_http(self):\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com'),\n-                                               'http://www.example.com')\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com'), 'http://www.example.com')\n \n     def test_preserve_http_without_subdomain(self):\n-        self.assertEqual(add_http_if_no_scheme('http://example.com'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('http://example.com'),\n             'http://example.com')\n \n     def test_preserve_http_path(self):\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page.html'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('http://www.example.com/some/page.html'),\n             'http://www.example.com/some/page.html')\n \n     def test_preserve_http_port(self):\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com:80'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('http://www.example.com:80'),\n             'http://www.example.com:80')\n \n     def test_preserve_http_fragment(self):\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page#frag'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('http://www.example.com/some/page#frag'),\n             'http://www.example.com/some/page#frag')\n \n     def test_preserve_http_query(self):\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com/do?a=1&b=2&c=3'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('http://www.example.com/do?a=1&b=2&c=3'),\n             'http://www.example.com/do?a=1&b=2&c=3')\n \n     def test_preserve_http_username_password(self):\n-        self.assertEqual(add_http_if_no_scheme('http://username:password@www.example.com'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('http://username:password@www.example.com'),\n             'http://username:password@www.example.com')\n \n     def test_preserve_http_complete_url(self):\n-        self.assertEqual(add_http_if_no_scheme('http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n             'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')\n \n     def test_protocol_relative(self):\n-        self.assertEqual(add_http_if_no_scheme('//www.example.com'),\n-                                               'http://www.example.com')\n+        self.assertEqual(\n+            add_http_if_no_scheme('//www.example.com'), 'http://www.example.com')\n \n     def test_protocol_relative_without_subdomain(self):\n-        self.assertEqual(add_http_if_no_scheme('//example.com'),\n-                                               'http://example.com')\n+        self.assertEqual(\n+            add_http_if_no_scheme('//example.com'), 'http://example.com')\n \n     def test_protocol_relative_path(self):\n-        self.assertEqual(add_http_if_no_scheme('//www.example.com/some/page.html'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('//www.example.com/some/page.html'),\n             'http://www.example.com/some/page.html')\n \n     def test_protocol_relative_port(self):\n-        self.assertEqual(add_http_if_no_scheme('//www.example.com:80'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('//www.example.com:80'),\n             'http://www.example.com:80')\n \n     def test_protocol_relative_fragment(self):\n-        self.assertEqual(add_http_if_no_scheme('//www.example.com/some/page#frag'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('//www.example.com/some/page#frag'),\n             'http://www.example.com/some/page#frag')\n \n     def test_protocol_relative_query(self):\n-        self.assertEqual(add_http_if_no_scheme('//www.example.com/do?a=1&b=2&c=3'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('//www.example.com/do?a=1&b=2&c=3'),\n             'http://www.example.com/do?a=1&b=2&c=3')\n \n     def test_protocol_relative_username_password(self):\n-        self.assertEqual(add_http_if_no_scheme('//username:password@www.example.com'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('//username:password@www.example.com'),\n             'http://username:password@www.example.com')\n \n     def test_protocol_relative_complete_url(self):\n-        self.assertEqual(add_http_if_no_scheme('//username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('//username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n             'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')\n \n     def test_preserve_https(self):\n-        self.assertEqual(add_http_if_no_scheme('https://www.example.com'),\n+        self.assertEqual(\n+            add_http_if_no_scheme('https://www.example.com'),\n             'https://www.example.com')\n \n     def test_preserve_ftp(self):\n-        self.assertEqual(add_http_if_no_scheme('ftp://www.example.com'),\n-                                               'ftp://www.example.com')\n+        self.assertEqual(add_http_if_no_scheme('ftp://www.example.com'), 'ftp://www.example.com')\n \n \n class GuessSchemeTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#63600243e08cb7e783798bd6c59fb97595488e9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 239 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,8 +43,7 @@ class MediaPipeline:\n         if allow_redirects:\n             self.handle_httpstatus_list = SequenceExclude(range(300, 400))\n \n-    def _key_for_pipe(self, key, base_class_name=None,\n-                      settings=None):\n+    def _key_for_pipe(self, key, base_class_name=None, settings=None):\n         \"\"\"\n         >>> MediaPipeline()._key_for_pipe(\"IMAGES\")\n         'IMAGES'\n@@ -55,8 +54,11 @@ class MediaPipeline:\n         \"\"\"\n         class_name = self.__class__.__name__\n         formatted_key = \"{}_{}\".format(class_name.upper(), key)\n-        if class_name == base_class_name or not base_class_name \\\n-            or (settings and not settings.get(formatted_key)):\n+        if (\n+            not base_class_name\n+            or class_name == base_class_name\n+            or settings and not settings.get(formatted_key)\n+        ):\n             return key\n         return formatted_key\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#286fca733f23fa41165edcdcc7ab7593cc6b074f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 966 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -248,7 +248,7 @@ class BaseSettings(MutableMapping):\n         :type name: str\n \n         :param value: the value to associate with the setting\n-        :type default: object\n+        :type value: object\n \n         :param priority: the priority of the setting. Should be a key of\n             :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#628c4a531914b6803ae0ec4991363aad52069ca1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 612 | Contributors (this commit): 20 | Commits (past 90d): 5 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -230,6 +230,20 @@ class GCSFilesStore:\n         bucket, prefix = uri[5:].split('/', 1)\n         self.bucket = client.bucket(bucket)\n         self.prefix = prefix\n+        permissions = self.bucket.test_iam_permissions(\n+            ['storage.objects.get', 'storage.objects.create']\n+        )\n+        if 'storage.objects.get' not in permissions:\n+            logger.warning(\n+                \"No 'storage.objects.get' permission for GSC bucket %(bucket)s. \"\n+                \"Checking if files are up to date will be impossible. Files will be downloaded every time.\",\n+                {'bucket': bucket}\n+            )\n+        if 'storage.objects.create' not in permissions:\n+            logger.error(\n+                \"No 'storage.objects.create' permission for GSC bucket %(bucket)s. Saving files will be impossible!\",\n+                {'bucket': bucket}\n+            )\n \n     def stat_file(self, path, info):\n         def _onsuccess(blob):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8643e8d3557449393989b15b9b8f2ec813f3e6ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 23 | Files Changed: 6 | Hunks: 14 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 60 | Churn Cumulative: 4996 | Contributors (this commit): 49 | Commits (past 90d): 16 | Contributors (cumulative): 80 | DMM Complexity: 1.0\n\nDIFF:\n@@ -178,12 +178,11 @@ def _get_clickable(clickdata, form):\n     if the latter is given. If not, it returns the first\n     clickable element found\n     \"\"\"\n-    clickables = [\n-        el for el in form.xpath(\n+    clickables = list(form.xpath(\n         'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n         '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n-            namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n-        ]\n+        namespaces={\"re\": \"http://exslt.org/regular-expressions\"}\n+    ))\n     if not clickables:\n         return\n \n\n@@ -822,11 +822,15 @@ class S3TestCase(unittest.TestCase):\n     def test_request_signing2(self):\n         # puts an object into the johnsmith bucket.\n         date = 'Tue, 27 Mar 2007 21:15:45 +0000'\n-        req = Request('s3://johnsmith/photos/puppy.jpg', method='PUT', headers={\n+        req = Request(\n+            's3://johnsmith/photos/puppy.jpg',\n+            method='PUT',\n+            headers={\n                 'Content-Type': 'image/jpeg',\n                 'Date': date,\n                 'Content-Length': '94328',\n-            })\n+            },\n+        )\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         self.assertEqual(httpreq.headers['Authorization'],\n@@ -906,8 +910,7 @@ class S3TestCase(unittest.TestCase):\n         # ensure that spaces are quoted properly before signing\n         date = 'Tue, 27 Mar 2007 19:42:41 +0000'\n         req = Request(\n-            (\"s3://johnsmith/photos/my puppy.jpg\"\n-             \"?response-content-disposition=my puppy.jpg\"),\n+            \"s3://johnsmith/photos/my puppy.jpg?response-content-disposition=my puppy.jpg\",\n             method='GET',\n             headers={'Date': date},\n         )\n\n@@ -20,7 +20,7 @@ FORMAT = {\n     'x-gzip': ('html-gzip.bin', 'gzip'),\n     'rawdeflate': ('html-rawdeflate.bin', 'deflate'),\n     'zlibdeflate': ('html-zlibdeflate.bin', 'deflate'),\n-        'br': ('html-br.bin', 'br')\n+    'br': ('html-br.bin', 'br'),\n }\n \n \n\n@@ -51,7 +51,7 @@ class MockCrawler(Crawler):\n             SCHEDULER_MEMORY_QUEUE='scrapy.squeues.LifoMemoryQueue',\n             SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,\n             JOBDIR=jobdir,\n-                DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter'\n+            DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter',\n         )\n         super(MockCrawler, self).__init__(Spider, settings)\n         self.engine = MockEngine(downloader=MockDownloader())\n@@ -305,9 +305,11 @@ class StartUrlsSpider(Spider):\n class TestIntegrationWithDownloaderAwareInMemory(TestCase):\n     def setUp(self):\n         self.crawler = get_crawler(\n-                    StartUrlsSpider,\n-                    {'SCHEDULER_PRIORITY_QUEUE': 'scrapy.pqueues.DownloaderAwarePriorityQueue',\n-                     'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n+            spidercls=StartUrlsSpider,\n+            settings_dict={\n+                'SCHEDULER_PRIORITY_QUEUE': 'scrapy.pqueues.DownloaderAwarePriorityQueue',\n+                'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n+            },\n         )\n \n     @defer.inlineCallbacks\n@@ -330,7 +332,7 @@ class TestIncompatibility(unittest.TestCase):\n     def _incompatible(self):\n         settings = dict(\n             SCHEDULER_PRIORITY_QUEUE='scrapy.pqueues.DownloaderAwarePriorityQueue',\n-                CONCURRENT_REQUESTS_PER_IP=1\n+            CONCURRENT_REQUESTS_PER_IP=1,\n         )\n         crawler = Crawler(Spider, settings)\n         scheduler = Scheduler.from_crawler(crawler)\n\n@@ -218,7 +218,8 @@ def create_skipped_scheme_t(args):\n     return do_expected\n \n \n-for k, args in enumerate([\n+for k, args in enumerate(\n+    [\n         ('/index', 'file://'),\n         ('/index.html', 'file://'),\n         ('./index.html', 'file://'),\n@@ -242,17 +243,24 @@ for k, args in enumerate([\n         # some corner cases (default to http://)\n         ('/', 'http://'),\n         ('.../test', 'http://'),\n-\n-        ], start=1):\n+    ],\n+    start=1,\n+):\n     t_method = create_guess_scheme_t(args)\n     t_method.__name__ = 'test_uri_%03d' % k\n     setattr(GuessSchemeTest, t_method.__name__, t_method)\n \n # TODO: the following tests do not pass with current implementation\n-for k, args in enumerate([\n-            (r'C:\\absolute\\path\\to\\a\\file.html', 'file://',\n-             'Windows filepath are not supported for scrapy shell'),\n-        ], start=1):\n+for k, args in enumerate(\n+    [\n+        (\n+            r'C:\\absolute\\path\\to\\a\\file.html',\n+            'file://',\n+            'Windows filepath are not supported for scrapy shell',\n+        ),\n+    ],\n+    start=1,\n+):\n     t_method = create_skipped_scheme_t(args)\n     t_method.__name__ = 'test_uri_skipped_%03d' % k\n     setattr(GuessSchemeTest, t_method.__name__, t_method)\n\n@@ -149,7 +149,8 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             headers={\n                 'X-Meta-Single': 'single',\n                 'X-Meta-Multivalued': ['value1', 'value2'],\n-                }))\n+            },\n+        ))\n \n         self._test(factory,\n             b\"GET /bar HTTP/1.0\\r\\n\"\n@@ -165,7 +166,8 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             headers=Headers({\n                 'X-Meta-Single': 'single',\n                 'X-Meta-Multivalued': ['value1', 'value2'],\n-                })))\n+            }),\n+        ))\n \n         self._test(factory,\n             b\"GET /bar HTTP/1.0\\r\\n\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d0bb04f08936435202488404d08c0b82f25aa1e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 819 | Contributors (this commit): 22 | Commits (past 90d): 9 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -250,7 +250,7 @@ class CsvItemExporter(BaseItemExporter):\n \n class PickleItemExporter(BaseItemExporter):\n \n-    def __init__(self, file, protocol=2, **kwargs):\n+    def __init__(self, file, protocol=4, **kwargs):\n         super().__init__(**kwargs)\n         self.file = file\n         self.protocol = protocol\n\n@@ -250,7 +250,7 @@ class DbmCacheStorage:\n             'headers': dict(response.headers),\n             'body': response.body,\n         }\n-        self.db['%s_data' % key] = pickle.dumps(data, protocol=2)\n+        self.db['%s_data' % key] = pickle.dumps(data, protocol=4)\n         self.db['%s_time' % key] = str(time())\n \n     def _read_data(self, spider, request):\n@@ -317,7 +317,7 @@ class FilesystemCacheStorage:\n         with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n             f.write(to_bytes(repr(metadata)))\n         with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n-            pickle.dump(metadata, f, protocol=2)\n+            pickle.dump(metadata, f, protocol=4)\n         with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n             f.write(headers_dict_to_raw(response.headers))\n         with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:\n\n@@ -26,7 +26,7 @@ class SpiderState:\n     def spider_closed(self, spider):\n         if self.jobdir:\n             with open(self.statefn, 'wb') as f:\n-                pickle.dump(spider.state, f, protocol=2)\n+                pickle.dump(spider.state, f, protocol=4)\n \n     def spider_opened(self, spider):\n         if self.jobdir and os.path.exists(self.statefn):\n\n@@ -81,7 +81,7 @@ def _scrapy_non_serialization_queue(queue_class):\n \n def _pickle_serialize(obj):\n     try:\n-        return pickle.dumps(obj, protocol=2)\n+        return pickle.dumps(obj, protocol=4)\n     # Python <= 3.4 raises pickle.PicklingError here while\n     # 3.5 <= Python < 3.6 raises AttributeError and\n     # Python >= 3.6 raises TypeError\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
