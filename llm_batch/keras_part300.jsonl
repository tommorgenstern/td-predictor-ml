{"custom_id": "keras#d9fae78e554db4bb4d4590564ce49b3d70e64d0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 9 | Files Changed: 5 | Hunks: 10 | Methods Changed: 9 | Complexity Δ (Sum/Max): 5/6 | Churn Δ: 32 | Churn Cumulative: 8428 | Contributors (this commit): 103 | Commits (past 90d): 32 | Contributors (cumulative): 123 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1408,16 +1408,32 @@ class Function(object):\n             if v not in unique_variables_to_update:\n                 unique_variables_to_update[v] = nv\n         updates = unique_variables_to_update.items()\n+        self.outputs = outputs\n         self.function = theano.function(inputs, outputs, updates=updates,\n                                         allow_input_downcast=True,\n                                         on_unused_input='ignore',\n                                         name=name,\n                                         **kwargs)\n+        self._metrics = [x for x in outputs if hasattr(x, '_is_metric')]\n+        self._metrics_function = theano.function(\n+            [], self._metrics,\n+            name=name + '_metrics' if name else None)\n         self.name = name\n \n     def __call__(self, inputs):\n         assert isinstance(inputs, (list, tuple))\n-        return self.function(*inputs)\n+        outputs = self.function(*inputs)\n+        if self._metrics:\n+            metrics = self._metrics_function()\n+        i = 0\n+        j = 0\n+        for x in self.outputs:\n+            if hasattr(x, '_is_metric'):\n+                v = metrics[j]\n+                outputs[i] = v\n+                j += 1\n+            i += 1\n+        return outputs\n \n \n def _raise_invalid_arg(key):\n\n@@ -76,9 +76,6 @@ class Metric(Layer):\n     @K.symbolic\n     def __call__(self, *args, **kwargs):\n         \"\"\"Accumulates statistics and then computes metric result value.\"\"\"\n-        if K.backend() != 'tensorflow':\n-            raise RuntimeError(\n-                'Metric calling only supported with TensorFlow backend.')\n         update_op = self.update_state(*args, **kwargs)\n         with K.control_dependencies(update_op):  # For TF\n             result_t = self.result()\n\n@@ -56,6 +56,7 @@ def result_wrapper(result_fn):\n     def decorated(metric_obj, *args, **kwargs):\n         result_t = K.identity(result_fn(*args, **kwargs))\n         metric_obj._call_result = result_t\n+        result_t._is_metric = True\n         return result_t\n \n     return decorated\n\n@@ -34,7 +34,7 @@ def test_image_classification():\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n     model.summary()\n-    history = model.fit(x_train, y_train, epochs=10, batch_size=16,\n+    history = model.fit(x_train, y_train, epochs=12, batch_size=16,\n                         validation_data=(x_test, y_test),\n                         verbose=0)\n     assert history.history['val_accuracy'][-1] > 0.75\n@@ -68,7 +68,7 @@ def test_image_data_generator_training():\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n     history = model.fit_generator(img_gen.flow(x_train, y_train, batch_size=16),\n-                                  epochs=12,\n+                                  epochs=15,\n                                   validation_data=img_gen.flow(x_test, y_test,\n                                                                batch_size=16),\n                                   verbose=0)\n\n@@ -35,7 +35,7 @@ def test_temporal_classification():\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n     model.summary()\n-    history = model.fit(x_train, y_train, epochs=4, batch_size=10,\n+    history = model.fit(x_train, y_train, epochs=5, batch_size=10,\n                         validation_data=(x_test, y_test),\n                         verbose=0)\n     assert(history.history['accuracy'][-1] >= 0.8)\n@@ -66,10 +66,10 @@ def test_temporal_classification_functional():\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n-    history = model.fit(x_train, y_train, epochs=4, batch_size=10,\n+    history = model.fit(x_train, y_train, epochs=5, batch_size=10,\n                         validation_data=(x_test, y_test),\n                         verbose=0)\n-    assert(history.history['accuracy'][-1] >= 0.8)\n+    assert(history.history['accuracy'][-1] >= 0.75)\n \n \n def test_temporal_regression():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3625bf48cabc4b6fe6145d117ef37b0316f7bb4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 11396 | Contributors (this commit): 98 | Commits (past 90d): 14 | Contributors (cumulative): 103 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1167,6 +1167,7 @@ def _create_mean_metric(value, name=None):\n     _call_metric(metric_obj, value)\n     return metric_obj\n \n+\n @K.symbolic\n def _call_metric(metric_obj, *args, **kwargs):\n     update_op = metric_obj.update_state(*args, **kwargs)\n\n@@ -692,7 +692,8 @@ class Model(Network):\n                         y_true, y_pred, sample_weight=sample_weight)\n \n                 if len(self.outputs) > 1:\n-                    update_ops = self._output_loss_metrics[i].update_state(output_loss)\n+                    update_ops = self._output_loss_metrics[i].update_state(\n+                        output_loss)\n                     with K.control_dependencies(update_ops):  # For TF\n                         self._output_loss_metrics[i].result()\n                 if total_loss is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#555ca942df407b8c1bf1d48383c60fa1bf09cc1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 29 | Files Changed: 10 | Hunks: 28 | Methods Changed: 17 | Complexity Δ (Sum/Max): 8/3 | Churn Δ: 86 | Churn Cumulative: 43027 | Contributors (this commit): 260 | Commits (past 90d): 34 | Contributors (cumulative): 384 | DMM Complexity: 0.0\n\nDIFF:\n@@ -326,11 +326,11 @@ def learning_phase():\n     else:\n         if isinstance(lp, int):\n             return lp\n-        if lp in _LEARNING_PHASE_CACHE:\n-            return _LEARNING_PHASE_CACHE[lp]\n+        if id(lp) in _LEARNING_PHASE_CACHE:\n+            return _LEARNING_PHASE_CACHE[id(lp)]\n         with name_scope(''):\n             int_lp = tf.cast(lp, 'int32', name='learning_phase')\n-        _LEARNING_PHASE_CACHE[lp] = int_lp\n+        _LEARNING_PHASE_CACHE[id(lp)] = int_lp\n         return int_lp\n \n \n\n@@ -462,7 +462,7 @@ class Layer(object):\n             inputs_ls = to_list(inputs)\n             output_ls_copy = []\n             for x in output_ls:\n-                if x in inputs_ls:\n+                if id(x) in [id(i) for i in inputs_ls]:\n                     x = K.identity(x)\n                 output_ls_copy.append(x)\n             output = unpack_singleton(output_ls_copy)\n@@ -968,10 +968,12 @@ class Layer(object):\n                 (e.g. L2 weight regularization, which only depends\n                 on the layer's weights variables, not on any inputs tensors).\n         \"\"\"\n-        if losses is None or losses == []:\n+        if losses is None:\n             return\n         # Update self.losses\n         losses = to_list(losses)\n+        if losses == []:\n+            return\n         if hasattr(self, '_losses'):\n             self._losses += losses\n         # Update self._per_input_updates\n@@ -1000,10 +1002,12 @@ class Layer(object):\n                 the updates as conditional on these inputs.\n                 If None is passed, the updates are assumed unconditional.\n         \"\"\"\n-        if updates is None or updates == []:\n+        if updates is None:\n             return\n         # Update self.updates\n         updates = to_list(updates)\n+        if updates == []:\n+            return\n         if hasattr(self, '_updates'):\n             self._updates += updates\n         # Update self._per_input_updates\n\n@@ -154,7 +154,7 @@ class Network(Layer):\n \n         # User-provided argument validation.\n         # Check for redundancy in inputs.\n-        if len(set(self.inputs)) != len(self.inputs):\n+        if len(set(id(x) for x in self.inputs)) != len(self.inputs):\n             raise ValueError('The list of inputs passed to the model '\n                              'is redundant. '\n                              'All inputs should only appear once.'\n@@ -445,8 +445,13 @@ class Network(Layer):\n         # Add any potential unconditional model-level loss.\n         losses += self.get_losses_for(None)\n \n-        unique_tensors = list(\n-            set(x for x in losses if not isinstance(x, (float, int))))\n+        unique_tensors = []\n+        unique_tensors_ids = set()\n+        for x in losses:\n+            if not isinstance(x, (float, int)):\n+                if id(x) not in unique_tensors_ids:\n+                    unique_tensors.append(x)\n+                    unique_tensors_ids.add(id(x))\n         non_tensors = [x for x in losses if isinstance(x, (float, int))]\n         return unique_tensors + non_tensors\n \n@@ -1502,7 +1507,7 @@ def _map_graph_network(inputs, outputs):\n             layer = node.outbound_layer\n             if layer:\n                 for x in node.input_tensors:\n-                    if x not in computable_tensors:\n+                    if id(x) not in [id(ct) for ct in computable_tensors]:\n                         raise ValueError('Graph disconnected: '\n                                          'cannot obtain value for tensor ' +\n                                          str(x) + ' at layer \"' +\n\n@@ -835,8 +835,9 @@ def preprocess_weights_for_loading(layer, weights,\n \n         # non-trainable weights\n         for sublayer in layer.layers:\n+            ref_ids = [id(w) for w in sublayer.trainable_weights]\n             num_weights = len([l for l in sublayer.weights\n-                               if l not in sublayer.trainable_weights])\n+                               if id(l) not in ref_ids])\n             if num_weights > 0:\n                 new_weights.extend(preprocess_weights_for_loading(\n                     layer=sublayer,\n\n@@ -89,7 +89,7 @@ def _clone_functional_model(model, input_tensors=None):\n         input_tensors = _input_tensors\n \n     for x, y in zip(model.inputs, input_tensors):\n-        tensor_map[x] = (y, None)  # tensor, mask\n+        tensor_map[id(x)] = (y, None)  # tensor, mask\n \n     # Iterated over every node in the reference model, in depth order.\n     depth_keys = list(model._nodes_by_depth.keys())\n@@ -121,8 +121,8 @@ def _clone_functional_model(model, input_tensors=None):\n             # then call node.inbound_layer on them.\n             computed_data = []  # List of tuples (input, mask).\n             for x in reference_input_tensors:\n-                if x in tensor_map:\n-                    computed_data.append(tensor_map[x])\n+                if id(x) in tensor_map:\n+                    computed_data.append(tensor_map[id(x)])\n \n             if len(computed_data) == len(reference_input_tensors):\n                 # Call layer.\n@@ -163,14 +163,14 @@ def _clone_functional_model(model, input_tensors=None):\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,\n                                       output_masks):\n-                    tensor_map[x] = (y, mask)\n+                    tensor_map[id(x)] = (y, mask)\n \n     # Check that we did compute the model outputs,\n     # then instantiate a new model from inputs and outputs.\n     output_tensors = []\n     for x in model.outputs:\n-        assert x in tensor_map, 'Could not compute output ' + str(x)\n-        tensor, _ = tensor_map[x]\n+        assert id(x) in tensor_map, 'Could not compute output ' + str(x)\n+        tensor, _ = tensor_map[id(x)]\n         output_tensors.append(tensor)\n     return Model(input_tensors, output_tensors, name=model.name)\n \n\n@@ -89,7 +89,7 @@ class Optimizer(object):\n \n     def get_gradients(self, loss, params):\n         grads = K.gradients(loss, params)\n-        if None in grads:\n+        if any(x is None for x in grads):\n             raise ValueError('An operation has `None` for gradient. '\n                              'Please make sure that all of your ops have a '\n                              'gradient defined (i.e. are differentiable). '\n\n@@ -18,7 +18,13 @@ def count_params(weights):\n     # Returns\n         The total number of scalars composing the weights\n     \"\"\"\n-    return int(np.sum([K.count_params(p) for p in set(weights)]))\n+    weight_ids = set()\n+    total = 0\n+    for w in weights:\n+        if id(w) not in weight_ids:\n+            weight_ids.add(id(w))\n+            total += int(K.count_params(w))\n+    return total\n \n \n def print_summary(model, line_length=None, positions=None, print_fn=None):\n@@ -278,6 +284,7 @@ def get_source_inputs(tensor, layer=None, node_index=None):\n             return node.input_tensors\n         else:\n             source_tensors = []\n+            source_tensors_ids = set()\n             for i in range(len(node.inbound_layers)):\n                 x = node.input_tensors[i]\n                 layer = node.inbound_layers[i]\n@@ -287,6 +294,7 @@ def get_source_inputs(tensor, layer=None, node_index=None):\n                                                      node_index)\n                 # Avoid input redundancy.\n                 for x in previous_sources:\n-                    if x not in source_tensors:\n+                    if id(x) not in source_tensors_ids:\n                         source_tensors.append(x)\n+                        source_tensors_ids.add(id(x))\n             return source_tensors\n\n@@ -220,8 +220,8 @@ def test_node_construction():\n     test_layer = Dense(16, name='test_layer')\n     a_test = test_layer(a)\n     assert K.int_shape(test_layer.kernel) == (32, 16)\n-    assert test_layer.input == a\n-    assert test_layer.output == a_test\n+    assert test_layer.input is a\n+    assert test_layer.output is a_test\n     assert test_layer.input_mask is None\n     assert test_layer.output_mask is None\n     assert test_layer.input_shape == (None, 32)\n@@ -236,10 +236,10 @@ def test_node_construction():\n     with pytest.raises(AttributeError):\n         dense.output_mask\n \n-    assert dense.get_input_at(0) == a\n-    assert dense.get_input_at(1) == b\n-    assert dense.get_output_at(0) == a_2\n-    assert dense.get_output_at(1) == b_2\n+    assert dense.get_input_at(0) is a\n+    assert dense.get_input_at(1)is b\n+    assert dense.get_output_at(0) is a_2\n+    assert dense.get_output_at(1) is b_2\n     assert dense.get_input_shape_at(0) == (None, 32)\n     assert dense.get_input_shape_at(1) == (None, 32)\n     assert dense.get_output_shape_at(0) == (None, 16)\n@@ -298,7 +298,9 @@ def test_multi_input_layer():\n     assert [x.shape for x in fn_outputs] == [(10, 64), (10, 5)]\n \n     # test get_source_inputs\n-    assert get_source_inputs(c) == [a, b]\n+    source_inputs = get_source_inputs(c)\n+    assert source_inputs[0] is a\n+    assert source_inputs[1] is b\n \n     # serialization / deserialization\n     json_config = model.to_json()\n\n@@ -21,6 +21,9 @@ from keras import backend as K\n from keras.utils import Sequence\n from keras.callbacks import Callback\n \n+if K.backend() == 'tensorflow':\n+    import tensorflow as tf\n+\n \n class RandomSequence(Sequence):\n     def __init__(self, batch_size, sequence_length=12):\n@@ -1246,6 +1249,9 @@ def test_target_tensors():\n                          sample_weight={'dense_a': np.random.random((10,))})\n \n \n+@pytest.mark.skipif(K.backend() == 'tensorflow' and\n+                    tf.__version__.startswith('2'),\n+                    reason='Cannot have tensors as dict keys in TF2')\n def test_model_custom_target_tensors():\n     a = Input(shape=(3,), name='input_a')\n     b = Input(shape=(3,), name='input_b')\n\n@@ -382,7 +382,8 @@ def test_specify_initial_state_keras_tensor(layer_class):\n         output = layer(inputs, initial_state=initial_state[0])\n     else:\n         output = layer(inputs, initial_state=initial_state)\n-    assert initial_state[0] in layer._inbound_nodes[0].input_tensors\n+    assert id(initial_state[0]) in [\n+        id(x) for x in layer._inbound_nodes[0].input_tensors]\n \n     model = Model([inputs] + initial_state, output)\n     model.compile(loss='categorical_crossentropy', optimizer='adam')\n@@ -450,7 +451,8 @@ def test_initial_states_as_other_inputs(layer_class):\n \n     layer = layer_class(units)\n     output = layer(inputs)\n-    assert initial_state[0] in layer._inbound_nodes[0].input_tensors\n+    assert id(initial_state[0]) in [\n+        id(x) for x in layer._inbound_nodes[0].input_tensors]\n \n     model = Model(inputs, output)\n     model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#200358fde3858e0a6a7394c556dada32210009c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 4811 | Contributors (this commit): 44 | Commits (past 90d): 10 | Contributors (cumulative): 51 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2342,6 +2342,10 @@ def stop_gradient(variables):\n \n \n def switch(condition, then_expression, else_expression):\n+    if callable(then_expression):\n+        then_expression = then_expression()\n+    if callable(else_expression):\n+        else_expression = else_expression()\n     ndim_cond = ndim(condition)\n     ndim_expr = ndim(then_expression)\n     if ndim_cond > ndim_expr:\n\n@@ -98,7 +98,7 @@ class TestMean(object):\n \n         # check update_state() and result()\n         result = m([1, 5])\n-        assert np.isclose(K.eval(result), 106 / 3)\n+        assert np.isclose(K.eval(result), 106. / 3)\n         assert K.eval(m.total) == 106  # 100 + 1 + 5\n         assert K.eval(m.count) == 3\n \n@@ -120,14 +120,14 @@ class TestMean(object):\n \n         # check scalar weight\n         result_t = m(100, sample_weight=0.5)\n-        assert K.eval(result_t) == 50 / 0.5\n+        assert K.eval(result_t) == 50. / 0.5\n         assert K.eval(m.total) == 50\n         assert K.eval(m.count) == 0.5\n \n         # check weights not scalar and weights rank matches values rank\n         result_t = m([1, 5], sample_weight=[1, 0.2])\n         result = K.eval(result_t)\n-        assert np.isclose(result, 52 / 1.7)\n+        assert np.isclose(result, 52. / 1.7)\n         assert np.isclose(K.eval(m.total), 52)  # 50 + 1 + 5 * 0.2\n         assert np.isclose(K.eval(m.count), 1.7)  # 0.5 + 1.2\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#18c11a30d6d73405af61206ef3443d9b091e3af8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 2997 | Contributors (this commit): 33 | Commits (past 90d): 7 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -1487,13 +1487,14 @@ def test_training_and_eval_methods_on_symbolic_tensors_multi_io():\n         epochs=1,\n         steps_per_epoch=2,\n         verbose=0)\n-    with pytest.raises(ValueError) as excinfo:\n+    with pytest.raises(ValueError,\n+                       match='should specify the `steps_per_epoch`'):\n         model.fit(\n             [input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n             epochs=1,\n             batch_size=5,\n             verbose=0)\n-    assert 'should specify the `steps_per_epoch`' in str(excinfo.value)\n+\n     model.train_on_batch([input_a_tf, input_b_tf], [output_d_tf, output_e_tf])\n \n     # Test with dictionary inputs\n@@ -1534,7 +1535,8 @@ def test_training_and_eval_methods_on_symbolic_tensors_multi_io():\n         validation_steps=2,\n         verbose=0)\n     # Test with validation split\n-    with pytest.raises(ValueError) as excinfo:\n+    with pytest.raises(ValueError,\n+                       match='you cannot use `validation_split`'):\n         model.fit(\n             [input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n             epochs=2,\n@@ -1542,7 +1544,6 @@ def test_training_and_eval_methods_on_symbolic_tensors_multi_io():\n             verbose=0,\n             validation_split=0.2,\n             validation_steps=2)\n-    assert 'you cannot use `validation_split`' in str(excinfo.value)\n \n     # Test evaluation / prediction methods\n     model.evaluate([input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a996d3d9773a98a85eeab243e3d0d58750c0500c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 40 | Files Changed: 2 | Hunks: 38 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 78 | Churn Cumulative: 3939 | Contributors (this commit): 37 | Commits (past 90d): 14 | Contributors (cumulative): 40 | DMM Complexity: None\n\nDIFF:\n@@ -818,21 +818,19 @@ def test_check_not_failing():\n \n def test_check_last_is_one():\n     a = np.random.random((2, 3, 1))\n-    with pytest.raises(ValueError) as exc:\n+    with pytest.raises(ValueError,\n+                       match='You are passing a target array'):\n         training_utils.check_loss_and_target_compatibility(\n             [a], [losses.CategoricalCrossentropy()], [a.shape])\n \n-    assert 'You are passing a target array' in str(exc)\n-\n \n def test_check_bad_shape():\n     a = np.random.random((2, 3, 5))\n-    with pytest.raises(ValueError) as exc:\n+    with pytest.raises(ValueError,\n+                       match='targets to have the same shape'):\n         training_utils.check_loss_and_target_compatibility(\n             [a], [losses.CategoricalCrossentropy()], [(2, 3, 6)])\n \n-    assert 'targets to have the same shape' in str(exc)\n-\n \n @pytest.mark.skipif(K.backend() != 'tensorflow',\n                     reason='Requires TensorFlow backend')\n\n@@ -205,14 +205,14 @@ class TestMeanSquaredError:\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mse_obj(y_true, y_pred)\n-        assert np.isclose(K.eval(loss), 49.5, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 49.5, atol=1e-3)\n \n     def test_scalar_weighted(self):\n         mse_obj = losses.MeanSquaredError()\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), 113.85, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 113.85, atol=1e-3)\n \n     def test_sample_weighted(self):\n         mse_obj = losses.MeanSquaredError()\n@@ -220,7 +220,7 @@ class TestMeanSquaredError:\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n         loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 767.8 / 6, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 767.8 / 6, atol=1e-3)\n \n     def test_timestep_weighted(self):\n         mse_obj = losses.MeanSquaredError()\n@@ -228,7 +228,7 @@ class TestMeanSquaredError:\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n         sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n         loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 97.833, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 97.833, atol=1e-3)\n \n     def test_zero_weighted(self):\n         mse_obj = losses.MeanSquaredError()\n@@ -251,7 +251,7 @@ class TestMeanSquaredError:\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.allclose(K.eval(loss), [84.3333, 143.3666], rtol=1e-3)\n+        assert np.allclose(K.eval(loss), [84.3333, 143.3666], atol=1e-3)\n \n     def test_sum_reduction(self):\n         mse_obj = losses.MeanSquaredError(\n@@ -259,7 +259,7 @@ class TestMeanSquaredError:\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), 227.69998, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 227.69998, atol=1e-3)\n \n \n @skipif_not_tf\n@@ -275,21 +275,21 @@ class TestMeanAbsoluteError(object):\n         mae_obj = losses.MeanAbsoluteError()\n         y_true = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mae_obj(y_true, y_true)\n-        assert np.isclose(K.eval(loss), 0.0, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)\n \n     def test_unweighted(self):\n         mae_obj = losses.MeanAbsoluteError()\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mae_obj(y_true, y_pred)\n-        assert np.isclose(K.eval(loss), 5.5, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 5.5, atol=1e-3)\n \n     def test_scalar_weighted(self):\n         mae_obj = losses.MeanAbsoluteError()\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), 12.65, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 12.65, atol=1e-3)\n \n     def test_sample_weighted(self):\n         mae_obj = losses.MeanAbsoluteError()\n@@ -297,7 +297,7 @@ class TestMeanAbsoluteError(object):\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n         loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 81.4 / 6, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 81.4 / 6, atol=1e-3)\n \n     def test_timestep_weighted(self):\n         mae_obj = losses.MeanAbsoluteError()\n@@ -305,14 +305,14 @@ class TestMeanAbsoluteError(object):\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n         sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n         loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 13.833, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 13.833, atol=1e-3)\n \n     def test_zero_weighted(self):\n         mae_obj = losses.MeanAbsoluteError()\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mae_obj(y_true, y_pred, sample_weight=0)\n-        assert np.isclose(K.eval(loss), 0.0, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)\n \n     def test_invalid_sample_weight(self):\n         mae_obj = losses.MeanAbsoluteError()\n@@ -328,7 +328,7 @@ class TestMeanAbsoluteError(object):\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.allclose(K.eval(loss), [10.7333, 14.5666], rtol=1e-3)\n+        assert np.allclose(K.eval(loss), [10.7333, 14.5666], atol=1e-3)\n \n     def test_sum_reduction(self):\n         mae_obj = losses.MeanAbsoluteError(\n@@ -336,7 +336,7 @@ class TestMeanAbsoluteError(object):\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), 25.29999, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 25.29999, atol=1e-3)\n \n \n @skipif_not_tf\n@@ -352,21 +352,21 @@ class TestMeanAbsolutePercentageError(object):\n         mape_obj = losses.MeanAbsolutePercentageError()\n         y_true = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mape_obj(y_true, y_true)\n-        assert np.allclose(K.eval(loss), 0.0, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 0.0, atol=1e-3)\n \n     def test_unweighted(self):\n         mape_obj = losses.MeanAbsolutePercentageError()\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mape_obj(y_true, y_pred)\n-        assert np.allclose(K.eval(loss), 211.8518, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 211.8518, atol=1e-3)\n \n     def test_scalar_weighted(self):\n         mape_obj = losses.MeanAbsolutePercentageError()\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.allclose(K.eval(loss), 487.259, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 487.259, atol=1e-3)\n \n     def test_sample_weighted(self):\n         mape_obj = losses.MeanAbsolutePercentageError()\n@@ -374,7 +374,7 @@ class TestMeanAbsolutePercentageError(object):\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n         loss = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(K.eval(loss), 422.8888, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 422.8888, atol=1e-3)\n \n     def test_timestep_weighted(self):\n         mape_obj = losses.MeanAbsolutePercentageError()\n@@ -382,14 +382,14 @@ class TestMeanAbsolutePercentageError(object):\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n         sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n         loss = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(K.eval(loss), 694.4445, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 694.4445, atol=1e-3)\n \n     def test_zero_weighted(self):\n         mape_obj = losses.MeanAbsolutePercentageError()\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mape_obj(y_true, y_pred, sample_weight=0)\n-        assert np.allclose(K.eval(loss), 0.0, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 0.0, atol=1e-3)\n \n     def test_no_reduction(self):\n         mape_obj = losses.MeanAbsolutePercentageError(\n@@ -397,7 +397,7 @@ class TestMeanAbsolutePercentageError(object):\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.allclose(K.eval(loss), [621.8518, 352.6666], rtol=1e-3)\n+        assert np.allclose(K.eval(loss), [621.8518, 352.6666], atol=1e-3)\n \n \n @skipif_not_tf\n@@ -414,14 +414,14 @@ class TestMeanSquaredLogarithmicError(object):\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = msle_obj(y_true, y_pred)\n-        assert np.allclose(K.eval(loss), 1.4370, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 1.4370, atol=1e-3)\n \n     def test_scalar_weighted(self):\n         msle_obj = losses.MeanSquaredLogarithmicError()\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = msle_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.allclose(K.eval(loss), 3.3051, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 3.3051, atol=1e-3)\n \n     def test_sample_weighted(self):\n         msle_obj = losses.MeanSquaredLogarithmicError()\n@@ -429,7 +429,7 @@ class TestMeanSquaredLogarithmicError(object):\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n         loss = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(K.eval(loss), 3.7856, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 3.7856, atol=1e-3)\n \n     def test_timestep_weighted(self):\n         msle_obj = losses.MeanSquaredLogarithmicError()\n@@ -437,14 +437,14 @@ class TestMeanSquaredLogarithmicError(object):\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n         sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n         loss = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(K.eval(loss), 2.6473, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 2.6473, atol=1e-3)\n \n     def test_zero_weighted(self):\n         msle_obj = losses.MeanSquaredLogarithmicError()\n         y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n         loss = msle_obj(y_true, y_pred, sample_weight=0)\n-        assert np.allclose(K.eval(loss), 0.0, rtol=1e-3)\n+        assert np.allclose(K.eval(loss), 0.0, atol=1e-3)\n \n \n @skipif_not_tf\n@@ -460,7 +460,7 @@ class TestBinaryCrossentropy(object):\n         y_true = K.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n         bce_obj = losses.BinaryCrossentropy()\n         loss = bce_obj(y_true, y_true)\n-        assert np.isclose(K.eval(loss), 0.0, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)\n \n         # Test with logits.\n         logits = K.constant([[100.0, -100.0, -100.0],\n@@ -486,7 +486,7 @@ class TestBinaryCrossentropy(object):\n         #      = [0, 15.33, 0, 0]\n         # Reduced loss = 15.33 / 4\n \n-        assert np.isclose(K.eval(loss), 3.833, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 3.833, atol=1e-3)\n \n         # Test with logits.\n         y_true = K.constant([[1., 0., 1.], [0., 1., 1.]])\n@@ -505,7 +505,7 @@ class TestBinaryCrossentropy(object):\n         #      = [(0 + 0 + 0) / 3, 200 / 3]\n         # Reduced loss = (0 + 66.666) / 2\n \n-        assert np.isclose(K.eval(loss), 33.333, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 33.333, atol=1e-3)\n \n     def test_scalar_weighted(self):\n         bce_obj = losses.BinaryCrossentropy()\n@@ -524,7 +524,7 @@ class TestBinaryCrossentropy(object):\n         # Weighted loss = [0, 15.33 * 2.3, 0, 0]\n         # Reduced loss = 15.33 * 2.3 / 4\n \n-        assert np.isclose(K.eval(loss), 8.817, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 8.817, atol=1e-3)\n \n         # Test with logits.\n         y_true = K.constant([[1, 0, 1], [0, 1, 1]])\n@@ -538,7 +538,7 @@ class TestBinaryCrossentropy(object):\n         # Weighted loss = [0 * 2.3, 66.666 * 2.3]\n         # Reduced loss = (0 + 66.666 * 2.3) / 2\n \n-        assert np.isclose(K.eval(loss), 76.667, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 76.667, atol=1e-3)\n \n     def test_sample_weighted(self):\n         bce_obj = losses.BinaryCrossentropy()\n@@ -557,7 +557,7 @@ class TestBinaryCrossentropy(object):\n         #      = [0, 15.33, 0, 0]\n         # Reduced loss = 15.33 * 1.2 / 4\n \n-        assert np.isclose(K.eval(loss), 4.6, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 4.6, atol=1e-3)\n \n         # Test with logits.\n         y_true = K.constant([[1, 0, 1], [0, 1, 1]])\n@@ -572,7 +572,7 @@ class TestBinaryCrossentropy(object):\n         # Weighted loss = [0 * 4, 66.666 * 3]\n         # Reduced loss = (0 + 66.666 * 3) / 2\n \n-        assert np.isclose(K.eval(loss), 100, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 100, atol=1e-3)\n \n     def test_no_reduction(self):\n         y_true = K.constant([[1, 0, 1], [0, 1, 1]])\n@@ -585,7 +585,7 @@ class TestBinaryCrossentropy(object):\n         #            (where x = logits and z = y_true)\n         # Loss = [(0 + 0 + 0)/3, (200)/3]\n \n-        assert np.allclose(K.eval(loss), (0., 66.6666), rtol=1e-3)\n+        assert np.allclose(K.eval(loss), (0., 66.6666), atol=1e-3)\n \n     def test_label_smoothing(self):\n         logits = K.constant([[100.0, -100.0, -100.0]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d36c7b2d937aa85574b0ff993d6e8e8f15ccc6aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 3033 | Contributors (this commit): 33 | Commits (past 90d): 9 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -832,32 +832,6 @@ def test_check_bad_shape():\n             [a], [losses.CategoricalCrossentropy()], [(2, 3, 6)])\n \n \n-@pytest.mark.parametrize('input_metrics,expected_output', [\n-    (None, [[], []]),\n-    (['mse', 'mae'], [['mse', 'mae'], ['mse', 'mae']]),\n-    ({'layer_1': 'mae', 'layer_2': 'mse'}, [['mae'], ['mse']]),\n-])\n-def test_collect_metrics(input_metrics, expected_output):\n-    output_names = ['layer_1', 'layer_2']\n-\n-    output_metrics = training_utils.collect_metrics(input_metrics,\n-                                                    output_names)\n-    assert output_metrics == expected_output\n-\n-\n-def test_collect_metrics_with_invalid_metrics_format():\n-    with pytest.raises(TypeError):\n-        training_utils.collect_metrics({'a', 'set', 'type'}, [])\n-\n-\n-def test_collect_metrics_with_invalid_layer_name():\n-    with pytest.warns(Warning) as w:\n-        training_utils.collect_metrics({'unknown_layer': 'mse'}, ['layer_1'])\n-\n-    warning_raised = all(['unknown_layer' in str(w_.message) for w_ in w])\n-    assert warning_raised, 'Warning was raised for unknown_layer'\n-\n-\n @pytest.mark.skipif(K.backend() != 'tensorflow',\n                     reason='Requires TensorFlow backend')\n def test_model_with_input_feed_tensor():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#255a1acdc71c70ff4ed6905df588cda05edf0939", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 813 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -677,12 +677,16 @@ def huber_loss(y_true, y_pred, delta=1.0):\n def categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0):\n     y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n     y_true = K.cast(y_true, y_pred.dtype)\n-    label_smoothing = K.cast_to_floatx(label_smoothing)\n \n     def _smooth_labels():\n         num_classes = K.cast(K.shape(y_true)[1], y_pred.dtype)\n         return y_true * (1.0 - label_smoothing) + (label_smoothing / num_classes)\n-    y_true = K.switch(K.greater(label_smoothing, 0), _smooth_labels, lambda: y_true)\n+\n+    if label_smoothing is not 0:\n+        label_smoothing = K.cast_to_floatx(label_smoothing)\n+        y_true = K.switch(K.greater(label_smoothing, 0),\n+                          _smooth_labels,\n+                          lambda: y_true)\n     return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n \n \n@@ -694,6 +698,7 @@ def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n def binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0):\n     y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n     y_true = K.cast(y_true, y_pred.dtype)\n+    if label_smoothing is not 0:\n         label_smoothing = K.cast_to_floatx(label_smoothing)\n         y_true = K.switch(\n             K.greater(label_smoothing, 0),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2b4d8ffed21e6eaf31baf0b2405c878e70252b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 1 | Churn Cumulative: 7111 | Contributors (this commit): 39 | Commits (past 90d): 2 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -2119,6 +2119,5 @@ class TestBackend(object):\n                            KNP.eval(KNP.clip(x, min_val, max_val)))\n \n \n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10c8e50ba3fe2fb2a9693d0e1065c88849ee2721", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 7115 | Contributors (this commit): 39 | Commits (past 90d): 3 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -576,8 +576,8 @@ class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n \n-    @pytest.mark.skipif(K.backend() == 'theano',\n-                        reason='theano returns tuples for update ops')\n+    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+                        reason='theano returns tuples for updates; cntk buggy')\n     def test_update(self):\n         x = np.ones((3, 4))\n         x_var = K.variable(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4fe07cbf4b3633670db0991ff4cb9d845235bc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 846 | Contributors (this commit): 10 | Commits (past 90d): 12 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -678,15 +678,14 @@ def categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=\n     y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n     y_true = K.cast(y_true, y_pred.dtype)\n \n+    if label_smoothing is not 0:\n+        smoothing = K.cast_to_floatx(label_smoothing)\n+\n         def _smooth_labels():\n             num_classes = K.cast(K.shape(y_true)[1], y_pred.dtype)\n-        return y_true * (1.0 - label_smoothing) + (label_smoothing / num_classes)\n+            return y_true * (1.0 - smoothing) + (smoothing / num_classes)\n \n-    if label_smoothing is not 0:\n-        label_smoothing = K.cast_to_floatx(label_smoothing)\n-        y_true = K.switch(K.greater(label_smoothing, 0),\n-                          _smooth_labels,\n-                          lambda: y_true)\n+        y_true = K.switch(K.greater(smoothing, 0), _smooth_labels, lambda: y_true)\n     return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n \n \n@@ -699,10 +698,9 @@ def binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0):\n     y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n     y_true = K.cast(y_true, y_pred.dtype)\n     if label_smoothing is not 0:\n-        label_smoothing = K.cast_to_floatx(label_smoothing)\n-        y_true = K.switch(\n-            K.greater(label_smoothing, 0),\n-            lambda: y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing,\n+        smoothing = K.cast_to_floatx(label_smoothing)\n+        y_true = K.switch(K.greater(smoothing, 0),\n+                          lambda: y_true * (1.0 - smoothing) + 0.5 * smoothing,\n                           lambda: y_true)\n     return K.mean(\n         K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n\n@@ -19,7 +19,7 @@ accepted_name = ['from_config']\n accepted_module = ['keras.legacy.layers', 'keras.utils.generic_utils']\n \n # Functions or classes with less than 'MIN_CODE_SIZE' lines can be ignored\n-MIN_CODE_SIZE = 12\n+MIN_CODE_SIZE = 15\n \n \n def handle_class_init(name, member):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#040749502e1ecf3dbaab3a9b10d6c3d34a9454a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 4464 | Contributors (this commit): 41 | Commits (past 90d): 5 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -348,7 +348,11 @@ def int_shape(x):\n     if hasattr(x, '_keras_shape'):\n         return x._keras_shape\n \n+    if hasattr(x, 'shape'):\n         shape = x.shape\n+    else:\n+        shape = np.array(x).shape\n+\n     if hasattr(x, 'dynamic_axes'):\n         dynamic_shape = [None for a in x.dynamic_axes]\n         shape = tuple(dynamic_shape) + shape\n\n@@ -79,8 +79,8 @@ def _test_optimizer(optimizer, target=0.75):\n \n \n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason=\"Only Tensorflow raises a \"\n-                           \"ValueError if the gradient is null.\")\n+                    reason='Only Tensorflow raises a '\n+                           'ValueError if the gradient is null.')\n def test_no_grad():\n     inp = Input([3])\n     x = Dense(10)(inp)\n@@ -94,6 +94,8 @@ def test_no_grad():\n                 batch_size=10, epochs=10)\n \n \n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='Flaky with CNTK')\n def test_sgd():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n     _test_optimizer(sgd)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91658c513b0ff122b1df702fa6b2c9ba35464c52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 3036 | Contributors (this commit): 33 | Commits (past 90d): 10 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -699,7 +699,8 @@ def test_training_with_loss_instance():\n \n @pytest.mark.skipif(sys.version_info < (3,),\n                     reason='Cannot catch warnings in python 2')\n-def test_warnings():\n+def DISABLED_test_warnings():\n+    \"\"\"This test hangs Travis.\"\"\"\n     a = Input(shape=(3,), name='input_a')\n     b = Input(shape=(3,), name='input_b')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67fb3de0b3ac959cdda1160c41969737c57ce75b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 447 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -94,8 +94,6 @@ def test_no_grad():\n                 batch_size=10, epochs=10)\n \n \n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='Flaky with CNTK')\n def test_sgd():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n     _test_optimizer(sgd)\n@@ -140,6 +138,8 @@ def test_clipnorm():\n     _test_optimizer(sgd)\n \n \n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='Flaky with CNTK')\n def test_clipvalue():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipvalue=0.5)\n     _test_optimizer(sgd)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
