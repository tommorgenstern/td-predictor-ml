{"custom_id": "keras#0e7f3e04b040cd44f2ee124f55cb4054bea29206", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 687 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -279,8 +279,6 @@ class LSTMConv2D(RecurrentConv2D):\n         self.border_mode = border_mode\n         self.subsample = sub_sample\n \n-        \n-\n         if dim_ordering == 'th':\n             warnings.warn('Be carefull if used with convolution3D layers:\\n'\n                           'th in convolution 3D corresponds to '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6074a18ec4f23eaf06ac0f3e9be33361e05e192a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1510 | Contributors (this commit): 17 | Commits (past 90d): 10 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -450,7 +450,7 @@ class Adamax(Optimizer):\n             lr *= (1. / (1. + self.decay * self.iterations))\n \n         t = self.iterations + 1\n-        lr_t = self.lr / (1. - K.pow(self.beta_1, t))\n+        lr_t = lr / (1. - K.pow(self.beta_1, t))\n \n         shapes = [K.get_variable_shape(p) for p in params]\n         # zero init of 1st moment\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1bc0468ada74dcc3e2da45f5e373dbd7d0a901b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 1436 | Contributors (this commit): 8 | Commits (past 90d): 13 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -75,10 +75,12 @@ def deprocess_image(x):\n         x = x.transpose((1, 2, 0))\n     else:\n         x = x.reshape((img_width, img_height, 3))\n-    x = x[:, :, ::-1]\n+    # Remove zero-center by mean pixel\n     x[:, :, 0] += 103.939\n     x[:, :, 1] += 116.779\n     x[:, :, 2] += 123.68\n+    # 'BGR'->'RGB'\n+    x = x[:, :, ::-1]\n     x = np.clip(x, 0, 255).astype('uint8')\n     return x\n \n\n@@ -108,10 +108,12 @@ def deprocess_image(x):\n         x = x.transpose((1, 2, 0))\r\n     else:\r\n         x = x.reshape((img_nrows, img_ncols, 3))\r\n-    x = x[:, :, ::-1]\r\n+    # Remove zero-center by mean pixel\r\n     x[:, :, 0] += 103.939\r\n     x[:, :, 1] += 116.779\r\n     x[:, :, 2] += 123.68\r\n+    # 'BGR'->'RGB'\r\n+    x = x[:, :, ::-1]\r\n     x = np.clip(x, 0, 255).astype('uint8')\r\n     return x\r\n \r\n\n@@ -91,10 +91,12 @@ def deprocess_image(x):\n         x = x.transpose((1, 2, 0))\n     else:\n         x = x.reshape((img_nrows, img_ncols, 3))\n-    x = x[:, :, ::-1]\n+    # Remove zero-center by mean pixel\n     x[:, :, 0] += 103.939\n     x[:, :, 1] += 116.779\n     x[:, :, 2] += 123.68\n+    # 'BGR'->'RGB'\n+    x = x[:, :, ::-1]\n     x = np.clip(x, 0, 255).astype('uint8')\n     return x\n \n\n@@ -14,17 +14,19 @@ def preprocess_input(x, dim_ordering='default'):\n     assert dim_ordering in {'tf', 'th'}\n \n     if dim_ordering == 'th':\n+        # 'RGB'->'BGR'\n+        x = x[:, ::-1, :, :]\n+        # Zero-center by mean pixel\n         x[:, 0, :, :] -= 103.939\n         x[:, 1, :, :] -= 116.779\n         x[:, 2, :, :] -= 123.68\n-        # 'RGB'->'BGR'\n-        x = x[:, ::-1, :, :]\n     else:\n+        # 'RGB'->'BGR'\n+        x = x[:, :, :, ::-1]\n+        # Zero-center by mean pixel\n         x[:, :, :, 0] -= 103.939\n         x[:, :, :, 1] -= 116.779\n         x[:, :, :, 2] -= 123.68\n-        # 'RGB'->'BGR'\n-        x = x[:, :, :, ::-1]\n     return x\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#169c0896d6b7a9109b4fc462d24674e9b350a5e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 235 | Lines Deleted: 18 | Files Changed: 4 | Hunks: 27 | Methods Changed: 10 | Complexity Δ (Sum/Max): 28/19 | Churn Δ: 253 | Churn Cumulative: 12043 | Contributors (this commit): 68 | Commits (past 90d): 109 | Contributors (cumulative): 118 | DMM Complexity: 0.5648854961832062\n\nDIFF:\n@@ -845,6 +845,14 @@ def temporal_padding(x, padding=1):\n     return tf.pad(x, pattern)\n \n \n+def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n+    '''Pad the middle dimension of a 3D tensor\n+    with \"left_pad\" zeros left and \"right_pad\" right.\n+    '''\n+    pattern = [[0, 0], [left_pad, right_pad], [0, 0]]\n+    return tf.pad(x, pattern)\n+\n+\n def spatial_2d_padding(x, padding=(1, 1), dim_ordering=_IMAGE_DIM_ORDERING):\n     '''Pads the 2nd and 3rd dimensions of a 4D tensor\n     with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n@@ -859,6 +867,23 @@ def spatial_2d_padding(x, padding=(1, 1), dim_ordering=_IMAGE_DIM_ORDERING):\n     return tf.pad(x, pattern)\n \n \n+def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1, left_pad=1, right_pad=1, dim_ordering=_IMAGE_DIM_ORDERING):\n+    '''Pad the rows and columns of a 4D tensor\n+    with \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\"  (resp.) zeros rows on top, bottom; cols on left, right.\n+    '''\n+    if dim_ordering == 'th':\n+        pattern = [[0, 0],\n+                   [0, 0],\n+                   [top_pad, bottom_pad],\n+                   [left_pad, right_pad]]\n+    else:\n+        pattern = [[0, 0],\n+                   [top_pad, bottom_pad],\n+                   [left_pad, right_pad],\n+                   [0, 0]]\n+    return tf.pad(x, pattern)\n+\n+\n def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering=_IMAGE_DIM_ORDERING):\n     '''Pads 5D tensor with zeros for the depth, height, width dimension with\n     \"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right\n\n@@ -586,6 +586,21 @@ def temporal_padding(x, padding=1):\n     return T.set_subtensor(output[:, padding:x.shape[1] + padding, :], x)\n \n \n+def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n+    '''Pad the middle dimension of a 3D tensor\n+    with \"left_pad\" zeros left and \"right_pad\" right.\n+\n+    Apologies for the inane API, but Theano makes this\n+    really hard.\n+    '''\n+    input_shape = x.shape\n+    output_shape = (input_shape[0],\n+                    input_shape[1] + left_pad + right_pad,\n+                    input_shape[2])\n+    output = T.zeros(output_shape)\n+    return T.set_subtensor(output[:, left_pad:x.shape[1] + left_pad, :], x)\n+\n+\n def spatial_2d_padding(x, padding=(1, 1), dim_ordering=_IMAGE_DIM_ORDERING):\n     '''Pad the 2nd and 3rd dimensions of a 4D tensor\n     with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n@@ -617,6 +632,38 @@ def spatial_2d_padding(x, padding=(1, 1), dim_ordering=_IMAGE_DIM_ORDERING):\n     return T.set_subtensor(output[indices], x)\n \n \n+def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1, left_pad=1, right_pad=1, dim_ordering=_IMAGE_DIM_ORDERING):\n+    '''Pad the rows and columns of a 4D tensor\n+    with \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\"  (resp.) zeros rows on top, bottom; cols on left, right.\n+    '''\n+    input_shape = x.shape\n+    if dim_ordering == 'th':\n+        output_shape = (input_shape[0],\n+                        input_shape[1],\n+                        input_shape[2] + top_pad + bottom_pad,\n+                        input_shape[3] + left_pad + right_pad)\n+        output = T.zeros(output_shape)\n+        indices = (slice(None),\n+                   slice(None),\n+                   slice(top_pad, input_shape[2] + top_pad),\n+                   slice(left_pad, input_shape[3] + left_pad))\n+\n+    elif dim_ordering == 'tf':\n+        output_shape = (input_shape[0],\n+                        input_shape[1] + top_pad + bottom_pad,\n+                        input_shape[2] + left_pad + right_pad,\n+                        input_shape[3])\n+        print(output_shape)\n+        output = T.zeros(output_shape)\n+        indices = (slice(None),\n+                   slice(top_pad, input_shape[1] + top_pad),\n+                   slice(left_pad, input_shape[2] + left_pad),\n+                   slice(None))\n+    else:\n+        raise Exception('Invalid dim_ordering: ' + dim_ordering)\n+    return T.set_subtensor(output[indices], x)\n+\n+\n def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering=_IMAGE_DIM_ORDERING):\n     '''Pad the 2nd, 3rd and 4th dimensions of a 5D tensor\n     with \"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right.\n\n@@ -1412,9 +1412,15 @@ class ZeroPadding1D(Layer):\n     '''Zero-padding layer for 1D input (e.g. temporal sequence).\n \n     # Arguments\n-        padding: int\n+        padding: int or tuple of int (length 2) or dictionary\n+            For symmetric padding: int\n             How many zeros to add at the beginning and end of\n             the padding dimension (axis 1).\n+            For asymmetric padding: tuple of int (length 2)\n+            How many zeros to add at the beginning and at the end of\n+            the padding dimension '(left_pad, right_pad)' or\n+            '{'left_pad': left_pad, 'right_pad': right_pad}'.\n+            If any key is missing, default value of 0 will be used for the missing key.\n \n     # Input shape\n         3D tensor with shape (samples, axis_to_pad, features)\n@@ -1426,16 +1432,31 @@ class ZeroPadding1D(Layer):\n     def __init__(self, padding=1, **kwargs):\n         super(ZeroPadding1D, self).__init__(**kwargs)\n         self.padding = padding\n+\n+        if isinstance(padding, int):\n+            self.left_pad = padding\n+            self.right_pad = padding\n+        elif isinstance(padding, dict):\n+            if set(padding.keys()) <= {'left_pad', 'right_pad'}:\n+                self.left_pad = padding.get('left_pad', 0)\n+                self.right_pad = padding.get('right_pad', 0)\n+            else:\n+                raise ValueError('Unexpected key is found in the padding argument. '\n+                                 'Keys have to be in {\"left_pad\", \"right_pad\"}')\n+        else:\n+            padding = tuple(padding)\n+            self.left_pad = padding[0]\n+            self.right_pad = padding[1]\n         self.input_spec = [InputSpec(ndim=3)]\n \n     def get_output_shape_for(self, input_shape):\n-        length = input_shape[1] + self.padding * 2 if input_shape[1] is not None else None\n+        length = input_shape[1] + self.left_pad + self.right_pad if input_shape[1] is not None else None\n         return (input_shape[0],\n                 length,\n                 input_shape[2])\n \n     def call(self, x, mask=None):\n-        return K.temporal_padding(x, padding=self.padding)\n+        return K.asymmetric_temporal_padding(x, left_pad=self.left_pad, right_pad=self.right_pad)\n \n     def get_config(self):\n         config = {'padding': self.padding}\n@@ -1447,9 +1468,16 @@ class ZeroPadding2D(Layer):\n     '''Zero-padding layer for 2D input (e.g. picture).\n \n     # Arguments\n-        padding: tuple of int (length 2)\n+        padding: tuple of int (length 2) or tuple of int (length 4) or dictionary\n+            For symmetric padding tuple of int (length 2)\n             How many zeros to add at the beginning and end of\n-            the 2 padding dimensions (axis 3 and 4).\n+            the 2 padding dimensions (rows and cols).\n+            For asymmetric padding tuple of int (length 4)\n+            How many zeros to add at the beginning and at the end of\n+            the 2 padding dimensions (rows and cols).\n+            '(top_pad, bottom_pad, left_pad, right_pad)' or\n+            '{'top_pad': top_pad, 'bottom_pad': bottom_pad, 'left_pad': left_pad, 'right_pad': right_pad}'\n+            If any key is missing, default value of 0 will be used for the missing key.\n         dim_ordering: 'th' or 'tf'.\n             In 'th' mode, the channels dimension (the depth)\n             is at index 1, in 'tf' mode is it at index 3.\n@@ -1459,42 +1487,78 @@ class ZeroPadding2D(Layer):\n \n     # Input shape\n         4D tensor with shape:\n-        (samples, depth, first_axis_to_pad, second_axis_to_pad)\n+        `(samples, channels, rows, cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n \n     # Output shape\n         4D tensor with shape:\n-        (samples, depth, first_padded_axis, second_padded_axis)\n+        `(samples, channels, padded_rows, padded_cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, padded_rows, padded_cols, channels)` if dim_ordering='tf'.\n     '''\n \n-    def __init__(self, padding=(1, 1), dim_ordering='default', **kwargs):\n+    def __init__(self,\n+                 padding=(1, 1),\n+                 dim_ordering='default',\n+                 **kwargs):\n         super(ZeroPadding2D, self).__init__(**kwargs)\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n-        self.padding = tuple(padding)\n+\n+        self.padding = padding\n+        try:\n+            if set(padding.keys()) <= {'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}:\n+                self.top_pad = padding.get('top_pad', 0)\n+                self.bottom_pad = padding.get('bottom_pad', 0)\n+                self.left_pad = padding.get('left_pad', 0)\n+                self.right_pad = padding.get('right_pad', 0)\n+            else:\n+                raise ValueError('Unexpected key is found in the padding argument. '\n+                                 'Keys have to be in {\"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\"}')\n+        except AttributeError:\n+            padding = tuple(padding)\n+            if len(padding) == 2:\n+                self.top_pad = padding[0]\n+                self.bottom_pad = padding[0]\n+                self.left_pad = padding[1]\n+                self.right_pad = padding[1]\n+            elif len(padding) == 4:\n+                self.top_pad = padding[0]\n+                self.bottom_pad = padding[1]\n+                self.left_pad = padding[2]\n+                self.right_pad = padding[3]\n+            else:\n+                raise TypeError('padding should be tuple of int of length 2 or 4, or dict')\n+\n         assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=4)]\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n-            width = input_shape[2] + 2 * self.padding[0] if input_shape[2] is not None else None\n-            height = input_shape[3] + 2 * self.padding[1] if input_shape[3] is not None else None\n+            rows = input_shape[2] + self.top_pad + self.bottom_pad if input_shape[2] is not None else None\n+            cols = input_shape[3] + self.left_pad + self.right_pad if input_shape[3] is not None else None\n             return (input_shape[0],\n                     input_shape[1],\n-                    width,\n-                    height)\n+                    rows,\n+                    cols)\n         elif self.dim_ordering == 'tf':\n-            width = input_shape[1] + 2 * self.padding[0] if input_shape[1] is not None else None\n-            height = input_shape[2] + 2 * self.padding[1] if input_shape[2] is not None else None\n+            rows = input_shape[1] + self.top_pad + self.bottom_pad if input_shape[1] is not None else None\n+            cols = input_shape[2] + self.left_pad + self.right_pad if input_shape[2] is not None else None\n             return (input_shape[0],\n-                    width,\n-                    height,\n+                    rows,\n+                    cols,\n                     input_shape[3])\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n     def call(self, x, mask=None):\n-        return K.spatial_2d_padding(x, padding=self.padding,\n+        return K.asymmetric_spatial_2d_padding(x,\n+                                               top_pad=self.top_pad,\n+                                               bottom_pad=self.bottom_pad,\n+                                               left_pad=self.left_pad,\n+                                               right_pad=self.right_pad,\n                                                dim_ordering=self.dim_ordering)\n \n     def get_config(self):\n\n@@ -377,29 +377,110 @@ def test_averagepooling_3d():\n                    input_shape=(3, 4, 11, 12, 10))\n \n \n+@keras_test\n+def test_zero_padding_1d():\n+    nb_samples = 2\n+    input_dim = 2\n+    nb_steps = 11\n+    input = np.ones((nb_samples, nb_steps, input_dim))\n+\n+    # basic test\n+    layer_test(convolutional.ZeroPadding1D,\n+               kwargs={'padding': 2},\n+               input_shape=input.shape)\n+    layer_test(convolutional.ZeroPadding1D,\n+               kwargs={'padding': (1, 2)},\n+               input_shape=input.shape)\n+    layer_test(convolutional.ZeroPadding1D,\n+               kwargs={'padding': {'left_pad': 1, 'right_pad': 2}},\n+               input_shape=input.shape)\n+\n+    # correctness test\n+    layer = convolutional.ZeroPadding1D(padding=2)\n+    layer.set_input(K.variable(input), shape=input.shape)\n+\n+    out = K.eval(layer.output)\n+    for offset in [0, 1, -1, -2]:\n+        assert_allclose(out[:, offset, :], 0.)\n+    assert_allclose(out[:, 2:-2, :], 1.)\n+\n+    layer = convolutional.ZeroPadding1D(padding=(1, 2))\n+    layer.set_input(K.variable(input), shape=input.shape)\n+\n+    out = K.eval(layer.output)\n+    for left_offset in [0]:\n+        assert_allclose(out[:, left_offset, :], 0.)\n+    for right_offset in [-1, -2]:\n+        assert_allclose(out[:, right_offset, :], 0.)\n+    assert_allclose(out[:, 1:-2, :], 1.)\n+    layer.get_config()\n+\n+\n @keras_test\n def test_zero_padding_2d():\n     nb_samples = 2\n     stack_size = 2\n     input_nb_row = 11\n     input_nb_col = 12\n+    dim_ordering = K.image_dim_ordering()\n+    assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n \n+    if dim_ordering == 'tf':\n         input = np.ones((nb_samples, input_nb_row, input_nb_col, stack_size))\n+    elif dim_ordering == 'th':\n+        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n \n     # basic test\n     layer_test(convolutional.ZeroPadding2D,\n                kwargs={'padding': (2, 2)},\n                input_shape=input.shape)\n+    layer_test(convolutional.ZeroPadding2D,\n+               kwargs={'padding': (1, 2, 3, 4)},\n+               input_shape=input.shape)\n+    layer_test(convolutional.ZeroPadding2D,\n+               kwargs={'padding': {'top_pad': 1, 'bottom_pad': 2, 'left_pad': 3, 'right_pad': 4}},\n+               input_shape=input.shape)\n \n     # correctness test\n     layer = convolutional.ZeroPadding2D(padding=(2, 2))\n     layer.set_input(K.variable(input), shape=input.shape)\n \n     out = K.eval(layer.output)\n+    if dim_ordering == 'tf':\n         for offset in [0, 1, -1, -2]:\n             assert_allclose(out[:, offset, :, :], 0.)\n             assert_allclose(out[:, :, offset, :], 0.)\n         assert_allclose(out[:, 2:-2, 2:-2, :], 1.)\n+    elif dim_ordering == 'th':\n+        for offset in [0, 1, -1, -2]:\n+            assert_allclose(out[:, :, offset, :], 0.)\n+            assert_allclose(out[:, :, :, offset], 0.)\n+        assert_allclose(out[:, 2:-2, 2:-2, :], 1.)\n+\n+    layer = convolutional.ZeroPadding2D(padding=(1, 2, 3, 4))\n+    layer.set_input(K.variable(input), shape=input.shape)\n+\n+    out = K.eval(layer.output)\n+    if dim_ordering == 'tf':\n+        for top_offset in [0]:\n+            assert_allclose(out[:, top_offset, :, :], 0.)\n+        for bottom_offset in [-1, -2]:\n+            assert_allclose(out[:, bottom_offset, :, :], 0.)\n+        for left_offset in [0, 1, 2]:\n+            assert_allclose(out[:, :, left_offset, :], 0.)\n+        for right_offset in [-1, -2, -3, -4]:\n+            assert_allclose(out[:, :, right_offset, :], 0.)\n+        assert_allclose(out[:, 1:-2, 3:-4, :], 1.)\n+    elif dim_ordering == 'th':\n+        for top_offset in [0]:\n+            assert_allclose(out[:, :, top_offset, :], 0.)\n+        for bottom_offset in [-1, -2]:\n+            assert_allclose(out[:, :, bottom_offset, :], 0.)\n+        for left_offset in [0, 1, 2]:\n+            assert_allclose(out[:, :, :, left_offset], 0.)\n+        for right_offset in [-1, -2, -3, -4]:\n+            assert_allclose(out[:, :, :, right_offset], 0.)\n+        assert_allclose(out[:, :, 1:-2, 3:-4], 1.)\n     layer.get_config()\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5dd8c5c10ccc286a67ba4846a81701207b606e0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 26 | Files Changed: 2 | Hunks: 18 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 65 | Churn Cumulative: 6593 | Contributors (this commit): 31 | Commits (past 90d): 35 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1412,14 +1412,15 @@ class ZeroPadding1D(Layer):\n     '''Zero-padding layer for 1D input (e.g. temporal sequence).\n \n     # Arguments\n-        padding: int or tuple of int (length 2) or dictionary\n-            For symmetric padding: int\n+        padding: int, or tuple of int (length 2), or dictionary.\n+            - If int:\n             How many zeros to add at the beginning and end of\n             the padding dimension (axis 1).\n-            For asymmetric padding: tuple of int (length 2)\n+            - If tuple of int (length 2)\n             How many zeros to add at the beginning and at the end of\n-            the padding dimension '(left_pad, right_pad)' or\n-            '{'left_pad': left_pad, 'right_pad': right_pad}'.\n+            the padding dimension, in order '(left_pad, right_pad)'.\n+            - If dictionary: should contain the keys\n+            {'left_pad', 'right_pad'}.\n             If any key is missing, default value of 0 will be used for the missing key.\n \n     # Input shape\n@@ -1436,15 +1437,21 @@ class ZeroPadding1D(Layer):\n         if isinstance(padding, int):\n             self.left_pad = padding\n             self.right_pad = padding\n+\n         elif isinstance(padding, dict):\n             if set(padding.keys()) <= {'left_pad', 'right_pad'}:\n                 self.left_pad = padding.get('left_pad', 0)\n                 self.right_pad = padding.get('right_pad', 0)\n             else:\n-                raise ValueError('Unexpected key is found in the padding argument. '\n-                                 'Keys have to be in {\"left_pad\", \"right_pad\"}')\n+                raise ValueError('Unexpected key found in `padding` dictionary. '\n+                                 'Keys have to be in {\"left_pad\", \"right_pad\"}. '\n+                                 'Found: ' + str(padding.keys()))\n         else:\n             padding = tuple(padding)\n+            if len(padding) != 2:\n+                raise ValueError('`padding` should be int, or dict with keys '\n+                                 '{\"left_pad\", \"right_pad\"}, or tuple of length 2. '\n+                                 'Found: ' + str(padding))\n             self.left_pad = padding[0]\n             self.right_pad = padding[1]\n         self.input_spec = [InputSpec(ndim=3)]\n@@ -1468,15 +1475,16 @@ class ZeroPadding2D(Layer):\n     '''Zero-padding layer for 2D input (e.g. picture).\n \n     # Arguments\n-        padding: tuple of int (length 2) or tuple of int (length 4) or dictionary\n-            For symmetric padding tuple of int (length 2)\n+        padding: tuple of int (length 2), or tuple of int (length 4), or dictionary.\n+            - If tuple of int (length 2):\n             How many zeros to add at the beginning and end of\n             the 2 padding dimensions (rows and cols).\n-            For asymmetric padding tuple of int (length 4)\n+            - If tuple of int (length 4):\n             How many zeros to add at the beginning and at the end of\n-            the 2 padding dimensions (rows and cols).\n-            '(top_pad, bottom_pad, left_pad, right_pad)' or\n-            '{'top_pad': top_pad, 'bottom_pad': bottom_pad, 'left_pad': left_pad, 'right_pad': right_pad}'\n+            the 2 padding dimensions (rows and cols), in the order\n+            '(top_pad, bottom_pad, left_pad, right_pad)'.\n+            - If dictionary: should contain the keys\n+            {'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}.\n             If any key is missing, default value of 0 will be used for the missing key.\n         dim_ordering: 'th' or 'tf'.\n             In 'th' mode, the channels dimension (the depth)\n@@ -1507,16 +1515,18 @@ class ZeroPadding2D(Layer):\n             dim_ordering = K.image_dim_ordering()\n \n         self.padding = padding\n-        try:\n+        if isinstance(padding, dict):\n             if set(padding.keys()) <= {'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}:\n                 self.top_pad = padding.get('top_pad', 0)\n                 self.bottom_pad = padding.get('bottom_pad', 0)\n                 self.left_pad = padding.get('left_pad', 0)\n                 self.right_pad = padding.get('right_pad', 0)\n             else:\n-                raise ValueError('Unexpected key is found in the padding argument. '\n-                                 'Keys have to be in {\"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\"}')\n-        except AttributeError:\n+                raise ValueError('Unexpected key found in `padding` dictionary. '\n+                                 'Keys have to be in {\"top_pad\", \"bottom_pad\", '\n+                                 '\"left_pad\", \"right_pad\"}.'\n+                                 'Found: ' + str(padding.keys()))\n+        else:\n             padding = tuple(padding)\n             if len(padding) == 2:\n                 self.top_pad = padding[0]\n@@ -1529,9 +1539,11 @@ class ZeroPadding2D(Layer):\n                 self.left_pad = padding[2]\n                 self.right_pad = padding[3]\n             else:\n-                raise TypeError('padding should be tuple of int of length 2 or 4, or dict')\n+                raise TypeError('`padding` should be tuple of int '\n+                                'of length 2 or 4, or dict. '\n+                                'Found: ' + str(padding))\n \n-        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        assert dim_ordering in {'tf', 'th'}, '`dim_ordering` must be in {\"tf\", \"th\"}.'\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=4)]\n \n@@ -1574,6 +1586,7 @@ class ZeroPadding3D(Layer):\n         padding: tuple of int (length 3)\n             How many zeros to add at the beginning and end of\n             the 3 padding dimensions (axis 3, 4 and 5).\n+            Currentl only symmetric padding is supported.\n         dim_ordering: 'th' or 'tf'.\n             In 'th' mode, the channels dimension (the depth)\n             is at index 1, in 'tf' mode is it at index 4.\n\n@@ -381,7 +381,7 @@ def test_averagepooling_3d():\n def test_zero_padding_1d():\n     nb_samples = 2\n     input_dim = 2\n-    nb_steps = 11\n+    nb_steps = 5\n     input = np.ones((nb_samples, nb_steps, input_dim))\n \n     # basic test\n@@ -420,8 +420,8 @@ def test_zero_padding_1d():\n def test_zero_padding_2d():\n     nb_samples = 2\n     stack_size = 2\n-    input_nb_row = 11\n-    input_nb_col = 12\n+    input_nb_row = 4\n+    input_nb_col = 5\n     dim_ordering = K.image_dim_ordering()\n     assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n \n@@ -487,9 +487,9 @@ def test_zero_padding_2d():\n def test_zero_padding_3d():\n     nb_samples = 2\n     stack_size = 2\n-    input_len_dim1 = 10\n-    input_len_dim2 = 11\n-    input_len_dim3 = 12\n+    input_len_dim1 = 4\n+    input_len_dim2 = 5\n+    input_len_dim3 = 3\n \n     input = np.ones((nb_samples,\n                      input_len_dim1, input_len_dim2, input_len_dim3,\n@@ -608,7 +608,7 @@ def test_upsampling_3d():\n @keras_test\n def test_cropping_1d():\n     nb_samples = 2\n-    time_length = 10\n+    time_length = 4\n     input_len_dim1 = 2\n     input = np.random.rand(nb_samples, time_length, input_len_dim1)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1a72b36444c8027c27c2dc02dc03d5b69a5e389", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 33 | Files Changed: 2 | Hunks: 18 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 144 | Churn Cumulative: 1020 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -193,7 +193,7 @@ class RecurrentConv2D(Layer):\n                   'go_backwards': self.go_backwards,\n                   'stateful': self.stateful}\n         if self.stateful:\n-            config['batch_input_shape'] = self.input_shape\n+            config['batch_input_shape'] = self.input_spec[0].shape\n \n         base_config = super(RecurrentConv2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -202,23 +202,28 @@ class RecurrentConv2D(Layer):\n class LSTMConv2D(RecurrentConv2D):\n     '''\n     # Input shape\n+        - if dim_ordering='th'\n             5D tensor with shape:\n-            `(samples,time, channels, rows, cols)` if dim_ordering='th'\n-            or 5D tensor with shape:\n-            `(samples,time, rows, cols, channels)` if dim_ordering='tf'.\n+            `(samples,time, channels, rows, cols)`\n+        - if dim_ordering='tf'\n+            5D tensor with shape:\n+            `(samples,time, rows, cols, channels)`\n \n      # Output shape\n-        if return_sequences=False\n-\n-            4D tensor with shape:\n-            `(samples, nb_filter, o_row, o_col)` if dim_ordering='th'\n-            or 4D tensor with shape:\n-            `(samples, o_row, o_col, nb_filter)` if dim_ordering='tf'.\n-        if return_sequences=True\n+        - if `return_sequences`\n+             - if dim_ordering='th'\n                 5D tensor with shape:\n-            `(samples, time, nb_filter, o_row, o_col)` if dim_ordering='th'\n-            or 5D tensor with shape:\n-            `(samples, time, o_row, o_col, nb_filter)` if dim_ordering='tf'.\n+                `(samples, time, nb_filter, o_row, o_col)`\n+             - if dim_ordering='tf'\n+                5D tensor with shape:\n+                `(samples, time, o_row, o_col, nb_filter)`\n+        - else\n+            - if dim_ordering ='th'\n+                4D tensor with shape:\n+                `(samples, nb_filter, o_row, o_col)`\n+            - if dim_ordering='tf'\n+                4D tensor with shape:\n+                `(samples, o_row, o_col, nb_filter)`\n \n         where o_row and o_col depend on the shape of the filter and\n         the border_mode\n@@ -230,10 +235,10 @@ class LSTMConv2D(RecurrentConv2D):\n             border_mode: 'valid' or 'same'.\n             sub_sample: tuple of length 2. Factor by which to subsample output.\n                 Also called strides elsewhere.\n-            dim_ordering: \"tf\" if the feature are at the last dimension or \"th\"\n-            stateful : has not been checked yet.\n-\n-\n+            dim_ordering: 'tf' if the feature are at the last dimension or 'th'\n+            stateful : Boolean (default False). If True, the last state\n+                for each sample at index i in a batch will be used as initial\n+                state for the sample of index i in the following batch.\n             init: weight initialization function.\n                 Can be the name of an existing function (str),\n                 or a Theano function\n@@ -241,8 +246,7 @@ class LSTMConv2D(RecurrentConv2D):\n             inner_init: initialization function of the inner cells.\n             forget_bias_init: initialization function for the bias of the\n             forget gate.\n-                [Jozefowicz et al.]\n-                (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+                [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n                 recommend initializing with ones.\n             activation: activation function.\n                 Can be the name of an existing function (str),\n@@ -448,8 +452,8 @@ class LSTMConv2D(RecurrentConv2D):\n         assert len(states) == 4\n         h_tm1 = states[0]\n         c_tm1 = states[1]\n-        B_W = states[2]\n-        B_U = states[3]\n+        B_U = states[2]\n+        B_W = states[3]\n \n         x_i = self.conv_step(x * B_W[0], self.W_i, self.b_i,\n                              border_mode=self.border_mode)\n@@ -462,13 +466,13 @@ class LSTMConv2D(RecurrentConv2D):\n \n         # U : from nb_filter to nb_filter\n         # Same because must be stable in the ouptut space\n-        h_i = self.conv_step_hidden(h_tm1, self.U_i * B_U[0],\n+        h_i = self.conv_step_hidden(h_tm1 * B_U[0], self.U_i,\n                                     border_mode='same')\n-        h_f = self.conv_step_hidden(h_tm1, self.U_f * B_U[1],\n+        h_f = self.conv_step_hidden(h_tm1 * B_U[1], self.U_f,\n                                     border_mode='same')\n-        h_c = self.conv_step_hidden(h_tm1, self.U_c * B_U[2],\n+        h_c = self.conv_step_hidden(h_tm1 * B_U[2], self.U_c,\n                                     border_mode='same')\n-        h_o = self.conv_step_hidden(h_tm1, self.U_o * B_U[3],\n+        h_o = self.conv_step_hidden(h_tm1 * B_U[3], self.U_o,\n                                     border_mode='same')\n \n         i = self.inner_activation(x_i + h_i)\n@@ -482,8 +486,11 @@ class LSTMConv2D(RecurrentConv2D):\n     def get_constants(self, x):\n         constants = []\n         if 0 < self.dropout_U < 1:\n-            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.concatenate([ones] * self.output_dim, 1)\n+            ones = K.zeros_like(x)\n+            ones = K.sum(ones, axis=1)\n+            ones = self.conv_step(ones, K.zeros(self.W_shape),\n+                                  border_mode=self.border_mode)\n+            ones = ones + 1\n             B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones)\n                    for _ in range(4)]\n             constants.append(B_U)\n@@ -491,10 +498,9 @@ class LSTMConv2D(RecurrentConv2D):\n             constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n \n         if 0 < self.dropout_W < 1:\n-            input_shape = self.input_spec[0].shape\n-            input_dim = input_shape[-1]\n-            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.concatenate([ones] * input_dim, 1)\n+            ones = K.zeros_like(x)\n+            ones = K.sum(ones, axis=1)\n+            ones = ones + 1\n             B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones)\n                    for _ in range(4)]\n             constants.append(B_W)\n\n@@ -6,10 +6,11 @@ from keras import backend as K\n from keras.models import Sequential\n from keras.layers import recurrent_convolutional\n from keras.utils.test_utils import layer_test\n+from keras import regularizers\n \n \n def test_recurrent_convolutional():\n-    # First test for ouptput shape:\n+\n     nb_row = 4\n     nb_col = 4\n     nb_filter = 20\n@@ -30,6 +31,7 @@ def test_recurrent_convolutional():\n                                    input_channel)\n \n         for return_sequences in [True, False]:\n+            # test for ouptput shape:\n             output = layer_test(recurrent_convolutional.LSTMConv2D,\n                                 kwargs={'dim_ordering': dim_ordering,\n                                         'return_sequences': return_sequences,\n@@ -51,5 +53,75 @@ def test_recurrent_convolutional():\n \n             assert output.shape == tuple(output_shape)\n \n+            # Tests for statefulness\n+            model = Sequential()\n+            kwargs = {'dim_ordering': dim_ordering,\n+                      'return_sequences': return_sequences,\n+                      'nb_filter': nb_filter,\n+                      'nb_row': nb_row,\n+                      'nb_col': nb_col,\n+                      'stateful': True,\n+                      'batch_input_shape': input.shape,\n+                      'border_mode': \"same\"}\n+            layer = recurrent_convolutional.LSTMConv2D(**kwargs)\n+\n+            model.add(layer)\n+            model.compile(optimizer='sgd', loss='mse')\n+            out1 = model.predict(np.ones_like(input))\n+            assert(out1.shape == tuple(output_shape))\n+\n+            # train once so that the states change\n+            model.train_on_batch(np.ones_like(input),\n+                                 np.ones_like(output))\n+            out2 = model.predict(np.ones_like(input))\n+\n+            # if the state is not reset, output should be different\n+            assert(out1.max() != out2.max())\n+\n+            # check that output changes after states are reset\n+            # (even though the model itself didn't change)\n+            layer.reset_states()\n+            out3 = model.predict(np.ones_like(input))\n+            assert(out2.max() != out3.max())\n+\n+            # check that container-level reset_states() works\n+            model.reset_states()\n+            out4 = model.predict(np.ones_like(input))\n+            assert_allclose(out3, out4, atol=1e-5)\n+\n+            # check that the call to `predict` updated the states\n+            out5 = model.predict(np.ones_like(input))\n+            assert(out4.max() != out5.max())\n+\n+            # check regularizers\n+            kwargs = {'dim_ordering': dim_ordering,\n+                      'return_sequences': return_sequences,\n+                      'nb_filter': nb_filter,\n+                      'nb_row': nb_row,\n+                      'nb_col': nb_col,\n+                      'stateful': True,\n+                      'batch_input_shape': input.shape,\n+                      'W_regularizer': regularizers.WeightRegularizer(l1=0.01),\n+                      'U_regularizer': regularizers.WeightRegularizer(l1=0.01),\n+                      'b_regularizer': 'l2',\n+                      'border_mode': \"same\"}\n+\n+            layer = recurrent_convolutional.LSTMConv2D(**kwargs)\n+            layer.set_input(K.variable(np.ones(input.shape)),\n+                            shape=input.shape)\n+            K.eval(layer.output)\n+\n+            # check dropout\n+            layer_test(recurrent_convolutional.LSTMConv2D,\n+                       kwargs={'dim_ordering': dim_ordering,\n+                               'return_sequences': return_sequences,\n+                               'nb_filter': nb_filter,\n+                               'nb_row': nb_row,\n+                               'nb_col': nb_col,\n+                               'border_mode': \"same\",\n+                               'dropout_W': 0.1,\n+                               'dropout_U': 0.1},\n+                       input_shape=input.shape)\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#731e1bb2063ceeeee9fac4eaab8a12a1ac68ba2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 267 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -53,6 +53,10 @@ def test_recurrent_convolutional():\n \n             assert output.shape == tuple(output_shape)\n \n+            # No need to check statefulness for both\n+            if dim_ordering == 'th':\n+                continue\n+\n             # Tests for statefulness\n             model = Sequential()\n             kwargs = {'dim_ordering': dim_ordering,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c96373a411b1a9754ccad16ab5d23b80ed35d25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 269 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -54,7 +54,7 @@ def test_recurrent_convolutional():\n             assert output.shape == tuple(output_shape)\n \n             # No need to check statefulness for both\n-            if dim_ordering == 'th':\n+            if dim_ordering == 'th' or return_sequences:\n                 continue\n \n             # Tests for statefulness\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d53a1cd0c0fba16c9d68af58804ed53a3890f2c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 490 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -109,7 +109,7 @@ def paint_text(text, w, h):\n     a = np.frombuffer(buf, np.uint8)\n     a.shape = (h, w, 4)\n     a = a[:, :, 0]  # grab single channel\n-    a /= 255\n+    a = a.astype(np.float32) / 255\n     a = np.expand_dims(a, 0)\n     a = speckle(a)\n     a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1)\n@@ -396,7 +396,7 @@ pool_size_1 = 4\n pool_size_2 = 2\n time_dense_size = 32\n rnn_size = 512\n-time_steps = img_w / (pool_size_1 * pool_size_2)\n+time_steps = img_w // (pool_size_1 * pool_size_2)\n \n if K.image_dim_ordering() == 'th':\n     input_shape = (1, img_h, img_w)\n@@ -411,7 +411,7 @@ img_gen = TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_cle\n                              minibatch_size=32,\n                              img_w=img_w,\n                              img_h=img_h,\n-                             downsample_width=img_w / (pool_size_1 * pool_size_2) - 2,\n+                             downsample_width=img_w // (pool_size_1 * pool_size_2) - 2,\n                              val_split=words_per_epoch - val_words)\n \n act = 'relu'\n@@ -423,7 +423,7 @@ inner = Convolution2D(conv_num_filters, filter_size, filter_size, border_mode='s\n                       activation=act, name='conv2')(inner)\n inner = MaxPooling2D(pool_size=(pool_size_2, pool_size_2), name='max2')(inner)\n \n-conv_to_rnn_dims = ((img_h / (pool_size_1 * pool_size_2)) * conv_num_filters, img_w / (pool_size_1 * pool_size_2))\n+conv_to_rnn_dims = ((img_h // (pool_size_1 * pool_size_2)) * conv_num_filters, img_w // (pool_size_1 * pool_size_2))\n inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n inner = Permute(dims=(2, 1), name='permute')(inner)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86f28494a5460e7ae323c38ed5f5ada86d65d37c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1515 | Contributors (this commit): 17 | Commits (past 90d): 11 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -230,6 +230,7 @@ class RMSprop(Optimizer):\n     def get_config(self):\n         config = {'lr': float(K.get_value(self.lr)),\n                   'rho': float(K.get_value(self.rho)),\n+                  'decay': float(K.get_value(self.decay)),\n                   'epsilon': self.epsilon}\n         base_config = super(RMSprop, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -281,6 +282,7 @@ class Adagrad(Optimizer):\n \n     def get_config(self):\n         config = {'lr': float(K.get_value(self.lr)),\n+                  'decay': float(K.get_value(self.decay)),\n                   'epsilon': self.epsilon}\n         base_config = super(Adagrad, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -346,6 +348,7 @@ class Adadelta(Optimizer):\n     def get_config(self):\n         config = {'lr': float(K.get_value(self.lr)),\n                   'rho': self.rho,\n+                  'decay': float(K.get_value(self.decay)),\n                   'epsilon': self.epsilon}\n         base_config = super(Adadelta, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -411,6 +414,7 @@ class Adam(Optimizer):\n         config = {'lr': float(K.get_value(self.lr)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n+                  'decay': float(K.get_value(self.decay)),\n                   'epsilon': self.epsilon}\n         base_config = super(Adam, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -480,6 +484,7 @@ class Adamax(Optimizer):\n         config = {'lr': float(K.get_value(self.lr)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n+                  'decay': float(K.get_value(self.decay)),\n                   'epsilon': self.epsilon}\n         base_config = super(Adamax, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#79c133143297a37b630f791b02c88d58579e05ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 168 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,6 @@ from keras.layers import Dense\n from keras.layers import Embedding\n from keras.layers import GlobalAveragePooling1D\n from keras.datasets import imdb\n-from keras import backend as K\n \n \n def create_ngram_set(input_list, ngram_value=2):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#044071f0d52543a6e7751865f8f9ed2f8ff3919b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 26 | Churn Cumulative: 2930 | Contributors (this commit): 38 | Commits (past 90d): 48 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,11 @@\n import tensorflow as tf\n-from tensorflow.python.ops import control_flow_ops\n+\n from tensorflow.python.training import moving_averages\n try:\n     from tensorflow.python.ops import ctc_ops as ctc\n except ImportError:\n     import tensorflow.contrib.ctc as ctc\n+\n import numpy as np\n import os\n import copy\n@@ -1299,6 +1300,16 @@ def rnn(step_function, inputs, initial_states,\n     return last_output, outputs, new_states\n \n \n+def _cond(condition, then_lambda, else_lambda):\n+    '''Backwards compatible interface to tf.cond prior to public introduction.'''\n+    try:\n+        cond_fn = tf.cond\n+    except AttributeError:\n+        from tensorflow.python.ops import control_flow_ops\n+        cond_fn = control_flow_ops.cond\n+    return cond_fn(condition, then_lambda, else_lambda)\n+\n+\n def switch(condition, then_expression, else_expression):\n     '''Switches between two operations depending on a scalar value (int or bool).\n     Note that both `then_expression` and `else_expression`\n@@ -1310,9 +1321,8 @@ def switch(condition, then_expression, else_expression):\n         else_expression: TensorFlow operation.\n     '''\n     x_shape = copy.copy(then_expression.get_shape())\n-    x = control_flow_ops.cond(tf.cast(condition, 'bool'),\n-                              lambda: then_expression,\n-                              lambda: else_expression)\n+    x = _cond(tf.cast(condition, 'bool'),\n+              lambda: then_expression, lambda: else_expression)\n     x.set_shape(x_shape)\n     return x\n \n@@ -1327,9 +1337,7 @@ def in_train_phase(x, alt):\n         return alt\n     # else: assume learning phase is a placeholder.\n     x_shape = copy.copy(x.get_shape())\n-    x = control_flow_ops.cond(tf.cast(_LEARNING_PHASE, 'bool'),\n-                              lambda: x,\n-                              lambda: alt)\n+    x = _cond(tf.cast(_LEARNING_PHASE, 'bool'), lambda: x, lambda: alt)\n     x._uses_learning_phase = True\n     x.set_shape(x_shape)\n     return x\n@@ -1344,9 +1352,7 @@ def in_test_phase(x, alt):\n     elif _LEARNING_PHASE is 0:\n         return x\n     x_shape = copy.copy(x.get_shape())\n-    x = control_flow_ops.cond(tf.cast(_LEARNING_PHASE, 'bool'),\n-                              lambda: alt,\n-                              lambda: x)\n+    x = _cond(tf.cast(_LEARNING_PHASE, 'bool'), lambda: alt, lambda: x)\n     x._uses_learning_phase = True\n     x.set_shape(x_shape)\n     return x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
