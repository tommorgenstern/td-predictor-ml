{"custom_id": "keras#5a22094d046015d81a3f9196a7f2fa07e9dedf74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 1710 | Contributors (this commit): 7 | Commits (past 90d): 38 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import numpy as np\n-\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n@@ -212,7 +210,7 @@ class Hashing(Layer):\n         if not isinstance(\n             inputs, (tf.Tensor, tf.SparseTensor, tf.RaggedTensor)\n         ):\n-            inputs = tf.convert_to_tensor(np.array(inputs))\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         if isinstance(inputs, tf.SparseTensor):\n             indices = tf.SparseTensor(\n                 indices=inputs.indices,\n\n@@ -389,7 +389,9 @@ class HashingTest(testing.TestCase, parameterized.TestCase):\n     def test_hash_list_input(self, input_data, expected):\n         layer = layers.Hashing(num_bins=2)\n         out_data = layer(input_data)\n-        self.assertAllEqual(expected, np.array(out_data).tolist())\n+        self.assertAllEqual(\n+            expected, backend.convert_to_numpy(out_data).tolist()\n+        )\n \n \n # TODO: support tf.RaggedTensor.\n\n@@ -7,6 +7,7 @@ import tensorflow as tf\n import keras_core\n from keras_core import layers\n from keras_core import models\n+from keras_core import ops\n from keras_core import testing\n from keras_core.legacy.saving import legacy_h5_format\n from keras_core.saving import object_registration\n@@ -125,8 +126,8 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         self._check_reloading_model(ref_input, model)\n \n     def test_saving_lambda(self):\n-        mean = np.random.random((4, 2, 3))\n-        std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n+        mean = ops.random.uniform((4, 2, 3))\n+        std = ops.abs(ops.random.uniform((4, 2, 3))) + 1e-5\n         inputs = layers.Input(shape=(4, 2, 3))\n         output = layers.Lambda(\n             lambda image, mu, std: (image - mu) / std,\n\n@@ -177,10 +177,16 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         seed = ops.zeros((2,))\n         seed0 = ops.convert_to_numpy(seed)\n         x1, y1, seed = random_numbers(seed)\n+        x1 = ops.convert_to_numpy(x1)\n+        y1 = ops.convert_to_numpy(y1)\n         seed1 = ops.convert_to_numpy(seed)\n         x2, y2, seed = random_numbers(seed)\n+        x2 = ops.convert_to_numpy(x2)\n+        y2 = ops.convert_to_numpy(y2)\n         seed2 = ops.convert_to_numpy(seed)\n         x3, y3, seed = random_numbers(seed)\n+        x3 = ops.convert_to_numpy(x3)\n+        y3 = ops.convert_to_numpy(y3)\n         seed3 = ops.convert_to_numpy(seed)\n \n         self.assertNotEqual(seed0[1], seed1[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ccd4c0e1352eead1e76cbc91d391cb14dfb01f36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 890 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 27 | Complexity Δ (Sum/Max): 40/39 | Churn Δ: 890 | Churn Cumulative: 1071 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: 0.9962546816479401\n\nDIFF:\n@@ -0,0 +1,889 @@\n+\"\"\"\n+Title: Denoising Diffusion Implicit Models\n+Author: [András Béres](https://www.linkedin.com/in/andras-beres-789190210)\n+Date created: 2022/06/24\n+Last modified: 2022/06/24\n+Description: Generating images of flowers with denoising diffusion implicit models.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+### What are diffusion models?\n+\n+Recently, [denoising diffusion models](https://arxiv.org/abs/2006.11239), including\n+[score-based generative models](https://arxiv.org/abs/1907.05600), gained popularity as a\n+powerful class of generative models, that can [rival](https://arxiv.org/abs/2105.05233)\n+even [generative adversarial networks (GANs)](https://arxiv.org/abs/1406.2661) in image\n+synthesis quality. They tend to generate more diverse samples, while being stable to\n+train and easy to scale. Recent large diffusion models, such as\n+[DALL-E 2](https://openai.com/dall-e-2/) and [Imagen](https://imagen.research.google/),\n+have shown incredible text-to-image generation capability. One of their drawbacks is\n+however, that they are slower to sample from, because they require multiple forward passes\n+for generating an image.\n+\n+Diffusion refers to the process of turning a structured signal (an image) into noise\n+step-by-step. By simulating diffusion, we can generate noisy images from our training\n+images, and can train a neural network to try to denoise them. Using the trained network\n+we can simulate the opposite of diffusion, reverse diffusion, which is the process of an\n+image emerging from noise.\n+\n+![diffusion process gif](https://i.imgur.com/dipPOfa.gif)\n+\n+One-sentence summary: **diffusion models are trained to denoise noisy images, and can\n+generate images by iteratively denoising pure noise.**\n+\n+### Goal of this example\n+\n+This code example intends to be a minimal but feature-complete (with a generation quality\n+metric) implementation of diffusion models, with modest compute requirements and\n+reasonable performance. My implementation choices and hyperparameter tuning were done\n+with these goals in mind.\n+\n+Since currently the literature of diffusion models is\n+[mathematically quite complex](https://arxiv.org/abs/2206.00364)\n+with multiple theoretical frameworks\n+([score matching](https://arxiv.org/abs/1907.05600),\n+[differential equations](https://arxiv.org/abs/2011.13456),\n+[Markov chains](https://arxiv.org/abs/2006.11239)) and sometimes even\n+[conflicting notations (see Appendix C.2)](https://arxiv.org/abs/2010.02502),\n+it can be daunting trying to understand\n+them. My view of these models in this example will be that they learn to separate a\n+noisy image into its image and Gaussian noise components.\n+\n+In this example I made effort to break down all long mathematical expressions into\n+digestible pieces and gave all variables explanatory names. I also included numerous\n+links to relevant literature to help interested readers dive deeper into the topic, in\n+the hope that this code example will become a good starting point for practitioners\n+learning about diffusion models.\n+\n+In the following sections, we will implement a continuous time version of\n+[Denoising Diffusion Implicit Models (DDIMs)](https://arxiv.org/abs/2010.02502)\n+with deterministic sampling.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import math\n+import matplotlib.pyplot as plt\n+import tensorflow as tf\n+import tensorflow_datasets as tfds\n+\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import ops\n+\n+\"\"\"\n+## Hyperparameters\n+\"\"\"\n+\n+# data\n+dataset_name = \"oxford_flowers102\"\n+dataset_repetitions = 5\n+num_epochs = 1  # train for at least 50 epochs for good results\n+image_size = 64\n+# KID = Kernel Inception Distance, see related section\n+kid_image_size = 75\n+kid_diffusion_steps = 5\n+plot_diffusion_steps = 20\n+\n+# sampling\n+min_signal_rate = 0.02\n+max_signal_rate = 0.95\n+\n+# architecture\n+embedding_dims = 32\n+embedding_max_frequency = 1000.0\n+widths = [32, 64, 96, 128]\n+block_depth = 2\n+\n+# optimization\n+batch_size = 64\n+ema = 0.999\n+learning_rate = 1e-3\n+weight_decay = 1e-4\n+\n+\"\"\"\n+## Data pipeline\n+\n+We will use the\n+[Oxford Flowers 102](https://www.tensorflow.org/datasets/catalog/oxford_flowers102)\n+dataset for\n+generating images of flowers, which is a diverse natural dataset containing around 8,000\n+images. Unfortunately the official splits are imbalanced, as most of the images are\n+contained in the test split. We create new splits (80% train, 20% validation) using the\n+[Tensorflow Datasets slicing API](https://www.tensorflow.org/datasets/splits). We apply\n+center crops as preprocessing, and repeat the dataset multiple times (reason given in the\n+next section).\n+\"\"\"\n+\n+\n+def preprocess_image(data):\n+    # center crop image\n+    height = ops.shape(data[\"image\"])[0]\n+    width = ops.shape(data[\"image\"])[1]\n+    crop_size = ops.minimum(height, width)\n+    image = tf.image.crop_to_bounding_box(\n+        data[\"image\"],\n+        (height - crop_size) // 2,\n+        (width - crop_size) // 2,\n+        crop_size,\n+        crop_size,\n+    )\n+\n+    # resize and clip\n+    # for image downsampling it is important to turn on antialiasing\n+    image = tf.image.resize(\n+        image, size=[image_size, image_size], antialias=True\n+    )\n+    return ops.clip(image / 255.0, 0.0, 1.0)\n+\n+\n+def prepare_dataset(split):\n+    # the validation dataset is shuffled as well, because data order matters\n+    # for the KID estimation\n+    return (\n+        tfds.load(dataset_name, split=split, shuffle_files=True)\n+        .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n+        .cache()\n+        .repeat(dataset_repetitions)\n+        .shuffle(10 * batch_size)\n+        .batch(batch_size, drop_remainder=True)\n+        .prefetch(buffer_size=tf.data.AUTOTUNE)\n+    )\n+\n+\n+# load dataset\n+train_dataset = prepare_dataset(\"train[:80%]+validation[:80%]+test[:80%]\")\n+val_dataset = prepare_dataset(\"train[80%:]+validation[80%:]+test[80%:]\")\n+\n+\"\"\"\n+## Kernel inception distance\n+\n+[Kernel Inception Distance (KID)](https://arxiv.org/abs/1801.01401) is an image quality\n+metric which was proposed as a replacement for the popular\n+[Frechet Inception Distance (FID)](https://arxiv.org/abs/1706.08500).\n+I prefer KID to FID because it is simpler to\n+implement, can be estimated per-batch, and is computationally lighter. More details\n+[here](https://keras.io/examples/generative/gan_ada/#kernel-inception-distance).\n+\n+In this example, the images are evaluated at the minimal possible resolution of the\n+Inception network (75x75 instead of 299x299), and the metric is only measured on the\n+validation set for computational efficiency. We also limit the number of sampling steps\n+at evaluation to 5 for the same reason.\n+\n+Since the dataset is relatively small, we go over the train and validation splits\n+multiple times per epoch, because the KID estimation is noisy and compute-intensive, so\n+we want to evaluate only after many iterations, but for many iterations.\n+\n+\"\"\"\n+\n+\n+@keras.saving.register_keras_serializable()\n+class KID(keras.metrics.Metric):\n+    def __init__(self, name, **kwargs):\n+        super().__init__(name=name, **kwargs)\n+\n+        # KID is estimated per batch and is averaged across batches\n+        self.kid_tracker = keras.metrics.Mean(name=\"kid_tracker\")\n+\n+        # a pretrained InceptionV3 is used without its classification layer\n+        # transform the pixel values to the 0-255 range, then use the same\n+        # preprocessing as during pretraining\n+        self.encoder = keras.Sequential(\n+            [\n+                keras.Input(shape=(image_size, image_size, 3)),\n+                layers.Rescaling(255.0),\n+                layers.Resizing(height=kid_image_size, width=kid_image_size),\n+                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n+                keras.applications.InceptionV3(\n+                    include_top=False,\n+                    input_shape=(kid_image_size, kid_image_size, 3),\n+                    weights=\"imagenet\",\n+                ),\n+                layers.GlobalAveragePooling2D(),\n+            ],\n+            name=\"inception_encoder\",\n+        )\n+\n+    def polynomial_kernel(self, features_1, features_2):\n+        feature_dimensions = ops.cast(ops.shape(features_1)[1], dtype=\"float32\")\n+        return (\n+            features_1 @ ops.transpose(features_2) / feature_dimensions + 1.0\n+        ) ** 3.0\n+\n+    def update_state(self, real_images, generated_images, sample_weight=None):\n+        real_features = self.encoder(real_images, training=False)\n+        generated_features = self.encoder(generated_images, training=False)\n+\n+        # compute polynomial kernels using the two sets of features\n+        kernel_real = self.polynomial_kernel(real_features, real_features)\n+        kernel_generated = self.polynomial_kernel(\n+            generated_features, generated_features\n+        )\n+        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n+\n+        # estimate the squared maximum mean discrepancy using the average kernel values\n+        batch_size = real_features.shape[0]\n+        batch_size_f = ops.cast(batch_size, dtype=\"float32\")\n+        mean_kernel_real = ops.sum(\n+            kernel_real * (1.0 - ops.eye(batch_size))\n+        ) / (batch_size_f * (batch_size_f - 1.0))\n+        mean_kernel_generated = ops.sum(\n+            kernel_generated * (1.0 - ops.eye(batch_size))\n+        ) / (batch_size_f * (batch_size_f - 1.0))\n+        mean_kernel_cross = ops.mean(kernel_cross)\n+        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n+\n+        # update the average KID estimate\n+        self.kid_tracker.update_state(kid)\n+\n+    def result(self):\n+        return self.kid_tracker.result()\n+\n+    def reset_state(self):\n+        self.kid_tracker.reset_state()\n+\n+\n+\"\"\"\n+## Network architecture\n+\n+Here we specify the architecture of the neural network that we will use for denoising. We\n+build a [U-Net](https://arxiv.org/abs/1505.04597) with identical input and output\n+dimensions. U-Net is a popular semantic segmentation architecture, whose main idea is\n+that it progressively downsamples and then upsamples its input image, and adds skip\n+connections between layers having the same resolution. These help with gradient flow and\n+avoid introducing a representation bottleneck, unlike usual\n+[autoencoders](https://www.deeplearningbook.org/contents/autoencoders.html). Based on\n+this, one can view\n+[diffusion models as denoising autoencoders](https://benanne.github.io/2022/01/31/diffusion.html)\n+without a bottleneck.\n+\n+The network takes two inputs, the noisy images and the variances of their noise\n+components. The latter is required since denoising a signal requires different operations\n+at different levels of noise. We transform the noise variances using sinusoidal\n+embeddings, similarly to positional encodings used both in\n+[transformers](https://arxiv.org/abs/1706.03762) and\n+[NeRF](https://arxiv.org/abs/2003.08934). This helps the network to be\n+[highly sensitive](https://arxiv.org/abs/2006.10739) to the noise level, which is\n+crucial for good performance. We implement sinusoidal embeddings using a\n+[Lambda layer](https://keras.io/api/layers/core_layers/lambda/).\n+\n+Some other considerations:\n+\n+* We build the network using the\n+[Keras Functional API](https://keras.io/guides/functional_api/), and use\n+[closures](https://twitter.com/fchollet/status/1441927912836321280) to build blocks of\n+layers in a consistent style.\n+* [Diffusion models](https://arxiv.org/abs/2006.11239) embed the index of the timestep of\n+the diffusion process instead of the noise variance, while\n+[score-based models (Table 1)](https://arxiv.org/abs/2206.00364)\n+usually use some function of the noise level. I\n+prefer the latter so that we can change the sampling schedule at inference time, without\n+retraining the network.\n+* [Diffusion models](https://arxiv.org/abs/2006.11239) input the embedding to each\n+convolution block separately. We only input it at the start of the network for\n+simplicity, which in my experience barely decreases performance, because the skip and\n+residual connections help the information propagate through the network properly.\n+* In the literature it is common to use\n+[attention layers](https://keras.io/api/layers/attention_layers/multi_head_attention/)\n+at lower resolutions for better global coherence. I omitted it for simplicity.\n+* We disable the learnable center and scale parameters of the batch normalization layers,\n+since the following convolution layers make them redundant.\n+* We initialize the last convolution's kernel to all zeros as a good practice, making the\n+network predict only zeros after initialization, which is the mean of its targets. This\n+will improve behaviour at the start of training and make the mean squared error loss\n+start at exactly 1.\n+\"\"\"\n+\n+\n+@keras.saving.register_keras_serializable()\n+def sinusoidal_embedding(x):\n+    embedding_min_frequency = 1.0\n+    frequencies = ops.exp(\n+        ops.linspace(\n+            ops.log(embedding_min_frequency),\n+            ops.log(embedding_max_frequency),\n+            embedding_dims // 2,\n+        )\n+    )\n+    angular_speeds = ops.cast(2.0 * math.pi * frequencies, \"float32\")\n+    embeddings = ops.concatenate(\n+        [ops.sin(angular_speeds * x), ops.cos(angular_speeds * x)], axis=3\n+    )\n+    return embeddings\n+\n+\n+def ResidualBlock(width):\n+    def apply(x):\n+        input_width = x.shape[3]\n+        if input_width == width:\n+            residual = x\n+        else:\n+            residual = layers.Conv2D(width, kernel_size=1)(x)\n+        x = layers.BatchNormalization(center=False, scale=False)(x)\n+        x = layers.Conv2D(\n+            width, kernel_size=3, padding=\"same\", activation=\"swish\"\n+        )(x)\n+        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n+        x = layers.Add()([x, residual])\n+        return x\n+\n+    return apply\n+\n+\n+def DownBlock(width, block_depth):\n+    def apply(x):\n+        x, skips = x\n+        for _ in range(block_depth):\n+            x = ResidualBlock(width)(x)\n+            skips.append(x)\n+        x = layers.AveragePooling2D(pool_size=2)(x)\n+        return x\n+\n+    return apply\n+\n+\n+def UpBlock(width, block_depth):\n+    def apply(x):\n+        x, skips = x\n+        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n+        for _ in range(block_depth):\n+            x = layers.Concatenate()([x, skips.pop()])\n+            x = ResidualBlock(width)(x)\n+        return x\n+\n+    return apply\n+\n+\n+def get_network(image_size, widths, block_depth):\n+    noisy_images = keras.Input(shape=(image_size, image_size, 3))\n+    noise_variances = keras.Input(shape=(1, 1, 1))\n+\n+    e = layers.Lambda(sinusoidal_embedding, output_shape=(1, 1, 32))(\n+        noise_variances\n+    )\n+    e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)\n+\n+    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n+    x = layers.Concatenate()([x, e])\n+\n+    skips = []\n+    for width in widths[:-1]:\n+        x = DownBlock(width, block_depth)([x, skips])\n+\n+    for _ in range(block_depth):\n+        x = ResidualBlock(widths[-1])(x)\n+\n+    for width in reversed(widths[:-1]):\n+        x = UpBlock(width, block_depth)([x, skips])\n+\n+    x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)\n+\n+    return keras.Model([noisy_images, noise_variances], x, name=\"residual_unet\")\n+\n+\n+\"\"\"\n+This showcases the power of the Functional API. Note how we built a relatively complex\n+U-Net with skip connections, residual blocks, multiple inputs, and sinusoidal embeddings\n+in 80 lines of code!\n+\"\"\"\n+\n+\"\"\"\n+## Diffusion model\n+\n+### Diffusion schedule\n+\n+Let us say, that a diffusion process starts at time = 0, and ends at time = 1. This\n+variable will be called diffusion time, and can be either discrete (common in diffusion\n+models) or continuous (common in score-based models). I choose the latter, so that the\n+number of sampling steps can be changed at inference time.\n+\n+We need to have a function that tells us at each point in the diffusion process the noise\n+levels and signal levels of the noisy image corresponding to the actual diffusion time.\n+This will be called the diffusion schedule (see `diffusion_schedule()`).\n+\n+This schedule outputs two quantities: the `noise_rate` and the `signal_rate`\n+(corresponding to sqrt(1 - alpha) and sqrt(alpha) in the DDIM paper, respectively). We\n+generate the noisy image by weighting the random noise and the training image by their\n+corresponding rates and adding them together.\n+\n+Since the (standard normal) random noises and the (normalized) images both have zero mean\n+and unit variance, the noise rate and signal rate can be interpreted as the standard\n+deviation of their components in the noisy image, while the squares of their rates can be\n+interpreted as their variance (or their power in the signal processing sense). The rates\n+will always be set so that their squared sum is 1, meaning that the noisy images will\n+always have unit variance, just like its unscaled components.\n+\n+We will use a simplified, continuous version of the\n+[cosine schedule (Section 3.2)](https://arxiv.org/abs/2102.09672),\n+that is quite commonly used in the literature.\n+This schedule is symmetric, slow towards the start and end of the diffusion process, and\n+it also has a nice geometric interpretation, using the\n+[trigonometric properties of the unit circle](https://en.wikipedia.org/wiki/Unit_circle#/media/File:Circle-trig6.svg):\n+\n+![diffusion schedule gif](https://i.imgur.com/JW9W0fA.gif)\n+\n+### Training process\n+\n+The training procedure (see `train_step()` and `denoise()`) of denoising diffusion models\n+is the following: we sample random diffusion times uniformly, and mix the training images\n+with random gaussian noises at rates corresponding to the diffusion times. Then, we train\n+the model to separate the noisy image to its two components.\n+\n+Usually, the neural network is trained to predict the unscaled noise component, from\n+which the predicted image component can be calculated using the signal and noise rates.\n+Pixelwise\n+[mean squared error](https://keras.io/api/losses/regression_losses/#mean_squared_error-function) should\n+be used theoretically, however I recommend using\n+[mean absolute error](https://keras.io/api/losses/regression_losses/#mean_absolute_error-function)\n+instead (similarly to\n+[this](https://github.com/lucidrains/denoising-diffusion-pytorch/blob/master/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L371)\n+implementation), which produces better results on this dataset.\n+\n+### Sampling (reverse diffusion)\n+\n+When sampling (see `reverse_diffusion()`), at each step we take the previous estimate of\n+the noisy image and separate it into image and noise using our network. Then we recombine\n+these components using the signal and noise rate of the following step.\n+\n+Though a similar view is shown in\n+[Equation 12 of DDIMs](https://arxiv.org/abs/2010.02502), I believe the above explanation\n+of the sampling equation is not widely known.\n+\n+This example only implements the deterministic sampling procedure from DDIM, which\n+corresponds to *eta = 0* in the paper. One can also use stochastic sampling (in which\n+case the model becomes a\n+[Denoising Diffusion Probabilistic Model (DDPM)](https://arxiv.org/abs/2006.11239)),\n+where a part of the predicted noise is\n+replaced with the same or larger amount of random noise\n+([see Equation 16 and below](https://arxiv.org/abs/2010.02502)).\n+\n+Stochastic sampling can be used without retraining the network (since both models are\n+trained the same way), and it can improve sample quality, while on the other hand\n+requiring more sampling steps usually.\n+\"\"\"\n+\n+\n+@keras.saving.register_keras_serializable()\n+class DiffusionModel(keras.Model):\n+    def __init__(self, image_size, widths, block_depth):\n+        super().__init__()\n+\n+        self.normalizer = layers.Normalization()\n+        self.network = get_network(image_size, widths, block_depth)\n+        self.ema_network = keras.models.clone_model(self.network)\n+\n+    def compile(self, **kwargs):\n+        super().compile(**kwargs)\n+\n+        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n+        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n+        self.kid = KID(name=\"kid\")\n+\n+    @property\n+    def metrics(self):\n+        return [self.noise_loss_tracker, self.image_loss_tracker, self.kid]\n+\n+    def denormalize(self, images):\n+        # convert the pixel values back to 0-1 range\n+        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n+        return ops.clip(images, 0.0, 1.0)\n+\n+    def diffusion_schedule(self, diffusion_times):\n+        # diffusion times -> angles\n+        start_angle = ops.cast(ops.arccos(max_signal_rate), \"float32\")\n+        end_angle = ops.cast(ops.arccos(min_signal_rate), \"float32\")\n+\n+        diffusion_angles = start_angle + diffusion_times * (\n+            end_angle - start_angle\n+        )\n+\n+        # angles -> signal and noise rates\n+        signal_rates = ops.cos(diffusion_angles)\n+        noise_rates = ops.sin(diffusion_angles)\n+        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n+\n+        return noise_rates, signal_rates\n+\n+    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n+        # the exponential moving average weights are used at evaluation\n+        if training:\n+            network = self.network\n+        else:\n+            network = self.ema_network\n+\n+        # predict noise component and calculate the image component using it\n+        pred_noises = network(\n+            [noisy_images, noise_rates**2], training=training\n+        )\n+        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n+\n+        return pred_noises, pred_images\n+\n+    def reverse_diffusion(self, initial_noise, diffusion_steps):\n+        # reverse diffusion = sampling\n+        num_images = initial_noise.shape[0]\n+        step_size = 1.0 / diffusion_steps\n+\n+        # important line:\n+        # at the first sampling step, the \"noisy image\" is pure noise\n+        # but its signal rate is assumed to be nonzero (min_signal_rate)\n+        next_noisy_images = initial_noise\n+        for step in range(diffusion_steps):\n+            noisy_images = next_noisy_images\n+\n+            # separate the current noisy image to its components\n+            diffusion_times = ops.ones((num_images, 1, 1, 1)) - step * step_size\n+            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n+            pred_noises, pred_images = self.denoise(\n+                noisy_images, noise_rates, signal_rates, training=False\n+            )\n+            # network used in eval mode\n+\n+            # remix the predicted components using the next signal and noise rates\n+            next_diffusion_times = diffusion_times - step_size\n+            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n+                next_diffusion_times\n+            )\n+            next_noisy_images = (\n+                next_signal_rates * pred_images + next_noise_rates * pred_noises\n+            )\n+            # this new noisy image will be used in the next step\n+\n+        return pred_images\n+\n+    def generate(self, num_images, diffusion_steps):\n+        # noise -> images -> denormalized images\n+        initial_noise = keras.random.normal(\n+            shape=(num_images, image_size, image_size, 3)\n+        )\n+        generated_images = self.reverse_diffusion(\n+            initial_noise, diffusion_steps\n+        )\n+        generated_images = self.denormalize(generated_images)\n+        return generated_images\n+\n+    def train_step(self, images):\n+        # normalize images to have standard deviation of 1, like the noises\n+        images = self.normalizer(images, training=True)\n+        noises = keras.random.normal(\n+            shape=(batch_size, image_size, image_size, 3)\n+        )\n+\n+        # sample uniform random diffusion times\n+        diffusion_times = keras.random.uniform(\n+            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n+        )\n+        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n+        # mix the images with noises accordingly\n+        noisy_images = signal_rates * images + noise_rates * noises\n+\n+        with tf.GradientTape() as tape:\n+            # train the network to separate noisy images to their components\n+            pred_noises, pred_images = self.denoise(\n+                noisy_images, noise_rates, signal_rates, training=True\n+            )\n+\n+            noise_loss = self.loss(noises, pred_noises)  # used for training\n+            image_loss = self.loss(images, pred_images)  # only used as metric\n+\n+        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n+        self.optimizer.apply_gradients(\n+            zip(gradients, self.network.trainable_weights)\n+        )\n+\n+        self.noise_loss_tracker.update_state(noise_loss)\n+        self.image_loss_tracker.update_state(image_loss)\n+\n+        # track the exponential moving averages of weights\n+        for weight, ema_weight in zip(\n+            self.network.weights, self.ema_network.weights\n+        ):\n+            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n+\n+        # KID is not measured during the training phase for computational efficiency\n+        return {m.name: m.result() for m in self.metrics[:-1]}\n+\n+    def test_step(self, images):\n+        # normalize images to have standard deviation of 1, like the noises\n+        images = self.normalizer(images, training=False)\n+        noises = keras.random.normal(\n+            shape=(batch_size, image_size, image_size, 3)\n+        )\n+\n+        # sample uniform random diffusion times\n+        diffusion_times = keras.random.uniform(\n+            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n+        )\n+        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n+        # mix the images with noises accordingly\n+        noisy_images = signal_rates * images + noise_rates * noises\n+\n+        # use the network to separate noisy images to their components\n+        pred_noises, pred_images = self.denoise(\n+            noisy_images, noise_rates, signal_rates, training=False\n+        )\n+\n+        noise_loss = self.loss(noises, pred_noises)\n+        image_loss = self.loss(images, pred_images)\n+\n+        self.image_loss_tracker.update_state(image_loss)\n+        self.noise_loss_tracker.update_state(noise_loss)\n+\n+        # measure KID between real and generated images\n+        # this is computationally demanding, kid_diffusion_steps has to be small\n+        images = self.denormalize(images)\n+        generated_images = self.generate(\n+            num_images=batch_size, diffusion_steps=kid_diffusion_steps\n+        )\n+        self.kid.update_state(images, generated_images)\n+\n+        return {m.name: m.result() for m in self.metrics}\n+\n+    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6):\n+        # plot random generated images for visual evaluation of generation quality\n+        generated_images = self.generate(\n+            num_images=num_rows * num_cols,\n+            diffusion_steps=plot_diffusion_steps,\n+        )\n+\n+        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n+        for row in range(num_rows):\n+            for col in range(num_cols):\n+                index = row * num_cols + col\n+                plt.subplot(num_rows, num_cols, index + 1)\n+                plt.imshow(generated_images[index])\n+                plt.axis(\"off\")\n+        plt.tight_layout()\n+        plt.show()\n+        plt.close()\n+\n+\n+\"\"\"\n+## Training\n+\"\"\"\n+\n+# create and compile the model\n+model = DiffusionModel(image_size, widths, block_depth)\n+# below tensorflow 2.9:\n+# pip install tensorflow_addons\n+# import tensorflow_addons as tfa\n+# optimizer=tfa.optimizers.AdamW\n+model.compile(\n+    optimizer=keras.optimizers.AdamW(\n+        learning_rate=learning_rate, weight_decay=weight_decay\n+    ),\n+    loss=keras.losses.mean_absolute_error,\n+)\n+# pixelwise mean absolute error is used as loss\n+\n+# save the best model based on the validation KID metric\n+checkpoint_path = \"checkpoints/diffusion_model.weights.h5\"\n+checkpoint_callback = keras.callbacks.ModelCheckpoint(\n+    filepath=checkpoint_path,\n+    save_weights_only=True,\n+    monitor=\"val_kid\",\n+    mode=\"min\",\n+    save_best_only=True,\n+)\n+\n+# calculate mean and variance of training dataset for normalization\n+model.normalizer.adapt(train_dataset)\n+\n+# run training and plot generated images periodically\n+model.fit(\n+    train_dataset,\n+    epochs=num_epochs,\n+    validation_data=val_dataset,\n+    callbacks=[\n+        keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images),\n+        checkpoint_callback,\n+    ],\n+)\n+\n+\"\"\"\n+## Inference\n+\"\"\"\n+\n+# load the best model and generate images\n+model.load_weights(checkpoint_path)\n+model.plot_images()\n+\n+\"\"\"\n+## Results\n+\n+By running the training for at least 50 epochs (takes 2 hours on a T4 GPU and 30 minutes\n+on an A100 GPU), one can get high quality image generations using this code example.\n+\n+The evolution of a batch of images over a 80 epoch training (color artifacts are due to\n+GIF compression):\n+\n+![flowers training gif](https://i.imgur.com/FSCKtZq.gif)\n+\n+Images generated using between 1 and 20 sampling steps from the same initial noise:\n+\n+![flowers sampling steps gif](https://i.imgur.com/tM5LyH3.gif)\n+\n+Interpolation (spherical) between initial noise samples:\n+\n+![flowers interpolation gif](https://i.imgur.com/hk5Hd5o.gif)\n+\n+Deterministic sampling process (noisy images on top, predicted images on bottom, 40\n+steps):\n+\n+![flowers deterministic generation gif](https://i.imgur.com/wCvzynh.gif)\n+\n+Stochastic sampling process (noisy images on top, predicted images on bottom, 80 steps):\n+\n+![flowers stochastic generation gif](https://i.imgur.com/kRXOGzd.gif)\n+\n+Trained model and demo available on HuggingFace:\n+\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![model badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-DDIM-black.svg)](https://huggingface.co/keras-io/denoising-diffusion-implicit-models) | [![spaces badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-DDIM-black.svg)](https://huggingface.co/spaces/keras-io/denoising-diffusion-implicit-models) |\n+\"\"\"\n+\n+\"\"\"\n+## Lessons learned\n+\n+During preparation for this code example I have run numerous experiments using\n+[this repository](https://github.com/beresandras/clear-diffusion-keras).\n+In this section I list\n+the lessons learned and my recommendations in my subjective order of importance.\n+\n+### Algorithmic tips\n+\n+* **min. and max. signal rates**: I found the min. signal rate to be an important\n+hyperparameter. Setting it too low will make the generated images oversaturated, while\n+setting it too high will make them undersaturated. I recommend tuning it carefully. Also,\n+setting it to 0 will lead to a division by zero error. The max. signal rate can be set to\n+1, but I found that setting it lower slightly improves generation quality.\n+* **loss function**: While large models tend to use mean squared error (MSE) loss, I\n+recommend using mean absolute error (MAE) on this dataset. In my experience MSE loss\n+generates more diverse samples (it also seems to hallucinate more\n+[Section 3](https://arxiv.org/abs/2111.05826)), while MAE loss leads to smoother images.\n+I recommend trying both.\n+* **weight decay**: I did occasionally run into diverged trainings when scaling up the\n+model, and found that weight decay helps in avoiding instabilities at a low performance\n+cost. This is why I use\n+[AdamW](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/AdamW)\n+instead of [Adam](https://keras.io/api/optimizers/adam/) in this example.\n+* **exponential moving average of weights**: This helps to reduce the variance of the KID\n+metric, and helps in averaging out short-term changes during training.\n+* **image augmentations**: Though I did not use image augmentations in this example, in\n+my experience adding horizontal flips to the training increases generation performance,\n+while random crops do not. Since we use a supervised denoising loss, overfitting can be\n+an issue, so image augmentations might be important on small datasets. One should also be\n+careful not to use\n+[leaky augmentations](https://keras.io/examples/generative/gan_ada/#invertible-data-augmentation),\n+which can be done following\n+[this method (end of Section 5)](https://arxiv.org/abs/2206.00364) for instance.\n+* **data normalization**: In the literature the pixel values of images are usually\n+converted to the -1 to 1 range. For theoretical correctness, I normalize the images to\n+have zero mean and unit variance instead, exactly like the random noises.\n+* **noise level input**: I chose to input the noise variance to the network, as it is\n+symmetrical under our sampling schedule. One could also input the noise rate (similar\n+performance), the signal rate (lower performance), or even the\n+[log-signal-to-noise ratio (Appendix B.1)](https://arxiv.org/abs/2107.00630)\n+(did not try, as its range is highly\n+dependent on the min. and max. signal rates, and would require adjusting the min.\n+embedding frequency accordingly).\n+* **gradient clipping**: Using global gradient clipping with a value of 1 can help with\n+training stability for large models, but decreased performance significantly in my\n+experience.\n+* **residual connection downscaling**: For\n+[deeper models (Appendix B)](https://arxiv.org/abs/2205.11487), scaling the residual\n+connections with 1/sqrt(2) can be helpful, but did not help in my case.\n+* **learning rate**: For me, [Adam optimizer's](https://keras.io/api/optimizers/adam/)\n+default learning rate of 1e-3 worked very well, but lower learning rates are more common\n+in the [literature (Tables 11-13)](https://arxiv.org/abs/2105.05233).\n+\n+### Architectural tips\n+\n+* **sinusoidal embedding**: Using sinusoidal embeddings on the noise level input of the\n+network is crucial for good performance. I recommend setting the min. embedding frequency\n+to the reciprocal of the range of this input, and since we use the noise variance in this\n+example, it can be left always at 1. The max. embedding frequency controls the smallest\n+change in the noise variance that the network will be sensitive to, and the embedding\n+dimensions set the number of frequency components in the embedding. In my experience the\n+performance is not too sensitive to these values.\n+* **skip connections**: Using skip connections in the network architecture is absolutely\n+critical, without them the model will fail to learn to denoise at a good performance.\n+* **residual connections**: In my experience residual connections also significantly\n+improve performance, but this might be due to the fact that we only input the noise\n+level embeddings to the first layer of the network instead of to all of them.\n+* **normalization**: When scaling up the model, I did occasionally encounter diverged\n+trainings, using normalization layers helped to mitigate this issue. In the literature it\n+is common to use\n+[GroupNormalization](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/GroupNormalization)\n+(with 8 groups for example) or\n+[LayerNormalization](https://keras.io/api/layers/normalization_layers/layer_normalization/)\n+in the network, I however chose to use\n+[BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/),\n+as it gave similar benefits in my experiments but was computationally lighter.\n+* **activations**: The choice of activation functions had a larger effect on generation\n+quality than I expected. In my experiments using non-monotonic activation functions\n+outperformed monotonic ones (such as\n+[ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu)), with\n+[Swish](https://www.tensorflow.org/api_docs/python/tf/keras/activations/swish) performing\n+the best (this is also what [Imagen uses, page 41](https://arxiv.org/abs/2205.11487)).\n+* **attention**: As mentioned earlier, it is common in the literature to use\n+[attention layers](https://keras.io/api/layers/attention_layers/multi_head_attention/) at low\n+resolutions for better global coherence. I omitted them for simplicity.\n+* **upsampling**:\n+[Bilinear and nearest neighbour upsampling](https://keras.io/api/layers/reshaping_layers/up_sampling2d/)\n+in the network performed similarly, however I did not try\n+[transposed convolutions](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/).\n+\n+For a similar list about GANs check out\n+[this Keras tutorial](https://keras.io/examples/generative/gan_ada/#gan-tips-and-tricks).\n+\"\"\"\n+\n+\"\"\"\n+## What to try next?\n+\n+If you would like to dive in deeper to the topic, I recommend checking out\n+[this repository](https://github.com/beresandras/clear-diffusion-keras) that I created in\n+preparation for this code example, which implements a wider range of features in a\n+similar style, such as:\n+\n+* stochastic sampling\n+* second-order sampling based on the\n+[differential equation view of DDIMs (Equation 13)](https://arxiv.org/abs/2010.02502)\n+* more diffusion schedules\n+* more network output types: predicting image or\n+[velocity (Appendix D)](https://arxiv.org/abs/2202.00512) instead of noise\n+* more datasets\n+\"\"\"\n+\n+\"\"\"\n+## Related works\n+\n+* [Score-based generative modeling](https://yang-song.github.io/blog/2021/score/)\n+(blogpost)\n+* [What are diffusion models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)\n+(blogpost)\n+* [Annotated diffusion model](https://huggingface.co/blog/annotated-diffusion) (blogpost)\n+* [CVPR 2022 tutorial on diffusion models](https://cvpr2022-tutorial-diffusion-models.github.io/)\n+(slides available)\n+* [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364):\n+attempts unifying diffusion methods under a common framework\n+* High-level video overviews: [1](https://www.youtube.com/watch?v=yTAMrHVG1ew),\n+[2](https://www.youtube.com/watch?v=344w5h24-h8)\n+* Detailed technical videos: [1](https://www.youtube.com/watch?v=fbLgFrlTnGU),\n+[2](https://www.youtube.com/watch?v=W-O7AZNzbzQ)\n+* Score-based generative models: [NCSN](https://arxiv.org/abs/1907.05600),\n+[NCSN+](https://arxiv.org/abs/2006.09011), [NCSN++](https://arxiv.org/abs/2011.13456)\n+* Denoising diffusion models: [DDPM](https://arxiv.org/abs/2006.11239),\n+[DDIM](https://arxiv.org/abs/2010.02502), [DDPM+](https://arxiv.org/abs/2102.09672),\n+[DDPM++](https://arxiv.org/abs/2105.05233)\n+* Large diffusion models: [GLIDE](https://arxiv.org/abs/2112.10741),\n+[DALL-E 2](https://arxiv.org/abs/2204.06125/), [Imagen](https://arxiv.org/abs/2205.11487)\n+\n+\n+\"\"\"\n\n@@ -69,6 +69,7 @@ class Concatenate(Merge):\n                 if axis != concat_axis and axis_value == 1:\n                     del reduced_inputs_shapes[i][axis]\n \n+            if len(reduced_inputs_shapes[i]) > self.axis:\n                 del reduced_inputs_shapes[i][self.axis]\n             shape_set.add(tuple(reduced_inputs_shapes[i]))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ffc3271a558351231099400360d73363a67f527", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 967 | Lines Deleted: 152 | Files Changed: 6 | Hunks: 86 | Methods Changed: 60 | Complexity Δ (Sum/Max): 82/24 | Churn Δ: 1119 | Churn Cumulative: 3710 | Contributors (this commit): 14 | Commits (past 90d): 63 | Contributors (cumulative): 43 | DMM Complexity: 0.46405228758169936\n\nDIFF:\n@@ -41,7 +41,7 @@ def in_top_k(targets, predictions, k):\n     topk_values = top_k(predictions, k)[0]\n     targets_values = jnp.take_along_axis(predictions, targets, axis=-1)\n     mask = targets_values >= topk_values\n-    return jax.numpy.any(mask, axis=1)\n+    return jnp.any(mask, axis=1)\n \n \n def logsumexp(x, axis=None, keepdims=False):\n@@ -59,13 +59,13 @@ def qr(x, mode=\"reduced\"):\n             \"Expected one of {'reduced', 'complete'}. \"\n             f\"Received: mode={mode}\"\n         )\n-    return jax.numpy.linalg.qr(x, mode=mode)\n+    return jnp.linalg.qr(x, mode=mode)\n \n \n def extract_sequences(x, sequence_length, sequence_stride):\n     *batch_shape, signal_length = x.shape\n     batch_shape = list(batch_shape)\n-    x = jax.numpy.reshape(x, (math.prod(batch_shape), signal_length, 1))\n+    x = jnp.reshape(x, (math.prod(batch_shape), signal_length, 1))\n     x = jax.lax.conv_general_dilated_patches(\n         x,\n         (sequence_length,),\n@@ -73,54 +73,57 @@ def extract_sequences(x, sequence_length, sequence_stride):\n         \"VALID\",\n         dimension_numbers=(\"NTC\", \"OIT\", \"NTC\"),\n     )\n-    return jax.numpy.reshape(x, (*batch_shape, *x.shape[-2:]))\n+    return jnp.reshape(x, (*batch_shape, *x.shape[-2:]))\n \n \n-def _get_complex_tensor_from_tuple(a):\n-    if not isinstance(a, (tuple, list)) or len(a) != 2:\n+def _get_complex_tensor_from_tuple(x):\n+    if not isinstance(x, (tuple, list)) or len(x) != 2:\n         raise ValueError(\n-            \"Input `a` should be a tuple of two tensors - real and imaginary.\"\n-            f\"Received: a={a}\"\n+            \"Input `x` should be a tuple of two tensors - real and imaginary.\"\n+            f\"Received: x={x}\"\n         )\n     # `convert_to_tensor` does not support passing complex tensors. We separate\n     # the input out into real and imaginary and convert them separately.\n-    real, imag = a\n+    real, imag = x\n     # Check shapes.\n     if real.shape != imag.shape:\n         raise ValueError(\n-            \"Input `a` should be a tuple of two tensors - real and imaginary.\"\n+            \"Input `x` should be a tuple of two tensors - real and imaginary.\"\n             \"Both the real and imaginary parts should have the same shape. \"\n-            f\"Received: a[0].shape = {real.shape}, a[1].shape = {imag.shape}\"\n+            f\"Received: x[0].shape = {real.shape}, x[1].shape = {imag.shape}\"\n         )\n     # Ensure dtype is float.\n     if not jnp.issubdtype(real.dtype, jnp.floating) or not jnp.issubdtype(\n         imag.dtype, jnp.floating\n     ):\n         raise ValueError(\n-            \"At least one tensor in input `a` is not of type float.\"\n-            f\"Received: a={a}.\"\n+            \"At least one tensor in input `x` is not of type float.\"\n+            f\"Received: x={x}.\"\n         )\n     complex_input = jax.lax.complex(real, imag)\n     return complex_input\n \n \n-def fft(a):\n-    complex_input = _get_complex_tensor_from_tuple(a)\n-    complex_output = jax.numpy.fft.fft(complex_input)\n-    return jax.numpy.real(complex_output), jax.numpy.imag(complex_output)\n+def fft(x):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n+    complex_output = jnp.fft.fft(complex_input)\n+    return jnp.real(complex_output), jnp.imag(complex_output)\n \n \n-def fft2(a):\n-    complex_input = _get_complex_tensor_from_tuple(a)\n-    complex_output = jax.numpy.fft.fft2(complex_input)\n-    return jax.numpy.real(complex_output), jax.numpy.imag(complex_output)\n+def fft2(x):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n+    complex_output = jnp.fft.fft2(complex_input)\n+    return jnp.real(complex_output), jnp.imag(complex_output)\n \n \n def rfft(x, fft_length=None):\n-    complex_output = jax.numpy.fft.rfft(\n-        x, n=fft_length, axis=-1, norm=\"backward\"\n-    )\n-    return jax.numpy.real(complex_output), jax.numpy.imag(complex_output)\n+    complex_output = jnp.fft.rfft(x, n=fft_length, axis=-1, norm=\"backward\")\n+    return jnp.real(complex_output), jnp.imag(complex_output)\n+\n+\n+def irfft(x, fft_length=None):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n+    return jnp.fft.irfft(complex_input, n=fft_length, axis=-1, norm=\"backward\")\n \n \n def stft(\n@@ -143,13 +146,15 @@ def stft(\n                 \"If a string is passed to `window`, it must be one of \"\n                 f'`\"hann\"`, `\"hamming\"`. Received: window={window}'\n             )\n+    x = convert_to_tensor(x)\n \n     if center:\n         pad_width = [(0, 0) for _ in range(len(x.shape))]\n         pad_width[-1] = (fft_length // 2, fft_length // 2)\n         x = jnp.pad(x, pad_width, mode=\"reflect\")\n \n-    x = extract_sequences(x, fft_length, sequence_stride)\n+    l_pad = (fft_length - sequence_length) // 2\n+    r_pad = fft_length - sequence_length - l_pad\n \n     if window is not None:\n         if isinstance(window, str):\n@@ -163,12 +168,82 @@ def stft(\n                 \"The shape of `window` must be equal to [sequence_length].\"\n                 f\"Received: window shape={win.shape}\"\n             )\n+        win = jnp.pad(win, [[l_pad, r_pad]])\n+    else:\n+        win = jnp.ones((sequence_length + l_pad + r_pad), dtype=x.dtype)\n+\n+    result = jax.scipy.signal.stft(\n+        x,\n+        fs=1.0,\n+        window=win,\n+        nperseg=(sequence_length + l_pad + r_pad),\n+        noverlap=(sequence_length + l_pad + r_pad - sequence_stride),\n+        nfft=fft_length,\n+        boundary=None,\n+        padded=False,\n+    )[-1]\n+    # scale and swap to (..., num_sequences, fft_bins)\n+    scale = jnp.sqrt(1.0 / win.sum() ** 2)\n+    result = result / scale\n+    result = jnp.swapaxes(result, -2, -1)\n+    return jnp.real(result), jnp.imag(result)\n+\n+\n+def istft(\n+    x,\n+    sequence_length,\n+    sequence_stride,\n+    fft_length,\n+    length=None,\n+    window=\"hann\",\n+    center=True,\n+):\n+    x = _get_complex_tensor_from_tuple(x)\n+    dtype = jnp.real(x).dtype\n+\n+    expected_output_len = fft_length + sequence_stride * (x.shape[-2] - 1)\n     l_pad = (fft_length - sequence_length) // 2\n     r_pad = fft_length - sequence_length - l_pad\n-        win = jnp.pad(win, [[l_pad, r_pad]])\n-        x = jnp.multiply(x, win)\n \n-    return rfft(x, fft_length)\n+    if window is not None:\n+        if isinstance(window, str):\n+            win = convert_to_tensor(\n+                scipy.signal.get_window(window, sequence_length), dtype=dtype\n+            )\n+        else:\n+            win = convert_to_tensor(window, dtype=dtype)\n+        if len(win.shape) != 1 or win.shape[-1] != sequence_length:\n+            raise ValueError(\n+                \"The shape of `window` must be equal to [sequence_length].\"\n+                f\"Received: window shape={win.shape}\"\n+            )\n+        win = jnp.pad(win, [[l_pad, r_pad]])\n+    else:\n+        win = jnp.ones((sequence_length + l_pad + r_pad), dtype=dtype)\n+\n+    x = jax.scipy.signal.istft(\n+        x,\n+        fs=1.0,\n+        window=win,\n+        nperseg=(sequence_length + l_pad + r_pad),\n+        noverlap=(sequence_length + l_pad + r_pad - sequence_stride),\n+        nfft=fft_length,\n+        boundary=False,\n+        time_axis=-2,\n+        freq_axis=-1,\n+    )[-1]\n+\n+    # scale\n+    x = x / win.sum() if window is not None else x / sequence_stride\n+\n+    start = 0 if center is False else fft_length // 2\n+    if length is not None:\n+        end = start + length\n+    elif center is True:\n+        end = -(fft_length // 2)\n+    else:\n+        end = expected_output_len\n+    return x[..., start:end]\n \n \n def rsqrt(x):\n\n@@ -124,19 +124,59 @@ def extract_sequences(x, sequence_length, sequence_stride):\n     return np.reshape(x, (*batch_shape, *x.shape[-2:]))\n \n \n-def fft(a):\n-    real, imag = jax_fft(a)\n+def _get_complex_tensor_from_tuple(x):\n+    if not isinstance(x, (tuple, list)) or len(x) != 2:\n+        raise ValueError(\n+            \"Input `x` should be a tuple of two tensors - real and imaginary.\"\n+            f\"Received: x={x}\"\n+        )\n+    # `convert_to_tensor` does not support passing complex tensors. We separate\n+    # the input out into real and imaginary and convert them separately.\n+    real, imag = x\n+    # Check shapes.\n+    if real.shape != imag.shape:\n+        raise ValueError(\n+            \"Input `x` should be a tuple of two tensors - real and imaginary.\"\n+            \"Both the real and imaginary parts should have the same shape. \"\n+            f\"Received: x[0].shape = {real.shape}, x[1].shape = {imag.shape}\"\n+        )\n+    # Ensure dtype is float.\n+    if not np.issubdtype(real.dtype, np.floating) or not np.issubdtype(\n+        imag.dtype, np.floating\n+    ):\n+        raise ValueError(\n+            \"At least one tensor in input `x` is not of type float.\"\n+            f\"Received: x={x}.\"\n+        )\n+    complex_input = real + 1j * imag\n+    return complex_input\n+\n+\n+def fft(x):\n+    real, imag = jax_fft(x)\n     return np.array(real), np.array(imag)\n \n \n-def fft2(a):\n-    real, imag = jax_fft2(a)\n+def fft2(x):\n+    real, imag = jax_fft2(x)\n     return np.array(real), np.array(imag)\n \n \n def rfft(x, fft_length=None):\n     complex_output = np.fft.rfft(x, n=fft_length, axis=-1, norm=\"backward\")\n-    return np.real(complex_output), np.imag(complex_output)\n+    # numpy always outputs complex128, so we need to recast the dtype\n+    return (\n+        np.real(complex_output).astype(x.dtype),\n+        np.imag(complex_output).astype(x.dtype),\n+    )\n+\n+\n+def irfft(x, fft_length=None):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n+    # numpy always outputs float64, so we need to recast the dtype\n+    return np.fft.irfft(\n+        complex_input, n=fft_length, axis=-1, norm=\"backward\"\n+    ).astype(x[0].dtype)\n \n \n def stft(\n@@ -159,17 +199,21 @@ def stft(\n                 \"If a string is passed to `window`, it must be one of \"\n                 f'`\"hann\"`, `\"hamming\"`. Received: window={window}'\n             )\n+    x = convert_to_tensor(x)\n+    ori_dtype = x.dtype\n \n     if center:\n         pad_width = [(0, 0) for _ in range(len(x.shape))]\n         pad_width[-1] = (fft_length // 2, fft_length // 2)\n         x = np.pad(x, pad_width, mode=\"reflect\")\n-    x = extract_sequences(x, fft_length, sequence_stride)\n+\n+    l_pad = (fft_length - sequence_length) // 2\n+    r_pad = fft_length - sequence_length - l_pad\n \n     if window is not None:\n         if isinstance(window, str):\n-            win = scipy.signal.get_window(window, sequence_length).astype(\n-                x.dtype\n+            win = convert_to_tensor(\n+                scipy.signal.get_window(window, sequence_length), dtype=x.dtype\n             )\n         else:\n             win = convert_to_tensor(window, dtype=x.dtype)\n@@ -178,9 +222,79 @@ def stft(\n                 \"The shape of `window` must be equal to [sequence_length].\"\n                 f\"Received: window shape={win.shape}\"\n             )\n+        win = np.pad(win, [[l_pad, r_pad]])\n+    else:\n+        win = np.ones((sequence_length + l_pad + r_pad), dtype=x.dtype)\n+\n+    x = scipy.signal.stft(\n+        x,\n+        fs=1.0,\n+        window=win,\n+        nperseg=(sequence_length + l_pad + r_pad),\n+        noverlap=(sequence_length + l_pad + r_pad - sequence_stride),\n+        nfft=fft_length,\n+        boundary=None,\n+        padded=False,\n+    )[-1]\n+\n+    # scale and swap to (..., num_sequences, fft_bins)\n+    x = x / np.sqrt(1.0 / win.sum() ** 2)\n+    x = np.swapaxes(x, -2, -1)\n+    return np.real(x).astype(ori_dtype), np.imag(x).astype(ori_dtype)\n+\n+\n+def istft(\n+    x,\n+    sequence_length,\n+    sequence_stride,\n+    fft_length,\n+    length=None,\n+    window=\"hann\",\n+    center=True,\n+):\n+    x = _get_complex_tensor_from_tuple(x)\n+    dtype = np.real(x).dtype\n+\n+    expected_output_len = fft_length + sequence_stride * (x.shape[-2] - 1)\n     l_pad = (fft_length - sequence_length) // 2\n     r_pad = fft_length - sequence_length - l_pad\n-        win = np.pad(win, [[l_pad, r_pad]])\n-        x = np.multiply(x, win)\n \n-    return rfft(x, fft_length)\n+    if window is not None:\n+        if isinstance(window, str):\n+            win = convert_to_tensor(\n+                scipy.signal.get_window(window, sequence_length), dtype=dtype\n+            )\n+        else:\n+            win = convert_to_tensor(window, dtype=dtype)\n+        if len(win.shape) != 1 or win.shape[-1] != sequence_length:\n+            raise ValueError(\n+                \"The shape of `window` must be equal to [sequence_length].\"\n+                f\"Received: window shape={win.shape}\"\n+            )\n+        win = np.pad(win, [[l_pad, r_pad]])\n+    else:\n+        win = np.ones((sequence_length + l_pad + r_pad), dtype=dtype)\n+\n+    x = scipy.signal.istft(\n+        x,\n+        fs=1.0,\n+        window=win,\n+        nperseg=(sequence_length + l_pad + r_pad),\n+        noverlap=(sequence_length + l_pad + r_pad - sequence_stride),\n+        nfft=fft_length,\n+        boundary=False,\n+        time_axis=-2,\n+        freq_axis=-1,\n+    )[-1]\n+\n+    # scale\n+    x = x / win.sum() if window is not None else x / sequence_stride\n+\n+    start = 0 if center is False else fft_length // 2\n+    if length is not None:\n+        end = start + length\n+    elif center is True:\n+        end = -(fft_length // 2)\n+    else:\n+        end = expected_output_len\n+    return x[..., start:end]\n\n@@ -58,42 +58,42 @@ def extract_sequences(x, sequence_length, sequence_stride):\n     )\n \n \n-def _get_complex_tensor_from_tuple(a):\n-    if not isinstance(a, (tuple, list)) or len(a) != 2:\n+def _get_complex_tensor_from_tuple(x):\n+    if not isinstance(x, (tuple, list)) or len(x) != 2:\n         raise ValueError(\n-            \"Input `a` should be a tuple of two tensors - real and imaginary.\"\n-            f\"Received: a={a}\"\n+            \"Input `x` should be a tuple of two tensors - real and imaginary.\"\n+            f\"Received: x={x}\"\n         )\n     # `convert_to_tensor` does not support passing complex tensors. We separate\n     # the input out into real and imaginary and convert them separately.\n-    real, imag = a\n+    real, imag = x\n     real = convert_to_tensor(real)\n     imag = convert_to_tensor(imag)\n     # Check shapes.\n     if real.shape != imag.shape:\n         raise ValueError(\n-            \"Input `a` should be a tuple of two tensors - real and imaginary.\"\n+            \"Input `x` should be a tuple of two tensors - real and imaginary.\"\n             \"Both the real and imaginary parts should have the same shape. \"\n-            f\"Received: a[0].shape = {real.shape}, a[1].shape = {imag.shape}\"\n+            f\"Received: x[0].shape = {real.shape}, x[1].shape = {imag.shape}\"\n         )\n     # Ensure dtype is float.\n     if not real.dtype.is_floating or not imag.dtype.is_floating:\n         raise ValueError(\n-            \"At least one tensor in input `a` is not of type float.\"\n-            f\"Received: a={a}.\"\n+            \"At least one tensor in input `x` is not of type float.\"\n+            f\"Received: x={x}.\"\n         )\n     complex_input = tf.dtypes.complex(real, imag)\n     return complex_input\n \n \n-def fft(a):\n-    complex_input = _get_complex_tensor_from_tuple(a)\n+def fft(x):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n     complex_output = tf.signal.fft(complex_input)\n     return tf.math.real(complex_output), tf.math.imag(complex_output)\n \n \n-def fft2(a):\n-    complex_input = _get_complex_tensor_from_tuple(a)\n+def fft2(x):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n     complex_output = tf.signal.fft2d(complex_input)\n     return tf.math.real(complex_output), tf.math.imag(complex_output)\n \n@@ -105,6 +105,13 @@ def rfft(x, fft_length=None):\n     return tf.math.real(complex_output), tf.math.imag(complex_output)\n \n \n+def irfft(x, fft_length=None):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n+    if fft_length is not None:\n+        fft_length = [fft_length]\n+    return tf.signal.irfft(complex_input, fft_length)\n+\n+\n def stft(\n     x, sequence_length, sequence_stride, fft_length, window=\"hann\", center=True\n ):\n@@ -132,31 +139,102 @@ def stft(\n         pad_width[-1] = (fft_length // 2, fft_length // 2)\n         x = tf.pad(x, pad_width, mode=\"reflect\")\n \n-    x = extract_sequences(x, fft_length, sequence_stride)\n+    l_pad = (fft_length - sequence_length) // 2\n+    r_pad = fft_length - sequence_length - l_pad\n \n     if window is not None:\n         if isinstance(window, str):\n             if window == \"hann\":\n-                win = tf.signal.hann_window(\n+                win_array = tf.signal.hann_window(\n                     sequence_length, periodic=True, dtype=x.dtype\n                 )\n             else:\n-                win = tf.signal.hamming_window(\n+                win_array = tf.signal.hamming_window(\n                     sequence_length, periodic=True, dtype=x.dtype\n                 )\n         else:\n-            win = convert_to_tensor(window, dtype=x.dtype)\n-        if len(win.shape) != 1 or win.shape[-1] != sequence_length:\n+            win_array = convert_to_tensor(window, dtype=x.dtype)\n+        if len(win_array.shape) != 1 or win_array.shape[-1] != sequence_length:\n             raise ValueError(\n                 \"The shape of `window` must be equal to [sequence_length].\"\n-                f\"Received: window shape={win.shape}\"\n+                f\"Received: window shape={win_array.shape}\"\n+            )\n+        win_array = tf.pad(win_array, [[l_pad, r_pad]])\n+\n+        def win(frame_step, dtype):\n+            return win_array\n+\n+    else:\n+        win = None\n+\n+    result = tf.signal.stft(\n+        x,\n+        frame_length=(sequence_length + l_pad + r_pad),\n+        frame_step=sequence_stride,\n+        fft_length=fft_length,\n+        window_fn=win,\n+    )\n+    return tf.math.real(result), tf.math.imag(result)\n+\n+\n+def istft(\n+    x,\n+    sequence_length,\n+    sequence_stride,\n+    fft_length,\n+    length=None,\n+    window=\"hann\",\n+    center=True,\n+):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n+    dtype = tf.math.real(complex_input).dtype\n+\n+    expected_output_len = fft_length + sequence_stride * (\n+        tf.shape(complex_input)[-2] - 1\n     )\n     l_pad = (fft_length - sequence_length) // 2\n     r_pad = fft_length - sequence_length - l_pad\n-        win = tf.pad(win, [[l_pad, r_pad]])\n-        x = tf.multiply(x, win)\n \n-    return rfft(x, fft_length)\n+    if window is not None:\n+        if isinstance(window, str):\n+            if window == \"hann\":\n+                win_array = tf.signal.hann_window(\n+                    sequence_length, periodic=True, dtype=dtype\n+                )\n+            else:\n+                win_array = tf.signal.hamming_window(\n+                    sequence_length, periodic=True, dtype=dtype\n+                )\n+        else:\n+            win_array = convert_to_tensor(window, dtype=dtype)\n+        if len(win_array.shape) != 1 or win_array.shape[-1] != sequence_length:\n+            raise ValueError(\n+                \"The shape of `window` must be equal to [sequence_length].\"\n+                f\"Received: window shape={win_array.shape}\"\n+            )\n+        win_array = tf.pad(win_array, [[l_pad, r_pad]])\n+        win = tf.signal.inverse_stft_window_fn(\n+            sequence_stride, lambda frame_step, dtype: win_array\n+        )\n+    else:\n+        win = None\n+\n+    x = tf.signal.inverse_stft(\n+        complex_input,\n+        frame_length=(sequence_length + l_pad + r_pad),\n+        frame_step=sequence_stride,\n+        fft_length=fft_length,\n+        window_fn=win,\n+    )\n+\n+    start = 0 if center is False else fft_length // 2\n+    if length is not None:\n+        end = start + length\n+    elif center is True:\n+        end = -(fft_length // 2)\n+    else:\n+        end = expected_output_len\n+    return x[..., start:end]\n \n \n def rsqrt(x):\n\n@@ -1,3 +1,5 @@\n+import math\n+\n import torch\n \n from keras_core.backend import standardize_dtype\n@@ -130,43 +132,93 @@ def extract_sequences(x, sequence_length, sequence_stride):\n     )\n \n \n-def _get_complex_tensor_from_tuple(a):\n-    if not isinstance(a, (tuple, list)) or len(a) != 2:\n+def _overlap_sequences(x, sequence_stride):\n+    # Ref: https://github.com/google/jax/blob/main/jax/_src/scipy/signal.py\n+    x = convert_to_tensor(x)\n+    *batch_shape, num_sequences, sequence_length = x.shape\n+    if sequence_stride > sequence_length:\n         raise ValueError(\n-            \"Input `a` should be a tuple of two tensors - real and imaginary.\"\n-            f\"Received: a={a}\"\n+            \"`sequence_stride` must equal or less than x.shape[-1]. \"\n+            f\"Received: sequence_stride={sequence_stride}, \"\n+            f\"x.shape[-1]={sequence_length}\"\n+        )\n+    if sequence_stride < (sequence_length / num_sequences):\n+        raise ValueError(\n+            \"`sequence_stride` must equal or greater than \"\n+            \"x.shape[-1] / x.shape[-2]. \"\n+            f\"Received: sequence_stride={sequence_stride}, \"\n+            f\"x.shape[-1]={sequence_length}, x.shape[-2]={num_sequences}\"\n+        )\n+    flat_batchsize = math.prod(batch_shape)\n+    x = torch.reshape(x, (flat_batchsize, num_sequences, sequence_length))\n+    output_size = sequence_stride * (num_sequences - 1) + sequence_length\n+    nstep_per_segment = 1 + (sequence_length - 1) // sequence_stride\n+    # Here, we use shorter notation for axes.\n+    # B: batch_size, N: num_sequences, S: nstep_per_segment,\n+    # T: sequence_length divided by S\n+    padded_segment_len = nstep_per_segment * sequence_stride\n+    x = torch.nn.functional.pad(\n+        x, (0, padded_segment_len - sequence_length, 0, 0, 0, 0)\n+    )\n+    x = torch.reshape(\n+        x, (flat_batchsize, num_sequences, nstep_per_segment, sequence_stride)\n+    )\n+    # For obtaining shifted signals, this routine reinterprets flattened array\n+    # with a shrinked axis.  With appropriate truncation/ padding, this\n+    # operation pushes the last padded elements of the previous row to the head\n+    # of the current row.\n+    # See implementation of `overlap_and_add` in Tensorflow for details.\n+    x = torch.permute(x, (0, 2, 1, 3))  # x: (B, S, N, T)\n+    x = torch.nn.functional.pad(x, (0, 0, 0, num_sequences, 0, 0, 0, 0))\n+    # x: (B, S, N*2, T)\n+    shrinked = x.shape[2] - 1\n+    x = torch.reshape(x, (flat_batchsize, -1))\n+    x = x[:, : (nstep_per_segment * shrinked * sequence_stride)]\n+    x = torch.reshape(\n+        x, (flat_batchsize, nstep_per_segment, shrinked * sequence_stride)\n+    )\n+    # Finally, sum shifted segments, and truncate results to the output_size.\n+    x = torch.sum(x, dim=1)[:, :output_size]\n+    return torch.reshape(x, tuple(batch_shape) + (-1,))\n+\n+\n+def _get_complex_tensor_from_tuple(x):\n+    if not isinstance(x, (tuple, list)) or len(x) != 2:\n+        raise ValueError(\n+            \"Input `x` should be a tuple of two tensors - real and imaginary.\"\n+            f\"Received: x={x}\"\n         )\n     # `convert_to_tensor` does not support passing complex tensors. We separate\n     # the input out into real and imaginary and convert them separately.\n-    real, imag = a\n+    real, imag = x\n     real = convert_to_tensor(real)\n     imag = convert_to_tensor(imag)\n     # Check shape.\n     if real.shape != imag.shape:\n         raise ValueError(\n-            \"Input `a` should be a tuple of two tensors - real and imaginary.\"\n+            \"Input `x` should be a tuple of two tensors - real and imaginary.\"\n             \"Both the real and imaginary parts should have the same shape. \"\n-            f\"Received: a[0].shape = {real.shape}, a[1].shape = {imag.shape}\"\n+            f\"Received: x[0].shape = {real.shape}, x[1].shape = {imag.shape}\"\n         )\n     # Ensure dtype is float.\n     if not torch.is_floating_point(real) or not torch.is_floating_point(imag):\n         raise ValueError(\n-            \"At least one tensor in input `a` is not of type float.\"\n-            f\"Received: a={a}.\"\n+            \"At least one tensor in input `x` is not of type float.\"\n+            f\"Received: x={x}.\"\n         )\n \n     complex_input = torch.complex(real, imag)\n     return complex_input\n \n \n-def fft(a):\n-    complex_input = _get_complex_tensor_from_tuple(a)\n+def fft(x):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n     complex_output = torch.fft.fft(complex_input)\n     return complex_output.real, complex_output.imag\n \n \n-def fft2(a):\n-    complex_input = _get_complex_tensor_from_tuple(a)\n+def fft2(x):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n     complex_output = torch.fft.fft2(complex_input)\n     return complex_output.real, complex_output.imag\n \n@@ -177,6 +229,11 @@ def rfft(x, fft_length=None):\n     return complex_output.real, complex_output.imag\n \n \n+def irfft(x, fft_length=None):\n+    complex_input = _get_complex_tensor_from_tuple(x)\n+    return torch.fft.irfft(complex_input, n=fft_length, dim=-1, norm=\"backward\")\n+\n+\n def stft(\n     x, sequence_length, sequence_stride, fft_length, window=\"hann\", center=True\n ):\n@@ -199,16 +256,60 @@ def stft(\n             )\n     x = convert_to_tensor(x)\n \n-    if center:\n-        pad_width = [(0, 0) for _ in range(x.ndim)]\n-        pad_width[-1] = (fft_length // 2, fft_length // 2)\n-        # torch does not support reflect padding when x.ndim >= 3\n-        if x.ndim < 3:\n-            x = pad(x, pad_width, \"reflect\")\n+    if window is not None:\n+        if isinstance(window, str):\n+            if window == \"hann\":\n+                win = torch.hann_window(\n+                    sequence_length,\n+                    periodic=True,\n+                    dtype=x.dtype,\n+                    device=get_device(),\n+                )\n             else:\n-            x = pad(x, pad_width, \"constant\")\n+                win = torch.hamming_window(\n+                    sequence_length,\n+                    periodic=True,\n+                    dtype=x.dtype,\n+                    device=get_device(),\n+                )\n+        else:\n+            win = convert_to_tensor(window, dtype=x.dtype)\n+        if len(win.shape) != 1 or win.shape[-1] != sequence_length:\n+            raise ValueError(\n+                \"The shape of `window` must be equal to [sequence_length].\"\n+                f\"Received: window shape={win.shape}\"\n+            )\n+    else:\n+        win = torch.ones((sequence_length,), dtype=x.dtype, device=get_device())\n \n-    x = extract_sequences(x, fft_length, sequence_stride)\n+    x = torch.stft(\n+        x,\n+        n_fft=fft_length,\n+        hop_length=sequence_stride,\n+        win_length=sequence_length,\n+        window=win,\n+        center=center,\n+        return_complex=True,\n+    )\n+    x = torch.swapaxes(x, -2, -1)\n+    return x.real, x.imag\n+\n+\n+def istft(\n+    x,\n+    sequence_length,\n+    sequence_stride,\n+    fft_length,\n+    length=None,\n+    window=\"hann\",\n+    center=True,\n+):\n+    # ref:\n+    # torch: aten/src/ATen/native/SpectralOps.cpp\n+    # tf: tf.signal.inverse_stft_window_fn\n+    x = irfft(x, fft_length)\n+\n+    expected_output_len = fft_length + sequence_stride * (x.shape[-2] - 1)\n \n     if window is not None:\n         if isinstance(window, str):\n@@ -236,9 +337,29 @@ def stft(\n         l_pad = (fft_length - sequence_length) // 2\n         r_pad = fft_length - sequence_length - l_pad\n         win = pad(win, [[l_pad, r_pad]], \"constant\")\n+\n+        # square and sum\n+        _sequence_length = sequence_length + l_pad + r_pad\n+        denom = torch.square(win)\n+        overlaps = -(-_sequence_length // sequence_stride)\n+        denom = pad(denom, [(0, overlaps * sequence_stride - _sequence_length)])\n+        denom = torch.reshape(denom, [overlaps, sequence_stride])\n+        denom = torch.sum(denom, 0, keepdims=True)\n+        denom = torch.tile(denom, [overlaps, 1])\n+        denom = torch.reshape(denom, [overlaps * sequence_stride])\n+        win = torch.divide(win, denom[:_sequence_length])\n         x = torch.multiply(x, win)\n \n-    return rfft(x, fft_length)\n+    x = _overlap_sequences(x, sequence_stride)\n+\n+    start = 0 if center is False else fft_length // 2\n+    if length is not None:\n+        end = start + length\n+    elif center is True:\n+        end = -(fft_length // 2)\n+    else:\n+        end = expected_output_len\n+    return x[..., start:end]\n \n \n def rsqrt(x):\n\n@@ -392,7 +392,7 @@ class FFT(Operation):\n         # Both real and imaginary parts should have the same shape.\n         if real.shape != imag.shape:\n             raise ValueError(\n-                \"Input `a` should be a tuple of two tensors - real and \"\n+                \"Input `x` should be a tuple of two tensors - real and \"\n                 \"imaginary. Both the real and imaginary parts should have the \"\n                 f\"same shape. Received: x[0].shape = {real.shape}, \"\n                 f\"x[1].shape = {imag.shape}\"\n@@ -422,6 +422,32 @@ class FFT(Operation):\n         return backend.math.fft(x)\n \n \n+@keras_core_export(\"keras_core.ops.fft\")\n+def fft(x):\n+    \"\"\"Computes the Fast Fourier Transform along last axis of input.\n+\n+    Args:\n+        x: Tuple of the real and imaginary parts of the input tensor. Both\n+            tensors in the tuple should be of floating type.\n+\n+    Returns:\n+        A tuple containing two tensors - the real and imaginary parts of the\n+        output tensor.\n+\n+    Example:\n+\n+    >>> x = (\n+    ...     keras_core.ops.convert_to_tensor([1., 2.]),\n+    ...     keras_core.ops.convert_to_tensor([0., 1.]),\n+    ... )\n+    >>> fft(x)\n+    (array([ 3., -1.], dtype=float32), array([ 1., -1.], dtype=float32))\n+    \"\"\"\n+    if any_symbolic_tensors(x):\n+        return FFT().symbolic_call(x)\n+    return backend.math.fft(x)\n+\n+\n class FFT2(Operation):\n     def compute_output_spec(self, x):\n         if not isinstance(x, (tuple, list)) or len(x) != 2:\n@@ -464,32 +490,6 @@ class FFT2(Operation):\n         return backend.math.fft2(x)\n \n \n-@keras_core_export(\"keras_core.ops.fft\")\n-def fft(x):\n-    \"\"\"Computes the Fast Fourier Transform along last axis of input.\n-\n-    Args:\n-        x: Tuple of the real and imaginary parts of the input tensor. Both\n-            tensors in the tuple should be of floating type.\n-\n-    Returns:\n-        A tuple containing two tensors - the real and imaginary parts of the\n-        output tensor.\n-\n-    Example:\n-\n-    >>> x = (\n-    ...     keras_core.ops.convert_to_tensor([1., 2.]),\n-    ...     keras_core.ops.convert_to_tensor([0., 1.]),\n-    ... )\n-    >>> fft(x)\n-    (array([ 3., -1.], dtype=float32), array([ 1., -1.], dtype=float32))\n-    \"\"\"\n-    if any_symbolic_tensors(x):\n-        return FFT().symbolic_call(x)\n-    return backend.math.fft(x)\n-\n-\n @keras_core_export(\"keras_core.ops.fft2\")\n def fft2(x):\n     \"\"\"Computes the 2D Fast Fourier Transform along the last two axes of input.\n@@ -534,7 +534,10 @@ class RFFT(Operation):\n         if self.fft_length is not None:\n             new_last_dimension = self.fft_length // 2 + 1\n         else:\n+            if x.shape[-1] is not None:\n                 new_last_dimension = x.shape[-1] // 2 + 1\n+            else:\n+                new_last_dimension = None\n         new_shape = x.shape[:-1] + (new_last_dimension,)\n \n         return (\n@@ -586,6 +589,91 @@ def rfft(x, fft_length=None):\n     return backend.math.rfft(x, fft_length)\n \n \n+class IRFFT(Operation):\n+    def __init__(self, fft_length=None):\n+        super().__init__()\n+        self.fft_length = fft_length\n+\n+    def compute_output_spec(self, x):\n+        if not isinstance(x, (tuple, list)) or len(x) != 2:\n+            raise ValueError(\n+                \"Input `x` should be a tuple of two tensors - real and \"\n+                f\"imaginary. Received: x={x}\"\n+            )\n+        real, imag = x\n+        # Both real and imaginary parts should have the same shape.\n+        if real.shape != imag.shape:\n+            raise ValueError(\n+                \"Input `x` should be a tuple of two tensors - real and \"\n+                \"imaginary. Both the real and imaginary parts should have the \"\n+                f\"same shape. Received: x[0].shape = {real.shape}, \"\n+                f\"x[1].shape = {imag.shape}\"\n+            )\n+        # We are calculating 1D IRFFT. Hence, rank >= 1.\n+        if len(real.shape) < 1:\n+            raise ValueError(\n+                f\"Input should have rank >= 1. \"\n+                f\"Received: input.shape = {real.shape}\"\n+            )\n+\n+        if self.fft_length is not None:\n+            new_last_dimension = self.fft_length\n+        else:\n+            if real.shape[-1] is not None:\n+                new_last_dimension = 2 * (real.shape[-1] - 1)\n+            else:\n+                new_last_dimension = None\n+        new_shape = real.shape[:-1] + (new_last_dimension,)\n+        return KerasTensor(shape=new_shape, dtype=real.dtype)\n+\n+    def call(self, x):\n+        return backend.math.irfft(x, fft_length=self.fft_length)\n+\n+\n+@keras_core_export(\"keras_core.ops.irfft\")\n+def irfft(x, fft_length=None):\n+    \"\"\"Inverse real-valued Fast Fourier transform along the last axis.\n+\n+    Computes the inverse 1D Discrete Fourier Transform of a real-valued signal\n+    over the inner-most dimension of input.\n+\n+    The inner-most dimension of the input is assumed to be the result of RFFT:\n+    the `fft_length / 2 + 1` unique components of the DFT of a real-valued\n+    signal. If `fft_length` is not provided, it is computed from the size of the\n+    inner-most dimension of the input `(fft_length = 2 * (inner - 1))`. If the\n+    FFT length used to compute is odd, it should be provided since it cannot\n+    be inferred properly.\n+\n+    Along the axis IRFFT is computed on, if `fft_length / 2 + 1` is smaller than\n+    the corresponding dimension of the input, the dimension is cropped. If it is\n+    larger, the dimension is padded with zeros.\n+\n+    Args:\n+        x: Tuple of the real and imaginary parts of the input tensor. Both\n+            tensors in the tuple should be of floating type.\n+        fft_length: An integer representing the number of the fft length. If not\n+            specified, it is inferred from the length of the last axis of `x`.\n+            Defaults to `None`.\n+\n+    Returns:\n+        A tensor containing the inverse real-valued Fast Fourier Transform\n+        along the last axis of `x`.\n+\n+    Examples:\n+\n+    >>> real = keras_core.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n+    >>> imag = keras_core.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n+    >>> irfft((real, imag))\n+    array([0.66666667, -0.9106836, 0.24401694])\n+\n+    >>> irfft(rfft(real, 5), 5)\n+    array([0.0, 1.0, 2.0, 3.0, 4.0])\n+    \"\"\"\n+    if any_symbolic_tensors(x):\n+        return IRFFT(fft_length).symbolic_call(x)\n+    return backend.math.irfft(x, fft_length)\n+\n+\n class STFT(Operation):\n     def __init__(\n         self,\n@@ -603,10 +691,10 @@ class STFT(Operation):\n         self.center = center\n \n     def compute_output_spec(self, x):\n-        if len(x.shape) < 1:\n+        if len(x.shape) not in {1, 2}:\n             raise ValueError(\n-                f\"Input should have rank >= 1. \"\n-                f\"Received: input shape = {x.shape}\"\n+                f\"Input should have rank 1 (single sequence) or 2 \"\n+                f\"(batched sequences). Received: input shape = {x.shape}\"\n             )\n         if x.shape[-1] is not None:\n             padded = 0 if self.center is False else (self.fft_length // 2) * 2\n@@ -687,6 +775,127 @@ def stft(\n     )\n \n \n+class ISTFT(Operation):\n+    def __init__(\n+        self,\n+        sequence_length,\n+        sequence_stride,\n+        fft_length,\n+        length=None,\n+        window=\"hann\",\n+        center=True,\n+    ):\n+        super().__init__()\n+        self.sequence_length = sequence_length\n+        self.sequence_stride = sequence_stride\n+        self.fft_length = fft_length\n+        self.length = length\n+        self.window = window\n+        self.center = center\n+\n+    def compute_output_spec(self, x):\n+        if not isinstance(x, (tuple, list)) or len(x) != 2:\n+            raise ValueError(\n+                \"Input `x` should be a tuple of two tensors - real and \"\n+                f\"imaginary. Received: x={x}\"\n+            )\n+        real, imag = x\n+        # Both real and imaginary parts should have the same shape.\n+        if real.shape != imag.shape:\n+            raise ValueError(\n+                \"Input `x` should be a tuple of two tensors - real and \"\n+                \"imaginary. Both the real and imaginary parts should have the \"\n+                f\"same shape. Received: x[0].shape = {real.shape}, \"\n+                f\"x[1].shape = {imag.shape}\"\n+            )\n+        if len(real.shape) < 2:\n+            raise ValueError(\n+                f\"Input should have rank >= 2. \"\n+                f\"Received: input.shape = {real.shape}\"\n+            )\n+        if real.shape[-2] is not None:\n+            output_size = (\n+                real.shape[-2] - 1\n+            ) * self.sequence_stride + self.fft_length\n+            if self.length is not None:\n+                output_size = self.length\n+            elif self.center:\n+                output_size = output_size - (self.fft_length // 2) * 2\n+        else:\n+            output_size = None\n+        new_shape = real.shape[:-2] + (output_size,)\n+        return KerasTensor(shape=new_shape, dtype=real.dtype)\n+\n+    def call(self, x):\n+        return backend.math.istft(\n+            x,\n+            sequence_length=self.sequence_length,\n+            sequence_stride=self.sequence_stride,\n+            fft_length=self.fft_length,\n+            length=self.length,\n+            window=self.window,\n+            center=self.center,\n+        )\n+\n+\n+@keras_core_export(\"keras_core.ops.istft\")\n+def istft(\n+    x,\n+    sequence_length,\n+    sequence_stride,\n+    fft_length,\n+    length=None,\n+    window=\"hann\",\n+    center=True,\n+):\n+    \"\"\"Inverse Short-Time Fourier Transform along the last axis of the input.\n+\n+    To reconstruct an original waveform, the parameters should be the same in\n+    `stft`.\n+\n+    Args:\n+        x: Tuple of the real and imaginary parts of the input tensor. Both\n+            tensors in the tuple should be of floating type.\n+        sequence_length: An integer representing the sequence length.\n+        sequence_stride: An integer representing the sequence hop size.\n+        fft_length: An integer representing the size of the FFT that produced\n+            `stft`.\n+        length: An integer representing the output is clipped to exactly length.\n+            If not specified, no padding or clipping take place. Defaults to\n+            `None`.\n+        window: A string, a tensor of the window or `None`. If `window` is a\n+            string, available values are `\"hann\"` and `\"hamming\"`. If `window`\n+            is a tensor, it will be used directly as the window and its length\n+            must be `sequence_length`. If `window` is `None`, no windowing is\n+            used. Defaults to `\"hann\"`.\n+        center: Whether `x` was padded on both sides so that the t-th sequence\n+            is centered at time `t * sequence_stride`. Defaults to `True`.\n+\n+    Returns:\n+        A tensor containing the inverse Short-Time Fourier Transform along the\n+        last axis of `x`.\n+\n+    Example:\n+\n+    >>> x = keras_core.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n+    >>> istft(stft(x, 1, 1, 1), 1, 1, 1)\n+    array([0.0, 1.0, 2.0, 3.0, 4.0])\n+    \"\"\"\n+    if any_symbolic_tensors(x):\n+        return ISTFT(\n+            sequence_length, sequence_stride, fft_length, length, center, window\n+        ).symbolic_call(x)\n+    return backend.math.istft(\n+        x,\n+        sequence_length=sequence_length,\n+        sequence_stride=sequence_stride,\n+        fft_length=fft_length,\n+        length=length,\n+        window=window,\n+        center=center,\n+    )\n+\n+\n class Rsqrt(Operation):\n     \"\"\"Computes reciprocal of square root of x element-wise.\n \n\n@@ -1,3 +1,5 @@\n+import math\n+\n import numpy as np\n import pytest\n import scipy.signal\n@@ -12,58 +14,127 @@ from keras_core.ops import math as kmath\n def _stft(\n     x, sequence_length, sequence_stride, fft_length, window=\"hann\", center=True\n ):\n-    # pure numpy version of stft\n-    if backend.standardize_dtype(x.dtype) not in {\"float32\", \"float64\"}:\n-        raise TypeError(\n-            \"Invalid input type. Expected `float32` or `float64`. \"\n-            f\"Received: input type={x.dtype}\"\n-        )\n-    if fft_length < sequence_length:\n-        raise ValueError(\n-            \"`fft_length` must equal or larger than `sequence_length`. \"\n-            f\"Received: sequence_length={sequence_length}, \"\n-            f\"fft_length={fft_length}\"\n-        )\n+    # pure numpy version of stft that matches librosa's implementation\n+    x = np.array(x)\n+    ori_dtype = x.dtype\n \n     if center:\n         pad_width = [(0, 0) for _ in range(len(x.shape))]\n         pad_width[-1] = (fft_length // 2, fft_length // 2)\n-        if backend.backend() != \"torch\" or len(x.shape) < 3:\n         x = np.pad(x, pad_width, mode=\"reflect\")\n-        else:\n-            # torch not support reflect padding for N-D cases when N >= 3\n-            x = np.pad(x, pad_width, mode=\"constant\")\n \n-    # extract_sequences\n-    *batch_shape, _ = x.shape\n-    batch_shape = list(batch_shape)\n-    shape = x.shape[:-1] + (\n-        (x.shape[-1] - (fft_length - sequence_stride)) // sequence_stride,\n-        fft_length,\n-    )\n-    strides = x.strides[:-1] + (sequence_stride * x.strides[-1], x.strides[-1])\n-    x = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)\n-    x = np.reshape(x, (*batch_shape, *x.shape[-2:]))\n+    l_pad = (fft_length - sequence_length) // 2\n+    r_pad = fft_length - sequence_length - l_pad\n \n     if window is not None:\n         if isinstance(window, str):\n             window = scipy.signal.get_window(window, sequence_length)\n         win = np.array(window, dtype=x.dtype)\n-        if len(win.shape) != 1 or win.shape[-1] != sequence_length:\n-            raise ValueError(\n-                \"The shape of `window` must be equal to [sequence_length].\"\n-                f\"Received: window shape={win.shape}\"\n+        win = np.pad(win, [[l_pad, r_pad]])\n+    else:\n+        win = np.ones((sequence_length + l_pad + r_pad), dtype=x.dtype)\n+\n+    x = scipy.signal.stft(\n+        x,\n+        fs=1.0,\n+        window=win,\n+        nperseg=(sequence_length + l_pad + r_pad),\n+        noverlap=(sequence_length + l_pad + r_pad - sequence_stride),\n+        nfft=fft_length,\n+        boundary=None,\n+        padded=False,\n+    )[-1]\n+\n+    # scale and swap to (..., num_sequences, fft_bins)\n+    x = x / np.sqrt(1.0 / win.sum() ** 2)\n+    x = np.swapaxes(x, -2, -1)\n+    return np.real(x).astype(ori_dtype), np.imag(x).astype(ori_dtype)\n+\n+\n+def _istft(\n+    x,\n+    sequence_length,\n+    sequence_stride,\n+    fft_length,\n+    length=None,\n+    window=\"hann\",\n+    center=True,\n+):\n+    # pure numpy version of istft that matches librosa's implementation\n+    complex_input = x[0] + 1j * x[1]\n+    x = np.fft.irfft(\n+        complex_input, n=fft_length, axis=-1, norm=\"backward\"\n+    ).astype(x[0].dtype)\n+\n+    expected_output_len = fft_length + sequence_stride * (x.shape[-2] - 1)\n+\n+    if window is not None:\n+        if isinstance(window, str):\n+            win = np.array(\n+                scipy.signal.get_window(window, sequence_length), dtype=x.dtype\n             )\n+        else:\n+            win = np.array(window, dtype=x.dtype)\n         l_pad = (fft_length - sequence_length) // 2\n         r_pad = fft_length - sequence_length - l_pad\n         win = np.pad(win, [[l_pad, r_pad]])\n+\n+        # square and sum\n+        _sequence_length = sequence_length + l_pad + r_pad\n+        denom = np.square(win)\n+        overlaps = -(-_sequence_length // sequence_stride)\n+        denom = np.pad(\n+            denom, [(0, overlaps * sequence_stride - _sequence_length)]\n+        )\n+        denom = np.reshape(denom, [overlaps, sequence_stride])\n+        denom = np.sum(denom, 0, keepdims=True)\n+        denom = np.tile(denom, [overlaps, 1])\n+        denom = np.reshape(denom, [overlaps * sequence_stride])\n+        win = np.divide(win, denom[:_sequence_length])\n         x = np.multiply(x, win)\n \n-    x = np.fft.rfft(x, fft_length)\n-    return np.real(x), np.imag(x)\n+    # overlap_sequences\n+    def _overlap_sequences(x, sequence_stride):\n+        *batch_shape, num_sequences, sequence_length = x.shape\n+        flat_batchsize = math.prod(batch_shape)\n+        x = np.reshape(x, (flat_batchsize, num_sequences, sequence_length))\n+        output_size = sequence_stride * (num_sequences - 1) + sequence_length\n+        nstep_per_segment = 1 + (sequence_length - 1) // sequence_stride\n+        padded_segment_len = nstep_per_segment * sequence_stride\n+        x = np.pad(\n+            x, ((0, 0), (0, 0), (0, padded_segment_len - sequence_length))\n+        )\n+        x = np.reshape(\n+            x,\n+            (flat_batchsize, num_sequences, nstep_per_segment, sequence_stride),\n+        )\n+        x = x.transpose((0, 2, 1, 3))\n+        x = np.pad(x, ((0, 0), (0, 0), (0, num_sequences), (0, 0)))\n+        shrinked = x.shape[2] - 1\n+        x = np.reshape(x, (flat_batchsize, -1))\n+        x = x[:, : (nstep_per_segment * shrinked * sequence_stride)]\n+        x = np.reshape(\n+            x, (flat_batchsize, nstep_per_segment, shrinked * sequence_stride)\n+        )\n+        x = np.sum(x, axis=1)[:, :output_size]\n+        return np.reshape(x, tuple(batch_shape) + (-1,))\n+\n+    x = _overlap_sequences(x, sequence_stride)\n+\n+    if backend.backend() in {\"numpy\", \"jax\"}:\n+        x = np.nan_to_num(x)\n+\n+    start = 0 if center is False else fft_length // 2\n+    if length is not None:\n+        end = start + length\n+    elif center:\n+        end = -(fft_length // 2)\n+    else:\n+        end = expected_output_len\n+    return x[..., start:end]\n \n \n-class MathOpsDynamicShapeTest(testing.TestCase):\n+class MathOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n     def test_segment_sum(self):\n         data = KerasTensor((None, 4), dtype=\"float32\")\n         segment_ids = KerasTensor((10,), dtype=\"int32\")\n@@ -154,14 +225,24 @@ class MathOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(real_output.shape, ref_shape)\n         self.assertEqual(imag_output.shape, ref_shape)\n \n-    def test_rfft(self):\n+    @parameterized.parameters([(None,), (1,), (5,)])\n+    def test_rfft(self, fft_length):\n         x = KerasTensor((None, 4, 3), dtype=\"float32\")\n-        real_output, imag_output = kmath.rfft(x)\n-        ref = np.fft.rfft(np.ones((2, 4, 3)))\n+        real_output, imag_output = kmath.rfft(x, fft_length=fft_length)\n+        ref = np.fft.rfft(np.ones((2, 4, 3)), n=fft_length)\n         ref_shape = (None,) + ref.shape[1:]\n         self.assertEqual(real_output.shape, ref_shape)\n         self.assertEqual(imag_output.shape, ref_shape)\n \n+    @parameterized.parameters([(None,), (1,), (5,)])\n+    def test_irfft(self, fft_length):\n+        real = KerasTensor((None, 4, 3), dtype=\"float32\")\n+        imag = KerasTensor((None, 4, 3), dtype=\"float32\")\n+        output = kmath.irfft((real, imag), fft_length=fft_length)\n+        ref = np.fft.irfft(np.ones((2, 4, 3)), n=fft_length)\n+        ref_shape = (None,) + ref.shape[1:]\n+        self.assertEqual(output.shape, ref_shape)\n+\n     def test_stft(self):\n         x = KerasTensor((None, 32), dtype=\"float32\")\n         sequence_length = 10\n@@ -178,6 +259,24 @@ class MathOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(real_output.shape, real_ref_shape)\n         self.assertEqual(imag_output.shape, imag_ref_shape)\n \n+    def test_istft(self):\n+        sequence_length = 10\n+        sequence_stride = 3\n+        fft_length = 15\n+        real = KerasTensor((None, 32), dtype=\"float32\")\n+        imag = KerasTensor((None, 32), dtype=\"float32\")\n+        output = kmath.istft(\n+            (real, imag), sequence_length, sequence_stride, fft_length\n+        )\n+        ref = _istft(\n+            (np.ones((5, 32)), np.ones((5, 32))),\n+            sequence_length,\n+            sequence_stride,\n+            fft_length,\n+        )\n+        ref_shape = (None,) + ref.shape[1:]\n+        self.assertEqual(output.shape, ref_shape)\n+\n     def test_rsqrt(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(kmath.rsqrt(x).shape, (None, 3))\n@@ -275,6 +374,13 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(real_output.shape, ref.shape)\n         self.assertEqual(imag_output.shape, ref.shape)\n \n+    def test_irfft(self):\n+        real = KerasTensor((2, 4, 3), dtype=\"float32\")\n+        imag = KerasTensor((2, 4, 3), dtype=\"float32\")\n+        output = kmath.irfft((real, imag))\n+        ref = np.fft.irfft(np.ones((2, 4, 3)))\n+        self.assertEqual(output.shape, ref.shape)\n+\n     def test_rsqrt(self):\n         x = KerasTensor([4, 3], dtype=\"float32\")\n         self.assertEqual(kmath.rsqrt(x).shape, (4, 3))\n@@ -293,6 +399,26 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(real_output.shape, real_ref.shape)\n         self.assertEqual(imag_output.shape, imag_ref.shape)\n \n+    def test_istft(self):\n+        # sequence_stride must <= x[0].shape[-1]\n+        # sequence_stride must >= fft_length / num_sequences\n+        sequence_length = 10\n+        sequence_stride = 3\n+        fft_length = 15\n+        num_sequences = fft_length // sequence_stride + 1\n+        real = KerasTensor((num_sequences, 32), dtype=\"float32\")\n+        imag = KerasTensor((num_sequences, 32), dtype=\"float32\")\n+        output = kmath.istft(\n+            (real, imag), sequence_length, sequence_stride, fft_length\n+        )\n+        ref = _istft(\n+            (np.ones((num_sequences, 32)), np.ones((num_sequences, 32))),\n+            sequence_length,\n+            sequence_stride,\n+            fft_length,\n+        )\n+        self.assertEqual(output.shape, ref.shape)\n+\n \n class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     @pytest.mark.skipif(\n@@ -577,6 +703,24 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(real_ref, real_output, atol=1e-5, rtol=1e-5)\n         self.assertAllClose(imag_ref, imag_output, atol=1e-5, rtol=1e-5)\n \n+    @parameterized.parameters([(None,), (3,), (15,)])\n+    def test_irfft(self, n):\n+        # Test 1D.\n+        real = np.random.random((10,))\n+        imag = np.random.random((10,))\n+        complex_arr = real + 1j * imag\n+        output = kmath.irfft((real, imag), fft_length=n)\n+        ref = np.fft.irfft(complex_arr, n=n)\n+        self.assertAllClose(output, ref, atol=1e-5, rtol=1e-5)\n+\n+        # Test N-D case.\n+        real = np.random.random((2, 3, 10))\n+        imag = np.random.random((2, 3, 10))\n+        complex_arr = real + 1j * imag\n+        output = kmath.irfft((real, imag), fft_length=n)\n+        ref = np.fft.irfft(complex_arr, n=n)\n+        self.assertAllClose(output, ref, atol=1e-5, rtol=1e-5)\n+\n     @parameterized.parameters(\n         [\n             (32, 8, 32, \"hann\", True),\n@@ -602,8 +746,8 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(real_ref, real_output, atol=1e-5, rtol=1e-5)\n         self.assertAllClose(imag_ref, imag_output, atol=1e-5, rtol=1e-5)\n \n-        # Test N-D case.\n-        x = np.random.random((2, 3, 32))\n+        # Test 2D case.\n+        x = np.random.random((3, 32))\n         real_output, imag_output = kmath.stft(\n             x, sequence_length, sequence_stride, fft_length, window, center\n         )\n@@ -613,6 +757,80 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(real_ref, real_output, atol=1e-5, rtol=1e-5)\n         self.assertAllClose(imag_ref, imag_output, atol=1e-5, rtol=1e-5)\n \n+    @parameterized.parameters(\n+        [\n+            (32, 8, 32, \"hann\", True),\n+            (8, 8, 16, \"hann\", True),\n+            (4, 4, 7, \"hann\", True),\n+            (32, 8, 32, \"hamming\", True),\n+            (8, 4, 8, \"hann\", False),\n+            (32, 8, 32, np.ones((32,)), True),\n+            (32, 8, 32, None, True),\n+        ]\n+    )\n+    def test_istft(\n+        self, sequence_length, sequence_stride, fft_length, window, center\n+    ):\n+        # sequence_stride must <= x[0].shape[-1]\n+        # sequence_stride must >= fft_length / num_sequences\n+        # Test 1D case.\n+        x = np.random.random((256,))\n+        real_x, imag_x = _stft(\n+            x, sequence_length, sequence_stride, fft_length, window, center\n+        )\n+        output = kmath.istft(\n+            (real_x, imag_x),\n+            sequence_length,\n+            sequence_stride,\n+            fft_length,\n+            window=window,\n+            center=center,\n+        )\n+        ref = _istft(\n+            (real_x, imag_x),\n+            sequence_length,\n+            sequence_stride,\n+            fft_length,\n+            window=window,\n+            center=center,\n+        )\n+        if backend.backend() in (\"numpy\", \"jax\"):\n+            # numpy and jax have different implementation for the boundary of\n+            # the output, so we need to truncate 5% befroe assertAllClose\n+            truncated_len = int(output.shape[-1] * 0.05)\n+            output = output[..., truncated_len:-truncated_len]\n+            ref = ref[..., truncated_len:-truncated_len]\n+        self.assertAllClose(output, ref, atol=1e-5, rtol=1e-5)\n+\n+        # Test 2D case.\n+        x = np.random.random((3, 256))\n+        real_x, imag_x = _stft(\n+            x, sequence_length, sequence_stride, fft_length, window, center\n+        )\n+        output = kmath.istft(\n+            (real_x, imag_x),\n+            sequence_length,\n+            sequence_stride,\n+            fft_length,\n+            window=window,\n+            center=center,\n+        )\n+        ref = _istft(\n+            (real_x, imag_x),\n+            sequence_length,\n+            sequence_stride,\n+            fft_length,\n+            window=window,\n+            center=center,\n+        )\n+        if backend.backend() in (\"numpy\", \"jax\"):\n+            # numpy and jax have different implementation for the boundary of\n+            # the output, so we need to truncate 5% befroe assertAllClose\n+            truncated_len = int(output.shape[-1] * 0.05)\n+            output = output[..., truncated_len:-truncated_len]\n+            ref = ref[..., truncated_len:-truncated_len]\n+        self.assertAllClose(output, ref, atol=1e-5, rtol=1e-5)\n+\n     @pytest.mark.skipif(\n         backend.backend() == \"numpy\",\n         reason=\"Numpy does not support rsqrt.\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
