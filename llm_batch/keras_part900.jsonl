{"custom_id": "keras#c7b66fc23fbeb7197d7c4a6491d7fda6d3502c6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 10 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ class AveragePooling2D(BasePooling):\n     (when `input_shape >= pool_size`)\n \n     The resulting output shape when using the `\"same\"` padding option is:\n-    `output_shape = math.floor((input_shape - 1) / strides) + 1`\n+    `output_shape = input_shape`\n \n     Args:\n         pool_size: int or tuple of 2 integers, factors by which to downscale\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#78c729e9042ffd0c7485c48600f14ed505a55d0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 162 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -237,12 +237,12 @@ class ModelCheckpoint(MonitorCallback):\n                 )\n                 return True\n             else:\n+                best_str = \"None\" if self.best is None else f\"{self.best:.5f}\"\n                 if self._is_improvement(current, self.best):\n                     if self.verbose > 0:\n                         io_utils.print_msg(\n                             f\"\\nEpoch {epoch + 1}: {self.monitor} \"\n-                            \"improved \"\n-                            f\"from {self.best:.5f} to {current:.5f}, \"\n+                            f\"improved from {best_str} to {current:.5f}, \"\n                             f\"saving model to {filepath}\"\n                         )\n                     self.best = current\n@@ -251,8 +251,7 @@ class ModelCheckpoint(MonitorCallback):\n                     if self.verbose > 0:\n                         io_utils.print_msg(\n                             f\"\\nEpoch {epoch + 1}: \"\n-                            f\"{self.monitor} did not improve \"\n-                            f\"from {self.best:.5f}\"\n+                            f\"{self.monitor} did not improve from {best_str}\"\n                         )\n                     return False\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#43e9a2e0c939f620c1b627a8b95df9209053782a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 91 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ try:\n     # import to happen first for all tests.\n     import torch  # noqa: F401\n except ImportError:\n-    pass\n+    torch = None\n \n import pytest  # noqa: E402\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86b4b404729f51201e9fa20ddb07d1c6a3acfdf1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 728 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -249,6 +249,14 @@ class OpenVINOKerasTensor:\n         )\n         return OpenVINOKerasTensor(ov_opset.power(first, other).output(0))\n \n+    def __rpow__(self, other):\n+        other = get_ov_output(other)\n+        first = self.output\n+        first, other = align_operand_types(\n+            first, other, \"OpenVINOKerasTensor::__rpow__\"\n+        )\n+        return OpenVINOKerasTensor(ov_opset.power(other, first).output(0))\n+\n     def __getitem__(self, indices):\n         # now it has limited functionaly\n         # and supports only a case with one integer index in indices\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#379be262ba29491bcc74484ab67dc9c9548d5204", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 27 | Churn Cumulative: 1904 | Contributors (this commit): 23 | Commits (past 90d): 31 | Contributors (cumulative): 23 | DMM Complexity: 0.20833333333333334\n\nDIFF:\n@@ -1510,7 +1510,32 @@ def vectorize(pyfunc, *, excluded=None, signature=None):\n \n \n def where(condition, x1=None, x2=None):\n-    raise NotImplementedError(\"`where` is not supported with openvino backend\")\n+    condition = get_ov_output(condition)\n+    if x1 is None and x2 is None:\n+        nonzero_indices = ov_opset.non_zero(condition)\n+        return OpenVINOKerasTensor(nonzero_indices.output(0))\n+    if x1 is None:\n+        return OpenVINOKerasTensor(condition)\n+    if x2 is None:\n+        raise ValueError(\"x2 must be provided if x1 is specified.\")\n+\n+    def cast_literal_like_tensor(literal, x):\n+        ov_type = get_ov_output(x).get_element_type()\n+        is_bool = ov_type == Type.boolean\n+        is_float_to_int = isinstance(literal, float) and ov_type.is_integral()\n+        if is_bool or is_float_to_int:\n+            return get_ov_output(literal), get_ov_output(x)\n+        return get_ov_output(literal, ov_type), get_ov_output(x)\n+\n+    if isinstance(x1, (int, float)):\n+        x1, x2 = cast_literal_like_tensor(x1, x2)\n+    elif isinstance(x2, (int, float)):\n+        x2, x1 = cast_literal_like_tensor(x2, x1)\n+    else:\n+        x1 = get_ov_output(x1)\n+        x2 = get_ov_output(x2)\n+    x1, x2 = _align_operand_types(x1, x2, \"select()\")\n+    return OpenVINOKerasTensor(ov_opset.select(condition, x1, x2).output(0))\n \n \n def divide(x1, x2):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a0949a8689f3ef2441e22d6141770fb686eed5c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 50 | Churn Cumulative: 778 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -257,6 +257,56 @@ class OpenVINOKerasTensor:\n         )\n         return OpenVINOKerasTensor(ov_opset.power(other, first).output(0))\n \n+    def __lt__(self, other):\n+        first = self.output\n+        other = get_ov_output(other)\n+        first, other = align_operand_types(\n+            first, other, \"OpenVINOKerasTensor::__lt__\"\n+        )\n+        return OpenVINOKerasTensor(ov_opset.less(first, other).output(0))\n+\n+    def __gt__(self, other):\n+        first = self.output\n+        other = get_ov_output(other)\n+        first, other = align_operand_types(\n+            first, other, \"OpenVINOKerasTensor::__gt__\"\n+        )\n+        return OpenVINOKerasTensor(ov_opset.greater(first, other).output(0))\n+\n+    def __le__(self, other):\n+        first = self.output\n+        other = get_ov_output(other)\n+        first, other = align_operand_types(\n+            first, other, \"OpenVINOKerasTensor::__le__\"\n+        )\n+        return OpenVINOKerasTensor(ov_opset.less_equal(first, other).output(0))\n+\n+    def __ge__(self, other):\n+        first = self.output\n+        other = get_ov_output(other)\n+        first, other = align_operand_types(\n+            first, other, \"OpenVINOKerasTensor::__ge__\"\n+        )\n+        return OpenVINOKerasTensor(\n+            ov_opset.greater_equal(first, other).output(0)\n+        )\n+\n+    def __eq__(self, other):\n+        first = self.output\n+        other = get_ov_output(other)\n+        first, other = align_operand_types(\n+            first, other, \"OpenVINOKerasTensor::__eq__\"\n+        )\n+        return OpenVINOKerasTensor(ov_opset.equal(first, other).output(0))\n+\n+    def __ne__(self, other):\n+        first = self.output\n+        other = get_ov_output(other)\n+        first, other = align_operand_types(\n+            first, other, \"OpenVINOKerasTensor::__ne__\"\n+        )\n+        return OpenVINOKerasTensor(ov_opset.not_equal(first, other).output(0))\n+\n     def __getitem__(self, indices):\n         # now it has limited functionaly\n         # and supports only a case with one integer index in indices\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#771b00130fdb0198eab85a0017e67f2924ca4768", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 38 | Churn Cumulative: 1608 | Contributors (this commit): 15 | Commits (past 90d): 13 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4579,13 +4579,19 @@ def not_equal(x1, x2):\n \n \n class OnesLike(Operation):\n-    def call(self, x, dtype=None):\n-        return backend.numpy.ones_like(x, dtype=dtype)\n+    def __init__(self, dtype=None):\n+        super().__init__()\n+        self.dtype = (\n+            backend.standardize_dtype(dtype) if dtype is not None else None\n+        )\n \n-    def compute_output_spec(self, x, dtype=None):\n-        if dtype is None:\n-            dtype = x.dtype\n-        return KerasTensor(x.shape, dtype=dtype)\n+    def call(self, x):\n+        return backend.numpy.ones_like(x, dtype=self.dtype)\n+\n+    def compute_output_spec(self, x):\n+        dtype = x.dtype if self.dtype is None else self.dtype\n+        sparse = getattr(x, \"sparse\", False)\n+        return KerasTensor(x.shape, dtype=dtype, sparse=sparse)\n \n \n @keras_export([\"keras.ops.ones_like\", \"keras.ops.numpy.ones_like\"])\n@@ -4600,18 +4606,24 @@ def ones_like(x, dtype=None):\n         A tensor of ones with the same shape and type as `x`.\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return OnesLike().symbolic_call(x, dtype=dtype)\n+        return OnesLike(dtype=dtype).symbolic_call(x)\n     return backend.numpy.ones_like(x, dtype=dtype)\n \n \n class ZerosLike(Operation):\n-    def call(self, x, dtype=None):\n-        return backend.numpy.zeros_like(x, dtype=dtype)\n+    def __init__(self, dtype=None):\n+        super().__init__()\n+        self.dtype = (\n+            backend.standardize_dtype(dtype) if dtype is not None else None\n+        )\n+\n+    def call(self, x):\n+        return backend.numpy.zeros_like(x, dtype=self.dtype)\n \n     def compute_output_spec(self, x, dtype=None):\n-        if dtype is None:\n-            dtype = x.dtype\n-        return KerasTensor(x.shape, dtype=dtype)\n+        dtype = x.dtype if self.dtype is None else self.dtype\n+        sparse = getattr(x, \"sparse\", False)\n+        return KerasTensor(x.shape, dtype=dtype, sparse=sparse)\n \n \n @keras_export(\n@@ -4631,7 +4643,7 @@ def zeros_like(x, dtype=None):\n         A tensor of zeros with the same shape and type as `x`.\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return ZerosLike().symbolic_call(x, dtype=dtype)\n+        return ZerosLike(dtype=dtype).symbolic_call(x)\n     return backend.numpy.zeros_like(x, dtype=dtype)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1d0358fa8402345aaf9ba75ac58bbfdc63f773b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 209 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,11 @@ import inspect\n \n from absl.testing import parameterized\n \n+try:\n     from keras.api import ops as api_ops_root\n+except ImportError:\n+    from keras import ops as api_ops_root\n+\n from keras.src import backend\n from keras.src import ops\n from keras.src import testing\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f379556456c8209b928051b01bc5a307c36bfe8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 36 | Churn Cumulative: 814 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -625,31 +625,55 @@ def scatter_update(inputs, indices, updates):\n \n def slice(inputs, start_indices, shape):\n     inputs = get_ov_output(inputs)\n+    if isinstance(start_indices, (list, np.ndarray)):\n+        start_indices = tuple(start_indices)\n+    if isinstance(shape, (list, np.ndarray)):\n+        shape = tuple(shape)\n     assert isinstance(start_indices, tuple), (\n         \"`slice` is not supported by openvino backend\"\n-        \" for `start_indices` of type {}\".format(type(shape))\n+        \" for `start_indices` of type {}\".format(type(start_indices))\n     )\n     assert isinstance(shape, tuple), (\n         \"`slice` is not supported by openvino backend\"\n-        \" for `lengths` of type {}\".format(type(shape))\n+        \" for `shape` of type {}\".format(type(shape))\n     )\n \n     axes = []\n     start = []\n     stop = []\n+\n+    def prepare_slice_index(val):\n+        val_type = val.get_element_type()\n+        if not val_type.is_integral():\n+            raise ValueError(\n+                \"`slice` is not supported by OpenVINO backend \"\n+                \"for `start_indices` or `shape` with non-integer types\"\n+            )\n+        if val_type != Type.i32:\n+            val = ov_opset.convert(val, Type.i32).output(0)\n+        if len(val.get_partial_shape()) == 0:\n+            val = ov_opset.unsqueeze(\n+                val, ov_opset.constant(0, Type.i32)\n+            ).output(0)\n+        return val\n+\n     for idx, length in enumerate(shape):\n         if length is not None and length >= 0:\n             axes.append(idx)\n-            start.append(start_indices[idx])\n-            stop.append(start_indices[idx] + length)\n+            start_val = prepare_slice_index(get_ov_output(start_indices[idx]))\n+            stop_val = prepare_slice_index(\n+                get_ov_output(start_indices[idx] + length)\n+            )\n+            start.append(start_val)\n+            stop.append(stop_val)\n \n     if len(axes) == 0:\n         return inputs\n \n     step = [1] * len(start)\n     step = ov_opset.constant(step, Type.i32).output(0)\n-    start = ov_opset.constant(start, Type.i32).output(0)\n-    stop = ov_opset.constant(stop, Type.i32).output(0)\n+    start = ov_opset.concat(start, axis=0).output(0)\n+    stop = ov_opset.concat(stop, axis=0).output(0)\n     axes = ov_opset.constant(axes, Type.i32).output(0)\n     return OpenVINOKerasTensor(\n         ov_opset.slice(inputs, start, stop, step, axes).output(0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24f104ecb31ccf367c4c0764515e4ba840d680e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 62 | Files Changed: 3 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 65 | Churn Cumulative: 930 | Contributors (this commit): 13 | Commits (past 90d): 11 | Contributors (cumulative): 22 | DMM Complexity: 0.1282051282051282\n\nDIFF:\n@@ -271,7 +271,8 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n     ):\n         BackendLayer.__init__(self)\n         self._lock = False\n-        Operation.__init__(self, dtype=dtype, name=name)\n+        Operation.__init__(self, name=name)\n+        self._dtype_policy = dtype_policies.get(dtype)\n         self.activity_regularizer = regularizers.get(activity_regularizer)\n         input_dim_arg = kwargs.pop(\"input_dim\", None)\n         if input_dim_arg is not None:\n\n@@ -14,7 +14,7 @@ from keras.src.utils.naming import auto_name\n \n @keras_export(\"keras.Operation\")\n class Operation:\n-    def __init__(self, dtype=None, name=None):\n+    def __init__(self, name=None):\n         if name is None:\n             name = auto_name(self.__class__.__name__)\n         if not isinstance(name, str) or \"/\" in name:\n@@ -23,7 +23,6 @@ class Operation:\n                 \"cannot contain character `/`. \"\n                 f\"Received: name={name} (of type {type(name)})\"\n             )\n-        self._dtype_policy = dtype_policies.get(dtype)\n         self.name = name\n         self._inbound_nodes = []\n         self._outbound_nodes = []\n@@ -124,17 +123,6 @@ class Operation:\n         arg_names = inspect.getfullargspec(cls.__init__).args\n         kwargs.update(dict(zip(arg_names[1 : len(args) + 1], args)))\n \n-        # Explicitly serialize `dtype` to support auto_config\n-        dtype = kwargs.get(\"dtype\", None)\n-        if dtype is not None and isinstance(dtype, dtype_policies.DTypePolicy):\n-            # For backward compatibility, we use a str (`name`) for\n-            # `DTypePolicy`\n-            if dtype.quantization_mode is None:\n-                kwargs[\"dtype\"] = dtype.name\n-            # Otherwise, use `dtype_policies.serialize`\n-            else:\n-                kwargs[\"dtype\"] = dtype_policies.serialize(dtype)\n-\n         # For safety, we only rely on auto-configs for a small set of\n         # serializable types.\n         supported_types = (str, int, float, bool, type(None))\n\n@@ -2,7 +2,6 @@ import numpy as np\n \n from conftest import skip_if_backend\n from keras.src import backend\n-from keras.src import dtype_policies\n from keras.src import testing\n from keras.src.backend.common import keras_tensor\n from keras.src.ops import numpy as knp\n@@ -45,22 +44,6 @@ class OpWithCustomConstructor(operation.Operation):\n         return keras_tensor.KerasTensor(x.shape, x.dtype)\n \n \n-class OpWithCustomDtype(operation.Operation):\n-    def __init__(self, dtype):\n-        if not isinstance(dtype, (str, dtype_policies.DTypePolicy)):\n-            raise AssertionError(\n-                \"`dtype` must be a instance of `DTypePolicy` or str. \"\n-                f\"Received: dtype={dtype} of type {type(dtype)}\"\n-            )\n-        super().__init__(dtype=dtype)\n-\n-    def call(self, x):\n-        return x\n-\n-    def compute_output_spec(self, x):\n-        return keras_tensor.KerasTensor(x.shape, x.dtype)\n-\n-\n class OperationTest(testing.TestCase):\n     def test_symbolic_call(self):\n         x = keras_tensor.KerasTensor(shape=(2, 3), name=\"x\")\n@@ -181,34 +164,3 @@ class OperationTest(testing.TestCase):\n             ValueError, \"must be a string and cannot contain character `/`.\"\n         ):\n             OpWithMultipleOutputs(name=\"test/op\")\n-\n-    def test_dtype(self):\n-        # Test dtype argument\n-        op = OpWithCustomDtype(dtype=\"bfloat16\")\n-        self.assertEqual(op._dtype_policy.name, \"bfloat16\")\n-\n-        policy = dtype_policies.DTypePolicy(\"mixed_bfloat16\")\n-        op = OpWithCustomDtype(dtype=policy)\n-        self.assertEqual(op._dtype_policy.name, \"mixed_bfloat16\")\n-\n-        # Test dtype config to ensure it remains unchanged\n-        config = op.get_config()\n-        copied_config = config.copy()\n-        OpWithCustomDtype.from_config(config)\n-        self.assertEqual(config, copied_config)\n-\n-        # Test floating dtype serialization\n-        op = OpWithCustomDtype(dtype=\"mixed_bfloat16\")\n-        config = op.get_config()\n-        self.assertEqual(config[\"dtype\"], \"mixed_bfloat16\")  # A plain string\n-        revived_op = OpWithCustomDtype.from_config(config)\n-        self.assertEqual(op._dtype_policy.name, revived_op._dtype_policy.name)\n-\n-        # Test quantized dtype serialization\n-        policy = dtype_policies.QuantizedDTypePolicy(\"int8\", \"bfloat16\")\n-        op = OpWithCustomDtype(policy)\n-        self.assertEqual(op._dtype_policy.name, \"int8_from_bfloat16\")\n-        config = op.get_config()  # A serialized config\n-        self.assertEqual(config[\"dtype\"], dtype_policies.serialize(policy))\n-        revived_op = OpWithCustomDtype.from_config(config)\n-        self.assertEqual(op._dtype_policy.name, revived_op._dtype_policy.name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9286df3460cb980992c392e9f0fa65f72ba010ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 249 | Lines Deleted: 25 | Files Changed: 2 | Hunks: 34 | Methods Changed: 21 | Complexity Δ (Sum/Max): 53/46 | Churn Δ: 274 | Churn Cumulative: 2124 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 0.29651162790697677\n\nDIFF:\n@@ -1040,15 +1040,20 @@ class H5IOStore:\n         # will mistakenly using `__len__` to determine the value.\n         return self.h5_file.__bool__()\n \n-    def _get_h5_file(self, path_or_io):\n+    def _get_h5_file(self, path_or_io, mode=None):\n+        mode = mode or self.mode\n+        if mode not in (\"r\", \"w\", \"a\"):\n+            raise ValueError(\n+                f\"`mode` should be either 'r', 'w' or 'a'. Received: {mode}\"\n+            )\n         if self.archive:\n-            if self.mode == \"w\":\n+            if mode == \"w\":\n                 self.io_file = io.BytesIO()\n             else:\n                 self.io_file = self.archive.open(str(path_or_io), \"r\")\n-            return h5py.File(self.io_file, mode=self.mode)\n+            return h5py.File(self.io_file, mode=mode)\n         else:\n-            return h5py.File(path_or_io, mode=self.mode)\n+            return h5py.File(path_or_io, mode=mode)\n \n     def make(self, path, metadata=None):\n         \"\"\"Make a new H5 entry group.\n@@ -1148,10 +1153,16 @@ class H5IOStore:\n             and value.attrs[\"dtype\"] == \"bfloat16\"\n         ):\n             value = np.array(value, dtype=ml_dtypes.bfloat16)\n+        elif (\n+            hasattr(value, \"shape\")\n+            and hasattr(value, \"dtype\")\n+            and not isinstance(value, np.ndarray)\n+        ):\n+            value = np.array(value)\n         return value\n \n     def __setitem__(self, key, value):\n-        if self.mode != \"w\":\n+        if self.mode not in (\"w\", \"a\"):\n             raise ValueError(\"Setting a value is only allowed in write mode.\")\n         if not self._h5_entry_initialized:\n             self._create_h5_group(self._h5_entry_path)\n@@ -1164,7 +1175,7 @@ class H5IOStore:\n             self._h5_entry_group[key] = value\n \n     def __delitem__(self, key):\n-        if self.mode != \"w\":\n+        if self.mode not in (\"w\", \"a\"):\n             raise ValueError(\"Deleting a value is only allowed in write mode.\")\n         del self._h5_entry_group[key]\n \n@@ -1202,7 +1213,7 @@ class ShardedH5IOStore(H5IOStore):\n         self.archive = archive\n         self.io_file = None\n \n-        self.max_shard_size = float(max_shard_size)\n+        self.max_shard_size = float(max_shard_size) * 1024**3  # To bytes.\n         self.base_name = self.path.stem.replace(\".weights\", \"\")\n \n         if self.path.suffix != \".json\":\n@@ -1226,6 +1237,7 @@ class ShardedH5IOStore(H5IOStore):\n         self.current_shard_size = 0\n         self.total_shard_size = 0  # In bytes.\n         self.current_shard_path = None\n+        self.current_shard_filenames = []\n         if self.mode == \"w\":\n             self.sharding_config = {\n                 \"metadata\": {\n@@ -1243,6 +1255,27 @@ class ShardedH5IOStore(H5IOStore):\n                     self.sharding_config = json.load(map_file)\n         self.h5_file = self._create_new_shard_file()\n \n+    def make(self, path, metadata=None):\n+        \"\"\"Make a new H5 entry group.\n+\n+        This method is only available in write mode. It defers the creation of\n+        the H5 entry group until `__setitem__` is called, preventing the\n+        creation of empty groups.\n+\n+        The information about the current shard is reset.\n+\n+        Args:\n+            path: `str`. The variable path.\n+            metadata: Optional `dict`. The metadata to save with the H5 entry\n+                group. Defaults to `None`.\n+        \"\"\"\n+        self.current_shard_filenames = []\n+        if self.h5_file is not None:\n+            self.current_shard_filenames.append(\n+                pathlib.Path(self.h5_file.filename).name\n+            )\n+        return super().make(path, metadata)\n+\n     def get(self, path):\n         \"\"\"Get the H5 entry group.\n \n@@ -1259,9 +1292,17 @@ class ShardedH5IOStore(H5IOStore):\n \n         # If not found, check shard map and switch files.\n         weight_map = self.sharding_config[\"weight_map\"]\n-        filename = weight_map.get(parsed_path) or weight_map.get(\n+        filenames = weight_map.get(parsed_path) or weight_map.get(\n             \"/\" + parsed_path + \"/vars\"\n         )\n+        if filenames is not None:\n+            if not isinstance(filenames, list):\n+                filenames = [filenames]\n+            self.current_shard_filenames = filenames\n+            filename = filenames[0]\n+        else:\n+            self.current_shard_filenames = []\n+            filename = None\n \n         if filename is not None and filename != self.current_shard_path.name:\n             self.close()\n@@ -1269,7 +1310,9 @@ class ShardedH5IOStore(H5IOStore):\n         return super().get(path)\n \n     def close(self):\n+        if self.h5_file is not None:\n             self.h5_file.close()\n+            self.h5_file = None\n         if self.mode == \"w\":\n             self.sharding_config[\"metadata\"][\"total_size\"] = (\n                 self.total_shard_size\n@@ -1289,28 +1332,128 @@ class ShardedH5IOStore(H5IOStore):\n     # Shard-specific methods.\n \n     def _create_new_shard_file(self):\n+        \"\"\"Create a new shard file and return the H5 file object.\"\"\"\n         new_shard_path = (\n             f\"{self.base_name}_{self.current_shard_index:05}.weights.h5\"\n         )\n         self.current_shard_index += 1\n         self.current_shard_path = self.path.with_name(new_shard_path)\n-        return self._get_h5_file(self.current_shard_path)\n+        h5_file = self._get_h5_file(self.current_shard_path)\n+        self.current_shard_filenames.append(pathlib.Path(h5_file.filename).name)\n+        self._h5_entry_initialized = False\n+        return h5_file\n+\n+    def _switch_h5_file(self, filename, mode):\n+        \"\"\"Switch to a different H5 file with the specified mode.\n+\n+        This is useful for retrieving information from all shards, such as the\n+        total length, keys, and items.\n+        \"\"\"\n+        if mode not in (\"r\", \"a\"):\n+            raise ValueError(\n+                f\"`mode` should be either 'r' or 'a'. Received: {mode}\"\n+            )\n+        self.close()\n+        self.h5_file = self._get_h5_file(\n+            self.path.with_name(filename), mode=mode\n+        )\n+        self._get_h5_group(self._h5_entry_path)\n+\n+    def _restore_h5_file(self):\n+        \"\"\"Ensure the current shard is the last one created.\n+\n+        We use mode=\"a\" to avoid truncating the file during the switching.\n+        \"\"\"\n+        if (\n+            pathlib.Path(self.h5_file.filename).name\n+            != self.current_shard_path.name\n+        ):\n+            self._switch_h5_file(self.current_shard_path.name, mode=\"a\")\n \n     # H5 entry level methods.\n \n+    def _get_h5_group(self, path):\n+        \"\"\"Get the H5 entry group. If it doesn't exist, return an empty dict.\"\"\"\n+        try:\n+            if not path:\n+                self._h5_entry_group = self.h5_file[\"vars\"]\n+            else:\n+                self._h5_entry_group = self.h5_file[path][\"vars\"]\n+            self._h5_entry_initialized = True\n+        except KeyError:\n+            self._h5_entry_group = {}\n+            self._h5_entry_initialized = False\n+\n+    # Dict methods.\n+\n+    def __len__(self):\n+        total_len = self._h5_entry_group.__len__()\n+        for filename in self.current_shard_filenames:\n+            if filename == self.current_shard_path.name:\n+                continue\n+            self._switch_h5_file(filename, mode=\"r\")\n+            total_len += self._h5_entry_group.__len__()\n+        self._restore_h5_file()\n+        return total_len\n+\n+    def keys(self):\n+        keys = set(self._h5_entry_group.keys())\n+        for filename in self.current_shard_filenames:\n+            if filename == self.current_shard_path.name:\n+                continue\n+            self._switch_h5_file(filename, mode=\"r\")\n+            keys.update(self._h5_entry_group.keys())\n+        self._restore_h5_file()\n+        return keys\n+\n+    def items(self):\n+        yield from self._h5_entry_group.items()\n+        for filename in self.current_shard_filenames:\n+            if filename == self.current_shard_path.name:\n+                continue\n+            self._switch_h5_file(filename, mode=\"r\")\n+            yield from self._h5_entry_group.items()\n+        self._restore_h5_file()\n+\n+    def values(self):\n+        yield from self._h5_entry_group.values()\n+        for filename in self.current_shard_filenames:\n+            if filename == self.current_shard_path.name:\n+                continue\n+            self._switch_h5_file(filename, mode=\"r\")\n+            yield from self._h5_entry_group.values()\n+        self._restore_h5_file()\n+\n+    def __getitem__(self, key):\n+        if key in self._h5_entry_group:\n+            return super().__getitem__(key)\n+\n+        for filename in self.current_shard_filenames:\n+            if filename == self.current_shard_path.name:\n+                continue\n+            self._switch_h5_file(filename, mode=\"r\")\n+            if key in self._h5_entry_group:\n+                item = super().__getitem__(key)\n+                self._restore_h5_file()\n+                return item\n+        raise KeyError(\n+            f\"Key '{key}' not found in any of the shards: \"\n+            f\"{self.current_shard_filenames}\"\n+        )\n+\n     def __setitem__(self, key, value):\n+        self._restore_h5_file()\n+\n         # Accumulate `current_shard_size`.\n         value = backend.convert_to_numpy(value)\n         dtype = backend.standardize_dtype(value.dtype)\n         weight_counts = math.prod(value.shape)\n         per_param_size = dtype_utils.dtype_size(dtype)\n-        value_size = weight_counts * per_param_size / (8.0 * 1024**3)  # To GB.\n-        self.total_shard_size += weight_counts * per_param_size / 8  # In bytes.\n+        value_size = weight_counts * per_param_size / 8  # In bytes.\n+        self.total_shard_size += value_size\n         if value_size > self.max_shard_size:\n-            value_size_str = readable_memory_size(value_size * 1024**3)\n-            max_shard_size_str = readable_memory_size(\n-                self.max_shard_size * 1024**3\n-            )\n+            value_size_str = readable_memory_size(value_size)\n+            max_shard_size_str = readable_memory_size(self.max_shard_size)\n             raise ValueError(\n                 f\"The size of {key} is {value_size_str} which \"\n                 f\"exceeds the maximum shard size {max_shard_size_str}. You \"\n@@ -1323,17 +1466,54 @@ class ShardedH5IOStore(H5IOStore):\n         if self.current_shard_size > self.max_shard_size:\n             self.close()\n             self.h5_file = self._create_new_shard_file()\n-            self.make(self._h5_entry_path)\n             self.current_shard_size = value_size\n \n         super().__setitem__(key, value)\n \n+        # Update the weight map.\n         variable_path = self._h5_entry_group.name\n-        if variable_path not in self.sharding_config[\"weight_map\"]:\n-            self.sharding_config[\"weight_map\"][variable_path] = (\n-                self.current_shard_path.name\n+        shard_filename = self.current_shard_path.name\n+        weight_map = self.sharding_config[\"weight_map\"]\n+        if variable_path not in weight_map:\n+            weight_map[variable_path] = shard_filename\n+        else:\n+            if not isinstance(weight_map[variable_path], list):\n+                weight_map[variable_path] = [weight_map[variable_path]]\n+            if shard_filename not in weight_map[variable_path]:\n+                weight_map[variable_path].append(shard_filename)\n+\n+    def __delitem__(self, key):\n+        if key in self._h5_entry_group:\n+            super().__delitem__(key)\n+            return\n+\n+        for filename in self.current_shard_filenames:\n+            if filename == self.current_shard_path.name:\n+                continue\n+            self._switch_h5_file(filename, mode=\"a\")\n+            if key in self._h5_entry_group:\n+                super().__delitem__(key)\n+                self._restore_h5_file()\n+                return\n+        raise KeyError(\n+            f\"Key '{key}' not found in any of the shards: \"\n+            f\"{self.current_shard_filenames}\"\n         )\n \n+    def __contains__(self, item):\n+        if item in self._h5_entry_group:\n+            return True\n+\n+        for filename in self.current_shard_filenames:\n+            if filename == self.current_shard_path.name:\n+                continue\n+            self._switch_h5_file(filename, mode=\"r\")\n+            if item in self._h5_entry_group:\n+                self._restore_h5_file()\n+                return True\n+        self._restore_h5_file()\n+        return False\n+\n \n class NpzIOStore:\n     def __init__(self, root_path, archive=None, mode=\"r\"):\n\n@@ -789,10 +789,10 @@ class SavingTest(testing.TestCase):\n             model, temp_filepath, max_shard_size=max_shard_size\n         )\n         self.assertIn(\"mymodel.weights.json\", os.listdir(temp_filepath.parent))\n-        if max_shard_size == 512:\n+        if max_shard_size == 1:\n             # 1 sharded file + 1 config file = 2.\n             self.assertLen(os.listdir(temp_filepath.parent), 2)\n-        elif max_shard_size == 10:\n+        elif max_shard_size == 0.01:\n             # 3 sharded file + 1 config file = 4.\n             self.assertLen(os.listdir(temp_filepath.parent), 4)\n \n@@ -1272,24 +1272,33 @@ class SavingH5IOStoreTest(testing.TestCase):\n         name = \"sharded_store\"\n         temp_filepath = Path(os.path.join(self.get_temp_dir(), f\"{name}.json\"))\n \n-        # Pre-defined data.\n-        a = np.random.random((2, 4)).astype(\"float32\")\n-        b = np.random.random((4, 8)).astype(\"int32\")\n+        # Pre-defined data. Each has about 0.0037GB.\n+        a = np.random.random((1000, 1000)).astype(\"float32\")\n+        b = np.random.random((1000, 1000)).astype(\"int32\")\n \n         # Set.\n-        store = saving_lib.ShardedH5IOStore(temp_filepath, mode=\"w\")\n+        store = saving_lib.ShardedH5IOStore(\n+            temp_filepath, max_shard_size=0.005, mode=\"w\"\n+        )\n         vars_store = store.make(\"vars\")\n         vars_store[\"a\"] = a\n         vars_store[\"b\"] = b\n         vars_store[\"c\"] = 42\n+        self.assertLen(store.sharding_config[\"weight_map\"][\"/vars/vars\"], 2)\n+        self.assertLen(vars_store, 3)\n         self.assertAllClose(vars_store[\"a\"], a)\n         self.assertAllClose(vars_store[\"b\"], b)\n         self.assertEqual(int(vars_store[\"c\"][()]), 42)\n \n         # Delete.\n         del vars_store[\"c\"]\n+        self.assertLen(vars_store, 2)\n+        del vars_store[\"a\"]  # Delete from an older shard.\n+        self.assertLen(vars_store, 1)\n+        vars_store[\"a\"] = a\n \n         # Contain.\n+        self.assertIn(\"a\", vars_store)\n         self.assertNotIn(\"c\", vars_store)\n \n         store.close()\n@@ -1301,10 +1310,33 @@ class SavingH5IOStoreTest(testing.TestCase):\n         # Get.\n         store = saving_lib.ShardedH5IOStore(temp_filepath, mode=\"r\")\n         vars_store = store.get(\"vars\")\n+        self.assertLen(vars_store, 2)\n         self.assertAllClose(vars_store[\"a\"], a)\n         self.assertAllClose(vars_store[\"b\"], b)\n         self.assertNotIn(\"c\", vars_store)\n \n+        # Keys.\n+        for key in [\"a\", \"b\"]:\n+            self.assertIn(key, vars_store.keys())\n+\n+        # Items.\n+        for key, value in vars_store.items():\n+            if key == \"a\":\n+                self.assertAllClose(value, a)\n+            elif key == \"b\":\n+                self.assertAllClose(value, b)\n+            else:\n+                raise ValueError(f\"Unexpected key: {key}\")\n+\n+        # Values.\n+        for value in vars_store.values():\n+            if backend.standardize_dtype(value.dtype) == \"float32\":\n+                self.assertAllClose(value, a)\n+            elif backend.standardize_dtype(value.dtype) == \"int32\":\n+                self.assertAllClose(value, b)\n+            else:\n+                raise ValueError(f\"Unexpected value: {value}\")\n+\n     def test_sharded_h5_io_store_exception_raised(self):\n         temp_filepath = Path(os.path.join(self.get_temp_dir(), \"store.h5\"))\n \n@@ -1334,4 +1366,16 @@ class SavingH5IOStoreTest(testing.TestCase):\n                 \"float32\"\n             )\n \n+        # Bad `get`.\n+        with self.assertRaisesRegex(\n+            KeyError, r\"Key 'abc' not found in any of the shards:\"\n+        ):\n+            vars_store[\"abc\"]\n+\n+        # Bad `del`.\n+        with self.assertRaisesRegex(\n+            KeyError, r\"Key 'abc' not found in any of the shards:\"\n+        ):\n+            del vars_store[\"abc\"]\n+\n         store.close()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0c0ec1a42dcc4c34247b1fb6d54fc96402ef6d51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 168 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 14 | Methods Changed: 4 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 194 | Churn Cumulative: 1547 | Contributors (this commit): 14 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: 0.03571428571428571\n\nDIFF:\n@@ -1062,6 +1062,8 @@ def _can_use_flash_attention(query, key, value, bias, raise_error=False):\n             q_seqlen=None,\n             kv_seqlen=None,\n             layout=_normalize_layout(\"BTNH\"),\n+            q_offsets=None,\n+            kv_offsets=None,\n         )\n         check_is_flash_attention(\n             query,\n@@ -1126,16 +1128,45 @@ def wrap_flash_attention(\n     decoder_segment_ids,\n     custom_mask=None,\n     attn_logits_soft_cap=None,\n+    head_shards=1,\n+    q_seq_shards=1,\n ):\n+    \"\"\"Applies a wrapped flash attention mechanism using the Splash kernel.\n+    This function prepares the appropriate attention mask (causal or custom),\n+    constructs a multi-head mask, and applies the Splash multi-head attention\n+    kernel to the provided query, key, and value tensors. It supports optional\n+    sharding and soft capping of attention logits.\n+    Args:\n+        query: jax.Array. The query tensor of shape\n+            (batch, num_heads, seq_len, head_dim).\n+        key: jax.Array. The key tensor of shape\n+            (batch, num_heads, seq_len, head_dim).\n+        value: jax.Array. The value tensor of shape\n+            (batch, num_heads, seq_len, head_dim).\n+        decoder_segment_ids: Optional. Segment IDs for the decoder, used for\n+            sharding or masking.\n+        custom_mask: Optional[jax.Array]. A custom attention mask to apply. If\n+            None, a causal mask is used.\n+        attn_logits_soft_cap: Optional[float]. If provided, applies a soft cap\n+            to the attention logits.\n+        head_shards: int, default=1. Number of shards for the attention heads.\n+        q_seq_shards: int, default=1. Number of shards for the query sequence\n+            dimension.\n+    Returns:\n+        jax.Array: The result of applying the Splash multi-head attention\n+            kernel to the inputs.\n+    Raises:\n+        AssertionError: If sharding along the sequence dimension is attempted\n+            with decoder_segment_ids.\n+    \"\"\"\n     if decoder_segment_ids is not None:\n         assert query.shape[2] == decoder_segment_ids.q.shape[1], (\n-            \"Sharding along sequence dimension not allowed in tpu kernel \"\n-            \"attention\"\n+            \"Sharding along sequence dimension not allowed\"\n+            \" in TPU kernel attention\"\n         )\n \n     if custom_mask is not None:\n         mask = splash_attention_mask.NumpyMask(array=custom_mask)\n-\n     else:\n         mask = splash_attention_mask.CausalMask(\n             shape=(query.shape[2], query.shape[2])\n@@ -1147,8 +1178,8 @@ def wrap_flash_attention(\n     )\n     splash_kernel = splash_attention_kernel.make_splash_mha(\n         mask=multi_head_mask,\n-        head_shards=1,\n-        q_seq_shards=1,\n+        head_shards=head_shards,\n+        q_seq_shards=q_seq_shards,\n         attn_logits_soft_cap=attn_logits_soft_cap,\n     )\n \n@@ -1168,6 +1199,38 @@ def dot_product_attention(\n     flash_attention=None,\n     attn_logits_soft_cap=None,\n ):\n+    \"\"\"Computes dot-product attention given query, key, and value.\n+\n+    This is the core computation of attention that is used in transformers.\n+    For TPU platforms, flash attention optimizations are automatically applied\n+    when possible, and sharding parameters are inferred from the layout map\n+    in the current distribution context.\n+\n+    Args:\n+        query: Queries with shape `[batch, time, heads,\n+            depth_k]`.\n+        key: Keys with shape `[batch, time, heads,\n+            depth_k]`.\n+        value: Values with shape `[batch, time, heads,\n+            depth_v]`.\n+        bias: Optional bias with shape broadcastable to\n+            `[batch, heads, dest_time, source_time]`.\n+        mask: Optional mask with shape broadcastable to\n+            `[batch, heads, dest_time, source_time]`.\n+        scale: Float. Optional scale that is applied to the attention\n+            computation.\n+        is_causal: Boolean. Specifying whether causal masking is applied.\n+        flash_attention: Boolean. Whether to use flash attention optimization\n+            for increased performance. Default to None, which means it will\n+            be auto-determined based on the platform, input shapes and\n+            compatibility.\n+        attn_logits_soft_cap: Float. Optional float to softly cap attention\n+            logits to avoid numerical stability issues. Applied as:\n+            `logits = logits / (1.0 + abs(logits) / attn_logits_soft_cap)`.\n+\n+    Returns:\n+        JAX Array of shape `[batch, time, heads, depth_v]`.\n+    \"\"\"\n     query = convert_to_tensor(query)\n     key = convert_to_tensor(key)\n     value = convert_to_tensor(value)\n@@ -1177,6 +1240,12 @@ def dot_product_attention(\n             f\"Received: query.shape={query.shape}, key.shape={key.shape}, \"\n             f\"value.shape={value.shape}.\"\n         )\n+\n+    # Check platform\n+    platform = jax.devices()[0].platform\n+    is_tpu = platform == \"tpu\"\n+\n+    # Determine flash attention compatibility\n     if flash_attention is None:\n         flash_attention = _can_use_flash_attention(query, key, value, bias)\n     elif flash_attention is True:\n@@ -1184,30 +1253,86 @@ def dot_product_attention(\n         # use flash attention\n         _can_use_flash_attention(query, key, value, bias, raise_error=True)\n \n-    if jax.devices()[0].platform == \"tpu\":\n-        # Transpose to ('batch', 'heads', 'length', 'kv')\n-        query = jnp.transpose(query, axes=(0, 2, 1, 3))\n-        key = jnp.transpose(key, axes=(0, 2, 1, 3))\n-        value = jnp.transpose(value, axes=(0, 2, 1, 3))\n-        B, H, S, KV = query.shape\n-\n-        segment_ids = jnp.ones([B, S])\n-        # {token_ids, padding_mask, segment_ids} enable packing\n-        out = wrap_flash_attention(\n-            query,\n-            key,\n-            value,\n-            decoder_segment_ids=splash_attention_kernel.SegmentIds(\n-                segment_ids, segment_ids\n-            ),\n-            custom_mask=mask,\n-            attn_logits_soft_cap=attn_logits_soft_cap,\n+    # TPU-specific flash attention path\n+    if is_tpu and flash_attention:\n+        # Get sharding parameters from distribution context\n+        try:\n+            from keras.src.distribution.distribution_lib import ModelParallel\n+            from keras.src.distribution.distribution_lib import (\n+                distribution as get_dist,\n             )\n-        out = jnp.transpose(out, axes=(0, 2, 1, 3))\n-        return out\n \n-    # `dot_product_attention` is only available in jax>=0.4.31\n+            # Get current distribution if available\n+            dist = get_dist()\n+            if dist and isinstance(dist, ModelParallel):\n+                mesh = dist.device_mesh\n+                if \"model\" in mesh.axis_names:\n+                    model_dim_index = mesh.axis_names.index(\"model\")\n+                    # Set head_shards based on the model dimension of the mesh\n+                    head_shards = mesh.shape[model_dim_index]\n+                    # Typically keep q_seq_shards=1 for best performance\n+                    q_seq_shards = 1\n+        except (ImportError, ValueError, AttributeError):\n+            # Use default values if detection fails\n+            head_shards = 1\n+            q_seq_shards = 1\n+        # Transpose to ('batch', 'heads', 'length', 'head_dim')\n+        query_tpu_layout = jnp.transpose(query, axes=(0, 2, 1, 3))\n+        key_tpu_layout = jnp.transpose(key, axes=(0, 2, 1, 3))\n+        value_tpu_layout = jnp.transpose(value, axes=(0, 2, 1, 3))\n+\n+        bs, num_heads, q_len, head_dim = query_tpu_layout.shape\n+\n+        # Apply scale to query if provided\n+        if scale is not None:\n+            # TPU kernel applies 1/sqrt(head_dim) internally, to achieve\n+            # overall QK^T * scale, scale query by (scale * sqrt(head_dim))\n+            query_tpu_layout = query_tpu_layout * (scale * math.sqrt(head_dim))\n+\n+        # Create segment IDs for Splash Attention (for packing/batching)\n+        segment_ids = jnp.zeros([bs, q_len], dtype=jnp.int32)\n+        decoder_segment_ids = splash_attention_kernel.SegmentIds(\n+            q=segment_ids, kv=segment_ids\n+        )\n+\n+        # Process mask for Splash Attention\n+        custom_mask = None\n+        if mask is not None:\n+            mask_bool = mask.astype(\"bool\") if mask.dtype != jnp.bool_ else mask\n+\n+            if mask_bool.ndim == 3 and mask_bool.shape[0] == bs:\n+                custom_mask = mask_bool[0]\n+            elif mask_bool.ndim == 4 and mask_bool.shape[0] == bs:\n+                custom_mask = mask_bool[0, 0]\n+\n+            if is_causal and custom_mask is not None:\n+                causal_mask = jnp.tril(\n+                    jnp.ones((q_len, q_len), dtype=jnp.bool_)\n+                )\n+                custom_mask = jnp.logical_and(custom_mask, causal_mask)\n+\n+        if custom_mask is None and is_causal:\n+            custom_mask = jnp.tril(jnp.ones((q_len, q_len), dtype=jnp.bool_))\n+\n+        try:\n+            output = wrap_flash_attention(\n+                query_tpu_layout,\n+                key_tpu_layout,\n+                value_tpu_layout,\n+                decoder_segment_ids=decoder_segment_ids,\n+                custom_mask=custom_mask,\n+                attn_logits_soft_cap=attn_logits_soft_cap,\n+                head_shards=head_shards,\n+                q_seq_shards=q_seq_shards,\n+            )\n+            # Transpose output back to Keras layout\n+            return jnp.transpose(output, axes=(0, 2, 1, 3))\n+        except Exception:\n+            flash_attention = False\n+\n+    # JAX native dot_product_attention for GPU or fallback for TPU\n     if hasattr(jax.nn, \"dot_product_attention\"):\n+        try:\n             return jax.nn.dot_product_attention(\n                 query,\n                 key,\n@@ -1218,6 +1343,20 @@ def dot_product_attention(\n                 is_causal=is_causal,\n                 implementation=\"cudnn\" if flash_attention else \"xla\",\n             )\n+        except Exception:\n+            # If flash attention fails, fall back to XLA implementation\n+            if flash_attention:\n+                return jax.nn.dot_product_attention(\n+                    query,\n+                    key,\n+                    value,\n+                    bias=bias,\n+                    mask=mask,\n+                    scale=scale,\n+                    is_causal=is_causal,\n+                    implementation=\"xla\",\n+                )\n+            raise\n \n     if flash_attention:\n         raise RuntimeError(\n@@ -1228,6 +1367,9 @@ def dot_product_attention(\n     # Ref: jax.nn.dot_product_attention\n     # https://github.com/jax-ml/jax/blob/jax-v0.4.33/jax/_src/nn/functions.py#L886\n     # Not support `query_seq_lengths` and `key_value_seq_lengths` args\n+\n+    # Fallback to custom XLA implementation\n+    # This is the reference implementation from jax.nn.dot_product_attention\n     output_shape = query.shape\n     _, _, K, H = key.shape\n     scale = (1.0 / jnp.sqrt(H)) if scale is None else scale\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
