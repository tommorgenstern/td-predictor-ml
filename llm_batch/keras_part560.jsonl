{"custom_id": "keras#ffa1f81b9ebce39682ea597b89ba6422f9d2b97b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 25 | Files Changed: 16 | Hunks: 21 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 6863 | Contributors (this commit): 8 | Commits (past 90d): 126 | Contributors (cumulative): 57 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,5 @@\n import jax\n import numpy as np\n-from jax import numpy as jnp\n \n from keras_core import backend\n from keras_core import initializers\n\n@@ -23,9 +23,9 @@ from keras_core.utils.io_utils import print_msg\n # Import backend functions.\n if backend() == \"tensorflow\":\n     print_msg(\"Using TensorFlow backend\")\n-    from keras_core.backend.tensorflow import *\n+    from keras_core.backend.tensorflow import *  # noqa: F403\n elif backend() == \"jax\":\n     print_msg(\"Using JAX backend.\")\n-    from keras_core.backend.jax import *\n+    from keras_core.backend.jax import *  # noqa: F403\n else:\n     raise ValueError(f\"Unable to import backend : {backend()}\")\n\n@@ -1,8 +1,6 @@\n import json\n import os\n \n-from keras_core.utils.io_utils import print_msg\n-\n # The type of float to use throughout a session.\n _FLOATX = \"float32\"\n \n\n@@ -1,5 +1,3 @@\n-import copy\n-\n from keras_core.api_export import keras_core_export\n from keras_core.callbacks.callback import Callback\n from keras_core.utils import io_utils\n\n@@ -1,5 +1,3 @@\n-import warnings\n-\n from keras_core.api_export import keras_core_export\n \n \n\n@@ -1,6 +1,5 @@\n import numpy as np\n \n-from keras_core import operations as ops\n from keras_core import testing\n from keras_core.losses import losses\n \n\n@@ -1,7 +1,6 @@\n from keras_core import backend\n from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n-from keras_core.backend import floatx\n from keras_core.losses.loss import squeeze_to_same_rank\n from keras_core.metrics import reduction_metrics\n \n\n@@ -1,6 +1,5 @@\n from keras_core import backend\n from keras_core import initializers\n-from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.metrics import metrics_utils\n from keras_core.metrics.metric import Metric\n\n@@ -1,11 +1,8 @@\n-from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n-from keras_core.losses import losses\n from keras_core.losses.losses import categorical_hinge\n from keras_core.losses.losses import hinge\n from keras_core.losses.losses import squared_hinge\n from keras_core.metrics import reduction_metrics\n-from keras_core.metrics.metric import Metric\n \n \n @keras_core_export(\"keras_core.metrics.Hinge\")\n\n@@ -1,6 +1,5 @@\n import numpy as np\n \n-from keras_core import operations as ops\n from keras_core import testing\n from keras_core.metrics import hinge_metrics\n \n\n@@ -1,9 +1,6 @@\n-from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n-from keras_core.losses import losses\n from keras_core.losses.losses import mean_squared_error\n from keras_core.metrics import reduction_metrics\n-from keras_core.metrics.metric import Metric\n \n \n @keras_core_export(\"keras_core.metrics.MeanSquaredError\")\n\n@@ -1,6 +1,5 @@\n import numpy as np\n \n-from keras_core import operations as ops\n from keras_core import testing\n from keras_core.metrics import regression_metrics\n \n\n@@ -9,5 +9,5 @@ from keras_core.backend import is_tensor\n from keras_core.backend import name_scope\n from keras_core.backend import random\n from keras_core.backend import shape\n-from keras_core.operations.nn import *\n-from keras_core.operations.numpy import *\n+from keras_core.operations.nn import *  # noqa: F403\n+from keras_core.operations.numpy import *  # noqa: F403\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n import tensorflow as tf\n-from tensorflow.python.ops.numpy_ops import np_config\n \n from keras_core import testing\n from keras_core.backend import backend\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import tensorflow as tf\n \n-from keras_core import backend\n from keras_core import testing\n from keras_core.trainers.data_adapters import tf_dataset_adapter\n \n\n@@ -1,6 +1,5 @@\n import math\n import re\n-import string\n \n from tensorflow import nest\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1aa2c513a2befc22c26d7f215ba43fa16483e1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 11/8 | Churn Δ: 68 | Churn Cumulative: 5174 | Contributors (this commit): 4 | Commits (past 90d): 27 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -91,7 +91,7 @@ class Embedding(Layer):\n     def call(self, inputs):\n         if inputs.dtype != \"int32\" and inputs.dtype != \"int64\":\n             inputs = ops.cast(inputs, \"int32\")\n-        one_hot_data = backend.nn.one_hot(inputs, num_classes=self.input_dim)\n+        one_hot_data = ops.one_hot(inputs, num_classes=self.input_dim)\n         return ops.matmul(one_hot_data, self.embeddings)\n \n     def compute_mask(self, inputs, mask=None):\n\n@@ -982,3 +982,32 @@ def conv_transpose(\n         data_format,\n         dilation_rate,\n     )\n+\n+\n+class OneHot(Operation):\n+    def __init__(self, num_classes, axis=-1):\n+        super().__init__()\n+        self.num_classes = num_classes\n+        self.axis = axis\n+\n+    def call(self, x):\n+        return backend.nn.one_hot(x, self.num_classes, axis=self.axis)\n+\n+    def compute_output_spec(self, x):\n+        x_shape = list(getattr(x, \"shape\", []))\n+        if self.axis == -1:\n+            x_shape.append(self.num_classes)\n+        elif self.axis >= 0 and self.axis < len(x_shape):\n+            x_shape.insert(self.axis, self.num_classes)\n+        else:\n+            raise ValueError(\n+                f\"axis must be -1 or between [0, {len(x.shape)}), but \"\n+                f\"received {self.axis}.\"\n+            )\n+        return KerasTensor(x_shape)\n+\n+\n+def one_hot(x, num_classes, axis=-1):\n+    if any_symbolic_tensors((x,)):\n+        return OneHot(num_classes, axis=axis).symbolic_call(x)\n+    return backend.nn.one_hot(x, num_classes, axis=axis)\n\n@@ -279,6 +279,12 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n             (None, 21, 21, 5),\n         )\n \n+    def test_one_hot(self):\n+        x = KerasTensor([None, 3, 1])\n+        self.assertEqual(knn.one_hot(x, 5).shape, (None, 3, 1, 5))\n+        self.assertEqual(knn.one_hot(x, 5, 1).shape, (None, 5, 3, 1))\n+        self.assertEqual(knn.one_hot(x, 5, 2).shape, (None, 3, 5, 1))\n+\n \n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n@@ -541,6 +547,12 @@ class NNOpsStaticShapeTest(testing.TestCase):\n             (2, 21, 21, 5),\n         )\n \n+    def test_one_hot(self):\n+        x = KerasTensor([2, 3, 1])\n+        self.assertEqual(knn.one_hot(x, 5).shape, (2, 3, 1, 5))\n+        self.assertEqual(knn.one_hot(x, 5, 1).shape, (2, 5, 3, 1))\n+        self.assertEqual(knn.one_hot(x, 5, 2).shape, (2, 3, 5, 1))\n+\n \n class NNOpsCorrectnessTest(testing.TestCase):\n     def test_relu(self):\n@@ -918,3 +930,28 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             inputs_2d, kernel, [2, 8, 8, 5], 2, padding=\"SAME\"\n         )\n         self.assertAllClose(outputs, expected)\n+\n+    def test_one_hot(self):\n+        # Test 1D one-hot.\n+        indices_1d = np.array([0, 1, 2, 3])\n+        self.assertAllClose(\n+            knn.one_hot(indices_1d, 4), tf.one_hot(indices_1d, 4)\n+        )\n+        self.assertAllClose(\n+            knn.one_hot(indices_1d, 4, axis=0),\n+            tf.one_hot(indices_1d, 4, axis=0),\n+        )\n+\n+        # Test 2D one-hot.\n+        indices_2d = np.array([[0, 1], [2, 3]])\n+        self.assertAllClose(\n+            knn.one_hot(indices_2d, 4), tf.one_hot(indices_2d, 4)\n+        )\n+        self.assertAllClose(\n+            knn.one_hot(indices_2d, 4, axis=2),\n+            tf.one_hot(indices_2d, 4, axis=2),\n+        )\n+        self.assertAllClose(\n+            knn.one_hot(indices_2d, 4, axis=1),\n+            tf.one_hot(indices_2d, 4, axis=1),\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#66e586ec70eed8edb1a0f3a3e0911e3522f51565", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 94 | Lines Deleted: 1 | Files Changed: 5 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 10/8 | Churn Δ: 95 | Churn Cumulative: 828 | Contributors (this commit): 6 | Commits (past 90d): 56 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,3 +8,4 @@ from keras_core.layers.regularization.activity_regularization import (\n     ActivityRegularization,\n )\n from keras_core.layers.regularization.dropout import Dropout\n+from keras_core.layers.regularization.gaussian_dropout import GaussianDropout\n\n@@ -1,4 +1,3 @@\n-from keras_core import backend\n from keras_core import constraints\n from keras_core import initializers\n from keras_core import operations as ops\n\n@@ -338,6 +338,7 @@ class Layer(Operation):\n             kwargs[\"training\"] = training\n \n         # TODO: Populate mask argument(s)\n+        # if self._mask_has_training_arg():\n \n         # Call the layer.\n         try:\n\n@@ -0,0 +1,63 @@\n+import math\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export(\"keras_core.layers.GaussianDropout\")\n+class GaussianDropout(layers.Layer):\n+    \"\"\"Apply multiplicative 1-centered Gaussian noise.\n+\n+    As it is a regularization layer, it is only active at training time.\n+\n+    Args:\n+        rate: Float, drop probability (as with `Dropout`).\n+            The multiplicative noise will have\n+            standard deviation `sqrt(rate / (1 - rate))`.\n+        seed: Integer, optional random seed to enable deterministic behavior.\n+\n+    Call arguments:\n+        inputs: Input tensor (of any rank).\n+        training: Python boolean indicating whether the layer should behave in\n+            training mode (adding dropout) or in inference mode (doing nothing).\n+    \"\"\"\n+\n+    def __init__(\n+        self, rate, noise_shape=None, seed=None, name=None, dtype=None\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n+            raise ValueError(\n+                f\"Invalid value received for argument \"\n+                \"`rate`. Expected a float value between 0 and 1. \"\n+                f\"Received: rate={rate}\"\n+            )\n+        self.rate = rate\n+        self.seed = seed\n+        self.noise_shape = noise_shape\n+        self.seed_generator = backend.random.SeedGenerator(seed)\n+        self.supports_masking = True\n+\n+    def call(self, inputs, training=False):\n+        if training and self.rate > 0:\n+            stddev = math.sqrt(self.rate / (1.0 - self.rate))\n+            return inputs * backend.random.normal(\n+                shape=ops.shape(inputs),\n+                mean=1.0,\n+                stddev=stddev,\n+                seed=self.seed_generator,\n+            )\n+        return inputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"rate\": self.rate,\n+            \"seed\": self.seed,\n+        }\n+        return {**base_config, **config}\n\n@@ -0,0 +1,29 @@\n+import numpy as np\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class GaussianDropoutTest(testing.TestCase):\n+    def test_gaussian_dropout_basics(self):\n+        self.run_layer_test(\n+            layers.Dropout,\n+            init_kwargs={\n+                \"rate\": 0.2,\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=1,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    def test_gaussian_dropout_correctness(self):\n+        inputs = np.ones((20, 500))\n+        layer = layers.GaussianDropout(0.3, seed=1337)\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(\n+            np.std(outputs), np.sqrt(0.3 / (1 - 0.3)), atol=0.02\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd3323b875a3bf83b15770b732135d47ac4e7a97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 7 | Files Changed: 6 | Hunks: 8 | Methods Changed: 10 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 99 | Churn Cumulative: 297 | Contributors (this commit): 2 | Commits (past 90d): 23 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,3 +9,4 @@ from keras_core.layers.regularization.activity_regularization import (\n )\n from keras_core.layers.regularization.dropout import Dropout\n from keras_core.layers.regularization.gaussian_dropout import GaussianDropout\n+from keras_core.layers.regularization.gaussian_noise import GaussianNoise\n\n@@ -42,7 +42,7 @@ class Dropout(layers.Layer):\n         self, rate, noise_shape=None, seed=None, name=None, dtype=None\n     ):\n         super().__init__(name=name, dtype=dtype)\n-        if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n+        if not 0 <= rate <= 1:\n             raise ValueError(\n                 f\"Invalid value received for argument \"\n                 \"`rate`. Expected a float value between 0 and 1. \"\n\n@@ -24,11 +24,9 @@ class GaussianDropout(layers.Layer):\n             training mode (adding dropout) or in inference mode (doing nothing).\n     \"\"\"\n \n-    def __init__(\n-        self, rate, noise_shape=None, seed=None, name=None, dtype=None\n-    ):\n+    def __init__(self, rate, seed=None, name=None, dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n+        if not 0 <= rate <= 1:\n             raise ValueError(\n                 f\"Invalid value received for argument \"\n                 \"`rate`. Expected a float value between 0 and 1. \"\n@@ -36,7 +34,6 @@ class GaussianDropout(layers.Layer):\n             )\n         self.rate = rate\n         self.seed = seed\n-        self.noise_shape = noise_shape\n         self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n \n\n@@ -7,7 +7,7 @@ from keras_core import testing\n class GaussianDropoutTest(testing.TestCase):\n     def test_gaussian_dropout_basics(self):\n         self.run_layer_test(\n-            layers.Dropout,\n+            layers.GaussianDropout,\n             init_kwargs={\n                 \"rate\": 0.2,\n             },\n\n@@ -0,0 +1,60 @@\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export(\"keras_core.layers.GaussianNoise\")\n+class GaussianNoise(layers.Layer):\n+    \"\"\"Apply additive zero-centered Gaussian noise.\n+\n+    This is useful to mitigate overfitting\n+    (you could see it as a form of random data augmentation).\n+    Gaussian Noise (GS) is a natural choice as corruption process\n+    for real valued inputs.\n+\n+    As it is a regularization layer, it is only active at training time.\n+\n+    Args:\n+        stddev: Float, standard deviation of the noise distribution.\n+        seed: Integer, optional random seed to enable deterministic behavior.\n+\n+    Call arguments:\n+        inputs: Input tensor (of any rank).\n+        training: Python boolean indicating whether the layer should behave in\n+            training mode (adding noise) or in inference mode (doing nothing).\n+    \"\"\"\n+\n+    def __init__(self, stddev, seed=None, name=None, dtype=None):\n+        super().__init__(name=name, dtype=dtype)\n+        if not 0 <= stddev <= 1:\n+            raise ValueError(\n+                f\"Invalid value received for argument \"\n+                \"`stddev`. Expected a float value between 0 and 1. \"\n+                f\"Received: stddev={stddev}\"\n+            )\n+        self.stddev = stddev\n+        self.seed = seed\n+        self.seed_generator = backend.random.SeedGenerator(seed)\n+        self.supports_masking = True\n+\n+    def call(self, inputs, training=False):\n+        if training and self.stddev > 0:\n+            return inputs + backend.random.normal(\n+                shape=ops.shape(inputs),\n+                mean=0.0,\n+                stddev=self.stddev,\n+                seed=self.seed_generator,\n+            )\n+        return inputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"stddev\": self.stddev,\n+            \"seed\": self.seed,\n+        }\n+        return {**base_config, **config}\n\n@@ -0,0 +1,27 @@\n+import numpy as np\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class GaussianNoiseTest(testing.TestCase):\n+    def test_gaussian_noise_basics(self):\n+        self.run_layer_test(\n+            layers.GaussianNoise,\n+            init_kwargs={\n+                \"stddev\": 0.2,\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=1,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    def test_gaussian_noise_correctness(self):\n+        inputs = np.ones((20, 500))\n+        layer = layers.GaussianNoise(0.3, seed=1337)\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(np.std(outputs), 0.3, atol=0.02)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#da55728a35cb01616ff9f88a9330d7334f4dc537", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 210 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -144,7 +144,7 @@ def categorical_accuracy(y_true, y_pred):\n     return matches\n \n \n-@keras_core_export(\"keras_core.metrics.BinaryAccuracy\")\n+@keras_core_export(\"keras_core.metrics.CategoricalAccuracy\")\n class CategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Calculates how often predictions match one-hot labels.\n \n@@ -157,7 +157,7 @@ class CategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     operation that simply divides `total` by `count`.\n \n     `y_pred` and `y_true` should be passed in as vectors of probabilities,\n-    rather than as labels. If necessary, use `tf.one_hot` to expand `y_true` as\n+    rather than as labels. If necessary, use `ops.one_hot` to expand `y_true` as\n     a vector.\n \n     If `sample_weight` is `None`, weights default to 1.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1b7c53d0505b9fca65a98a7400a991cea982d1b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 248 | Lines Deleted: 32 | Files Changed: 8 | Hunks: 38 | Methods Changed: 18 | Complexity Δ (Sum/Max): 272/118 | Churn Δ: 280 | Churn Cumulative: 16649 | Contributors (this commit): 21 | Commits (past 90d): 25 | Contributors (cumulative): 73 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n+from keras.saving import object_registration\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n@@ -515,6 +516,7 @@ class AttentionSubclassTest(test_combinations.TestCase):\n         self.assertEqual(output.shape.as_list(), [None, 40, 80])\n \n \n+@object_registration.register_keras_serializable()\n class TestModel(keras.Model):\n     def __init__(self):\n         super().__init__()\n@@ -540,12 +542,19 @@ class TestModel(keras.Model):\n \n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class KerasModelSavingTest(test_combinations.TestCase):\n-    def test_keras_saving_subclass(self):\n+    @parameterized.parameters(\"tf\", \"keras_v3\")\n+    def test_keras_saving_subclass(self, save_format):\n         model = TestModel()\n         query = keras.Input(shape=(40, 80))\n         _ = model(query)\n         model_path = self.get_temp_dir() + \"/tmp_model\"\n-        keras.models.save_model(model, model_path, save_format=\"tf\")\n+        if save_format == \"keras_v3\":\n+            if not tf.__internal__.tf2.enabled():\n+                self.skipTest(\n+                    \"TF2 must be enabled to use the new `.keras` saving.\"\n+                )\n+            model_path += \".keras\"\n+        keras.models.save_model(model, model_path, save_format=save_format)\n         reloaded_model = keras.models.load_model(model_path)\n         self.assertEqual(\n             len(model.trainable_variables),\n@@ -556,7 +565,7 @@ class KerasModelSavingTest(test_combinations.TestCase):\n         ):\n             self.assertAllEqual(src_v, loaded_v)\n \n-    @parameterized.parameters(\"h5\", \"tf\")\n+    @parameterized.parameters(\"h5\", \"tf\", \"keras_v3\")\n     def test_keras_saving_functional(self, save_format):\n         model = TestModel()\n         query = keras.Input(shape=(40, 80))\n@@ -565,6 +574,12 @@ class KerasModelSavingTest(test_combinations.TestCase):\n         )(query, query)\n         model = keras.Model(inputs=query, outputs=output)\n         model_path = self.get_temp_dir() + \"/tmp_model\"\n+        if save_format == \"keras_v3\":\n+            if not tf.__internal__.tf2.enabled():\n+                self.skipTest(\n+                    \"TF2 must be enabled to use the new `.keras` saving.\"\n+                )\n+            model_path += \".keras\"\n         keras.models.save_model(model, model_path, save_format=save_format)\n         reloaded_model = keras.models.load_model(model_path)\n         self.assertEqual(\n\n@@ -51,9 +51,24 @@ class SpectralNormalizationTest(test_combinations.TestCase):\n         # initialize model\n         model.predict(tf.random.uniform((2, 1)))\n \n+        with self.subTest(\"h5\"):\n             model.save(\"test.h5\")\n             new_model = keras.models.load_model(\"test.h5\")\n \n+            self.assertEqual(\n+                model.layers[0].get_config(), new_model.layers[0].get_config()\n+            )\n+        with self.subTest(\"savedmodel\"):\n+            model.save(\"test\")\n+            new_model = keras.models.load_model(\"test\")\n+\n+            self.assertEqual(\n+                model.layers[0].get_config(), new_model.layers[0].get_config()\n+            )\n+        with self.subTest(\"keras_v3\"):\n+            model.save(\"test.keras\")\n+            new_model = keras.models.load_model(\"test.keras\")\n+\n             self.assertEqual(\n                 model.layers[0].get_config(), new_model.layers[0].get_config()\n             )\n\n@@ -154,7 +154,7 @@ class HashedCrossingTest(test_combinations.TestCase):\n             tf.sparse.to_dense(original_outputs),\n         )\n \n-    def test_saved_model_keras(self):\n+    def test_saving_keras(self):\n         string_in = keras.Input(shape=(1,), dtype=tf.string)\n         int_in = keras.Input(shape=(1,), dtype=tf.int64)\n         out = hashed_crossing.HashedCrossing(num_bins=10)((string_in, int_in))\n@@ -167,12 +167,34 @@ class HashedCrossingTest(test_combinations.TestCase):\n         output_data = model((string_data, int_data))\n         self.assertAllClose(output_data, expected_output)\n \n+        with self.subTest(\"savedmodel\"):\n             # Save the model to disk.\n             output_path = os.path.join(self.get_temp_dir(), \"saved_model\")\n             model.save(output_path, save_format=\"tf\")\n             loaded_model = keras.models.load_model(\n                 output_path,\n-            custom_objects={\"HashedCrossing\": hashed_crossing.HashedCrossing},\n+                custom_objects={\n+                    \"HashedCrossing\": hashed_crossing.HashedCrossing\n+                },\n+            )\n+\n+            # Validate correctness of the new model.\n+            new_output_data = loaded_model((string_data, int_data))\n+            self.assertAllClose(new_output_data, expected_output)\n+\n+        with self.subTest(\"keras_v3\"):\n+            if not tf.__internal__.tf2.enabled():\n+                self.skipTest(\n+                    \"TF2 must be enabled to use the new `.keras` saving.\"\n+                )\n+            # Save the model to disk.\n+            output_path = os.path.join(self.get_temp_dir(), \"model.keras\")\n+            model.save(output_path, save_format=\"keras_v3\")\n+            loaded_model = keras.models.load_model(\n+                output_path,\n+                custom_objects={\n+                    \"HashedCrossing\": hashed_crossing.HashedCrossing\n+                },\n             )\n \n             # Validate correctness of the new model.\n\n@@ -414,6 +414,30 @@ class HashingTest(test_combinations.TestCase):\n         new_output_data = loaded_model(input_data)\n         self.assertAllClose(new_output_data, original_output_data)\n \n+    @test_utils.run_v2_only\n+    def test_save_keras_v3(self):\n+        input_data = np.array(\n+            [\"omar\", \"stringer\", \"marlo\", \"wire\", \"skywalker\"]\n+        )\n+\n+        inputs = keras.Input(shape=(None,), dtype=tf.string)\n+        outputs = hashing.Hashing(num_bins=100)(inputs)\n+        model = keras.Model(inputs=inputs, outputs=outputs)\n+\n+        original_output_data = model(input_data)\n+\n+        # Save the model to disk.\n+        output_path = os.path.join(self.get_temp_dir(), \"tf_keras_model.keras\")\n+        model.save(output_path, save_format=\"keras_v3\")\n+        loaded_model = keras.models.load_model(output_path)\n+\n+        # Ensure that the loaded model is unique (so that the save/load is real)\n+        self.assertIsNot(model, loaded_model)\n+\n+        # Validate correctness of the new model.\n+        new_output_data = loaded_model(input_data)\n+        self.assertAllClose(new_output_data, original_output_data)\n+\n     @parameterized.named_parameters(\n         (\n             \"list_input\",\n\n@@ -2211,6 +2211,7 @@ class IndexLookupSavingTest(\n             ]\n         )\n         expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+        vocab_file = self._write_to_temp_file(\"temp\", vocab_data)\n \n         # Build and validate a golden model.\n         input_data = keras.Input(shape=(None,), dtype=tf.string)\n@@ -2220,27 +2221,52 @@ class IndexLookupSavingTest(\n             mask_token=\"\",\n             oov_token=\"[OOV]\",\n             vocabulary_dtype=tf.string,\n+            vocabulary=vocab_file,\n         )\n-        layer.set_vocabulary(vocab_data)\n         int_data = layer(input_data)\n         model = keras.Model(inputs=input_data, outputs=int_data)\n         output_dataset = model.predict(input_array)\n         self.assertAllEqual(output_dataset, expected_output)\n \n+        with self.subTest(\"keras_v3\"):\n             # Save the model to disk.\n-        output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n-        model.save(output_path, save_format=\"tf\")\n-\n-        # Delete the session and graph to ensure that the loaded model is\n-        # generated from scratch.\n-        keras.backend.clear_session()\n+            output_path = os.path.join(\n+                self.get_temp_dir(), \"tf_keras_model.keras\"\n+            )\n+            model.save(output_path, save_format=\"keras_v3\")\n \n             loaded_model = keras.models.load_model(\n                 output_path,\n                 custom_objects={\"IndexLookup\": index_lookup.IndexLookup},\n             )\n \n-        # Ensure that the loaded model is unique (so that the save/load is real)\n+            # Ensure that the loaded model is unique\n+            # (so that the save/load is real)\n+            self.assertIsNot(model, loaded_model)\n+\n+            # Validate correctness of the new model.\n+            new_output_dataset = loaded_model.predict(input_array)\n+            self.assertAllEqual(new_output_dataset, expected_output)\n+\n+        with self.subTest(\"savedmodel\"):\n+            # Save the model to disk.\n+            output_path = os.path.join(\n+                self.get_temp_dir(), \"tf_keras_saved_model\"\n+            )\n+            model.save(output_path, save_format=\"tf\")\n+\n+            # Delete the session and graph to ensure that the loaded model is\n+            # generated from scratch.\n+            keras.backend.clear_session()\n+            tf.io.gfile.remove(vocab_file)\n+\n+            loaded_model = keras.models.load_model(\n+                output_path,\n+                custom_objects={\"IndexLookup\": index_lookup.IndexLookup},\n+            )\n+\n+            # Ensure that the loaded model is unique\n+            # (so that the save/load is real)\n             self.assertIsNot(model, loaded_model)\n \n             # Validate correctness of the new model.\n@@ -2401,8 +2427,58 @@ class IndexLookupSavingTest(\n         output_dataset = model.predict(input_array)\n         self.assertAllEqual(output_dataset, expected_output)\n \n+        with self.subTest(\"keras_v3\"):\n             # Save the model to disk.\n-        output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n+            output_path = os.path.join(\n+                self.get_temp_dir(), \"tf_keras_model.keras\"\n+            )\n+            model.save(output_path, save_format=\"keras_v3\")\n+\n+            loaded_model = keras.models.load_model(\n+                output_path,\n+                custom_objects={\"IndexLookup\": index_lookup.IndexLookup},\n+            )\n+\n+            # Ensure that the loaded model is unique\n+            # (so that the save/load is real)\n+            self.assertIsNot(model, loaded_model)\n+\n+            # Validate correctness of the new model.\n+            new_output_dataset = loaded_model.predict(input_array)\n+            self.assertAllEqual(new_output_dataset, expected_output)\n+\n+            # Try re-saving the layer. This simulates saving a layer\n+            # contained at a hub Module.\n+            input_data_2 = keras.Input(shape=(None,), dtype=tf.string)\n+            output_2 = loaded_model(input_data_2)\n+            model_2 = keras.Model(inputs=input_data_2, outputs=output_2)\n+            new_output_dataset = model_2.predict(input_array)\n+            self.assertAllEqual(new_output_dataset, expected_output)\n+\n+            # Save the model to disk.\n+            output_path = os.path.join(\n+                self.get_temp_dir(), \"tf_keras_model_2.keras\"\n+            )\n+            model_2.save(output_path, save_format=\"keras_v3\")\n+\n+            loaded_model = keras.models.load_model(\n+                output_path,\n+                custom_objects={\"IndexLookup\": index_lookup.IndexLookup},\n+            )\n+\n+            # Ensure that the loaded model is unique\n+            # (so that the save/load is real)\n+            self.assertIsNot(model, loaded_model)\n+\n+            # Validate correctness of the new model.\n+            new_output_dataset = loaded_model.predict(input_array)\n+            self.assertAllEqual(new_output_dataset, expected_output)\n+\n+        with self.subTest(\"saved_model\"):\n+            # Save the model to disk.\n+            output_path = os.path.join(\n+                self.get_temp_dir(), \"tf_keras_saved_model\"\n+            )\n             model.save(output_path, save_format=\"tf\")\n \n             # Delete the session and graph to ensure that the loaded model is\n@@ -2415,15 +2491,16 @@ class IndexLookupSavingTest(\n                 custom_objects={\"IndexLookup\": index_lookup.IndexLookup},\n             )\n \n-        # Ensure that the loaded model is unique (so that the save/load is real)\n+            # Ensure that the loaded model is unique\n+            # (so that the save/load is real)\n             self.assertIsNot(model, loaded_model)\n \n             # Validate correctness of the new model.\n             new_output_dataset = loaded_model.predict(input_array)\n             self.assertAllEqual(new_output_dataset, expected_output)\n \n-        # Try re-saving the layer. This simulates saving a layer contained at\n-        # a hub Module.\n+            # Try re-saving the layer. This simulates saving a layer\n+            # contained at a hub Module.\n             input_data_2 = keras.Input(shape=(None,), dtype=tf.string)\n             output_2 = loaded_model(input_data_2)\n             model_2 = keras.Model(inputs=input_data_2, outputs=output_2)\n@@ -2445,7 +2522,8 @@ class IndexLookupSavingTest(\n                 custom_objects={\"IndexLookup\": index_lookup.IndexLookup},\n             )\n \n-        # Ensure that the loaded model is unique (so that the save/load is real)\n+            # Ensure that the loaded model is unique\n+            # (so that the save/load is real)\n             self.assertIsNot(model, loaded_model)\n \n             # Validate correctness of the new model.\n\n@@ -630,8 +630,36 @@ class IntegerLookupSavingTest(\n         output_dataset = model.predict(input_array)\n         self.assertAllEqual(output_dataset, expected_output)\n \n+        with self.subTest(\"keras_v3\"):\n+            if not tf.__internal__.tf2.enabled():\n+                self.skipTest(\n+                    \"TF2 must be enabled to use the new `.keras` saving.\"\n+                )\n+\n             # Save the model to disk.\n-        output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n+            output_path = os.path.join(\n+                self.get_temp_dir(), \"tf_keras_model.keras\"\n+            )\n+            model.save(output_path, save_format=\"keras_v3\")\n+\n+            loaded_model = keras.models.load_model(\n+                output_path,\n+                custom_objects={\"IntegerLookup\": integer_lookup.IntegerLookup},\n+            )\n+\n+            # Ensure that the loaded model is unique\n+            # (so that the save/load is real)\n+            self.assertIsNot(model, loaded_model)\n+\n+            # Validate correctness of the new model.\n+            new_output_dataset = loaded_model.predict(input_array)\n+            self.assertAllEqual(new_output_dataset, expected_output)\n+\n+        with self.subTest(\"savedmodel\"):\n+            # Save the model to disk.\n+            output_path = os.path.join(\n+                self.get_temp_dir(), \"tf_keras_saved_model\"\n+            )\n             model.save(output_path, save_format=\"tf\")\n \n             # Delete the session and graph to ensure that the loaded model is\n@@ -645,7 +673,8 @@ class IntegerLookupSavingTest(\n                 custom_objects={\"IntegerLookup\": integer_lookup.IntegerLookup},\n             )\n \n-        # Ensure that the loaded model is unique (so that the save/load is real)\n+            # Ensure that the loaded model is unique\n+            # (so that the save/load is real)\n             self.assertIsNot(model, loaded_model)\n \n             # Validate correctness of the new model.\n\n@@ -392,7 +392,7 @@ class NormalizationAdaptTest(\n         {\"adapted\": True},\n         {\"adapted\": False},\n     )\n-    def test_saved_model_tf(self, adapted):\n+    def test_saving_tf(self, adapted):\n         input_data = [[0.0], [2.0], [0.0], [2.0]]\n         expected_output = [[-1.0], [1.0], [-1.0], [1.0]]\n \n@@ -422,10 +422,10 @@ class NormalizationAdaptTest(\n         self.assertAllClose(new_output_data, expected_output)\n \n     @parameterized.product(\n-        save_format=[\"tf\", \"h5\"],\n+        save_format=[\"tf\", \"h5\", \"keras_v3\"],\n         adapt=[True, False],\n     )\n-    def test_saved_model_keras(self, save_format, adapt):\n+    def test_saving_keras(self, save_format, adapt):\n         input_data = [[0.0], [2.0], [0.0], [2.0]]\n         expected_output = [[-1.0], [1.0], [-1.0], [1.0]]\n \n@@ -443,7 +443,13 @@ class NormalizationAdaptTest(\n         self.assertAllClose(output_data, expected_output)\n \n         # Save the model to disk.\n-        output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n+        output_path = os.path.join(self.get_temp_dir(), \"tf_keras_model\")\n+        if save_format == \"keras_v3\":\n+            if not tf.__internal__.tf2.enabled():\n+                self.skipTest(\n+                    \"TF2 must be enabled to use the new `.keras` saving.\"\n+                )\n+            output_path += \".keras\"\n         model.save(output_path, save_format=save_format)\n         loaded_model = keras.models.load_model(\n             output_path, custom_objects={\"Normalization\": cls}\n@@ -457,10 +463,10 @@ class NormalizationAdaptTest(\n         self.assertAllClose(new_output_data, expected_output)\n \n     @parameterized.product(\n-        save_format=[\"tf\", \"h5\"],\n+        save_format=[\"tf\", \"h5\", \"keras_v3\"],\n         adapt=[True, False],\n     )\n-    def test_saved_model_keras_invert(self, save_format, adapt):\n+    def test_saving_keras_invert(self, save_format, adapt):\n         expected_output = [[0.0], [2.0], [0.0], [2.0]]\n         input_data = [[-1.0], [1.0], [-1.0], [1.0]]\n \n@@ -478,9 +484,13 @@ class NormalizationAdaptTest(\n         self.assertAllClose(output_data, expected_output)\n \n         # Save the model to disk.\n-        output_path = os.path.join(\n-            self.get_temp_dir(), \"tf_keras_saved_model_invert\"\n+        output_path = os.path.join(self.get_temp_dir(), \"tf_keras_model_invert\")\n+        if save_format == \"keras_v3\":\n+            if not tf.__internal__.tf2.enabled():\n+                self.skipTest(\n+                    \"TF2 must be enabled to use the new `.keras` saving.\"\n                 )\n+            output_path += \".keras\"\n         model.save(output_path, save_format=save_format)\n         loaded_model = keras.models.load_model(\n             output_path, custom_objects={\"Normalization\": cls}\n\n@@ -67,7 +67,7 @@ class DropoutTest(test_combinations.TestCase):\n             rng_state_var, dropout._random_generator._generator._state_var\n         )\n \n-    def test_dropout_with_savemodel(self):\n+    def test_dropout_with_saving(self):\n         inputs = keras.Input(shape=(5, 10))\n         layer = keras.layers.Dropout(0.5, force_generator=True)\n         outputs = layer(inputs)\n@@ -83,8 +83,10 @@ class DropoutTest(test_combinations.TestCase):\n         # Make sure the layer does dropout value when training\n         self.assertNotAllClose(train, predict)\n \n+        with self.subTest(\"savedmodel\"):\n             model.save(\n-            os.path.join(self.get_temp_dir(), \"savedmodel\"), save_format=\"tf\"\n+                os.path.join(self.get_temp_dir(), \"savedmodel\"),\n+                save_format=\"tf\",\n             )\n             loaded_model = keras.models.load_model(\n                 os.path.join(self.get_temp_dir(), \"savedmodel\")\n@@ -97,8 +99,29 @@ class DropoutTest(test_combinations.TestCase):\n             self.assertNotAllClose(train, train2)\n             self.assertIsNotNone(loaded_model.layers[1]._random_generator)\n \n-        # Also make sure the checkpoint doesn't contain any variable from the\n-        # dropout layer, to keep the backward compatibility.\n+        with self.subTest(\"keras_v3\"):\n+            if not tf.__internal__.tf2.enabled():\n+                self.skipTest(\n+                    \"TF2 must be enabled to use the new `.keras` saving.\"\n+                )\n+            model.save(\n+                os.path.join(self.get_temp_dir(), \"model.keras\"),\n+                save_format=\"keras_v3\",\n+            )\n+            loaded_model = keras.models.load_model(\n+                os.path.join(self.get_temp_dir(), \"model.keras\")\n+            )\n+            predict2 = loaded_model(np.ones((20, 5, 10)))\n+\n+            self.assertAllClose(predict, predict2)\n+            # Make sure the model dropout different value after loading\n+            train2 = loaded_model(np.ones((20, 5, 10)), training=True)\n+            self.assertNotAllClose(train, train2)\n+            self.assertIsNotNone(loaded_model.layers[1]._random_generator)\n+\n+        with self.subTest(\"checkpoint\"):\n+            # Also make sure the checkpoint doesn't contain any variable from\n+            # the dropout layer, to keep the backward compatibility.\n             checkpoint = tf.train.Checkpoint(model)\n             save_path = checkpoint.save(\n                 os.path.join(self.get_temp_dir(), \"checkpoint\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e5e3c9f833ca1d4ef97bd3d65d3480df9331c584", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 206 | Lines Deleted: 59 | Files Changed: 2 | Hunks: 30 | Methods Changed: 22 | Complexity Δ (Sum/Max): 22/13 | Churn Δ: 265 | Churn Cumulative: 1183 | Contributors (this commit): 6 | Commits (past 90d): 61 | Contributors (cumulative): 9 | DMM Complexity: 0.5714285714285714\n\nDIFF:\n@@ -36,9 +36,6 @@ from keras_core.operations.operation import Operation\n from keras_core.utils import summary_utils\n from keras_core.utils.tracking import Tracker\n \n-# TODO: cache all call signature processing.\n-# See layer_utils.CallFunctionSpec() in Keras.\n-\n \n @keras_core_export([\"keras_core.Layer\", \"keras_core.layers.Layer\"])\n class Layer(Operation):\n@@ -277,8 +274,8 @@ class Layer(Operation):\n         self._supports_masking = value\n \n     @utils.default\n-    def compute_mask(self, *args, **kwargs):\n-        raise NotImplementedError\n+    def compute_mask(self, inputs, previous_mask):\n+        return previous_mask\n \n     def __call__(self, *args, **kwargs):\n         self._check_super_called()\n@@ -290,7 +287,7 @@ class Layer(Operation):\n             if isinstance(x, np.ndarray) or backend.is_tensor(x):\n                 # TODO: only cast when float!\n                 return backend.convert_to_tensor(x, dtype=self.compute_dtype)\n-            # TODO: cast KerasTensor too\n+            # TODO: cast KerasTensor too?\n             return x\n \n         args = nest.map_structure(maybe_convert, args)\n@@ -306,27 +303,28 @@ class Layer(Operation):\n                     f\"(of type {type(arg)})\"\n                 )\n \n+        call_spec = CallSpec(self.call, args, kwargs)\n+\n         # 4. Check input spec for 1st positional arg.\n         # TODO: consider extending this to all args and kwargs.\n-        self._assert_input_compatibility(*args)\n+        self._assert_input_compatibility(call_spec.first_arg)\n         ######################################\n \n         ###############\n-        # Call build. #\n-        self._maybe_build(*args, **kwargs)\n+        # 5. Call build. #\n+        self._maybe_build(call_spec)\n         ###############\n \n         # Maintains info about the `Layer.call` stack.\n         call_context = self._get_call_context()\n \n-        # Infer training value\n+        # 6. Infer training value\n         # Training phase for `Layer.call` is set via (in order of priority):\n         # (1) The `training` argument passed to this `Layer.call`, if not None\n         # (2) The training argument of an outer `Layer.call`.\n         # (4) Any non-None default value for `training` in the call signature\n         # (5) False (treating the layer as if it's in inference)\n-        arguments_dict = get_arguments_dict(self.call, *args, **kwargs)\n-        training = arguments_dict.get(\"training\", None)\n+        training = call_spec.arguments_dict.get(\"training\", None)\n         if training is None:\n             training = call_context.training\n             if training is None:\n@@ -337,10 +335,34 @@ class Layer(Operation):\n         if self._call_has_training_arg():\n             kwargs[\"training\"] = training\n \n-        # TODO: Populate mask argument(s)\n-        # if self._mask_has_training_arg():\n+        # 7. Populate mask argument(s)\n+        if self.supports_masking:\n+            if len(call_spec.tensor_arguments_dict) == 1:\n+                if (\n+                    \"mask\" in call_spec.argument_names\n+                    and call_spec.arguments_dict[\"mask\"] is None\n+                ):\n+                    arg_name = list(call_spec.tensor_arguments_dict.keys())[0]\n+                    only_tensor_arg = call_spec.tensor_arguments_dict[arg_name]\n+                    mask = nest.map_structure(\n+                        lambda x: getattr(x, \"_keras_mask\", None),\n+                        only_tensor_arg,\n+                    )\n+                    kwargs[\"mask\"] = mask\n+            elif len(call_spec.tensor_arguments_dict) > 1:\n+                for k, v in call_spec.tensor_arguments_dict.items():\n+                    expected_mask_arg_name = f\"{k}_mask\"\n+                    if expected_mask_arg_name in call_spec.argument_names:\n+                        if (\n+                            call_spec.arguments_dict[expected_mask_arg_name]\n+                            is None\n+                        ):\n+                            mask = nest.map_structure(\n+                                lambda x: getattr(x, \"_keras_mask\", None), v\n+                            )\n+                            kwargs[expected_mask_arg_name] = mask\n \n-        # Call the layer.\n+        # 8. Call the layer.\n         try:\n             with backend.name_scope(self.name):\n                 if self.autocast and self.compute_dtype != self.variable_dtype:\n@@ -356,8 +378,16 @@ class Layer(Operation):\n                         if backend.is_tensor(output):\n                             self.add_loss(self.activity_regularizer(output))\n \n-            # TODO: Set masks on outputs\n-            # self._set_mask_metadata(inputs, outputs, previous_mask)\n+            if self.supports_masking:\n+                # Set masks on outputs,\n+                # provided only the first positional input arg and its mask.\n+                # TODO: consider extending this to all args and kwargs.\n+                previous_mask = getattr(\n+                    call_spec.first_arg, \"_keras_mask\", None\n+                )\n+                self._set_mask_metadata(\n+                    call_spec.first_arg, outputs, previous_mask\n+                )\n         finally:\n             # Destroy call context if we created it\n             self._maybe_reset_call_context()\n@@ -422,8 +452,8 @@ class Layer(Operation):\n             return super().compute_output_spec(*args, **kwargs)\n         else:\n             # Use compute_output_shape() to return the right output spec\n-            arguments_dict = get_arguments_dict(self.call, *args, **kwargs)\n-            shapes_dict = get_shapes_dict(arguments_dict)\n+            call_spec = CallSpec(self.call, args, kwargs)\n+            shapes_dict = get_shapes_dict(call_spec)\n             if len(shapes_dict) == 1:\n                 # Single arg: pass it positionally\n                 input_shape = tuple(shapes_dict.values())[0]\n@@ -553,10 +583,9 @@ class Layer(Operation):\n             )\n         return summary_utils.count_params(self.weights)\n \n-    def _maybe_build(self, *args, **kwargs):\n+    def _maybe_build(self, call_spec):\n         if not self.built:\n-            arguments_dict = get_arguments_dict(self.call, *args, **kwargs)\n-            shapes_dict = get_shapes_dict(arguments_dict)\n+            shapes_dict = get_shapes_dict(call_spec)\n             self._build_shapes_dict = shapes_dict\n             failure = False\n             if len(shapes_dict) == 1:\n@@ -578,14 +607,32 @@ class Layer(Operation):\n                 # and check that build() expects the right args.\n                 check_build_signature(self.build, shapes_dict)\n                 with backend.name_scope(self.name):\n-                    if utils.is_default(\n-                        self.build\n-                    ) and might_have_unbuilt_state(self):\n+                    if utils.is_default(self.build):\n+                        if might_have_unbuilt_state(self):\n                             status = self._build_by_run_for_kwargs(shapes_dict)\n                             if not status:\n                                 failure = True\n                     else:\n+                        run_build = True\n+                        build_args = set(\n+                            inspect.signature(self.build).parameters.keys()\n+                        )\n+                        for key in shapes_dict.keys():\n+                            if key not in build_args:\n+                                run_build = False\n+                        if run_build:\n                             self.build(**shapes_dict)\n+                        else:\n+                            raise ValueError(\n+                                \"In a layer with multiple tensor arguments \"\n+                                \"in call(), the build() method should accept \"\n+                                \"corresponding `*_shape` arguments, e.g. \"\n+                                \"if the call signature is `def call(self, x1, x2)` \"\n+                                \"then the build signature should be \"\n+                                \"`def build(self, x1_shape, x2_shape)`. \"\n+                                \"Keras will not build this layer automatically \"\n+                                \"since it does not conform to this.\"\n+                            )\n             if failure:\n                 raise ValueError(\n                     f\"Layer '{self.name}' looks like it has \"\n@@ -601,7 +648,7 @@ class Layer(Operation):\n \n             # Check input spec again (after build, since self.input_spec\n             # may have been updated\n-            self._assert_input_compatibility(*args)\n+            self._assert_input_compatibility(call_spec.first_arg)\n \n     def _build_by_run_for_single_pos_arg(self, input_shape):\n         # Case: all inputs are in the first arg (possibly nested).\n@@ -651,12 +698,16 @@ class Layer(Operation):\n             return False\n \n     def __repr__(self):\n-        # TODO: improve\n-        return f\"<{self.__class__.__name__} name={self.name}>\"\n+        return (\n+            f\"<{self.__class__.__name__} \"\n+            f\"name={self.name}, built={self.built}>\"\n+        )\n \n     def __str__(self):\n-        # TODO: improve\n-        return f\"<{self.__class__.__name__} name={self.name}>\"\n+        return (\n+            f\"<{self.__class__.__name__} \"\n+            f\"name={self.name}, built={self.built}>\"\n+        )\n \n     def __setattr__(self, name, value):\n         # Track Variables, Layers, Metrics\n@@ -672,10 +723,10 @@ class Layer(Operation):\n                 \"Go add it!\"\n             )\n \n-    def _assert_input_compatibility(self, *args):\n-        if args and self.input_spec:\n+    def _assert_input_compatibility(self, arg_0):\n+        if self.input_spec:\n             input_spec.assert_input_compatibility(\n-                self.input_spec, args[0], layer_name=self.name\n+                self.input_spec, arg_0, layer_name=self.name\n             )\n \n     def _call_has_training_arg(self):\n@@ -732,12 +783,6 @@ class Layer(Operation):\n         return layers\n \n     def _set_mask_metadata(self, inputs, outputs, previous_mask):\n-        # Many `Layer`s don't need to call `compute_mask`.\n-        # This method is optimized to do as little work as needed for the common\n-        # case.\n-        if not self._supports_masking:\n-            return\n-\n         flat_outputs = nest.flatten(outputs)\n \n         mask_already_computed = all(\n@@ -752,10 +797,55 @@ class Layer(Operation):\n \n         flat_masks = nest.flatten(output_masks)\n         for tensor, mask in zip(flat_outputs, flat_masks):\n+            if getattr(tensor, \"_keras_mask\", None) is None:\n                 tensor._keras_mask = mask\n \n \n-def get_arguments_dict(fn, *args, **kwargs):\n+def is_backend_tensor_or_symbolic(x):\n+    return backend.is_tensor(x) or isinstance(x, backend.KerasTensor)\n+\n+\n+class CallSpec:\n+    def __init__(self, call_fn, args, kwargs):\n+        sig = inspect.signature(call_fn)\n+        bound_args = sig.bind(*args, **kwargs)\n+        bound_args.apply_defaults()\n+        arg_dict = {}\n+        arg_names = []\n+        tensor_arg_dict = {}\n+        tensor_args = []\n+        tensor_arg_names = []\n+        nested_tensor_arg_names = []\n+        for name, value in bound_args.arguments.items():\n+            arg_dict[name] = value\n+            arg_names.append(name)\n+            if is_backend_tensor_or_symbolic(value):\n+                tensor_args.append(value)\n+                tensor_arg_names.append(name)\n+                tensor_arg_dict[name] = value\n+            elif nest.is_nested(value):\n+                flat_values = nest.flatten(value)\n+                if all(is_backend_tensor_or_symbolic(x) for x in flat_values):\n+                    tensor_args.append(value)\n+                    tensor_arg_names.append(name)\n+                    tensor_arg_dict[name] = value\n+                    nested_tensor_arg_names.append(name)\n+                elif any(is_backend_tensor_or_symbolic(x) for x in flat_values):\n+                    raise ValueError(\n+                        \"In a nested call() argument, \"\n+                        \"you cannot mix tensors and non-tensors. \"\n+                        \"Received invalid mixed argument: \"\n+                        f\"{name}={value}\"\n+                    )\n+        self.arguments_dict = arg_dict\n+        self.argument_names = arg_names\n+        self.tensor_arguments_dict = tensor_arg_dict\n+        self.tensor_arguments_names = tensor_arg_names\n+        self.nested_tensor_argument_names = nested_tensor_arg_names\n+        self.first_arg = arg_dict[arg_names[0]]\n+\n+\n+def get_arguments_dict(fn, args, kwargs):\n     \"\"\"Return a dict mapping argument names to their values.\"\"\"\n     sig = inspect.signature(fn)\n     bound_args = sig.bind(*args, **kwargs)\n@@ -765,37 +855,24 @@ def get_arguments_dict(fn, *args, **kwargs):\n     return arg_dict\n \n \n-def get_shapes_dict(arguments_dict):\n+def get_shapes_dict(call_spec):\n     \"\"\"Convert the call() arguments dict into a dict of input shape arguments.\n \n     Example:\n \n     ```\n-    >>> get_shapes_dict(\n-        {\"input_a\": KerasTensor(shape=(2, 3)), \"training\": False})\n+    >>> get_shapes_dict(call_spec)\n     {\"input_a_shape\": (2, 3)}\n     ```\n     \"\"\"\n     shapes_dict = {}\n-    for k, v in arguments_dict.items():\n-        if isinstance(v, KerasTensor) or backend.is_tensor(v):\n-            shapes_dict[f\"{k}_shape\"] = backend.standardize_shape(v.shape)\n-        elif nest.is_nested(v):\n-            flat = nest.flatten(v)\n-            if any(\n-                isinstance(x, KerasTensor) or backend.is_tensor(x) for x in flat\n-            ):\n-                if not all(\n-                    isinstance(x, KerasTensor) or backend.is_tensor(x)\n-                    for x in flat\n-                ):\n-                    raise ValueError(\n-                        \"You cannot mix tensors and non-tensors in a nested \"\n-                        f\"argument. Invalid argument: {k}={v}\"\n-                    )\n+    for k, v in call_spec.tensor_arguments_dict.items():\n+        if k in call_spec.nested_tensor_argument_names:\n             shapes_dict[f\"{k}_shape\"] = nest.map_structure(\n                 lambda x: backend.standardize_shape(x.shape), v\n             )\n+        else:\n+            shapes_dict[f\"{k}_shape\"] = backend.standardize_shape(v.shape)\n     return shapes_dict\n \n \n\n@@ -262,3 +262,73 @@ class LayerTest(testing.TestCase):\n         self.assertEqual(layer.variable_dtype, \"float32\")\n         self.assertEqual(y.dtype.name, \"float16\")\n         self.assertEqual(layer.kernel.dtype, \"float32\")\n+\n+    def test_masking(self):\n+        class BasicMaskedLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.supports_masking = True\n+\n+            def call(self, x, mask=None):\n+                assert mask is not None\n+                return x\n+\n+        layer = BasicMaskedLayer()\n+        x = backend.numpy.ones((4, 4))\n+        x._keras_mask = backend.numpy.ones((4,))\n+        layer(x)\n+\n+        class NestedInputMaskedLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.supports_masking = True\n+\n+            def call(self, x, mask=None):\n+                assert isinstance(x, list)\n+                assert len(x) == 2\n+                assert isinstance(mask, list)\n+                assert len(mask) == 2\n+                return x\n+\n+        layer = NestedInputMaskedLayer()\n+        x1 = backend.numpy.ones((4, 4))\n+        x1._keras_mask = backend.numpy.ones((4,))\n+        x2 = backend.numpy.ones((4, 4))\n+        x2._keras_mask = backend.numpy.ones((4,))\n+        layer([x1, x2])\n+\n+        class PositionalInputsMaskedLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.supports_masking = True\n+\n+            def call(self, x1, x2, x1_mask=None, x2_mask=None):\n+                assert x1_mask is not None\n+                assert x2_mask is not None\n+                return x1 + x2\n+\n+        layer = PositionalInputsMaskedLayer()\n+        layer(x1, x2)\n+        layer(x1=x1, x2=x2)\n+\n+        class PositionalNestedInputsMaskedLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.supports_masking = True\n+\n+            def call(self, x1, x2, x1_mask=None, x2_mask=None):\n+                assert isinstance(x1, tuple)\n+                assert x1_mask is not None\n+                assert x2_mask is not None\n+                assert isinstance(x1_mask, tuple)\n+                return x1[0] + x1[1] + x2\n+\n+        layer = PositionalNestedInputsMaskedLayer()\n+        x1_1 = backend.numpy.ones((4, 4))\n+        x1_1._keras_mask = backend.numpy.ones((4,))\n+        x1_2 = backend.numpy.ones((4, 4))\n+        x1_2._keras_mask = backend.numpy.ones((4,))\n+        x2 = backend.numpy.ones((4, 4))\n+        x2._keras_mask = backend.numpy.ones((4,))\n+        layer((x1_1, x1_2), x2)\n+        layer(x1=(x1_1, x1_2), x2=x2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
