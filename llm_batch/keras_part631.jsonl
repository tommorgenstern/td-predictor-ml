{"custom_id": "keras#40b1bc9169525216bf19cf15724aeca5a9e6df96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 58 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 61 | Churn Cumulative: 13733 | Contributors (this commit): 118 | Commits (past 90d): 12 | Contributors (cumulative): 123 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1519,12 +1519,9 @@ class ModelCheckpoint(Callback):\n             self.epochs_since_last_save = 0\n             filepath = self._get_file_path(epoch, batch, logs)\n \n+            # Create host directory if it doesn't exist.\n             dirname = os.path.dirname(filepath)\n-            if (\n-                dirname\n-                and not dirname.startswith(\"gs://\")\n-                and not tf.io.gfile.exists(dirname)\n-            ):\n+            if dirname and not tf.io.gfile.exists(dirname):\n                 tf.io.gfile.makedirs(dirname)\n \n             try:\n\n@@ -30,8 +30,6 @@ try:\n except ImportError:\n     h5py = None\n \n-is_oss = True\n-\n \n @keras_export(\"keras.saving.save_model\", \"keras.models.save_model\")\n def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n@@ -120,16 +118,6 @@ def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n     \"\"\"\n     save_format = get_save_format(filepath, save_format)\n \n-    # Supports GCS URIs through bigstore via a temporary file\n-    gs_filepath = None\n-    if str(filepath).startswith(\"gs://\"):\n-        gs_filepath = filepath\n-        if not is_oss:\n-            gs_filepath = filepath.replace(\"gs://\", \"/bigstore/\")\n-        filepath = os.path.join(\n-            saving_lib.get_temp_dir(), os.path.basename(gs_filepath)\n-        )\n-\n     # Deprecation warnings\n     if save_format == \"h5\":\n         warnings.warn(\n@@ -158,7 +146,7 @@ def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n         saving_lib.save_model(model, filepath)\n     else:\n         # Legacy case\n-        legacy_sm_saving_lib.save_model(\n+        return legacy_sm_saving_lib.save_model(\n             model,\n             filepath,\n             overwrite=overwrite,\n@@ -166,10 +154,6 @@ def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n             **kwargs,\n         )\n \n-    # Copy from temporary directory to GCS filepath\n-    if gs_filepath is not None:\n-        tf.io.gfile.copy(filepath, gs_filepath, overwrite=overwrite)\n-\n \n @keras_export(\"keras.saving.load_model\", \"keras.models.load_model\")\n def load_model(\n@@ -215,17 +199,6 @@ def load_model(\n     It is recommended that you use layer attributes to\n     access specific variables, e.g. `model.get_layer(\"dense_1\").kernel`.\n     \"\"\"\n-    # Supports GCS URIs by copying data to temporary file\n-    if str(filepath).startswith(\"gs://\"):\n-        gs_filepath = filepath\n-        if not is_oss:\n-            gs_filepath = filepath.replace(\"gs://\", \"/bigstore/\")\n-        filepath = os.path.join(\n-            saving_lib.get_temp_dir(), os.path.basename(gs_filepath)\n-        )\n-        if gs_filepath is not None:\n-            tf.io.gfile.copy(gs_filepath, filepath, overwrite=True)\n-\n     is_keras_zip = str(filepath).endswith(\".keras\") and zipfile.is_zipfile(\n         filepath\n     )\n@@ -268,16 +241,6 @@ def load_model(\n \n \n def save_weights(model, filepath, overwrite=True, **kwargs):\n-    # Supports GCS URIs through bigstore via a temporary file\n-    gs_filepath = None\n-    if str(filepath).startswith(\"gs://\"):\n-        gs_filepath = filepath\n-        if not is_oss:\n-            gs_filepath = filepath.replace(\"gs://\", \"/bigstore/\")\n-        filepath = os.path.join(\n-            saving_lib.get_temp_dir(), os.path.basename(gs_filepath)\n-        )\n-\n     if str(filepath).endswith(\".weights.h5\"):\n         # If file exists and should not be overwritten.\n         try:\n@@ -294,23 +257,8 @@ def save_weights(model, filepath, overwrite=True, **kwargs):\n             model, filepath, overwrite=overwrite, **kwargs\n         )\n \n-    # Copy from temporary directory to GCS filepath\n-    if gs_filepath is not None:\n-        tf.io.gfile.copy(filepath, gs_filepath, overwrite=overwrite)\n-\n \n def load_weights(model, filepath, skip_mismatch=False, **kwargs):\n-    # Supports GCS URIs by copying data to temporary file\n-    if str(filepath).startswith(\"gs://\"):\n-        gs_filepath = filepath\n-        if not is_oss:\n-            gs_filepath = filepath.replace(\"gs://\", \"/bigstore/\")\n-        filepath = os.path.join(\n-            saving_lib.get_temp_dir(), os.path.basename(gs_filepath)\n-        )\n-        if gs_filepath is not None:\n-            tf.io.gfile.copy(gs_filepath, filepath, overwrite=True)\n-\n     if str(filepath).endswith(\".keras\") and zipfile.is_zipfile(filepath):\n         saving_lib.load_weights_only(\n             model, filepath, skip_mismatch=skip_mismatch\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b1470a35e5d5dd7e8a8b11a93cfa8834a07160b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 2828 | Contributors (this commit): 7 | Commits (past 90d): 28 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,8 +15,25 @@ def segment_sum(data, segment_ids, num_segments=None, **kwargs):\n         .type(torch.int64)\n     )\n     num_segments = num_segments or len(torch.unique(segment_ids))\n-    shape = (num_segments,) + tuple(data.shape[1:])\n+\n+    # .scatter_add does not support -1 in the indices.\n+    # Add all out-of-bound indices value to an extra dimension after\n+    # num_segments, which is removed before returning the result.\n+\n+    # Replacing the out-of-bound indices.\n+    segment_ids = torch.where(segment_ids >= 0, segment_ids, num_segments)\n+    segment_ids = torch.where(\n+        segment_ids < num_segments, segment_ids, num_segments\n+    )\n+\n+    # Add one more dimension to the result shape with the \"+1\".\n+    shape = (num_segments + 1,) + tuple(data.shape[1:])\n+\n     result = torch.zeros(*shape).scatter_add(0, segment_ids, data.float())\n+\n+    # Removing the extra dimension.\n+    result = result[:-1, ...]\n+\n     return result.type(data.dtype)\n \n \n\n@@ -58,7 +58,7 @@ def max(x, axis=None, keepdims=False, initial=None):\n         result = result.values\n \n     if initial is not None:\n-        return torch.maximum(result, initial)\n+        return torch.maximum(result, torch.full(result.shape, initial))\n     return result\n \n \n\n@@ -103,6 +103,15 @@ class MathOpsCorrectnessTest(testing.TestCase):\n         )\n         self.assertAllClose(outputs, expected)\n \n+        # Test 1D with -1 case.\n+        data = np.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=np.float32)\n+        segment_ids = np.array([0, 0, 1, 1, -1, 2, 2, -1], dtype=np.int32)\n+        outputs = kmath.segment_sum(data, segment_ids, num_segments=4)\n+        expected = tf.math.unsorted_segment_sum(\n+            data, segment_ids, num_segments=4\n+        )\n+        self.assertAllClose(outputs, expected)\n+\n         # Test N-D case.\n         data = np.random.rand(9, 3, 3)\n         segment_ids = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2], dtype=np.int32)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#20c94d8b8fabfed7d71cb2d084ec11d890ae74ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 23 | Churn Cumulative: 318 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -102,11 +102,23 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     return sample * stddev + mean\n \n \n+def _get_concrete_noise_shape(inputs, noise_shape):\n+    if noise_shape is None:\n+        return inputs.shape\n+\n+    concrete_inputs_shape = inputs.shape\n+    noise_shape = []\n+    for i, value in enumerate(noise_shape):\n+        noise_shape.append(concrete_inputs_shape[i] if value is None else value)\n+    return noise_shape\n+\n+\n def dropout(inputs, rate, noise_shape=None, seed=None):\n     seed = draw_seed(seed)\n     keep_prob = 1.0 - rate\n-    if noise_shape is None:\n-        noise_shape = inputs.shape\n+    # The `noise_shape` may contain `None` so we need to convert it\n+    # into a concrete shape before passing it on to jax.\n+    noise_shape = _get_concrete_noise_shape(inputs, noise_shape)\n     mask = jax.random.bernoulli(seed, p=keep_prob, shape=noise_shape)\n     mask = jax.numpy.broadcast_to(mask, inputs.shape)\n     return jax.lax.select(\n\n@@ -83,3 +83,10 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         keras_core.utils.traceback_utils.disable_traceback_filtering()\n         x = train_step(x)\n         assert isinstance(x, jnp.ndarray)\n+\n+    def test_dropout_noise_shape(self):\n+        inputs = knp.ones((2, 3, 5, 7))\n+        x = random.dropout(\n+            inputs, rate=0.3, noise_shape=[None, 3, 5, None], seed=0\n+        )\n+        self.assertEqual(x.shape, (2, 3, 5, 7))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35082455e52a52879e415a83f7f3a5061052dab5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 446 | Lines Deleted: 27 | Files Changed: 6 | Hunks: 35 | Methods Changed: 30 | Complexity Δ (Sum/Max): 37/13 | Churn Δ: 473 | Churn Cumulative: 1069 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,111 @@\n+\"\"\" Benchmark activation layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.activation_benchmark \\\n+    --benchmark_name=benchmark_elu \\\n+    --num_samples=8192 \\\n+    --batch_size=1024 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_elu(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ELU\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_prelu(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"PReLU\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_softmax(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Softmax\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_elu\": benchmark_elu,\n+    \"benchmark_prelu\": benchmark_prelu,\n+    \"benchmark_softmax\": benchmark_softmax,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -0,0 +1,132 @@\n+\"\"\" Benchmark attention layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.attention_benchmark \\\n+    --benchmark_name=benchmark_attention \\\n+    --num_samples=8192 \\\n+    --batch_size=1024 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_attention(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Attention\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 64], [256, 64]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_multi_head_attention(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"MultiHeadAttention\"\n+    init_args = {\n+        \"num_heads\": 4,\n+        \"key_dim\": 16,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 64], [256, 64], [256, 64]],\n+        flat_call_inputs=True,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_additive_attention(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"AdditiveAttention\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 64], [256, 64], [256, 64]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_attention\": benchmark_attention,\n+    \"benchmark_multi_head_attention\": benchmark_multi_head_attention,\n+    \"benchmark_additive_attention\": benchmark_additive_attention,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -11,7 +11,8 @@ FLAGS = flags.FLAGS\n flags.DEFINE_string(\n     \"benchmark_name\",\n     None,\n-    \"The name of benchmark to run.\",\n+    \"The name of benchmark to run. If None, all benchmarks in the file will be \"\n+    \"run.\",\n )\n \n flags.DEFINE_integer(\n@@ -34,7 +35,7 @@ flags.DEFINE_bool(\n \n \n class BenchmarkMetricsCallback:\n-    def __init__(self, start_batch=2, stop_batch=None):\n+    def __init__(self, start_batch=1, stop_batch=None):\n         self.start_batch = start_batch\n         self.stop_batch = stop_batch\n \n@@ -47,7 +48,7 @@ class BenchmarkMetricsCallback:\n     def on_train_batch_end(self, batch, logs=None):\n         if batch == self.stop_batch:\n             self.state[\"benchmark_end\"] = time.time()\n-            throughput = (self.stop_batch - self.start_batch) / (\n+            throughput = (self.stop_batch - self.start_batch + 1) / (\n                 self.state[\"benchmark_end\"] - self.state[\"benchmark_begin\"]\n             )\n             self.state[\"throughput\"] = throughput\n@@ -59,14 +60,14 @@ class BenchmarkMetricsCallback:\n     def on_predict_batch_end(self, batch, logs=None):\n         if batch == self.stop_batch:\n             self.state[\"benchmark_end\"] = time.time()\n-            throughput = (self.stop_batch - self.start_batch) / (\n+            throughput = (self.stop_batch - self.start_batch + 1) / (\n                 self.state[\"benchmark_end\"] - self.state[\"benchmark_begin\"]\n             )\n             self.state[\"throughput\"] = throughput\n \n \n class KerasCoreBenchmarkMetricsCallback(keras_core.callbacks.Callback):\n-    def __init__(self, start_batch=2, stop_batch=None):\n+    def __init__(self, start_batch=1, stop_batch=None):\n         self._callback = BenchmarkMetricsCallback(start_batch, stop_batch)\n \n     def on_train_batch_begin(self, batch, logs=None):\n@@ -83,7 +84,7 @@ class KerasCoreBenchmarkMetricsCallback(keras_core.callbacks.Callback):\n \n \n class TFKerasBenchmarkMetricsCallback(tf.keras.callbacks.Callback):\n-    def __init__(self, start_batch=2, stop_batch=None):\n+    def __init__(self, start_batch=1, stop_batch=None):\n         self._callback = BenchmarkMetricsCallback(start_batch, stop_batch)\n \n     def on_train_batch_begin(self, batch, logs=None):\n@@ -105,6 +106,7 @@ class LayerBenchmark:\n         layer_name,\n         init_args,\n         input_shape,\n+        flat_call_inputs=True,\n         jit_compile=True,\n     ):\n         self.layer_name = layer_name\n@@ -115,8 +117,12 @@ class LayerBenchmark:\n         self._keras_core_layer = _keras_core_layer_class(**init_args)\n         self._tf_keras_layer = _tf_keras_layer_class(**init_args)\n \n-        self._keras_core_model = keras_core.Sequential([self._keras_core_layer])\n-        self._tf_keras_model = tf.keras.Sequential([self._tf_keras_layer])\n+        self._keras_core_model = self._build_keras_core_model(\n+            input_shape, flat_call_inputs\n+        )\n+        self._tf_keras_model = self._build_tf_keras_model(\n+            input_shape, flat_call_inputs\n+        )\n \n         self._keras_core_model.compile(\n             loss=\"mse\", optimizer=\"sgd\", jit_compile=jit_compile\n@@ -125,12 +131,49 @@ class LayerBenchmark:\n             loss=\"mse\", optimizer=\"sgd\", jit_compile=jit_compile\n         )\n \n+        self.flat_call_inputs = flat_call_inputs\n         self.jit_compile = jit_compile\n         self.input_shape = input_shape\n \n+    def _build_keras_core_model(self, input_shape, flat_call_inputs=True):\n+        inputs = []\n+        if not isinstance(input_shape[0], (tuple, list)):\n+            input_shape = [input_shape]\n+\n+        for shape in input_shape:\n+            inputs.append(keras_core.Input(shape=shape))\n+\n+        if flat_call_inputs:\n+            outputs = self._keras_core_layer(*inputs)\n+        else:\n+            outputs = self._keras_core_layer(inputs)\n+        return keras_core.Model(inputs=inputs, outputs=outputs)\n+\n+    def _build_tf_keras_model(self, input_shape, flat_call_inputs=True):\n+        inputs = []\n+        if not isinstance(input_shape[0], (tuple, list)):\n+            input_shape = [input_shape]\n+\n+        for shape in input_shape:\n+            inputs.append(tf.keras.Input(shape=shape))\n+\n+        if flat_call_inputs:\n+            outputs = self._tf_keras_layer(*inputs)\n+        else:\n+            outputs = self._tf_keras_layer(inputs)\n+        return tf.keras.Model(inputs=inputs, outputs=outputs)\n+\n     def benchmark_predict(self, num_samples, batch_size):\n+        if isinstance(self.input_shape[0], (tuple, list)):\n+            # The layer has multiple inputs.\n+            data = []\n+            for data_shape in self.input_shape:\n+                data_shape = [num_samples] + list(data_shape)\n+                data.append(np.random.normal(size=data_shape))\n+        else:\n             data_shape = [num_samples] + list(self.input_shape)\n             data = np.random.normal(size=data_shape)\n+\n         num_iterations = num_samples // batch_size - 1\n         callback = KerasCoreBenchmarkMetricsCallback(stop_batch=num_iterations)\n         tf_keras_callback = TFKerasBenchmarkMetricsCallback(\n@@ -149,21 +192,35 @@ class LayerBenchmark:\n             callbacks=[tf_keras_callback],\n         )\n \n-        keras_core_throughput = callback._callback.state[\"throughput\"]\n-        tf_keras_throughput = tf_keras_callback._callback.state[\"throughput\"]\n+        keras_core_throughput = (\n+            callback._callback.state[\"throughput\"] * batch_size\n+        )\n+        tf_keras_throughput = (\n+            tf_keras_callback._callback.state[\"throughput\"] * batch_size\n+        )\n         print(\n             f\"Keras Core throughput of forward pass of {self.layer_name}: \"\n-            f\"{keras_core_throughput} samples/sec.\"\n+            f\"{keras_core_throughput:.2f} samples/sec.\"\n         )\n         print(\n             f\"TF Keras throughput of forward pass of {self.layer_name}: \"\n-            f\"{tf_keras_throughput} samples/sec.\"\n+            f\"{tf_keras_throughput:.2f} samples/sec.\"\n         )\n \n     def benchmark_train(self, num_samples, batch_size):\n+        if isinstance(self.input_shape[0], (tuple, list)):\n+            # The layer has multiple inputs.\n+            data = []\n+            for data_shape in self.input_shape:\n+                data_shape = [num_samples] + list(data_shape)\n+                data.append(np.random.normal(size=data_shape))\n+        else:\n             data_shape = [num_samples] + list(self.input_shape)\n-        data = np.random.normal(size=data_shape)\n+            data = [np.random.normal(size=data_shape)]\n+        if self.flat_call_inputs:\n             # Scale by a small factor to avoid zero gradients.\n+            label = np.array(self._keras_core_layer(*data)) * 1.001\n+        else:\n             label = np.array(self._keras_core_layer(data)) * 1.001\n \n         num_iterations = num_samples // batch_size - 1\n@@ -185,13 +242,17 @@ class LayerBenchmark:\n             callbacks=[tf_keras_callback],\n         )\n \n-        keras_core_throughput = callback._callback.state[\"throughput\"]\n-        tf_keras_throughput = tf_keras_callback._callback.state[\"throughput\"]\n+        keras_core_throughput = (\n+            callback._callback.state[\"throughput\"] * batch_size\n+        )\n+        tf_keras_throughput = (\n+            tf_keras_callback._callback.state[\"throughput\"] * batch_size\n+        )\n         print(\n             f\"Keras Core throughput of forward & backward pass of \"\n-            f\"{self.layer_name}: {keras_core_throughput} samples/sec.\"\n+            f\"{self.layer_name}: {keras_core_throughput:.2f} samples/sec.\"\n         )\n         print(\n             f\"TF Keras  throughput of forward & backward pass of \"\n-            f\"{self.layer_name}: {tf_keras_throughput} samples/sec.\"\n+            f\"{self.layer_name}: {tf_keras_throughput:.2f} samples/sec.\"\n         )\n\n@@ -6,7 +6,7 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.conv_benchmark \\\n     --benchmark_name=benchmark_conv2D \\\n-    --num_samples=1000 \\\n+    --num_samples=2000 \\\n     --batch_size=20 \\\n     --jit_compile=True\n ```\n@@ -34,7 +34,7 @@ def benchmark_conv1D(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 4],\n+        input_shape=[256, 16],\n         jit_compile=jit_compile,\n     )\n \n@@ -62,7 +62,7 @@ def benchmark_conv2D(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 32, 4],\n+        input_shape=[128, 128, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -146,7 +146,7 @@ def benchmark_depthwise_conv2D(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 32, 4],\n+        input_shape=[128, 128, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -202,7 +202,7 @@ def benchmark_conv2D_transpose(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 32, 4],\n+        input_shape=[128, 128, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -263,6 +263,11 @@ def main(_):\n     batch_size = FLAGS.batch_size\n     jit_compile = FLAGS.jit_compile\n \n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n     if benchmark_name not in BENCHMARK_NAMES:\n         raise ValueError(\n             f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n\n@@ -0,0 +1,105 @@\n+\"\"\" Benchmark core layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.core_benchmark \\\n+    --benchmark_name=benchmark_dense \\\n+    --num_samples=8192 \\\n+    --batch_size=1024 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_dense(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Dense\"\n+    init_args = {\"units\": 128}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[\n+            256,\n+        ],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_einsum_dense(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"EinsumDense\"\n+    init_args = {\n+        \"equation\": \"abc,cd->abd\",\n+        \"output_shape\": (None, 128),\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 32],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_dense\": benchmark_dense,\n+    \"benchmark_einsum_dense\": benchmark_einsum_dense,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.normalization_benchmark \\\n     --benchmark_name=benchmark_batch_normalization \\\n-    --num_samples=1000 \\\n-    --batch_size=20 \\\n+    --num_samples=2400 \\\n+    --batch_size=300 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -30,7 +30,7 @@ def benchmark_batch_normalization(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 32, 4],\n+        input_shape=[256, 256, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -57,7 +57,7 @@ def benchmark_group_normalization(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 32, 4],\n+        input_shape=[256, 256, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -82,7 +82,7 @@ def benchmark_layer_normalization(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 32, 4],\n+        input_shape=[256, 256, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -110,6 +110,11 @@ def main(_):\n     batch_size = FLAGS.batch_size\n     jit_compile = FLAGS.jit_compile\n \n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n     if benchmark_name not in BENCHMARK_NAMES:\n         raise ValueError(\n             f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44a8cacd51495a1e7ed47b8568774a7c7b22b07d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 375 | Lines Deleted: 18 | Files Changed: 4 | Hunks: 19 | Methods Changed: 24 | Complexity Δ (Sum/Max): 43/24 | Churn Δ: 393 | Churn Cumulative: 40237 | Contributors (this commit): 151 | Commits (past 90d): 28 | Contributors (cumulative): 172 | DMM Complexity: 0.7807017543859649\n\nDIFF:\n@@ -0,0 +1,177 @@\n+# Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Steps per execution autotuning for Keras engine.\"\"\"\n+\n+import logging\n+import threading\n+import time\n+\n+import numpy as np\n+\n+\n+class StepsPerExecutionTuner:\n+    \"\"\"Steps per execution tuner class.\n+\n+    Args:\n+        optimizer: The optimizer used for training/evaluation/prediction. Used\n+            to measure iterations and global throughput\n+            (`optimizer.iterations`/second).\n+        spe_variable: A `tf.Variable` representing the `steps_per_execution`\n+            variable used during training/evaluation/prediction. Must be\n+            updatable with `spe_variable.assign`.\n+        interval: Optional int, the amount of seconds to wait between calls to\n+            measure throughput and tune `spe_variable`. Defaults to 5.\n+        change_spe_interval: Optional int, the number of throughput measurements\n+            before tuning. Defaults to 10.\n+        change_threshold: Optional float, the percent different in throughput to\n+            trigger a `steps_per_execution` change. For example, `0.1` triggers\n+            changes if throughput ()\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        optimizer,\n+        spe_variable,\n+        interval=5,\n+        change_spe_interval=10,\n+        change_threshold=0.1,\n+    ):\n+        self.optimizer = optimizer\n+        self._steps_per_execution = spe_variable\n+        self.interval = interval\n+        self.change_spe_interval = change_spe_interval\n+        self.spe_change_threshold = change_threshold\n+        self.steps_per_execution_stop_event = threading.Event()\n+        self.thread = None\n+\n+    def start(self):\n+        \"\"\"Starts steps per execution tuning thread.\n+\n+        Returns a `threading.Thread` which will run every `self.interval`\n+            seconds to measure throughput and tune steps per execution.\n+        \"\"\"\n+        if self.thread and self.thread.is_alive():\n+            return self.thread\n+        self._begin_tuning()\n+        self.thread = threading.Thread(\n+            target=self._steps_per_execution_interval_call, daemon=True\n+        )  # needed to shut down successfully\n+        self.thread.start()\n+        return self.thread\n+\n+    def _steps_per_execution_interval_call(self):\n+        while not self.steps_per_execution_stop_event.is_set():\n+            self._measure_and_tune()\n+            self.steps_per_execution_stop_event.wait(self.interval)\n+\n+    def _begin_tuning(self):\n+        self.start_time = time.time()\n+        self.init_iterations = self.optimizer.iterations.numpy()\n+        self.init_spe = self._steps_per_execution.numpy().item()\n+        self.spe_last_logged = {\n+            \"iteration\": self.init_iterations,\n+            \"time_secs\": self.start_time,\n+        }\n+        self.rgsps = []  # rgsps = recent global steps per second\n+        self.avg_rgsps = 0\n+        self.prev_avg_rgsps = 0\n+        self.spe_tune_last_action_add = True\n+        self.spe_measurement_count = 0\n+\n+    def stop(self):\n+        \"\"\"Stops steps per execution tuning thread.\"\"\"\n+        if not self.steps_per_execution_stop_event.is_set():\n+            self.steps_per_execution_stop_event.set()\n+\n+    def _should_tune(self):\n+        epoch_boundary = False\n+        if self.rgsps[-1] == 0:\n+            epoch_boundary = True\n+\n+        return (\n+            self.spe_measurement_count % self.change_spe_interval == 0\n+            and self.rgsps\n+            and not epoch_boundary\n+        )\n+\n+    def _tune(self):\n+        \"\"\"Changes the steps per execution using the following algorithm.\n+\n+        If there is more than a 10% increase in the throughput, then the last\n+        recorded action is repeated (i.e. if increasing the SPE caused an\n+        increase in throughput, it is increased again). If there is more than a\n+        10% decrease in the throughput, then the opposite of the last action is\n+        performed (i.e. if increasing the SPE decreased the throughput, then the\n+        SPE is decreased).\n+        \"\"\"\n+        self.avg_rgsps = sum(self.rgsps) / len(self.rgsps)\n+        fast_threshold = (1 + self.spe_change_threshold) * self.prev_avg_rgsps\n+        slow_threshold = (1 - self.spe_change_threshold) * self.prev_avg_rgsps\n+\n+        if self.spe_tune_last_action_add:\n+            repeat_action_mult = 1.5\n+            opposite_action_mult = 0.5\n+        else:\n+            repeat_action_mult = 0.5\n+            opposite_action_mult = 1.5\n+\n+        spe_variable = self.steps_per_execution\n+        spe_limit = spe_variable.dtype.max / 1.5\n+        current_spe = spe_variable.numpy().item()\n+        if self.avg_rgsps > fast_threshold:\n+            # Note that our first iteration will always trigger this as our\n+            # threshold should be 0\n+            new_spe = current_spe * repeat_action_mult\n+        elif self.avg_rgsps < slow_threshold:\n+            new_spe = current_spe * opposite_action_mult\n+            self.spe_tune_last_action_add = not self.spe_tune_last_action_add\n+        else:\n+            new_spe = current_spe\n+\n+        if current_spe >= spe_limit:\n+            new_spe = current_spe\n+        elif current_spe == 0:\n+            new_spe = self.init_spe\n+\n+        self.steps_per_execution.assign(np.round(new_spe))\n+        self.prev_avg_rgsps = self.avg_rgsps\n+\n+    def _measure_and_tune(self):\n+        self.spe_measurement_count += 1\n+\n+        cur_iteration = self.optimizer.iterations.numpy()\n+\n+        cur_time_secs = time.time()\n+        recent_gsps = (cur_iteration - self.spe_last_logged[\"iteration\"]) / (\n+            cur_time_secs - self.spe_last_logged[\"time_secs\"]\n+        )\n+\n+        self.rgsps.append(recent_gsps)\n+        if len(self.rgsps) > self.change_spe_interval:\n+            self.rgsps.pop(0)\n+\n+        if cur_iteration == 0:  # No need to tune, we have no measurements\n+            self.start_time = cur_time_secs\n+            return\n+\n+        self.spe_last_logged[\"iteration\"] = cur_iteration\n+        self.spe_last_logged[\"time_secs\"] = cur_time_secs\n+\n+        try:\n+            if self._should_tune():\n+                self._tune()\n+        except RuntimeError:\n+            logging.exception(\"Steps per execution autotuner failed to run.\")\n+            return\n\n@@ -0,0 +1,60 @@\n+# Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Test steps_per_execution_tuning.\"\"\"\n+\n+import time\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras.engine import steps_per_execution_tuning\n+from keras.testing_infra import test_combinations\n+\n+\n+class mockOptimizer:\n+    def __init__(self, iterations):\n+        self.iterations = tf.Variable(iterations)\n+\n+\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class StepsPerExecutionTuningTest(test_combinations.TestCase):\n+    def test_variables(self):\n+        spe_variable = tf.Variable(1)\n+        tuner = steps_per_execution_tuning.StepsPerExecutionTuner(\n+            mockOptimizer(5), spe_variable, 5, 50, 0.5\n+        )\n+        assert tuner.optimizer.iterations.numpy() == 5\n+        assert tuner._steps_per_execution.numpy().item() == 1\n+        assert tuner.interval == 5\n+        assert tuner.change_spe_interval == 50\n+        assert tuner.spe_change_threshold == 0.5\n+        assert not tuner.steps_per_execution_stop_event.is_set()\n+\n+    def test_start_stop(self):\n+        spe_variable = tf.Variable(1)\n+        tuner = steps_per_execution_tuning.StepsPerExecutionTuner(\n+            mockOptimizer(5), spe_variable, interval=0.2\n+        )\n+        tuner.start()\n+        assert not tuner.steps_per_execution_stop_event.is_set()\n+        assert tuner.start_time > 0\n+        time.sleep(0.5)  # should be enough time for 2 measurements\n+        tuner.stop()\n+        assert tuner.steps_per_execution_stop_event.is_set()\n+        assert tuner.spe_measurement_count > 0\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n@@ -22,6 +22,12 @@ import weakref\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n+from tensorflow.python.distribute import distribute_utils\n+from tensorflow.python.distribute import input_ops\n+from tensorflow.python.eager import context\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util.tf_export import keras_export\n+from tensorflow.tools.docs import doc_controls\n \n from keras import backend\n from keras import callbacks as callbacks_module\n@@ -33,6 +39,7 @@ from keras.engine import base_layer_utils\n from keras.engine import compile_utils\n from keras.engine import data_adapter\n from keras.engine import input_layer as input_layer_module\n+from keras.engine import steps_per_execution_tuning\n from keras.engine import training_utils\n from keras.metrics import base_metric\n from keras.mixed_precision import loss_scale_optimizer as lso\n@@ -54,14 +61,6 @@ from keras.utils import traceback_utils\n from keras.utils import version_utils\n from keras.utils.mode_keys import ModeKeys\n \n-# isort: off\n-from tensorflow.python.eager import context\n-from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.util.tf_export import keras_export\n-from tensorflow.python.distribute import distribute_utils\n-from tensorflow.python.distribute import input_ops\n-from tensorflow.tools.docs import doc_controls\n-\n try:\n     import h5py\n except ImportError:\n@@ -320,6 +319,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self._checkpoint = tf.train.Checkpoint(root=weakref.ref(self))\n \n         self._steps_per_execution = None\n+        self._enable_tune_steps_per_execution = False\n \n         self._layout_map = layout_map_lib.get_current_layout_map()\n \n@@ -706,16 +706,19 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               `run_eagerly=True` is not supported when using\n               `tf.distribute.experimental.ParameterServerStrategy`. Defaults to\n                `False`.\n-            steps_per_execution: Int. The number of batches to\n-              run during each `tf.function` call. Running multiple batches\n-              inside a single `tf.function` call can greatly improve performance\n-              on TPUs or small models with a large Python overhead. At most, one\n-              full epoch will be run each execution. If a number larger than the\n-              size of the epoch is passed, the execution will be truncated to\n-              the size of the epoch. Note that if `steps_per_execution` is set\n-              to `N`, `Callback.on_batch_begin` and `Callback.on_batch_end`\n-              methods will only be called every `N` batches (i.e. before/after\n-              each `tf.function` execution). Defaults to `1`.\n+            steps_per_execution: Int or `'auto'`. The number of batches to\n+              run during each `tf.function` call. If set to \"auto\", keras will\n+              automatically tune `steps_per_execution` during runtime. Running\n+              multiple batches inside a single `tf.function` call can greatly\n+              improve performance on TPUs, when used with distributed strategies\n+              such as `ParameterServerStrategy`, or with small models with a\n+              large Python overhead. At most, one full epoch will be run each\n+              execution. If a number larger than the size of the epoch is\n+              passed, the execution will be truncated to the size of the epoch.\n+              Note that if `steps_per_execution` is set to `N`,\n+              `Callback.on_batch_begin` and `Callback.on_batch_end` methods will\n+              only be called every `N` batches (i.e. before/after each\n+              `tf.function` execution). Defaults to `1`.\n             jit_compile: If `True`, compile the model training step with XLA.\n               [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n               for machine learning.\n@@ -799,6 +802,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 mesh=mesh,\n             )\n \n+            if steps_per_execution == \"auto\":\n+                self._configure_steps_per_execution(1)\n+                self._steps_per_execution_tuner = (\n+                    steps_per_execution_tuning.StepsPerExecutionTuner(\n+                        self.optimizer, self._steps_per_execution\n+                    )\n+                )\n+            else:\n                 self._configure_steps_per_execution(steps_per_execution or 1)\n \n             self._pss_evaluation_shards = self._infer_exact_eval_shards(\n@@ -994,6 +1005,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     def run_eagerly(self, value):\n         self._run_eagerly = value\n \n+    @property\n+    def enable_tune_steps_per_execution(self):\n+        return self._enable_tune_steps_per_execution\n+\n+    @enable_tune_steps_per_execution.setter\n+    def enable_tune_steps_per_execution(self, value):\n+        self._enable_tune_steps_per_execution = value\n+\n     @property\n     def jit_compile(self):\n         \"\"\"Specify whether to compile the model with XLA.\n@@ -1357,6 +1376,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n+            and not self.enable_tune_steps_per_execution\n         ):\n \n             def train_function(iterator):\n@@ -1739,6 +1759,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             self._train_counter.assign(0)\n             callbacks.on_train_begin()\n             training_logs = None\n+            if self.enable_tune_steps_per_execution:\n+                self._steps_per_execution_tuner.start()\n             # Handle fault-tolerance for multi-worker.\n             # TODO(omalleyt): Fix the ordering issues that mean this has to\n             # happen after `callbacks.on_train_begin`.\n@@ -1845,6 +1867,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             # If eval data_handler exists, delete it after all epochs are done.\n             if getattr(self, \"_eval_data_handler\", None) is not None:\n                 del self._eval_data_handler\n+            if self.enable_tune_steps_per_execution:\n+                self._steps_per_execution_tuner.stop()\n             callbacks.on_train_end(logs=training_logs)\n             return self.history\n \n@@ -2017,6 +2041,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n+            and not self.enable_tune_steps_per_execution\n         ):\n \n             def test_function(iterator):\n@@ -2238,6 +2263,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             test_function_runner = self._get_test_function_runner(callbacks)\n             self._test_counter.assign(0)\n             callbacks.on_test_begin()\n+            if self.enable_tune_steps_per_execution:\n+                self._steps_per_execution_tuner.start()\n             for (\n                 _,\n                 dataset_or_iterator,\n@@ -2262,6 +2289,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 logs = self._aggregate_exact_metrics(logs)\n             else:\n                 logs = self._validate_and_get_metrics_result(logs)\n+            if self.enable_tune_steps_per_execution:\n+                self._steps_per_execution_tuner.stop()\n             callbacks.on_test_end(logs=logs)\n \n             if return_dict:\n@@ -2386,6 +2415,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n+            and not self.enable_tune_steps_per_execution\n         ):\n \n             def predict_function(iterator):\n@@ -2598,6 +2628,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             self.predict_function = self.make_predict_function()\n             self._predict_counter.assign(0)\n             callbacks.on_predict_begin()\n+            if self.enable_tune_steps_per_execution:\n+                self._steps_per_execution_tuner.start()\n             batch_outputs = None\n             for _, iterator in data_handler.enumerate_epochs():  # Single epoch.\n                 with data_handler.catch_stop_iteration():\n@@ -2636,6 +2668,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     \"information of where went wrong, or file a \"\n                     \"issue/bug to `tf.keras`.\"\n                 )\n+            if self.enable_tune_steps_per_execution:\n+                self._steps_per_execution_tuner.stop()\n             callbacks.on_predict_end()\n         all_outputs = tf.__internal__.nest.map_structure_up_to(\n             batch_outputs, potentially_ragged_concat, outputs\n\n@@ -2391,6 +2391,92 @@ class TrainingTest(test_combinations.TestCase):\n         self.assertEqual(input_specs[2].shape.as_list(), [3, 3])\n \n \n+class TestAutotuneSPE(test_combinations.TestCase):\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_compile_fit_with_jit_compile(self):\n+        # Test with jit_compile = True\n+        model = sequential.Sequential([layers_module.Dense(1)])\n+        model.compile(\n+            \"sgd\",\n+            loss=\"mse\",\n+            run_eagerly=False,\n+            jit_compile=True,\n+            steps_per_execution=\"auto\",\n+        )\n+        x, y = np.ones((10, 1)), np.ones((10, 1))\n+        model.fit(x, y, epochs=2)\n+        # Test fcompile fit for a RNN model\n+        model = sequential.Sequential()\n+        model.add(\n+            layers_module.TimeDistributed(\n+                layers_module.Embedding(5, 6, mask_zero=True),\n+                input_shape=(None, None),\n+            )\n+        )  # N by t_1 by t_2 by 6\n+        model.add(\n+            layers_module.TimeDistributed(\n+                layers_module.SimpleRNN(7, return_sequences=True)\n+            )\n+        )\n+        model.add(\n+            layers_module.TimeDistributed(\n+                layers_module.SimpleRNN(8, return_sequences=False)\n+            )\n+        )\n+        model.add(layers_module.SimpleRNN(1, return_sequences=False))\n+        model.compile(\n+            optimizer=\"sgd\",\n+            loss=\"mse\",\n+            jit_compile=True,\n+            steps_per_execution=\"auto\",\n+        )\n+        model_input = np.random.randint(\n+            low=1, high=5, size=(10, 3, 4), dtype=\"int32\"\n+        )\n+        for i in range(4):\n+            model_input[i, i:, i:] = 0\n+        model.fit(\n+            model_input, np.random.random((10, 1)), epochs=1, batch_size=10\n+        )\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_compile_fit_evaluate_predict_with_mirrored_strategy(self):\n+        # Test with jit_compile = True\n+        strategy = tf.distribute.MirroredStrategy()\n+        with strategy.scope():\n+            model = sequential.Sequential([layers_module.Dense(1)])\n+        model.compile(\n+            \"sgd\",\n+            loss=\"mse\",\n+            run_eagerly=False,\n+            jit_compile=True,\n+            steps_per_execution=\"auto\",\n+        )\n+        x, y = np.ones((10, 1)), np.ones((10, 1))\n+        model.fit(x, y, epochs=2)\n+        model.evaluate(x, y)\n+        model.predict(x)\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_spe_tune_compile_fit_then_false_predict(self):\n+        strategy = tf.distribute.MirroredStrategy()\n+        with strategy.scope():\n+            model = sequential.Sequential([layers_module.Dense(1)])\n+        model.compile(\n+            \"sgd\",\n+            loss=\"mse\",\n+            run_eagerly=False,\n+            jit_compile=True,\n+            steps_per_execution=\"auto\",\n+        )\n+        x, y = np.ones((10, 1)), np.ones((10, 1))\n+        model.fit(x, y, epochs=2)\n+        model.evaluate(x, y)\n+        model.enable_tune_steps_per_execution = False\n+        model.predict(x)\n+        assert model.enable_tune_steps_per_execution == False\n+\n+\n class TestExceptionsAndWarnings(test_combinations.TestCase):\n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n     @test_combinations.run_with_all_model_types\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
