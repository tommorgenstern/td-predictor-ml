{"custom_id": "scrapy#3f156ad845e49c7e6684f10cee6ce555364eaa13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 26 | Files Changed: 3 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 61 | Churn Cumulative: 777 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,13 +1,14 @@\n import os\n from twisted.trial import unittest\n \n-from scrapy.utils.iterators import csviter, xmliter\n+from scrapy.utils.iterators import csviter, xmliter, _body_or_str\n from scrapy.contrib_exp.iterators import xmliter_lxml\n from scrapy.http import XmlResponse, TextResponse, Response\n from scrapy.tests import get_testdata\n \n FOOBAR_NL = u\"foo\" + os.linesep + u\"bar\"\n \n+\n class XmliterTestCase(unittest.TestCase):\n \n     xmliter = staticmethod(xmliter)\n@@ -173,7 +174,6 @@ class UtilsCsvTestCase(unittest.TestCase):\n                           {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n-\n     def test_csviter_headers(self):\n         sample = get_testdata('feeds', 'feed-sample3.csv').splitlines()\n         headers, body = sample[0].split(','), '\\n'.join(sample[1:])\n@@ -229,5 +229,29 @@ class UtilsCsvTestCase(unittest.TestCase):\n              {u'id': u'2', u'name': u'something', u'value': u'\\u255a\\u2569\\u2569\\u2569\\u2550\\u2550\\u2557'}])\n \n \n+class TestHelper(unittest.TestCase):\n+    bbody = b'utf8-body'\n+    ubody = bbody.decode('utf8')\n+    txtresponse = TextResponse(url='http://example.org/', body=bbody, encoding='utf-8')\n+    response = Response(url='http://example.org/', body=bbody)\n+\n+    def test_body_or_str(self):\n+        for obj in (self.bbody, self.ubody, self.txtresponse, self.response):\n+            r1 = _body_or_str(obj)\n+            self._assert_type_and_value(r1, self.ubody, obj)\n+            r2 = _body_or_str(obj, unicode=True)\n+            self._assert_type_and_value(r2, self.ubody, obj)\n+            r3 = _body_or_str(obj, unicode=False)\n+            self._assert_type_and_value(r3, self.bbody, obj)\n+            self.assertTrue(type(r1) is type(r2))\n+            self.assertTrue(type(r1) is not type(r3))\n+\n+\n+    def _assert_type_and_value(self, a, b, obj):\n+        self.assertTrue(type(a) is type(b),\n+                        'Got {}, expected {} for {!r}'.format(type(a), type(b), obj))\n+        self.assertEqual(a, b)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -3,33 +3,13 @@ import unittest\n import urlparse\n \n from scrapy.http import Response, TextResponse, HtmlResponse\n-from scrapy.utils.response import body_or_str, response_httprepr, open_in_browser, \\\n-    get_meta_refresh\n+from scrapy.utils.response import response_httprepr, open_in_browser, get_meta_refresh\n \n __doctests__ = ['scrapy.utils.response']\n \n class ResponseUtilsTest(unittest.TestCase):\n     dummy_response = TextResponse(url='http://example.org/', body='dummy_response')\n \n-    def test_body_or_str_input(self):\n-        self.assertTrue(isinstance(body_or_str(self.dummy_response), basestring))\n-        self.assertTrue(isinstance(body_or_str('text'), basestring))\n-        self.assertRaises(Exception, body_or_str, 2)\n-\n-    def test_body_or_str_extraction(self):\n-        self.assertEqual(body_or_str(self.dummy_response), 'dummy_response')\n-        self.assertEqual(body_or_str('text'), 'text')\n-\n-    def test_body_or_str_encoding(self):\n-        self.assertTrue(isinstance(body_or_str(self.dummy_response, unicode=False), str))\n-        self.assertTrue(isinstance(body_or_str(self.dummy_response, unicode=True), unicode))\n-\n-        self.assertTrue(isinstance(body_or_str('text', unicode=False), str))\n-        self.assertTrue(isinstance(body_or_str('text', unicode=True), unicode))\n-\n-        self.assertTrue(isinstance(body_or_str(u'text', unicode=False), str))\n-        self.assertTrue(isinstance(body_or_str(u'text', unicode=True), unicode))\n-\n     def test_response_httprepr(self):\n         r1 = Response(\"http://www.example.com\")\n         self.assertEqual(response_httprepr(r1), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n\n@@ -71,8 +71,13 @@ def _body_or_str(obj, unicode=True):\n     assert isinstance(obj, (Response, basestring)), \\\n         \"obj must be Response or basestring, not %s\" % type(obj).__name__\n     if isinstance(obj, Response):\n-        return obj.body_as_unicode() if unicode else obj.body\n-    elif isinstance(obj, str):\n-        return obj.decode('utf-8') if unicode else obj\n+        if not unicode:\n+            return obj.body\n+        elif isinstance(obj, TextResponse):\n+            return obj.body_as_unicode()\n         else:\n+            return obj.body.decode('utf-8')\n+    elif type(obj) is type(u''):\n         return obj if unicode else obj.encode('utf-8')\n+    else:\n+        return obj.decode('utf-8') if unicode else obj\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#526a944eda9f70447343112824e96d2c03f0583d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 255 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -94,10 +94,6 @@ class XmliterTestCase(unittest.TestCase):\n \n class LxmlXmliterTestCase(XmliterTestCase):\n     xmliter = staticmethod(xmliter_lxml)\n-    try:\n-        import lxml\n-    except ImportError:\n-        skip = \"lxml not available\"\n \n     def test_xmliter_iterate_namespace(self):\n         body = \"\"\"\\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#55bee912a2adf6273a5aa4e9e158be452d9f49f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 91 | Churn Cumulative: 232 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 0.7761194029850746\n\nDIFF:\n@@ -16,6 +16,14 @@ class HttpError(IgnoreRequest):\n \n class HttpErrorMiddleware(object):\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler.settings)\n+\n+    def __init__(self, settings):\n+        self.handle_httpstatus_all = settings.getbool('HANDLE_HTTPSTATUS_ALL')\n+        self.handle_httpstatus_list = settings.getlist('HANDLE_HTTPSTATUS_LIST')\n+\n     def process_spider_input(self, response, spider):\n         if 200 <= response.status < 300: # common case\n             return\n@@ -24,8 +32,10 @@ class HttpErrorMiddleware(object):\n             return\n         if 'handle_httpstatus_list' in meta:\n             allowed_statuses = meta['handle_httpstatus_list']\n+        elif self.handle_httpstatus_all:\n+            return\n         else:\n-            allowed_statuses = getattr(spider, 'handle_httpstatus_list', ())\n+            allowed_statuses = getattr(spider, 'handle_httpstatus_list', self.handle_httpstatus_list)\n         if response.status in allowed_statuses:\n             return\n         raise HttpError(response, 'Ignoring non-200 response')\n\n@@ -3,13 +3,14 @@ from unittest import TestCase\n from scrapy.http import Response, Request\n from scrapy.spider import BaseSpider\n from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpError\n+from scrapy.settings import Settings\n \n \n class TestHttpErrorMiddleware(TestCase):\n \n     def setUp(self):\n         self.spider = BaseSpider('foo')\n-        self.mw = HttpErrorMiddleware()\n+        self.mw = HttpErrorMiddleware(Settings({}))\n         self.req = Request('http://scrapytest.org')\n \n         self.res200 = Response('http://scrapytest.org', status=200)\n@@ -42,3 +43,79 @@ class TestHttpErrorMiddleware(TestCase):\n         self.assertEquals(None,\n             self.mw.process_spider_input(self.res404, self.spider))\n \n+class TestHttpErrorMiddlewareSettings(TestCase):\n+    \"\"\"Similar test, but with settings\"\"\"\n+\n+    def setUp(self):\n+        self.spider = BaseSpider('foo')\n+        self.mw = HttpErrorMiddleware(Settings({'HANDLE_HTTPSTATUS_LIST': (402,)}))\n+        self.req = Request('http://scrapytest.org')\n+\n+        self.res200 = Response('http://scrapytest.org', status=200)\n+        self.res200.request = self.req\n+        self.res404 = Response('http://scrapytest.org', status=404)\n+        self.res404.request = self.req\n+        self.res402 = Response('http://scrapytest.org', status=402)\n+        self.res402.request = self.req\n+\n+    def test_process_spider_input(self):\n+        self.assertEquals(None,\n+                self.mw.process_spider_input(self.res200, self.spider))\n+        self.assertRaises(HttpError,\n+                self.mw.process_spider_input, self.res404, self.spider)\n+        self.assertEquals(None,\n+                self.mw.process_spider_input(self.res402, self.spider))\n+\n+    def test_meta_overrides_settings(self):\n+        request = Request('http://scrapytest.org',\n+                              meta={'handle_httpstatus_list': [404]})\n+        res404 = self.res404.copy()\n+        res404.request = request\n+        res402 = self.res402.copy()\n+        res402.request = request\n+\n+        self.assertEquals(None,\n+            self.mw.process_spider_input(res404, self.spider))\n+        self.assertRaises(HttpError,\n+                self.mw.process_spider_input, res402, self.spider)\n+\n+    def test_spider_override_settings(self):\n+        self.spider.handle_httpstatus_list = [404]\n+        self.assertEquals(None,\n+            self.mw.process_spider_input(self.res404, self.spider))\n+        self.assertRaises(HttpError,\n+                self.mw.process_spider_input, self.res402, self.spider)\n+\n+class TestHttpErrorMiddlewareHandleAll(TestCase):\n+\n+    def setUp(self):\n+        self.spider = BaseSpider('foo')\n+        self.mw = HttpErrorMiddleware(Settings({'HANDLE_HTTPSTATUS_ALL': True}))\n+        self.req = Request('http://scrapytest.org')\n+\n+        self.res200 = Response('http://scrapytest.org', status=200)\n+        self.res200.request = self.req\n+        self.res404 = Response('http://scrapytest.org', status=404)\n+        self.res404.request = self.req\n+        self.res402 = Response('http://scrapytest.org', status=402)\n+        self.res402.request = self.req\n+\n+    def test_process_spider_input(self):\n+        self.assertEquals(None,\n+                self.mw.process_spider_input(self.res200, self.spider))\n+        self.assertEquals(None,\n+                self.mw.process_spider_input(self.res404, self.spider))\n+\n+    def test_meta_overrides_settings(self):\n+        request = Request('http://scrapytest.org',\n+                              meta={'handle_httpstatus_list': [404]})\n+        res404 = self.res404.copy()\n+        res404.request = request\n+        res402 = self.res402.copy()\n+        res402.request = request\n+\n+        self.assertEquals(None,\n+            self.mw.process_spider_input(res404, self.spider))\n+        self.assertRaises(HttpError,\n+                self.mw.process_spider_input, res402, self.spider)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f87be371a224b163ddb119cff31e17f4dd628b36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 240 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -21,8 +21,8 @@ class HttpErrorMiddleware(object):\n         return cls(crawler.settings)\n \n     def __init__(self, settings):\n-        self.handle_httpstatus_all = settings.getbool('HANDLE_HTTPSTATUS_ALL')\n-        self.handle_httpstatus_list = settings.getlist('HANDLE_HTTPSTATUS_LIST')\n+        self.handle_httpstatus_all = settings.getbool('HTTPERROR_ALLOW_ALL')\n+        self.handle_httpstatus_list = settings.getlist('HTTPERROR_ALLOWED_CODES')\n \n     def process_spider_input(self, response, spider):\n         if 200 <= response.status < 300: # common case\n\n@@ -48,7 +48,7 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n \n     def setUp(self):\n         self.spider = BaseSpider('foo')\n-        self.mw = HttpErrorMiddleware(Settings({'HANDLE_HTTPSTATUS_LIST': (402,)}))\n+        self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOWED_CODES': (402,)}))\n         self.req = Request('http://scrapytest.org')\n \n         self.res200 = Response('http://scrapytest.org', status=200)\n@@ -90,7 +90,7 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n \n     def setUp(self):\n         self.spider = BaseSpider('foo')\n-        self.mw = HttpErrorMiddleware(Settings({'HANDLE_HTTPSTATUS_ALL': True}))\n+        self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOW_ALL': True}))\n         self.req = Request('http://scrapytest.org')\n \n         self.res200 = Response('http://scrapytest.org', status=200)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#14f5817d6bef91366951b28fd59262ace8a1da93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 159 | Lines Deleted: 33 | Files Changed: 2 | Hunks: 28 | Methods Changed: 29 | Complexity Δ (Sum/Max): 15/8 | Churn Δ: 192 | Churn Cumulative: 983 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,16 +11,23 @@ from scrapy.item import Item\n from scrapy.selector import Selector\n from scrapy.utils.misc import arg_to_iter, extract_regex\n from scrapy.utils.python import flatten\n+from scrapy.utils.decorator import deprecated\n from .common import wrap_loader_context\n from .processor import Identity\n \n+\n class ItemLoader(object):\n \n     default_item_class = Item\n     default_input_processor = Identity()\n     default_output_processor = Identity()\n+    default_selector_class = Selector\n \n-    def __init__(self, item=None, **context):\n+    def __init__(self, item=None, selector=None, response=None, **context):\n+        if selector is None and response is not None:\n+            selector = self.default_selector_class(response)\n+        self.selector = selector\n+        context.update(selector=selector, response=response)\n         if item is None:\n             item = self.default_item_class()\n         self.item = context['item'] = item\n@@ -114,32 +121,56 @@ class ItemLoader(object):\n             value = default\n         return value\n \n-class XPathItemLoader(ItemLoader):\n-\n-    default_selector_class = Selector\n-\n-    def __init__(self, item=None, selector=None, response=None, **context):\n-        if selector is None and response is None:\n-            raise RuntimeError(\"%s must be instantiated with a selector \" \\\n-                \"or response\" % self.__class__.__name__)\n-        if selector is None:\n-            selector = self.default_selector_class(response)\n-        self.selector = selector\n-        context.update(selector=selector, response=response)\n-        super(XPathItemLoader, self).__init__(item, **context)\n+    def _check_selector_method(self):\n+        if self.selector is None:\n+            raise RuntimeError(\"To use XPath or CSS selectors, \"\n+                \"%s must be instantiated with a selector \"\n+                \"or a response\" % self.__class__.__name__)\n \n     def add_xpath(self, field_name, xpath, *processors, **kw):\n-        values = self._get_values(xpath, **kw)\n+        values = self._get_xpathvalues(xpath, **kw)\n         self.add_value(field_name, values, *processors, **kw)\n \n     def replace_xpath(self, field_name, xpath, *processors, **kw):\n-        values = self._get_values(xpath, **kw)\n+        values = self._get_xpathvalues(xpath, **kw)\n         self.replace_value(field_name, values, *processors, **kw)\n \n     def get_xpath(self, xpath, *processors, **kw):\n-        values = self._get_values(xpath, **kw)\n+        values = self._get_xpathvalues(xpath, **kw)\n         return self.get_value(values, *processors, **kw)\n \n+    @deprecated(use_instead='._get_xpathvalues()')\n     def _get_values(self, xpaths, **kw):\n+        return self._get_xpathvalues(xpaths, **kw)\n+\n+    def _get_xpathvalues(self, xpaths, **kw):\n+        self._check_selector_method()\n         xpaths = arg_to_iter(xpaths)\n         return flatten([self.selector.xpath(xpath).extract() for xpath in xpaths])\n+\n+    def add_css(self, field_name, css, *processors, **kw):\n+        values = self._get_cssvalues(css, **kw)\n+        self.add_value(field_name, values, *processors, **kw)\n+\n+    def replace_css(self, field_name, css, *processors, **kw):\n+        values = self._get_cssvalues(css, **kw)\n+        self.replace_value(field_name, values, *processors, **kw)\n+\n+    def get_css(self, css, *processors, **kw):\n+        values = self._get_cssvalues(css, **kw)\n+        return self.get_value(values, *processors, **kw)\n+\n+    def _get_cssvalues(self, csss, **kw):\n+        self._check_selector_method()\n+        csss = arg_to_iter(csss)\n+        return flatten([self.selector.css(css).extract() for css in csss])\n+\n+\n+class XPathItemLoader(ItemLoader):\n+    def __init__(self, *a, **kw):\n+        import warnings\n+        from scrapy.exceptions import ScrapyDeprecationWarning\n+        warnings.warn('%s is deprecated, instanciate scrapy.contrib.loader.ItemLoader '\n+                      'instead' % type(self).__name__,\n+                      category=ScrapyDeprecationWarning, stacklevel=1)\n+        super(XPathItemLoader, self).__init__(*a, **kw)\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.contrib.loader import ItemLoader, XPathItemLoader\n+from scrapy.contrib.loader import ItemLoader\n from scrapy.contrib.loader.processor import Join, Identity, TakeFirst, \\\n     Compose, MapCompose\n from scrapy.item import Item, Field\n@@ -38,7 +38,7 @@ def processor_with_args(value, other=None, loader_context=None):\n     return value\n \n \n-class ItemLoaderTest(unittest.TestCase):\n+class BasicItemLoaderTest(unittest.TestCase):\n \n     def test_load_item_using_default_loader(self):\n         i = TestItem()\n@@ -367,37 +367,78 @@ class ProcessorsTest(unittest.TestCase):\n                          [u'HELLO', u'THIS', u'IS', u'SCRAPY'])\n \n \n-class TestXPathItemLoader(XPathItemLoader):\n-    default_item_class = TestItem\n-    name_in = MapCompose(lambda v: v.title())\n+class SelectortemLoaderTest(unittest.TestCase):\n+    response = HtmlResponse(url=\"\", body=\"\"\"\n+    <html>\n+    <body>\n+    <div id=\"id\">marta</div>\n+    <p>paragraph</p>\n+    <a href=\"http://www.scrapy.org\">homepage</a>\n+    <img src=\"/images/logo.png\" width=\"244\" height=\"65\" alt=\"Scrapy\">\n+    </body>\n+    </html>\n+    \"\"\")\n \n-\n-class XPathItemLoaderTest(unittest.TestCase):\n-    response = HtmlResponse(url=\"\", body='<html><body><div id=\"id\">marta</div><p>paragraph</p></body></html>')\n+    def test_constructor(self):\n+        l = TestItemLoader()\n+        self.assertEqual(l.selector, None)\n \n     def test_constructor_errors(self):\n-        self.assertRaises(RuntimeError, XPathItemLoader)\n+        l = TestItemLoader()\n+        self.assertRaises(RuntimeError, l.add_xpath, 'url', '//a/@href')\n+        self.assertRaises(RuntimeError, l.replace_xpath, 'url', '//a/@href')\n+        self.assertRaises(RuntimeError, l.get_xpath, '//a/@href')\n+        self.assertRaises(RuntimeError, l.add_css, 'name', '#name::text')\n+        self.assertRaises(RuntimeError, l.replace_css, 'name', '#name::text')\n+        self.assertRaises(RuntimeError, l.get_css, '#name::text')\n \n     def test_constructor_with_selector(self):\n         sel = Selector(text=u\"<html><body><div>marta</div></body></html>\")\n-        l = TestXPathItemLoader(selector=sel)\n+        l = TestItemLoader(selector=sel)\n         self.assert_(l.selector is sel)\n+\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n+    def test_constructor_with_selector_css(self):\n+        sel = Selector(text=u\"<html><body><div>marta</div></body></html>\")\n+        l = TestItemLoader(selector=sel)\n+        self.assert_(l.selector is sel)\n+\n+        l.add_css('name', 'div::text')\n+        self.assertEqual(l.get_output_value('name'), [u'Marta'])\n+\n     def test_constructor_with_response(self):\n-        l = TestXPathItemLoader(response=self.response)\n+        l = TestItemLoader(response=self.response)\n         self.assert_(l.selector)\n+\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n+    def test_constructor_with_response_css(self):\n+        l = TestItemLoader(response=self.response)\n+        self.assert_(l.selector)\n+\n+        l.add_css('name', 'div::text')\n+        self.assertEqual(l.get_output_value('name'), [u'Marta'])\n+\n+        l.add_css('url', 'a::attr(href)')\n+        self.assertEqual(l.get_output_value('url'), [u'http://www.scrapy.org'])\n+\n+        # combining/accumulating CSS selectors and XPath expressions\n+        l.add_xpath('name', '//div/text()')\n+        self.assertEqual(l.get_output_value('name'), [u'Marta', u'Marta'])\n+\n+        l.add_xpath('url', '//img/@src')\n+        self.assertEqual(l.get_output_value('url'), [u'http://www.scrapy.org', u'/images/logo.png'])\n+\n     def test_add_xpath_re(self):\n-        l = TestXPathItemLoader(response=self.response)\n+        l = TestItemLoader(response=self.response)\n         l.add_xpath('name', '//div/text()', re='ma')\n         self.assertEqual(l.get_output_value('name'), [u'Ma'])\n \n     def test_replace_xpath(self):\n-        l = TestXPathItemLoader(response=self.response)\n+        l = TestItemLoader(response=self.response)\n         self.assert_(l.selector)\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n@@ -408,7 +449,7 @@ class XPathItemLoaderTest(unittest.TestCase):\n         self.assertEqual(l.get_output_value('name'), [u'Paragraph', 'Marta'])\n \n     def test_get_xpath(self):\n-        l = TestXPathItemLoader(response=self.response)\n+        l = TestItemLoader(response=self.response)\n         self.assertEqual(l.get_xpath('//p/text()'), [u'paragraph'])\n         self.assertEqual(l.get_xpath('//p/text()', TakeFirst()), u'paragraph')\n         self.assertEqual(l.get_xpath('//p/text()', TakeFirst(), re='pa'), u'pa')\n@@ -416,20 +457,74 @@ class XPathItemLoaderTest(unittest.TestCase):\n         self.assertEqual(l.get_xpath(['//p/text()', '//div/text()']), [u'paragraph', 'marta'])\n \n     def test_replace_xpath_multi_fields(self):\n-        l = TestXPathItemLoader(response=self.response)\n+        l = TestItemLoader(response=self.response)\n         l.add_xpath(None, '//div/text()', TakeFirst(), lambda x: {'name': x})\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n         l.replace_xpath(None, '//p/text()', TakeFirst(), lambda x: {'name': x})\n         self.assertEqual(l.get_output_value('name'), [u'Paragraph'])\n \n     def test_replace_xpath_re(self):\n-        l = TestXPathItemLoader(response=self.response)\n+        l = TestItemLoader(response=self.response)\n         self.assert_(l.selector)\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n         l.replace_xpath('name', '//div/text()', re='ma')\n         self.assertEqual(l.get_output_value('name'), [u'Ma'])\n \n+    def test_add_css_re(self):\n+        l = TestItemLoader(response=self.response)\n+        l.add_css('name', 'div::text', re='ma')\n+        self.assertEqual(l.get_output_value('name'), [u'Ma'])\n+\n+        l.add_css('url', 'a::attr(href)', re='http://(.+)')\n+        self.assertEqual(l.get_output_value('url'), [u'www.scrapy.org'])\n+\n+    def test_replace_css(self):\n+        l = TestItemLoader(response=self.response)\n+        self.assert_(l.selector)\n+        l.add_css('name', 'div::text')\n+        self.assertEqual(l.get_output_value('name'), [u'Marta'])\n+        l.replace_css('name', 'p::text')\n+        self.assertEqual(l.get_output_value('name'), [u'Paragraph'])\n+\n+        l.replace_css('name', ['p::text', 'div::text'])\n+        self.assertEqual(l.get_output_value('name'), [u'Paragraph', 'Marta'])\n+\n+        l.add_css('url', 'a::attr(href)', re='http://(.+)')\n+        self.assertEqual(l.get_output_value('url'), [u'www.scrapy.org'])\n+        l.replace_css('url', 'img::attr(src)')\n+        self.assertEqual(l.get_output_value('url'), [u'/images/logo.png'])\n+\n+    def test_get_css(self):\n+        l = TestItemLoader(response=self.response)\n+        self.assertEqual(l.get_css('p::text'), [u'paragraph'])\n+        self.assertEqual(l.get_css('p::text', TakeFirst()), u'paragraph')\n+        self.assertEqual(l.get_css('p::text', TakeFirst(), re='pa'), u'pa')\n+\n+        self.assertEqual(l.get_css(['p::text', 'div::text']), [u'paragraph', 'marta'])\n+        self.assertEqual(l.get_css(['a::attr(href)', 'img::attr(src)']),\n+            [u'http://www.scrapy.org', u'/images/logo.png'])\n+\n+    def test_replace_css_multi_fields(self):\n+        l = TestItemLoader(response=self.response)\n+        l.add_css(None, 'div::text', TakeFirst(), lambda x: {'name': x})\n+        self.assertEqual(l.get_output_value('name'), [u'Marta'])\n+        l.replace_css(None, 'p::text', TakeFirst(), lambda x: {'name': x})\n+        self.assertEqual(l.get_output_value('name'), [u'Paragraph'])\n+\n+        l.add_css(None, 'a::attr(href)', TakeFirst(), lambda x: {'url': x})\n+        self.assertEqual(l.get_output_value('url'), [u'http://www.scrapy.org'])\n+        l.replace_css(None, 'img::attr(src)', TakeFirst(), lambda x: {'url': x})\n+        self.assertEqual(l.get_output_value('url'), [u'/images/logo.png'])\n+\n+    def test_replace_css_re(self):\n+        l = TestItemLoader(response=self.response)\n+        self.assert_(l.selector)\n+        l.add_css('url', 'a::attr(href)')\n+        self.assertEqual(l.get_output_value('url'), [u'http://www.scrapy.org'])\n+        l.replace_css('url', 'a::attr(href)', re='http://www\\.(.+)')\n+        self.assertEqual(l.get_output_value('url'), [u'scrapy.org'])\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d99babe401d24b4b824977e81f7444fe470032c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 100 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ class HttpCompressionMiddleware(object):\n         return cls()\n     \n     def process_request(self, request, spider):\n-        request.headers.setdefault('Accept-Encoding', 'x-gzip,gzip,deflate')\n+        request.headers.setdefault('Accept-Encoding', 'gzip,deflate')\n \n     def process_response(self, request, response, spider):\n         if isinstance(response, Response):\n\n@@ -50,7 +50,7 @@ class HttpCompressionTest(TestCase):\n         request = Request('http://scrapytest.org')\n         assert 'Accept-Encoding' not in request.headers\n         self.mw.process_request(request, self.spider)\n-        self.assertEqual(request.headers.get('Accept-Encoding'), 'x-gzip,gzip,deflate')\n+        self.assertEqual(request.headers.get('Accept-Encoding'), 'gzip,deflate')\n \n     def test_process_response_gzip(self):\n         response = self._getresponse('gzip')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#13002564b9ec5e13fe1237040af106a0cfc39b38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 232 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,8 +44,7 @@ class OffsiteMiddleware(object):\n         allowed_domains = getattr(spider, 'allowed_domains', None)\n         if not allowed_domains:\n             return re.compile('') # allow all by default\n-        domains = [d.replace('.', r'\\.') for d in allowed_domains]\n-        regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n+        regex = r'^(.*\\.)?(%s)$' % '|'.join(re.escape(d) for d in allowed_domains)\n         return re.compile(regex)\n \n     def spider_opened(self, spider):\n\n@@ -23,7 +23,11 @@ class TestOffsiteMiddleware(TestCase):\n                        Request('http://sub.scrapy.org/1'),\n                        Request('http://offsite.tld/letmepass', dont_filter=True)]\n         offsite_reqs = [Request('http://scrapy2.org'),\n-                       Request('http://offsite.tld/')]\n+                       Request('http://offsite.tld/'),\n+                       Request('http://offsite.tld/scrapytest.org'),\n+                       Request('http://offsite.tld/rogue.scrapytest.org'),\n+                       Request('http://rogue.scrapytest.org.haha.com'),\n+                       Request('http://roguescrapytest.org')]\n         reqs = onsite_reqs + offsite_reqs\n \n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cfe588103c669c90d0090d4bc080bd962a54f2e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 229 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -102,6 +102,7 @@ setup_args = {\n     'cmdclass': cmdclasses,\n     'data_files': data_files,\n     'scripts': scripts,\n+    'include_package_data': True,\n     'classifiers': [\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 2.7',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a651a752487ef37edc50a6456629b54b9acbe7f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 480 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -77,7 +77,7 @@ def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8', prepend_level=\n \n     spider = ev.get('spider')\n     if spider:\n-        ev['system'] = spider.name\n+        ev['system'] = unicode_to_str(spider.name, encoding)\n \n     lvlname = level_names.get(level, 'NOLEVEL')\n     message = ev.get('message')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#58a98b0c04b7dd10720b4310de1aef5765570885", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 42 | Churn Cumulative: 921 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,8 +39,10 @@ class HTTP11DownloadHandler(object):\n \n class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     \"\"\"An endpoint that tunnels through proxies to allow HTTPS downloads. To\n-    accomplish that, this endpoint sends an HTTP CONNECT to the proxy. The\n-    HTTP CONNECT is always sent when using this endpoint, I think this could\n+    accomplish that, this endpoint sends an HTTP CONNECT to the proxy. If the\n+    proxy fails to open the tunnel this endpoint will behave as a\n+    L{TCP4ClientEndpoint}.\n+    The HTTP CONNECT is always sent when using this endpoint, I think this could\n     be improved as the CONNECT will be redundant if the connection associated\n     with this endpoint comes from the pool and a CONNECT has already been issued\n     for it.\n@@ -75,27 +77,53 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         \"\"\"Processes the response from the proxy. If the tunnel is successfully\n         created, notifies the client that we are ready to send requests.\n         \"\"\"\n+        # Restore the protocol dataReceived method.\n+        self._protocol.dataReceived = self._protocolDataReceived\n         if bytes.find('200 Connection established') > 0:\n             # The tunnel is ready, switch transport to TLS.\n             self._protocol.transport.startTLS(self._contextFactory,\n                                               self._protocolFactory)\n-            # Restore the protocol dataReceived method.\n-            self._protocol.dataReceived = self._protocolDataReceived\n             # Trigger the callback with the protocol as the value.\n             self._tunnelReadyDeferred.callback(self._protocol)\n         else:\n-            # Not sure if this is the best way to handle this error.\n-            raise SSLError\n+            # The proxy could not open the tunnel and will drop the connection;\n+            # (we need to check if this is common to every proxy). In order to\n+            # allow the client to send the request and get a response from the\n+            # proxy, we will intercept the connectionLost message and restore\n+            # the connection.\n+            self._protocolConnectionLost = self._protocol.connectionLost\n+            self._protocol.connectionLost = self.connectionLost\n \n-    def connect(self, protocolFactory):\n+    def connectFailed(self, reason):\n+        \"\"\"Propagates the errback to the appropriate deferred.\"\"\"\n+        self._tunnelReadyDeferred.errback(reason)\n+\n+    def connectionLost(self, reason):\n+        \"\"\"Restores the connection to the proxy server but does not request\n+        for it to open a tunnel.\n+        \"\"\"\n+        # Restore and call the protocol connection lost method.\n+        self._protocol.connectionLost = self._protocolConnectionLost\n+        self._protocol.connectionLost(reason)\n+        # Restore the connection to the proxy but don't open the tunnel.\n+        self.connect(self._protocolFactory, False)\n+\n+    def connect(self, protocolFactory, openTunnel=True):\n         # Store the protocol factory as we will need it to switch to TLS.\n         self._protocolFactory = protocolFactory\n         connectDeferred = super(TunnelingTCP4ClientEndpoint,\n                                 self).connect(protocolFactory)\n+        if openTunnel:\n             # Add a callback to open the tunnel when the connection is ready.\n             connectDeferred.addCallback(self.requestTunnel)\n+            connectDeferred.addErrback(self.connectFailed)\n             # Return a deferred that will be triggered when the tunnel is ready.\n             return self._tunnelReadyDeferred\n+        else:\n+            def cbCallback(protocol):\n+                self._tunnelReadyDeferred.callback(protocol)\n+            connectDeferred.addCallback(cbCallback)\n+            return connectDeferred\n \n \n class TunnelingAgent(Agent):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#628bfbcc3e0945edd3ad5df733f96c5f12a82449", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 57 | Files Changed: 1 | Hunks: 23 | Methods Changed: 13 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 84 | Churn Cumulative: 1005 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,12 +3,13 @@\n from time import time\n from cStringIO import StringIO\n from urlparse import urldefrag\n+from re import match\n \n from zope.interface import implements\n-from twisted.internet import defer, reactor, protocol, ssl\n+from twisted.internet import defer, reactor, protocol\n from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer\n-from twisted.internet.error import TimeoutError, SSLError\n+from twisted.internet.error import TimeoutError\n from twisted.web.http import PotentialDataLoss\n from scrapy.xlib.tx import Agent, ProxyAgent, ResponseDone, \\\n     HTTPConnectionPool, TCP4ClientEndpoint\n@@ -37,93 +38,64 @@ class HTTP11DownloadHandler(object):\n         return self._pool.closeCachedConnections()\n \n \n+class TunnelError(Exception):\n+    \"\"\"An HTTP CONNECT tunnel could not be established by the proxy.\"\"\"\n+\n+\n class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     \"\"\"An endpoint that tunnels through proxies to allow HTTPS downloads. To\n-    accomplish that, this endpoint sends an HTTP CONNECT to the proxy. If the\n-    proxy fails to open the tunnel this endpoint will behave as a\n-    L{TCP4ClientEndpoint}.\n+    accomplish that, this endpoint sends an HTTP CONNECT to the proxy.\n     The HTTP CONNECT is always sent when using this endpoint, I think this could\n     be improved as the CONNECT will be redundant if the connection associated\n     with this endpoint comes from the pool and a CONNECT has already been issued\n     for it.\n     \"\"\"\n \n-    def __init__(self, reactor, host, port, proxyHost, proxyPort,\n-                contextFactory, timeout=30, bindAddress=None):\n+    def __init__(self, reactor, host, port, proxyConf, contextFactory,\n+                 timeout=30, bindAddress=None):\n+        proxyHost, proxyPort = proxyConf\n         super(TunnelingTCP4ClientEndpoint, self).__init__(reactor, proxyHost,\n             proxyPort, timeout, bindAddress)\n         self._tunnelReadyDeferred = defer.Deferred()\n-        # Although we will connect to the proxy, we need the host and port of\n-        # the destination server in order to send the HTTP CONNECT.\n         self._tunneledHost = host\n         self._tunneledPort = port\n         self._contextFactory = contextFactory\n \n     def requestTunnel(self, protocol):\n         \"\"\"Asks the proxy to open a tunnel.\"\"\"\n-        # Ask for the proxy to open the tunnel.\n-        protocol.transport.write('CONNECT %s:%s HTTP/1.1\\n\\n' %\n-                                 (self._tunneledHost, self._tunneledPort))\n-        # This hack is not so nice. Substitute the dataReceived method\n-        # temporarily to intercept the response from the proxy.\n+        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\n\\n' % (self._tunneledHost,\n+                                                   self._tunneledPort)\n+        protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse\n-        # Store the protocol because we will have to pass it when triggering the\n-        # deferred returned in the connect method.\n         self._protocol = protocol\n         return protocol\n \n     def processProxyResponse(self, bytes):\n         \"\"\"Processes the response from the proxy. If the tunnel is successfully\n-        created, notifies the client that we are ready to send requests.\n+        created, notifies the client that we are ready to send requests. If not\n+        raises a TunnelError.\n         \"\"\"\n-        # Restore the protocol dataReceived method.\n         self._protocol.dataReceived = self._protocolDataReceived\n-        if bytes.find('200 Connection established') > 0:\n-            # The tunnel is ready, switch transport to TLS.\n+        if match('HTTP/1\\.. 200', bytes):\n             self._protocol.transport.startTLS(self._contextFactory,\n                                               self._protocolFactory)\n-            # Trigger the callback with the protocol as the value.\n             self._tunnelReadyDeferred.callback(self._protocol)\n         else:\n-            # The proxy could not open the tunnel and will drop the connection;\n-            # (we need to check if this is common to every proxy). In order to\n-            # allow the client to send the request and get a response from the\n-            # proxy, we will intercept the connectionLost message and restore\n-            # the connection.\n-            self._protocolConnectionLost = self._protocol.connectionLost\n-            self._protocol.connectionLost = self.connectionLost\n+            self._tunnelReadyDeferred.errback(\n+                TunnelError('Could not open CONNECT tunnel.'))\n \n     def connectFailed(self, reason):\n         \"\"\"Propagates the errback to the appropriate deferred.\"\"\"\n         self._tunnelReadyDeferred.errback(reason)\n \n-    def connectionLost(self, reason):\n-        \"\"\"Restores the connection to the proxy server but does not request\n-        for it to open a tunnel.\n-        \"\"\"\n-        # Restore and call the protocol connection lost method.\n-        self._protocol.connectionLost = self._protocolConnectionLost\n-        self._protocol.connectionLost(reason)\n-        # Restore the connection to the proxy but don't open the tunnel.\n-        self.connect(self._protocolFactory, False)\n-\n-    def connect(self, protocolFactory, openTunnel=True):\n-        # Store the protocol factory as we will need it to switch to TLS.\n+    def connect(self, protocolFactory):\n         self._protocolFactory = protocolFactory\n         connectDeferred = super(TunnelingTCP4ClientEndpoint,\n                                 self).connect(protocolFactory)\n-        if openTunnel:\n-            # Add a callback to open the tunnel when the connection is ready.\n         connectDeferred.addCallback(self.requestTunnel)\n         connectDeferred.addErrback(self.connectFailed)\n-            # Return a deferred that will be triggered when the tunnel is ready.\n         return self._tunnelReadyDeferred\n-        else:\n-            def cbCallback(protocol):\n-                self._tunnelReadyDeferred.callback(protocol)\n-            connectDeferred.addCallback(cbCallback)\n-            return connectDeferred\n \n \n class TunnelingAgent(Agent):\n@@ -134,17 +106,16 @@ class TunnelingAgent(Agent):\n     proxy involved.\n     \"\"\"\n \n-    def __init__(self, reactor, proxyHost, proxyPort, contextFactory=None,\n+    def __init__(self, reactor, proxyConf, contextFactory=None,\n                  connectTimeout=None, bindAddress=None, pool=None):\n         super(TunnelingAgent, self).__init__(reactor, contextFactory,\n             connectTimeout, bindAddress, pool)\n-        self._proxyHost = proxyHost\n-        self._proxyPort = proxyPort\n+        self._proxyConf = proxyConf\n \n     def _getEndpoint(self, scheme, host, port):\n         return TunnelingTCP4ClientEndpoint(self._reactor, host, port,\n-            self._proxyHost, self._proxyPort, self._contextFactory,\n-            self._connectTimeout, self._bindAddress)\n+            self._proxyConf, self._contextFactory, self._connectTimeout,\n+            self._bindAddress)\n \n \n class ScrapyAgent(object):\n@@ -153,8 +124,7 @@ class ScrapyAgent(object):\n     _ProxyAgent = ProxyAgent\n     _TunnelingAgent = TunnelingAgent\n \n-    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None,\n-                 pool=None):\n+    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None):\n         self._contextFactory = contextFactory\n         self._connectTimeout = connectTimeout\n         self._bindAddress = bindAddress\n@@ -167,8 +137,8 @@ class ScrapyAgent(object):\n             _, _, proxyHost, proxyPort, _ = _parse(proxy)\n             scheme = _parse(request.url)[0]\n             if  scheme == 'https':\n-                # We need to tunnel the proxy using an HTTP CONNECT.\n-                return self._TunnelingAgent(reactor, proxyHost, proxyPort,\n+                proxyConf = (proxyHost, proxyPort)\n+                return self._TunnelingAgent(reactor, proxyConf,\n                     contextFactory=self._contextFactory, connectTimeout=timeout,\n                     bindAddress=bindaddress, pool=self._pool)\n             else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f053cc1d27d565c8a44f4135949087d3197e18f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 1016 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,7 +53,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                  timeout=30, bindAddress=None):\n-        proxyHost, proxyPort = proxyConf\n+        proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n         super(TunnelingTCP4ClientEndpoint, self).__init__(reactor, proxyHost,\n             proxyPort, timeout, bindAddress)\n         self._tunnelReadyDeferred = defer.Deferred()\n@@ -63,8 +63,12 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     def requestTunnel(self, protocol):\n         \"\"\"Asks the proxy to open a tunnel.\"\"\"\n-        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\n\\n' % (self._tunneledHost,\n+        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\n' % (self._tunneledHost,\n                                                   self._tunneledPort)\n+        if self._proxyAuthHeader:\n+            tunnelReq += 'Proxy-Authorization: %s \\n\\n' % self._proxyAuthHeader\n+        else:\n+            tunnelReq += '\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse\n@@ -137,7 +141,8 @@ class ScrapyAgent(object):\n             _, _, proxyHost, proxyPort, _ = _parse(proxy)\n             scheme = _parse(request.url)[0]\n             if  scheme == 'https':\n-                proxyConf = (proxyHost, proxyPort)\n+                proxyConf = (proxyHost, proxyPort,\n+                             request.headers.get('Proxy-Authorization', None))\n                 return self._TunnelingAgent(reactor, proxyConf,\n                     contextFactory=self._contextFactory, connectTimeout=timeout,\n                     bindAddress=bindaddress, pool=self._pool)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23c3288a6d07834687f607ac95ce5c082f061bfa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 1021 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -138,9 +138,10 @@ class ScrapyAgent(object):\n         bindaddress = request.meta.get('bindaddress') or self._bindAddress\n         proxy = request.meta.get('proxy')\n         if proxy:\n-            _, _, proxyHost, proxyPort, _ = _parse(proxy)\n+            _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n             scheme = _parse(request.url)[0]\n-            if  scheme == 'https':\n+            skipConnectTunnel = proxyParams.find('noconnect') >= 0\n+            if  scheme == 'https' and not skipConnectTunnel:\n                 proxyConf = (proxyHost, proxyPort,\n                              request.headers.get('Proxy-Authorization', None))\n                 return self._TunnelingAgent(reactor, proxyConf,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
