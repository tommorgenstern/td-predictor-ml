{"custom_id": "scrapy#c7f821990117dc69f5c843e6bc1f2b0859c2f015", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 91 | Files Changed: 16 | Hunks: 39 | Methods Changed: 21 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 160 | Churn Cumulative: 5554 | Contributors (this commit): 10 | Commits (past 90d): 44 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ from scrapy.command import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n+from scrapy.settings.deprecated import check_deprecated_settings\n \n def _iter_command_classes(module_name):\n     # TODO: add `name` attribute to commands and and merge this function with\n@@ -80,10 +81,17 @@ def _run_print_help(parser, func, *a, **kw):\n             parser.print_help()\n         sys.exit(2)\n \n-def execute(argv=None):\n+def execute(argv=None, settings=None):\n     if argv is None:\n         argv = sys.argv\n+    if settings is None:\n         settings = get_project_settings()\n+    check_deprecated_settings(settings)\n+\n+    # backwards compatibility to support scrapy.conf.settings\n+    from scrapy import conf\n+    conf.settings = settings\n+\n     crawler = CrawlerProcess(settings)\n     crawler.install()\n     inproject = inside_project()\n\n@@ -28,7 +28,7 @@ class ScrapyCommand(object):\n     @property\n     def crawler(self):\n         if not log.started:\n-            log.start()\n+            log.start_from_settings(self.settings)\n         self._crawler.configure()\n         return self._crawler\n \n\n@@ -1,7 +1,5 @@\n-# This module is kept for backwards compatibility.\n+# This module is kept for backwards compatibility, so users can import\n+# scrapy.conf.settings and get the settings they expect\n #\n # TODO: Add deprecation warning once all scrapy.conf instances have been\n # removed from Scrapy codebase.\n-\n-from scrapy.utils.project import get_project_settings\n-settings = get_project_settings()\n\n@@ -54,6 +54,7 @@ class ExecutionEngine(object):\n         self.crawler = crawler\n         self.settings = crawler.settings\n         self.signals = crawler.signals\n+        self.logformatter = crawler.logformatter\n         self.slots = {}\n         self.running = False\n         self.paused = False\n@@ -193,7 +194,7 @@ class ExecutionEngine(object):\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n-                logkws = log.formatter.crawled(request, response, spider)\n+                logkws = self.logformatter.crawled(request, response, spider)\n                 log.msg(level=log.DEBUG, spider=spider, **logkws)\n                 self.signals.send_catch_log(signal=signals.response_received, \\\n                     response=response, request=request, spider=spider)\n\n@@ -67,6 +67,7 @@ class Scraper(object):\n         self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n         self.crawler = crawler\n         self.signals = crawler.signals\n+        self.logformatter = crawler.logformatter\n \n     @defer.inlineCallbacks\n     def open_spider(self, spider):\n@@ -198,14 +199,14 @@ class Scraper(object):\n         if isinstance(output, Failure):\n             ex = output.value\n             if isinstance(ex, DropItem):\n-                logkws = log.formatter.dropped(item, ex, response, spider)\n+                logkws = self.logformatter.dropped(item, ex, response, spider)\n                 log.msg(level=log.WARNING, spider=spider, **logkws)\n                 return self.signals.send_catch_log_deferred(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.err(output, 'Error processing %(item)s', item=item, spider=spider)\n         else:\n-            logkws = log.formatter.scraped(output, response, spider)\n+            logkws = self.logformatter.scraped(output, response, spider)\n             log.msg(level=log.DEBUG, spider=spider, **logkws)\n             return self.signals.send_catch_log_deferred(signal=signals.item_scraped, \\\n                 item=output, response=response, spider=spider)\n\n@@ -33,6 +33,8 @@ class Crawler(object):\n         if self.configured:\n             return\n         self.configured = True\n+        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n+        self.logformatter = lf_cls.from_crawler(self)\n         self.extensions = ExtensionManager.from_crawler(self)\n         spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n         self.spiders = spman_cls.from_crawler(self)\n\n@@ -9,12 +9,11 @@ from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n     html_body_declared_encoding, http_content_type_encoding\n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs\n-from scrapy.conf import settings\n \n \n class TextResponse(Response):\n \n-    _DEFAULT_ENCODING = settings['DEFAULT_RESPONSE_ENCODING']\n+    _DEFAULT_ENCODING = 'ascii'\n \n     def __init__(self, *args, **kwargs):\n         self._encoding = kwargs.pop('encoding', None)\n\n@@ -10,9 +10,7 @@ import warnings\n from twisted.python import log\n \n import scrapy\n-from scrapy.conf import settings\n from scrapy.utils.python import unicode_to_str\n-from scrapy.utils.misc import load_object\n  \n # Logging levels\n DEBUG = logging.DEBUG\n@@ -93,36 +91,23 @@ def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8', prepend_level=\n \n     return ev\n \n-def _get_log_level(level_name_or_id=None):\n-    if level_name_or_id is None:\n-        lvlname = settings['LOG_LEVEL']\n-        return globals()[lvlname]\n-    elif isinstance(level_name_or_id, int):\n+def _get_log_level(level_name_or_id):\n+    if isinstance(level_name_or_id, int):\n         return level_name_or_id\n     elif isinstance(level_name_or_id, basestring):\n         return globals()[level_name_or_id]\n     else:\n         raise ValueError(\"Unknown log level: %r\" % level_name_or_id)\n \n-def start(logfile=None, loglevel=None, logstdout=None):\n-    global started\n-    if started or not settings.getbool('LOG_ENABLED'):\n-        return\n-    started = True\n-\n-    if log.defaultObserver: # check twisted log not already started\n+def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8'):\n+    if log.defaultObserver or True: # check twisted log not already started\n         loglevel = _get_log_level(loglevel)\n-        logfile = logfile or settings['LOG_FILE']\n         file = open(logfile, 'a') if logfile else sys.stderr\n-        if logstdout is None:\n-            logstdout = settings.getbool('LOG_STDOUT')\n-        sflo = ScrapyFileLogObserver(file, loglevel, settings['LOG_ENCODING'])\n+        sflo = ScrapyFileLogObserver(file, loglevel, logencoding)\n         _oldshowwarning = warnings.showwarning\n         log.startLoggingWithObserver(sflo.emit, setStdout=logstdout)\n         # restore warnings, wrongly silenced by Twisted\n         warnings.showwarning = _oldshowwarning\n-        msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n-            settings['BOT_NAME']))\n \n def msg(message=None, _level=INFO, **kw):\n     kw['logLevel'] = kw.pop('level', _level)\n@@ -137,4 +122,13 @@ def err(_stuff=None, _why=None, **kw):\n     kw.setdefault('system', 'scrapy')\n     log.err(_stuff, _why, **kw)\n \n-formatter = load_object(settings['LOG_FORMATTER'])()\n+def start_from_settings(settings):\n+    global started\n+    if started or not settings.getbool('LOG_ENABLED'):\n+        return\n+    started = True\n+\n+    start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],\n+        settings['LOG_ENCODING'])\n+    msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n+        settings['BOT_NAME']))\n\n@@ -37,3 +37,7 @@ class LogFormatter(object):\n             'exception': exception,\n             'item': item,\n         }\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls()\n\n@@ -11,7 +11,6 @@ from cStringIO import StringIO\n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import isbinarytext\n-from scrapy.conf import settings\n \n class ResponseTypes(object):\n \n@@ -31,7 +30,6 @@ class ResponseTypes(object):\n     }\n \n     def __init__(self):\n-        self.CLASSES.update(settings.get('RESPONSE_CLASSES', {}))\n         self.classes = {}\n         self.mimetypes = MimeTypes()\n         mimedata = get_data('scrapy', 'mime.types')\n\n@@ -1,30 +1,26 @@\n \"\"\"\n XPath selectors\n \n-To select the backend explicitly use the SELECTORS_BACKEND variable in your\n-project settings.\n+To select the backend explicitly use the SCRAPY_SELECTORS_BACKEND environment\n+variable.\n \n Two backends are currently available: lxml (default) and libxml2.\n \n \"\"\"\n \n-from scrapy.conf import settings\n+import os\n \n-if settings['SELECTORS_BACKEND'] == 'lxml':\n-    from scrapy.selector.lxmlsel import *\n-elif settings['SELECTORS_BACKEND'] == 'libxml2':\n+backend = os.environ.get('SCRAPY_SELECTORS_BACKEND')\n+\n+if backend == 'libxml2':\n     from scrapy.selector.libxml2sel import *\n-elif settings['SELECTORS_BACKEND'] == 'dummy':\n-    from scrapy.selector.dummysel import *\n+elif backend == 'lxml':\n+    from scrapy.selector.lxmlsel import *\n else:\n     try:\n         import lxml\n     except ImportError:\n-        try:\n         import libxml2\n-        except ImportError:\n-            from scrapy.selector.dummysel import *\n-        else:\n         from scrapy.selector.libxml2sel import *\n     else:\n         from scrapy.selector.lxmlsel import *\n\n@@ -1,22 +0,0 @@\n-\"\"\"\n-Dummy selectors\n-\"\"\"\n-\n-from .list import XPathSelectorList as XPathSelectorList\n-\n-__all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector', \\\n-    'XPathSelectorList']\n-\n-class XPathSelector(object):\n-\n-    def __init__(self, *a, **kw):\n-        pass\n-\n-    def _raise(self, *a, **kw):\n-        raise RuntimeError(\"No selectors backend available. \" \\\n-            \"Please install libxml2 or lxml\")\n-\n-    select = re = extract = register_namespace = __nonzero__ = _raise\n-\n-XmlXPathSelector = XPathSelector\n-HtmlXPathSelector = XPathSelector\n\n@@ -41,8 +41,6 @@ DEFAULT_REQUEST_HEADERS = {\n     'Accept-Language': 'en',\n }\n \n-DEFAULT_RESPONSE_ENCODING = 'ascii'\n-\n DEPTH_LIMIT = 0\n DEPTH_STATS = True\n DEPTH_PRIORITY = 0\n@@ -197,8 +195,6 @@ SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'\n SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'\n \n-SELECTORS_BACKEND = None # possible values: libxml2, lxml\n-\n SPIDER_MANAGER_CLASS = 'scrapy.spidermanager.SpiderManager'\n \n SPIDER_MIDDLEWARES = {}\n@@ -224,7 +220,7 @@ TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))\n \n URLLENGTH_LIMIT = 2083\n \n-USER_AGENT = 'Scrapy/0.15 (+http://scrapy.org)'\n+USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % __import__('scrapy').__version__\n \n TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n\n@@ -0,0 +1,21 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+\n+DEPRECATED_SETTINGS = [\n+    ('TRACK_REFS', 'no longer needed (trackref is always enabled)'),\n+    ('RESPONSE_CLASSES', 'no longer supported'),\n+    ('DEFAULT_RESPONSE_ENCODING', 'no longer supported'),\n+    ('BOT_VERSION', 'no longer used (user agent defaults to Scrapy now)'),\n+    ('ENCODING_ALIASES', 'no longer needed (encoding discovery uses w3lib now)'),\n+    ('STATS_ENABLED', 'no longer supported (change STATS_CLASS instead)'),\n+    ('SQLITE_DB', 'no longer supported'),\n+    ('SELECTORS_BACKEND', 'use SCRAPY_SELECTORS_BACKEND environment variable instead'),\n+]\n+\n+def check_deprecated_settings(settings):\n+    deprecated = [x for x in DEPRECATED_SETTINGS if settings[x[0]] is not None]\n+    if deprecated:\n+        msg = \"You are using the following settings which are deprecated or obsolete\"\n+        msg += \" (ask scrapy-users@googlegroups.com for alternatives):\"\n+        msg = msg + \"\\n    \" + \"\\n    \".join(\"%s: %s\" % x for x in deprecated)\n+        warnings.warn(msg, ScrapyDeprecationWarning)\n\n@@ -11,7 +11,6 @@ class LogTest(unittest.TestCase):\n \n     def test_get_log_level(self):\n         default_log_level = getattr(log, default_settings.LOG_LEVEL)\n-        self.assertEqual(log._get_log_level(), default_log_level)\n         self.assertEqual(log._get_log_level('WARNING'), log.WARNING)\n         self.assertEqual(log._get_log_level(log.WARNING), log.WARNING)\n         self.assertRaises(ValueError, log._get_log_level, object())\n\n@@ -1,17 +0,0 @@\n-import unittest\n-\n-from scrapy.http import TextResponse\n-from scrapy.selector.dummysel import XmlXPathSelector, HtmlXPathSelector, \\\n-    XPathSelector\n-\n-class XPathSelectorTestCase(unittest.TestCase):\n-\n-    def test_raises(self):\n-        response = TextResponse(url=\"http://example.com\", body='test')\n-        for cls in [XmlXPathSelector, HtmlXPathSelector, XPathSelector]:\n-            sel = cls(response)\n-            self.assertRaises(RuntimeError, sel.select, '//h2')\n-            self.assertRaises(RuntimeError, sel.re, 'lala')\n-            self.assertRaises(RuntimeError, sel.extract)\n-            self.assertRaises(RuntimeError, sel.register_namespace, 'a', 'b')\n-            self.assertRaises(RuntimeError, sel.__nonzero__, 'a', 'b')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7afedb89e9587fee85ff6a4197e4901387b27c91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 17 | Churn Cumulative: 593 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -84,13 +84,22 @@ def _run_print_help(parser, func, *a, **kw):\n def execute(argv=None, settings=None):\n     if argv is None:\n         argv = sys.argv\n+\n+    # --- backwards compatibility for scrapy.conf.settings singleton ---\n+    if settings is None and 'scrapy.conf' in sys.modules:\n+        from scrapy import conf\n+        if hasattr(conf, 'settings'):\n+            settings = conf.settings\n+    # ------------------------------------------------------------------\n+\n     if settings is None:\n         settings = get_project_settings()\n     check_deprecated_settings(settings)\n \n-    # backwards compatibility to support scrapy.conf.settings\n+    # --- backwards compatibility for scrapy.conf.settings singleton ---\n     from scrapy import conf\n     conf.settings = settings\n+    # ------------------------------------------------------------------\n \n     crawler = CrawlerProcess(settings)\n     crawler.install()\n\n@@ -3,3 +3,9 @@\n #\n # TODO: Add deprecation warning once all scrapy.conf instances have been\n # removed from Scrapy codebase.\n+\n+import sys\n+\n+if 'scrapy.cmdline' not in sys.modules:\n+    from scrapy.utils.project import get_project_settings\n+    settings = get_project_settings()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8eacd6f9e6ab46c2662c2c7e1843dafb87689a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 139 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -39,7 +39,7 @@ class Command(ScrapyCommand):\n             raise UsageError()\n         elif len(args) > 1:\n             raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")\n-        for spname in args:\n+        spname = args[0]\n         spider = self.crawler.spiders.create(spname, **opts.spargs)\n         self.crawler.crawl(spider)\n         self.crawler.start()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b46b5a6ef04b1f0a2043191147d544db637281a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 54 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 74 | Churn Cumulative: 553 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 9 | DMM Complexity: 0.23076923076923078\n\nDIFF:\n@@ -4,58 +4,6 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.resolver import dnscache\n \n class AutoThrottle(object):\n-    \"\"\"\n-    ============\n-    AutoThrottle\n-    ============\n-\n-    This is an extension for automatically throttling crawling speed based on\n-    load.\n-\n-    Design goals\n-    ============\n-\n-    1. be nicer to sites instead of using default download delay of zero\n-\n-    2. automatically adjust scrapy to the optimum crawling speed, so the user\n-    doesn't have to tune the download delays and concurrent requests to find\n-    the optimum one. the user only needs to specify the maximum concurrent\n-    requests it allows, and the extension does the rest.\n-\n-    Download latencies\n-    ==================\n-\n-    In Scrapy, the download latency is the (real) time elapsed between\n-    establishing the TCP connection and receiving the HTTP headers.\n-\n-    Note that these latencies are very hard to measure accurately in a\n-    cooperative multitasking environment because Scrapy may be busy processing\n-    a spider callback, for example, and unable to attend downloads. However,\n-    the latencies should give a reasonable estimate of how busy Scrapy (and\n-    ultimately, the server) is. This extension builds on that premise.\n-\n-    Throttling rules\n-    ================\n-\n-    This adjusts download delays and concurrency based on the following rules:\n-\n-    1. spiders always start with one concurrent request and a download delay of\n-    START_DELAY\n-\n-    2. when a response is received, the download delay is adjusted to the\n-    average of previous download delay and the latency of the response.\n-\n-    3. after CONCURRENCY_CHECK_PERIOD responses have passed, the average\n-    latency of this period is checked against the previous one and:\n-\n-    3.1. if the latency remained constant (within standard deviation limits)\n-    and the concurrency is lower than MAX_CONCURRENCY, the concurrency is\n-    increased\n-\n-    3.2. if the latency has increased (beyond standard deviation limits) and\n-    the concurrency is higher than 1, the concurrency is decreased\n-\n-    \"\"\"\n \n     def __init__(self, crawler):\n         settings = crawler.settings    \n@@ -66,12 +14,27 @@ class AutoThrottle(object):\n         crawler.signals.connect(self.response_received, signal=signals.response_received)\n         self.START_DELAY = settings.getfloat(\"AUTOTHROTTLE_START_DELAY\", 5.0)\n         self.CONCURRENCY_CHECK_PERIOD = settings.getint(\"AUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD\", 10)\n-        self.MAX_CONCURRENCY = settings.getint(\"AUTOTHROTTLE_MAX_CONCURRENCY\", 8)\n+        self.MAX_CONCURRENCY = self._max_concurency(settings)\n+        self.MIN_DOWNLOAD_DELAY = self._min_download_delay(settings)\n         self.DEBUG = settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n-        self.MIN_DOWNLOAD_DELAY = settings.getint(\"AUTOTHROTTLE_MIN_DOWNLOAD_DELAY\")\n         self.last_latencies = [self.START_DELAY]\n         self.last_lat = self.START_DELAY, 0.0\n \n+    def _min_download_delay(self, settings):\n+        return max(settings.getint(\"AUTOTHROTTLE_MIN_DOWNLOAD_DELAY\"),\n+            settings.getint(\"DOWNLOAD_DELAY\"))\n+\n+    def _max_concurency(self, settings):\n+        delay = self._min_download_delay(settings)\n+        if delay == 0:\n+            candidates = [\"AUTOTHROTTLE_MAX_CONCURRENCY\",\n+                \"CONCURRENT_REQUESTS_PER_DOMAIN\", \"CONCURRENT_REQUESTS_PER_IP\"]\n+            candidates = [settings.getint(x) for x in candidates]\n+            candidates = [x for x in candidates if x > 0]\n+            if candidates:\n+                return min(candidates)\n+        return 1\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler)\n\n@@ -108,6 +108,7 @@ EXTENSIONS_BASE = {\n     'scrapy.contrib.feedexport.FeedExporter': 0,\n     'scrapy.contrib.logstats.LogStats': 0,\n     'scrapy.contrib.spiderstate.SpiderState': 0,\n+    'scrapy.contrib.throttle.AutoThrottle': 0,\n }\n \n FEED_URI = None\n\n@@ -10,6 +10,8 @@ DEPRECATED_SETTINGS = [\n     ('STATS_ENABLED', 'no longer supported (change STATS_CLASS instead)'),\n     ('SQLITE_DB', 'no longer supported'),\n     ('SELECTORS_BACKEND', 'use SCRAPY_SELECTORS_BACKEND environment variable instead'),\n+    ('AUTOTHROTTLE_MIN_DOWNLOAD_DELAY', 'use DOWNLOAD_DELAY instead'),\n+    ('AUTOTHROTTLE_MAX_CONCURRENCY', 'use CONCURRENT_REQUESTS_PER_DOMAIN instead'),\n ]\n \n def check_deprecated_settings(settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#11d29c70059ee82e5608aaf0c958be19ece6e97a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 187 | Lines Deleted: 32 | Files Changed: 5 | Hunks: 27 | Methods Changed: 22 | Complexity Δ (Sum/Max): 6/9 | Churn Δ: 219 | Churn Cumulative: 957 | Contributors (this commit): 6 | Commits (past 90d): 18 | Contributors (cumulative): 12 | DMM Complexity: 0.9655172413793104\n\nDIFF:\n@@ -1,24 +1,22 @@\n from collections import defaultdict\n from functools import wraps\n \n-from scrapy.conf import settings\n from scrapy.command import ScrapyCommand\n-from scrapy.http import Request\n from scrapy.contracts import ContractsManager\n-from scrapy.utils import display\n from scrapy.utils.misc import load_object\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.conf import build_component_list\n \n+\n def _generate(cb):\n     \"\"\" create a callback which does not return anything \"\"\"\n     @wraps(cb)\n     def wrapper(response):\n         output = cb(response)\n         output = list(iterate_spider_output(output))\n-        # display.pprint(output)\n     return wrapper\n \n+\n class Command(ScrapyCommand):\n     requires_project = True\n     default_settings = {'LOG_ENABLED': False}\n@@ -31,14 +29,15 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\", \\\n+        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n             help=\"only list contracts, without checking them\")\n \n-\n     def run(self, args, opts):\n         # load contracts\n-        contracts = build_component_list(settings['SPIDER_CONTRACTS_BASE'],\n-                settings['SPIDER_CONTRACTS'])\n+        contracts = build_component_list(\n+            self.settings['SPIDER_CONTRACTS_BASE'],\n+            self.settings['SPIDER_CONTRACTS'],\n+        )\n         self.conman = ContractsManager([load_object(c) for c in contracts])\n \n         # contract requests\n\n@@ -1,12 +1,12 @@\n import re\n-import inspect\n from functools import wraps\n \n from scrapy.http import Request\n from scrapy.utils.spider import iterate_spider_output\n-from scrapy.utils.misc import get_spec\n+from scrapy.utils.python import get_spec\n from scrapy.exceptions import ContractFail\n \n+\n class ContractsManager(object):\n     contracts = {}\n \n@@ -27,7 +27,7 @@ class ContractsManager(object):\n \n         return contracts\n \n-    def from_method(self, method):\n+    def from_method(self, method, fail=False):\n         contracts = self.extract_contracts(method)\n         if contracts:\n             # calculate request args\n@@ -43,12 +43,13 @@ class ContractsManager(object):\n \n                 # execute pre and post hooks in order\n                 for contract in reversed(contracts):\n-                    request = contract.add_pre_hook(request)\n+                    request = contract.add_pre_hook(request, fail)\n                 for contract in contracts:\n-                    request = contract.add_post_hook(request)\n+                    request = contract.add_post_hook(request, fail)\n \n                 return request\n \n+\n class Contract(object):\n     \"\"\" Abstract class for contracts \"\"\"\n \n@@ -56,25 +57,35 @@ class Contract(object):\n         self.method = method\n         self.args = args\n \n-    def add_pre_hook(self, request):\n+    def add_pre_hook(self, request, fail=False):\n         cb = request.callback\n+\n         @wraps(cb)\n         def wrapper(response):\n-            try: self.pre_process(response)\n+            try:\n+                self.pre_process(response)\n             except ContractFail as e:\n+                if fail:\n+                    raise\n+                else:\n                     print e.format(self.method)\n             return list(iterate_spider_output(cb(response)))\n \n         request.callback = wrapper\n         return request\n \n-    def add_post_hook(self, request):\n+    def add_post_hook(self, request, fail=False):\n         cb = request.callback\n+\n         @wraps(cb)\n         def wrapper(response):\n             output = list(iterate_spider_output(cb(response)))\n-            try: self.post_process(output)\n+            try:\n+                self.post_process(output)\n             except ContractFail as e:\n+                if fail:\n+                    raise\n+                else:\n                     print e.format(self.method)\n             return output\n \n\n@@ -0,0 +1,124 @@\n+from twisted.trial import unittest\n+\n+from scrapy.spider import BaseSpider\n+from scrapy.http import Request\n+from scrapy.item import Item, Field\n+from scrapy.exceptions import ContractFail\n+from scrapy.contracts import ContractsManager\n+from scrapy.contracts.default import (\n+    UrlContract,\n+    ReturnsContract,\n+    ScrapesContract,\n+)\n+\n+\n+class TestItem(Item):\n+    name = Field()\n+    url = Field()\n+\n+\n+class ResponseMock(object):\n+    url = 'http://scrapy.org'\n+\n+\n+class TestSpider(BaseSpider):\n+    name = 'demo_spider'\n+\n+    def returns_request(self, response):\n+        \"\"\" method which returns request\n+        @url http://scrapy.org\n+        @returns requests 1\n+        \"\"\"\n+        return Request('http://scrapy.org', callback=self.returns_item)\n+\n+    def returns_item(self, response):\n+        \"\"\" method which returns item\n+        @url http://scrapy.org\n+        @returns items 1 1\n+        \"\"\"\n+        return TestItem(url=response.url)\n+\n+    def returns_fail(self, response):\n+        \"\"\" method which returns item\n+        @url http://scrapy.org\n+        @returns items 0 0\n+        \"\"\"\n+        return TestItem(url=response.url)\n+\n+    def scrapes_item_ok(self, response):\n+        \"\"\" returns item with name and url\n+        @url http://scrapy.org\n+        @returns items 1 1\n+        @scrapes name url\n+        \"\"\"\n+        return TestItem(name='test', url=response.url)\n+\n+    def scrapes_item_fail(self, response):\n+        \"\"\" returns item with no name\n+        @url http://scrapy.org\n+        @returns items 1 1\n+        @scrapes name url\n+        \"\"\"\n+        return TestItem(url=response.url)\n+\n+    def parse_no_url(self, response):\n+        \"\"\" method with no url\n+        @returns items 1 1\n+        \"\"\"\n+        pass\n+\n+\n+class ContractsManagerTest(unittest.TestCase):\n+    contracts = [UrlContract, ReturnsContract, ScrapesContract]\n+\n+    def test_contracts(self):\n+        conman = ContractsManager(self.contracts)\n+\n+        # extract contracts correctly\n+        contracts = conman.extract_contracts(TestSpider.returns_request)\n+        self.assertEqual(len(contracts), 2)\n+        self.assertEqual(frozenset(map(type, contracts)),\n+            frozenset([UrlContract, ReturnsContract]))\n+\n+        # returns request for valid method\n+        request = conman.from_method(TestSpider.returns_request)\n+        self.assertIsNotNone(request)\n+\n+        # no request for missing url\n+        request = conman.from_method(TestSpider.parse_no_url)\n+        self.assertIsNone(request)\n+\n+    def test_returns(self):\n+        conman = ContractsManager(self.contracts)\n+\n+        spider = TestSpider()\n+        response = ResponseMock()\n+\n+        # returns_item\n+        request = conman.from_method(spider.returns_item, fail=True)\n+        output = request.callback(response)\n+        self.assertEqual(map(type, output), [TestItem])\n+\n+        # returns_request\n+        request = conman.from_method(spider.returns_request, fail=True)\n+        output = request.callback(response)\n+        self.assertEqual(map(type, output), [Request])\n+\n+        # returns_fail\n+        request = conman.from_method(spider.returns_fail, fail=True)\n+        self.assertRaises(ContractFail, request.callback, response)\n+\n+    def test_scrapes(self):\n+        conman = ContractsManager(self.contracts)\n+\n+        spider = TestSpider()\n+        response = ResponseMock()\n+\n+        # scrapes_item_ok\n+        request = conman.from_method(spider.scrapes_item_ok, fail=True)\n+        output = request.callback(response)\n+        self.assertEqual(map(type, output), [TestItem])\n+\n+        # scrapes_item_fail\n+        request = conman.from_method(spider.scrapes_item_fail, fail=True)\n+        self.assertRaises(ContractFail, request.callback, response)\n\n@@ -104,18 +104,3 @@ def md5sum(file):\n             break\n         m.update(d)\n     return m.hexdigest()\n-\n-def get_spec(func):\n-    \"\"\"Returns (args, kwargs) tuple for a function\n-\n-    >>> import re\n-    >>> get_spec(re.match)\n-    (['pattern', 'string'], {'flags': 0})\n-    \"\"\"\n-    spec = inspect.getargspec(func)\n-    defaults = spec.defaults or []\n-\n-    firstdefault = len(spec.args) - len(defaults)\n-    args = spec.args[:firstdefault]\n-    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n-    return args, kwargs\n\n@@ -158,6 +158,42 @@ def get_func_args(func):\n         raise TypeError('%s is not callable' % type(func))\n     return func_args\n \n+def get_spec(func):\n+    \"\"\"Returns (args, kwargs) tuple for a function\n+    >>> import re\n+    >>> get_spec(re.match)\n+    (['pattern', 'string'], {'flags': 0})\n+\n+    >>> class Test(object):\n+    ...     def __call__(self, val):\n+    ...         pass\n+    ...     def method(self, val, flags=0):\n+    ...         pass\n+\n+    >>> get_spec(Test)\n+    (['self', 'val'], {})\n+\n+    >>> get_spec(Test.method)\n+    (['self', 'val'], {'flags': 0})\n+\n+    >>> get_spec(Test().method)\n+    (['self', 'val'], {'flags': 0})\n+    \"\"\"\n+\n+    if inspect.isfunction(func) or inspect.ismethod(func):\n+        spec = inspect.getargspec(func)\n+    elif hasattr(func, '__call__'):\n+        spec = inspect.getargspec(func.__call__)\n+    else:\n+        raise TypeError('%s is not callable' % type(func))\n+\n+    defaults = spec.defaults or []\n+\n+    firstdefault = len(spec.args) - len(defaults)\n+    args = spec.args[:firstdefault]\n+    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n+    return args, kwargs\n+\n def equal_attributes(obj1, obj2, attributes):\n     \"\"\"Compare two objects attributes\"\"\"\n     # not attributes given return False by default\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#73e6bc1b106861db4d090dce6789ba08854107d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 245 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,6 @@\n \"\"\"Helper functions which doesn't fit anywhere else\"\"\"\n \n import re\n-import inspect\n import hashlib\n from pkgutil import iter_modules\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7685cb14a4eb41f599807ec02b5ed911c9a3c7c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 324 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -150,7 +150,7 @@ def _get_version(target, opts):\n         p = Popen(['hg', 'tip', '--template', '{rev}'], stdout=PIPE)\n         return 'r%s' % p.communicate()[0]\n     elif version == 'GIT':\n-        p = Popen(['git', 'rev-parse', 'HEAD'], stdout=PIPE)\n+        p = Popen(['git', 'describe', '--always'], stdout=PIPE)\n         return '%s' % p.communicate()[0].strip('\\n')\n     elif version:\n         return version\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e94bd8187ca7bdf21c391186d9da8425014db439", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 100 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,6 +38,10 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n             help=\"set spider argument (may be repeated)\")\n+        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", \\\n+            help=\"dump scraped items into FILE (use - for stdout)\")\n+        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\", default=\"jsonlines\", \\\n+            help=\"format to use for dumping items with -o (default: %default)\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -45,6 +49,12 @@ class Command(ScrapyCommand):\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n             raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n+        if opts.output:\n+            if opts.output == '-':\n+                self.settings.overrides['FEED_URI'] = 'stdout:'\n+            else:\n+                self.settings.overrides['FEED_URI'] = opts.output\n+            self.settings.overrides['FEED_FORMAT'] = opts.output_format\n \n     def run(self, args, opts):\n         if len(args) != 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#34f147731786a44d7170ee672aa208e194aac235", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 128 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -82,11 +82,11 @@ class ContractsManagerTest(unittest.TestCase):\n \n         # returns request for valid method\n         request = conman.from_method(TestSpider.returns_request)\n-        self.assertIsNotNone(request)\n+        self.assertNotEqual(request, None)\n \n         # no request for missing url\n         request = conman.from_method(TestSpider.parse_no_url)\n-        self.assertIsNone(request)\n+        self.assertEqual(request, None)\n \n     def test_returns(self):\n         conman = ContractsManager(self.contracts)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd13dfe82bd6ae0d0d15235e8505c0ea905dd841", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 206 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,10 +51,10 @@ def get_project_settings():\n     if ENVVAR not in os.environ:\n         project = os.environ.get('SCRAPY_PROJECT', 'default')\n         init_env(project)\n-    settings_module_path = os.environ.get(ENVVAR, 'scrapy_settings')\n-    try:\n+    settings_module_path = os.environ.get(ENVVAR)\n+    if settings_module_path:\n         settings_module = __import__(settings_module_path, {}, {}, [''])\n-    except ImportError:\n+    else:\n         settings_module = None\n     settings = CrawlerSettings(settings_module)\n \n\n@@ -9,8 +9,9 @@ class ProcessTest(object):\n     prefix = [sys.executable, '-m', 'scrapy.cmdline']\n     cwd = os.getcwd() # trial chdirs to temp dir\n \n-    def execute(self, args, check_code=True, settings='missing'):\n+    def execute(self, args, check_code=True, settings=None):\n         env = os.environ.copy()\n+        if settings is not None:\n             env['SCRAPY_SETTINGS_MODULE'] = settings\n         cmd = self.prefix + [self.command] + list(args)\n         pp = TestProcessProtocol()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eee8e70b7b3805aa131496cb31f0599f7ad61399", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 69 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -19,7 +19,7 @@ class HttpErrorMiddleware(object):\n     def process_spider_input(self, response, spider):\n         if 200 <= response.status < 300: # common case\n             return\n-        meta = response.request.meta\n+        meta = response.meta\n         if 'handle_httpstatus_all' in meta:\n             return\n         if 'handle_httpstatus_list' in meta:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0256a34f9788dd00f46efcef0047dd86c280cd6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 2784 | Contributors (this commit): 7 | Commits (past 90d): 12 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -200,16 +200,12 @@ class ExecutionEngine(object):\n                     response=response, request=request, spider=spider)\n             return response\n \n-        def _on_error(failure):\n-            failure.request = request\n-            return failure\n-\n         def _on_complete(_):\n             slot.nextcall.schedule()\n             return _\n \n         dwld = self.downloader.fetch(request, spider)\n-        dwld.addCallbacks(_on_success, _on_error)\n+        dwld.addCallbacks(_on_success)\n         dwld.addBoth(_on_complete)\n         return dwld\n \n\n@@ -136,6 +136,7 @@ class Scraper(object):\n                 request_result, request, spider)\n \n     def call_spider(self, result, request, spider):\n+        result.request = request\n         dfd = defer_result(result)\n         dfd.addCallbacks(request.callback or spider.parse, request.errback)\n         return dfd.addCallback(iterate_spider_output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#05ce20dab399c5fb28675a21a3bbc200316fd83c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 146 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ class DepthMiddleware(object):\n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n             if isinstance(request, Request):\n-                depth = response.request.meta['depth'] + 1\n+                depth = response.meta['depth'] + 1\n                 request.meta['depth'] = depth\n                 if self.prio:\n                     request.priority -= depth * self.prio\n@@ -42,8 +42,8 @@ class DepthMiddleware(object):\n             return True\n \n         # base case (depth=0)\n-        if self.stats and 'depth' not in response.request.meta: \n-            response.request.meta['depth'] = 0\n+        if self.stats and 'depth' not in response.meta:\n+            response.meta['depth'] = 0\n             if self.verbose_stats:\n                 self.stats.inc_value('request_depth_count/0', spider=spider)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ceeea27d218241383f47c26967125570f443e3f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 106 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,7 @@ class CoreStats(object):\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n         crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n+        crawler.signals.connect(o.response_received, signal=signals.response_received)\n         return o\n \n     def spider_opened(self, spider):\n@@ -29,6 +30,9 @@ class CoreStats(object):\n     def item_scraped(self, item, spider):\n         self.stats.inc_value('item_scraped_count', spider=spider)\n \n+    def response_received(self, spider):\n+        self.stats.inc_value('response_received_count', spider=spider)\n+\n     def item_dropped(self, item, spider, exception):\n         reason = exception.__class__.__name__\n         self.stats.inc_value('item_dropped_count', spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ae1a3ea523bc9859954717fd850697540e4c84d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 75 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,7 @@ class StatsCollector(object):\n         self._stats.clear()\n \n     def open_spider(self, spider):\n-        self._stats = {}\n+        pass\n \n     def close_spider(self, spider, reason):\n         if self._dump:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd3a109299a18c2a441ca9a33ddcc2b5208f7f1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 11 | Methods Changed: 10 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 29 | Churn Cumulative: 541 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,7 +28,7 @@ class ScrapyCommand(object):\n     @property\n     def crawler(self):\n         if not log.started:\n-            log.start_from_settings(self.settings)\n+            log.start_from_crawler(self._crawler)\n         self._crawler.configure()\n         return self._crawler\n \n\n@@ -33,15 +33,28 @@ started = False\n \n class ScrapyFileLogObserver(log.FileLogObserver):\n \n-    def __init__(self, f, level=INFO, encoding='utf-8'):\n+    def __init__(self, f, level=INFO, encoding='utf-8', crawler=None):\n         self.level = level\n         self.encoding = encoding\n+        if crawler:\n+            self.crawler = crawler\n+            self.emit = self._emit_with_crawler\n+        else:\n+            self.emit = self._emit\n         log.FileLogObserver.__init__(self, f)\n \n-    def emit(self, eventDict):\n+    def _emit(self, eventDict):\n         ev = _adapt_eventdict(eventDict, self.level, self.encoding)\n         if ev is not None:\n             log.FileLogObserver.emit(self, ev)\n+        return ev\n+\n+    def _emit_with_crawler(self, eventDict):\n+        ev = self._emit(eventDict)\n+        if ev:\n+            level = ev['logLevel']\n+            sname = 'log_count/%s' % level_names.get(level, level)\n+            self.crawler.stats.inc_value(sname)\n \n def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8', prepend_level=True):\n     \"\"\"Adapt Twisted log eventDict making it suitable for logging with a Scrapy\n@@ -99,15 +112,16 @@ def _get_log_level(level_name_or_id):\n     else:\n         raise ValueError(\"Unknown log level: %r\" % level_name_or_id)\n \n-def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8'):\n+def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8', crawler=None):\n     if log.defaultObserver: # check twisted log not already started\n         loglevel = _get_log_level(loglevel)\n         file = open(logfile, 'a') if logfile else sys.stderr\n-        sflo = ScrapyFileLogObserver(file, loglevel, logencoding)\n+        sflo = ScrapyFileLogObserver(file, loglevel, logencoding, crawler)\n         _oldshowwarning = warnings.showwarning\n         log.startLoggingWithObserver(sflo.emit, setStdout=logstdout)\n         # restore warnings, wrongly silenced by Twisted\n         warnings.showwarning = _oldshowwarning\n+        return sflo\n \n def msg(message=None, _level=INFO, **kw):\n     kw['logLevel'] = kw.pop('level', _level)\n@@ -122,13 +136,14 @@ def err(_stuff=None, _why=None, **kw):\n     kw.setdefault('system', 'scrapy')\n     log.err(_stuff, _why, **kw)\n \n-def start_from_settings(settings):\n+def start_from_crawler(crawler):\n     global started\n+    settings = crawler.settings\n     if started or not settings.getbool('LOG_ENABLED'):\n         return\n     started = True\n \n     start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],\n-        settings['LOG_ENCODING'])\n+        settings['LOG_ENCODING'], crawler)\n     msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n         settings['BOT_NAME']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a905d62f599c6270fcd994604c8eb37af421d22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 11 | Churn Cumulative: 552 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,6 +20,7 @@ class ScrapyCommand(object):\n \n     def __init__(self):\n         self.settings = None # set in scrapy.cmdline\n+        self.configured = False\n \n     def set_crawler(self, crawler):\n         assert not hasattr(self, '_crawler'), \"crawler already set\"\n@@ -27,9 +28,10 @@ class ScrapyCommand(object):\n \n     @property\n     def crawler(self):\n-        if not log.started:\n+        if not self.configured:\n             log.start_from_crawler(self._crawler)\n             self._crawler.configure()\n+            self.configured = True\n         return self._crawler\n \n     def syntax(self):\n\n@@ -29,8 +29,6 @@ level_names = {\n     SILENT: \"SILENT\",\n }\n \n-started = False\n-\n class ScrapyFileLogObserver(log.FileLogObserver):\n \n     def __init__(self, f, level=INFO, encoding='utf-8', crawler=None):\n@@ -113,7 +111,6 @@ def _get_log_level(level_name_or_id):\n         raise ValueError(\"Unknown log level: %r\" % level_name_or_id)\n \n def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8', crawler=None):\n-    if log.defaultObserver: # check twisted log not already started\n     loglevel = _get_log_level(loglevel)\n     file = open(logfile, 'a') if logfile else sys.stderr\n     sflo = ScrapyFileLogObserver(file, loglevel, logencoding, crawler)\n@@ -137,11 +134,9 @@ def err(_stuff=None, _why=None, **kw):\n     log.err(_stuff, _why, **kw)\n \n def start_from_crawler(crawler):\n-    global started\n     settings = crawler.settings\n-    if started or not settings.getbool('LOG_ENABLED'):\n+    if not settings.getbool('LOG_ENABLED'):\n         return\n-    started = True\n \n     start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],\n         settings['LOG_ENCODING'], crawler)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
