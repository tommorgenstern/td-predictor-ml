{"custom_id": "scrapy#06b9702984ad4cd2342ee9927a2c181a8d57c0d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 237 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -29,10 +29,10 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_delay(self):\n         spider = FollowAllSpider()\n-        yield docrawl(spider, {\"DOWNLOAD_DELAY\": 0.3})\n+        yield docrawl(spider, {\"DOWNLOAD_DELAY\": 1})\n         t = spider.times[0]\n         for t2 in spider.times[1:]:\n-            self.assertTrue(t2-t > 0.15, \"download delay too small: %s\" % (t2-t))\n+            self.assertTrue(t2-t > 0.45, \"download delay too small: %s\" % (t2-t))\n             t = t2\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#07969a9f3bec17189c1ef573b998d66320ed1699", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 22 | Churn Cumulative: 223 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,15 +13,10 @@ from scrapy import log\n \n class OffsiteMiddleware(object):\n \n-    def __init__(self):\n-        self.host_regexes = {}\n-        self.domains_seen = {}\n-\n     @classmethod\n     def from_crawler(cls, crawler):\n         o = cls()\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n-        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n \n     def process_spider_output(self, response, result, spider):\n@@ -31,15 +26,15 @@ class OffsiteMiddleware(object):\n                     yield x\n                 else:\n                     domain = urlparse_cached(x).hostname\n-                    if domain and domain not in self.domains_seen[spider]:\n-                        self.domains_seen[spider].add(domain)\n+                    if domain and domain not in self.domains_seen:\n+                        self.domains_seen.add(domain)\n                         log.msg(format=\"Filtered offsite request to %(domain)r: %(request)s\",\n                                 level=log.DEBUG, spider=spider, domain=domain, request=x)\n             else:\n                 yield x\n \n     def should_follow(self, request, spider):\n-        regex = self.host_regexes[spider]\n+        regex = self.host_regex\n         # hostanme can be None for wrong urls (like javascript links)\n         host = urlparse_cached(request).hostname or ''\n         return bool(regex.search(host))\n@@ -54,9 +49,5 @@ class OffsiteMiddleware(object):\n         return re.compile(regex)\n \n     def spider_opened(self, spider):\n-        self.host_regexes[spider] = self.get_host_regex(spider)\n-        self.domains_seen[spider] = set()\n-\n-    def spider_closed(self, spider):\n-        del self.host_regexes[spider]\n-        del self.domains_seen[spider]\n+        self.host_regex = self.get_host_regex(spider)\n+        self.domains_seen = set()\n\n@@ -29,9 +29,6 @@ class TestOffsiteMiddleware(TestCase):\n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n         self.assertEquals(out, onsite_reqs)\n \n-    def tearDown(self):\n-        self.mw.spider_closed(self.spider)\n-\n \n class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2abb9d08cb8e67c5716a56c3a05999aa9b330c62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 12 | Churn Cumulative: 865 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,7 +19,6 @@ class MediaPipeline(object):\n             self.waiting = defaultdict(list)\n \n     def __init__(self, download_func=None):\n-        self.spiderinfo = {}\n         self.download_func = download_func\n \n     @classmethod\n@@ -32,13 +31,10 @@ class MediaPipeline(object):\n         return pipe\n \n     def open_spider(self, spider):\n-        self.spiderinfo[spider] = self.SpiderInfo(spider)\n-\n-    def close_spider(self, spider):\n-        del self.spiderinfo[spider]\n+        self.spiderinfo = self.SpiderInfo(spider)\n \n     def process_item(self, item, spider):\n-        info = self.spiderinfo[spider]\n+        info = self.spiderinfo\n         requests = arg_to_iter(self.get_media_requests(item, info))\n         dlist = [self._process_request(r, info) for r in requests]\n         dfd = DeferredList(dlist, consumeErrors=1)\n\n@@ -8,7 +8,6 @@ from scrapy.http import Request, Response\n from scrapy.spider import BaseSpider\n from scrapy.utils.request import request_fingerprint\n from scrapy.contrib.pipeline.media import MediaPipeline\n-from scrapy.utils.test import get_crawler\n from scrapy.utils.signal import disconnect_all\n from scrapy import signals\n from scrapy import log\n@@ -27,13 +26,12 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         self.spider = BaseSpider('media.com')\n         self.pipe = self.pipeline_class(download_func=_mocked_download_func)\n         self.pipe.open_spider(self.spider)\n-        self.info = self.pipe.spiderinfo[self.spider]\n+        self.info = self.pipe.spiderinfo\n \n     def tearDown(self):\n         for name, signal in vars(signals).items():\n             if not name.startswith('_'):\n                 disconnect_all(signal)\n-        self.pipe.close_spider(self.spider)\n \n     def test_default_media_to_download(self):\n         request = Request('http://url')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fc2bc34898f24480b0f5f6a315b523211d768e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 356 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -148,7 +148,6 @@ class FeedExporter(object):\n         self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n         uripar = settings['FEED_URI_PARAMS']\n         self._uripar = load_object(uripar) if uripar else lambda x, y: None\n-        self.slots = {}\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -172,10 +171,10 @@ class FeedExporter(object):\n         file = storage.open(spider)\n         exporter = self._get_exporter(file)\n         exporter.start_exporting()\n-        self.slots[spider] = SpiderSlot(file, exporter, storage, uri)\n+        self.slot = SpiderSlot(file, exporter, storage, uri)\n \n     def close_spider(self, spider):\n-        slot = self.slots.pop(spider)\n+        slot = self.slot\n         if not slot.itemcount and not self.store_empty:\n             return\n         slot.exporter.finish_exporting()\n@@ -187,7 +186,7 @@ class FeedExporter(object):\n         return d\n \n     def item_scraped(self, item, spider):\n-        slot = self.slots[spider]\n+        slot = self.slot\n         slot.exporter.export_item(item)\n         slot.itemcount += 1\n         return item\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45d6d2044c48ef43108cc8a133f2b65ee77d2262", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 263 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -99,8 +99,8 @@ class Contract(object):\n \n             @wraps(cb)\n             def wrapper(response):\n-                try:\n                 output = list(iterate_spider_output(cb(response)))\n+                try:\n                     results.startTest(self.testcase_post)\n                     self.post_process(output)\n                     results.stopTest(self.testcase_post)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#919d7c4062f815f2ea6e5e8f606a24a01f6359bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 29 | Files Changed: 5 | Hunks: 26 | Methods Changed: 13 | Complexity Δ (Sum/Max): 20/17 | Churn Δ: 138 | Churn Cumulative: 1793 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: 0.8913043478260869\n\nDIFF:\n@@ -5,7 +5,7 @@ import inspect\n import pkg_resources\n \n import scrapy\n-from scrapy.crawler import CrawlerProcess\n+from scrapy.crawler import CrawlerProcess, MultiCrawlerProcess\n from scrapy.xlib import lsprofcalltree\n from scrapy.command import ScrapyCommand\n from scrapy.exceptions import UsageError\n@@ -117,8 +117,6 @@ def execute(argv=None, settings=None):\n         conf.settings = settings\n     # ------------------------------------------------------------------\n \n-    crawler = CrawlerProcess(settings)\n-    crawler.install()\n     inproject = inside_project()\n     cmds = _get_commands_dict(settings, inproject)\n     cmdname = _pop_command_name(argv)\n@@ -139,7 +137,15 @@ def execute(argv=None, settings=None):\n     cmd.add_options(parser)\n     opts, args = parser.parse_args(args=argv[1:])\n     _run_print_help(parser, cmd.process_options, args, opts)\n-    cmd.set_crawler(crawler)\n+\n+    if cmd.multi_crawlers:\n+        process = MultiCrawlerProcess(settings)\n+        cmd.process = process\n+    else:\n+        process = CrawlerProcess(settings)\n+        process.install()\n+        cmd.set_crawler(process)\n+\n     _run_print_help(parser, _run_command, cmd, args, opts)\n     sys.exit(cmd.exitcode)\n \n\n@@ -13,6 +13,7 @@ from scrapy.exceptions import UsageError\n class ScrapyCommand(object):\n \n     requires_project = False\n+    multi_crawlers = False\n \n     # default settings to be used for this command instead of global defaults\n     default_settings = {}\n@@ -21,7 +22,6 @@ class ScrapyCommand(object):\n \n     def __init__(self):\n         self.settings = None # set in scrapy.cmdline\n-        self.configured = False\n \n     def set_crawler(self, crawler):\n         assert not hasattr(self, '_crawler'), \"crawler already set\"\n@@ -29,10 +29,10 @@ class ScrapyCommand(object):\n \n     @property\n     def crawler(self):\n-        if not self.configured:\n+        if not self.multi_crawlers and not self._crawler.configured:\n             log.start_from_crawler(self._crawler)\n             self._crawler.configure()\n-            self.configured = True\n+\n         return self._crawler\n \n     def syntax(self):\n\n@@ -2,13 +2,11 @@ from collections import defaultdict\n from functools import wraps\n from unittest import TextTestRunner\n \n-from scrapy import signals\n from scrapy.command import ScrapyCommand\n from scrapy.contracts import ContractsManager\n from scrapy.utils.misc import load_object\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.conf import build_component_list\n-from scrapy.xlib.pydispatch import dispatcher\n \n \n def _generate(cb):\n@@ -22,7 +20,8 @@ def _generate(cb):\n \n class Command(ScrapyCommand):\n     requires_project = True\n-    default_settings = {'LOG_ENABLED': False}\n+    multi_crawlers = True\n+    default_settings = {'LOG_ENABLED': True}\n \n     def syntax(self):\n         return \"[options] <spider>\"\n@@ -48,17 +47,20 @@ class Command(ScrapyCommand):\n \n         # contract requests\n         contract_reqs = defaultdict(list)\n-        self.crawler.engine.has_capacity = lambda: True\n \n-        for spider in args or self.crawler.spiders.list():\n-            spider = self.crawler.spiders.create(spider)\n+        spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n+        spiders = spman_cls.from_settings(self.settings)\n+\n+        for spider in args or spiders.list():\n+            spider = spiders.create(spider)\n             requests = self.get_requests(spider)\n \n             if opts.list:\n                 for req in requests:\n                     contract_reqs[spider.name].append(req.callback.__name__)\n-            else:\n-                self.crawler.crawl(spider, requests)\n+            elif requests:\n+                crawler = self.process.create_crawler(spider.name)\n+                crawler.crawl(spider, requests)\n \n         # start checks\n         if opts.list:\n@@ -67,9 +69,8 @@ class Command(ScrapyCommand):\n                 for method in sorted(methods):\n                     print '  * %s' % method\n         else:\n-            dispatcher.connect(self.results.printErrors,\n-                    signals.engine_stopped)\n-            self.crawler.start()\n+            self.process.start()\n+            self.results.printErrors()\n \n     def get_requests(self, spider):\n         requests = []\n\n@@ -20,6 +20,8 @@ class Crawler(object):\n         self.signals = SignalManager(self)\n         self.stats = load_object(settings['STATS_CLASS'])(self)\n \n+        self.scheduled = {}\n+\n     def install(self):\n         import scrapy.project\n         assert not hasattr(scrapy.project, 'crawler'), \"crawler already installed\"\n@@ -47,7 +49,12 @@ class Crawler(object):\n         spider.set_crawler(self)\n         if requests is None:\n             requests = spider.start_requests()\n+\n+        if self.configured and self.engine.running:\n+            assert not self.scheduled\n             return self.engine.open_spider(spider, requests)\n+        else:\n+            self.scheduled.setdefault(spider, []).extend(requests)\n \n     def _spider_closed(self, spider=None):\n         if not self.engine.open_spiders:\n@@ -56,6 +63,10 @@ class Crawler(object):\n     @defer.inlineCallbacks\n     def start(self):\n         yield defer.maybeDeferred(self.configure)\n+\n+        for spider, requests in self.scheduled.iteritems():\n+            yield self.engine.open_spider(spider, requests)\n+\n         yield defer.maybeDeferred(self.engine.start)\n \n     @defer.inlineCallbacks\n@@ -64,28 +75,22 @@ class Crawler(object):\n             yield defer.maybeDeferred(self.engine.stop)\n \n \n-class CrawlerProcess(Crawler):\n-    \"\"\"A class to run a single Scrapy crawler in a process. It provides\n-    automatic control of the Twisted reactor and installs some convenient\n-    signals for shutting down the crawl.\n+class ProcessMixin(object):\n+    \"\"\" Mixin which provides automatic control of the Twisted reactor and\n+        installs some convenient signals for shutting it down\n     \"\"\"\n \n     def __init__(self, *a, **kw):\n-        super(CrawlerProcess, self).__init__(*a, **kw)\n-        self.signals.connect(self.stop, signals.engine_stopped)\n         install_shutdown_handlers(self._signal_shutdown)\n \n     def start(self):\n-        super(CrawlerProcess, self).start()\n         if self.settings.getbool('DNSCACHE_ENABLED'):\n             reactor.installResolver(CachingThreadedResolver(reactor))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n     def stop(self):\n-        d = super(CrawlerProcess, self).stop()\n-        d.addBoth(self._stop_reactor)\n-        return d\n+        raise NotImplementedError\n \n     def _stop_reactor(self, _=None):\n         try:\n@@ -106,3 +111,68 @@ class CrawlerProcess(Crawler):\n         log.msg(format='Received %(signame)s twice, forcing unclean shutdown',\n                 level=log.INFO, signame=signame)\n         reactor.callFromThread(self._stop_reactor)\n+\n+\n+class CrawlerProcess(Crawler, ProcessMixin):\n+    \"\"\" A class to run a single Scrapy crawler in a process\n+    \"\"\"\n+\n+    def __init__(self, *a, **kw):\n+        Crawler.__init__(self, *a, **kw)\n+        ProcessMixin.__init__(self, *a, **kw)\n+        self.signals.connect(self.stop, signals.engine_stopped)\n+\n+    def start(self):\n+        Crawler.start(self)\n+        ProcessMixin.start(self)\n+\n+    def stop(self):\n+        d = Crawler.stop(self)\n+        d.addBoth(self._stop_reactor)\n+        return d\n+\n+\n+class MultiCrawlerProcess(ProcessMixin):\n+    \"\"\" A class to run multiple scrapy crawlers in a process sequentially\n+    \"\"\"\n+\n+    def __init__(self, settings):\n+        super(MultiCrawlerProcess, self).__init__(settings)\n+\n+        self.settings = settings\n+        self.crawlers = {}\n+        self.stopping = False\n+\n+    def create_crawler(self, name):\n+        if name not in self.crawlers:\n+            self.crawlers[name] = Crawler(self.settings)\n+\n+        return self.crawlers[name]\n+\n+    def start_crawler(self):\n+        name, crawler = self.crawlers.popitem()\n+\n+        crawler.sflo = log.start_from_crawler(crawler)\n+        crawler.signals.connect(crawler.sflo.stop, signals.engine_stopped)\n+        crawler.signals.connect(self.check_done, signals.engine_stopped)\n+        crawler.start()\n+\n+        return name, crawler\n+\n+    def check_done(self, **kwargs):\n+        if self.crawlers and not self.stopping:\n+            self.start_crawler()\n+        else:\n+            self._stop_reactor()\n+\n+    def start(self):\n+        self.start_crawler()\n+        super(MultiCrawlerProcess, self).start()\n+\n+    @defer.inlineCallbacks\n+    def stop(self):\n+        self.stopping = True\n+\n+        for crawler in self.crawlers.itervalues():\n+            if crawler.configured:\n+                yield crawler.stop()\n\n@@ -138,9 +138,12 @@ def start_from_crawler(crawler):\n     if not settings.getbool('LOG_ENABLED'):\n         return\n \n-    start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],\n+    sflo = start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],\n         settings['LOG_ENCODING'], crawler)\n+\n     msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n         settings['BOT_NAME']))\n     msg(\"Optional features available: %s\" % \", \".join(scrapy.optional_features),\n         level=DEBUG)\n+\n+    return sflo\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f3d8b0802a6f9080c5eb9f65be00b27c27684230", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 695 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,7 +21,7 @@ def _generate(cb):\n class Command(ScrapyCommand):\n     requires_project = True\n     multi_crawlers = True\n-    default_settings = {'LOG_ENABLED': True}\n+    default_settings = {'LOG_ENABLED': False}\n \n     def syntax(self):\n         return \"[options] <spider>\"\n\n@@ -153,7 +153,9 @@ class MultiCrawlerProcess(ProcessMixin):\n         name, crawler = self.crawlers.popitem()\n \n         crawler.sflo = log.start_from_crawler(crawler)\n+        if crawler.sflo:\n             crawler.signals.connect(crawler.sflo.stop, signals.engine_stopped)\n+\n         crawler.signals.connect(self.check_done, signals.engine_stopped)\n         crawler.start()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2105ec58eb63d4ac30317c59b5cd21afdad0341d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 12 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -4,3 +4,5 @@ DATABASES = {\n         'NAME': ':memory:',\n     }\n }\n+\n+SECRET_KEY = 'top-secret'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3fe2a32683bce14e1fa1576cdac3a13b9f579b0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 389 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,9 +79,25 @@ def escape_ajax(url):\n     Return the crawleable url according to:\n     http://code.google.com/web/ajaxcrawling/docs/getting-started.html\n \n-    TODO: add support for urls with query arguments\n-\n     >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n     'www.example.com/ajax.html?_escaped_fragment_=key=value'\n+    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n+    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key=value'\n+    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n+    'www.example.com/ajax.html?_escaped_fragment_=key=value'\n+    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n+    'www.example.com/ajax.html?_escaped_fragment_='\n+\n+    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n+\n+    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n+    'www.example.com/ajax.html#key=value'\n+    >>> escape_ajax(\"www.example.com/ajax.html#\")\n+    'www.example.com/ajax.html#'\n+    >>> escape_ajax(\"www.example.com/ajax.html\")\n+    'www.example.com/ajax.html'\n     \"\"\"\n-    return url.replace('#!', '?_escaped_fragment_=')\n+    defrag, frag = urlparse.urldefrag(url)\n+    if not frag.startswith('!'):\n+        return url\n+    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aa7b529975748ef39b9f73fa087af064c80563c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 760 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,7 @@ class HTTP11DownloadHandler(object):\n \n     def __init__(self, settings):\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n+        self._pool._factory.noisy = False\n         self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n         self._contextFactory = self._contextFactoryClass()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9ff82d6695860d57a00b6ecf9c63928788d303fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 43 | Churn Cumulative: 223 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.7\n\nDIFF:\n@@ -127,5 +127,38 @@ Disallow: /forum/active/\n         self.assertEqual(list(sitemap_urls_from_robots(robots)),\n              ['http://example.com/sitemap.xml', 'http://example.com/sitemap-product-index.xml'])\n \n+    def test_sitemap_blanklines(self):\n+        \"\"\"Assert we can deal with starting blank lines before <xml> tag\"\"\"\n+        s = Sitemap(\"\"\"\\\n+\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n+\n+<!-- cache: cached = yes name = sitemap_jspCache key = sitemap -->\n+<sitemap>\n+<loc>http://www.example.com/sitemap1.xml</loc>\n+<lastmod>2013-07-15</lastmod>\n+</sitemap>\n+\n+<sitemap>\n+<loc>http://www.example.com/sitemap2.xml</loc>\n+<lastmod>2013-07-15</lastmod>\n+</sitemap>\n+\n+<sitemap>\n+<loc>http://www.example.com/sitemap3.xml</loc>\n+<lastmod>2013-07-15</lastmod>\n+</sitemap>\n+\n+<!-- end cache -->\n+</sitemapindex>\n+\"\"\")\n+        self.assertEqual(list(s), [\n+            {'lastmod': '2013-07-15', 'loc': 'http://www.example.com/sitemap1.xml'},\n+            {'lastmod': '2013-07-15', 'loc': 'http://www.example.com/sitemap2.xml'},\n+            {'lastmod': '2013-07-15', 'loc': 'http://www.example.com/sitemap3.xml'},\n+        ])\n+\n+\n if __name__ == '__main__':\n     unittest.main()\n\n@@ -4,18 +4,16 @@ Module for processing Sitemaps.\n Note: The main purpose of this module is to provide support for the\n SitemapSpider, its API is subject to change without notice.\n \"\"\"\n+import lxml.etree\n \n-from cStringIO import StringIO\n-from xml.etree.cElementTree import ElementTree\n \n class Sitemap(object):\n     \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n     (type=sitemapindex) files\"\"\"\n \n     def __init__(self, xmltext):\n-        tree = ElementTree()\n-        tree.parse(StringIO(xmltext))\n-        self._root = tree.getroot()\n+        xmlp = lxml.etree.XMLParser(recover=True)\n+        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n         rt = self._root.tag\n         self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt\n \n@@ -26,8 +24,10 @@ class Sitemap(object):\n                 tag = el.tag\n                 name = tag.split('}', 1)[1] if '}' in tag else tag\n                 d[name] = el.text.strip() if el.text else ''\n+            if 'loc' in d:\n                 yield d\n \n+\n def sitemap_urls_from_robots(robots_text):\n     \"\"\"Return an iterator over all sitemap urls contained in the given\n     robots.txt file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66aa1331fced51221349381230c9eec86fe9ee71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 593 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -14,6 +14,7 @@ from twisted.cred import portal, checkers, credentials\n from twisted.protocols.ftp import FTPClient, ConnectionLost\n from w3lib.url import path_to_file_uri\n \n+from scrapy import twisted_version\n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler, HttpDownloadHandler\n from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n@@ -337,6 +338,9 @@ class FTPTestCase(unittest.TestCase):\n     username = \"scrapy\"\n     password = \"passwd\"\n \n+    if twisted_version < (10, 2, 0):\n+        skip = \"Twisted pre 10.2.0 doesn't allow to set home path other than /home\"\n+\n     def setUp(self):\n         # setup dirs and test file\n         self.directory = self.mktemp()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d74b60051d6b4901c7c3058332a2d14c979b01d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 33 | Files Changed: 3 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 90 | Churn Cumulative: 679 | Contributors (this commit): 3 | Commits (past 90d): 22 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,31 +13,7 @@ def getarg(request, name, default=None, type=str):\n         return default\n \n \n-class Follow(Resource):\n-\n-    isLeaf = True\n-\n-    def render(self, request):\n-        total = getarg(request, \"total\", 100, type=int)\n-        show = getarg(request, \"show\", 1, type=int)\n-        order = getarg(request, \"order\", \"desc\")\n-        n = getarg(request, \"n\", total, type=int)\n-        if order == \"rand\":\n-            nlist = [random.randint(1, total) for _ in range(show)]\n-        else:  # order == \"desc\"\n-            nlist = range(n, max(n - show, 0), -1)\n-\n-        s = \"\"\"<html> <head></head> <body>\"\"\"\n-        args = request.args.copy()\n-        for nl in nlist:\n-            args[\"n\"] = [str(nl)]\n-            argstr = urllib.urlencode(args, doseq=True)\n-            s += \"<a href='/follow?%s'>follow %d</a><br>\" % (argstr, nl)\n-        s += \"\"\"</body>\"\"\"\n-        return s\n-\n-\n-class DeferMixin(Resource):\n+class DeferMixin(object):\n \n     def deferRequest(self, request, delay, f, *a, **kw):\n         def _cancelrequest(_):\n@@ -49,6 +25,37 @@ class DeferMixin(Resource):\n         return d\n \n \n+class Follow(DeferMixin, Resource):\n+\n+    isLeaf = True\n+\n+    def render(self, request):\n+        total = getarg(request, \"total\", 100, type=int)\n+        show = getarg(request, \"show\", 1, type=int)\n+        order = getarg(request, \"order\", \"desc\")\n+        maxlatency = getarg(request, \"maxlatency\", 0, type=float)\n+        n = getarg(request, \"n\", total, type=int)\n+        if order == \"rand\":\n+            nlist = [random.randint(1, total) for _ in range(show)]\n+        else:  # order == \"desc\"\n+            nlist = range(n, max(n - show, 0), -1)\n+\n+        lag = random.random() * maxlatency\n+        self.deferRequest(request, lag, self.renderRequest, request, nlist)\n+        return NOT_DONE_YET\n+\n+    def renderRequest(self, request, nlist):\n+        s = \"\"\"<html> <head></head> <body>\"\"\"\n+        args = request.args.copy()\n+        for nl in nlist:\n+            args[\"n\"] = [str(nl)]\n+            argstr = urllib.urlencode(args, doseq=True)\n+            s += \"<a href='/follow?%s'>follow %d</a><br>\" % (argstr, nl)\n+        s += \"\"\"</body>\"\"\"\n+        request.write(s)\n+        request.finish()\n+\n+\n class Delay(DeferMixin, Resource):\n \n     isLeaf = True\n\n@@ -3,6 +3,7 @@ Some spiders used for testing and benchmarking\n \"\"\"\n \n import time\n+from urllib import urlencode\n \n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n@@ -27,11 +28,12 @@ class FollowAllSpider(MetaSpider):\n     name = 'follow'\n     link_extractor = SgmlLinkExtractor()\n \n-    def __init__(self, total=10, show=20, order=\"rand\", *args, **kwargs):\n+    def __init__(self, total=10, show=20, order=\"rand\", maxlatency=0.0, *args, **kwargs):\n         super(FollowAllSpider, self).__init__(*args, **kwargs)\n         self.urls_visited = []\n         self.times = []\n-        url = \"http://localhost:8998/follow?total=%s&show=%s&order=%s\" % (total, show, order)\n+        qargs = {'total': total, 'show': show, 'order': order, 'maxlatency': maxlatency}\n+        url = \"http://localhost:8998/follow?%s\" % urlencode(qargs, doseq=1)\n         self.start_urls = [url]\n \n     def parse(self, response):\n\n@@ -11,6 +11,7 @@ def docrawl(spider, settings=None):\n     crawler.crawl(spider)\n     return crawler.start()\n \n+\n class CrawlTestCase(TestCase):\n \n     def setUp(self):\n@@ -28,12 +29,26 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_delay(self):\n-        spider = FollowAllSpider()\n-        yield docrawl(spider, {\"DOWNLOAD_DELAY\": 1})\n-        t = spider.times[0]\n-        for t2 in spider.times[1:]:\n-            self.assertTrue(t2-t > 0.45, \"download delay too small: %s\" % (t2-t))\n-            t = t2\n+        # short to long delays\n+        yield self._test_delay(0.2, False)\n+        yield self._test_delay(1, False)\n+        # randoms\n+        yield self._test_delay(0.2, True)\n+        yield self._test_delay(1, True)\n+\n+    @defer.inlineCallbacks\n+    def _test_delay(self, delay, randomize):\n+        settings = {\"DOWNLOAD_DELAY\": delay, 'RANDOMIZE_DOWNLOAD_DELAY': randomize}\n+        spider = FollowAllSpider(maxlatency=delay * 2)\n+        yield docrawl(spider, settings)\n+        t = spider.times\n+        totaltime = t[-1] - t[0]\n+        avgd = totaltime / (len(t) - 1)\n+        tolerance = 0.6 if randomize else 0.2\n+        self.assertTrue(avgd > delay * (1 - tolerance),\n+                        \"download delay too small: %s\" % avgd)\n+        self.assertTrue(avgd < delay * (1 + tolerance),\n+                        \"download delay too big: %s\" % avgd)\n \n     @defer.inlineCallbacks\n     def test_timeout_success(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d7e11082f290766bf469ba68628f3903ec5f738a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 266 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -47,8 +47,6 @@ class CrawlTestCase(TestCase):\n         tolerance = 0.6 if randomize else 0.2\n         self.assertTrue(avgd > delay * (1 - tolerance),\n                         \"download delay too small: %s\" % avgd)\n-        self.assertTrue(avgd < delay * (1 + tolerance),\n-                        \"download delay too big: %s\" % avgd)\n \n     @defer.inlineCallbacks\n     def test_timeout_success(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ebc136ddb068173f7bc44e8e60e415cef4537f8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 188 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 0.8461538461538461\n\nDIFF:\n@@ -23,6 +23,11 @@ class CookieJar(object):\n         # the cookiejar implementation iterates through all domains\n         # instead we restrict to potential matches on the domain\n         req_host = urlparse_cached(request).netloc\n+\n+        # Strip port numbers from netloc, if present\n+        if ':' in req_host:\n+            req_host = req_host.split(':')[0]\n+\n         if not IPV4_RE.search(req_host):\n             hosts = potential_domain_matches(req_host)\n             if req_host.find(\".\") == -1:\n\n@@ -113,3 +113,19 @@ class CookiesMiddlewareTest(TestCase):\n         req4 = Request('http://scrapytest.org/', meta=res2.meta)\n         assert self.mw.process_request(req4, self.spider) is None\n         self.assertEquals(req4.headers.get('Cookie'), 'C2=value2; galleta=dulce')\n+\n+        #cookies from hosts with port\n+        req5_1 = Request('http://scrapytest.org:1104/')\n+        assert self.mw.process_request(req5_1, self.spider) is None\n+\n+        headers = {'Set-Cookie': 'C1=value1; path=/'}\n+        res5_1 = Response('http://scrapytest.org:1104/', headers=headers, request=req5_1)\n+        assert self.mw.process_response(req5_1, res5_1, self.spider) is res5_1\n+\n+        req5_2 = Request('http://scrapytest.org:1104/some-redirected-path')\n+        assert self.mw.process_request(req5_2, self.spider) is None\n+        self.assertEquals(req5_2.headers.get('Cookie'), 'C1=value1')\n+\n+        req5_3 = Request('http://scrapytest.org/some-redirected-path')\n+        assert self.mw.process_request(req5_3, self.spider) is None\n+        self.assertEquals(req5_3.headers.get('Cookie'), 'C1=value1')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66ff34cf05f5ed1624e8efb39d2b311ffe3ba5f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 99 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,11 +22,7 @@ class CookieJar(object):\n \n         # the cookiejar implementation iterates through all domains\n         # instead we restrict to potential matches on the domain\n-        req_host = urlparse_cached(request).netloc\n-\n-        # Strip port numbers from netloc, if present\n-        if ':' in req_host:\n-            req_host = req_host.split(':')[0]\n+        req_host = urlparse_cached(request).hostname\n \n         if not IPV4_RE.search(req_host):\n             hosts = potential_domain_matches(req_host)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb770852e87d97196e31f27c33ee8eee89aecc27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 201 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 9 | DMM Complexity: 0.6\n\nDIFF:\n@@ -23,6 +23,8 @@ class CookieJar(object):\n         # the cookiejar implementation iterates through all domains\n         # instead we restrict to potential matches on the domain\n         req_host = urlparse_cached(request).hostname\n+        if not req_host:\n+            return\n \n         if not IPV4_RE.search(req_host):\n             hosts = potential_domain_matches(req_host)\n\n@@ -129,3 +129,8 @@ class CookiesMiddlewareTest(TestCase):\n         req5_3 = Request('http://scrapytest.org/some-redirected-path')\n         assert self.mw.process_request(req5_3, self.spider) is None\n         self.assertEquals(req5_3.headers.get('Cookie'), 'C1=value1')\n+\n+        #skip cookie retrieval for not http request\n+        req6 = Request('file:///scrapy/sometempfile')\n+        assert self.mw.process_request(req6, self.spider) is None\n+        self.assertEquals(req6.headers.get('Cookie'), None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d227d530f6b898d6b6d35b539e8804c81d1a3ea3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 245 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,12 +3,19 @@ import zlib\n from scrapy.utils.gz import gunzip\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n+from scrapy.exceptions import NotConfigured\n \n \n class HttpCompressionMiddleware(object):\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n     \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        if not crawler.settings.getbool('COMPRESSION_ENABLED'):\n+            raise NotConfigured\n+        return cls()\n+    \n     def process_request(self, request, spider):\n         request.headers.setdefault('Accept-Encoding', 'x-gzip,gzip,deflate')\n \n\n@@ -26,6 +26,8 @@ CLOSESPIDER_ERRORCOUNT = 0\n \n COMMANDS_MODULE = ''\n \n+COMPRESSION_ENABLED = True\n+\n CONCURRENT_ITEMS = 100\n \n CONCURRENT_REQUESTS = 16\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#915d7cf247a87054a3026be7c12eee6f90edade9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 39 | Churn Cumulative: 380 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 0.5\n\nDIFF:\n@@ -13,6 +13,10 @@ class SitemapSpider(BaseSpider):\n     sitemap_follow = ['']\n \n     def __init__(self, *a, **kw):\n+        self._alternate = False\n+        if 'alternate' in kw and kw.pop('alternate') == True:\n+            self._alternate = True\n+\n         super(SitemapSpider, self).__init__(*a, **kw)\n         self._cbs = []\n         for r, c in self.sitemap_rules:\n@@ -37,7 +41,7 @@ class SitemapSpider(BaseSpider):\n \n             s = Sitemap(body)\n             if s.type == 'sitemapindex':\n-                for loc in iterloc(s):\n+                for loc in iterloc(s, self._alternate):\n                     if any(x.search(loc) for x in self._follow):\n                         yield Request(loc, callback=self._parse_sitemap)\n             elif s.type == 'urlset':\n@@ -65,6 +69,11 @@ def regex(x):\n         return re.compile(x)\n     return x\n \n-def iterloc(it):\n+def iterloc(it, alt=False):\n     for d in it:\n         yield d['loc']\n+\n+        # Also consider alternate URLs (xhtml:link rel=\"alternate\")\n+        if alt == True and 'alternate' in d:\n+            for l in d['alternate']:\n+                yield l\n\n@@ -159,6 +159,27 @@ Disallow: /forum/active/\n             {'lastmod': '2013-07-15', 'loc': 'http://www.example.com/sitemap3.xml'},\n         ])\n \n+    def test_alternate(self):\n+        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n+        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n+        <url>\n+            <loc>http://www.example.com/english/</loc>\n+            <xhtml:link rel=\"alternate\" hreflang=\"de\"\n+                href=\"http://www.example.com/deutsch/\"/>\n+            <xhtml:link rel=\"alternate\" hreflang=\"de-ch\"\n+                href=\"http://www.example.com/schweiz-deutsch/\"/>\n+            <xhtml:link rel=\"alternate\" hreflang=\"en\"\n+                href=\"http://www.example.com/english/\"/>\n+        </url>\n+    </urlset>\"\"\")\n+        \n+        self.assertEqual(list(s), [\n+            {'loc': 'http://www.example.com/english/',\n+             'alternate': ['http://www.example.com/deutsch/', 'http://www.example.com/schweiz-deutsch/', 'http://www.example.com/english/']\n+            }\n+        ])\n+\n \n if __name__ == '__main__':\n     unittest.main()\n\n@@ -23,7 +23,12 @@ class Sitemap(object):\n             for el in elem.getchildren():\n                 tag = el.tag\n                 name = tag.split('}', 1)[1] if '}' in tag else tag\n+\n+                if name == 'link':\n+                    d.setdefault('alternate', []).append(el.get('href'))\n+                else:\n                     d[name] = el.text.strip() if el.text else ''\n+            \n             if 'loc' in d:\n                 yield d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4dc76e7cca5a4e80fc373c4726c1564ed3c0c2a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 38 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ def gunzip(data):\n         try:\n             chunk = f.read(8196)\n             output += chunk\n-        except (IOError, struct.error):\n+        except (IOError, EOFError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n             # see issue 87 about catching struct.error\n             # some pages are quite small so output is '' and f.extrabuf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b43b5f575e6cfc47dd5568df3dab99ff49485692", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 762 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,11 +20,11 @@ from scrapy.utils.misc import load_object\n from scrapy import log\n \n \n-\n class HTTP11DownloadHandler(object):\n \n     def __init__(self, settings):\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n+        self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self._pool._factory.noisy = False\n         self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n         self._contextFactory = self._contextFactoryClass()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#456b6f2ef5875d89b7ede5ab25d3676c1ef93921", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 8 | Complexity Δ (Sum/Max): 13/9 | Churn Δ: 62 | Churn Cumulative: 1167 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ import json\n import cPickle as pickle\n from xml.sax.saxutils import XMLGenerator\n from scrapy.utils.serialize import ScrapyJSONEncoder\n-\n+from scrapy.item import BaseItem\n \n __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter', \\\n     'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter', \\\n@@ -200,7 +200,6 @@ class MarshalItemExporter(BaseItemExporter):\n     def export_item(self, item):\n         marshal.dump(dict(self._get_serialized_fields(item)), self.file)\n \n-\n class PprintItemExporter(BaseItemExporter):\n \n     def __init__(self, file, **kwargs):\n@@ -210,3 +209,25 @@ class PprintItemExporter(BaseItemExporter):\n     def export_item(self, item):\n         itemdict = dict(self._get_serialized_fields(item))\n         self.file.write(pprint.pformat(itemdict) + '\\n')\n+\n+class PythonItemExporter(BaseItemExporter):\n+\n+    def serialize_field(self, field, name, value):\n+        serializer = field.get('serializer', self._serialize_value)\n+        return serializer(value)\n+\n+    def _serialize_value(self, value):\n+        if isinstance(value, BaseItem):\n+            return self.export_item(value)\n+        if isinstance(value, dict):\n+            return dict(self._serialize_dict(value))\n+        if hasattr(value, '__iter__'):\n+            return [self._serialize_value(v) for v in value]\n+        return self._to_str_if_unicode(value)\n+\n+    def _serialize_dict(self, value):\n+        for key, val in value.iteritems():\n+            yield key, self._serialize_value(val)\n+    \n+    def export_item(self, item):\n+        return dict(self._get_serialized_fields(item))\n\n@@ -5,7 +5,7 @@ from scrapy.item import Item, Field\n from scrapy.utils.python import str_to_unicode\n from scrapy.contrib.exporter import BaseItemExporter, PprintItemExporter, \\\n     PickleItemExporter, CsvItemExporter, XmlItemExporter, JsonLinesItemExporter, \\\n-    JsonItemExporter\n+    JsonItemExporter, PythonItemExporter\n \n class TestItem(Item):\n     name = Field()\n@@ -69,6 +69,40 @@ class BaseItemExporterTest(unittest.TestCase):\n         self.assertEqual(ie.serialize_field(i.fields['name'], 'name', i['name']), 'John\\xc2\\xa3')\n         self.assertEqual(ie.serialize_field(i.fields['age'], 'age', i['age']), '24')\n \n+class PythonItemExporterTest(BaseItemExporterTest):\n+    def _get_exporter(self, **kwargs):\n+        return PythonItemExporter(**kwargs)\n+\n+    def test_nested_item(self):\n+        i1 = TestItem(name=u'Joseph', age='22')\n+        i2 = TestItem(name=u'Maria', age=i1)\n+        i3 = TestItem(name=u'Jesus', age=i2)\n+        ie = self._get_exporter()\n+        exported = ie.export_item(i3)\n+        self.assertEqual(type(exported), dict)\n+        self.assertEqual(exported, {'age': {'age': {'age': '22', 'name': u'Joseph'}, 'name': u'Maria'}, 'name': 'Jesus'})\n+        self.assertEqual(type(exported['age']), dict)\n+        self.assertEqual(type(exported['age']['age']), dict)\n+\n+    def test_export_list(self):\n+        i1 = TestItem(name=u'Joseph', age='22')\n+        i2 = TestItem(name=u'Maria', age=[i1])\n+        i3 = TestItem(name=u'Jesus', age=[i2])\n+        ie = self._get_exporter()\n+        exported = ie.export_item(i3)\n+        self.assertEqual(exported, {'age': [{'age': [{'age': '22', 'name': u'Joseph'}], 'name': u'Maria'}], 'name': 'Jesus'})\n+        self.assertEqual(type(exported['age'][0]), dict)\n+        self.assertEqual(type(exported['age'][0]['age'][0]), dict)\n+\n+    def test_export_item_dict_list(self):\n+        i1 = TestItem(name=u'Joseph', age='22')\n+        i2 = dict(name=u'Maria', age=[i1])\n+        i3 = TestItem(name=u'Jesus', age=[i2])\n+        ie = self._get_exporter()\n+        exported = ie.export_item(i3)\n+        self.assertEqual(exported, {'age': [{'age': [{'age': '22', 'name': u'Joseph'}], 'name': u'Maria'}], 'name': 'Jesus'})\n+        self.assertEqual(type(exported['age'][0]), dict)\n+        self.assertEqual(type(exported['age'][0]['age'][0]), dict)\n   \n class PprintItemExporterTest(BaseItemExporterTest):\n \n@@ -78,7 +112,6 @@ class PprintItemExporterTest(BaseItemExporterTest):\n     def _check_output(self):\n         self._assert_expected_item(eval(self.output.getvalue()))\n \n-\n class PickleItemExporterTest(BaseItemExporterTest):\n \n     def _get_exporter(self, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
