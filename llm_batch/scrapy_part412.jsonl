{"custom_id": "scrapy#0ce693dfa91a4ebf8418fd04c5595a570939e480", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 2020 | Contributors (this commit): 23 | Commits (past 90d): 8 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -85,8 +85,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 \"either disable or make universal 1 of those 2 sets of \"\n                 \"spider middlewares. Making a spider middleware universal \"\n                 \"means having it define both methods. See the release notes \"\n-                \"of Scrapy VERSION for details: \"\n-                \"https://docs.scrapy.org/en/VERSION/news.html\"\n+                \"of Scrapy 2.13 for details: \"\n+                \"https://docs.scrapy.org/en/2.13/news.html\"\n             )\n \n         self._use_start_requests = bool(deprecated_middlewares)\n@@ -103,15 +103,15 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 f\"been deprecated in favor of a new method, process_start(), \"\n                 f\"to support asynchronous code execution. \"\n                 f\"process_start_requests() will stop being called in a future \"\n-                f\"version of Scrapy. If you use Scrapy VERSION or higher \"\n+                f\"version of Scrapy. If you use Scrapy 2.13 or higher \"\n                 f\"only, replace process_start_requests() with \"\n                 f\"process_start(); note that process_start() is a coroutine \"\n                 f\"(async def). If you need to maintain compatibility with \"\n                 f\"lower Scrapy versions, when defining \"\n                 f\"process_start_requests() in a spider middleware class, \"\n                 f\"define process_start() as well. See the release notes of \"\n-                f\"Scrapy VERSION for details: \"\n-                f\"https://docs.scrapy.org/en/VERSION/news.html\",\n+                f\"Scrapy 2.13 for details: \"\n+                f\"https://docs.scrapy.org/en/2.13/news.html\",\n                 ScrapyDeprecationWarning,\n             )\n \n@@ -435,15 +435,15 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 f\"start_requests() has been deprecated in favor of a new \"\n                 f\"method, start(), to support asynchronous code \"\n                 f\"execution. start_requests() will stop being called in a \"\n-                f\"future version of Scrapy. If you use Scrapy VERSION or \"\n+                f\"future version of Scrapy. If you use Scrapy 2.13 or \"\n                 f\"higher only, replace start_requests() with start(); \"\n                 f\"note that start() is a coroutine (async def). If you \"\n                 f\"need to maintain compatibility with lower Scrapy versions, \"\n                 f\"when overriding start_requests() in a spider class, \"\n                 f\"override start() as well; you can use super() to \"\n                 f\"reuse the inherited start() implementation without \"\n-                f\"copy-pasting. See the release notes of Scrapy VERSION for \"\n-                f\"details: https://docs.scrapy.org/en/VERSION/news.html\",\n+                f\"copy-pasting. See the release notes of Scrapy 2.13 for \"\n+                f\"details: https://docs.scrapy.org/en/2.13/news.html\",\n                 ScrapyDeprecationWarning,\n             )\n \n@@ -469,8 +469,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 f\"deprecated spider middlewares (and earlier Scrapy versions) \"\n                 f\"by defining a sync start_requests() method that works \"\n                 f\"similarly to its existing start() method. See the \"\n-                f\"release notes of Scrapy VERSION for details: \"\n-                f\"https://docs.scrapy.org/en/VERSION/news.html\"\n+                f\"release notes of Scrapy 2.13 for details: \"\n+                f\"https://docs.scrapy.org/en/2.13/news.html\"\n             )\n \n     # This method is only needed until _async compatibility methods are removed.\n\n@@ -83,7 +83,7 @@ class Spider(object_ref):\n     async def start(self) -> AsyncIterator[Any]:\n         \"\"\"Yield the initial :class:`~scrapy.Request` objects to send.\n \n-        .. versionadded:: VERSION\n+        .. versionadded:: 2.13\n \n         For example:\n \n@@ -115,7 +115,7 @@ class Spider(object_ref):\n             async def start(self):\n                 yield {\"foo\": \"bar\"}\n \n-        To write spiders that work on Scrapy versions lower than VERSION,\n+        To write spiders that work on Scrapy versions lower than 2.13,\n         define also a synchronous ``start_requests()`` method that returns an\n         iterable. For example:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#128cb551eb493601d4a4cd6d7a087e09a07d7092", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 42 | Files Changed: 1 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 114 | Churn Cumulative: 844 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -15,8 +15,7 @@ from scrapy.utils.test import get_crawler\n \n \n class TestBase:\n-    storage_class = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n-    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n+    \"\"\"Base class with common setup and helper methods.\"\"\"\n \n     def setup_method(self):\n         self.yesterday = email.utils.formatdate(time.time() - 86400)\n@@ -90,23 +89,10 @@ class TestBase:\n         )\n         assert request1.body == request2.body\n \n-    def test_dont_cache(self):\n-        with self._middleware() as mw:\n-            self.request.meta[\"dont_cache\"] = True\n-            mw.process_response(self.request, self.response, self.spider)\n-            assert mw.storage.retrieve_response(self.spider, self.request) is None\n \n-        with self._middleware() as mw:\n-            self.request.meta[\"dont_cache\"] = False\n-            mw.process_response(self.request, self.response, self.spider)\n-            if mw.policy.should_cache_response(self.response, self.request):\n-                assert isinstance(\n-                    mw.storage.retrieve_response(self.spider, self.request),\n-                    self.response.__class__,\n-                )\n+class StorageTestMixin:\n+    \"\"\"Mixin containing storage-specific test methods.\"\"\"\n \n-\n-class TestDefaultStorage(TestBase):\n     def test_storage(self):\n         with self._storage() as storage:\n             request2 = self.request.copy()\n@@ -143,31 +129,27 @@ class TestDefaultStorage(TestBase):\n             self.assertEqualResponse(response, cached_response)\n \n \n-class TestDbmStorage(TestDefaultStorage):\n-    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n+class PolicyTestMixin:\n+    \"\"\"Mixin containing policy-specific test methods.\"\"\"\n+\n+    def test_dont_cache(self):\n+        with self._middleware() as mw:\n+            self.request.meta[\"dont_cache\"] = True\n+            mw.process_response(self.request, self.response, self.spider)\n+            assert mw.storage.retrieve_response(self.spider, self.request) is None\n+\n+        with self._middleware() as mw:\n+            self.request.meta[\"dont_cache\"] = False\n+            mw.process_response(self.request, self.response, self.spider)\n+            if mw.policy.should_cache_response(self.response, self.request):\n+                assert isinstance(\n+                    mw.storage.retrieve_response(self.spider, self.request),\n+                    self.response.__class__,\n+                )\n \n \n-class TestDbmStorageWithCustomDbmModule(TestDbmStorage):\n-    dbm_module = \"tests.mocks.dummydbm\"\n-\n-    def _get_settings(self, **new_settings):\n-        new_settings.setdefault(\"HTTPCACHE_DBM_MODULE\", self.dbm_module)\n-        return super()._get_settings(**new_settings)\n-\n-    def test_custom_dbm_module_loaded(self):\n-        # make sure our dbm module has been loaded\n-        with self._storage() as storage:\n-            assert storage.dbmodule.__name__ == self.dbm_module\n-\n-\n-class TestFilesystemStorageGzip(TestDefaultStorage):\n-    def _get_settings(self, **new_settings):\n-        new_settings.setdefault(\"HTTPCACHE_GZIP\", True)\n-        return super()._get_settings(**new_settings)\n-\n-\n-class TestDummyPolicy(TestBase):\n-    policy_class = \"scrapy.extensions.httpcache.DummyPolicy\"\n+class DummyPolicyTestMixin(PolicyTestMixin):\n+    \"\"\"Mixin containing dummy policy specific test methods.\"\"\"\n \n     def test_middleware(self):\n         with self._middleware() as mw:\n@@ -258,8 +240,8 @@ class TestDummyPolicy(TestBase):\n             assert \"cached\" in response.flags\n \n \n-class TestRFC2616Policy(TestDefaultStorage):\n-    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n+class RFC2616PolicyTestMixin(PolicyTestMixin):\n+    \"\"\"Mixin containing RFC2616 policy specific test methods.\"\"\"\n \n     def _process_requestresponse(self, mw, request, response):\n         result = None\n@@ -562,3 +544,51 @@ class TestRFC2616Policy(TestDefaultStorage):\n                 res2 = self._process_requestresponse(mw, req0, None)\n                 self.assertEqualResponse(res1, res2)\n                 assert \"cached\" in res2.flags\n+\n+\n+# Concrete test classes that combine storage and policy mixins\n+\n+\n+class TestFilesystemStorageWithDummyPolicy(\n+    TestBase, StorageTestMixin, DummyPolicyTestMixin\n+):\n+    storage_class = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n+    policy_class = \"scrapy.extensions.httpcache.DummyPolicy\"\n+\n+\n+class TestFilesystemStorageWithRFC2616Policy(\n+    TestBase, StorageTestMixin, RFC2616PolicyTestMixin\n+):\n+    storage_class = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n+    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n+\n+\n+class TestDbmStorageWithDummyPolicy(TestBase, StorageTestMixin, DummyPolicyTestMixin):\n+    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n+    policy_class = \"scrapy.extensions.httpcache.DummyPolicy\"\n+\n+\n+class TestDbmStorageWithRFC2616Policy(\n+    TestBase, StorageTestMixin, RFC2616PolicyTestMixin\n+):\n+    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n+    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n+\n+\n+class TestDbmStorageWithCustomDbmModule(TestDbmStorageWithDummyPolicy):\n+    dbm_module = \"tests.mocks.dummydbm\"\n+\n+    def _get_settings(self, **new_settings):\n+        new_settings.setdefault(\"HTTPCACHE_DBM_MODULE\", self.dbm_module)\n+        return super()._get_settings(**new_settings)\n+\n+    def test_custom_dbm_module_loaded(self):\n+        # make sure our dbm module has been loaded\n+        with self._storage() as storage:\n+            assert storage.dbmodule.__name__ == self.dbm_module\n+\n+\n+class TestFilesystemStorageGzipWithDummyPolicy(TestFilesystemStorageWithDummyPolicy):\n+    def _get_settings(self, **new_settings):\n+        new_settings.setdefault(\"HTTPCACHE_GZIP\", True)\n+        return super()._get_settings(**new_settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b86f00327a3d113a97525f68d38f77529d090f30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 166 | Lines Deleted: 129 | Files Changed: 8 | Hunks: 64 | Methods Changed: 41 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 295 | Churn Cumulative: 14285 | Contributors (this commit): 76 | Commits (past 90d): 29 | Contributors (cumulative): 190 | DMM Complexity: 0.08571428571428572\n\nDIFF:\n@@ -282,6 +282,7 @@ class Command(BaseRunSpiderCommand):\n     ) -> list[Any]:\n         items, requests, opts, depth, spider, callback = args\n         if opts.pipelines:\n+            assert self.pcrawler.engine\n             itemproc = self.pcrawler.engine.scraper.itemproc\n             for item in items:\n                 itemproc.process_item(item, spider)\n\n@@ -5,29 +5,32 @@ import warnings\n from collections import deque\n from datetime import datetime\n from time import time\n-from typing import TYPE_CHECKING, Any, TypeVar, cast\n+from typing import TYPE_CHECKING, Any, cast\n \n from twisted.internet import task\n-from twisted.internet.defer import Deferred\n+from twisted.internet.defer import Deferred, inlineCallbacks\n \n from scrapy import Request, Spider, signals\n from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.resolver import dnscache\n-from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.defer import (\n+    deferred_from_coro,\n+    maybe_deferred_to_future,\n+    mustbe_deferred,\n+)\n from scrapy.utils.httpobj import urlparse_cached\n \n if TYPE_CHECKING:\n+    from collections.abc import Generator\n+\n     from scrapy.crawler import Crawler\n     from scrapy.http import Response\n     from scrapy.settings import BaseSettings\n     from scrapy.signalmanager import SignalManager\n \n \n-_T = TypeVar(\"_T\")\n-\n-\n class Slot:\n     \"\"\"Downloader slot\"\"\"\n \n@@ -114,16 +117,17 @@ class Downloader:\n             \"DOWNLOAD_SLOTS\", {}\n         )\n \n-    def fetch(self, request: Request, spider: Spider) -> Deferred[Response | Request]:\n-        def _deactivate(response: _T) -> _T:\n-            self.active.remove(request)\n-            return response\n-\n+    @inlineCallbacks\n+    def fetch(\n+        self, request: Request, spider: Spider\n+    ) -> Generator[Deferred[Any], Any, Response | Request]:\n         self.active.add(request)\n-        dfd: Deferred[Response | Request] = self.middleware.download(\n-            self._enqueue_request, request, spider\n+        try:\n+            return (\n+                yield self.middleware.download(self._enqueue_request, request, spider)\n             )\n-        return dfd.addBoth(_deactivate)\n+        finally:\n+            self.active.remove(request)\n \n     def needs_backout(self) -> bool:\n         return len(self.active) >= self.total_concurrency\n@@ -164,22 +168,23 @@ class Downloader:\n         )\n         return self.get_slot_key(request)\n \n-    def _enqueue_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n+    @inlineCallbacks\n+    def _enqueue_request(\n+        self, request: Request, spider: Spider\n+    ) -> Generator[Deferred[Any], Any, Response]:\n         key, slot = self._get_slot(request, spider)\n         request.meta[self.DOWNLOAD_SLOT] = key\n-\n-        def _deactivate(response: Response) -> Response:\n-            slot.active.remove(request)\n-            return response\n-\n         slot.active.add(request)\n         self.signals.send_catch_log(\n             signal=signals.request_reached_downloader, request=request, spider=spider\n         )\n-        deferred: Deferred[Response] = Deferred().addBoth(_deactivate)\n-        slot.queue.append((request, deferred))\n+        d: Deferred[Response] = Deferred()\n+        slot.queue.append((request, d))\n         self._process_queue(spider, slot)\n-        return deferred\n+        try:\n+            return (yield d)\n+        finally:\n+            slot.active.remove(request)\n \n     def _process_queue(self, spider: Spider, slot: Slot) -> None:\n         from twisted.internet import reactor\n@@ -202,26 +207,23 @@ class Downloader:\n         while slot.queue and slot.free_transfer_slots() > 0:\n             slot.lastseen = now\n             request, deferred = slot.queue.popleft()\n-            dfd = self._download(slot, request, spider)\n+            dfd = deferred_from_coro(self._download(slot, request, spider))\n             dfd.chainDeferred(deferred)\n             # prevent burst if inter-request delays were configured\n             if delay:\n                 self._process_queue(spider, slot)\n                 break\n \n-    def _download(\n-        self, slot: Slot, request: Request, spider: Spider\n-    ) -> Deferred[Response]:\n-        # The order is very important for the following deferreds. Do not change!\n-\n-        # 1. Create the download deferred\n-        dfd: Deferred[Response] = mustbe_deferred(\n-            self.handlers.download_request, request, spider\n+    async def _download(self, slot: Slot, request: Request, spider: Spider) -> Response:\n+        # The order is very important for the following logic. Do not change!\n+        slot.transferring.add(request)\n+        try:\n+            # 1. Download the response\n+            response: Response = await maybe_deferred_to_future(\n+                mustbe_deferred(self.handlers.download_request, request, spider)\n             )\n-\n             # 2. Notify response_downloaded listeners about the recent download\n             # before querying queue for next request\n-        def _downloaded(response: Response) -> Response:\n             self.signals.send_catch_log(\n                 signal=signals.response_downloaded,\n                 response=response,\n@@ -229,24 +231,16 @@ class Downloader:\n                 spider=spider,\n             )\n             return response\n-\n-        dfd.addCallback(_downloaded)\n-\n+        finally:\n             # 3. After response arrives, remove the request from transferring\n             # state to free up the transferring slot so it can be used by the\n             # following requests (perhaps those which came from the downloader\n             # middleware itself)\n-        slot.transferring.add(request)\n-\n-        def finish_transferring(_: _T) -> _T:\n             slot.transferring.remove(request)\n             self._process_queue(spider, slot)\n             self.signals.send_catch_log(\n                 signal=signals.request_left_downloader, request=request, spider=spider\n             )\n-            return _\n-\n-        return dfd.addBoth(finish_transferring)\n \n     def close(self) -> None:\n         self._slot_gc_loop.stop()\n\n@@ -20,8 +20,6 @@ from scrapy.utils.defer import deferred_from_coro, mustbe_deferred\n if TYPE_CHECKING:\n     from collections.abc import Generator\n \n-    from twisted.python.failure import Failure\n-\n     from scrapy import Spider\n     from scrapy.settings import BaseSettings\n \n@@ -41,12 +39,13 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         if hasattr(mw, \"process_exception\"):\n             self.methods[\"process_exception\"].appendleft(mw.process_exception)\n \n+    @inlineCallbacks\n     def download(\n         self,\n         download_func: Callable[[Request, Spider], Deferred[Response]],\n         request: Request,\n         spider: Spider,\n-    ) -> Deferred[Response | Request]:\n+    ) -> Generator[Deferred[Any], Any, Response | Request]:\n         @inlineCallbacks\n         def process_request(\n             request: Request,\n@@ -92,9 +91,8 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n \n         @inlineCallbacks\n         def process_exception(\n-            failure: Failure,\n-        ) -> Generator[Deferred[Any], Any, Failure | Response | Request]:\n-            exception = failure.value\n+            exception: Exception,\n+        ) -> Generator[Deferred[Any], Any, Response | Request]:\n             for method in self.methods[\"process_exception\"]:\n                 method = cast(Callable, method)\n                 response = yield deferred_from_coro(\n@@ -109,11 +107,12 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                     )\n                 if response:\n                     return response\n-            return failure\n+            raise exception\n \n-        deferred: Deferred[Response | Request] = mustbe_deferred(\n-            process_request, request\n-        )\n-        deferred.addErrback(process_exception)\n-        deferred.addCallback(process_response)\n-        return deferred\n+        try:\n+            result: Response | Request = yield mustbe_deferred(process_request, request)\n+        except Exception as ex:\n+            # either returns a request or response (which we pass to process_response())\n+            # or reraises the exception\n+            result = yield process_exception(ex)\n+        return (yield process_response(result))\n\n@@ -10,7 +10,7 @@ from __future__ import annotations\n import logging\n from time import time\n from traceback import format_exc\n-from typing import TYPE_CHECKING, Any, TypeVar, cast\n+from typing import TYPE_CHECKING, Any, cast\n \n from twisted.internet.defer import Deferred, inlineCallbacks, succeed\n from twisted.internet.task import LoopingCall\n@@ -42,8 +42,6 @@ if TYPE_CHECKING:\n \n logger = logging.getLogger(__name__)\n \n-_T = TypeVar(\"_T\")\n-\n \n class _Slot:\n     def __init__(\n@@ -349,28 +347,32 @@ class ExecutionEngine:\n                 signals.request_dropped, request=request, spider=self.spider\n             )\n \n-    def download(self, request: Request) -> Deferred[Response]:\n+    @inlineCallbacks\n+    def download(self, request: Request) -> Generator[Deferred[Any], Any, Response]:\n         \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n         if self.spider is None:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n-        d: Deferred[Response | Request] = self._download(request)\n-        # Deferred.addBoth() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n-        d2: Deferred[Response] = d.addBoth(self._downloaded, request)  # type: ignore[call-overload]\n-        return d2\n-\n-    def _downloaded(\n-        self, result: Response | Request | Failure, request: Request\n-    ) -> Deferred[Response] | Response | Failure:\n-        assert self._slot is not None  # typing\n+        try:\n+            response_or_request = yield self._download(request)\n+        finally:\n+            assert self._slot is not None\n             self._slot.remove_request(request)\n-        return self.download(result) if isinstance(result, Request) else result\n+        if isinstance(response_or_request, Request):\n+            return (yield self.download(response_or_request))\n+        return response_or_request\n \n-    def _download(self, request: Request) -> Deferred[Response | Request]:\n+    @inlineCallbacks\n+    def _download(\n+        self, request: Request\n+    ) -> Generator[Deferred[Any], Any, Response | Request]:\n         assert self._slot is not None  # typing\n+        assert self.spider is not None\n \n         self._slot.add_request(request)\n-\n-        def _on_success(result: Response | Request) -> Response | Request:\n+        try:\n+            result: Response | Request = yield self.downloader.fetch(\n+                request, self.spider\n+            )\n             if not isinstance(result, (Response, Request)):\n                 raise TypeError(\n                     f\"Incorrect type: expected Response or Request, got {type(result)}: {result!r}\"\n@@ -391,17 +393,8 @@ class ExecutionEngine:\n                     spider=self.spider,\n                 )\n             return result\n-\n-        def _on_complete(_: _T) -> _T:\n-            assert self._slot is not None\n+        finally:\n             self._slot.nextcall.schedule()\n-            return _\n-\n-        assert self.spider is not None\n-        dwld: Deferred[Response | Request] = self.downloader.fetch(request, self.spider)\n-        dwld.addCallback(_on_success)\n-        dwld.addBoth(_on_complete)\n-        return dwld\n \n     @deferred_f_from_coro_f\n     async def open_spider(\n\n@@ -111,11 +111,11 @@ class Scraper:\n         assert crawler.logformatter\n         self.logformatter: LogFormatter = crawler.logformatter\n \n-    @inlineCallbacks\n-    def open_spider(self, spider: Spider) -> Generator[Deferred[Any], Any, None]:\n+    @deferred_f_from_coro_f\n+    async def open_spider(self, spider: Spider) -> None:\n         \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n         self.slot = Slot(self.crawler.settings.getint(\"SCRAPER_SLOT_MAX_ACTIVE_SIZE\"))\n-        yield self.itemproc.open_spider(spider)\n+        await maybe_deferred_to_future(self.itemproc.open_spider(spider))\n \n     def close_spider(self, spider: Spider | None = None) -> Deferred[Spider]:\n         \"\"\"Close a spider being scraped and release its resources\"\"\"\n@@ -191,11 +191,9 @@ class Scraper:\n         if isinstance(result, Response):\n             try:\n                 # call the spider middlewares and the request callback with the response\n-                output = await maybe_deferred_to_future(\n-                    self.spidermw.scrape_response(\n+                output = await self.spidermw.scrape_response_async(\n                     self.call_spider, result, request, self.crawler.spider\n                 )\n-                )\n             except Exception:\n                 self.handle_spider_error(Failure(), request, result)\n             else:\n@@ -363,12 +361,19 @@ class Scraper:\n             self.crawler.engine.crawl(request=output)\n             return\n         if output is not None:\n-            await maybe_deferred_to_future(\n-                self.start_itemproc(output, response=response)\n-            )\n+            await self.start_itemproc_async(output, response=response)\n \n-    @deferred_f_from_coro_f\n-    async def start_itemproc(self, item: Any, *, response: Response | None) -> None:\n+    def start_itemproc(self, item: Any, *, response: Response | None) -> Deferred[None]:\n+        \"\"\"Send *item* to the item pipelines for processing.\n+\n+        *response* is the source of the item data. If the item does not come\n+        from response data, e.g. it was hard-coded, set it to ``None``.\n+        \"\"\"\n+        return deferred_from_coro(self.start_itemproc_async(item, response=response))\n+\n+    async def start_itemproc_async(\n+        self, item: Any, *, response: Response | None\n+    ) -> None:\n         \"\"\"Send *item* to the item pipelines for processing.\n \n         *response* is the source of the item data. If the item does not come\n\n@@ -23,7 +23,6 @@ from scrapy.middleware import MiddlewareManager\n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import (\n-    deferred_f_from_coro_f,\n     deferred_from_coro,\n     maybe_deferred_to_future,\n     mustbe_deferred,\n@@ -169,7 +168,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 exception_result = cast(\n                     Union[Failure, MutableChain[_T]],\n                     self._process_spider_exception(\n-                        response, spider, Failure(ex), exception_processor_index\n+                        response, spider, ex, exception_processor_index\n                     ),\n                 )\n                 if isinstance(exception_result, Failure):\n@@ -185,7 +184,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 exception_result = cast(\n                     Union[Failure, MutableAsyncChain[_T]],\n                     self._process_spider_exception(\n-                        response, spider, Failure(ex), exception_processor_index\n+                        response, spider, ex, exception_processor_index\n                     ),\n                 )\n                 if isinstance(exception_result, Failure):\n@@ -201,13 +200,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         self,\n         response: Response,\n         spider: Spider,\n-        _failure: Failure,\n+        exception: Exception,\n         start_index: int = 0,\n-    ) -> Failure | MutableChain[_T] | MutableAsyncChain[_T]:\n-        exception = _failure.value\n+    ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n         # don't handle _InvalidOutput exception\n         if isinstance(exception, _InvalidOutput):\n-            return _failure\n+            raise exception\n         method_list = islice(\n             self.methods[\"process_spider_exception\"], start_index, None\n         )\n@@ -242,7 +240,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 f\"or an iterable, got {type(result)}\"\n             )\n             raise _InvalidOutput(msg)\n-        return _failure\n+        raise exception\n \n     # This method cannot be made async def, as _process_spider_exception relies on the Deferred result\n     # being available immediately which doesn't work when it's a wrapped coroutine.\n@@ -308,7 +306,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             except Exception as ex:\n                 exception_result: Failure | MutableChain[_T] | MutableAsyncChain[_T] = (\n                     self._process_spider_exception(\n-                        response, spider, Failure(ex), method_index + 1\n+                        response, spider, ex, method_index + 1\n                     )\n                 )\n                 if isinstance(exception_result, Failure):\n@@ -369,24 +367,36 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         request: Request,\n         spider: Spider,\n     ) -> Deferred[MutableChain[_T] | MutableAsyncChain[_T]]:\n+        return deferred_from_coro(\n+            self.scrape_response_async(scrape_func, response, request, spider)\n+        )\n+\n+    async def scrape_response_async(\n+        self,\n+        scrape_func: ScrapeFunc[_T],\n+        response: Response,\n+        request: Request,\n+        spider: Spider,\n+    ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n         async def process_callback_output(\n             result: Iterable[_T] | AsyncIterator[_T],\n         ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n             return await self._process_callback_output(response, spider, result)\n \n         def process_spider_exception(\n-            _failure: Failure,\n-        ) -> Failure | MutableChain[_T] | MutableAsyncChain[_T]:\n-            return self._process_spider_exception(response, spider, _failure)\n+            exception: Exception,\n+        ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n+            return self._process_spider_exception(response, spider, exception)\n \n-        dfd: Deferred[Iterable[_T] | AsyncIterator[_T]] = mustbe_deferred(\n+        try:\n+            it: Iterable[_T] | AsyncIterator[_T] = await maybe_deferred_to_future(\n+                mustbe_deferred(\n                     self._process_spider_input, scrape_func, response, request, spider\n                 )\n-        dfd2: Deferred[MutableChain[_T] | MutableAsyncChain[_T]] = dfd.addCallback(\n-            deferred_f_from_coro_f(process_callback_output)\n             )\n-        dfd2.addErrback(process_spider_exception)\n-        return dfd2\n+            return await process_callback_output(it)\n+        except Exception as ex:\n+            return process_spider_exception(ex)\n \n     async def process_start(self, spider: Spider) -> AsyncIterator[Any] | None:\n         self._check_deprecated_start_requests_use(spider)\n\n@@ -10,7 +10,6 @@ from twisted.internet.defer import (\n     Deferred,\n     DeferredList,\n     inlineCallbacks,\n-    maybeDeferred,\n )\n from zope.interface.verify import verifyClass\n \n@@ -175,7 +174,7 @@ class Crawler:\n         if self.crawling:\n             self.crawling = False\n             assert self.engine\n-            yield maybeDeferred(self.engine.stop)\n+            yield self.engine.stop()\n \n     @staticmethod\n     def _get_component(\n@@ -277,12 +276,6 @@ class CrawlerRunner:\n     process. See :ref:`run-from-script` for an example.\n     \"\"\"\n \n-    crawlers = property(\n-        lambda self: self._crawlers,\n-        doc=\"Set of :class:`crawlers <scrapy.crawler.Crawler>` started by \"\n-        \":meth:`crawl` and managed by this class.\",\n-    )\n-\n     @staticmethod\n     def _get_spider_loader(settings: BaseSettings) -> SpiderLoaderProtocol:\n         \"\"\"Get SpiderLoader instance from settings\"\"\"\n@@ -303,6 +296,12 @@ class CrawlerRunner:\n         self._active: set[Deferred[None]] = set()\n         self.bootstrap_failed = False\n \n+    @property\n+    def crawlers(self) -> set[Crawler]:\n+        \"\"\"Set of :class:`crawlers <scrapy.crawler.Crawler>` started by\n+        :meth:`crawl` and managed by this class.\"\"\"\n+        return self._crawlers\n+\n     def crawl(\n         self,\n         crawler_or_spidercls: type[Spider] | str | Crawler,\n@@ -338,18 +337,19 @@ class CrawlerRunner:\n         crawler = self.create_crawler(crawler_or_spidercls)\n         return self._crawl(crawler, *args, **kwargs)\n \n-    def _crawl(self, crawler: Crawler, *args: Any, **kwargs: Any) -> Deferred[None]:\n+    @inlineCallbacks\n+    def _crawl(\n+        self, crawler: Crawler, *args: Any, **kwargs: Any\n+    ) -> Generator[Deferred[Any], Any, None]:\n         self.crawlers.add(crawler)\n         d = crawler.crawl(*args, **kwargs)\n         self._active.add(d)\n-\n-        def _done(result: _T) -> _T:\n+        try:\n+            yield d\n+        finally:\n             self.crawlers.discard(crawler)\n             self._active.discard(d)\n             self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n-            return result\n-\n-        return d.addBoth(_done)\n \n     def create_crawler(\n         self, crawler_or_spidercls: type[Spider] | str | Crawler\n@@ -501,10 +501,12 @@ class CrawlerProcess(CrawlerRunner):\n             )\n         reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n \n-    def _graceful_stop_reactor(self) -> Deferred[Any]:\n-        d = self.stop()\n-        d.addBoth(self._stop_reactor)\n-        return d\n+    @inlineCallbacks\n+    def _graceful_stop_reactor(self) -> Generator[Deferred[Any], Any, None]:\n+        try:\n+            yield self.stop()\n+        finally:\n+            self._stop_reactor()\n \n     def _stop_reactor(self, _: Any = None) -> None:\n         from twisted.internet import reactor\n\n@@ -131,6 +131,39 @@ class TestResponseFromProcessRequest(TestManagerBase):\n         assert not download_func.called\n \n \n+class TestResponseFromProcessException(TestManagerBase):\n+    \"\"\"Tests middleware returning a response from process_exception.\"\"\"\n+\n+    @deferred_f_from_coro_f\n+    async def test_process_response_called(self):\n+        resp = Response(\"http://example.com/index.html\")\n+        calls = []\n+\n+        def download_func(request, spider):\n+            raise ValueError(\"test\")\n+\n+        class ResponseMiddleware:\n+            def process_response(self, request, response, spider):\n+                calls.append(\"process_response\")\n+                return resp\n+\n+            def process_exception(self, request, exception, spider):\n+                calls.append(\"process_exception\")\n+                return resp\n+\n+        self.mwman._add_middleware(ResponseMiddleware())\n+\n+        req = Request(\"http://example.com/index.html\")\n+        result = await maybe_deferred_to_future(\n+            self.mwman.download(download_func, req, self.spider)\n+        )\n+        assert result is resp\n+        assert calls == [\n+            \"process_exception\",\n+            \"process_response\",\n+        ]\n+\n+\n class TestInvalidOutput(TestManagerBase):\n     @deferred_f_from_coro_f\n     async def test_invalid_process_request(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#82acef30517496d622a80f24adb5b3599e63f64a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 491 | Lines Deleted: 52 | Files Changed: 11 | Hunks: 32 | Methods Changed: 42 | Complexity Δ (Sum/Max): 42/15 | Churn Δ: 543 | Churn Cumulative: 6332 | Contributors (this commit): 47 | Commits (past 90d): 25 | Contributors (cumulative): 87 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,8 @@ collect_ignore = [\n     \"tests/mockserver.py\",\n     \"tests/pipelines.py\",\n     \"tests/spiders.py\",\n+    # contains scripts to be run by tests/test_crawler.py::AsyncCrawlerRunnerSubprocess\n+    *_py_files(\"tests/AsyncCrawlerRunner\"),\n     # contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess\n     *_py_files(\"tests/CrawlerProcess\"),\n     # contains scripts to be run by tests/test_crawler.py::CrawlerRunnerSubprocess\n\n@@ -1,5 +1,6 @@\n from __future__ import annotations\n \n+import asyncio\n import contextlib\n import logging\n import pprint\n@@ -20,6 +21,7 @@ from scrapy.extension import ExtensionManager\n from scrapy.interfaces import ISpiderLoader\n from scrapy.settings import BaseSettings, Settings, overridden_settings\n from scrapy.signalmanager import SignalManager\n+from scrapy.utils.defer import deferred_to_future\n from scrapy.utils.log import (\n     LogCounterHandler,\n     configure_logging,\n@@ -263,19 +265,7 @@ class Crawler:\n         return self._get_component(cls, self.engine.scraper.spidermw.middlewares)\n \n \n-class CrawlerRunner:\n-    \"\"\"\n-    This is a convenient helper class that keeps track of, manages and runs\n-    crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n-\n-    The CrawlerRunner object must be instantiated with a\n-    :class:`~scrapy.settings.Settings` object.\n-\n-    This class shouldn't be needed (since Scrapy is responsible of using it\n-    accordingly) unless writing scripts that manually handle the crawling\n-    process. See :ref:`run-from-script` for an example.\n-    \"\"\"\n-\n+class CrawlerRunnerBase:\n     @staticmethod\n     def _get_spider_loader(settings: BaseSettings) -> SpiderLoaderProtocol:\n         \"\"\"Get SpiderLoader instance from settings\"\"\"\n@@ -293,7 +283,6 @@ class CrawlerRunner:\n         self.settings: Settings = settings\n         self.spider_loader: SpiderLoaderProtocol = self._get_spider_loader(settings)\n         self._crawlers: set[Crawler] = set()\n-        self._active: set[Deferred[None]] = set()\n         self.bootstrap_failed = False\n \n     @property\n@@ -302,6 +291,57 @@ class CrawlerRunner:\n         :meth:`crawl` and managed by this class.\"\"\"\n         return self._crawlers\n \n+    def create_crawler(\n+        self, crawler_or_spidercls: type[Spider] | str | Crawler\n+    ) -> Crawler:\n+        \"\"\"\n+        Return a :class:`~scrapy.crawler.Crawler` object.\n+\n+        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n+        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n+          is constructed for it.\n+        * If ``crawler_or_spidercls`` is a string, this function finds\n+          a spider with this name in a Scrapy project (using spider loader),\n+          then creates a Crawler instance for it.\n+        \"\"\"\n+        if isinstance(crawler_or_spidercls, Spider):\n+            raise ValueError(\n+                \"The crawler_or_spidercls argument cannot be a spider object, \"\n+                \"it must be a spider class (or a Crawler object)\"\n+            )\n+        if isinstance(crawler_or_spidercls, Crawler):\n+            return crawler_or_spidercls\n+        return self._create_crawler(crawler_or_spidercls)\n+\n+    def _create_crawler(self, spidercls: str | type[Spider]) -> Crawler:\n+        if isinstance(spidercls, str):\n+            spidercls = self.spider_loader.load(spidercls)\n+        return Crawler(spidercls, self.settings)\n+\n+    def _stop(self) -> Deferred[Any]:\n+        return DeferredList([c.stop() for c in list(self.crawlers)])\n+\n+\n+class CrawlerRunner(CrawlerRunnerBase):\n+    \"\"\"\n+    This is a convenient helper class that keeps track of, manages and runs\n+    crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n+\n+    The CrawlerRunner object must be instantiated with a\n+    :class:`~scrapy.settings.Settings` object.\n+\n+    This class shouldn't be needed (since Scrapy is responsible of using it\n+    accordingly) unless writing scripts that manually handle the crawling\n+    process. See :ref:`run-from-script` for an example.\n+\n+    This class provides Deferred-based APIs. Use :class:`AsyncCrawlerRunner`\n+    for modern coroutine APIs.\n+    \"\"\"\n+\n+    def __init__(self, settings: dict[str, Any] | Settings | None = None):\n+        super().__init__(settings)\n+        self._active: set[Deferred[None]] = set()\n+\n     def crawl(\n         self,\n         crawler_or_spidercls: type[Spider] | str | Crawler,\n@@ -351,40 +391,13 @@ class CrawlerRunner:\n             self._active.discard(d)\n             self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n \n-    def create_crawler(\n-        self, crawler_or_spidercls: type[Spider] | str | Crawler\n-    ) -> Crawler:\n-        \"\"\"\n-        Return a :class:`~scrapy.crawler.Crawler` object.\n-\n-        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n-        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n-          is constructed for it.\n-        * If ``crawler_or_spidercls`` is a string, this function finds\n-          a spider with this name in a Scrapy project (using spider loader),\n-          then creates a Crawler instance for it.\n-        \"\"\"\n-        if isinstance(crawler_or_spidercls, Spider):\n-            raise ValueError(\n-                \"The crawler_or_spidercls argument cannot be a spider object, \"\n-                \"it must be a spider class (or a Crawler object)\"\n-            )\n-        if isinstance(crawler_or_spidercls, Crawler):\n-            return crawler_or_spidercls\n-        return self._create_crawler(crawler_or_spidercls)\n-\n-    def _create_crawler(self, spidercls: str | type[Spider]) -> Crawler:\n-        if isinstance(spidercls, str):\n-            spidercls = self.spider_loader.load(spidercls)\n-        return Crawler(spidercls, self.settings)\n-\n     def stop(self) -> Deferred[Any]:\n         \"\"\"\n         Stops simultaneously all the crawling jobs taking place.\n \n         Returns a deferred that is fired when they all have ended.\n         \"\"\"\n-        return DeferredList([c.stop() for c in list(self.crawlers)])\n+        return self._stop()\n \n     @inlineCallbacks\n     def join(self) -> Generator[Deferred[Any], Any, None]:\n@@ -398,6 +411,96 @@ class CrawlerRunner:\n             yield DeferredList(self._active)\n \n \n+class AsyncCrawlerRunner(CrawlerRunnerBase):\n+    \"\"\"\n+    This is a convenient helper class that keeps track of, manages and runs\n+    crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n+\n+    The AsyncCrawlerRunner object must be instantiated with a\n+    :class:`~scrapy.settings.Settings` object.\n+\n+    This class shouldn't be needed (since Scrapy is responsible of using it\n+    accordingly) unless writing scripts that manually handle the crawling\n+    process. See :ref:`run-from-script` for an example.\n+\n+    This class provides coroutine APIs. It requires\n+    :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`.\n+    \"\"\"\n+\n+    def __init__(self, settings: dict[str, Any] | Settings | None = None):\n+        super().__init__(settings)\n+        self._active: set[asyncio.Future[None]] = set()\n+\n+    def crawl(\n+        self,\n+        crawler_or_spidercls: type[Spider] | str | Crawler,\n+        *args: Any,\n+        **kwargs: Any,\n+    ) -> asyncio.Future[None]:\n+        \"\"\"\n+        Run a crawler with the provided arguments.\n+\n+        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n+        keeping track of it so it can be stopped later.\n+\n+        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n+        instance, this method will try to create one using this parameter as\n+        the spider class given to it.\n+\n+        Returns a :class:`~asyncio.Future` object which completes when the\n+        crawling is finished.\n+\n+        :param crawler_or_spidercls: already created crawler, or a spider class\n+            or spider's name inside the project to create it\n+        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n+            :class:`~scrapy.spiders.Spider` subclass or string\n+\n+        :param args: arguments to initialize the spider\n+\n+        :param kwargs: keyword arguments to initialize the spider\n+        \"\"\"\n+        if isinstance(crawler_or_spidercls, Spider):\n+            raise ValueError(\n+                \"The crawler_or_spidercls argument cannot be a spider object, \"\n+                \"it must be a spider class (or a Crawler object)\"\n+            )\n+        if not is_asyncio_reactor_installed():\n+            raise RuntimeError(\"AsyncCrawlerRunner requires AsyncioSelectorReactor.\")\n+        crawler = self.create_crawler(crawler_or_spidercls)\n+        return self._crawl(crawler, *args, **kwargs)\n+\n+    def _crawl(\n+        self, crawler: Crawler, *args: Any, **kwargs: Any\n+    ) -> asyncio.Future[None]:\n+        self.crawlers.add(crawler)\n+        future = deferred_to_future(crawler.crawl(*args, **kwargs))\n+        self._active.add(future)\n+\n+        def _done(_: asyncio.Future[None]) -> None:\n+            self.crawlers.discard(crawler)\n+            self._active.discard(future)\n+            self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n+\n+        future.add_done_callback(_done)\n+        return future\n+\n+    async def stop(self) -> None:\n+        \"\"\"\n+        Stops simultaneously all the crawling jobs taking place.\n+\n+        Completes when they all have ended.\n+        \"\"\"\n+        await deferred_to_future(self._stop())\n+\n+    async def join(self) -> None:\n+        \"\"\"\n+        Completes when all managed :attr:`crawlers` have completed their\n+        executions.\n+        \"\"\"\n+        while self._active:\n+            await asyncio.gather(*self._active)\n+\n+\n class CrawlerProcess(CrawlerRunner):\n     \"\"\"\n     A class to run multiple scrapy crawlers in a process simultaneously.\n@@ -458,7 +561,6 @@ class CrawlerProcess(CrawlerRunner):\n             spidercls = self.spider_loader.load(spidercls)\n         init_reactor = not self._initialized_reactor\n         self._initialized_reactor = True\n-        # temporary cast until self.spider_loader is typed\n         return Crawler(spidercls, self.settings, init_reactor=init_reactor)\n \n     def start(\n\n@@ -0,0 +1,28 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import AsyncCrawlerRunner\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+from scrapy.utils.log import configure_logging\n+from scrapy.utils.reactor import install_reactor\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+@deferred_f_from_coro_f\n+async def main(reactor):\n+    configure_logging()\n+    runner = AsyncCrawlerRunner()\n+    runner.crawl(NoRequestsSpider)\n+    runner.crawl(NoRequestsSpider)\n+    await runner.join()\n+\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+react(main)\n\n@@ -0,0 +1,27 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import AsyncCrawlerRunner\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+from scrapy.utils.log import configure_logging\n+from scrapy.utils.reactor import install_reactor\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+@deferred_f_from_coro_f\n+async def main(reactor):\n+    configure_logging()\n+    runner = AsyncCrawlerRunner()\n+    await runner.crawl(NoRequestsSpider)\n+    await runner.crawl(NoRequestsSpider)\n+\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+react(main)\n\n@@ -0,0 +1,26 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import AsyncCrawlerRunner\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+from scrapy.utils.log import configure_logging\n+from scrapy.utils.reactor import install_reactor\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+@deferred_f_from_coro_f\n+async def main(reactor):\n+    configure_logging()\n+    runner = AsyncCrawlerRunner()\n+    await runner.crawl(NoRequestsSpider)\n+\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+react(main)\n\n@@ -0,0 +1,24 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import AsyncCrawlerRunner\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+from scrapy.utils.log import configure_logging\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+@deferred_f_from_coro_f\n+async def main(reactor):\n+    configure_logging()\n+    runner = AsyncCrawlerRunner()\n+    await runner.crawl(NoRequestsSpider)\n+\n+\n+react(main)\n\n@@ -0,0 +1,28 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": None,\n+    }\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+def main(reactor):\n+    configure_logging(\n+        {\"LOG_FORMAT\": \"%(levelname)s: %(message)s\", \"LOG_LEVEL\": \"DEBUG\"}\n+    )\n+    runner = CrawlerRunner()\n+    return runner.crawl(NoRequestsSpider)\n+\n+\n+react(main)\n\n@@ -0,0 +1,26 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+from scrapy.utils.reactor import install_reactor\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+def main(reactor):\n+    configure_logging()\n+    runner = CrawlerRunner()\n+    runner.crawl(NoRequestsSpider)\n+    runner.crawl(NoRequestsSpider)\n+    return runner.join()\n+\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+react(main)\n\n@@ -0,0 +1,27 @@\n+from twisted.internet.defer import inlineCallbacks\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+from scrapy.utils.reactor import install_reactor\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+@inlineCallbacks\n+def main(reactor):\n+    configure_logging()\n+    runner = CrawlerRunner()\n+    yield runner.crawl(NoRequestsSpider)\n+    yield runner.crawl(NoRequestsSpider)\n+\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+react(main)\n\n@@ -0,0 +1,24 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+from scrapy.utils.reactor import install_reactor\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+def main(reactor):\n+    configure_logging()\n+    runner = CrawlerRunner()\n+    return runner.crawl(NoRequestsSpider)\n+\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+react(main)\n\n@@ -18,11 +18,12 @@ from zope.interface.exceptions import MultipleInvalid\n \n import scrapy\n from scrapy import Spider\n-from scrapy.crawler import Crawler, CrawlerProcess, CrawlerRunner\n+from scrapy.crawler import AsyncCrawlerRunner, Crawler, CrawlerProcess, CrawlerRunner\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extensions.throttle import AutoThrottle\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n+from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.test import get_crawler, get_reactor_settings\n@@ -558,6 +559,26 @@ class TestCrawlerRunner(TestBaseCrawler):\n         self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n \n \n+class TestAsyncCrawlerRunner(TestBaseCrawler):\n+    def test_spider_manager_verify_interface(self):\n+        settings = Settings(\n+            {\n+                \"SPIDER_LOADER_CLASS\": SpiderLoaderWithWrongInterface,\n+            }\n+        )\n+        with pytest.raises(MultipleInvalid):\n+            AsyncCrawlerRunner(settings)\n+\n+    def test_crawler_runner_accepts_dict(self):\n+        runner = AsyncCrawlerRunner({\"foo\": \"bar\"})\n+        assert runner.settings[\"foo\"] == \"bar\"\n+        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n+\n+    def test_crawler_runner_accepts_None(self):\n+        runner = AsyncCrawlerRunner()\n+        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n+\n+\n class TestCrawlerProcess(TestBaseCrawler):\n     def test_crawler_process_accepts_dict(self):\n         runner = CrawlerProcess({\"foo\": \"bar\"})\n@@ -587,20 +608,25 @@ class NoRequestsSpider(scrapy.Spider):\n \n @pytest.mark.usefixtures(\"reactor_pytest\")\n class TestCrawlerRunnerHasSpider(unittest.TestCase):\n-    def _runner(self):\n+    @staticmethod\n+    def _runner():\n         return CrawlerRunner(get_reactor_settings())\n \n+    @staticmethod\n+    def _crawl(runner, spider):\n+        return runner.crawl(spider)\n+\n     @inlineCallbacks\n     def test_crawler_runner_bootstrap_successful(self):\n         runner = self._runner()\n-        yield runner.crawl(NoRequestsSpider)\n+        yield self._crawl(runner, NoRequestsSpider)\n         assert not runner.bootstrap_failed\n \n     @inlineCallbacks\n     def test_crawler_runner_bootstrap_successful_for_several(self):\n         runner = self._runner()\n-        yield runner.crawl(NoRequestsSpider)\n-        yield runner.crawl(NoRequestsSpider)\n+        yield self._crawl(runner, NoRequestsSpider)\n+        yield self._crawl(runner, NoRequestsSpider)\n         assert not runner.bootstrap_failed\n \n     @inlineCallbacks\n@@ -608,7 +634,7 @@ class TestCrawlerRunnerHasSpider(unittest.TestCase):\n         runner = self._runner()\n \n         try:\n-            yield runner.crawl(ExceptionSpider)\n+            yield self._crawl(runner, ExceptionSpider)\n         except ValueError:\n             pass\n         else:\n@@ -621,13 +647,13 @@ class TestCrawlerRunnerHasSpider(unittest.TestCase):\n         runner = self._runner()\n \n         try:\n-            yield runner.crawl(ExceptionSpider)\n+            yield self._crawl(runner, ExceptionSpider)\n         except ValueError:\n             pass\n         else:\n             pytest.fail(\"Exception should be raised from spider\")\n \n-        yield runner.crawl(NoRequestsSpider)\n+        yield self._crawl(runner, NoRequestsSpider)\n \n         assert runner.bootstrap_failed\n \n@@ -643,7 +669,7 @@ class TestCrawlerRunnerHasSpider(unittest.TestCase):\n                 Exception,\n                 match=r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\",\n             ):\n-                yield runner.crawl(NoRequestsSpider)\n+                yield self._crawl(runner, NoRequestsSpider)\n         else:\n             CrawlerRunner(\n                 settings={\n@@ -652,6 +678,20 @@ class TestCrawlerRunnerHasSpider(unittest.TestCase):\n             )\n \n \n+@pytest.mark.only_asyncio\n+class TestAsyncCrawlerRunnerHasSpider(TestCrawlerRunnerHasSpider):\n+    @staticmethod\n+    def _runner():\n+        return AsyncCrawlerRunner(get_reactor_settings())\n+\n+    @staticmethod\n+    def _crawl(runner, spider):\n+        return deferred_from_coro(runner.crawl(spider))\n+\n+    def test_crawler_runner_asyncio_enabled_true(self):\n+        pytest.skip(\"This test is only for CrawlerRunner\")\n+\n+\n class ScriptRunnerMixin:\n     script_dir: Path\n \n@@ -923,6 +963,48 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n class TestCrawlerRunnerSubprocess(ScriptRunnerMixin):\n     script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"\n \n+    def test_simple(self):\n+        log = self.run_script(\"simple.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n+        )\n+\n+    def test_explicit_default_reactor(self):\n+        log = self.run_script(\"explicit_default_reactor.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            not in log\n+        )\n+\n+    def test_multi_parallel(self):\n+        log = self.run_script(\"multi_parallel.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n+        )\n+        assert re.search(\n+            r\"Spider opened.+Spider opened.+Closing spider.+Closing spider\",\n+            log,\n+            re.DOTALL,\n+        )\n+\n+    def test_multi_seq(self):\n+        log = self.run_script(\"multi_seq.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n+        )\n+        assert re.search(\n+            r\"Spider opened.+Closing spider.+Spider opened.+Closing spider\",\n+            log,\n+            re.DOTALL,\n+        )\n+\n     def test_response_ip_address(self):\n         log = self.run_script(\"ip_address.py\")\n         assert \"INFO: Spider closed (finished)\" in log\n@@ -939,6 +1021,49 @@ class TestCrawlerRunnerSubprocess(ScriptRunnerMixin):\n         assert \"DEBUG: Using asyncio event loop\" in log\n \n \n+class TestAsyncCrawlerRunnerSubprocess(ScriptRunnerMixin):\n+    script_dir = Path(__file__).parent.resolve() / \"AsyncCrawlerRunner\"\n+\n+    def test_simple(self):\n+        log = self.run_script(\"simple.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n+        )\n+\n+    def test_simple_default_reactor(self):\n+        log = self.run_script(\"simple_default_reactor.py\")\n+        assert \"Spider closed (finished)\" not in log\n+        assert \"RuntimeError: AsyncCrawlerRunner requires AsyncioSelectorReactor\" in log\n+\n+    def test_multi_parallel(self):\n+        log = self.run_script(\"multi_parallel.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n+        )\n+        assert re.search(\n+            r\"Spider opened.+Spider opened.+Closing spider.+Closing spider\",\n+            log,\n+            re.DOTALL,\n+        )\n+\n+    def test_multi_seq(self):\n+        log = self.run_script(\"multi_seq.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n+        )\n+        assert re.search(\n+            r\"Spider opened.+Closing spider.+Spider opened.+Closing spider\",\n+            log,\n+            re.DOTALL,\n+        )\n+\n+\n @pytest.mark.parametrize(\n     (\"settings\", \"items\"),\n     [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
