{"custom_id": "scrapy#8ae418df44b7a107a4abe1a718721742d7e33fc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 303 | Lines Deleted: 300 | Files Changed: 4 | Hunks: 120 | Methods Changed: 82 | Complexity Δ (Sum/Max): -16/0 | Churn Δ: 603 | Churn Cumulative: 5690 | Contributors (this commit): 37 | Commits (past 90d): 13 | Contributors (cumulative): 40 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,6 +9,7 @@ import pytest\n from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n from scrapy.http import Request\n from scrapy.spiders import Spider\n+from scrapy.utils.defer import deferred_f_from_coro_f\n from tests.test_downloader_handlers_http_base import TestHttpBase, TestHttpProxyBase\n \n if TYPE_CHECKING:\n@@ -25,12 +26,11 @@ class HTTP10DownloadHandlerMixin:\n class TestHttp10(HTTP10DownloadHandlerMixin, TestHttpBase):\n     \"\"\"HTTP 1.0 test case\"\"\"\n \n-    def test_protocol(self):\n+    @deferred_f_from_coro_f\n+    async def test_protocol(self):\n         request = Request(self.getURL(\"host\"), method=\"GET\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.protocol)\n-        d.addCallback(self.assertEqual, \"HTTP/1.0\")\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.protocol == \"HTTP/1.0\"\n \n \n class TestHttps10(TestHttp10):\n\n@@ -15,6 +15,10 @@ from twisted.web.http import H2_ENABLED\n \n from scrapy.http import Request\n from scrapy.spiders import Spider\n+from scrapy.utils.defer import (\n+    deferred_f_from_coro_f,\n+    maybe_deferred_to_future,\n+)\n from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.test import get_crawler\n from tests.mockserver import ssl_context_factory\n@@ -50,15 +54,14 @@ class H2DownloadHandlerMixin:\n class TestHttps2(H2DownloadHandlerMixin, TestHttps11Base):\n     HTTP2_DATALOSS_SKIP_REASON = \"Content-Length mismatch raises InvalidBodyLengthError\"\n \n-    def test_protocol(self):\n+    @deferred_f_from_coro_f\n+    async def test_protocol(self):\n         request = Request(self.getURL(\"host\"), method=\"GET\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.protocol)\n-        d.addCallback(self.assertEqual, \"h2\")\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.protocol == \"h2\"\n \n-    @defer.inlineCallbacks\n-    def test_download_with_maxsize_very_large_file(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_with_maxsize_very_large_file(self):\n         from twisted.internet import reactor\n \n         with mock.patch(\"scrapy.core.http2.stream.logger\") as logger:\n@@ -67,8 +70,10 @@ class TestHttps2(H2DownloadHandlerMixin, TestHttps11Base):\n             def check(logger):\n                 logger.error.assert_called_once_with(mock.ANY)\n \n-            d = self.download_request(request, Spider(\"foo\", download_maxsize=1500))\n-            yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+            with pytest.raises((defer.CancelledError, error.ConnectionAborted)):\n+                await self.download_request(\n+                    request, Spider(\"foo\", download_maxsize=1500)\n+                )\n \n             # As the error message is logged in the dataReceived callback, we\n             # have to give a bit of time to the reactor to process the queue\n@@ -76,13 +81,13 @@ class TestHttps2(H2DownloadHandlerMixin, TestHttps11Base):\n             d = defer.Deferred()\n             d.addCallback(check)\n             reactor.callLater(0.1, d.callback, logger)\n-            yield d\n+            await maybe_deferred_to_future(d)\n \n-    @defer.inlineCallbacks\n-    def test_unsupported_scheme(self):\n+    @deferred_f_from_coro_f\n+    async def test_unsupported_scheme(self):\n         request = Request(\"ftp://unsupported.scheme\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        yield self.assertFailure(d, SchemeNotSupported)\n+        with pytest.raises(SchemeNotSupported):\n+            await self.download_request(request, Spider(\"foo\"))\n \n     def test_download_broken_content_cause_data_loss(self, url=\"broken\"):\n         pytest.skip(self.HTTP2_DATALOSS_SKIP_REASON)\n@@ -102,49 +107,43 @@ class TestHttps2(H2DownloadHandlerMixin, TestHttps11Base):\n     def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n         pytest.skip(self.HTTP2_DATALOSS_SKIP_REASON)\n \n-    def test_concurrent_requests_same_domain(self):\n+    @deferred_f_from_coro_f\n+    async def test_concurrent_requests_same_domain(self):\n         spider = Spider(\"foo\")\n \n         request1 = Request(self.getURL(\"file\"))\n-        d1 = self.download_request(request1, spider)\n-        d1.addCallback(lambda r: r.body)\n-        d1.addCallback(self.assertEqual, b\"0123456789\")\n+        response1 = await self.download_request(request1, spider)\n+        assert response1.body == b\"0123456789\"\n \n         request2 = Request(self.getURL(\"echo\"), method=\"POST\")\n-        d2 = self.download_request(request2, spider)\n-        d2.addCallback(lambda r: r.headers[\"Content-Length\"])\n-        d2.addCallback(self.assertEqual, b\"79\")\n-\n-        return defer.DeferredList([d1, d2])\n+        response2 = await self.download_request(request2, spider)\n+        assert response2.headers[\"Content-Length\"] == b\"79\"\n \n     @pytest.mark.xfail(reason=\"https://github.com/python-hyper/h2/issues/1247\")\n-    def test_connect_request(self):\n+    @deferred_f_from_coro_f\n+    async def test_connect_request(self):\n         request = Request(self.getURL(\"file\"), method=\"CONNECT\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"\")\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.body == b\"\"\n \n-    def test_custom_content_length_good(self):\n+    @deferred_f_from_coro_f\n+    async def test_custom_content_length_good(self):\n         request = Request(self.getURL(\"contentlength\"))\n         custom_content_length = str(len(request.body))\n         request.headers[\"Content-Length\"] = custom_content_length\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.text)\n-        d.addCallback(self.assertEqual, custom_content_length)\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.text == custom_content_length\n \n-    def test_custom_content_length_bad(self):\n+    @deferred_f_from_coro_f\n+    async def test_custom_content_length_bad(self):\n         request = Request(self.getURL(\"contentlength\"))\n         actual_content_length = str(len(request.body))\n         bad_content_length = str(len(request.body) + 1)\n         request.headers[\"Content-Length\"] = bad_content_length\n-        log = LogCapture()\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.text)\n-        d.addCallback(self.assertEqual, actual_content_length)\n-        d.addCallback(\n-            lambda _: log.check_present(\n+        with LogCapture() as log:\n+            response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.text == actual_content_length\n+        log.check_present(\n             (\n                 \"scrapy.core.http2.stream\",\n                 \"WARNING\",\n@@ -153,19 +152,15 @@ class TestHttps2(H2DownloadHandlerMixin, TestHttps11Base):\n                 f\"{actual_content_length!r} instead\",\n             )\n         )\n-        )\n-        d.addCallback(lambda _: log.uninstall())\n-        return d\n \n-    def test_duplicate_header(self):\n+    @deferred_f_from_coro_f\n+    async def test_duplicate_header(self):\n         request = Request(self.getURL(\"echo\"))\n         header, value1, value2 = \"Custom-Header\", \"foo\", \"bar\"\n         request.headers.appendlist(header, value1)\n         request.headers.appendlist(header, value2)\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: json.loads(r.text)[\"headers\"][header])\n-        d.addCallback(self.assertEqual, [value1, value2])\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert json.loads(response.text)[\"headers\"][header] == [value1, value2]\n \n \n class Https2WrongHostnameTestCase(H2DownloadHandlerMixin, TestHttpsWrongHostnameBase):\n@@ -222,12 +217,13 @@ class Https2ProxyTestCase(H2DownloadHandlerMixin, TestHttpProxyBase):\n         self.download_handler = build_from_crawler(\n             self.download_handler_cls, get_crawler()\n         )\n-        self.download_request = self.download_handler.download_request\n \n     def getURL(self, path):\n         return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n \n-    @defer.inlineCallbacks\n-    def test_download_with_proxy_https_timeout(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_with_proxy_https_timeout(self):\n         with pytest.raises(NotImplementedError):\n-            yield super().test_download_with_proxy_https_timeout()\n+            await maybe_deferred_to_future(\n+                super().test_download_with_proxy_https_timeout()\n+            )\n\n@@ -22,10 +22,14 @@ from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import HtmlResponse, Request\n+from scrapy.http import HtmlResponse, Request, Response\n from scrapy.http.response.text import TextResponse\n from scrapy.responsetypes import responsetypes\n from scrapy.spiders import Spider\n+from scrapy.utils.defer import (\n+    deferred_f_from_coro_f,\n+    maybe_deferred_to_future,\n+)\n from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n@@ -95,28 +99,33 @@ class TestFile(unittest.TestCase):\n         # add a special char to check that they are handled correctly\n         self.fd, self.tmpname = mkstemp(suffix=\"^\")\n         Path(self.tmpname).write_text(\"0123456789\", encoding=\"utf-8\")\n-        handler = build_from_crawler(FileDownloadHandler, get_crawler())\n-        self.download_request = handler.download_request\n+        self.download_handler = build_from_crawler(FileDownloadHandler, get_crawler())\n \n     def tearDown(self):\n         os.close(self.fd)\n         Path(self.tmpname).unlink()\n \n-    def test_download(self):\n-        def _test(response):\n+    async def download_request(self, request: Request, spider: Spider) -> Response:\n+        return await maybe_deferred_to_future(\n+            self.download_handler.download_request(request, spider)\n+        )\n+\n+    @deferred_f_from_coro_f\n+    async def test_download(self):\n+        request = Request(path_to_file_uri(self.tmpname))\n+        assert request.url.upper().endswith(\"%5E\")\n+        response = await self.download_request(request, Spider(\"foo\"))\n         assert response.url == request.url\n         assert response.status == 200\n         assert response.body == b\"0123456789\"\n         assert response.protocol is None\n \n-        request = Request(path_to_file_uri(self.tmpname))\n-        assert request.url.upper().endswith(\"%5E\")\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_non_existent(self):\n+    @deferred_f_from_coro_f\n+    async def test_non_existent(self):\n         request = Request(path_to_file_uri(mkdtemp()))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        return self.assertFailure(d, OSError)\n+        # the specific exception differs between platforms\n+        with pytest.raises(OSError):  # noqa: PT011\n+            await self.download_request(request, Spider(\"foo\"))\n \n \n class HttpDownloadHandlerMock:\n@@ -479,69 +488,65 @@ class TestDataURI(unittest.TestCase):\n     def setUp(self):\n         crawler = get_crawler()\n         self.download_handler = build_from_crawler(DataURIDownloadHandler, crawler)\n-        self.download_request = self.download_handler.download_request\n         self.spider = Spider(\"foo\")\n \n-    def test_response_attrs(self):\n-        uri = \"data:,A%20brief%20note\"\n+    async def download_request(self, request: Request, spider: Spider) -> Response:\n+        return await maybe_deferred_to_future(\n+            self.download_handler.download_request(request, spider)\n+        )\n \n-        def _test(response):\n+    @deferred_f_from_coro_f\n+    async def test_response_attrs(self):\n+        uri = \"data:,A%20brief%20note\"\n+        request = Request(uri)\n+        response = await self.download_request(request, self.spider)\n         assert response.url == uri\n         assert not response.headers\n \n-        request = Request(uri)\n-        return self.download_request(request, self.spider).addCallback(_test)\n-\n-    def test_default_mediatype_encoding(self):\n-        def _test(response):\n+    @deferred_f_from_coro_f\n+    async def test_default_mediatype_encoding(self):\n+        request = Request(\"data:,A%20brief%20note\")\n+        response = await self.download_request(request, self.spider)\n         assert response.text == \"A brief note\"\n         assert type(response) is responsetypes.from_mimetype(\"text/plain\")  # pylint: disable=unidiomatic-typecheck\n         assert response.encoding == \"US-ASCII\"\n \n-        request = Request(\"data:,A%20brief%20note\")\n-        return self.download_request(request, self.spider).addCallback(_test)\n-\n-    def test_default_mediatype(self):\n-        def _test(response):\n+    @deferred_f_from_coro_f\n+    async def test_default_mediatype(self):\n+        request = Request(\"data:;charset=iso-8859-7,%be%d3%be\")\n+        response = await self.download_request(request, self.spider)\n         assert response.text == \"\\u038e\\u03a3\\u038e\"\n         assert type(response) is responsetypes.from_mimetype(\"text/plain\")  # pylint: disable=unidiomatic-typecheck\n         assert response.encoding == \"iso-8859-7\"\n \n-        request = Request(\"data:;charset=iso-8859-7,%be%d3%be\")\n-        return self.download_request(request, self.spider).addCallback(_test)\n-\n-    def test_text_charset(self):\n-        def _test(response):\n+    @deferred_f_from_coro_f\n+    async def test_text_charset(self):\n+        request = Request(\"data:text/plain;charset=iso-8859-7,%be%d3%be\")\n+        response = await self.download_request(request, self.spider)\n         assert response.text == \"\\u038e\\u03a3\\u038e\"\n         assert response.body == b\"\\xbe\\xd3\\xbe\"\n         assert response.encoding == \"iso-8859-7\"\n \n-        request = Request(\"data:text/plain;charset=iso-8859-7,%be%d3%be\")\n-        return self.download_request(request, self.spider).addCallback(_test)\n-\n-    def test_mediatype_parameters(self):\n-        def _test(response):\n-            assert response.text == \"\\u038e\\u03a3\\u038e\"\n-            assert type(response) is responsetypes.from_mimetype(\"text/plain\")  # pylint: disable=unidiomatic-typecheck\n-            assert response.encoding == \"utf-8\"\n-\n+    @deferred_f_from_coro_f\n+    async def test_mediatype_parameters(self):\n         request = Request(\n             \"data:text/plain;foo=%22foo;bar%5C%22%22;\"\n             \"charset=utf-8;bar=%22foo;%5C%22 foo ;/,%22\"\n             \",%CE%8E%CE%A3%CE%8E\"\n         )\n-        return self.download_request(request, self.spider).addCallback(_test)\n+        response = await self.download_request(request, self.spider)\n+        assert response.text == \"\\u038e\\u03a3\\u038e\"\n+        assert type(response) is responsetypes.from_mimetype(\"text/plain\")  # pylint: disable=unidiomatic-typecheck\n+        assert response.encoding == \"utf-8\"\n \n-    def test_base64(self):\n-        def _test(response):\n+    @deferred_f_from_coro_f\n+    async def test_base64(self):\n+        request = Request(\"data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D\")\n+        response = await self.download_request(request, self.spider)\n         assert response.text == \"Hello, world.\"\n \n-        request = Request(\"data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D\")\n-        return self.download_request(request, self.spider).addCallback(_test)\n-\n-    def test_protocol(self):\n-        def _test(response):\n-            assert response.protocol is None\n-\n+    @deferred_f_from_coro_f\n+    async def test_protocol(self):\n         request = Request(\"data:,\")\n-        return self.download_request(request, self.spider).addCallback(_test)\n+        response = await self.download_request(request, self.spider)\n+        assert response.protocol is None\n\n@@ -2,6 +2,7 @@\n \n from __future__ import annotations\n \n+import json\n import shutil\n import sys\n from abc import ABC, abstractmethod\n@@ -13,14 +14,20 @@ from unittest import mock\n import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer, error\n+from twisted.internet.defer import maybeDeferred\n from twisted.protocols.policies import WrappingFactory\n from twisted.trial import unittest\n from twisted.web import resource, server, static, util\n from twisted.web._newclient import ResponseFailed\n from twisted.web.http import _DataLoss\n \n-from scrapy.http import Headers, HtmlResponse, Request, TextResponse\n+from scrapy.http import Headers, HtmlResponse, Request, Response, TextResponse\n from scrapy.spiders import Spider\n+from scrapy.utils.defer import (\n+    deferred_f_from_coro_f,\n+    deferred_from_coro,\n+    maybe_deferred_to_future,\n+)\n from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n@@ -178,7 +185,6 @@ class TestHttpBase(unittest.TestCase, ABC):\n         self.download_handler = build_from_crawler(\n             self.download_handler_cls, get_crawler()\n         )\n-        self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n     def tearDown(self):\n@@ -190,36 +196,37 @@ class TestHttpBase(unittest.TestCase, ABC):\n     def getURL(self, path):\n         return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n \n-    def test_download(self):\n+    async def download_request(self, request: Request, spider: Spider) -> Response:\n+        return await maybe_deferred_to_future(\n+            self.download_handler.download_request(request, spider)\n+        )\n+\n+    @deferred_f_from_coro_f\n+    async def test_download(self):\n         request = Request(self.getURL(\"file\"))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.body == b\"0123456789\"\n \n-    def test_download_head(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_head(self):\n         request = Request(self.getURL(\"file\"), method=\"HEAD\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"\")\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.body == b\"\"\n \n-    def test_redirect_status(self):\n+    @deferred_f_from_coro_f\n+    async def test_redirect_status(self):\n         request = Request(self.getURL(\"redirect\"))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.status)\n-        d.addCallback(self.assertEqual, 302)\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.status == 302\n \n-    def test_redirect_status_head(self):\n+    @deferred_f_from_coro_f\n+    async def test_redirect_status_head(self):\n         request = Request(self.getURL(\"redirect\"), method=\"HEAD\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.status)\n-        d.addCallback(self.assertEqual, 302)\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.status == 302\n \n-    @defer.inlineCallbacks\n-    def test_timeout_download_from_spider_nodata_rcvd(self):\n+    @deferred_f_from_coro_f\n+    async def test_timeout_download_from_spider_nodata_rcvd(self):\n         if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n             # https://twistedmatrix.com/trac/ticket/10279\n             raise unittest.SkipTest(\n@@ -230,11 +237,12 @@ class TestHttpBase(unittest.TestCase, ABC):\n         spider = Spider(\"foo\")\n         meta = {\"download_timeout\": 0.5}\n         request = Request(self.getURL(\"wait\"), meta=meta)\n-        d = self.download_request(request, spider)\n-        yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n+        d = deferred_from_coro(self.download_request(request, spider))\n+        with pytest.raises((defer.TimeoutError, error.TimeoutError)):\n+            await maybe_deferred_to_future(d)\n \n-    @defer.inlineCallbacks\n-    def test_timeout_download_from_spider_server_hangs(self):\n+    @deferred_f_from_coro_f\n+    async def test_timeout_download_from_spider_server_hangs(self):\n         if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n             # https://twistedmatrix.com/trac/ticket/10279\n             raise unittest.SkipTest(\n@@ -244,28 +252,27 @@ class TestHttpBase(unittest.TestCase, ABC):\n         spider = Spider(\"foo\")\n         meta = {\"download_timeout\": 0.5}\n         request = Request(self.getURL(\"hang-after-headers\"), meta=meta)\n-        d = self.download_request(request, spider)\n-        yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n+        d = deferred_from_coro(self.download_request(request, spider))\n+        with pytest.raises((defer.TimeoutError, error.TimeoutError)):\n+            await maybe_deferred_to_future(d)\n \n-    def test_host_header_not_in_request_headers(self):\n-        def _test(response):\n+    @deferred_f_from_coro_f\n+    async def test_host_header_not_in_request_headers(self):\n+        request = Request(self.getURL(\"host\"))\n+        response = await self.download_request(request, Spider(\"foo\"))\n         assert response.body == to_bytes(f\"{self.host}:{self.portno}\")\n         assert not request.headers\n \n-        request = Request(self.getURL(\"host\"))\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_host_header_seted_in_request_headers(self):\n+    @deferred_f_from_coro_f\n+    async def test_host_header_set_in_request_headers(self):\n         host = self.host + \":\" + str(self.portno)\n-\n-        def _test(response):\n+        request = Request(self.getURL(\"host\"), headers={\"Host\": host})\n+        response = await self.download_request(request, Spider(\"foo\"))\n         assert response.body == host.encode()\n         assert request.headers.get(\"Host\") == host.encode()\n \n-        request = Request(self.getURL(\"host\"), headers={\"Host\": host})\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_content_length_zero_bodyless_post_request_headers(self):\n+    @deferred_f_from_coro_f\n+    async def test_content_length_zero_bodyless_post_request_headers(self):\n         \"\"\"Tests if \"Content-Length: 0\" is sent for bodyless POST requests.\n \n         This is not strictly required by HTTP RFCs but can cause trouble\n@@ -276,105 +283,93 @@ class TestHttpBase(unittest.TestCase, ABC):\n         https://github.com/kennethreitz/requests/issues/405\n         https://bugs.python.org/issue14721\n         \"\"\"\n-\n-        def _test(response):\n+        request = Request(self.getURL(\"contentlength\"), method=\"POST\")\n+        response = await self.download_request(request, Spider(\"foo\"))\n         assert response.body == b\"0\"\n \n-        request = Request(self.getURL(\"contentlength\"), method=\"POST\")\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_content_length_zero_bodyless_post_only_one(self):\n-        def _test(response):\n-            import json\n-\n+    @deferred_f_from_coro_f\n+    async def test_content_length_zero_bodyless_post_only_one(self):\n+        request = Request(self.getURL(\"echo\"), method=\"POST\")\n+        response = await self.download_request(request, Spider(\"foo\"))\n         headers = Headers(json.loads(response.text)[\"headers\"])\n         contentlengths = headers.getlist(\"Content-Length\")\n         assert len(contentlengths) == 1\n         assert contentlengths == [b\"0\"]\n \n-        request = Request(self.getURL(\"echo\"), method=\"POST\")\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_payload(self):\n+    @deferred_f_from_coro_f\n+    async def test_payload(self):\n         body = b\"1\" * 100  # PayloadResource requires body length to be 100\n         request = Request(self.getURL(\"payload\"), method=\"POST\", body=body)\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, body)\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.body == body\n \n-    def test_response_header_content_length(self):\n+    @deferred_f_from_coro_f\n+    async def test_response_header_content_length(self):\n         request = Request(self.getURL(\"file\"), method=b\"GET\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.headers[b\"content-length\"])\n-        d.addCallback(self.assertEqual, b\"159\")\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.headers[b\"content-length\"] == b\"159\"\n \n-    def _test_response_class(self, filename, body, response_class):\n-        def _test(response):\n+    async def _test_response_class(\n+        self, filename: str, body: bytes, response_class: type[Response]\n+    ) -> None:\n+        request = Request(self.getURL(filename), body=body)\n+        response = await self.download_request(request, Spider(\"foo\"))\n         assert type(response) is response_class  # pylint: disable=unidiomatic-typecheck\n \n-        request = Request(self.getURL(filename), body=body)\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+    @deferred_f_from_coro_f\n+    async def test_response_class_from_url(self):\n+        await self._test_response_class(\"foo.html\", b\"\", HtmlResponse)\n \n-    def test_response_class_from_url(self):\n-        return self._test_response_class(\"foo.html\", b\"\", HtmlResponse)\n-\n-    def test_response_class_from_body(self):\n-        return self._test_response_class(\n+    @deferred_f_from_coro_f\n+    async def test_response_class_from_body(self):\n+        await self._test_response_class(\n             \"foo\",\n             b\"<!DOCTYPE html>\\n<title>.</title>\",\n             HtmlResponse,\n         )\n \n-    def test_get_duplicate_header(self):\n-        def _test(response):\n-            assert response.headers.getlist(b\"Set-Cookie\") == [b\"a=b\", b\"c=d\"]\n-\n+    @deferred_f_from_coro_f\n+    async def test_get_duplicate_header(self):\n         request = Request(self.getURL(\"duplicate-header\"))\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.headers.getlist(b\"Set-Cookie\") == [b\"a=b\", b\"c=d\"]\n \n \n class TestHttp11Base(TestHttpBase):\n     \"\"\"HTTP 1.1 test case\"\"\"\n \n-    def test_download_without_maxsize_limit(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_without_maxsize_limit(self):\n         request = Request(self.getURL(\"file\"))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.body == b\"0123456789\"\n \n-    def test_response_class_choosing_request(self):\n+    @deferred_f_from_coro_f\n+    async def test_response_class_choosing_request(self):\n         \"\"\"Tests choosing of correct response type\n         in case of Content-Type is empty but body contains text.\n         \"\"\"\n         body = b\"Some plain text\\ndata with tabs\\t and null bytes\\0\"\n-\n-        def _test_type(response):\n+        request = Request(self.getURL(\"nocontenttype\"), body=body)\n+        response = await self.download_request(request, Spider(\"foo\"))\n         assert type(response) is TextResponse  # pylint: disable=unidiomatic-typecheck\n \n-        request = Request(self.getURL(\"nocontenttype\"), body=body)\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(_test_type)\n-        return d\n-\n-    @defer.inlineCallbacks\n-    def test_download_with_maxsize(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_with_maxsize(self):\n         request = Request(self.getURL(\"file\"))\n \n         # 10 is minimal size for this request and the limit is only counted on\n         # response body. (regardless of headers)\n-        d = self.download_request(request, Spider(\"foo\", download_maxsize=10))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        yield d\n+        response = await self.download_request(\n+            request, Spider(\"foo\", download_maxsize=10)\n+        )\n+        assert response.body == b\"0123456789\"\n \n-        d = self.download_request(request, Spider(\"foo\", download_maxsize=9))\n-        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+        with pytest.raises((defer.CancelledError, error.ConnectionAborted)):\n+            await self.download_request(request, Spider(\"foo\", download_maxsize=9))\n \n-    @defer.inlineCallbacks\n-    def test_download_with_maxsize_very_large_file(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_with_maxsize_very_large_file(self):\n         from twisted.internet import reactor\n \n         # TODO: the logger check is specific to scrapy.core.downloader.handlers.http11\n@@ -384,8 +379,10 @@ class TestHttp11Base(TestHttpBase):\n             def check(logger):\n                 logger.warning.assert_called_once_with(mock.ANY, mock.ANY)\n \n-            d = self.download_request(request, Spider(\"foo\", download_maxsize=1500))\n-            yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+            with pytest.raises((defer.CancelledError, error.ConnectionAborted)):\n+                await self.download_request(\n+                    request, Spider(\"foo\", download_maxsize=1500)\n+                )\n \n             # As the error message is logged in the dataReceived callback, we\n             # have to give a bit of time to the reactor to process the queue\n@@ -393,84 +390,81 @@ class TestHttp11Base(TestHttpBase):\n             d = defer.Deferred()\n             d.addCallback(check)\n             reactor.callLater(0.1, d.callback, logger)\n-            yield d\n+            await maybe_deferred_to_future(d)\n \n-    @defer.inlineCallbacks\n-    def test_download_with_maxsize_per_req(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_with_maxsize_per_req(self):\n         meta = {\"download_maxsize\": 2}\n         request = Request(self.getURL(\"file\"), meta=meta)\n-        d = self.download_request(request, Spider(\"foo\"))\n-        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+        with pytest.raises((defer.CancelledError, error.ConnectionAborted)):\n+            await self.download_request(request, Spider(\"foo\"))\n \n-    @defer.inlineCallbacks\n-    def test_download_with_small_maxsize_per_spider(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_with_small_maxsize_per_spider(self):\n         request = Request(self.getURL(\"file\"))\n-        d = self.download_request(request, Spider(\"foo\", download_maxsize=2))\n-        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+        with pytest.raises((defer.CancelledError, error.ConnectionAborted)):\n+            await self.download_request(request, Spider(\"foo\", download_maxsize=2))\n \n-    def test_download_with_large_maxsize_per_spider(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_with_large_maxsize_per_spider(self):\n         request = Request(self.getURL(\"file\"))\n-        d = self.download_request(request, Spider(\"foo\", download_maxsize=100))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        return d\n+        response = await self.download_request(\n+            request, Spider(\"foo\", download_maxsize=100)\n+        )\n+        assert response.body == b\"0123456789\"\n \n-    def test_download_chunked_content(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_chunked_content(self):\n         request = Request(self.getURL(\"chunked\"))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"chunked content\\n\")\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.body == b\"chunked content\\n\"\n \n-    def test_download_broken_content_cause_data_loss(self, url=\"broken\"):\n+    @deferred_f_from_coro_f\n+    async def test_download_broken_content_cause_data_loss(\n+        self, url: str = \"broken\"\n+    ) -> None:\n         # TODO: this one checks for Twisted-specific exceptions\n         request = Request(self.getURL(url))\n-        d = self.download_request(request, Spider(\"foo\"))\n-\n-        def checkDataLoss(failure):\n-            if failure.check(ResponseFailed) and any(\n-                r.check(_DataLoss) for r in failure.value.reasons\n-            ):\n-                return None\n-            return failure\n-\n-        d.addCallback(lambda _: self.fail(\"No DataLoss exception\"))\n-        d.addErrback(checkDataLoss)\n-        return d\n+        with pytest.raises(ResponseFailed) as exc_info:\n+            await self.download_request(request, Spider(\"foo\"))\n+        assert any(r.check(_DataLoss) for r in exc_info.value.reasons)\n \n     def test_download_broken_chunked_content_cause_data_loss(self):\n         return self.test_download_broken_content_cause_data_loss(\"broken-chunked\")\n \n-    def test_download_broken_content_allow_data_loss(self, url=\"broken\"):\n+    @deferred_f_from_coro_f\n+    async def test_download_broken_content_allow_data_loss(\n+        self, url: str = \"broken\"\n+    ) -> None:\n         request = Request(self.getURL(url), meta={\"download_fail_on_dataloss\": False})\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.flags)\n-        d.addCallback(self.assertEqual, [\"dataloss\"])\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.flags == [\"dataloss\"]\n \n     def test_download_broken_chunked_content_allow_data_loss(self):\n         return self.test_download_broken_content_allow_data_loss(\"broken-chunked\")\n \n-    def test_download_broken_content_allow_data_loss_via_setting(self, url=\"broken\"):\n+    @deferred_f_from_coro_f\n+    async def test_download_broken_content_allow_data_loss_via_setting(\n+        self, url: str = \"broken\"\n+    ) -> None:\n         crawler = get_crawler(settings_dict={\"DOWNLOAD_FAIL_ON_DATALOSS\": False})\n         download_handler = build_from_crawler(self.download_handler_cls, crawler)\n         request = Request(self.getURL(url))\n-        d = download_handler.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.flags)\n-        d.addCallback(self.assertEqual, [\"dataloss\"])\n-        return d\n+        response = await maybe_deferred_to_future(\n+            download_handler.download_request(request, Spider(\"foo\"))\n+        )\n+        assert response.flags == [\"dataloss\"]\n \n     def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n         return self.test_download_broken_content_allow_data_loss_via_setting(\n             \"broken-chunked\"\n         )\n \n-    def test_protocol(self):\n+    @deferred_f_from_coro_f\n+    async def test_protocol(self):\n         request = Request(self.getURL(\"host\"), method=\"GET\")\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.protocol)\n-        d.addCallback(self.assertEqual, \"HTTP/1.1\")\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.protocol == \"HTTP/1.1\"\n \n \n class TestHttps11Base(TestHttp11Base):\n@@ -481,8 +475,8 @@ class TestHttps11Base(TestHttp11Base):\n         'subject \"/C=IE/O=Scrapy/CN=localhost\"'\n     )\n \n-    @defer.inlineCallbacks\n-    def test_tls_logging(self):\n+    @deferred_f_from_coro_f\n+    async def test_tls_logging(self):\n         crawler = get_crawler(\n             settings_dict={\"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\": True}\n         )\n@@ -490,15 +484,15 @@ class TestHttps11Base(TestHttp11Base):\n         try:\n             with LogCapture() as log_capture:\n                 request = Request(self.getURL(\"file\"))\n-                d = download_handler.download_request(request, Spider(\"foo\"))\n-                d.addCallback(lambda r: r.body)\n-                d.addCallback(self.assertEqual, b\"0123456789\")\n-                yield d\n+                response = await maybe_deferred_to_future(\n+                    download_handler.download_request(request, Spider(\"foo\"))\n+                )\n+                assert response.body == b\"0123456789\"\n                 log_capture.check_present(\n                     (\"scrapy.core.downloader.tls\", \"DEBUG\", self.tls_log_message)\n                 )\n         finally:\n-            yield download_handler.close()\n+            await maybe_deferred_to_future(maybeDeferred(download_handler.close))\n \n \n class TestSimpleHttpsBase(unittest.TestCase, ABC):\n@@ -536,7 +530,6 @@ class TestSimpleHttpsBase(unittest.TestCase, ABC):\n             settings_dict = None\n         crawler = get_crawler(settings_dict=settings_dict)\n         self.download_handler = build_from_crawler(self.download_handler_cls, crawler)\n-        self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n     def tearDown(self):\n@@ -548,12 +541,16 @@ class TestSimpleHttpsBase(unittest.TestCase, ABC):\n     def getURL(self, path):\n         return f\"https://{self.host}:{self.portno}/{path}\"\n \n-    def test_download(self):\n+    async def download_request(self, request: Request, spider: Spider) -> Response:\n+        return await maybe_deferred_to_future(\n+            self.download_handler.download_request(request, spider)\n+        )\n+\n+    @deferred_f_from_coro_f\n+    async def test_download(self):\n         request = Request(self.getURL(\"file\"))\n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        return d\n+        response = await self.download_request(request, Spider(\"foo\"))\n+        assert response.body == b\"0123456789\"\n \n \n class TestHttpsWrongHostnameBase(TestSimpleHttpsBase):\n@@ -604,26 +601,30 @@ class TestHttpMockServerBase(unittest.TestCase, ABC):\n     def tearDownClass(cls):\n         cls.mockserver.__exit__(None, None, None)\n \n-    @defer.inlineCallbacks\n-    def test_download_with_content_length(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_with_content_length(self):\n         crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n         # http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid\n         # download it\n-        yield crawler.crawl(\n+        await maybe_deferred_to_future(\n+            crawler.crawl(\n                 seed=Request(\n                     url=self.mockserver.url(\"/partial\", is_secure=self.is_secure),\n                     meta={\"download_maxsize\": 1000},\n                 )\n             )\n+        )\n         failure = crawler.spider.meta[\"failure\"]\n         assert isinstance(failure.value, defer.CancelledError)\n \n-    @defer.inlineCallbacks\n-    def test_download(self):\n+    @deferred_f_from_coro_f\n+    async def test_download(self):\n         crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n-        yield crawler.crawl(\n+        await maybe_deferred_to_future(\n+            crawler.crawl(\n                 seed=Request(url=self.mockserver.url(\"\", is_secure=self.is_secure))\n             )\n+        )\n         failure = crawler.spider.meta.get(\"failure\")\n         assert failure is None\n         reason = crawler.spider.meta[\"close_reason\"]\n@@ -663,7 +664,6 @@ class TestHttpProxyBase(unittest.TestCase, ABC):\n         self.download_handler = build_from_crawler(\n             self.download_handler_cls, get_crawler()\n         )\n-        self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n     def tearDown(self):\n@@ -674,42 +674,44 @@ class TestHttpProxyBase(unittest.TestCase, ABC):\n     def getURL(self, path):\n         return f\"http://127.0.0.1:{self.portno}/{path}\"\n \n-    def test_download_with_proxy(self):\n-        def _test(response):\n+    async def download_request(self, request: Request, spider: Spider) -> Response:\n+        return await maybe_deferred_to_future(\n+            self.download_handler.download_request(request, spider)\n+        )\n+\n+    @deferred_f_from_coro_f\n+    async def test_download_with_proxy(self):\n+        http_proxy = self.getURL(\"\")\n+        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n+        response = await self.download_request(request, Spider(\"foo\"))\n         assert response.status == 200\n         assert response.url == request.url\n         assert response.body == self.expected_http_proxy_request_body\n \n-        http_proxy = self.getURL(\"\")\n-        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    def test_download_without_proxy(self):\n-        def _test(response):\n+    @deferred_f_from_coro_f\n+    async def test_download_without_proxy(self):\n+        request = Request(self.getURL(\"path/to/resource\"))\n+        response = await self.download_request(request, Spider(\"foo\"))\n         assert response.status == 200\n         assert response.url == request.url\n         assert response.body == b\"/path/to/resource\"\n \n-        request = Request(self.getURL(\"path/to/resource\"))\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n-\n-    @defer.inlineCallbacks\n-    def test_download_with_proxy_https_timeout(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_with_proxy_https_timeout(self):\n         if NON_EXISTING_RESOLVABLE:\n             pytest.skip(\"Non-existing hosts are resolvable\")\n         http_proxy = self.getURL(\"\")\n         domain = \"https://no-such-domain.nosuch\"\n         request = Request(domain, meta={\"proxy\": http_proxy, \"download_timeout\": 0.2})\n-        d = self.download_request(request, Spider(\"foo\"))\n-        timeout = yield self.assertFailure(d, error.TimeoutError)\n-        assert domain in timeout.osError\n+        with pytest.raises(error.TimeoutError) as exc_info:\n+            await self.download_request(request, Spider(\"foo\"))\n+        assert domain in exc_info.value.osError\n \n-    def test_download_with_proxy_without_http_scheme(self):\n-        def _test(response):\n+    @deferred_f_from_coro_f\n+    async def test_download_with_proxy_without_http_scheme(self):\n+        http_proxy = self.getURL(\"\").replace(\"http://\", \"\")\n+        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n+        response = await self.download_request(request, Spider(\"foo\"))\n         assert response.status == 200\n         assert response.url == request.url\n         assert response.body == self.expected_http_proxy_request_body\n-\n-        http_proxy = self.getURL(\"\").replace(\"http://\", \"\")\n-        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n-        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9cc23641ccc988d1f623902397496db5d9fe499b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 77 | Churn Cumulative: 3007 | Contributors (this commit): 43 | Commits (past 90d): 8 | Contributors (cumulative): 68 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,6 @@\n # For the full list of built-in configuration values, see the documentation:\n # https://www.sphinx-doc.org/en/master/usage/configuration.html\n \n-# pylint: disable=import-error\n import os\n import sys\n from collections.abc import Sequence\n\n@@ -8,6 +8,7 @@ See documentation in docs/topics/spiders.rst\n from __future__ import annotations\n \n import copy\n+import warnings\n from collections.abc import AsyncIterator, Awaitable, Callable\n from typing import TYPE_CHECKING, Any, Optional, TypeVar, cast\n \n@@ -18,6 +19,8 @@ from scrapy.link import Link\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import Spider\n from scrapy.utils.asyncgen import collect_asyncgen\n+from scrapy.utils.deprecate import method_is_overridden\n+from scrapy.utils.python import global_object_name\n from scrapy.utils.spider import iterate_spider_output\n \n if TYPE_CHECKING:\n@@ -95,9 +98,17 @@ class CrawlSpider(Spider):\n     def __init__(self, *a: Any, **kw: Any):\n         super().__init__(*a, **kw)\n         self._compile_rules()\n+        if method_is_overridden(self.__class__, CrawlSpider, \"_parse_response\"):\n+            warnings.warn(\n+                f\"The CrawlSpider._parse_response method, which the \"\n+                f\"{global_object_name(self.__class__)} class overrides, is \"\n+                f\"deprecated: it will be removed in future Scrapy releases. \"\n+                f\"Please override the CrawlSpider.parse_with_rules method \"\n+                f\"instead.\"\n+            )\n \n     def _parse(self, response: Response, **kwargs: Any) -> Any:\n-        return self._parse_response(\n+        return self.parse_with_rules(\n             response=response,\n             callback=self.parse_start_url,\n             cb_kwargs=kwargs,\n@@ -137,7 +148,7 @@ class CrawlSpider(Spider):\n \n     def _callback(self, response: Response, **cb_kwargs: Any) -> Any:\n         rule = self._rules[cast(int, response.meta[\"rule\"])]\n-        return self._parse_response(\n+        return self.parse_with_rules(\n             response,\n             cast(\"CallbackT\", rule.callback),\n             {**rule.cb_kwargs, **cb_kwargs},\n@@ -150,7 +161,7 @@ class CrawlSpider(Spider):\n             failure, cast(Callable[[Failure], Any], rule.errback)\n         )\n \n-    async def _parse_response(\n+    async def parse_with_rules(\n         self,\n         response: Response,\n         callback: CallbackT | None,\n@@ -171,6 +182,21 @@ class CrawlSpider(Spider):\n             for request_or_item in self._requests_to_follow(response):\n                 yield request_or_item\n \n+    def _parse_response(\n+        self,\n+        response: Response,\n+        callback: CallbackT | None,\n+        cb_kwargs: dict[str, Any],\n+        follow: bool = True,\n+    ) -> AsyncIterator[Any]:\n+        warnings.warn(\n+            \"The CrawlSpider._parse_response method is deprecated: \"\n+            \"it will be removed in future Scrapy releases. \"\n+            \"Please use the CrawlSpider.parse_with_rules method instead.\",\n+            stacklevel=2,\n+        )\n+        return self.parse_with_rules(response, callback, cb_kwargs, follow)\n+\n     def _handle_failure(\n         self, failure: Failure, errback: Callable[[Failure], Any] | None\n     ) -> Iterable[Any]:\n\n@@ -476,6 +476,50 @@ class TestCrawlSpider(TestSpider):\n         assert \"Error while reading start items and requests\" in str(log)\n         assert \"did you miss an 's'?\" in str(log)\n \n+    def test_parse_response_use(self):\n+        class _CrawlSpider(CrawlSpider):\n+            name = \"test\"\n+            start_urls = \"https://www.example.com\"\n+            _follow_links = False\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            spider = _CrawlSpider()\n+            assert len(w) == 0\n+            spider._parse_response(\n+                TextResponse(spider.start_urls, body=b\"\"), None, None\n+            )\n+            assert len(w) == 1\n+\n+    def test_parse_response_override(self):\n+        class _CrawlSpider(CrawlSpider):\n+            def _parse_response(self, response, callback, cb_kwargs, follow=True):\n+                pass\n+\n+            name = \"test\"\n+            start_urls = \"https://www.example.com\"\n+            _follow_links = False\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            assert len(w) == 0\n+            spider = _CrawlSpider()\n+            assert len(w) == 1\n+            spider._parse_response(\n+                TextResponse(spider.start_urls, body=b\"\"), None, None\n+            )\n+            assert len(w) == 1\n+\n+    def test_parse_with_rules(self):\n+        class _CrawlSpider(CrawlSpider):\n+            name = \"test\"\n+            start_urls = \"https://www.example.com\"\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            spider = _CrawlSpider()\n+            spider.parse_with_rules(\n+                TextResponse(spider.start_urls, body=b\"\"), None, None\n+            )\n+            assert len(w) == 0\n+\n \n class TestSitemapSpider(TestSpider):\n     spider_class = SitemapSpider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
