{"custom_id": "scrapy#3f03a2ca509b7e8961ced2f5f98bf162077bf03f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 2059 | Contributors (this commit): 7 | Commits (past 90d): 18 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,7 +79,8 @@ class RFC2616Policy(object):\n \n     def is_cached_response_fresh(self, cachedresponse, request):\n         cc = self._parse_cachecontrol(cachedresponse)\n-        if 'no-cache' in cc:\n+        ccreq = self._parse_cachecontrol(request)\n+        if 'no-cache' in cc or 'no-cache' in ccreq:\n             return False\n \n         now = time()\n\n@@ -241,15 +241,27 @@ class RFC2616MiddlewareTest(DefaultStorageTest):\n                         headers={'Expires': self.tomorrow})\n         req0 = Request('http://example.com')\n         req1 = req0.replace(headers={'Cache-Control': 'no-store'})\n+        req2 = req0.replace(headers={'Cache-Control': 'no-cache'})\n         with self._middleware() as mw:\n+            # response for a request with no-store must not be cached\n             res1 = self._process_requestresponse(mw, req1, res0)\n             self.assertEqualResponse(res1, res0)\n             assert mw.storage.retrieve_response(self.spider, req1) is None\n+            # Re-do request without no-store and expect it to be cached\n             res2 = self._process_requestresponse(mw, req0, res0)\n             assert 'cached' not in res2.flags\n             res3 = mw.process_request(req0, self.spider)\n             assert 'cached' in res3.flags\n             self.assertEqualResponse(res2, res3)\n+            # request with no-cache directive must not return cached response\n+            # but it allows new response to be stored\n+            res0b = res0.replace(body='foo')\n+            res4 = self._process_requestresponse(mw, req2, res0b)\n+            self.assertEqualResponse(res4, res0b)\n+            assert 'cached' not in res4.flags\n+            res5 = self._process_requestresponse(mw, req0, None)\n+            self.assertEqualResponse(res5, res0b)\n+            assert 'cached' in res5.flags\n \n     def test_response_cacheability(self):\n         responses = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe5d0ce2e0605106eba76aab2e8e011097f0029d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 84 | Churn Cumulative: 84 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,84 @@\n+from twisted.trial.unittest import TestCase\n+from twisted.python.failure import Failure\n+\n+from scrapy.http import Request, Response\n+from scrapy.spider import BaseSpider\n+from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n+from scrapy.utils.test import get_crawler\n+from scrapy.stats import stats\n+\n+\n+class ManagerTestCase(TestCase):\n+\n+    settings_dict = None\n+\n+    def setUp(self):\n+        self.crawler = get_crawler(self.settings_dict)\n+        self.spider = BaseSpider('foo')\n+        self.spider.set_crawler(self.crawler)\n+        self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n+        # some mw depends on stats collector\n+        stats.open_spider(self.spider)\n+        return self.mwman.open_spider(self.spider)\n+\n+    def tearDown(self):\n+        stats.close_spider(self.spider, '')\n+        return self.mwman.close_spider(self.spider)\n+\n+    def _download(self,request, response=None):\n+        \"\"\"Executes downloader mw manager's download method and returns\n+        the result (Request or Response) or raise exception in case of\n+        failure.\n+        \"\"\"\n+        if not response:\n+            response = Response(request.url, request=request)\n+        def download_func(**kwargs):\n+            return response\n+        dfd = self.mwman.download(download_func, request, self.spider)\n+        # catch deferred result and return the value\n+        results = []\n+        dfd.addBoth(results.append)\n+        self._wait(dfd)\n+        ret = results[0]\n+        if isinstance(ret, Failure):\n+            ret.raiseException()\n+        return ret\n+\n+\n+class DefaultsTest(ManagerTestCase):\n+    \"\"\"Tests default behavior with default settings\"\"\"\n+\n+    def test_request_response(self):\n+        req = Request('http://example.com/index.html')\n+        resp = Response(req.url, status=200, request=req)\n+        ret = self._download(req, resp)\n+        self.assertTrue(isinstance(resp, Response), \"Non-response returned\")\n+\n+\n+class GzippedRedirectionTest(ManagerTestCase):\n+    \"\"\"Regression test for a failure when redirecting a compressed\n+    request.\n+\n+    This happens when httpcompression middleware is executed before redirect\n+    middleware and attempts to decompress a non-compressed body.\n+    In particular when some website returns a 30x response with header\n+    'Content-Encoding: gzip' giving as result the error below:\n+\n+        exceptions.IOError: Not a gzipped file\n+\n+    \"\"\"\n+\n+    def test_gzipped_redirection(self):\n+        req = Request('http://example.com')\n+        body = '<p>You are being redirected</p>'\n+        resp = Response(req.url, status=302, body=body, request=req, headers={\n+            'Content-Length': len(body),\n+            'Content-Type': 'text/html',\n+            'Content-Encoding': 'gzip',\n+            'Location': 'http://example.com/login',\n+        })\n+        ret = self._download(request=req, response=resp)\n+        self.assertTrue(isinstance(ret, Request),\n+                        \"Not redirected: {0!r}\".format(ret))\n+        self.assertEqual(ret.url, resp.headers['Location'],\n+                         \"Not redirected to location header\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#71db7f1b25dc6c147273fff5c2fc2adc43bfd7e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 153 | Lines Deleted: 86 | Files Changed: 4 | Hunks: 22 | Methods Changed: 18 | Complexity Δ (Sum/Max): 12/6 | Churn Δ: 239 | Churn Cumulative: 763 | Contributors (this commit): 10 | Commits (past 90d): 8 | Contributors (cumulative): 17 | DMM Complexity: 0.8958333333333334\n\nDIFF:\n@@ -6,13 +6,12 @@ from scrapy.utils.response import get_meta_refresh\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n \n-class RedirectMiddleware(object):\n-    \"\"\"Handle redirection of requests based on response status and meta-refresh html tag\"\"\"\n+class BaseRedirectMiddleware(object):\n \n     def __init__(self, settings):\n         if not settings.getbool('REDIRECT_ENABLED'):\n             raise NotConfigured\n-        self.max_metarefresh_delay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY')\n+\n         self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')\n         self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')\n \n@@ -20,35 +19,6 @@ class RedirectMiddleware(object):\n     def from_crawler(cls, crawler):\n         return cls(crawler.settings)\n \n-    def process_response(self, request, response, spider):\n-        if 'dont_redirect' in request.meta:\n-            return response\n-        if request.method.upper() == 'HEAD':\n-            if response.status in [301, 302, 303, 307] and 'Location' in response.headers:\n-                redirected_url = urljoin(request.url, response.headers['location'])\n-                redirected = request.replace(url=redirected_url)\n-                return self._redirect(redirected, request, spider, response.status)\n-            else:\n-                return response\n-\n-        if response.status in [302, 303] and 'Location' in response.headers:\n-            redirected_url = urljoin(request.url, response.headers['location'])\n-            redirected = self._redirect_request_using_get(request, redirected_url)\n-            return self._redirect(redirected, request, spider, response.status)\n-\n-        if response.status in [301, 307] and 'Location' in response.headers:\n-            redirected_url = urljoin(request.url, response.headers['location'])\n-            redirected = request.replace(url=redirected_url)\n-            return self._redirect(redirected, request, spider, response.status)\n-\n-        if isinstance(response, HtmlResponse):\n-            interval, url = get_meta_refresh(response)\n-            if url and interval < self.max_metarefresh_delay:\n-                redirected = self._redirect_request_using_get(request, url)\n-                return self._redirect(redirected, request, spider, 'meta refresh')\n-\n-        return response\n-\n     def _redirect(self, redirected, request, spider, reason):\n         ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)\n         redirects = request.meta.get('redirect_times', 0) + 1\n@@ -76,3 +46,49 @@ class RedirectMiddleware(object):\n         return redirected\n \n \n+class RedirectMiddleware(BaseRedirectMiddleware):\n+    \"\"\"Handle redirection of requests based on response status and meta-refresh html tag\"\"\"\n+\n+    def process_response(self, request, response, spider):\n+        if 'dont_redirect' in request.meta:\n+            return response\n+\n+        if request.method == 'HEAD':\n+            if response.status in [301, 302, 303, 307] and 'Location' in response.headers:\n+                redirected_url = urljoin(request.url, response.headers['location'])\n+                redirected = request.replace(url=redirected_url)\n+                return self._redirect(redirected, request, spider, response.status)\n+            else:\n+                return response\n+\n+        if response.status in [302, 303] and 'Location' in response.headers:\n+            redirected_url = urljoin(request.url, response.headers['location'])\n+            redirected = self._redirect_request_using_get(request, redirected_url)\n+            return self._redirect(redirected, request, spider, response.status)\n+\n+        if response.status in [301, 307] and 'Location' in response.headers:\n+            redirected_url = urljoin(request.url, response.headers['location'])\n+            redirected = request.replace(url=redirected_url)\n+            return self._redirect(redirected, request, spider, response.status)\n+\n+        return response\n+\n+\n+class MetaRefreshMiddleware(BaseRedirectMiddleware):\n+\n+    def __init__(self, settings):\n+        super(MetaRefreshMiddleware, self).__init__(settings)\n+        self.max_metarefresh_delay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY')\n+\n+    def process_response(self, request, response, spider):\n+        if 'dont_redirect' in request.meta or request.method == 'HEAD' or \\\n+                not isinstance(response, HtmlResponse):\n+            return response\n+\n+        if isinstance(response, HtmlResponse):\n+            interval, url = get_meta_refresh(response)\n+            if url and interval < self.max_metarefresh_delay:\n+                redirected = self._redirect_request_using_get(request, url)\n+                return self._redirect(redirected, request, spider, 'meta refresh')\n+\n+        return response\n\n@@ -74,10 +74,11 @@ DOWNLOADER_MIDDLEWARES_BASE = {\n     'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n     'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n+    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n+    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n     'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n     'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n-    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 800,\n     'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n     'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n     'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n\n@@ -5,7 +5,6 @@ from scrapy.http import Request, Response\n from scrapy.spider import BaseSpider\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.utils.test import get_crawler\n-from scrapy.stats import stats\n \n \n class ManagerTestCase(TestCase):\n@@ -18,11 +17,11 @@ class ManagerTestCase(TestCase):\n         self.spider.set_crawler(self.crawler)\n         self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n         # some mw depends on stats collector\n-        stats.open_spider(self.spider)\n+        self.crawler.stats.open_spider(self.spider)\n         return self.mwman.open_spider(self.spider)\n \n     def tearDown(self):\n-        stats.close_spider(self.spider, '')\n+        self.crawler.stats.close_spider(self.spider, '')\n         return self.mwman.close_spider(self.spider)\n \n     def _download(self, request, response=None):\n@@ -31,9 +30,11 @@ class ManagerTestCase(TestCase):\n         failure.\n         \"\"\"\n         if not response:\n-            response = Response(request.url, request=request)\n+            response = Response(request.url)\n+\n         def download_func(**kwargs):\n             return response\n+\n         dfd = self.mwman.download(download_func, request, self.spider)\n         # catch deferred result and return the value\n         results = []\n@@ -50,12 +51,11 @@ class DefaultsTest(ManagerTestCase):\n \n     def test_request_response(self):\n         req = Request('http://example.com/index.html')\n-        resp = Response(req.url, status=200, request=req)\n+        resp = Response(req.url, status=200)\n         ret = self._download(req, resp)\n-        self.assertTrue(isinstance(resp, Response), \"Non-response returned\")\n+        self.assertTrue(isinstance(ret, Response), \"Non-response returned\")\n \n-\n-class GzippedRedirectionTest(ManagerTestCase):\n+    def test_3xx_and_invalid_gzipped_body_must_redirect(self):\n         \"\"\"Regression test for a failure when redirecting a compressed\n         request.\n \n@@ -67,11 +67,9 @@ class GzippedRedirectionTest(ManagerTestCase):\n             exceptions.IOError: Not a gzipped file\n \n         \"\"\"\n-\n-    def test_gzipped_redirection(self):\n         req = Request('http://example.com')\n         body = '<p>You are being redirected</p>'\n-        resp = Response(req.url, status=302, body=body, request=req, headers={\n+        resp = Response(req.url, status=302, body=body, headers={\n             'Content-Length': len(body),\n             'Content-Type': 'text/html',\n             'Content-Encoding': 'gzip',\n@@ -82,3 +80,14 @@ class GzippedRedirectionTest(ManagerTestCase):\n                         \"Not redirected: {0!r}\".format(ret))\n         self.assertEqual(ret.url, resp.headers['Location'],\n                          \"Not redirected to location header\")\n+\n+    def test_200_and_invalid_gzipped_body_must_fail(self):\n+        req = Request('http://example.com')\n+        body = '<p>You are being redirected</p>'\n+        resp = Response(req.url, status=200, body=body, headers={\n+            'Content-Length': len(body),\n+            'Content-Type': 'text/html',\n+            'Content-Encoding': 'gzip',\n+            'Location': 'http://example.com/login',\n+        })\n+        self.assertRaises(IOError, self._download, request=req, response=resp)\n\n@@ -1,11 +1,12 @@\n import unittest\n \n-from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware\n+from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware, MetaRefreshMiddleware\n from scrapy.spider import BaseSpider\n from scrapy.exceptions import IgnoreRequest\n-from scrapy.http import Request, Response, HtmlResponse, Headers\n+from scrapy.http import Request, Response, HtmlResponse\n from scrapy.utils.test import get_crawler\n \n+\n class RedirectMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n@@ -86,46 +87,6 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         del rsp.headers['Location']\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n-    def test_meta_refresh(self):\n-        body = \"\"\"<html>\n-            <head><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n-            </html>\"\"\"\n-        req = Request(url='http://example.org')\n-        rsp = HtmlResponse(url='http://example.org', body=body)\n-        req2 = self.mw.process_response(req, rsp, self.spider)\n-\n-        assert isinstance(req2, Request)\n-        self.assertEqual(req2.url, 'http://example.org/newpage')\n-\n-    def test_meta_refresh_with_high_interval(self):\n-        # meta-refresh with high intervals don't trigger redirects\n-        body = \"\"\"<html>\n-            <head><meta http-equiv=\"refresh\" content=\"1000;url=http://example.org/newpage\" /></head>\n-            </html>\"\"\"\n-        req = Request(url='http://example.org')\n-        rsp = HtmlResponse(url='http://example.org', body=body)\n-        rsp2 = self.mw.process_response(req, rsp, self.spider)\n-\n-        assert rsp is rsp2\n-\n-    def test_meta_refresh_trough_posted_request(self):\n-        body = \"\"\"<html>\n-            <head><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n-            </html>\"\"\"\n-        req = Request(url='http://example.org', method='POST', body='test',\n-            headers={'Content-Type': 'text/plain', 'Content-length': '4'})\n-        rsp = HtmlResponse(url='http://example.org', body=body)\n-        req2 = self.mw.process_response(req, rsp, self.spider)\n-\n-        assert isinstance(req2, Request)\n-        self.assertEqual(req2.url, 'http://example.org/newpage')\n-        self.assertEqual(req2.method, 'GET')\n-        assert 'Content-Type' not in req2.headers, \\\n-            \"Content-Type header must not be present in redirected request\"\n-        assert 'Content-Length' not in req2.headers, \\\n-            \"Content-Length header must not be present in redirected request\"\n-        assert not req2.body, \\\n-            \"Redirected body must be empty, not '%s'\" % req2.body\n \n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n@@ -159,5 +120,85 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')\n         self.assertEqual(req3.meta['redirect_urls'], ['http://scrapytest.org/first', 'http://scrapytest.org/redirected'])\n \n+class MetaRefreshMiddlewareTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        crawler = get_crawler()\n+        self.spider = BaseSpider('foo')\n+        self.mw = MetaRefreshMiddleware.from_crawler(crawler)\n+\n+    def _body(self, interval=5, url='http://example.org/newpage'):\n+        return \"\"\"<html><head><meta http-equiv=\"refresh\" content=\"{0};url={1}\"/></head></html>\"\"\"\\\n+                .format(interval, url)\n+\n+    def test_priority_adjust(self):\n+        req = Request('http://a.com')\n+        rsp = HtmlResponse(req.url, body=self._body())\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n+        assert req2.priority > req.priority\n+\n+    def test_meta_refresh(self):\n+        req = Request(url='http://example.org')\n+        rsp = HtmlResponse(req.url, body=self._body())\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(req2, Request)\n+        self.assertEqual(req2.url, 'http://example.org/newpage')\n+\n+    def test_meta_refresh_with_high_interval(self):\n+        # meta-refresh with high intervals don't trigger redirects\n+        req = Request(url='http://example.org')\n+        rsp = HtmlResponse(url='http://example.org', body=self._body(interval=1000))\n+        rsp2 = self.mw.process_response(req, rsp, self.spider)\n+        assert rsp is rsp2\n+\n+    def test_meta_refresh_trough_posted_request(self):\n+        req = Request(url='http://example.org', method='POST', body='test',\n+                      headers={'Content-Type': 'text/plain', 'Content-length': '4'})\n+        rsp = HtmlResponse(req.url, body=self._body())\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n+\n+        assert isinstance(req2, Request)\n+        self.assertEqual(req2.url, 'http://example.org/newpage')\n+        self.assertEqual(req2.method, 'GET')\n+        assert 'Content-Type' not in req2.headers, \\\n+            \"Content-Type header must not be present in redirected request\"\n+        assert 'Content-Length' not in req2.headers, \\\n+            \"Content-Length header must not be present in redirected request\"\n+        assert not req2.body, \\\n+            \"Redirected body must be empty, not '%s'\" % req2.body\n+\n+    def test_max_redirect_times(self):\n+        self.mw.max_redirect_times = 1\n+        req = Request('http://scrapytest.org/max')\n+        rsp = HtmlResponse(req.url, body=self._body())\n+\n+        req = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(req, Request)\n+        assert 'redirect_times' in req.meta\n+        self.assertEqual(req.meta['redirect_times'], 1)\n+        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)\n+\n+    def test_ttl(self):\n+        self.mw.max_redirect_times = 100\n+        req = Request('http://scrapytest.org/302', meta={'redirect_ttl': 1})\n+        rsp = HtmlResponse(req.url, body=self._body())\n+\n+        req = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(req, Request)\n+        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)\n+\n+    def test_redirect_urls(self):\n+        req1 = Request('http://scrapytest.org/first')\n+        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'))\n+        req2 = self.mw.process_response(req1, rsp1, self.spider)\n+        assert isinstance(req2, Request), req2\n+        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'))\n+        req3 = self.mw.process_response(req2, rsp2, self.spider)\n+        assert isinstance(req3, Request), req3\n+        self.assertEqual(req2.url, 'http://scrapytest.org/redirected')\n+        self.assertEqual(req2.meta['redirect_urls'], ['http://scrapytest.org/first'])\n+        self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')\n+        self.assertEqual(req3.meta['redirect_urls'], ['http://scrapytest.org/first', 'http://scrapytest.org/redirected'])\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#defc4f89b542b756276f0920921dc00fe3ec4675", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 438 | Contributors (this commit): 9 | Commits (past 90d): 9 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,8 +8,10 @@ from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n class BaseRedirectMiddleware(object):\n \n+    enabled_setting = 'REDIRECT_ENABLED'\n+\n     def __init__(self, settings):\n-        if not settings.getbool('REDIRECT_ENABLED'):\n+        if not settings.getbool(self.enabled_setting):\n             raise NotConfigured\n \n         self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')\n@@ -76,9 +78,12 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n class MetaRefreshMiddleware(BaseRedirectMiddleware):\n \n+    enabled_setting = 'METAREFRESH_ENABLED'\n+\n     def __init__(self, settings):\n         super(MetaRefreshMiddleware, self).__init__(settings)\n-        self.max_metarefresh_delay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY')\n+        self._maxdelay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY',\n+                                         settings.getint('METAREFRESH_MAXDELAY'))\n \n     def process_response(self, request, response, spider):\n         if 'dont_redirect' in request.meta or request.method == 'HEAD' or \\\n@@ -87,7 +92,7 @@ class MetaRefreshMiddleware(BaseRedirectMiddleware):\n \n         if isinstance(response, HtmlResponse):\n             interval, url = get_meta_refresh(response)\n-            if url and interval < self.max_metarefresh_delay:\n+            if url and interval < self._maxdelay:\n                 redirected = self._redirect_request_using_get(request, url)\n                 return self._redirect(redirected, request, spider, 'meta refresh')\n \n\n@@ -175,12 +175,14 @@ MEMUSAGE_NOTIFY_MAIL = []\n MEMUSAGE_REPORT = False\n MEMUSAGE_WARNING_MB = 0\n \n+METAREFRESH_ENABLED = True\n+METAREFRESH_MAXDELAY = 100\n+\n NEWSPIDER_MODULE = ''\n \n RANDOMIZE_DOWNLOAD_DELAY = True\n \n REDIRECT_ENABLED = True\n-REDIRECT_MAX_METAREFRESH_DELAY = 100\n REDIRECT_MAX_TIMES = 20  # uses Firefox default setting\n REDIRECT_PRIORITY_ADJUST = +2\n \n\n@@ -12,6 +12,8 @@ DEPRECATED_SETTINGS = [\n     ('SELECTORS_BACKEND', 'use SCRAPY_SELECTORS_BACKEND environment variable instead'),\n     ('AUTOTHROTTLE_MIN_DOWNLOAD_DELAY', 'use DOWNLOAD_DELAY instead'),\n     ('AUTOTHROTTLE_MAX_CONCURRENCY', 'use CONCURRENT_REQUESTS_PER_DOMAIN instead'),\n+    ('AUTOTHROTTLE_MAX_CONCURRENCY', 'use CONCURRENT_REQUESTS_PER_DOMAIN instead'),\n+    ('REDIRECT_MAX_METAREFRESH_DELAY', 'use METAREFRESH_MAXDELAY instead'),\n ]\n \n def check_deprecated_settings(settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9527c5819a9f1e5ad3d5f660a2d9069f60fbcbe2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 232 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,8 @@ Scrapy developers, if you add a setting here remember to:\n \n \"\"\"\n \n-import sys, os\n+import os\n+import sys\n from os.path import join, abspath, dirname\n \n BOT_NAME = 'scrapybot'\n\n@@ -16,6 +16,7 @@ DEPRECATED_SETTINGS = [\n     ('REDIRECT_MAX_METAREFRESH_DELAY', 'use METAREFRESH_MAXDELAY instead'),\n ]\n \n+\n def check_deprecated_settings(settings):\n     deprecated = [x for x in DEPRECATED_SETTINGS if settings[x[0]] is not None]\n     if deprecated:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#487299e06867af10118e6aecd5fff72e132d0e7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 19 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 663 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,7 +53,7 @@ class TakeFirst(object):\n \n     def __call__(self, values):\n         for value in values:\n-            if value:\n+            if value is not None and value != '':\n                 return value\n \n \n\n@@ -7,33 +7,37 @@ from scrapy.item import Item, Field\n from scrapy.selector import HtmlXPathSelector\n from scrapy.http import HtmlResponse\n \n-# test items\n \n+# test items\n class NameItem(Item):\n     name = Field()\n \n+\n class TestItem(NameItem):\n     url = Field()\n     summary = Field()\n \n-# test item loaders\n \n+# test item loaders\n class NameItemLoader(ItemLoader):\n     default_item_class = TestItem\n \n+\n class TestItemLoader(NameItemLoader):\n     name_in = MapCompose(lambda v: v.title())\n \n+\n class DefaultedItemLoader(NameItemLoader):\n     default_input_processor = MapCompose(lambda v: v[:-1])\n \n-# test processors\n \n+# test processors\n def processor_with_args(value, other=None, loader_context=None):\n     if 'key' in loader_context:\n         return loader_context['key']\n     return value\n \n+\n class ItemLoaderTest(unittest.TestCase):\n \n     def test_load_item_using_default_loader(self):\n@@ -96,7 +100,6 @@ class ItemLoaderTest(unittest.TestCase):\n         il.replace_value('name', u'name:bar', re=u'name:(.*)$')\n         self.assertEqual([u'bar'], il.get_collected_values('name'))\n \n-\n     def test_iter_on_input_processor_input(self):\n         class NameFirstItemLoader(NameItemLoader):\n             name_in = TakeFirst()\n@@ -215,7 +218,6 @@ class ItemLoaderTest(unittest.TestCase):\n         il.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(il.get_output_value('name'), u'Mar Ta')\n \n-\n     def test_output_processor_error(self):\n         class TestItemLoader(ItemLoader):\n             default_item_class = TestItem\n@@ -240,7 +242,6 @@ class ItemLoaderTest(unittest.TestCase):\n         assert 'ValueError' in s, s\n         assert expected_exc_str in s, s\n \n-\n     def test_output_processor_using_classes(self):\n         il = TestItemLoader()\n         il.add_value('name', [u'mar', u'ta'])\n@@ -331,11 +332,13 @@ class ItemLoaderTest(unittest.TestCase):\n         item = il.load_item()\n         self.assertEqual(item['name'], u'Mart')\n \n+\n class ProcessorsTest(unittest.TestCase):\n \n     def test_take_first(self):\n         proc = TakeFirst()\n         self.assertEqual(proc([None, '', 'hello', 'world']), 'hello')\n+        self.assertEqual(proc([None, '', 0, 'hello', 'world']), 0)\n \n     def test_identity(self):\n         proc = Identity()\n@@ -363,10 +366,12 @@ class ProcessorsTest(unittest.TestCase):\n         self.assertEqual(proc([u'hello', u'world', u'this', u'is', u'scrapy']),\n                          [u'HELLO', u'THIS', u'IS', u'SCRAPY'])\n \n+\n class TestXPathItemLoader(XPathItemLoader):\n     default_item_class = TestItem\n     name_in = MapCompose(lambda v: v.title())\n \n+\n class XPathItemLoaderTest(unittest.TestCase):\n     response = HtmlResponse(url=\"\", body='<html><body><div id=\"id\">marta</div><p>paragraph</p></body></html>')\n \n@@ -428,4 +433,3 @@ class XPathItemLoaderTest(unittest.TestCase):\n \n if __name__ == \"__main__\":\n     unittest.main()\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#864a7aef87172ea3ff182ea56caa95e6912652d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 160 | Lines Deleted: 159 | Files Changed: 4 | Hunks: 12 | Methods Changed: 21 | Complexity Δ (Sum/Max): 0/41 | Churn Δ: 319 | Churn Cumulative: 3090 | Contributors (this commit): 9 | Commits (past 90d): 32 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -8,153 +8,6 @@ from scrapy.utils.misc import load_object\n from scrapy.contrib.httpcache import rfc1123_to_epoch, parse_cachecontrol\n \n \n-class DummyPolicy(object):\n-\n-    def __init__(self, settings):\n-        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n-        self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n-\n-    def should_cache_request(self, request):\n-        return urlparse_cached(request).scheme not in self.ignore_schemes\n-\n-    def should_cache_response(self, response, request):\n-        return response.status not in self.ignore_http_codes\n-\n-    def is_cached_response_fresh(self, response, request):\n-        return True\n-\n-    def is_cached_response_valid(self, cachedresponse, response, request):\n-        return True\n-\n-\n-class RFC2616Policy(object):\n-\n-    MAXAGE = 3600 * 24 * 365  # one year\n-\n-    def __init__(self, settings):\n-        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n-        self._cc_parsed = WeakKeyDictionary()\n-\n-    def _parse_cachecontrol(self, r):\n-        if r not in self._cc_parsed:\n-            cch = r.headers.get('Cache-Control', '')\n-            self._cc_parsed[r] = parse_cachecontrol(cch)\n-        return self._cc_parsed[r]\n-\n-    def should_cache_request(self, request):\n-        if urlparse_cached(request).scheme in self.ignore_schemes:\n-            return False\n-        cc = self._parse_cachecontrol(request)\n-        # obey user-agent directive \"Cache-Control: no-store\"\n-        if 'no-store' in cc:\n-            return False\n-        # Any other is eligible for caching\n-        return True\n-\n-    def should_cache_response(self, response, request):\n-        # What is cacheable - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec14.9.1\n-        # Response cacheability - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n-        # Status code 206 is not included because cache can not deal with partial contents\n-        cc = self._parse_cachecontrol(response)\n-        # obey directive \"Cache-Control: no-store\"\n-        if 'no-store' in cc:\n-            return False\n-        # Never cache 304 (Not Modified) responses\n-        elif response.status == 304:\n-            return False\n-        # Any hint on response expiration is good\n-        elif 'max-age' in cc or 'Expires' in response.headers:\n-            return True\n-        # Firefox fallbacks this statuses to one year expiration if none is set\n-        elif response.status in (300, 301, 308):\n-            return True\n-        # Other statuses without expiration requires at least one validator\n-        elif response.status in (200, 203, 401):\n-            return 'Last-Modified' in response.headers or 'ETag' in response.headers\n-        # Any other is probably not eligible for caching\n-        # Makes no sense to cache responses that does not contain expiration\n-        # info and can not be revalidated\n-        else:\n-            return False\n-\n-    def is_cached_response_fresh(self, cachedresponse, request):\n-        cc = self._parse_cachecontrol(cachedresponse)\n-        ccreq = self._parse_cachecontrol(request)\n-        if 'no-cache' in cc or 'no-cache' in ccreq:\n-            return False\n-\n-        now = time()\n-        freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n-        currentage = self._compute_current_age(cachedresponse, request, now)\n-        if currentage < freshnesslifetime:\n-            return True\n-        # Cached response is stale, try to set validators if any\n-        self._set_conditional_validators(request, cachedresponse)\n-        return False\n-\n-    def is_cached_response_valid(self, cachedresponse, response, request):\n-        return response.status == 304\n-\n-    def _set_conditional_validators(self, request, cachedresponse):\n-        if 'Last-Modified' in cachedresponse.headers:\n-            request.headers['If-Modified-Since'] = cachedresponse.headers['Last-Modified']\n-\n-        if 'ETag' in cachedresponse.headers:\n-            request.headers['If-None-Match'] = cachedresponse.headers['ETag']\n-\n-    def _compute_freshness_lifetime(self, response, request, now):\n-        # Reference nsHttpResponseHead::ComputeFresshnessLifetime\n-        # http://dxr.mozilla.org/mozilla-central/netwerk/protocol/http/nsHttpResponseHead.cpp.html#l259\n-        cc = self._parse_cachecontrol(response)\n-        if 'max-age' in cc:\n-            try:\n-                return max(0, int(cc['max-age']))\n-            except ValueError:\n-                pass\n-\n-        # Parse date header or synthesize it if none exists\n-        date = rfc1123_to_epoch(response.headers.get('Date')) or now\n-\n-        # Try HTTP/1.0 Expires header\n-        if 'Expires' in response.headers:\n-            expires = rfc1123_to_epoch(response.headers['Expires'])\n-            # When parsing Expires header fails RFC 2616 section 14.21 says we\n-            # should treat this as an expiration time in the past.\n-            return max(0, expires - date) if expires else 0\n-\n-        # Fallback to heuristic using last-modified header\n-        # This is not in RFC but on Firefox caching implementation\n-        lastmodified = rfc1123_to_epoch(response.headers.get('Last-Modified'))\n-        if lastmodified and lastmodified <= date:\n-            return (date - lastmodified) / 10\n-\n-        # This request can be cached indefinitely\n-        if response.status in (300, 301, 308):\n-            return self.MAXAGE\n-\n-        # Insufficient information to compute fresshness lifetime\n-        return 0\n-\n-    def _compute_current_age(self, response, request, now):\n-        # Reference nsHttpResponseHead::ComputeCurrentAge\n-        # http://dxr.mozilla.org/mozilla-central/netwerk/protocol/http/nsHttpResponseHead.cpp.html\n-        currentage = 0\n-        # If Date header is not set we assume it is a fast connection, and\n-        # clock is in sync with the server\n-        date = rfc1123_to_epoch(response.headers.get('Date')) or now\n-        if now > date:\n-            currentage = now - date\n-\n-        if 'Age' in response.headers:\n-            try:\n-                age = int(response.headers['Age'])\n-                currentage = max(currentage, age)\n-            except ValueError:\n-                pass\n-\n-        return currentage\n-\n-\n class HttpCacheMiddleware(object):\n \n     def __init__(self, settings, stats):\n\n@@ -1,12 +1,161 @@\n import os\n-from time import time\n import cPickle as pickle\n+from time import time\n+from weakref import WeakKeyDictionary\n from email.utils import mktime_tz, parsedate_tz\n from w3lib.http import headers_raw_to_dict, headers_dict_to_raw\n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.project import data_path\n+from scrapy.utils.httpobj import urlparse_cached\n+\n+\n+class DummyPolicy(object):\n+\n+    def __init__(self, settings):\n+        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n+        self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n+\n+    def should_cache_request(self, request):\n+        return urlparse_cached(request).scheme not in self.ignore_schemes\n+\n+    def should_cache_response(self, response, request):\n+        return response.status not in self.ignore_http_codes\n+\n+    def is_cached_response_fresh(self, response, request):\n+        return True\n+\n+    def is_cached_response_valid(self, cachedresponse, response, request):\n+        return True\n+\n+\n+class RFC2616Policy(object):\n+\n+    MAXAGE = 3600 * 24 * 365  # one year\n+\n+    def __init__(self, settings):\n+        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n+        self._cc_parsed = WeakKeyDictionary()\n+\n+    def _parse_cachecontrol(self, r):\n+        if r not in self._cc_parsed:\n+            cch = r.headers.get('Cache-Control', '')\n+            self._cc_parsed[r] = parse_cachecontrol(cch)\n+        return self._cc_parsed[r]\n+\n+    def should_cache_request(self, request):\n+        if urlparse_cached(request).scheme in self.ignore_schemes:\n+            return False\n+        cc = self._parse_cachecontrol(request)\n+        # obey user-agent directive \"Cache-Control: no-store\"\n+        if 'no-store' in cc:\n+            return False\n+        # Any other is eligible for caching\n+        return True\n+\n+    def should_cache_response(self, response, request):\n+        # What is cacheable - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec14.9.1\n+        # Response cacheability - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n+        # Status code 206 is not included because cache can not deal with partial contents\n+        cc = self._parse_cachecontrol(response)\n+        # obey directive \"Cache-Control: no-store\"\n+        if 'no-store' in cc:\n+            return False\n+        # Never cache 304 (Not Modified) responses\n+        elif response.status == 304:\n+            return False\n+        # Any hint on response expiration is good\n+        elif 'max-age' in cc or 'Expires' in response.headers:\n+            return True\n+        # Firefox fallbacks this statuses to one year expiration if none is set\n+        elif response.status in (300, 301, 308):\n+            return True\n+        # Other statuses without expiration requires at least one validator\n+        elif response.status in (200, 203, 401):\n+            return 'Last-Modified' in response.headers or 'ETag' in response.headers\n+        # Any other is probably not eligible for caching\n+        # Makes no sense to cache responses that does not contain expiration\n+        # info and can not be revalidated\n+        else:\n+            return False\n+\n+    def is_cached_response_fresh(self, cachedresponse, request):\n+        cc = self._parse_cachecontrol(cachedresponse)\n+        ccreq = self._parse_cachecontrol(request)\n+        if 'no-cache' in cc or 'no-cache' in ccreq:\n+            return False\n+\n+        now = time()\n+        freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n+        currentage = self._compute_current_age(cachedresponse, request, now)\n+        if currentage < freshnesslifetime:\n+            return True\n+        # Cached response is stale, try to set validators if any\n+        self._set_conditional_validators(request, cachedresponse)\n+        return False\n+\n+    def is_cached_response_valid(self, cachedresponse, response, request):\n+        return response.status == 304\n+\n+    def _set_conditional_validators(self, request, cachedresponse):\n+        if 'Last-Modified' in cachedresponse.headers:\n+            request.headers['If-Modified-Since'] = cachedresponse.headers['Last-Modified']\n+\n+        if 'ETag' in cachedresponse.headers:\n+            request.headers['If-None-Match'] = cachedresponse.headers['ETag']\n+\n+    def _compute_freshness_lifetime(self, response, request, now):\n+        # Reference nsHttpResponseHead::ComputeFresshnessLifetime\n+        # http://dxr.mozilla.org/mozilla-central/netwerk/protocol/http/nsHttpResponseHead.cpp.html#l259\n+        cc = self._parse_cachecontrol(response)\n+        if 'max-age' in cc:\n+            try:\n+                return max(0, int(cc['max-age']))\n+            except ValueError:\n+                pass\n+\n+        # Parse date header or synthesize it if none exists\n+        date = rfc1123_to_epoch(response.headers.get('Date')) or now\n+\n+        # Try HTTP/1.0 Expires header\n+        if 'Expires' in response.headers:\n+            expires = rfc1123_to_epoch(response.headers['Expires'])\n+            # When parsing Expires header fails RFC 2616 section 14.21 says we\n+            # should treat this as an expiration time in the past.\n+            return max(0, expires - date) if expires else 0\n+\n+        # Fallback to heuristic using last-modified header\n+        # This is not in RFC but on Firefox caching implementation\n+        lastmodified = rfc1123_to_epoch(response.headers.get('Last-Modified'))\n+        if lastmodified and lastmodified <= date:\n+            return (date - lastmodified) / 10\n+\n+        # This request can be cached indefinitely\n+        if response.status in (300, 301, 308):\n+            return self.MAXAGE\n+\n+        # Insufficient information to compute fresshness lifetime\n+        return 0\n+\n+    def _compute_current_age(self, response, request, now):\n+        # Reference nsHttpResponseHead::ComputeCurrentAge\n+        # http://dxr.mozilla.org/mozilla-central/netwerk/protocol/http/nsHttpResponseHead.cpp.html\n+        currentage = 0\n+        # If Date header is not set we assume it is a fast connection, and\n+        # clock is in sync with the server\n+        date = rfc1123_to_epoch(response.headers.get('Date')) or now\n+        if now > date:\n+            currentage = now - date\n+\n+        if 'Age' in response.headers:\n+            try:\n+                age = int(response.headers['Age'])\n+                currentage = max(currentage, age)\n+            except ValueError:\n+                pass\n+\n+        return currentage\n \n \n class DbmCacheStorage(object):\n@@ -51,11 +200,13 @@ class DbmCacheStorage(object):\n         key = self._request_key(request)\n         db = self.db\n         tkey = '%s_time' % key\n-        if not db.has_key(tkey):\n+        if tkey not in db:\n             return  # not found\n+\n         ts = db[tkey]\n         if 0 < self.expiration_secs < time() - float(ts):\n             return  # expired\n+\n         return pickle.loads(db['%s_data' % key])\n \n     def _request_key(self, request):\n\n@@ -141,7 +141,7 @@ HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n HTTPCACHE_DBM_MODULE = 'anydbm'\n-HTTPCACHE_POLICY = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'\n+HTTPCACHE_POLICY = 'scrapy.contrib.httpcache.RFC2616Policy'\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n\n@@ -16,7 +16,7 @@ from scrapy.contrib.downloadermiddleware.httpcache import HttpCacheMiddleware\n class _BaseTest(unittest.TestCase):\n \n     storage_class = 'scrapy.contrib.httpcache.DbmCacheStorage'\n-    policy_class = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'\n+    policy_class = 'scrapy.contrib.httpcache.RFC2616Policy'\n \n     def setUp(self):\n         self.yesterday = email.utils.formatdate(time.time() - 86400)\n@@ -121,7 +121,9 @@ class FilesystemStorageTest(DefaultStorageTest):\n     storage_class = 'scrapy.contrib.httpcache.FilesystemCacheStorage'\n \n \n-class DefaultMiddlewaretest(_BaseTest):\n+class DummyPolicyTest(_BaseTest):\n+\n+    policy_class = 'scrapy.contrib.httpcache.DummyPolicy'\n \n     def test_middleware(self):\n         with self._middleware() as mw:\n@@ -211,14 +213,9 @@ class DefaultMiddlewaretest(_BaseTest):\n             assert 'cached' in response.flags\n \n \n-class DummyMiddlewareTest(DefaultStorageTest):\n+class RFC2616PolicyTest(DefaultStorageTest):\n \n-    policy_class = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'\n-\n-\n-class RFC2616MiddlewareTest(DefaultStorageTest):\n-\n-    policy_class = 'scrapy.contrib.downloadermiddleware.httpcache.RFC2616Policy'\n+    policy_class = 'scrapy.contrib.httpcache.RFC2616Policy'\n \n     def _process_requestresponse(self, mw, request, response):\n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3cbc4d0b94045d80132b4f43735b95ac9c841896", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 12 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 421 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -40,3 +40,10 @@ except ImportError:\n     pass\n else:\n     optional_features.add('libxml2')\n+\n+try:\n+    import django\n+except ImportError:\n+    pass\n+else:\n+    optional_features.add('django')\n\n@@ -1,6 +1,7 @@\n-from django.core.exceptions import ValidationError\n-\n from scrapy.item import Field, Item, ItemMeta\n+from scrapy import optional_features\n+if 'django' in optional_features:\n+    from django.core.exceptions import ValidationError\n \n \n class DjangoItemMeta(ItemMeta):\n@@ -13,7 +14,7 @@ class DjangoItemMeta(ItemMeta):\n             cls._model_fields = []\n             cls._model_meta = cls.django_model._meta\n             for model_field in cls._model_meta.fields:\n-                if model_field.auto_created == False:\n+                if not model_field.auto_created:\n                     if model_field.name not in cls.fields:\n                         cls.fields[model_field.name] = Field()\n                     cls._model_fields.append(model_field.name)\n\n@@ -2,33 +2,22 @@ import os\n from twisted.trial import unittest\n \n from scrapy.contrib.djangoitem import DjangoItem, Field\n+from scrapy import optional_features\n \n os.environ['DJANGO_SETTINGS_MODULE'] = 'scrapy.tests.test_djangoitem.settings'\n \n-try:\n-    import django\n-except ImportError:\n-    django = None\n-\n-if django:\n+if 'django' in optional_features:\n     from .models import Person, IdentifiedPerson\n-else:\n-    Person = None\n-    IdentifiedPerson = None\n-\n \n     class BasePersonItem(DjangoItem):\n         django_model = Person\n \n-\n     class NewFieldPersonItem(BasePersonItem):\n         other = Field()\n \n-\n     class OverrideFieldPersonItem(BasePersonItem):\n         age = Field()\n \n-\n     class IdentifiedPersonItem(DjangoItem):\n         django_model = IdentifiedPerson\n \n@@ -36,7 +25,7 @@ class IdentifiedPersonItem(DjangoItem):\n class DjangoItemTest(unittest.TestCase):\n \n     def setUp(self):\n-        if not django:\n+        if 'django' not in optional_features:\n             raise unittest.SkipTest(\"Django is not available\")\n \n     def test_base(self):\n@@ -111,7 +100,6 @@ class DjangoItemTest(unittest.TestCase):\n         i['age'] = '22'\n         self.assertTrue(i.is_valid())\n \n-\n     def test_default_field_values(self):\n         i = BasePersonItem()\n         person = i.save(commit=False)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
