{"custom_id": "scrapy#55ac26228b853aec6d543bfaf3ab85ebbf5ab002", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 172 | Files Changed: 3 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 70/77 | Churn Δ: 172 | Churn Cumulative: 921 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -456,6 +456,3 @@ class AddonManager(Mapping):\n         \"\"\"\n         for name in self:\n             self._call_addon(name, \"check_configuration\", crawler)\n-\n-\n-from scrapy.addons.builtins import *  # noqa\n\n@@ -1,146 +0,0 @@\n-import scrapy\n-from scrapy.addons import Addon\n-\n-__all__ = [\n-    \"make_builtin_addon\",\n-    \"depth\",\n-    \"httperror\",\n-    \"offsite\",\n-    \"referer\",\n-    \"urllength\",\n-    \"ajaxcrawl\",\n-    \"chunked\",\n-    \"cookies\",\n-    \"defaultheaders\",\n-    \"downloadtimeout\",\n-    \"httpauth\",\n-    \"httpcache\",\n-    \"httpcompression\",\n-    \"httpproxy\",\n-    \"metarefresh\",\n-    \"redirect\",\n-    \"retry\",\n-    \"robotstxt\",\n-    \"stats\",\n-    \"useragent\",\n-    \"autothrottle\",\n-    \"corestats\",\n-    \"closespider\",\n-    \"debugger\",\n-    \"feedexport\",\n-    \"logstats\",\n-    \"memdebug\",\n-    \"memusage\",\n-    \"spiderstate\",\n-    \"stacktracedump\",\n-    \"statsmailer\",\n-    \"telnetconsole\",\n-]\n-\n-\n-def make_builtin_addon(addon_name, addon_default_config=None, addon_version=None):\n-    class ThisAddon(Addon):\n-        name = addon_name\n-        version = addon_version or scrapy.__version__\n-        default_config = addon_default_config or {}\n-\n-    return ThisAddon\n-\n-\n-# XXX: Below are CLASSES that have lowercase names. This is in line with the\n-#      original SEP-021 but violates PEP8.\n-# We might consider prepending all built-in addon names with scrapy_ or similar\n-# to reduce the chance of name clashes.\n-\n-# SPIDER MIDDLEWARES\n-\n-depth = make_builtin_addon(\"depth\")\n-\n-httperror = make_builtin_addon(\"httperror\")\n-\n-offsite = make_builtin_addon(\"offsite\")\n-\n-referer = make_builtin_addon(\"referer\")\n-\n-urllength = make_builtin_addon(\"urllength\")\n-\n-\n-# DOWNLOADER MIDDLEWARES\n-\n-ajaxcrawl = make_builtin_addon(\"ajaxcrawl\", {\"enabled\": True})\n-\n-chunked = make_builtin_addon(\"chunked\")\n-\n-cookies = make_builtin_addon(\"cookies\")\n-\n-defaultheaders = make_builtin_addon(\"defaultheaders\")\n-\n-\n-# Assume every config entry is a header\n-def defaultheaders_export_config(self, config, settings):\n-    conf = self.default_config or {}\n-    conf.update(config)\n-    settings.set(\"DEFAULT_REQUEST_HEADERS\", conf, \"addon\")\n-\n-\n-defaultheaders.export_config = defaultheaders_export_config\n-\n-downloadtimeout = make_builtin_addon(\"downloadtimeout\")\n-downloadtimeout.config_mapping = {\n-    \"timeout\": \"DOWNLOAD_TIMEOUT\",\n-    \"download_timeout\": \"DOWNLOAD_TIMEOUT\",\n-}\n-\n-httpauth = make_builtin_addon(\"httpauth\")\n-\n-httpcache = make_builtin_addon(\"httpcache\", {\"enabled\": True})\n-\n-httpcompression = make_builtin_addon(\"httpcompression\")\n-httpcompression.config_mapping = {\"enabled\": \"COMPRESSION_ENABLED\"}\n-\n-httpproxy = make_builtin_addon(\"httpproxy\")\n-\n-metarefresh = make_builtin_addon(\"metarefresh\")\n-metarefresh.config_mapping = {\"max_times\": \"REDIRECT_MAX_TIMES\"}\n-\n-redirect = make_builtin_addon(\"redirect\")\n-\n-retry = make_builtin_addon(\"retry\")\n-\n-robotstxt = make_builtin_addon(\"robotstxt\", {\"obey\": True})\n-\n-stats = make_builtin_addon(\"stats\")\n-\n-useragent = make_builtin_addon(\"useragent\")\n-useragent.config_mapping = {\"user_agent\": \"USER_AGENT\"}\n-\n-\n-# ITEM PIPELINES\n-\n-\n-# EXTENSIONS\n-\n-autothrottle = make_builtin_addon(\"autothrottle\", {\"enabled\": True})\n-\n-corestats = make_builtin_addon(\"corestats\")\n-\n-closespider = make_builtin_addon(\"closespider\")\n-\n-debugger = make_builtin_addon(\"debugger\")\n-\n-feedexport = make_builtin_addon(\"feedexport\")\n-feedexport.settings_prefix = \"FEED\"\n-\n-logstats = make_builtin_addon(\"logstats\")\n-\n-memdebug = make_builtin_addon(\"memdebug\", {\"enabled\": True})\n-\n-memusage = make_builtin_addon(\"memusage\", {\"enabled\": True})\n-\n-spiderstate = make_builtin_addon(\"spiderstate\")\n-\n-stacktracedump = make_builtin_addon(\"stacktracedump\")\n-\n-statsmailer = make_builtin_addon(\"statsmailer\")\n-\n-telnetconsole = make_builtin_addon(\"telnetconsole\")\n\n@@ -1,23 +0,0 @@\n-import unittest\n-\n-import scrapy\n-import scrapy.addons\n-from scrapy.addons.builtins import make_builtin_addon\n-from scrapy.settings import Settings\n-\n-\n-class BuiltinAddonsTest(unittest.TestCase):\n-    def test_make_builtin_addon(self):\n-        httpcache = make_builtin_addon(\"httpcache\", {\"enabled\": True})\n-        self.assertEqual(httpcache.name, \"httpcache\")\n-        self.assertEqual(httpcache.default_config, {\"enabled\": True})\n-        self.assertEqual(httpcache.version, scrapy.__version__)\n-        httpcache = make_builtin_addon(\"httpcache\", {\"enabled\": True}, \"99.9\")\n-        self.assertEqual(httpcache.version, \"99.9\")\n-\n-    def test_defaultheaders_export_config(self):\n-        settings = Settings()\n-        dh = scrapy.addons.defaultheaders()\n-        dh.export_config({\"X-Test-Header\": \"val\"}, settings)\n-        self.assertIn(\"X-Test-Header\", settings[\"DEFAULT_REQUEST_HEADERS\"])\n-        self.assertEqual(settings[\"DEFAULT_REQUEST_HEADERS\"][\"X-Test-Header\"], \"val\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fdbc141b23b95f8f744712f86c28a4d4d4d8c52f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 31 | Churn Cumulative: 880 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,9 +2,11 @@ import warnings\n from collections import OrderedDict, defaultdict\n from collections.abc import Mapping\n from inspect import isclass\n+from typing import Dict\n \n import zope.interface\n-from pkg_resources import Distribution, Requirement, WorkingSet\n+from packaging.requirements import Requirement\n+from packaging.version import Version\n from zope.interface.verify import verifyObject\n \n from scrapy.interfaces import IAddon\n@@ -303,7 +305,7 @@ class AddonManager(Mapping):\n         for a, c in zip(addons, configs):\n             self.add(a, c)\n \n-    def check_dependency_clashes(self):\n+    def check_dependency_clashes(self) -> None:\n         \"\"\"Check for incompatibilities in add-on dependencies.\n \n         Add-ons can provide information about their dependencies in their\n@@ -320,24 +322,20 @@ class AddonManager(Mapping):\n         add-on.\n         \"\"\"\n         # Collect all active add-ons and the components they provide\n-        ws = WorkingSet(\"\")\n+        versions: Dict[str, Version] = {}\n \n-        def add_dist(project_name, version, **kwargs):\n-            if project_name in ws.entry_keys.get(\"scrapy\", []):\n+        def add_version(project_name, version):\n+            if project_name in versions:\n                 raise ImportError(\n                     f\"Component {project_name} provided by multiple add-ons\"\n                 )\n-            else:\n-                dist = Distribution(\n-                    project_name=project_name, version=version, **kwargs\n-                )\n-                ws.add(dist, entry=\"scrapy\")\n+            versions[project_name] = Version(version)\n \n         for name in self:\n             ver = self[name].version\n-            add_dist(name, ver)\n+            add_version(name, ver)\n             for provides_name in getattr(self[name], \"provides\", []):\n-                add_dist(provides_name, ver)\n+                add_version(provides_name, ver)\n \n         # Collect all required and modified components\n         def compile_attribute_dict(attribute_name):\n@@ -352,10 +350,8 @@ class AddonManager(Mapping):\n \n         req_or_mod = set(required.keys()).union(modified.keys())\n         for reqstr in req_or_mod:\n-            req = Requirement.parse(reqstr)\n-            # May raise VersionConflict. Do we want to catch it and raise\n-            # our own exception or is it helpful enough?\n-            if ws.find(req) is None:\n+            req = Requirement(reqstr)\n+            if req.name not in versions or versions[req.name] not in req.specifier:\n                 raise ImportError(\n                     f\"Add-ons {required[reqstr] + modified[reqstr]} require\"\n                     f\" or modify missing component {reqstr}\"\n\n@@ -4,7 +4,6 @@ import warnings\n from collections import OrderedDict\n from unittest import mock\n \n-from pkg_resources import VersionConflict\n from zope.interface import directlyProvides\n from zope.interface.exceptions import BrokenImplementation, MultipleInvalid\n from zope.interface.verify import verifyObject\n@@ -347,7 +346,7 @@ class AddonManagerTest(unittest.TestCase):\n         self.assertRaises(ImportError, check_with, requires)\n         self.assertRaises(ImportError, check_with, modifies)\n         self.assertRaises(ImportError, check_with, provides, provides2)\n-        self.assertRaises(VersionConflict, check_with, provides, requires_newer)\n+        self.assertRaises(ImportError, check_with, provides, requires_newer)\n         with warnings.catch_warnings(record=True) as w:\n             check_with(provides, modifies)\n             check_with(provides)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ebb8256f029b2027a3603685569378ccce73070", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 122 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -156,10 +156,10 @@ class AddonManager(Mapping):\n \n         addons = AddonManager()\n         # ... load some add-ons here\n-        print addons.enabled  # prints names of all enabled add-ons\n-        print addons['TestAddon'].version  # prints version of add-on with name\n+        print(addons.enabled)  # prints names of all enabled add-ons\n+        print(addons['TestAddon'].version)  # prints version of add-on with name\n                                            # 'TestAddon'\n-        print addons.configs['TestAddon']  # prints configuration of 'TestAddon'\n+        print(addons.configs['TestAddon'])  # prints configuration of 'TestAddon'\n \n     \"\"\"\n \n\n@@ -1,10 +1,8 @@\n-import zope.interface\n+from zope.interface import moduleProvides\n \n from scrapy.interfaces import IAddon\n \n-zope.interface.moduleProvides(IAddon)\n-\n-FROM = \"test_addons.addonmod\"\n+moduleProvides(IAddon)\n \n name = \"AddonModule\"\n version = \"1.0\"\n\n@@ -1,13 +1,9 @@\n-import zope.interface\n+from zope.interface import implementer\n \n from scrapy.interfaces import IAddon\n \n \n-class Addon(object):\n-    FROM = \"test_addons.addons\"\n-\n-\n-@zope.interface.declarations.implementer(IAddon)\n+@implementer(IAddon)\n class GoodAddon(object):\n     name = \"GoodAddon\"\n     version = \"1.0\"\n@@ -28,7 +24,7 @@ class GoodAddon(object):\n         pass\n \n \n-@zope.interface.declarations.implementer(IAddon)\n+@implementer(IAddon)\n class BrokenAddon(object):\n     name = \"BrokenAddon\"\n     # No version\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e76464fbf1338bb791b4ae2ad97dd33f72dd0a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 147 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -17,11 +17,6 @@ class ISpiderLoader(Interface):\n         \"\"\"Return the list of spiders names that can handle the given request\"\"\"\n \n \n-# ISpiderManager is deprecated, don't use it!\n-# An alias is kept for backwards compatibility.\n-ISpiderManager = ISpiderLoader\n-\n-\n class IAddon(Interface):\n     \"\"\"Scrapy add-on\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#815af431209686018b1bc2cbb80fd946cfe93614", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 26 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/1 | Churn Δ: 35 | Churn Cumulative: 1264 | Contributors (this commit): 39 | Commits (past 90d): 11 | Contributors (cumulative): 51 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,7 +11,7 @@ from zope.interface.verify import verifyObject\n \n from scrapy.interfaces import IAddon\n from scrapy.utils.conf import build_component_list\n-from scrapy.utils.misc import load_module_or_object\n+from scrapy.utils.misc import load_object\n \n \n @zope.interface.implementer(IAddon)\n@@ -252,8 +252,8 @@ class AddonManager(Mapping):\n         \"\"\"\n         if isinstance(path, str):\n             try:\n-                obj = load_module_or_object(path)\n-            except NameError:\n+                obj = load_object(path)\n+            except (ValueError, NameError, ImportError):\n                 raise NameError(f\"Could not find add-on '{path}'\")\n         else:\n             obj = path\n\n@@ -72,22 +72,6 @@ def load_object(path: Union[str, Callable]) -> Any:\n     return obj\n \n \n-def load_module_or_object(path):\n-    \"\"\"Load python module or (non-module) object from given path.\n-\n-    Path can be both a Python or a file path.\n-    \"\"\"\n-    try:\n-        return import_module(path)\n-    except ImportError:\n-        pass\n-    try:\n-        return load_object(path)\n-    except (ValueError, NameError, ImportError):\n-        pass\n-    raise NameError(f\"Could not load '{path}'\")\n-\n-\n def walk_modules(path):\n     \"\"\"Loads a module and all its submodules from the given module path and\n     returns them. If *any* module throws an exception while importing, that\n\n@@ -8,7 +8,6 @@ from scrapy.item import Field, Item\n from scrapy.utils.misc import (\n     arg_to_iter,\n     create_instance,\n-    load_module_or_object,\n     load_object,\n     rel_has_nofollow,\n     set_environ,\n@@ -31,17 +30,17 @@ class UtilsMiscTestCase(unittest.TestCase):\n         obj = load_object(\"scrapy.utils.misc.load_object\")\n         self.assertIs(obj, load_object)\n \n+    def test_load_object_module(self):\n+        testmod = load_object(__name__ + \".testmod\")\n+        self.assertTrue(hasattr(testmod, \"TESTVAR\"))\n+        obj = load_object(\"scrapy.utils.misc.load_object\")\n+        self.assertIs(obj, load_object)\n+\n     def test_load_object_exceptions(self):\n         self.assertRaises(ImportError, load_object, \"nomodule999.mod.function\")\n         self.assertRaises(NameError, load_object, \"scrapy.utils.misc.load_object999\")\n         self.assertRaises(TypeError, load_object, {})\n \n-    def test_load_module_or_object(self):\n-        testmod = load_module_or_object(__name__ + \".testmod\")\n-        self.assertTrue(hasattr(testmod, \"TESTVAR\"))\n-        obj = load_object(\"scrapy.utils.misc.load_object\")\n-        self.assertIs(obj, load_object)\n-\n     def test_walk_modules(self):\n         mods = walk_modules(\"tests.test_utils_misc.test_walk_modules\")\n         expected = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7cfdca8f9b1b1f16aacae958f7a5c8824df056e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 80 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@ from unittest import TestCase\n \n from pytest import mark\n \n+from scrapy.utils.defer import deferred_f_from_coro_f\n from scrapy.utils.reactor import (\n     install_reactor,\n     is_asyncio_reactor_installed,\n@@ -29,6 +30,8 @@ class AsyncioTest(TestCase):\n \n         assert original_reactor == reactor\n \n+    @mark.only_asyncio()\n+    @deferred_f_from_coro_f\n     async def test_set_asyncio_event_loop(self):\n         install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n-        assert set_asyncio_event_loop() is asyncio.get_running_loop()\n+        assert set_asyncio_event_loop(None) is asyncio.get_running_loop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7f78a8305fd7c08705cc0125e0f6e99dbaa0d10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 35 | Churn Cumulative: 1299 | Contributors (this commit): 39 | Commits (past 90d): 14 | Contributors (cumulative): 51 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,7 +11,7 @@ from zope.interface.verify import verifyObject\n \n from scrapy.interfaces import IAddon\n from scrapy.utils.conf import build_component_list\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import load_module_or_object\n \n \n @zope.interface.implementer(IAddon)\n@@ -252,8 +252,8 @@ class AddonManager(Mapping):\n         \"\"\"\n         if isinstance(path, str):\n             try:\n-                obj = load_object(path)\n-            except (ValueError, NameError, ImportError):\n+                obj = load_module_or_object(path)\n+            except NameError:\n                 raise NameError(f\"Could not find add-on '{path}'\")\n         else:\n             obj = path\n\n@@ -72,6 +72,22 @@ def load_object(path: Union[str, Callable]) -> Any:\n     return obj\n \n \n+def load_module_or_object(path):\n+    \"\"\"Load python module or (non-module) object from given path.\n+\n+    Path can be both a Python or a file path.\n+    \"\"\"\n+    try:\n+        return import_module(path)\n+    except ImportError:\n+        pass\n+    try:\n+        return load_object(path)\n+    except (ValueError, NameError, ImportError):\n+        pass\n+    raise NameError(f\"Could not load '{path}'\")\n+\n+\n def walk_modules(path):\n     \"\"\"Loads a module and all its submodules from the given module path and\n     returns them. If *any* module throws an exception while importing, that\n\n@@ -8,6 +8,7 @@ from scrapy.item import Field, Item\n from scrapy.utils.misc import (\n     arg_to_iter,\n     create_instance,\n+    load_module_or_object,\n     load_object,\n     rel_has_nofollow,\n     set_environ,\n@@ -30,17 +31,17 @@ class UtilsMiscTestCase(unittest.TestCase):\n         obj = load_object(\"scrapy.utils.misc.load_object\")\n         self.assertIs(obj, load_object)\n \n-    def test_load_object_module(self):\n-        testmod = load_object(__name__ + \".testmod\")\n-        self.assertTrue(hasattr(testmod, \"TESTVAR\"))\n-        obj = load_object(\"scrapy.utils.misc.load_object\")\n-        self.assertIs(obj, load_object)\n-\n     def test_load_object_exceptions(self):\n         self.assertRaises(ImportError, load_object, \"nomodule999.mod.function\")\n         self.assertRaises(NameError, load_object, \"scrapy.utils.misc.load_object999\")\n         self.assertRaises(TypeError, load_object, {})\n \n+    def test_load_module_or_object(self):\n+        testmod = load_module_or_object(__name__ + \".testmod\")\n+        self.assertTrue(hasattr(testmod, \"TESTVAR\"))\n+        obj = load_object(\"scrapy.utils.misc.load_object\")\n+        self.assertIs(obj, load_object)\n+\n     def test_walk_modules(self):\n         mods = walk_modules(\"tests.test_utils_misc.test_walk_modules\")\n         expected = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f9ebb66c32fc11eee86261d55e232b7e12f6a09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 54 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 67 | Churn Cumulative: 965 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 0.6875\n\nDIFF:\n@@ -261,32 +261,6 @@ class AddonManager(Mapping):\n             obj = AddonManager.get_addon(obj._addon)\n         return obj\n \n-    def load_dict(self, addonsdict):\n-        \"\"\"Load add-ons and configurations from given dictionary.\n-\n-        Each add-on should be an entry in the dictionary, where the key\n-        corresponds to the add-on path. The value should be a dictionary\n-        representing the add-on configuration.\n-\n-        Example add-on dictionary::\n-\n-            addonsdict = {\n-                'path.to.addon1': {\n-                    'setting1': 'value',\n-                    'setting2': 42,\n-                    },\n-                'path/to/addon2.py': {\n-                    'addon2setting': True,\n-                    },\n-                }\n-\n-        :param addonsdict: dictionary where keys correspond to add-on paths \\\n-            and values correspond to their configuration\n-        :type addonsdict: ``dict``\n-        \"\"\"\n-        for addonpath, addoncfg in addonsdict.items():\n-            self.add(addonpath, addoncfg)\n-\n     def load_settings(self, settings):\n         \"\"\"Load add-ons and configurations from settings object.\n \n\n@@ -1,7 +1,6 @@\n import itertools\n import unittest\n import warnings\n-from collections import OrderedDict\n from unittest import mock\n \n from zope.interface import directlyProvides\n@@ -178,10 +177,16 @@ class AddonManagerTest(unittest.TestCase):\n         x._addon._addon = addons.GoodAddon(\"inner\")\n         self.assertIs(self.manager.get_addon(x), x._addon._addon)\n \n-    def test_load_dict_load_settings(self):\n-        def _test_load_method(func, *args, **kwargs):\n+    def test_load_settings(self):\n+        settings = BaseSettings()\n+        settings.set(\n+            \"ADDONS\",\n+            {\"tests.test_addons.addonmod\": 0, \"tests.test_addons.addons.GoodAddon\": 0},\n+        )\n+        settings.set(\"ADDONMODULE\", {\"key\": \"val1\"})\n+        settings.set(\"GOODADDON\", {\"key\": \"val2\"})\n         manager = AddonManager()\n-            getattr(manager, func)(*args, **kwargs)\n+        manager.load_settings(settings)\n         self.assertCountEqual(manager, [\"GoodAddon\", \"AddonModule\"])\n         self.assertIsInstance(manager[\"GoodAddon\"], addons.GoodAddon)\n         self.assertCountEqual(manager.configs[\"GoodAddon\"], [\"key\"])\n@@ -190,38 +195,18 @@ class AddonManagerTest(unittest.TestCase):\n         self.assertIn(\"key\", manager.configs[\"AddonModule\"])\n         self.assertEqual(manager.configs[\"AddonModule\"][\"key\"], \"val1\")\n \n-        addonsdict = {\n-            \"tests.test_addons.addonmod\": {\"key\": \"val1\"},\n-            \"tests.test_addons.addons.GoodAddon\": {\"key\": \"val2\"},\n-        }\n-        _test_load_method(\"load_dict\", addonsdict)\n-\n-        settings = BaseSettings()\n-        settings.set(\n-            \"ADDONS\",\n-            {\"tests.test_addons.addonmod\": 0, \"tests.test_addons.addons.GoodAddon\": 0},\n-        )\n-        settings.set(\"ADDONMODULE\", {\"key\": \"val1\"})\n-        settings.set(\"GOODADDON\", {\"key\": \"val2\"})\n-        _test_load_method(\"load_settings\", settings)\n-\n-    def test_load_dict_load_settings_order(self):\n-        def _test_load_method(expected_order, func, *args, **kwargs):\n-            manager = AddonManager()\n-            getattr(manager, func)(*args, **kwargs)\n-            self.assertEqual(list(manager.keys()), expected_order)\n-\n+    def test_load_settings_order(self):\n         # Get three addons named 0, 1, 2\n         addonlist = [addons.GoodAddon(str(x)) for x in range(3)]\n         # Test both methods for every possible mutation\n         for ordered_addons in itertools.permutations(addonlist):\n             expected_order = [a.name for a in ordered_addons]\n-            addonsdict = OrderedDict((a, {}) for a in ordered_addons)\n-            _test_load_method(expected_order, \"load_dict\", addonsdict)\n             settings = BaseSettings(\n                 {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n             )\n-            _test_load_method(expected_order, \"load_settings\", settings)\n+            manager = AddonManager()\n+            manager.load_settings(settings)\n+            self.assertEqual(list(manager.keys()), expected_order)\n \n     def test_enabled_disabled(self):\n         manager = AddonManager()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0258c87dab6d7336263ff53c7747f6400d8cd527", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 2037 | Contributors (this commit): 31 | Commits (past 90d): 4 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ import logging\n import pprint\n import signal\n import warnings\n-from typing import TYPE_CHECKING, Optional, Type, Union\n+from typing import TYPE_CHECKING, Optional, Set, Type, Union\n \n from twisted.internet import defer\n from zope.interface.exceptions import DoesNotImplement\n@@ -57,7 +57,7 @@ class Crawler:\n         spidercls: Type[Spider],\n         settings: Union[None, dict, Settings] = None,\n         init_reactor: bool = False,\n-        addons=None,\n+        addons: Optional[AddonManager] = None,\n     ):\n         if isinstance(spidercls, Spider):\n             raise ValueError(\"The spidercls argument must be a class, not an object\")\n@@ -69,7 +69,7 @@ class Crawler:\n         self.settings: Settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n \n-        self.addons = addons if addons is not None else AddonManager()\n+        self.addons: AddonManager = addons if addons is not None else AddonManager()\n         self.addons.load_settings(self.settings)\n         self.addons.update_addons()\n         self.addons.check_dependency_clashes()\n@@ -199,14 +199,14 @@ class CrawlerRunner:\n             )\n         return loader_cls.from_settings(settings.frozencopy())\n \n-    def __init__(self, settings=None, addons=None):\n+    def __init__(self, settings=None, addons: Optional[AddonManager] = None):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         self.settings = settings\n-        self.addons = addons\n+        self.addons: Optional[AddonManager] = addons\n         self.spider_loader = self._get_spider_loader(settings)\n-        self._crawlers = set()\n-        self._active = set()\n+        self._crawlers: Set[Crawler] = set()\n+        self._active: Set[defer.Deferred] = set()\n         self.bootstrap_failed = False\n \n     @property\n@@ -285,7 +285,7 @@ class CrawlerRunner:\n     def _create_crawler(self, spidercls):\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n-        return Crawler(spidercls, self.settings, self.addons)\n+        return Crawler(spidercls, self.settings, addons=self.addons)\n \n     def stop(self):\n         \"\"\"\n@@ -331,7 +331,12 @@ class CrawlerProcess(CrawlerRunner):\n     process. See :ref:`run-from-script` for an example.\n     \"\"\"\n \n-    def __init__(self, settings=None, install_root_handler=True, addons=None):\n+    def __init__(\n+        self,\n+        settings=None,\n+        install_root_handler: bool = True,\n+        addons: Optional[AddonManager] = None,\n+    ):\n         super().__init__(settings, addons)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#79bf8b1f2e8eae5dfb02443b4be5d474502793ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 20 | Churn Cumulative: 2450 | Contributors (this commit): 22 | Commits (past 90d): 8 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -198,7 +198,7 @@ class AddonManagerTest(unittest.TestCase):\n     def test_load_settings_order(self):\n         # Get three addons named 0, 1, 2\n         addonlist = [addons.GoodAddon(str(x)) for x in range(3)]\n-        # Test both methods for every possible mutation\n+        # Test for every possible ordering\n         for ordered_addons in itertools.permutations(addonlist):\n             expected_order = [a.name for a in ordered_addons]\n             settings = BaseSettings(\n\n@@ -421,10 +421,8 @@ with multiples lines\n         addonmgr.add(FailedCheckAddon())\n         crawler = get_crawler(SimpleSpider)\n         crawler.addons = addonmgr\n-        # Doesn't work in 'precise' test environment:\n-        # with self.assertRaises(ValueError):\n-        #    yield crawler.crawl()\n-        yield self.assertFailure(crawler.crawl(), ValueError)\n+        with self.assertRaises(ValueError):\n+            yield crawler.crawl()\n \n \n class CrawlSpiderTestCase(TestCase):\n\n@@ -87,15 +87,3 @@ class MiddlewareManagerTest(unittest.TestCase):\n         mwman = TestMiddlewareManager.from_settings(settings)\n         classes = [x.__class__ for x in mwman.middlewares]\n         self.assertEqual(classes, [M1, M3])\n-\n-    def test_instances_from_settings(self):\n-        settings = Settings()\n-\n-        class InstanceTestMiddlewareManager(MiddlewareManager):\n-            @classmethod\n-            def _get_mwlist_from_settings(cls, settings):\n-                return [\"tests.test_middleware.M1\", M2]\n-\n-        mwman = InstanceTestMiddlewareManager.from_settings(settings)\n-        self.assertIsInstance(mwman.middlewares[0], M1)\n-        self.assertIsInstance(mwman.middlewares[1], M2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#54287f733972dcaf13706664ba89ab1c52e3a290", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 98 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -30,9 +30,7 @@ class Addon(object):\n \n     component_type = None\n     \"\"\"Component setting into which to export via :meth:`export_component`. Can\n-    be any of the dictionary-like component setting names (e.g.\n-    ``DOWNLOADER_MIDDLEWARES``) or any of their abbreviations in\n-    :attr:`~scrapy.addons.COMPONENT_TYPE_ABBR`. If ``None``,\n+    be any of the dictionary-like component setting names. If ``None``,\n     :meth:`export_component` will do nothing.\n     \"\"\"\n \n@@ -50,9 +48,8 @@ class Addon(object):\n     component = None\n     \"\"\"Component to be inserted via :meth:`export_component`. This can be\n     anything that can be used in the dictionary-like component settings, i.e.\n-    a class path, a class, or an instance. If ``None``, it is assumed that the\n-    add-on itself is also provides the component interface, and ``self`` will be\n-    used.\n+    a class path or a class. If ``None``, it is assumed that the add-on itself\n+    also provides the component interface, and ``self`` will be used.\n     \"\"\"\n \n     settings_prefix = None\n@@ -241,13 +238,13 @@ class AddonManager(Mapping):\n     def get_addon(path):\n         \"\"\"Get an add-on object by its Python or file path.\n \n-        ``path`` is assumed to be either a Python or a file path of a Scrapy\n-        add-on. If the object or module pointed to by ``path`` has an attribute\n-        named ``_addon`` that attribute will be assumed to be the add-on.\n-        :meth:`get_addon` will keep following ``_addon`` attributes until it\n-        finds an object that does not have an attribute named ``_addon``.\n+        ``path`` is assumed to be an import path of an add-on. If the object or\n+        module pointed to by ``path`` has an attribute named ``_addon`` that\n+        attribute will be assumed to be the add-on. :meth:`get_addon` will keep\n+        following ``_addon`` attributes until it finds an object that does not\n+        have an attribute named ``_addon``.\n \n-        :param path: Python or file path to an add-on\n+        :param path: Import path of an add-on\n         :type path: ``str``\n         \"\"\"\n         if isinstance(path, str):\n@@ -366,7 +363,7 @@ class AddonManager(Mapping):\n         elif addon in self._disable_on_add:\n             self._disable_on_add.remove(addon)\n         else:\n-            raise ValueError(\"Add-ons need to be added before they can be \" \"enabled\")\n+            raise ValueError(\"Add-ons need to be added before they can be enabled\")\n \n     @property\n     def disabled(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#639c2bcc47ecd700917aa2a48b9c63f41327f977", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 13 | Churn Cumulative: 15 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -45,6 +45,12 @@ class PeriodicLog:\n                 if crawler.settings.getbool(\"PERIODIC_LOG_STATS\")\n                 else None\n             )\n+        except TypeError:\n+            ext_stats = (\n+                {\"enabled\": True}\n+                if crawler.settings.getbool(\"PERIODIC_LOG_STATS\")\n+                else None\n+            )\n \n         try:\n             ext_delta = crawler.settings.getdict(\"PERIODIC_LOG_DELTA\")\n@@ -54,6 +60,13 @@ class PeriodicLog:\n                 if crawler.settings.getdict(\"PERIODIC_LOG_DELTA\")\n                 else None\n             )\n+        except TypeError:\n+            ext_stats = (\n+                {\"enabled\": True}\n+                if crawler.settings.getbool(\"PERIODIC_LOG_DELTA\")\n+                else None\n+            )\n+\n         ext_timing_enabled = crawler.settings.getbool(\n             \"PERIODIC_LOG_TIMING_ENABLED\", False\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e65eeb07beb6805d7f6d0a139c98f58c41a7bc3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 79 | Churn Cumulative: 79 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,79 @@\n+import datetime\n+import unittest\n+\n+from scrapy.crawler import Crawler\n+from scrapy.exceptions import NotConfigured\n+from scrapy.extensions.periodic_log import PeriodicLog\n+\n+from .spiders import MetaSpider\n+\n+stats_dump_1 = {\n+    \"log_count/INFO\": 10,\n+    \"log_count/WARNING\": 1,\n+    \"start_time\": datetime.datetime(2023, 6, 16, 8, 59, 18, 993170),\n+    \"scheduler/enqueued/memory\": 190,\n+    \"scheduler/enqueued\": 190,\n+    \"scheduler/dequeued/memory\": 166,\n+    \"scheduler/dequeued\": 166,\n+    \"downloader/request_count\": 166,\n+    \"downloader/request_method_count/GET\": 166,\n+    \"downloader/request_bytes\": 56803,\n+    \"downloader/response_count\": 150,\n+    \"downloader/response_status_count/200\": 150,\n+    \"downloader/response_bytes\": 595698,\n+    \"httpcompression/response_bytes\": 3186068,\n+    \"httpcompression/response_count\": 150,\n+    \"response_received_count\": 150,\n+    \"request_depth_max\": 9,\n+    \"dupefilter/filtered\": 180,\n+    \"item_scraped_count\": 140,\n+}\n+stats_dump_2 = {\n+    \"log_count/INFO\": 12,\n+    \"log_count/WARNING\": 1,\n+    \"start_time\": datetime.datetime(2023, 6, 16, 8, 59, 18, 993170),\n+    \"scheduler/enqueued/memory\": 337,\n+    \"scheduler/enqueued\": 337,\n+    \"scheduler/dequeued/memory\": 280,\n+    \"scheduler/dequeued\": 280,\n+    \"downloader/request_count\": 280,\n+    \"downloader/request_method_count/GET\": 280,\n+    \"downloader/request_bytes\": 95754,\n+    \"downloader/response_count\": 264,\n+    \"downloader/response_status_count/200\": 264,\n+    \"downloader/response_bytes\": 1046274,\n+    \"httpcompression/response_bytes\": 5614484,\n+    \"httpcompression/response_count\": 264,\n+    \"response_received_count\": 264,\n+    \"request_depth_max\": 16,\n+    \"dupefilter/filtered\": 320,\n+    \"item_scraped_count\": 248,\n+}\n+\n+\n+class TestPeriodicLog(unittest.TestCase):\n+    def test_extension_enabled(self):\n+        extension = PeriodicLog.from_crawler(\n+            Crawler(\n+                MetaSpider,\n+                settings={\"PERIODIC_LOG_STATS\": True, \"LOGSTATS_INTERVAL\": 60},\n+            )\n+        )\n+        # Test enabled\n+        assert extension\n+\n+        # Raise not configured if not set by settings\n+        with self.assertRaises(NotConfigured):\n+            PeriodicLog.from_crawler(Crawler(MetaSpider))\n+\n+    def test_periodic_log_stats(self):\n+        pass\n+\n+    def test_log_delta(self):\n+        pass\n+\n+    def test_settings_include(self):\n+        pass\n+\n+    def test_settings_exclude(self):\n+        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#777a6ea4128cf00d53fa71dc48385bf036cb64fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 36 | Files Changed: 3 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 117 | Churn Cumulative: 2521 | Contributors (this commit): 69 | Commits (past 90d): 4 | Contributors (cumulative): 91 | DMM Complexity: 0.5952380952380952\n\nDIFF:\n@@ -9,31 +9,36 @@ RETRY_HTTP_CODES - which HTTP response codes to retry\n Failed pages are collected on the scraping process and rescheduled at the end,\n once the spider has finished crawling all regular (non failed) pages.\n \"\"\"\n+import warnings\n from logging import Logger, getLogger\n from typing import Optional, Union\n \n-from twisted.internet import defer\n-from twisted.internet.error import (\n-    ConnectError,\n-    ConnectionDone,\n-    ConnectionLost,\n-    ConnectionRefusedError,\n-    DNSLookupError,\n-    TCPTimedOutError,\n-    TimeoutError,\n-)\n-from twisted.web.client import ResponseFailed\n-\n-from scrapy.core.downloader.handlers.http11 import TunnelError\n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http.request import Request\n+from scrapy.settings import Settings\n from scrapy.spiders import Spider\n+from scrapy.utils.misc import load_object\n from scrapy.utils.python import global_object_name\n from scrapy.utils.response import response_status_message\n \n retry_logger = getLogger(__name__)\n \n \n+class BackwardsCompatibilityMetaclass(type):\n+    @property\n+    def EXCEPTIONS_TO_RETRY(cls):\n+        warnings.warn(\n+            \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n+            \"Use the RETRY_EXCEPTIONS setting instead.\",\n+            ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return tuple(\n+            load_object(x) if isinstance(x, str) else x\n+            for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\n+        )\n+\n+\n def get_retry_request(\n     request: Request,\n     *,\n@@ -121,23 +126,7 @@ def get_retry_request(\n     return None\n \n \n-class RetryMiddleware:\n-    # IOError is raised by the HttpCompression middleware when trying to\n-    # decompress an empty response\n-    EXCEPTIONS_TO_RETRY = (\n-        defer.TimeoutError,\n-        TimeoutError,\n-        DNSLookupError,\n-        ConnectionRefusedError,\n-        ConnectionDone,\n-        ConnectError,\n-        ConnectionLost,\n-        TCPTimedOutError,\n-        ResponseFailed,\n-        IOError,\n-        TunnelError,\n-    )\n-\n+class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n     def __init__(self, settings):\n         if not settings.getbool(\"RETRY_ENABLED\"):\n             raise NotConfigured\n@@ -147,6 +136,16 @@ class RetryMiddleware:\n         )\n         self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n \n+        if not hasattr(\n+            self, \"EXCEPTIONS_TO_RETRY\"\n+        ):  # If EXCEPTIONS_TO_RETRY is not \"overriden\"\n+            self.exceptions_to_retry = tuple(\n+                load_object(x) if isinstance(x, str) else x\n+                for x in settings.getlist(\"RETRY_EXCEPTIONS\")\n+            )\n+        else:\n+            self.exceptions_to_retry = self.EXCEPTIONS_TO_RETRY\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler.settings)\n@@ -160,7 +159,7 @@ class RetryMiddleware:\n         return response\n \n     def process_exception(self, request, exception, spider):\n-        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) and not request.meta.get(\n+        if isinstance(exception, self.exceptions_to_retry) and not request.meta.get(\n             \"dont_retry\", False\n         ):\n             return self._retry(request, exception, spider)\n\n@@ -258,6 +258,21 @@ RETRY_ENABLED = True\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\n RETRY_PRIORITY_ADJUST = -1\n+RETRY_EXCEPTIONS = [\n+    \"twisted.internet.defer.TimeoutError\",\n+    \"twisted.internet.error.TimeoutError\",\n+    \"twisted.internet.error.DNSLookupError\",\n+    \"twisted.internet.error.ConnectionRefusedError\",\n+    \"twisted.internet.error.ConnectionDone\",\n+    \"twisted.internet.error.ConnectError\",\n+    \"twisted.internet.error.ConnectionLost\",\n+    \"twisted.internet.error.TCPTimedOutError\",\n+    \"twisted.web.client.ResponseFailed\",\n+    # IOError is raised by the HttpCompression middleware when trying to\n+    # decompress an empty response\n+    IOError,\n+    \"scrapy.core.downloader.handlers.http11.TunnelError\",\n+]\n \n ROBOTSTXT_OBEY = False\n ROBOTSTXT_PARSER = \"scrapy.robotstxt.ProtegoRobotParser\"\n\n@@ -1,5 +1,6 @@\n import logging\n import unittest\n+import warnings\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -15,6 +16,7 @@ from twisted.web.client import ResponseFailed\n from scrapy.downloadermiddlewares.retry import RetryMiddleware, get_retry_request\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response\n+from scrapy.settings.default_settings import RETRY_EXCEPTIONS\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n@@ -110,19 +112,48 @@ class RetryTest(unittest.TestCase):\n             == 2\n         )\n \n-    def _test_retry_exception(self, req, exception):\n+    def test_exception_to_retry_added(self):\n+        exc = ValueError\n+        settings_dict = {\n+            \"RETRY_EXCEPTIONS\": list(RETRY_EXCEPTIONS) + [exc],\n+        }\n+        crawler = get_crawler(Spider, settings_dict=settings_dict)\n+        mw = RetryMiddleware.from_crawler(crawler)\n+        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n+        self._test_retry_exception(req, exc(\"foo\"), mw)\n+\n+    def test_exception_to_retry_customMiddleware(self):\n+        exc = ValueError\n+\n+        with warnings.catch_warnings(record=True) as warns:\n+\n+            class MyRetryMiddleware(RetryMiddleware):\n+                EXCEPTIONS_TO_RETRY = RetryMiddleware.EXCEPTIONS_TO_RETRY + (exc,)\n+\n+            self.assertEqual(len(warns), 1)\n+\n+        mw2 = MyRetryMiddleware.from_crawler(self.crawler)\n+        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n+        req = mw2.process_exception(req, exc(\"foo\"), self.spider)\n+        assert isinstance(req, Request)\n+        self.assertEqual(req.meta[\"retry_times\"], 1)\n+\n+    def _test_retry_exception(self, req, exception, mw=None):\n+        if mw is None:\n+            mw = self.mw\n+\n         # first retry\n-        req = self.mw.process_exception(req, exception, self.spider)\n+        req = mw.process_exception(req, exception, self.spider)\n         assert isinstance(req, Request)\n         self.assertEqual(req.meta[\"retry_times\"], 1)\n \n         # second retry\n-        req = self.mw.process_exception(req, exception, self.spider)\n+        req = mw.process_exception(req, exception, self.spider)\n         assert isinstance(req, Request)\n         self.assertEqual(req.meta[\"retry_times\"], 2)\n \n         # discard it\n-        req = self.mw.process_exception(req, exception, self.spider)\n+        req = mw.process_exception(req, exception, self.spider)\n         self.assertEqual(req, None)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2122278d4bb7177a550bc0b04dd96d1fc1fad1a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1220 | Contributors (this commit): 39 | Commits (past 90d): 2 | Contributors (cumulative): 52 | DMM Complexity: None\n\nDIFF:\n@@ -34,8 +34,8 @@ twisted_version = (_txv.major, _txv.minor, _txv.micro)\n \n \n # Check minimum required Python version\n-if sys.version_info < (3, 7):\n-    print(f\"Scrapy {__version__} requires Python 3.7+\")\n+if sys.version_info < (3, 8):\n+    print(f\"Scrapy {__version__} requires Python 3.8+\")\n     sys.exit(1)\n \n \n\n@@ -80,7 +80,6 @@ setup(\n         \"Operating System :: OS Independent\",\n         \"Programming Language :: Python\",\n         \"Programming Language :: Python :: 3\",\n-        \"Programming Language :: Python :: 3.7\",\n         \"Programming Language :: Python :: 3.8\",\n         \"Programming Language :: Python :: 3.9\",\n         \"Programming Language :: Python :: 3.10\",\n@@ -91,7 +90,7 @@ setup(\n         \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n         \"Topic :: Software Development :: Libraries :: Python Modules\",\n     ],\n-    python_requires=\">=3.7\",\n+    python_requires=\">=3.8\",\n     install_requires=install_requires,\n     extras_require=extras_require,\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#075b89eab5e201246a5bddc4a6ce9c7921de082b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 791 | Contributors (this commit): 36 | Commits (past 90d): 2 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ def has_environment_marker_platform_impl_support():\n \n install_requires = [\n     \"Twisted>=18.9.0\",\n-    \"cryptography>=3.4.6\",\n+    \"cryptography>=36.0.0\",\n     \"cssselect>=0.9.1\",\n     \"itemloaders>=1.0.1\",\n     \"parsel>=1.5.0\",\n@@ -34,7 +34,7 @@ install_requires = [\n     \"setuptools\",\n     \"packaging\",\n     \"tldextract\",\n-    \"lxml>=4.3.0\",\n+    \"lxml>=4.4.1\",\n ]\n extras_require = {}\n cpython_dependencies = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
