{"custom_id": "scrapy#414dd1119a7e15c612de7bd0fe560b6ca30b505f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 727 | Contributors (this commit): 9 | Commits (past 90d): 10 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@ Spider Middleware manager\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n-from collections.abc import Iterable, AsyncIterable\n+from collections.abc import Iterable\n from itertools import islice\n from typing import Callable, Union, Any\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7dc857668f16e8c52ff44662aceb32f93ae3d80e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 1208 | Contributors (this commit): 24 | Commits (past 90d): 3 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -3,12 +3,14 @@ extracts information from them\"\"\"\n \n import logging\n from collections import deque\n+from collections.abc import Iterable\n+from typing import Union\n \n from itemadapter import is_item\n from twisted.internet import defer\n from twisted.python.failure import Failure\n \n-from scrapy import signals\n+from scrapy import signals, Spider\n from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy.http import Request, Response\n@@ -120,7 +122,7 @@ class Scraper:\n             response, request, deferred = slot.next_response_request_deferred()\n             self._scrape(response, request, spider).chainDeferred(deferred)\n \n-    def _scrape(self, result, request, spider):\n+    def _scrape(self, result: Union[Response, Failure], request: Request, spider: Spider):\n         \"\"\"\n         Handle the downloaded response or failure through the spider callback/errback\n         \"\"\"\n@@ -131,7 +133,7 @@ class Scraper:\n         dfd.addCallback(self.handle_spider_output, request, result, spider)\n         return dfd\n \n-    def _scrape2(self, result, request, spider):\n+    def _scrape2(self, result: Union[Response, Failure], request: Request, spider: Spider):\n         \"\"\"\n         Handle the different cases of request's result been a Response or a Failure\n         \"\"\"\n@@ -141,7 +143,7 @@ class Scraper:\n             dfd = self.call_spider(result, request, spider)\n             return dfd.addErrback(self._log_download_errors, result, request, spider)\n \n-    def call_spider(self, result, request, spider):\n+    def call_spider(self, result: Union[Response, Failure], request: Request, spider: Spider):\n         if isinstance(result, Response):\n             if getattr(result, \"request\", None) is None:\n                 result.request = request\n@@ -156,7 +158,7 @@ class Scraper:\n             dfd.addErrback(request.errback)\n         return dfd.addCallback(iterate_spider_output)\n \n-    def handle_spider_error(self, _failure, request, response, spider):\n+    def handle_spider_error(self, _failure: Failure, request: Request, response: Response, spider: Spider):\n         exc = _failure.value\n         if isinstance(exc, CloseSpider):\n             self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n@@ -177,7 +179,7 @@ class Scraper:\n             spider=spider\n         )\n \n-    def handle_spider_output(self, result, request, response, spider):\n+    def handle_spider_output(self, result: Iterable, request: Request, response: Response, spider: Spider):\n         if not result:\n             return defer_succeed(None)\n         it = iter_errback(result, self.handle_spider_error, request, response, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a71d6ef29da8e3dfa906fbaa74ee1db64567f856", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 502 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -418,7 +418,7 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n \n     def testPayloadDisabledCipher(self):\n         if sys.implementation.name == \"pypy\" and parse_version(cryptography.__version__) <= parse_version(\"2.3.1\"):\n-            self.skipTest(\"This does work in PyPy with cryptography<=2.3.1\")\n+            self.skipTest(\"This test expects a failure, but the code does work in PyPy with cryptography<=2.3.1\")\n         s = \"0123456789\" * 10\n         settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': 'ECDHE-RSA-AES256-GCM-SHA384'})\n         client_context_factory = create_instance(ScrapyClientContextFactory, settings=settings, crawler=None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91f81445524630843561c8a3c71b7fa0f081d783", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 56 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 56 | Churn Cumulative: 833 | Contributors (this commit): 17 | Commits (past 90d): 19 | Contributors (cumulative): 27 | DMM Complexity: 0.4634146341463415\n\nDIFF:\n@@ -4,14 +4,12 @@ Base class for Scrapy spiders\n See documentation in docs/topics/spiders.rst\n \"\"\"\n import logging\n-import warnings\n from typing import Optional\n \n from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n-from scrapy.utils.deprecate import method_is_overridden\n \n \n class Spider(object_ref):\n@@ -57,35 +55,14 @@ class Spider(object_ref):\n         crawler.signals.connect(self.close, signals.spider_closed)\n \n     def start_requests(self):\n-        cls = self.__class__\n         if not self.start_urls and hasattr(self, 'start_url'):\n             raise AttributeError(\n                 \"Crawling could not start: 'start_urls' not found \"\n                 \"or empty (but found 'start_url' attribute instead, \"\n                 \"did you miss an 's'?)\")\n-        if method_is_overridden(cls, Spider, 'make_requests_from_url'):\n-            warnings.warn(\n-                \"Spider.make_requests_from_url method is deprecated; it \"\n-                \"won't be called in future Scrapy releases. Please \"\n-                \"override Spider.start_requests method instead \"\n-                f\"(see {cls.__module__}.{cls.__name__}).\",\n-            )\n-            for url in self.start_urls:\n-                yield self.make_requests_from_url(url)\n-        else:\n         for url in self.start_urls:\n             yield Request(url, dont_filter=True)\n \n-    def make_requests_from_url(self, url):\n-        \"\"\" This method is deprecated. \"\"\"\n-        warnings.warn(\n-            \"Spider.make_requests_from_url method is deprecated: \"\n-            \"it will be removed and not be called by the default \"\n-            \"Spider.start_requests method in future Scrapy releases. \"\n-            \"Please override Spider.start_requests method instead.\"\n-        )\n-        return Request(url, dont_filter=True)\n-\n     def _parse(self, response, **kwargs):\n         return self.parse(response, **kwargs)\n \n\n@@ -584,39 +584,6 @@ class DeprecationTest(unittest.TestCase):\n         assert issubclass(CrawlSpider, Spider)\n         assert isinstance(CrawlSpider(name='foo'), Spider)\n \n-    def test_make_requests_from_url_deprecated(self):\n-        class MySpider4(Spider):\n-            name = 'spider1'\n-            start_urls = ['http://example.com']\n-\n-        class MySpider5(Spider):\n-            name = 'spider2'\n-            start_urls = ['http://example.com']\n-\n-            def make_requests_from_url(self, url):\n-                return Request(url + \"/foo\", dont_filter=True)\n-\n-        with warnings.catch_warnings(record=True) as w:\n-            # spider without overridden make_requests_from_url method\n-            # doesn't issue a warning\n-            spider1 = MySpider4()\n-            self.assertEqual(len(list(spider1.start_requests())), 1)\n-            self.assertEqual(len(w), 0)\n-\n-            # spider without overridden make_requests_from_url method\n-            # should issue a warning when called directly\n-            request = spider1.make_requests_from_url(\"http://www.example.com\")\n-            self.assertTrue(isinstance(request, Request))\n-            self.assertEqual(len(w), 1)\n-\n-            # spider with overridden make_requests_from_url issues a warning,\n-            # but the method still works\n-            spider2 = MySpider5()\n-            requests = list(spider2.start_requests())\n-            self.assertEqual(len(requests), 1)\n-            self.assertEqual(requests[0].url, 'http://example.com/foo')\n-            self.assertEqual(len(w), 2)\n-\n \n class NoParseMethodSpiderTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b6f77806b0ec414a28cfcbac3fa2d928040548d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 575 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -56,7 +56,7 @@ class TestSpider(Spider):\n     name_re = re.compile(r\"<h1>(.*?)</h1>\", re.M)\n     price_re = re.compile(r\">Price: \\$(.*?)<\", re.M)\n \n-    item_cls = TestItem\n+    item_cls: type = TestItem\n \n     def parse(self, response):\n         xlink = LinkExtractor()\n@@ -66,15 +66,15 @@ class TestSpider(Spider):\n                 yield Request(url=link.url, callback=self.parse_item)\n \n     def parse_item(self, response):\n-        item = self.item_cls()\n+        adapter = ItemAdapter(self.item_cls())\n         m = self.name_re.search(response.text)\n         if m:\n-            item['name'] = m.group(1)\n-        item['url'] = response.url\n+            adapter['name'] = m.group(1)\n+        adapter['url'] = response.url\n         m = self.price_re.search(response.text)\n         if m:\n-            item['price'] = m.group(1)\n-        return item\n+            adapter['price'] = m.group(1)\n+        return adapter.item\n \n \n class TestDupeFilterSpider(TestSpider):\n@@ -87,7 +87,7 @@ class DictItemsSpider(TestSpider):\n \n \n class AttrsItemsSpider(TestSpider):\n-    item_class = AttrsItem\n+    item_cls = AttrsItem\n \n \n try:\n@@ -97,14 +97,12 @@ except ImportError:\n else:\n     TestDataClass = make_dataclass(\"TestDataClass\", [(\"name\", str), (\"url\", str), (\"price\", int)])\n \n-    class DataClassItemsSpider(DictItemsSpider):\n+    class _dataclass_spider(DictItemsSpider):\n         def parse_item(self, response):\n             item = super().parse_item(response)\n-            return TestDataClass(\n-                name=item.get('name'),\n-                url=item.get('url'),\n-                price=item.get('price'),\n-            )\n+            return TestDataClass(**item)\n+\n+    DataClassItemsSpider = _dataclass_spider\n \n \n class ItemZeroDivisionErrorSpider(TestSpider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4673f05ddec85f18a890e606a6b85282035e26be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 1240 | Contributors (this commit): 24 | Commits (past 90d): 4 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -82,28 +82,26 @@ class Scraper:\n \n     def close_spider(self, spider):\n         \"\"\"Close a spider being scraped and release its resources\"\"\"\n-        slot = self.slot\n-        slot.closing = defer.Deferred()\n-        slot.closing.addCallback(self.itemproc.close_spider)\n-        self._check_if_closing(spider, slot)\n-        return slot.closing\n+        self.slot.closing = defer.Deferred()\n+        self.slot.closing.addCallback(self.itemproc.close_spider)\n+        self._check_if_closing(spider)\n+        return self.slot.closing\n \n     def is_idle(self):\n         \"\"\"Return True if there isn't any more spiders to process\"\"\"\n         return not self.slot\n \n-    def _check_if_closing(self, spider, slot):\n-        if slot.closing and slot.is_idle():\n-            slot.closing.callback(spider)\n+    def _check_if_closing(self, spider):\n+        if self.slot.closing and self.slot.is_idle():\n+            self.slot.closing.callback(spider)\n \n     def enqueue_scrape(self, response, request, spider):\n-        slot = self.slot\n-        dfd = slot.add_response_request(response, request)\n+        dfd = self.slot.add_response_request(response, request)\n \n         def finish_scraping(_):\n-            slot.finish_response(response, request)\n-            self._check_if_closing(spider, slot)\n-            self._scrape_next(spider, slot)\n+            self.slot.finish_response(response, request)\n+            self._check_if_closing(spider)\n+            self._scrape_next(spider)\n             return _\n \n         dfd.addBoth(finish_scraping)\n@@ -112,12 +110,12 @@ class Scraper:\n                                    {'request': request},\n                                    exc_info=failure_to_exc_info(f),\n                                    extra={'spider': spider}))\n-        self._scrape_next(spider, slot)\n+        self._scrape_next(spider)\n         return dfd\n \n-    def _scrape_next(self, spider, slot):\n-        while slot.queue:\n-            response, request, deferred = slot.next_response_request_deferred()\n+    def _scrape_next(self, spider):\n+        while self.slot.queue:\n+            response, request, deferred = self.slot.next_response_request_deferred()\n             self._scrape(response, request, spider).chainDeferred(deferred)\n \n     def _scrape(self, result, request, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d8d1dc5b508832598591ae9d3d3694f77f70492d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 579 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -97,13 +97,11 @@ except ImportError:\n else:\n     TestDataClass = make_dataclass(\"TestDataClass\", [(\"name\", str), (\"url\", str), (\"price\", int)])\n \n-    class _dataclass_spider(DictItemsSpider):\n+    class DataClassItemsSpider(DictItemsSpider):  # type: ignore[no-redef]\n         def parse_item(self, response):\n             item = super().parse_item(response)\n             return TestDataClass(**item)\n \n-    DataClassItemsSpider = _dataclass_spider\n-\n \n class ItemZeroDivisionErrorSpider(TestSpider):\n     custom_settings = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#76fa2257ef0280fc82e123457c791254cc2f185e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 37 | Files Changed: 3 | Hunks: 37 | Methods Changed: 36 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 81 | Churn Cumulative: 1503 | Contributors (this commit): 24 | Commits (past 90d): 19 | Contributors (cumulative): 37 | DMM Complexity: 0.9459459459459459\n\nDIFF:\n@@ -3,10 +3,10 @@ Spider Middleware manager\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n-from collections.abc import Iterable\n from itertools import islice\n-from typing import Callable, Union, Any\n+from typing import Callable, Union, Any, Generator, Iterable\n \n+from twisted.internet.defer import Deferred\n from twisted.python.failure import Failure\n \n from scrapy import Request, Spider\n@@ -18,13 +18,13 @@ from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.python import MutableChain\n \n \n-def _isiterable(o):\n-    return isinstance(o, Iterable)\n-\n-\n ScrapeFunc = Callable[[Union[Response, Failure], Request, Spider], Any]\n \n \n+def _isiterable(o) -> bool:\n+    return isinstance(o, Iterable)\n+\n+\n class SpiderMiddlewareManager(MiddlewareManager):\n \n     component_name = 'spider middleware'\n@@ -44,7 +44,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         process_spider_exception = getattr(mw, 'process_spider_exception', None)\n         self.methods['process_spider_exception'].appendleft(process_spider_exception)\n \n-    def _process_spider_input(self, scrape_func: ScrapeFunc, response: Response, request: Request, spider: Spider):\n+    def _process_spider_input(self, scrape_func: ScrapeFunc, response: Response, request: Request,\n+                              spider: Spider) -> Any:\n         for method in self.methods['process_spider_input']:\n             try:\n                 result = method(response=response, spider=spider)\n@@ -59,7 +60,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         return scrape_func(response, request, spider)\n \n     def _evaluate_iterable(self, response: Response, spider: Spider, iterable: Iterable,\n-                           exception_processor_index: int, recover_to: MutableChain):\n+                           exception_processor_index: int, recover_to: MutableChain) -> Generator:\n         try:\n             for r in iterable:\n                 yield r\n@@ -70,7 +71,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 raise\n             recover_to.extend(exception_result)\n \n-    def _process_spider_exception(self, response: Response, spider: Spider, _failure: Failure, start_index=0):\n+    def _process_spider_exception(self, response: Response, spider: Spider, _failure: Failure,\n+                                  start_index: int = 0) -> Union[Failure, MutableChain]:\n         exception = _failure.value\n         # don't handle _InvalidOutput exception\n         if isinstance(exception, _InvalidOutput):\n@@ -93,7 +95,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         return _failure\n \n     def _process_spider_output(self, response: Response, spider: Spider,\n-                               result: Iterable, start_index=0):\n+                               result: Iterable, start_index: int = 0) -> MutableChain:\n         # items in this iterable do not need to go through the process_spider_output\n         # chain, they went through it already from the process_spider_exception method\n         recovered = MutableChain()\n@@ -119,21 +121,22 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n         return MutableChain(result, recovered)\n \n-    def _process_callback_output(self, response: Response, spider: Spider, result: Iterable):\n+    def _process_callback_output(self, response: Response, spider: Spider, result: Iterable) -> MutableChain:\n         recovered = MutableChain()\n         result = self._evaluate_iterable(response, spider, result, 0, recovered)\n         return MutableChain(self._process_spider_output(response, spider, result), recovered)\n \n-    def scrape_response(self, scrape_func: ScrapeFunc, response: Response, request: Request, spider: Spider):\n-        def process_callback_output(result: Iterable):\n+    def scrape_response(self, scrape_func: ScrapeFunc, response: Response, request: Request,\n+                        spider: Spider) -> Deferred:\n+        def process_callback_output(result: Iterable) -> MutableChain:\n             return self._process_callback_output(response, spider, result)\n \n-        def process_spider_exception(_failure: Failure):\n+        def process_spider_exception(_failure: Failure) -> Union[Failure, MutableChain]:\n             return self._process_spider_exception(response, spider, _failure)\n \n         dfd = mustbe_deferred(self._process_spider_input, scrape_func, response, request, spider)\n         dfd.addCallbacks(callback=process_callback_output, errback=process_spider_exception)\n         return dfd\n \n-    def process_start_requests(self, start_requests, spider: Spider):\n+    def process_start_requests(self, start_requests, spider: Spider) -> Deferred:\n         return self._process_chain('process_start_requests', start_requests, spider)\n\n@@ -1,9 +1,13 @@\n import logging\n import pprint\n from collections import defaultdict, deque\n-from typing import Callable\n+from typing import Callable, Dict, Deque\n \n+from twisted.internet import defer\n+\n+from scrapy import Spider\n from scrapy.exceptions import NotConfigured\n+from scrapy.settings import Settings\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.defer import process_parallel, process_chain, process_chain_both\n \n@@ -17,16 +21,16 @@ class MiddlewareManager:\n \n     def __init__(self, *middlewares):\n         self.middlewares = middlewares\n-        self.methods: dict[str, deque[Callable]] = defaultdict(deque)\n+        self.methods: Dict[str, Deque[Callable]] = defaultdict(deque)\n         for mw in middlewares:\n             self._add_middleware(mw)\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings):\n+    def _get_mwlist_from_settings(cls, settings: Settings) -> list:\n         raise NotImplementedError\n \n     @classmethod\n-    def from_settings(cls, settings, crawler=None):\n+    def from_settings(cls, settings: Settings, crawler=None):\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n         enabled = []\n@@ -53,24 +57,24 @@ class MiddlewareManager:\n     def from_crawler(cls, crawler):\n         return cls.from_settings(crawler.settings, crawler)\n \n-    def _add_middleware(self, mw):\n+    def _add_middleware(self, mw) -> None:\n         if hasattr(mw, 'open_spider'):\n             self.methods['open_spider'].append(mw.open_spider)\n         if hasattr(mw, 'close_spider'):\n             self.methods['close_spider'].appendleft(mw.close_spider)\n \n-    def _process_parallel(self, methodname: str, obj, *args):\n+    def _process_parallel(self, methodname: str, obj, *args) -> defer.Deferred:\n         return process_parallel(self.methods[methodname], obj, *args)\n \n-    def _process_chain(self, methodname: str, obj, *args):\n+    def _process_chain(self, methodname: str, obj, *args) -> defer.Deferred:\n         return process_chain(self.methods[methodname], obj, *args)\n \n-    def _process_chain_both(self, cb_methodname: str, eb_methodname: str, obj, *args):\n+    def _process_chain_both(self, cb_methodname: str, eb_methodname: str, obj, *args) -> defer.Deferred:\n         return process_chain_both(self.methods[cb_methodname],\n                                   self.methods[eb_methodname], obj, *args)\n \n-    def open_spider(self, spider):\n+    def open_spider(self, spider: Spider) -> defer.Deferred:\n         return self._process_parallel('open_spider', spider)\n \n-    def close_spider(self, spider):\n+    def close_spider(self, spider: Spider) -> defer.Deferred:\n         return self._process_parallel('close_spider', spider)\n\n@@ -5,7 +5,7 @@ import asyncio\n import inspect\n from collections.abc import Coroutine\n from functools import wraps\n-from typing import Callable, Iterable, Any\n+from typing import Callable, Iterable, Any, Generator\n \n from twisted.internet import defer, task\n from twisted.python import failure\n@@ -15,7 +15,7 @@ from scrapy.exceptions import IgnoreRequest\n from scrapy.utils.reactor import is_asyncio_reactor_installed\n \n \n-def defer_fail(_failure: Failure):\n+def defer_fail(_failure: Failure) -> defer.Deferred:\n     \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n     next reactor loop\n \n@@ -28,7 +28,7 @@ def defer_fail(_failure: Failure):\n     return d\n \n \n-def defer_succeed(result):\n+def defer_succeed(result) -> defer.Deferred:\n     \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n     next reactor loop\n \n@@ -41,7 +41,7 @@ def defer_succeed(result):\n     return d\n \n \n-def defer_result(result):\n+def defer_result(result) -> defer.Deferred:\n     if isinstance(result, defer.Deferred):\n         return result\n     elif isinstance(result, failure.Failure):\n@@ -50,7 +50,7 @@ def defer_result(result):\n         return defer_succeed(result)\n \n \n-def mustbe_deferred(f: Callable, *args, **kw):\n+def mustbe_deferred(f: Callable, *args, **kw) -> defer.Deferred:\n     \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n     callback/errback to next reactor loop\n     \"\"\"\n@@ -67,7 +67,7 @@ def mustbe_deferred(f: Callable, *args, **kw):\n         return defer_result(result)\n \n \n-def parallel(iterable: Iterable, count: int, callable: Callable, *args, **named):\n+def parallel(iterable: Iterable, count: int, callable: Callable, *args, **named) -> defer.DeferredList:\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n \n@@ -78,7 +78,7 @@ def parallel(iterable: Iterable, count: int, callable: Callable, *args, **named)\n     return defer.DeferredList([coop.coiterate(work) for _ in range(count)])\n \n \n-def process_chain(callbacks: Iterable[Callable], input, *a, **kw):\n+def process_chain(callbacks: Iterable[Callable], input, *a, **kw) -> defer.Deferred:\n     \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n     d = defer.Deferred()\n     for x in callbacks:\n@@ -87,7 +87,7 @@ def process_chain(callbacks: Iterable[Callable], input, *a, **kw):\n     return d\n \n \n-def process_chain_both(callbacks: Iterable[Callable], errbacks: Iterable[Callable], input, *a, **kw):\n+def process_chain_both(callbacks: Iterable[Callable], errbacks: Iterable[Callable], input, *a, **kw) -> defer.Deferred:\n     \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n     d = defer.Deferred()\n     for cb, eb in zip(callbacks, errbacks):\n@@ -103,7 +103,7 @@ def process_chain_both(callbacks: Iterable[Callable], errbacks: Iterable[Callabl\n     return d\n \n \n-def process_parallel(callbacks: Iterable[Callable], input, *a, **kw):\n+def process_parallel(callbacks: Iterable[Callable], input, *a, **kw) -> defer.Deferred:\n     \"\"\"Return a Deferred with the output of all successful calls to the given\n     callbacks\n     \"\"\"\n@@ -113,7 +113,7 @@ def process_parallel(callbacks: Iterable[Callable], input, *a, **kw):\n     return d\n \n \n-def iter_errback(iterable: Iterable, errback: Callable, *a, **kw):\n+def iter_errback(iterable: Iterable, errback: Callable, *a, **kw) -> Generator:\n     \"\"\"Wraps an iterable calling an errback if an error is caught while\n     iterating it.\n     \"\"\"\n@@ -142,7 +142,7 @@ def deferred_from_coro(o) -> Any:\n     return o\n \n \n-def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]):\n+def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]) -> Callable:\n     \"\"\" Converts a coroutine function into a function that returns a Deferred.\n \n     The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.\n@@ -154,7 +154,7 @@ def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]):\n     return f\n \n \n-def maybeDeferred_coro(f: Callable, *args, **kw):\n+def maybeDeferred_coro(f: Callable, *args, **kw) -> defer.Deferred:\n     \"\"\" Copy of defer.maybeDeferred that also converts coroutines to Deferreds. \"\"\"\n     try:\n         result = f(*args, **kw)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#335a25675278543d4c85123bbbed99f228b26416", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 762 | Contributors (this commit): 9 | Commits (past 90d): 12 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ Spider Middleware manager\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n from itertools import islice\n-from typing import Callable, Union, Any, Generator, Iterable\n+from typing import Any, Callable, Generator, Iterable, Union\n \n from twisted.internet.defer import Deferred\n from twisted.python.failure import Failure\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0e75125749ec1dce14614468bf06cd8e8842649", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 187 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n import logging\n import pprint\n from collections import defaultdict, deque\n-from typing import Callable, Dict, Deque\n+from typing import Callable, Deque, Dict\n \n from twisted.internet import defer\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8de04c823f5e10d012aaa2dd94a4f5a9f70b119", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 560 | Contributors (this commit): 17 | Commits (past 90d): 7 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ import asyncio\n import inspect\n from collections.abc import Coroutine\n from functools import wraps\n-from typing import Callable, Iterable, Any, Generator\n+from typing import Any, Callable, Generator, Iterable\n \n from twisted.internet import defer, task\n from twisted.python import failure\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cef0a8b3d653d847efe32dfc2850e5992b627408", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 199 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@ import pprint\n from collections import defaultdict, deque\n from typing import Callable, Deque, Dict\n \n-from twisted.internet import defer\n+from twisted.internet.defer import Deferred\n \n from scrapy import Spider\n from scrapy.exceptions import NotConfigured\n@@ -63,18 +63,18 @@ class MiddlewareManager:\n         if hasattr(mw, 'close_spider'):\n             self.methods['close_spider'].appendleft(mw.close_spider)\n \n-    def _process_parallel(self, methodname: str, obj, *args) -> defer.Deferred:\n+    def _process_parallel(self, methodname: str, obj, *args) -> Deferred:\n         return process_parallel(self.methods[methodname], obj, *args)\n \n-    def _process_chain(self, methodname: str, obj, *args) -> defer.Deferred:\n+    def _process_chain(self, methodname: str, obj, *args) -> Deferred:\n         return process_chain(self.methods[methodname], obj, *args)\n \n-    def _process_chain_both(self, cb_methodname: str, eb_methodname: str, obj, *args) -> defer.Deferred:\n+    def _process_chain_both(self, cb_methodname: str, eb_methodname: str, obj, *args) -> Deferred:\n         return process_chain_both(self.methods[cb_methodname],\n                                   self.methods[eb_methodname], obj, *args)\n \n-    def open_spider(self, spider: Spider) -> defer.Deferred:\n+    def open_spider(self, spider: Spider) -> Deferred:\n         return self._process_parallel('open_spider', spider)\n \n-    def close_spider(self, spider: Spider) -> defer.Deferred:\n+    def close_spider(self, spider: Spider) -> Deferred:\n         return self._process_parallel('close_spider', spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#08e4eaf97369ba6daa4b5d84e00fd4d36b78e00a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 22 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 608 | Contributors (this commit): 17 | Commits (past 90d): 8 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,9 @@ from collections.abc import Coroutine\n from functools import wraps\n from typing import Any, Callable, Generator, Iterable\n \n-from twisted.internet import defer, task\n+from twisted.internet import defer\n+from twisted.internet.defer import Deferred, DeferredList, ensureDeferred\n+from twisted.internet.task import Cooperator\n from twisted.python import failure\n from twisted.python.failure import Failure\n \n@@ -15,7 +17,7 @@ from scrapy.exceptions import IgnoreRequest\n from scrapy.utils.reactor import is_asyncio_reactor_installed\n \n \n-def defer_fail(_failure: Failure) -> defer.Deferred:\n+def defer_fail(_failure: Failure) -> Deferred:\n     \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n     next reactor loop\n \n@@ -23,12 +25,12 @@ def defer_fail(_failure: Failure) -> defer.Deferred:\n     before attending pending delayed calls, so do not set delay to zero.\n     \"\"\"\n     from twisted.internet import reactor\n-    d = defer.Deferred()\n+    d = Deferred()\n     reactor.callLater(0.1, d.errback, _failure)\n     return d\n \n \n-def defer_succeed(result) -> defer.Deferred:\n+def defer_succeed(result) -> Deferred:\n     \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n     next reactor loop\n \n@@ -36,13 +38,13 @@ def defer_succeed(result) -> defer.Deferred:\n     before attending pending delayed calls, so do not set delay to zero.\n     \"\"\"\n     from twisted.internet import reactor\n-    d = defer.Deferred()\n+    d = Deferred()\n     reactor.callLater(0.1, d.callback, result)\n     return d\n \n \n-def defer_result(result) -> defer.Deferred:\n-    if isinstance(result, defer.Deferred):\n+def defer_result(result) -> Deferred:\n+    if isinstance(result, Deferred):\n         return result\n     elif isinstance(result, failure.Failure):\n         return defer_fail(result)\n@@ -50,7 +52,7 @@ def defer_result(result) -> defer.Deferred:\n         return defer_succeed(result)\n \n \n-def mustbe_deferred(f: Callable, *args, **kw) -> defer.Deferred:\n+def mustbe_deferred(f: Callable, *args, **kw) -> Deferred:\n     \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n     callback/errback to next reactor loop\n     \"\"\"\n@@ -67,29 +69,29 @@ def mustbe_deferred(f: Callable, *args, **kw) -> defer.Deferred:\n         return defer_result(result)\n \n \n-def parallel(iterable: Iterable, count: int, callable: Callable, *args, **named) -> defer.DeferredList:\n+def parallel(iterable: Iterable, count: int, callable: Callable, *args, **named) -> DeferredList:\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n \n     Taken from: https://jcalderone.livejournal.com/24285.html\n     \"\"\"\n-    coop = task.Cooperator()\n+    coop = Cooperator()\n     work = (callable(elem, *args, **named) for elem in iterable)\n-    return defer.DeferredList([coop.coiterate(work) for _ in range(count)])\n+    return DeferredList([coop.coiterate(work) for _ in range(count)])\n \n \n-def process_chain(callbacks: Iterable[Callable], input, *a, **kw) -> defer.Deferred:\n+def process_chain(callbacks: Iterable[Callable], input, *a, **kw) -> Deferred:\n     \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n-    d = defer.Deferred()\n+    d = Deferred()\n     for x in callbacks:\n         d.addCallback(x, *a, **kw)\n     d.callback(input)\n     return d\n \n \n-def process_chain_both(callbacks: Iterable[Callable], errbacks: Iterable[Callable], input, *a, **kw) -> defer.Deferred:\n+def process_chain_both(callbacks: Iterable[Callable], errbacks: Iterable[Callable], input, *a, **kw) -> Deferred:\n     \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n-    d = defer.Deferred()\n+    d = Deferred()\n     for cb, eb in zip(callbacks, errbacks):\n         d.addCallbacks(\n             callback=cb, errback=eb,\n@@ -103,12 +105,12 @@ def process_chain_both(callbacks: Iterable[Callable], errbacks: Iterable[Callabl\n     return d\n \n \n-def process_parallel(callbacks: Iterable[Callable], input, *a, **kw) -> defer.Deferred:\n+def process_parallel(callbacks: Iterable[Callable], input, *a, **kw) -> Deferred:\n     \"\"\"Return a Deferred with the output of all successful calls to the given\n     callbacks\n     \"\"\"\n     dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n-    d = defer.DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n+    d = DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n     d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n     return d\n \n@@ -129,16 +131,16 @@ def iter_errback(iterable: Iterable, errback: Callable, *a, **kw) -> Generator:\n \n def deferred_from_coro(o) -> Any:\n     \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n-    if isinstance(o, defer.Deferred):\n+    if isinstance(o, Deferred):\n         return o\n     if asyncio.isfuture(o) or inspect.isawaitable(o):\n         if not is_asyncio_reactor_installed():\n             # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n             # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n-            return defer.ensureDeferred(o)\n+            return ensureDeferred(o)\n         else:\n             # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n-            return defer.Deferred.fromFuture(asyncio.ensure_future(o))\n+            return Deferred.fromFuture(asyncio.ensure_future(o))\n     return o\n \n \n@@ -154,14 +156,14 @@ def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]) -> Callable:\n     return f\n \n \n-def maybeDeferred_coro(f: Callable, *args, **kw) -> defer.Deferred:\n+def maybeDeferred_coro(f: Callable, *args, **kw) -> Deferred:\n     \"\"\" Copy of defer.maybeDeferred that also converts coroutines to Deferreds. \"\"\"\n     try:\n         result = f(*args, **kw)\n     except:  # noqa: E722\n-        return defer.fail(failure.Failure(captureVars=defer.Deferred.debug))\n+        return defer.fail(failure.Failure(captureVars=Deferred.debug))\n \n-    if isinstance(result, defer.Deferred):\n+    if isinstance(result, Deferred):\n         return result\n     elif asyncio.isfuture(result) or inspect.isawaitable(result):\n         return deferred_from_coro(result)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#61197d3dba53cd67c41e5d23e1f0c11a864f539b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 57 | Files Changed: 5 | Hunks: 46 | Methods Changed: 41 | Complexity Δ (Sum/Max): 47/19 | Churn Δ: 128 | Churn Cumulative: 3721 | Contributors (this commit): 45 | Commits (past 90d): 36 | Contributors (cumulative): 77 | DMM Complexity: 0.8947368421052632\n\nDIFF:\n@@ -1,10 +1,8 @@\n \"\"\"This module implements the Scraper component which parses responses and\n extracts information from them\"\"\"\n-import collections\n import logging\n from collections import deque\n-from collections.abc import Iterable\n-from typing import Union\n+from typing import AsyncGenerator, AsyncIterable, Generator, Iterable, Union\n \n from itemadapter import is_item\n from twisted.internet import defer\n@@ -188,7 +186,8 @@ class Scraper:\n     def handle_spider_output(self, result: Iterable, request: Request, response: Response, spider: Spider):\n         if not result:\n             return defer_succeed(None)\n-        if isinstance(result, collections.abc.AsyncIterable):\n+        it: Union[Generator, AsyncGenerator]\n+        if isinstance(result, AsyncIterable):\n             it = aiter_errback(result, self.handle_spider_error, request, response, spider)\n             dfd = parallel_async(it, self.concurrent_items, self._process_spidermw_output,\n                                  request, response, spider)\n\n@@ -3,9 +3,8 @@ Spider Middleware manager\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n-import collections.abc\n from itertools import islice\n-from typing import Any, Callable, Generator, Iterable, Union, AsyncIterable\n+from typing import Any, Callable, Generator, Iterable, Union, AsyncIterable, AsyncGenerator\n \n from twisted.internet.defer import Deferred\n from twisted.python.failure import Failure\n@@ -61,8 +60,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 return scrape_func(Failure(), request, spider)\n         return scrape_func(response, request, spider)\n \n-    def _evaluate_iterable(self, response: Response, spider: Spider, iterable: Iterable,\n-                           exception_processor_index: int, recover_to: MutableChain) -> Generator:\n+    def _evaluate_iterable(self, response: Response, spider: Spider, iterable: Union[Iterable, AsyncIterable],\n+                           exception_processor_index: int, recover_to: Union[MutableChain, MutableAsyncChain]\n+                           ) -> Union[Generator, AsyncGenerator]:\n         @_process_iterable_universal\n         async def _evaluate_async_iterable(iterable):\n             try:\n@@ -100,11 +100,13 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         return _failure\n \n     def _process_spider_output(self, response: Response, spider: Spider,\n-                               result: Iterable, start_index: int = 0) -> MutableChain:\n+                               result: Union[Iterable, AsyncIterable], start_index: int = 0\n+                               ) -> Union[MutableChain, MutableAsyncChain]:\n         # items in this iterable do not need to go through the process_spider_output\n         # chain, they went through it already from the process_spider_exception method\n-        last_result_async = isinstance(result, collections.abc.AsyncIterator)\n-        if last_result_async:\n+        recovered: Union[MutableChain, MutableAsyncChain]\n+        last_result_is_async = isinstance(result, AsyncIterable)\n+        if last_result_is_async:\n             recovered = MutableAsyncChain()\n         else:\n             recovered = MutableChain()\n@@ -127,30 +129,32 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 msg = (f\"Middleware {method.__qualname__} must return an \"\n                        f\"iterable, got {type(result)}\")\n                 raise _InvalidOutput(msg)\n-            if last_result_async and isinstance(result, collections.abc.Iterator):\n+            if last_result_is_async and isinstance(result, Iterable):\n                 raise TypeError(f\"Synchronous {method.__qualname__} called with an async iterable\")\n-            last_result_async = isinstance(result, collections.abc.AsyncIterator)\n+            last_result_is_async = isinstance(result, AsyncIterable)\n \n-        if last_result_async:\n+        if last_result_is_async:\n             return MutableAsyncChain(result, recovered)\n         else:\n-            return MutableChain(result, recovered)\n+            return MutableChain(result, recovered)  # type: ignore[arg-type]\n \n-    def _process_callback_output(self, response: Response, spider: Spider, result: Iterable) -> MutableChain:\n-        if isinstance(result, collections.abc.AsyncIterator):\n+    def _process_callback_output(self, response: Response, spider: Spider, result: Union[Iterable, AsyncIterable]\n+                                 ) -> Union[MutableChain, MutableAsyncChain]:\n+        recovered: Union[MutableChain, MutableAsyncChain]\n+        if isinstance(result, AsyncIterable):\n             recovered = MutableAsyncChain()\n         else:\n             recovered = MutableChain()\n         result = self._evaluate_iterable(response, spider, result, 0, recovered)\n         result = self._process_spider_output(response, spider, result)\n-        if isinstance(result, collections.abc.AsyncIterator):\n+        if isinstance(result, AsyncIterable):\n             return MutableAsyncChain(result, recovered)\n         else:\n-            return MutableChain(result, recovered)\n+            return MutableChain(result, recovered)  # type: ignore[arg-type]\n \n     def scrape_response(self, scrape_func: ScrapeFunc, response: Response, request: Request,\n                         spider: Spider) -> Deferred:\n-        def process_callback_output(result: Iterable) -> MutableChain:\n+        def process_callback_output(result: Union[Iterable, AsyncIterable]) -> Union[MutableChain, MutableAsyncChain]:\n             return self._process_callback_output(response, spider, result)\n \n         def process_spider_exception(_failure: Failure) -> Union[Failure, MutableChain]:\n\n@@ -1,7 +1,6 @@\n-import collections\n import functools\n import inspect\n-from collections.abc import AsyncIterable\n+from typing import AsyncGenerator, AsyncIterable, Callable, Generator, Iterable, Union\n \n \n async def collect_asyncgen(result: AsyncIterable):\n@@ -11,9 +10,9 @@ async def collect_asyncgen(result: AsyncIterable):\n     return results\n \n \n-async def as_async_generator(it):\n-    \"\"\" Wraps an iterator (sync or async) into an async generator. \"\"\"\n-    if isinstance(it, collections.abc.AsyncIterator):\n+async def as_async_generator(it: Union[Iterable, AsyncIterable]) -> AsyncGenerator:\n+    \"\"\" Wraps an iterable (sync or async) into an async generator. \"\"\"\n+    if isinstance(it, AsyncIterable):\n         async for r in it:\n             yield r\n     else:\n@@ -22,7 +21,7 @@ async def as_async_generator(it):\n \n \n # https://stackoverflow.com/a/66170760/113586\n-def _process_iterable_universal(process_async):\n+def _process_iterable_universal(process_async: Callable):\n     \"\"\" Takes a function that takes an async iterable, args and kwargs. Returns\n     a function that takes any iterable, args and kwargs.\n \n@@ -33,7 +32,7 @@ def _process_iterable_universal(process_async):\n     # If this stops working, all internal uses can be just replaced with manually-written\n     # process_sync functions.\n \n-    def process_sync(iterable, *args, **kwargs):\n+    def process_sync(iterable: Iterable, *args, **kwargs) -> Generator:\n         agen = process_async(as_async_generator(iterable), *args, **kwargs)\n         if not inspect.isasyncgen(agen):\n             raise ValueError(f\"process_async returned wrong type {type(agen)}\")\n@@ -52,11 +51,11 @@ def _process_iterable_universal(process_async):\n                           f\"you can't use {_process_iterable_universal.__name__} with it.\")\n \n     @functools.wraps(process_async)\n-    def process(iterable, *args, **kwargs):\n-        if inspect.isasyncgen(iterable):\n+    def process(iterable: Union[Iterable, AsyncIterable], *args, **kwargs) -> Union[Generator, AsyncGenerator]:\n+        if isinstance(iterable, AsyncIterable):\n             # call process_async directly\n             return process_async(iterable, *args, **kwargs)\n-        if hasattr(iterable, '__iter__'):\n+        if isinstance(iterable, Iterable):\n             # convert process_async to process_sync\n             return process_sync(iterable, *args, **kwargs)\n         raise TypeError(f\"Wrong iterable type {type(iterable)}\")\n\n@@ -3,9 +3,21 @@ Helper functions for dealing with Twisted deferreds\n \"\"\"\n import asyncio\n import inspect\n-from collections.abc import Coroutine\n+from asyncio import Future\n from functools import wraps\n-from typing import Any, Callable, Generator, Iterable\n+from typing import (\n+    Any,\n+    AsyncGenerator,\n+    AsyncIterable,\n+    Callable,\n+    Coroutine,\n+    Generator,\n+    Iterable,\n+    Iterator,\n+    List,\n+    Optional,\n+    Union\n+)\n \n from twisted.internet import defer\n from twisted.internet.defer import Deferred, DeferredList, ensureDeferred\n@@ -80,8 +92,8 @@ def parallel(iterable: Iterable, count: int, callable: Callable, *args, **named)\n     return DeferredList([coop.coiterate(work) for _ in range(count)])\n \n \n-class _AsyncCooperatorAdapter:\n-    \"\"\" A class that wraps an async iterator into a normal iterator suitable\n+class _AsyncCooperatorAdapter(Iterator):\n+    \"\"\" A class that wraps an async iterable into a normal iterator suitable\n     for using in Cooperator.coiterate(). As it's only needed for parallel_async(),\n     it calls the callable directly in the callback, instead of providing a more\n     generic interface.\n@@ -125,30 +137,30 @@ class _AsyncCooperatorAdapter:\n     Cooperator/CooperativeTask and use it instead of this adapter to achieve the same\n     goal.\n     \"\"\"\n-    def __init__(self, aiterator, callable, *callable_args, **callable_kwargs):\n-        self.aiterator = aiterator\n+    def __init__(self, aiterable: AsyncIterable, callable: Callable, *callable_args, **callable_kwargs):\n+        self.aiterator = aiterable.__aiter__()\n         self.callable = callable\n         self.callable_args = callable_args\n         self.callable_kwargs = callable_kwargs\n         self.finished = False\n-        self.waiting_deferreds = []\n-        self.anext_deferred = None\n+        self.waiting_deferreds: List[Deferred] = []\n+        self.anext_deferred: Optional[Deferred] = None\n \n-    def _callback(self, result):\n+    def _callback(self, result: Any) -> None:\n         # This gets called when the result from aiterator.__anext__() is available.\n         # It calls the callable on it and sends the result to the oldest waiting Deferred\n         # (by chaining if the result is a Deferred too or by firing if not).\n         self.anext_deferred = None\n         result = self.callable(result, *self.callable_args, **self.callable_kwargs)\n         d = self.waiting_deferreds.pop(0)\n-        if isinstance(result, defer.Deferred):\n+        if isinstance(result, Deferred):\n             result.chainDeferred(d)\n         else:\n             d.callback(None)\n         if self.waiting_deferreds:\n             self._call_anext()\n \n-    def _errback(self, failure):\n+    def _errback(self, failure: Failure) -> None:\n         # This gets called on any exceptions in aiterator.__anext__().\n         # It handles StopAsyncIteration by stopping the iteration and reraises all others.\n         self.anext_deferred = None\n@@ -157,29 +169,29 @@ class _AsyncCooperatorAdapter:\n         for d in self.waiting_deferreds:\n             d.callback(None)\n \n-    def _call_anext(self):\n+    def _call_anext(self) -> None:\n         # This starts waiting for the next result from aiterator.\n         # If aiterator is exhausted, _errback will be called.\n         self.anext_deferred = deferred_from_coro(self.aiterator.__anext__())\n         self.anext_deferred.addCallbacks(self._callback, self._errback)\n \n-    def __next__(self):\n+    def __next__(self) -> Deferred:\n         # This puts a new Deferred into self.waiting_deferreds and returns it.\n         # It also calls __anext__() if needed.\n         if self.finished:\n             raise StopIteration\n-        d = defer.Deferred()\n+        d = Deferred()\n         self.waiting_deferreds.append(d)\n         if not self.anext_deferred:\n             self._call_anext()\n         return d\n \n \n-def parallel_async(async_iterable, count, callable, *args, **named):\n-    \"\"\" Like parallel but for async iterables \"\"\"\n+def parallel_async(async_iterable: AsyncIterable, count: int, callable: Callable, *args, **named) -> DeferredList:\n+    \"\"\" Like parallel but for async iterators \"\"\"\n     coop = Cooperator()\n     work = _AsyncCooperatorAdapter(async_iterable, callable, *args, **named)\n-    dl = defer.DeferredList([coop.coiterate(work) for _ in range(count)])\n+    dl = DeferredList([coop.coiterate(work) for _ in range(count)])\n     return dl\n \n \n@@ -232,7 +244,7 @@ def iter_errback(iterable: Iterable, errback: Callable, *a, **kw) -> Generator:\n             errback(failure.Failure(), *a, **kw)\n \n \n-async def aiter_errback(aiterable, errback, *a, **kw):\n+async def aiter_errback(aiterable: AsyncIterable, errback: Callable, *a, **kw) -> AsyncGenerator:\n     \"\"\"Wraps an async iterable calling an errback if an error is caught while\n     iterating it. Similar to scrapy.utils.defer.iter_errback()\n     \"\"\"\n@@ -290,13 +302,13 @@ def maybeDeferred_coro(f: Callable, *args, **kw) -> Deferred:\n         return defer.succeed(result)\n \n \n-def deferred_to_future(d):\n+def deferred_to_future(d: Deferred) -> Future:\n     \"\"\" Wraps a Deferred into a Future. Requires the asyncio reactor.\n     \"\"\"\n     return d.asFuture(asyncio.get_event_loop())\n \n \n-def maybe_deferred_to_future(d):\n+def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n     \"\"\" Converts a Deferred to something that can be awaited in a callback or other user coroutine.\n \n     If the asyncio reactor is installed, coroutines are wrapped into Futures, and only Futures can be\n\n@@ -8,9 +8,9 @@ import re\n import sys\n import warnings\n import weakref\n-from collections.abc import Iterable\n from functools import partial, wraps\n from itertools import chain\n+from typing import AsyncIterable, Iterable, Union\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.asyncgen import as_async_generator\n@@ -345,7 +345,7 @@ class MutableChain(Iterable):\n     def __init__(self, *args: Iterable):\n         self.data = chain.from_iterable(args)\n \n-    def extend(self, *iterables: Iterable):\n+    def extend(self, *iterables: Iterable) -> None:\n         self.data = chain(self.data, chain.from_iterable(iterables))\n \n     def __iter__(self):\n@@ -359,22 +359,22 @@ class MutableChain(Iterable):\n         return self.__next__()\n \n \n-async def _async_chain(*iterables):\n+async def _async_chain(*iterables: Union[Iterable, AsyncIterable]):\n     for it in iterables:\n         async for o in as_async_generator(it):\n             yield o\n \n \n-class MutableAsyncChain:\n+class MutableAsyncChain(AsyncIterable):\n     \"\"\"\n     Similar to MutableChain but for async iterables\n     \"\"\"\n \n-    def __init__(self, *args):\n+    def __init__(self, *args: Union[Iterable, AsyncIterable]):\n         self.data = _async_chain(*args)\n \n-    def extend(self, *aiterables):\n-        self.data = _async_chain(self.data, _async_chain(*aiterables))\n+    def extend(self, *iterables: Union[Iterable, AsyncIterable]) -> None:\n+        self.data = _async_chain(self.data, _async_chain(*iterables))\n \n     def __aiter__(self):\n         return self\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
