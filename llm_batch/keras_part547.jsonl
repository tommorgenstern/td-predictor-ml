{"custom_id": "keras#a8c426fc595b82f64d53310c11f11620d1ca0cd9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 126 | Files Changed: 34 | Hunks: 60 | Methods Changed: 42 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 213 | Churn Cumulative: 15813 | Contributors (this commit): 6 | Commits (past 90d): 292 | Contributors (cumulative): 107 | DMM Complexity: 0.4666666666666667\n\nDIFF:\n@@ -15,3 +15,7 @@ def get(identifier):\n     if identifier == \"relu\":\n         return relu\n     return identifier\n+\n+\n+def serialize(activation):\n+    return activation.__name__\n\n@@ -28,10 +28,7 @@ class KerasVariable:\n         raise NotImplementedError\n \n     def __repr__(self):\n-        return (\n-            f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, \"\n-            \"name={self.name}>\"\n-        )\n+        return f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, name={self.name}>\"\n \n \n ALLOWED_DTYPES = {\n@@ -73,12 +70,10 @@ def standardize_shape(shape, fully_defined=False):\n             continue\n         if not isinstance(e, int):\n             raise ValueError(\n-                f\"Cannot convert '{shape}' to a shape. \"\n-                f\"Found invalid entry '{e}'\"\n+                f\"Cannot convert '{shape}' to a shape. Found invalid entry '{e}'\"\n             )\n         if e < 0:\n             raise ValueError(\n-                f\"Cannot convert '{shape}' to a shape. \"\n-                \"Negative dimensions are not allowed.\"\n+                f\"Cannot convert '{shape}' to a shape. Negative dimensions are not allowed.\"\n             )\n     return shape\n\n@@ -214,8 +214,7 @@ class JAXTrainer(base_trainer.Trainer):\n                     break\n \n             # Update variable values\n-            # NOTE: doing this after each step would be a big performance\n-            # bottleneck.\n+            # NOTE: doing this after each step would be a big performance bottleneck.\n             for ref_v, v in zip(self.trainable_variables, trainable_variables):\n                 ref_v.assign(v)\n             for ref_v, v in zip(\n@@ -400,8 +399,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 metrics_variables,\n             )\n             logs, state = test_step(state, data)\n-            # Note that trainable variables are not returned since they're\n-            # immutable here.\n+            # Note that trainable variables are not returned since they're immutable here.\n             non_trainable_variables, metrics_variables = state\n \n             callbacks.on_test_batch_end(step, logs)\n@@ -436,7 +434,7 @@ class JAXTrainer(base_trainer.Trainer):\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 # Build model\n-                self(data)\n+                y_pred = self(data)\n                 break\n \n         # Container that configures and calls callbacks.\n\n@@ -21,10 +21,7 @@ class KerasTensor:\n         return operations.Cast(dtype=dtype)(self)\n \n     def __repr__(self):\n-        return (\n-            f\"<KerasTensor shape={self.shape}, dtype={self.dtype}, \"\n-            \"name={self.name}>\"\n-        )\n+        return f\"<KerasTensor shape={self.shape}, dtype={self.dtype}, name={self.name}>\"\n \n     def __iter__(self):\n         raise NotImplementedError(\n\n@@ -28,8 +28,7 @@ class StatelessScope:\n                     \"Invalid variable value in VariableSwapScope: \"\n                     \"all values in argument `mapping` must be tensors with \"\n                     \"a shape that matches the corresponding variable shape. \"\n-                    f\"For variable {k}, received invalid value {v} with shape \"\n-                    f\"{v.shape}.\"\n+                    f\"For variable {k}, received invalid value {v} with shape {v.shape}.\"\n                 )\n             self.state_mapping[id(k)] = v\n \n\n@@ -58,7 +58,7 @@ class Variable(KerasVariable, tf.__internal__.types.Tensor):\n     def ndim(self):\n         return self.value.ndim\n \n-    def numpy(self):  # noqa: F811\n+    def numpy(self):\n         return self.value.numpy()\n \n     # Overload native accessor.\n\n@@ -26,8 +26,8 @@ class CallbackList(Callback):\n \n         Args:\n             callbacks: List of `Callback` instances.\n-            add_history: Whether a `History` callback should be added, if one\n-                does not already exist in the `callbacks` list.\n+            add_history: Whether a `History` callback should be added, if one does\n+                not already exist in the `callbacks` list.\n             add_progbar: Whether a `ProgbarLogger` callback should be added, if\n                 one does not already exist in the `callbacks` list.\n             model: The `Model` these callbacks are used with.\n\n@@ -57,9 +57,9 @@ class Zeros(Initializer):\n         Args:\n             shape: Shape of the tensor.\n             dtype: Optional dtype of the tensor. Only numeric or boolean dtypes\n-                are supported. If not specified, `keras_core.backend.floatx()`\n-                is used, which default to `float32` unless you configured it\n-                otherwise (via `keras_core.backend.set_floatx(float_dtype)`).\n+                are supported. If not specified, `keras_core.backend.floatx()` is\n+                used, which default to `float32` unless you configured it otherwise\n+                (via `keras_core.backend.set_floatx(float_dtype)`).\n             **kwargs: Additional keyword arguments.\n         \"\"\"\n         dtype = standardize_dtype(dtype)\n@@ -89,9 +89,9 @@ class Ones(Initializer):\n         Args:\n             shape: Shape of the tensor.\n             dtype: Optional dtype of the tensor. Only numeric or boolean dtypes\n-                are supported. If not specified, `keras_core.backend.floatx()`\n-                is used, which default to `float32` unless you configured it\n-                otherwise (via `keras_core.backend.set_floatx(float_dtype)`).\n+                are supported. If not specified, `keras_core.backend.floatx()` is\n+                used, which default to `float32` unless you configured it otherwise\n+                (via `keras_core.backend.set_floatx(float_dtype)`).\n             **kwargs: Additional keyword arguments.\n         \"\"\"\n         dtype = standardize_dtype(dtype)\n\n@@ -41,8 +41,8 @@ class Initializer:\n \n     Note that we don't have to implement `from_config()` in the example above\n     since the constructor arguments of the class the keys in the config returned\n-    by `get_config()` are the same. In this case, the default `from_config()`\n-    works fine.\n+    by `get_config()` are the same. In this case, the default `from_config()` works\n+    fine.\n     \"\"\"\n \n     def __call__(self, shape, dtype=None):\n\n@@ -61,10 +61,15 @@ class Dense(Layer):\n         # TODO\n         config = {\n             \"units\": self.units,\n+            \"activation\": activations.serialize(self.activation),\n             \"use_bias\": self.use_bias,\n             \"kernel_initializer\": initializers.serialize(\n                 self.kernel_initializer\n             ),\n             \"bias_initializer\": initializers.serialize(self.bias_initializer),\n+            \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\n+            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n+            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n+            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n         }\n         return {**base_config, **config}\n\n@@ -17,13 +17,11 @@ class InputLayer(Layer):\n         super().__init__(name=name)\n         if shape is not None and batch_shape is not None:\n             raise ValueError(\n-                \"You cannot pass both `shape` and `batch_shape` at the \"\n-                \"same time.\"\n+                \"You cannot pass both `shape` and `batch_shape` at the same time.\"\n             )\n         if batch_size is not None and batch_shape is not None:\n             raise ValueError(\n-                \"You cannot pass both `batch_size` and `batch_shape` at the \"\n-                \"same time.\"\n+                \"You cannot pass both `batch_size` and `batch_shape` at the same time.\"\n             )\n         if shape is None and batch_shape is None:\n             raise ValueError(\"You must pass a `shape` argument.\")\n@@ -38,8 +36,7 @@ class InputLayer(Layer):\n             if not isinstance(input_tensor, backend.KerasTensor):\n                 raise ValueError(\n                     \"Argument `input_tensor` must be a KerasTensor. \"\n-                    f\"Received invalid type: input_tensor={input_tensor} \"\n-                    f\"(of type {type(input_tensor)})\"\n+                    f\"Received invalid type: input_tensor={input_tensor} (of type {type(input_tensor)})\"\n                 )\n         else:\n             input_tensor = backend.KerasTensor(\n\n@@ -7,8 +7,8 @@ from keras_core.api_export import keras_core_export\n class Dropout(layers.Layer):\n     \"\"\"Applies dropout to the input.\n \n-    The `Dropout` layer randomly sets input units to 0 with a frequency of\n-    `rate` at each step during training time, which helps prevent overfitting.\n+    The `Dropout` layer randomly sets input units to 0 with a frequency of `rate`\n+    at each step during training time, which helps prevent overfitting.\n     Inputs not set to 0 are scaled up by `1 / (1 - rate)` such that the sum over\n     all inputs is unchanged.\n \n\n@@ -4,10 +4,6 @@ from keras_core.losses.losses import LossFunctionWrapper\n from keras_core.losses.losses import MeanSquaredError\n \n \n-def deserialize(obj):\n-    raise NotImplementedError\n-\n-\n @keras_core_export(\"keras_core.losses.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras loss as a `function`/`Loss` class instance.\n@@ -33,8 +29,8 @@ def get(identifier):\n \n     Args:\n         identifier: A loss identifier. One of None or string name of a loss\n-            function/class or loss configuration dictionary or a loss function\n-            or a loss class instance.\n+            function/class or loss configuration dictionary or a loss function or a\n+            loss class instance.\n \n     Returns:\n         A Keras loss as a `function`/ `Loss` class instance.\n\n@@ -54,7 +54,7 @@ class Loss:\n \n def standardize_reduction(reduction):\n     allowed = {\"sum_over_batch_size\", \"sum\", None}\n-    if reduction not in allowed:\n+    if not reduction in allowed:\n         raise ValueError(\n             \"Invalid value for argument `reduction`. \"\n             f\"Expected on of {allowed}. Received: \"\n\n@@ -106,8 +106,7 @@ class LossTest(testing.TestCase):\n             loss,\n         )\n \n-    # @testing.parametrize(\n-    #     \"uprank\", [\"mask\", \"sample_weight\", \"y_true\", \"y_pred\"])\n+    # @testing.parametrize(\"uprank\", [\"mask\", \"sample_weight\", \"y_true\", \"y_pred\"])\n     # TODO: use parameterization decorator\n     def test_rank_adjustment(self):\n         for uprank in [\"mask\", \"sample_weight\", \"ys\"]:\n\n@@ -6,10 +6,6 @@ from keras_core.metrics.reduction_metrics import Sum\n from keras_core.metrics.regression_metrics import MeanSquaredError\n \n \n-def deserialize(obj):\n-    raise NotImplementedError\n-\n-\n @keras_core_export(\"keras_core.metrics.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras metric as a `function`/`Metric` class instance.\n@@ -35,8 +31,8 @@ def get(identifier):\n \n     Args:\n         identifier: A metric identifier. One of None or string name of a metric\n-            function/class or metric configuration dictionary or a metric\n-            function or a metric class instance\n+            function/class or metric configuration dictionary or a metric function\n+            or a metric class instance\n \n     Returns:\n         A Keras metric as a `function`/ `Metric` class instance.\n\n@@ -19,7 +19,7 @@ class MeanSquaredErrorTest(testing.TestCase):\n             [[0, 0, 1, 1, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0], [1, 1, 1, 1, 1]]\n         )\n \n-        mse_obj.update_state(y_true, y_pred)\n+        update_op = mse_obj.update_state(y_true, y_pred)\n         result = mse_obj.result()\n         self.assertAllClose(0.5, result, atol=1e-5)\n \n\n@@ -148,6 +148,7 @@ class Functional(Function, Model):\n \n     def _adjust_input_rank(self, flat_inputs):\n         flat_ref_shapes = [x.shape for x in self._inputs]\n+        names = [x.name for x in self._inputs]\n         adjusted = []\n         for x, ref_shape in zip(flat_inputs, flat_ref_shapes):\n             x_rank = len(x.shape)\n\n@@ -334,9 +334,8 @@ def max_pool(\n             dimensions only.\n         pool_size: int or tuple/list of integers of size\n             `len(inputs_spatial_shape)`, specifying the size of the pooling\n-            window for each spatial dimension of the input tensor. If\n-            `pool_size` is int, then every spatial dimension shares the same\n-            `pool_size`.\n+            window for each spatial dimension of the input tensor. If `pool_size`\n+            is int, then every spatial dimension shares the same `pool_size`.\n         strides: int or tuple/list of integers of size\n             `len(inputs_spatial_shape)`. The stride of the sliding window for\n             each spatial dimension of the input tensor. If `strides` is int,\n@@ -436,9 +435,8 @@ def average_pool(\n             dimensions only.\n         pool_size: int or tuple/list of integers of size\n             `len(inputs_spatial_shape)`, specifying the size of the pooling\n-            window for each spatial dimension of the input tensor. If\n-            `pool_size` is int, then every spatial dimension shares the same\n-            `pool_size`.\n+            window for each spatial dimension of the input tensor. If `pool_size`\n+            is int, then every spatial dimension shares the same `pool_size`.\n         strides: int or tuple/list of integers of size\n             `len(inputs_spatial_shape)`. The stride of the sliding window for\n             each spatial dimension of the input tensor. If `strides` is int,\n@@ -513,8 +511,7 @@ class Conv(Operation):\n             raise ValueError(\n                 \"Dilation must be None, scalar or tuple/list of length of \"\n                 \"inputs' spatial shape, but received \"\n-                f\"`dilation_rate={self.dilation_rate}` and \"\n-                f\"input of shape {input_shape}.\"\n+                f\"`dilation_rate={self.dilation_rate}` and input of shape {input_shape}.\"\n             )\n         spatial_shape = np.array(spatial_shape)\n         kernel_spatial_shape = np.array(kernel.shape[:-2])\n@@ -654,8 +651,7 @@ class DepthwiseConv(Operation):\n             raise ValueError(\n                 \"Dilation must be None, scalar or tuple/list of length of \"\n                 \"inputs' spatial shape, but received \"\n-                f\"`dilation_rate={self.dilation_rate}` and input of \"\n-                f\"shape {input_shape}.\"\n+                f\"`dilation_rate={self.dilation_rate}` and input of shape {input_shape}.\"\n             )\n         spatial_shape = np.array(spatial_shape)\n         kernel_spatial_shape = np.array(kernel.shape[:-2])\n\n@@ -281,10 +281,6 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         )\n \n \n-@pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Not have other backend support yet.\",\n-)\n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n         x = KerasTensor([1, 2, 3])\n@@ -547,10 +543,6 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         )\n \n \n-@pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Not have other backend support yet.\",\n-)\n class NNOpsCorrectnessTest(testing.TestCase):\n     def test_relu(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n\n@@ -10,9 +10,9 @@ class Node:\n     \"\"\"A `Node` describes an operation `__call__()` event.\n \n     A Keras Function is a DAG with `Node` instances as nodes, and\n-    `KerasTensor` instances as edges. Nodes aren't `Operation` instances,\n-    because a single operation could be called multiple times, which would\n-    result in graph cycles.\n+    `KerasTensor` instances as edges. Nodes aren't `Operation` instances, because a\n+    single operation could be called multiple times, which would result in graph\n+    cycles.\n \n     A `__call__()` event involves input tensors (and other input arguments),\n     the operation that was called, and the resulting output tensors.\n@@ -89,11 +89,7 @@ class Node:\n \n     @property\n     def parent_nodes(self):\n-        \"\"\"The parent `Node`s.\n-\n-        Returns:\n-            all the `Node`s whose output this node immediately depends on.\n-        \"\"\"\n+        \"\"\"Returns all the `Node`s whose output this node immediately depends on.\"\"\"\n         node_deps = []\n         for kt in self.arguments.keras_tensors:\n             op = kt._keras_history.operation\n@@ -120,9 +116,9 @@ class KerasHistory(\n       operation: The Operation instance that produced the Tensor.\n       node_index: The specific call to the Operation that produced this Tensor.\n         Operations can be called multiple times in order to share weights. A new\n-        node is created every time an Operation is called. The corresponding\n-        node that represents the call event that produced the Tensor can be\n-        found at `op._inbound_nodes[node_index]`.\n+        node is created every time an Operation is called. The corresponding node\n+        that represents the call event that produced the Tensor can be found at\n+        `op._inbound_nodes[node_index]`.\n       tensor_index: The output index for this Tensor.\n         Always zero if the Operation that produced this Tensor\n         only has one output. Nested structures of\n\n@@ -1716,8 +1716,7 @@ class Meshgrid(Operation):\n         super().__init__()\n         if indexing not in (\"xy\", \"ij\"):\n             raise ValueError(\n-                \"Valid values for `indexing` are 'xy' and 'ij', \"\n-                \"but received {index}.\"\n+                \"Valid values for `indexing` are 'xy' and 'ij', but received {index}.\"\n             )\n         self.indexing = indexing\n \n\n@@ -37,8 +37,7 @@ class Operation:\n             return backend.compute_output_spec(self.call, *args, **kwargs)\n         except Exception as e:\n             raise RuntimeError(\n-                \"Could not automatically infer the output shape / dtype of \"\n-                \"this operation. \"\n+                \"Could not automatically infer the output shape / dtype of this operation. \"\n                 \"Please implement the `compute_output_spec` method \"\n                 f\"on your object ({self.__class__.__name__}). \"\n                 f\"Error encountered: {e}\"\n\n@@ -287,7 +287,7 @@ class Optimizer:\n     def _filter_empty_gradients(self, grads_and_vars):\n         filtered = [(g, v) for g, v in grads_and_vars if g is not None]\n         if not filtered:\n-            raise ValueError(\"No gradients provided for any variable.\")\n+            raise ValueError(f\"No gradients provided for any variable.\")\n         if len(filtered) < len(grads_and_vars):\n             missing_grad_vars = [v for g, v in grads_and_vars if g is None]\n             warnings.warn(\n\n@@ -30,5 +30,5 @@ class RegularizersTest(testing.TestCase):\n     def test_orthogonal_regularizer(self):\n         value = np.random.random((4, 4))\n         x = backend.Variable(value)\n-        regularizers.OrthogonalRegularizer(factor=0.1, mode=\"rows\")(x)\n+        y = regularizers.OrthogonalRegularizer(factor=0.1, mode=\"rows\")(x)\n         # TODO\n\n@@ -146,7 +146,7 @@ def serialize_keras_object(obj):\n             \"class_name\": \"__tensor__\",\n             \"config\": {\n                 \"value\": np.array(obj).tolist(),\n-                \"dtype\": str(obj.dtype),\n+                \"dtype\": backend.standardize_dtype(obj.dtype),\n             },\n         }\n     if type(obj).__module__ == np.__name__:\n@@ -155,7 +155,7 @@ def serialize_keras_object(obj):\n                 \"class_name\": \"__numpy__\",\n                 \"config\": {\n                     \"value\": obj.tolist(),\n-                    \"dtype\": backend.standardize_dytpe(obj.dtype),\n+                    \"dtype\": backend.standardize_dtype(obj.dtype),\n                 },\n             }\n         else:\n@@ -261,8 +261,7 @@ def serialize_with_public_class(cls, inner_config=None):\n     Keras API or has been registered as serializable via\n     `keras_core.saving.register_keras_serializable()`.\n     \"\"\"\n-    # This gets the `keras_core.*` exported name, such as\n-    # \"keras_core.optimizers.Adam\".\n+    # This gets the `keras_core.*` exported name, such as \"keras_core.optimizers.Adam\".\n     keras_api_name = api_export.get_name_from_symbol(cls)\n \n     # Case of custom or unknown class object\n@@ -294,8 +293,8 @@ def serialize_with_public_fn(fn, config, fn_module_name=None):\n \n     Called to check and retrieve the config of any function that has a public\n     Keras API or has been registered as serializable via\n-    `keras_core.saving.register_keras_serializable()`. If function's module name\n-    is already known, returns corresponding config.\n+    `keras_core.saving.register_keras_serializable()`. If function's module name is\n+    already known, returns corresponding config.\n     \"\"\"\n     if fn_module_name:\n         return {\n@@ -374,9 +373,9 @@ def deserialize_keras_object(\n     - `module`: String. The path of the python module. Built-in Keras classes\n       expect to have prefix `keras_core`.\n     - `registered_name`: String. The key the class is registered under via\n-      `keras_core.saving.register_keras_serializable(package, name)` API. The\n-      key has the format of '{package}>{name}', where `package` and `name` are\n-      the arguments passed to `register_keras_serializable()`. If `name` is not\n+      `keras_core.saving.register_keras_serializable(package, name)` API. The key has\n+      the format of '{package}>{name}', where `package` and `name` are the\n+      arguments passed to `register_keras_serializable()`. If `name` is not\n       provided, it uses the class name. If `registered_name` successfully\n       resolves to a class (that was registered), the `class_name` and `config`\n       values in the dict will not be used. `registered_name` is only used for\n\n@@ -410,7 +410,7 @@ class CompileLoss(losses_module.Loss):\n                     raise ValueError(\n                         f\"When there is only a single output, the `loss_weights` argument \"\n                         \"must be a Python float. \"\n-                        f\"Received instead: loss_weights={loss_weights} of type {type(loss_weights)}\"\n+                        f\"Received instead:\\loss_weights={loss_weights} of type {type(loss_weights)}\"\n                     )\n                 flat_loss_weights.append(loss_weights)\n             else:\n@@ -447,7 +447,7 @@ class CompileLoss(losses_module.Loss):\n                         \"For a model with multiple outputs, \"\n                         f\"when providing the `loss_weights` argument as a list, \"\n                         \"it should have as many entries as the model has outputs. \"\n-                        f\"Received: loss_weights={loss_weights} of length {len(loss_weights)} \"\n+                        f\"Received:\\loss_weights={loss_weights}\\nof length {len(loss_weights)} \"\n                         f\"whereas the model has {len(y_pred)} outputs.\"\n                     )\n                 if not all(isinstance(e, float) for e in loss_weights):\n@@ -503,7 +503,7 @@ class CompileLoss(losses_module.Loss):\n                         raise ValueError(\n                             f\"In the dict argument `loss_weights`, key \"\n                             f\"'{name}' does not correspond to any model output. \"\n-                            f\"Received: loss_weights={loss_weights}\"\n+                            f\"Received:\\loss_weights={loss_weights}\"\n                         )\n                     if not isinstance(loss_weights[name], float):\n                         raise ValueError(\n\n@@ -88,8 +88,7 @@ class TestArrayDataAdapter(testing.TestCase):\n             self.assertEqual(len(batch), 3)\n             bx, by, bw = batch\n             self.assertTrue(isinstance(bx, dict))\n-            # NOTE: the y list was converted to a tuple for tf.data\n-            # compatibility.\n+            # NOTE: the y list was converted to a tuple for tf.data compatibility.\n             self.assertTrue(isinstance(by, tuple))\n             self.assertTrue(isinstance(bw, tuple))\n \n@@ -119,8 +118,7 @@ class TestArrayDataAdapter(testing.TestCase):\n             self.assertEqual(len(batch), 3)\n             bx, by, bw = batch\n             self.assertTrue(isinstance(bx, dict))\n-            # NOTE: the y list was converted to a tuple for tf.data\n-            # compatibility.\n+            # NOTE: the y list was converted to a tuple for tf.data compatibility.\n             self.assertTrue(isinstance(by, tuple))\n             self.assertTrue(isinstance(bw, tuple))\n \n\n@@ -23,8 +23,8 @@ class DataAdapter(object):\n \n         Returns:\n             A `tf.data.Dataset`. Caller might use the dataset in different\n-            context, e.g. iter(dataset) in eager to get the value directly, or\n-            in graph mode, provide the iterator tensor to Keras model function.\n+            context, e.g. iter(dataset) in eager to get the value directly, or in\n+            graph mode, provide the iterator tensor to Keras model function.\n         \"\"\"\n         raise NotImplementedError\n \n@@ -38,9 +38,9 @@ class DataAdapter(object):\n         may or may not have an end state.\n \n         Returns:\n-            int, the number of batches for the dataset, or None if it is\n-            unknown.  The caller could use this to control the loop of training,\n-            show progress bar, or handle unexpected StopIteration error.\n+            int, the number of batches for the dataset, or None if it is unknown.\n+            The caller could use this to control the loop of training, show\n+            progress bar, or handle unexpected StopIteration error.\n         \"\"\"\n         raise NotImplementedError\n \n\n@@ -141,10 +141,10 @@ def train_validation_split(arrays, validation_split):\n     The last part of data will become validation data.\n \n     Args:\n-        arrays: Tensors to split. Allowed inputs are arbitrarily nested\n-            structures of Tensors and NumPy arrays.\n-        validation_split: Float between 0 and 1. The proportion of the dataset\n-            to include in the validation split. The rest of the dataset will be\n+        arrays: Tensors to split. Allowed inputs are arbitrarily nested structures\n+            of Tensors and NumPy arrays.\n+        validation_split: Float between 0 and 1. The proportion of the dataset to\n+            include in the validation split. The rest of the dataset will be\n             included in the training split.\n \n     Returns:\n@@ -158,8 +158,7 @@ def train_validation_split(arrays, validation_split):\n     unsplitable = [type(t) for t in flat_arrays if not _can_split(t)]\n     if unsplitable:\n         raise ValueError(\n-            \"Argument `validation_split` is only supported \"\n-            \"for tf.Tensors or NumPy \"\n+            \"Argument `validation_split` is only supported for tf.Tensors or NumPy \"\n             \"arrays. Found incompatible type in the input: {unsplitable}\"\n         )\n \n\n@@ -10,8 +10,7 @@ class TFDatasetAdapter(DataAdapter):\n     def __init__(self, dataset, class_weight=None):\n         if not isinstance(dataset, tf.data.Dataset):\n             raise ValueError(\n-                \"Expected argument `dataset` to be a tf.data.Dataset. \"\n-                f\"Received: {dataset}\"\n+                f\"Expected argument `dataset` to be a tf.data.Dataset. Received: {dataset}\"\n             )\n         if class_weight is not None:\n             dataset = dataset.map(\n@@ -73,8 +72,7 @@ def make_class_weight_map_fn(class_weight):\n         x, y, sw = data_adapter_utils.unpack_x_y_sample_weight(data)\n         if sw is not None:\n             raise ValueError(\n-                \"You cannot `class_weight` and `sample_weight` \"\n-                \"at the same time.\"\n+                \"You cannot `class_weight` and `sample_weight` at the same time.\"\n             )\n         if tf.nest.is_nested(y):\n             raise ValueError(\n\n@@ -138,8 +138,8 @@ class Trainer:\n             loss = self._compile_loss(y, y_pred, sample_weight)\n             if loss is not None:\n                 losses.append(loss)\n-        for loss in self.losses:\n-            losses.append(ops.cast(loss, dtype=backend.floatx()))\n+        for l in self.losses:\n+            losses.append(ops.cast(l, dtype=backend.floatx()))\n         if len(losses) == 0:\n             raise ValueError(\n                 \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n\n@@ -74,8 +74,7 @@ class TestTrainer(testing.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         model.fit(x, y, batch_size=2, epochs=1)\n \n-        # The model should have 3 metrics: loss_tracker, compile_metrics,\n-        # my_metric.\n+        # The model should have 3 metrics: loss_tracker, compile_metrics, my_metric\n         self.assertEqual(len(model.metrics), 3)\n         self.assertEqual(model.metrics[0], model._loss_tracker)\n         self.assertEqual(model.metrics[1], model.my_metric)\n\n@@ -42,8 +42,7 @@ class Tracker:\n         self.tracker = Tracker(\n             # Format: `name: (test_fn, store)`\n             {\n-                \"variables\":\n-                    (lambda x: isinstance(x, Variable), self._variables),\n+                \"variables\": (lambda x: isinstance(x, Variable), self._variables),\n                 \"metrics\": (lambda x: isinstance(x, Metric), self._metrics),\n                 \"layers\": (lambda x: isinstance(x, Layer), self._layers),\n             }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e82672951a148cbca0c4734dce7108522072c44a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 124 | Lines Deleted: 76 | Files Changed: 32 | Hunks: 55 | Methods Changed: 39 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 200 | Churn Cumulative: 15891 | Contributors (this commit): 6 | Commits (past 90d): 313 | Contributors (cumulative): 104 | DMM Complexity: 0.6818181818181818\n\nDIFF:\n@@ -28,7 +28,10 @@ class KerasVariable:\n         raise NotImplementedError\n \n     def __repr__(self):\n-        return f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, name={self.name}>\"\n+        return (\n+            f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, \"\n+            \"name={self.name}>\"\n+        )\n \n \n ALLOWED_DTYPES = {\n@@ -70,10 +73,12 @@ def standardize_shape(shape, fully_defined=False):\n             continue\n         if not isinstance(e, int):\n             raise ValueError(\n-                f\"Cannot convert '{shape}' to a shape. Found invalid entry '{e}'\"\n+                f\"Cannot convert '{shape}' to a shape. \"\n+                f\"Found invalid entry '{e}'\"\n             )\n         if e < 0:\n             raise ValueError(\n-                f\"Cannot convert '{shape}' to a shape. Negative dimensions are not allowed.\"\n+                f\"Cannot convert '{shape}' to a shape. \"\n+                \"Negative dimensions are not allowed.\"\n             )\n     return shape\n\n@@ -214,7 +214,8 @@ class JAXTrainer(base_trainer.Trainer):\n                     break\n \n             # Update variable values\n-            # NOTE: doing this after each step would be a big performance bottleneck.\n+            # NOTE: doing this after each step would be a big performance\n+            # bottleneck.\n             for ref_v, v in zip(self.trainable_variables, trainable_variables):\n                 ref_v.assign(v)\n             for ref_v, v in zip(\n@@ -399,7 +400,8 @@ class JAXTrainer(base_trainer.Trainer):\n                 metrics_variables,\n             )\n             logs, state = test_step(state, data)\n-            # Note that trainable variables are not returned since they're immutable here.\n+            # Note that trainable variables are not returned since they're\n+            # immutable here.\n             non_trainable_variables, metrics_variables = state\n \n             callbacks.on_test_batch_end(step, logs)\n@@ -434,7 +436,7 @@ class JAXTrainer(base_trainer.Trainer):\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 # Build model\n-                y_pred = self(data)\n+                self(data)\n                 break\n \n         # Container that configures and calls callbacks.\n\n@@ -21,7 +21,10 @@ class KerasTensor:\n         return operations.Cast(dtype=dtype)(self)\n \n     def __repr__(self):\n-        return f\"<KerasTensor shape={self.shape}, dtype={self.dtype}, name={self.name}>\"\n+        return (\n+            f\"<KerasTensor shape={self.shape}, dtype={self.dtype}, \"\n+            \"name={self.name}>\"\n+        )\n \n     def __iter__(self):\n         raise NotImplementedError(\n\n@@ -28,7 +28,8 @@ class StatelessScope:\n                     \"Invalid variable value in VariableSwapScope: \"\n                     \"all values in argument `mapping` must be tensors with \"\n                     \"a shape that matches the corresponding variable shape. \"\n-                    f\"For variable {k}, received invalid value {v} with shape {v.shape}.\"\n+                    f\"For variable {k}, received invalid value {v} with shape \"\n+                    f\"{v.shape}.\"\n                 )\n             self.state_mapping[id(k)] = v\n \n\n@@ -58,7 +58,7 @@ class Variable(KerasVariable, tf.__internal__.types.Tensor):\n     def ndim(self):\n         return self.value.ndim\n \n-    def numpy(self):\n+    def numpy(self):  # noqa: F811\n         return self.value.numpy()\n \n     # Overload native accessor.\n\n@@ -26,8 +26,8 @@ class CallbackList(Callback):\n \n         Args:\n             callbacks: List of `Callback` instances.\n-            add_history: Whether a `History` callback should be added, if one does\n-                not already exist in the `callbacks` list.\n+            add_history: Whether a `History` callback should be added, if one\n+                does not already exist in the `callbacks` list.\n             add_progbar: Whether a `ProgbarLogger` callback should be added, if\n                 one does not already exist in the `callbacks` list.\n             model: The `Model` these callbacks are used with.\n\n@@ -57,9 +57,9 @@ class Zeros(Initializer):\n         Args:\n             shape: Shape of the tensor.\n             dtype: Optional dtype of the tensor. Only numeric or boolean dtypes\n-                are supported. If not specified, `keras_core.backend.floatx()` is\n-                used, which default to `float32` unless you configured it otherwise\n-                (via `keras_core.backend.set_floatx(float_dtype)`).\n+                are supported. If not specified, `keras_core.backend.floatx()`\n+                is used, which default to `float32` unless you configured it\n+                otherwise (via `keras_core.backend.set_floatx(float_dtype)`).\n             **kwargs: Additional keyword arguments.\n         \"\"\"\n         dtype = standardize_dtype(dtype)\n@@ -89,9 +89,9 @@ class Ones(Initializer):\n         Args:\n             shape: Shape of the tensor.\n             dtype: Optional dtype of the tensor. Only numeric or boolean dtypes\n-                are supported. If not specified, `keras_core.backend.floatx()` is\n-                used, which default to `float32` unless you configured it otherwise\n-                (via `keras_core.backend.set_floatx(float_dtype)`).\n+                are supported. If not specified, `keras_core.backend.floatx()`\n+                is used, which default to `float32` unless you configured it\n+                otherwise (via `keras_core.backend.set_floatx(float_dtype)`).\n             **kwargs: Additional keyword arguments.\n         \"\"\"\n         dtype = standardize_dtype(dtype)\n\n@@ -41,8 +41,8 @@ class Initializer:\n \n     Note that we don't have to implement `from_config()` in the example above\n     since the constructor arguments of the class the keys in the config returned\n-    by `get_config()` are the same. In this case, the default `from_config()` works\n-    fine.\n+    by `get_config()` are the same. In this case, the default `from_config()`\n+    works fine.\n     \"\"\"\n \n     def __call__(self, shape, dtype=None):\n\n@@ -17,11 +17,13 @@ class InputLayer(Layer):\n         super().__init__(name=name)\n         if shape is not None and batch_shape is not None:\n             raise ValueError(\n-                \"You cannot pass both `shape` and `batch_shape` at the same time.\"\n+                \"You cannot pass both `shape` and `batch_shape` at the \"\n+                \"same time.\"\n             )\n         if batch_size is not None and batch_shape is not None:\n             raise ValueError(\n-                \"You cannot pass both `batch_size` and `batch_shape` at the same time.\"\n+                \"You cannot pass both `batch_size` and `batch_shape` at the \"\n+                \"same time.\"\n             )\n         if shape is None and batch_shape is None:\n             raise ValueError(\"You must pass a `shape` argument.\")\n@@ -36,7 +38,8 @@ class InputLayer(Layer):\n             if not isinstance(input_tensor, backend.KerasTensor):\n                 raise ValueError(\n                     \"Argument `input_tensor` must be a KerasTensor. \"\n-                    f\"Received invalid type: input_tensor={input_tensor} (of type {type(input_tensor)})\"\n+                    f\"Received invalid type: input_tensor={input_tensor} \"\n+                    f\"(of type {type(input_tensor)})\"\n                 )\n         else:\n             input_tensor = backend.KerasTensor(\n\n@@ -7,8 +7,8 @@ from keras_core.api_export import keras_core_export\n class Dropout(layers.Layer):\n     \"\"\"Applies dropout to the input.\n \n-    The `Dropout` layer randomly sets input units to 0 with a frequency of `rate`\n-    at each step during training time, which helps prevent overfitting.\n+    The `Dropout` layer randomly sets input units to 0 with a frequency of\n+    `rate` at each step during training time, which helps prevent overfitting.\n     Inputs not set to 0 are scaled up by `1 / (1 - rate)` such that the sum over\n     all inputs is unchanged.\n \n\n@@ -4,6 +4,10 @@ from keras_core.losses.losses import LossFunctionWrapper\n from keras_core.losses.losses import MeanSquaredError\n \n \n+def deserialize(obj):\n+    raise NotImplementedError\n+\n+\n @keras_core_export(\"keras_core.losses.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras loss as a `function`/`Loss` class instance.\n@@ -29,8 +33,8 @@ def get(identifier):\n \n     Args:\n         identifier: A loss identifier. One of None or string name of a loss\n-            function/class or loss configuration dictionary or a loss function or a\n-            loss class instance.\n+            function/class or loss configuration dictionary or a loss function\n+            or a loss class instance.\n \n     Returns:\n         A Keras loss as a `function`/ `Loss` class instance.\n\n@@ -54,7 +54,7 @@ class Loss:\n \n def standardize_reduction(reduction):\n     allowed = {\"sum_over_batch_size\", \"sum\", None}\n-    if not reduction in allowed:\n+    if reduction not in allowed:\n         raise ValueError(\n             \"Invalid value for argument `reduction`. \"\n             f\"Expected on of {allowed}. Received: \"\n\n@@ -106,7 +106,8 @@ class LossTest(testing.TestCase):\n             loss,\n         )\n \n-    # @testing.parametrize(\"uprank\", [\"mask\", \"sample_weight\", \"y_true\", \"y_pred\"])\n+    # @testing.parametrize(\n+    #     \"uprank\", [\"mask\", \"sample_weight\", \"y_true\", \"y_pred\"])\n     # TODO: use parameterization decorator\n     def test_rank_adjustment(self):\n         for uprank in [\"mask\", \"sample_weight\", \"ys\"]:\n\n@@ -6,6 +6,10 @@ from keras_core.metrics.reduction_metrics import Sum\n from keras_core.metrics.regression_metrics import MeanSquaredError\n \n \n+def deserialize(obj):\n+    raise NotImplementedError\n+\n+\n @keras_core_export(\"keras_core.metrics.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras metric as a `function`/`Metric` class instance.\n@@ -31,8 +35,8 @@ def get(identifier):\n \n     Args:\n         identifier: A metric identifier. One of None or string name of a metric\n-            function/class or metric configuration dictionary or a metric function\n-            or a metric class instance\n+            function/class or metric configuration dictionary or a metric\n+            function or a metric class instance\n \n     Returns:\n         A Keras metric as a `function`/ `Metric` class instance.\n\n@@ -19,7 +19,7 @@ class MeanSquaredErrorTest(testing.TestCase):\n             [[0, 0, 1, 1, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0], [1, 1, 1, 1, 1]]\n         )\n \n-        update_op = mse_obj.update_state(y_true, y_pred)\n+        mse_obj.update_state(y_true, y_pred)\n         result = mse_obj.result()\n         self.assertAllClose(0.5, result, atol=1e-5)\n \n\n@@ -148,7 +148,6 @@ class Functional(Function, Model):\n \n     def _adjust_input_rank(self, flat_inputs):\n         flat_ref_shapes = [x.shape for x in self._inputs]\n-        names = [x.name for x in self._inputs]\n         adjusted = []\n         for x, ref_shape in zip(flat_inputs, flat_ref_shapes):\n             x_rank = len(x.shape)\n\n@@ -334,8 +334,9 @@ def max_pool(\n             dimensions only.\n         pool_size: int or tuple/list of integers of size\n             `len(inputs_spatial_shape)`, specifying the size of the pooling\n-            window for each spatial dimension of the input tensor. If `pool_size`\n-            is int, then every spatial dimension shares the same `pool_size`.\n+            window for each spatial dimension of the input tensor. If\n+            `pool_size` is int, then every spatial dimension shares the same\n+            `pool_size`.\n         strides: int or tuple/list of integers of size\n             `len(inputs_spatial_shape)`. The stride of the sliding window for\n             each spatial dimension of the input tensor. If `strides` is int,\n@@ -435,8 +436,9 @@ def average_pool(\n             dimensions only.\n         pool_size: int or tuple/list of integers of size\n             `len(inputs_spatial_shape)`, specifying the size of the pooling\n-            window for each spatial dimension of the input tensor. If `pool_size`\n-            is int, then every spatial dimension shares the same `pool_size`.\n+            window for each spatial dimension of the input tensor. If\n+            `pool_size` is int, then every spatial dimension shares the same\n+            `pool_size`.\n         strides: int or tuple/list of integers of size\n             `len(inputs_spatial_shape)`. The stride of the sliding window for\n             each spatial dimension of the input tensor. If `strides` is int,\n@@ -511,7 +513,8 @@ class Conv(Operation):\n             raise ValueError(\n                 \"Dilation must be None, scalar or tuple/list of length of \"\n                 \"inputs' spatial shape, but received \"\n-                f\"`dilation_rate={self.dilation_rate}` and input of shape {input_shape}.\"\n+                f\"`dilation_rate={self.dilation_rate}` and \"\n+                f\"input of shape {input_shape}.\"\n             )\n         spatial_shape = np.array(spatial_shape)\n         kernel_spatial_shape = np.array(kernel.shape[:-2])\n@@ -651,7 +654,8 @@ class DepthwiseConv(Operation):\n             raise ValueError(\n                 \"Dilation must be None, scalar or tuple/list of length of \"\n                 \"inputs' spatial shape, but received \"\n-                f\"`dilation_rate={self.dilation_rate}` and input of shape {input_shape}.\"\n+                f\"`dilation_rate={self.dilation_rate}` and input of \"\n+                f\"shape {input_shape}.\"\n             )\n         spatial_shape = np.array(spatial_shape)\n         kernel_spatial_shape = np.array(kernel.shape[:-2])\n\n@@ -281,6 +281,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         )\n \n \n+@pytest.mark.skipif(\n+    backend() != \"tensorflow\",\n+    reason=\"Not have other backend support yet.\",\n+)\n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n         x = KerasTensor([1, 2, 3])\n@@ -543,6 +547,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         )\n \n \n+@pytest.mark.skipif(\n+    backend() != \"tensorflow\",\n+    reason=\"Not have other backend support yet.\",\n+)\n class NNOpsCorrectnessTest(testing.TestCase):\n     def test_relu(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n\n@@ -10,9 +10,9 @@ class Node:\n     \"\"\"A `Node` describes an operation `__call__()` event.\n \n     A Keras Function is a DAG with `Node` instances as nodes, and\n-    `KerasTensor` instances as edges. Nodes aren't `Operation` instances, because a\n-    single operation could be called multiple times, which would result in graph\n-    cycles.\n+    `KerasTensor` instances as edges. Nodes aren't `Operation` instances,\n+    because a single operation could be called multiple times, which would\n+    result in graph cycles.\n \n     A `__call__()` event involves input tensors (and other input arguments),\n     the operation that was called, and the resulting output tensors.\n@@ -89,7 +89,11 @@ class Node:\n \n     @property\n     def parent_nodes(self):\n-        \"\"\"Returns all the `Node`s whose output this node immediately depends on.\"\"\"\n+        \"\"\"The parent `Node`s.\n+\n+        Returns:\n+            all the `Node`s whose output this node immediately depends on.\n+        \"\"\"\n         node_deps = []\n         for kt in self.arguments.keras_tensors:\n             op = kt._keras_history.operation\n@@ -116,9 +120,9 @@ class KerasHistory(\n       operation: The Operation instance that produced the Tensor.\n       node_index: The specific call to the Operation that produced this Tensor.\n         Operations can be called multiple times in order to share weights. A new\n-        node is created every time an Operation is called. The corresponding node\n-        that represents the call event that produced the Tensor can be found at\n-        `op._inbound_nodes[node_index]`.\n+        node is created every time an Operation is called. The corresponding\n+        node that represents the call event that produced the Tensor can be\n+        found at `op._inbound_nodes[node_index]`.\n       tensor_index: The output index for this Tensor.\n         Always zero if the Operation that produced this Tensor\n         only has one output. Nested structures of\n\n@@ -1716,7 +1716,8 @@ class Meshgrid(Operation):\n         super().__init__()\n         if indexing not in (\"xy\", \"ij\"):\n             raise ValueError(\n-                \"Valid values for `indexing` are 'xy' and 'ij', but received {index}.\"\n+                \"Valid values for `indexing` are 'xy' and 'ij', \"\n+                \"but received {index}.\"\n             )\n         self.indexing = indexing\n \n\n@@ -37,7 +37,8 @@ class Operation:\n             return backend.compute_output_spec(self.call, *args, **kwargs)\n         except Exception as e:\n             raise RuntimeError(\n-                \"Could not automatically infer the output shape / dtype of this operation. \"\n+                \"Could not automatically infer the output shape / dtype of \"\n+                \"this operation. \"\n                 \"Please implement the `compute_output_spec` method \"\n                 f\"on your object ({self.__class__.__name__}). \"\n                 f\"Error encountered: {e}\"\n\n@@ -287,7 +287,7 @@ class Optimizer:\n     def _filter_empty_gradients(self, grads_and_vars):\n         filtered = [(g, v) for g, v in grads_and_vars if g is not None]\n         if not filtered:\n-            raise ValueError(f\"No gradients provided for any variable.\")\n+            raise ValueError(\"No gradients provided for any variable.\")\n         if len(filtered) < len(grads_and_vars):\n             missing_grad_vars = [v for g, v in grads_and_vars if g is None]\n             warnings.warn(\n\n@@ -30,5 +30,5 @@ class RegularizersTest(testing.TestCase):\n     def test_orthogonal_regularizer(self):\n         value = np.random.random((4, 4))\n         x = backend.Variable(value)\n-        y = regularizers.OrthogonalRegularizer(factor=0.1, mode=\"rows\")(x)\n+        regularizers.OrthogonalRegularizer(factor=0.1, mode=\"rows\")(x)\n         # TODO\n\n@@ -261,7 +261,8 @@ def serialize_with_public_class(cls, inner_config=None):\n     Keras API or has been registered as serializable via\n     `keras_core.saving.register_keras_serializable()`.\n     \"\"\"\n-    # This gets the `keras_core.*` exported name, such as \"keras_core.optimizers.Adam\".\n+    # This gets the `keras_core.*` exported name, such as\n+    # \"keras_core.optimizers.Adam\".\n     keras_api_name = api_export.get_name_from_symbol(cls)\n \n     # Case of custom or unknown class object\n@@ -293,8 +294,8 @@ def serialize_with_public_fn(fn, config, fn_module_name=None):\n \n     Called to check and retrieve the config of any function that has a public\n     Keras API or has been registered as serializable via\n-    `keras_core.saving.register_keras_serializable()`. If function's module name is\n-    already known, returns corresponding config.\n+    `keras_core.saving.register_keras_serializable()`. If function's module name\n+    is already known, returns corresponding config.\n     \"\"\"\n     if fn_module_name:\n         return {\n@@ -373,9 +374,9 @@ def deserialize_keras_object(\n     - `module`: String. The path of the python module. Built-in Keras classes\n       expect to have prefix `keras_core`.\n     - `registered_name`: String. The key the class is registered under via\n-      `keras_core.saving.register_keras_serializable(package, name)` API. The key has\n-      the format of '{package}>{name}', where `package` and `name` are the\n-      arguments passed to `register_keras_serializable()`. If `name` is not\n+      `keras_core.saving.register_keras_serializable(package, name)` API. The\n+      key has the format of '{package}>{name}', where `package` and `name` are\n+      the arguments passed to `register_keras_serializable()`. If `name` is not\n       provided, it uses the class name. If `registered_name` successfully\n       resolves to a class (that was registered), the `class_name` and `config`\n       values in the dict will not be used. `registered_name` is only used for\n\n@@ -410,7 +410,7 @@ class CompileLoss(losses_module.Loss):\n                     raise ValueError(\n                         f\"When there is only a single output, the `loss_weights` argument \"\n                         \"must be a Python float. \"\n-                        f\"Received instead:\\loss_weights={loss_weights} of type {type(loss_weights)}\"\n+                        f\"Received instead: loss_weights={loss_weights} of type {type(loss_weights)}\"\n                     )\n                 flat_loss_weights.append(loss_weights)\n             else:\n@@ -447,7 +447,7 @@ class CompileLoss(losses_module.Loss):\n                         \"For a model with multiple outputs, \"\n                         f\"when providing the `loss_weights` argument as a list, \"\n                         \"it should have as many entries as the model has outputs. \"\n-                        f\"Received:\\loss_weights={loss_weights}\\nof length {len(loss_weights)} \"\n+                        f\"Received: loss_weights={loss_weights} of length {len(loss_weights)} \"\n                         f\"whereas the model has {len(y_pred)} outputs.\"\n                     )\n                 if not all(isinstance(e, float) for e in loss_weights):\n@@ -503,7 +503,7 @@ class CompileLoss(losses_module.Loss):\n                         raise ValueError(\n                             f\"In the dict argument `loss_weights`, key \"\n                             f\"'{name}' does not correspond to any model output. \"\n-                            f\"Received:\\loss_weights={loss_weights}\"\n+                            f\"Received: loss_weights={loss_weights}\"\n                         )\n                     if not isinstance(loss_weights[name], float):\n                         raise ValueError(\n\n@@ -88,7 +88,8 @@ class TestArrayDataAdapter(testing.TestCase):\n             self.assertEqual(len(batch), 3)\n             bx, by, bw = batch\n             self.assertTrue(isinstance(bx, dict))\n-            # NOTE: the y list was converted to a tuple for tf.data compatibility.\n+            # NOTE: the y list was converted to a tuple for tf.data\n+            # compatibility.\n             self.assertTrue(isinstance(by, tuple))\n             self.assertTrue(isinstance(bw, tuple))\n \n@@ -118,7 +119,8 @@ class TestArrayDataAdapter(testing.TestCase):\n             self.assertEqual(len(batch), 3)\n             bx, by, bw = batch\n             self.assertTrue(isinstance(bx, dict))\n-            # NOTE: the y list was converted to a tuple for tf.data compatibility.\n+            # NOTE: the y list was converted to a tuple for tf.data\n+            # compatibility.\n             self.assertTrue(isinstance(by, tuple))\n             self.assertTrue(isinstance(bw, tuple))\n \n\n@@ -23,8 +23,8 @@ class DataAdapter(object):\n \n         Returns:\n             A `tf.data.Dataset`. Caller might use the dataset in different\n-            context, e.g. iter(dataset) in eager to get the value directly, or in\n-            graph mode, provide the iterator tensor to Keras model function.\n+            context, e.g. iter(dataset) in eager to get the value directly, or\n+            in graph mode, provide the iterator tensor to Keras model function.\n         \"\"\"\n         raise NotImplementedError\n \n@@ -38,9 +38,9 @@ class DataAdapter(object):\n         may or may not have an end state.\n \n         Returns:\n-            int, the number of batches for the dataset, or None if it is unknown.\n-            The caller could use this to control the loop of training, show\n-            progress bar, or handle unexpected StopIteration error.\n+            int, the number of batches for the dataset, or None if it is\n+            unknown.  The caller could use this to control the loop of training,\n+            show progress bar, or handle unexpected StopIteration error.\n         \"\"\"\n         raise NotImplementedError\n \n\n@@ -141,10 +141,10 @@ def train_validation_split(arrays, validation_split):\n     The last part of data will become validation data.\n \n     Args:\n-        arrays: Tensors to split. Allowed inputs are arbitrarily nested structures\n-            of Tensors and NumPy arrays.\n-        validation_split: Float between 0 and 1. The proportion of the dataset to\n-            include in the validation split. The rest of the dataset will be\n+        arrays: Tensors to split. Allowed inputs are arbitrarily nested\n+            structures of Tensors and NumPy arrays.\n+        validation_split: Float between 0 and 1. The proportion of the dataset\n+            to include in the validation split. The rest of the dataset will be\n             included in the training split.\n \n     Returns:\n@@ -158,7 +158,8 @@ def train_validation_split(arrays, validation_split):\n     unsplitable = [type(t) for t in flat_arrays if not _can_split(t)]\n     if unsplitable:\n         raise ValueError(\n-            \"Argument `validation_split` is only supported for tf.Tensors or NumPy \"\n+            \"Argument `validation_split` is only supported \"\n+            \"for tf.Tensors or NumPy \"\n             \"arrays. Found incompatible type in the input: {unsplitable}\"\n         )\n \n\n@@ -10,7 +10,8 @@ class TFDatasetAdapter(DataAdapter):\n     def __init__(self, dataset, class_weight=None):\n         if not isinstance(dataset, tf.data.Dataset):\n             raise ValueError(\n-                f\"Expected argument `dataset` to be a tf.data.Dataset. Received: {dataset}\"\n+                \"Expected argument `dataset` to be a tf.data.Dataset. \"\n+                f\"Received: {dataset}\"\n             )\n         if class_weight is not None:\n             dataset = dataset.map(\n@@ -72,7 +73,8 @@ def make_class_weight_map_fn(class_weight):\n         x, y, sw = data_adapter_utils.unpack_x_y_sample_weight(data)\n         if sw is not None:\n             raise ValueError(\n-                \"You cannot `class_weight` and `sample_weight` at the same time.\"\n+                \"You cannot `class_weight` and `sample_weight` \"\n+                \"at the same time.\"\n             )\n         if tf.nest.is_nested(y):\n             raise ValueError(\n\n@@ -138,8 +138,8 @@ class Trainer:\n             loss = self._compile_loss(y, y_pred, sample_weight)\n             if loss is not None:\n                 losses.append(loss)\n-        for l in self.losses:\n-            losses.append(ops.cast(l, dtype=backend.floatx()))\n+        for loss in self.losses:\n+            losses.append(ops.cast(loss, dtype=backend.floatx()))\n         if len(losses) == 0:\n             raise ValueError(\n                 \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n\n@@ -74,7 +74,8 @@ class TestTrainer(testing.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         model.fit(x, y, batch_size=2, epochs=1)\n \n-        # The model should have 3 metrics: loss_tracker, compile_metrics, my_metric\n+        # The model should have 3 metrics: loss_tracker, compile_metrics,\n+        # my_metric.\n         self.assertEqual(len(model.metrics), 3)\n         self.assertEqual(model.metrics[0], model._loss_tracker)\n         self.assertEqual(model.metrics[1], model.my_metric)\n\n@@ -42,7 +42,8 @@ class Tracker:\n         self.tracker = Tracker(\n             # Format: `name: (test_fn, store)`\n             {\n-                \"variables\": (lambda x: isinstance(x, Variable), self._variables),\n+                \"variables\":\n+                    (lambda x: isinstance(x, Variable), self._variables),\n                 \"metrics\": (lambda x: isinstance(x, Metric), self._metrics),\n                 \"layers\": (lambda x: isinstance(x, Layer), self._layers),\n             }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
