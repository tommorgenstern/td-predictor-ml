{"custom_id": "scrapy#0bbfca6c1d1327a9919f2c0efc0c75aadb5b0033", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 333 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -130,9 +130,7 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n         ) as exc_info:\n             q.push(sel)\n         assert isinstance(exc_info.value.__context__, TypeError)\n-        # This seems to help with https://github.com/scrapy/queuelib/issues/70.\n-        # It will need to remain under a queuelib version check after that bug is fixed.\n-        del exc_info\n+        q.close()\n \n \n class ChunkSize1PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b7db1a068895254aae1618fb684baf9cb0c2784", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 1002 | Contributors (this commit): 20 | Commits (past 90d): 4 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -138,11 +138,60 @@ class Request(object_ref):\n             )\n         if not (callable(errback) or errback is None):\n             raise TypeError(f\"errback must be a callable, got {type(errback).__name__}\")\n+\n+        #: :class:`~collections.abc.Callable` to parse the\n+        #: :class:`~scrapy.http.Response` to this request once received.\n+        #:\n+        #: The callable must expect the response as its first parameter, and\n+        #: support any additional keyword arguments set through\n+        #: :attr:`cb_kwargs`.\n+        #:\n+        #: In addition to an arbitrary callable, the following values are also\n+        #: supported:\n+        #:\n+        #: -   ``None`` (default), which indicates that the\n+        #:     :meth:`~scrapy.Spider.parse` method of the spider must be used.\n+        #:\n+        #: -   :func:`~scrapy.http.request.NO_CALLBACK`.\n+        #:\n+        #: If an unhandled exception is raised during request or response\n+        #: processing, i.e. by a :ref:`spider middleware\n+        #: <topics-spider-middleware>`, :ref:`downloader middleware\n+        #: <topics-downloader-middleware>` or download handler\n+        #: (:setting:`DOWNLOAD_HANDLERS`), :attr:`errback` is called instead.\n+        #:\n+        #: .. tip::\n+        #:     :class:`~scrapy.spidermiddlewares.httperror.HttpErrorMiddleware`\n+        #:     raises exceptions for non-2xx responses by default, sending them\n+        #:     to the :attr:`errback` instead.\n+        #:\n+        #: .. seealso::\n+        #:     :ref:`topics-request-response-ref-request-callback-arguments`\n         self.callback: CallbackT | None = callback\n+\n+        #: :class:`~collections.abc.Callable` to handle exceptions raised\n+        #: during request or response processing.\n+        #:\n+        #: The callable must expect a :exc:`~twisted.python.failure.Failure` as\n+        #: its first parameter.\n+        #:\n+        #: .. seealso:: :ref:`topics-request-response-ref-errbacks`\n         self.errback: Callable[[Failure], Any] | None = errback\n \n         self.cookies: CookiesT = cookies or {}\n         self.headers: Headers = Headers(headers or {}, encoding=encoding)\n+\n+        #: Whether this request may be filtered out by :ref:`components\n+        #: <topics-components>` that support filtering out requests (``False``,\n+        #: default), or those components should not filter out this request\n+        #: (``True``).\n+        #:\n+        #: This attribute is commonly set to ``True`` to prevent duplicate\n+        #: requests from being filtered out.\n+        #:\n+        #: When defining the start URLs of a spider through\n+        #: :attr:`~scrapy.Spider.start_urls`, this attribute is enabled by\n+        #: default. See :meth:`~scrapy.Spider.start_requests`.\n         self.dont_filter: bool = dont_filter\n \n         self._meta: dict[str, Any] | None = dict(meta) if meta else None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5a0690c89d718b33bd63c1cd724c50c9ceb809e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 39 | Files Changed: 6 | Hunks: 16 | Methods Changed: 6 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 55 | Churn Cumulative: 9598 | Contributors (this commit): 71 | Commits (past 90d): 17 | Contributors (cumulative): 149 | DMM Complexity: 0.24\n\nDIFF:\n@@ -6,7 +6,7 @@ import json\n import logging\n from typing import TYPE_CHECKING, Any, TypeVar, overload\n \n-from itemadapter import ItemAdapter, is_item\n+from itemadapter import ItemAdapter\n from twisted.internet.defer import Deferred, maybeDeferred\n from w3lib.url import is_url\n \n@@ -211,10 +211,10 @@ class Command(BaseRunSpiderCommand):\n     ) -> tuple[list[Any], list[Request], argparse.Namespace, int, Spider, CallbackT]:\n         items, requests = [], []\n         for x in spider_output:\n-            if is_item(x):\n-                items.append(x)\n-            elif isinstance(x, Request):\n+            if isinstance(x, Request):\n                 requests.append(x)\n+            else:\n+                items.append(x)\n         return items, requests, opts, depth, spider, callback\n \n     def run_callback(\n\n@@ -11,7 +11,6 @@ import logging\n from time import time\n from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n-from itemadapter import is_item\n from twisted.internet.defer import Deferred, inlineCallbacks, succeed\n from twisted.internet.task import LoopingCall\n from twisted.python.failure import Failure\n@@ -194,14 +193,8 @@ class ExecutionEngine:\n             else:\n                 if isinstance(request_or_item, Request):\n                     self.crawl(request_or_item)\n-                elif is_item(request_or_item):\n-                    self.scraper.start_itemproc(request_or_item, response=None)\n                 else:\n-                    logger.error(\n-                        f\"Got {request_or_item!r} among start requests. Only \"\n-                        f\"requests and items are supported. It will be \"\n-                        f\"ignored.\"\n-                    )\n+                    self.scraper.start_itemproc(request_or_item, response=None)\n \n         if self.spider_is_idle() and self.slot.close_if_idle:\n             self._spider_idle()\n\n@@ -8,7 +8,6 @@ from collections import deque\n from collections.abc import AsyncIterable, Iterator\n from typing import TYPE_CHECKING, Any, TypeVar, Union, cast\n \n-from itemadapter import is_item\n from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.python.failure import Failure\n \n@@ -298,17 +297,10 @@ class Scraper:\n         if isinstance(output, Request):\n             assert self.crawler.engine is not None  # typing\n             self.crawler.engine.crawl(request=output)\n-        elif is_item(output):\n-            return self.start_itemproc(output, response=response)\n         elif output is None:\n             pass\n         else:\n-            typename = type(output).__name__\n-            logger.error(\n-                \"Spider must return request, item, or None, got %(typename)r in %(request)s\",\n-                {\"request\": request, \"typename\": typename},\n-                extra={\"spider\": spider},\n-            )\n+            return self.start_itemproc(output, response=response)\n         return None\n \n     def start_itemproc(self, item: Any, *, response: Response | None) -> Deferred[Any]:\n\n@@ -356,12 +356,12 @@ class PythonItemExporter(BaseItemExporter):\n     def _serialize_value(self, value: Any) -> Any:\n         if isinstance(value, Item):\n             return self.export_item(value)\n+        if isinstance(value, (str, bytes)):\n+            return to_unicode(value, encoding=self.encoding)\n         if is_item(value):\n             return dict(self._serialize_item(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n-        if isinstance(value, (str, bytes)):\n-            return to_unicode(value, encoding=self.encoding)\n         return value\n \n     def _serialize_item(self, item: Any) -> Iterable[tuple[str | bytes, Any]]:\n\n@@ -28,12 +28,12 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n             return str(o)\n         if isinstance(o, defer.Deferred):\n             return str(o)\n-        if is_item(o):\n-            return ItemAdapter(o).asdict()\n         if isinstance(o, Request):\n             return f\"<{type(o).__name__} {o.method} {o.url}>\"\n         if isinstance(o, Response):\n             return f\"<{type(o).__name__} {o.status} {o.url}>\"\n+        if is_item(o):\n+            return ItemAdapter(o).asdict()\n         return super().default(o)\n \n \n\n@@ -1,6 +1,5 @@\n import json\n import logging\n-import re\n import unittest\n from ipaddress import IPv4Address\n from socket import gethostbyname\n@@ -195,23 +194,16 @@ class TestCrawl(TestCase):\n \n     @defer.inlineCallbacks\n     def test_start_requests_unsupported_output(self):\n+        \"\"\"Anything that is not a request is assumed to be an item, avoiding a\n+        potentially expensive call to itemadapter.is_item, and letting instead\n+        things fail when ItemAdapter is actually used on the corresponding\n+        non-item object.\"\"\"\n+\n         with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n             crawler = get_crawler(StartRequestsGoodAndBadOutput)\n             yield crawler.crawl(mockserver=self.mockserver)\n \n-        assert len(log.records) == 2\n-        assert log.records[0].msg == (\n-            \"Got 'data:,b' among start requests. Only requests and items \"\n-            \"are supported. It will be ignored.\"\n-        )\n-        assert re.match(\n-            (\n-                r\"^Got <object object at 0x[0-9a-fA-F]+> among start \"\n-                r\"requests\\. Only requests and items are supported\\. It \"\n-                r\"will be ignored\\.$\"\n-            ),\n-            log.records[1].msg,\n-        )\n+        assert len(log.records) == 0\n \n     @defer.inlineCallbacks\n     def test_start_requests_laziness(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb654aa1a8d2ef6433957fcc1361420b6141094e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 79 | Files Changed: 6 | Hunks: 50 | Methods Changed: 55 | Complexity Δ (Sum/Max): -5/2 | Churn Δ: 141 | Churn Cumulative: 6711 | Contributors (this commit): 56 | Commits (past 90d): 26 | Contributors (cumulative): 127 | DMM Complexity: 0.0\n\nDIFF:\n@@ -156,7 +156,7 @@ def assert_samelines(\n         category=ScrapyDeprecationWarning,\n         stacklevel=2,\n     )\n-    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n+    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)  # noqa: PT009\n \n \n def get_from_asyncio_queue(value: _T) -> Awaitable[_T]:\n\n@@ -1,3 +1,4 @@\n+from typing import Any\n from unittest import mock\n \n import pytest\n@@ -171,7 +172,11 @@ Disallow: /some/randome/page.html\n         middleware = RobotsTxtMiddleware(self.crawler)\n         middleware._logerror = mock.MagicMock(side_effect=middleware._logerror)\n         deferred = middleware.process_request(Request(\"http://site.local\"), None)\n-        deferred.addCallback(lambda _: self.assertTrue(middleware._logerror.called))\n+\n+        def check_called(_: Any) -> None:\n+            assert middleware._logerror.called\n+\n+        deferred.addCallback(check_called)\n         return deferred\n \n     def test_robotstxt_immediate_error(self):\n@@ -202,7 +207,11 @@ Disallow: /some/randome/page.html\n         mw_module_logger.error = mock.MagicMock()\n \n         d = self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n-        d.addCallback(lambda _: self.assertFalse(mw_module_logger.error.called))\n+\n+        def check_not_called(_: Any) -> None:\n+            assert not mw_module_logger.error.called  # type: ignore[attr-defined]\n+\n+        d.addCallback(check_not_called)\n         return d\n \n     def test_robotstxt_user_agent_setting(self):\n\n@@ -433,10 +433,12 @@ class TestEngine(TestEngineBase):\n         e = ExecutionEngine(get_crawler(MySpider), lambda _: None)\n         yield e.open_spider(MySpider(), [])\n         e.start()\n+\n+        def cb(exc: BaseException) -> None:\n+            assert str(exc), \"Engine already running\"\n+\n         try:\n-            yield self.assertFailure(e.start(), RuntimeError).addBoth(\n-                lambda exc: self.assertEqual(str(exc), \"Engine already running\")\n-            )\n+            yield self.assertFailure(e.start(), RuntimeError).addBoth(cb)\n         finally:\n             yield e.stop()\n \n\n@@ -266,17 +266,11 @@ class TestFilesPipeline(unittest.TestCase):\n \n \n class FilesPipelineTestCaseFieldsMixin:\n-    def setup_method(self):\n-        self.tempdir = mkdtemp()\n-\n-    def teardown_method(self):\n-        rmtree(self.tempdir)\n-\n-    def test_item_fields_default(self):\n+    def test_item_fields_default(self, tmp_path):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", file_urls=[url])\n         pipeline = FilesPipeline.from_crawler(\n-            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+            get_crawler(None, {\"FILES_STORE\": tmp_path})\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n         assert requests[0].url == url\n@@ -286,14 +280,14 @@ class FilesPipelineTestCaseFieldsMixin:\n         assert files == [results[0][1]]\n         assert isinstance(item, self.item_class)\n \n-    def test_item_fields_override_settings(self):\n+    def test_item_fields_override_settings(self, tmp_path):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", custom_file_urls=[url])\n         pipeline = FilesPipeline.from_crawler(\n             get_crawler(\n                 None,\n                 {\n-                    \"FILES_STORE\": self.tempdir,\n+                    \"FILES_STORE\": tmp_path,\n                     \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                     \"FILES_RESULT_FIELD\": \"custom_files\",\n                 },\n@@ -368,13 +362,7 @@ class TestFilesPipelineCustomSettings:\n         (\"FILES_RESULT_FIELD\", \"FILES_RESULT_FIELD\", \"files_result_field\"),\n     }\n \n-    def setup_method(self):\n-        self.tempdir = mkdtemp()\n-\n-    def teardown_method(self):\n-        rmtree(self.tempdir)\n-\n-    def _generate_fake_settings(self, prefix=None):\n+    def _generate_fake_settings(self, tmp_path, prefix=None):\n         def random_string():\n             return \"\".join([chr(random.randint(97, 123)) for _ in range(10)])\n \n@@ -382,7 +370,7 @@ class TestFilesPipelineCustomSettings:\n             \"FILES_EXPIRES\": random.randint(100, 1000),\n             \"FILES_URLS_FIELD\": random_string(),\n             \"FILES_RESULT_FIELD\": random_string(),\n-            \"FILES_STORE\": self.tempdir,\n+            \"FILES_STORE\": tmp_path,\n         }\n         if not prefix:\n             return settings\n@@ -400,16 +388,16 @@ class TestFilesPipelineCustomSettings:\n \n         return UserDefinedFilePipeline\n \n-    def test_different_settings_for_different_instances(self):\n+    def test_different_settings_for_different_instances(self, tmp_path):\n         \"\"\"\n         If there are different instances with different settings they should keep\n         different settings.\n         \"\"\"\n-        custom_settings = self._generate_fake_settings()\n+        custom_settings = self._generate_fake_settings(tmp_path)\n         another_pipeline = FilesPipeline.from_crawler(\n             get_crawler(None, custom_settings)\n         )\n-        one_pipeline = FilesPipeline(self.tempdir, crawler=get_crawler(None))\n+        one_pipeline = FilesPipeline(tmp_path, crawler=get_crawler(None))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             default_value = self.default_cls_settings[pipe_attr]\n             assert getattr(one_pipeline, pipe_attr) == default_value\n@@ -417,24 +405,24 @@ class TestFilesPipelineCustomSettings:\n             assert default_value != custom_value\n             assert getattr(another_pipeline, pipe_ins_attr) == custom_value\n \n-    def test_subclass_attributes_preserved_if_no_settings(self):\n+    def test_subclass_attributes_preserved_if_no_settings(self, tmp_path):\n         \"\"\"\n         If subclasses override class attributes and there are no special settings those values should be kept.\n         \"\"\"\n         pipe_cls = self._generate_fake_pipeline()\n-        pipe = pipe_cls.from_crawler(get_crawler(None, {\"FILES_STORE\": self.tempdir}))\n+        pipe = pipe_cls.from_crawler(get_crawler(None, {\"FILES_STORE\": tmp_path}))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             custom_value = getattr(pipe, pipe_ins_attr)\n             assert custom_value != self.default_cls_settings[pipe_attr]\n             assert getattr(pipe, pipe_ins_attr) == getattr(pipe, pipe_attr)\n \n-    def test_subclass_attrs_preserved_custom_settings(self):\n+    def test_subclass_attrs_preserved_custom_settings(self, tmp_path):\n         \"\"\"\n         If file settings are defined but they are not defined for subclass\n         settings should be preserved.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline()\n-        settings = self._generate_fake_settings()\n+        settings = self._generate_fake_settings(tmp_path)\n         pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             value = getattr(pipeline, pipe_ins_attr)\n@@ -442,7 +430,7 @@ class TestFilesPipelineCustomSettings:\n             assert value != self.default_cls_settings[pipe_attr]\n             assert value == setting_value\n \n-    def test_no_custom_settings_for_subclasses(self):\n+    def test_no_custom_settings_for_subclasses(self, tmp_path):\n         \"\"\"\n         If there are no settings for subclass and no subclass attributes, pipeline should use\n         attributes of base class.\n@@ -452,14 +440,14 @@ class TestFilesPipelineCustomSettings:\n             pass\n \n         user_pipeline = UserDefinedFilesPipeline.from_crawler(\n-            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+            get_crawler(None, {\"FILES_STORE\": tmp_path})\n         )\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = self.default_cls_settings.get(pipe_attr.upper())\n             assert getattr(user_pipeline, pipe_ins_attr) == custom_value\n \n-    def test_custom_settings_for_subclasses(self):\n+    def test_custom_settings_for_subclasses(self, tmp_path):\n         \"\"\"\n         If there are custom settings for subclass and NO class attributes, pipeline should use custom\n         settings.\n@@ -469,7 +457,7 @@ class TestFilesPipelineCustomSettings:\n             pass\n \n         prefix = UserDefinedFilesPipeline.__name__.upper()\n-        settings = self._generate_fake_settings(prefix=prefix)\n+        settings = self._generate_fake_settings(tmp_path, prefix=prefix)\n         user_pipeline = UserDefinedFilesPipeline.from_crawler(\n             get_crawler(None, settings)\n         )\n@@ -479,14 +467,14 @@ class TestFilesPipelineCustomSettings:\n             assert custom_value != self.default_cls_settings[pipe_attr]\n             assert getattr(user_pipeline, pipe_inst_attr) == custom_value\n \n-    def test_custom_settings_and_class_attrs_for_subclasses(self):\n+    def test_custom_settings_and_class_attrs_for_subclasses(self, tmp_path):\n         \"\"\"\n         If there are custom settings for subclass AND class attributes\n         setting keys are preferred and override attributes.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline()\n         prefix = pipeline_cls.__name__.upper()\n-        settings = self._generate_fake_settings(prefix=prefix)\n+        settings = self._generate_fake_settings(tmp_path, prefix=prefix)\n         user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for (\n             pipe_cls_attr,\n@@ -497,13 +485,13 @@ class TestFilesPipelineCustomSettings:\n             assert custom_value != self.default_cls_settings[pipe_cls_attr]\n             assert getattr(user_pipeline, pipe_inst_attr) == custom_value\n \n-    def test_cls_attrs_with_DEFAULT_prefix(self):\n+    def test_cls_attrs_with_DEFAULT_prefix(self, tmp_path):\n         class UserDefinedFilesPipeline(FilesPipeline):\n             DEFAULT_FILES_RESULT_FIELD = \"this\"\n             DEFAULT_FILES_URLS_FIELD = \"that\"\n \n         pipeline = UserDefinedFilesPipeline.from_crawler(\n-            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+            get_crawler(None, {\"FILES_STORE\": tmp_path})\n         )\n         assert (\n             pipeline.files_result_field\n@@ -514,12 +502,12 @@ class TestFilesPipelineCustomSettings:\n             == UserDefinedFilesPipeline.DEFAULT_FILES_URLS_FIELD\n         )\n \n-    def test_user_defined_subclass_default_key_names(self):\n+    def test_user_defined_subclass_default_key_names(self, tmp_path):\n         \"\"\"Test situation when user defines subclass of FilesPipeline,\n         but uses attribute names for default pipeline (without prefixing\n         them with pipeline class name).\n         \"\"\"\n-        settings = self._generate_fake_settings()\n+        settings = self._generate_fake_settings(tmp_path)\n \n         class UserPipe(FilesPipeline):\n             pass\n\n@@ -314,13 +314,7 @@ class TestImagesPipelineCustomSettings:\n         \"IMAGES_RESULT_FIELD\": \"images\",\n     }\n \n-    def setup_method(self):\n-        self.tempdir = mkdtemp()\n-\n-    def teardown_method(self):\n-        rmtree(self.tempdir)\n-\n-    def _generate_fake_settings(self, prefix=None):\n+    def _generate_fake_settings(self, tmp_path, prefix=None):\n         \"\"\"\n         :param prefix: string for setting keys\n         :return: dictionary of image pipeline settings\n@@ -331,7 +325,7 @@ class TestImagesPipelineCustomSettings:\n \n         settings = {\n             \"IMAGES_EXPIRES\": random.randint(100, 1000),\n-            \"IMAGES_STORE\": self.tempdir,\n+            \"IMAGES_STORE\": tmp_path,\n             \"IMAGES_RESULT_FIELD\": random_string(),\n             \"IMAGES_URLS_FIELD\": random_string(),\n             \"IMAGES_MIN_WIDTH\": random.randint(1, 1000),\n@@ -368,13 +362,13 @@ class TestImagesPipelineCustomSettings:\n \n         return UserDefinedImagePipeline\n \n-    def test_different_settings_for_different_instances(self):\n+    def test_different_settings_for_different_instances(self, tmp_path):\n         \"\"\"\n         If there are two instances of ImagesPipeline class with different settings, they should\n         have different settings.\n         \"\"\"\n-        custom_settings = self._generate_fake_settings()\n-        default_sts_pipe = ImagesPipeline(self.tempdir, crawler=get_crawler(None))\n+        custom_settings = self._generate_fake_settings(tmp_path)\n+        default_sts_pipe = ImagesPipeline(tmp_path, crawler=get_crawler(None))\n         user_sts_pipe = ImagesPipeline.from_crawler(get_crawler(None, custom_settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_default_value = self.default_pipeline_settings.get(pipe_attr)\n@@ -385,14 +379,14 @@ class TestImagesPipelineCustomSettings:\n             )\n             assert getattr(user_sts_pipe, pipe_attr.lower()) == custom_value\n \n-    def test_subclass_attrs_preserved_default_settings(self):\n+    def test_subclass_attrs_preserved_default_settings(self, tmp_path):\n         \"\"\"\n         If image settings are not defined at all subclass of ImagePipeline takes values\n         from class attributes.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         pipeline = pipeline_cls.from_crawler(\n-            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n+            get_crawler(None, {\"IMAGES_STORE\": tmp_path})\n         )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Instance attribute (lowercase) must be equal to class attribute (uppercase).\n@@ -400,13 +394,13 @@ class TestImagesPipelineCustomSettings:\n             assert attr_value != self.default_pipeline_settings[pipe_attr]\n             assert attr_value == getattr(pipeline, pipe_attr)\n \n-    def test_subclass_attrs_preserved_custom_settings(self):\n+    def test_subclass_attrs_preserved_custom_settings(self, tmp_path):\n         \"\"\"\n         If image settings are defined but they are not defined for subclass default\n         values taken from settings should be preserved.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n-        settings = self._generate_fake_settings()\n+        settings = self._generate_fake_settings(tmp_path)\n         pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Instance attribute (lowercase) must be equal to\n@@ -416,7 +410,7 @@ class TestImagesPipelineCustomSettings:\n             setings_value = settings.get(settings_attr)\n             assert value == setings_value\n \n-    def test_no_custom_settings_for_subclasses(self):\n+    def test_no_custom_settings_for_subclasses(self, tmp_path):\n         \"\"\"\n         If there are no settings for subclass and no subclass attributes, pipeline should use\n         attributes of base class.\n@@ -426,14 +420,14 @@ class TestImagesPipelineCustomSettings:\n             pass\n \n         user_pipeline = UserDefinedImagePipeline.from_crawler(\n-            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n+            get_crawler(None, {\"IMAGES_STORE\": tmp_path})\n         )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = self.default_pipeline_settings.get(pipe_attr.upper())\n             assert getattr(user_pipeline, pipe_attr.lower()) == custom_value\n \n-    def test_custom_settings_for_subclasses(self):\n+    def test_custom_settings_for_subclasses(self, tmp_path):\n         \"\"\"\n         If there are custom settings for subclass and NO class attributes, pipeline should use custom\n         settings.\n@@ -443,7 +437,7 @@ class TestImagesPipelineCustomSettings:\n             pass\n \n         prefix = UserDefinedImagePipeline.__name__.upper()\n-        settings = self._generate_fake_settings(prefix=prefix)\n+        settings = self._generate_fake_settings(tmp_path, prefix=prefix)\n         user_pipeline = UserDefinedImagePipeline.from_crawler(\n             get_crawler(None, settings)\n         )\n@@ -453,27 +447,27 @@ class TestImagesPipelineCustomSettings:\n             assert custom_value != self.default_pipeline_settings[pipe_attr]\n             assert getattr(user_pipeline, pipe_attr.lower()) == custom_value\n \n-    def test_custom_settings_and_class_attrs_for_subclasses(self):\n+    def test_custom_settings_and_class_attrs_for_subclasses(self, tmp_path):\n         \"\"\"\n         If there are custom settings for subclass AND class attributes\n         setting keys are preferred and override attributes.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         prefix = pipeline_cls.__name__.upper()\n-        settings = self._generate_fake_settings(prefix=prefix)\n+        settings = self._generate_fake_settings(tmp_path, prefix=prefix)\n         user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n             assert custom_value != self.default_pipeline_settings[pipe_attr]\n             assert getattr(user_pipeline, pipe_attr.lower()) == custom_value\n \n-    def test_cls_attrs_with_DEFAULT_prefix(self):\n+    def test_cls_attrs_with_DEFAULT_prefix(self, tmp_path):\n         class UserDefinedImagePipeline(ImagesPipeline):\n             DEFAULT_IMAGES_URLS_FIELD = \"something\"\n             DEFAULT_IMAGES_RESULT_FIELD = \"something_else\"\n \n         pipeline = UserDefinedImagePipeline.from_crawler(\n-            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n+            get_crawler(None, {\"IMAGES_STORE\": tmp_path})\n         )\n         assert (\n             pipeline.images_result_field\n@@ -484,12 +478,12 @@ class TestImagesPipelineCustomSettings:\n             == UserDefinedImagePipeline.DEFAULT_IMAGES_URLS_FIELD\n         )\n \n-    def test_user_defined_subclass_default_key_names(self):\n+    def test_user_defined_subclass_default_key_names(self, tmp_path):\n         \"\"\"Test situation when user defines subclass of ImagePipeline,\n         but uses attribute names for default pipeline (without prefixing\n         them with pipeline class name).\n         \"\"\"\n-        settings = self._generate_fake_settings()\n+        settings = self._generate_fake_settings(tmp_path)\n \n         class UserPipe(ImagesPipeline):\n             pass\n\n@@ -1,24 +1,14 @@\n-from pathlib import Path\n-from shutil import rmtree\n-from tempfile import mkdtemp\n-\n from scrapy.utils.template import render_templatefile\n \n \n class TestUtilsRenderTemplateFile:\n-    def setup_method(self):\n-        self.tmp_path = mkdtemp()\n-\n-    def teardown_method(self):\n-        rmtree(self.tmp_path)\n-\n-    def test_simple_render(self):\n+    def test_simple_render(self, tmp_path):\n         context = {\"project_name\": \"proj\", \"name\": \"spi\", \"classname\": \"TheSpider\"}\n         template = \"from ${project_name}.spiders.${name} import ${classname}\"\n         rendered = \"from proj.spiders.spi import TheSpider\"\n \n-        template_path = Path(self.tmp_path, \"templ.py.tmpl\")\n-        render_path = Path(self.tmp_path, \"templ.py\")\n+        template_path = tmp_path / \"templ.py.tmpl\"\n+        render_path = tmp_path / \"templ.py\"\n \n         template_path.write_text(template, encoding=\"utf8\")\n         assert template_path.is_file()  # Failure of test itself\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d0dabbc09706b082e2250790cd7a00c033ad8021", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 120 | Lines Deleted: 79 | Files Changed: 19 | Hunks: 60 | Methods Changed: 39 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 199 | Churn Cumulative: 15737 | Contributors (this commit): 119 | Commits (past 90d): 68 | Contributors (cumulative): 326 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,7 +51,7 @@ def chdir(tmpdir):\n def pytest_addoption(parser):\n     parser.addoption(\n         \"--reactor\",\n-        default=\"default\",\n+        default=\"asyncio\",\n         choices=[\"default\", \"asyncio\"],\n     )\n \n@@ -67,17 +67,17 @@ def reactor_pytest(request):\n \n @pytest.fixture(autouse=True)\n def only_asyncio(request, reactor_pytest):\n-    if request.node.get_closest_marker(\"only_asyncio\") and reactor_pytest != \"asyncio\":\n-        pytest.skip(\"This test is only run with --reactor=asyncio\")\n+    if request.node.get_closest_marker(\"only_asyncio\") and reactor_pytest == \"default\":\n+        pytest.skip(\"This test is only run without --reactor=default\")\n \n \n @pytest.fixture(autouse=True)\n def only_not_asyncio(request, reactor_pytest):\n     if (\n         request.node.get_closest_marker(\"only_not_asyncio\")\n-        and reactor_pytest == \"asyncio\"\n+        and reactor_pytest != \"default\"\n     ):\n-        pytest.skip(\"This test is only run without --reactor=asyncio\")\n+        pytest.skip(\"This test is only run with --reactor=default\")\n \n \n @pytest.fixture(autouse=True)\n@@ -117,7 +117,7 @@ def requires_boto3(request):\n \n \n def pytest_configure(config):\n-    if config.getoption(\"--reactor\") == \"asyncio\":\n+    if config.getoption(\"--reactor\") != \"default\":\n         install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n \n \n\n@@ -68,7 +68,7 @@ class ImagesPipeline(FilesPipeline):\n             self._Image = Image\n         except ImportError:\n             raise NotConfigured(\n-                \"ImagesPipeline requires installing Pillow 4.0.0 or later\"\n+                \"ImagesPipeline requires installing Pillow 8.0.0 or later\"\n             )\n \n         super().__init__(\n\n@@ -341,7 +341,7 @@ TELNETCONSOLE_HOST = \"127.0.0.1\"\n TELNETCONSOLE_USERNAME = \"scrapy\"\n TELNETCONSOLE_PASSWORD = None\n \n-TWISTED_REACTOR = None\n+TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n \n SPIDER_CONTRACTS = {}\n SPIDER_CONTRACTS_BASE = {\n\n@@ -182,11 +182,9 @@ def log_scrapy_info(settings: Settings) -> None:\n \n \n def log_reactor_info() -> None:\n-    from twisted.internet import reactor\n+    from twisted.internet import asyncioreactor, reactor\n \n     logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n-    from twisted.internet import asyncioreactor\n-\n     if isinstance(reactor, asyncioreactor.AsyncioSelectorReactor):\n         logger.debug(\n             \"Using asyncio event loop: %s.%s\",\n\n@@ -18,6 +18,7 @@ from twisted.trial.unittest import SkipTest\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.deprecate import create_deprecated_class\n+from scrapy.utils.reactor import is_asyncio_reactor_installed\n from scrapy.utils.spider import DefaultSpider\n \n if TYPE_CHECKING:\n@@ -109,6 +110,19 @@ def get_ftp_content_and_delete(\n TestSpider = create_deprecated_class(\"TestSpider\", DefaultSpider)\n \n \n+def get_reactor_settings() -> dict[str, Any]:\n+    \"\"\"Return a settings dict that works with the installed reactor.\n+\n+    ``Crawler._apply_settings()`` checks that the installed reactor matches the\n+    settings, so tests that run the crawler in the current process may need to\n+    pass a correct ``\"TWISTED_REACTOR\"`` setting value when creating it.\n+    \"\"\"\n+    settings: dict[str, Any] = {}\n+    if not is_asyncio_reactor_installed():\n+        settings[\"TWISTED_REACTOR\"] = None\n+    return settings\n+\n+\n def get_crawler(\n     spidercls: type[Spider] | None = None,\n     settings_dict: dict[str, Any] | None = None,\n@@ -120,9 +134,12 @@ def get_crawler(\n     \"\"\"\n     from scrapy.crawler import CrawlerRunner\n \n-    # Set by default settings that prevent deprecation warnings.\n-    settings: dict[str, Any] = {}\n-    settings.update(settings_dict or {})\n+    # When needed, useful settings can be added here, e.g. ones that prevent\n+    # deprecation warnings.\n+    settings: dict[str, Any] = {\n+        **get_reactor_settings(),\n+        **(settings_dict or {}),\n+    }\n     runner = CrawlerRunner(settings)\n     crawler = runner.create_crawler(spidercls or DefaultSpider)\n     crawler._apply_settings()\n\n@@ -1,14 +1,8 @@\n-import asyncio\n-import sys\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+from scrapy.utils.reactor import install_reactor\n \n-from twisted.internet import asyncioreactor\n-\n-if sys.platform == \"win32\":\n-    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n-asyncioreactor.install(asyncio.get_event_loop())\n-\n-import scrapy  # noqa: E402\n-from scrapy.crawler import CrawlerProcess  # noqa: E402\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n \n \n class NoRequestsSpider(scrapy.Spider):\n\n@@ -1,4 +1,5 @@\n from twisted.internet import reactor  # noqa: F401\n+from twisted.python import log\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n@@ -13,5 +14,6 @@ class NoRequestsSpider(scrapy.Spider):\n \n process = CrawlerProcess(settings={})\n \n-process.crawl(NoRequestsSpider)\n+d = process.crawl(NoRequestsSpider)\n+d.addErrback(log.err)\n process.start()\n\n@@ -1,4 +1,5 @@\n from twisted.internet import selectreactor\n+from twisted.python import log\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n@@ -15,5 +16,6 @@ class NoRequestsSpider(scrapy.Spider):\n \n process = CrawlerProcess(settings={})\n \n-process.crawl(NoRequestsSpider)\n+d = process.crawl(NoRequestsSpider)\n+d.addErrback(log.err)\n process.start()\n\n@@ -1,3 +1,9 @@\n+# ruff: noqa: E402\n+\n+from scrapy.utils.reactor import install_reactor\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+\n from urllib.parse import urlparse\n \n from twisted.internet import reactor\n\n@@ -9,7 +9,7 @@ from scrapy import Spider\n from scrapy.crawler import Crawler, CrawlerRunner\n from scrapy.exceptions import NotConfigured\n from scrapy.settings import BaseSettings, Settings\n-from scrapy.utils.test import get_crawler\n+from scrapy.utils.test import get_crawler, get_reactor_settings\n \n \n class SimpleAddon:\n@@ -105,6 +105,7 @@ class TestAddonManager(unittest.TestCase):\n         }\n         settings_dict = {\n             \"ADDONS\": {get_addon_cls(config): 1},\n+            **get_reactor_settings(),\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n         assert crawler.settings.getint(\"KEY\") == 15\n@@ -119,6 +120,7 @@ class TestAddonManager(unittest.TestCase):\n         settings_dict = {\n             \"KEY\": 20,  # priority=project\n             \"ADDONS\": {get_addon_cls(config): 1},\n+            **get_reactor_settings(),\n         }\n         settings = Settings(settings_dict)\n         settings.set(\"KEY\", 0, priority=\"default\")\n@@ -196,6 +198,7 @@ class TestAddonManager(unittest.TestCase):\n                 return spider\n \n         settings = Settings()\n+        settings.setdict(get_reactor_settings())\n         settings.set(\"KEY\", \"default\", priority=\"default\")\n         runner = CrawlerRunner(settings)\n         crawler = runner.create_crawler(MySpider)\n\n@@ -18,7 +18,7 @@ from scrapy.exceptions import StopDownload\n from scrapy.http import Request\n from scrapy.http.response import Response\n from scrapy.utils.python import to_unicode\n-from scrapy.utils.test import get_crawler\n+from scrapy.utils.test import get_crawler, get_reactor_settings\n from tests import NON_EXISTING_RESOLVABLE\n from tests.mockserver import MockServer\n from tests.spiders import (\n@@ -412,7 +412,7 @@ with multiples lines\n \n     @defer.inlineCallbacks\n     def test_crawl_multiple(self):\n-        runner = CrawlerRunner()\n+        runner = CrawlerRunner(get_reactor_settings())\n         runner.crawl(\n             SimpleSpider,\n             self.mockserver.url(\"/status?n=200\"),\n\n@@ -25,7 +25,7 @@ from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n-from scrapy.utils.test import get_crawler\n+from scrapy.utils.test import get_crawler, get_reactor_settings\n from tests.mockserver import MockServer, get_mockserver_env\n \n BASE_SETTINGS: dict[str, Any] = {}\n@@ -35,6 +35,7 @@ def get_raw_crawler(spidercls=None, settings_dict=None):\n     \"\"\"get_crawler alternative that only calls the __init__ method of the\n     crawler.\"\"\"\n     settings = Settings()\n+    settings.setdict(get_reactor_settings())\n     settings.setdict(settings_dict or {})\n     return Crawler(spidercls or DefaultSpider, settings)\n \n@@ -48,7 +49,12 @@ class TestBaseCrawler(unittest.TestCase):\n class TestCrawler(TestBaseCrawler):\n     def test_populate_spidercls_settings(self):\n         spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n-        project_settings = {**BASE_SETTINGS, \"TEST1\": \"project\", \"TEST3\": \"project\"}\n+        project_settings = {\n+            **BASE_SETTINGS,\n+            \"TEST1\": \"project\",\n+            \"TEST3\": \"project\",\n+            **get_reactor_settings(),\n+        }\n \n         class CustomSettingsSpider(DefaultSpider):\n             custom_settings = spider_settings\n@@ -581,7 +587,7 @@ class NoRequestsSpider(scrapy.Spider):\n @pytest.mark.usefixtures(\"reactor_pytest\")\n class TestCrawlerRunnerHasSpider(unittest.TestCase):\n     def _runner(self):\n-        return CrawlerRunner()\n+        return CrawlerRunner(get_reactor_settings())\n \n     @inlineCallbacks\n     def test_crawler_runner_bootstrap_successful(self):\n@@ -626,13 +632,7 @@ class TestCrawlerRunnerHasSpider(unittest.TestCase):\n \n     @inlineCallbacks\n     def test_crawler_runner_asyncio_enabled_true(self):\n-        if self.reactor_pytest == \"asyncio\":\n-            CrawlerRunner(\n-                settings={\n-                    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-                }\n-            )\n-        else:\n+        if self.reactor_pytest == \"default\":\n             runner = CrawlerRunner(\n                 settings={\n                     \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n@@ -643,6 +643,12 @@ class TestCrawlerRunnerHasSpider(unittest.TestCase):\n                 match=r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\",\n             ):\n                 yield runner.crawl(NoRequestsSpider)\n+        else:\n+            CrawlerRunner(\n+                settings={\n+                    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+                }\n+            )\n \n \n class ScriptRunnerMixin:\n@@ -672,7 +678,7 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         assert \"Spider closed (finished)\" in log\n         assert (\n             \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n-            not in log\n+            in log\n         )\n \n     def test_multi(self):\n@@ -680,18 +686,17 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         assert \"Spider closed (finished)\" in log\n         assert (\n             \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n-            not in log\n+            in log\n         )\n         assert \"ReactorAlreadyInstalledError\" not in log\n \n     def test_reactor_default(self):\n         log = self.run_script(\"reactor_default.py\")\n-        assert \"Spider closed (finished)\" in log\n+        assert \"Spider closed (finished)\" not in log\n         assert (\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n-            not in log\n-        )\n-        assert \"ReactorAlreadyInstalledError\" not in log\n+            \"does not match the requested one \"\n+            \"(twisted.internet.asyncioreactor.AsyncioSelectorReactor)\"\n+        ) in log\n \n     def test_reactor_default_twisted_reactor_select(self):\n         log = self.run_script(\"reactor_default_twisted_reactor_select.py\")\n@@ -716,8 +721,11 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n \n     def test_reactor_select(self):\n         log = self.run_script(\"reactor_select.py\")\n-        assert \"Spider closed (finished)\" in log\n-        assert \"ReactorAlreadyInstalledError\" not in log\n+        assert \"Spider closed (finished)\" not in log\n+        assert (\n+            \"does not match the requested one \"\n+            \"(twisted.internet.asyncioreactor.AsyncioSelectorReactor)\"\n+        ) in log\n \n     def test_reactor_select_twisted_reactor_select(self):\n         log = self.run_script(\"reactor_select_twisted_reactor_select.py\")\n\n@@ -33,7 +33,7 @@ class TestScrapyUtils:\n         tox_config_file_path = Path(__file__).parent / \"..\" / \"tox.ini\"\n         config_parser = ConfigParser()\n         config_parser.read(tox_config_file_path)\n-        pattern = r\"Twisted\\[http2\\]==([\\d.]+)\"\n+        pattern = r\"Twisted==([\\d.]+)\"\n         match = re.search(pattern, config_parser[\"pinned\"][\"deps\"])\n         pinned_twisted_version_string = match[1]\n \n\n@@ -307,7 +307,7 @@ class TestHttp(unittest.TestCase, ABC):\n \n     @defer.inlineCallbacks\n     def test_timeout_download_from_spider_nodata_rcvd(self):\n-        if self.reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n+        if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n             # https://twistedmatrix.com/trac/ticket/10279\n             raise unittest.SkipTest(\n                 \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n@@ -322,7 +322,7 @@ class TestHttp(unittest.TestCase, ABC):\n \n     @defer.inlineCallbacks\n     def test_timeout_download_from_spider_server_hangs(self):\n-        if self.reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n+        if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n             # https://twistedmatrix.com/trac/ticket/10279\n             raise unittest.SkipTest(\n                 \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n@@ -1136,7 +1136,7 @@ class TestFTPBase(unittest.TestCase):\n \n class TestFTP(TestFTPBase):\n     def test_invalid_credentials(self):\n-        if self.reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n+        if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n             raise unittest.SkipTest(\n                 \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n             )\n\n@@ -64,7 +64,7 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_delay(self):\n-        crawler = CrawlerRunner().create_crawler(DownloaderSlotsSettingsTestSpider)\n+        crawler = get_crawler(DownloaderSlotsSettingsTestSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         slots = crawler.engine.downloader.slots\n         times = crawler.spider.times\n\n@@ -1,9 +1,11 @@\n-import datetime\n-import typing\n-import unittest\n+from __future__ import annotations\n+\n+import datetime\n+import unittest\n+from typing import Any, Callable\n \n-from scrapy.crawler import Crawler\n from scrapy.extensions.periodic_log import PeriodicLog\n+from scrapy.utils.test import get_crawler\n \n from .spiders import MetaSpider\n \n@@ -59,9 +61,8 @@ class CustomPeriodicLog(PeriodicLog):\n         self.stats._stats = stats_dump_2\n \n \n-def extension(settings=None):\n-    crawler = Crawler(MetaSpider, settings=settings)\n-    crawler._apply_settings()\n+def extension(settings: dict[str, Any] | None = None) -> CustomPeriodicLog:\n+    crawler = get_crawler(MetaSpider, settings)\n     return CustomPeriodicLog.from_crawler(crawler)\n \n \n@@ -94,7 +95,7 @@ class TestPeriodicLog(unittest.TestCase):\n             ext.spider_closed(spider, reason=\"finished\")\n             return ext, a, b\n \n-        def check(settings: dict, condition: typing.Callable):\n+        def check(settings: dict[str, Any], condition: Callable) -> None:\n             ext, a, b = emulate(settings)\n             assert list(a[\"delta\"].keys()) == [\n                 k for k, v in ext.stats._stats.items() if condition(k, v)\n@@ -151,7 +152,7 @@ class TestPeriodicLog(unittest.TestCase):\n             ext.spider_closed(spider, reason=\"finished\")\n             return ext, a, b\n \n-        def check(settings: dict, condition: typing.Callable):\n+        def check(settings: dict[str, Any], condition: Callable) -> None:\n             ext, a, b = emulate(settings)\n             assert list(a[\"stats\"].keys()) == [\n                 k for k, v in ext.stats._stats.items() if condition(k, v)\n\n@@ -3,18 +3,22 @@ from __future__ import annotations\n import shutil\n from pathlib import Path\n from tempfile import mkdtemp\n+from typing import TYPE_CHECKING, Any\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from w3lib.url import add_or_replace_parameter\n \n-from scrapy import signals\n-from scrapy.crawler import CrawlerRunner\n+from scrapy import Spider, signals\n from scrapy.utils.misc import load_object\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n from tests.spiders import SimpleSpider\n \n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n \n class MediaDownloadSpider(SimpleSpider):\n     name = \"mediadownload\"\n@@ -80,7 +84,6 @@ class TestFileDownloadCrawl(TestCase):\n             \"ITEM_PIPELINES\": {self.pipeline_class: 1},\n             self.store_setting_key: str(self.tmpmediastore),\n         }\n-        self.runner = CrawlerRunner(self.settings)\n         self.items = []\n \n     def tearDown(self):\n@@ -90,10 +93,12 @@ class TestFileDownloadCrawl(TestCase):\n     def _on_item_scraped(self, item):\n         self.items.append(item)\n \n-    def _create_crawler(self, spider_class, runner=None, **kwargs):\n-        if runner is None:\n-            runner = self.runner\n-        crawler = runner.create_crawler(spider_class, **kwargs)\n+    def _create_crawler(\n+        self, spider_class: type[Spider], settings: dict[str, Any] | None = None\n+    ) -> Crawler:\n+        if settings is None:\n+            settings = self.settings\n+        crawler = get_crawler(spider_class, settings)\n         crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n         return crawler\n \n@@ -175,10 +180,11 @@ class TestFileDownloadCrawl(TestCase):\n \n     @defer.inlineCallbacks\n     def test_download_media_redirected_allowed(self):\n-        settings = dict(self.settings)\n-        settings.update({\"MEDIA_ALLOW_REDIRECTS\": True})\n-        runner = CrawlerRunner(settings)\n-        crawler = self._create_crawler(RedirectedMediaDownloadSpider, runner=runner)\n+        settings = {\n+            **self.settings,\n+            \"MEDIA_ALLOW_REDIRECTS\": True,\n+        }\n+        crawler = self._create_crawler(RedirectedMediaDownloadSpider, settings)\n         with LogCapture() as log:\n             yield crawler.crawl(\n                 self.mockserver.url(\"/files/images/\"),\n@@ -201,8 +207,7 @@ class TestFileDownloadCrawl(TestCase):\n             **self.settings,\n             \"ITEM_PIPELINES\": {ExceptionRaisingMediaPipeline: 1},\n         }\n-        runner = CrawlerRunner(settings)\n-        crawler = self._create_crawler(MediaDownloadSpider, runner=runner)\n+        crawler = self._create_crawler(MediaDownloadSpider, settings)\n         with LogCapture() as log:\n             yield crawler.crawl(\n                 self.mockserver.url(\"/files/images/\"),\n\n@@ -27,7 +27,7 @@ from scrapy.spiders import (\n     XMLFeedSpider,\n )\n from scrapy.spiders.init import InitSpider\n-from scrapy.utils.test import get_crawler\n+from scrapy.utils.test import get_crawler, get_reactor_settings\n from tests import get_testdata, tests_datadir\n \n \n@@ -108,7 +108,11 @@ class TestSpider(unittest.TestCase):\n     @inlineCallbacks\n     def test_settings_in_from_crawler(self):\n         spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n-        project_settings = {\"TEST1\": \"project\", \"TEST3\": \"project\"}\n+        project_settings = {\n+            \"TEST1\": \"project\",\n+            \"TEST3\": \"project\",\n+            **get_reactor_settings(),\n+        }\n \n         class TestSpider(self.spider_class):\n             name = \"test\"\n\n@@ -2,6 +2,7 @@ import asyncio\n import warnings\n \n import pytest\n+from twisted.trial.unittest import TestCase\n \n from scrapy.utils.defer import deferred_f_from_coro_f\n from scrapy.utils.reactor import (\n@@ -12,10 +13,10 @@ from scrapy.utils.reactor import (\n \n \n @pytest.mark.usefixtures(\"reactor_pytest\")\n-class TestAsyncio:\n+class TestAsyncio(TestCase):\n     def test_is_asyncio_reactor_installed(self):\n         # the result should depend only on the pytest --reactor argument\n-        assert is_asyncio_reactor_installed() == (self.reactor_pytest == \"asyncio\")\n+        assert is_asyncio_reactor_installed() == (self.reactor_pytest != \"default\")\n \n     def test_install_asyncio_reactor(self):\n         from twisted.internet import reactor as original_reactor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
