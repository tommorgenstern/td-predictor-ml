{"custom_id": "keras#3b76158c490f32e20df1f100ef7308ab000b20e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 723 | Contributors (this commit): 11 | Commits (past 90d): 27 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -110,7 +110,7 @@ class Convolution2D(Layer):\n         super(Convolution2D, self).__init__()\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n-        self.subsample = subsample\n+        self.subsample = tuple(subsample)\n         self.border_mode = border_mode\n         self.nb_filter = nb_filter\n         self.stack_size = stack_size\n@@ -228,7 +228,7 @@ class MaxPooling2D(Layer):\n     def __init__(self, poolsize=(2, 2), stride=None, ignore_border=True):\n         super(MaxPooling2D, self).__init__()\n         self.input = T.tensor4()\n-        self.poolsize = poolsize\n+        self.poolsize = tuple(poolsize)\n         self.stride = stride\n         self.ignore_border = ignore_border\n \n@@ -264,7 +264,7 @@ class UpSample2D(Layer):\n     def __init__(self, size=(2, 2)):\n         super(UpSample2D, self).__init__()\n         self.input = T.tensor4()\n-        self.size = size\n+        self.size = tuple(size)\n \n     def get_output(self, train):\n         X = self.get_input(train)\n@@ -280,7 +280,7 @@ class UpSample2D(Layer):\n class ZeroPadding2D(Layer):\n     def __init__(self, pad=(1, 1)):\n         super(ZeroPadding2D, self).__init__()\n-        self.pad = pad\n+        self.pad = tuple(pad)\n         self.input = T.tensor4()\n \n     def get_output(self, train):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ed53ae5a488b08cd9d65bc12f03859b56e79a88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 153 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ from six.moves import range\n def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.):\n     \"\"\"\n         Pad each sequence to the same length: \n-        the length of the longuest sequence.\n+        the length of the longest sequence.\n \n         If maxlen is provided, any sequence longer\n         than maxlen is truncated to maxlen. Truncation happens off either the beginning (default) or\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d8307622d3f2c67feb5258caeaaca91f2cd9848", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 6 | Churn Cumulative: 1804 | Contributors (this commit): 22 | Commits (past 90d): 33 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -343,7 +343,9 @@ class Reshape(Layer):\n     '''\n     def __init__(self, *dims):\n         super(Reshape, self).__init__()\n-        self.dims = dims\n+        if type(dims[0]) in [list, tuple]:\n+            dims = dims[0]\n+        self.dims = tuple(dims)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n@@ -361,7 +363,7 @@ class Permute(Layer):\n     '''\n     def __init__(self, dims):\n         super(Permute, self).__init__()\n-        self.dims = dims\n+        self.dims = tuple(dims)\n \n     def get_output(self, train):\n         X = self.get_input(train)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#57bb9e261316548652c2fc6a893d724bdbbfc87d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2914 | Contributors (this commit): 23 | Commits (past 90d): 45 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -312,17 +312,17 @@ class Model(object):\n             pp.pprint(config)\n         return config\n \n-    def to_yaml(self):\n+    def to_yaml(self, **kwargs):\n         # dump model configuration to yaml string\n         import yaml\n         config = self.get_config()\n-        return yaml.dump(config)\n+        return yaml.dump(config, **kwargs)\n \n-    def to_json(self):\n+    def to_json(self, **kwargs):\n         # dump model configuration to json string\n         import json\n         config = self.get_config()\n-        return json.dumps(config)\n+        return json.dumps(config, **kwargs)\n \n \n class Sequential(Model, containers.Sequential):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#88a0ab5e930360c8c51fff740dd261dc751fcd7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 15 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 37 | Churn Cumulative: 3226 | Contributors (this commit): 29 | Commits (past 90d): 59 | Contributors (cumulative): 36 | DMM Complexity: 0.0\n\nDIFF:\n@@ -100,29 +100,29 @@ def standardize_weights(y, sample_weight=None, class_weight=None):\n         return np.ones(y.shape[:-1] + (1,))\n \n \n-def model_from_yaml(yaml_string):\n+def model_from_yaml(yaml_string, custom_layers={}):\n     '''\n         Returns a model generated from a local yaml file,\n         which is either created by hand or from to_yaml method of Sequential or Graph\n     '''\n     import yaml\n     config = yaml.load(yaml_string)\n-    return model_from_config(config)\n+    return model_from_config(config, custom_layers=custom_layers)\n \n \n-def model_from_json(json_string):\n+def model_from_json(json_string, custom_layers={}):\n     import json\n     config = json.loads(json_string)\n-    return model_from_config(config)\n+    return model_from_config(config, custom_layers=custom_layers)\n \n \n-def model_from_config(config):\n+def model_from_config(config, custom_layers={}):\n     model_name = config.get('name')\n     if model_name not in {'Graph', 'Sequential'}:\n         raise Exception('Unrecognized model:', model_name)\n \n     # Create a container then set class to appropriate model\n-    model = container_from_config(config)\n+    model = container_from_config(config, custom_layers=custom_layers)\n     if model_name == 'Graph':\n         model.__class__ = Graph\n     elif model_name == 'Sequential':\n\n@@ -5,9 +5,9 @@ import sys\n import six\n \n \n-def get_from_module(identifier, module_params, module_name, instantiate=False, kwargs=None):\n+def get_from_module(identifier, module_params, module_name, instantiate=False, kwargs=None, custom_layers={}):\n     if isinstance(identifier, six.string_types):\n-        res = module_params.get(identifier)\n+        res = module_params.get(identifier, custom_layers.get(identifier))\n         if not res:\n             raise Exception('Invalid ' + str(module_name) + ': ' + str(identifier))\n         if instantiate and not kwargs:\n\n@@ -17,7 +17,7 @@ from .. import regularizers\n from .. import constraints\n \n \n-def container_from_config(original_layer_dict):\n+def container_from_config(original_layer_dict, custom_layers={}):\n     layer_dict = copy.deepcopy(original_layer_dict)\n     name = layer_dict.get('name')\n \n@@ -26,7 +26,7 @@ def container_from_config(original_layer_dict):\n         layers = layer_dict.get('layers')\n         layer_list = []\n         for layer in layers:\n-            init_layer = container_from_config(layer)\n+            init_layer = container_from_config(layer, custom_layers=custom_layers)\n             layer_list.append(init_layer)\n         merge_layer = Merge(layer_list, mode)\n         return merge_layer\n@@ -35,7 +35,7 @@ def container_from_config(original_layer_dict):\n         layers = layer_dict.get('layers')\n         layer_list = []\n         for layer in layers:\n-            init_layer = container_from_config(layer)\n+            init_layer = container_from_config(layer, custom_layers=custom_layers)\n             layer_list.append(init_layer)\n         seq_layer = containers.Sequential(layer_list)\n         return seq_layer\n@@ -49,7 +49,8 @@ def container_from_config(original_layer_dict):\n \n         nodes = layer_dict.get('node_config')\n         for node in nodes:\n-            layer = container_from_config(layer_dict['nodes'].get(node['name']))\n+            layer = container_from_config(layer_dict['nodes'].get(node['name']),\n+                                          custom_layers=custom_layers)\n             node['layer'] = layer\n             graph_layer.add_node(**node)\n \n@@ -59,8 +60,10 @@ def container_from_config(original_layer_dict):\n         return graph_layer\n \n     elif name == 'AutoEncoder':\n-        kwargs = {'encoder': container_from_config(layer_dict.get('encoder_config')),\n-                  'decoder': container_from_config(layer_dict.get('decoder_config'))}\n+        kwargs = {'encoder': container_from_config(layer_dict.get('encoder_config'),\n+                                                   custom_layers=custom_layers),\n+                  'decoder': container_from_config(layer_dict.get('decoder_config'),\n+                                                   custom_layers=custom_layers)}\n         for kwarg in ['output_reconstruction', 'weights']:\n             if kwarg in layer_dict:\n                 kwargs[kwarg] = layer_dict[kwarg]\n@@ -79,7 +82,7 @@ def container_from_config(original_layer_dict):\n                 if vname in [x for x, y in inspect.getmembers(regularizers, predicate=inspect.isclass)]:\n                     layer_dict[k] = regularizers.get(vname, v)\n \n-        base_layer = get_layer(name, layer_dict)\n+        base_layer = get_layer(name, layer_dict, custom_layers=custom_layers)\n         return base_layer\n \n \n@@ -121,5 +124,5 @@ def print_layer_shapes(model, input_shapes):\n \n \n from .generic_utils import get_from_module\n-def get_layer(identifier, kwargs=None):\n-    return get_from_module(identifier, globals(), 'layer', instantiate=True, kwargs=kwargs)\n+def get_layer(identifier, kwargs=None, custom_layers={}):\n+    return get_from_module(identifier, globals(), 'layer', instantiate=True, kwargs=kwargs, custom_layers=custom_layers)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ee07e6ef74f224c3f41da63a6bfdfb8cd008966c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 6 | Churn Cumulative: 1810 | Contributors (this commit): 23 | Commits (past 90d): 32 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -341,10 +341,12 @@ class Reshape(Layer):\n         Can't be used as first layer in a model (no fixed input!)\n         First dimension is assumed to be nb_samples.\n     '''\n-    def __init__(self, *dims):\n+    def __init__(self, *dims, **kwargs):\n         super(Reshape, self).__init__()\n-        if type(dims[0]) in [list, tuple]:\n+        if len(dims) > 0 and type(dims[0]) in [list, tuple]:\n             dims = dims[0]\n+        if len(dims) == 0 and 'dims' in kwargs:\n+            dims = kwargs['dims']\n         self.dims = tuple(dims)\n \n     def get_output(self, train=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c506fbda4ac1d3df0f9a6111ba9bf75a20bc7b68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 369 | Lines Deleted: 48 | Files Changed: 9 | Hunks: 58 | Methods Changed: 42 | Complexity Δ (Sum/Max): 84/36 | Churn Δ: 417 | Churn Cumulative: 5073 | Contributors (this commit): 36 | Commits (past 90d): 95 | Contributors (cumulative): 69 | DMM Complexity: 0.6851851851851852\n\nDIFF:\n@@ -45,6 +45,10 @@ class Sequential(Layer):\n         self.constraints += constraints\n         self.updates += updates\n \n+    @property\n+    def output_shape(self):\n+        return self.layers[-1].output_shape\n+\n     def get_output(self, train=False):\n         return self.layers[-1].get_output(train)\n \n@@ -83,6 +87,7 @@ class Sequential(Layer):\n     def count_params(self):\n         return sum([layer.count_params() for layer in self.layers])\n \n+\n class Graph(Layer):\n     '''\n         Implement a NN graph with arbitrary layer connections,\n@@ -147,6 +152,15 @@ class Graph(Layer):\n     def input(self):\n         return self.get_input()\n \n+    @property\n+    def output_shape(self):\n+        if self.nb_output == 1:\n+            # return tuple\n+            return self.outputs[self.output_order[0]].output_shape\n+        else:\n+            # return dictionary mapping output names to shape tuples\n+            return dict([(k, v.output_shape) for k, v in self.outputs.items()])\n+\n     def get_output(self, train=False):\n         if len(self.inputs) == len(self.outputs) == 1:\n             return self.outputs[self.output_order[0]].get_output(train)\n\n@@ -13,6 +13,37 @@ if on_gpu():\n     from theano.sandbox.cuda import dnn\n \n \n+def conv_output_length(input_length, filter_size, border_mode, stride):\n+    assert border_mode in {'same', 'full', 'valid'}\n+    if border_mode == 'same':\n+        output_length = input_length\n+    elif border_mode == 'full':\n+        output_length = input_length + filter_size - 1\n+    elif border_mode == 'valid':\n+        output_length = input_length - filter_size + 1\n+    return (output_length + stride - 1) // stride\n+\n+\n+def pool_output_length(input_length, pool_size, ignore_border, stride):\n+    if ignore_border:\n+        output_length = input_length - pool_size + 1\n+        output_length = (output_length + stride - 1) // stride\n+    else:\n+        if pool_size == input_length:\n+            output_length = min(input_length, stride - stride % 2)\n+            if output_length <= 0:\n+                output_length = 1\n+        elif stride >= pool_size:\n+            output_length = (input_length + stride - 1) // stride\n+        else:\n+            output_length = (input_length - pool_size + stride - 1) // stride\n+            if output_length <= 0:\n+                output_length = 1\n+            else:\n+                output_length += 1\n+    return output_length\n+\n+\n class Convolution1D(Layer):\n     def __init__(self, input_dim, nb_filter, filter_length,\n                  init='uniform', activation='linear', weights=None,\n@@ -30,7 +61,7 @@ class Convolution1D(Layer):\n         self.subsample_length = subsample_length\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n-        self.subsample = (1, subsample_length)\n+        self.subsample = (subsample_length, 1)\n         self.border_mode = border_mode\n \n         self.input = T.tensor3()\n@@ -64,13 +95,19 @@ class Convolution1D(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def get_output(self, train):\n+    @property\n+    def output_shape(self):\n+        length = conv_output_length(self.input_shape[1], self.filter_length, self.border_mode, self.subsample[0])\n+        return (self.input_shape[0], length, self.nb_filter)\n+\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         X = T.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 2, 1, 3)\n \n         border_mode = self.border_mode\n         if border_mode == 'same':\n             border_mode = 'full'\n+            assert self.subsample == (1, 1)\n \n         conv_out = T.nnet.conv.conv2d(X, self.W, border_mode=border_mode, subsample=self.subsample)\n         if self.border_mode == 'same':\n@@ -149,7 +186,16 @@ class Convolution2D(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def get_output(self, train):\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        rows = input_shape[2]\n+        cols = input_shape[3]\n+        rows = conv_output_length(rows, self.nb_row, self.border_mode, self.subsample[0])\n+        cols = conv_output_length(cols, self.nb_col, self.border_mode, self.subsample[1])\n+        return (input_shape[0], self.nb_filter, rows, cols)\n+\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         border_mode = self.border_mode\n         if on_gpu() and dnn.dnn_available():\n@@ -168,6 +214,7 @@ class Convolution2D(Layer):\n         else:\n             if border_mode == 'same':\n                 border_mode = 'full'\n+                assert(self.subsample == (1, 1))\n \n             conv_out = T.nnet.conv.conv2d(X, self.W,\n                                           border_mode=border_mode,\n@@ -197,20 +244,25 @@ class Convolution2D(Layer):\n \n \n class MaxPooling1D(Layer):\n-    def __init__(self, pool_length=2, stride=None, ignore_border=True):\n+    def __init__(self, pool_length=2, stride=1, ignore_border=True):\n         super(MaxPooling1D, self).__init__()\n+        if type(stride) is not int or not stride:\n+            raise Exception('\"stride\" argument in MaxPooling1D should be an int > 0.')\n         self.pool_length = pool_length\n         self.stride = stride\n-        if self.stride:\n         self.st = (self.stride, 1)\n-        else:\n-            self.st = None\n \n         self.input = T.tensor3()\n         self.poolsize = (pool_length, 1)\n         self.ignore_border = ignore_border\n \n-    def get_output(self, train):\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        length = pool_output_length(input_shape[1], self.pool_length, self.ignore_border, self.stride)\n+        return (input_shape[0], length, input_shape[2])\n+\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         X = T.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 2, 1, 3)\n         output = downsample.max_pool_2d(X, ds=self.poolsize, st=self.st, ignore_border=self.ignore_border)\n@@ -225,14 +277,21 @@ class MaxPooling1D(Layer):\n \n \n class MaxPooling2D(Layer):\n-    def __init__(self, poolsize=(2, 2), stride=None, ignore_border=True):\n+    def __init__(self, poolsize=(2, 2), stride=(1, 1), ignore_border=True):\n         super(MaxPooling2D, self).__init__()\n         self.input = T.tensor4()\n         self.poolsize = tuple(poolsize)\n-        self.stride = stride\n+        self.stride = tuple(stride)\n         self.ignore_border = ignore_border\n \n-    def get_output(self, train):\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        rows = pool_output_length(input_shape[2], self.poolsize[0], self.ignore_border, self.stride[0])\n+        cols = pool_output_length(input_shape[3], self.poolsize[1], self.ignore_border, self.stride[1])\n+        return (input_shape[0], input_shape[1], rows, cols)\n+\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         output = downsample.max_pool_2d(X, ds=self.poolsize, st=self.stride, ignore_border=self.ignore_border)\n         return output\n@@ -250,7 +309,12 @@ class UpSample1D(Layer):\n         self.length = length\n         self.input = T.tensor3()\n \n-    def get_output(self, train):\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        return (input_shape[0], self.length * input_shape[1], input_shape[2])\n+\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         output = theano.tensor.extra_ops.repeat(X, self.length, axis=1)\n         return output\n@@ -266,7 +330,12 @@ class UpSample2D(Layer):\n         self.input = T.tensor4()\n         self.size = tuple(size)\n \n-    def get_output(self, train):\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        return (input_shape[0], input_shape[1], self.size[0] * input_shape[2], self.size[1] * input_shape[3])\n+\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         Y = theano.tensor.extra_ops.repeat(X, self.size[0], axis=2)\n         output = theano.tensor.extra_ops.repeat(Y, self.size[1], axis=3)\n@@ -283,7 +352,12 @@ class ZeroPadding2D(Layer):\n         self.pad = tuple(pad)\n         self.input = T.tensor4()\n \n-    def get_output(self, train):\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        return (input_shape[0], input_shape[1], input_shape[2] + 2 * self.pad[0], input_shape[3] + 2 * self.pad[1])\n+\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         pad = self.pad\n         in_shape = X.shape\n\n@@ -6,6 +6,7 @@ import theano.tensor as T\n import numpy as np\n \n from collections import OrderedDict\n+import copy\n \n from .. import activations, initializations, regularizers, constraints\n from ..utils.theano_utils import shared_zeros, floatX\n@@ -37,14 +38,33 @@ class Layer(object):\n     def nb_output(self):\n         return 1\n \n+    @property\n+    def input_shape(self):\n+        # if layer is not connected (e.g. input layer),\n+        # input shape can be set manually via _input_shape attribute.\n+        if hasattr(self, 'previous'):\n+            return self.previous.output_shape\n+        elif hasattr(self, '_input_shape'):\n+            return self._input_shape\n+        else:\n+            raise NotImplementedError\n+\n+    @property\n+    def output_shape(self):\n+        # default assumption: tensor shape unchanged.\n+        return self.input_shape\n+\n     def get_output(self, train=False):\n         return self.get_input(train)\n \n     def get_input(self, train=False):\n         if hasattr(self, 'previous'):\n             return self.previous.get_output(train=train)\n-        else:\n+        elif hasattr(self, 'input'):\n             return self.input\n+        else:\n+            raise Exception('Layer is not connected\\\n+                and is not an input layer.')\n \n     def supports_masked_input(self):\n         ''' Whether or not this layer respects the output mask of its previous layer in its calculations. If you try\n@@ -115,6 +135,7 @@ class Layer(object):\n     def count_params(self):\n         return sum([np.prod(p.shape.eval()) for p in self.params])\n \n+\n class MaskedLayer(Layer):\n     '''\n     If your layer trivially supports masking (by simply copying the input mask to the output), then subclass MaskedLayer\n@@ -163,20 +184,25 @@ class Masking(MaskedLayer):\n         return {\"name\": self.__class__.__name__,\n                 \"mask_value\": self.mask_value}\n \n+\n class TimeDistributedMerge(Layer):\n-    def __init__(self, mode='sum'):\n-        '''\n-        Sum/multiply/avearge over a time distributed layer's outputs.\n+    '''Sum/multiply/average over the outputs of a TimeDistributed layer.\n+\n     mode: {'sum', 'mul', 'ave'}\n-        Tensor input dimensions:   (nb_sample, shared_dimension, input_dim)\n-        Tensor output dimensions:  (nb_sample, output_dim)\n+    Tensor input dimensions:   (nb_sample, time, features)\n+    Tensor output dimensions:  (nb_sample, features)\n     '''\n+    def __init__(self, mode='sum'):\n         self.mode = mode\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n         self.updates = []\n \n+    @property\n+    def output_shape(self):\n+        return (None, self.input_shape[2])\n+\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         if self.mode == 'sum' or self.mode == 'ave':\n@@ -202,6 +228,8 @@ class Merge(Layer):\n         '''\n         if len(layers) < 2:\n             raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n+        if mode not in {'sum', 'mul', 'concat', 'ave'}:\n+            raise Exception(\"Invalid merge mode: \" + str(mode))\n         self.mode = mode\n         self.concat_axis = concat_axis\n         self.layers = layers\n@@ -219,6 +247,17 @@ class Merge(Layer):\n                     self.params.append(p)\n                     self.constraints.append(c)\n \n+    @property\n+    def output_shape(self):\n+        input_shapes = [layer.output_shape for layer in self.layers]\n+        if self.mode in ['sum', 'mul', 'ave']:\n+            return input_shapes[0]\n+        elif self.mode == 'concat':\n+            output_shape = list(input_shapes[0])\n+            for shape in input_shapes[1:]:\n+                output_shape[self.concat_axis] += shape[concat_axis]\n+            return tuple(output_shape)\n+\n     def get_params(self):\n         return self.params, self.regularizers, self.constraints, self.updates\n \n@@ -347,6 +386,10 @@ class Reshape(Layer):\n             dims = dims[0]\n         self.dims = tuple(dims)\n \n+    @property\n+    def output_shape(self):\n+        return make_tuple(self.input_shape[0], *self.dims)\n+\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         nshape = make_tuple(X.shape[0], *self.dims)\n@@ -359,13 +402,22 @@ class Reshape(Layer):\n \n class Permute(Layer):\n     '''\n-        Permute the dimensions of the data according to the given tuple\n+        Permute the dimensions of the input according to the given tuple.\n     '''\n     def __init__(self, dims):\n         super(Permute, self).__init__()\n         self.dims = tuple(dims)\n \n-    def get_output(self, train):\n+    @property\n+    def output_shape(self):\n+        input_shape = list(self.input_shape)\n+        output_shape = copy.copy(input_shape)\n+        for i, dim in enumerate(self.dims):\n+            target_dim = input_shape[dim]\n+            output_shape[i+1] = target_dim\n+        return tuple(output_shape)\n+\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         return X.dimshuffle((0,) + self.dims)\n \n@@ -382,6 +434,11 @@ class Flatten(Layer):\n     def __init__(self):\n         super(Flatten, self).__init__()\n \n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        return (input_shape[0], np.prod(input_shape[1:]))\n+\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         size = theano.tensor.prod(X.shape) // X.shape[0]\n@@ -400,6 +457,11 @@ class RepeatVector(Layer):\n         super(RepeatVector, self).__init__()\n         self.n = n\n \n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        return (input_shape[0], self.n, input_shape[1])\n+\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         tensors = [X]*self.n\n@@ -461,6 +523,10 @@ class Dense(Layer):\n         self.W.name = '%s_W' % name\n         self.b.name = '%s_b' % name\n \n+    @property\n+    def output_shape(self):\n+        return (self.input_shape[0], self.output_dim)\n+\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         output = self.activation(T.dot(X, self.W) + self.b)\n@@ -504,10 +570,10 @@ class ActivityRegularization(Layer):\n \n class TimeDistributedDense(MaskedLayer):\n     '''\n-       Apply a same DenseLayer for each dimension[1] (shared_dimension) input\n-       Especially useful after a recurrent network with 'return_sequence=True'\n-       Tensor input dimensions:   (nb_sample, shared_dimension, input_dim)\n-       Tensor output dimensions:  (nb_sample, shared_dimension, output_dim)\n+       Apply a same Dense layer for each dimension[1] (time_dimension) input.\n+       Especially useful after a recurrent network with 'return_sequence=True'.\n+       Tensor input dimensions:   (nb_sample, time_dimension, input_dim)\n+       Tensor output dimensions:  (nb_sample, time_dimension, output_dim)\n \n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None,\n@@ -550,6 +616,11 @@ class TimeDistributedDense(MaskedLayer):\n         if weights is not None:\n             self.set_weights(weights)\n \n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        return (input_shape[0], input_shape[1], self.output_dim)\n+\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         output = self.activation(T.dot(X.dimshuffle(1, 0, 2), self.W) + self.b)\n@@ -569,10 +640,14 @@ class TimeDistributedDense(MaskedLayer):\n \n \n class AutoEncoder(Layer):\n-    '''\n-        A customizable autoencoder model.\n-        If output_reconstruction then dim(input) = dim(output)\n-        else dim(output) = dim(hidden)\n+    '''A customizable autoencoder model.\n+\n+    Tensor input dimensions: same as encoder input\n+    Tensor output dimensions:\n+        if output_reconstruction:\n+            same as encoder output\n+        else:\n+            same as decoder output\n     '''\n     def __init__(self, encoder, decoder, output_reconstruction=True, weights=None):\n \n@@ -624,6 +699,17 @@ class AutoEncoder(Layer):\n     def _get_hidden(self, train=False):\n         return self.encoder.get_output(train)\n \n+    @property\n+    def input_shape(self):\n+        self.encoder.previous.output_shape\n+\n+    @property\n+    def output_shape(self):\n+        if self.output_reconstruction:\n+            return self.encoder.previous.output_shape\n+        else:\n+            return self.decoder.previous.output_shape\n+\n     def get_output(self, train=False):\n         if not train and not self.output_reconstruction:\n             return self.encoder.get_output(train)\n@@ -682,6 +768,10 @@ class MaxoutDense(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n+    @property\n+    def output_shape(self):\n+        return (self.input_shape[0], self.output_dim)\n+\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         # -- don't need activation since it's just linear.\n\n@@ -57,6 +57,10 @@ class Embedding(Layer):\n         else:\n             return T.ones_like(X) * (1 - T.eq(X, 0))\n \n+    @property\n+    def output_shape(self):\n+        return (self.input_shape[0], None, self.output_dim)\n+\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         out = self.W[X]\n@@ -116,6 +120,10 @@ class WordContextProduct(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n+    @property\n+    def output_shape(self):\n+        return (self.input_shape[0], 1)\n+\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         w = self.W_w[X[:, 0]]  # nb_samples, proj_dim\n\n@@ -19,18 +19,18 @@ class BatchNormalization(Layer):\n     def __init__(self, input_shape, epsilon=1e-6, mode=0, momentum=0.9, weights=None):\n         super(BatchNormalization, self).__init__()\n         self.init = initializations.get(\"uniform\")\n-        self.input_shape = input_shape\n+        self._input_shape = input_shape\n         self.epsilon = epsilon\n         self.mode = mode\n         self.momentum = momentum\n-        self.input = ndim_tensor(len(self.input_shape) + 1)\n+        self.input = ndim_tensor(len(input_shape) + 1)\n \n-        self.gamma = self.init((self.input_shape))\n-        self.beta = shared_zeros(self.input_shape)\n+        self.gamma = self.init((input_shape))\n+        self.beta = shared_zeros(input_shape)\n \n         self.params = [self.gamma, self.beta]\n-        self.running_mean = shared_zeros(self.input_shape)\n-        self.running_std = shared_ones((self.input_shape))\n+        self.running_mean = shared_zeros(input_shape)\n+        self.running_std = shared_ones((input_shape))\n         if weights is not None:\n             self.set_weights(weights)\n \n@@ -66,7 +66,7 @@ class BatchNormalization(Layer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_shape\": self.input_shape,\n+                \"input_shape\": self._input_shape,\n                 \"epsilon\": self.epsilon,\n                 \"mode\": self.mode}\n \n\n@@ -34,6 +34,14 @@ class Recurrent(MaskedLayer):\n             mask = T.concatenate([padding, mask], axis=0)\n         return mask.astype('int8')\n \n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        if self.return_sequences:\n+            return (input_shape[0], input_shape[1], self.output_dim)\n+        else:\n+            return (input_shape[0], self.output_dim)\n+\n \n class SimpleRNN(Recurrent):\n     '''\n\n@@ -23,6 +23,8 @@ class TestConvolutions(unittest.TestCase):\n         for weight in [None, weights_in]:\n             for border_mode in ['valid', 'full', 'same']:\n                 for subsample_length in [1, 3]:\n+                    if border_mode == 'same' and subsample_length != 1:\n+                        continue\n                     for W_regularizer in [None, 'l2']:\n                         for b_regularizer in [None, 'l2']:\n                             for act_regularizer in [None, 'l2']:\n@@ -49,7 +51,7 @@ class TestConvolutions(unittest.TestCase):\n \n         input = np.ones((nb_samples, nb_steps, input_dim))\n         for ignore_border in [True, False]:\n-            for stride in [None, 2]:\n+            for stride in [1, 2]:\n                 layer = convolutional.MaxPooling1D(stride=stride, ignore_border=ignore_border)\n                 layer.input = theano.shared(value=input)\n                 for train in [True, False]:\n@@ -77,6 +79,8 @@ class TestConvolutions(unittest.TestCase):\n         for weight in [None, weights_in]:\n             for border_mode in ['valid', 'full', 'same']:\n                 for subsample in [(1, 1), (2, 3)]:\n+                    if border_mode == 'same' and subsample != (1, 1):\n+                        continue\n                     for W_regularizer in [None, 'l2']:\n                         for b_regularizer in [None, 'l2']:\n                             for act_regularizer in [None, 'l2']:\n@@ -104,7 +108,7 @@ class TestConvolutions(unittest.TestCase):\n \n         input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n         for ignore_border in [True, False]:\n-            for stride in [None, (2, 2)]:\n+            for stride in [(1, 1), (2, 2)]:\n                 layer = convolutional.MaxPooling2D(stride=stride, ignore_border=ignore_border, poolsize=poolsize)\n                 layer.input = theano.shared(value=input)\n                 for train in [True, False]:\n\n@@ -12,11 +12,6 @@ class TestLayerBase(unittest.TestCase):\n         input_dim = 5\n         layer = core.Layer()\n \n-        # As long as there is no input, an error should be raised.\n-        for train in [True, False]:\n-            self.assertRaises(AttributeError, layer.get_input, train)\n-            self.assertRaises(AttributeError, layer.get_output, train)\n-\n         # Once an input is provided, it should be reachable through the\n         # appropriate getters\n         input = np.ones((nb_samples, input_dim))\n@@ -34,10 +29,6 @@ class TestLayerBase(unittest.TestCase):\n         input = np.ones((nb_samples, input_dim))\n         layer1.input = theano.shared(value=input)\n \n-        # As long as there is no previous layer, an error should be raised.\n-        for train in [True, False]:\n-            self.assertRaises(AttributeError, layer2.get_input, train)\n-\n         # After connecting, input of layer1 should be passed through\n         layer2.set_previous(layer1)\n         for train in [True, False]:\n\n@@ -0,0 +1,132 @@\n+import unittest\n+import numpy as np\n+import theano\n+from keras.utils.theano_utils import ndim_tensor\n+from keras.layers.core import *\n+from keras.layers.convolutional import *\n+from keras.layers.recurrent import SimpleRNN\n+\n+\n+def check_layer_output_shape(layer, input_data):\n+    ndim = len(input_data.shape)\n+    layer.input = ndim_tensor(ndim)\n+    layer._input_shape = input_data.shape\n+    expected_output_shape = layer.output_shape\n+\n+    function = theano.function([layer.input], [layer.get_output()])\n+    output = function(input_data)[0]\n+    print output.shape, '(exact) vs.', expected_output_shape, '(predicted)'\n+\n+    assert output.shape == expected_output_shape\n+\n+\n+class TestShapeInference(unittest.TestCase):\n+    # ########\n+    # # Core #\n+    # ########\n+    def test_Reshape(self):\n+        layer = Reshape(2, 3)\n+        input_data = np.random.random((2, 6))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_Permute(self):\n+        layer = Permute(dims=(1, 3, 2))\n+        input_data = np.random.random((2, 2, 4, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_Flatten(self):\n+        layer = Flatten()\n+        input_data = np.random.random((2, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_RepeatVector(self):\n+        layer = RepeatVector(2)\n+        input_data = np.random.random((2, 2))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_Dense(self):\n+        layer = Dense(2, 3)\n+        input_data = np.random.random((2, 2))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_TimeDistributedDense(self):\n+        layer = TimeDistributedDense(3, 2)\n+        input_data = np.random.random((2, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+    #################\n+    # Convolutional #\n+    #################\n+    def test_Convolution1D(self):\n+        for border_mode in ['same', 'full', 'valid']:\n+            for filter_length in [2, 3]:\n+                for subsample_length in [1, 2]:\n+                    if subsample_length > 1 and border_mode == 'same':\n+                        continue\n+                    for input_data_shape in [(2, 3, 2), (2, 4, 2)]:\n+                        layer = Convolution1D(input_dim=2, nb_filter=1, filter_length=filter_length,\n+                                              border_mode=border_mode, subsample_length=subsample_length)\n+                        input_data = np.random.random(input_data_shape)\n+                        check_layer_output_shape(layer, input_data)\n+\n+    def test_Convolution2D(self):\n+        for border_mode in ['same', 'full', 'valid']:\n+            for nb_row, nb_col in [(2, 1), (3, 2)]:\n+                for subsample in [(1, 1), (2, 2)]:\n+                    if (subsample[0] > 1 or subsample[1] > 1) and border_mode == 'same':\n+                        continue\n+                    for input_data_shape in [(2, 1, 3, 3), (2, 1, 4, 4)]:\n+                        print 'border_mode:', border_mode\n+                        print 'subsample:', subsample\n+                        print 'input_data_shape:', input_data_shape\n+                        layer = Convolution2D(nb_filter=1, stack_size=1, nb_row=nb_row, nb_col=nb_row,\n+                                              border_mode=border_mode, subsample=subsample)\n+                        input_data = np.random.random(input_data_shape)\n+                        check_layer_output_shape(layer, input_data)\n+\n+    def test_MaxPooling1D(self):\n+        for ignore_border in [True, False]:\n+            for stride in [1, 2]:\n+                for pool_length in [1, 2]:\n+                    for input_data_shape in [(2, 1, 3), (2, 1, 4)]:\n+                        layer = MaxPooling1D(pool_length=pool_length, stride=stride, ignore_border=ignore_border)\n+                        input_data = np.random.random(input_data_shape)\n+                        check_layer_output_shape(layer, input_data)\n+\n+    def test_MaxPooling2D(self):\n+        for ignore_border in [True, False]:\n+            for stride in [(1, 1), (2, 2)]:\n+                for poolsize in [(2, 2), (3, 3), (4, 4)]:\n+                    for input_data_shape in [(2, 1, 3, 3), (2, 1, 4, 4), (2, 1, 5, 5), (2, 1, 6, 6)]:\n+                        layer = MaxPooling2D(poolsize=poolsize, stride=stride, ignore_border=ignore_border)\n+                        input_data = np.random.random(input_data_shape)\n+                        check_layer_output_shape(layer, input_data)\n+\n+    def test_UpSample1D(self):\n+        layer = UpSample1D(length=2)\n+        input_data = np.random.random((2, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_UpSample2D(self):\n+        layer = UpSample2D(size=(2, 2))\n+        input_data = np.random.random((2, 1, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+    def test_ZeroPadding2D(self):\n+        layer = ZeroPadding2D((1, 2))\n+        input_data = np.random.random((2, 1, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+    # #############\n+    # # Recurrent #\n+    # #############\n+    def test_SimpleRNN(self):\n+        # all recurrent layers inherit output_shape\n+        # from the same base recurrent layer\n+        layer = SimpleRNN(3, 2)\n+        input_data = np.random.random((2, 2, 3))\n+        check_layer_output_shape(layer, input_data)\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f1eb97000d9ad3be1d227078dadd6ce9852193a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 135 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,9 +76,6 @@ class TestShapeInference(unittest.TestCase):\n                     if (subsample[0] > 1 or subsample[1] > 1) and border_mode == 'same':\n                         continue\n                     for input_data_shape in [(2, 1, 3, 3), (2, 1, 4, 4)]:\n-                        print 'border_mode:', border_mode\n-                        print 'subsample:', subsample\n-                        print 'input_data_shape:', input_data_shape\n                         layer = Convolution2D(nb_filter=1, stack_size=1, nb_row=nb_row, nb_col=nb_row,\n                                               border_mode=border_mode, subsample=subsample)\n                         input_data = np.random.random(input_data_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#19c736a4caa9f35b65730393703ab5b8daafac68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 137 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,8 +15,6 @@ def check_layer_output_shape(layer, input_data):\n \n     function = theano.function([layer.input], [layer.get_output()])\n     output = function(input_data)[0]\n-    print output.shape, '(exact) vs.', expected_output_shape, '(predicted)'\n-\n     assert output.shape == expected_output_shape\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd2c6dbafd3a58bfda03bc11704a2b2cb807f6a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 841 | Contributors (this commit): 11 | Commits (past 90d): 26 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -253,7 +253,7 @@ class MaxPooling1D(Layer):\n         self.st = (self.stride, 1)\n \n         self.input = T.tensor3()\n-        self.poolsize = (pool_length, 1)\n+        self.pool_size = (pool_length, 1)\n         self.ignore_border = ignore_border\n \n     @property\n@@ -265,7 +265,7 @@ class MaxPooling1D(Layer):\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         X = T.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 2, 1, 3)\n-        output = downsample.max_pool_2d(X, ds=self.poolsize, st=self.st, ignore_border=self.ignore_border)\n+        output = downsample.max_pool_2d(X, ds=self.pool_size, st=self.st, ignore_border=self.ignore_border)\n         output = output.dimshuffle(0, 2, 1, 3)\n         return T.reshape(output, (output.shape[0], output.shape[1], output.shape[2]))\n \n@@ -277,28 +277,28 @@ class MaxPooling1D(Layer):\n \n \n class MaxPooling2D(Layer):\n-    def __init__(self, poolsize=(2, 2), stride=(1, 1), ignore_border=True):\n+    def __init__(self, pool_size=(2, 2), stride=(1, 1), ignore_border=True):\n         super(MaxPooling2D, self).__init__()\n         self.input = T.tensor4()\n-        self.poolsize = tuple(poolsize)\n+        self.pool_size = tuple(pool_size)\n         self.stride = tuple(stride)\n         self.ignore_border = ignore_border\n \n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n-        rows = pool_output_length(input_shape[2], self.poolsize[0], self.ignore_border, self.stride[0])\n-        cols = pool_output_length(input_shape[3], self.poolsize[1], self.ignore_border, self.stride[1])\n+        rows = pool_output_length(input_shape[2], self.pool_size[0], self.ignore_border, self.stride[0])\n+        cols = pool_output_length(input_shape[3], self.pool_size[1], self.ignore_border, self.stride[1])\n         return (input_shape[0], input_shape[1], rows, cols)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        output = downsample.max_pool_2d(X, ds=self.poolsize, st=self.stride, ignore_border=self.ignore_border)\n+        output = downsample.max_pool_2d(X, ds=self.pool_size, st=self.stride, ignore_border=self.ignore_border)\n         return output\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"poolsize\": self.poolsize,\n+                \"pool_size\": self.pool_size,\n                 \"ignore_border\": self.ignore_border,\n                 \"stride\": self.stride}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7219bb4b964249a515b05b0ee872019bbe9b64ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 1942 | Contributors (this commit): 23 | Commits (past 90d): 29 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -380,20 +380,18 @@ class Reshape(Layer):\n         Can't be used as first layer in a model (no fixed input!)\n         First dimension is assumed to be nb_samples.\n     '''\n-    def __init__(self, *dims):\n+    def __init__(self, dims):\n         super(Reshape, self).__init__()\n-        if type(dims[0]) in [list, tuple]:\n-            dims = dims[0]\n         self.dims = tuple(dims)\n \n     @property\n     def output_shape(self):\n-        return make_tuple(self.input_shape[0], *self.dims)\n+        return (self.input_shape[0],) + self.dims\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        nshape = make_tuple(X.shape[0], *self.dims)\n-        return theano.tensor.reshape(X, nshape)\n+        new_shape = (X.shape[0],) + self.dims\n+        return theano.tensor.reshape(X, new_shape)\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9be4480eabc05955fe808cc7be2a1a3cb537e9e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 79 | Churn Cumulative: 920 | Contributors (this commit): 11 | Commits (past 90d): 27 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -346,26 +346,87 @@ class UpSample2D(Layer):\n                 \"size\": self.size}\n \n \n-class ZeroPadding2D(Layer):\n-    def __init__(self, pad=(1, 1)):\n+class ZeroPadding1D(Layer):\n+    \"\"\"Zero-padding layer for 1D input (e.g. temporal sequence).\n+\n+    Input shape\n+    -----------\n+    3D tensor with shape (samples, axis_to_pad, features)\n+\n+    Output shape\n+    ------------\n+    3D tensor with shape (samples, padded_axis, features)\n+\n+    Arguments\n+    ---------\n+    padding: int\n+        How many zeros to add at the beginning and end of\n+        the padding dimension (axis 1).\n+    \"\"\"\n+    def __init__(self, padding=1):\n         super(ZeroPadding2D, self).__init__()\n-        self.pad = tuple(pad)\n+        self.padding = padding\n         self.input = T.tensor4()\n \n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n-        return (input_shape[0], input_shape[1], input_shape[2] + 2 * self.pad[0], input_shape[3] + 2 * self.pad[1])\n+        return (input_shape[0], input_shape[1] + self.padding * 2, input_shape[2])\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        pad = self.pad\n-        in_shape = X.shape\n-        out_shape = (in_shape[0], in_shape[1], in_shape[2] + 2 * pad[0], in_shape[3] + 2 * pad[1])\n+        output = T.zeros(self.output_shape)\n+        return T.set_subtensor(output[:, self.padding:X.shape[1]+self.padding, :], X)\n+\n+    def get_config(self):\n+        return {\"name\": self.__class__.__name__,\n+                \"padding\": self.padding}\n+\n+\n+class ZeroPadding2D(Layer):\n+    \"\"\"Zero-padding layer for 1D input (e.g. temporal sequence).\n+\n+    Input shape\n+    -----------\n+    4D tensor with shape (samples, depth, first_axis_to_pad, second_axis_to_pad)\n+\n+    Output shape\n+    ------------\n+    4D tensor with shape (samples, depth, first_padded_axis, second_padded_axis)\n+\n+    Arguments\n+    ---------\n+    padding: tuple of int (length 2)\n+        How many zeros to add at the beginning and end of\n+        the 2 padding dimensions (axis 3 and 4).\n+    \"\"\"\n+    def __init__(self, padding=(1, 1)):\n+        super(ZeroPadding2D, self).__init__()\n+        self.padding = tuple(padding)\n+        self.input = T.tensor4()\n+\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        return (input_shape[0],\n+                input_shape[1],\n+                input_shape[2] + 2 * self.padding[0],\n+                input_shape[3] + 2 * self.padding[1])\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+        input_shape = X.shape\n+        out_shape = (input_shape[0],\n+                     input_shape[1],\n+                     input_shape[2] + 2 * self.padding[0],\n+                     input_shape[3] + 2 * self.padding[1])\n         out = T.zeros(out_shape)\n-        indices = (slice(None), slice(None), slice(pad[0], in_shape[2] + pad[0]), slice(pad[1], in_shape[3] + pad[1]))\n+        indices = (slice(None),\n+                   slice(None),\n+                   slice(pad[0], input_shape[2] + self.padding[0]),\n+                   slice(pad[1], input_shape[3] + self.padding[1]))\n         return T.set_subtensor(out[indices], X)\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"pad\": self.pad}\n+                \"padding\": self.padding}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
