{"custom_id": "scrapy#95dd2df7b5dc6836a784a2b373009b17ca2eb475", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 267 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,6 @@ from OpenSSL import SSL\n from twisted.internet.ssl import ClientContextFactory\n \n from scrapy import twisted_version\n-from scrapy.utils.misc import create_instance\n \n if twisted_version >= (14, 0, 0):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6453800cd612297aa635636477c68a109a4a542", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 270 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -35,10 +35,7 @@ if twisted_version >= (14, 0, 0):\n \n         @classmethod\n         def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):\n-            if settings:\n             tls_verbose_logging = settings.getbool('DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING')\n-            else:\n-                tls_verbose_logging = False\n             return cls(method=method, tls_verbose_logging=tls_verbose_logging, *args, **kwargs)\n \n         def getCertificateOptions(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e622af4e5b0f49c88101ad370941b33e4833e1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 237 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,13 @@\n import os\n import sys\n import numbers\n-import configparser\n from operator import itemgetter\n \n import six\n+if six.PY2:\n+    from ConfigParser import SafeConfigParser as ConfigParser\n+else:\n+    from configparser import ConfigParser\n \n from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n@@ -94,7 +97,7 @@ def init_env(project='default', set_syspath=True):\n def get_config(use_closest=True):\n     \"\"\"Get Scrapy config file as a ConfigParser\"\"\"\n     sources = get_sources(use_closest)\n-    cfg = configparser.ConfigParser()\n+    cfg = ConfigParser()\n     cfg.read(sources)\n     return cfg\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7551689c75a1f2b4dbed72184f1dabab2f6c3c4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 364 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -189,6 +189,19 @@ class S3FilesStore(object):\n             'X-Amz-Grant-Read': 'GrantRead',\n             'X-Amz-Grant-Read-ACP': 'GrantReadACP',\n             'X-Amz-Grant-Write-ACP': 'GrantWriteACP',\n+            'X-Amz-Object-Lock-Legal-Hold': 'ObjectLockLegalHoldStatus',\n+            'X-Amz-Object-Lock-Mode': 'ObjectLockMode',\n+            'X-Amz-Object-Lock-Retain-Until-Date': 'ObjectLockRetainUntilDate',\n+            'X-Amz-Request-Payer': 'RequestPayer',\n+            'X-Amz-Server-Side-Encryption': 'ServerSideEncryption',\n+            'X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id': 'SSEKMSKeyId',\n+            'X-Amz-Server-Side-Encryption-Context': 'SSEKMSEncryptionContext',\n+            'X-Amz-Server-Side-Encryption-Customer-Algorithm': 'SSECustomerAlgorithm',\n+            'X-Amz-Server-Side-Encryption-Customer-Key': 'SSECustomerKey',\n+            'X-Amz-Server-Side-Encryption-Customer-Key-Md5': 'SSECustomerKeyMD5',\n+            'X-Amz-Storage-Class': 'StorageClass',\n+            'X-Amz-Tagging': 'Tagging',\n+            'X-Amz-Website-Redirect-Location': 'WebsiteRedirectLocation',\n         })\n         extra = {}\n         for key, value in six.iteritems(headers):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f21dc24a266a9e45088fc71daee97cbe9b11d4a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 60 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,12 +23,12 @@ def get_temp_key_info(ssl_object):\n \n     # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n     temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n-    pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p)\n-    if temp_key_p == pyOpenSSLutil.ffi.NULL:\n+    if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n         return None\n-\n     temp_key = temp_key_p[0]\n-    pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)\n+    if temp_key == pyOpenSSLutil.ffi.NULL:\n+        return None\n+    temp_key = pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)\n     key_info = []\n     key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)\n     if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:\n@@ -38,7 +38,7 @@ def get_temp_key_info(ssl_object):\n     elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:\n         key_info.append('ECDH')\n         ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)\n-        pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n+        ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n         nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key))\n         cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)\n         if cname == pyOpenSSLutil.ffi.NULL:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a25e09ecdd6bb53779b713dead19637128259ee2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 467 | Contributors (this commit): 18 | Commits (past 90d): 2 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -69,7 +69,8 @@ setup(\n         'Twisted>=13.1.0,<=19.2.0;python_version==\"3.4\"',\n         'w3lib>=1.17.0',\n         'queuelib',\n-        'lxml',\n+        'lxml;python_version!=\"3.4\"',\n+        'lxml<=4.3.5;python_version==\"3.4\"',\n         'pyOpenSSL',\n         'cssselect>=0.9',\n         'six>=1.5.2',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#783d61d32aba53208b2b0bb9a1d82a56dbcbabd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 39 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -5,11 +5,6 @@ from six.moves import copyreg\n if six.PY2:\n     from urlparse import urlparse\n     \n-    # workaround for https://bugs.python.org/issue7904 - Python < 2.7\n-    if urlparse('s3://bucket/key').netloc != 'bucket':\n-        from urlparse import uses_netloc\n-        uses_netloc.append('s3')\n-\n     # workaround for https://bugs.python.org/issue9374 - Python < 2.7.4\n     if urlparse('s3://bucket/key?key=value').query != 'key=value':\n         from urlparse import uses_query\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e813953bd4b25b60d30981a948e48fee43f8dad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 295 | Lines Deleted: 26 | Files Changed: 5 | Hunks: 21 | Methods Changed: 26 | Complexity Δ (Sum/Max): 36/19 | Churn Δ: 321 | Churn Cumulative: 1326 | Contributors (this commit): 51 | Commits (past 90d): 10 | Contributors (cumulative): 67 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,8 +5,8 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n \"\"\"\n \n import logging\n-\n-from six.moves.urllib import robotparser\n+import sys\n+import re\n \n from twisted.internet.defer import Deferred, maybeDeferred\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n@@ -14,6 +14,7 @@ from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.python import to_native_str\n+from scrapy.utils.misc import load_object\n \n logger = logging.getLogger(__name__)\n \n@@ -24,10 +25,13 @@ class RobotsTxtMiddleware(object):\n     def __init__(self, crawler):\n         if not crawler.settings.getbool('ROBOTSTXT_OBEY'):\n             raise NotConfigured\n-\n+        self._default_useragent = crawler.settings.get('USER_AGENT', 'Scrapy')\n         self.crawler = crawler\n-        self._useragent = crawler.settings.get('USER_AGENT')\n         self._parsers = {}\n+        self._parserimpl = load_object(crawler.settings.get('ROBOTSTXT_PARSER'))\n+\n+        # check if parser dependencies are met, this should throw an error otherwise.\n+        self._parserimpl.from_crawler(self.crawler, b'')\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -43,7 +47,8 @@ class RobotsTxtMiddleware(object):\n     def process_request_2(self, rp, request, spider):\n         if rp is None:\n             return\n-        if not rp.can_fetch(to_native_str(self._useragent), request.url):\n+        useragent = request.headers.get(b'User-Agent', self._default_useragent)\n+        if not rp.allowed(request.url, useragent):\n             logger.debug(\"Forbidden by robots.txt: %(request)s\",\n                          {'request': request}, extra={'spider': spider})\n             self.crawler.stats.inc_value('robotstxt/forbidden')\n@@ -62,13 +67,14 @@ class RobotsTxtMiddleware(object):\n                 meta={'dont_obey_robotstxt': True}\n             )\n             dfd = self.crawler.engine.download(robotsreq, spider)\n-            dfd.addCallback(self._parse_robots, netloc)\n+            dfd.addCallback(self._parse_robots, netloc, spider)\n             dfd.addErrback(self._logerror, robotsreq, spider)\n             dfd.addErrback(self._robots_error, netloc)\n             self.crawler.stats.inc_value('robotstxt/request_count')\n \n         if isinstance(self._parsers[netloc], Deferred):\n             d = Deferred()\n+\n             def cb(result):\n                 d.callback(result)\n                 return result\n@@ -85,27 +91,10 @@ class RobotsTxtMiddleware(object):\n                          extra={'spider': spider})\n         return failure\n \n-    def _parse_robots(self, response, netloc):\n+    def _parse_robots(self, response, netloc, spider):\n         self.crawler.stats.inc_value('robotstxt/response_count')\n-        self.crawler.stats.inc_value(\n-            'robotstxt/response_status_count/{}'.format(response.status))\n-        rp = robotparser.RobotFileParser(response.url)\n-        body = ''\n-        if hasattr(response, 'text'):\n-            body = response.text\n-        else:  # last effort try\n-            try:\n-                body = response.body.decode('utf-8')\n-            except UnicodeDecodeError:\n-                # If we found garbage, disregard it:,\n-                # but keep the lookup cached (in self._parsers)\n-                # Running rp.parse() will set rp state from\n-                # 'disallow all' to 'allow any'.\n-                self.crawler.stats.inc_value('robotstxt/unicode_error_count')\n-        # stdlib's robotparser expects native 'str' ;\n-        # with unicode input, non-ASCII encoded bytes decoding fails in Python2\n-        rp.parse(to_native_str(body).splitlines())\n-\n+        self.crawler.stats.inc_value('robotstxt/response_status_count/{}'.format(response.status))\n+        rp = self._parserimpl.from_crawler(self.crawler, response.body)\n         rp_dfd = self._parsers[netloc]\n         self._parsers[netloc] = rp\n         rp_dfd.callback(rp)\n\n@@ -0,0 +1,112 @@\n+import sys\n+import logging\n+from abc import ABCMeta, abstractmethod\n+from six import with_metaclass\n+\n+from scrapy.utils.python import to_native_str, to_unicode\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class RobotParser(with_metaclass(ABCMeta)):\n+    @classmethod\n+    @abstractmethod\n+    def from_crawler(cls, crawler, robotstxt_body):\n+        \"\"\"Parse the content of a robots.txt_ file as bytes. This must be a class method.\n+        It must return a new instance of the parser backend.\n+\n+        :param crawler: crawler which made the request\n+        :type crawler: :class:`~scrapy.crawler.Crawler` instance\n+\n+        :param robotstxt_body: content of a robots.txt_ file.\n+        :type robotstxt_body: bytes\n+        \"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def allowed(self, url, user_agent):\n+        \"\"\"Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.\n+\n+        :param url: Absolute URL\n+        :type url: string\n+\n+        :param user_agent: User agent\n+        :type user_agent: string\n+        \"\"\"\n+        pass\n+\n+\n+class PythonRobotParser(RobotParser):\n+    def __init__(self, robotstxt_body, spider):\n+        from six.moves.urllib_robotparser import RobotFileParser\n+        self.spider = spider\n+        try:\n+            robotstxt_body = to_native_str(robotstxt_body)\n+        except UnicodeDecodeError:\n+            # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n+            # Switch to 'allow all' state.\n+            logger.warning(\"Failure while parsing robots.txt using %(parser)s.\"\n+                           \" File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.\",\n+                           {'parser': \"RobotFileParser\"},\n+                           exc_info=sys.exc_info(),\n+                           extra={'spider': self.spider})\n+            robotstxt_body = ''\n+        self.rp = RobotFileParser()\n+        self.rp.parse(robotstxt_body.splitlines())\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, robotstxt_body):\n+        spider = None if not crawler else crawler.spider\n+        o = cls(robotstxt_body, spider)\n+        return o\n+\n+    def allowed(self, url, user_agent):\n+        user_agent = to_native_str(user_agent)\n+        url = to_native_str(url)\n+        return self.rp.can_fetch(user_agent, url)\n+\n+\n+class ReppyRobotParser(RobotParser):\n+    def __init__(self, robotstxt_body, spider):\n+        from reppy.robots import Robots\n+        self.spider = spider\n+        self.rp = Robots.parse('', robotstxt_body)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, robotstxt_body):\n+        spider = None if not crawler else crawler.spider\n+        o = cls(robotstxt_body, spider)\n+        return o\n+\n+    def allowed(self, url, user_agent):\n+        return self.rp.allowed(url, user_agent)\n+\n+\n+class RerpRobotParser(RobotParser):\n+    def __init__(self, robotstxt_body, spider):\n+        from robotexclusionrulesparser import RobotExclusionRulesParser\n+        self.spider = spider\n+        self.rp = RobotExclusionRulesParser()\n+        try:\n+            robotstxt_body = robotstxt_body.decode('utf-8')\n+        except UnicodeDecodeError:\n+            # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n+            # Switch to 'allow all' state.\n+            logger.warning(\"Failure while parsing robots.txt using %(parser)s.\"\n+                           \" File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.\",\n+                           {'parser': \"RobotExclusionRulesParser\"},\n+                           exc_info=sys.exc_info(),\n+                           extra={'spider': self.spider})\n+            robotstxt_body = ''\n+        self.rp.parse(robotstxt_body)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, robotstxt_body):\n+        spider = None if not crawler else crawler.spider\n+        o = cls(robotstxt_body, spider)\n+        return o\n+\n+    def allowed(self, url, user_agent):\n+        user_agent = to_unicode(user_agent)\n+        url = to_unicode(url)\n+        return self.rp.is_allowed(user_agent, url)\n\n@@ -245,6 +245,7 @@ RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\n RETRY_PRIORITY_ADJUST = -1\n \n ROBOTSTXT_OBEY = False\n+ROBOTSTXT_PARSER = 'scrapy.robotstxt.PythonRobotParser'\n \n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'\n\n@@ -10,6 +10,7 @@ from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response, TextResponse\n from scrapy.settings import Settings\n from tests import mock\n+from tests.test_robotstxt_interface import rerp_available, reppy_available\n \n \n class RobotsTxtMiddlewareTest(unittest.TestCase):\n@@ -41,6 +42,7 @@ User-Agent: UnicödeBöt\n Disallow: /some/randome/page.html\n \"\"\".encode('utf-8')\n         response = TextResponse('http://site.local/robots.txt', body=ROBOTS)\n+\n         def return_response(request, spider):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.callback, response)\n@@ -77,6 +79,7 @@ Disallow: /some/randome/page.html\n         crawler = self.crawler\n         crawler.settings.set('ROBOTSTXT_OBEY', True)\n         response = Response('http://site.local/robots.txt', body=b'GIF89a\\xd3\\x00\\xfe\\x00\\xa2')\n+\n         def return_response(request, spider):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.callback, response)\n@@ -99,6 +102,7 @@ Disallow: /some/randome/page.html\n         crawler = self.crawler\n         crawler.settings.set('ROBOTSTXT_OBEY', True)\n         response = Response('http://site.local/robots.txt')\n+\n         def return_response(request, spider):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.callback, response)\n@@ -118,6 +122,7 @@ Disallow: /some/randome/page.html\n     def test_robotstxt_error(self):\n         self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n         err = error.DNSLookupError('Robotstxt address not found')\n+\n         def return_failure(request, spider):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.errback, failure.Failure(err))\n@@ -133,6 +138,7 @@ Disallow: /some/randome/page.html\n     def test_robotstxt_immediate_error(self):\n         self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n         err = error.DNSLookupError('Robotstxt address not found')\n+\n         def immediate_failure(request, spider):\n             deferred = Deferred()\n             deferred.errback(failure.Failure(err))\n@@ -144,6 +150,7 @@ Disallow: /some/randome/page.html\n \n     def test_ignore_robotstxt_request(self):\n         self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n+\n         def ignore_request(request, spider):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.errback, failure.Failure(IgnoreRequest()))\n@@ -167,3 +174,21 @@ Disallow: /some/randome/page.html\n         spider = None  # not actually used\n         return self.assertFailure(maybeDeferred(middleware.process_request, request, spider),\n                                   IgnoreRequest)\n+\n+\n+class RobotsTxtMiddlewareWithRerpTest(RobotsTxtMiddlewareTest):\n+    if not rerp_available():\n+        skip = \"Rerp parser is not installed\"\n+\n+    def setUp(self):\n+        super(RobotsTxtMiddlewareWithRerpTest, self).setUp()\n+        self.crawler.settings.set('ROBOTSTXT_PARSER', 'scrapy.robotstxt.RerpRobotParser')\n+\n+\n+class RobotsTxtMiddlewareWithReppyTest(RobotsTxtMiddlewareTest):\n+    if not reppy_available():\n+        skip = \"Reppy parser is not installed\"\n+\n+    def setUp(self):\n+        super(RobotsTxtMiddlewareWithReppyTest, self).setUp()\n+        self.crawler.settings.set('ROBOTSTXT_PARSER', 'scrapy.robotstxt.ReppyRobotParser')\n\n@@ -0,0 +1,142 @@\n+# coding=utf-8\n+from twisted.trial import unittest\n+from scrapy.utils.python import to_native_str\n+\n+\n+def reppy_available():\n+    # check if reppy parser is installed\n+    try:\n+        from reppy.robots import Robots\n+    except ImportError:\n+        return False\n+    return True\n+\n+\n+def rerp_available():\n+    # check if robotexclusionrulesparser is installed\n+    try:\n+        from robotexclusionrulesparser import RobotExclusionRulesParser\n+    except ImportError:\n+        return False\n+    return True\n+\n+\n+class BaseRobotParserTest:\n+    def _setUp(self, parser_cls):\n+        self.parser_cls = parser_cls\n+\n+    def test_allowed(self):\n+        robotstxt_robotstxt_body = (\"User-agent: * \\n\"\n+                                    \"Disallow: /disallowed \\n\"\n+                                    \"Allow: /allowed \\n\"\n+                                    \"Crawl-delay: 10\".encode('utf-8'))\n+        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+        self.assertTrue(rp.allowed(\"https://www.site.local/allowed\", \"*\"))\n+        self.assertFalse(rp.allowed(\"https://www.site.local/disallowed\", \"*\"))\n+\n+    def test_allowed_wildcards(self):\n+        robotstxt_robotstxt_body = \"\"\"User-agent: first\n+                                Disallow: /disallowed/*/end$    \n+\n+                                User-agent: second\n+                                Allow: /*allowed\n+                                Disallow: /\n+                                \"\"\".encode('utf-8')\n+        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+\n+        self.assertTrue(rp.allowed(\"https://www.site.local/disallowed\", \"first\"))\n+        self.assertFalse(rp.allowed(\"https://www.site.local/disallowed/xyz/end\", \"first\"))\n+        self.assertFalse(rp.allowed(\"https://www.site.local/disallowed/abc/end\", \"first\"))\n+        self.assertTrue(rp.allowed(\"https://www.site.local/disallowed/xyz/endinglater\", \"first\"))\n+\n+        self.assertTrue(rp.allowed(\"https://www.site.local/allowed\", \"second\"))\n+        self.assertTrue(rp.allowed(\"https://www.site.local/is_still_allowed\", \"second\"))\n+        self.assertTrue(rp.allowed(\"https://www.site.local/is_allowed_too\", \"second\"))\n+\n+    def test_length_based_precedence(self):\n+        robotstxt_robotstxt_body = (\"User-agent: * \\n\"\n+                                    \"Disallow: / \\n\"\n+                                    \"Allow: /page\".encode('utf-8'))\n+        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+        self.assertTrue(rp.allowed(\"https://www.site.local/page\", \"*\"))\n+\n+    def test_order_based_precedence(self):\n+        robotstxt_robotstxt_body = (\"User-agent: * \\n\"\n+                                    \"Disallow: / \\n\"\n+                                    \"Allow: /page\".encode('utf-8'))\n+        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+        self.assertFalse(rp.allowed(\"https://www.site.local/page\", \"*\"))\n+\n+    def test_empty_response(self):\n+        \"\"\"empty response should equal 'allow all'\"\"\"\n+        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=b'')\n+        self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n+        self.assertTrue(rp.allowed(\"https://site.local/\", \"chrome\"))\n+        self.assertTrue(rp.allowed(\"https://site.local/index.html\", \"*\"))\n+        self.assertTrue(rp.allowed(\"https://site.local/disallowed\", \"*\"))\n+\n+    def test_garbage_response(self):\n+        \"\"\"garbage response should be discarded, equal 'allow all'\"\"\"\n+        robotstxt_robotstxt_body = b'GIF89a\\xd3\\x00\\xfe\\x00\\xa2'\n+        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+        self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n+        self.assertTrue(rp.allowed(\"https://site.local/\", \"chrome\"))\n+        self.assertTrue(rp.allowed(\"https://site.local/index.html\", \"*\"))\n+        self.assertTrue(rp.allowed(\"https://site.local/disallowed\", \"*\"))\n+\n+    def test_unicode_url_and_useragent(self):\n+        robotstxt_robotstxt_body = u\"\"\"\n+        User-Agent: *\n+        Disallow: /admin/\n+        Disallow: /static/\n+        # taken from https://en.wikipedia.org/robots.txt\n+        Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\n+        Disallow: /wiki/Käyttäjä:\n+\n+        User-Agent: UnicödeBöt\n+        Disallow: /some/randome/page.html\"\"\".encode('utf-8')\n+        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+        self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n+        self.assertFalse(rp.allowed(\"https://site.local/admin/\", \"*\"))\n+        self.assertFalse(rp.allowed(\"https://site.local/static/\", \"*\"))\n+        self.assertTrue(rp.allowed(\"https://site.local/admin/\", u\"UnicödeBöt\"))\n+        self.assertFalse(rp.allowed(\"https://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\", \"*\"))\n+        self.assertFalse(rp.allowed(u\"https://site.local/wiki/Käyttäjä:\", \"*\"))\n+        self.assertTrue(rp.allowed(\"https://site.local/some/randome/page.html\", \"*\"))\n+        self.assertFalse(rp.allowed(\"https://site.local/some/randome/page.html\", u\"UnicödeBöt\"))\n+\n+\n+class PythonRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n+    def setUp(self):\n+        from scrapy.robotstxt import PythonRobotParser\n+        super(PythonRobotParserTest, self)._setUp(PythonRobotParser)\n+\n+    def test_length_based_precedence(self):\n+        raise unittest.SkipTest(\"RobotFileParser does not support length based directives precedence.\")\n+\n+    def test_allowed_wildcards(self):\n+        raise unittest.SkipTest(\"RobotFileParser does not support wildcards.\")\n+\n+\n+class ReppyRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n+    if not reppy_available():\n+        skip = \"Reppy parser is not installed\"\n+\n+    def setUp(self):\n+        from scrapy.robotstxt import ReppyRobotParser\n+        super(ReppyRobotParserTest, self)._setUp(ReppyRobotParser)\n+\n+    def test_order_based_precedence(self):\n+        raise unittest.SkipTest(\"Rerp does not support order based directives precedence.\")\n+\n+\n+class RerpRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n+    if not rerp_available():\n+        skip = \"Rerp parser is not installed\"\n+\n+    def setUp(self):\n+        from scrapy.robotstxt import RerpRobotParser\n+        super(RerpRobotParserTest, self)._setUp(RerpRobotParser)\n+\n+    def test_length_based_precedence(self):\n+        raise unittest.SkipTest(\"Rerp does not support length based directives precedence.\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a4cd94244a01a7029e9f03959805a2e45a07874", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 333 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -95,7 +95,7 @@ class ScrapyHTTPPageGetter(HTTPClient):\n \n class ScrapyHTTPClientFactory(HTTPClientFactory):\n     \"\"\"Scrapy implementation of the HTTPClientFactory overwriting the\n-    serUrl method to make use of our Url object that cache the parse\n+    setUrl method to make use of our Url object that cache the parse\n     result.\n     \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#18d0affc015cd1be02cc9b227c4d89007dd8c816", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 59 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -3,10 +3,10 @@ from twisted.internet import reactor, error\n def listen_tcp(portrange, host, factory):\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n     assert len(portrange) <= 2, \"invalid portrange: %s\" % portrange\n-    if not hasattr(portrange, '__iter__'):\n-        return reactor.listenTCP(portrange, factory, interface=host)\n     if not portrange:\n         return reactor.listenTCP(0, factory, interface=host)\n+    if not hasattr(portrange, '__iter__'):\n+        return reactor.listenTCP(portrange, factory, interface=host)\n     if len(portrange) == 1:\n         return reactor.listenTCP(portrange[0], factory, interface=host)\n     for x in range(portrange[0], portrange[1]+1):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8621bbc2929b904dfe14985dd1f94aeedc86648", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 165 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -84,6 +84,6 @@ class ScrapesContract(Contract):\n     def post_process(self, output):\n         for x in output:\n             if isinstance(x, (BaseItem, dict)):\n-                for arg in self.args:\n-                    if not arg in x:\n-                        raise ContractFail(\"'%s' field is missing\" % arg)\n+                missing = [arg for arg in self.args if arg not in x]\n+                if missing:\n+                    raise ContractFail(\"'%s' field is missing\" % \" \".join(missing))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bff335cf7f90a0dc3aeafc199aaa81697d5f122b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 168 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -86,4 +86,5 @@ class ScrapesContract(Contract):\n             if isinstance(x, (BaseItem, dict)):\n                 missing = [arg for arg in self.args if arg not in x]\n                 if missing:\n-                    raise ContractFail(\"'%s' field is missing\" % \" \".join(missing))\n+                    raise ContractFail(\n+                        \"Missing fields: %s\" % \", \".join(missing))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b755a41a1046e031015130971c9b9cc8885176e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 1398 | Contributors (this commit): 19 | Commits (past 90d): 4 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,7 +11,7 @@ import posixpath\n from tempfile import NamedTemporaryFile\n from datetime import datetime\n import six\n-from six.moves.urllib.parse import urlparse\n+from six.moves.urllib.parse import urlparse, unquote\n from ftplib import FTP\n \n from zope.interface import Interface, implementer\n@@ -162,7 +162,7 @@ class FTPFeedStorage(BlockingFeedStorage):\n         self.host = u.hostname\n         self.port = int(u.port or '21')\n         self.username = u.username\n-        self.password = u.password\n+        self.password = unquote(u.password)\n         self.path = u.path\n         self.use_active_mode = use_active_mode\n \n\n@@ -6,7 +6,8 @@ import warnings\n from io import BytesIO\n import tempfile\n import shutil\n-from six.moves.urllib.parse import urljoin, urlparse\n+import string\n+from six.moves.urllib.parse import urljoin, urlparse, quote\n from six.moves.urllib.request import pathname2url\n \n from zope.interface.verify import verifyObject\n@@ -98,6 +99,12 @@ class FTPFeedStorageTest(unittest.TestCase):\n         verifyObject(IFeedStorage, st)\n         return self._assert_stores(st, path)\n \n+    def test_uri_auth_quote(self):\n+        # RFC3986: 3.2.1. User Information\n+        pw_quoted = quote(string.punctuation, safe='')\n+        st = FTPFeedStorage('ftp://foo:%s@example.com/some_path' % pw_quoted)\n+        self.assertEqual(st.password, string.punctuation)\n+\n     @defer.inlineCallbacks\n     def _assert_stores(self, storage, path):\n         spider = self.get_test_spider()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5dbeece8da6686709eb00af9ffb91dd30a832a2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 493 | Contributors (this commit): 19 | Commits (past 90d): 3 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -53,7 +53,6 @@ setup(\n         'Programming Language :: Python :: 2',\n         'Programming Language :: Python :: 2.7',\n         'Programming Language :: Python :: 3',\n-        'Programming Language :: Python :: 3.4',\n         'Programming Language :: Python :: 3.5',\n         'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n@@ -63,20 +62,21 @@ setup(\n         'Topic :: Software Development :: Libraries :: Application Frameworks',\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n-    python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*',\n+    python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*',\n     install_requires=[\n-        'Twisted>=13.1.0;python_version!=\"3.4\"',\n-        'Twisted>=13.1.0,<=19.2.0;python_version==\"3.4\"',\n-        'w3lib>=1.17.0',\n-        'queuelib',\n-        'lxml;python_version!=\"3.4\"',\n-        'lxml<=4.3.5;python_version==\"3.4\"',\n-        'pyOpenSSL',\n-        'cssselect>=0.9',\n-        'six>=1.5.2',\n-        'parsel>=1.5',\n+        'Twisted>=16.0.0;python_version==\"2.7\"',\n+        'Twisted>=17.9.0;python_version>=\"3.5\"',\n+        'cryptography>=2.0',\n+        'cssselect>=0.9.1',\n+        'lxml>=3.5.0',\n+        'parsel>=1.5.0',\n         'PyDispatcher>=2.0.5',\n-        'service_identity',\n+        'pyOpenSSL>=16.2.0',\n+        'queuelib>=1.4.2',\n+        'service_identity>=16.0.0',\n+        'six>=1.10.0',\n+        'w3lib>=1.17.0',\n+        'zope.interface>=4.1.3',\n     ],\n     extras_require=extras_require,\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#595c995ee640e360e5b574577594b273cc8af071", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 26 | Churn Cumulative: 76 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,10 @@\n import platform\n import sys\n \n+import cryptography\n import cssselect\n import lxml.etree\n+import OpenSSL\n import parsel\n import twisted\n import w3lib\n@@ -13,15 +15,6 @@ import scrapy\n def scrapy_components_versions():\n     lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n     libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n-    try:\n-        w3lib_version = w3lib.__version__\n-    except AttributeError:\n-        w3lib_version = \"<1.14.3\"\n-    try:\n-        import cryptography\n-        cryptography_version = cryptography.__version__\n-    except ImportError:\n-        cryptography_version = \"unknown\"\n \n     return [\n         (\"Scrapy\", scrapy.__version__),\n@@ -29,22 +22,17 @@ def scrapy_components_versions():\n         (\"libxml2\", libxml2_version),\n         (\"cssselect\", cssselect.__version__),\n         (\"parsel\", parsel.__version__),\n-        (\"w3lib\", w3lib_version),\n+        (\"w3lib\", w3lib.__version__),\n         (\"Twisted\", twisted.version.short()),\n         (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n         (\"pyOpenSSL\", _get_openssl_version()),\n-        (\"cryptography\", cryptography_version),\n+        (\"cryptography\", cryptography.__version__),\n         (\"Platform\",  platform.platform()),\n     ]\n \n \n def _get_openssl_version():\n-    try:\n-        import OpenSSL\n-        openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\\\n-            .decode('ascii', errors='replace')\n-    # pyOpenSSL 0.12 does not expose openssl version\n-    except AttributeError:\n-        openssl = 'Unknown OpenSSL version'\n-\n+    openssl = OpenSSL.SSL.SSLeay_version(\n+        OpenSSL.SSL.SSLEAY_VERSION\n+    ).decode('ascii', errors='replace')\n     return '{} ({})'.format(OpenSSL.version.__version__, openssl)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d76b6944c9081684e4961c15dc3426a4d4c67a3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 414 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 5 | Methods Changed: 18 | Complexity Δ (Sum/Max): 31/14 | Churn Δ: 414 | Churn Cumulative: 1826 | Contributors (this commit): 21 | Commits (past 90d): 7 | Contributors (cumulative): 29 | DMM Complexity: 0.875\n\nDIFF:\n@@ -12,6 +12,7 @@ from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import escape_ajax\n from scrapy.http.common import obsolete_setter\n+from scrapy.utils.curl import curl_to_request_kwargs\n \n \n class Request(object_ref):\n@@ -103,3 +104,34 @@ class Request(object_ref):\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n+\n+    @classmethod\n+    def from_curl(cls, curl_command, ignore_unknown_options=True, **kwargs):\n+        \"\"\"Create a Request object from a string containing a `cURL\n+        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n+        URL, the headers, the cookies and the body. It accepts the same\n+        arguments as the :class:`Request` class, taking preference and\n+        overriding the values of the same arguments contained in the cURL\n+        command.\n+\n+        Unrecognized options are ignored by default. To raise an error when\n+        finding unknown options call this method by passing\n+        ``ignore_unknown_options=False``.\n+\n+        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n+                     subclasses, such as :class:`~scrapy.http.JSONRequest`, or\n+                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n+                     :ref:`downloader middlewares <topics-downloader-middleware>`\n+                     and\n+                     :ref:`spider middlewares <topics-spider-middleware>`\n+                     enabled, such as\n+                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n+                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n+                     or\n+                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n+                     may modify the :class:`~scrapy.http.Request` object.\n+\n+       \"\"\"\n+        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n+        request_kwargs.update(kwargs)\n+        return cls(**request_kwargs)\n\n@@ -0,0 +1,95 @@\n+import argparse\n+import warnings\n+from shlex import split\n+\n+from six.moves.http_cookies import SimpleCookie\n+from six.moves.urllib.parse import urlparse\n+from six import string_types, iteritems\n+from w3lib.http import basic_auth_header\n+\n+\n+class CurlParser(argparse.ArgumentParser):\n+    def error(self, message):\n+        error_msg = \\\n+            'There was an error parsing the curl command: {}'.format(message)\n+        raise ValueError(error_msg)\n+\n+\n+curl_parser = CurlParser()\n+curl_parser.add_argument('url')\n+curl_parser.add_argument('-H', '--header', dest='headers', action='append')\n+curl_parser.add_argument('-X', '--request', dest='method', default='get')\n+curl_parser.add_argument('-d', '--data', dest='data')\n+curl_parser.add_argument('-u', '--user', dest='auth')\n+\n+\n+safe_to_ignore_arguments = [\n+    ['--compressed'],\n+    # `--compressed` argument is not safe to ignore, but it's included here\n+    # because the `HttpCompressionMiddleware` is enabled by default\n+    ['-s', '--silent'],\n+    ['-v', '--verbose'],\n+    ['-#', '--progress-bar']\n+]\n+\n+for argument in safe_to_ignore_arguments:\n+    curl_parser.add_argument(*argument, action='store_true')\n+\n+\n+def curl_to_request_kwargs(curl_command, ignore_unknown_options=True):\n+    \"\"\"Convert a cURL command syntax to Request kwargs.\n+\n+    :param str curl_command: string containing the curl command\n+    :param bool ignore_unknown_options: If true, only a warning is emitted when\n+    cURL options are unknown. Otherwise raises an error. (default: True)\n+    :return: dictionary of Request kwargs\n+    \"\"\"\n+\n+    curl_args = split(curl_command)\n+\n+    if curl_args[0] != 'curl':\n+        raise ValueError('A curl command must start with \"curl\"')\n+\n+    parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])\n+\n+    if argv:\n+        msg = 'Unrecognized options: {}'.format(', '.join(argv))\n+        if ignore_unknown_options:\n+            warnings.warn(msg)\n+        else:\n+            raise ValueError(msg)\n+\n+    url = parsed_args.url\n+\n+    # curl automatically prepends 'http' if the scheme is missing, but Request\n+    # needs the scheme to work\n+    parsed_url = urlparse(url)\n+    if not parsed_url.scheme:\n+        url = 'http://' + url\n+\n+    result = {'method': parsed_args.method.upper(), 'url': url}\n+\n+    headers = []\n+    cookies = {}\n+    for header in parsed_args.headers or ():\n+        name, val = header.split(':', 1)\n+        name = name.strip()\n+        val = val.strip()\n+        if name.title() == 'Cookie':\n+            for name, morsel in iteritems(SimpleCookie(val)):\n+                cookies[name] = morsel.value\n+        else:\n+            headers.append((name, val))\n+\n+    if parsed_args.auth:\n+        user, password = parsed_args.auth.split(':', 1)\n+        headers.append(('Authorization', basic_auth_header(user, password)))\n+\n+    if headers:\n+        result['headers'] = headers\n+    if cookies:\n+        result['cookies'] = cookies\n+    if parsed_args.data:\n+        result['body'] = parsed_args.data\n+\n+    return result\n\n@@ -269,6 +269,82 @@ class RequestTest(unittest.TestCase):\n         with self.assertRaises(TypeError):\n             self.request_class('http://example.com', a_function, errback='a_function')\n \n+    def test_from_curl(self):\n+        # Note: more curated tests regarding curl conversion are in\n+        # `test_utils_curl.py`\n+        curl_command = (\n+            \"curl 'http://httpbin.org/post' -X POST -H 'Cookie: _gauges_unique\"\n+            \"_year=1; _gauges_unique=1; _gauges_unique_month=1; _gauges_unique\"\n+            \"_hour=1; _gauges_unique_day=1' -H 'Origin: http://httpbin.org' -H\"\n+            \" 'Accept-Encoding: gzip, deflate' -H 'Accept-Language: en-US,en;q\"\n+            \"=0.9,ru;q=0.8,es;q=0.7' -H 'Upgrade-Insecure-Requests: 1' -H 'Use\"\n+            \"r-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTM\"\n+            \"L, like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.3202.75 S\"\n+            \"afari/537.36' -H 'Content-Type: application /x-www-form-urlencode\"\n+            \"d' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=\"\n+            \"0.9,image/webp,image/apng,*/*;q=0.8' -H 'Cache-Control: max-age=0\"\n+            \"' -H 'Referer: http://httpbin.org/forms/post' -H 'Connection: kee\"\n+            \"p-alive' --data 'custname=John+Smith&custtel=500&custemail=jsmith\"\n+            \"%40example.org&size=small&topping=cheese&topping=onion&delivery=1\"\n+            \"2%3A15&comments=' --compressed\"\n+        )\n+        r = self.request_class.from_curl(curl_command)\n+        self.assertEqual(r.method, \"POST\")\n+        self.assertEqual(r.url, \"http://httpbin.org/post\")\n+        self.assertEqual(r.body,\n+                         b\"custname=John+Smith&custtel=500&custemail=jsmith%40\"\n+                         b\"example.org&size=small&topping=cheese&topping=onion\"\n+                         b\"&delivery=12%3A15&comments=\")\n+        self.assertEqual(r.cookies, {\n+            '_gauges_unique_year': '1',\n+            '_gauges_unique': '1',\n+            '_gauges_unique_month': '1',\n+            '_gauges_unique_hour': '1',\n+            '_gauges_unique_day': '1'\n+        })\n+        self.assertEqual(r.headers, {\n+            b'Origin': [b'http://httpbin.org'],\n+            b'Accept-Encoding': [b'gzip, deflate'],\n+            b'Accept-Language': [b'en-US,en;q=0.9,ru;q=0.8,es;q=0.7'],\n+            b'Upgrade-Insecure-Requests': [b'1'],\n+            b'User-Agent': [b'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.'\n+                            b'36 (KHTML, like Gecko) Ubuntu Chromium/62.0.3202'\n+                            b'.75 Chrome/62.0.3202.75 Safari/537.36'],\n+            b'Content-Type': [b'application /x-www-form-urlencoded'],\n+            b'Accept': [b'text/html,application/xhtml+xml,application/xml;q=0.'\n+                        b'9,image/webp,image/apng,*/*;q=0.8'],\n+            b'Cache-Control': [b'max-age=0'],\n+            b'Referer': [b'http://httpbin.org/forms/post'],\n+            b'Connection': [b'keep-alive']})\n+\n+    def test_from_curl_with_kwargs(self):\n+        r = self.request_class.from_curl(\n+            'curl -X PATCH \"http://example.org\"',\n+            method=\"POST\",\n+            meta={'key': 'value'}\n+        )\n+        self.assertEqual(r.method, \"POST\")\n+        self.assertEqual(r.meta, {\"key\": \"value\"})\n+\n+    def test_from_curl_ignore_unknown_options(self):\n+        # By default: it works and ignores the unknown options: --foo and -z\n+        with warnings.catch_warnings():  # avoid warning when executing tests\n+            warnings.simplefilter('ignore')\n+            r = self.request_class.from_curl(\n+                'curl -X DELETE \"http://example.org\" --foo -z',\n+            )\n+            self.assertEqual(r.method, \"DELETE\")\n+\n+        # If `ignore_unknon_options` is set to `False` it raises an error with\n+        # the unknown options: --foo and -z\n+        self.assertRaises(\n+            ValueError,\n+            lambda: self.request_class.from_curl(\n+                'curl -X PATCH \"http://example.org\" --foo -z',\n+                ignore_unknown_options=False,\n+            ),\n+        )\n+\n \n class FormRequestTest(RequestTest):\n \n\n@@ -0,0 +1,211 @@\n+import unittest\n+import warnings\n+\n+from six import assertRaisesRegex\n+from w3lib.http import basic_auth_header\n+\n+from scrapy import Request\n+from scrapy.utils.curl import curl_to_request_kwargs\n+\n+\n+class CurlToRequestKwargsTest(unittest.TestCase):\n+    maxDiff = 5000\n+\n+    def _test_command(self, curl_command, expected_result):\n+        result = curl_to_request_kwargs(curl_command)\n+        self.assertEqual(result, expected_result)\n+        try:\n+            Request(**result)\n+        except TypeError as e:\n+            self.fail(\"Request kwargs are not correct {}\".format(e))\n+\n+    def test_get(self):\n+        curl_command = \"curl http://example.org/\"\n+        expected_result = {\"method\": \"GET\", \"url\": \"http://example.org/\"}\n+        self._test_command(curl_command, expected_result)\n+\n+    def test_get_without_scheme(self):\n+        curl_command = \"curl www.example.org\"\n+        expected_result = {\"method\": \"GET\", \"url\": \"http://www.example.org\"}\n+        self._test_command(curl_command, expected_result)\n+\n+    def test_get_basic_auth(self):\n+        curl_command = 'curl \"https://api.test.com/\" -u ' \\\n+                       '\"some_username:some_password\"'\n+        expected_result = {\n+            \"method\": \"GET\",\n+            \"url\": \"https://api.test.com/\",\n+            \"headers\": [\n+                (\n+                    \"Authorization\",\n+                    basic_auth_header(\"some_username\", \"some_password\")\n+                )\n+            ],\n+        }\n+        self._test_command(curl_command, expected_result)\n+\n+    def test_get_complex(self):\n+        curl_command = (\n+            \"curl 'http://httpbin.org/get' -H 'Accept-Encoding: gzip, deflate'\"\n+            \" -H 'Accept-Language: en-US,en;q=0.9,ru;q=0.8,es;q=0.7' -H 'Upgra\"\n+            \"de-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0 (X11; Linux \"\n+            \"x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/62\"\n+            \".0.3202.75 Chrome/62.0.3202.75 Safari/537.36' -H 'Accept: text/ht\"\n+            \"ml,application/xhtml+xml,application/xml;q=0.9,image/webp,image/a\"\n+            \"png,*/*;q=0.8' -H 'Referer: http://httpbin.org/' -H 'Cookie: _gau\"\n+            \"ges_unique_year=1; _gauges_unique=1; _gauges_unique_month=1; _gau\"\n+            \"ges_unique_hour=1; _gauges_unique_day=1' -H 'Connection: keep-ali\"\n+            \"ve' --compressed\"\n+        )\n+        expected_result = {\n+            \"method\": \"GET\",\n+            \"url\": \"http://httpbin.org/get\",\n+            \"headers\": [\n+                (\"Accept-Encoding\", \"gzip, deflate\"),\n+                (\"Accept-Language\", \"en-US,en;q=0.9,ru;q=0.8,es;q=0.7\"),\n+                (\"Upgrade-Insecure-Requests\", \"1\"),\n+                (\n+                    \"User-Agent\",\n+                    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML\"\n+                    \", like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.32\"\n+                    \"02.75 Safari/537.36\",\n+                ),\n+                (\n+                    \"Accept\",\n+                    \"text/html,application/xhtml+xml,application/xml;q=0.9,ima\"\n+                    \"ge/webp,image/apng,*/*;q=0.8\",\n+                ),\n+                (\"Referer\", \"http://httpbin.org/\"),\n+                (\"Connection\", \"keep-alive\"),\n+            ],\n+            \"cookies\": {\n+                '_gauges_unique_year': '1',\n+                '_gauges_unique_hour': '1',\n+                '_gauges_unique_day': '1',\n+                '_gauges_unique': '1',\n+                '_gauges_unique_month': '1'\n+            },\n+        }\n+        self._test_command(curl_command, expected_result)\n+\n+    def test_post(self):\n+        curl_command = (\n+            \"curl 'http://httpbin.org/post' -X POST -H 'Cookie: _gauges_unique\"\n+            \"_year=1; _gauges_unique=1; _gauges_unique_month=1; _gauges_unique\"\n+            \"_hour=1; _gauges_unique_day=1' -H 'Origin: http://httpbin.org' -H\"\n+            \" 'Accept-Encoding: gzip, deflate' -H 'Accept-Language: en-US,en;q\"\n+            \"=0.9,ru;q=0.8,es;q=0.7' -H 'Upgrade-Insecure-Requests: 1' -H 'Use\"\n+            \"r-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTM\"\n+            \"L, like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.3202.75 S\"\n+            \"afari/537.36' -H 'Content-Type: application/x-www-form-urlencoded\"\n+            \"' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0\"\n+            \".9,image/webp,image/apng,*/*;q=0.8' -H 'Cache-Control: max-age=0'\"\n+            \" -H 'Referer: http://httpbin.org/forms/post' -H 'Connection: keep\"\n+            \"-alive' --data 'custname=John+Smith&custtel=500&custemail=jsmith%\"\n+            \"40example.org&size=small&topping=cheese&topping=onion&delivery=12\"\n+            \"%3A15&comments=' --compressed\"\n+        )\n+        expected_result = {\n+            \"method\": \"POST\",\n+            \"url\": \"http://httpbin.org/post\",\n+            \"body\": \"custname=John+Smith&custtel=500&custemail=jsmith%40exampl\"\n+                    \"e.org&size=small&topping=cheese&topping=onion&delivery=12\"\n+                    \"%3A15&comments=\",\n+            \"cookies\": {\n+                '_gauges_unique_year': '1',\n+                '_gauges_unique_hour': '1',\n+                '_gauges_unique_day': '1',\n+                '_gauges_unique': '1',\n+                '_gauges_unique_month': '1'\n+            },\n+            \"headers\": [\n+                (\"Origin\", \"http://httpbin.org\"),\n+                (\"Accept-Encoding\", \"gzip, deflate\"),\n+                (\"Accept-Language\", \"en-US,en;q=0.9,ru;q=0.8,es;q=0.7\"),\n+                (\"Upgrade-Insecure-Requests\", \"1\"),\n+                (\n+                    \"User-Agent\",\n+                    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML\"\n+                    \", like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.32\"\n+                    \"02.75 Safari/537.36\",\n+                ),\n+                (\"Content-Type\", \"application/x-www-form-urlencoded\"),\n+                (\n+                    \"Accept\",\n+                    \"text/html,application/xhtml+xml,application/xml;q=0.9,ima\"\n+                    \"ge/webp,image/apng,*/*;q=0.8\",\n+                ),\n+                (\"Cache-Control\", \"max-age=0\"),\n+                (\"Referer\", \"http://httpbin.org/forms/post\"),\n+                (\"Connection\", \"keep-alive\"),\n+            ],\n+        }\n+        self._test_command(curl_command, expected_result)\n+\n+    def test_patch(self):\n+        curl_command = (\n+            'curl \"https://example.com/api/fake\" -u \"username:password\" -H \"Ac'\n+            'cept: application/vnd.go.cd.v4+json\" -H \"Content-Type: applicatio'\n+            'n/json\" -X PATCH -d \\'{\"hostname\": \"agent02.example.com\",  \"agent'\n+            '_config_state\": \"Enabled\", \"resources\": [\"Java\",\"Linux\"], \"enviro'\n+            'nments\": [\"Dev\"]}\\''\n+        )\n+        expected_result = {\n+            \"method\": \"PATCH\",\n+            \"url\": \"https://example.com/api/fake\",\n+            \"headers\": [\n+                (\"Accept\", \"application/vnd.go.cd.v4+json\"),\n+                (\"Content-Type\", \"application/json\"),\n+                (\"Authorization\", basic_auth_header(\"username\", \"password\")),\n+            ],\n+            \"body\": '{\"hostname\": \"agent02.example.com\",  \"agent_config_state\"'\n+                    ': \"Enabled\", \"resources\": [\"Java\",\"Linux\"], \"environments'\n+                    '\": [\"Dev\"]}',\n+        }\n+        self._test_command(curl_command, expected_result)\n+\n+    def test_delete(self):\n+        curl_command = 'curl -X \"DELETE\" https://www.url.com/page'\n+        expected_result = {\n+            \"method\": \"DELETE\", \"url\": \"https://www.url.com/page\"\n+        }\n+        self._test_command(curl_command, expected_result)\n+\n+    def test_get_silent(self):\n+        curl_command = 'curl --silent \"www.example.com\"'\n+        expected_result = {\"method\": \"GET\", \"url\": \"http://www.example.com\"}\n+        self.assertEqual(curl_to_request_kwargs(curl_command), expected_result)\n+\n+    def test_too_few_arguments_error(self):\n+        assertRaisesRegex(\n+            self,\n+            ValueError,\n+            r\"too few arguments|the following arguments are required:\\s*url\",\n+            lambda: curl_to_request_kwargs(\"curl\"),\n+        )\n+\n+    def test_ignore_unknown_options(self):\n+        # case 1: ignore_unknown_options=True:\n+        with warnings.catch_warnings():  # avoid warning when executing tests\n+            warnings.simplefilter('ignore')\n+            curl_command = 'curl --bar --baz http://www.example.com'\n+            expected_result = \\\n+                {\"method\": \"GET\", \"url\": \"http://www.example.com\"}\n+            self.assertEqual(curl_to_request_kwargs(curl_command), expected_result)\n+\n+        # case 2: ignore_unknown_options=False (raise exception):\n+        assertRaisesRegex(\n+            self,\n+            ValueError,\n+            \"Unrecognized options:.*--bar.*--baz\",\n+            lambda: curl_to_request_kwargs(\n+                \"curl --bar --baz http://www.example.com\",\n+                ignore_unknown_options=False\n+            ),\n+        )\n+\n+    def test_must_start_with_curl_error(self):\n+        self.assertRaises(\n+            ValueError,\n+            lambda: curl_to_request_kwargs(\"carl -X POST http://example.org\")\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
