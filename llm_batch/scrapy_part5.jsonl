{"custom_id": "scrapy#5e2db86cde2160c2aecf2945e8868a75af74932c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 241 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -111,9 +111,9 @@ class Cache(object):\n                     os.makedirs(linkpath)\n \n     def is_cached(self, domain, key):\n-        pickled_meta = os.path.join(self.requestpath(domain, key), 'pickled_meta')\n-        if os.path.exists(pickled_meta):\n-            with open(pickled_meta, 'rb') as f:\n+        requestpath = self.requestpath(domain, key)\n+        if os.path.exists(requestpath):\n+            with open(os.path.join(requestpath, 'pickled_meta'), 'r') as f:\n                 metadata = pickle.load(f)\n             if datetime.datetime.utcnow() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS')):\n                 return True\n@@ -133,7 +133,7 @@ class Cache(object):\n \n         requestpath = self.requestpath(domain, key)\n         metadata = responsebody = responseheaders = None\n-        with open(os.path.join(requestpath, 'pickled_meta'), 'rb') as f:\n+        with open(os.path.join(requestpath, 'pickled_meta'), 'r') as f:\n             metadata = pickle.load(f)\n         with open(os.path.join(requestpath, 'response_body')) as f:\n             responsebody = f.read()\n@@ -169,8 +169,8 @@ class Cache(object):\n         with open(os.path.join(requestpath, 'meta_data'), 'w') as f:\n             f.write(repr(metadata))\n         # pickled metadata (to recover without using eval)\n-        with open(os.path.join(requestpath, 'pickled_meta'), 'wb') as f:\n-            pickle.dump(metadata, f, -1)\n+        with open(os.path.join(requestpath, 'pickled_meta'), 'w') as f:\n+            pickle.dump(metadata, f)\n         # response\n         with open(os.path.join(requestpath, 'response_headers'), 'w') as f:\n             f.write(headers_dict_to_raw(response.headers))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a27b085c04ba89e4c21f6cc52c7ee7fb5866ab5f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 618 | Contributors (this commit): 2 | Commits (past 90d): 20 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -162,7 +162,7 @@ class ClusterMaster(pb.Root):\n             self.pending = []\n         self.loading = []\n         self.nodes = {}\n-        self.statistics = {\"domains\": {}, \"scraped_total\": 0, \"start_time\": datetime.datetime.utcnow(), \"pending\": self.print_pending() }\n+        self.statistics = {\"domains\": {}, \"scraped_total\": 0, \"start_time\": datetime.datetime.utcnow()}\n         self.global_settings = {}\n         #load cluster global settings\n         for sname in settings.getlist('GLOBAL_CLUSTER_SETTINGS'):\n@@ -210,6 +210,7 @@ class ClusterMaster(pb.Root):\n                 self.nodes[name].update_status()\n                 self.nodes[name].update_statistics()\n                 self.statistics[\"scraped_total\"] += self.nodes[name].statistics[\"scraped_total\"]\n+                self.statistics[\"timestamp\"] = datetime.datetime.utcnow()\n                 for domain in self.nodes[name].statistics[\"domains\"]:\n                     if not domain in self.statistics[\"domains\"] or self.nodes[name].statistics[\"domains\"][domain][\"last_start_time\"] >= self.statistics[\"domains\"][domain][\"last_start_time\"]:\n                         self.statistics[\"domains\"][domain] = self.nodes[name].statistics[\"domains\"][domain]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a7301cc9a262e0ba05a85566b292f6a01851589e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 182 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -86,6 +86,9 @@ class XPathSelector(object):\n class XPathSelectorList(list):\n     \"\"\"List of XPathSelector objects\"\"\"\n \n+    def __getslice__(self, i, j):\n+        return XPathSelectorList(list.__getslice__(self, i, j))\n+\n     def x(self, xpath):\n         \"\"\"Perform the given XPath query on each XPathSelector of the list and\n         return a new (flattened) XPathSelectorList of the results\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8a0fdaa89bfa0877fd79f6277880169adebbc177", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 57 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ def start(logfile=None, loglevel=None, log_stdout=None):\n     loglevel = loglevel or settings['LOGLEVEL']\n     log_stdout = log_stdout or settings.getbool('LOG_STDOUT')\n \n-    file = open(logfile, 'w') if logfile else sys.stderr\n+    file = open(logfile, 'a') if logfile else sys.stderr\n     level = int(getattr(sys.modules[__name__], loglevel)) if loglevel else DEBUG\n     log.startLogging(file, setStdout=log_stdout)\n     setattr(sys.modules[__name__], 'log_level', level)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e32fe10f2def8a9112a0aab66e84001b18aa17d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 335 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -105,10 +105,10 @@ class ClusterWorker(pb.Root):\n                 try:\n                     import pysvn\n                     c = pysvn.Client()\n-                    r = c.update(settings[\"CLUSTER_WORKER_SVNWORKDIR\"] or \".\")\n-                    log.msg(\"Updated to revision %s.\" %r[0].number )\n-                except:\n-                    pass\n+                    r = c.update(settings.get(\"CLUSTER_WORKER_SVNWORKDIR\", \".\"))\n+                    log.msg(\"Updated to revision %s.\" %r[0].number, level=log.DEBUG)\n+                except pysvn.ClientError, e:\n+                    log.msg(\"Unable to svn update: %s\" % e) \n                 proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n                 return self.status(0, \"Started process %s.\" % scrapy_proc)\n             return self.status(2, \"Domain %s already running.\" % domain )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a06868c21061e9dfdd00ab0124e9b8897c0aa07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 337 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -96,7 +96,7 @@ class ClusterWorker(pb.Root):\n         \"\"\"Spawn process to run the given domain.\"\"\"\n         if len(self.running) < self.maxproc:\n             if not domain in self.running:\n-                logfile = os.path.join(self.logdir, time.strftime(\"%F\"), domain, time.strftime(\"%FT%T.log\"))\n+                logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n                 if not os.path.exists(os.path.dirname(logfile)):\n                     os.makedirs(os.path.dirname(logfile))\n                 scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#980369ba6068c5d0023394b3b370e97784b4c33d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 227 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ def main():\n     parser.add_option(\"--list\", metavar=\"FILE\", dest=\"list\", help=\"Specify a file from where to read domains, one per line.\")\n     parser.add_option(\"--now\", action=\"store_true\", dest=\"now\", help=\"Schedule domains to run with priority now.\")\n     parser.add_option(\"--output\", metavar=\"FILE\", dest=\"output\", help=\"Output file. If not given, output to stdout.\")\n-    parser.add_option(\"--port\", dest=\"port\", type=\"int\", help=\"Cluster master port. Default: 8080.\", default=8080)\n+    parser.add_option(\"--port\", dest=\"port\", type=\"int\", help=\"Cluster master port. Default: 8060.\", default=8060)\n     parser.add_option(\"--remove\", dest=\"remove\", action=\"store_true\", help=\"Remove from schedule domains given as args.\")\n     parser.add_option(\"--schedule\", dest=\"schedule\", action=\"store_true\", help=\"Schedule domains given as args.\")\n     parser.add_option(\"--server\", dest=\"server\", help=\"Cluster master server name. Default: localhost.\", default=\"localhost\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b5b79042abcdf503fd5e41ec7998059c86638966", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 60 | Files Changed: 2 | Hunks: 20 | Methods Changed: 21 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 107 | Churn Cumulative: 1062 | Contributors (this commit): 2 | Commits (past 90d): 40 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -27,14 +27,20 @@ def my_import(name):\n         mod = getattr(mod, comp)\n     return mod\n \n-class Node:\n-    def __init__(self, remote, status, name, master):\n+class Node(pb.Referenceable):\n+    def __init__(self, remote, name, master):\n         self.__remote = remote\n-        self._set_status(status)\n+        self.alive = False\n         self.name = name\n         self.master = master\n         self.available = True\n-        self.statistics = {\"domains\": {}, \"scraped_total\": 0 }\n+        try:\n+            deferred = self.__remote.callRemote(\"set_master\", self)\n+        except pb.DeadReferenceError:\n+            self._set_status(None)\n+            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n+        else:\n+            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n             \n     def status_as_dict(self, verbosity=0):\n         status = {\"alive\": self.alive}\n@@ -92,19 +98,6 @@ class Node:\n         else:\n             deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n \n-    def update_statistics(self):\n-        \n-        def _set_statistics(statistics):\n-            self.statistics = statistics\n-\n-        try:\n-            deferred = self.__remote.callRemote(\"statistics\")\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n-        else:\n-            deferred.addCallbacks(callback=_set_statistics, errback=lambda reason: log.msg(reason, log.ERROR))\n-        \n     def stop(self, domain):\n         try:\n             deferred = self.__remote.callRemote(\"stop\", domain)\n@@ -142,8 +135,10 @@ class Node:\n         else:\n             deferred.addCallbacks(callback=_run_callback, errback=_run_errback)\n         \n+    def remote_update(self, status):\n+        self._set_status(status)\n \n-class ClusterMaster(pb.Root):\n+class ClusterMaster:\n \n     def __init__(self):\n \n@@ -170,18 +165,14 @@ class ClusterMaster(pb.Root):\n         \n         dispatcher.connect(self._engine_started, signal=signals.engine_started)\n         dispatcher.connect(self._engine_stopped, signal=signals.engine_stopped)\n-        port = settings.getint('CLUSTER_MASTER_PORT')\n-        scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n         \n     def load_nodes(self):\n-\n-        \"\"\"Loads nodes from the CLUSTER_MASTER_NODES setting\"\"\"\n-\n+        \"\"\"Loads nodes listed in CLUSTER_MASTER_NODES setting\"\"\"\n         for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n             self.load_node(name, url)\n             \n     def load_node(self, name, url):\n-\n+        \"\"\"Creates the remote reference for each worker node\"\"\"\n         def _make_callback(_factory, _name, _url):\n \n             def _errback(_reason):\n@@ -203,25 +194,17 @@ class ClusterMaster(pb.Root):\n             _make_callback(factory, name, url)\n \n     def update_nodes(self):\n-        self.statistics[\"scraped_total\"] = 0\n         for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n             if name in self.nodes and self.nodes[name].alive:\n                 log.msg(\"Updating node. name: %s, url: %s\" % (name, url) )\n                 self.nodes[name].update_status()\n-                self.nodes[name].update_statistics()\n-                self.statistics[\"scraped_total\"] += self.nodes[name].statistics[\"scraped_total\"]\n-                self.statistics[\"timestamp\"] = datetime.datetime.utcnow()\n-                for domain in self.nodes[name].statistics[\"domains\"]:\n-                    if not domain in self.statistics[\"domains\"] or self.nodes[name].statistics[\"domains\"][domain][\"last_start_time\"] >= self.statistics[\"domains\"][domain][\"last_start_time\"]:\n-                        self.statistics[\"domains\"][domain] = self.nodes[name].statistics[\"domains\"][domain]\n             else:\n                 log.msg(\"Reloading node. name: %s, url: %s\" % (name, url) )\n                 self.load_node(name, url)\n \n     def add_node(self, cworker, name):\n         \"\"\"Add node given its node\"\"\"\n-        node = Node(cworker, None, name, self)\n-        node.update_status()\n+        node = Node(cworker, name, self)\n         self.nodes[name] = node\n         log.msg(\"Added cluster worker %s\" % name)\n \n@@ -242,7 +225,7 @@ class ClusterMaster(pb.Root):\n             else:\n                 break\n         for domain in domains:\n-            pd = self.find_ifpending(domain)\n+            pd = self.find_inpending(domain)\n             if pd: #domain already pending, so just change priority if new is higher\n                 if priority < pd['priority']:\n                     self.pending.remove(pd)\n@@ -300,7 +283,7 @@ class ClusterMaster(pb.Root):\n     def available_nodes(self):\n         return (node for node in self.nodes.itervalues() if node.available)\n \n-    def find_ifpending(self, domain):\n+    def find_inpending(self, domain):\n         for p in self.pending:\n             if domain == p['domain']:\n                 return p\n\n@@ -35,13 +35,12 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         log.msg(\"ClusterWorker: started domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         self.transport.closeStdin()\n         self.status = \"running\"\n-        self.procman.statistics[\"domains\"][self.domain] = {\"status\": \"running\", \"last_start_time\": self.start_time}\n+        self.procman.update_master()\n \n     def processEnded(self, status_object):\n         log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         del self.procman.running[self.domain]\n-        self.procman.statistics[\"domains\"][self.domain].update({\"status\": \"scraped\", \"last_end_time\": datetime.datetime.utcnow()})\n-        self.procman.statistics[\"scraped_total\"] += 1\n+        self.procman.update_master()\n \n class ClusterWorker(pb.Root):\n \n@@ -53,11 +52,34 @@ class ClusterWorker(pb.Root):\n         self.logdir = settings['CLUSTER_LOGDIR']\n         self.running = {}\n         self.starttime = datetime.datetime.utcnow()\n-        self.statistics = {\"domains\": {}, \"scraped_total\": 0}\n         port = settings.getint('CLUSTER_WORKER_PORT')\n         scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n         log.msg(\"PYTHONPATH: %s\" % repr(sys.path))\n \n+    def status(self, rcode=0, rstring=None):\n+        status = {}\n+        status[\"running\"] = [ self.running[k].as_dict() for k in self.running.keys() ]\n+        status[\"starttime\"] = self.starttime\n+        status[\"timestamp\"] = datetime.datetime.utcnow()\n+        status[\"maxproc\"] = self.maxproc\n+        status[\"loadavg\"] = os.getloadavg()\n+        status[\"logdir\"] = self.logdir\n+        status[\"callresponse\"] = (rcode, rstring) if rstring else (0, \"Status Response.\")\n+        return status\n+\n+    def update_master(self):\n+        try:\n+            deferred = self.__master.callRemote(\"update\", self.status())\n+        except pb.DeadReferenceError:\n+            self.__master = None\n+            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n+        else:\n+            deferred.addCallbacks(callback=lambda x: x, errback=lambda reason: log.msg(reason, log.ERROR))\n+        \n+    def remote_set_master(self, master):\n+        self.__master = master\n+        return self.status()\n+\n     def remote_stop(self, domain):\n         \"\"\"Stop running domain.\"\"\"\n         if domain in self.running:\n@@ -72,26 +94,6 @@ class ClusterWorker(pb.Root):\n     def remote_status(self):\n         return self.status()\n \n-    def remote_statistics(self):\n-        #This can detect processes that were abnormally killed (for example, by the kernel\n-        #because of a memory ran out.)\n-        for domain in self.statistics[\"domains\"]:\n-            if self.statistics[\"domains\"][domain][\"status\"] == \"running\" and not domain in self.running:\n-                self.statistics[\"domains\"][domain][\"status\"] = \"lost\"\n-\n-        return self.statistics\n-    \n-    def status(self, rcode=0, rstring=None):\n-        status = {}\n-        status[\"running\"] = [ self.running[k].as_dict() for k in self.running.keys() ]\n-        status[\"starttime\"] = self.starttime\n-        status[\"timestamp\"] = datetime.datetime.utcnow()\n-        status[\"maxproc\"] = self.maxproc\n-        status[\"loadavg\"] = os.getloadavg()\n-        status[\"logdir\"] = self.logdir\n-        status[\"callresponse\"] = (rcode, rstring) if rstring else (0, \"Status Response.\")\n-        return status\n-\n     def remote_run(self, domain, spider_settings=None):\n         \"\"\"Spawn process to run the given domain.\"\"\"\n         if len(self.running) < self.maxproc:\n@@ -108,7 +110,9 @@ class ClusterWorker(pb.Root):\n                     r = c.update(settings.get(\"CLUSTER_WORKER_SVNWORKDIR\", \".\"))\n                     log.msg(\"Updated to revision %s.\" %r[0].number, level=log.DEBUG)\n                 except pysvn.ClientError, e:\n-                    log.msg(\"Unable to svn update: %s\" % e) \n+                    log.msg(\"Unable to svn update: %s\" % e, level=log.WARNING)\n+                except ImportError:\n+                    log.msg(\"pysvn module not available.\", level=log.WARNING)\n                 proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n                 return self.status(0, \"Started process %s.\" % scrapy_proc)\n             return self.status(2, \"Domain %s already running.\" % domain )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0dffb68ecf0cfd54061d8eda3cec750832c16cca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 672 | Contributors (this commit): 2 | Commits (past 90d): 22 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -236,7 +236,6 @@ class ClusterMaster:\n                 final_spider_settings.update(self.global_settings)\n                 final_spider_settings.update(spider_settings or {})\n                 self.pending.insert(i, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})\n-        self.update_nodes()\n \n     def stop(self, domains):\n         to_stop = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b8e3e75322f6e60746df4585f1071fc611a14d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 674 | Contributors (this commit): 2 | Commits (past 90d): 23 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -119,11 +119,13 @@ class Node(pb.Referenceable):\n             if status['callresponse'][0] == 1:\n                 #slots are complete. Reschedule in master with priority reduced by one.\n                 #self.master.loading check should avoid this to happen\n+                self.master.loading.remove(pending['domain'])\n                 self.master.schedule([pending['domain']], pending['settings'], pending['priority'] - 1)\n                 log.msg(\"Domain %s rescheduled: no proc space in node.\" % pending['domain'], log.WARNING)\n             elif status['callresponse'][0] == 2:\n                 #domain already running in node. Reschedule with same priority.\n                 #self.master.loading check should avoid this to happen\n+                self.master.loading.remove(pending['domain'])\n                 self.master.schedule([pending['domain']], pending['settings'], pending['priority'])\n                 log.msg(\"Domain %s rescheduled: already running in node.\" % pending['domain'], log.WARNING)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e86ee64727c1ca738d8db4526bc4329e591a15c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 285 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,7 +11,7 @@ USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n DOWNLOAD_TIMEOUT = 180      # 3mins\n CONCURRENT_DOMAINS = 8    # number of domains to scrape in parallel\n REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n-CACHE2_EXPIRATION_SECS = 48 * 60 * 60 # seconds while cached response is still valid\n+CACHE2_EXPIRATION_SECS = 48 * 60 * 60 # seconds while cached response is still valid (a negative value means \"never expires\")\n \n LOG_ENABLED = True  #\n LOGLEVEL = 'DEBUG'   # default loglevel\n\n@@ -1,6 +1,7 @@\n from __future__ import with_statement\n \n import os\n+import sys\n import hashlib\n import datetime\n import urlparse\n@@ -33,7 +34,11 @@ class CacheMiddleware(object):\n \n         key = request.fingerprint()\n         domain = spider.domain_name\n+        try:\n             response = self.cache.retrieve_response(domain, key)\n+        except:\n+            log.msg(\"Corrupt cache for %s\" % request.url, log.WARNING)\n+            response = False\n         if response:\n             response.cached = True\n             if not 200 <= int(response.status) < 300:\n@@ -115,11 +120,16 @@ class Cache(object):\n         if os.path.exists(requestpath):\n             with open(os.path.join(requestpath, 'pickled_meta'), 'r') as f:\n                 metadata = pickle.load(f)\n-            if datetime.datetime.utcnow() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS')):\n+            expiration_secs = settings.getint('CACHE2_EXPIRATION_SECS')\n+            if expiration_secs >= 0:\n+                if datetime.datetime.utcnow() <= metadata['timestamp'] + datetime.timedelta(seconds=expiration_secs):\n                     return True\n                 else:\n                     log.msg('dropping old cached response from %s' % metadata['timestamp'])\n                     return False\n+            else:\n+                # disabled cache expiration\n+                return True\n         else:\n             return False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f3f34d0816863ea78d10e9db8d94c14e9d866f3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 12 | Methods Changed: 13 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 48 | Churn Cumulative: 1113 | Contributors (this commit): 2 | Commits (past 90d): 44 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -68,9 +68,6 @@ class Node(pb.Referenceable):\n         else:\n             self.alive = True\n             self.running = status['running']\n-            for proc in self.running:\n-                if proc[\"domain\"] in self.master.loading:\n-                    self.master.loading.remove(proc[\"domain\"])\n             self.maxproc = status['maxproc']\n             self.starttime = status['starttime']\n             self.timestamp = status['timestamp']\n@@ -137,8 +134,28 @@ class Node(pb.Referenceable):\n         else:\n             deferred.addCallbacks(callback=_run_callback, errback=_run_errback)\n         \n-    def remote_update(self, status):\n+    def remote_update(self, status, domain, domain_status):\n         self._set_status(status)\n+        if domain in self.master.loading and domain_status == \"running\":\n+            self.master.loading.remove(domain)\n+            self.master.statistics[\"domains\"][\"running\"].add(domain)\n+        elif domain_status == \"scraped\":\n+            self.master.statistics[\"domains\"][\"running\"].remove(domain)\n+            self.master.statistics[\"domains\"][\"scraped\"][domain] = self.master.statistics[\"domains\"][\"scraped\"].get(domain, 0) + 1\n+            self.master.statistics[\"scraped_count\"] = self.master.statistics.get(\"scraped_count\", 0) + 1\n+            if domain in self.master.statistics[\"domains\"][\"lost\"]:\n+                self.master.statistics[\"domains\"][\"lost\"].remove(domain)\n+\n+class ScrapyPBClientFactory(pb.PBClientFactory):\n+    def __init__(self, master, nodename):\n+        pb.PBClientFactory.__init__(self)\n+        self.master = master\n+        self.nodename = nodename\n+        \n+    def clientConnectionLost(self, *args, **kargs):\n+        pb.PBClientFactory.clientConnectionLost(self, *args, **kargs)\n+        del self.master.nodes[self.nodename]\n+        log.msg(\"Removed node %s.\" % self.nodename )\n \n class ClusterMaster:\n \n@@ -159,7 +176,9 @@ class ClusterMaster:\n             self.pending = []\n         self.loading = []\n         self.nodes = {}\n-        self.statistics = {\"domains\": {}, \"scraped_total\": 0, \"start_time\": datetime.datetime.utcnow()}\n+        self.start_time = datetime.datetime.utcnow()\n+        #on how statistics works, see self.update_nodes() and Nodes.remote_update()\n+        self.statistics = {\"domains\": {\"running\": set(), \"scraped\": {}, \"lost_count\": {}, \"lost\": set()}, \"scraped_count\": 0 }\n         self.global_settings = {}\n         #load cluster global settings\n         for sname in settings.getlist('GLOBAL_CLUSTER_SETTINGS'):\n@@ -187,7 +206,7 @@ class ClusterMaster:\n         port = eval(port)\n         log.msg(\"Connecting to cluster worker %s...\" % name)\n         log.msg(\"Server: %s, Port: %s\" % (server, port))\n-        factory = pb.PBClientFactory()\n+        factory = ScrapyPBClientFactory(self, name)\n         try:\n             reactor.connectTCP(server, port, factory)\n         except Exception, err:\n@@ -204,6 +223,12 @@ class ClusterMaster:\n                 log.msg(\"Reloading node. name: %s, url: %s\" % (name, url) )\n                 self.load_node(name, url)\n         \n+        real_running = set(self.running.keys())\n+        lost = self.statistics[\"domains\"][\"running\"].difference(real_running)\n+        for domain in lost:\n+            self.statistics[\"domains\"][\"lost_count\"][domain] = self.statistics[\"domains\"][\"lost_count\"].get(domain, 0) + 1\n+        self.statistics[\"domains\"][\"lost\"] = self.statistics[\"domains\"][\"lost\"].union(lost)\n+            \n     def add_node(self, cworker, name):\n         \"\"\"Add node given its node\"\"\"\n         node = Node(cworker, name, self)\n\n@@ -35,12 +35,13 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         log.msg(\"ClusterWorker: started domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         self.transport.closeStdin()\n         self.status = \"running\"\n-        self.procman.update_master()\n+        self.procman.update_master(self.domain, \"running\")\n \n-    def processEnded(self, status_object):\n+    def processEnded(self, reason):\n         log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n+        log.msg(\"Reason type: %s. value: %s\" % (reason.type, reason.value) )\n         del self.procman.running[self.domain]\n-        self.procman.update_master()\n+        self.procman.update_master(self.domain, \"scraped\")\n \n class ClusterWorker(pb.Root):\n \n@@ -67,9 +68,9 @@ class ClusterWorker(pb.Root):\n         status[\"callresponse\"] = (rcode, rstring) if rstring else (0, \"Status Response.\")\n         return status\n \n-    def update_master(self):\n+    def update_master(self, domain, domain_status):\n         try:\n-            deferred = self.__master.callRemote(\"update\", self.status())\n+            deferred = self.__master.callRemote(\"update\", self.status(), domain, domain_status)\n         except pb.DeadReferenceError:\n             self.__master = None\n             log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9dd144f48741a27b296c597c7458d399911601e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 14 | Files Changed: 6 | Hunks: 14 | Methods Changed: 13 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 31 | Churn Cumulative: 1277 | Contributors (this commit): 3 | Commits (past 90d): 27 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,14 @@ from scrapy.utils.misc import load_class\n from scrapy.extension import extensions\n from scrapy.conf import settings\n \n+def get_url(url):\n+    from scrapy.http import Request\n+    from scrapy.core.downloader.handlers import download_any\n+    request = Request(url)\n+    spider = spiders.fromurl(url)\n+\n+    return download_any(request, spider)\n+\n def load_url(url, response):\n     vars = {}\n     itemcls = load_class(settings['DEFAULT_ITEM_CLASS'])\n@@ -17,6 +25,7 @@ def load_url(url, response):\n     vars['xxs'] = XmlXPathSelector(response)\n     vars['hxs'] = HtmlXPathSelector(response)\n     vars['spider'] = spiders.fromurl(url)\n+    vars['get'] = get_url\n     return vars\n \n def print_vars(vars):\n\n@@ -153,10 +153,9 @@ class Cache(object):\n         url = metadata['url']\n         original_url = metadata.get('original_url', url)\n         headers = Headers(responseheaders)\n-        parent = metadata.get('parent')\n         status = metadata['status']\n \n-        response = Response(domain=domain, url=url, original_url=original_url, headers=headers, status=status, body=responsebody, parent=parent)\n+        response = Response(domain=domain, url=url, original_url=original_url, headers=headers, status=status, body=responsebody)\n         response.cached = True\n         return response\n \n@@ -168,7 +167,6 @@ class Cache(object):\n         metadata = {\n                 'url':request.url,\n                 'method': request.method,\n-                'parent': response.parent,\n                 'status': response.status,\n                 'domain': response.domain,\n                 'original_url': response.original_url,\n\n@@ -64,7 +64,7 @@ class HistoryMiddleware(object):\n         key = urlkey(url)\n         if response:\n             redirect_url = response.url\n-            parentkey = urlkey(response.parent) if response.parent else None\n+            parentkey = urlkey(response.request.headers.get('referer')) if response.request else None\n             version = response.version()\n         else:\n             redirect_url, parentkey, version = url, None, None\n\n@@ -43,9 +43,8 @@ def download_http(request, spider):\n     def _response(body):\n         body = body or ''\n         status = factory.status\n-        parent = request.headers.get('Referer')\n         headers = Headers(factory.response_headers)\n-        r = Response(domain=spider.domain_name, url=request.url, headers=headers, status=status, body=body, parent=parent)\n+        r = Response(domain=spider.domain_name, url=request.url, headers=headers, status=status, body=body)\n         signals.send_catch_log(signal=signals.request_uploaded, sender='download_http', request=request, spider=spider)\n         signals.send_catch_log(signal=signals.response_downloaded, sender='download_http', response=r, spider=spider)\n         return r\n\n@@ -345,7 +345,7 @@ class ExecutionEngine(object):\n             if isinstance(response, Response):\n                 response.request = request # tie request to obtained response\n                 cached = 'cached' if response.cached else 'live'\n-                log.msg(\"Crawled %s <%s> from <%s>\" % (cached, response.url, response.parent), level=log.DEBUG, domain=domain)\n+                log.msg(\"Crawled %s <%s> from <%s>\" % (cached, response.url, request.headers.get('referer')), level=log.DEBUG, domain=domain)\n                 return response\n             elif isinstance(response, Request):\n                 redirected = response # proper alias\n\n@@ -20,11 +20,10 @@ class Response(object) :\n         * headers - HTTP headers\n         * status - HTTP status code\n         * body - Body object containing the content of the response\n-        * parent - the URL of the referring page\n     \"\"\"\n     _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n \n-    def __init__(self, domain, url, original_url=None, headers=None, status=200, body=None, parent=None):\n+    def __init__(self, domain, url, original_url=None, headers=None, status=200, body=None):\n         self.domain = domain\n         self.url = Url(url)\n         self.original_url = Url(original_url) if original_url else url # different if redirected or escaped\n@@ -34,7 +33,6 @@ class Response(object) :\n             self.body = ResponseBody(body, self.headers_encoding())\n         else:\n             self.body = body\n-        self.parent = parent\n         self.cached = False\n         self.request = None # request which originated this response\n \n@@ -52,8 +50,8 @@ class Response(object) :\n                 return encoding.group(1)\n \n     def __repr__(self):\n-        return \"Response(domain=%s, url=%s, original_url=%s, headers=%s, status=%s, body=%s, parent=%s)\" % \\\n-                (repr(self.domain), repr(self.url), repr(self.original_url), repr(self.headers), repr(self.status), repr(self.body), repr(self.parent))\n+        return \"Response(domain=%s, url=%s, original_url=%s, headers=%s, status=%s, body=%s)\" % \\\n+                (repr(self.domain), repr(self.url), repr(self.original_url), repr(self.headers), repr(self.status), repr(self.body))\n \n     def __str__(self):\n         version = '%s..%s' % (self.version()[:4], self.version()[-4:])\n@@ -84,8 +82,7 @@ class Response(object) :\n                            original_url=kw.get('original_url', self.original_url),\n                            headers=kw.get('headers', sameheaders()),\n                            status=kw.get('status', self.status),\n-                           body=kw.get('body', samebody()),\n-                           parent=kw.get('parent', self.parent))\n+                           body=kw.get('body', samebody()))\n         return newresp\n \n     def to_string(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d4337610d9f76df6f40070324c4c94a7b069507a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 165 | Files Changed: 8 | Hunks: 6 | Methods Changed: 12 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 165 | Churn Cumulative: 210 | Contributors (this commit): 2 | Commits (past 90d): 21 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n\n@@ -1,107 +0,0 @@\n-from datetime import datetime\n-\n-from django.db import models\n-from django.template.defaultfilters import slugify\n-from django.utils.translation import ugettext_lazy as _\n-\n-\n-class Group(models.Model):\n-    name = models.CharField(_(\"name\"), max_length=64, core=True, blank=False)\n-    slug = models.SlugField(_(\"slug\"), editable=False, unique=True,\n-                            prepopulate_from=(\"name\",))\n-\n-    # automatic dates\n-    created = models.DateTimeField(_(\"created\"), core=True, editable=False)\n-    updated = models.DateTimeField(_(\"updated\"), core=True, editable=False)\n-\n-    def save(self):\n-        if not self.id:\n-            self.created = datetime.now()\n-        self.updated = datetime.now()\n-        self.slug = slugify(self.name)\n-        super(Group, self).save()\n-\n-    def __unicode__(self):\n-        return self.name\n-\n-    class Meta:\n-        verbose_name = _(\"group\")\n-        verbose_name_plural = _(\"groups\")\n-\n-    class Admin:\n-        list_display = (\"name\", \"slug\",)\n-\n-\n-class Link(models.Model):\n-    text = models.CharField(_(\"text\"), max_length=1024, core=True, blank=False)\n-    address = models.CharField(_(\"address\"), max_length=1024, core=True,\n-                               blank=False)\n-\n-    # automatic dates\n-    created = models.DateTimeField(core=True, editable=False)\n-    updated = models.DateTimeField(core=True, editable=False)\n-\n-    def save(self):\n-        if not self.id:\n-            self.created = datetime.now()\n-        self.updated = datetime.now()\n-        super(Link, self).save()\n-\n-    def __unicode__(self):\n-        return self.address\n-\n-    class Meta:\n-        verbose_name = _(\"link\")\n-        verbose_name_plural = _(\"links\")\n-\n-    class Admin:\n-        list_display = (\"text\", \"address\",)\n-\n-\n-class GroupLink(models.Model):\n-    group = models.ForeignKey(Group, core=True, null=False, blank=False)\n-    link = models.ForeignKey(Link, core=True, null=False, blank=False)\n-    position = models.IntegerField(_(\"position\"), core=True, default=0)\n-\n-    # automatic dates\n-    created = models.DateTimeField(core=True, editable=False)\n-    updated = models.DateTimeField(core=True, editable=False)\n-\n-    @property\n-    def address(self):\n-        return self.link.address\n-\n-    @property\n-    def text(self):\n-        return self.link.text\n-\n-    def position_up(self):\n-        self.position += 1\n-        self.save()\n-\n-    def position_down(self):\n-        self.position -= 1\n-        self.save()\n-\n-    def save(self):\n-        if not self.id:\n-            self.created = datetime.now()\n-        self.updated = datetime.now()\n-        super(GroupLink, self).save()\n-\n-    # ugly, but django-admin isn't very versatile right now\n-    def position_link(self):\n-        return _(\"%(position)s (<a href='/link/%(id)s/position/up/'>Up</a>\" \\\n-               \" | <a href='/link/%(id)s/position/down/'>Down</a>)\") % \\\n-               { \"position\": self.position, \"id\": self.id }\n-    position_link.short_description = u\"order\"\n-    position_link.allow_tags = True\n-\n-    class Admin:\n-        list_display = (\"group\", \"link\", \"position_link\")\n-        list_filter = (\"group\",)\n-\n-    class Meta:\n-        verbose_name = _(\"group link\")\n-        verbose_name_plural = _(\"group links\")\n-        ordering = [ \"position\", ]\n\n\n@@ -1,31 +0,0 @@\n-from django import template\n-\n-from scrapyorg.lib.templatetags import *\n-\n-from scrapyorg.link.models import GroupLink\n-\n-\n-register = template.Library()\n-\n-\n-@register.tag(name=\"load_links\")\n-def do_load_links(parser, token):\n-    syntax_msg = 'Template tag syntax: load_links \"SLUG_NAME\" as \"VAR_NAME\"'\n-\n-    try:\n-        tag, slug, _as, var_name = token.split_contents()\n-        if not is_string(slug) or not is_string(var_name):\n-            raise_syntax(syntax_msg)\n-    except:\n-        raise_syntax(syntax_msg)\n-    return LoadLinksNode(unquoute(slug), unquoute(var_name))\n-\n-\n-class LoadLinksNode(template.Node):\n-    def __init__(self, slug, var_name):\n-        self.slug = slug\n-        self.var_name = var_name\n-\n-    def render(self, context):\n-        context[self.var_name] = GroupLink.objects.filter(group__slug=self.slug)\n-        return ''\n\n@@ -1,9 +0,0 @@\n-from django.conf.urls.defaults import *\n-\n-from scrapyorg.link.views import *\n-\n-\n-urlpatterns = patterns('',\n-    (r\"^(?P<grouplink_id>\\d+)/position/up/$\", position_up),\n-    (r\"^(?P<grouplink_id>\\d+)/position/down/$\", position_down),\n-)\n\n@@ -1,16 +0,0 @@\n-from django.shortcuts import get_object_or_404\n-from django.http import HttpResponseRedirect\n-\n-from scrapyorg.link.models import GroupLink\n-\n-\n-def position_up(request, grouplink_id):\n-    grouplink = get_object_or_404(GroupLink, pk=grouplink_id)\n-    grouplink.position_up()\n-    return HttpResponseRedirect(\"/admin/link/grouplink/\")\n-\n-\n-def position_down(request, grouplink_id):\n-    grouplink = get_object_or_404(GroupLink, pk=grouplink_id)\n-    grouplink.position_down()\n-    return HttpResponseRedirect(\"/admin/link/grouplink/\")\n\n@@ -79,7 +79,6 @@ INSTALLED_APPS = (\n     'django.contrib.admin',\n     'django.contrib.markup',\n     'django.contrib.flatpages',\n-    'scrapyorg.link',\n     'scrapyorg.article',\n     'scrapyorg.download',\n     'scrapyorg.blog',\n\n@@ -7,7 +7,6 @@ urlpatterns = patterns('',\n     (r\"^$\", direct_to_template, { \"template\": \"home.html\" }),\n     (r\"^article/\", include(\"scrapyorg.article.urls\")),\n     (r\"^download/\", include(\"scrapyorg.download.urls\")),\n-    (r\"^link/\", include(\"scrapyorg.link.urls\")),\n     (r\"^weblog/\", include(\"scrapyorg.blog.urls\")),\n \n     (r\"^admin/\", include(\"django.contrib.admin.urls\")),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#202e08a9d3301e3b50b6b68ae7364c2d58f13160", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 29 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -78,7 +78,6 @@ INSTALLED_APPS = (\n     'django.contrib.sites',\n     'django.contrib.admin',\n     'django.contrib.markup',\n-    'django.contrib.flatpages',\n     'scrapyorg.article',\n     'scrapyorg.download',\n     'scrapyorg.blog',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#baee62cf44d0483e560160939b944e810a431e6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 8 | Files Changed: 6 | Hunks: 10 | Methods Changed: 2 | Complexity Δ (Sum/Max): 13/7 | Churn Δ: 21 | Churn Cumulative: 38 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -49,8 +49,8 @@ class Article(models.Model):\n     \n     # ugly, but django-admin isn't very versatile right now\n     def position_link(self):\n-        return _(\"%(position)s (<a href='/article/%(id)s/position/up/'>Up</a>\" \\\n-               \" | <a href='/article/%(id)s/position/down/'>Down</a>)\") % \\\n+        return _(\"%(position)s (<a href='/admin/article/article/%(id)s/position/up/'>Up</a>\" \\\n+                 \" | <a href='/admin/article/article/%(id)s/position/down/'>Down</a>)\") % \\\n                { \"position\": self.position, \"id\": self.id }\n     position_link.short_description = u\"position\"\n     position_link.allow_tags = True\n\n@@ -1,15 +1,17 @@\n from django.shortcuts import get_object_or_404\n from django.http import HttpResponseRedirect\n+from django.contrib.admin.views.decorators import staff_member_required\n \n from scrapyorg.article.models import Article\n \n \n+@staff_member_required\n def position_up(request, article_id):\n     article = get_object_or_404(Article, pk=article_id)\n     article.position_up()\n     return HttpResponseRedirect(\"/admin/article/article/\")\n \n-\n+@staff_member_required\n def position_down(request, article_id):\n     article = get_object_or_404(Article, pk=article_id)\n     article.position_down()\n\n@@ -32,9 +32,8 @@ class DownloadLink(models.Model):\n \n     # ugly, but django-admin isn't very versatile right now\n     def public_link(self):\n-        return _(\"%(status)s (<a href='/download/%(id)s/public/toggle/'>toggle</a>)\") % \\\n-               { 'status': self.public and _(\"Yes\") or _(\"No\"),\n-                 'id': self.id }\n+        return _(\"%s (<a href='%s/toggle/'>toggle</a>)\") % \\\n+               (self.public and _(\"Yes\") or _(\"No\"), self.id )\n     public_link.short_description = u\"public\"\n     public_link.allow_tags = True\n \n\n@@ -4,5 +4,5 @@ from scrapyorg.download.views import *\n \n \n urlpatterns = patterns('',\n-    (r\"^(?P<link_id>\\d+)/public/toggle/$\", toggle_public),\n+    (r\"^(?P<link_id>\\d+)/toggle/$\", toggle_public),\n )\n\n@@ -1,9 +1,11 @@\n from django.shortcuts import get_object_or_404\n from django.http import HttpResponseRedirect\n+from django.contrib.admin.views.decorators import staff_member_required\n \n from scrapyorg.download.models import DownloadLink\n \n \n+@staff_member_required\n def toggle_public(request, link_id):\n     link = get_object_or_404(DownloadLink, pk=link_id)\n     link.toggle_public()\n\n@@ -6,9 +6,11 @@ from django.conf import settings\n urlpatterns = patterns('',\n     (r\"^$\", direct_to_template, { \"template\": \"home.html\" }),\n     (r\"^article/\", include(\"scrapyorg.article.urls\")),\n-    (r\"^download/\", include(\"scrapyorg.download.urls\")),\n     (r\"^weblog/\", include(\"scrapyorg.blog.urls\")),\n \n+    # admin\n+    (r\"^admin/article/article/\", include(\"scrapyorg.article.urls\")),\n+    (r\"^admin/download/downloadlink/\", include(\"scrapyorg.download.urls\")),\n     (r\"^admin/\", include(\"django.contrib.admin.urls\")),\n )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9f8ce8f11620be80f0e6bb0072f33b6ddaf4f0a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 13 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,8 +32,12 @@ class DownloadLink(models.Model):\n \n     # ugly, but django-admin isn't very versatile right now\n     def public_link(self):\n-        return _(\"%s (<a href='%s/toggle/'>toggle</a>)\") % \\\n-               (self.public and _(\"Yes\") or _(\"No\"), self.id )\n+        img_url = \"/media/img/admin/icon-%s.gif\" % \\\n+                  (self.public and \"yes\" or \"no\")\n+        html = _('<img alt=\"%s\" src=\"' + img_url + '\"/> ' \\\n+                 '(<a href=\"%s/toggle/\">Toggle</a>)')\n+        return html % (_(str(self.public)), self.id)\n+\n     public_link.short_description = u\"public\"\n     public_link.allow_tags = True\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
