{"custom_id": "keras#18ff39368281ffd64e5869722c30c5e54596d94b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 168 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -64,6 +64,8 @@ def filter_traceback(fn):\n       return fn(*args, **kwargs)\n     except Exception as e:  # pylint: disable=broad-except\n       filtered_tb = _process_traceback_frames(e.__traceback__)\n+      # To get the full stack trace, call:\n+      # `tf.debugging.disable_traceback_filtering()`\n       raise e.with_traceback(filtered_tb) from None\n     finally:\n       del filtered_tb\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f0eea6087403cfce9847744d755ad2d1dae53be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4070 | Contributors (this commit): 37 | Commits (past 90d): 4 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -448,7 +448,7 @@ class Sequence:\n   # Here, `x_set` is list of path to the images\n   # and `y_set` are the associated classes.\n \n-  class CIFAR10Sequence(Sequence):\n+  class CIFAR10Sequence(tf.keras.utils.Sequence):\n \n       def __init__(self, x_set, y_set, batch_size):\n           self.x, self.y = x_set, y_set\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b9d76c7662142ab46077dbf3fb014d59a612ada", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 208 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 25/25 | Churn Δ: 208 | Churn Cumulative: 208 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.2717391304347826\n\nDIFF:\n@@ -0,0 +1,208 @@\n+\"\"\"Keras audio dataset loading utilities.\"\"\" \n+\n+import tensorflow.compat.v2 as tf\n+# pylint: disable=g-classes-have-attributes\n+\n+import numpy as np\n+from keras.utils import dataset_utils\n+from tensorflow.python.util.tf_export import keras_export\n+\n+ALLOWED_FORMATS = ('.wav',)\n+\n+\n+\n+@keras_export('keras.utils.audio_dataset_from_directory',\n+              'keras.preprocessing.audio_dataset_from_directory',\n+              v1=[])\n+def audio_dataset_from_directory(directory,\n+    labels=\"inferred\", \n+    label_mode=\"int\", \n+    class_names=None, \n+    batch_size=32, \n+    sampling_rate=None, \n+    output_sequence_length=None, \n+    ragged=False, \n+    shuffle=True, \n+    seed=None, \n+    validation_split=None, \n+    subset=None, \n+    follow_links=False):\n+\n+    \"\"\"Generates a `tf.data.Dataset` from audio files in a directory.\n+    If your directory structure is:\n+    ```\n+    main_directory/\n+    ...class_a/\n+    ......a_audio_1.wav\n+    ......a_audio_2.wav\n+    ...class_b/\n+    ......b_audio_1.wav\n+    ......b_audio_2.wav\n+    ```\n+    Then calling `audio_dataset_from_directory(main_directory, labels='inferred')`\n+    will return a `tf.data.Dataset` that yields batches of audio files from\n+    the subdirectories `class_a` and `class_b`, together with labels\n+    0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n+    Only `.wav` files are supported at this time.\n+    Args:\n+      directory: Directory where the data is located.\n+          If `labels` is \"inferred\", it should contain\n+          subdirectories, each containing audio files for a class.\n+          Otherwise, the directory structure is ignored.\n+      labels: Either \"inferred\"\n+          (labels are generated from the directory structure),\n+          None (no labels),\n+          or a list/tuple of integer labels of the same size as the number of\n+          audio files found in the directory. Labels should be sorted according\n+          to the alphanumeric order of the audio file paths\n+          (obtained via `os.walk(directory)` in Python).\n+      label_mode: String describing the encoding of `labels`. Options are:\n+          - 'int': means that the labels are encoded as integers\n+              (e.g. for `sparse_categorical_crossentropy` loss).\n+          - 'categorical' means that the labels are\n+              encoded as a categorical vector\n+              (e.g. for `categorical_crossentropy` loss).\n+          - 'binary' means that the labels (there can be only 2)\n+              are encoded as `float32` scalars with values 0 or 1\n+              (e.g. for `binary_crossentropy`).\n+          - None (no labels).\n+      class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n+          list of class names (must match names of subdirectories). Used\n+          to control the order of the classes\n+          (otherwise alphanumerical order is used).\n+      batch_size: Size of the batches of data. Default: 32.\n+        If `None`, the data will not be batched\n+        (the dataset will yield individual samples).\n+      sampling_rate: Number of samples taken each second.\n+      output_sequence_length: Maximum length of a audio sequence. audio files longer than this will \n+      be truncated to `output_sequence_length`\n+      shuffle: Whether to shuffle the data. Default: True.\n+          If set to False, sorts the data in alphanumeric order.\n+      seed: Optional random seed for shuffling and transformations.\n+      validation_split: Optional float between 0 and 1,\n+          fraction of data to reserve for validation.\n+      subset: Subset of the data to return.\n+          One of \"training\" or \"validation\".\n+          Only used if `validation_split` is set.\n+      follow_links: Whether to visits subdirectories pointed to by symlinks.\n+          Defaults to False.\n+    Returns:\n+      A `tf.data.Dataset` object.\n+        - If `label_mode` is None, it yields `string` tensors of shape\n+          `(batch_size,)`, containing the contents of a batch of audio files.\n+        - Otherwise, it yields a tuple `(audio, labels)`, where `audio`\n+          has shape `(batch_size, sequence_length, num_channels)` and `labels` follows the format described\n+          below.\n+    Rules regarding labels format:\n+      - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n+        `(batch_size,)`.\n+      - if `label_mode` is `binary`, the labels are a `float32` tensor of\n+        1s and 0s of shape `(batch_size, 1)`.\n+      - if `label_mode` is `categorical`, the labels are a `float32` tensor\n+        of shape `(batch_size, num_classes)`, representing a one-hot\n+        encoding of the class index.\n+    \"\"\"\n+\n+\n+    if labels not in ('inferred', None):\n+        if not isinstance(labels, (list, tuple)):\n+            raise ValueError(\n+                '`labels` argument should be a list/tuple of integer labels, of '\n+                'the same size as the number of audio files in the target '\n+                'directory. If you wish to infer the labels from the subdirectory '\n+                'names in the target directory, pass `labels=\"inferred\"`. '\n+                'If you wish to get a dataset that only contains audio samples '\n+                f'(no labels), pass `labels=None`. Received: labels={labels}')\n+        if class_names:\n+            raise ValueError('You can only pass `class_names` if '\n+                        f'`labels=\"inferred\"`. Received: labels={labels}, and '\n+                        f'class_names={class_names}')\n+    if label_mode not in {'int', 'categorical', 'binary', None}:\n+        raise ValueError('`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", 'f'or None. Received: label_mode={label_mode}')\n+    \n+\n+\n+    if not ragged and output_sequence_length is None:\n+        raise Exception(f'The dataset should be ragged dataset or fixed sequence length dataset, found ragged={ragged} and output_sequence_length={output_sequence_length}')\n+    elif ragged and output_sequence_length is not None:\n+        raise Exception('Cannot set both `ragged` and `output_sequence_length`')\n+    \n+\n+\n+    if labels is None or label_mode is None:\n+        labels = None\n+        label_mode = None\n+\n+    dataset_utils.check_validation_split_arg(validation_split, subset, shuffle, seed)\n+\n+    if seed is None:\n+        seed = np.random.randint(1e6)\n+        \n+    file_paths, labels, class_names = dataset_utils.index_directory(directory, labels, formats=ALLOWED_FORMATS, class_names=class_names, \n+        shuffle=shuffle, \n+        seed=seed, \n+        follow_links=follow_links)\n+\n+    if label_mode == 'binary' and len(class_names) != 2:\n+        raise ValueError(f'When passing `label_mode=\"binary\"`, there must be exactly 2 'f'class_names. Received: class_names={class_names}')\n+\n+    file_paths, labels = dataset_utils.get_training_or_validation_split(file_paths, labels, validation_split, subset)\n+    if not file_paths:\n+        raise ValueError(f'No audio files found in directory {directory}. 'f'Allowed format(s): {ALLOWED_FORMATS}')\n+\n+\n+    if ragged:\n+        dataset = paths_and_labels_to_ragged_dataset(file_paths=file_paths, labels=labels, \n+            label_mode=label_mode, \n+            num_classes=len(class_names), \n+            sampling_rate=sampling_rate)\n+    else:\n+        dataset = paths_and_labels_to_dataset(file_paths=file_paths, labels=labels, label_mode=label_mode, \n+            num_classes=len(class_names), \n+            sampling_rate=sampling_rate, \n+            output_sequence_length=output_sequence_length)\n+\n+    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n+    if batch_size is not None:\n+        if shuffle:\n+            dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n+        dataset = dataset.batch(batch_size)\n+    else:\n+        if shuffle:\n+            dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n+\n+    # Users may need to reference `class_names`.\n+    dataset.class_names = class_names\n+    return dataset\n+\n+\n+def prepare_audio(path, sampling_rate, output_sequence_length=-1):\n+    \"\"\"Reads and prepare the audio file.\"\"\"\n+    audio = tf.io.read_file(path)\n+    audio, _ = tf.audio.decode_wav(contents=audio, desired_samples=output_sequence_length )\n+    if sampling_rate is not None:\n+        audio = tf.audio.encode_wav(audio, sampling_rate)\n+        audio, _ = tf.audio.decode_wav(audio)\n+    return audio\n+\n+def paths_and_labels_to_dataset(file_paths,\n+                                labels,\n+                                label_mode,\n+                                num_classes, sampling_rate, output_sequence_length):\n+  \"\"\"Constructs a fixed size dataset of audio and labels.\"\"\"\n+  path_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n+  audio_ds = path_ds.map(lambda x: prepare_audio(x, sampling_rate, output_sequence_length), num_parallel_calls=tf.data.AUTOTUNE)\n+  if label_mode:\n+    label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n+    audio_ds = tf.data.Dataset.zip((audio_ds, label_ds))\n+  return audio_ds\n+\n+def paths_and_labels_to_ragged_dataset(file_paths, labels, label_mode, num_classes, sampling_rate):\n+    \"\"\"Constructs a ragged dataset of audio and labels.\"\"\"\n+    audio_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n+    audio_ds = audio_ds.map(lambda x: prepare_audio(x, sampling_rate), num_parallel_calls=tf.data.AUTOTUNE)\n+    audio_ds = audio_ds.map(lambda x: tf.RaggedTensor.from_tensor(x), num_parallel_calls=tf.data.AUTOTUNE)\n+    if label_mode:\n+        label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n+        audio_ds = tf.data.Dataset.zip((audio_ds, label_ds))\n+    return audio_ds\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24bb6350952d7a7225239677ced46e143f3bf377", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 54 | Files Changed: 1 | Hunks: 33 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 149 | Churn Cumulative: 357 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.3\n\nDIFF:\n@@ -1,20 +1,32 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n \"\"\"Keras audio dataset loading utilities.\"\"\"\n \n import tensorflow.compat.v2 as tf\n+\n # pylint: disable=g-classes-have-attributes\n \n import numpy as np\n from keras.utils import dataset_utils\n from tensorflow.python.util.tf_export import keras_export\n \n-ALLOWED_FORMATS = ('.wav',)\n+ALLOWED_FORMATS = (\".wav\",)\n \n \n-\n-@keras_export('keras.utils.audio_dataset_from_directory',\n-              'keras.preprocessing.audio_dataset_from_directory',\n-              v1=[])\n-def audio_dataset_from_directory(directory,\n+def audio_dataset_from_directory(\n+    directory,\n     labels=\"inferred\",\n     label_mode=\"int\",\n     class_names=None,\n@@ -26,10 +38,12 @@ def audio_dataset_from_directory(directory,\n     seed=None,\n     validation_split=None,\n     subset=None,\n-    follow_links=False):\n-\n+    follow_links=False,\n+):\n     \"\"\"Generates a `tf.data.Dataset` from audio files in a directory.\n+\n     If your directory structure is:\n+\n     ```\n     main_directory/\n     ...class_a/\n@@ -39,11 +53,14 @@ def audio_dataset_from_directory(directory,\n     ......b_audio_1.wav\n     ......b_audio_2.wav\n     ```\n+    \n     Then calling `audio_dataset_from_directory(main_directory, labels='inferred')`\n     will return a `tf.data.Dataset` that yields batches of audio files from\n     the subdirectories `class_a` and `class_b`, together with labels\n     0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n+    \n     Only `.wav` files are supported at this time.\n+\n     Args:\n       directory: Directory where the data is located.\n           If `labels` is \"inferred\", it should contain\n@@ -86,6 +103,7 @@ def audio_dataset_from_directory(directory,\n           Only used if `validation_split` is set.\n       follow_links: Whether to visits subdirectories pointed to by symlinks.\n           Defaults to False.\n+\n     Returns:\n       A `tf.data.Dataset` object.\n         - If `label_mode` is None, it yields `string` tensors of shape\n@@ -93,6 +111,7 @@ def audio_dataset_from_directory(directory,\n         - Otherwise, it yields a tuple `(audio, labels)`, where `audio`\n           has shape `(batch_size, sequence_length, num_channels)` and `labels` follows the format described\n           below.\n+\n     Rules regarding labels format:\n       - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n         `(batch_size,)`.\n@@ -102,32 +121,33 @@ def audio_dataset_from_directory(directory,\n         of shape `(batch_size, num_classes)`, representing a one-hot\n         encoding of the class index.\n     \"\"\"\n-\n-\n-    if labels not in ('inferred', None):\n+    if labels not in (\"inferred\", None):\n         if not isinstance(labels, (list, tuple)):\n             raise ValueError(\n-                '`labels` argument should be a list/tuple of integer labels, of '\n-                'the same size as the number of audio files in the target '\n-                'directory. If you wish to infer the labels from the subdirectory '\n+                \"`labels` argument should be a list/tuple of integer labels, of \"\n+                \"the same size as the number of audio files in the target \"\n+                \"directory. If you wish to infer the labels from the subdirectory \"\n                 'names in the target directory, pass `labels=\"inferred\"`. '\n-                'If you wish to get a dataset that only contains audio samples '\n-                f'(no labels), pass `labels=None`. Received: labels={labels}')\n+                \"If you wish to get a dataset that only contains audio samples \"\n+                f\"(no labels), pass `labels=None`. Received: labels={labels}\"\n+            )\n         if class_names:\n-            raise ValueError('You can only pass `class_names` if '\n+            raise ValueError(\n+                \"You can only pass `class_names` if \"\n                 f'`labels=\"inferred\"`. Received: labels={labels}, and '\n-                        f'class_names={class_names}')\n-    if label_mode not in {'int', 'categorical', 'binary', None}:\n-        raise ValueError('`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", 'f'or None. Received: label_mode={label_mode}')\n-    \n-\n+                f\"class_names={class_names}\"\n+            )\n+    if label_mode not in {\"int\", \"categorical\", \"binary\", None}:\n+        raise ValueError(\n+            f'`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", or None. Received: label_mode={label_mode}'\n+        )\n \n     if not ragged and output_sequence_length is None:\n-        raise Exception(f'The dataset should be ragged dataset or fixed sequence length dataset, found ragged={ragged} and output_sequence_length={output_sequence_length}')\n+        raise Exception(\n+            f\"The dataset should be ragged dataset or fixed sequence length dataset, found ragged={ragged} and output_sequence_length={output_sequence_length}\"\n+        )\n     elif ragged and output_sequence_length is not None:\n-        raise Exception('Cannot set both `ragged` and `output_sequence_length`')\n-    \n-\n+        raise Exception(\"Cannot set both `ragged` and `output_sequence_length`\")\n \n     if labels is None or label_mode is None:\n         labels = None\n@@ -138,29 +158,40 @@ def audio_dataset_from_directory(directory,\n     if seed is None:\n         seed = np.random.randint(1e6)\n \n-    file_paths, labels, class_names = dataset_utils.index_directory(directory, labels, formats=ALLOWED_FORMATS, class_names=class_names, \n+    file_paths, labels, class_names = dataset_utils.index_directory(\n+        directory,\n+        labels,\n+        formats=ALLOWED_FORMATS,\n+        class_names=class_names,\n         shuffle=shuffle,\n         seed=seed,\n-        follow_links=follow_links)\n+        follow_links=follow_links,\n+    )\n \n-    if label_mode == 'binary' and len(class_names) != 2:\n-        raise ValueError(f'When passing `label_mode=\"binary\"`, there must be exactly 2 'f'class_names. Received: class_names={class_names}')\n+    if label_mode == \"binary\" and len(class_names) != 2:\n+        raise ValueError(\n+            f'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n+            f\"class_names. Received: class_names={class_names}\"\n+        )\n \n-    file_paths, labels = dataset_utils.get_training_or_validation_split(file_paths, labels, validation_split, subset)\n+    file_paths, labels = dataset_utils.get_training_or_validation_split(\n+        file_paths, labels, validation_split, subset\n+    )\n     if not file_paths:\n-        raise ValueError(f'No audio files found in directory {directory}. 'f'Allowed format(s): {ALLOWED_FORMATS}')\n+        raise ValueError(\n+            f\"No audio files found in directory {directory}. \"\n+            f\"Allowed format(s): {ALLOWED_FORMATS}\"\n+        )\n \n-\n-    if ragged:\n-        dataset = paths_and_labels_to_ragged_dataset(file_paths=file_paths, labels=labels, \n+    dataset = paths_and_labels_to_dataset(\n+        file_paths=file_paths,\n+        labels=labels,\n         label_mode=label_mode,\n-            num_classes=len(class_names), \n-            sampling_rate=sampling_rate)\n-    else:\n-        dataset = paths_and_labels_to_dataset(file_paths=file_paths, labels=labels, label_mode=label_mode, \n         num_classes=len(class_names),\n         sampling_rate=sampling_rate,\n-            output_sequence_length=output_sequence_length)\n+        output_sequence_length=output_sequence_length,\n+        ragged=ragged,\n+    )\n \n     dataset = dataset.prefetch(tf.data.AUTOTUNE)\n     if batch_size is not None:\n@@ -176,32 +207,42 @@ def audio_dataset_from_directory(directory,\n     return dataset\n \n \n-def prepare_audio(path, sampling_rate, output_sequence_length=-1):\n+def prepare_audio(path, sampling_rate, output_sequence_length=None):\n     \"\"\"Reads and prepare the audio file.\"\"\"\n     audio = tf.io.read_file(path)\n-    audio, _ = tf.audio.decode_wav(contents=audio, desired_samples=output_sequence_length )\n+    if output_sequence_length is None:\n+        output_sequence_length = -1\n+    audio, _ = tf.audio.decode_wav(\n+        contents=audio, desired_samples=output_sequence_length\n+    )\n     if sampling_rate is not None:\n         audio = tf.audio.encode_wav(audio, sampling_rate)\n         audio, _ = tf.audio.decode_wav(audio)\n     return audio\n \n-def paths_and_labels_to_dataset(file_paths,\n+\n+def paths_and_labels_to_dataset(\n+    file_paths,\n     labels,\n     label_mode,\n-                                num_classes, sampling_rate, output_sequence_length):\n+    num_classes,\n+    sampling_rate,\n+    output_sequence_length,\n+    ragged,\n+):\n     \"\"\"Constructs a fixed size dataset of audio and labels.\"\"\"\n     path_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n-  audio_ds = path_ds.map(lambda x: prepare_audio(x, sampling_rate, output_sequence_length), num_parallel_calls=tf.data.AUTOTUNE)\n-  if label_mode:\n-    label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n-    audio_ds = tf.data.Dataset.zip((audio_ds, label_ds))\n-  return audio_ds\n+    audio_ds = path_ds.map(\n+        lambda x: prepare_audio(x, sampling_rate, output_sequence_length),\n+        num_parallel_calls=tf.data.AUTOTUNE,\n+    )\n+\n+    if ragged:\n+        audio_ds = audio_ds.map(\n+            lambda x: tf.RaggedTensor.from_tensor(x),\n+            num_parallel_calls=tf.data.AUTOTUNE,\n+        )\n \n-def paths_and_labels_to_ragged_dataset(file_paths, labels, label_mode, num_classes, sampling_rate):\n-    \"\"\"Constructs a ragged dataset of audio and labels.\"\"\"\n-    audio_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n-    audio_ds = audio_ds.map(lambda x: prepare_audio(x, sampling_rate), num_parallel_calls=tf.data.AUTOTUNE)\n-    audio_ds = audio_ds.map(lambda x: tf.RaggedTensor.from_tensor(x), num_parallel_calls=tf.data.AUTOTUNE)\n     if label_mode:\n         label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n         audio_ds = tf.data.Dataset.zip((audio_ds, label_ds))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a824de17ff91b405dce5a20102edcd4ab771978d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 359 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -19,12 +19,14 @@ import tensorflow.compat.v2 as tf\n # pylint: disable=g-classes-have-attributes\n \n import numpy as np\n+\n from keras.utils import dataset_utils\n from tensorflow.python.util.tf_export import keras_export\n \n ALLOWED_FORMATS = (\".wav\",)\n \n \n+@keras_export(\"keras.utils.image_dataset_from_directory\", v1=[])\n def audio_dataset_from_directory(\n     directory,\n     labels=\"inferred\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b7aec6378893a3449e6e1987283fe2fdb6eb1686", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 361 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ from tensorflow.python.util.tf_export import keras_export\n ALLOWED_FORMATS = (\".wav\",)\n \n \n-@keras_export(\"keras.utils.image_dataset_from_directory\", v1=[])\n+@keras_export(\"keras.utils.audio_dataset_from_directory\", v1=[])\n def audio_dataset_from_directory(\n     directory,\n     labels=\"inferred\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0449d0d67a7f1d366863d423323dfbf020695c43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 138 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 155 | Churn Cumulative: 909 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.8055555555555556\n\nDIFF:\n@@ -109,7 +109,7 @@ def audio_dataset_from_directory(\n       validation_split: Optional float between 0 and 1,\n           fraction of data to reserve for validation.\n       subset: Subset of the data to return.\n-          One of \"training\" or \"validation\".\n+          One of \"training\", \"validation\" or \"both\".\n           Only used if `validation_split` is set.\n       follow_links: Whether to visits subdirectories pointed to by symlinks.\n           Defaults to False.\n@@ -161,15 +161,19 @@ def audio_dataset_from_directory(\n \n     if sampling_rate is not None:\n         if not isinstance(sampling_rate, int):\n-            raise ValueError(\"`sampling_rate` should have an integer value\")\n+            raise ValueError(\n+                f\"`sampling_rate` should have an integer value. Received: sampling_rate={sampling_rate}\"\n+            )\n \n         if sampling_rate <= 0:\n             raise ValueError(\n-                f\"`sampling_rate` should have value bigger than zero found sampling_rate={sampling_rate}\"\n+                f\"`sampling_rate` should be higher than 0. Received: sampling_rate={sampling_rate}\"\n             )\n \n         if tfio is None:\n-            raise ImportError(\"Cannot import tensorflow_io, install `tensorflow-io`\")\n+            raise ImportError(\n+                \"To use the argument `sampling_rate`, you should install tensorflow_io. You can install it via pip install tensorflow-io.\"\n+            )\n \n     if labels is None or label_mode is None:\n         labels = None\n@@ -196,6 +200,134 @@ def audio_dataset_from_directory(\n             f\"class_names. Received: class_names={class_names}\"\n         )\n \n+    if subset == \"both\":\n+        train_dataset, val_dataset = get_training_and_validation_dataset(\n+            file_paths=file_paths,\n+            labels=labels,\n+            validation_split=validation_split,\n+            directory=directory,\n+            label_mode=label_mode,\n+            class_names=class_names,\n+            sampling_rate=sampling_rate,\n+            output_sequence_length=output_sequence_length,\n+            ragged=ragged,\n+        )\n+\n+        train_dataset = prepare_dataset(\n+            dataset=train_dataset,\n+            batch_size=batch_size,\n+            shuffle=shuffle,\n+            seed=seed,\n+            class_names=class_names,\n+        )\n+        val_dataset = prepare_dataset(\n+            val_dataset, batch_size=batch_size, shuffle=False, class_names=class_names\n+        )\n+        return train_dataset, val_dataset\n+\n+    else:\n+        dataset = get_dataset(\n+            file_paths=file_paths,\n+            labels=labels,\n+            directory=directory,\n+            validation_split=validation_split,\n+            subset=subset,\n+            label_mode=label_mode,\n+            class_names=class_names,\n+            sampling_rate=sampling_rate,\n+            output_sequence_length=output_sequence_length,\n+            ragged=ragged,\n+        )\n+\n+        dataset = prepare_dataset(\n+            dataset=dataset,\n+            batch_size=batch_size,\n+            shuffle=shuffle,\n+            seed=seed,\n+            class_names=class_names,\n+        )\n+        return dataset\n+\n+\n+def prepare_dataset(dataset, batch_size, shuffle, seed, class_names):\n+    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n+    if batch_size is not None:\n+        if shuffle:\n+            dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n+        dataset = dataset.batch(batch_size)\n+    else:\n+        if shuffle:\n+            dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n+\n+    # Users may need to reference `class_names`.\n+    dataset.class_names = class_names\n+    return dataset\n+\n+\n+def get_training_and_validation_dataset(\n+    file_paths,\n+    labels,\n+    validation_split,\n+    directory,\n+    label_mode,\n+    class_names,\n+    sampling_rate,\n+    output_sequence_length,\n+    ragged,\n+):\n+    file_paths_train, labels_train = dataset_utils.get_training_or_validation_split(\n+        file_paths, labels, validation_split, \"training\"\n+    )\n+    if not file_paths:\n+        raise ValueError(\n+            f\"No training audio files found in directory {directory}. \"\n+            f\"Allowed format(s): {ALLOWED_FORMATS}\"\n+        )\n+\n+    file_paths_val, labels_val = dataset_utils.get_training_or_validation_split(\n+        file_paths, labels, validation_split, \"validation\"\n+    )\n+    if not file_paths:\n+        raise ValueError(\n+            f\"No validation audio files found in directory {directory}. \"\n+            f\"Allowed format(s): {ALLOWED_FORMATS}\"\n+        )\n+\n+    train_dataset = paths_and_labels_to_dataset(\n+        file_paths=file_paths_train,\n+        labels=labels_train,\n+        label_mode=label_mode,\n+        num_classes=len(class_names),\n+        sampling_rate=sampling_rate,\n+        output_sequence_length=output_sequence_length,\n+        ragged=ragged,\n+    )\n+\n+    val_dataset = paths_and_labels_to_dataset(\n+        file_paths=file_paths_val,\n+        labels=labels_val,\n+        label_mode=label_mode,\n+        num_classes=len(class_names),\n+        sampling_rate=sampling_rate,\n+        output_sequence_length=output_sequence_length,\n+        ragged=ragged,\n+    )\n+\n+    return train_dataset, val_dataset\n+\n+\n+def get_dataset(\n+    file_paths,\n+    labels,\n+    directory,\n+    validation_split,\n+    subset,\n+    label_mode,\n+    class_names,\n+    sampling_rate,\n+    output_sequence_length,\n+    ragged,\n+):\n     file_paths, labels = dataset_utils.get_training_or_validation_split(\n         file_paths, labels, validation_split, subset\n     )\n@@ -215,17 +347,6 @@ def audio_dataset_from_directory(\n         ragged=ragged,\n     )\n \n-    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n-    if batch_size is not None:\n-        if shuffle:\n-            dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-        dataset = dataset.batch(batch_size)\n-    else:\n-        if shuffle:\n-            dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n-\n-    # Users may need to reference `class_names`.\n-    dataset.class_names = class_names\n     return dataset\n \n \n\n@@ -249,14 +249,14 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         directory = self._prepare_directory(num_classes=3, count=5)\n \n         with self.assertRaisesRegex(\n-            ValueError, \"`sampling_rate` should have value bigger than zero\"\n+            ValueError, \"`sampling_rate` should be higher than 0. Received:\"\n         ):\n             _ = audio_dataset.audio_dataset_from_directory(\n                 directory, ragged=False, output_sequence_length=10, sampling_rate=-1\n             )\n \n         with self.assertRaisesRegex(\n-            ValueError, \"`sampling_rate` should have an integer value\"\n+            ValueError, \"`sampling_rate` should have an integer value. Received:\"\n         ):\n             _ = audio_dataset.audio_dataset_from_directory(\n                 directory, ragged=False, output_sequence_length=10, sampling_rate=1.2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fde0451b69547cd7a3818eb55a3f5399b30dc0d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 555 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -221,7 +221,7 @@ def audio_dataset_from_directory(\n             class_names=class_names,\n         )\n         val_dataset = prepare_dataset(\n-            val_dataset, batch_size=batch_size, shuffle=False, class_names=class_names\n+            dataset=val_dataset, batch_size=batch_size, shuffle=False, class_names=class_names\n         )\n         return train_dataset, val_dataset\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c5e2633190e5eb87654c98622c80c1637aaa0c17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 557 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -221,7 +221,7 @@ def audio_dataset_from_directory(\n             class_names=class_names,\n         )\n         val_dataset = prepare_dataset(\n-            dataset=val_dataset, batch_size=batch_size, shuffle=False, class_names=class_names\n+            dataset=val_dataset, batch_size=batch_size, shuffle=False, seed=seed, class_names=class_names\n         )\n         return train_dataset, val_dataset\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b580d944ab28a6c8cb0dee7abe5c7c3dbb113741", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 563 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -221,7 +221,11 @@ def audio_dataset_from_directory(\n             class_names=class_names,\n         )\n         val_dataset = prepare_dataset(\n-            dataset=val_dataset, batch_size=batch_size, shuffle=False, seed=seed, class_names=class_names\n+            dataset=val_dataset,\n+            batch_size=batch_size,\n+            shuffle=False,\n+            seed=seed,\n+            class_names=class_names,\n         )\n         return train_dataset, val_dataset\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#05f66318f0ce37eb31d3a4fba822908afeec5b96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 571 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,7 +20,7 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n \n-from keras.utils import dataset_utils\n+from keras.preprocessing import dataset_utils\n from tensorflow.python.util.tf_export import keras_export\n \n try:\n@@ -179,7 +179,7 @@ def audio_dataset_from_directory(\n         labels = None\n         label_mode = None\n \n-    dataset_utils.check_validation_split_arg(validation_split, subset, shuffle, seed)\n+    # dataset_utils.check_validation_split_arg(validation_split, subset, shuffle, seed)\n \n     if seed is None:\n         seed = np.random.randint(1e6)\n@@ -282,7 +282,7 @@ def get_training_and_validation_dataset(\n     file_paths_train, labels_train = dataset_utils.get_training_or_validation_split(\n         file_paths, labels, validation_split, \"training\"\n     )\n-    if not file_paths:\n+    if not file_paths_train:\n         raise ValueError(\n             f\"No training audio files found in directory {directory}. \"\n             f\"Allowed format(s): {ALLOWED_FORMATS}\"\n@@ -291,7 +291,7 @@ def get_training_and_validation_dataset(\n     file_paths_val, labels_val = dataset_utils.get_training_or_validation_split(\n         file_paths, labels, validation_split, \"validation\"\n     )\n-    if not file_paths:\n+    if not file_paths_val:\n         raise ValueError(\n             f\"No validation audio files found in directory {directory}. \"\n             f\"Allowed format(s): {ALLOWED_FORMATS}\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1be7c29fdf9e0d2bb0d98190801b01d34cb7337", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 575 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,7 +20,7 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n \n-from keras.preprocessing import dataset_utils\n+from keras.utils import dataset_utils\n from tensorflow.python.util.tf_export import keras_export\n \n try:\n@@ -179,7 +179,7 @@ def audio_dataset_from_directory(\n         labels = None\n         label_mode = None\n \n-    # dataset_utils.check_validation_split_arg(validation_split, subset, shuffle, seed)\n+    dataset_utils.check_validation_split_arg(validation_split, subset, shuffle, seed)\n \n     if seed is None:\n         seed = np.random.randint(1e6)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bff415286bd5883e73c1572cf409e88265bdc32b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 577 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8108350d3bbf9ed86d0d856b520c3bb9f3f12e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 579 | Contributors (this commit): 1 | Commits (past 90d): 13 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -154,7 +154,7 @@ def audio_dataset_from_directory(\n \n     if not ragged and output_sequence_length is None:\n         raise ValueError(\n-            f\"The dataset should be ragged dataset or fixed sequence length dataset, found ragged={ragged} and output_sequence_length={output_sequence_length}\"\n+            f\"The dataset should be ragged dataset or fixed sequence length dataset. Received: ragged={ragged} and output_sequence_length={output_sequence_length}\"\n         )\n     elif ragged and output_sequence_length is not None:\n         raise ValueError(\"Cannot set both `ragged` and `output_sequence_length`\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fb4b7dfabd00b95f3b3c28d66b4dc6d5be131f98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 46 | Churn Cumulative: 46 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,46 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras audio dataset loading utilities.\"\"\"\n+\n+import tensorflow.compat.v2 as tf #type: ignore\n+\n+# pylint: disable=g-classes-have-attributes\n+\n+import numpy as np\n+\n+from keras.utils import dataset_utils\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+\n+def split_dataset(dataset, left_size=None, right_size=None, shuffle=False, seed=None):\n+    \"\"\"Split a dataset into a left half and a right half (e.g. training / validation).\n+    \n+    Args:\n+        dataset: A `tf.data.Dataset` object or a list/tuple of arrays with the same length.\n+        left_size: If float, it should be in range `[0, 1]` range and signifies the fraction of the\n+            data to pack in the left dataset. If integer, it signifies the number of samples\n+            to pack in the left dataset. If `None`, it defaults to the complement to `right_size`.\n+        right_size: If float, it should be in range `[0, 1]` range and signifies the fraction of the\n+            data to pack in the right dataset. If integer, it signifies the number of samples\n+            to pack in the right dataset. If `None`, it defaults to the complement to `left_size`.\n+        shuffle: Boolean, whether to shuffle the data before splitting it.\n+        seed: A random seed for shuffling.\n+\n+    Returns:\n+        A tuple of two `tf.data.Dataset` objects: the left and right splits.\n+    \"\"\"\n+\n+    pass\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5bcbc8410a39dd0771fe554965417b1c0f2657a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 46 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 46 | Churn Cumulative: 92 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,46 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Keras audio dataset loading utilities.\"\"\"\n-\n-import tensorflow.compat.v2 as tf #type: ignore\n-\n-# pylint: disable=g-classes-have-attributes\n-\n-import numpy as np\n-\n-from keras.utils import dataset_utils\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-\n-def split_dataset(dataset, left_size=None, right_size=None, shuffle=False, seed=None):\n-    \"\"\"Split a dataset into a left half and a right half (e.g. training / validation).\n-    \n-    Args:\n-        dataset: A `tf.data.Dataset` object or a list/tuple of arrays with the same length.\n-        left_size: If float, it should be in range `[0, 1]` range and signifies the fraction of the\n-            data to pack in the left dataset. If integer, it signifies the number of samples\n-            to pack in the left dataset. If `None`, it defaults to the complement to `right_size`.\n-        right_size: If float, it should be in range `[0, 1]` range and signifies the fraction of the\n-            data to pack in the right dataset. If integer, it signifies the number of samples\n-            to pack in the right dataset. If `None`, it defaults to the complement to `left_size`.\n-        shuffle: Boolean, whether to shuffle the data before splitting it.\n-        seed: A random seed for shuffling.\n-\n-    Returns:\n-        A tuple of two `tf.data.Dataset` objects: the left and right splits.\n-    \"\"\"\n-\n-    pass\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c2d41478ca78ce282de3d1e738e97fdc40d9379", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 84/84 | Churn Δ: 122 | Churn Cumulative: 122 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,13 +14,135 @@\n # ==============================================================================\n \"\"\"Keras image dataset loading utilities.\"\"\"\n \n+\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-classes-have-attributes\n \n import multiprocessing\n import os\n+import time\n \n import numpy as np\n+from tensorflow.python.util.tf_export import keras_export\n+\n+@keras_export('keras.utils.split_dataset')\n+def split_dataset(dataset, \n+                  left_size=None, \n+                  right_size=None, \n+                  shuffle=False, \n+                  seed=None):\n+  \"\"\"Split a dataset into a left half and a right half (e.g. training / validation).\n+  \n+  Args:\n+      dataset: A `tf.data.Dataset` object or \n+               a list/tuple of arrays with the same length.   \n+      left_size: If float, it should be in range `[0, 1]` range \n+                 and signifies the fraction of the data to pack in \n+                 the left dataset. If integer, it signifies the number \n+                 of samples to pack in the left dataset. \n+                 If `None`, it defaults to the complement to `right_size`.       \n+      right_size: If float, it should be in range `[0, 1]` range \n+                  and signifies the fraction of the data to pack \n+                  in the right dataset. If integer, it signifies \n+                  the number of samples to pack in the right dataset.\n+                  If `None`, it defaults to the complement to `left_size`.     \n+      shuffle: Boolean, whether to shuffle the data before splitting it.\n+      seed: A random seed for shuffling.\n+\n+  Returns:\n+      A tuple of two `tf.data.Dataset` objects: the left and right splits.\n+  \"\"\"\n+  \n+  if dataset and not isinstance(dataset,(tf.data.Dataset,list)):\n+    raise TypeError('`dataset` must be either a tf.data.Dataset object'\n+                     'or a list/tuple of arrays',f'Got {type(dataset)}')\n+    \n+  dataset_as_list = []\n+  data_size_warning_flag = False\n+  start_time = time.time()\n+  \n+  for datum in list(dataset):\n+    cur_time = time.time()\n+    # warns user if the dataset is too large to iterate within 10s\n+    if int(cur_time - start_time) > 10 and not data_size_warning_flag:\n+      Warning('Takes too long to process the dataset,'\n+              'the utility is only meant for small datasets that fit in memory')\n+      data_size_warning_flag = True\n+    dataset_as_list.append(datum)\n+  \n+  if shuffle: \n+    if seed:\n+      np.random.seed(seed)\n+    np.random.shuffle(dataset)\n+    \n+  total_size = len(dataset_as_list)\n+   \n+  if right_size is None and left_size is None:\n+    raise ValueError('Both `left_size` and `right_size`cannot be `None`'\n+                     ' either one of them must specified')\n+    \n+  # if left_size is None, it defaults to the complement to right_size\n+  # raises error if right_size not in range [0, 1] or [0, total_size]\n+  elif left_size is None and right_size:\n+    if type(right_size) == float:\n+      if right_size < 0 or right_size > 1:\n+        raise ValueError('`right_size` must be in range `[0, 1]`')\n+      right_size = int(right_size*total_size)\n+      left_size = total_size - right_size\n+    elif type(right_size) == int:\n+      if right_size < 0 or right_size > total_size:\n+        raise ValueError('`right_size` must be in range `[0, 'f'{total_size}]`')\n+      left_size = total_size - right_size\n+      \n+  # if right_size is None, it defaults to the complement to left_size\n+  # raise error if left_size  not in range [0, 1] or [0, total_size]\n+  elif left_size and right_size is None:\n+    if type(left_size) == float:\n+      if left_size < 0 or left_size > 1:\n+        raise ValueError('`left_size` must be in range `[0, 1]`')\n+      left_size = int(left_size*total_size)\n+      right_size = total_size - left_size\n+    elif type(left_size) == int:\n+      if left_size < 0 or left_size > total_size:\n+        raise ValueError('`left_size` must be in range `[0, 'f'{total_size}]`')\n+      right_size = total_size - left_size\n+    \n+  # if both left and right sizes are specified, \n+  # raise error if they are not in range [0, 1] or [0, total_size]\n+  elif left_size and right_size:\n+    if type(left_size) == int and type(right_size) == int:\n+      if left_size < 0 or left_size > total_size:\n+        raise ValueError('`left_size` must be in range `[0, 'f'{total_size}]`')\n+      elif right_size < 0 or right_size > total_size:\n+        raise ValueError('`right_size` must be in range `[0, 'f'{total_size}]`')\n+      elif left_size + right_size != total_size:\n+        raise ValueError('The sum of `left_size` and `right_size`'\n+                         'must be equal to the total size of the dataset.')\n+    elif type(left_size) == float and type(right_size) == float:\n+      if left_size < 0 or left_size > 1:\n+        raise ValueError('`left_size` must be in range `[0, 1]`')\n+      elif right_size < 0 or right_size > 1:\n+        raise ValueError('`right_size` must be in range `[0, 1]`')\n+      elif left_size + right_size != 1:\n+        raise ValueError('The sum of `left_size` and `right_size`'\n+                         'must be equal to 1.')\n+      left_size = int(left_size*total_size)\n+      right_size = int(right_size*total_size)\n+    else:\n+      raise ValueError('`left_size` and `right_size` must be either '\n+                       'both floats or both integers.')\n+  \n+  left_dataset = dataset_as_list[:int(left_size)]\n+  right_dataset = dataset_as_list[int(-1*right_size):]\n+  \n+  left_dataset = tf.data.Dataset.from_tensor_slices(left_dataset)\n+  right_dataset = tf.data.Dataset.from_tensor_slices(right_dataset)\n+  \n+  left_dataset = left_dataset.prefetch(tf.data.AUTOTUNE)\n+  right_dataset = right_dataset.prefetch(tf.data.AUTOTUNE)\n+  \n+  return left_dataset, right_dataset\n+\n   \n \n def index_directory(directory,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
