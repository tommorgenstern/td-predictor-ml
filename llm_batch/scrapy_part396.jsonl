{"custom_id": "scrapy#ca345a3b73904ffd6d2e8ffb17c45ebb69639d26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/2 | Churn Δ: 74 | Churn Cumulative: 2050 | Contributors (this commit): 75 | Commits (past 90d): 11 | Contributors (cumulative): 112 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,8 @@ These exceptions are documented in docs/topics/exceptions.rst. Please don't add\n new exceptions here without documenting them there.\n \"\"\"\n \n+from __future__ import annotations\n+\n from typing import Any\n \n # Internal\n@@ -58,6 +60,10 @@ class StopDownload(Exception):\n class DropItem(Exception):\n     \"\"\"Drop item from the item pipeline\"\"\"\n \n+    def __init__(self, message: str, log_level: str | None = None):\n+        super().__init__(message)\n+        self.log_level = log_level\n+\n \n class NotSupported(Exception):\n     \"\"\"Indicates a feature or method is not supported\"\"\"\n\n@@ -120,8 +120,12 @@ class LogFormatter:\n         spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n+        if (level := getattr(exception, \"log_level\", None)) is None:\n+            level = spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"]\n+        if isinstance(level, str):\n+            level = getattr(logging, level)\n         return {\n-            \"level\": logging.WARNING,\n+            \"level\": level,\n             \"msg\": DROPPEDMSG,\n             \"args\": {\n                 \"exception\": exception,\n\n@@ -49,6 +49,8 @@ CONCURRENT_REQUESTS_PER_IP = 0\n COOKIES_ENABLED = True\n COOKIES_DEBUG = False\n \n+DEFAULT_DROPITEM_LOG_LEVEL = \"WARNING\"\n+\n DEFAULT_ITEM_CLASS = \"scrapy.item.Item\"\n \n DEFAULT_REQUEST_HEADERS = {\n\n@@ -1,5 +1,7 @@\n+import logging\n import unittest\n \n+import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.python.failure import Failure\n@@ -26,6 +28,7 @@ class LogFormatterTestCase(unittest.TestCase):\n     def setUp(self):\n         self.formatter = LogFormatter()\n         self.spider = Spider(\"default\")\n+        self.spider.crawler = get_crawler()\n \n     def test_crawled_with_referer(self):\n         req = Request(\"http://www.example.com\")\n@@ -68,6 +71,62 @@ class LogFormatterTestCase(unittest.TestCase):\n         assert all(isinstance(x, str) for x in lines)\n         self.assertEqual(lines, [\"Dropped: \\u2018\", \"{}\"])\n \n+    def test_dropitem_default_log_level(self):\n+        item = {}\n+        exception = DropItem(\"Test drop\")\n+        response = Response(\"http://www.example.com\")\n+        spider = Spider(\"foo\")\n+        spider.crawler = get_crawler(Spider)\n+\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.WARNING)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = logging.INFO\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.INFO)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = \"INFO\"\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.INFO)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = 10\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.DEBUG)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = 0\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.NOTSET)\n+\n+        unsupported_value = object()\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = unsupported_value\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], unsupported_value)\n+\n+        with pytest.raises(TypeError):\n+            logging.log(logkws[\"level\"], \"message\")\n+\n+    def test_dropitem_custom_log_level(self):\n+        item = {}\n+        response = Response(\"http://www.example.com\")\n+\n+        exception = DropItem(\"Test drop\", log_level=\"INFO\")\n+        logkws = self.formatter.dropped(item, exception, response, self.spider)\n+        self.assertEqual(logkws[\"level\"], logging.INFO)\n+\n+        exception = DropItem(\"Test drop\", log_level=\"ERROR\")\n+        logkws = self.formatter.dropped(item, exception, response, self.spider)\n+        self.assertEqual(logkws[\"level\"], logging.ERROR)\n+\n     def test_item_error(self):\n         # In practice, the complete traceback is shown by passing the\n         # 'exc_info' argument to the logging function\n@@ -145,6 +204,7 @@ class LogformatterSubclassTest(LogFormatterTestCase):\n     def setUp(self):\n         self.formatter = LogFormatterSubclass()\n         self.spider = Spider(\"default\")\n+        self.spider.crawler = get_crawler(Spider)\n \n     def test_crawled_with_referer(self):\n         req = Request(\"http://www.example.com\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d7168577b859d15a15c2bcbc6c4f607a62be7478", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 445 | Contributors (this commit): 25 | Commits (past 90d): 6 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -117,13 +117,6 @@ html_theme = \"sphinx_rtd_theme\"\n # documentation.\n # html_theme_options = {}\n \n-# Add any paths that contain custom themes here, relative to this directory.\n-# Add path to the RTD explicitly to robustify builds (otherwise might\n-# fail in a clean Debian build env)\n-import sphinx_rtd_theme\n-\n-html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n-\n # The style sheet to use for HTML and HTML Help pages. A file of that name\n # must exist either in Sphinx' static/ path, or in one of the custom paths\n # given in html_static_path.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0c828b7f665d8c82e17787996634bc072e416ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 179 | Files Changed: 2 | Hunks: 26 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/3 | Churn Δ: 229 | Churn Cumulative: 674 | Contributors (this commit): 25 | Commits (past 90d): 8 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,18 @@\n+\"\"\"\n+Must be included after 'sphinx.ext.autodoc'. Fixes unwanted 'alias of' behavior.\n+https://github.com/sphinx-doc/sphinx/issues/4422\n+\"\"\"\n+\n+# pylint: disable=import-error\n+from sphinx.application import Sphinx\n+\n+\n+def maybe_skip_member(app: Sphinx, what, name: str, obj, skip: bool, options) -> bool:\n+    if not skip:\n+        # autodocs was generating a text \"alias of\" for the following members\n+        return name in {\"default_item_class\", \"default_selector_class\"}\n+    return skip\n+\n+\n+def setup(app: Sphinx) -> None:\n+    app.connect(\"autodoc-skip-member\", maybe_skip_member)\n\n@@ -1,17 +1,12 @@\n-# Scrapy documentation build configuration file, created by\n-# sphinx-quickstart on Mon Nov 24 12:02:52 2008.\n+# Configuration file for the Sphinx documentation builder.\n #\n-# This file is execfile()d with the current directory set to its containing dir.\n-#\n-# The contents of this file are pickled, so don't put values in the namespace\n-# that aren't pickleable (module imports are okay, they're removed automatically).\n-#\n-# All configuration values have a default; values that are commented out\n-# serve to show the default.\n+# For the full list of built-in configuration values, see the documentation:\n+# https://www.sphinx-doc.org/en/master/usage/configuration.html\n \n # pylint: disable=import-error\n import os\n import sys\n+from collections.abc import Sequence\n from pathlib import Path\n \n # If your extensions are in another directory, add it here. If the directory\n@@ -20,36 +15,30 @@ sys.path.append(str(Path(__file__).parent / \"_ext\"))\n sys.path.insert(0, str(Path(__file__).parent.parent))\n \n \n-# General configuration\n-# ---------------------\n+# -- Project information -----------------------------------------------------\n+# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n+\n+project = \"Scrapy\"\n+project_copyright = \"Scrapy developers\"\n+author = \"Scrapy developers\"\n+\n+\n+# -- General configuration ---------------------------------------------------\n+# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n \n-# Add any Sphinx extension module names here, as strings. They can be extensions\n-# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n extensions = [\n     \"hoverxref.extension\",\n     \"notfound.extension\",\n     \"scrapydocs\",\n     \"sphinx.ext.autodoc\",\n+    \"scrapyfixautodoc\",  # Must be after \"sphinx.ext.autodoc\"\n     \"sphinx.ext.coverage\",\n     \"sphinx.ext.intersphinx\",\n     \"sphinx.ext.viewcode\",\n ]\n \n-# Add any paths that contain templates here, relative to this directory.\n templates_path = [\"_templates\"]\n-\n-# The suffix of source filenames.\n-source_suffix = \".rst\"\n-\n-# The encoding of source files.\n-# source_encoding = 'utf-8'\n-\n-# The master toctree document.\n-master_doc = \"index\"\n-\n-# General information about the project.\n-project = \"Scrapy\"\n-copyright = \"Scrapy developers\"\n+exclude_patterns = [\"build\", \"Thumbs.db\", \".DS_Store\"]\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n@@ -65,118 +54,17 @@ except ImportError:\n     version = \"\"\n     release = \"\"\n \n-# The language for content autogenerated by Sphinx. Refer to documentation\n-# for a list of supported languages.\n-language = \"en\"\n-\n-# There are two options for replacing |today|: either, you set today to some\n-# non-false value, then it is used:\n-# today = ''\n-# Else, today_fmt is used as the format for a strftime call.\n-# today_fmt = '%B %d, %Y'\n-\n-# List of documents that shouldn't be included in the build.\n-# unused_docs = []\n-\n-exclude_patterns = [\"build\"]\n-\n-# List of directories, relative to source directory, that shouldn't be searched\n-# for source files.\n-exclude_trees = [\".build\"]\n-\n-# The reST default role (used for this markup: `text`) to use for all documents.\n-# default_role = None\n-\n-# If true, '()' will be appended to :func: etc. cross-reference text.\n-# add_function_parentheses = True\n-\n-# If true, the current module name will be prepended to all description\n-# unit titles (such as .. function::).\n-# add_module_names = True\n-\n-# If true, sectionauthor and moduleauthor directives will be shown in the\n-# output. They are ignored by default.\n-# show_authors = False\n-\n-# The name of the Pygments (syntax highlighting) style to use.\n-pygments_style = \"sphinx\"\n-\n-# List of Sphinx warnings that will not be raised\n suppress_warnings = [\"epub.unknown_project_files\"]\n \n \n-# Options for HTML output\n-# -----------------------\n+# -- Options for HTML output -------------------------------------------------\n+# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n \n-# The theme to use for HTML and HTML Help pages.  See the documentation for\n-# a list of builtin themes.\n html_theme = \"sphinx_rtd_theme\"\n-\n-# Theme options are theme-specific and customize the look and feel of a theme\n-# further.  For a list of options available for each theme, see the\n-# documentation.\n-# html_theme_options = {}\n-\n-# The style sheet to use for HTML and HTML Help pages. A file of that name\n-# must exist either in Sphinx' static/ path, or in one of the custom paths\n-# given in html_static_path.\n-# html_style = 'scrapydoc.css'\n-\n-# The name for this set of Sphinx documents.  If None, it defaults to\n-# \"<project> v<release> documentation\".\n-# html_title = None\n-\n-# A shorter title for the navigation bar.  Default is the same as html_title.\n-# html_short_title = None\n-\n-# The name of an image file (relative to this directory) to place at the top\n-# of the sidebar.\n-# html_logo = None\n-\n-# The name of an image file (within the static path) to use as favicon of the\n-# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n-# pixels large.\n-# html_favicon = None\n-\n-# Add any paths that contain custom static files (such as style sheets) here,\n-# relative to this directory. They are copied after the builtin static files,\n-# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n html_static_path = [\"_static\"]\n \n-# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n-# using the given strftime format.\n html_last_updated_fmt = \"%b %d, %Y\"\n \n-# Custom sidebar templates, maps document names to template names.\n-# html_sidebars = {}\n-\n-# Additional templates that should be rendered to pages, maps page names to\n-# template names.\n-# html_additional_pages = {}\n-\n-# If false, no module index is generated.\n-# html_use_modindex = True\n-\n-# If false, no index is generated.\n-# html_use_index = True\n-\n-# If true, the index is split into individual pages for each letter.\n-# html_split_index = False\n-\n-# If true, the reST sources are included in the HTML build as _sources/<name>.\n-html_copy_source = True\n-\n-# If true, an OpenSearch description file will be output, and all pages will\n-# contain a <link> tag referring to it.  The value of this option must be the\n-# base URL from which the finished HTML is served.\n-# html_use_opensearch = ''\n-\n-# If nonempty, this is the file name suffix for HTML files (e.g. \".xhtml\").\n-# html_file_suffix = ''\n-\n-# Output file base name for HTML help builder.\n-htmlhelp_basename = \"Scrapydoc\"\n-\n html_css_files = [\n     \"custom.css\",\n ]\n@@ -184,14 +72,8 @@ html_css_files = [\n # Set canonical URL from the Read the Docs Domain\n html_baseurl = os.environ.get(\"READTHEDOCS_CANONICAL_URL\", \"\")\n \n-# Options for LaTeX output\n-# ------------------------\n-\n-# The paper size ('letter' or 'a4').\n-# latex_paper_size = 'letter'\n-\n-# The font size ('10pt', '11pt' or '12pt').\n-# latex_font_size = '10pt'\n+# -- Options for LaTeX output ------------------------------------------------\n+# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-latex-output\n \n # Grouping the document tree into LaTeX files. List of tuples\n # (source start file, target name, title, author, document class [howto/manual]).\n@@ -199,39 +81,22 @@ latex_documents = [\n     (\"index\", \"Scrapy.tex\", \"Scrapy Documentation\", \"Scrapy developers\", \"manual\"),\n ]\n \n-# The name of an image file (relative to this directory) to place at the top of\n-# the title page.\n-# latex_logo = None\n \n-# For \"manual\" documents, if this is true, then toplevel headings are parts,\n-# not chapters.\n-# latex_use_parts = False\n+# -- Options for the linkcheck builder ---------------------------------------\n+# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-the-linkcheck-builder\n \n-# Additional stuff for the LaTeX preamble.\n-# latex_preamble = ''\n-\n-# Documents to append as an appendix to all manuals.\n-# latex_appendices = []\n-\n-# If false, no module index is generated.\n-# latex_use_modindex = True\n-\n-\n-# Options for the linkcheck builder\n-# ---------------------------------\n-\n-# A list of regular expressions that match URIs that should not be checked when\n-# doing a linkcheck build.\n linkcheck_ignore = [\n     r\"http://localhost:\\d+\",\n     \"http://hg.scrapy.org\",\n-    \"http://directory.google.com/\",\n+    r\"https://github.com/scrapy/scrapy/commit/\\w+\",\n     r\"https://github.com/scrapy/scrapy/issues/\\d+\",\n ]\n \n+linkcheck_anchors_ignore_for_url = [\"https://github.com/pyca/cryptography/issues/2692\"]\n+\n+# -- Options for the Coverage extension --------------------------------------\n+# https://www.sphinx-doc.org/en/master/usage/extensions/coverage.html#configuration\n \n-# Options for the Coverage extension\n-# ----------------------------------\n coverage_ignore_pyobjects = [\n     # Contract’s add_pre_hook and add_post_hook are not documented because\n     # they should be transparent to contract developers, for whom pre_hook and\n@@ -272,8 +137,8 @@ coverage_ignore_pyobjects = [\n ]\n \n \n-# Options for the InterSphinx extension\n-# -------------------------------------\n+# -- Options for the InterSphinx extension -----------------------------------\n+# https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#configuration\n \n intersphinx_mapping = {\n     \"attrs\": (\"https://www.attrs.org/en/stable/\", None),\n@@ -290,11 +155,11 @@ intersphinx_mapping = {\n     \"twistedapi\": (\"https://docs.twisted.org/en/stable/api/\", None),\n     \"w3lib\": (\"https://w3lib.readthedocs.io/en/latest\", None),\n }\n-intersphinx_disabled_reftypes = []\n+intersphinx_disabled_reftypes: Sequence[str] = []\n \n \n-# Options for sphinx-hoverxref options\n-# ------------------------------------\n+# -- Options for sphinx-hoverxref extension ----------------------------------\n+# https://sphinx-hoverxref.readthedocs.io/en/latest/configuration.html\n \n hoverxref_auto_ref = True\n hoverxref_role_types = {\n@@ -309,15 +174,3 @@ hoverxref_role_types = {\n     \"signal\": \"tooltip\",\n }\n hoverxref_roles = [\"command\", \"reqmeta\", \"setting\", \"signal\"]\n-\n-\n-def setup(app):\n-    app.connect(\"autodoc-skip-member\", maybe_skip_member)\n-\n-\n-def maybe_skip_member(app, what, name, obj, skip, options):\n-    if not skip:\n-        # autodocs was generating a text \"alias of\" for the following members\n-        # https://github.com/sphinx-doc/sphinx/issues/4422\n-        return name in {\"default_item_class\", \"default_selector_class\"}\n-    return skip\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#14219b1fca86f2229d2b69455bdbb4eb952cf504", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 7717 | Contributors (this commit): 46 | Commits (past 90d): 7 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2622,18 +2622,24 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             stubs = []\n \n             def open(self, *args, **kwargs):\n+                from botocore import __version__ as botocore_version\n                 from botocore.stub import ANY, Stubber\n+                from packaging.version import Version\n+\n+                expected_params = {\n+                    \"Body\": ANY,\n+                    \"Bucket\": bucket,\n+                    \"Key\": ANY,\n+                }\n+                if Version(botocore_version) >= Version(\"1.36.0\"):\n+                    expected_params[\"ChecksumAlgorithm\"] = ANY\n \n                 stub = Stubber(self.s3_client)\n                 stub.activate()\n                 CustomS3FeedStorage.stubs.append(stub)\n                 stub.add_response(\n                     \"put_object\",\n-                    expected_params={\n-                        \"Body\": ANY,\n-                        \"Bucket\": bucket,\n-                        \"Key\": ANY,\n-                    },\n+                    expected_params=expected_params,\n                     service_response={},\n                 )\n                 return super().open(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9bc0029d27d8ed719e2cc2e9077a81450b995b96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 1 | Files Changed: 5 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 46 | Churn Cumulative: 3912 | Contributors (this commit): 42 | Commits (past 90d): 12 | Contributors (cumulative): 60 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import Settings\n+    from scrapy.settings import BaseSettings, Settings\n \n \n logger = logging.getLogger(__name__)\n@@ -36,6 +36,7 @@ class AddonManager:\n             try:\n                 addoncls = load_object(clspath)\n                 addon = build_from_crawler(addoncls, self.crawler)\n+                if hasattr(addon, \"update_settings\"):\n                     addon.update_settings(settings)\n                 self.addons.append(addon)\n             except NotConfigured as e:\n@@ -52,3 +53,20 @@ class AddonManager:\n             },\n             extra={\"crawler\": self.crawler},\n         )\n+\n+    @classmethod\n+    def load_pre_crawler_settings(cls, settings: BaseSettings):\n+        \"\"\"Update early settings that do not require a crawler instance, such as SPIDER_MODULES.\n+\n+        Similar to the load_settings method, this loads each add-on configured in the\n+        ``ADDONS`` setting and calls their 'update_pre_crawler_settings' class method if present.\n+        This method doesn't have access to the crawler instance or the addons list.\n+\n+        :param settings: The :class:`~scrapy.settings.BaseSettings` object from \\\n+            which to read the early add-on configuration\n+        :type settings: :class:`~scrapy.settings.Settings`\n+        \"\"\"\n+        for clspath in build_component_list(settings[\"ADDONS\"]):\n+            addoncls = load_object(clspath)\n+            if hasattr(addoncls, \"update_pre_crawler_settings\"):\n+                addoncls.update_pre_crawler_settings(settings)\n\n@@ -292,6 +292,7 @@ class CrawlerRunner:\n     def __init__(self, settings: dict[str, Any] | Settings | None = None):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n+        AddonManager.load_pre_crawler_settings(settings)\n         self.settings: Settings = settings\n         self.spider_loader: SpiderLoader = self._get_spider_loader(settings)\n         self._crawlers: set[Crawler] = set()\n\n@@ -97,6 +97,25 @@ class SpiderLoaderTest(unittest.TestCase):\n         self.spider_loader = SpiderLoader.from_settings(settings)\n         assert len(self.spider_loader._spiders) == 0\n \n+    def test_load_spider_module_from_addons(self):\n+        module = \"tests.test_spiderloader.spiders_from_addons.spider0\"\n+\n+        class SpiderModuleAddon:\n+            @classmethod\n+            def update_pre_crawler_settings(cls, settings):\n+                settings.set(\n+                    \"SPIDER_MODULES\",\n+                    [module],\n+                    \"project\",\n+                )\n+\n+        runner = CrawlerRunner({\"ADDONS\": {SpiderModuleAddon: 1}})\n+\n+        crawler = runner.create_crawler(\"spider_from_addon\")\n+        self.assertTrue(issubclass(crawler.spidercls, scrapy.Spider))\n+        self.assertEqual(crawler.spidercls.name, \"spider_from_addon\")\n+        self.assertTrue(len(crawler.settings[\"SPIDER_MODULES\"]) == 1)\n+\n     def test_crawler_runner_loading(self):\n         module = \"tests.test_spiderloader.test_spiders.spider1\"\n         runner = CrawlerRunner(\n\n\n@@ -0,0 +1,6 @@\n+from scrapy.spiders import Spider\n+\n+\n+class SpiderFromAddon(Spider):\n+    name = \"spider_from_addon\"\n+    allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e61ff352439d3e5c85785fc26b5503e4fed67b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 60 | Files Changed: 1 | Hunks: 21 | Methods Changed: 22 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 130 | Churn Cumulative: 410 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,63 +1,67 @@\n # pylint: disable=import-error\n+from collections.abc import Sequence\n from operator import itemgetter\n+from typing import Any, TypedDict\n \n from docutils import nodes\n+from docutils.nodes import Element, General, Node, document\n from docutils.parsers.rst import Directive\n-from docutils.parsers.rst.roles import set_classes\n+from sphinx.application import Sphinx\n from sphinx.util.nodes import make_refnode\n \n \n-class settingslist_node(nodes.General, nodes.Element):\n+class SettingData(TypedDict):\n+    docname: str\n+    setting_name: str\n+    refid: str\n+\n+\n+class SettingslistNode(General, Element):\n     pass\n \n \n class SettingsListDirective(Directive):\n-    def run(self):\n-        return [settingslist_node(\"\")]\n+    def run(self) -> Sequence[Node]:\n+        return [SettingslistNode()]\n \n \n-def is_setting_index(node):\n-    if node.tagname == \"index\" and node[\"entries\"]:\n+def is_setting_index(node: Node) -> bool:\n+    if node.tagname == \"index\" and node[\"entries\"]:  # type: ignore[index,attr-defined]\n         # index entries for setting directives look like:\n         # [('pair', 'SETTING_NAME; setting', 'std:setting-SETTING_NAME', '')]\n-        entry_type, info, refid = node[\"entries\"][0][:3]\n+        entry_type, info, refid = node[\"entries\"][0][:3]  # type: ignore[index]\n         return entry_type == \"pair\" and info.endswith(\"; setting\")\n     return False\n \n \n-def get_setting_target(node):\n-    # target nodes are placed next to the node in the doc tree\n-    return node.parent[node.parent.index(node) + 1]\n-\n-\n-def get_setting_name_and_refid(node):\n+def get_setting_name_and_refid(node: Node) -> tuple[str, str]:\n     \"\"\"Extract setting name from directive index node\"\"\"\n-    entry_type, info, refid = node[\"entries\"][0][:3]\n+    entry_type, info, refid = node[\"entries\"][0][:3]  # type: ignore[index]\n     return info.replace(\"; setting\", \"\"), refid\n \n \n-def collect_scrapy_settings_refs(app, doctree):\n+def collect_scrapy_settings_refs(app: Sphinx, doctree: document) -> None:\n     env = app.builder.env\n \n     if not hasattr(env, \"scrapy_all_settings\"):\n-        env.scrapy_all_settings = []\n-\n-    for node in doctree.traverse(is_setting_index):\n-        targetnode = get_setting_target(node)\n-        assert isinstance(targetnode, nodes.target), \"Next node is not a target\"\n+        emptyList: list[SettingData] = []\n+        env.scrapy_all_settings = emptyList  # type: ignore[attr-defined]\n \n+    for node in doctree.findall(is_setting_index):\n         setting_name, refid = get_setting_name_and_refid(node)\n \n-        env.scrapy_all_settings.append(\n-            {\n-                \"docname\": env.docname,\n-                \"setting_name\": setting_name,\n-                \"refid\": refid,\n-            }\n+        env.scrapy_all_settings.append(  # type: ignore[attr-defined]\n+            SettingData(\n+                docname=env.docname,\n+                setting_name=setting_name,\n+                refid=refid,\n+            )\n         )\n \n \n-def make_setting_element(setting_data, app, fromdocname):\n+def make_setting_element(\n+    setting_data: SettingData, app: Sphinx, fromdocname: str\n+) -> Any:\n     refnode = make_refnode(\n         app.builder,\n         fromdocname,\n@@ -73,22 +77,56 @@ def make_setting_element(setting_data, app, fromdocname):\n     return item\n \n \n-def replace_settingslist_nodes(app, doctree, fromdocname):\n+def replace_settingslist_nodes(\n+    app: Sphinx, doctree: document, fromdocname: str\n+) -> None:\n     env = app.builder.env\n \n-    for node in doctree.traverse(settingslist_node):\n+    for node in doctree.findall(SettingslistNode):\n         settings_list = nodes.bullet_list()\n         settings_list.extend(\n             [\n                 make_setting_element(d, app, fromdocname)\n-                for d in sorted(env.scrapy_all_settings, key=itemgetter(\"setting_name\"))\n+                for d in sorted(env.scrapy_all_settings, key=itemgetter(\"setting_name\"))  # type: ignore[attr-defined]\n                 if fromdocname != d[\"docname\"]\n             ]\n         )\n         node.replace_self(settings_list)\n \n \n-def setup(app):\n+def source_role(\n+    name, rawtext, text: str, lineno, inliner, options=None, content=None\n+) -> tuple[list[Any], list[Any]]:\n+    ref = \"https://github.com/scrapy/scrapy/blob/master/\" + text\n+    node = nodes.reference(rawtext, text, refuri=ref, **options)\n+    return [node], []\n+\n+\n+def issue_role(\n+    name, rawtext, text: str, lineno, inliner, options=None, content=None\n+) -> tuple[list[Any], list[Any]]:\n+    ref = \"https://github.com/scrapy/scrapy/issues/\" + text\n+    node = nodes.reference(rawtext, \"issue \" + text, refuri=ref)\n+    return [node], []\n+\n+\n+def commit_role(\n+    name, rawtext, text: str, lineno, inliner, options=None, content=None\n+) -> tuple[list[Any], list[Any]]:\n+    ref = \"https://github.com/scrapy/scrapy/commit/\" + text\n+    node = nodes.reference(rawtext, \"commit \" + text, refuri=ref)\n+    return [node], []\n+\n+\n+def rev_role(\n+    name, rawtext, text: str, lineno, inliner, options=None, content=None\n+) -> tuple[list[Any], list[Any]]:\n+    ref = \"http://hg.scrapy.org/scrapy/changeset/\" + text\n+    node = nodes.reference(rawtext, \"r\" + text, refuri=ref)\n+    return [node], []\n+\n+\n+def setup(app: Sphinx) -> None:\n     app.add_crossref_type(\n         directivename=\"setting\",\n         rolename=\"setting\",\n@@ -114,36 +152,8 @@ def setup(app):\n     app.add_role(\"issue\", issue_role)\n     app.add_role(\"rev\", rev_role)\n \n-    app.add_node(settingslist_node)\n+    app.add_node(SettingslistNode)\n     app.add_directive(\"settingslist\", SettingsListDirective)\n \n     app.connect(\"doctree-read\", collect_scrapy_settings_refs)\n     app.connect(\"doctree-resolved\", replace_settingslist_nodes)\n-\n-\n-def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n-    ref = \"https://github.com/scrapy/scrapy/blob/master/\" + text\n-    set_classes(options)\n-    node = nodes.reference(rawtext, text, refuri=ref, **options)\n-    return [node], []\n-\n-\n-def issue_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n-    ref = \"https://github.com/scrapy/scrapy/issues/\" + text\n-    set_classes(options)\n-    node = nodes.reference(rawtext, \"issue \" + text, refuri=ref, **options)\n-    return [node], []\n-\n-\n-def commit_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n-    ref = \"https://github.com/scrapy/scrapy/commit/\" + text\n-    set_classes(options)\n-    node = nodes.reference(rawtext, \"commit \" + text, refuri=ref, **options)\n-    return [node], []\n-\n-\n-def rev_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n-    ref = \"http://hg.scrapy.org/scrapy/changeset/\" + text\n-    set_classes(options)\n-    node = nodes.reference(rawtext, \"r\" + text, refuri=ref, **options)\n-    return [node], []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c03fb2abb8c354c56c4e8363fc602d49f956c280", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 40 | Churn Cumulative: 10258 | Contributors (this commit): 55 | Commits (past 90d): 18 | Contributors (cumulative): 93 | DMM Complexity: 1.0\n\nDIFF:\n@@ -276,7 +276,14 @@ class S3FeedStorage(BlockingFeedStorage):\n \n \n class GCSFeedStorage(BlockingFeedStorage):\n-    def __init__(self, uri: str, project_id: str | None, acl: str | None):\n+    def __init__(\n+        self,\n+        uri: str,\n+        project_id: str | None,\n+        acl: str | None,\n+        *,\n+        feed_options: dict[str, Any] | None = None,\n+    ):\n         self.project_id: str | None = project_id\n         self.acl: str | None = acl\n         u = urlparse(uri)\n@@ -284,12 +291,26 @@ class GCSFeedStorage(BlockingFeedStorage):\n         self.bucket_name: str = u.hostname\n         self.blob_name: str = u.path[1:]  # remove first \"/\"\n \n+        if feed_options and feed_options.get(\"overwrite\", True) is False:\n+            logger.warning(\n+                \"GCS does not support appending to files. To \"\n+                \"suppress this warning, remove the overwrite \"\n+                \"option from your FEEDS setting or set it to True.\"\n+            )\n+\n     @classmethod\n-    def from_crawler(cls, crawler: Crawler, uri: str) -> Self:\n+    def from_crawler(\n+        cls,\n+        crawler: Crawler,\n+        uri: str,\n+        *,\n+        feed_options: dict[str, Any] | None = None,\n+    ) -> Self:\n         return cls(\n             uri,\n             crawler.settings[\"GCS_PROJECT_ID\"],\n             crawler.settings[\"FEED_STORAGE_GCS_ACL\"] or None,\n+            feed_options=feed_options,\n         )\n \n     def _store_in_thread(self, file: IO[bytes]) -> None:\n\n@@ -523,6 +523,21 @@ class GCSFeedStorageTest(unittest.TestCase):\n             bucket_mock.blob.assert_called_once_with(\"export.csv\")\n             blob_mock.upload_from_file.assert_called_once_with(f, predefined_acl=acl)\n \n+    def test_overwrite_default(self):\n+        with LogCapture() as log:\n+            GCSFeedStorage(\"gs://mybucket/export.csv\", \"myproject-123\", \"custom-acl\")\n+        self.assertNotIn(\"GCS does not support appending to files\", str(log))\n+\n+    def test_overwrite_false(self):\n+        with LogCapture() as log:\n+            GCSFeedStorage(\n+                \"gs://mybucket/export.csv\",\n+                \"myproject-123\",\n+                \"custom-acl\",\n+                feed_options={\"overwrite\": False},\n+            )\n+        self.assertIn(\"GCS does not support appending to files\", str(log))\n+\n \n class StdoutFeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cec0aeca58730b592bec50299414d4bf30fc9ec0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 149 | Lines Deleted: 167 | Files Changed: 41 | Hunks: 75 | Methods Changed: 67 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 316 | Churn Cumulative: 45273 | Contributors (this commit): 207 | Commits (past 90d): 136 | Contributors (cumulative): 815 | DMM Complexity: 0.7619047619047619\n\nDIFF:\n@@ -188,9 +188,9 @@ class Command(ScrapyCommand):\n                 return True\n             return False\n \n-        assert (\n-            self.crawler_process is not None\n-        ), \"crawler_process must be set before calling run\"\n+        assert self.crawler_process is not None, (\n+            \"crawler_process must be set before calling run\"\n+        )\n \n         try:\n             spidercls = self.crawler_process.spider_loader.load(name)\n\n@@ -34,13 +34,12 @@ class DownloadHandlerProtocol(Protocol):\n class DownloadHandlers:\n     def __init__(self, crawler: Crawler):\n         self._crawler: Crawler = crawler\n-        self._schemes: dict[str, str | Callable[..., Any]] = (\n-            {}\n-        )  # stores acceptable schemes on instancing\n-        self._handlers: dict[str, DownloadHandlerProtocol] = (\n-            {}\n-        )  # stores instanced handlers for schemes\n-        self._notconfigured: dict[str, str] = {}  # remembers failed handlers\n+        # stores acceptable schemes on instancing\n+        self._schemes: dict[str, str | Callable[..., Any]] = {}\n+        # stores instanced handlers for schemes\n+        self._handlers: dict[str, DownloadHandlerProtocol] = {}\n+        # remembers failed handlers\n+        self._notconfigured: dict[str, str] = {}\n         handlers: dict[str, str | Callable[..., Any]] = without_none_values(\n             cast(\n                 \"dict[str, str | Callable[..., Any]]\",\n\n@@ -193,7 +193,7 @@ class Stream:\n             url.netloc == str(self._protocol.metadata[\"uri\"].host, \"utf-8\")\n             or url.netloc == str(self._protocol.metadata[\"uri\"].netloc, \"utf-8\")\n             or url.netloc\n-            == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n+            == f\"{self._protocol.metadata['ip_address']}:{self._protocol.metadata['uri'].port}\"\n         )\n \n     def _get_request_headers(self) -> list[tuple[str, str]]:\n@@ -339,7 +339,7 @@ class Stream:\n         if self._log_warnsize:\n             self.metadata[\"reached_warnsize\"] = True\n             warning_msg = (\n-                f'Received more ({self._response[\"flow_controlled_size\"]}) bytes than download '\n+                f\"Received more ({self._response['flow_controlled_size']}) bytes than download \"\n                 f\"warn size ({self._download_warnsize}) in request {self._request}\"\n             )\n             logger.warning(warning_msg)\n@@ -445,7 +445,7 @@ class Stream:\n                 ResponseFailed(\n                     [\n                         Failure(\n-                            f'Remote peer {self._protocol.metadata[\"ip_address\"]} sent RST_STREAM',\n+                            f\"Remote peer {self._protocol.metadata['ip_address']} sent RST_STREAM\",\n                             ProtocolError,\n                         )\n                     ]\n@@ -465,7 +465,7 @@ class Stream:\n                 InvalidHostname(\n                     self._request,\n                     str(self._protocol.metadata[\"uri\"].host, \"utf-8\"),\n-                    f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}',\n+                    f\"{self._protocol.metadata['ip_address']}:{self._protocol.metadata['uri'].port}\",\n                 )\n             )\n \n\n@@ -54,8 +54,7 @@ class CookiesMiddleware:\n     ) -> None:\n         for cookie in cookies:\n             cookie_domain = cookie.domain\n-            if cookie_domain.startswith(\".\"):\n-                cookie_domain = cookie_domain[1:]\n+            cookie_domain = cookie_domain.removeprefix(\".\")\n \n             hostname = urlparse_cached(request).hostname\n             assert hostname is not None\n\n@@ -89,5 +89,5 @@ class OffsiteMiddleware:\n                 warnings.warn(message)\n             else:\n                 domains.append(re.escape(domain))\n-        regex = rf'^(.*\\.)?({\"|\".join(domains)})$'\n+        regex = rf\"^(.*\\.)?({'|'.join(domains)})$\"\n         return re.compile(regex)\n\n@@ -63,7 +63,9 @@ class RobotsTxtMiddleware:\n         if request.url.startswith(\"data:\") or request.url.startswith(\"file:\"):\n             return None\n         d: Deferred[RobotParser | None] = maybeDeferred(\n-            self.robot_parser, request, spider  # type: ignore[call-overload]\n+            self.robot_parser,\n+            request,\n+            spider,  # type: ignore[call-overload]\n         )\n         d2: Deferred[None] = d.addCallback(self.process_request_2, request, spider)\n         return d2\n\n@@ -19,7 +19,7 @@ if TYPE_CHECKING:\n \n \n def get_header_size(\n-    headers: dict[str, list[str | bytes] | tuple[str | bytes, ...]]\n+    headers: dict[str, list[str | bytes] | tuple[str | bytes, ...]],\n ) -> int:\n     size = 0\n     for key, value in headers.items():\n\n@@ -84,9 +84,7 @@ class TelnetConsole(protocol.ServerFactory):\n             \"\"\"An implementation of IPortal\"\"\"\n \n             @defers\n-            def login(\n-                self_, credentials, mind, *interfaces\n-            ):  # pylint: disable=no-self-argument\n+            def login(self_, credentials, mind, *interfaces):  # pylint: disable=no-self-argument\n                 if not (\n                     credentials.username == self.username.encode(\"utf8\")\n                     and credentials.checkPassword(self.password.encode(\"utf8\"))\n\n@@ -105,7 +105,8 @@ class Headers(CaselessDict):\n \n     def values(self) -> list[bytes | None]:  # type: ignore[override]\n         return [\n-            self[k] for k in self.keys()  # pylint: disable=consider-using-dict-items\n+            self[k]\n+            for k in self.keys()  # pylint: disable=consider-using-dict-items\n         ]\n \n     def to_string(self) -> bytes:\n\n@@ -24,7 +24,6 @@ from scrapy.http.request import Request\n from scrapy.utils.python import is_listlike, to_bytes\n \n if TYPE_CHECKING:\n-\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n\n@@ -94,8 +94,7 @@ class Response(object_ref):\n             return self.request.meta  # type: ignore[union-attr]\n         except AttributeError:\n             raise AttributeError(\n-                \"Response.meta not available, this response \"\n-                \"is not tied to any request\"\n+                \"Response.meta not available, this response is not tied to any request\"\n             )\n \n     @property\n\n@@ -25,7 +25,6 @@ from scrapy.utils.response import get_base_url\n from scrapy.utils.url import url_has_any_extension, url_is_from_any_domain\n \n if TYPE_CHECKING:\n-\n     from lxml.html import HtmlElement\n \n     from scrapy import Selector\n\n@@ -202,7 +202,9 @@ class S3FilesStore:\n         return cast(\n             \"Deferred[dict[str, Any]]\",\n             deferToThread(\n-                self.s3_client.head_object, Bucket=self.bucket, Key=key_name  # type: ignore[attr-defined]\n+                self.s3_client.head_object,  # type: ignore[attr-defined]\n+                Bucket=self.bucket,\n+                Key=key_name,\n             ),\n         )\n \n\n@@ -81,8 +81,7 @@ class Selector(_ParselSelector, object_ref):\n     ):\n         if response is not None and text is not None:\n             raise ValueError(\n-                f\"{self.__class__.__name__}.__init__() received \"\n-                \"both response and text\"\n+                f\"{self.__class__.__name__}.__init__() received both response and text\"\n             )\n \n         st = _st(response, type)\n\n@@ -539,7 +539,7 @@ def iter_default_settings() -> Iterable[tuple[str, Any]]:\n \n \n def overridden_settings(\n-    settings: Mapping[_SettingsKeyT, Any]\n+    settings: Mapping[_SettingsKeyT, Any],\n ) -> Iterable[tuple[str, Any]]:\n     \"\"\"Return an iterable of the settings that have been overridden\"\"\"\n     for name, defvalue in iter_default_settings():\n\n@@ -333,7 +333,7 @@ TEMPLATES_DIR = str((Path(__file__).parent / \"..\" / \"templates\").resolve())\n \n URLLENGTH_LIMIT = 2083\n \n-USER_AGENT = f'Scrapy/{import_module(\"scrapy\").__version__} (+https://scrapy.org)'\n+USER_AGENT = f\"Scrapy/{import_module('scrapy').__version__} (+https://scrapy.org)\"\n \n TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n\n@@ -110,7 +110,7 @@ class OffsiteMiddleware:\n                 warnings.warn(message, PortWarning)\n             else:\n                 domains.append(re.escape(domain))\n-        regex = rf'^(.*\\.)?({\"|\".join(domains)})$'\n+        regex = rf\"^(.*\\.)?({'|'.join(domains)})$\"\n         return re.compile(regex)\n \n     def spider_opened(self, spider: Spider) -> None:\n\n@@ -147,16 +147,24 @@ def _pickle_serialize(obj: Any) -> bytes:\n \n # queue.*Queue aren't subclasses of queue.BaseQueue\n _PickleFifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.FifoDiskQueue), _pickle_serialize, pickle.loads  # type: ignore[arg-type]\n+    _with_mkdir(queue.FifoDiskQueue),  # type: ignore[arg-type]\n+    _pickle_serialize,\n+    pickle.loads,\n )\n _PickleLifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.LifoDiskQueue), _pickle_serialize, pickle.loads  # type: ignore[arg-type]\n+    _with_mkdir(queue.LifoDiskQueue),  # type: ignore[arg-type]\n+    _pickle_serialize,\n+    pickle.loads,\n )\n _MarshalFifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.FifoDiskQueue), marshal.dumps, marshal.loads  # type: ignore[arg-type]\n+    _with_mkdir(queue.FifoDiskQueue),  # type: ignore[arg-type]\n+    marshal.dumps,\n+    marshal.loads,\n )\n _MarshalLifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.LifoDiskQueue), marshal.dumps, marshal.loads  # type: ignore[arg-type]\n+    _with_mkdir(queue.LifoDiskQueue),  # type: ignore[arg-type]\n+    marshal.dumps,\n+    marshal.loads,\n )\n \n # public queue classes\n\n@@ -22,8 +22,7 @@ class DataAction(argparse.Action):\n         option_string: str | None = None,\n     ) -> None:\n         value = str(values)\n-        if value.startswith(\"$\"):\n-            value = value[1:]\n+        value = value.removeprefix(\"$\")\n         setattr(namespace, self.dest, value)\n \n \n@@ -96,7 +95,7 @@ def curl_to_request_kwargs(\n     parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])\n \n     if argv:\n-        msg = f'Unrecognized options: {\", \".join(argv)}'\n+        msg = f\"Unrecognized options: {', '.join(argv)}\"\n         if ignore_unknown_options:\n             warnings.warn(msg)\n         else:\n\n@@ -377,7 +377,7 @@ def deferred_from_coro(o: _T) -> Deferred | _T:\n \n \n def deferred_f_from_coro_f(\n-    coro_f: Callable[_P, Coroutine[Any, Any, _T]]\n+    coro_f: Callable[_P, Coroutine[Any, Any, _T]],\n ) -> Callable[_P, Deferred[_T]]:\n     \"\"\"Converts a coroutine function into a function that returns a Deferred.\n \n\n@@ -71,7 +71,7 @@ def xmliter(obj: Response | str | bytes, nodename: str) -> Iterator[Selector]:\n         nodetext = (\n             document_header\n             + match.group().replace(\n-                nodename, f'{nodename} {\" \".join(namespaces.values())}', 1\n+                nodename, f\"{nodename} {' '.join(namespaces.values())}\", 1\n             )\n             + header_end\n         )\n\n@@ -16,7 +16,6 @@ from scrapy.settings import Settings, _SettingsKeyT\n from scrapy.utils.versions import get_versions\n \n if TYPE_CHECKING:\n-\n     from scrapy.crawler import Crawler\n     from scrapy.logformatter import LogFormatterResult\n \n\n@@ -119,8 +119,7 @@ def to_unicode(\n         return text\n     if not isinstance(text, (bytes, str)):\n         raise TypeError(\n-            \"to_unicode must receive a bytes or str \"\n-            f\"object, got {type(text).__name__}\"\n+            f\"to_unicode must receive a bytes or str object, got {type(text).__name__}\"\n         )\n     if encoding is None:\n         encoding = \"utf-8\"\n@@ -183,7 +182,7 @@ _SelfT = TypeVar(\"_SelfT\")\n \n \n def memoizemethod_noargs(\n-    method: Callable[Concatenate[_SelfT, _P], _T]\n+    method: Callable[Concatenate[_SelfT, _P], _T],\n ) -> Callable[Concatenate[_SelfT, _P], _T]:\n     \"\"\"Decorator to cache the result of a method (without arguments) using a\n     weak reference to its object\n@@ -313,7 +312,7 @@ def without_none_values(iterable: Iterable[_KT]) -> Iterable[_KT]: ...\n \n \n def without_none_values(\n-    iterable: Mapping[_KT, _VT] | Iterable[_KT]\n+    iterable: Mapping[_KT, _VT] | Iterable[_KT],\n ) -> dict[_KT, _VT] | Iterable[_KT]:\n     \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n \n\n@@ -338,9 +338,9 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n             if self.fail_yielding:\n                 2 / 0\n \n-        assert (\n-            self.seedsseen\n-        ), \"All start requests consumed before any download happened\"\n+        assert self.seedsseen, (\n+            \"All start requests consumed before any download happened\"\n+        )\n \n     def parse(self, response):\n         self.seedsseen.append(response.meta.get(\"seed\"))\n\n@@ -529,7 +529,7 @@ class ContractsManagerTest(unittest.TestCase):\n                 return TestItem()\n \n         with MockServer() as mockserver:\n-            contract_doc = f'@url {mockserver.url(\"/status?n=200\")}'\n+            contract_doc = f\"@url {mockserver.url('/status?n=200')}\"\n \n             TestSameUrlSpider.parse_first.__doc__ = contract_doc\n             TestSameUrlSpider.parse_second.__doc__ = contract_doc\n@@ -567,7 +567,6 @@ class CustomFailContractPostProcess(Contract):\n \n \n class CustomContractPrePostProcess(unittest.TestCase):\n-\n     def setUp(self):\n         self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n \n\n@@ -94,7 +94,7 @@ class TestHttpProxyMiddleware(TestCase):\n \n     def test_proxy_auth_encoding(self):\n         # utf-8 encoding\n-        os.environ[\"http_proxy\"] = \"https://m\\u00E1n:pass@proxy:3128\"\n+        os.environ[\"http_proxy\"] = \"https://m\\u00e1n:pass@proxy:3128\"\n         mw = HttpProxyMiddleware(auth_encoding=\"utf-8\")\n         req = Request(\"http://scrapytest.org\")\n         assert mw.process_request(req, spider) is None\n@@ -103,7 +103,7 @@ class TestHttpProxyMiddleware(TestCase):\n \n         # proxy from request.meta\n         req = Request(\n-            \"http://scrapytest.org\", meta={\"proxy\": \"https://\\u00FCser:pass@proxy:3128\"}\n+            \"http://scrapytest.org\", meta={\"proxy\": \"https://\\u00fcser:pass@proxy:3128\"}\n         )\n         assert mw.process_request(req, spider) is None\n         self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n@@ -120,7 +120,7 @@ class TestHttpProxyMiddleware(TestCase):\n \n         # proxy from request.meta, latin-1 encoding\n         req = Request(\n-            \"http://scrapytest.org\", meta={\"proxy\": \"https://\\u00FCser:pass@proxy:3128\"}\n+            \"http://scrapytest.org\", meta={\"proxy\": \"https://\\u00fcser:pass@proxy:3128\"}\n         )\n         assert mw.process_request(req, spider) is None\n         self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n\n@@ -55,12 +55,12 @@ class Base:\n             assert isinstance(req2, Request)\n             self.assertEqual(req2.url, url2)\n             self.assertEqual(req2.method, \"GET\")\n-            assert (\n-                \"Content-Type\" not in req2.headers\n-            ), \"Content-Type header must not be present in redirected request\"\n-            assert (\n-                \"Content-Length\" not in req2.headers\n-            ), \"Content-Length header must not be present in redirected request\"\n+            assert \"Content-Type\" not in req2.headers, (\n+                \"Content-Type header must not be present in redirected request\"\n+            )\n+            assert \"Content-Length\" not in req2.headers, (\n+                \"Content-Length header must not be present in redirected request\"\n+            )\n             assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n \n         def test_max_redirect_times(self):\n@@ -1243,12 +1243,12 @@ class MetaRefreshMiddlewareTest(Base.Test):\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, \"http://example.org/newpage\")\n         self.assertEqual(req2.method, \"GET\")\n-        assert (\n-            \"Content-Type\" not in req2.headers\n-        ), \"Content-Type header must not be present in redirected request\"\n-        assert (\n-            \"Content-Length\" not in req2.headers\n-        ), \"Content-Length header must not be present in redirected request\"\n+        assert \"Content-Type\" not in req2.headers, (\n+            \"Content-Type header must not be present in redirected request\"\n+        )\n+        assert \"Content-Length\" not in req2.headers, (\n+            \"Content-Length header must not be present in redirected request\"\n+        )\n         assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n \n     def test_ignore_tags_default(self):\n\n@@ -93,6 +93,6 @@ def test_params():\n     _, actual = downloader._get_slot(request, spider=None)\n     expected = Slot(**params)\n     for param in params:\n-        assert getattr(expected, param) == getattr(\n-            actual, param\n-        ), f\"Slot.{param}: {getattr(expected, param)!r} != {getattr(actual, param)!r}\"\n+        assert getattr(expected, param) == getattr(actual, param), (\n+            f\"Slot.{param}: {getattr(expected, param)!r} != {getattr(actual, param)!r}\"\n+        )\n\n@@ -294,9 +294,9 @@ class EngineTest(unittest.TestCase):\n         ]\n         urls_visited = {rp[0].url for rp in run.respplug}\n         urls_expected = {run.geturl(p) for p in must_be_visited}\n-        assert (\n-            urls_expected <= urls_visited\n-        ), f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n+        assert urls_expected <= urls_visited, (\n+            f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n+        )\n \n     def _assert_scheduled_requests(self, run: CrawlerRun, count=None):\n         self.assertEqual(count, len(run.reqplug))\n@@ -496,9 +496,9 @@ def test_request_scheduled_signal(caplog):\n     drop_request = Request(\"https://drop.example\")\n     caplog.set_level(DEBUG)\n     engine._schedule_request(drop_request, spider)\n-    assert scheduler.enqueued == [\n-        keep_request\n-    ], f\"{scheduler.enqueued!r} != [{keep_request!r}]\"\n+    assert scheduler.enqueued == [keep_request], (\n+        f\"{scheduler.enqueued!r} != [{keep_request!r}]\"\n+    )\n     crawler.signals.disconnect(signal_handler, request_scheduled)\n \n \n\n@@ -67,6 +67,6 @@ class HeadersReceivedEngineTest(EngineTest):\n         must_be_visited = [\"/\", \"/redirect\", \"/redirected\"]\n         urls_visited = {rp[0].url for rp in run.respplug}\n         urls_expected = {run.geturl(p) for p in must_be_visited}\n-        assert (\n-            urls_expected <= urls_visited\n-        ), f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n+        assert urls_expected <= urls_visited, (\n+            f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n+        )\n\n@@ -390,14 +390,14 @@ class CsvItemExporterTest(BaseItemExporterTest):\n     def test_errors_default(self):\n         with self.assertRaises(UnicodeEncodeError):\n             self.assertExportResult(\n-                item={\"text\": \"W\\u0275\\u200Brd\"},\n+                item={\"text\": \"W\\u0275\\u200brd\"},\n                 expected=None,\n                 encoding=\"windows-1251\",\n             )\n \n     def test_errors_xmlcharrefreplace(self):\n         self.assertExportResult(\n-            item={\"text\": \"W\\u0275\\u200Brd\"},\n+            item={\"text\": \"W\\u0275\\u200brd\"},\n             include_headers_line=False,\n             expected=\"W&#629;&#8203;rd\\r\\n\",\n             encoding=\"windows-1251\",\n\n@@ -1190,8 +1190,7 @@ class FeedExportTest(FeedExportTestBase):\n             \"csv\": b\"baz,egg,foo\\r\\n,spam1,bar1\\r\\n\",\n             \"json\": b'[\\n{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n]',\n             \"jsonlines\": (\n-                b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n'\n-                b'{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n'\n+                b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n'\n             ),\n             \"xml\": (\n                 b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items>\\n<item>'\n@@ -2289,9 +2288,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings.update(\n             {\n                 \"FEEDS\": {\n-                    self._random_temp_filename()\n-                    / \"jl\"\n-                    / self._file_mark: {\"format\": \"jl\"},\n+                    self._random_temp_filename() / \"jl\" / self._file_mark: {\n+                        \"format\": \"jl\"\n+                    },\n                 },\n             }\n         )\n@@ -2311,9 +2310,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings.update(\n             {\n                 \"FEEDS\": {\n-                    self._random_temp_filename()\n-                    / \"csv\"\n-                    / self._file_mark: {\"format\": \"csv\"},\n+                    self._random_temp_filename() / \"csv\" / self._file_mark: {\n+                        \"format\": \"csv\"\n+                    },\n                 },\n             }\n         )\n@@ -2331,9 +2330,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings.update(\n             {\n                 \"FEEDS\": {\n-                    self._random_temp_filename()\n-                    / \"xml\"\n-                    / self._file_mark: {\"format\": \"xml\"},\n+                    self._random_temp_filename() / \"xml\" / self._file_mark: {\n+                        \"format\": \"xml\"\n+                    },\n                 },\n             }\n         )\n@@ -2352,12 +2351,12 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings.update(\n             {\n                 \"FEEDS\": {\n-                    self._random_temp_filename()\n-                    / \"xml\"\n-                    / self._file_mark: {\"format\": \"xml\"},\n-                    self._random_temp_filename()\n-                    / \"json\"\n-                    / self._file_mark: {\"format\": \"json\"},\n+                    self._random_temp_filename() / \"xml\" / self._file_mark: {\n+                        \"format\": \"xml\"\n+                    },\n+                    self._random_temp_filename() / \"json\" / self._file_mark: {\n+                        \"format\": \"json\"\n+                    },\n                 },\n             }\n         )\n@@ -2384,9 +2383,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings.update(\n             {\n                 \"FEEDS\": {\n-                    self._random_temp_filename()\n-                    / \"pickle\"\n-                    / self._file_mark: {\"format\": \"pickle\"},\n+                    self._random_temp_filename() / \"pickle\" / self._file_mark: {\n+                        \"format\": \"pickle\"\n+                    },\n                 },\n             }\n         )\n@@ -2406,9 +2405,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings.update(\n             {\n                 \"FEEDS\": {\n-                    self._random_temp_filename()\n-                    / \"marshal\"\n-                    / self._file_mark: {\"format\": \"marshal\"},\n+                    self._random_temp_filename() / \"marshal\" / self._file_mark: {\n+                        \"format\": \"marshal\"\n+                    },\n                 },\n             }\n         )\n@@ -2455,9 +2454,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         for fmt in (\"json\", \"jsonlines\", \"xml\", \"csv\"):\n             settings = {\n                 \"FEEDS\": {\n-                    self._random_temp_filename()\n-                    / fmt\n-                    / self._file_mark: {\"format\": fmt},\n+                    self._random_temp_filename() / fmt / self._file_mark: {\n+                        \"format\": fmt\n+                    },\n                 },\n                 \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n                 \"FEED_STORE_EMPTY\": False,\n@@ -2478,9 +2477,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         for fmt, expctd in formats:\n             settings = {\n                 \"FEEDS\": {\n-                    self._random_temp_filename()\n-                    / fmt\n-                    / self._file_mark: {\"format\": fmt},\n+                    self._random_temp_filename() / fmt / self._file_mark: {\n+                        \"format\": fmt\n+                    },\n                 },\n                 \"FEED_STORE_EMPTY\": True,\n                 \"FEED_EXPORT_INDENT\": None,\n@@ -2520,25 +2519,19 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n         settings = {\n             \"FEEDS\": {\n-                self._random_temp_filename()\n-                / \"json\"\n-                / self._file_mark: {\n+                self._random_temp_filename() / \"json\" / self._file_mark: {\n                     \"format\": \"json\",\n                     \"indent\": 0,\n                     \"fields\": [\"bar\"],\n                     \"encoding\": \"utf-8\",\n                 },\n-                self._random_temp_filename()\n-                / \"xml\"\n-                / self._file_mark: {\n+                self._random_temp_filename() / \"xml\" / self._file_mark: {\n                     \"format\": \"xml\",\n                     \"indent\": 2,\n                     \"fields\": [\"foo\"],\n                     \"encoding\": \"latin-1\",\n                 },\n-                self._random_temp_filename()\n-                / \"csv\"\n-                / self._file_mark: {\n+                self._random_temp_filename() / \"csv\" / self._file_mark: {\n                     \"format\": \"csv\",\n                     \"indent\": None,\n                     \"fields\": [\"foo\", \"bar\"],\n@@ -2563,9 +2556,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         }\n         settings = {\n             \"FEEDS\": {\n-                self._random_temp_filename()\n-                / \"json\"\n-                / self._file_mark: {\n+                self._random_temp_filename() / \"json\" / self._file_mark: {\n                     \"format\": \"json\",\n                     \"indent\": None,\n                     \"encoding\": \"utf-8\",\n@@ -2591,8 +2582,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         ]\n         settings = {\n             \"FEEDS\": {\n-                self._random_temp_filename()\n-                / \"%(batch_id)d\": {\n+                self._random_temp_filename() / \"%(batch_id)d\": {\n                     \"format\": \"json\",\n                 },\n             },\n\n@@ -226,9 +226,9 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r1.flags, r2.flags)\n \n         # make sure cb_kwargs dict is shallow copied\n-        assert (\n-            r1.cb_kwargs is not r2.cb_kwargs\n-        ), \"cb_kwargs must be a shallow copy, not identical\"\n+        assert r1.cb_kwargs is not r2.cb_kwargs, (\n+            \"cb_kwargs must be a shallow copy, not identical\"\n+        )\n         self.assertEqual(r1.cb_kwargs, r2.cb_kwargs)\n \n         # make sure meta dict is shallow copied\n@@ -236,9 +236,9 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r1.meta, r2.meta)\n \n         # make sure headers attribute is shallow copied\n-        assert (\n-            r1.headers is not r2.headers\n-        ), \"headers must be a shallow copy, not identical\"\n+        assert r1.headers is not r2.headers, (\n+            \"headers must be a shallow copy, not identical\"\n+        )\n         self.assertEqual(r1.headers, r2.headers)\n         self.assertEqual(r1.encoding, r2.encoding)\n         self.assertEqual(r1.dont_filter, r2.dont_filter)\n\n@@ -99,9 +99,9 @@ class BaseResponseTest(unittest.TestCase):\n         self.assertEqual(r1.flags, r2.flags)\n \n         # make sure headers attribute is shallow copied\n-        assert (\n-            r1.headers is not r2.headers\n-        ), \"headers must be a shallow copy, not identical\"\n+        assert r1.headers is not r2.headers, (\n+            \"headers must be a shallow copy, not identical\"\n+        )\n         self.assertEqual(r1.headers, r2.headers)\n \n     def test_copy_meta(self):\n\n@@ -289,9 +289,7 @@ class ItemMetaTest(unittest.TestCase):\n class ItemMetaClassCellRegression(unittest.TestCase):\n     def test_item_meta_classcell_regression(self):\n         class MyItem(Item, metaclass=ItemMeta):\n-            def __init__(\n-                self, *args, **kwargs\n-            ):  # pylint: disable=useless-parent-delegation\n+            def __init__(self, *args, **kwargs):  # pylint: disable=useless-parent-delegation\n                 # This call to super() trigger the __classcell__ propagation\n                 # requirement. When not done properly raises an error:\n                 # TypeError: __class__ set to <class '__main__.MyItem'>\n\n@@ -215,7 +215,7 @@ class FilesPipelineTestCase(unittest.TestCase):\n \n         class CustomFilesPipeline(FilesPipeline):\n             def file_path(self, request, response=None, info=None, item=None):\n-                return f'full/{item.get(\"path\")}'\n+                return f\"full/{item.get('path')}\"\n \n         file_path = CustomFilesPipeline.from_crawler(\n             get_crawler(None, {\"FILES_STORE\": self.tempdir})\n\n@@ -35,7 +35,6 @@ def _mocked_download_func(request, info):\n \n \n class UserDefinedPipeline(MediaPipeline):\n-\n     def media_to_download(self, request, info, *, item=None):\n         pass\n \n@@ -376,7 +375,6 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n \n \n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n-\n     def _assert_request_no3xx(self, pipeline_class, settings):\n         pipe = pipeline_class(crawler=get_crawler(None, settings))\n         request = Request(\"http://url\")\n@@ -403,11 +401,9 @@ class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n                 self.assertNotIn(status, request.meta[\"handle_httpstatus_list\"])\n \n     def test_subclass_standard_setting(self):\n-\n         self._assert_request_no3xx(UserDefinedPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n \n     def test_subclass_specific_setting(self):\n-\n         self._assert_request_no3xx(\n             UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n         )\n\n@@ -27,10 +27,7 @@ class BaseRobotParserTest:\n \n     def test_allowed(self):\n         robotstxt_robotstxt_body = (\n-            b\"User-agent: * \\n\"\n-            b\"Disallow: /disallowed \\n\"\n-            b\"Allow: /allowed \\n\"\n-            b\"Crawl-delay: 10\"\n+            b\"User-agent: * \\nDisallow: /disallowed \\nAllow: /allowed \\nCrawl-delay: 10\"\n         )\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n@@ -140,7 +137,7 @@ class DecodeRobotsTxtTest(unittest.TestCase):\n         self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n \n     def test_decode_non_utf8(self):\n-        robotstxt_body = b\"User-agent: *\\n\\xFFDisallow: /\\n\"\n+        robotstxt_body = b\"User-agent: *\\n\\xffDisallow: /\\n\"\n         decoded_content = decode_robotstxt(robotstxt_body, spider=None)\n         self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n \n\n@@ -107,9 +107,9 @@ class SelectorTestCase(unittest.TestCase):\n         \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n         x = Selector(text=\"\")\n         weakref.ref(x)\n-        assert not hasattr(\n-            x, \"__dict__\"\n-        ), f\"{x.__class__.__name__} does not use __slots__\"\n+        assert not hasattr(x, \"__dict__\"), (\n+            f\"{x.__class__.__name__} does not use __slots__\"\n+        )\n \n     def test_selector_bad_args(self):\n         with self.assertRaisesRegex(ValueError, \"received both response and text\"):\n\n@@ -158,18 +158,18 @@ class ResponseUtilsTest(unittest.TestCase):\n         )\n \n         assert open_in_browser(r1, _openfunc=check_base_url), \"Inject base url\"\n-        assert open_in_browser(\n-            r2, _openfunc=check_base_url\n-        ), \"Inject base url with argumented head\"\n-        assert open_in_browser(\n-            r3, _openfunc=check_base_url\n-        ), \"Inject unique base url with misleading tag\"\n-        assert open_in_browser(\n-            r4, _openfunc=check_base_url\n-        ), \"Inject unique base url with misleading comment\"\n-        assert open_in_browser(\n-            r5, _openfunc=check_base_url\n-        ), \"Inject unique base url with conditional comment\"\n+        assert open_in_browser(r2, _openfunc=check_base_url), (\n+            \"Inject base url with argumented head\"\n+        )\n+        assert open_in_browser(r3, _openfunc=check_base_url), (\n+            \"Inject unique base url with misleading tag\"\n+        )\n+        assert open_in_browser(r4, _openfunc=check_base_url), (\n+            \"Inject unique base url with misleading comment\"\n+        )\n+        assert open_in_browser(r5, _openfunc=check_base_url), (\n+            \"Inject unique base url with conditional comment\"\n+        )\n \n     def test_open_in_browser_redos_comment(self):\n         MAX_CPU_TIME = 0.02\n@@ -240,6 +240,6 @@ class ResponseUtilsTest(unittest.TestCase):\n     ),\n )\n def test_remove_html_comments(input_body, output_body):\n-    assert (\n-        _remove_html_comments(input_body) == output_body\n-    ), f\"{_remove_html_comments(input_body)=} == {output_body=}\"\n+    assert _remove_html_comments(input_body) == output_body, (\n+        f\"{_remove_html_comments(input_body)=} == {output_body=}\"\n+    )\n\n@@ -321,9 +321,9 @@ class GuessSchemeTest(unittest.TestCase):\n def create_guess_scheme_t(args):\n     def do_expected(self):\n         url = guess_scheme(args[0])\n-        assert url.startswith(\n-            args[1]\n-        ), f\"Wrong scheme guessed: for `{args[0]}` got `{url}`, expected `{args[1]}...`\"\n+        assert url.startswith(args[1]), (\n+            f\"Wrong scheme guessed: for `{args[0]}` got `{url}`, expected `{args[1]}...`\"\n+        )\n \n     return do_expected\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
