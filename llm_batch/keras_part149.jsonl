{"custom_id": "keras#f2fe51a9d2bc9d228e3ac374579e87595f51fef6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 121 | Lines Deleted: 120 | Files Changed: 11 | Hunks: 69 | Methods Changed: 15 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 241 | Churn Cumulative: 26594 | Contributors (this commit): 114 | Commits (past 90d): 110 | Contributors (cumulative): 187 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,7 +19,7 @@ from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers import Dense, Activation\n from keras.layers import SimpleRNN\n-from keras.initializations import normal, identity\n+from keras import initializers\n from keras.optimizers import RMSprop\n from keras.utils import np_utils\n \n@@ -51,8 +51,8 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)\n print('Evaluate IRNN...')\n model = Sequential()\n model.add(SimpleRNN(output_dim=hidden_units,\n-                    init=lambda shape, name: normal(shape, scale=0.001, name=name),\n-                    inner_init=lambda shape, name: identity(shape, scale=1.0, name=name),\n+                    init=initializers.RandomNormal(stddev=0.001),\n+                    inner_init=initializers.Identity(gain=1.0),\n                     activation='relu',\n                     input_shape=X_train.shape[1:]))\n model.add(Dense(nb_classes))\n\n@@ -8,7 +8,7 @@ from . import utils\n from . import wrappers\r\n from . import callbacks\r\n from . import constraints\r\n-from . import initializations\r\n+from . import initializers\r\n from . import metrics\r\n from . import models\r\n from . import objectives\r\n\n@@ -356,6 +356,9 @@ def _compute_fans(shape, data_format='channels_first'):\n \n     # Returns\n         A tuple of scalars, `(fan_in, fan_out)`.\n+\n+    # Raises\n+        ValueError: in case of invalid `data_format` argument.\n     \"\"\"\n     if len(shape) == 2:\n         fan_in = shape[0]\n\n@@ -1,4 +1,4 @@\n-from .. import initializations\n+from .. import initializers\n from ..engine import Layer\n from .. import backend as K\n import numpy as np\n@@ -57,7 +57,7 @@ class PReLU(Layer):\n         Same shape as the input.\n \n     # Arguments\n-        init: initialization function for the weights.\n+        init: initializer function for the weights.\n         weights: initial weights, as a list of a single Numpy array.\n         shared_axes: the axes along which to share learnable\n             parameters for the activation function.\n@@ -74,7 +74,7 @@ class PReLU(Layer):\n \n     def __init__(self, init='zero', weights=None, shared_axes=None, **kwargs):\n         self.supports_masking = True\n-        self.init = initializations.get(init)\n+        self.init = initializers.get(init)\n         self.initial_weights = weights\n         if not isinstance(shared_axes, (list, tuple)):\n             self.shared_axes = [shared_axes]\n@@ -278,10 +278,10 @@ class SReLU(Layer):\n         Same shape as the input.\n \n     # Arguments\n-        t_left_init: initialization function for the left part intercept\n-        a_left_init: initialization function for the left part slope\n-        t_right_init: initialization function for the right part intercept\n-        a_right_init: initialization function for the right part slope\n+        t_left_init: initializer function for the left part intercept\n+        a_left_init: initializer function for the left part slope\n+        t_right_init: initializer function for the right part intercept\n+        a_right_init: initializer function for the right part slope\n         shared_axes: the axes along which to share learnable\n             parameters for the activation function.\n             For example, if the incoming feature maps\n@@ -317,10 +317,10 @@ class SReLU(Layer):\n                 param_shape[i - 1] = 1\n                 self.param_broadcast[i - 1] = True\n \n-        t_left_init = initializations.get(self.t_left_init)\n-        a_left_init = initializations.get(self.a_left_init)\n-        t_right_init = initializations.get(self.t_right_init)\n-        a_right_init = initializations.get(self.a_right_init)\n+        t_left_init = initializers.get(self.t_left_init)\n+        a_left_init = initializers.get(self.a_left_init)\n+        t_right_init = initializers.get(self.t_right_init)\n+        a_right_init = initializers.get(self.a_right_init)\n \n         self.t_left = t_left_init(param_shape,\n                                   name='{}_t_left'.format(self.name))\n\n@@ -1,10 +1,9 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n-import functools\n \n from .. import backend as K\n from .. import activations\n-from .. import initializations\n+from .. import initializers\n from .. import regularizers\n from .. import constraints\n from ..engine import Layer\n@@ -48,9 +47,9 @@ class Convolution1D(Layer):\n         nb_filter: Number of convolution kernels to use\n             (dimensionality of the output).\n         filter_length: The extension (spatial or temporal) of each filter.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)), or alternatively,\n-            Theano function to use for weights initialization.\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)), or alternatively,\n+            Theano function to use for weights initializer.\n             This parameter is only relevant\n             if you don't pass a `weights` argument.\n         activation: name of activation function to use\n@@ -102,7 +101,7 @@ class Convolution1D(Layer):\n             raise ValueError('Invalid border mode for Convolution1D:', border_mode)\n         self.nb_filter = nb_filter\n         self.filter_length = filter_length\n-        self.init = initializations.get(init)\n+        self.init = initializers.get(init, data_format='channels_last')\n         self.activation = activations.get(activation)\n         self.border_mode = border_mode\n         self.subsample_length = subsample_length\n@@ -130,8 +129,7 @@ class Convolution1D(Layer):\n         self.W_shape = (self.filter_length, 1, input_dim, self.nb_filter)\n \n         self.W = self.add_weight(self.W_shape,\n-                                 initializer=functools.partial(self.init,\n-                                                               data_format='channels_first'),\n+                                 initializer=self.init,\n                                  name='{}_W'.format(self.name),\n                                  regularizer=self.W_regularizer,\n                                  constraint=self.W_constraint)\n@@ -218,9 +216,9 @@ class AtrousConvolution1D(Convolution1D):\n         nb_filter: Number of convolution kernels to use\n             (dimensionality of the output).\n         filter_length: The extension (spatial or temporal) of each filter.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)), or alternatively,\n-            Theano function to use for weights initialization.\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)), or alternatively,\n+            Theano function to use for weights initializer.\n             This parameter is only relevant\n             if you don't pass a `weights` argument.\n         activation: name of activation function to use\n@@ -338,9 +336,9 @@ class Convolution2D(Layer):\n         nb_filter: Number of convolution filters to use.\n         nb_row: Number of rows in the convolution kernel.\n         nb_col: Number of columns in the convolution kernel.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)), or alternatively,\n-            Theano function to use for weights initialization.\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)), or alternatively,\n+            Theano function to use for weights initializer.\n             This parameter is only relevant if you don't pass\n             a `weights` argument.\n         activation: name of activation function to use\n@@ -399,7 +397,7 @@ class Convolution2D(Layer):\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n-        self.init = initializations.get(init)\n+        self.init = initializers.get(init, data_format=data_format)\n         self.activation = activations.get(activation)\n         self.border_mode = border_mode\n         self.subsample = tuple(subsample)\n@@ -429,8 +427,7 @@ class Convolution2D(Layer):\n         else:\n             raise ValueError('Invalid data_format:', self.data_format)\n         self.W = self.add_weight(self.W_shape,\n-                                 initializer=functools.partial(self.init,\n-                                                               data_format=self.data_format),\n+                                 initializer=self.init,\n                                  name='{}_W'.format(self.name),\n                                  regularizer=self.W_regularizer,\n                                  constraint=self.W_constraint)\n@@ -570,9 +567,9 @@ class Deconvolution2D(Convolution2D):\n              It is better to use\n              a dummy input and observe the actual output shape of\n              a layer, as specified in the examples.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)), or alternatively,\n-            Theano function to use for weights initialization.\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)), or alternatively,\n+            Theano function to use for weights initializer.\n             This parameter is only relevant if you don't pass\n             a `weights` argument.\n         activation: name of activation function to use\n@@ -715,9 +712,9 @@ class AtrousConvolution2D(Convolution2D):\n         nb_filter: Number of convolution filters to use.\n         nb_row: Number of rows in the convolution kernel.\n         nb_col: Number of columns in the convolution kernel.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)), or alternatively,\n-            Theano function to use for weights initialization.\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)), or alternatively,\n+            Theano function to use for weights initializer.\n             This parameter is only relevant if you don't pass\n             a `weights` argument.\n         activation: name of activation function to use\n@@ -870,9 +867,9 @@ class SeparableConvolution2D(Layer):\n         nb_filter: Number of convolution filters to use.\n         nb_row: Number of rows in the convolution kernel.\n         nb_col: Number of columns in the convolution kernel.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)), or alternatively,\n-            Theano function to use for weights initialization.\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)), or alternatively,\n+            Theano function to use for weights initializer.\n             This parameter is only relevant if you don't pass\n             a `weights` argument.\n         activation: name of activation function to use\n@@ -947,7 +944,7 @@ class SeparableConvolution2D(Layer):\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n-        self.init = initializations.get(init)\n+        self.init = initializers.get(init, data_format=data_format)\n         self.activation = activations.get(activation)\n         if border_mode not in {'valid', 'same'}:\n             raise ValueError('border_mode must be in {valid, same}.')\n@@ -985,14 +982,12 @@ class SeparableConvolution2D(Layer):\n             raise ValueError('Invalid data_format:', self.data_format)\n \n         self.depthwise_kernel = self.add_weight(depthwise_shape,\n-                                                initializer=functools.partial(self.init,\n-                                                                              data_format=self.data_format),\n+                                                initializer=self.init,\n                                                 regularizer=self.depthwise_regularizer,\n                                                 constraint=self.depthwise_constraint,\n                                                 name='{}_depthwise_kernel'.format(self.name))\n         self.pointwise_kernel = self.add_weight(pointwise_shape,\n-                                                initializer=functools.partial(self.init,\n-                                                                              data_format=self.data_format),\n+                                                initializer=self.init,\n                                                 regularizer=self.pointwise_regularizer,\n                                                 constraint=self.pointwise_constraint,\n                                                 name='{}_pointwise_kernel'.format(self.name))\n@@ -1083,9 +1078,9 @@ class Convolution3D(Layer):\n         kernel_dim1: Length of the first dimension in the convolution kernel.\n         kernel_dim2: Length of the second dimension in the convolution kernel.\n         kernel_dim3: Length of the third dimension in the convolution kernel.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)), or alternatively,\n-            Theano function to use for weights initialization.\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)), or alternatively,\n+            Theano function to use for weights initializer.\n             This parameter is only relevant if you don't pass\n             a `weights` argument.\n         activation: name of activation function to use\n@@ -1147,7 +1142,7 @@ class Convolution3D(Layer):\n         self.kernel_dim1 = kernel_dim1\n         self.kernel_dim2 = kernel_dim2\n         self.kernel_dim3 = kernel_dim3\n-        self.init = initializations.get(init)\n+        self.init = initializers.get(init, data_format=data_format)\n         self.activation = activations.get(activation)\n         self.border_mode = border_mode\n         self.subsample = tuple(subsample)\n@@ -1183,8 +1178,7 @@ class Convolution3D(Layer):\n             raise ValueError('Invalid data_format:', self.data_format)\n \n         self.W = self.add_weight(self.W_shape,\n-                                 initializer=functools.partial(self.init,\n-                                                               data_format=self.data_format),\n+                                 initializer=self.init,\n                                  name='{}_W'.format(self.name),\n                                  regularizer=self.W_regularizer,\n                                  constraint=self.W_constraint)\n\n@@ -1,6 +1,6 @@\n from .. import backend as K\n from .. import activations\n-from .. import initializations\n+from .. import initializers\n from .. import regularizers\n \n import numpy as np\n@@ -216,7 +216,6 @@ class ConvLSTM2D(ConvRecurrent2D):\n             - if data_format='channels_last'\n                 4D tensor with shape:\n                 `(samples, output_row, output_col, nb_filter)`\n-\n             where o_row and o_col depend on the shape of the filter and\n             the border_mode\n \n@@ -231,12 +230,12 @@ class ConvLSTM2D(ConvRecurrent2D):\n         stateful : Boolean (default False). If True, the last state\n             for each sample at index i in a batch will be used as initial\n             state for the sample of index i in the following batch.\n-            init: weight initialization function.\n+        init: weight initializer function.\n             Can be the name of an existing function (str),\n             or a Theano function\n-                (see: [initializations](../initializations.md)).\n-            inner_init: initialization function of the inner cells.\n-            forget_bias_init: initialization function for the bias of the\n+            (see: [initializers](../initializers.md)).\n+        inner_init: initializer function of the inner cells.\n+        forget_bias_init: initializer function for the bias of the\n         forget gate.\n             [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n             recommend initializing with ones.\n@@ -245,6 +244,9 @@ class ConvLSTM2D(ConvRecurrent2D):\n             or a Theano function (see: [activations](../activations.md)).\n         inner_activation: activation function for the inner cells.\n \n+    # Raises\n+        ValueError: in case of invalid constructor arguments.\n+\n     # References\n         - [Convolutional LSTM Network: A Machine Learning Approach for\n         Precipitation Nowcasting](http://arxiv.org/abs/1506.04214v1)\n@@ -264,13 +266,14 @@ class ConvLSTM2D(ConvRecurrent2D):\n         if data_format == 'default':\n             data_format = K.image_data_format()\n         if data_format not in {'channels_last', 'channels_first'}:\n-            raise ValueError('data_format must be in {tf,th}', data_format)\n+            raise ValueError('data_format must be in '\n+                             '{channels_last, channels_first}', data_format)\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n-        self.init = initializations.get(init)\n-        self.inner_init = initializations.get(inner_init)\n-        self.forget_bias_init = initializations.get(forget_bias_init)\n+        self.init = initializers.get(init, data_format=data_format)\n+        self.inner_init = initializers.get(inner_init)\n+        self.forget_bias_init = initializers.get(forget_bias_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n         self.border_mode = border_mode\n\n@@ -11,7 +11,7 @@ import warnings\n \n from .. import backend as K\n from .. import activations\n-from .. import initializations\n+from .. import initializers\n from .. import regularizers\n from .. import constraints\n from ..engine import InputSpec\n@@ -718,10 +718,10 @@ class Dense(Layer):\n \n     # Arguments\n         output_dim: int > 0.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)),\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)),\n             or alternatively, Theano function to use for weights\n-            initialization. This parameter is only relevant\n+            initializer. This parameter is only relevant\n             if you don't pass a `weights` argument.\n         activation: name of activation function to use\n             (see [activations](../activations.md)),\n@@ -763,7 +763,7 @@ class Dense(Layer):\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n                  bias=True, input_dim=None, **kwargs):\n-        self.init = initializations.get(init)\n+        self.init = initializers.get(init)\n         self.activation = activations.get(activation)\n         self.output_dim = output_dim\n         self.input_dim = input_dim\n@@ -885,10 +885,10 @@ class MaxoutDense(Layer):\n     # Arguments\n         output_dim: int > 0.\n         nb_feature: number of Dense layers to use internally.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)),\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)),\n             or alternatively, Theano function to use for weights\n-            initialization. This parameter is only relevant\n+            initializer. This parameter is only relevant\n             if you don't pass a `weights` argument.\n         weights: list of Numpy arrays to set as initial weights.\n             The list should have 2 elements, of shape `(input_dim, output_dim)`\n@@ -933,7 +933,7 @@ class MaxoutDense(Layer):\n                  **kwargs):\n         self.output_dim = output_dim\n         self.nb_feature = nb_feature\n-        self.init = initializations.get(init)\n+        self.init = initializers.get(init)\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n         self.b_regularizer = regularizers.get(b_regularizer)\n@@ -1008,10 +1008,10 @@ class Highway(Layer):\n     Highway layers are a natural extension of LSTMs to feedforward networks.\n \n     # Arguments\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)),\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)),\n             or alternatively, Theano function to use for weights\n-            initialization. This parameter is only relevant\n+            initializer. This parameter is only relevant\n             if you don't pass a `weights` argument.\n         activation: name of activation function to use\n             (see [activations](../activations.md)),\n@@ -1063,7 +1063,7 @@ class Highway(Layer):\n             kwargs.pop('transform_bias')\n             warnings.warn('`transform_bias` argument is deprecated and '\n                           'will be removed after 5/2017.')\n-        self.init = initializations.get(init)\n+        self.init = initializers.get(init)\n         self.activation = activations.get(activation)\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n@@ -1157,10 +1157,10 @@ class TimeDistributedDense(Layer):\n \n     # Arguments\n         output_dim: int > 0.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)),\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)),\n             or alternatively, Theano function to use for weights\n-            initialization. This parameter is only relevant\n+            initializer. This parameter is only relevant\n             if you don't pass a `weights` argument.\n         activation: name of activation function to use\n             (see [activations](../activations.md)),\n@@ -1206,7 +1206,7 @@ class TimeDistributedDense(Layer):\n                       'And will be removed on May 1st, 2017. '\n                       'Please use a `Dense` layer instead.')\n         self.output_dim = output_dim\n-        self.init = initializations.get(init)\n+        self.init = initializers.get(init)\n         self.activation = activations.get(activation)\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n\n@@ -1,7 +1,7 @@\n from __future__ import absolute_import\n \n from .. import backend as K\n-from .. import initializations\n+from .. import initializers\n from .. import regularizers\n from .. import constraints\n from ..engine import Layer\n@@ -33,9 +33,9 @@ class Embedding(Layer):\n       input_dim: int > 0. Size of the vocabulary, ie.\n           1 + maximum integer index occurring in the input data.\n       output_dim: int >= 0. Dimension of the dense embedding.\n-      init: name of initialization function for the weights\n-          of the layer (see: [initializations](../initializations.md)),\n-          or alternatively, Theano function to use for weights initialization.\n+      init: name of initializer function for the weights\n+          of the layer (see: [initializers](../initializers.md)),\n+          or alternatively, Theano function to use for weights initializer.\n           This parameter is only relevant if you don't pass a `weights` argument.\n       weights: list of Numpy arrays to set as initial weights.\n           The list should have 1 element, of shape `(input_dim, output_dim)`.\n@@ -74,7 +74,7 @@ class Embedding(Layer):\n                  weights=None, dropout=0., **kwargs):\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n-        self.init = initializations.get(init)\n+        self.init = initializers.get(init)\n         self.input_length = input_length\n         self.mask_zero = mask_zero\n         self.dropout = dropout\n\n@@ -3,7 +3,7 @@ from __future__ import absolute_import\n \n from keras import backend as K\n from keras.layers import activations\n-from keras.layers import initializations\n+from keras.layers import initializers\n from keras.layers import regularizers\n from keras.layers import constraints\n from keras.engine import Layer\n@@ -41,9 +41,9 @@ class LocallyConnected1D(Layer):\n     # Arguments\n         nb_filter: Dimensionality of the output.\n         filter_length: The extension (spatial or temporal) of each filter.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)),\n-            or alternatively, Theano function to use for weights initialization.\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)),\n+            or alternatively, Theano function to use for weights initializer.\n             This parameter is only relevant if you don't pass a `weights` argument.\n         activation: name of activation function to use\n             (see [activations](../activations.md)),\n@@ -92,7 +92,7 @@ class LocallyConnected1D(Layer):\n                              '(only \"valid\" is supported):', border_mode)\n         self.nb_filter = nb_filter\n         self.filter_length = filter_length\n-        self.init = initializations.get(init, data_format='channels_first')\n+        self.init = initializers.get(init, data_format='channels_first')\n         self.activation = activations.get(activation)\n \n         self.border_mode = border_mode\n@@ -215,9 +215,9 @@ class LocallyConnected2D(Layer):\n         nb_filter: Number of convolution filters to use.\n         nb_row: Number of rows in the convolution kernel.\n         nb_col: Number of columns in the convolution kernel.\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)), or alternatively,\n-            Theano function to use for weights initialization.\n+        init: name of initializer function for the weights of the layer\n+            (see [initializers](../initializers.md)), or alternatively,\n+            Theano function to use for weights initializer.\n             This parameter is only relevant if you don't pass\n             a `weights` argument.\n         activation: name of activation function to use\n@@ -273,7 +273,7 @@ class LocallyConnected2D(Layer):\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n-        self.init = initializations.get(init, data_format=data_format)\n+        self.init = initializers.get(init, data_format=data_format)\n         self.activation = activations.get(activation)\n \n         self.border_mode = border_mode\n\n@@ -1,5 +1,6 @@\n from ..engine import Layer, InputSpec\n-from .. import initializations, regularizers\n+from .. import initializers\n+from .. import regularizers\n from .. import backend as K\n \n \n@@ -39,13 +40,13 @@ class BatchNormalization(Layer):\n             List of 2 Numpy arrays, with shapes:\n             `[(input_shape,), (input_shape,)]`\n             Note that the order of this list is [gamma, beta, mean, std]\n-        beta_init: name of initialization function for shift parameter\n-            (see [initializations](../initializations.md)), or alternatively,\n-            Theano/TensorFlow function to use for weights initialization.\n+        beta_init: name of initializer function for shift parameter\n+            (see [initializers](../initializers.md)), or alternatively,\n+            Theano/TensorFlow function to use for weights initializer.\n             This parameter is only relevant if you don't pass a `weights` argument.\n-        gamma_init: name of initialization function for scale parameter (see\n-            [initializations](../initializations.md)), or alternatively,\n-            Theano/TensorFlow function to use for weights initialization.\n+        gamma_init: name of initializer function for scale parameter (see\n+            [initializers](../initializers.md)), or alternatively,\n+            Theano/TensorFlow function to use for weights initializer.\n             This parameter is only relevant if you don't pass a `weights` argument.\n         gamma_regularizer: instance of [WeightRegularizer](../regularizers.md)\n             (eg. L1 or L2 regularization), applied to the gamma vector.\n@@ -68,8 +69,8 @@ class BatchNormalization(Layer):\n                  weights=None, beta_init='zero', gamma_init='one',\n                  gamma_regularizer=None, beta_regularizer=None, **kwargs):\n         self.supports_masking = True\n-        self.beta_init = initializations.get(beta_init)\n-        self.gamma_init = initializations.get(gamma_init)\n+        self.beta_init = initializers.get(beta_init)\n+        self.gamma_init = initializers.get(gamma_init)\n         self.epsilon = epsilon\n         self.mode = mode\n         self.axis = axis\n\n@@ -4,7 +4,7 @@ import numpy as np\n \n from .. import backend as K\n from .. import activations\n-from .. import initializations\n+from .. import initializers\n from .. import regularizers\n from ..engine import Layer\n from ..engine import InputSpec\n@@ -274,10 +274,10 @@ class SimpleRNN(Recurrent):\n \n     # Arguments\n         output_dim: dimension of the internal projections and the final output.\n-        init: weight initialization function.\n+        init: weight initializer function.\n             Can be the name of an existing function (str),\n-            or a Theano function (see: [initializations](../initializations.md)).\n-        inner_init: initialization function of the inner cells.\n+            or a Theano function (see: [initializers](../initializers.md)).\n+        inner_init: initializer function of the inner cells.\n         activation: activation function.\n             Can be the name of an existing function (str),\n             or a Theano function (see: [activations](../activations.md)).\n@@ -300,8 +300,8 @@ class SimpleRNN(Recurrent):\n                  W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                  dropout_W=0., dropout_U=0., **kwargs):\n         self.output_dim = output_dim\n-        self.init = initializations.get(init)\n-        self.inner_init = initializations.get(inner_init)\n+        self.init = initializers.get(init)\n+        self.inner_init = initializers.get(inner_init)\n         self.activation = activations.get(activation)\n         self.W_regularizer = regularizers.get(W_regularizer)\n         self.U_regularizer = regularizers.get(U_regularizer)\n@@ -424,10 +424,10 @@ class GRU(Recurrent):\n \n     # Arguments\n         output_dim: dimension of the internal projections and the final output.\n-        init: weight initialization function.\n+        init: weight initializer function.\n             Can be the name of an existing function (str),\n-            or a Theano function (see: [initializations](../initializations.md)).\n-        inner_init: initialization function of the inner cells.\n+            or a Theano function (see: [initializers](../initializers.md)).\n+        inner_init: initializer function of the inner cells.\n         activation: activation function.\n             Can be the name of an existing function (str),\n             or a Theano function (see: [activations](../activations.md)).\n@@ -453,8 +453,8 @@ class GRU(Recurrent):\n                  W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                  dropout_W=0., dropout_U=0., **kwargs):\n         self.output_dim = output_dim\n-        self.init = initializations.get(init)\n-        self.inner_init = initializations.get(inner_init)\n+        self.init = initializers.get(init)\n+        self.inner_init = initializers.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n         self.W_regularizer = regularizers.get(W_regularizer)\n@@ -648,11 +648,11 @@ class LSTM(Recurrent):\n \n     # Arguments\n         output_dim: dimension of the internal projections and the final output.\n-        init: weight initialization function.\n+        init: weight initializer function.\n             Can be the name of an existing function (str),\n-            or a Theano function (see: [initializations](../initializations.md)).\n-        inner_init: initialization function of the inner cells.\n-        forget_bias_init: initialization function for the bias of the forget gate.\n+            or a Theano function (see: [initializers](../initializers.md)).\n+        inner_init: initializer function of the inner cells.\n+        forget_bias_init: initializer function for the bias of the forget gate.\n             [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n             recommend initializing with ones.\n         activation: activation function.\n@@ -682,9 +682,9 @@ class LSTM(Recurrent):\n                  W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                  dropout_W=0., dropout_U=0., **kwargs):\n         self.output_dim = output_dim\n-        self.init = initializations.get(init)\n-        self.inner_init = initializations.get(inner_init)\n-        self.forget_bias_init = initializations.get(forget_bias_init)\n+        self.init = initializers.get(init)\n+        self.inner_init = initializers.get(inner_init)\n+        self.forget_bias_init = initializers.get(forget_bias_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n         self.W_regularizer = regularizers.get(W_regularizer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c243f39ce503e3e34c8b0f0085a9c40d0b5858d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 5973 | Contributors (this commit): 54 | Commits (past 90d): 44 | Contributors (cumulative): 54 | DMM Complexity: 0.0\n\nDIFF:\n@@ -783,16 +783,20 @@ def dot(x, y):\n         (2, 4, 5)\n     ```\n     \"\"\"\n+    if hasattr(tf, 'unstack'):\n+        unstack = tf.unstack\n+    else:\n+        unstack = tf.unpack\n     if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\n         x_shape = []\n-        for i, s in zip(int_shape(x), tf.unpack(tf.shape(x))):\n+        for i, s in zip(int_shape(x), unstack(tf.shape(x))):\n             if i is not None:\n                 x_shape.append(i)\n             else:\n                 x_shape.append(s)\n         x_shape = tuple(x_shape)\n         y_shape = []\n-        for i, s in zip(int_shape(y), tf.unpack(tf.shape(y))):\n+        for i, s in zip(int_shape(y), unstack(tf.shape(y))):\n             if i is not None:\n                 y_shape.append(i)\n             else:\n@@ -1991,6 +1995,12 @@ def rnn(step_function, inputs, initial_states,\n     # TODO: remove later.\n     if hasattr(tf, 'select'):\n         tf.where = tf.select\n+    if hasattr(tf, 'stack'):\n+        stack = tf.stack\n+        unstack = tf.unstack\n+    else:\n+        stack = tf.pack\n+        unstack = tf.unpack\n \n     if unroll:\n         if not inputs.get_shape()[0]:\n@@ -2000,12 +2010,12 @@ def rnn(step_function, inputs, initial_states,\n         successive_states = []\n         successive_outputs = []\n \n-        input_list = tf.unpack(inputs)\n+        input_list = unstack(inputs)\n         if go_backwards:\n             input_list.reverse()\n \n         if mask is not None:\n-            mask_list = tf.unpack(mask)\n+            mask_list = unstack(mask)\n             if go_backwards:\n                 mask_list.reverse()\n \n@@ -2070,6 +2080,9 @@ def rnn(step_function, inputs, initial_states,\n             dtype=inputs.dtype,\n             size=time_steps,\n             tensor_array_name='input_ta')\n+        if hasattr(input_ta, 'unstack'):\n+            input_ta = input_ta.unstack(inputs)\n+        else:\n             input_ta = input_ta.unpack(inputs)\n         time = tf.constant(0, dtype='int32', name='time')\n \n@@ -2088,6 +2101,9 @@ def rnn(step_function, inputs, initial_states,\n                 dtype=tf.bool,\n                 size=time_steps,\n                 tensor_array_name='mask_ta')\n+            if hasattr(mask_ta, 'unstack'):\n+                mask_ta = mask_ta.unstack(mask)\n+            else:\n                 mask_ta = mask_ta.unpack(mask)\n \n             def _step(time, output_ta_t, *states):\n@@ -2125,6 +2141,9 @@ def rnn(step_function, inputs, initial_states,\n         output_ta = final_outputs[1]\n         new_states = final_outputs[2:]\n \n+        if hasattr(output_ta, 'stack'):\n+            outputs = output_ta.stack()\n+        else:\n             outputs = output_ta.pack()\n         last_output = output_ta.read(last_time - 1)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f8d97e6150cf318a505fb7343ecff4ede76ba4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1896 | Contributors (this commit): 22 | Commits (past 90d): 12 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -226,7 +226,7 @@ def array_to_img(x, dim_ordering='default', scale=True):\n     if dim_ordering == 'th':\n         x = x.transpose(1, 2, 0)\n     if scale:\n-        x += max(-np.min(x), 0)\n+        x = x + max(-np.min(x), 0)\n         x_max = np.max(x)\n         if x_max != 0:\n             x /= x_max\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#57429d1567b94955ac95afe87792411a420ff0ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 202 | Lines Deleted: 106 | Files Changed: 2 | Hunks: 34 | Methods Changed: 16 | Complexity Δ (Sum/Max): 4/9 | Churn Δ: 308 | Churn Cumulative: 3456 | Contributors (this commit): 31 | Commits (past 90d): 18 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -396,55 +396,94 @@ def standardize_weights(y, sample_weight=None, class_weight=None,\n             return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n \n \n-def generator_queue(generator, max_q_size=10,\n-                    wait_time=0.05, nb_worker=1, pickle_safe=False):\n+class GeneratorEnqueuer(object):\n     \"\"\"Builds a queue out of a data generator.\n-    If pickle_safe, use a multiprocessing approach. Else, use threading.\n     Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n-    \"\"\"\n-    generator_threads = []\n-    if pickle_safe:\n-        q = multiprocessing.Queue(maxsize=max_q_size)\n-        _stop = multiprocessing.Event()\n-    else:\n-        q = queue.Queue()\n-        _stop = threading.Event()\n \n-    try:\n+    # Arguments\n+        generator: a generator function which endlessly yields data\n+        pickle_safe: use multiprocessing if True, otherwise threading\n+    \"\"\"\n+\n+    def __init__(self, generator, pickle_safe=False):\n+        self._generator = generator\n+        self._pickle_safe = pickle_safe\n+        self._threads = []\n+        self._stop_event = None\n+\n+        self.queue = None\n+\n+    def start(self, nb_worker=1, max_q_size=10, wait_time=0.05):\n+        \"\"\"Kick off threads which add data from the generator into the queue.\n+\n+        # Arguments\n+            nb_worker: number of worker threads\n+            max_q_size: queue size (when full, threads could block on put())\n+            wait_time: time to sleep in-between calls to put()\n+        \"\"\"\n+\n         def data_generator_task():\n-            while not _stop.is_set():\n+            while not self._stop_event.is_set():\n                 try:\n-                    if pickle_safe or q.qsize() < max_q_size:\n-                        generator_output = next(generator)\n-                        q.put(generator_output)\n+                    if self._pickle_safe or self.queue.qsize() < max_q_size:\n+                        generator_output = next(self._generator)\n+                        self.queue.put(generator_output)\n                     else:\n                         time.sleep(wait_time)\n                 except Exception:\n-                    _stop.set()\n+                    self._stop_event.set()\n                     raise\n \n+        try:\n+            if self._pickle_safe:\n+                self.queue = multiprocessing.Queue(maxsize=max_q_size)\n+                self._stop_event = multiprocessing.Event()\n+            else:\n+                self.queue = queue.Queue()\n+                self._stop_event = threading.Event()\n+\n             for i in range(nb_worker):\n-            if pickle_safe:\n+                if self._pickle_safe:\n                     # Reset random seed else all children processes\n                     # share the same seed\n                     np.random.seed()\n                     thread = multiprocessing.Process(target=data_generator_task)\n+                    thread.daemon = True\n                 else:\n                     thread = threading.Thread(target=data_generator_task)\n-            generator_threads.append(thread)\n-            thread.daemon = True\n+                self._threads.append(thread)\n                 thread.start()\n         except:\n-        _stop.set()\n-        if pickle_safe:\n-            # Terminate all daemon processes\n-            for p in generator_threads:\n-                if p.is_alive():\n-                    p.terminate()\n-            q.close()\n+            self.stop()\n             raise\n \n-    return q, _stop, generator_threads\n+    def is_running(self):\n+        return self._stop_event is not None and not self._stop_event.is_set()\n+\n+    def stop(self, timeout=None):\n+        \"\"\"Stop running threads and wait for them to exit, if necessary.\n+        Should be called by the same thread which called start().\n+\n+        # Arguments\n+            timeout: maximum time to wait on thread.join()\n+        \"\"\"\n+        if self.is_running():\n+            self._stop_event.set()\n+\n+        for thread in self._threads:\n+            if thread.is_alive():\n+                if self._pickle_safe:\n+                    thread.terminate()\n+                else:\n+                    thread.join(timeout)\n+\n+        if self._pickle_safe:\n+            if self.queue is not None:\n+                self.queue.close()\n+\n+        self._threads = []\n+        self._stop_event = None\n+        self.queue = None\n \n \n class Model(Container):\n@@ -1462,12 +1501,11 @@ class Model(Container):\n         else:\n             self.validation_data = None\n \n-        # start generator thread storing batches into a queue\n-        data_gen_queue, _stop, generator_threads = generator_queue(\n-            generator,\n-            max_q_size=max_q_size,\n-            nb_worker=nb_worker,\n-            pickle_safe=pickle_safe)\n+        enqueuer = None\n+\n+        try:\n+            enqueuer = GeneratorEnqueuer(generator, pickle_safe=pickle_safe)\n+            enqueuer.start(max_q_size=max_q_size, nb_worker=nb_worker)\n \n             callback_model.stop_training = False\n             while epoch < nb_epoch:\n@@ -1476,15 +1514,14 @@ class Model(Container):\n                 batch_index = 0\n                 while samples_seen < samples_per_epoch:\n                     generator_output = None\n-                while not _stop.is_set():\n-                    if not data_gen_queue.empty():\n-                        generator_output = data_gen_queue.get()\n+                    while enqueuer.is_running():\n+                        if not enqueuer.queue.empty():\n+                            generator_output = enqueuer.queue.get()\n                             break\n                         else:\n                             time.sleep(wait_time)\n \n                     if not hasattr(generator_output, '__len__'):\n-                    _stop.set()\n                         raise ValueError('output of generator should be a tuple '\n                                          '(x, y, sample_weight) '\n                                          'or (x, y). Found: ' +\n@@ -1495,7 +1532,6 @@ class Model(Container):\n                     elif len(generator_output) == 3:\n                         x, y, sample_weight = generator_output\n                     else:\n-                    _stop.set()\n                         raise ValueError('output of generator should be a tuple '\n                                          '(x, y, sample_weight) '\n                                          'or (x, y). Found: ' +\n@@ -1512,13 +1548,9 @@ class Model(Container):\n                     batch_logs['size'] = batch_size\n                     callbacks.on_batch_begin(batch_index, batch_logs)\n \n-                try:\n                     outs = self.train_on_batch(x, y,\n                                                sample_weight=sample_weight,\n                                                class_weight=class_weight)\n-                except:\n-                    _stop.set()\n-                    raise\n \n                     if not isinstance(outs, list):\n                         outs = [outs]\n@@ -1566,18 +1598,10 @@ class Model(Container):\n                 if callback_model.stop_training:\n                     break\n \n-        _stop.set()\n-        if pickle_safe:\n-            # Terminate all daemon processes\n-            for p in generator_threads:\n-                if p.is_alive():\n-                    p.terminate()\n-            data_gen_queue.close()\n-        else:\n-            # Wait for all threads to finish\n-            for p in generator_threads:\n-                if p.is_alive():\n-                    p.join()\n+        finally:\n+            if enqueuer is not None:\n+                enqueuer.stop()\n+\n         callbacks.on_train_end()\n         return self.history\n \n@@ -1616,23 +1640,23 @@ class Model(Container):\n         wait_time = 0.01\n         all_outs = []\n         weights = []\n-        data_gen_queue, _stop, generator_threads = generator_queue(\n-            generator,\n-            max_q_size=max_q_size,\n-            nb_worker=nb_worker,\n-            pickle_safe=pickle_safe)\n+\n+        enqueuer = None\n+\n+        try:\n+            enqueuer = GeneratorEnqueuer(generator, pickle_safe=pickle_safe)\n+            enqueuer.start(nb_worker=nb_worker, max_q_size=max_q_size)\n \n             while processed_samples < val_samples:\n                 generator_output = None\n-            while not _stop.is_set():\n-                if not data_gen_queue.empty():\n-                    generator_output = data_gen_queue.get()\n+                while enqueuer.is_running():\n+                    if not enqueuer.queue.empty():\n+                        generator_output = enqueuer.queue.get()\n                         break\n                     else:\n                         time.sleep(wait_time)\n \n                 if not hasattr(generator_output, '__len__'):\n-                _stop.set()\n                     raise ValueError('output of generator should be a tuple '\n                                      '(x, y, sample_weight) '\n                                      'or (x, y). Found: ' + str(generator_output))\n@@ -1642,15 +1666,11 @@ class Model(Container):\n                 elif len(generator_output) == 3:\n                     x, y, sample_weight = generator_output\n                 else:\n-                _stop.set()\n                     raise ValueError('output of generator should be a tuple '\n                                      '(x, y, sample_weight) '\n                                      'or (x, y). Found: ' + str(generator_output))\n-            try:\n+\n                 outs = self.test_on_batch(x, y, sample_weight=sample_weight)\n-            except:\n-                _stop.set()\n-                raise\n \n                 if isinstance(x, list):\n                     nb_samples = len(x[0])\n@@ -1663,18 +1683,10 @@ class Model(Container):\n                 processed_samples += nb_samples\n                 weights.append(nb_samples)\n \n-        _stop.set()\n-        if pickle_safe:\n-            # Terminate all daemon processes\n-            for p in generator_threads:\n-                if p.is_alive():\n-                    p.terminate()\n-            data_gen_queue.close()\n-        else:\n-            # Wait for all threads to finish\n-            for p in generator_threads:\n-                if p.is_alive():\n-                    p.join()\n+        finally:\n+            if enqueuer is not None:\n+                enqueuer.stop()\n+\n         if not isinstance(outs, list):\n             return np.average(np.asarray(all_outs),\n                               weights=weights)\n@@ -1714,17 +1726,18 @@ class Model(Container):\n         processed_samples = 0\n         wait_time = 0.01\n         all_outs = []\n-        data_gen_queue, _stop, generator_threads = generator_queue(\n-            generator,\n-            max_q_size=max_q_size,\n-            nb_worker=nb_worker,\n-            pickle_safe=pickle_safe)\n+\n+        enqueuer = None\n+\n+        try:\n+            enqueuer = GeneratorEnqueuer(generator, pickle_safe=pickle_safe)\n+            enqueuer.start(nb_worker=nb_worker, max_q_size=max_q_size)\n \n             while processed_samples < val_samples:\n                 generator_output = None\n-            while not _stop.is_set():\n-                if not data_gen_queue.empty():\n-                    generator_output = data_gen_queue.get()\n+                while enqueuer.is_running():\n+                    if not enqueuer.queue.empty():\n+                        generator_output = enqueuer.queue.get()\n                         break\n                     else:\n                         time.sleep(wait_time)\n@@ -1736,7 +1749,6 @@ class Model(Container):\n                     elif len(generator_output) == 3:\n                         x, y, sample_weight = generator_output\n                     else:\n-                    _stop.set()\n                         raise ValueError('output of generator should be a tuple '\n                                          '(x, y, sample_weight) '\n                                          'or (x, y). Found: ' +\n@@ -1744,11 +1756,7 @@ class Model(Container):\n                 else:\n                     x = generator_output\n \n-            try:\n                 outs = self.predict_on_batch(x)\n-            except:\n-                _stop.set()\n-                raise\n \n                 if isinstance(x, list):\n                     nb_samples = len(x[0])\n@@ -1769,18 +1777,10 @@ class Model(Container):\n                     all_outs[i][processed_samples:(processed_samples + nb_samples)] = out\n                 processed_samples += nb_samples\n \n-        _stop.set()\n-        if pickle_safe:\n-            # Terminate all daemon processes\n-            for p in generator_threads:\n-                if p.is_alive():\n-                    p.terminate()\n-            data_gen_queue.close()\n-        else:\n-            # Wait for all threads to finish\n-            for p in generator_threads:\n-                if p.is_alive():\n-                    p.join()\n+        finally:\n+            if enqueuer is not None:\n+                enqueuer.stop()\n+\n         if len(all_outs) == 1:\n             return all_outs[0]\n         return all_outs\n\n@@ -177,6 +177,102 @@ def test_multiprocessing_evaluating():\n     assert reached_end\n \n \n+@keras_test\n+def test_multiprocessing_fit_error():\n+\n+    batch_size = 32\n+    good_batches = 5\n+\n+    def myGenerator():\n+        \"\"\"Raises an exception after a few good batches\"\"\"\n+        for i in range(good_batches):\n+            yield (np.random.randint(batch_size, 256, (500, 2)),\n+                   np.random.randint(batch_size, 2, 500))\n+        raise RuntimeError\n+\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2, )))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    samples = batch_size * (good_batches + 1)\n+\n+    with pytest.raises(Exception):\n+        model.fit_generator(\n+            myGenerator(), samples, 1,\n+            nb_worker=4, pickle_safe=True,\n+        )\n+\n+    with pytest.raises(Exception):\n+        model.fit_generator(\n+            myGenerator(), samples, 1,\n+            pickle_safe=False,\n+        )\n+\n+\n+@keras_test\n+def test_multiprocessing_evaluate_error():\n+\n+    batch_size = 32\n+    good_batches = 5\n+\n+    def myGenerator():\n+        \"\"\"Raises an exception after a few good batches\"\"\"\n+        for i in range(good_batches):\n+            yield (np.random.randint(batch_size, 256, (500, 2)),\n+                   np.random.randint(batch_size, 2, 500))\n+        raise RuntimeError\n+\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2, )))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    samples = batch_size * (good_batches + 1)\n+\n+    with pytest.raises(Exception):\n+        model.evaluate_generator(\n+            myGenerator(), samples, 1,\n+            nb_worker=4, pickle_safe=True,\n+        )\n+\n+    with pytest.raises(Exception):\n+        model.evaluate_generator(\n+            myGenerator(), samples, 1,\n+            pickle_safe=False,\n+        )\n+\n+\n+@keras_test\n+def test_multiprocessing_predict_error():\n+\n+    batch_size = 32\n+    good_batches = 5\n+\n+    def myGenerator():\n+        \"\"\"Raises an exception after a few good batches\"\"\"\n+        for i in range(good_batches):\n+            yield (np.random.randint(batch_size, 256, (500, 2)),\n+                   np.random.randint(batch_size, 2, 500))\n+        raise RuntimeError\n+\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2, )))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    samples = batch_size * (good_batches + 1)\n+\n+    with pytest.raises(Exception):\n+        model.predict_generator(\n+            myGenerator(), samples, 1,\n+            nb_worker=4, pickle_safe=True,\n+        )\n+\n+    with pytest.raises(Exception):\n+        model.predict_generator(\n+            myGenerator(), samples, 1,\n+            pickle_safe=False,\n+        )\n+\n+\n if __name__ == '__main__':\n \n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
