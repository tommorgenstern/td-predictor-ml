{"custom_id": "keras#a563a8446e8b7c568cefa18225d2156f75ae7c5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 12 | Files Changed: 6 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 19 | Churn Cumulative: 5753 | Contributors (this commit): 40 | Commits (past 90d): 63 | Contributors (cumulative): 55 | DMM Complexity: None\n\nDIFF:\n@@ -385,7 +385,7 @@ class Model(object):\n \n             # if obj is any numpy type\n             if type(obj).__module__ == np.__name__:\n-                return obj.item();\n+                return obj.item()\n \n             # if obj is a python 'type'\n             if type(obj).__name__ == type.__name__:\n\n@@ -7,7 +7,8 @@ from scipy import linalg\n \n from os import listdir\n from os.path import isfile, join\n-import random, math\n+import random\n+import math\n from six.moves import range\n \n '''\n@@ -74,8 +75,6 @@ def random_zoom(x, rg, fill_mode=\"nearest\", cval=0.):\n     return x # shape of result will be different from shape of input!\n \n \n-\n-\n def array_to_img(x, scale=True):\n     from PIL import Image\n     x = x.transpose(1, 2, 0) \n@@ -113,11 +112,10 @@ def load_img(path, grayscale=False):\n \n \n def list_pictures(directory, ext='jpg|jpeg|bmp|png'):\n-    return [join(directory,f) for f in listdir(directory) \\\n+    return [join(directory,f) for f in listdir(directory)\n             if isfile(join(directory,f)) and re.match('([\\w]+\\.(?:' + ext + '))', f)]\n \n \n-\n class ImageDataGenerator(object):\n     '''\n         Generate minibatches with \n@@ -141,7 +139,6 @@ class ImageDataGenerator(object):\n         self.std = None\n         self.principal_components = None\n \n-\n     def flow(self, X, y, batch_size=32, shuffle=False, seed=None, save_to_dir=None, save_prefix=\"\", save_format=\"jpeg\"):\n         if seed:\n             random.seed(seed)\n@@ -175,7 +172,6 @@ class ImageDataGenerator(object):\n \n             yield bX, y[b*batch_size:b*batch_size+nb_samples]\n \n-\n     def standardize(self, x):\n         if self.featurewise_center:\n             x -= self.mean\n@@ -194,7 +190,6 @@ class ImageDataGenerator(object):\n \n         return x\n \n-\n     def random_transform(self, x):\n         if self.rotation_range:\n             x = random_rotation(x, self.rotation_range)\n\n@@ -5,7 +5,8 @@\n '''\n from __future__ import absolute_import\n \n-import string, sys\n+import string\n+import sys\n import numpy as np\n from six.moves import range\n from six.moves import zip\n\n@@ -188,7 +188,6 @@ def test_upsampling_2d():\n     input_nb_row = 11\n     input_nb_col = 12\n \n-\n     for dim_ordering in ['th', 'tf']:\n         if dim_ordering == 'th':\n             input = np.random.rand(nb_samples, stack_size, input_nb_row,\n\n@@ -65,7 +65,6 @@ def _runner(layer_class):\n     assert(out4.max() != out5.max())\n \n \n-\n def test_SimpleRNN():\n     _runner(recurrent.SimpleRNN)\n \n\n@@ -357,6 +357,7 @@ def test_merge_overlap():\n \n def test_lambda():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n+\n     def func(X):\n         s = X[0]\n         for i in range(1, len(X)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#70f0fe515a6b9300f3de5e82f1e7d4c01ec40b0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 65 | Churn Cumulative: 263 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -163,29 +163,64 @@ loss += settings['dream_l2'] * K.sum(K.square(dream)) / (3 * img_width * img_hei\n # compute the gradients of the dream wrt the loss\n grads = K.gradients(loss, dream)\n \n-# set up helper functions to extract the loss and gradients\n-# from the computational graph as Numpy arrays\n-f_grads = K.function([dream], grads)\n-def eval_grads(x):\n-    x = x.reshape((1, 3, img_width, img_height))\n-    return np.array(f_grads([x])).flatten().astype('float64')\n+outputs = [loss]\n+if type(grads) in {list, tuple}:\n+    outputs += grads\n+else:\n+    outputs.append(grads)\n \n-f_loss = K.function([dream], [loss])\n-def eval_loss(x):\n+f_outputs = K.function([dream], outputs)\n+def eval_loss_and_grads(x):\n     x = x.reshape((1, 3, img_width, img_height))\n-    return f_loss([x])[0].astype('float64')\n+    outs = f_outputs([x])\n+    loss_value = outs[0]\n+    if len(outs[1:]) == 1:\n+        grad_values = outs[1].flatten().astype('float64')\n+    else:\n+        grad_values = np.array(outs[1:]).flatten().astype('float64')\n+    return loss_value, grad_values\n \n-# add a random jitter to the initial image. This will be reverted at decoding time\n-random_jitter = (settings['jitter'] * 2) * (np.random.random((3, img_width, img_height)) - 0.5)\n-x = preprocess_image(base_image_path)\n-x += random_jitter\n+# this Evaluator class makes it possible\n+# to compute loss and gradients in one pass\n+# while retrieving them via two separate functions,\n+# \"loss\" and \"grads\". This is done because scipy.optimize\n+# requires separate functions for loss and gradients,\n+# but computing them separately would be inefficient.\n+class Evaluator(object):\n+    def __init__(self):\n+        self.loss_value = None\n+        self.grads_values = None\n+\n+    def loss(self, x):\n+        assert self.loss_value is None\n+        loss_value, grad_values = eval_loss_and_grads(x)\n+        self.loss_value = loss_value\n+        self.grad_values = grad_values\n+        return self.loss_value\n+\n+    def grads(self, x):\n+        assert self.loss_value is not None\n+        grad_values = np.copy(self.grad_values)\n+        self.loss_value = None\n+        self.grad_values = None\n+        return grad_values\n+\n+evaluator = Evaluator()\n \n # run scipy-based optimization (L-BFGS) over the pixels of the generated image\n # so as to minimize the loss\n+x = preprocess_image(base_image_path)\n for i in range(5):\n+    print('Start of iteration', i)\n     start_time = time.time()\n-    x, min_val, info = fmin_l_bfgs_b(eval_loss, x.flatten(),\n-                                     fprime=eval_grads, maxfun=7)\n+\n+    # add a random jitter to the initial image. This will be reverted at decoding time\n+    random_jitter = (settings['jitter'] * 2) * (np.random.random((3, img_width, img_height)) - 0.5)\n+    x += random_jitter\n+\n+    # run L-BFGS for 7 steps\n+    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n+                                     fprime=evaluator.grads, maxfun=7)\n     print('Current loss value:', min_val)\n     # decode the dream and save it\n     x = x.reshape((3, img_width, img_height))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a18932cb658c012808dd78bbd7f5bf1fe712e6c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 56 | Churn Cumulative: 350 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,6 +53,7 @@ from scipy.misc import imread, imresize, imsave\n import numpy as np\n from scipy.optimize import fmin_l_bfgs_b\n import time\n+import os\n import argparse\n import h5py\n \n@@ -154,6 +155,7 @@ model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n # (trained on ImageNet, won the ILSVRC competition in 2014)\n # note: when there is a complete match between your model definition\n # and your weight savefile, you can simply call model.load_weights(filename)\n+assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n f = h5py.File(weights_path)\n for k in range(f.attrs['nb_layers']):\n     if k >= len(model.layers):\n@@ -226,17 +228,49 @@ loss += total_variation_weight * total_variation_loss(combination_image)\n # get the gradients of the generated image wrt the loss\n grads = K.gradients(loss, combination_image)\n \n-# set up helper functions to extract the loss and gradients\n-# from the computational graph as Numpy arrays\n-f_grads = K.function([combination_image], grads)\n-def eval_grads(x):\n-    x = x.reshape((1, 3, img_width, img_height))\n-    return np.array(f_grads([x])).flatten().astype('float64')\n+outputs = [loss]\n+if type(grads) in {list, tuple}:\n+    outputs += grads\n+else:\n+    outputs.append(grads)\n \n-f_loss = K.function([combination_image], [loss])\n-def eval_loss(x):\n+f_outputs = K.function([combination_image], outputs)\n+def eval_loss_and_grads(x):\n     x = x.reshape((1, 3, img_width, img_height))\n-    return f_loss([x])[0].astype('float64')\n+    outs = f_outputs([x])\n+    loss_value = outs[0]\n+    if len(outs[1:]) == 1:\n+        grad_values = outs[1].flatten().astype('float64')\n+    else:\n+        grad_values = np.array(outs[1:]).flatten().astype('float64')\n+    return loss_value, grad_values\n+\n+# this Evaluator class makes it possible\n+# to compute loss and gradients in one pass\n+# while retrieving them via two separate functions,\n+# \"loss\" and \"grads\". This is done because scipy.optimize\n+# requires separate functions for loss and gradients,\n+# but computing them separately would be inefficient.\n+class Evaluator(object):\n+    def __init__(self):\n+        self.loss_value = None\n+        self.grads_values = None\n+\n+    def loss(self, x):\n+        assert self.loss_value is None\n+        loss_value, grad_values = eval_loss_and_grads(x)\n+        self.loss_value = loss_value\n+        self.grad_values = grad_values\n+        return self.loss_value\n+\n+    def grads(self, x):\n+        assert self.loss_value is not None\n+        grad_values = np.copy(self.grad_values)\n+        self.loss_value = None\n+        self.grad_values = None\n+        return grad_values\n+\n+evaluator = Evaluator()\n \n # run scipy-based optimization (L-BFGS) over the pixels of the generated image\n # so as to minimize the neural style loss\n@@ -244,8 +278,8 @@ x = np.random.uniform(0, 255, (1, 3, img_width, img_height))\n for i in range(10):\n     print('Start of iteration', i)\n     start_time = time.time()\n-    x, min_val, info = fmin_l_bfgs_b(eval_loss, x.flatten(),\n-                                     fprime=eval_grads, maxfun=20)\n+    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n+                                     fprime=evaluator.grads, maxfun=20)\n     print('Current loss value:', min_val)\n     # save current generated image\n     img = deprocess_image(x.reshape((3, img_width, img_height)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83aaadaa9d69214880d20b1e2bd9715a6c37fbe6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 67 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,7 @@ from __future__ import absolute_import\n from __future__ import print_function\n import os\n import json\n+import sys\n from .common import epsilon, floatx, set_epsilon, set_floatx\n \n _keras_base_dir = os.path.expanduser('~')\n@@ -41,10 +42,10 @@ if 'KERAS_BACKEND' in os.environ:\n     _BACKEND = _backend\n \n if _BACKEND == 'theano':\n-    print('Using Theano backend.')\n+    sys.stderr.write('Using Theano backend.\\n')\n     from .theano_backend import *\n elif _BACKEND == 'tensorflow':\n-    print('Using TensorFlow backend.')\n+    sys.stderr.write('Using TensorFlow backend.\\n')\n     from .tensorflow_backend import *\n else:\n     raise Exception('Unknown backend: ' + str(_BACKEND))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3bfe4eace955f8b6b46b389f37375b9cc4ae94a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 8 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,8 @@\n+from __future__ import absolute_import\n+from .core import *\n+from .convolutional import *\n+from .recurrent import *\n+from .normalization import *\n+from .embeddings import *\n+from .noise import *\n+from .advanced_activations import *\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0abed354b5287f5f456d352d265f821f8a55a1c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 356 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -89,11 +89,17 @@ assert img_height == img_width, 'Due to the use of the Gram matrix, width and he\n def preprocess_image(image_path):\n     img = imresize(imread(image_path), (img_width, img_height))\n     img = img.transpose((2, 0, 1)).astype('float64')\n+    img[:, :, 0] -= 103.939\n+    img[:, :, 1] -= 116.779\n+    img[:, :, 2] -= 123.68\n     img = np.expand_dims(img, axis=0)\n     return img\n \n # util function to convert a tensor into a valid image\n def deprocess_image(x):\n+    x[:, :, 0] += 103.939\n+    x[:, :, 1] += 116.779\n+    x[:, :, 2] += 123.68\n     x = x.transpose((1, 2, 0))\n     x = np.clip(x, 0, 255).astype('uint8')\n     return x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0debec1c11803f0f7b6f2c75e2c6762bfdc4a468", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 399 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -74,6 +74,7 @@ class BatchNormalization(Layer):\n     def get_output(self, train):\n         X = self.get_input(train)\n         if self.mode == 0:\n+            if train:\n                 m = K.mean(X, axis=0)\n                 std = K.mean(K.square(X - m) + self.epsilon, axis=0)\n                 std = K.sqrt(std)\n@@ -81,6 +82,8 @@ class BatchNormalization(Layer):\n                 std_update = self.momentum * self.running_std + (1-self.momentum) * std\n                 self.updates = [(self.running_mean, mean_update),\n                                 (self.running_std, std_update)]\n+                X_normed = (X - m) / (std + self.epsilon)\n+            else:\n                 X_normed = ((X - self.running_mean) /\n                             (self.running_std + self.epsilon))\n         elif self.mode == 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#160196137fde3fd57c5051aea75ea9830eb25d62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 34 | Churn Cumulative: 798 | Contributors (this commit): 5 | Commits (past 90d): 22 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -96,38 +96,48 @@ def gather(reference, indices):\n \n # ELEMENT-WISE OPERATIONS\n \n-def max(x, axis=None, keepdims=False):\n+def normalize_axis(axis, ndim):\n+    if type(axis) is tuple:\n+        axis = list(axis)\n+    if type(axis) is list:\n+        for i, a in enumerate(axis):\n+            if a is not None and a < 0:\n+                axis[i] = a % ndim\n+    else:\n         if axis is not None and axis < 0:\n-        axis = axis % len(x.get_shape())\n+            axis = axis % ndim\n+    return axis\n+\n+\n+def max(x, axis=None, keepdims=False):\n+    axis = normalize_axis(axis, ndim(x))\n     return tf.reduce_max(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def min(x, axis=None, keepdims=False):\n-    if axis is not None and axis < 0:\n-        axis = axis % len(x.get_shape())\n+    axis = normalize_axis(axis, ndim(x))\n     return tf.reduce_min(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def sum(x, axis=None, keepdims=False):\n     '''Sum of the values in a tensor, alongside the specified axis.\n     '''\n-    if axis is not None and axis < 0:\n-        axis = axis % len(x.get_shape())\n+    axis = normalize_axis(axis, ndim(x))\n     return tf.reduce_sum(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def prod(x, axis=None, keepdims=False):\n     '''Multiply the values in a tensor, alongside the specified axis.\n     '''\n+    axis = normalize_axis(axis, ndim(x))\n     return tf.reduce_prod(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def std(x, axis=None, keepdims=False):\n-    if axis is not None and axis < 0:\n-        axis = axis % len(x.get_shape())\n+    axis = normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n         x = tf.cast(x, _FLOATX)\n-    m = tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)\n+    m = tf.reduce_mean(x, reduction_indices=axis, keep_dims=True)\n     devs_squared = tf.square(x - m)\n     return tf.sqrt(tf.reduce_mean(devs_squared,\n                                   reduction_indices=axis,\n@@ -135,8 +145,7 @@ def std(x, axis=None, keepdims=False):\n \n \n def mean(x, axis=None, keepdims=False):\n-    if axis is not None and axis < 0:\n-        axis = axis % len(x.get_shape())\n+    axis = normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n         x = tf.cast(x, _FLOATX)\n     return tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)\n@@ -147,8 +156,7 @@ def any(x, axis=None, keepdims=False):\n \n     Return array of int8 (0s and 1s).\n     '''\n-    if axis is not None and axis < 0:\n-        axis = axis % len(x.get_shape())\n+    axis = normalize_axis(axis, ndim(x))\n     x = tf.cast(x, tf.bool)\n     x = tf.reduce_any(x, reduction_indices=axis, keep_dims=keepdims)\n     return tf.cast(x, tf.int8)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#85a1225cb38fac515738f881edab8ee46adb95e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 448 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,13 +20,17 @@ class BatchNormalization(Layer):\n         epsilon: small float > 0. Fuzz parameter.\n         mode: integer, 0 or 1.\n             - 0: feature-wise normalization.\n-                If the input has multiple feature dimensions,\n-                each will be normalized separately\n-                (e.g. for an image input with shape\n-                `(channels, rows, cols)`,\n-                each combination of a channel, row and column\n-                will be normalized separately).\n+                Each feature map in the input will\n+                be normalized separately. The axis on which\n+                to normalize is specified by the `axis` argument.\n+                Note that if the input is a 4D image tensor\n+                using Theano conventions (samples, channels, rows, cols)\n+                then you should set `axis` to `1` to normalize along\n+                the channels axis.\n             - 1: sample-wise normalization. This mode assumes a 2D input.\n+        axis: integer, axis along which to normalize in mode 0. For instance,\n+            if your input tensor has shape (samples, channels, rows, cols),\n+            set axis to 1 to normalize per feature map (channels axis).\n         momentum: momentum in the computation of the\n             exponential average of the mean and standard deviation\n             of the data, for feature-wise normalization.\n@@ -37,25 +41,26 @@ class BatchNormalization(Layer):\n     # References\n         - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/pdf/1502.03167v3.pdf)\n     '''\n-    def __init__(self, epsilon=1e-6, mode=0, momentum=0.9,\n+    def __init__(self, epsilon=1e-6, mode=0, axis=-1, momentum=0.9,\n                  weights=None, **kwargs):\n         self.init = initializations.get(\"uniform\")\n         self.epsilon = epsilon\n         self.mode = mode\n+        self.axis = axis\n         self.momentum = momentum\n         self.initial_weights = weights\n         super(BatchNormalization, self).__init__(**kwargs)\n \n     def build(self):\n         input_shape = self.input_shape  # starts with samples axis\n-        input_shape = input_shape[1:]\n+        shape = (input_shape[self.axis],)\n \n-        self.gamma = self.init(input_shape)\n-        self.beta = K.zeros(input_shape)\n+        self.gamma = self.init(shape)\n+        self.beta = K.zeros(shape)\n \n         self.params = [self.gamma, self.beta]\n-        self.running_mean = K.zeros(input_shape)\n-        self.running_std = K.ones(input_shape)\n+        self.running_mean = K.zeros(shape)\n+        self.running_std = K.ones(shape)\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -74,18 +79,28 @@ class BatchNormalization(Layer):\n     def get_output(self, train):\n         X = self.get_input(train)\n         if self.mode == 0:\n+            input_shape = self.input_shape\n+            reduction_axes = range(len(input_shape))\n+            broadcast_shape = [1] * len(input_shape)\n+            broadcast_shape[self.axis] = input_shape[self.axis]\n+            del reduction_axes[self.axis]\n             if train:\n-                m = K.mean(X, axis=0)\n-                std = K.mean(K.square(X - m) + self.epsilon, axis=0)\n+                m = K.mean(X, axis=reduction_axes)\n+                brodcast_m = K.reshape(m, broadcast_shape)\n+                std = K.mean(K.square(X - brodcast_m) + self.epsilon, axis=reduction_axes)\n                 std = K.sqrt(std)\n+                brodcast_std = K.reshape(std, broadcast_shape)\n                 mean_update = self.momentum * self.running_mean + (1-self.momentum) * m\n                 std_update = self.momentum * self.running_std + (1-self.momentum) * std\n                 self.updates = [(self.running_mean, mean_update),\n                                 (self.running_std, std_update)]\n-                X_normed = (X - m) / (std + self.epsilon)\n+                X_normed = (X - brodcast_m) / (brodcast_std + self.epsilon)\n             else:\n-                X_normed = ((X - self.running_mean) /\n-                            (self.running_std + self.epsilon))\n+                brodcast_m = K.reshape(self.running_mean, broadcast_shape)\n+                brodcast_std = K.reshape(self.running_std, broadcast_shape)\n+                X_normed = ((X - brodcast_m) /\n+                            (brodcast_std + self.epsilon))\n+            out = K.reshape(self.gamma, broadcast_shape) * X_normed + K.reshape(self.beta, broadcast_shape)\n         elif self.mode == 1:\n             m = K.mean(X, axis=-1, keepdims=True)\n             std = K.std(X, axis=-1, keepdims=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c103559a606cb4d4c575e18741ec3c06c151afb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 436 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -114,16 +114,20 @@ class TestBackend(object):\n \n         check_single_tensor_operation('min', (4, 2))\n         check_single_tensor_operation('min', (4, 2), axis=1, keepdims=True)\n+        check_single_tensor_operation('min', (4, 2, 3), axis=[1, -1])\n \n         check_single_tensor_operation('mean', (4, 2))\n         check_single_tensor_operation('mean', (4, 2), axis=1, keepdims=True)\n         check_single_tensor_operation('mean', (4, 2, 3), axis=-1, keepdims=True)\n+        check_single_tensor_operation('mean', (4, 2, 3), axis=[1, -1])\n \n         check_single_tensor_operation('std', (4, 2))\n         check_single_tensor_operation('std', (4, 2), axis=1, keepdims=True)\n+        check_single_tensor_operation('std', (4, 2, 3), axis=[1, -1])\n \n         check_single_tensor_operation('prod', (4, 2))\n         check_single_tensor_operation('prod', (4, 2), axis=1, keepdims=True)\n+        check_single_tensor_operation('prod', (4, 2, 3), axis=[1, -1])\n \n         # does not work yet, wait for bool <-> int casting in TF (coming soon)\n         # check_single_tensor_operation('any', (4, 2))\n\n@@ -17,7 +17,7 @@ input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n def test_batchnorm_mode_0():\n     np.random.seed(1337)\n     model = Sequential()\n-    norm_m0 = normalization.BatchNormalization(input_shape=(10,))\n+    norm_m0 = normalization.BatchNormalization(mode=0, input_shape=(10,))\n     model.add(norm_m0)\n     model.compile(loss='mse', optimizer='sgd')\n \n@@ -31,6 +31,22 @@ def test_batchnorm_mode_0():\n     assert_allclose(K.eval(K.std(out)), 1.0, atol=1e-1)\n \n \n+def test_batchnorm_mode_0_convnet():\n+    model = Sequential()\n+    norm_m0 = normalization.BatchNormalization(mode=0, axis=1, input_shape=(3, 4, 4))\n+    model.add(norm_m0)\n+    model.compile(loss='mse', optimizer='sgd')\n+\n+    # centered on 5.0, variance 10.0\n+    X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\n+    model.fit(X, X, nb_epoch=5, verbose=0)\n+    norm_m0.input = K.variable(X)\n+    out = (norm_m0.get_output(train=True) - K.reshape(norm_m0.beta, (1, 3, 1, 1))) / K.reshape(norm_m0.gamma, (1, 3, 1, 1))\n+\n+    assert_allclose(K.eval(K.mean(out, axis=(0, 2, 3))), 0.0, atol=1e-1)\n+    assert_allclose(K.eval(K.std(out, axis=(0, 2, 3))), 1.0, atol=1e-1)\n+\n+\n def test_batchnorm_mode_1():\n     np.random.seed(1337)\n     norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e443527d28d24219c81dc1a2ec1c649de7bb16c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 97 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 97 | Churn Cumulative: 97 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,97 @@\n+'''Example script to predict sequence using stateful rnns.\n+At least 10 epochs are required before the generated text\n+starts sounding coherent.\n+'''\n+\n+import numpy as np\n+import matplotlib.pyplot as mpl\n+from keras.models import Sequential\n+from keras.layers.core import Dense\n+from keras.layers.recurrent import LSTM\n+\n+\n+# since we are using stateful rnn tsteps can be set to 1\n+tsteps = 1\n+batch_size = 25\n+epochs = 25\n+# number of elements ahead that are used to make the prediction\n+lahead = 1\n+\n+\n+def gen_cosine_amp(amp=100, period=25, x0=0, xn=50000, step=1, k=0.0001):\n+    \"\"\"\n+    Generates an absolute cosine time series with the amplitude exponentially\n+    decreasing\n+\n+    Keyword arguments:\n+    amp -- amplitude of the cosine function\n+    period -- period of the cosine function\n+    x0 -- initial x of the time series\n+    xn -- final x of the time series\n+    step -- step of the time series discretization\n+    k -- exponential rate\n+    \"\"\"\n+    cos = np.zeros(((xn - x0) * step, 1, 1))\n+    for i in range(len(cos)):\n+        idx = x0 + i * step\n+        cos[i, 0, 0] = amp * np.cos(idx / (2 * np.pi * period))\n+        cos[i, 0, 0] = cos[i, 0, 0] * np.exp(-k * idx)\n+    return cos\n+\n+\n+print('Creating Data')\n+cos = gen_cosine_amp()\n+print('Input shape:')\n+print(cos.shape)\n+print('Calculating expected predicted_out')\n+expected_out = np.zeros((len(cos), 1))\n+for i in range(len(cos) - lahead):\n+    expected_out[i, 0] = np.mean(cos[i + 1:i + lahead + 1])\n+\n+print('Output shape')\n+print(expected_out.shape)\n+\n+print('Creating Model')\n+model = Sequential()\n+model.add(\n+    LSTM(\n+        50,\n+        batch_input_shape=(\n+            batch_size,\n+            tsteps,\n+            1),\n+        return_sequences=True,\n+        stateful=True))\n+model.add(\n+    LSTM(\n+        50,\n+        batch_input_shape=(\n+            batch_size,\n+            tsteps,\n+            1),\n+        return_sequences=False,\n+        stateful=True))\n+model.add(Dense(1))\n+model.compile(loss='rmse', optimizer='rmsprop')\n+\n+print('Training')\n+for i in range(epochs):\n+    model.fit(\n+        cos,\n+        expected_out,\n+        batch_size=batch_size,\n+        verbose=1,\n+        nb_epoch=1)\n+    model.reset_states()\n+\n+print('Predicting')\n+predicted_out = model.predict(cos, batch_size=batch_size)\n+\n+print('Ploting Results')\n+mpl.subplot(2, 1, 1)\n+mpl.plot(expected_out)\n+mpl.title('Expected')\n+mpl.subplot(2, 1, 2)\n+mpl.plot(predicted_out)\n+mpl.title('Predicted')\n+mpl.show()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9d120bf9e02520e91cb7a3cc7453f27d9c404412", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 450 | Contributors (this commit): 7 | Commits (past 90d): 12 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -80,7 +80,7 @@ class BatchNormalization(Layer):\n         X = self.get_input(train)\n         if self.mode == 0:\n             input_shape = self.input_shape\n-            reduction_axes = range(len(input_shape))\n+            reduction_axes = list(range(len(input_shape)))\n             broadcast_shape = [1] * len(input_shape)\n             broadcast_shape[self.axis] = input_shape[self.axis]\n             del reduction_axes[self.axis]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c98633cd7c0922800bc63656a2d899aa768362e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 30 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,13 +66,15 @@ def test_batchnorm_shapes():\n     Test batch normalization with various input shapes\n     \"\"\"\n     for inp in input_shapes:\n-        norm_m0 = normalization.BatchNormalization(input_shape=inp.shape, mode=0)\n+        norm_m0 = normalization.BatchNormalization(batch_input_shape=inp.shape, mode=0)\n         norm_m0.input = K.variable(inp)\n-        out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n+        out = norm_m0.get_output(train=True)\n+        K.eval(out)\n \n-        norm_m1 = normalization.BatchNormalization(input_shape=inp.shape, mode=1)\n+        norm_m1 = normalization.BatchNormalization(batch_input_shape=inp.shape, mode=1)\n         norm_m1.input = K.variable(inp)\n-        out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n+        out = norm_m1.get_output(train=True)\n+        K.eval(out)\n \n \n def test_batchnorm_weight_init():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d1af488d1096a2a795f63516beaeb753c9da98c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1507 | Contributors (this commit): 14 | Commits (past 90d): 47 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -494,7 +494,6 @@ def switch(condition, then_expression, else_expression):\n                                            lambda: else_expression)\n \n \n-\n # NN OPERATIONS\n \n def relu(x, alpha=0., max_value=None):\n\n@@ -81,9 +81,9 @@ class BatchNormalization(Layer):\n         if self.mode == 0:\n             input_shape = self.input_shape\n             reduction_axes = list(range(len(input_shape)))\n+            del reduction_axes[self.axis]\n             broadcast_shape = [1] * len(input_shape)\n             broadcast_shape[self.axis] = input_shape[self.axis]\n-            del reduction_axes[self.axis]\n             if train:\n                 m = K.mean(X, axis=reduction_axes)\n                 brodcast_m = K.reshape(m, broadcast_shape)\n\n@@ -37,7 +37,6 @@ def _runner(layer_class):\n                                    mask_zero=True,\n                                    input_length=timesteps,\n                                    batch_input_shape=(nb_samples, timesteps)))\n-\n     layer = layer_class(output_dim, return_sequences=False,\n                         stateful=True,\n                         weights=None)\n@@ -88,6 +87,7 @@ def _runner(layer_class):\n \n     assert_allclose(out7, out6, atol=1e-5)\n \n+\n def test_SimpleRNN():\n     _runner(recurrent.SimpleRNN)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#852fe9cc7b02a7fd7edbd47efd0d66f3a960d99f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 13 | Churn Cumulative: 325 | Contributors (this commit): 7 | Commits (past 90d): 12 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,10 @@ def softmax(x):\n         # apply softmax to each timestep\n         def step(x, states):\n             return K.softmax(x), []\n-        last_output, outputs, states = K.rnn(step, x, [], masking=False)\n+        last_output, outputs, states = K.rnn(step, x,\n+                                             K.shape(x)[-1],\n+                                             [],\n+                                             mask=None)\n         return outputs\n     else:\n         raise Exception('Cannot apply softmax to a tensor that is not 2D or 3D. ' +\n\n@@ -32,6 +32,14 @@ def test_softmax():\n     assert_allclose(result, expected, rtol=1e-05)\n \n \n+def test_time_distributed_softmax():\n+    x = K.placeholder(shape=(1, 1, 5))\n+    f = K.function([x], [activations.softmax(x)])\n+    test_values = get_standard_values()\n+    test_values = np.reshape(test_values, (1, 1, np.size(test_values)))\n+    f([test_values])[0]\n+\n+\n def test_softplus():\n     '''\n     Test using a reference softplus implementation\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#45714e343f36292cec938dc01ebb8a2dec10cea4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 71 | Files Changed: 3 | Hunks: 39 | Methods Changed: 11 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 137 | Churn Cumulative: 4599 | Contributors (this commit): 41 | Commits (past 90d): 33 | Contributors (cumulative): 46 | DMM Complexity: 0.18181818181818182\n\nDIFF:\n@@ -17,9 +17,8 @@ from keras.preprocessing.image import ImageDataGenerator\n from keras.models import Sequential\n from keras.layers.core import Dense, Dropout, Activation, Flatten\n from keras.layers.convolutional import Convolution2D, MaxPooling2D\n-from keras.optimizers import SGD, Adadelta, Adagrad\n-from keras.utils import np_utils, generic_utils\n-from six.moves import range\n+from keras.optimizers import SGD\n+from keras.utils import np_utils\n \n batch_size = 32\n nb_classes = 10\n@@ -75,24 +74,23 @@ X_train /= 255\n X_test /= 255\n \n if not data_augmentation:\n-    print('Not using data augmentation or normalization')\n-    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n-    score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n-    print('Test score:', score)\n-\n+    print('Not using data augmentation.')\n+    model.fit(X_train, Y_train, batch_size=batch_size,\n+              nb_epoch=nb_epoch, show_accuracy=True,\n+              validation_data=(X_test, Y_test), shuffle=True)\n else:\n-    print('Using real time data augmentation')\n+    print('Using real-time data augmentation.')\n \n     # this will do preprocessing and realtime data augmentation\n     datagen = ImageDataGenerator(\n-        featurewise_center=True,  # set input mean to 0 over the dataset\n+        featurewise_center=False,  # set input mean to 0 over the dataset\n         samplewise_center=False,  # set each sample mean to 0\n-        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n+        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n         samplewise_std_normalization=False,  # divide each input by its std\n         zca_whitening=False,  # apply ZCA whitening\n-        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n-        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n-        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n+        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n+        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n+        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n         horizontal_flip=True,  # randomly flip images\n         vertical_flip=False)  # randomly flip images\n \n@@ -100,20 +98,9 @@ else:\n     # (std, mean, and principal components if ZCA whitening is applied)\n     datagen.fit(X_train)\n \n-    for e in range(nb_epoch):\n-        print('-'*40)\n-        print('Epoch', e)\n-        print('-'*40)\n-        print('Training...')\n-        # batch train with realtime data augmentation\n-        progbar = generic_utils.Progbar(X_train.shape[0])\n-        for X_batch, Y_batch in datagen.flow(X_train, Y_train):\n-            loss = model.train_on_batch(X_batch, Y_batch)\n-            progbar.add(X_batch.shape[0], values=[('train loss', loss[0])])\n-\n-        print('Testing...')\n-        # test time!\n-        progbar = generic_utils.Progbar(X_test.shape[0])\n-        for X_batch, Y_batch in datagen.flow(X_test, Y_test):\n-            score = model.test_on_batch(X_batch, Y_batch)\n-            progbar.add(X_batch.shape[0], values=[('test loss', score[0])])\n+    # fit the model on the batches generated by datagen.flow()\n+    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n+                        samples_per_epoch=X_train.shape[0],\n+                        nb_epoch=nb_epoch, show_accuracy=True,\n+                        validation_data=(X_test, Y_test),\n+                        nb_worker=1)\n\n@@ -908,13 +908,14 @@ class Sequential(Model, containers.Sequential):\n         def input_validation(generator_output):\n             if not hasattr(generator_output, '__len__'):\n                 _stop.set()\n-                raise Exception('The generator output must be a tuple.')\n+                raise Exception('The generator output must be a tuple. Found: ' + str(type(generator_output)))\n             if len(generator_output) == 2:\n                 X, y = generator_output\n                 if type(X) == list:\n                     assert len(set([len(a) for a in X] + [len(y)])) == 1\n                 else:\n                     assert len(X) == len(y)\n+                    X = [X]\n                 sample_weight = None\n             elif len(generator_output) == 3:\n                 X, y, sample_weight = generator_output\n@@ -922,6 +923,7 @@ class Sequential(Model, containers.Sequential):\n                     assert len(set([len(a) for a in X] + [len(y), len(sample_weight)])) == 1\n                 else:\n                     assert len(X) == len(y) == len(sample_weight)\n+                    X = [X]\n             else:\n                 _stop.set()\n                 raise Exception('The generator output tuple must have '\n@@ -937,14 +939,17 @@ class Sequential(Model, containers.Sequential):\n             while not _stop.is_set():\n                 try:\n                     if generator_queue.qsize() < max_queue_size:\n+                        try:\n                             generator_output = next(generator)\n+                        except ValueError:\n+                            continue\n                         generator_queue.put(generator_output)\n                         i += 1\n                     else:\n                         time.sleep(wait_time)\n                 except:\n                     _stop.set()\n-                    return\n+                    raise\n \n         generator_threads = [threading.Thread(target=generator_task) for _ in range(nb_worker)]\n         for thread in generator_threads:\n@@ -956,6 +961,7 @@ class Sequential(Model, containers.Sequential):\n             samples_seen = 0\n             batch_index = 0\n             while samples_seen < samples_per_epoch:\n+                generator_output = None\n                 while not _stop.is_set():\n                     if not generator_queue.empty():\n                         generator_output = generator_queue.get()\n\n@@ -11,45 +11,44 @@ import random\n import math\n from six.moves import range\n \n+'''Fairly basic set of tools for realtime data augmentation on image data.\n+Can easily be extended to include new transformations, new preprocessing methods, etc...\n '''\n-    Fairly basic set of tools for realtime data augmentation on image data.\n-    Can easily be extended to include new transforms, new preprocessing methods, etc...\n-'''\n+\n \n def random_rotation(x, rg, fill_mode=\"nearest\", cval=0.):\n     angle = random.uniform(-rg, rg)\n-    x = ndimage.interpolation.rotate(x, angle, axes=(1,2), reshape=False, mode=fill_mode, cval=cval)\n+    x = ndimage.interpolation.rotate(x, angle,\n+                                     axes=(1, 2),\n+                                     reshape=False,\n+                                     mode=fill_mode,\n+                                     cval=cval)\n     return x\n \n+\n def random_shift(x, wrg, hrg, fill_mode=\"nearest\", cval=0.):\n     crop_left_pixels = 0\n-    crop_right_pixels = 0\n     crop_top_pixels = 0\n-    crop_bottom_pixels = 0\n-\n-    original_w = x.shape[1]\n-    original_h = x.shape[2]\n \n     if wrg:\n         crop = random.uniform(0., wrg)\n         split = random.uniform(0, 1)\n         crop_left_pixels = int(split*crop*x.shape[1])\n-        crop_right_pixels = int((1-split)*crop*x.shape[1])\n-\n     if hrg:\n         crop = random.uniform(0., hrg)\n         split = random.uniform(0, 1)\n         crop_top_pixels = int(split*crop*x.shape[2])\n-        crop_bottom_pixels = int((1-split)*crop*x.shape[2])\n-\n-    x = ndimage.interpolation.shift(x, (0, crop_left_pixels, crop_top_pixels), mode=fill_mode, cval=cval)\n+    x = ndimage.interpolation.shift(x, (0, crop_left_pixels, crop_top_pixels),\n+                                    mode=fill_mode, cval=cval)\n     return x\n \n+\n def horizontal_flip(x):\n     for i in range(x.shape[0]):\n         x[i] = np.fliplr(x[i])\n     return x\n \n+\n def vertical_flip(x):\n     for i in range(x.shape[0]):\n         x[i] = np.flipud(x[i])\n@@ -60,18 +59,22 @@ def random_barrel_transform(x, intensity):\n     # TODO\n     pass\n \n+\n def random_shear(x, intensity):\n     # TODO\n     pass\n \n+\n def random_channel_shift(x, rg):\n     # TODO\n     pass\n \n+\n def random_zoom(x, rg, fill_mode=\"nearest\", cval=0.):\n     zoom_w = random.uniform(1.-rg, 1.)\n     zoom_h = random.uniform(1.-rg, 1.)\n-    x = ndimage.interpolation.zoom(x, zoom=(1., zoom_w, zoom_h), mode=fill_mode, cval=cval)\n+    x = ndimage.interpolation.zoom(x, zoom=(1., zoom_w, zoom_h),\n+                                   mode=fill_mode, cval=cval)\n     return x  # shape of result will be different from shape of input!\n \n \n@@ -106,7 +109,7 @@ def load_img(path, grayscale=False):\n     img = Image.open(path)\n     if grayscale:\n         img = img.convert('L')\n-    else: # Assure 3 channel even when loaded image is grayscale\n+    else:  # Ensure 3 channel even when loaded image is grayscale\n         img = img.convert('RGB')\n     return img\n \n@@ -117,8 +120,7 @@ def list_pictures(directory, ext='jpg|jpeg|bmp|png'):\n \n \n class ImageDataGenerator(object):\n-    '''\n-        Generate minibatches with \n+    '''Generate minibatches with\n     realtime data augmentation.\n     '''\n     def __init__(self,\n@@ -126,20 +128,20 @@ class ImageDataGenerator(object):\n                  samplewise_center=False,  # set each sample mean to 0\n                  featurewise_std_normalization=True,  # divide inputs by std of the dataset\n                  samplewise_std_normalization=False,  # divide each input by its std\n-\n                  zca_whitening=False,  # apply ZCA whitening\n                  rotation_range=0.,  # degrees (0 to 180)\n                  width_shift_range=0.,  # fraction of total width\n                  height_shift_range=0.,  # fraction of total height\n                  horizontal_flip=False,\n-                 vertical_flip=False,\n-                 ):\n+                 vertical_flip=False):\n         self.__dict__.update(locals())\n         self.mean = None\n         self.std = None\n         self.principal_components = None\n \n-    def flow(self, X, y, batch_size=32, shuffle=False, seed=None, save_to_dir=None, save_prefix=\"\", save_format=\"jpeg\"):\n+    def flow(self, X, y, batch_size=32, shuffle=False, seed=None,\n+             save_to_dir=None, save_prefix=\"\", save_format=\"jpeg\"):\n+        assert len(X) == len(y)\n         if seed:\n             random.seed(seed)\n \n@@ -150,27 +152,28 @@ class ImageDataGenerator(object):\n             np.random.seed(seed)\n             np.random.shuffle(y)\n \n-        nb_batch = int(math.ceil(float(X.shape[0])/batch_size))\n-        for b in range(nb_batch):\n-            batch_end = (b+1)*batch_size\n-            if batch_end > X.shape[0]:\n-                nb_samples = X.shape[0] - b*batch_size\n+        b = 0\n+        while 1:\n+            current_index = (b * batch_size) % X.shape[0]\n+            if X.shape[0] >= current_index + batch_size:\n+                current_batch_size = batch_size\n             else:\n-                nb_samples = batch_size\n-\n-            bX = np.zeros(tuple([nb_samples]+list(X.shape)[1:]))\n-            for i in range(nb_samples):\n-                x = X[b*batch_size+i]\n+                current_batch_size = X.shape[0] - current_index\n+            bX = np.zeros(tuple([current_batch_size] + list(X.shape)[1:]))\n+            for i in range(current_batch_size):\n+                x = X[current_index + i]\n                 x = self.random_transform(x.astype(\"float32\"))\n                 x = self.standardize(x)\n                 bX[i] = x\n-\n             if save_to_dir:\n-                for i in range(nb_samples):\n+                for i in range(current_batch_size):\n                     img = array_to_img(bX[i], scale=True)\n                     img.save(save_to_dir + \"/\" + save_prefix + \"_\" + str(i) + \".\" + save_format)\n-\n-            yield bX, y[b*batch_size:b*batch_size+nb_samples]\n+            if current_batch_size == batch_size:\n+                b += 1\n+            else:\n+                b = 0\n+            yield bX, y[current_index: current_index + batch_size]\n \n     def standardize(self, x):\n         if self.featurewise_center:\n@@ -213,8 +216,7 @@ class ImageDataGenerator(object):\n             augment=False,  # fit on randomly augmented samples\n             rounds=1,  # if augment, how many augmentation passes over the data do we use\n             seed=None):\n-        '''\n-            Required for featurewise_center, featurewise_std_normalization and zca_whitening.\n+        '''Required for featurewise_center, featurewise_std_normalization and zca_whitening.\n         '''\n         X = np.copy(X)\n         if augment:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
