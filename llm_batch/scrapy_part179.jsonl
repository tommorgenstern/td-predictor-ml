{"custom_id": "scrapy#c564334d00fab76ecaadd15d7944bec6d59027ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 9 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,12 @@\n import unittest\n from os.path import join\n \n-from tests import tests_datadir\n from scrapy.utils.gz import gunzip\n+from tests import tests_datadir\n \n SAMPLEDIR = join(tests_datadir, 'compressed')\n \n+\n class GzTest(unittest.TestCase):\n \n     def test_gunzip_basic(self):\n@@ -16,7 +17,7 @@ class GzTest(unittest.TestCase):\n     def test_gunzip_truncated(self):\n         with open(join(SAMPLEDIR, 'truncated-crc-error.gz'), 'rb') as f:\n             text = gunzip(f.read())\n-            assert text.endswith('</html')\n+            assert text.endswith(b'</html')\n \n     def test_gunzip_no_gzip_file_raises(self):\n         with open(join(SAMPLEDIR, 'feed-sample1.xml'), 'rb') as f:\n@@ -25,4 +26,4 @@ class GzTest(unittest.TestCase):\n     def test_gunzip_truncated_short(self):\n         with open(join(SAMPLEDIR, 'truncated-crc-error-short.gz'), 'rb') as f:\n             text = gunzip(f.read())\n-            assert text.endswith('</html>')\n+            assert text.endswith(b'</html>')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ccae49813ab0b3f38f40b9d27f54d0e15004aad2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 195 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,7 @@ See documentation in docs/topics/request-response.rst\n \n import copy\n \n+import six\n from w3lib.url import safe_url_string\n \n from scrapy.http.headers import Headers\n@@ -50,7 +51,7 @@ class Request(object_ref):\n     def _set_url(self, url):\n         if isinstance(url, str):\n             self._url = escape_ajax(safe_url_string(url))\n-        elif isinstance(url, unicode):\n+        elif isinstance(url, six.text_type):\n             if self.encoding is None:\n                 raise TypeError('Cannot convert unicode url - %s has no encoding' %\n                     type(self).__name__)\n@@ -68,7 +69,7 @@ class Request(object_ref):\n     def _set_body(self, body):\n         if isinstance(body, str):\n             self._body = body\n-        elif isinstance(body, unicode):\n+        elif isinstance(body, six.text_type):\n             if self.encoding is None:\n                 raise TypeError('Cannot convert unicode body - %s has no encoding' %\n                     type(self).__name__)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4161a8c82c371ee34841a824ac99de9e2c8cc090", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 11 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 856 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -4,18 +4,15 @@ requests in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n-\n-import copy\n-\n import six\n from w3lib.url import safe_url_string\n \n from scrapy.http.headers import Headers\n from scrapy.utils.trackref import object_ref\n-from scrapy.utils.decorator import deprecated\n from scrapy.utils.url import escape_ajax\n from scrapy.http.common import obsolete_setter\n \n+\n class Request(object_ref):\n \n     def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n@@ -98,7 +95,7 @@ class Request(object_ref):\n         \"\"\"Create a new Request with the same attributes except for those\n         given new values.\n         \"\"\"\n-        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', \\\n+        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta',\n                   'encoding', 'priority', 'dont_filter', 'callback', 'errback']:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n\n@@ -41,17 +41,20 @@ class FormRequest(Request):\n         method = kwargs.pop('method', form.method)\n         return cls(url=url, method=method, formdata=formdata, **kwargs)\n \n+\n def _get_form_url(form, url):\n     if url is None:\n         return form.action or form.base_url\n     return urljoin(form.base_url, url)\n \n+\n def _urlencode(seq, enc):\n     values = [(unicode_to_str(k, enc), unicode_to_str(v, enc))\n               for k, vs in seq\n               for v in (vs if hasattr(vs, '__iter__') else [vs])]\n     return urllib.urlencode(values, doseq=1)\n \n+\n def _get_form(response, formname, formnumber, formxpath):\n     \"\"\"Find the form element \"\"\"\n     from scrapy.selector.lxmldocument import LxmlDocument\n@@ -89,6 +92,7 @@ def _get_form(response, formname, formnumber, formxpath):\n         else:\n             return form\n \n+\n def _get_inputs(form, formdata, dont_click, clickdata, response):\n     try:\n         formdata = dict(formdata or ())\n@@ -99,8 +103,8 @@ def _get_inputs(form, formdata, dont_click, clickdata, response):\n                         '|descendant::select'\n                         '|descendant::input[@type!=\"submit\" and @type!=\"image\" and @type!=\"reset\"'\n                         'and ((@type!=\"checkbox\" and @type!=\"radio\") or @checked)]')\n-    values = [(k, u'' if v is None else v) \\\n-              for k, v in (_value(e) for e in inputs) \\\n+    values = [(k, u'' if v is None else v)\n+              for k, v in (_value(e) for e in inputs)\n               if k and k not in formdata]\n \n     if not dont_click:\n@@ -111,6 +115,7 @@ def _get_inputs(form, formdata, dont_click, clickdata, response):\n     values.extend(formdata.items())\n     return values\n \n+\n def _value(ele):\n     n = ele.name\n     v = ele.value\n@@ -118,6 +123,7 @@ def _value(ele):\n         return _select_value(ele, n, v)\n     return n, v\n \n+\n def _select_value(ele, n, v):\n     multiple = ele.multiple\n     if v is None and not multiple:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#560c32d330e53bbe03154ca6f55e3ed55675ccbb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 18 | Churn Cumulative: 18 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n class SitemapTest(unittest.TestCase):\n \n     def test_sitemap(self):\n-        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n   <url>\n     <loc>http://www.example.com/</loc>\n@@ -25,7 +25,7 @@ class SitemapTest(unittest.TestCase):\n             [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'}, {'priority': '0.8', 'loc': 'http://www.example.com/Special-Offers.html', 'lastmod': '2009-08-16', 'changefreq': 'weekly'}])\n \n     def test_sitemap_index(self):\n-        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n    <sitemap>\n       <loc>http://www.example.com/sitemap1.xml.gz</loc>\n@@ -43,7 +43,7 @@ class SitemapTest(unittest.TestCase):\n         \"\"\"Assert we can deal with trailing spaces inside <loc> tags - we've\n         seen those\n         \"\"\"\n-        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n   <url>\n     <loc> http://www.example.com/</loc>\n@@ -65,7 +65,7 @@ class SitemapTest(unittest.TestCase):\n     def test_sitemap_wrong_ns(self):\n         \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n         with these, though is not 100% confirmed\"\"\"\n-        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n   <url xmlns=\"\">\n     <loc> http://www.example.com/</loc>\n@@ -87,7 +87,7 @@ class SitemapTest(unittest.TestCase):\n     def test_sitemap_wrong_ns2(self):\n         \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n         with these, though is not 100% confirmed\"\"\"\n-        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <urlset>\n   <url xmlns=\"\">\n     <loc> http://www.example.com/</loc>\n@@ -129,7 +129,7 @@ Disallow: /forum/active/\n \n     def test_sitemap_blanklines(self):\n         \"\"\"Assert we can deal with starting blank lines before <xml> tag\"\"\"\n-        s = Sitemap(\"\"\"\\\n+        s = Sitemap(b\"\"\"\\\n \n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n@@ -160,7 +160,7 @@ Disallow: /forum/active/\n         ])\n \n     def test_comment(self):\n-        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n     <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n         xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n         <url>\n@@ -174,7 +174,7 @@ Disallow: /forum/active/\n         ])\n \n     def test_alternate(self):\n-        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n     <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n         xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n         <url>\n@@ -196,7 +196,7 @@ Disallow: /forum/active/\n         ])\n \n     def test_xml_entity_expansion(self):\n-        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n           <!DOCTYPE foo [\n           <!ELEMENT foo ANY >\n           <!ENTITY xxe SYSTEM \"file:///etc/passwd\" >\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#669b62c7419c12ec96c181a200fc54b0c83cce2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 184 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@ SpiderManager is the class which locates and manages all website-specific\n spiders\n \"\"\"\n \n-from zope.interface import implements\n+from zope.interface import implementer\n import six\n \n from scrapy import signals\n@@ -12,10 +12,9 @@ from scrapy.utils.misc import walk_modules\n from scrapy.utils.spider import iter_spider_classes\n \n \n+@implementer(ISpiderManager)\n class SpiderManager(object):\n \n-    implements(ISpiderManager)\n-\n     def __init__(self, spider_modules):\n         self.spider_modules = spider_modules\n         self._spiders = {}\n\n@@ -1,5 +1,7 @@\n import inspect\n \n+import six\n+\n from scrapy import log\n from scrapy.item import BaseItem\n from scrapy.utils.misc import  arg_to_iter\n@@ -16,7 +18,7 @@ def iter_spider_classes(module):\n     # singleton in scrapy.spider.spiders\n     from scrapy.spider import Spider\n \n-    for obj in vars(module).itervalues():\n+    for obj in six.itervalues(vars(module)):\n         if inspect.isclass(obj) and \\\n            issubclass(obj, Spider) and \\\n            obj.__module__ == module.__name__ and \\\n\n@@ -30,7 +30,7 @@ class UtilsSpidersTestCase(unittest.TestCase):\n     def test_iter_spider_classes(self):\n         import tests.test_utils_spider\n         it = iter_spider_classes(tests.test_utils_spider)\n-        self.assertEqual(set(it), set([MySpider1, MySpider2]))\n+        self.assertEqual(set(it), {MySpider1, MySpider2})\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#83432184c8e4c0f551510ac31f35d5d1f17bf4b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,12 +1,12 @@\n from unittest import TestCase\n \n+from six.moves.urllib.parse import urlparse\n+\n from scrapy.http import Response, Request\n from scrapy.spider import Spider\n from scrapy.contrib.spidermiddleware.offsite import OffsiteMiddleware\n from scrapy.utils.test import get_crawler\n \n-from six.moves.urllib.parse import urlparse\n-\n class TestOffsiteMiddleware(TestCase):\n \n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#444052a2f931703708bf8c5bb7ccb750cbac4195", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 11 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 292 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,15 +1,19 @@\n \"\"\"Helper functions which doesn't fit anywhere else\"\"\"\n-\n import re\n import hashlib\n-\n from importlib import import_module\n from pkgutil import iter_modules\n \n+import six\n from w3lib.html import remove_entities\n+\n from scrapy.utils.python import flatten\n from scrapy.item import BaseItem\n \n+\n+_ITERABLE_SINGLE_VALUES = dict, BaseItem, six.text_type, bytes\n+\n+\n def arg_to_iter(arg):\n     \"\"\"Convert an argument to an iterable. The argument can be a None, single\n     value, or an iterable.\n@@ -18,11 +22,12 @@ def arg_to_iter(arg):\n     \"\"\"\n     if arg is None:\n         return []\n-    elif not isinstance(arg, (dict, BaseItem)) and hasattr(arg, '__iter__'):\n+    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n         return arg\n     else:\n         return [arg]\n \n+\n def load_object(path):\n     \"\"\"Load an object given its absolute object path, and return it.\n \n@@ -48,6 +53,7 @@ def load_object(path):\n \n     return obj\n \n+\n def walk_modules(path, load=False):\n     \"\"\"Loads a module and all its submodules from a the given module path and\n     returns them. If *any* module throws an exception while importing, that\n@@ -69,6 +75,7 @@ def walk_modules(path, load=False):\n                 mods.append(submod)\n     return mods\n \n+\n def extract_regex(regex, text, encoding='utf-8'):\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n \n@@ -91,12 +98,13 @@ def extract_regex(regex, text, encoding='utf-8'):\n     else:\n         return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n \n+\n def md5sum(file):\n     \"\"\"Calculate the md5 checksum of a file-like object without reading its\n     whole content in memory.\n \n-    >>> from StringIO import StringIO\n-    >>> md5sum(StringIO('file content to hash'))\n+    >>> from io import BytesIO\n+    >>> md5sum(BytesIO(b'file content to hash'))\n     '784406af91dd5a54fbb9c84c2236595a'\n     \"\"\"\n     m = hashlib.md5()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#51bd9f7d6487f3a4f7333dceeb049421254ff25a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 8 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -10,6 +10,7 @@ if 'django' not in optional_features:\n \n if six.PY3:\n     for fn in open('tests/py3-ignores.txt'):\n+        if fn.strip():\n             collect_ignore.append(fn.strip())\n \n class LogObservers:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2c1226838e5f208663bd3ab7d90b81ad8600a80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 13 | Methods Changed: 1 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 25 | Churn Cumulative: 383 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,7 @@ from datetime import datetime\n from six.moves.urllib.parse import urlparse\n from ftplib import FTP\n \n-from zope.interface import Interface, implements\n+from zope.interface import Interface, implementer\n from twisted.internet import defer, threads\n from w3lib.url import file_uri_to_path\n \n@@ -35,10 +35,9 @@ class IFeedStorage(Interface):\n         \"\"\"Store the given file stream\"\"\"\n \n \n+@implementer(IFeedStorage)\n class BlockingFeedStorage(object):\n \n-    implements(IFeedStorage)\n-\n     def open(self, spider):\n         return TemporaryFile(prefix='feed-')\n \n@@ -49,10 +48,9 @@ class BlockingFeedStorage(object):\n         raise NotImplementedError\n \n \n+@implementer(IFeedStorage)\n class StdoutFeedStorage(object):\n \n-    implements(IFeedStorage)\n-\n     def __init__(self, uri, _stdout=sys.stdout):\n         self._stdout = _stdout\n \n@@ -62,9 +60,9 @@ class StdoutFeedStorage(object):\n     def store(self, file):\n         pass\n \n-class FileFeedStorage(object):\n \n-    implements(IFeedStorage)\n+@implementer(IFeedStorage)\n+class FileFeedStorage(object):\n \n     def __init__(self, uri):\n         self.path = file_uri_to_path(uri)\n@@ -78,6 +76,7 @@ class FileFeedStorage(object):\n     def store(self, file):\n         file.close()\n \n+\n class S3FeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri):\n@@ -131,6 +130,7 @@ class SpiderSlot(object):\n         self.uri = uri\n         self.itemcount = 0\n \n+\n class FeedExporter(object):\n \n     def __init__(self, settings):\n\n@@ -41,10 +41,11 @@ class FileFeedStorageTest(unittest.TestCase):\n     def _assert_stores(self, storage, path):\n         spider = Spider(\"default\")\n         file = storage.open(spider)\n-        file.write(\"content\")\n+        file.write(b\"content\")\n         yield storage.store(file)\n         self.failUnless(os.path.exists(path))\n-        self.failUnlessEqual(open(path).read(), \"content\")\n+        with open(path, 'rb') as fp:\n+            self.failUnlessEqual(fp.read(), b\"content\")\n \n \n class FTPFeedStorageTest(unittest.TestCase):\n@@ -65,10 +66,12 @@ class FTPFeedStorageTest(unittest.TestCase):\n         file.write(b\"content\")\n         yield storage.store(file)\n         self.failUnless(os.path.exists(path))\n-        self.failUnlessEqual(open(path).read(), b\"content\")\n+        with open(path, 'rb') as fp:\n+            self.failUnlessEqual(fp.read(), b\"content\")\n         # again, to check s3 objects are overwritten\n         yield storage.store(BytesIO(b\"new content\"))\n-        self.failUnlessEqual(open(path).read(), b\"new content\")\n+        with open(path, 'rb') as fp:\n+            self.failUnlessEqual(fp.read(), b\"new content\")\n \n \n class S3FeedStorageTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#08224c92f4b17c0b88b9df8f94479a9a66a352c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 27/26 | Churn Δ: 7 | Churn Cumulative: 34 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,7 +43,11 @@ class Command(ScrapyCommand):\n         elif exists(project_name):\n             print(\"Error: directory %r already exists\" % project_name)\n             sys.exit(1)\n-\n+        try:\n+            __import__(project_name, [], 0)\n+            print('Error: Project name can\\'t be %r, choose another project name' % project_name)\n+            sys.exit(1)\n+        except ImportError:\n             moduletpl = join(TEMPLATES_PATH, 'module')\n             copytree(moduletpl, join(project_name, project_name), ignore=IGNORE)\n             shutil.copy(join(TEMPLATES_PATH, 'scrapy.cfg'), project_name)\n\n@@ -64,6 +64,7 @@ class StartprojectTest(ProjectTest):\n \n         self.assertEqual(1, self.call('startproject', self.project_name))\n         self.assertEqual(1, self.call('startproject', 'wrong---project---name'))\n+        self.assertEqual(1, self.call('startproject', 'sys'))\n \n \n class CommandTest(ProjectTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#53e74a69ded1a187e77bb4ca199f0948b8b88d9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 42 | Churn Cumulative: 75 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,8 @@\n from __future__ import print_function\n-import sys\n-import string\n import re\n import shutil\n+import string\n+from importlib import import_module\n from os.path import join, exists, abspath\n from shutil import copytree, ignore_patterns\n \n@@ -11,6 +11,7 @@ from scrapy.command import ScrapyCommand\n from scrapy.utils.template import render_templatefile, string_camelcase\n from scrapy.exceptions import UsageError\n \n+\n TEMPLATES_PATH = join(scrapy.__path__[0], 'templates', 'project')\n \n TEMPLATES_TO_RENDER = (\n@@ -22,6 +23,7 @@ TEMPLATES_TO_RENDER = (\n \n IGNORE = ignore_patterns('*.pyc', '.svn')\n \n+\n class Command(ScrapyCommand):\n \n     requires_project = False\n@@ -32,22 +34,34 @@ class Command(ScrapyCommand):\n     def short_desc(self):\n         return \"Create new project\"\n \n+    def _is_valid_name(self, project_name):\n+        def _module_exists(module_name):\n+            try:\n+                import_module(module_name)\n+                return True\n+            except ImportError:\n+                return False\n+\n+        if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n+            print('Error: Project names must begin with a letter and contain'\\\n+                    ' only\\nletters, numbers and underscores')\n+        elif exists(project_name):\n+            print('Error: Directory %r already exists' % project_name)\n+        elif _module_exists(project_name):\n+            print('Error: Module %r already exists' % project_name)\n+        else:\n+            return True\n+        return False\n+\n     def run(self, args, opts):\n         if len(args) != 1:\n             raise UsageError()\n         project_name = args[0]\n-        if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n-            print('Error: Project names must begin with a letter and contain only\\n' \\\n-                'letters, numbers and underscores')\n-            sys.exit(1)\n-        elif exists(project_name):\n-            print(\"Error: directory %r already exists\" % project_name)\n-            sys.exit(1)\n-        try:\n-            __import__(project_name, [], 0)\n-            print('Error: Project name can\\'t be %r, choose another project name' % project_name)\n-            sys.exit(1)\n-        except ImportError:\n+\n+        if not self._is_valid_name(project_name):\n+            self.exitcode = 1\n+            return\n+\n         moduletpl = join(TEMPLATES_PATH, 'module')\n         copytree(moduletpl, join(project_name, project_name), ignore=IGNORE)\n         shutil.copy(join(TEMPLATES_PATH, 'scrapy.cfg'), project_name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#628a8d8a1542de1ecd1de3573dc21cb13a7103be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 34 | Files Changed: 6 | Hunks: 24 | Methods Changed: 17 | Complexity Δ (Sum/Max): 75/23 | Churn Δ: 69 | Churn Cumulative: 82 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -43,9 +43,9 @@ class FileFeedStorageTest(unittest.TestCase):\n         file = storage.open(spider)\n         file.write(b\"content\")\n         yield storage.store(file)\n-        self.failUnless(os.path.exists(path))\n+        self.assertTrue(os.path.exists(path))\n         with open(path, 'rb') as fp:\n-            self.failUnlessEqual(fp.read(), b\"content\")\n+            self.assertEqual(fp.read(), b\"content\")\n \n \n class FTPFeedStorageTest(unittest.TestCase):\n@@ -65,13 +65,13 @@ class FTPFeedStorageTest(unittest.TestCase):\n         file = storage.open(spider)\n         file.write(b\"content\")\n         yield storage.store(file)\n-        self.failUnless(os.path.exists(path))\n+        self.assertTrue(os.path.exists(path))\n         with open(path, 'rb') as fp:\n-            self.failUnlessEqual(fp.read(), b\"content\")\n+            self.assertEqual(fp.read(), b\"content\")\n         # again, to check s3 objects are overwritten\n         yield storage.store(BytesIO(b\"new content\"))\n         with open(path, 'rb') as fp:\n-            self.failUnlessEqual(fp.read(), b\"new content\")\n+            self.assertEqual(fp.read(), b\"new content\")\n \n \n class S3FeedStorageTest(unittest.TestCase):\n@@ -90,7 +90,8 @@ class S3FeedStorageTest(unittest.TestCase):\n         yield storage.store(file)\n         u = urlparse(uri)\n         key = connect_s3().get_bucket(u.hostname, validate=False).get_key(u.path)\n-        self.failUnlessEqual(key.get_contents_as_string(), \"content\")\n+        self.assertEqual(key.get_contents_as_string(), \"content\")\n+\n \n class StdoutFeedStorageTest(unittest.TestCase):\n \n\n@@ -84,21 +84,21 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n             a = 1/0\n         except:\n             log.err()\n-        self.failUnless('Traceback' in self.logged())\n-        self.failUnless('ZeroDivisionError' in self.logged())\n+        self.assertIn('Traceback', self.logged())\n+        self.assertIn('ZeroDivisionError', self.logged())\n \n     def test_err_why(self):\n         log.err(TypeError(\"bad type\"), \"Wrong type\")\n         self.assertEqual(self.first_log_line(), \"[scrapy] ERROR: Wrong type\")\n-        self.failUnless('TypeError' in self.logged())\n-        self.failUnless('bad type' in self.logged())\n+        self.assertIn('TypeError', self.logged())\n+        self.assertIn('bad type', self.logged())\n \n     def test_error_outside_scrapy(self):\n         \"\"\"Scrapy logger should still print outside errors\"\"\"\n         txlog.err(TypeError(\"bad type\"), \"Wrong type\")\n         self.assertEqual(self.first_log_line(), \"[-] ERROR: Wrong type\")\n-        self.failUnless('TypeError' in self.logged())\n-        self.failUnless('bad type' in self.logged())\n+        self.assertIn('TypeError', self.logged())\n+        self.assertIn('bad type', self.logged())\n \n # this test fails in twisted trial observer, not in scrapy observer\n #    def test_err_why_encoding(self):\n@@ -107,15 +107,15 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n \n     def test_err_exc(self):\n         log.err(TypeError(\"bad type\"))\n-        self.failUnless('Unhandled Error' in self.logged())\n-        self.failUnless('TypeError' in self.logged())\n-        self.failUnless('bad type' in self.logged())\n+        self.assertIn('Unhandled Error', self.logged())\n+        self.assertIn('TypeError', self.logged())\n+        self.assertIn('bad type', self.logged())\n \n     def test_err_failure(self):\n         log.err(failure.Failure(TypeError(\"bad type\")))\n-        self.failUnless('Unhandled Error' in self.logged())\n-        self.failUnless('TypeError' in self.logged())\n-        self.failUnless('bad type' in self.logged())\n+        self.assertIn('Unhandled Error', self.logged())\n+        self.assertIn('TypeError', self.logged())\n+        self.assertIn('bad type', self.logged())\n \n \n class Latin1ScrapyFileLogObserverTest(ScrapyFileLogObserverTest):\n\n@@ -75,10 +75,10 @@ class MiddlewareManagerTest(unittest.TestCase):\n     def test_enabled(self):\n         m1, m2, m3 = M1(), M2(), M3()\n         mwman = MiddlewareManager(m1, m2, m3)\n-        self.failUnlessEqual(mwman.middlewares, (m1, m2, m3))\n+        self.assertEqual(mwman.middlewares, (m1, m2, m3))\n \n     def test_enabled_from_settings(self):\n         settings = Settings()\n         mwman = TestMiddlewareManager.from_settings(settings)\n         classes = [x.__class__ for x in mwman.middlewares]\n-        self.failUnlessEqual(classes, [M1, M3])\n+        self.assertEqual(classes, [M1, M3])\n\n@@ -55,7 +55,7 @@ class DeferUtilsTest(unittest.TestCase):\n             yield process_chain([cb1, cb_fail, cb3], 'res', 'v1', 'v2')\n         except TypeError as e:\n             gotexc = True\n-        self.failUnless(gotexc)\n+        self.assertTrue(gotexc)\n \n     @defer.inlineCallbacks\n     def test_process_chain_both(self):\n@@ -87,7 +87,7 @@ class IterErrbackTest(unittest.TestCase):\n \n         errors = []\n         out = list(iter_errback(itergood(), errors.append))\n-        self.failUnlessEqual(out, range(10))\n+        self.assertEqual(out, range(10))\n         self.failIf(errors)\n \n     def test_iter_errback_bad(self):\n@@ -99,6 +99,6 @@ class IterErrbackTest(unittest.TestCase):\n \n         errors = []\n         out = list(iter_errback(iterbad(), errors.append))\n-        self.failUnlessEqual(out, [0, 1, 2, 3, 4])\n-        self.failUnlessEqual(len(errors), 1)\n-        self.failUnless(isinstance(errors[0].value, ZeroDivisionError))\n+        self.assertEqual(out, [0, 1, 2, 3, 4])\n+        self.assertEqual(len(errors), 1)\n+        self.assertIsInstance(errors[0].value, ZeroDivisionError)\n\n@@ -87,7 +87,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         a.x = 1\n         b.x = 1\n         # equal attribute\n-        self.failUnless(equal_attributes(a, b, ['x']))\n+        self.assertTrue(equal_attributes(a, b, ['x']))\n \n         b.y = 2\n         # obj1 has no attribute y\n@@ -95,7 +95,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n \n         a.y = 2\n         # equal attributes\n-        self.failUnless(equal_attributes(a, b, ['x', 'y']))\n+        self.assertTrue(equal_attributes(a, b, ['x', 'y']))\n \n         a.y = 1\n         # differente attributes\n@@ -104,7 +104,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         # test callable\n         a.meta = {}\n         b.meta = {}\n-        self.failUnless(equal_attributes(a, b, ['meta']))\n+        self.assertTrue(equal_attributes(a, b, ['meta']))\n \n         # compare ['meta']['a']\n         a.meta['z'] = 1\n@@ -114,7 +114,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         get_meta = operator.attrgetter('meta')\n         compare_z = lambda obj: get_z(get_meta(obj))\n \n-        self.failUnless(equal_attributes(a, b, [compare_z, 'x']))\n+        self.assertTrue(equal_attributes(a, b, [compare_z, 'x']))\n         # fail z equality\n         a.meta['z'] = 2\n         self.failIf(equal_attributes(a, b, [compare_z, 'x']))\n@@ -134,7 +134,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n     def test_stringify_dict(self):\n         d = {'a': 123, u'b': 'c', u'd': u'e', object(): u'e'}\n         d2 = stringify_dict(d, keys_only=False)\n-        self.failUnlessEqual(d, d2)\n+        self.assertEqual(d, d2)\n         self.failIf(d is d2) # shouldn't modify in place\n         self.failIf(any(isinstance(x, unicode) for x in d2.keys()))\n         self.failIf(any(isinstance(x, unicode) for x in d2.values()))\n@@ -143,7 +143,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         tuples = [('a', 123), (u'b', 'c'), (u'd', u'e'), (object(), u'e')]\n         d = dict(tuples)\n         d2 = stringify_dict(tuples, keys_only=False)\n-        self.failUnlessEqual(d, d2)\n+        self.assertEqual(d, d2)\n         self.failIf(d is d2) # shouldn't modify in place\n         self.failIf(any(isinstance(x, unicode) for x in d2.keys()), d2.keys())\n         self.failIf(any(isinstance(x, unicode) for x in d2.values()))\n@@ -151,7 +151,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n     def test_stringify_dict_keys_only(self):\n         d = {'a': 123, u'b': 'c', u'd': u'e', object(): u'e'}\n         d2 = stringify_dict(d)\n-        self.failUnlessEqual(d, d2)\n+        self.assertEqual(d, d2)\n         self.failIf(d is d2) # shouldn't modify in place\n         self.failIf(any(isinstance(x, unicode) for x in d2.keys()))\n \n\n@@ -77,8 +77,8 @@ class SendCatchLogTest2(unittest.TestCase):\n         txlog.addObserver(log_events.append)\n         dispatcher.connect(test_handler, test_signal)\n         send_catch_log(test_signal)\n-        self.failUnless(log_events)\n-        self.failUnless(\"Cannot return deferreds from signal handler\" in str(log_events))\n+        self.assertTrue(log_events)\n+        self.assertIn(\"Cannot return deferreds from signal handler\", str(log_events))\n         txlog.removeObserver(log_events.append)\n         self.flushLoggedErrors()\n         dispatcher.disconnect(test_handler, test_signal)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f59a69a971b615bd06ef0011f5fd476eeb4f962", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 34 | Churn Cumulative: 74 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -5,15 +5,17 @@ For actual link extractors implementation see scrapy.contrib.linkextractor, or\n its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n \n+import six\n+\n class Link(object):\n     \"\"\"Link objects represent an extracted link by the LinkExtractor.\"\"\"\n \n     __slots__ = ['url', 'text', 'fragment', 'nofollow']\n \n     def __init__(self, url, text='', fragment='', nofollow=False):\n-        if isinstance(url, unicode):\n+        if isinstance(url, six.text_type):\n             import warnings\n-            warnings.warn(\"Do not instantiate Link objects with unicode urls. \" \\\n+            warnings.warn(\"Do not instantiate Link objects with unicode urls. \"\n                 \"Assuming utf-8 encoding (which could be wrong)\")\n             url = url.encode('utf-8')\n         self.url = url\n\n@@ -14,38 +14,38 @@ class LinkTest(unittest.TestCase):\n         self.assertNotEqual(hash(link1), hash(link2))\n \n     def test_eq_and_hash(self):\n-        l1 = Link(\"http://www.example.com\")\n-        l2 = Link(\"http://www.example.com/other\")\n-        l3 = Link(\"http://www.example.com\")\n+        l1 = Link(b\"http://www.example.com\")\n+        l2 = Link(b\"http://www.example.com/other\")\n+        l3 = Link(b\"http://www.example.com\")\n \n         self._assert_same_links(l1, l1)\n         self._assert_different_links(l1, l2)\n         self._assert_same_links(l1, l3)\n \n-        l4 = Link(\"http://www.example.com\", text=\"test\")\n-        l5 = Link(\"http://www.example.com\", text=\"test2\")\n-        l6 = Link(\"http://www.example.com\", text=\"test\")\n+        l4 = Link(b\"http://www.example.com\", text=\"test\")\n+        l5 = Link(b\"http://www.example.com\", text=\"test2\")\n+        l6 = Link(b\"http://www.example.com\", text=\"test\")\n \n         self._assert_same_links(l4, l4)\n         self._assert_different_links(l4, l5)\n         self._assert_same_links(l4, l6)\n \n-        l7 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=False)\n-        l8 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=False)\n-        l9 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n-        l10 = Link(\"http://www.example.com\", text=\"test\", fragment='other', nofollow=False)\n+        l7 = Link(b\"http://www.example.com\", text=\"test\", fragment='something', nofollow=False)\n+        l8 = Link(b\"http://www.example.com\", text=\"test\", fragment='something', nofollow=False)\n+        l9 = Link(b\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n+        l10 = Link(b\"http://www.example.com\", text=\"test\", fragment='other', nofollow=False)\n         self._assert_same_links(l7, l8)\n         self._assert_different_links(l7, l9)\n         self._assert_different_links(l7, l10)\n \n     def test_repr(self):\n-        l1 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n+        l1 = Link(b\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n         l2 = eval(repr(l1))\n         self._assert_same_links(l1, l2)\n \n     def test_unicode_url(self):\n         with warnings.catch_warnings(record=True) as w:\n-            l = Link(u\"http://www.example.com/\\xa3\")\n-            assert isinstance(l.url, str)\n-            assert l.url == 'http://www.example.com/\\xc2\\xa3'\n+            link = Link(u\"http://www.example.com/\\xa3\")\n+            self.assertIsInstance(link.url, bytes)\n+            self.assertEqual(link.url, b'http://www.example.com/\\xc2\\xa3')\n             assert len(w) == 1, \"warning not issued\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e62bbf0766568b902f99d963030e57b96cc2aae6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 261 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -52,10 +52,6 @@ if twisted_version >= (11, 1, 0):\n     optional_features.add('http11')\n \n # Declare top-level shortcuts\n-if sys.version_info[0] == 2:\n-    # Top-level shortcuts are not ready for Python 3 (like most of Scrapy);\n-    # skip them here to make at least some parts of Scrapy\n-    # importable in Python 3.\n from scrapy.spider import Spider\n from scrapy.http import Request, FormRequest\n from scrapy.selector import Selector\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8f45dc6dd4fbc371ff6fd4e90d7e086319ad0c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 122 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 155 | Churn Cumulative: 389 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,128 +1,40 @@\n-# Scrapy setup.py script\n-#\n-# It doesn't depend on setuptools, but if setuptools is available it'll use\n-# some of its features, like package dependencies.\n-\n-from distutils.command.install_data import install_data\n-from distutils.command.install import INSTALL_SCHEMES\n-from subprocess import Popen, PIPE\n-import os\n-import sys\n-\n-class osx_install_data(install_data):\n-    # On MacOS, the platform-specific lib dir is /System/Library/Framework/Python/.../\n-    # which is wrong. Python 2.5 supplied with MacOS 10.5 has an Apple-specific fix\n-    # for this in distutils.command.install_data#306. It fixes install_lib but not\n-    # install_data, which is why we roll our own install_data class.\n-\n-    def finalize_options(self):\n-        # By the time finalize_options is called, install.install_lib is set to the\n-        # fixed directory, so we set the installdir to install_lib. The\n-        # install_data class uses ('install_data', 'install_dir') instead.\n-        self.set_undefined_options('install', ('install_lib', 'install_dir'))\n-        install_data.finalize_options(self)\n-\n-if sys.platform == \"darwin\":\n-    cmdclasses = {'install_data': osx_install_data}\n-else:\n-    cmdclasses = {'install_data': install_data}\n-\n-def fullsplit(path, result=None):\n-    \"\"\"\n-    Split a pathname into components (the opposite of os.path.join) in a\n-    platform-neutral way.\n-    \"\"\"\n-    if result is None:\n-        result = []\n-    head, tail = os.path.split(path)\n-    if head == '':\n-        return [tail] + result\n-    if head == path:\n-        return result\n-    return fullsplit(head, [tail] + result)\n-\n-# Tell distutils to put the data_files in platform-specific installation\n-# locations. See here for an explanation:\n-# http://groups.google.com/group/comp.lang.python/browse_thread/thread/35ec7b2fed36eaec/2105ee4d9e8042cb\n-for scheme in INSTALL_SCHEMES.values():\n-    scheme['data'] = scheme['purelib']\n-\n-# Compile the list of packages available, because distutils doesn't have\n-# an easy way to do this.\n-packages, data_files = [], []\n-root_dir = os.path.dirname(__file__)\n-if root_dir != '':\n-    os.chdir(root_dir)\n-\n-def is_not_module(filename):\n-    return os.path.splitext(filename)[1] not in ['.py', '.pyc', '.pyo']\n-\n-for scrapy_dir in ['scrapy']:\n-    for dirpath, dirnames, filenames in os.walk(scrapy_dir):\n-        # Ignore dirnames that start with '.'\n-        for i, dirname in enumerate(dirnames):\n-            if dirname.startswith('.'): del dirnames[i]\n-        if '__init__.py' in filenames:\n-            packages.append('.'.join(fullsplit(dirpath)))\n-            data = [f for f in filenames if is_not_module(f)]\n-            if data:\n-                data_files.append([dirpath, [os.path.join(dirpath, f) for f in data]])\n-        elif filenames:\n-            data_files.append([dirpath, [os.path.join(dirpath, f) for f in filenames]])\n-\n-# Small hack for working with bdist_wininst.\n-# See http://mail.python.org/pipermail/distutils-sig/2004-August/004134.html\n-if len(sys.argv) > 1 and sys.argv[1] == 'bdist_wininst':\n-    for file_info in data_files:\n-        file_info[0] = '\\\\PURELIB\\\\%s' % file_info[0]\n-\n-scripts = ['bin/scrapy']\n-if os.name == 'nt':\n-    scripts.append('extras/scrapy.bat')\n-\n-if os.environ.get('SCRAPY_VERSION_FROM_GIT'):\n-    v = Popen(\"git describe\", shell=True, stdout=PIPE).communicate()[0]\n-    with open('scrapy/VERSION', 'w+') as f:\n-        f.write(v.strip())\n-with open(os.path.join(os.path.dirname(__file__), 'scrapy/VERSION')) as f:\n-    version = f.read().strip()\n+from os.path import dirname, join\n+from setuptools import setup, find_packages\n \n \n-setup_args = {\n-    'name': 'Scrapy',\n-    'version': version,\n-    'url': 'http://scrapy.org',\n-    'description': 'A high-level Python Screen Scraping framework',\n-    'long_description': open('README.rst').read(),\n-    'author': 'Scrapy developers',\n-    'maintainer': 'Pablo Hoffman',\n-    'maintainer_email': 'pablo@pablohoffman.com',\n-    'license': 'BSD',\n-    'packages': packages,\n-    'cmdclass': cmdclasses,\n-    'data_files': data_files,\n-    'scripts': scripts,\n-    'include_package_data': True,\n-    'classifiers': [\n-        'Programming Language :: Python',\n-        'Programming Language :: Python :: 2.7',\n+with open(join(dirname(__file__), 'scrapy/VERSION'), 'rb') as f:\n+    version = f.read().decode('ascii').strip()\n+\n+\n+setup(\n+    name='Scrapy',\n+    version=version,\n+    url='http://scrapy.org',\n+    description='A high-level Python Screen Scraping framework',\n+    long_description=open('README.rst').read(),\n+    author='Scrapy developers',\n+    maintainer='Pablo Hoffman',\n+    maintainer_email='pablo@pablohoffman.com',\n+    license='BSD',\n+    packages=find_packages(exclude=['tests']),\n+    include_package_data=True,\n+    entry_points={\n+        'console_scripts': ['scrapy = scrapy.cmdline:execute']\n+    },\n+    classifiers=[\n+        'Development Status :: 5 - Production/Stable',\n+        'Environment :: Console',\n+        'Intended Audience :: Developers',\n         'License :: OSI Approved :: BSD License',\n         'Operating System :: OS Independent',\n-        'Development Status :: 5 - Production/Stable',\n-        'Intended Audience :: Developers',\n-        'Environment :: Console',\n+        'Programming Language :: Python',\n+        'Programming Language :: Python :: 2',\n+        'Programming Language :: Python :: 2.7',\n+        'Topic :: Internet :: WWW/HTTP',\n         'Topic :: Software Development :: Libraries :: Application Frameworks',\n         'Topic :: Software Development :: Libraries :: Python Modules',\n-        'Topic :: Internet :: WWW/HTTP',\n-    ]\n-}\n-\n-try:\n-    from setuptools import setup\n-except ImportError:\n-    from distutils.core import setup\n-else:\n-    setup_args['install_requires'] = [\n+    ],\n+    install_requires=[\n         'Twisted>=10.0.0',\n         'w3lib>=1.2',\n         'queuelib',\n@@ -130,6 +42,5 @@ else:\n         'pyOpenSSL',\n         'cssselect>=0.9',\n         'six>=1.5.2',\n-    ]\n-\n-setup(**setup_args)\n+    ],\n+)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fc4e59cf4f2251fc5fa0c818dda8d68174a8839", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 391 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -16,7 +16,7 @@ setup(\n     maintainer='Pablo Hoffman',\n     maintainer_email='pablo@pablohoffman.com',\n     license='BSD',\n-    packages=find_packages(exclude=['tests']),\n+    packages=find_packages(exclude=('tests', 'tests.*')),\n     include_package_data=True,\n     entry_points={\n         'console_scripts': ['scrapy = scrapy.cmdline:execute']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fcd34b656143c9d05975a27118cd3fbf0d842a76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 392 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -18,6 +18,7 @@ setup(\n     license='BSD',\n     packages=find_packages(exclude=('tests', 'tests.*')),\n     include_package_data=True,\n+    zip_safe=False,\n     entry_points={\n         'console_scripts': ['scrapy = scrapy.cmdline:execute']\n     },\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
