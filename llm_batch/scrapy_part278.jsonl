{"custom_id": "scrapy#bfdd552a32b3a67dfa2895b483c187d87b63b50e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 15 | Churn Cumulative: 99 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import asyncio\n+\n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n@@ -33,6 +35,13 @@ class AsyncDefPipeline:\n         return item\n \n \n+class AsyncDefAsyncioPipeline:\n+    async def process_item(self, item, spider):\n+        await asyncio.sleep(0.2)\n+        item['pipeline_passed'] = True\n+        return item\n+\n+\n class ItemSpider(Spider):\n     name = 'itemspider'\n \n@@ -82,3 +91,9 @@ class PipelineTestCase(unittest.TestCase):\n         crawler = self._create_crawler(AsyncDefPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(len(self.items), 1)\n+\n+    @defer.inlineCallbacks\n+    def test_asyncdef_asyncio_pipeline(self):\n+        crawler = self._create_crawler(AsyncDefAsyncioPipeline)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertEqual(len(self.items), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bdef948aaebced3d28ea7b81525ea9edf7cc650c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 101 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n import asyncio\n \n+from pytest import mark\n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n@@ -92,6 +93,7 @@ class PipelineTestCase(unittest.TestCase):\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(len(self.items), 1)\n \n+    @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_asyncdef_asyncio_pipeline(self):\n         crawler = self._create_crawler(AsyncDefAsyncioPipeline)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d8c54c0f2a98dc627efe8a2b58cfd311f2105cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 18 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,6 @@ from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import deferred_f_from_coro_f\n \n \n-\n class ItemPipelineManager(MiddlewareManager):\n \n     component_name = 'item pipeline'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7d859848800ef05760e4f57dc206a4d756460a9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 105 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n \n from scrapy import Spider, signals, Request\n-from scrapy.utils.test import get_crawler\n+from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n \n from tests.mockserver import MockServer\n \n@@ -39,7 +39,7 @@ class AsyncDefPipeline:\n class AsyncDefAsyncioPipeline:\n     async def process_item(self, item, spider):\n         await asyncio.sleep(0.2)\n-        item['pipeline_passed'] = True\n+        item['pipeline_passed'] = await get_from_asyncio_queue(True)\n         return item\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3faef2d08277f43659400470ec408ee29400018a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 49 | Churn Cumulative: 570 | Contributors (this commit): 16 | Commits (past 90d): 14 | Contributors (cumulative): 24 | DMM Complexity: 0.38095238095238093\n\nDIFF:\n@@ -140,3 +140,20 @@ def deferred_from_coro(o):\n             # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n             return defer.Deferred.fromFuture(asyncio.ensure_future(o))\n     return o\n+\n+\n+def maybeDeferred_coro(f, *args, **kw):\n+    \"\"\" Copy of defer.maybeDeferred that also converts coroutines to Deferreds. \"\"\"\n+    try:\n+        result = f(*args, **kw)\n+    except:  # noqa: E722\n+        return defer.fail(failure.Failure(captureVars=defer.Deferred.debug))\n+\n+    if isinstance(result, defer.Deferred):\n+        return result\n+    elif _isfuture(result) or inspect.isawaitable(result):\n+        return deferred_from_coro(result)\n+    elif isinstance(result, failure.Failure):\n+        return defer.fail(result)\n+    else:\n+        return defer.succeed(result)\n\n@@ -2,12 +2,14 @@\n \n import logging\n \n-from twisted.internet.defer import maybeDeferred, DeferredList, Deferred\n+from twisted.internet.defer import DeferredList, Deferred\n from twisted.python.failure import Failure\n \n from pydispatch.dispatcher import Any, Anonymous, liveReceivers, \\\n     getAllReceivers, disconnect\n from pydispatch.robustapply import robustApply\n+\n+from scrapy.utils.defer import maybeDeferred_coro\n from scrapy.utils.log import failure_to_exc_info\n \n logger = logging.getLogger(__name__)\n@@ -61,7 +63,7 @@ def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n     spider = named.get('spider', None)\n     dfds = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n-        d = maybeDeferred(robustApply, receiver, signal=signal, sender=sender,\n+        d = maybeDeferred_coro(robustApply, receiver, signal=signal, sender=sender,\n                 *arguments, **named)\n         d.addErrback(logerror, receiver)\n         d.addBoth(lambda result: (receiver, result))\n\n@@ -1,3 +1,6 @@\n+import asyncio\n+\n+from pytest import mark\n from testfixtures import LogCapture\n from twisted.trial import unittest\n from twisted.python.failure import Failure\n@@ -5,6 +8,7 @@ from twisted.internet import defer, reactor\n from pydispatch import dispatcher\n \n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n+from scrapy.utils.test import get_from_asyncio_queue\n \n \n class SendCatchLogTest(unittest.TestCase):\n@@ -54,7 +58,7 @@ class SendCatchLogDeferredTest(SendCatchLogTest):\n         return send_catch_log_deferred(signal, *a, **kw)\n \n \n-class SendCatchLogDeferredTest2(SendCatchLogTest):\n+class SendCatchLogDeferredTest2(SendCatchLogDeferredTest):\n \n     def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n@@ -63,8 +67,24 @@ class SendCatchLogDeferredTest2(SendCatchLogTest):\n         reactor.callLater(0, d.callback, \"OK\")\n         return d\n \n-    def _get_result(self, signal, *a, **kw):\n-        return send_catch_log_deferred(signal, *a, **kw)\n+\n+class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n+\n+    async def ok_handler(self, arg, handlers_called):\n+        handlers_called.add(self.ok_handler)\n+        assert arg == 'test'\n+        await defer.succeed(42)\n+        return \"OK\"\n+\n+\n+@mark.only_asyncio()\n+class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n+\n+    async def ok_handler(self, arg, handlers_called):\n+        handlers_called.add(self.ok_handler)\n+        assert arg == 'test'\n+        await asyncio.sleep(0.2)\n+        return await get_from_asyncio_queue(\"OK\")\n \n \n class SendCatchLogTest2(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a91a13b4434ae60be6fded94e4ed08ba322b4f69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 46 | Churn Cumulative: 1015 | Contributors (this commit): 15 | Commits (past 90d): 15 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import logging\n import inspect\n \n from scrapy.spiders import Spider\n+from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import arg_to_iter\n \n \n@@ -9,7 +10,7 @@ logger = logging.getLogger(__name__)\n \n \n def iterate_spider_output(result):\n-    return arg_to_iter(result)\n+    return arg_to_iter(deferred_from_coro(result))\n \n \n def iter_spider_classes(module):\n\n@@ -1,14 +1,18 @@\n \"\"\"\n Some spiders used for testing and benchmarking\n \"\"\"\n+import asyncio\n import time\n from urllib.parse import urlencode\n \n+from twisted.internet import defer\n+\n from scrapy.http import Request\n from scrapy.item import Item\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import Spider\n from scrapy.spiders.crawl import CrawlSpider, Rule\n+from scrapy.utils.test import get_from_asyncio_queue\n \n \n class MockServerSpider(Spider):\n@@ -83,6 +87,25 @@ class SimpleSpider(MetaSpider):\n         self.logger.info(\"Got response %d\" % response.status)\n \n \n+class AsyncDefSpider(SimpleSpider):\n+\n+    name = 'asyncdef'\n+\n+    async def parse(self, response):\n+        await defer.succeed(42)\n+        self.logger.info(\"Got response %d\" % response.status)\n+\n+\n+class AsyncDefAsyncioSpider(SimpleSpider):\n+\n+    name = 'asyncdef_asyncio'\n+\n+    async def parse(self, response):\n+        await asyncio.sleep(0.2)\n+        status = await get_from_asyncio_queue(response.status)\n+        self.logger.info(\"Got response %d\" % status)\n+\n+\n class ItemSpider(FollowAllSpider):\n \n     name = 'item'\n\n@@ -1,6 +1,7 @@\n import json\n import logging\n \n+from pytest import mark\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n@@ -10,7 +11,8 @@ from scrapy.http import Request\n from scrapy.utils.python import to_unicode\n from tests.mockserver import MockServer\n from tests.spiders import (FollowAllSpider, DelaySpider, SimpleSpider, BrokenStartRequestsSpider,\n-                           SingleRequestSpider, DuplicateStartRequestsSpider, CrawlSpiderWithErrback)\n+                           SingleRequestSpider, DuplicateStartRequestsSpider, CrawlSpiderWithErrback,\n+                           AsyncDefSpider, AsyncDefAsyncioSpider)\n \n \n class CrawlTestCase(TestCase):\n@@ -308,3 +310,19 @@ with multiples lines\n         self.assertIn(\"[callback] status 201\", str(log))\n         self.assertIn(\"[errback] status 404\", str(log))\n         self.assertIn(\"[errback] status 500\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_async_def_parse(self):\n+        self.runner.crawl(AsyncDefSpider, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        with LogCapture() as log:\n+            yield self.runner.join()\n+        self.assertIn(\"Got response 200\", str(log))\n+\n+    @mark.only_asyncio()\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncio_parse(self):\n+        runner = CrawlerRunner({\"ASYNCIO_REACTOR\": True})\n+        runner.crawl(AsyncDefAsyncioSpider, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        with LogCapture() as log:\n+            yield runner.join()\n+        self.assertIn(\"Got response 200\", str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ce1ad31071326d22387f8444abefd6f3e18ed86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 50 | Churn Cumulative: 98 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,10 @@\n-\n from testfixtures import LogCapture\n-from twisted.trial.unittest import TestCase\n from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n \n-from scrapy import Spider, Request\n+from scrapy import Request, Spider\n from scrapy.utils.test import get_crawler\n+\n from tests.mockserver import MockServer\n \n \n@@ -74,7 +74,7 @@ class ProcessSpiderInputSpiderWithErrback(ProcessSpiderInputSpiderWithoutErrback\n     name = 'ProcessSpiderInputSpiderWithErrback'\n \n     def start_requests(self):\n-        yield Request(url=self.mockserver.url('/status?n=200'), callback=self.parse, errback=self.errback)\n+        yield Request(self.mockserver.url('/status?n=200'), self.parse, errback=self.errback)\n \n     def errback(self, failure):\n         self.logger.info('Got a Failure on the Request errback')\n@@ -100,6 +100,17 @@ class GeneratorCallbackSpider(Spider):\n         raise ImportError()\n \n \n+# ================================================================================\n+# (2.1) exceptions from a spider callback (generator, middleware right after callback)\n+class GeneratorCallbackSpiderMiddlewareRightAfterSpider(GeneratorCallbackSpider):\n+    name = 'GeneratorCallbackSpiderMiddlewareRightAfterSpider'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            __name__ + '.LogExceptionMiddleware': 100000,\n+        },\n+    }\n+\n+\n # ================================================================================\n # (3) exceptions from a spider callback (not a generator)\n class NotGeneratorCallbackSpider(Spider):\n@@ -117,6 +128,17 @@ class NotGeneratorCallbackSpider(Spider):\n         return [{'test': 1}, {'test': 1/0}]\n \n \n+# ================================================================================\n+# (3.1) exceptions from a spider callback (not a generator, middleware right after callback)\n+class NotGeneratorCallbackSpiderMiddlewareRightAfterSpider(NotGeneratorCallbackSpider):\n+    name = 'NotGeneratorCallbackSpiderMiddlewareRightAfterSpider'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            __name__ + '.LogExceptionMiddleware': 100000,\n+        },\n+    }\n+\n+\n # ================================================================================\n # (4) exceptions from a middleware process_spider_output method (generator)\n class GeneratorOutputChainSpider(Spider):\n@@ -320,6 +342,16 @@ class TestSpiderMiddleware(TestCase):\n         self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n         self.assertIn(\"'item_scraped_count': 2\", str(log2))\n \n+    @defer.inlineCallbacks\n+    def test_generator_callback_right_after_callback(self):\n+        \"\"\"\n+        (2.1) Special case of (2): Exceptions should be caught\n+        even if the middleware is placed right after the spider\n+        \"\"\"\n+        log21 = yield self.crawl_log(GeneratorCallbackSpiderMiddlewareRightAfterSpider)\n+        self.assertIn(\"Middleware: ImportError exception caught\", str(log21))\n+        self.assertIn(\"'item_scraped_count': 2\", str(log21))\n+\n     @defer.inlineCallbacks\n     def test_not_a_generator_callback(self):\n         \"\"\"\n@@ -330,6 +362,16 @@ class TestSpiderMiddleware(TestCase):\n         self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log3))\n         self.assertNotIn(\"item_scraped_count\", str(log3))\n \n+    @defer.inlineCallbacks\n+    def test_not_a_generator_callback_right_after_callback(self):\n+        \"\"\"\n+        (3.1) Special case of (3): Exceptions should be caught\n+        even if the middleware is placed right after the spider\n+        \"\"\"\n+        log31 = yield self.crawl_log(NotGeneratorCallbackSpiderMiddlewareRightAfterSpider)\n+        self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log31))\n+        self.assertNotIn(\"item_scraped_count\", str(log31))\n+\n     @defer.inlineCallbacks\n     def test_generator_output_chain(self):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c088c04f449c3383b7867c25b387f13949b1d6c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 28 | Files Changed: 2 | Hunks: 18 | Methods Changed: 9 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 72 | Churn Cumulative: 1244 | Contributors (this commit): 21 | Commits (past 90d): 14 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,13 +3,14 @@ Spider Middleware manager\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n-from itertools import chain, islice\n+from itertools import islice\n \n from twisted.python.failure import Failure\n+\n from scrapy.exceptions import _InvalidOutput\n from scrapy.middleware import MiddlewareManager\n-from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.conf import build_component_list\n+from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.python import MutableChain\n \n \n@@ -17,6 +18,13 @@ def _isiterable(possible_iterator):\n     return hasattr(possible_iterator, '__iter__')\n \n \n+def _fname(f):\n+    return \"%s.%s\".format(\n+        f.__self__.__class__.__name__,\n+        f.__func__.__name__\n+    )\n+\n+\n class SpiderMiddlewareManager(MiddlewareManager):\n \n     component_name = 'spider middleware'\n@@ -31,27 +39,36 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             self.methods['process_spider_input'].append(mw.process_spider_input)\n         if hasattr(mw, 'process_start_requests'):\n             self.methods['process_start_requests'].appendleft(mw.process_start_requests)\n-        self.methods['process_spider_output'].appendleft(getattr(mw, 'process_spider_output', None))\n-        self.methods['process_spider_exception'].appendleft(getattr(mw, 'process_spider_exception', None))\n+        process_spider_output = getattr(mw, 'process_spider_output', None)\n+        self.methods['process_spider_output'].appendleft(process_spider_output)\n+        process_spider_exception = getattr(mw, 'process_spider_exception', None)\n+        self.methods['process_spider_exception'].appendleft(process_spider_exception)\n \n     def scrape_response(self, scrape_func, response, request, spider):\n-        fname = lambda f: '%s.%s' % (\n-                f.__self__.__class__.__name__,\n-                f.__func__.__name__)\n \n         def process_spider_input(response):\n             for method in self.methods['process_spider_input']:\n                 try:\n                     result = method(response=response, spider=spider)\n                     if result is not None:\n-                        raise _InvalidOutput('Middleware {} must return None or raise an exception, got {}'\n-                                             .format(fname(method), type(result)))\n+                        msg = \"Middleware {} must return None or raise an exception, got {}\"\n+                        raise _InvalidOutput(msg.format(_fname(method), type(result)))\n                 except _InvalidOutput:\n                     raise\n                 except Exception:\n                     return scrape_func(Failure(), request, spider)\n             return scrape_func(response, request, spider)\n \n+        def _evaluate_iterable(iterable, method_index, recover_to):\n+            try:\n+                for r in iterable:\n+                    yield r\n+            except Exception as ex:\n+                exception_result = process_spider_exception(Failure(ex), method_index)\n+                if isinstance(exception_result, Failure):\n+                    raise\n+                recover_to.extend(exception_result)\n+\n         def process_spider_exception(_failure, start_index=0):\n             exception = _failure.value\n             # don't handle _InvalidOutput exception\n@@ -69,8 +86,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 elif result is None:\n                     continue\n                 else:\n-                    raise _InvalidOutput('Middleware {} must return None or an iterable, got {}'\n-                                         .format(fname(method), type(result)))\n+                    msg = \"Middleware {} must return None or an iterable, got {}\"\n+                    raise _InvalidOutput(msg.format(_fname(method), type(result)))\n             return _failure\n \n         def process_spider_output(result, start_index=0):\n@@ -78,38 +95,36 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             # chain, they went through it already from the process_spider_exception method\n             recovered = MutableChain()\n \n-            def evaluate_iterable(iterable, index):\n-                try:\n-                    for r in iterable:\n-                        yield r\n-                except Exception as ex:\n-                    exception_result = process_spider_exception(Failure(ex), index+1)\n-                    if isinstance(exception_result, Failure):\n-                        raise\n-                    recovered.extend(exception_result)\n-\n             method_list = islice(self.methods['process_spider_output'], start_index, None)\n             for method_index, method in enumerate(method_list, start=start_index):\n                 if method is None:\n                     continue\n-                # the following might fail directly if the output value is not a generator\n                 try:\n+                    # might fail directly if the output value is not a generator\n                     result = method(response=response, result=result, spider=spider)\n                 except Exception as ex:\n                     exception_result = process_spider_exception(Failure(ex), method_index+1)\n                     if isinstance(exception_result, Failure):\n                         raise\n                     return exception_result\n-                if _isiterable(result):\n-                    result = evaluate_iterable(result, method_index)\n                 else:\n-                    raise _InvalidOutput('Middleware {} must return an iterable, got {}'\n-                                         .format(fname(method), type(result)))\n+                    if _isiterable(result):\n+                        result = _evaluate_iterable(result, method_index+1, recovered)\n+                    else:\n+                        msg = \"Middleware {} must return an iterable, got {}\"\n+                        raise _InvalidOutput(msg.format(_fname(method), type(result)))\n \n-            return chain(result, recovered)\n+            return MutableChain(result, recovered)\n+\n+        def process_callback_output(result):\n+            if isinstance(result, Failure):\n+                return process_spider_exception(result)\n+            recovered = MutableChain()\n+            result = _evaluate_iterable(result, 0, recovered)\n+            return MutableChain(process_spider_output(result), recovered)\n \n         dfd = mustbe_deferred(process_spider_input, response)\n-        dfd.addCallbacks(callback=process_spider_output, errback=process_spider_exception)\n+        dfd.addCallbacks(callback=process_callback_output, errback=process_callback_output)\n         return dfd\n \n     def process_start_requests(self, start_requests, spider):\n\n@@ -375,6 +375,7 @@ class MutableChain(object):\n     \"\"\"\n     Thin wrapper around itertools.chain, allowing to add iterables \"in-place\"\n     \"\"\"\n+\n     def __init__(self, *args):\n         self.data = chain(*args)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6e928f47209396d0f6c0b155eb07115c4855c93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 761 | Contributors (this commit): 19 | Commits (past 90d): 11 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -1,15 +1,15 @@\n \"\"\"\n This module contains essential stuff that should've come with Python itself ;)\n \"\"\"\n+import errno\n import gc\n+import inspect\n import os\n import re\n-import inspect\n+import sys\n import weakref\n-import errno\n from functools import partial, wraps\n from itertools import chain\n-import sys\n \n from scrapy.utils.decorators import deprecated\n \n@@ -371,7 +371,7 @@ else:\n         gc.collect()\n \n \n-class MutableChain(object):\n+class MutableChain:\n     \"\"\"\n     Thin wrapper around itertools.chain, allowing to add iterables \"in-place\"\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9770ca35fb494503298270902569ed897a365d32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 495 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -117,14 +117,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             return MutableChain(result, recovered)\n \n         def process_callback_output(result):\n-            if isinstance(result, Failure):\n-                return process_spider_exception(result)\n             recovered = MutableChain()\n             result = _evaluate_iterable(result, 0, recovered)\n             return MutableChain(process_spider_output(result), recovered)\n \n         dfd = mustbe_deferred(process_spider_input, response)\n-        dfd.addCallbacks(callback=process_callback_output, errback=process_callback_output)\n+        dfd.addCallbacks(callback=process_callback_output, errback=process_spider_exception)\n         return dfd\n \n     def process_start_requests(self, start_requests, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#735c0ceb7890dbea607dd6e4c0a14a4ce2b0afd2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 39 | Files Changed: 2 | Hunks: 20 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 87 | Churn Cumulative: 1782 | Contributors (this commit): 22 | Commits (past 90d): 18 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,34 +4,36 @@ import signal\n import warnings\n \n from twisted.internet import defer\n-from zope.interface.verify import verifyClass, DoesNotImplement\n+from zope.interface.verify import DoesNotImplement, verifyClass\n \n-from scrapy import Spider\n+from scrapy import signals, Spider\n from scrapy.core.engine import ExecutionEngine\n-from scrapy.resolver import CachingThreadedResolver\n-from scrapy.interfaces import ISpiderLoader\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extension import ExtensionManager\n+from scrapy.interfaces import ISpiderLoader\n+from scrapy.resolver import CachingHostnameResolver\n from scrapy.settings import overridden_settings, Settings\n from scrapy.signalmanager import SignalManager\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.asyncio import install_asyncio_reactor, is_asyncio_reactor_installed\n-from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n-from scrapy.utils.misc import load_object\n from scrapy.utils.log import (\n-    LogCounterHandler, configure_logging, log_scrapy_info,\n-    get_scrapy_root_handler, install_scrapy_root_handler)\n-from scrapy import signals\n+    configure_logging,\n+    get_scrapy_root_handler,\n+    install_scrapy_root_handler,\n+    log_scrapy_info,\n+    LogCounterHandler,\n+)\n+from scrapy.utils.misc import load_object\n+from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n \n \n logger = logging.getLogger(__name__)\n \n \n-class Crawler(object):\n+class Crawler:\n \n     def __init__(self, spidercls, settings=None):\n         if isinstance(spidercls, Spider):\n-            raise ValueError(\n-                'The spidercls argument must be a class, not an object')\n+            raise ValueError('The spidercls argument must be a class, not an object')\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n@@ -110,7 +112,7 @@ class Crawler(object):\n             yield defer.maybeDeferred(self.engine.stop)\n \n \n-class CrawlerRunner(object):\n+class CrawlerRunner:\n     \"\"\"\n     This is a convenient helper class that keeps track of, manages and runs\n     crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n@@ -303,7 +305,7 @@ class CrawlerProcess(CrawlerRunner):\n                 return\n             d.addBoth(self._stop_reactor)\n \n-        reactor.installResolver(self._get_dns_resolver())\n+        reactor.installNameResolver(self._get_dns_resolver())\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n@@ -315,10 +317,10 @@ class CrawlerProcess(CrawlerRunner):\n             cache_size = self.settings.getint('DNSCACHE_SIZE')\n         else:\n             cache_size = 0\n-        return CachingThreadedResolver(\n-            reactor=reactor,\n+        return CachingHostnameResolver(\n+            resolver=reactor.nameResolver,\n             cache_size=cache_size,\n-            timeout=self.settings.getfloat('DNS_TIMEOUT')\n+            timeout=self.settings.getfloat('DNS_TIMEOUT'),\n         )\n \n     def _graceful_stop_reactor(self):\n\n@@ -1,32 +1,39 @@\n-from twisted.internet import defer\n-from twisted.internet.base import ThreadedResolver\n+from twisted.internet.interfaces import IHostnameResolver, IResolutionReceiver\n+from zope.interface.declarations import implementer, provider\n \n from scrapy.utils.datatypes import LocalCache\n \n-# TODO: cache misses\n \n+# TODO: cache misses\n dnscache = LocalCache(10000)\n \n \n-class CachingThreadedResolver(ThreadedResolver):\n-    def __init__(self, reactor, cache_size, timeout):\n-        super(CachingThreadedResolver, self).__init__(reactor)\n-        dnscache.limit = cache_size\n+@implementer(IHostnameResolver)\n+class CachingHostnameResolver(object):\n+\n+    def __init__(self, resolver, cache_size, timeout):\n+        self.resolver = resolver\n         self.timeout = timeout\n+        dnscache.limit = cache_size\n \n-    def getHostByName(self, name, timeout=None):\n-        if name in dnscache:\n-            return defer.succeed(dnscache[name])\n-        # in Twisted<=16.6, getHostByName() is always called with\n-        # a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),\n-        # so the input argument above is simply overridden\n-        # to enforce Scrapy's DNS_TIMEOUT setting's value\n-        timeout = (self.timeout,)\n-        d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n-        if dnscache.limit:\n-            d.addCallback(self._cache_result, name)\n-        return d\n+    def resolveHostName(self, resolutionReceiver, hostName, portNumber=0,\n+                        addressTypes=None, transportSemantics='TCP'):\n \n-    def _cache_result(self, result, name):\n-        dnscache[name] = result\n+        @provider(IResolutionReceiver)\n+        class CachingResolutionReceiver(resolutionReceiver):\n+            def resolutionBegan(self, resolution):\n+                super(CachingResolutionReceiver, self).resolutionBegan(resolution)\n+                self.resolution = resolution\n+\n+            def resolutionComplete(self):\n+                super(CachingResolutionReceiver, self).resolutionComplete()\n+                dnscache[hostName] = self.resolution\n+\n+        try:\n+            result = dnscache[hostName]\n+        except KeyError:\n+            result = self.resolver.resolveHostName(\n+                CachingResolutionReceiver(), hostName, portNumber, addressTypes, transportSemantics\n+            )\n+        finally:\n             return result\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f1c184631e8cefc191dc0077083138c241bfd6a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 119 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from twisted.internet import reactor\n from twisted.internet.interfaces import IHostnameResolver, IResolutionReceiver\n from zope.interface.declarations import implementer, provider\n \n@@ -21,9 +22,14 @@ class CachingHostnameResolver(object):\n \n         @provider(IResolutionReceiver)\n         class CachingResolutionReceiver(resolutionReceiver):\n+\n+            def __init__(self, timeout):\n+                self.timeout = timeout\n+\n             def resolutionBegan(self, resolution):\n                 super(CachingResolutionReceiver, self).resolutionBegan(resolution)\n                 self.resolution = resolution\n+                # reactor.callLater(self.timeout, resolution.cancel)\n \n             def resolutionComplete(self):\n                 super(CachingResolutionReceiver, self).resolutionComplete()\n@@ -33,7 +39,11 @@ class CachingHostnameResolver(object):\n             result = dnscache[hostName]\n         except KeyError:\n             result = self.resolver.resolveHostName(\n-                CachingResolutionReceiver(), hostName, portNumber, addressTypes, transportSemantics\n+                CachingResolutionReceiver(self.timeout),\n+                hostName,\n+                portNumber,\n+                addressTypes,\n+                transportSemantics\n             )\n         finally:\n             return result\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#55babf9acd6cd357f4211a86af01a1f2abe2f0cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 127 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,7 @@ dnscache = LocalCache(10000)\n \n \n @implementer(IHostnameResolver)\n-class CachingHostnameResolver(object):\n+class CachingHostnameResolver:\n \n     def __init__(self, resolver, cache_size, timeout):\n         self.resolver = resolver\n@@ -25,14 +25,20 @@ class CachingHostnameResolver(object):\n \n             def __init__(self, timeout):\n                 self.timeout = timeout\n+                self.resolved = False\n \n             def resolutionBegan(self, resolution):\n                 super(CachingResolutionReceiver, self).resolutionBegan(resolution)\n                 self.resolution = resolution\n                 # reactor.callLater(self.timeout, resolution.cancel)\n \n+            def addressResolved(self, address):\n+                super(CachingResolutionReceiver, self).addressResolved(address)\n+                self.resolved = True\n+\n             def resolutionComplete(self):\n                 super(CachingResolutionReceiver, self).resolutionComplete()\n+                if self.resolved:\n                     dnscache[hostName] = self.resolution\n \n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8c3de288fa2564473f49198b40efdeaa2428f1ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 1815 | Contributors (this commit): 22 | Commits (past 90d): 16 | Contributors (cumulative): 28 | DMM Complexity: 0.0\n\nDIFF:\n@@ -320,7 +320,6 @@ class CrawlerProcess(CrawlerRunner):\n         return CachingHostnameResolver(\n             resolver=reactor.nameResolver,\n             cache_size=cache_size,\n-            timeout=self.settings.getfloat('DNS_TIMEOUT'),\n         )\n \n     def _graceful_stop_reactor(self):\n\n@@ -1,4 +1,3 @@\n-from twisted.internet import reactor\n from twisted.internet.interfaces import IHostnameResolver, IResolutionReceiver\n from zope.interface.declarations import implementer, provider\n \n@@ -12,9 +11,8 @@ dnscache = LocalCache(10000)\n @implementer(IHostnameResolver)\n class CachingHostnameResolver:\n \n-    def __init__(self, resolver, cache_size, timeout):\n+    def __init__(self, resolver, cache_size):\n         self.resolver = resolver\n-        self.timeout = timeout\n         dnscache.limit = cache_size\n \n     def resolveHostName(self, resolutionReceiver, hostName, portNumber=0,\n@@ -23,14 +21,10 @@ class CachingHostnameResolver:\n         @provider(IResolutionReceiver)\n         class CachingResolutionReceiver(resolutionReceiver):\n \n-            def __init__(self, timeout):\n-                self.timeout = timeout\n-                self.resolved = False\n-\n             def resolutionBegan(self, resolution):\n                 super(CachingResolutionReceiver, self).resolutionBegan(resolution)\n                 self.resolution = resolution\n-                # reactor.callLater(self.timeout, resolution.cancel)\n+                self.resolved = False\n \n             def addressResolved(self, address):\n                 super(CachingResolutionReceiver, self).addressResolved(address)\n@@ -45,7 +39,7 @@ class CachingHostnameResolver:\n             result = dnscache[hostName]\n         except KeyError:\n             result = self.resolver.resolveHostName(\n-                CachingResolutionReceiver(self.timeout),\n+                CachingResolutionReceiver(),\n                 hostName,\n                 portNumber,\n                 addressTypes,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e69cf415c8326349c871fb23faba2ed822aa08ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/9 | Churn Δ: 69 | Churn Cumulative: 2478 | Contributors (this commit): 55 | Commits (past 90d): 23 | Contributors (cumulative): 77 | DMM Complexity: 1.0\n\nDIFF:\n@@ -305,23 +305,13 @@ class CrawlerProcess(CrawlerRunner):\n                 return\n             d.addBoth(self._stop_reactor)\n \n-        reactor.installNameResolver(self._get_dns_resolver())\n+        resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n+        resolver_class.install(reactor, self.settings)\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n-    def _get_dns_resolver(self):\n-        from twisted.internet import reactor\n-        if self.settings.getbool('DNSCACHE_ENABLED'):\n-            cache_size = self.settings.getint('DNSCACHE_SIZE')\n-        else:\n-            cache_size = 0\n-        return CachingHostnameResolver(\n-            resolver=reactor.nameResolver,\n-            cache_size=cache_size,\n-        )\n-\n     def _graceful_stop_reactor(self):\n         d = self.stop()\n         d.addBoth(self._stop_reactor)\n\n@@ -1,4 +1,6 @@\n-from twisted.internet.interfaces import IHostnameResolver, IResolutionReceiver\n+from twisted.internet import defer\n+from twisted.internet.base import ThreadedResolver\n+from twisted.internet.interfaces import IHostnameResolver, IResolutionReceiver, IResolverSimple\n from zope.interface.declarations import implementer, provider\n \n from scrapy.utils.datatypes import LocalCache\n@@ -8,8 +10,58 @@ from scrapy.utils.datatypes import LocalCache\n dnscache = LocalCache(10000)\n \n \n+@implementer(IResolverSimple)\n+class CachingThreadedResolver(ThreadedResolver):\n+    \"\"\"\n+    Default caching resolver. IPv4 only, supports setting a timeout value for DNS requests\n+    \"\"\"\n+\n+    @classmethod\n+    def install(cls, reactor, settings):\n+        if settings.getbool('DNSCACHE_ENABLED'):\n+            cache_size = settings.getint('DNSCACHE_SIZE')\n+        else:\n+            cache_size = 0\n+        resolver = cls(reactor, cache_size, settings.getfloat('DNS_TIMEOUT'))\n+        reactor.installResolver(resolver)\n+\n+    def __init__(self, reactor, cache_size, timeout):\n+        super(CachingThreadedResolver, self).__init__(reactor)\n+        dnscache.limit = cache_size\n+        self.timeout = timeout\n+\n+    def getHostByName(self, name, timeout=None):\n+        if name in dnscache:\n+            return defer.succeed(dnscache[name])\n+        # in Twisted<=16.6, getHostByName() is always called with\n+        # a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),\n+        # so the input argument above is simply overridden\n+        # to enforce Scrapy's DNS_TIMEOUT setting's value\n+        timeout = (self.timeout,)\n+        d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n+        if dnscache.limit:\n+            d.addCallback(self._cache_result, name)\n+        return d\n+\n+    def _cache_result(self, result, name):\n+        dnscache[name] = result\n+        return result\n+\n+\n @implementer(IHostnameResolver)\n class CachingHostnameResolver:\n+    \"\"\"\n+    Experimental caching resolver, supporting IPv4 and IPv6\n+    \"\"\"\n+\n+    @classmethod\n+    def install(cls, reactor, settings):\n+        if settings.getbool('DNSCACHE_ENABLED'):\n+            cache_size = settings.getint('DNSCACHE_SIZE')\n+        else:\n+            cache_size = 0\n+        resolver = cls(reactor.nameResolver, cache_size)\n+        reactor.installNameResolver(resolver)\n \n     def __init__(self, resolver, cache_size):\n         self.resolver = resolver\n\n@@ -60,6 +60,7 @@ DEPTH_PRIORITY = 0\n \n DNSCACHE_ENABLED = True\n DNSCACHE_SIZE = 10000\n+DNS_RESOLVER = 'scrapy.resolver.CachingThreadedResolver'\n DNS_TIMEOUT = 60\n \n DOWNLOAD_DELAY = 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f155b059a43b2cc48149a26c6584910038a23f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1691 | Contributors (this commit): 22 | Commits (past 90d): 14 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,6 @@ from scrapy.core.engine import ExecutionEngine\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extension import ExtensionManager\n from scrapy.interfaces import ISpiderLoader\n-from scrapy.resolver import CachingHostnameResolver\n from scrapy.settings import overridden_settings, Settings\n from scrapy.signalmanager import SignalManager\n from scrapy.utils.asyncio import install_asyncio_reactor, is_asyncio_reactor_installed\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f45b4c7f8d191c6855c12f3497ece6b6121289df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 50 | Churn Cumulative: 1934 | Contributors (this commit): 22 | Commits (past 90d): 21 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -305,7 +305,7 @@ class CrawlerProcess(CrawlerRunner):\n             d.addBoth(self._stop_reactor)\n \n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n-        resolver_class.install(reactor, self.settings)\n+        resolver_class.install_on_reactor(reactor, crawler=self)\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n\n@@ -4,6 +4,7 @@ from twisted.internet.interfaces import IHostnameResolver, IResolutionReceiver,\n from zope.interface.declarations import implementer, provider\n \n from scrapy.utils.datatypes import LocalCache\n+from scrapy.utils.misc import create_instance\n \n \n # TODO: cache misses\n@@ -13,23 +14,27 @@ dnscache = LocalCache(10000)\n @implementer(IResolverSimple)\n class CachingThreadedResolver(ThreadedResolver):\n     \"\"\"\n-    Default caching resolver. IPv4 only, supports setting a timeout value for DNS requests\n+    Default caching resolver. IPv4 only, supports setting a timeout value for DNS requests.\n     \"\"\"\n \n-    @classmethod\n-    def install(cls, reactor, settings):\n-        if settings.getbool('DNSCACHE_ENABLED'):\n-            cache_size = settings.getint('DNSCACHE_SIZE')\n-        else:\n-            cache_size = 0\n-        resolver = cls(reactor, cache_size, settings.getfloat('DNS_TIMEOUT'))\n-        reactor.installResolver(resolver)\n-\n     def __init__(self, reactor, cache_size, timeout):\n         super(CachingThreadedResolver, self).__init__(reactor)\n         dnscache.limit = cache_size\n         self.timeout = timeout\n \n+    @classmethod\n+    def from_crawler(cls, crawler, reactor):\n+        if crawler.settings.getbool('DNSCACHE_ENABLED'):\n+            cache_size = crawler.settings.getint('DNSCACHE_SIZE')\n+        else:\n+            cache_size = 0\n+        return cls(reactor, cache_size, crawler.settings.getfloat('DNS_TIMEOUT'))\n+\n+    @classmethod\n+    def install_on_reactor(cls, reactor, crawler):\n+        resolver = create_instance(cls, None, crawler, reactor=reactor)\n+        reactor.installResolver(resolver)\n+\n     def getHostByName(self, name, timeout=None):\n         if name in dnscache:\n             return defer.succeed(dnscache[name])\n@@ -51,21 +56,26 @@ class CachingThreadedResolver(ThreadedResolver):\n @implementer(IHostnameResolver)\n class CachingHostnameResolver:\n     \"\"\"\n-    Experimental caching resolver, supporting IPv4 and IPv6\n+    Experimental caching resolver. Resolves IPv4 and IPv6 addresses,\n+    does not support setting a timeout value for DNS requests.\n     \"\"\"\n \n+    def __init__(self, reactor, cache_size):\n+        self.resolver = reactor.nameResolver\n+        dnscache.limit = cache_size\n+\n     @classmethod\n-    def install(cls, reactor, settings):\n-        if settings.getbool('DNSCACHE_ENABLED'):\n-            cache_size = settings.getint('DNSCACHE_SIZE')\n+    def from_crawler(cls, crawler, reactor):\n+        if crawler.settings.getbool('DNSCACHE_ENABLED'):\n+            cache_size = crawler.settings.getint('DNSCACHE_SIZE')\n         else:\n             cache_size = 0\n-        resolver = cls(reactor.nameResolver, cache_size)\n-        reactor.installNameResolver(resolver)\n+        return cls(reactor, cache_size)\n \n-    def __init__(self, resolver, cache_size):\n-        self.resolver = resolver\n-        dnscache.limit = cache_size\n+    @classmethod\n+    def install_on_reactor(cls, reactor, crawler):\n+        resolver = create_instance(cls, None, crawler, reactor=reactor)\n+        reactor.installNameResolver(resolver)\n \n     def resolveHostName(self, resolutionReceiver, hostName, portNumber=0,\n                         addressTypes=None, transportSemantics='TCP'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
