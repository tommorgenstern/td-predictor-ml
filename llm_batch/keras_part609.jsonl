{"custom_id": "keras#022c095075bcd854af08a052932ea4a56d9c1627", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 203 | Lines Deleted: 17 | Files Changed: 10 | Hunks: 21 | Methods Changed: 16 | Complexity Δ (Sum/Max): 14/9 | Churn Δ: 220 | Churn Cumulative: 3867 | Contributors (this commit): 16 | Commits (past 90d): 212 | Contributors (cumulative): 37 | DMM Complexity: 0.9809523809523809\n\nDIFF:\n@@ -4,8 +4,8 @@ from keras_core.backend.tensorflow import math\n from keras_core.backend.tensorflow import nn\n from keras_core.backend.tensorflow import numpy\n from keras_core.backend.tensorflow import random\n-from keras_core.backend.tensorflow.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.tensorflow.core import DYNAMIC_BATCH_SIZE_OK\n+from keras_core.backend.tensorflow.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.tensorflow.core import Variable\n from keras_core.backend.tensorflow.core import cast\n from keras_core.backend.tensorflow.core import compute_output_spec\n\n@@ -72,6 +72,7 @@ from keras_core.layers.pooling.max_pooling3d import MaxPooling3D\n from keras_core.layers.preprocessing.category_encoding import CategoryEncoding\n from keras_core.layers.preprocessing.center_crop import CenterCrop\n from keras_core.layers.preprocessing.discretization import Discretization\n+from keras_core.layers.preprocessing.hashed_crossing import HashedCrossing\n from keras_core.layers.preprocessing.hashing import Hashing\n from keras_core.layers.preprocessing.integer_lookup import IntegerLookup\n from keras_core.layers.preprocessing.normalization import Normalization\n\n@@ -66,7 +66,11 @@ class Layer(Operation):\n         self._trainable_variables = []\n         self._non_trainable_variables = []\n         self._supports_masking = not utils.is_default(self.compute_mask)\n+        # Whether to automatically convert (+ auto-cast) inputs to `call()`.\n+        self._convert_input_args = True\n+        # Whether to allow non-tensors as positional arguments in `call()`.\n         self._allow_non_tensor_positional_args = False\n+        # Dict of shapes that were used to call `build()`.\n         self._build_shapes_dict = None\n         self._call_signature_parameters = [\n             p.name for p in inspect.signature(self.call).parameters.values()\n@@ -344,6 +348,7 @@ class Layer(Operation):\n                 return backend.convert_to_tensor(x, dtype=self.compute_dtype)\n             return x\n \n+        if self._convert_input_args:\n             args = nest.map_structure(maybe_convert, args)\n             kwargs = nest.map_structure(maybe_convert, kwargs)\n \n\n@@ -96,9 +96,10 @@ class Discretization(Layer):\n         output_mode=\"int\",\n         sparse=False,\n         name=None,\n+        dtype=None,\n         **kwargs,\n     ):\n-        super().__init__(name=name)\n+        super().__init__(name=name, dtype=dtype)\n         if sparse and backend.backend() != \"tensorflow\":\n             raise ValueError(\n                 \"`sparse` can only be set to True with the \"\n@@ -111,6 +112,7 @@ class Discretization(Layer):\n             output_mode=output_mode,\n             sparse=sparse,\n             name=name,\n+            dtype=dtype,\n             **kwargs,\n         )\n         self.bin_boundaries = (\n@@ -172,16 +174,6 @@ class Discretization(Layer):\n     def reset_state(self):\n         self.layer.reset_state()\n \n-    def get_config(self):\n-        return {\n-            \"bin_boundaries\": self.bin_boundaries,\n-            \"num_bins\": self.num_bins,\n-            \"epsilon\": self.epsilon,\n-            \"output_mode\": self.output_mode,\n-            \"sparse\": self.sparse,\n-            \"name\": self.name,\n-        }\n-\n     def compute_output_shape(self, input_shape):\n         return input_shape\n \n@@ -192,3 +184,14 @@ class Discretization(Layer):\n         if backend.backend() != \"tensorflow\":\n             outputs = backend.convert_to_tensor(outputs)\n         return outputs\n+\n+    def get_config(self):\n+        return {\n+            \"bin_boundaries\": self.bin_boundaries,\n+            \"num_bins\": self.num_bins,\n+            \"epsilon\": self.epsilon,\n+            \"output_mode\": self.output_mode,\n+            \"sparse\": self.sparse,\n+            \"name\": self.name,\n+            \"dtype\": self.dtype,\n+        }\n\n@@ -0,0 +1,113 @@\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.HashedCrossing\")\n+class HashedCrossing(Layer):\n+    \"\"\"A preprocessing layer which crosses features using the \"hashing trick\".\n+\n+    This layer performs crosses of categorical features using the \"hashing\n+    trick\". Conceptually, the transformation can be thought of as:\n+    `hash(concatenate(features)) % num_bins.\n+\n+    This layer currently only performs crosses of scalar inputs and batches of\n+    scalar inputs. Valid input shapes are `(batch_size, 1)`, `(batch_size,)` and\n+    `()`.\n+\n+    **Note:** This layer wraps `tf.keras.layers.HashedCrossing`. It cannot\n+    be used as part of the compiled computation graph of a model with\n+    any backend other than TensorFlow.\n+    It can however be used with any backend when running eagerly.\n+    It can also always be used as part of an input preprocessing pipeline\n+    with any backend (outside the model itself), which is how we recommend\n+    to use this layer.\n+\n+    Args:\n+        num_bins: Number of hash bins.\n+        output_mode: Specification for the output of the layer. Values can be\n+            `\"int\"`, or `\"one_hot\"` configuring the layer as follows:\n+            - `\"int\"`: Return the integer bin indices directly.\n+            - `\"one_hot\"`: Encodes each individual element in the input into an\n+                array the same size as `num_bins`, containing a 1 at the input's\n+                bin index. Defaults to `\"int\"`.\n+        sparse: Boolean. Only applicable to `\"one_hot\"` mode and only valid\n+            when using the TensorFlow backend. If `True`, returns\n+            a `SparseTensor` instead of a dense `Tensor`. Defaults to `False`.\n+        **kwargs: Keyword arguments to construct a layer.\n+\n+    Examples:\n+\n+    **Crossing two scalar features.**\n+\n+    >>> layer = keras_core.layers.HashedCrossing(\n+    ...     num_bins=5)\n+    >>> feat1 = np.array(['A', 'B', 'A', 'B', 'A'])\n+    >>> feat2 = np.array([101, 101, 101, 102, 102])\n+    >>> layer((feat1, feat2))\n+    array([1, 4, 1, 1, 3])\n+\n+    **Crossing and one-hotting two scalar features.**\n+\n+    >>> layer = keras_core.layers.HashedCrossing(\n+    ...     num_bins=5, output_mode='one_hot')\n+    >>> feat1 = np.array(['A', 'B', 'A', 'B', 'A'])\n+    >>> feat2 = np.array([101, 101, 101, 102, 102])\n+    >>> layer((feat1, feat2))\n+    array([[0., 1., 0., 0., 0.],\n+            [0., 0., 0., 0., 1.],\n+            [0., 1., 0., 0., 0.],\n+            [0., 1., 0., 0., 0.],\n+            [0., 0., 0., 1., 0.]], dtype=float32)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        num_bins,\n+        output_mode=\"int\",\n+        sparse=False,\n+        name=None,\n+        dtype=None,\n+        **kwargs,\n+    ):\n+        if output_mode == \"int\" and dtype is None:\n+            dtype = \"int64\"\n+        super().__init__(name=name, dtype=dtype)\n+        if sparse and backend.backend() != \"tensorflow\":\n+            raise ValueError(\n+                \"`sparse` can only be set to True with the \"\n+                \"TensorFlow backend.\"\n+            )\n+        self.layer = tf.keras.layers.HashedCrossing(\n+            num_bins=num_bins,\n+            output_mode=output_mode,\n+            sparse=sparse,\n+            name=name,\n+            dtype=dtype,\n+            **kwargs,\n+        )\n+        self.num_bins = num_bins\n+        self.output_mode = output_mode\n+        self.sparse = sparse\n+        self._allow_non_tensor_positional_args = True\n+        self._convert_input_args = False\n+\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n+\n+    def call(self, inputs):\n+        outputs = self.layer.call(inputs)\n+        if backend.backend() != \"tensorflow\":\n+            outputs = backend.convert_to_tensor(outputs)\n+        return outputs\n+\n+    def get_config(self):\n+        return {\n+            \"num_bins\": self.num_bins,\n+            \"output_mode\": self.output_mode,\n+            \"sparse\": self.sparse,\n+            \"name\": self.name,\n+            \"dtype\": self.dtype,\n+        }\n\n@@ -0,0 +1,57 @@\n+import numpy as np\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class HashedCrossingTest(testing.TestCase):\n+    def test_basics(self):\n+        self.run_layer_test(\n+            layers.HashedCrossing,\n+            init_kwargs={\n+                \"num_bins\": 3,\n+                \"output_mode\": \"int\",\n+            },\n+            input_data=([1, 2], [4, 5]),\n+            expected_output_shape=(2,),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+        self.run_layer_test(\n+            layers.HashedCrossing,\n+            init_kwargs={\"num_bins\": 4, \"output_mode\": \"one_hot\"},\n+            input_data=([1, 2], [4, 5]),\n+            expected_output_shape=(2, 4),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    def test_correctness(self):\n+        layer = layers.HashedCrossing(num_bins=5)\n+        feat1 = np.array([\"A\", \"B\", \"A\", \"B\", \"A\"])\n+        feat2 = np.array([101, 101, 101, 102, 102])\n+        output = layer((feat1, feat2))\n+        self.assertAllClose(np.array([1, 4, 1, 1, 3]), output)\n+\n+        layer = layers.HashedCrossing(num_bins=5, output_mode=\"one_hot\")\n+        feat1 = np.array([\"A\", \"B\", \"A\", \"B\", \"A\"])\n+        feat2 = np.array([101, 101, 101, 102, 102])\n+        output = layer((feat1, feat2))\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.0, 1.0, 0.0, 0.0, 0.0],\n+                    [0.0, 0.0, 0.0, 0.0, 1.0],\n+                    [0.0, 1.0, 0.0, 0.0, 0.0],\n+                    [0.0, 1.0, 0.0, 0.0, 0.0],\n+                    [0.0, 0.0, 0.0, 1.0, 0.0],\n+                ]\n+            ),\n+            output,\n+        )\n\n@@ -167,6 +167,7 @@ class Hashing(Layer):\n                     \"of 2 integers, or a single integer. \"\n                     f\"Received: salt={salt}.\"\n                 )\n+        self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs):\n\n@@ -68,8 +68,8 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         layer.adapt(data)\n         self.assertTrue(layer.built)\n         output = layer(x)\n-        self.assertAllClose(np.var(output, axis=0), 1.0)\n-        self.assertAllClose(np.mean(output, axis=0), 0.0)\n+        self.assertAllClose(np.var(output, axis=0), 1.0, atol=1e-5)\n+        self.assertAllClose(np.mean(output, axis=0), 0.0, atol=1e-5)\n \n         # Test in high-dim and with tuple axis.\n         x = np.random.random((32, 4, 3, 5))\n@@ -84,8 +84,8 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         layer.adapt(data)\n         self.assertTrue(layer.built)\n         output = layer(x)\n-        self.assertAllClose(np.var(output, axis=(0, 3)), 1.0)\n-        self.assertAllClose(np.mean(output, axis=(0, 3)), 0.0)\n+        self.assertAllClose(np.var(output, axis=(0, 3)), 1.0, atol=1e-5)\n+        self.assertAllClose(np.mean(output, axis=(0, 3)), 0.0, atol=1e-5)\n \n     def test_normalization_errors(self):\n         # TODO\n\n@@ -301,8 +301,11 @@ class StringLookup(Layer):\n         sparse=False,\n         encoding=\"utf-8\",\n         name=None,\n-        **kwargs\n+        dtype=None,\n+        **kwargs,\n     ):\n+        if output_mode == \"int\" and dtype is None:\n+            dtype = \"int64\"\n         super().__init__(name=name)\n         if sparse and backend.backend() != \"tensorflow\":\n             raise ValueError(\n@@ -322,8 +325,10 @@ class StringLookup(Layer):\n             sparse=sparse,\n             encoding=encoding,\n             name=name,\n+            dtype=dtype,\n             **kwargs,\n         )\n+        self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def adapt(self, data, batch_size=None, steps=None):\n\n@@ -236,6 +236,7 @@ class TextVectorization(Layer):\n             name=name,\n             **kwargs,\n         )\n+        self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def compute_output_shape(self, input_shape):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#807b9f0b3b49c41f72e162c30c589df54b7b86ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1135 | Files Changed: 2 | Hunks: 2 | Methods Changed: 34 | Complexity Δ (Sum/Max): -125/0 | Churn Δ: 1135 | Churn Cumulative: 4540 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 0.5257270693512305\n\nDIFF:\n@@ -1,757 +0,0 @@\n-from tensorflow import data as tf_data\n-\n-from keras_core import layers\n-from keras_core import operations as ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.saving import saving_lib\n-from keras_core.saving import serialization_lib\n-from keras_core.utils.naming import auto_name\n-\n-\n-class Cross:\n-    def __init__(self, feature_names, crossing_dim, output_mode=\"one_hot\"):\n-        if output_mode not in {\"int\", \"one_hot\"}:\n-            raise ValueError(\n-                \"Invalid value for argument `output_mode`. \"\n-                \"Expected one of {'int', 'one_hot'}. \"\n-                f\"Received: output_mode={output_mode}\"\n-            )\n-        self.feature_names = tuple(feature_names)\n-        self.crossing_dim = crossing_dim\n-        self.output_mode = output_mode\n-\n-    @property\n-    def name(self):\n-        return \"_X_\".join(self.feature_names)\n-\n-    def get_config(self):\n-        return {\n-            \"feature_names\": self.feature_names,\n-            \"crossing_dim\": self.crossing_dim,\n-            \"output_mode\": self.output_mode,\n-        }\n-\n-    @classmethod\n-    def from_config(cls, config):\n-        return cls(**config)\n-\n-\n-class Feature:\n-    def __init__(self, dtype, preprocessor, output_mode):\n-        if output_mode not in {\"int\", \"one_hot\", \"float\"}:\n-            raise ValueError(\n-                \"Invalid value for argument `output_mode`. \"\n-                \"Expected one of {'int', 'one_hot', 'float'}. \"\n-                f\"Received: output_mode={output_mode}\"\n-            )\n-        self.dtype = dtype\n-        if isinstance(preprocessor, dict):\n-            preprocessor = serialization_lib.deserialize_keras_object(\n-                preprocessor\n-            )\n-        self.preprocessor = preprocessor\n-        self.output_mode = output_mode\n-\n-    def get_config(self):\n-        return {\n-            \"dtype\": self.dtype,\n-            \"preprocessor\": serialization_lib.serialize_keras_object(\n-                self.preprocessor\n-            ),\n-            \"output_mode\": self.output_mode,\n-        }\n-\n-    @classmethod\n-    def from_config(cls, config):\n-        return cls(**config)\n-\n-\n-@keras_core_export(\"keras_core.utils.FeatureSpace\")\n-class FeatureSpace(Layer):\n-    \"\"\"One-stop utility for preprocessing and encoding structured data.\n-\n-    Arguments:\n-        feature_names: Dict mapping the names of your features to their\n-            type specification, e.g. `{\"my_feature\": \"integer_categorical\"}`\n-            or `{\"my_feature\": FeatureSpace.integer_categorical()}`.\n-            For a complete list of all supported types, see\n-            \"Available feature types\" paragraph below.\n-        output_mode: One of `\"concat\"` or `\"dict\"`. In concat mode, all\n-            features get concatenated together into a single vector.\n-            In dict mode, the FeatureSpace returns a dict of individually\n-            encoded features (with the same keys as the input dict keys).\n-        crosses: List of features to be crossed together, e.g.\n-            `crosses=[(\"feature_1\", \"feature_2\")]`. The features will be\n-            \"crossed\" by hashing their combined value into\n-            a fixed-length vector.\n-        crossing_dim: Default vector size for hashing crossed features.\n-            Defaults to `32`.\n-        hashing_dim: Default vector size for hashing features of type\n-            `\"integer_hashed\"` and `\"string_hashed\"`. Defaults to `32`.\n-        num_discretization_bins: Default number of bins to be used for\n-            discretizing features of type `\"float_discretized\"`.\n-            Defaults to `32`.\n-\n-    **Available feature types:**\n-\n-    Note that all features can be referred to by their string name,\n-    e.g. `\"integer_categorical\"`. When using the string name, the default\n-    argument values are used.\n-\n-    ```python\n-    # Plain float values.\n-    FeatureSpace.float(name=None)\n-\n-    # Float values to be preprocessed via featurewise standardization\n-    # (i.e. via a `keras.layers.Normalization` layer).\n-    FeatureSpace.float_normalized(name=None)\n-\n-    # Float values to be preprocessed via linear rescaling\n-    # (i.e. via a `keras.layers.Rescaling` layer).\n-    FeatureSpace.float_rescaled(scale=1., offset=0., name=None)\n-\n-    # Float values to be discretized. By default, the discrete\n-    # representation will then be one-hot encoded.\n-    FeatureSpace.float_discretized(\n-        num_bins, bin_boundaries=None, output_mode=\"one_hot\", name=None)\n-\n-    # Integer values to be indexed. By default, the discrete\n-    # representation will then be one-hot encoded.\n-    FeatureSpace.integer_categorical(\n-        max_tokens=None, num_oov_indices=1, output_mode=\"one_hot\", name=None)\n-\n-    # String values to be indexed. By default, the discrete\n-    # representation will then be one-hot encoded.\n-    FeatureSpace.string_categorical(\n-        max_tokens=None, num_oov_indices=1, output_mode=\"one_hot\", name=None)\n-\n-    # Integer values to be hashed into a fixed number of bins.\n-    # By default, the discrete representation will then be one-hot encoded.\n-    FeatureSpace.integer_hashed(num_bins, output_mode=\"one_hot\", name=None)\n-\n-    # String values to be hashed into a fixed number of bins.\n-    # By default, the discrete representation will then be one-hot encoded.\n-    FeatureSpace.string_hashed(num_bins, output_mode=\"one_hot\", name=None)\n-    ```\n-\n-    Examples:\n-\n-    **Basic usage with a dict of input data:**\n-\n-    ```python\n-    raw_data = {\n-        \"float_values\": [0.0, 0.1, 0.2, 0.3],\n-        \"string_values\": [\"zero\", \"one\", \"two\", \"three\"],\n-        \"int_values\": [0, 1, 2, 3],\n-    }\n-    dataset = tf.data.Dataset.from_tensor_slices(raw_data)\n-\n-    feature_space = FeatureSpace(\n-        features={\n-            \"float_values\": \"float_normalized\",\n-            \"string_values\": \"string_categorical\",\n-            \"int_values\": \"integer_categorical\",\n-        },\n-        crosses=[(\"string_values\", \"int_values\")],\n-        output_mode=\"concat\",\n-    )\n-    # Before you start using the FeatureSpace,\n-    # you must `adapt()` it on some data.\n-    feature_space.adapt(dataset)\n-\n-    # You can call the FeatureSpace on a dict of data (batched or unbatched).\n-    output_vector = feature_space(raw_data)\n-    ```\n-\n-    **Basic usage with `tf.data`:**\n-\n-    ```python\n-    # Unlabeled data\n-    preprocessed_ds = unlabeled_dataset.map(feature_space)\n-\n-    # Labeled data\n-    preprocessed_ds = labeled_dataset.map(lambda x, y: (feature_space(x), y))\n-    ```\n-\n-    **Basic usage with the Keras Functional API:**\n-\n-    ```python\n-    # Retrieve a dict Keras Input objects\n-    inputs = feature_space.get_inputs()\n-    # Retrieve the corresponding encoded Keras tensors\n-    encoded_features = feature_space.get_encoded_features()\n-    # Build a Functional model\n-    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(encoded_features)\n-    model = keras.Model(inputs, outputs)\n-    ```\n-\n-    **Customizing each feature or feature cross:**\n-\n-    ```python\n-    feature_space = FeatureSpace(\n-        features={\n-            \"float_values\": FeatureSpace.float_normalized(),\n-            \"string_values\": FeatureSpace.string_categorical(max_tokens=10),\n-            \"int_values\": FeatureSpace.integer_categorical(max_tokens=10),\n-        },\n-        crosses=[\n-            FeatureSpace.cross((\"string_values\", \"int_values\"), crossing_dim=32)\n-        ],\n-        output_mode=\"concat\",\n-    )\n-    ```\n-\n-    **Returning a dict of integer-encoded features:**\n-\n-    ```python\n-    feature_space = FeatureSpace(\n-        features={\n-            \"string_values\": FeatureSpace.string_categorical(output_mode=\"int\"),\n-            \"int_values\": FeatureSpace.integer_categorical(output_mode=\"int\"),\n-        },\n-        crosses=[\n-            FeatureSpace.cross(\n-                feature_names=(\"string_values\", \"int_values\"),\n-                crossing_dim=32,\n-                output_mode=\"int\",\n-            )\n-        ],\n-        output_mode=\"dict\",\n-    )\n-    ```\n-\n-    **Specifying your own Keras preprocessing layer:**\n-\n-    ```python\n-    # Let's say that one of the features is a short text paragraph that\n-    # we want to encode as a vector (one vector per paragraph) via TF-IDF.\n-    data = {\n-        \"text\": [\"1st string\", \"2nd string\", \"3rd string\"],\n-    }\n-\n-    # There's a Keras layer for this: TextVectorization.\n-    custom_layer = layers.TextVectorization(output_mode=\"tf_idf\")\n-\n-    # We can use FeatureSpace.feature to create a custom feature\n-    # that will use our preprocessing layer.\n-    feature_space = FeatureSpace(\n-        features={\n-            \"text\": FeatureSpace.feature(\n-                preprocessor=custom_layer, dtype=\"string\", output_mode=\"float\"\n-            ),\n-        },\n-        output_mode=\"concat\",\n-    )\n-    feature_space.adapt(tf.data.Dataset.from_tensor_slices(data))\n-    output_vector = feature_space(data)\n-    ```\n-\n-    **Retrieving the underlying Keras preprocessing layers:**\n-\n-    ```python\n-    # The preprocessing layer of each feature is available in `.preprocessors`.\n-    preprocessing_layer = feature_space.preprocessors[\"feature1\"]\n-\n-    # The crossing layer of each feature cross is available in `.crossers`.\n-    # It's an instance of keras.layers.HashedCrossing.\n-    crossing_layer = feature_space.crossers[\"feature1_X_feature2\"]\n-    ```\n-\n-    **Saving and reloading a FeatureSpace:**\n-\n-    ```python\n-    feature_space.save(\"myfeaturespace.keras\")\n-    reloaded_feature_space = keras.models.load_model(\"myfeaturespace.keras\")\n-    ```\n-    \"\"\"\n-\n-    @classmethod\n-    def cross(cls, feature_names, crossing_dim, output_mode=\"one_hot\"):\n-        return Cross(feature_names, crossing_dim, output_mode=output_mode)\n-\n-    @classmethod\n-    def feature(cls, dtype, preprocessor, output_mode):\n-        return Feature(dtype, preprocessor, output_mode)\n-\n-    @classmethod\n-    def float(cls, name=None):\n-        from keras.layers.core import identity\n-\n-        name = name or auto_name(\"float\")\n-        preprocessor = identity.Identity(\n-            dtype=\"float32\", name=f\"{name}_preprocessor\"\n-        )\n-        return Feature(\n-            dtype=\"float32\", preprocessor=preprocessor, output_mode=\"float\"\n-        )\n-\n-    @classmethod\n-    def float_rescaled(cls, scale=1.0, offset=0.0, name=None):\n-        name = name or auto_name(\"float_rescaled\")\n-        preprocessor = layers.Rescaling(\n-            scale=scale, offset=offset, name=f\"{name}_preprocessor\"\n-        )\n-        return Feature(\n-            dtype=\"float32\", preprocessor=preprocessor, output_mode=\"float\"\n-        )\n-\n-    @classmethod\n-    def float_normalized(cls, name=None):\n-        name = name or auto_name(\"float_normalized\")\n-        preprocessor = layers.Normalization(\n-            axis=-1, name=f\"{name}_preprocessor\"\n-        )\n-        return Feature(\n-            dtype=\"float32\", preprocessor=preprocessor, output_mode=\"float\"\n-        )\n-\n-    @classmethod\n-    def float_discretized(\n-        cls, num_bins, bin_boundaries=None, output_mode=\"one_hot\", name=None\n-    ):\n-        name = name or auto_name(\"float_discretized\")\n-        preprocessor = layers.Discretization(\n-            num_bins=num_bins,\n-            bin_boundaries=bin_boundaries,\n-            name=f\"{name}_preprocessor\",\n-        )\n-        return Feature(\n-            dtype=\"float32\", preprocessor=preprocessor, output_mode=output_mode\n-        )\n-\n-    @classmethod\n-    def integer_categorical(\n-        cls,\n-        max_tokens=None,\n-        num_oov_indices=1,\n-        output_mode=\"one_hot\",\n-        name=None,\n-    ):\n-        name = name or auto_name(\"integer_categorical\")\n-        preprocessor = layers.IntegerLookup(\n-            name=f\"{name}_preprocessor\",\n-            max_tokens=max_tokens,\n-            num_oov_indices=num_oov_indices,\n-        )\n-        return Feature(\n-            dtype=\"int64\", preprocessor=preprocessor, output_mode=output_mode\n-        )\n-\n-    @classmethod\n-    def string_categorical(\n-        cls,\n-        max_tokens=None,\n-        num_oov_indices=1,\n-        output_mode=\"one_hot\",\n-        name=None,\n-    ):\n-        name = name or auto_name(\"string_categorical\")\n-        preprocessor = layers.StringLookup(\n-            name=f\"{name}_preprocessor\",\n-            max_tokens=max_tokens,\n-            num_oov_indices=num_oov_indices,\n-        )\n-        return Feature(\n-            dtype=\"string\", preprocessor=preprocessor, output_mode=output_mode\n-        )\n-\n-    @classmethod\n-    def string_hashed(cls, num_bins, output_mode=\"one_hot\", name=None):\n-        name = name or auto_name(\"string_hashed\")\n-        preprocessor = layers.Hashing(\n-            name=f\"{name}_preprocessor\", num_bins=num_bins\n-        )\n-        return Feature(\n-            dtype=\"string\", preprocessor=preprocessor, output_mode=output_mode\n-        )\n-\n-    @classmethod\n-    def integer_hashed(cls, num_bins, output_mode=\"one_hot\", name=None):\n-        name = name or auto_name(\"integer_hashed\")\n-        preprocessor = layers.Hashing(\n-            name=f\"{name}_preprocessor\", num_bins=num_bins\n-        )\n-        return Feature(\n-            dtype=\"int64\", preprocessor=preprocessor, output_mode=output_mode\n-        )\n-\n-    def __init__(\n-        self,\n-        features,\n-        output_mode=\"concat\",\n-        crosses=None,\n-        crossing_dim=32,\n-        hashing_dim=32,\n-        num_discretization_bins=32,\n-        name=None,\n-    ):\n-        super().__init__(name=name)\n-        if not features:\n-            raise ValueError(\"The `features` argument cannot be None or empty.\")\n-        self.crossing_dim = crossing_dim\n-        self.hashing_dim = hashing_dim\n-        self.num_discretization_bins = num_discretization_bins\n-        self.features = {\n-            name: self._standardize_feature(name, value)\n-            for name, value in features.items()\n-        }\n-        self.crosses = []\n-        if crosses:\n-            feature_set = set(features.keys())\n-            for cross in crosses:\n-                if isinstance(cross, dict):\n-                    cross = serialization_lib.deserialize_keras_object(cross)\n-                if isinstance(cross, Cross):\n-                    self.crosses.append(cross)\n-                else:\n-                    if not crossing_dim:\n-                        raise ValueError(\n-                            \"When specifying `crosses`, the argument \"\n-                            \"`crossing_dim` \"\n-                            \"(dimensionality of the crossing space) \"\n-                            \"should be specified as well.\"\n-                        )\n-                    for key in cross:\n-                        if key not in feature_set:\n-                            raise ValueError(\n-                                \"All features referenced \"\n-                                \"in the `crosses` argument \"\n-                                \"should be present in the `features` dict. \"\n-                                f\"Received unknown features: {cross}\"\n-                            )\n-                    self.crosses.append(Cross(cross, crossing_dim=crossing_dim))\n-        self.crosses_by_name = {cross.name: cross for cross in self.crosses}\n-\n-        if output_mode not in {\"dict\", \"concat\"}:\n-            raise ValueError(\n-                \"Invalid value for argument `output_mode`. \"\n-                \"Expected one of {'dict', 'concat'}. \"\n-                f\"Received: output_mode={output_mode}\"\n-            )\n-        self.output_mode = output_mode\n-\n-        self.inputs = {\n-            name: self._feature_to_input(name, value)\n-            for name, value in self.features.items()\n-        }\n-        self.preprocessors = {\n-            name: value.preprocessor for name, value in self.features.items()\n-        }\n-        self.encoded_features = None\n-        self.crossers = {\n-            cross.name: self._cross_to_crosser(cross) for cross in self.crosses\n-        }\n-        self.one_hot_encoders = {}\n-        self.built = False\n-        self._is_adapted = False\n-        self.concat = None\n-        self._preprocessed_features_names = None\n-        self._crossed_features_names = None\n-\n-    def _feature_to_input(self, name, feature):\n-        return layers.Input(shape=(1,), dtype=feature.dtype, name=name)\n-\n-    def _standardize_feature(self, name, feature):\n-        if isinstance(feature, Feature):\n-            return feature\n-\n-        if isinstance(feature, dict):\n-            return serialization_lib.deserialize_keras_object(feature)\n-\n-        if feature == \"float\":\n-            return self.float(name=name)\n-        elif feature == \"float_normalized\":\n-            return self.float_normalized(name=name)\n-        elif feature == \"float_rescaled\":\n-            return self.float_rescaled(name=name)\n-        elif feature == \"float_discretized\":\n-            return self.float_discretized(\n-                name=name, num_bins=self.num_discretization_bins\n-            )\n-        elif feature == \"integer_categorical\":\n-            return self.integer_categorical(name=name)\n-        elif feature == \"string_categorical\":\n-            return self.string_categorical(name=name)\n-        elif feature == \"integer_hashed\":\n-            return self.integer_hashed(self.hashing_dim, name=name)\n-        elif feature == \"string_hashed\":\n-            return self.string_hashed(self.hashing_dim, name=name)\n-        else:\n-            raise ValueError(f\"Invalid feature type: {feature}\")\n-\n-    def _cross_to_crosser(self, cross):\n-        return layers.HashedCrossing(cross.crossing_dim, name=cross.name)\n-\n-    def _list_adaptable_preprocessors(self):\n-        adaptable_preprocessors = []\n-        for name in self.features.keys():\n-            preprocessor = self.preprocessors[name]\n-            # Special case: a Normalization layer with preset mean/variance.\n-            # Not adaptable.\n-            if isinstance(preprocessor, layers.Normalization):\n-                if preprocessor.input_mean is not None:\n-                    continue\n-            if hasattr(preprocessor, \"adapt\"):\n-                adaptable_preprocessors.append(name)\n-        return adaptable_preprocessors\n-\n-    def adapt(self, dataset):\n-        if not isinstance(dataset, tf_data.Dataset):\n-            raise ValueError(\n-                \"`adapt()` can only be called on a tf.data.Dataset. \"\n-                f\"Received instead: {dataset} (of type {type(dataset)})\"\n-            )\n-\n-        for name in self._list_adaptable_preprocessors():\n-            # Call adapt() on each individual adaptable layer.\n-\n-            # TODO: consider rewriting this to instead iterate on the\n-            # dataset once, split each batch into individual features,\n-            # and call the layer's `_adapt_function` on each batch\n-            # to simulate the behavior of adapt() in a more performant fashion.\n-\n-            feature_dataset = dataset.map(lambda x: x[name])\n-            preprocessor = self.preprocessors[name]\n-            # TODO: consider adding an adapt progress bar.\n-            # Sample 1 element to check the rank\n-            for x in feature_dataset.take(1):\n-                pass\n-            if x.shape.rank == 0:\n-                # The dataset yields unbatched scalars; batch it.\n-                feature_dataset = feature_dataset.batch(32)\n-            if x.shape.rank in {0, 1}:\n-                # If the rank is 1, add a dimension\n-                # so we can reduce on axis=-1.\n-                # Note: if rank was previously 0, it is now 1.\n-                feature_dataset = feature_dataset.map(\n-                    lambda x: ops.expand_dims(x, -1)\n-                )\n-            preprocessor.adapt(feature_dataset)\n-        self._is_adapted = True\n-        self.get_encoded_features()  # Finish building the layer\n-        self.built = True\n-\n-    def get_inputs(self):\n-        self._check_if_built()\n-        return self.inputs\n-\n-    def get_encoded_features(self):\n-        self._check_if_adapted()\n-\n-        if self.encoded_features is None:\n-            preprocessed_features = self._preprocess_features(self.inputs)\n-            crossed_features = self._cross_features(preprocessed_features)\n-            merged_features = self._merge_features(\n-                preprocessed_features, crossed_features\n-            )\n-            self.encoded_features = merged_features\n-        return self.encoded_features\n-\n-    def _preprocess_features(self, features):\n-        return {\n-            name: self.preprocessors[name](features[name])\n-            for name in features.keys()\n-        }\n-\n-    def _cross_features(self, features):\n-        all_outputs = {}\n-        for cross in self.crosses:\n-            inputs = [features[name] for name in cross.feature_names]\n-            outputs = self.crossers[cross.name](inputs)\n-            all_outputs[cross.name] = outputs\n-        return all_outputs\n-\n-    def _merge_features(self, preprocessed_features, crossed_features):\n-        if not self._preprocessed_features_names:\n-            self._preprocessed_features_names = sorted(\n-                preprocessed_features.keys()\n-            )\n-            self._crossed_features_names = sorted(crossed_features.keys())\n-\n-        all_names = (\n-            self._preprocessed_features_names + self._crossed_features_names\n-        )\n-        all_features = [\n-            preprocessed_features[name]\n-            for name in self._preprocessed_features_names\n-        ] + [crossed_features[name] for name in self._crossed_features_names]\n-\n-        if self.output_mode == \"dict\":\n-            output_dict = {}\n-        else:\n-            features_to_concat = []\n-\n-        if self.built:\n-            # Fast mode.\n-            for name, feature in zip(all_names, all_features):\n-                encoder = self.one_hot_encoders.get(name, None)\n-                if encoder:\n-                    feature = encoder(feature)\n-                if self.output_mode == \"dict\":\n-                    output_dict[name] = feature\n-                else:\n-                    features_to_concat.append(feature)\n-            if self.output_mode == \"dict\":\n-                return output_dict\n-            else:\n-                return self.concat(features_to_concat)\n-\n-        # If the object isn't built,\n-        # we create the encoder and concat layers below\n-        all_specs = [\n-            self.features[name] for name in self._preprocessed_features_names\n-        ] + [\n-            self.crosses_by_name[name] for name in self._crossed_features_names\n-        ]\n-        for name, feature, spec in zip(all_names, all_features, all_specs):\n-            dtype = feature.dtype\n-\n-            if spec.output_mode == \"one_hot\":\n-                preprocessor = self.preprocessors.get(\n-                    name\n-                ) or self.crossers.get(name)\n-                cardinality = None\n-                if not feature.dtype.startswith(\"int\"):\n-                    raise ValueError(\n-                        f\"Feature '{name}' has `output_mode='one_hot'`. \"\n-                        \"Thus its preprocessor should return an int64 dtype. \"\n-                        f\"Instead it returns a {dtype} dtype.\"\n-                    )\n-\n-                if isinstance(\n-                    preprocessor, (layers.IntegerLookup, layers.StringLookup)\n-                ):\n-                    cardinality = preprocessor.vocabulary_size()\n-                elif isinstance(preprocessor, layers.CategoryEncoding):\n-                    cardinality = preprocessor.num_tokens\n-                elif isinstance(preprocessor, layers.Discretization):\n-                    cardinality = preprocessor.num_bins\n-                elif isinstance(\n-                    preprocessor, (layers.HashedCrossing, layers.Hashing)\n-                ):\n-                    cardinality = preprocessor.num_bins\n-                else:\n-                    raise ValueError(\n-                        f\"Feature '{name}' has `output_mode='one_hot'`. \"\n-                        \"However it isn't a standard feature and the \"\n-                        \"dimensionality of its output space is not known, \"\n-                        \"thus it cannot be one-hot encoded. \"\n-                        \"Try using `output_mode='int'`.\"\n-                    )\n-                if cardinality is not None:\n-                    encoder = layers.CategoryEncoding(\n-                        num_tokens=cardinality, output_mode=\"multi_hot\"\n-                    )\n-                    self.one_hot_encoders[name] = encoder\n-                    feature = encoder(feature)\n-\n-            if self.output_mode == \"concat\":\n-                dtype = feature.dtype\n-                if dtype.startswith(\"int\") or dtype == \"string\":\n-                    raise ValueError(\n-                        f\"Cannot concatenate features because feature '{name}' \"\n-                        f\"has not been encoded (it has dtype {dtype}). \"\n-                        \"Consider using `output_mode='dict'`.\"\n-                    )\n-                features_to_concat.append(feature)\n-            else:\n-                output_dict[name] = feature\n-\n-        if self.output_mode == \"concat\":\n-            self.concat = layers.Concatenate(axis=-1)\n-            return self.concat(features_to_concat)\n-        else:\n-            return output_dict\n-\n-    def _check_if_adapted(self):\n-        if not self._is_adapted:\n-            if not self._list_adaptable_preprocessors():\n-                self._is_adapted = True\n-            else:\n-                raise ValueError(\n-                    \"You need to call `.adapt(dataset)` on the FeatureSpace \"\n-                    \"before you can start using it.\"\n-                )\n-\n-    def _check_if_built(self):\n-        if not self.built:\n-            self._check_if_adapted()\n-            # Finishes building\n-            self.get_encoded_features()\n-            self.built = True\n-\n-    def __call__(self, data):\n-        self._check_if_built()\n-        if not isinstance(data, dict):\n-            raise ValueError(\n-                \"A FeatureSpace can only be called with a dict. \"\n-                f\"Received: data={data} (of type {type(data)}\"\n-            )\n-\n-        data = {\n-            key: ops.convert_to_tensor(value) for key, value in data.items()\n-        }\n-        rebatched = False\n-        for name, x in data.items():\n-            if x.shape.rank == 0:\n-                data[name] = ops.reshape(x, (1, 1))\n-                rebatched = True\n-            elif x.shape.rank == 1:\n-                data[name] = ops.expand_dims(x, -1)\n-\n-        preprocessed_data = self._preprocess_features(data)\n-        crossed_data = self._cross_features(preprocessed_data)\n-        merged_data = self._merge_features(preprocessed_data, crossed_data)\n-        if rebatched:\n-            if self.output_mode == \"concat\":\n-                assert merged_data.shape[0] == 1\n-                return ops.squeeze(merged_data, axis=0)\n-            else:\n-                for name, x in merged_data.items():\n-                    if x.shape.rank == 2 and x.shape[0] == 1:\n-                        merged_data[name] = ops.squeeze(x, axis=0)\n-        return merged_data\n-\n-    def get_config(self):\n-        return {\n-            \"features\": serialization_lib.serialize_keras_object(self.features),\n-            \"output_mode\": self.output_mode,\n-            \"crosses\": serialization_lib.serialize_keras_object(self.crosses),\n-            \"crossing_dim\": self.crossing_dim,\n-            \"hashing_dim\": self.hashing_dim,\n-            \"num_discretization_bins\": self.num_discretization_bins,\n-        }\n-\n-    @classmethod\n-    def from_config(cls, config):\n-        return cls(**config)\n-\n-    def get_build_config(self):\n-        return {\n-            name: feature.preprocessor.get_build_config()\n-            for name, feature in self.features.items()\n-        }\n-\n-    def build_from_config(self, config):\n-        for name in config.keys():\n-            self.features[name].preprocessor.build_from_config(config[name])\n-        self._is_adapted = True\n-\n-    def save(self, filepath):\n-        \"\"\"Save the `FeatureSpace` instance to a `.keras` file.\n-\n-        You can reload it via `keras.models.load_model()`:\n-\n-        ```python\n-        feature_space.save(\"myfeaturespace.keras\")\n-        reloaded_feature_space = keras.models.load_model(\"myfeaturespace.keras\")\n-        ```\n-        \"\"\"\n-        saving_lib.save_model(self, filepath)\n-\n-    def save_own_variables(self, store):\n-        return\n-\n-    def load_own_variables(self, store):\n-        return\n\n@@ -1,378 +0,0 @@\n-# from keras_core import testing\n-# from keras_core.utils import feature_space\n-# from keras_core import operations as ops\n-# from tensorflow import nest\n-# from tensorflow import data as tf_data\n-# from keras_core import layers\n-# from keras_core import models\n-# import os\n-\n-\n-# class FeatureSpaceTest(testing.TestCase):\n-#     def _get_train_data_dict(\n-#         self, as_dataset=False, as_tf_tensors=False, as_labeled_dataset=False\n-#     ):\n-#         data = {\n-#             \"float_1\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n-#             \"float_2\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n-#             \"float_3\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n-#             \"string_1\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n-#             \"string_2\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n-#             \"int_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n-#             \"int_2\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n-#             \"int_3\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n-#         }\n-#         if as_dataset:\n-#             return tf_data.Dataset.from_tensor_slices(data)\n-#         elif as_tf_tensors:\n-#             return nest.map_structure(ops.convert_to_tensor, data)\n-#         elif as_labeled_dataset:\n-#             labels = [0, 1, 0, 1, 0, 0, 1, 0, 1, 1]\n-#             return tf_data.Dataset.from_tensor_slices((data, labels))\n-#         return data\n-\n-#     def test_basic_usage(self):\n-#         fs = feature_space.FeatureSpace(\n-#             features={\n-#                 \"float_1\": \"float\",\n-#                 \"float_2\": \"float_normalized\",\n-#                 \"float_3\": \"float_discretized\",\n-#                 \"string_1\": \"string_categorical\",\n-#                 \"string_2\": \"string_hashed\",\n-#                 \"int_1\": \"integer_categorical\",\n-#                 \"int_2\": \"integer_hashed\",\n-#                 \"int_3\": \"integer_categorical\",\n-#             },\n-#             crosses=[(\"float_3\", \"string_1\"), (\"string_2\", \"int_2\")],\n-#             output_mode=\"concat\",\n-#         )\n-#         # Test unbatched adapt\n-#         fs.adapt(self._get_train_data_dict(as_dataset=True))\n-#         # Test batched adapt\n-#         fs.adapt(self._get_train_data_dict(as_dataset=True).batch(4))\n-\n-#         # Test unbatched call on raw data\n-#         data = {\n-#             key: value[0] for key, value in self._get_train_data_dict().items()\n-#         }\n-#         out = fs(data)\n-#         self.assertEqual(out.shape, [195])\n-\n-#         # Test unbatched call on TF tensors\n-#         data = self._get_train_data_dict(as_tf_tensors=True)\n-#         data = {key: value[0] for key, value in data.items()}\n-#         out = fs(data)\n-#         self.assertEqual(out.shape, [195])\n-\n-#         # Test batched call on raw data\n-#         out = fs(self._get_train_data_dict())\n-#         self.assertEqual(out.shape, [10, 195])\n-\n-#         # Test batched call on TF tensors\n-#         out = fs(self._get_train_data_dict(as_tf_tensors=True))\n-#         self.assertEqual(out.shape, [10, 195])\n-\n-#     def test_output_mode_dict(self):\n-#         fs = feature_space.FeatureSpace(\n-#             features={\n-#                 \"float_1\": \"float\",\n-#                 \"float_2\": \"float_normalized\",\n-#                 \"float_3\": \"float_discretized\",\n-#                 \"string_1\": \"string_categorical\",\n-#                 \"string_2\": \"string_hashed\",\n-#                 \"int_1\": \"integer_categorical\",\n-#                 \"int_2\": \"integer_hashed\",\n-#                 \"int_3\": \"integer_categorical\",\n-#             },\n-#             crosses=[(\"float_3\", \"string_1\"), (\"string_2\", \"int_2\")],\n-#             output_mode=\"dict\",\n-#         )\n-#         fs.adapt(self._get_train_data_dict(as_dataset=True))\n-\n-#         # Test unbatched call on raw data\n-#         data = {\n-#             key: value[0] for key, value in self._get_train_data_dict().items()\n-#         }\n-#         out = fs(data)\n-#         self.assertIsInstance(out, dict)\n-#         self.assertLen(out, 10)\n-#         self.assertEqual(out[\"string_1\"].shape, [11])\n-#         self.assertEqual(out[\"int_2\"].shape, [32])\n-#         self.assertEqual(out[\"string_2_X_int_2\"].shape, [32])\n-\n-#         # Test batched call on raw data\n-#         out = fs(self._get_train_data_dict())\n-#         self.assertIsInstance(out, dict)\n-#         self.assertLen(out, 10)\n-#         self.assertEqual(out[\"string_1\"].shape, [10, 11])\n-#         self.assertEqual(out[\"int_2\"].shape, [10, 32])\n-#         self.assertEqual(out[\"string_2_X_int_2\"].shape, [10, 32])\n-\n-#         # Test batched call on TF tensors\n-#         out = fs(self._get_train_data_dict(as_tf_tensors=True))\n-#         self.assertIsInstance(out, dict)\n-#         self.assertLen(out, 10)\n-#         self.assertEqual(out[\"string_1\"].shape, [10, 11])\n-#         self.assertEqual(out[\"int_2\"].shape, [10, 32])\n-#         self.assertEqual(out[\"string_2_X_int_2\"].shape, [10, 32])\n-\n-#     def test_output_mode_dict_of_ints(self):\n-#         cls = feature_space.FeatureSpace\n-#         fs = feature_space.FeatureSpace(\n-#             features={\n-#                 \"float_1\": \"float\",\n-#                 \"float_2\": \"float_normalized\",\n-#                 \"float_3\": \"float_discretized\",\n-#                 \"string_1\": cls.string_categorical(output_mode=\"int\"),\n-#                 \"string_2\": cls.string_hashed(num_bins=32, output_mode=\"int\"),\n-#                 \"int_1\": cls.integer_categorical(output_mode=\"int\"),\n-#                 \"int_2\": cls.integer_hashed(num_bins=32, output_mode=\"int\"),\n-#                 \"int_3\": cls.integer_categorical(output_mode=\"int\"),\n-#             },\n-#             crosses=[\n-#                 cls.cross(\n-#                     (\"float_3\", \"string_1\"), output_mode=\"int\", crossing_dim=32\n-#                 ),\n-#                 cls.cross(\n-#                     (\"string_2\", \"int_2\"), output_mode=\"int\", crossing_dim=32\n-#                 ),\n-#             ],\n-#             output_mode=\"dict\",\n-#         )\n-#         fs.adapt(self._get_train_data_dict(as_dataset=True))\n-#         data = {\n-#             key: value[0] for key, value in self._get_train_data_dict().items()\n-#         }\n-#         out = fs(data)\n-#         self.assertIsInstance(out, dict)\n-#         self.assertLen(out, 10)\n-#         self.assertEqual(out[\"string_1\"].shape, [1])\n-#         self.assertEqual(out[\"string_1\"].dtype.name, \"int64\")\n-#         self.assertEqual(out[\"int_2\"].shape, [1])\n-#         self.assertEqual(out[\"int_2\"].dtype.name, \"int64\")\n-#         self.assertEqual(out[\"string_2_X_int_2\"].shape, [1])\n-#         self.assertEqual(out[\"string_2_X_int_2\"].dtype.name, \"int64\")\n-\n-#     def test_functional_api_sync_processing(self):\n-#         fs = feature_space.FeatureSpace(\n-#             features={\n-#                 \"float_1\": \"float\",\n-#                 \"float_2\": \"float_normalized\",\n-#                 \"float_3\": \"float_discretized\",\n-#                 \"string_1\": \"string_categorical\",\n-#                 \"string_2\": \"string_hashed\",\n-#                 \"int_1\": \"integer_categorical\",\n-#                 \"int_2\": \"integer_hashed\",\n-#                 \"int_3\": \"integer_categorical\",\n-#             },\n-#             crosses=[(\"float_3\", \"string_1\"), (\"string_2\", \"int_2\")],\n-#             output_mode=\"concat\",\n-#         )\n-#         fs.adapt(self._get_train_data_dict(as_dataset=True))\n-#         inputs = fs.get_inputs()\n-#         features = fs.get_encoded_features()\n-#         outputs = layers.Dense(1)(features)\n-#         model = models.Model(inputs=inputs, outputs=outputs)\n-#         model.compile(\"adam\", \"mse\")\n-#         ds = self._get_train_data_dict(as_labeled_dataset=True)\n-#         model.fit(ds.batch(4))\n-#         model.evaluate(ds.batch(4))\n-#         ds = self._get_train_data_dict(as_dataset=True)\n-#         model.predict(ds.batch(4))\n-\n-#     def test_tf_data_async_processing(self):\n-#         fs = feature_space.FeatureSpace(\n-#             features={\n-#                 \"float_1\": \"float\",\n-#                 \"float_2\": \"float_normalized\",\n-#                 \"float_3\": \"float_discretized\",\n-#                 \"string_1\": \"string_categorical\",\n-#                 \"string_2\": \"string_hashed\",\n-#                 \"int_1\": \"integer_categorical\",\n-#                 \"int_2\": \"integer_hashed\",\n-#                 \"int_3\": \"integer_categorical\",\n-#             },\n-#             crosses=[(\"float_3\", \"string_1\"), (\"string_2\", \"int_2\")],\n-#             output_mode=\"concat\",\n-#         )\n-#         fs.adapt(self._get_train_data_dict(as_dataset=True))\n-#         features = fs.get_encoded_features()\n-#         outputs = layers.Dense(1)(features)\n-#         model = models.Model(inputs=features, outputs=outputs)\n-#         model.compile(\"adam\", \"mse\")\n-#         ds = self._get_train_data_dict(as_labeled_dataset=True)\n-#         # Try map before batch\n-#         ds = ds.map(lambda x, y: (fs(x), y))\n-#         model.fit(ds.batch(4))\n-#         # Try map after batch\n-#         ds = self._get_train_data_dict(as_labeled_dataset=True)\n-#         ds = ds.batch(4)\n-#         ds = ds.map(lambda x, y: (fs(x), y))\n-#         model.evaluate(ds)\n-#         ds = self._get_train_data_dict(as_dataset=True)\n-#         ds = ds.map(fs)\n-#         model.predict(ds.batch(4))\n-\n-#     def test_advanced_usage(self):\n-#         cls = feature_space.FeatureSpace\n-#         fs = feature_space.FeatureSpace(\n-#             features={\n-#                 \"float_1\": cls.float(),\n-#                 \"float_2\": cls.float_normalized(),\n-#                 \"float_3\": cls.float_discretized(num_bins=3),\n-#                 \"string_1\": cls.string_categorical(max_tokens=5),\n-#                 \"string_2\": cls.string_hashed(num_bins=32),\n-#                 \"int_1\": cls.integer_categorical(\n-#                     max_tokens=5, num_oov_indices=2\n-#                 ),\n-#                 \"int_2\": cls.integer_hashed(num_bins=32),\n-#                 \"int_3\": cls.integer_categorical(max_tokens=5),\n-#             },\n-#             crosses=[\n-#                 cls.cross((\"float_3\", \"string_1\"), crossing_dim=32),\n-#                 cls.cross((\"string_2\", \"int_2\"), crossing_dim=32),\n-#             ],\n-#             output_mode=\"concat\",\n-#         )\n-#         fs.adapt(self._get_train_data_dict(as_dataset=True))\n-#         data = {\n-#             key: value[0] for key, value in self._get_train_data_dict().items()\n-#         }\n-#         out = fs(data)\n-#         self.assertEqual(out.shape, [148])\n-\n-#     def test_manual_kpl(self):\n-#         data = {\n-#             \"text\": [\"1st string\", \"2nd string\", \"3rd string\"],\n-#         }\n-#         cls = feature_space.FeatureSpace\n-\n-#         # Test with a tf-idf TextVectorization layer\n-#         tv = layers.TextVectorization(output_mode=\"tf_idf\")\n-#         fs = feature_space.FeatureSpace(\n-#             features={\n-#                 \"text\": cls.feature(\n-#                     preprocessor=tv, dtype=\"string\", output_mode=\"float\"\n-#                 ),\n-#             },\n-#             output_mode=\"concat\",\n-#         )\n-#         fs.adapt(tf_data.Dataset.from_tensor_slices(data))\n-#         out = fs(data)\n-#         self.assertEqual(out.shape, [3, 5])\n-\n-#     def test_no_adapt(self):\n-#         data = {\n-#             \"int_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n-#         }\n-#         fs = feature_space.FeatureSpace(\n-#             {\n-#                 \"int_1\": \"integer_hashed\",\n-#             },\n-#             output_mode=\"concat\",\n-#         )\n-#         out = fs(data)\n-#         self.assertEqual(out.shape, [10, 32])\n-\n-#     def test_saving(self):\n-#         cls = feature_space.FeatureSpace\n-#         fs = feature_space.FeatureSpace(\n-#             features={\n-#                 \"float_1\": cls.float(),\n-#                 \"float_2\": cls.float_normalized(),\n-#                 \"float_3\": cls.float_discretized(num_bins=3),\n-#                 \"string_1\": cls.string_categorical(max_tokens=5),\n-#                 \"string_2\": cls.string_hashed(num_bins=32),\n-#                 \"int_1\": cls.integer_categorical(\n-#                     max_tokens=5, num_oov_indices=2\n-#                 ),\n-#                 \"int_2\": cls.integer_hashed(num_bins=32),\n-#                 \"int_3\": cls.integer_categorical(max_tokens=5),\n-#             },\n-#             crosses=[\n-#                 cls.cross((\"float_3\", \"string_1\"), crossing_dim=32),\n-#                 cls.cross((\"string_2\", \"int_2\"), crossing_dim=32),\n-#             ],\n-#             output_mode=\"concat\",\n-#         )\n-#         fs.adapt(self._get_train_data_dict(as_dataset=True))\n-#         data = {\n-#             key: value[0] for key, value in self._get_train_data_dict().items()\n-#         }\n-#         ref_out = fs(data)\n-\n-#         temp_filepath = os.path.join(self.get_temp_dir(), \"fs.keras\")\n-#         fs.save(temp_filepath)\n-#         fs = models.models.load_model(temp_filepath)\n-\n-#         # Save again immediately after loading to test idempotency\n-#         temp_filepath = os.path.join(self.get_temp_dir(), \"fs2.keras\")\n-#         fs.save(temp_filepath)\n-\n-#         # Test correctness of the first saved FS\n-#         out = fs(data)\n-#         self.assertAllClose(out, ref_out)\n-\n-#         inputs = fs.get_inputs()\n-#         outputs = fs.get_encoded_features()\n-#         model = models.Model(inputs=inputs, outputs=outputs)\n-#         ds = self._get_train_data_dict(as_dataset=True)\n-#         out = model.predict(ds.batch(4))\n-#         self.assertAllClose(out[0], ref_out)\n-\n-#         # Test correctness of the re-saved FS\n-#         fs = models.models.load_model(temp_filepath)\n-#         out = fs(data)\n-#         self.assertAllClose(out, ref_out)\n-\n-#     def test_errors(self):\n-#         # Test no features\n-#         with self.assertRaisesRegex(ValueError, \"cannot be None or empty\"):\n-#             feature_space.FeatureSpace(features={})\n-#         # Test no crossing dim\n-#         with self.assertRaisesRegex(ValueError, \"`crossing_dim`\"):\n-#             feature_space.FeatureSpace(\n-#                 features={\n-#                     \"f1\": \"integer_categorical\",\n-#                     \"f2\": \"integer_categorical\",\n-#                 },\n-#                 crosses=[(\"f1\", \"f2\")],\n-#                 crossing_dim=None,\n-#             )\n-#         # Test wrong cross feature name\n-#         with self.assertRaisesRegex(ValueError, \"should be present in \"):\n-#             feature_space.FeatureSpace(\n-#                 features={\n-#                     \"f1\": \"integer_categorical\",\n-#                     \"f2\": \"integer_categorical\",\n-#                 },\n-#                 crosses=[(\"f1\", \"unknown\")],\n-#                 crossing_dim=32,\n-#             )\n-#         # Test wrong output mode\n-#         with self.assertRaisesRegex(ValueError, \"for argument `output_mode`\"):\n-#             feature_space.FeatureSpace(\n-#                 features={\n-#                     \"f1\": \"integer_categorical\",\n-#                     \"f2\": \"integer_categorical\",\n-#                 },\n-#                 output_mode=\"unknown\",\n-#             )\n-#         # Test call before adapt\n-#         with self.assertRaisesRegex(ValueError, \"You need to call `.adapt\"):\n-#             fs = feature_space.FeatureSpace(\n-#                 features={\n-#                     \"f1\": \"integer_categorical\",\n-#                     \"f2\": \"integer_categorical\",\n-#                 }\n-#             )\n-#             fs({\"f1\": [0], \"f2\": [0]})\n-#         # Test get_encoded_features before adapt\n-#         with self.assertRaisesRegex(ValueError, \"You need to call `.adapt\"):\n-#             fs = feature_space.FeatureSpace(\n-#                 features={\n-#                     \"f1\": \"integer_categorical\",\n-#                     \"f2\": \"integer_categorical\",\n-#                 }\n-#             )\n-#             fs.get_encoded_features()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
