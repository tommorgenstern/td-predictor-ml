{"custom_id": "scrapy#1b2c9e49d819ef013a99620f097f92b3367d9c44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 95 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ from unittest import TestCase\n from scrapy.spider import spiders\n from scrapy.core.exceptions import HttpException\n from scrapy.http import Response, Request\n-from scrapy.contrib_exp.downloadermiddleware.cookies import CookiesMiddleware\n+from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware\n \n \n class CookiesMiddlewareTest(TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#315a24add45043e1040cc1011b62013f0f90a9f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 127 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy, Cookie\n class CookieJar(object):\n     def __init__(self, policy=None):\n         self.jar = _CookieJar(policy or DefaultCookiePolicy())\n-        self.jar._cookies_jar = _DummyLock()\n+        self.jar._cookies_lock = _DummyLock()\n \n     def extract_cookies(self, response, request):\n         wreq = WrappedRequest(request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45f55c0df0bc1aec7068b9f6e39d1aa5eee0b046", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 15 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 39 | Churn Cumulative: 237 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ class CookiesMiddleware(object):\n \n     def process_response(self, request, response, spider):\n         if request.meta.get('dont_merge_cookies', False):\n-            return\n+            return response\n \n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n         jar = self.jars[spider.domain_name]\n\n@@ -23,65 +23,64 @@ class CookiesMiddlewareTest(TestCase):\n     def test_basic(self):\n         headers = {'Set-Cookie': 'C1=value1; path=/'}\n         req = Request('http://scrapytest.org/')\n-        self.mw.process_request(req, self.spider)\n-\n+        assert self.mw.process_request(req, self.spider) is None\n         assert 'Cookie' not in req.headers\n \n         res = Response('http://scrapytest.org/', headers=headers)\n-        self.mw.process_response(req, res, self.spider)\n+        assert self.mw.process_response(req, res, self.spider) is res\n \n         #assert res.cookies\n \n         req2 = Request('http://scrapytest.org/sub1/')\n-        self.mw.process_request(req2, self.spider)\n+        assert self.mw.process_request(req2, self.spider) is None\n         self.assertEquals(req2.headers.get('Cookie'), \"C1=value1\")\n \n     def test_http_exception(self):\n+        req = Request('http://scrapytest.org/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        assert 'Cookie' not in req.headers\n+\n         headers = {'Set-Cookie': 'C1=value1; path=/'}\n         res = Response('http://scrapytest.org/', headers=headers)\n         exc = HttpException(302, 'Redirect', res)\n-\n-        req = Request('http://scrapytest.org/')\n-        self.mw.process_request(req, self.spider)\n-        assert 'Cookie' not in req.headers\n-\n-        self.mw.process_exception(req, exc, self.spider)\n+        assert self.mw.process_exception(req, exc, self.spider) is None\n         #assert exc.response.cookies\n \n         req2 = Request('http://scrapytest.org/sub1/')\n-        self.mw.process_request(req2, self.spider)\n+        assert self.mw.process_request(req2, self.spider) is None\n         self.assertEquals(req2.headers.get('Cookie'), \"C1=value1\")\n \n-\n     def test_dont_merge_cookies(self):\n         # merge some cookies into jar\n         headers = {'Set-Cookie': 'C1=value1; path=/'}\n         req = Request('http://scrapytest.org/')\n         res = Response('http://scrapytest.org/', headers=headers)\n-        self.mw.process_response(req, res, self.spider)\n+        assert self.mw.process_response(req, res, self.spider) is res\n \n         # test Cookie header is not seted to request\n         req = Request('http://scrapytest.org/dontmerge', meta={'dont_merge_cookies': 1})\n-        self.mw.process_request(req, self.spider)\n+        assert self.mw.process_request(req, self.spider) is None\n         assert 'Cookie' not in req.headers\n \n         # check that returned cookies are not merged back to jar\n         res = Response('http://scrapytest.org/dontmerge', headers={'Set-Cookie': 'dont=mergeme; path=/'})\n-        self.mw.process_response(req, res, self.spider)\n+        assert self.mw.process_response(req, res, self.spider) is res\n+\n         req = Request('http://scrapytest.org/mergeme')\n-        self.mw.process_request(req, self.spider)\n+        assert self.mw.process_request(req, self.spider) is None\n         self.assertEquals(req.headers.get('Cookie'), 'C1=value1')\n \n     def test_merge_request_cookies(self):\n         req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'})\n-        self.mw.process_request(req, self.spider)\n+        assert self.mw.process_request(req, self.spider) is None\n         self.assertEquals(req.headers.get('Cookie'), 'galleta=salada')\n \n         headers = {'Set-Cookie': 'C1=value1; path=/'}\n         res = Response('http://scrapytest.org/', headers=headers)\n-        self.mw.process_response(req, res, self.spider)\n+        assert self.mw.process_response(req, res, self.spider) is res\n+\n         req2 = Request('http://scrapytest.org/sub1/')\n-        self.mw.process_request(req2, self.spider)\n+        assert self.mw.process_request(req2, self.spider) is None\n         self.assertEquals(req2.headers.get('Cookie'), \"C1=value1; galleta=salada\")\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68f54a1a637242a7f6abfabf96f8e25df2a20f1a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 1161 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1100,7 +1100,7 @@ class LWPCookieTests(TestCase):\n                 'PART_NUMBER=ROCKET_LAUNCHER_0001; NO_A_BOT; GOOD_CUSTOMER')\n \n \n-class WrapperRequestResponse(TestCase):\n+class CookielibWrappersTest(TestCase):\n \n     def FakeRequest(self, url, data='', headers={}):\n         tail = '?' + urllib.quote(data) if data else ''\n@@ -1147,4 +1147,13 @@ class WrapperRequestResponse(TestCase):\n         req = self.FakeRequest(\"http://www.acme.com:2345/resource.html\", headers={\"Host\": \"www.acme.com:5432\"})\n         self.assertEquals(request_host(req), \"www.acme.com\")\n \n+    def test_threadlock_override(self):\n+        try:\n+            import threading as _threading\n+        except ImportError:\n+            import dummy_threading as _threading\n+\n+        c = CookieJar()\n+        assert not isinstance(c.jar._cookies_lock, _threading._RLock)\n+\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a86a89f41a05845157531b616fc7f83032bda197", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 72/72 | Churn Δ: 4 | Churn Cumulative: 109 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ from pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.http import Response\n-from scrapy.utils.cookies import CookieJar\n+from scrapy.http.cookies import CookieJar\n from scrapy.core.exceptions import HttpException\n from scrapy.conf import settings\n from scrapy import log\n\n\n@@ -7,7 +7,7 @@ from unittest import TestCase\n from test import test_support\n \n from scrapy.http import Request, Response, Headers\n-from scrapy.utils.cookies import WrappedRequest, WrappedResponse, CookieJar, DefaultCookiePolicy\n+from scrapy.http.cookies import WrappedRequest, WrappedResponse, CookieJar, DefaultCookiePolicy\n \n \n def interact_2965(cookiejar, url, *set_cookie_hdrs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0842fafd63e82051a27d8d0fe6c3cba0551799b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 58 | Churn Cumulative: 65 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,14 @@\n+\"\"\"Set User-Agent header per spider or use a default value from settings\"\"\"\n+\n+from scrapy.conf import settings\n+\n+\n class UserAgentMiddleware(object):\n     \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n \n-    def process_request(self, request, spider):\n-        if getattr(spider, 'user_agent', None):\n-            request.headers.setdefault('User-Agent', spider.user_agent)\n+    default_useragent = settings.get('USER_AGENT')\n \n+    def process_request(self, request, spider):\n+        ua = getattr(spider, 'user_agent', self.default_useragent)\n+        if ua:\n+            request.headers.setdefault('User-Agent', ua)\n\n@@ -0,0 +1,45 @@\n+from unittest import TestCase\n+\n+from scrapy.spider import spiders\n+from scrapy.http import Request\n+from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware\n+from scrapy.conf import settings\n+\n+\n+class CookiesMiddlewareTest(TestCase):\n+\n+    def setUp(self):\n+        spiders.spider_modules = ['scrapy.tests.test_spiders']\n+        spiders.reload()\n+        self.spider = spiders.fromdomain('scrapytest.org')\n+        self.mw = UserAgentMiddleware()\n+\n+    def tearDown(self):\n+        del self.mw\n+\n+    def test_default_agent(self):\n+        self.mw.default_useragent = 'default_useragent'\n+        req = Request('http://scrapytest.org/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEquals(req.headers['User-Agent'], 'default_useragent')\n+\n+    def test_spider_agent(self):\n+        self.spider.user_agent = 'spider_useragent'\n+        req = Request('http://scrapytest.org/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEquals(req.headers['User-Agent'], 'spider_useragent')\n+\n+    def test_header_agent(self):\n+        self.mw.default_useragent = 'default_useragent'\n+        self.spider.user_agent = 'spider_useragent'\n+        req = Request('http://scrapytest.org/', headers={'User-Agent': 'header_useragent'})\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEquals(req.headers['User-Agent'], 'header_useragent')\n+\n+    def test_no_agent(self):\n+        self.mw.default_useragent = None\n+        self.spider.user_agent = None\n+        req = Request('http://scrapytest.org/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        assert 'User-Agent' not in req.headers\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8eaa9219f6873381102175b7bd1f0709918e5b46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 456 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -272,7 +272,7 @@ class WebClientTestCase(unittest.TestCase):\n     def testFactoryInfo(self):\n         url = self.getURL('file')\n         scheme, host, port, path = client._parse(url)\n-        factory = client.HTTPClientFactory(url)\n+        factory = client.ScrapyHTTPClientFactory(url)\n         reactor.connectTCP(host, port, factory)\n         return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n \n@@ -280,7 +280,7 @@ class WebClientTestCase(unittest.TestCase):\n         self.assertEquals(factory.status, '200')\n         self.assert_(factory.version.startswith('HTTP/'))\n         self.assertEquals(factory.message, 'OK')\n-        self.assertEquals(factory.response_headers['content-length'][0], '10')\n+        self.assertEquals(factory.response_headers['content-length'], '10')\n \n     def testRedirect(self):\n         return client.getPage(self.getURL(\"redirect\")).addCallback(self._cbRedirect)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b553bb3dff52d018811900cb133985acf11beeaa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 108 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,6 @@ class CookiesMiddleware(object):\n         jar.extract_cookies(response, request)\n         self._debug_set_cookie(response)\n \n-        # TODO: set current cookies in jar to response.cookies?\n         return response\n \n     # cookies should be set on non-200 responses too\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1eac716edf6328810bb5916322481b25a90c5ba4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 227 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -45,6 +45,8 @@ COMMANDS_SETTINGS_MODULE = ''\n \n CONCURRENT_DOMAINS = 8    # number of domains to scrape in parallel\n \n+COOKIES_DEBUG = False\n+\n DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n \n DEFAULT_SPIDER = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#78fa0bc2d8b506fc342ccc967382d4014ab34f40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 686 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -236,7 +236,7 @@ class ExecutionEngine(object):\n \n             def _onsuccess_per_item(item):\n                 if isinstance(item, ScrapedItem):\n-                    log.msg(\"Scraped %s in <%s>\" % (item, request.url), log.DEBUG, domain=domain)\n+                    log.msg(\"Scraped %s in <%s>\" % (item, request.url), log.INFO, domain=domain)\n                     signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, item=item, spider=spider, response=response)\n                     piped = self.pipeline.pipe(item, spider)\n                     piped.addBoth(_onpipelinefinish, item)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#560dd3fd8ee470f72bc215f29b0e4e4ab3d77185", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 74 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,6 @@ class UserAgentMiddleware(object):\n     default_useragent = settings.get('USER_AGENT')\n \n     def process_request(self, request, spider):\n-        ua = getattr(spider, 'user_agent', self.default_useragent)\n+        ua = getattr(spider, 'user_agent', None) or self.default_useragent\n         if ua:\n             request.headers.setdefault('User-Agent', ua)\n\n@@ -23,7 +23,14 @@ class CookiesMiddlewareTest(TestCase):\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEquals(req.headers['User-Agent'], 'default_useragent')\n \n+        # None or not present user_agent attribute is the same\n+        self.spider.user_agent = None\n+        req = Request('http://scrapytest.org/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEquals(req.headers['User-Agent'], 'default_useragent')\n+\n     def test_spider_agent(self):\n+        self.mw.default_useragent = 'default_useragent'\n         self.spider.user_agent = 'spider_useragent'\n         req = Request('http://scrapytest.org/')\n         assert self.mw.process_request(req, self.spider) is None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4e7ee5dac82eb50c3c647438cba2b9c4370c782", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 21 | Churn Cumulative: 216 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,6 +24,7 @@ class XMLFeedSpider(BaseSpider):\n \n     iterator = 'iternodes'\n     itertag = 'item'\n+    namespaces = ()\n \n     def process_results(self, response, results):\n         \"\"\"This overridable method is called for each result (item or request)\n@@ -42,6 +43,10 @@ class XMLFeedSpider(BaseSpider):\n         \"\"\"\n         return response\n \n+    def parse_item(self, response, selector):\n+        \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n+        raise NotImplemented\n+        \n     def parse_nodes(self, response, nodes):\n         \"\"\"This method is called for the nodes matching the provided tag name\n         (itertag). Receives the response and an XPathSelector for each node.\n@@ -67,14 +72,22 @@ class XMLFeedSpider(BaseSpider):\n         if self.iterator == 'iternodes':\n             nodes = xmliter(response, self.itertag)\n         elif self.iterator == 'xml':\n-            nodes = XmlXPathSelector(response).x('//%s' % self.itertag)\n+            selector = XmlXPathSelector(response)\n+            self._register_namespaces(selector)\n+            nodes = selector.x('//%s' % self.itertag)\n         elif self.iterator == 'html':\n-            nodes = HtmlXPathSelector(response).x('//%s' % self.itertag)\n+            selector = HtmlXPathSelector(response)\n+            self._register_namespaces(selector)\n+            nodes = selector.x('//%s' % self.itertag)\n         else:\n             raise NotSupported('Unsupported node iterator')\n \n         return self.parse_nodes(response, nodes)\n \n+    def _register_namespaces(self, selector):\n+        for (prefix, uri) in self.namespaces:\n+            selector.register_namespace(prefix, uri)\n+\n class CSVFeedSpider(BaseSpider):\n     \"\"\"Spider for parsing CSV feeds.\n     It receives a CSV file in a response; iterates through each of its rows,\n@@ -95,6 +108,10 @@ class CSVFeedSpider(BaseSpider):\n         \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n         return response\n \n+    def parse_row(self, row):\n+        \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n+        raise NotImplemented\n+\n     def parse_rows(self, response):\n         \"\"\"Receives a response and a dict (representing each row) with a key for\n         each provided (or detected) header of the CSV file.  This spider also\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#940d0090af06d2cfa2577a7a67b2e63590ad1c57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 12 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-# -*- coding: latin-1 -*-\n \"\"\"Tests for cookielib.py.\"\"\"\n \n import re, os, time\n@@ -950,18 +949,21 @@ class LWPCookieTests(TestCase):\n                       \"foo  =   bar; version    =   1\")\n \n         cookie = interact_2965(\n-            c, \"http://www.acme.com/foo%2f%25/<<%0anewå/æøå\",\n+            c, \"http://www.acme.com/foo%2f%25/<<%0anew\\xc3\\xa5/\\xc3\\xa6\\xc3\\xb8\\xc3\\xa5\",\n             'bar=baz; path=\"/foo/\"; version=1');\n         version_re = re.compile(r'^\\$version=\\\"?1\\\"?', re.I)\n         # FIXME: test disabled until fix\n         #self.assert_(\"foo=bar\" in cookie and version_re.search(cookie))\n \n         cookie = interact_2965(\n-            c, \"http://www.acme.com/foo/%25/<<%0anewå/æøå\")\n+            c, \"http://www.acme.com/foo/%25/<<%0anew\\xc3\\xa5/\\xc3\\xa6\\xc3\\xb8\\xc3\\xa5\")\n         self.assert_(not cookie)\n \n         # unicode URL doesn't raise exception\n-        cookie = interact_2965(c, u\"http://www.acme.com/\\xfc\")\n+        ## NOTE: this test is commented because it doesn't really test anything\n+        ## useful any it conflicts with Scrapy management of response urls which\n+        ## are not allowed to be unicode (except for encoding-aware responses)\n+        #cookie = interact_2965(c, u\"http://www.acme.com/\\xfc\")\n \n     def test_netscape_misc(self):\n         # Some additional Netscape cookies tests.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc62bba80e91ce5bcda17c868cceaf51cca2d13c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 51 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,4 +43,5 @@ class CloseDomain(object):\n             self.mail.send(self.notify, subj, body)\n \n     def domain_closed(self, domain):\n+        if domain in self.tasks:\n             scrapyengine.removetask(self.tasks[domain])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#239b056ddd5fbfb3861569a83b36e7c94434acd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 33 | Churn Cumulative: 766 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ LinkExtractor provides en efficient way to extract links from pages\n See documentation in docs/ref/link-extractors.rst\n \"\"\"\n \n-from scrapy.utils.python import FixedSGMLParser, unique as unique_list\n+from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_unicode\n from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin\n \n class LinkExtractor(FixedSGMLParser):\n@@ -53,7 +53,7 @@ class LinkExtractor(FixedSGMLParser):\n         for link in links:\n             link.url = urljoin(base_url, link.url)\n             link.url = safe_url_string(link.url, response_encoding)\n-            link.text = link.text.decode(response_encoding)\n+            link.text = str_to_unicode(link.text, response_encoding)\n             ret.append(link)\n \n         return ret\n\n@@ -110,11 +110,6 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual([link for link in lx.extract_links(self.response)],\n             [ Link(url='http://example.com/sample2.jpg', text=u'') ])\n \n-#        lx = RegexLinkExtractor(restrict_xpaths=('//div[@id=\"subwrapper\"]', ))\n-#        self.assertEqual([link for link in lx.extract_links(self.response)],\n-#            [ Link(url='http://example.com/sample1.html', text=u''),\n-#              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n-\n     def test_extraction_using_single_values(self):\n         '''Test the extractor's behaviour among different situations'''\n \n@@ -164,6 +159,30 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.matches('http://blah2.com/blah1'), False)\n         self.assertEqual(lx.matches('http://blah2.com/blah2'), False)\n \n+    def test_restrict_xpaths(self):\n+        lx = RegexLinkExtractor(restrict_xpaths=('//div[@id=\"subwrapper\"]', ))\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://example.com/sample1.html', text=u''),\n+              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n+\n+    def test_restrict_xpaths_encoding(self):\n+        \"\"\"Test restrict_xpaths with encodings\"\"\"\n+        html = \"\"\"<html><head><title>Page title<title>\n+        <body><p><a href=\"item/12.html\">Item 12</a></p>\n+        <div class='links'>\n+        <p><a href=\"/about.html\">About us\\xa3</a></p>\n+        </div>\n+        <div>\n+        <p><a href=\"/nofollow.html\">This shouldn't be followed</a></p>\n+        </div>\n+        </body></html>\"\"\"\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html, encoding='windows-1252')\n+\n+        lx = RegexLinkExtractor(restrict_xpaths=\"//div[@class='links']\") \n+        self.assertEqual(lx.extract_links(response),\n+                         [Link(url='http://example.org/about.html', text=u'About us\\xa3')])\n+\n+\n class HTMLImageLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n         base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d4cc91ae5443db37afe32813d2d6ba6d008a2edd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 48 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 635 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -8,31 +8,6 @@ from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_u\n from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin\n \n class LinkExtractor(FixedSGMLParser):\n-    \"\"\"LinkExtractor are used to extract links from web pages. They are\n-    instantiated and later \"applied\" to a Response using the extract_links\n-    method which must receive a Response object and return a list of Link objects\n-    containing the (absolute) urls to follow, and the links texts.\n-\n-    This is the base LinkExtractor class that provides enough basic\n-    functionality for extracting links to follow, but you could override this\n-    class or create a new one if you need some additional functionality. The\n-    only requisite is that the new (or overrided) class must provide a\n-    extract_links method that receives a Response and returns a list of Link objects.\n-\n-    This LinkExtractor always returns percent-encoded URLs, using the detected encoding\n-    from the response.\n-\n-    The constructor arguments are:\n-\n-    * tag (string or function)\n-      * a tag name which is used to search for links (defaults to \"a\")\n-      * a function which receives a tag name and returns whether to scan it\n-    * attr (string or function)\n-      * an attribute name which is used to search for links (defaults to \"href\")\n-      * a function which receives an attribute name and returns whether to scan it\n-    * unique - if True the same urls won't be extracted twice, otherwise the\n-      same urls will be extracted multiple times (with potentially different link texts)\n-    \"\"\"\n \n     def __init__(self, tag=\"a\", attr=\"href\", unique=False):\n         FixedSGMLParser.__init__(self)\n\n@@ -1,6 +1,6 @@\n \"\"\"\n This module provides some LinkExtractors, which extend the base LinkExtractor\n-(scrapy.link.LinkExtractor) with some addutional useful features.\n+(scrapy.link.LinkExtractor) with some additional useful features.\n \n See documentation in docs/ref/link-extractors.rst\n \"\"\"\n@@ -18,28 +18,6 @@ _matches = lambda url, regexs: any((r.search(url) for r in regexs))\n _is_valid_url = lambda url: url.split('://', 1)[0] in set(['http', 'https', 'file'])\n \n class RegexLinkExtractor(LinkExtractor):\n-    \"\"\"RegexLinkExtractor implements extends the base LinkExtractor by\n-    providing several mechanisms to extract the links.\n-\n-    It's constructor parameters are:\n-\n-    allow - list of regexes that the (absolute urls) must match to be extracted\n-    deny - ignore urls that match any of these regexes\n-    allow_domains - only extract urls from these domains\n-    deny_domains - ignore urls from these dmoains\n-    restrict_xpaths - restrict url extraction to given xpaths contents\n-    tags - look for urls in this tags\n-    attrs - look for urls in this attrs\n-    canonicalize - canonicalize all extracted urls using scrapy.utils.url.canonicalize_url\n-\n-    Both 'allow' and 'deny' arguments can be a list of regexes strings or regex\n-    python objects (already compiled)\n-\n-    Url matching is always performed against the absolute urls, never the\n-    relative urls found in pages.\n-\n-    If no allow/deny arguments are given, match all links.\n-    \"\"\"\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), \n                  tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2de03b5e2ad6e72603a54818774e764a5315e6fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 29 | Churn Cumulative: 1197 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: 0.35294117647058826\n\nDIFF:\n@@ -9,10 +9,11 @@ from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin\n \n class LinkExtractor(FixedSGMLParser):\n \n-    def __init__(self, tag=\"a\", attr=\"href\", unique=False):\n+    def __init__(self, tag=\"a\", attr=\"href\", unique=False, process_value=None):\n         FixedSGMLParser.__init__(self)\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n+        self.process_value = (lambda v: v) if process_value is None else process_value\n         self.current_link = None\n         self.unique = unique\n \n@@ -48,7 +49,9 @@ class LinkExtractor(FixedSGMLParser):\n         if self.scan_tag(tag):\n             for attr, value in attrs:\n                 if self.scan_attr(attr):\n-                    link = Link(url=value)\n+                    url = self.process_value(value)\n+                    if url is not None:\n+                        link = Link(url=url)\n                         self.links.append(link)\n                         self.current_link = link\n \n\n@@ -20,7 +20,7 @@ _is_valid_url = lambda url: url.split('://', 1)[0] in set(['http', 'https', 'fil\n class RegexLinkExtractor(LinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), \n-                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True):\n+                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None):\n         self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)]\n         self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)]\n         self.allow_domains = set(arg_to_iter(allow_domains))\n@@ -29,7 +29,8 @@ class RegexLinkExtractor(LinkExtractor):\n         self.canonicalize = canonicalize\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n-        LinkExtractor.__init__(self, tag=tag_func, attr=attr_func, unique=unique)\n+        LinkExtractor.__init__(self, tag=tag_func, attr=attr_func, \n+            unique=unique, process_value=process_value)\n \n     def extract_links(self, response):\n         if self.restrict_xpaths:\n\n@@ -1,4 +1,5 @@\n import os\n+import re\n import unittest\n \n from scrapy.http import HtmlResponse\n@@ -182,6 +183,22 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://example.org/about.html', text=u'About us\\xa3')])\n \n+    def test_process_value(self):\n+        \"\"\"Test restrict_xpaths with encodings\"\"\"\n+        html = \"\"\"\n+        <a href=\"javascript:goToPage('../other/page.html','photo','width=600,height=540,scrollbars'); return false\">Link text</a>\n+        <a href=\"/about.html\">About us</a>\n+        \"\"\"\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html, encoding='windows-1252')\n+\n+        def process_value(value):\n+            m = re.search(\"javascript:goToPage\\('(.*?)'\", value)\n+            if m:\n+                return m.group(1)\n+\n+        lx = RegexLinkExtractor(process_value=process_value)\n+        self.assertEqual(lx.extract_links(response),\n+                         [Link(url='http://example.org/other/page.html', text='Link text')])\n \n class HTMLImageLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ba69c29e37cfd2e752ae185abffb64c8a68e3796", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 17664 | Files Changed: 511 | Hunks: 203 | Methods Changed: 1154 | Complexity Δ (Sum/Max): -2184/0 | Churn Δ: 17664 | Churn Cumulative: 29890 | Contributors (this commit): 5 | Commits (past 90d): 516 | Contributors (cumulative): 736 | DMM Complexity: 0.24230607082630692\n\nDIFF:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@@ -1,18 +0,0 @@\n-\"\"\"\n-Scrapy - a screen scraping framework written in Python\n-\"\"\"\n-\n-__version__ = \"0.1.0\"\n-\n-import sys, os\n-\n-if sys.version_info < (2,5):\n-    print \"Scrapy %s requires Python 2.5 or above\" % __version__\n-    sys.exit(1)\n-\n-# add external python libraries bundled into scrapy\n-sys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), \"xlib\"))\n-\n-# monkey patches to fix external library issues\n-from scrapy.patches import monkeypatches\n-monkeypatches.apply_patches()\n\n@@ -1,39 +0,0 @@\n-#!/usr/bin/env python\n-\"\"\"Scrapy admin script is used to create new scrapy projects and similar\n-tasks\"\"\"\n-\n-import os\n-import shutil\n-from optparse import OptionParser\n-\n-import scrapy\n-\n-usage = \"\"\"\n-scrapy-admin.py [options] [command]\n- \n-Available commands:\n-     \n-    startproject <project_name>\n-      Starts a new project with name 'project_name'\n-\"\"\"\n-\n-def main():\n-    parser = OptionParser(usage=usage)\n-    opts, args = parser.parse_args()\n-    \n-    if not args:\n-        parser.print_help()\n-\n-    cmd = args[0]\n-    if cmd == \"startproject\":\n-        if len(args) >= 2:\n-            project_name = args[1]\n-            project_tplpath = os.path.join(scrapy.__path__[0], \"conf\", \"project_template\")\n-            shutil.copytree(project_tplpath, project_name)\n-        else:\n-            print \"scrapy-admin.py: missing project name\"\n-    else:\n-        print \"scrapy-admin.py: unknown command: %s\" % cmd\n-\n-if __name__ == '__main__':\n-    main()\n\n@@ -1,157 +0,0 @@\n-from __future__ import with_statement\n-\n-import sys\n-import os\n-import optparse\n-\n-import scrapy\n-from scrapy import log\n-from scrapy.spider import spiders\n-from scrapy.conf import settings\n-\n-def find_commands(dir):\n-    try:\n-        return [f[:-3] for f in os.listdir(dir) if not f.startswith('_') and f.endswith('.py')]\n-    except OSError:\n-        return []\n-\n-def builtin_commands_dict():\n-    d = {}\n-    scrapy_dir = scrapy.__path__[0]\n-    commands_dir = os.path.join(scrapy_dir, 'command', 'commands')\n-    for cmdname in find_commands(commands_dir):\n-        modname = 'scrapy.command.commands.%s' % cmdname\n-        command = getattr(__import__(modname, {}, {}, [cmdname]), 'Command', None)\n-        if callable(command):\n-            d[cmdname] = command()\n-        else:\n-            print 'WARNING: Builtin command module %s exists but Command class not found' % modname\n-    return d\n-\n-def custom_commands_dict():\n-    d = {}\n-    cmdsmod = settings['COMMANDS_MODULE']\n-    if cmdsmod:\n-        mod = __import__(cmdsmod, {}, {}, [''])\n-        for cmdname in find_commands(mod.__path__[0]):\n-            modname = '%s.%s' % (cmdsmod, cmdname)\n-            command = getattr(__import__(modname, {}, {}, [cmdname]), 'Command', None)\n-            if callable(command):\n-                d[cmdname] = command()\n-            else:\n-                print 'WARNING: Custom command module %s exists but Command class not found' % modname\n-    return d\n-\n-def getcmdname(argv):\n-    for arg in argv[1:]:\n-        if not arg.startswith('-'):\n-            return arg\n-\n-def usage(argv):\n-    s  = \"usage: %s <subcommand> [options] [args]\\n\" % argv[0]\n-    s += \"       %s <subcommand> -h\\n\\n\" % argv[0]\n-    s += \"Built-in subcommands:\\n\"\n-\n-    builtin_cmds = builtin_commands_dict()\n-    custom_cmds = custom_commands_dict()\n-\n-    filtered_builtin_cmds = [(name, cls) for name, cls in builtin_cmds.iteritems() if name not in custom_cmds]\n-\n-    for cmdname, cmdclass in filtered_builtin_cmds:\n-        s += \"  %s %s\\n\" % (cmdname, cmdclass.syntax())\n-        s += \"    %s\\n\" % cmdclass.short_desc()\n-\n-    if custom_cmds:\n-        s += \"\\n\"\n-        s += \"Custom (or overloaded) subcommands:\\n\"\n-        for cmdname, cmdclass in custom_cmds.iteritems():\n-            s += \"  %s %s\\n\" % (cmdname, cmdclass.syntax())\n-            s += \"    %s\\n\" % cmdclass.short_desc()\n-\n-    return s\n-\n-\n-def update_defaults(defaults, module):\n-    settingsdict = vars(module)\n-    for k, v in settingsdict.iteritems():\n-        if not k.startswith(\"_\"):\n-            defaults[k] = v\n-\n-def command_settings(cmdname):\n-    try:\n-        module = __import__('%s.%s' % ('scrapy.conf.commands', cmdname), {}, {}, [''])\n-        update_defaults(settings.defaults, module)\n-    except ImportError:\n-        pass\n-\n-    basepath = settings['COMMANDS_SETTINGS_MODULE']\n-    if basepath:\n-        try:\n-            module = __import__('%s.%s' % (basepath, cmdname), {}, {}, [''])\n-            update_defaults(settings.defaults, module)\n-        except ImportError:\n-            pass\n-\n-# This dict holds information about the executed command for later use\n-command_executed = {}\n-\n-def execute():\n-    execute_with_args(sys.argv)\n-\n-def execute_with_args(argv):\n-    spiders.load()\n-    cmds = builtin_commands_dict()\n-    cmds.update(custom_commands_dict())\n-\n-    cmdname = getcmdname(argv)\n-    command_settings(cmdname)\n-\n-    if not cmdname:\n-        print \"Scrapy %s\\n\" % scrapy.__version__\n-        print usage(argv)\n-        sys.exit()\n-\n-    parser = optparse.OptionParser()\n-\n-    if cmdname in cmds:\n-        cmd = cmds[cmdname]\n-        cmd.add_options(parser)\n-        parser.usage = \"%%prog %s %s\" % (cmdname, cmd.syntax())\n-        parser.description = cmd.long_desc()\n-    else:\n-        print \"Scrapy %s\\n\" % scrapy.__version__\n-        print \"Unknown command: %s\\n\" % cmdname\n-        print 'Type \"%s -h\" for help' % argv[0]\n-        sys.exit()\n-\n-    (opts, args) = parser.parse_args(args=argv[1:])\n-    del args[0]  # args[0] is cmdname\n-\n-    # storing command executed info for later reference\n-    command_executed['name'] = cmdname\n-    command_executed['class'] = cmd\n-    command_executed['args'] = args[:]\n-    command_executed['opts'] = opts.__dict__.copy()\n-\n-    cmd.process_options(args, opts)\n-    log.start() # start logging\n-    if opts.profile:\n-        log.msg(\"Profiling enabled. Analyze later with: python -m pstats %s\" % opts.profile)\n-        import cProfile\n-        loc = locals()\n-        p = cProfile.Profile()\n-        p.runctx('ret = cmd.run(args, opts)', globals(), loc)\n-        p.dump_stats(opts.profile)\n-        try:\n-            import lsprofcalltree\n-            fn = opts.profile + \".cachegrind\"\n-            k = lsprofcalltree.KCacheGrind(p)\n-            with open(fn, 'w') as f:\n-                k.output(f)\n-        except ImportError:\n-            pass\n-        ret = loc['ret']\n-    else:\n-        ret = cmd.run(args, opts)\n-    if ret is False:\n-        parser.print_help()\n\n@@ -1,40 +0,0 @@\n-from scrapy.command import ScrapyCommand\n-from scrapy.core.manager import scrapymanager\n-from scrapy.replay import Replay\n-from scrapy.conf import settings\n-\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options] [domain|url] ...\"\n-\n-    def short_desc(self):\n-        return \"Run the web scraping engine from the command line\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--nocache\", dest=\"nocache\", action=\"store_true\", help=\"disable HTTP cache\")\n-        parser.add_option(\"--nopipeline\", dest=\"nopipeline\", action=\"store_true\", help=\"disable scraped item pipeline\")\n-        parser.add_option(\"--restrict\", dest=\"restrict\", action=\"store_true\", help=\"restrict crawling only to the given urls\")\n-        parser.add_option(\"--record\", dest=\"record\", help=\"use FILE for recording session (see replay command)\", metavar=\"FILE\")\n-        parser.add_option(\"--record-dir\", dest=\"recorddir\", help=\"use DIR for recording (instead of file)\", metavar=\"DIR\")\n-\n-    def process_options(self, args, opts):\n-        ScrapyCommand.process_options(self, args, opts)\n-        if opts.nopipeline:\n-            settings.overrides['ITEM_PIPELINES'] = []\n-\n-        if opts.nocache:\n-            settings.overrides['CACHE2_DIR'] = None\n-\n-        if opts.restrict:\n-            settings.overrides['RESTRICT_TO_URLS'] = args\n-\n-        if opts.record or opts.recorddir:\n-            # self.replay is used for preventing Replay signals handler from\n-            # disconnecting since pydispatcher uses weak references \n-            self.replay = Replay(opts.record or opts.recorddir, mode='record', usedir=bool(opts.recorddir))\n-            self.replay.record(args=args, opts=opts.__dict__)\n-\n-    def run(self, args, opts):\n-        scrapymanager.runonce(*args, **opts.__dict__)\n\n@@ -1,64 +0,0 @@\n-import os\n-import string\n-\n-from scrapy.spider import spiders\n-from scrapy.command import ScrapyCommand\n-from scrapy.conf import settings\n-\n-class Command(ScrapyCommand):\n-\n-    \"\"\" Default template file name \"\"\"\n-    template_name = 'spider.tmpl'\n-\n-    \"\"\" Childs can define custom tvars \"\"\"\n-    custom_tvars = {}\n-\n-    def syntax(self):\n-        return \"<spider_name> <spider_domain_name>\"\n-\n-    def short_desc(self):\n-        return \"Generate new spider based on predefined template\"\n-\n-    def run(self, args, opts):\n-        if len(args) != 2:\n-            return False\n-\n-        name = args[0]\n-        site = args[1]\n-        spiders_dict = spiders.asdict()\n-        if not name in spiders_dict.keys():\n-            self._genspider(name, site)\n-        else:\n-            print \"Spider '%s' exist\" % name\n-\n-    def _genspider(self, name, site):\n-        \"\"\" Generate spider \"\"\"\n-        tvars = {\n-            'name': name,\n-            'site': site,\n-            'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('-')])\n-        }\n-        tvars.update(self.custom_tvars)\n-\n-        spiders_module = __import__(settings['NEWSPIDER_MODULE'], {}, {}, [''])\n-        spidersdir = os.path.abspath(os.path.dirname(spiders_module.__file__))\n-        if name[0] not in string.letters: # must start with a letter, for valid python modules\n-            name = \"a\" + name\n-        name = name.replace('-', '_') # - are replaced by _, for valid python modules\n-        self._genfiles(self.template_name, '%s/%s.py' % (spidersdir, name), tvars)\n-\n-    def _genfiles(self, template_name, source_name, tvars):\n-        \"\"\" Generate source from template, substitute variables \"\"\"\n-        template_file = os.path.join(settings['TEMPLATES_DIR'], template_name)\n-        tmpl = open(template_file)\n-        clines = []\n-        for l in tmpl.readlines():\n-            for key, val in tvars.items():\n-                l = l.replace('@%s@' % key, val)\n-            clines.append(l)\n-        tmpl.close()\n-        source = ''.join(clines)\n-        if not os.path.exists(source_name):\n-            sfile = open(source_name, \"w\")\n-            sfile.write(source)\n-            sfile.close()\n\n@@ -1,25 +0,0 @@\n-from scrapy.command import ScrapyCommand\n-from scrapy.fetcher import fetch\n-from scrapy.spider import spiders\n-from scrapy.item import ScrapedItem\n-\n-def get_item_attr(pagedata, attr=\"guid\"):\n-    spider = spiders.fromurl(pagedata.url)\n-    items = spider.parse(pagedata)\n-    attrs = [getattr(i, attr) for i in items if isinstance(i, ScrapedItem)]\n-    return attrs\n-\n-def get_attr(url, attr=\"guid\"):\n-    pagedatas = fetch([url])\n-    if pagedatas:\n-        return get_item_attr(pagedatas[0], attr)\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"<url> <attribute>\"\n-\n-    def short_desc(self):\n-        return \"Print an attribute from the item scraped in the given URL\"\n-\n-    def run(self, args, opts):\n-        print get_attr(args[0], args[1])\n\n@@ -1,46 +0,0 @@\n-from scrapy.command import ScrapyCommand\n-from scrapy.fetcher import fetch\n-from scrapy.http import Request\n-from scrapy.item import ScrapedItem\n-from scrapy.spider import spiders\n-from scrapy.utils import display\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options] <url>\"\n-\n-    def short_desc(self):\n-        return \"Parse the URL and print their results\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", help=\"don't show extracted links\")\n-        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n-        parser.add_option(\"--identify\", dest=\"identify\", action=\"store_true\", help=\"try to use identify instead of parse\")\n-        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n-\n-    def pipeline_process(self, item, opts):\n-        return item\n-\n-    def run(self, args, opts):\n-        if not args:\n-            print \"A URL is required\"\n-            return\n-\n-        responses = fetch([args[0]])\n-        if responses:\n-            response = responses[0]\n-            spider = spiders.fromurl(response.url)\n-            result = spider.parse(response) if not opts.identify else spider.identify(response)\n-\n-            items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n-            links = [i for i in result if isinstance(i, Request)]\n-\n-            display.nocolour = opts.nocolour\n-            if not opts.noitems:\n-                print \"# Scraped Items\", \"-\"*60\n-                display.pprint(items)\n-\n-            if not opts.nolinks:\n-                print \"# Links\", \"-\"*68\n-                display.pprint(links)\n\n@@ -1,184 +0,0 @@\n-import os\n-\n-from scrapy.command import ScrapyCommand\n-from scrapy.replay import Replay\n-from scrapy.utils import display\n-from scrapy.conf import settings\n-from scrapy.command import cmdline\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options] <replay_file> [action]\"\n-\n-    def short_desc(self):\n-        return \"Replay a session previously recorded with crawl --record\"\n-\n-    def help(self):\n-        s = \"Replay a session previously recorded with crawl --record\\n\"\n-        s += \"\\n\"\n-        s += \"Available actions:\\n\"\n-        s += \"  crawl: just replay the crawl (default if action omitted)\\n\"\n-        s += \"  diff: replay the crawl and show differences in items scraped/passed\\n\"\n-        s += \"  update: replay the crawl and update both scraped and passed items\\n\"\n-        s += \"  showitems: show items stored\\n\"\n-        s += \"  showpages: show all responses downloaded (not only HTML pages)\\n\"\n-        return s\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\", help=\"show verbose output (full items/responses)\")\n-        parser.add_option(\"-t\", \"--item-type\", dest=\"itype\", help=\"item type (scraped, passed). default: scraped\", metavar=\"TYPE\")\n-        parser.add_option(\"--output\", dest=\"outfile\", help=\"write output to FILE. if omitted uses stdout\", metavar=\"FILE\")\n-        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"disable colorized output (for console only)\")\n-        parser.add_option(\"-i\", \"--ignore\", dest=\"ignores\", action=\"append\", help=\"item attribute to ignore. can be passed multiple times\", metavar=\"ATTR\")\n-        parser.add_option(\"--target\", dest=\"targets\", action=\"append\", help=\"crawl TARGET instead of recorded urls/domains. can be passed multiple times\")\n-        # adding option to update \n-        parser.add_option(\"--pages\", dest=\"pages\", action=\"store_true\", help=\"update all the pages in the replay file, recording it again.\")\n-        parser.add_option(\"-q\", \"--quiet\", dest=\"quiet\", action=\"store_true\", help=\"no verbose mode in option diff.\")\n-\n-    def process_options(self, args, opts):\n-        if args:\n-            ScrapyCommand.process_options(self, args, opts)\n-            self.opts = opts\n-            self.action = args[1] if len(args) > 1 else 'crawl'\n-            mode = 'update' if self.action == 'update' else 'play'\n-            usedir = args and os.path.isdir(args[0])\n-            self.replay = Replay(args[0], mode=mode, usedir=usedir)\n-            if self.action not in ['crawl', 'diff', 'update']:\n-                settings.overrides['LOG_ENABLED'] = False\n-\n-    def run(self, args, opts):\n-        if not args:\n-            print \"A <replay_dir> is required\"\n-            return\n-\n-        display.nocolour = opts.nocolour\n-\n-        if opts.itype == 'passed':\n-            self.before_db = self.replay.passed_old\n-            self.now_db = self.replay.passed_new\n-        else: # default is 'scraped'\n-            opts.itype = 'scraped'\n-            self.before_db = self.replay.scraped_old\n-            self.now_db = self.replay.scraped_new\n-\n-        actionfunc = getattr(self, 'action_%s' % self.action, None)\n-        if actionfunc:\n-            rep = actionfunc(opts)\n-            if rep:\n-                if opts.outfile:\n-                    f = open(opts.outfile, \"w\")\n-                    f.write(rep)\n-                    f.close()\n-                else:\n-                    print rep,\n-            self.replay.cleanup()\n-        else:\n-            print \"Unknown replay action: %s\" % self.action\n-\n-    def action_crawl(self, opts):\n-        self.replay.play(args=opts.targets)\n-\n-    def action_update(self, opts):\n-        self.replay.update(args=opts.targets, opts=opts.__dict__)\n-        if (opts.pages):\n-            args = ['scrapy-crawl', 'crawl']\n-            args.extend(self.replay.options['args'])\n-            for k in self.replay.options['opts']:                \n-                if self.replay.options['opts'][k]:                   \n-                    args.append(\"--%s\" % k)\n-                    if self.replay.options['opts'][k] != True:\n-                        args.append(self.replay.options['opts'][k])\n-            cmdline.execute_with_args(args)\n-\n-    def action_showitems(self, opts):\n-        s = \"\"\n-        s += self._format_items(self.before_db.values())\n-        s += \">>> Total: %d items %s\\n\" % (len(self.before_db), opts.itype)\n-        return s\n-\n-    def action_showpages(self, opts):\n-        s = \"\"\n-        if self.opts.verbose:\n-            for r in self.replay.responses_old.values():\n-                s += \">>> %s\\n\" % str(r)\n-                s += display.pformat(r)\n-        else:\n-            s += \"\\n\".join([str(r) for r in self.replay.responses_old.values()]) + \"\\n\"\n-        s += \">>> Total: %d responses received\\n\" % len(self.replay.responses_old)\n-        return s\n-\n-    def action_diff(self, opts):\n-        self.action_crawl(opts)\n-\n-        guids_before = set(self.before_db.keys())\n-        guids_now = set(map(str, self.now_db.keys()))\n-\n-        guids_new = guids_now - guids_before\n-        guids_missing = guids_before - guids_now\n-        guids_both = guids_now & guids_before\n-\n-        changed_items, chreport = self._report_differences(self.before_db, self.now_db, guids_both)\n-\n-        ok_items = len(guids_both) - changed_items\n-        new_items = len(guids_new)\n-        missing_items = len(guids_missing)\n-\n-        if (new_items - missing_items - changed_items) == 0 and opts.quiet:\n-            s = \"\"\n-        else:\n-            s = \"CRAWLING DIFFERENCES REPORT\\n\\n\"\n-\n-            s += \"Total items     : %d\\n\" % (len(guids_both) + new_items + missing_items)\n-            s += \"  Items OK      : %d\\n\" % ok_items\n-            s += \"  New items     : %d\\n\" % new_items\n-            s += \"  Missing items : %d\\n\" % missing_items\n-            s += \"  Changed items : %d\\n\" % changed_items\n-\n-            s += \"\\n\"\n-            s += \"- NEW ITEMS (%d) -----------------------------------------\\n\" % new_items\n-            s += self._format_items([self.now_db[g] for g in guids_new])\n-\n-            s += \"\\n\"\n-            s += \"- MISSING ITEMS (%d) -------------------------------------\\n\" % missing_items\n-            s += self._format_items([self.before_db[g] for g in guids_missing])\n-\n-            s += \"\\n\"\n-            s += \"- CHANGED ITEMS (%d) -------------------------------------\\n\" % changed_items\n-\n-            s += chreport\n-\n-        return s\n-\n-    def _report_differences(self, old_items, new_items, guids):\n-        items_old = [old_items[g] for g in guids]\n-        items_new = [new_items[g] for g in guids]\n-\n-        c = 0\n-        s = \"\"\n-        for old, new in zip(items_old, items_new):\n-            d = self._item_diff(old, new)\n-            if d:\n-                c += 1\n-                s += d\n-        return c, s\n-\n-    def _item_diff(self, old, new):\n-        delta = new - old\n-\n-        s = \"\"\n-        if delta.diff:\n-            s += \">>> Item guid=%s name=%s\\n\" % (old.guid, old.name)\n-            s += display.pformat(delta.diff) + \"\\n\"\n-        return s\n-\n-    def _format_items(self, items):\n-        if self.opts.verbose:\n-            s = display.pformat(items)\n-        else:\n-            s = \"\"\n-            for i in items:\n-                s += \"%s\\n\" % str(i)\n-                s += \"  <%s>\\n\" % i.url\n-        return s\n-\n\n@@ -1,158 +0,0 @@\n-from twisted.internet import reactor\n-\n-import scrapy\n-from scrapy.command import ScrapyCommand\n-from scrapy.spider import spiders\n-from scrapy.xpath import XmlXPathSelector, HtmlXPathSelector\n-from scrapy.utils.misc import load_class\n-from scrapy.extension import extensions\n-from scrapy.conf import settings\n-from scrapy.core.manager import scrapymanager\n-from scrapy.http import Request, Response\n-from scrapy.core.downloader.handlers import download_any\n-from scrapy.fetcher import get_or_create_spider\n-from scrapy.utils.decompressor import Decompressor\n-\n-#This code comes from twisted 8. We define here while\n-#using old twisted version.\n-def blockingCallFromThread(reactor, f, *a, **kw):\n-    \"\"\"\n-    Run a function in the reactor from a thread, and wait for the result\n-    synchronously, i.e. until the callback chain returned by the function\n-    get a result.\n-\n-    @param reactor: The L{IReactorThreads} provider which will be used to\n-        schedule the function call.\n-    @param f: the callable to run in the reactor thread\n-    @type f: any callable.\n-    @param a: the arguments to pass to C{f}.\n-    @param kw: the keyword arguments to pass to C{f}.\n-\n-    @return: the result of the callback chain.\n-    @raise: any error raised during the callback chain.\n-    \"\"\"\n-    import Queue\n-    from twisted.python import failure\n-    from twisted.internet import defer\n-    queue = Queue.Queue()\n-    def _callFromThread():\n-        result = defer.maybeDeferred(f, *a, **kw)\n-        result.addBoth(queue.put)\n-    reactor.callFromThread(_callFromThread)\n-    result = queue.get()\n-    if isinstance(result, failure.Failure):\n-        result.raiseException()\n-    return result\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[url]\"\n-\n-    def short_desc(self):\n-        return \"Interactive scraping console\"\n-\n-    def long_desc(self):\n-        return \"Interactive console for scraping the given url. For scraping local files you can use a URL like file://path/to/file.html\"\n-\n-    def update_vars(self):\n-        \"\"\" You can use this function to update the local variables that will be available in the scrape console \"\"\"\n-        pass\n-\n-    def get_url(self, url, decompress=False):\n-\n-        print \"Downloading URL...           \",\n-        r = Request(url)\n-        spider = get_or_create_spider(url)\n-        try:\n-            result = blockingCallFromThread(reactor, download_any, r, spider)\n-            if isinstance(result, Response):\n-                print \"Done.\"\n-                if decompress:\n-                    print \"Decompressing response...\",\n-                    d = Decompressor()\n-                    result = d.extract(result)\n-                    print \"Done.\"\n-                result.request = r\n-                self.generate_vars(url, result)\n-                return True\n-        except Exception, e:\n-            print \"Error: %s\" % e\n-\n-    def generate_vars(self, url, response):\n-        itemcls = load_class(settings['DEFAULT_ITEM_CLASS'])\n-        item = itemcls()\n-        self.vars['item'] = item\n-        if url:\n-            self.vars['xxs'] = XmlXPathSelector(response)\n-            self.vars['hxs'] = HtmlXPathSelector(response)\n-            self.vars['url'] = url\n-            self.vars['response'] = response\n-            self.vars['spider'] = spiders.fromurl(url)\n-        self.update_vars()\n-        self.user_ns.update(self.vars)\n-        self.print_vars()\n-        \n-    def print_vars(self):\n-        print '-' * 78\n-        print \"Available local variables:\"\n-        for key, val in self.vars.iteritems():\n-            if isinstance(val, basestring):\n-                print \"   %s: %s\" % (key, val)\n-            else:\n-                print \"   %s: %s\" % (key, val.__class__)\n-        print \"Available commands:\"\n-        print \"   get <url>: Fetches an url and updates all variables.\"\n-        print \"   getd <url>: Similar to get, but filter with decompress.\"\n-        print \"   scrapehelp: Prints this help.\"\n-        print '-' * 78\n-    \n-    def run(self, args, opts):\n-        self.vars = {}\n-        self.user_ns = {}\n-        url = None\n-        if args:\n-            url = args[0]\n-\n-        print \"Scrapy %s - Interactive scraping console\\n\" % scrapy.__version__\n-\n-        print \"Enabling Scrapy extensions...\",\n-        extensions.load()\n-        print \"done\"\n-\n-        def _console_thread():\n-            \n-            def _get_magic(shell, arg):\n-                self.get_url(arg.strip())\n-            def _help_magic(shell, _):\n-                self.print_vars()\n-            def _getd_magic(shell, arg):\n-                self.get_url(arg.strip(), decompress=True)\n-                \n-            if url:\n-                result = self.get_url(url)\n-                if not result:\n-                    self.generate_vars(None, None)\n-            else:\n-                self.generate_vars(None, None)\n-            try: # use IPython if available\n-                import IPython\n-                shell = IPython.Shell.IPShell(argv=[], user_ns=self.user_ns)\n-                ip = shell.IP.getapi()\n-                ip.expose_magic(\"get\", _get_magic)\n-                ip.expose_magic(\"getd\", _getd_magic)\n-                ip.expose_magic(\"scrapehelp\", _help_magic)\n-                shell.mainloop()\n-                reactor.callFromThread(scrapymanager.stop)\n-            except ImportError:\n-                import code\n-                try: # readline module is only available on unix systems\n-                    import readline\n-                except ImportError:\n-                    pass\n-                else:\n-                    import rlcompleter\n-                    readline.parse_and_bind(\"tab:complete\")\n-                code.interact(local=self.vars)\n-\n-        reactor.callInThread(_console_thread)\n-        scrapymanager.start()\n\\ No newline at end of file\n\n@@ -1,30 +0,0 @@\n-import pprint\n-from scrapy.command import ScrapyCommand\n-from scrapy.conf import settings\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"<domain> [domain ...]\"\n-\n-    def short_desc(self):\n-        return \"Show all stats history stored for the given domain(s)\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-p\", \"--path\", dest=\"path\", help=\"restrict stats to PATH\", metavar=\"PATH\")\n-\n-    def run(self, args, opts):\n-        if not args:\n-            print \"A domain is required\"\n-            return\n-        if not settings['SCRAPING_DB']:\n-            print \"SCRAPING_DB setting is required for this command\"\n-            return\n-\n-        from scrapy.store.db import DomainDataHistory\n-        ddh = DomainDataHistory(settings['SCRAPING_DB'], 'domain_data_history')\n-\n-        for domain in args:\n-            print \"# %s\" % domain\n-            pprint.pprint(list(ddh.getall(domain, opts.path)))\n-\n\n@@ -1,78 +0,0 @@\n-\"\"\"\n-Base class for Scrapy commands\n-\"\"\"\n-import os\n-from scrapy.conf import settings\n-\n-class ScrapyCommand(object):\n-    def syntax(self):\n-        \"\"\"\n-        Command syntax (preferably one-line). Do not include command name.\n-        \"\"\"\n-        return \"\"\n-\n-    def short_desc(self):\n-        \"\"\"\n-        A short description of the command\n-        \"\"\"\n-        return \"\"\n-\n-    def long_desc(self):\n-        \"\"\"\n-        A long description of the command. Return short description when not\n-        available. It cannot contain newlines, since contents will be formatted\n-        by optparser which removes newlines and wraps text.\n-        \"\"\"\n-        return self.short_desc()\n-\n-    def help(self):\n-        \"\"\"\n-        An extensive help for the command. It will be shown when using the\n-        \"help\" command. It can contain newlines, since not post-formatting will\n-        be applied to its contents.\n-        \"\"\"\n-        return self.long_desc()\n-\n-    def add_options(self, parser):\n-        \"\"\"\n-        Populate option parse with options available for this command\n-        \"\"\"\n-        parser.add_option(\"-f\", \"--logfile\", dest=\"logfile\", help=\"logfile to use. if omitted stderr will be used\", metavar=\"FILE\")\n-        parser.add_option(\"-o\", \"--loglevel\", dest=\"loglevel\", default=None, help=\"log level\")\n-        parser.add_option(\"--default-spider\", dest=\"default_spider\", default=None, help=\"default spider (domain) to use if no spider is found\")\n-        parser.add_option(\"--spider\", dest=\"spider\", default=None, help=\"Force using the given spider when the arguments are urls\")\n-        parser.add_option(\"--nolog\", dest=\"nolog\", action=\"store_true\", help=\"disable all log messages\")\n-        parser.add_option(\"--profile\", dest=\"profile\", default=None, help=\"write profiling stats in FILE, to analyze later with: python -m pstats FILE\", metavar=\"FILE\")\n-        parser.add_option(\"--pidfile\", dest=\"pidfile\", help=\"Write process pid to file FILE\", metavar=\"FILE\")\n-        \n-    def process_options(self, args, opts):\n-        if opts.logfile:\n-            settings.overrides['LOG_ENABLED'] = True\n-            settings.overrides['LOGFILE'] = opts.logfile\n-\n-        if opts.loglevel:\n-            settings.overrides['LOG_ENABLED'] = True\n-            settings.overrides['LOGLEVEL'] = opts.loglevel\n-\n-        if opts.nolog:\n-            settings.overrides['LOG_ENABLED'] = False\n-\n-        if opts.default_spider:\n-            from scrapy.spider import spiders\n-            spiders.default_domain = opts.default_spider\n-            \n-        if opts.spider:\n-            from scrapy.spider import spiders\n-            spiders.force_domain = opts.spider\n-\n-        if opts.pidfile:\n-            pid = os.getpid()\n-            open(opts.pidfile, \"w\").write(str(pid))\n-\n-    def run(self, args, opts):\n-        \"\"\"\n-        Entry point for running commands\n-        \"\"\"\n-        raise NotImplementedError\n-\n-\n\n@@ -1,67 +0,0 @@\n-import os\n-import cPickle as pickle\n-\n-SETTINGS_MODULE = os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings')\n-\n-class Settings(object):\n-    \"\"\"Class to obtain configuration values from settings module\n-       which can be overriden by environment variables prepended by SCRAPY_\"\"\"\n-\n-    # settings in precedence order\n-    overrides = None\n-    settings = None\n-    defaults = None\n-    core = None\n-\n-    def __init__(self):\n-        pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n-        self.overrides = pickle.loads(pickled_settings) if pickled_settings else {}\n-        self.settings = self._import(SETTINGS_MODULE)\n-        self.defaults = {}\n-        self.core = self._import('scrapy.conf.core_settings')\n-\n-    def _import(self, modulepath):\n-        return __import__(modulepath, {}, {}, [''])\n-\n-    def __getitem__(self, opt_name):\n-        if opt_name in self.overrides:\n-            return self.overrides[opt_name]\n-\n-        if 'SCRAPY_' + opt_name in os.environ:\n-            return os.environ['SCRAPY_' + opt_name]\n-\n-        if hasattr(self.settings, opt_name):\n-            return getattr(self.settings, opt_name)\n-\n-        if opt_name in self.defaults:\n-            return self.defaults[opt_name]\n-\n-        if hasattr(self.core, opt_name):\n-            return getattr(self.core, opt_name)\n-\n-    def get(self, name, default=None):\n-        return self[name] if self[name] is not None else default\n-\n-    def getbool(self, name, default=False):\n-        \"\"\"\n-        True is: 1, '1', True\n-        False is: 0, '0', False, None\n-        \"\"\"\n-        return bool(int(self.get(name, default)))\n-\n-    def getint(self, name, default=0):\n-        return int(self.get(name, default))\n-\n-    def getfloat(self, name, default=0.0):\n-        return float(self.get(name, default))\n-\n-    def getlist(self, name, default=None):\n-        value = self.get(name)\n-        if value is None:\n-            return []\n-        elif hasattr(value, '__iter__'):\n-            return value\n-        else:\n-            return str(value).split(',')\n-\n-settings = Settings()\n\n@@ -1,30 +0,0 @@\n-import scrapy\n-# Scrapy core settings\n-\n-BOT_NAME = 'scrapy'\n-BOT_VERSION = scrapy.__version__\n-\n-ENGINE_DEBUG = False\n-\n-# Download configuration options\n-USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n-DOWNLOAD_TIMEOUT = 180      # 3mins\n-CONCURRENT_DOMAINS = 8    # number of domains to scrape in parallel\n-REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n-CACHE2_EXPIRATION_SECS = 48 * 60 * 60 # seconds while cached response is still valid (a negative value means \"never expires\")\n-\n-LOG_ENABLED = True  #\n-LOGLEVEL = 'DEBUG'   # default loglevel\n-LOGFILE = None      # None means sys.stderr by default\n-LOG_STDOUT = False   #\n-\n-DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n-SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n-MEMORYSTORE = 'scrapy.core.scheduler.MemoryStore'\n-PRIORITIZER = 'scrapy.core.prioritizers.RandomPrioritizer'\n-\n-EXTENSIONS = []\n-\n-# contrib.middleware.retry.RetryMiddleware default settings\n-RETRY_TIMES = 3\n-RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']\n\n@@ -1,6 +0,0 @@\n-# Define here the models for your scraped items\n-\n-from scrapy.item import ScrapedItem\n-\n-class MyItem(ScrapedItem):\n-    pass\n\n@@ -1,4 +0,0 @@\n-#!/usr/bin/env python\n-\n-from scrapy.command.cmdline import execute\n-execute()\n\n@@ -1,95 +0,0 @@\n-import $project_name\n-\n-# ---------------------------------------------------------------------------\n-# - Scrapy settings for $project_name                                    -\n-# ---------------------------------------------------------------------------\n-\n-BOT_NAME = 'scrapybot'\n-BOT_VERSION = '1.0'\n-\n-SPIDER_MODULES = ['$project_name.spiders']\n-NEWSPIDER_MODULE = '$project_name.spiders'\n-TEMPLATES_DIR = '%s/templates' % $project_name.__path__[0]\n-ENABLED_SPIDERS_FILE = '%s/conf/enabled_spiders.list' % $project_name.__path__[0]\n-DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n-USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n-DOWNLOAD_TIMEOUT = 600\n-\n-# uncomment if you want to add your own custom scrapy commands\n-#COMMANDS_MODULE = '$project_name.commands'\n-#COMMANDS_SETTINGS_MODULE = '$project_name.conf.commands'\n-\n-#Global timeout between sucessive downloads (can be overrided by spider\n-#attribute download_timeout\n-#DOWNLOAD_TIMEOUT = 0\n-\n-MYSQL_CONNECTION_SETTINGS = {\"charset\": \"utf8\" }\n-MYSQL_CONNECTION_PING_PERIOD = 600\n-\n-SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n-SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n-\n-#CACHE2_DIR = '/tmp/cache2'  # if set, enables HTTP cache\n-#CACHE2_IGNORE_MISSING = 0   # ignore requests not in cache\n-#CACHE2_SECTORIZE = 1         # sectorize domains to distribute storage among servers\n-\n-#STATS_ENABLED = 1   # enable stats\n-#STATS_CLEANUP = 0   # cleanup domain stats when a domain is closed (saves memory)\n-#STATS_DEBUG = 0     # log stats on domain closed\n-\n-EXTENSIONS = (\n-    'scrapy.management.web.WebConsole',\n-    'scrapy.management.telnet.TelnetConsole',\n-)\n-\n-DOWNLOADER_MIDDLEWARES = (\n-    # Engine side\n-    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n-    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n-    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n-    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n-    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n-    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n-    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n-    'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n-    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n-    # Downloader side\n-)\n-\n-SPIDER_MIDDLEWARES = (\n-    # Engine side\n-    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',\n-    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware',\n-    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware',\n-    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware',\n-    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n-    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n-    'scrapy.contrib.spidermiddleware.urlfilter.UrlFilterMiddleware',\n-    # Spider side\n-)\n-\n-# Item pipelines are usually configured by commands (see conf/commands)\n-#ITEM_PIPELINES = (\n-#)\n-\n-#DEPTH_LIMIT = 10  # limit the maximum link depth to follow\n-#DEPTH_STATS = 1    # enable depth stats\n-\n-# Limit URL length. See: http://www.boutell.com/newfaq/misc/urllength.html\n-URLLENGTH_LIMIT = 2083\n-\n-#WEBCONSOLE_ENABLED = 1\n-#WEBCONSOLE_PORT = 8060  # if not set uses a dynamic port\n-\n-#TELNETCONSOLE_ENABLED = 1\n-#TELNETCONSOLE_PORT = 2020  # if not set uses a dynamic port\n-\n-# global mail sending settings\n-#MAIL_HOST = 'localhost'\n-#MAIL_FROM = 'scrapybot@localhost'\n-\n-# scrapy webservice\n-WS_ENABLED = 0\n-\n-SPIDERPROFILER_ENABLED = 0\n\n@@ -1,85 +0,0 @@\n-from scrapy.item.adaptors import AdaptorPipe as BaseAdaptorPipe\n-from scrapy.utils.serialization import serialize\n-\n-class DuplicatedAdaptorName(Exception): pass\n-\n-class _Adaptor(object):\n-    \"\"\"\n-    Adaptors instances should be instantiated and used only\n-    inside the AdaptorPipe.\n-    \"\"\"\n-    def __init__(self, function, match_function):\n-        self.basefunction = function\n-        self.match_function = match_function\n-    def __repr__(self):\n-        return self.basefunction.func_name\n-    def __call__(self, *args):\n-        return self.basefunction(*args)\n-        \n-class AdaptorPipe(BaseAdaptorPipe):\n-\n-    def __init__(self, attribute_names, adaptors=None):\n-        \"\"\"\n-        If \"adaptors\" is given, constructs pipeline from this.\n-        \"adaptors\" is an ordered tuple of 2-elements tuples, each of which\n-        has the same parameters you give to the insertadaptor method, except \n-        'after' and 'before', because you define the adaptors order in the tuple.\n-        Example:\n-        (\n-          (my_function, lambda x: x in my_list)\n-          ...\n-        )\n-        \"\"\"\n-        self.__attribute_names = [ n for n in attribute_names ]\n-        self.__adaptorspipe = []\n-        self.pipes = {}\n-        if adaptors:\n-            for entry in adaptors:\n-                self.insertadaptor(compile_pipe=False, *entry)\n-            self._compile_pipe()\n-\n-    @property\n-    def adaptors_names(self):\n-        _adaptors = []\n-        for a in self.__adaptorspipe:\n-            _adaptors.append(a.basefunction.func_name)\n-        return _adaptors\n-    \n-    def insertadaptor(self, function, match_function=lambda x: True, compile_pipe=True, after=None, before=None):\n-        \"\"\"\n-        Inserts a \"function\" as an adaptor that will apply when match_function returns True (by\n-        default always apply)\n-        If \"after\" is given, inserts the adaptor after the already inserted adaptor\n-        of the name given in this parameter, If \"before\" is given, inserts it before\n-        the adaptor of the given name. \"name\" is the name of the adaptor.\n-        \"\"\"\n-        if function.func_name in self.adaptors_names:\n-            raise DuplicatedAdaptorName(function.func_name)\n-        else:\n-            adaptor = _Adaptor(function, match_function)\n-            #by default append adaptor at end of pipe\n-            pos = len(self.adaptors_names)\n-            if after:\n-                pos = self.adaptors_names.index(after) + 1\n-            elif before:\n-                pos = self.adaptors_names.index(before)\n-            self.__adaptorspipe.insert(pos, adaptor)\n-            if compile_pipe:\n-                self._compile_pipe()\n-            return pos\n-\n-    def removeadaptor(self, adaptorname):\n-        pos = self.adaptors_names.index(adaptorname)\n-        self.__adaptorspipe.pop(pos)\n-        self._compile_pipe()\n-\n-    def _compile_pipe(self):\n-        for attrname in self.__attribute_names:\n-            adaptors_pipe = []\n-            for adaptor in self.__adaptorspipe:\n-                if adaptor.match_function(attrname):\n-                    adaptors_pipe.append(adaptor)\n-            self.pipes[attrname] = adaptors_pipe\n-\n-    def __repr__(self):\n-        return serialize(self.pipes, \"pprint\")\n\\ No newline at end of file\n\n@@ -1,46 +0,0 @@\n-\"\"\"\n-CloseDomain is an extension that forces spiders to be closed after a given\n-time has expired.\n-\n-\"\"\"\n-import datetime\n-import pprint\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.mail import MailSender\n-from scrapy.stats import stats\n-from scrapy.conf import settings\n-\n-class CloseDomain(object):\n-    def __init__(self):\n-        self.timeout = settings.getint('CLOSEDOMAIN_TIMEOUT')\n-        if not self.timeout:\n-            raise NotConfigured\n-\n-        self.tasks = {}\n-        self.mail = MailSender()\n-        self.notify = settings.getlist('CLOSEDOMAIN_NOTIFY')\n-\n-        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n-\n-    def domain_opened(self, domain):\n-        self.tasks[domain] = scrapyengine.addtask(self.close_domain, self.timeout, args=[domain])\n-        \n-    def close_domain(self, domain): \n-        log.msg(\"Domain was opened for more than %d seconds, closing it...\" % self.timeout, domain=domain)\n-        scrapyengine.close_domain(domain)\n-        if self.notify:\n-            body = \"Closed domain %s because it remained opened for more than %s\\n\\n\" % (domain, datetime.timedelta(seconds=self.timeout))\n-            body += \"DOMAIN STATS ------------------------------------------------------\\n\\n\"\n-            body += pprint.pformat(stats.get(domain, None))\n-            subj = \"Closed domain by timeout: %s\" % domain\n-            self.mail.send(self.notify, subj, body)\n-\n-    def domain_closed(self, domain):\n-        scrapyengine.removetask(self.tasks[domain])\n\n@@ -1,17 +0,0 @@\n-\"\"\"\n-Extensions for debugging Scrapy \n-\"\"\"\n-import signal\n-import traceback\n-\n-class StackTraceDebug(object):\n-    def __init__(self):\n-        try:\n-            signal.signal(signal.SIGUSR1, self.dump_stacktrace)\n-        except AttributeError:\n-            # win32 platforms don't support SIGUSR signals\n-            pass\n-\n-    def dump_stacktrace(self, signum, frame):\n-        print \"Got signal. Dumping stack trace...\"\n-        traceback.print_stack(frame)\n\n@@ -1,194 +0,0 @@\n-from __future__ import with_statement\n-\n-import os\n-import sys\n-import hashlib\n-import datetime\n-import urlparse\n-import cPickle as pickle\n-from pydispatch import dispatcher\n-from twisted.internet import defer\n-\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.core.engine import scrapyengine\n-from scrapy.http import Response, Headers\n-from scrapy.http.headers import headers_dict_to_raw\n-from scrapy.core.exceptions import UsageError, NotConfigured, HttpException, IgnoreRequest\n-from scrapy.conf import settings\n-\n-\n-class CacheMiddleware(object):\n-    def __init__(self):\n-        if not settings['CACHE2_DIR']:\n-            raise NotConfigured\n-        self.cache = Cache(settings['CACHE2_DIR'], sectorize=settings.getbool('CACHE2_SECTORIZE'))\n-        self.ignore_missing = settings.getbool('CACHE2_IGNORE_MISSING')\n-        dispatcher.connect(self.open_domain, signal=signals.domain_open)\n-\n-    def open_domain(self, domain):\n-        self.cache.open_domain(domain)\n-\n-    def process_request(self, request, spider):\n-        if not is_cacheable(request):\n-            return\n-\n-        key = request.fingerprint()\n-        domain = spider.domain_name\n-        try:\n-            response = self.cache.retrieve_response(domain, key)\n-        except:\n-            log.msg(\"Corrupt cache for %s\" % request.url, log.WARNING)\n-            response = False\n-        if response:\n-            response.cached = True\n-            if not 200 <= int(response.status) < 300:\n-                raise HttpException(response.status, None, response)\n-            return response\n-        elif self.ignore_missing:\n-            raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n-\n-    def process_response(self, request, response, spider):\n-        if not is_cacheable(request):\n-            return response\n-\n-        if isinstance(response, Response) and not response.cached:\n-            key = request.fingerprint()\n-            domain = spider.domain_name\n-            self.cache.store(domain, key, request, response)\n-\n-        return response\n-\n-    def process_exception(self, request, exception, spider):\n-        if not is_cacheable(request):\n-            return\n-\n-        if isinstance(exception, HttpException) and isinstance(exception.response, Response):\n-            key = request.fingerprint()\n-            domain = spider.domain_name\n-            self.cache.store(domain,key, request, exception.response)\n-\n-def is_cacheable(request):\n-    scheme, _, _, _, _ = urlparse.urlsplit(request.url)\n-    return scheme in ['http', 'https']\n-\n-\n-class Cache(object):\n-    DOMAIN_SECTORDIR = 'data'\n-    DOMAIN_LINKDIR = 'domains'\n-\n-    def __init__(self, cachedir, sectorize=False):\n-        self.cachedir = cachedir\n-        self.sectorize = sectorize\n-\n-        self.baselinkpath = os.path.join(self.cachedir, self.DOMAIN_LINKDIR)\n-        if not os.path.exists(self.baselinkpath):\n-            os.makedirs(self.baselinkpath)\n-\n-        self.basesectorpath = os.path.join(self.cachedir, self.DOMAIN_SECTORDIR)\n-        if not os.path.exists(self.basesectorpath):\n-            os.makedirs(self.basesectorpath)\n-\n-    def domainsectorpath(self, domain):\n-        sector = hashlib.sha1(domain).hexdigest()[0]\n-        return os.path.join(self.basesectorpath, sector, domain)\n-\n-    def domainlinkpath(self, domain):\n-        return os.path.join(self.baselinkpath, domain)\n-\n-    def requestpath(self, domain, key):\n-        linkpath = self.domainlinkpath(domain)\n-        return os.path.join(linkpath, key[0:2], key)\n-\n-    def open_domain(self, domain):\n-        if domain:\n-            linkpath = self.domainlinkpath(domain)\n-            if self.sectorize:\n-                sectorpath = self.domainsectorpath(domain)\n-                if not os.path.exists(sectorpath):\n-                    os.makedirs(sectorpath)\n-                if not os.path.exists(linkpath):\n-                    try:\n-                        os.symlink(sectorpath, linkpath)\n-                    except:\n-                        os.makedirs(linkpath) # windows filesystem\n-            else:\n-                if not os.path.exists(linkpath):\n-                    os.makedirs(linkpath)\n-\n-    def is_cached(self, domain, key):\n-        requestpath = self.requestpath(domain, key)\n-        if os.path.exists(requestpath):\n-            with open(os.path.join(requestpath, 'pickled_meta'), 'r') as f:\n-                metadata = pickle.load(f)\n-            expiration_secs = settings.getint('CACHE2_EXPIRATION_SECS')\n-            if expiration_secs >= 0:\n-                if datetime.datetime.utcnow() <= metadata['timestamp'] + datetime.timedelta(seconds=expiration_secs):\n-                    return True\n-                else:\n-                    log.msg('dropping old cached response from %s' % metadata['timestamp'])\n-                    return False\n-            else:\n-                # disabled cache expiration\n-                return True\n-        else:\n-            return False\n-\n-    def retrieve_response(self, domain, key):\n-        \"\"\"\n-        Return response dictionary if request has correspondent cache record;\n-        return None if not.\n-        \"\"\"\n-        if not self.is_cached(domain, key):\n-            return None # not cached\n-\n-        requestpath = self.requestpath(domain, key)\n-        metadata = responsebody = responseheaders = None\n-        with open(os.path.join(requestpath, 'pickled_meta'), 'r') as f:\n-            metadata = pickle.load(f)\n-        with open(os.path.join(requestpath, 'response_body')) as f:\n-            responsebody = f.read()\n-        with open(os.path.join(requestpath, 'response_headers')) as f:\n-            responseheaders = f.read()\n-\n-        url = metadata['url']\n-        original_url = metadata.get('original_url', url)\n-        headers = Headers(responseheaders)\n-        status = metadata['status']\n-\n-        response = Response(domain=domain, url=url, original_url=original_url, headers=headers, status=status, body=responsebody)\n-        response.cached = True\n-        return response\n-\n-    def store(self, domain, key, request, response):\n-        requestpath = self.requestpath(domain, key)\n-        if not os.path.exists(requestpath):\n-            os.makedirs(requestpath)\n-\n-        metadata = {\n-                'url':request.url,\n-                'method': request.method,\n-                'status': response.status,\n-                'domain': response.domain,\n-                'original_url': response.original_url,\n-                'timestamp': datetime.datetime.utcnow(),\n-            }\n-\n-        # metadata\n-        with open(os.path.join(requestpath, 'meta_data'), 'w') as f:\n-            f.write(repr(metadata))\n-        # pickled metadata (to recover without using eval)\n-        with open(os.path.join(requestpath, 'pickled_meta'), 'w') as f:\n-            pickle.dump(metadata, f)\n-        # response\n-        with open(os.path.join(requestpath, 'response_headers'), 'w') as f:\n-            f.write(headers_dict_to_raw(response.headers))\n-        with open(os.path.join(requestpath, 'response_body'), 'w') as f:\n-            f.write(response.body.get_content())\n-        # request\n-        with open(os.path.join(requestpath, 'request_headers'), 'w') as f:\n-            f.write(headers_dict_to_raw(request.headers))\n-        if request.body:\n-            with open(os.path.join(requestpath, 'request_body'), 'w') as f:\n-                f.write(request.body)\n-\n\n@@ -1,10 +0,0 @@\n-class CommonMiddleware(object):\n-    \"\"\"This middleware provides common/basic functionality, and should always\n-    be enabled\"\"\"\n-\n-    def process_request(self, request, spider):\n-        request.headers.setdefault('Accept-Language', 'en')\n-        request.headers.setdefault('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')\n-        if request.method == 'POST':\n-            request.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')\n-\n\n@@ -1,42 +0,0 @@\n-import zlib\n-from gzip import GzipFile\n-from cStringIO import StringIO\n-\n-from scrapy.http import Response, ResponseBody\n-\n-\n-class CompressionMiddleware(object):\n-    \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n-    sent/received from web sites\"\"\"\n-\n-    def process_request(self, request, spider):\n-        request.headers.setdefault('Accept-Encoding', 'gzip,deflate')\n-\n-    def process_response(self, request, response, spider):\n-        if isinstance(response, Response):\n-            content_encoding = response.headers.get('Content-Encoding')\n-            if content_encoding:\n-                encoding = content_encoding[0].lower()\n-                raw_body = response.body.get_content()\n-                declared_encoding = response.body.declared_encoding\n-                decoded_body = self._decode(raw_body, encoding)\n-                response.body = ResponseBody(decoded_body, declared_encoding)\n-                response.headers['Content-Encoding'] = content_encoding[1:]\n-        return response\n-\n-    def _decode(self, body, encoding):\n-        if encoding == 'gzip':\n-            body = GzipFile(fileobj=StringIO(body)).read()\n-\n-        if encoding == 'deflate':\n-            try:\n-                body = zlib.decompress(body)\n-            except zlib.error:\n-                # ugly hack to work with raw deflate content that may\n-                # be sent by microsof servers. For more information, see:\n-                # http://carsten.codimi.de/gzip.yaws/\n-                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n-                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n-                body = zlib.decompress(body, -15)\n-        return body\n-\n\n@@ -1,27 +0,0 @@\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n-from scrapy.utils.misc import dict_updatedefault\n-\n-class CookiesMiddleware(object):\n-    \"\"\"This middleware enables working with sites that need cookies\"\"\"\n-\n-    def __init__(self):\n-        self.cookies = {}\n-        dispatcher.connect(self.domain_open, signals.domain_open)\n-        dispatcher.connect(self.domain_closed, signals.domain_closed)\n-\n-    def process_request(self, request, spider):\n-        dict_updatedefault(request.cookies, self.cookies[spider.domain_name])\n-\n-    def process_response(self, request, response, spider):\n-        cookies = self.cookies[spider.domain_name]\n-        cookies.update(request.cookies)\n-        return response\n-\n-    def domain_open(self, domain):\n-        self.cookies[domain] = {}\n-\n-    def domain_closed(self, domain):\n-        del self.cookies[domain]\n\n@@ -1,20 +0,0 @@\n-from scrapy import log\n-from scrapy.conf import settings\n-\n-class CrawlDebug(object):\n-    def __init__(self):\n-        self.enabled = settings.getbool('CRAWL_DEBUG')\n-    \n-    def process_request(self, request, spider):\n-        if self.enabled:\n-            log.msg(\"Crawling %s\" % repr(request), domain=spider.domain_name, level=log.DEBUG)\n-\n-    def process_exception(self, request, exception, spider):\n-        if self.enabled:\n-            log.msg(\"Crawl exception %s in %s\" % (exception, repr(request)), domain=spider.domain_name, level=log.DEBUG)\n-\n-    def process_response(self, request, response, spider):\n-        if self.enabled:\n-            log.msg(\"Fetched %s from %s\" % (response.info(), repr(request)), domain=spider.domain_name, level=log.DEBUG)\n-        return response\n-\n\n@@ -1,8 +0,0 @@\n-class HttpAuthMiddleware(object):\n-    \"\"\"This middleware allows spiders to use HTTP auth in a cleaner way\n-    (http_user and http_pass spider class attributes)\"\"\"\n-\n-    def process_request(self, request, spider):\n-        if getattr(spider, 'http_user', None) or getattr(spider, 'http_pass', None):\n-            request.httpauth(spider.http_user, spider.http_pass)\n-\n\n@@ -1,66 +0,0 @@\n-import re\n-\n-from scrapy import log\n-from scrapy.http import Request, Response\n-from scrapy.core.exceptions import HttpException\n-from scrapy.utils.url import urljoin_rfc as urljoin\n-\n-class RedirectLoop(Exception):\n-    pass\n-\n-META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\\d+);url=([^\"\\']+)', re.IGNORECASE)\n-# some sites use meta-refresh for redirecting to a session expired page, so we\n-# restrict automatic redirection to a maximum delay (in number of seconds)\n-META_REFRESH_MAXSEC = 100\n-MAX_REDIRECT_LOOP = 10\n-\n-class RedirectMiddleware(object):\n-    def process_exception(self, request, exception, spider):\n-        if isinstance(exception, HttpException):\n-            status = exception.status\n-            response = exception.response\n-            \n-            if status in ['302', '303']:\n-                redirected_url = urljoin(request.url, response.headers['location'][0])\n-                if not getattr(spider, \"no_redirect\", False):\n-                    redirected = request.copy()\n-                    redirected.url = redirected_url\n-                    redirected.method = 'GET'\n-                    redirected.body = None\n-                    # This is needed to avoid redirection loops with requests that contain dont_filter = True\n-                    # Example (9 May 2008): http://www.55max.com/product/001_photography.asp?3233,0,0,0,Michael+Banks\n-                    if isinstance(redirected.dont_filter, int):\n-                        if not hasattr(redirected, \"original_dont_filter\"):\n-                            redirected.original_dont_filter = redirected.dont_filter\n-                        if redirected.dont_filter <= -MAX_REDIRECT_LOOP:\n-                            raise RedirectLoop(\"Exited redirect loop with %s consecutive visits to the same url.\" % (redirected.original_dont_filter + MAX_REDIRECT_LOOP) )\n-                        redirected.dont_filter -= 1\n-                    else:\n-                        redirected.dont_filter = False\n-                    log.msg(\"Redirecting (%s) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n-                    return redirected\n-                log.msg(\"Ignored redirecting (%s) to %s from %s (disabled by spider)\" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)\n-                return response\n-\n-            if status in ['301', '307']:\n-                redirected_url = urljoin(request.url, response.headers['location'][0])\n-                if not getattr(spider, \"no_redirect\", False):\n-                    redirected = request.copy()\n-                    redirected.url = redirected_url\n-                    # This is needed to avoid redirection loops with requests that contain dont_filter = True\n-                    # Example (9 May 2008): http://www.55max.com/product/001_photography.asp?3233,0,0,0,Michael+Banks\n-                    redirected.dont_filter = False\n-                    log.msg(\"Redirecting (%s) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n-                    return redirected\n-                log.msg(\"Ignored redirecting (%s) to %s from %s (disabled by spider)\" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)\n-                return response\n-\n-    def process_response(self, request, response, spider):\n-        if isinstance(response, Response):\n-            m = META_REFRESH_RE.search(response.body.to_string()[0:4096])\n-            if m and int(m.group(1)) < META_REFRESH_MAXSEC:\n-                redirected = request.copy()\n-                redirected.url = urljoin(request.url, m.group(2))\n-                log.msg(\"Redirecting (meta refresh) to %s from %s\" % (redirected, request), level=log.DEBUG, domain=spider.domain_name)\n-                return redirected\n-        return response\n\n@@ -1,63 +0,0 @@\n-\"\"\"\n-An extension to retry failed requests that are potentially caused by temporary\n-problems such as a connection timeout or HTTP 500 error.\n-\n-You can change the behaviour of this moddileware by modifing the scraping settings: \n-RETRY_TIMES - how many times to retry a failed page\n-RETRY_HTTP_CODES - which HTTP response codes to retry\n-\n-Failed pages are collected on the scraping process and rescheduled at the end,\n-once the spider has finished crawling all regular (non failed) pages. Once\n-there is no more failed pages to retry this middleware sends a signal\n-(retry_complete), so other extensions could connect to that signal.\n-\n-Default values are located in scrapy.conf.core_settings\n-\n-About HTTP errors to consider:\n-\n-- You may want to remove 400 from RETRY_HTTP_CODES, if you stick to the HTTP\n-  protocol. It's included by default because it's a common code used to\n-  indicate server overload, which would be something we want to retry\n-- 200 is included by default (and shoudln't be removed) to check for partial\n-  downloads errors, which means the TCP connection has broken in the middle of\n-  a HTTP download \n-\"\"\" \n-\n-from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookupError, \\\n-                                   ConnectionRefusedError, ConnectionDone, ConnectError\n-from twisted.internet.defer import TimeoutError as UserTimeoutError \n-\n-from scrapy import log\n-from scrapy.core.exceptions import HttpException\n-from scrapy.conf import settings\n-\n-class RetryMiddleware(object):\n-\n-    EXCEPTIONS_TO_RETRY = (ServerTimeoutError, UserTimeoutError, DNSLookupError, \n-                           ConnectionRefusedError, ConnectionDone, ConnectError)\n-\n-    def __init__(self):\n-        self.failed_count = {}\n-        self.max_retries = settings.getint('RETRY_TIMES')\n-\n-    def process_exception(self, request, exception, spider):\n-        retry = False\n-\n-        if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n-            retry = True\n-        elif isinstance(exception, HttpException):\n-            if exception.status in settings.getlist('RETRY_HTTP_CODES'):\n-                retry = True\n-\n-        if retry:\n-            fp = request.fingerprint()\n-            count = self.failed_count[fp] = self.failed_count.get(fp, 0) + 1\n-\n-            if self.failed_count[fp] < self.max_retries:\n-                log.msg(\"Retrying %s (failed %d times): %s\" % (request, count, exception), level=log.DEBUG, domain=spider.domain_name)\n-                retryreq = request.copy()\n-                retryreq.dont_filter = True\n-                return retryreq\n-            else:\n-                log.msg(\"Discarding %s (failed %d times): %s\" % (request, count, exception), domain=spider.domain_name, level=log.DEBUG)\n-\n\n@@ -1,60 +0,0 @@\n-\"\"\"\n-jhis is an expertimental middleware to respect robots.txt policies. The biggest\n-problem it has is that it uses urllib directly (in RobotFileParser.read()\n-method) and that conflicts with twisted networking, so it should be ported to\n-use twisted networking API, but that is not as trivial as it may seem.\n-\n-This code is left here for future reference, when we resume the work on this\n-subject.\n-\n-\"\"\"\n-\n-import re\n-import urlparse\n-import robotparser\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.core.exceptions import IgnoreRequest\n-from scrapy.conf import settings\n-\n-BASEURL_RE = re.compile(\"http://.*?/\")\n-\n-class RobotsMiddleware(object):\n-\n-    def __init__(self):\n-        self._parsers = {}\n-        self._spiderdomains = {}\n-        self._pending = {}\n-        dispatcher.connect(self.domain_open, signals.domain_open)\n-        dispatcher.connect(self.domain_closed, signals.domain_closed)\n-\n-    def process_request(self, request, spider):\n-        agent = getattr(spider, 'user_agent', None) or settings['USER_AGENT']\n-        rp = self.robot_parser(request.url, spider.domain_name)\n-        if rp and not rp.can_fetch(agent, request.url):\n-            raise IgnoreRequest(\"URL forbidden by robots.txt: %s\" % request.url)\n-\n-    def robot_parser(self, url, spiderdomain):\n-        urldomain = urlparse.urlparse(url).hostname\n-        if urldomain in self._parsers:\n-            rp = self._parsers[urldomain]\n-        else:\n-            rp = robotparser.RobotFileParser()\n-            m = BASEURL_RE.search(url)\n-            if m:\n-                rp.set_url(\"%srobots.txt\" % m.group())\n-                rp.read()\n-            self._parsers[urldomain] = rp\n-            self._spiderdomains[spiderdomain].add(urldomain)\n-        return rp\n-\n-    def domain_open(self, domain):\n-        self._spiderdomains[domain] = set()\n-\n-    def domain_closed(self, domain):\n-        for urldomain in self._spiderdomains[domain]:\n-            del self._parsers[urldomain]\n-        del self._spiderdomains[domain]\n\n@@ -1,7 +0,0 @@\n-class UserAgentMiddleware(object):\n-    \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n-\n-    def process_request(self, request, spider):\n-        if getattr(spider, 'user_agent', None):\n-            request.headers.setdefault('User-Agent', spider.user_agent)\n-\n\n@@ -1,88 +0,0 @@\n-import hashlib\n-from datetime import datetime\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.utils.misc import load_class\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n-from scrapy.conf import settings\n-\n-class HistoryMiddleware(object):\n-    # How often we should re-check links we know about\n-    MIN_CHECK_DAYS = 4\n-    # How often we should process pages that have not changed (need to include depth)\n-    MIN_PROCESS_UNCHANGED_DAYS = 12\n-\n-    def __init__(self):\n-        historycls = load_class(settings['MEMORYSTORE'])\n-        if not historycls:\n-            raise NotConfigured\n-        self.historydata = historycls()\n-        dispatcher.connect(self.open_domain, signal=signals.domain_open)\n-        dispatcher.connect(self.close_domain, signal=signals.domain_closed)\n-\n-    def process_request(self, request, spider):\n-        key = urlkey(request.url)\n-        status = self.historydata.status(domain, key)\n-        if status:\n-            _url, version, last_checked = status\n-            d = datetime.now() - last_checked\n-            if d.days < self.MIN_CHECK_DAYS:\n-                raise IgnoreRequest(\"Not scraping %s (scraped %s ago)\" % (request.url, d))\n-            request.context['history_response_version'] = version\n-\n-    def process_response(self, request, response, spider):\n-        version = request.context.get('history_response_version')\n-        if version == response.version():\n-            del request.content['history_response_version']\n-            hist = self.historydata.version_info(domain, version)\n-            if hist:\n-                versionkey, created = hist\n-                # if versionkey != urlkey(url) this means\n-                # the same content is available on a different url\n-                delta = datetime.now() - created\n-                if delta.days < self.MIN_PROCESS_UNCHANGED_DAYS:\n-                    message = \"skipping %s: unchanged for %s\" % (response.url, delta)\n-                    raise IgnoreRequest(message)\n-        self.record_visit(domain, request, response)\n-        return response\n-\n-    def process_exception(self, request, exception, spider):\n-        self.record_visit(spider.domain_name, request, None)\n-\n-    def open_domain(self, domain):\n-        self.historydata.open(domain)\n-\n-    def close_domain(self, domain):\n-        self.historydata.close_site(domain)\n-\n-    def record_visit(self, domain, request, response):\n-        \"\"\"record the fact that the url has been visited\"\"\"\n-        url = request.url\n-        post_version = hash(request.body)\n-        key = urlkey(url)\n-        if response:\n-            redirect_url = response.url\n-            parentkey = urlkey(response.request.headers.get('referer')) if response.request else None\n-            version = response.version()\n-        else:\n-            redirect_url, parentkey, version = url, None, None\n-        self.historydata.store(domain, key, url, parentkey, version, post_version)\n-\n-\n-def urlkey(url):\n-    \"\"\"Generate a 'key' for a given url\n-\n-    >>> urlkey(\"http://www.example.com/\")\n-    '89e6a0649e06d83370cdf2cbfb05f363934a8d0c'\n-    >>> urlkey(\"http://www.example.com/\") == urlkey(\"http://www.example.com/?\")\n-    True\n-    \"\"\"\n-    from scrapy.utils.c14n import canonicalize\n-    return hash(canonicalize(url))\n-\n-\n-def hash(value):\n-    return hashlib.sha1(value).hexdigest() if value else None\n\n@@ -1,94 +0,0 @@\n-from datetime import datetime\n-\n-from twisted.internet import defer\n-\n-from scrapy import log\n-from scrapy.core.scheduler import Scheduler\n-from scrapy.core.exceptions import IgnoreRequest\n-\n-class RulesScheduler(Scheduler):\n-    \"\"\"Scheduler that uses rules to determine if we should follow links\n-\n-     TODO:\n-     * take into account where in chain of links we are (less depth should\n-       be crawled more often)\n-     * Be more strict about scraping product pages that rarely lead to new\n-       versions of products. The same applies to pages with links. Particularly\n-       useful for filtering out when there are many urls for the same product.\n-       (but be careful to also filter out pages that almost always lead to new\n-       output).\n-    \"\"\"\n-\n-    # if these parameters change, then update bin/unavailable.py\n-\n-    # How often we should re-check links we know about\n-    MIN_CHECK_DAYS = 4\n-\n-    # How often we should process pages that have not changed (need to include depth)\n-    MIN_PROCESS_UNCHANGED_DAYS = 12\n-\n-    def enqueue_request(self, domain, request, priority=1):\n-        \"\"\"Add a page to be scraped for a domain that is currently being scraped.\n-\n-        The url will only be added if we have not checked it already within\n-        a specified time period.\n-        \"\"\"\n-        requestid = request.fingerprint()\n-        added = self.groupfilter.add(domain, requestid)\n-\n-        if request.dont_filter or added:\n-            key = urlkey(request.url) # we can not use fingerprint unless lost crawled history\n-            status = self.historydata.status(domain, key)\n-            now = datetime.now()\n-            version = None\n-            if status:\n-                _url, version, last_checked = status\n-                d = now - last_checked\n-                if d.days < self.MIN_CHECK_DAYS:\n-                    log.msg(\"Not scraping %s (scraped %s ago)\" % (request.url, d), level=log.DEBUG)\n-                    return\n-            # put the version in the pending pages to avoid querying DB again\n-            record = (request, version, now)\n-            self.pending_requests[domain].put(record, priority)\n-\n-    def next_request(self, domain):\n-        \"\"\"Get the next page from the superclass. This will add a callback\n-        to prevent processing the page unless its content has been\n-        changed.\n-\n-        In the event that it a page is not processed, the record_visit method\n-        is called to update the last_checked time.\n-        \"\"\"\n-        pending_list = self.pending_requests.get(domain)\n-        if not pending_list :\n-            return None\n-        request, version, timestamp = pending_list.get_nowait()[1]\n-        post_version = hash(request.body)\n-\n-        def callback(pagedata):\n-            \"\"\"process other callback if we pass the checks\"\"\"\n-            if version == pagedata.version():\n-                hist = self.historydata.version_info(domain, version)\n-                if hist:\n-                    versionkey, created = hist\n-                    # if versionkey != urlkey(url) this means\n-                    # the same content is available on a different url\n-                    delta = timestamp - created\n-                    if delta.days < self.MIN_PROCESS_UNCHANGED_DAYS:\n-                        message = \"skipping %s: unchanged for %s\" % (pagedata.url, delta)\n-                        raise IgnoreRequest(message)\n-            self.record_visit(domain, request.url, pagedata.url,\n-                              pagedata.parent,  pagedata.version(),\n-                              post_version)\n-            return pagedata\n-\n-        def errback(error) :\n-            self.record_visit(domain, request.url, request.url, None, None,\n-                              post_version)\n-            return error\n-\n-        d = defer.Deferred()\n-        d.addCallbacks(callback, errback)\n-        request.prepend_callback(d)\n-\n-        return request\n\n@@ -1,208 +0,0 @@\n-\"\"\"\n-This module contains some extra base models for scraped items which could be\n-useful in some Scrapy implementations\n-\"\"\"\n-\n-import hashlib\n-\n-from pprint import PrettyPrinter\n-from scrapy.item import ScrapedItem\n-from scrapy.core.exceptions import UsageError, DropItem\n-\n-class ValidationError(DropItem):\n-    \"\"\"Indicates a data validation error\"\"\"\n-    def __init__(self,problem,value=None):\n-        self.problem = problem\n-        self.value = value\n-\n-    def __str__(self):\n-        if self.value is not None:\n-            return '%s \"%s\"' % (self.problem, self.value)\n-        else:\n-            return '%s' % (self.problem)\n-\n-class ValidationPipeline(object):\n-    def process_item(self, domain, response, item):\n-        item.validate()\n-        return item\n-\n-class RobustScrapedItem(ScrapedItem):\n-    \"\"\"\n-    A more robust scraped item class with a built-in validation mechanism and \n-    minimal versioning support\n-    \"\"\"\n-\n-    ATTRIBUTES = {\n-        'guid': basestring, # a global unique identifier\n-        'url': basestring,  # the main URL where this item was scraped from\n-    }\n-    \n-    def __init__(self, data=None):\n-        \"\"\"\n-        A scraped item can be initialised with a dictionary that will be\n-        squirted directly into the object.\n-        \"\"\"\n-        if isinstance(data, dict):\n-            for attr, value in data.iteritems():\n-                setattr(self, attr, value)\n-        elif data is not None:\n-            raise UsageError(\"Initialize with dict, not %s\" % data.__class__.__name__)\n-\n-        self.__dict__['_version'] = None\n-\n-    def __getattr__(self, attr):\n-        # Return None for valid attributes not set, raise AttributeError for invalid attributes\n-        # Note that this method is called only when the attribute is not found in \n-        # self.__dict__ or the class/instance methods.\n-        if attr in self.ATTRIBUTES:  \n-            return None\n-        else:\n-            raise AttributeError(attr)\n-\n-    def __setattr__(self, attr, value):\n-        \"\"\"\n-        Set an attribute checking it matches the attribute type declared in self.ATTRIBUTES\n-        \"\"\"\n-        if not attr.startswith('_') and attr not in self.ATTRIBUTES:\n-            raise AttributeError('Attribute \"%s\" is not a valid attribute name. You must add it to %s.ATTRIBUTES' % (attr, self.__class__.__name__))\n-\n-        if value is None:\n-            self.__dict__.pop(attr, None)\n-            return\n-\n-        type1 = self.ATTRIBUTES[attr]\n-        if hasattr(type1, '__iter__'):\n-            if not hasattr(value, '__iter__'):\n-                raise TypeError('Attribute \"%s\" must be a sequence' % attr)\n-            type2 = type1[0]\n-            for i in value:\n-                if not isinstance(i, type2):\n-                    raise TypeError('Attribute \"%s\" cannot contain %s, only %s' % (attr, i.__class__.__name__, type2.__name__))\n-        else:\n-            if not isinstance(value, type1):\n-                raise TypeError('Attribute \"%s\" must be %s, not %s' % (attr, type1.__name__, value.__class__.__name__))\n-\n-        self.__dict__[attr] = value\n-        self.__dict__['_version'] = None\n-    \n-    def __delattr__(self, attr):\n-        \"\"\"\n-        Delete an attribute from the ScrapedItem instance if it exists.\n-        If not, raise an AttributeError.\n-        \"\"\"\n-        if attr in self.__dict__:\n-            del self.__dict__[attr]\n-            self.__dict__['_version'] = None\n-        else:\n-            raise AttributeError(\"Attribute '%s' doesn't exist\" % attr)\n-    \n-    def __eq__(self, other):\n-        if isinstance(other, type(self)):\n-            return self.version == other.version\n-        \n-    def __ne__(self, other):\n-        return self.version != other.version\n-    \n-    def __sub__(self, other):\n-        return RobustItemDelta(other, self)\n-\n-    def __repr__(self):\n-        # Generate this format so that it can be deserialized easily:\n-        # ClassName({...})\n-        reprdict = {}\n-        for k, v in self.__dict__.iteritems():\n-            if not k.startswith('_'):\n-                reprdict[k] = v\n-        return \"%s(%s)\" % (self.__class__.__name__, repr(reprdict))\n-\n-    def __str__(self) :\n-        return \"%s: GUID=%s, url=%s\" % ( self.__class__.__name__ , self.guid, self.url )\n-            \n-    def validate(self):\n-        \"\"\"Method used to validate item attributes data\"\"\"\n-        if not self.guid:\n-            raise ValidationError('A guid is required')\n-\n-    def copy(self):\n-        \"\"\"Create a new ScrapedItem object based on the current one\"\"\"\n-        import copy\n-        return copy.deepcopy(self)\n-\n-    @property\n-    def version(self):\n-        \"\"\"\n-        Return a (cached) 40 char hash of all the item attributes.\n-\n-        WARNING: This cached version won't work if mutable products are\n-        modified directly like:\n-\n-        item.features.append('feature')\n-        \"\"\"\n-        if self._version:\n-            return self._version\n-        hash_ = hashlib.sha1()\n-        hash_.update(\"\".join([\"\".join([n, str(v)]) for n,v in sorted(self.__dict__.iteritems())]))\n-        return hash_.hexdigest()\n-\n-\n-class RobustItemDelta(object):\n-    \"\"\"\n-    This class represents the difference between\n-    a pair of RobustScrapedItems.\n-    \"\"\"\n-\n-    def __init__(self, old_item, new_item):\n-        if not isinstance(old_item, RobustScrapedItem) or \\\n-           not isinstance(new_item, RobustScrapedItem):\n-            raise TypeError(\"Both arguments must be RobustScrapedItem instances\")\n-\n-        if old_item.guid != new_item.guid:\n-            raise AttributeError(\"Item GUIDs must be equal in order to create a RobustItemDelta object\")\n-\n-        self.old_item = old_item\n-        self.new_item = new_item\n-        self.diff = self.do_diff()\n-\n-    def do_diff(self):\n-        \"\"\"\n-        This method should retreive a dictionary\n-        containing the changes between both items\n-        as in this example:\n-\n-        >>> delta.do_diff()\n-        >>> {'attrib': {'new': 'New value', 'old': 'Old value'}, # Common attributes\n-             'attrib2': {'new': 'New value 2', 'old': 'Old value 2'},\n-             'attrib3': [{'new': 'New list value', 'old': 'Old list value'}, # List attributes\n-                         {'new': 'New list value 2', 'old': 'Old list value 2'}]}\n-        \"\"\"\n-        \n-        if self.old_item == self.new_item:\n-            return {}\n-\n-        diff = {}\n-        for key, value in self.old_item.__dict__.items():\n-            if key in self.old_item.ATTRIBUTES.keys():\n-                new_value = getattr(self.new_item, key)\n-                if value != new_value:\n-                    diff[key] = {'new': new_value, 'old': value}\n-        for key, value in self.new_item.__dict__.items():\n-            if value and key in self.new_item.ATTRIBUTES.keys():\n-                if not getattr(self.old_item, key):\n-                    diff[key] = {'new': value, 'old': None}\n-        return diff\n-    \n-    def __eq__(self, other):\n-        if isinstance(other, RobustItemDelta):\n-            if other.old_item == self.old_item and \\\n-               other.new_item == self.new_item and \\\n-               other.diff == self.diff:\n-                return True\n-        return False\n-\n-    def __repr__(self):\n-        if self.diff:\n-            pp = PrettyPrinter(indent=3)\n-            return pp.pformat(self.diff)\n-        else:\n-            return 'No differences found between the provided items.'\n-\n\n@@ -1,69 +0,0 @@\n-import pprint\n-import gc\n-import socket\n-\n-import libxml2\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.mail import MailSender\n-from scrapy.stats import stats\n-from scrapy.extension import extensions\n-from scrapy.conf import settings\n-\n-class MemoryDebugger(object):\n-\n-    def __init__(self):\n-        if not settings.getbool('MEMDEBUG_ENABLED'):\n-            raise NotConfigured\n-\n-        self.mail = MailSender()\n-        self.rcpts = settings.getlist('MEMDEBUG_NOTIFY')\n-\n-        self.domains_scraped = []\n-        dispatcher.connect(self.domain_opened, signals.domain_opened)\n-        dispatcher.connect(self.engine_started, signals.engine_started)\n-        dispatcher.connect(self.engine_stopped, signals.engine_stopped)\n-\n-    def engine_started(self):\n-        libxml2.debugMemory(1)\n-\n-    def engine_stopped(self):\n-        figures = self.collect_figures()\n-        report = self.create_report(figures)\n-        self.print_or_send_report(report)\n-\n-    def collect_figures(self):\n-        libxml2.cleanupParser()\n-        gc.collect()\n-\n-        figures = []\n-        if 'MemoryUsage' in extensions.enabled:\n-            memusage = extensions.enabled['MemoryUsage']\n-            memusage.update()\n-            figures.append((\"Memory usage at startup\", int(memusage.data['startup']/1024/1024), \"Mb\"))\n-            figures.append((\"Maximum memory usage\", int(memusage.data['max']/1024/1024), \"Mb\"))\n-            figures.append((\"Memory usage at shutdown\", int(memusage.virtual/1024/1024), \"Mb\"))\n-        figures.append((\"Objects in gc.garbage\", len(gc.garbage), \"\"))\n-        figures.append((\"libxml2 memory leak\", libxml2.debugMemory(1), \"bytes\"))\n-        return figures\n-\n-    def create_report(self, figures):\n-        s = \"\"\n-        s += \"SCRAPY MEMORY DEBUGGER RESULTS\\n\\n\"\n-        for f in figures:\n-            s += \"%-30s : %s %s\\n\" % f\n-        s += \"\\n\"\n-        if stats:\n-            s += \"SCRAPING STATS --------------------------------------------------\\n\\n\"\n-            s += pprint.pformat(stats)\n-        return s\n-\n-    def print_or_send_report(self, report):\n-        if self.rcpts:\n-            self.mail.send(self.rcpts, \"Scrapy Memory Debugger results at %s\" % socket.gethostname(), report)\n-        print report\n-\n-    def domain_opened(self, domain):\n-        self.domains_scraped.append(domain)\n\n@@ -1,122 +0,0 @@\n-import sys\n-import os\n-import pprint\n-import socket\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.core.manager import scrapymanager\n-from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.mail import MailSender\n-from scrapy.stats import stats\n-from scrapy.conf import settings\n-\n-class MemoryUsage(object):\n-    \n-    _proc_status = '/proc/%d/status' % os.getpid()\n-    _scale = {'kB': 1024.0, 'mB': 1024.0*1024.0,\n-              'KB': 1024.0, 'MB': 1024.0*1024.0}\n-\n-    def __init__(self):\n-        if not settings.getbool('MEMUSAGE_ENABLED'):\n-            raise NotConfigured\n-        if sys.platform != 'linux2':\n-            raise NotConfigured(\"MemoryUsage extension is only available on Linux\")\n-\n-        self.warned = False\n-\n-        self.data = {}\n-        self.data['startup'] = 0\n-        self.data['max'] = 0\n-\n-        scrapyengine.addtask(self.update, 60.0, now=True)\n-\n-        self.notify_mails = settings.getlist('MEMUSAGE_NOTIFY')\n-        self.limit = settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n-        self.warning = settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n-        self.report = settings.getbool('MEMUSAGE_REPORT')\n-\n-        if self.limit:\n-            scrapyengine.addtask(self._check_limit, 60.0, now=True)\n-        if self.warning:\n-            scrapyengine.addtask(self._check_warning, 60.0, now=True)\n-\n-        self.mail = MailSender()\n-\n-        dispatcher.connect(self.engine_started, signal=signals.engine_started)\n-\n-\n-    @property\n-    def virtual(self):\n-        return self._vmvalue('VmSize:')\n-\n-    @property\n-    def resident(self):\n-        return self._vmvalue('VmRSS:')\n-        \n-    @property\n-    def stacksize(self):\n-        return self._vmvalue('VmStk:')\n-\n-    def engine_started(self):\n-        self.data['startup'] = self.virtual\n-\n-    def update(self):\n-        if self.virtual > self.data['max']:\n-            self.data['max'] = self.virtual\n-\n-    def _vmvalue(self, VmKey):\n-        # get pseudo file  /proc/<pid>/status\n-        try:\n-            t = open(self._proc_status)\n-            v = t.read()\n-            t.close()\n-        except:\n-            return 0.0  # non-Linux?\n-        # get VmKey line e.g. 'VmRSS:  9999  kB\\n ...'\n-        i = v.index(VmKey)\n-        v = v[i:].split(None, 3)  # whitespace\n-        if len(v) < 3:\n-            return 0.0  # invalid format?\n-        # convert Vm value to bytes\n-        return float(v[1]) * self._scale[v[2]]\n-\n-    def _check_limit(self):\n-        if self.virtual > self.limit:\n-            mem = self.limit/1024/1024\n-            log.msg(\"Memory usage exceeded %dM. Shutting down Scrapy...\" % mem, level=log.ERROR)\n-            if self.notify_mails:\n-                subj = \"%s terminated: memory usage exceeded %dM at %s\" % (settings['BOT_NAME'], mem, socket.gethostname())\n-                self._send_report(self.notify_mails, subj)\n-            scrapymanager.stop()\n-\n-    def _check_warning(self):\n-        if self.warned: # warn only once\n-            return\n-        if self.virtual > self.warning:\n-            mem = self.warning/1024/1024\n-            log.msg(\"Memory usage reached %dM\" % mem, level=log.WARNING)\n-            if self.notify_mails:\n-                subj = \"%s warning: memory usage reached %dM at %s\" % (settings['BOT_NAME'], mem, socket.gethostname())\n-                self._send_report(self.notify_mails, subj)\n-            self.warned = True\n-\n-    def _send_report(self, rcpts, subject):\n-        \"\"\"send notification mail with some additional useful info\"\"\"\n-        s = \"Memory usage at engine startup : %dM\\r\\n\" % (self.data['startup']/1024/1024)\n-        s += \"Maximum memory usage           : %dM\\r\\n\" % (self.data['max']/1024/1024)\n-        s += \"Current memory usage           : %dM\\r\\n\" % (self.virtual/1024/1024)\n-\n-        s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n-        s += \"\\r\\n\"\n-        s += scrapyengine.getstatus()\n-        s += \"\\r\\n\"\n-\n-        if stats:\n-            s += \"SCRAPING STATS ------------------------------------------------------ \\r\\n\"\n-            s += \"\\r\\n\"\n-            s += pprint.pformat(stats)\n-        self.mail.send(rcpts, subject, s)\n\n@@ -1,181 +0,0 @@\n-import re\n-import os\n-import time\n-import hashlib\n-import urllib\n-import urlparse\n-from cStringIO import StringIO\n-\n-import Image\n-\n-from scrapy import log\n-from scrapy.stats import stats\n-from scrapy.core.exceptions import DropItem, NotConfigured\n-from scrapy.core.exceptions import HttpException\n-from scrapy.conf import settings\n-\n-from scrapy.contrib.pipeline.media import MediaPipeline\n-\n-# the age at which we download images again\n-IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n-\n-class NoimagesDrop(DropItem):\n-    pass\n-\n-class ImageException(Exception):\n-    \"\"\"General image error exception\"\"\"\n-\n-\n-class ImagesPipeline(MediaPipeline):\n-    MEDIA_TYPE = 'image'\n-    THUMBS = None\n-#     THUMBS = (\n-#             (\"50\", (50, 50)),\n-#             (\"110\", (110, 110)),\n-#             (\"270\", (270, 270))\n-#     )\n-    MIN_WIDTH = 0\n-    MIN_HEIGHT = 0\n-\n-    def __init__(self):\n-        if not settings['IMAGES_DIR']:\n-            raise NotConfigured\n-\n-        self.BASEDIRNAME = settings['IMAGES_DIR']\n-        self.mkdir(self.BASEDIRNAME)\n-\n-        self.MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n-        self.MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n-        MediaPipeline.__init__(self)\n-\n-    def media_to_download(self, request, info):\n-        relative, absolute = self._get_paths(request)\n-        if not should_download(absolute):\n-            self.inc_stats(info.domain, 'uptodate')\n-            referer = request.headers.get('Referer')\n-            log.msg('Image (uptodate): Downloaded %s from %s referred in <%s>' % \\\n-                    (self.MEDIA_TYPE, request, referer), level=log.DEBUG, domain=info.domain)\n-            return relative\n-\n-    def media_downloaded(self, response, request, info):\n-        mtype = self.MEDIA_TYPE\n-        referer = request.headers.get('Referer')\n-\n-        if not response or not response.body.to_string():\n-            msg = 'Image (empty): Empty %s (no content) in %s referred in <%s>: Empty image (no-content)' % (mtype, request, referer)\n-            log.msg(msg, level=log.WARNING, domain=info.domain)\n-            raise ImageException(msg)\n-\n-        result = self.save_image(response, request, info) # save and thumbs response\n-\n-        status = 'cached' if getattr(response, 'cached', False) else 'downloaded'\n-        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % (status, mtype, request, referer)\n-        log.msg(msg, level=log.DEBUG, domain=info.domain)\n-        self.inc_stats(info.domain, status)\n-        return result\n-\n-    def media_failed(self, failure, request, info):\n-        referer = request.headers.get('Referer')\n-        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)\n-        msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' % (self.MEDIA_TYPE, request, referer, errmsg)\n-        log.msg(msg, level=log.WARNING, domain=info.domain)\n-        raise ImageException(msg)\n-\n-    def save_image(self, response, request, info):\n-        mtype = self.MEDIA_TYPE\n-        relpath, abspath = self._get_paths(request)\n-        dirname = os.path.dirname(abspath)\n-        self.mkdir(dirname, info)\n-\n-        try:\n-            save_image_with_thumbnails(response, abspath, self.THUMBS, self.MIN_WIDTH, self.MIN_HEIGHT)\n-        except ImageException, ex:\n-            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n-            raise ex\n-        except Exception, ex:\n-            referer = request.headers.get('Referer')\n-            msg = 'Image (processing-error): Error thumbnailing %s from %s referred in <%s>: %s' % (mtype, request, referer, ex)\n-            log.msg(msg, level=log.WARNING, domain=info.domain)\n-            raise ImageException(msg)\n-\n-        return relpath # success value sent as input result for item_media_downloaded\n-\n-    def _get_paths(self, request):\n-        relative = image_path(request.url)\n-        absolute = os.path.join(self.BASEDIRNAME, relative)\n-        return relative, absolute\n-\n-    def mkdir(self, dirname, info=None):\n-        already_created = info.extra.setdefault('created_directories', set()) if info else set()\n-        if dirname not in already_created:\n-            if not os.path.exists(dirname):\n-                os.makedirs(dirname)\n-            already_created.add(dirname)\n-\n-    def inc_stats(self, domain, status):\n-        stats.incpath('%s/image_count' % domain)\n-        stats.incpath('%s/image_status_count/%s' % (domain, status))\n-\n-\n-\n-def should_download(path):\n-    \"\"\"Should the image downloader download the image to the location specified\n-    \"\"\"\n-    try:\n-        mtime = os.path.getmtime(path)\n-        age_seconds = time.time() - mtime\n-        age_days = age_seconds / 60 / 60 / 24\n-        return age_days > IMAGE_EXPIRES\n-    except:\n-        return True\n-\n-_MULTIPLE_SLASHES_REGEXP = re.compile(r\"\\/{2,}\")\n-_FINAL_SLASH_REGEXP = re.compile(r\"\\/$\")\n-def image_path(url):\n-    \"\"\"Return the relative path on the target filesystem for an image to be\n-    downloaded to.\n-    \"\"\"\n-    _, netloc, urlpath, query, _ = urlparse.urlsplit(url)\n-    urlpath = _MULTIPLE_SLASHES_REGEXP.sub('/', urlpath)\n-    urlpath = _FINAL_SLASH_REGEXP.sub('.jpg', urlpath)\n-    if os.sep != '/':\n-        urlpath.replace('/', os.sep)\n-    if query:\n-        img_path = os.path.join(netloc, hashlib.sha1(url).hexdigest())\n-    else:\n-        img_path = os.path.join(netloc, urlpath[1:])\n-    return urllib.unquote(img_path)\n-\n-\n-def thumbnail_name(image, sizestr):\n-    \"\"\"Get the name of a thumbnail image given the name of the original file.\n-\n-    There will can be many types of thumbnails, so we will have a \"name\" for\n-    each type.\n-    \"\"\"\n-    return os.path.splitext(image)[0] + '_' + sizestr + '.jpg'\n-\n-def save_scaled_image(image, img_path, name, size):\n-    thumb = image.copy() if image.mode == 'RGB' else image.convert('RGB')\n-    thumb.thumbnail(size, Image.ANTIALIAS)\n-    filename = thumbnail_name(img_path, name)\n-    thumb.save(filename, 'JPEG')\n-\n-def save_image_with_thumbnails(response, path, thumbsizes, min_width=0, min_height=0):\n-    memoryfile = StringIO(response.body.to_string())\n-    im = Image.open(memoryfile)\n-    if im.mode != 'RGB':\n-        log.msg(\"Found non-RGB image during scraping %s\" % path, level=log.WARNING)\n-    for name, size in thumbsizes or []:\n-        save_scaled_image(im, path, name, size)\n-    try:\n-        im.save(path)\n-    except Exception, ex:\n-        log.msg(\"Image (processing-error): cannot process %s, so writing direct file: Error: %s\" % (path, ex))\n-        f = open(path, 'wb')\n-        f.write(response.body.to_string())\n-        f.close()\n-    width, height = im.size\n-    if width < min_width or height < min_height:\n-        raise ImageException(\"Image too small (%dx%d < %dx%d): %s\" % (width, height, min_width, min_height, response.url))\n-\n\n@@ -1,179 +0,0 @@\n-from twisted.internet import defer\n-\n-from scrapy.utils.defer import mustbe_deferred, defer_result\n-from scrapy import log\n-from scrapy.core.engine import scrapyengine\n-from scrapy.spider import spiders\n-\n-\n-class DomainInfo(object):\n-    def __init__(self, domain):\n-        self.domain = domain\n-        self.spider = spiders.fromdomain(domain)\n-        self.downloading = {}\n-        self.downloaded = {}\n-        self.waiting = {}\n-        self.extra = {}\n-\n-\n-class MediaPipeline(object):\n-    def __init__(self):\n-        self.domaininfo = {}\n-\n-    def open_domain(self, domain):\n-        self.domaininfo[domain] = DomainInfo(domain)\n-\n-    def close_domain(self, domain):\n-        del self.domaininfo[domain]\n-\n-    def process_item(self, domain, response, item):\n-        info = self.domaininfo[domain]\n-        requests = self.get_media_requests(item, info)\n-        assert requests is None or hasattr(requests, '__iter__'), \\\n-                'get_media_requests should return None or iterable'\n-\n-        def _bugtrap(_failure, request):\n-            log.msg('Unhandled ERROR in MediaPipeline.item_media_{downloaded,failed} for %s: %s' % (request, _failure), log.ERROR, domain=domain)\n-\n-        lst = []\n-        for request in requests or ():\n-            dfd = self._enqueue(request, info)\n-            dfd.addCallbacks(\n-                    callback=self.item_media_downloaded,\n-                    callbackArgs=(item, request, info),\n-                    errback=self.item_media_failed,\n-                    errbackArgs=(item, request, info),\n-                    )\n-            dfd.addErrback(_bugtrap, request)\n-            lst.append(dfd)\n-\n-        dlst = defer.DeferredList(lst, consumeErrors=False)\n-        dlst.addBoth(lambda _: self.item_completed(item, info))\n-        return dlst\n-\n-    def _enqueue(self, request, info):\n-        wad = request.deferred or defer.Deferred()\n-\n-        fp = request.fingerprint()\n-        if fp in info.downloaded:\n-            cached = info.downloaded[fp]\n-            defer_result(cached).chainDeferred(wad)\n-        else:\n-            info.waiting.setdefault(fp, []).append(wad)\n-            if fp not in info.downloading:\n-                self._download(request, info, fp)\n-\n-        return wad\n-\n-    def _download(self, request, info, fp):\n-        def _bugtrap(_failure):\n-            log.msg('Unhandled ERROR in MediaPipeline._downloaded: %s' % (_failure), log.ERROR, domain=info.domain)\n-\n-        result = self.media_to_download(request, info)\n-        if result is not None:\n-            dwld = defer_result(result)\n-        else:\n-            dwld = mustbe_deferred(self.download, request, info)\n-            dwld.addCallbacks(\n-                    callback=self.media_downloaded,\n-                    callbackArgs=(request, info),\n-                    errback=self.media_failed,\n-                    errbackArgs=(request, info),\n-                    )\n-\n-        dwld.addBoth(self._downloaded, info, fp)\n-        dwld.addErrback(_bugtrap)\n-        info.downloading[fp] = (request, dwld)\n-\n-    def _downloaded(self, result, info, fp):\n-        info.downloaded[fp] = result # cache result\n-        waiting = info.waiting[fp] # client list\n-        del info.waiting[fp]\n-        del info.downloading[fp]\n-        for wad in waiting:\n-            defer_result(result).chainDeferred(wad)\n-\n-\n-    ### Overradiable Interface\n-    def download(self, request, info):\n-        \"\"\" Defines how to request the download of media\n-\n-        Default gives high priority to media requests and use scheduler,\n-        shouldn't be necessary to override.\n-\n-        This methods is called only if result for request isn't cached,\n-        request fingerprint is used as cache key.\n-\n-        \"\"\"\n-        return scrapyengine.schedule(request, info.spider, priority=0)\n-\n-    def media_to_download(self, request, info):\n-        \"\"\" Ongoing request hook pre-cache\n-\n-        This method is called every time a media is requested for download, and\n-        only once for the same request because return value is cached as media\n-        result.\n-\n-        returning a non-None value implies:\n-            - the return value is cached and piped into `item_media_downloaded` or `item_media_failed`\n-            - prevents downloading, this means calling `download` method.\n-            - `media_downloaded` or `media_failed` isn't called.\n-\n-        \"\"\"\n-\n-    def get_media_requests(self, item, info):\n-        \"\"\" Return a list of Request objects to download for this item\n-\n-        Should return None or an iterable\n-\n-        Defaults return None (no media to download)\n-\n-        \"\"\"\n-\n-    def media_downloaded(self, response, request, info):\n-        \"\"\" Method called on success download of media request\n-\n-        Return value is cached and used as input for `item_media_downloaded` method.\n-        Default implementation returns None.\n-\n-        WARNING: returning the response object can eat your memory.\n-\n-        \"\"\"\n-\n-    def media_failed(self, failure, request, info):\n-        \"\"\" Method called when media request failed due to any kind of download error.\n-\n-        Return value is cached and used as input for `item_media_failed` method.\n-        Default implementation returns same Failure object.\n-        \"\"\"\n-        return failure\n-\n-    def item_media_downloaded(self, result, item, request, info):\n-        \"\"\" Method to handle result of requested media for item.\n-\n-        result is the return value of `media_downloaded` hook, or the non-Failure instance\n-        returned by `media_failed` hook.\n-\n-        return value of this method isn't important and is recommended to return None.\n-        \"\"\"\n-\n-    def item_media_failed(self, failure, item, request, info):\n-        \"\"\" Method to handle failed result of requested media for item.\n-\n-        result is the returned Failure instance of `media_failed` hook, or Failure instance\n-        of an exception raised by `media_downloaded` hook.\n-\n-        return value of this method isn't important and is recommended to return None.\n-        \"\"\"\n-\n-    def item_completed(self, item, info):\n-        \"\"\" Method called when all media requests for a single item has returned a result or failure.\n-\n-        The return value of this method is used as output of pipeline stage.\n-\n-        `item_completed` can return item itself or raise DropItem exception.\n-\n-        Default returns item\n-        \"\"\"\n-        return item\n-\n\n@@ -1,154 +0,0 @@\n-import time\n-import hashlib\n-import rfc822\n-from cStringIO import StringIO\n-\n-import Image\n-import boto\n-\n-from scrapy import log\n-from scrapy.stats import stats\n-from scrapy.core.exceptions import DropItem, NotConfigured\n-from scrapy.core.exceptions import HttpException\n-from scrapy.contrib.pipeline.media import MediaPipeline\n-from scrapy.conf import settings\n-\n-class NoimagesDrop(DropItem):\n-    \"\"\"Product with no images exception\"\"\"\n-\n-class ImageException(Exception):\n-    \"\"\"General image error exception\"\"\"\n-\n-class S3ImagesPipeline(MediaPipeline):\n-    MEDIA_TYPE = 'image'\n-    THUMBS = (\n-        (\"50\", (50, 50)),\n-        (\"110\", (110, 110)),\n-        (\"270\", (270, 270))\n-    )\n-\n-    def __init__(self):\n-        if not settings['S3_IMAGES']:\n-            raise NotConfigured\n-\n-        # days to wait before redownloading images\n-        self.image_refresh_days = settings.getint('IMAGES_REFRESH_DAYS', 90)\n-        \n-        self.bucket_name = settings['S3_BUCKET']\n-        self.prefix = settings['S3_PREFIX']\n-        access_key = settings['AWS_ACCESS_KEY_ID']\n-        secret_key = settings['AWS_SECRET_ACCESS_KEY']\n-        conn = boto.connect_s3(access_key, secret_key)\n-        self.bucket = conn.get_bucket(self.bucket_name)\n-\n-        MediaPipeline.__init__(self)\n-\n-    def media_to_download(self, request, info):\n-        key = self.s3_image_key(request.url)\n-        if not self.s3_should_download(request.url):\n-            self.inc_stats(info.domain, 'uptodate')\n-            referer = request.headers.get('Referer')\n-            log.msg('Image (uptodate) type=%s at <%s> referred from <%s>' % \\\n-                    (self.MEDIA_TYPE, request.url, referer), level=log.DEBUG, domain=info.domain)\n-            return key\n-\n-    def media_downloaded(self, response, request, info):\n-        mtype = self.MEDIA_TYPE\n-        referer = request.headers.get('Referer')\n-\n-        if not response or not response.body.to_string():\n-            msg = 'Image (empty): Empty %s (no content) in %s referred in <%s>: Empty image (no-content)' % (mtype, request, referer)\n-            log.msg(msg, level=log.WARNING, domain=info.domain)\n-            raise ImageException(msg)\n-\n-        result = self.save_image(response, request, info) # save and thumbs response\n-\n-        status = 'cached' if getattr(response, 'cached', False) else 'downloaded'\n-        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % (status, mtype, request, referer)\n-        log.msg(msg, level=log.DEBUG, domain=info.domain)\n-        self.inc_stats(info.domain, status)\n-        return result\n-\n-    def media_failed(self, failure, request, info):\n-        referer = request.headers.get('Referer')\n-        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)\n-        msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' % (self.MEDIA_TYPE, request, referer, errmsg)\n-        log.msg(msg, level=log.WARNING, domain=info.domain)\n-        raise ImageException(msg)\n-\n-    def save_image(self, response, request, info):\n-        try:\n-            key = self.s3_image_key(request.url)\n-            self.s3_store_image(response, request.url)\n-        except ImageException, ex:\n-            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n-            raise ex\n-        except Exception, ex:\n-            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n-            raise ex\n-\n-        return key # success value sent as input result for item_media_downloaded\n-\n-    def inc_stats(self, domain, status):\n-        stats.incpath('%s/image_count' % domain)\n-        stats.incpath('%s/image_status_count/%s' % (domain, status))\n-\n-    def s3_image_key(self, url):\n-        \"\"\"Return the relative path on the target filesystem for an image to be\n-        downloaded to.\n-        \"\"\"\n-        image_guid = hashlib.sha1(url).hexdigest()\n-        return '%s/full/%s.jpg' % (self.prefix, image_guid)\n-\n-    def s3_thumb_key(self, url, thumb_id):\n-        \"\"\"Return the relative path on the target filesystem for an image to be\n-        downloaded to.\n-        \"\"\"\n-        image_guid = hashlib.sha1(url).hexdigest()\n-        return '%s/thumbs/%s/%s.jpg' % (self.prefix, thumb_id, image_guid)\n-\n-    def s3_should_download(self, url):\n-        \"\"\"Return if the image should be downloaded by checking if it's already in\n-        the S3 storage and not too old\"\"\"\n-        key = self.s3_image_key(url)\n-        k = self.bucket.get_key(key)\n-        if k is None:\n-            return True\n-        modified_tuple = rfc822.parsedate_tz(k.last_modified)\n-        modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n-        age_seconds = time.time() - modified_stamp\n-        age_days = age_seconds / 60 / 60 / 24\n-        return age_days > self.image_refresh_days\n-\n-    def s3_store_image(self, response, url):\n-        \"\"\"Upload image to S3 storage\"\"\"\n-        buf = StringIO(response.body.to_string())\n-        image = Image.open(buf)\n-        key = self.s3_image_key(url)\n-        self._s3_put_image(image, key)\n-        self.s3_store_thumbnails(image, url)\n-\n-    def s3_store_thumbnails(self, image, url):\n-        \"\"\"Upload image thumbnails to S3 storage\"\"\"\n-        for thumb_id, size in self.THUMBS or []:\n-            thumb = image.copy() if image.mode == 'RGB' else image.convert('RGB')\n-            thumb.thumbnail(size, Image.ANTIALIAS)\n-            key = self.s3_thumb_key(url, thumb_id)\n-            self._s3_put_image(thumb, key)\n-\n-    def s3_public_url(self, key):\n-        return \"http://%s.s3.amazonaws.com/%s\" % (self.bucket_name, key)\n-\n-    def _s3_put_image(self, image, key):\n-        buf = StringIO()\n-        try:\n-            image.save(buf, 'JPEG')\n-        except Exception, ex:\n-            raise ImageException(\"Cannot process image. Error: %s\" % ex)\n-\n-        buf.seek(0)\n-        k = self.bucket.new_key(key)\n-        k.content_type = 'image/jpeg'\n-        k.set_contents_from_file(buf, policy='public-read')\n-        log.msg(\"Uploaded to S3: %s\" % self.s3_public_url(key), level=log.DEBUG)\n-\n\n@@ -1,37 +0,0 @@\n-\"\"\"\n-A pipeline to persist objects using shove. \n-\n-New is a \"new generation\" shelve. For more information see: \n-http://pypi.python.org/pypi/shove\n-\"\"\"\n-\n-from string import Template\n-\n-from shove import Shove\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy.conf import settings\n-\n-class ShoveItemPipeline(object):\n-\n-    def __init__(self):\n-        self.uritpl = settings['SHOVEITEM_STORE_URI']\n-        if not self.uritpl:\n-            raise NotConfigured\n-        self.opts = settings['SHOVEITEM_STORE_OPT'] or {}\n-        self.stores = {}\n-\n-        dispatcher.connect(self.domain_open, signal=signals.domain_open)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n-\n-    def process_item(self, domain, response, item):\n-        self.stores[domain][str(item.guid)] = item\n-        return item\n-\n-    def domain_open(self, domain):\n-        uri = Template(self.uritpl).substitute(domain=domain)\n-        self.stores[domain] = Shove(uri, **self.opts)\n-\n-    def domain_closed(self, domain):\n-        self.stores[domain].sync()\n\n@@ -1,15 +0,0 @@\n-\"\"\"\n-Pipeline to print Items\n-\"\"\"\n-from scrapy import log\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-\n-class ShowItemPipeline(object):\n-    def __init__(self):\n-        if not settings['DEBUG_SHOWITEM']:\n-            raise NotConfigured\n-\n-    def process_item(self, domain, response, item):\n-        log.msg(\"Scraped: \\n%s\" % repr(item), log.DEBUG, domain=domain)\n-        return item\n\n@@ -1,26 +0,0 @@\n-\"\"\"\n-The ResponseSoup extension causes the Response objects to grow a new method\n-(\"getsoup\") which returns a (cached) BeautifulSoup object of its body, and a\n-\"soup\" attribute with the same effect. The soup argument is provided for\n-convenience, but you cannot pass any BeautifulSoup constructor arguments (which\n-you can do with the getsoup() method).\n-\n-For more information about BeautifulSoup see:\n-http://www.crummy.com/software/BeautifulSoup/documentation.html\n-\"\"\" \n-\n-from BeautifulSoup import BeautifulSoup\n-\n-from scrapy.http import Response\n-\n-\n-class ResponseSoup(object):\n-    def __init__(self):\n-        setattr(Response, 'getsoup', getsoup)\n-        setattr(Response, 'soup', property(getsoup))\n-\n-def getsoup(response, **kwargs):\n-    if not hasattr(response, '_soup'):\n-        body = response.body.to_string() if response.body is not None else \"\"\n-        setattr(response, '_soup', BeautifulSoup(body, **kwargs))\n-    return response._soup\n\n@@ -1,36 +0,0 @@\n-\"\"\"\n-DepthMiddleware is a scrape middleware used for tracking the depth of each\n-Request inside the site being scraped. It can be used to limit the maximum\n-depth to scrape or things like that\n-\"\"\"\n-\n-from scrapy import log\n-from scrapy.http import Request\n-from scrapy.stats import stats\n-from scrapy.conf import settings\n-\n-class DepthMiddleware(object):\n-\n-    def __init__(self):\n-        self.maxdepth = settings.getint('DEPTH_LIMIT')\n-        self.stats = settings.getbool('DEPTH_STATS')\n-        if self.stats and self.maxdepth:\n-            stats.setpath('_envinfo/request_depth_limit', self.maxdepth)\n-\n-    def process_result(self, response, result, spider):\n-        def _filter(request):\n-            if isinstance(request, Request):\n-                request.depth = response.request.depth + 1\n-                if self.maxdepth and request.depth > self.maxdepth:\n-                    log.msg(\"Ignoring link (depth > %d): %s \" % (self.maxdepth, request.url), level=log.DEBUG, domain=spider.domain_name)\n-                    return False\n-                elif self.stats:\n-                    stats.incpath('%s/request_depth_count/%s' % (spider.domain_name, request.depth))\n-                    if request.depth > stats.getpath('%s/request_depth_max' % spider.domain_name, 0):\n-                        stats.setpath('%s/request_depth_max' % spider.domain_name, request.depth)\n-            return True\n-\n-        if self.stats and response.request.depth == 0: # otherwise we loose stats for depth=0 \n-            stats.incpath('%s/request_depth_count/0' % spider.domain_name)\n-\n-        return (r for r in result or () if _filter(r))\n\n@@ -1,37 +0,0 @@\n-\"\"\"\n-RequestLimitMiddleware: Limits the scheduler request queue from the point of\n-view of the spider. That is, if the scheduler queue contains an equal or\n-greater ammount of requests than the specified limit, the new requests\n-(generated by the spider) will be ignored.\n-\n-The limit is setted from the spider attribute \"requests_queue_size\". If not\n-found, from the scrapy setting \"REQUESTS_QUEUE_SIZE\". If not found, no limit\n-will be applied. If given a value of 0, no limit will be applied.\n-\"\"\"\n-\n-from scrapy.core.engine import scrapyengine\n-from scrapy.conf import settings\n-from scrapy.http import Request\n-from scrapy import log\n-\n-class RequestLimitMiddleware(object):\n-    #_last_queue_size = 0\n-    def process_result(self, response, result, spider):\n-        requests = []\n-        other = []\n-        [requests.append(r) if isinstance(r, Request) else other.append(r) for r in result]\n-\n-        max_pending = spider.requests_queue_size if hasattr(spider,\"requests_queue_size\") else settings.getint(\"REQUESTS_QUEUE_SIZE\")\n-        if not max_pending:\n-            accepted = requests\n-        else:\n-            free_slots = max_pending - len(scrapyengine.scheduler.pending_requests[spider.domain_name])\n-            accepted = requests[:free_slots]\n-            dropped = set(requests) - set(accepted)\n-            if dropped:\n-                for r in dropped:\n-                    log.msg(\"Ignoring link (max schedule queue size reached): %s \" % r.url, level=log.WARNING, domain=spider.domain_name)\n-        #actual_size = len(scrapyengine.scheduler.pending_requests[spider.domain_name])\n-        #log.msg(\"queue size: %d (%+d)\" % (actual_size, actual_size - self._last_queue_size) )\n-        #self._last_queue_size = actual_size\n-        return accepted + other\n\n@@ -1,18 +0,0 @@\n-\"\"\"\n-OffsiteMiddleware: Filters out Requests for URLs outside the domains covered by\n-the spider.\n-\"\"\"\n-\n-from scrapy import log\n-from scrapy.http import Request\n-from scrapy.utils.url import url_is_from_spider\n-\n-class OffsiteMiddleware(object):\n-    def process_result(self, response, result, spider):\n-        def _filter(r):\n-            if isinstance(r, Request) and not url_is_from_spider(r.url, spider):\n-                log.msg(\"Ignoring link (offsite): %s \" % r.url, level=log.DEBUG, domain=spider.domain_name)\n-                return False\n-            return True\n-        return (r for r in result or () if _filter(r))\n-\n\n@@ -1,15 +0,0 @@\n-\"\"\"\n-RefererMiddleware: populates Request referer field, based on the Response which\n-originated it.\n-\"\"\"\n-\n-from scrapy.http import Request\n-\n-class RefererMiddleware(object):\n-    def process_result(self, response, result, spider):\n-        def _set_referer(r):\n-            if isinstance(r, Request):\n-                r.headers.setdefault('Referer', response.url)\n-            return r\n-        return (_set_referer(r) for r in result or ())\n-\n\n@@ -1,21 +0,0 @@\n-\"\"\"\n-RestrictMiddleware: restricts crawling to fixed set of particular URLs\n-\"\"\"\n-\n-from scrapy.http import Request\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-\n-class RestrictMiddleware(object):\n-    def __init__(self):\n-        self.allowed_urls = set(settings.getlist('RESTRICT_TO_URLS'))\n-        if not self.allowed_urls:\n-            raise NotConfigured\n-\n-    def process_result(self, response, result, spider):\n-        def _filter(r):\n-            if isinstance(r, Request) and r.url not in self.allowed_urls:\n-                return False\n-            return True\n-        return (r for r in result or () if _filter(r))\n-\n\n@@ -1,17 +0,0 @@\n-\"\"\"\n-UrlFilterMiddleware: canonicalizes URLs to filter out duplicated ones\n-\"\"\"\n-\n-from scrapy.http import Request\n-from scrapy.utils.url import canonicalize_url\n-\n-class UrlFilterMiddleware(object):\n-    def process_result(self, response, result, spider):\n-        disabled = getattr(spider, 'urlfilter_disabled', False)\n-        for r in result:\n-            if isinstance(r, Request) and not disabled:\n-                curl = canonicalize_url(r.url)\n-                # only assign if different to avoid re-calculating fingerprint\n-                if curl != r.url: \n-                    r.url = curl\n-            yield r\n\n@@ -1,24 +0,0 @@\n-\"\"\"\n-UrlLengthMiddleware: Filters out requests with URLs longer than URLLENGTH_LIMIT\n-\"\"\"\n-\n-from scrapy import log\n-from scrapy.http import Request\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-\n-class UrlLengthMiddleware(object):\n-    def __init__(self):\n-        self.maxlength = settings.getint('URLLENGTH_LIMIT')\n-        if not self.maxlength:\n-            raise NotConfigured\n-\n-    def process_result(self, response, result, spider):\n-        def _filter(request):\n-            if isinstance(request, Request) and len(request.url) > self.maxlength:\n-                log.msg(\"Ignoring link (url length > %d): %s \" % (self.maxlength, request.url), level=log.DEBUG, domain=spider.domain_name)\n-                return False\n-            else:\n-                return True\n-\n-        return (r for r in result or () if _filter(r))\n\n@@ -1,60 +0,0 @@\n-\"\"\"\n-This module contains BasicSpider, a spider class which provides support for\n-basic crawling.\n-\"\"\"\n-\n-from scrapy.http import Request\n-from scrapy.spider import BaseSpider\n-from scrapy.core.exceptions import UsageError\n-\n-class BasicSpider(BaseSpider):\n-    \"\"\"BasicSpider extends BaseSpider by providing support for simple crawling\n-    by following links contained in web pages. \n-    \n-    With BasicSpider you can write a basic spider very easily and quickly. For\n-    more information refer to the Scrapy tutorial\"\"\"\n-\n-    def __init__(self):\n-        super(BaseSpider, self).__init__()\n-        \n-        self._links_callback = []\n-\n-        for attr in dir(self):\n-            if attr.startswith('links_'):\n-                suffix = attr.split('_', 1)[1]\n-                value = getattr(self, attr)\n-                try:\n-                    callback = getattr(self, 'parse_%s' % suffix)\n-                except AttributeError:\n-                    raise UsageError(\"%s defines links_%s but doesn't provide a parse_%s method\" % \\\n-                                     (type(self).__name__, suffix, suffix))\n-                self._links_callback.append((value, callback))\n-\n-    def parse(self, response):\n-        \"\"\"This function is called by the core for all the start_urls. Do not\n-        override this function, override parse_start_url instead.\"\"\"\n-        return self._parse_wrapper(response, self.parse_start_url)\n-\n-    def parse_start_url(self, response):\n-        \"\"\"Callback function for processing start_urls. It must return a list\n-        of ScrapedItems and/or Requests.\"\"\"\n-        return []\n-\n-    def _links_to_follow(self, response):\n-        res = []\n-        links_to_follow = {}\n-        for lx, callback in self._links_callback:\n-            for url, link_text in lx.extract_urls(response).iteritems():\n-                links_to_follow[url] = (callback, link_text)\n-\n-        for url, cb_link in links_to_follow.iteritems():\n-            request = Request(url=url, link_text=link_text)\n-            request.append_callback(self._parse_wrapper, callback)\n-            res.append(request)\n-        return res\n-\n-    def _parse_wrapper(self, response, callback):\n-        res = self._links_to_follow(response)\n-        res += callback(response) or ()\n-        return res\n-\n\n@@ -1,82 +0,0 @@\n-\"\"\"\n-Live statistics extension\n-\"\"\"\n-from datetime import datetime\n-from pydispatch import dispatcher\n-from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n-from scrapy.management.web import banner\n-\n-class SpiderStats(object):\n-    def __init__(self):\n-        self.scraped = 0\n-        self.crawled = 0\n-        self.started = None\n-        self.finished = None\n-\n-class LiveStats(object):\n-    webconsole_id = 'livestats'\n-    webconsole_name = 'Spider live statistics of current run'\n-\n-    def __init__(self):\n-        self.domains = {}\n-        dispatcher.connect(self.domain_open, signal=signals.domain_open)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n-        dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n-        dispatcher.connect(self.response_downloaded, signal=signals.response_downloaded)\n-\n-        from scrapy.management.web import webconsole_discover_module\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def domain_open(self, domain, spider):\n-        pstats = SpiderStats()\n-        self.domains[spider.domain_name] = pstats\n-        pstats.started = datetime.now()\n-        pstats.finished = None\n-\n-    def domain_closed(self, domain, spider):\n-        self.domains[spider.domain_name].finished = datetime.now()\n-\n-    def item_scraped(self, item, spider):\n-        self.domains[spider.domain_name].scraped += 1\n-\n-    def response_downloaded(self, response, spider):\n-        #sometimes we download responses without opening/closing domains,\n-        #for example from scrapy shell\n-        if self.domains.get(spider.domain_name):\n-            self.domains[spider.domain_name].crawled += 1\n-            \n-    def webconsole_render(self, wc_request):\n-        sch = scrapyengine.scheduler\n-        dwl = scrapyengine.downloader\n-\n-        totdomains = totscraped = totcrawled = totscheduled = totactive = totpending = 0\n-        s = banner(self)\n-        s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>Domain</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Pending</th><th>Downloader<br/>Active</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\\n\"\n-        for d in sorted(self.domains.keys()):\n-            scheduled = len(sch.pending_requests[d]) if d in sch.pending_requests else 0\n-            active, pending = len(dwl.active_requests(d)), len(dwl.request_queue(d))\n-            stats = self.domains[d]\n-            runtime = stats.finished - stats.started if stats.finished else datetime.now() - stats.started\n-\n-            s += '<tr><td>%s</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td>%s</td><td>%s</td><td>%s</td></tr>\\n' % \\\n-                 (d, stats.scraped, stats.crawled, scheduled, pending, active, str(stats.started), str(stats.finished), str(runtime))\n-\n-            totdomains += 1\n-            totscraped += stats.scraped\n-            totcrawled += stats.crawled\n-            totscheduled += scheduled\n-            totactive += active\n-            totpending += pending\n-        s += '<tr><td><b>%d domains</b></td><td align=\"right\"><b>%d</b></td><td align=\"right\"><b>%d</b></td><td align=\"right\"><b>%d</b></td><td align=\"right\"><b>%d</b></td><td align=\"right\"><b>%d</b></td><td/><td/></tr>\\n' % \\\n-             (totdomains, totscraped, totcrawled, totscheduled, totactive, totpending)\n-        s += \"</table>\\n\"\n-\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return s\n-\n-    def webconsole_discover_module(self):\n-        return self\n\n@@ -1,41 +0,0 @@\n-\"\"\"\n-Scheduler information module for Scrapy webconsole\n-\"\"\"\n-from pydispatch import dispatcher\n-from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n-from scrapy.spider import spiders\n-from scrapy.management.web import banner\n-\n-class SchedulerStats(object):\n-    webconsole_id = 'scheduler'\n-    webconsole_name = 'Scheduler queue'\n-\n-    def __init__(self):\n-        from scrapy.management.web import webconsole_discover_module\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_render(self, wc_request):\n-        s = banner(self)\n-        s += \"<ul>\\n\"\n-        for domain, requests in scrapyengine.scheduler.pending_requests.iteritems():\n-            s += \"<li>\\n\"\n-            s += \"%s (<b>%s</b> pages)\\n\" % (domain, len(requests))\n-            s += \"<ul>\\n\"\n-            # requests is a tuple of request and deffered now, as I understand\n-            for r, d in requests:\n-                if hasattr(r, 'url'):\n-                    s += \"<li><a href='%s'>%s</a></li>\\n\" % (r.url, r.url)\n-                #else:\n-                #    s += \"<li>%s</li>\\n\" % (repr(r))\n-            s += \"</ul>\\n\"\n-            s += \"</li>\\n\" \n-        s += \"</ul>\\n\"\n-\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return s\n-\n-    def webconsole_discover_module(self):\n-        return self\n\n@@ -1,208 +0,0 @@\n-\"\"\"\n-Extensions for allowing spider control from web console\n-\"\"\"\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy.core.manager import scrapymanager\n-from scrapy.core.engine import scrapyengine\n-from scrapy.spider import spiders\n-from scrapy.management.web import banner\n-from scrapy.conf import settings\n-\n-class Spiderctl(object):\n-    webconsole_id = 'spiderctl'\n-    webconsole_name = 'Spider control panel'\n-\n-    def __init__(self):\n-        self.running = set()\n-        self.finished = set()\n-        dispatcher.connect(self.domain_open, signal=signals.domain_open)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n-\n-        from scrapy.management.web import webconsole_discover_module\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def domain_open(self, domain, spider):\n-        self.running.add(domain)\n-\n-    def domain_closed(self, domain, spider):\n-        if domain in self.running:\n-            self.running.remove(domain)\n-        self.finished.add(domain)\n-\n-    def webconsole_render(self, wc_request):\n-        if wc_request.args:\n-            changes = self.webconsole_control(wc_request)\n-\n-        enabled_domains = spiders.asdict(include_disabled=False).keys()\n-        self.scheduled = set(scrapyengine.scheduler.pending_domains_count)\n-        self.not_scheduled = [d for d in enabled_domains if d not in self.scheduled\n-                                                        and d not in self.running\n-                                                        and d not in self.finished]\n-\n-        s = banner(self)\n-        s += '<p><a href=\"?reloadspiders=1\">Reload spiders</a></p>\\n'\n-        s += '<table border=1\">\\n'\n-        s += \"<tr><th>Running (%d/%d)</th><th>Scheduled (%d)</th><th>Finished (%d)</th><th>Not scheduled (%d)</th></tr>\\n\" % \\\n-                (len(self.running),\n-                 settings['CONCURRENT_DOMAINS'],\n-                 len(self.scheduled),\n-                 len(self.finished),\n-                 len(self.not_scheduled))\n-        s += \"<tr>\\n\"\n-\n-        # running\n-        s += \"<td valign='top'>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += '<select name=\"stop_running_domains\" multiple=\"multiple\">\\n'\n-        for domain in sorted(self.running):\n-            s += \"<option>%s</option>\\n\" % domain\n-        s += '</select><br>\\n'\n-        s += '<br />'\n-        s += '<input type=\"submit\" value=\"Stop selected\">\\n'\n-        s += '</form>\\n'\n-        s += \"</td>\\n\"\n-\n-        # scheduled\n-        s += \"<td valign='top'>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += '<select name=\"remove_pending_domains\" multiple=\"multiple\">\\n'\n-        for domain in sorted(self.scheduled):\n-            s += \"<option>%s</option>\\n\" % domain\n-        s += '</select><br>\\n'\n-        s += '<br />'\n-        s += '<input type=\"submit\" value=\"Remove selected\">\\n'\n-        s += '</form>\\n'\n-\n-        s += \"</td>\\n\"\n-\n-        # finished\n-        s += \"<td valign='top'>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += '<select name=\"rerun_finished_domains\" multiple=\"multiple\">\\n'\n-        for domain in sorted(self.finished):\n-            s += \"<option>%s</option>\\n\" % domain\n-        s += '</select><br>\\n'\n-        s += '<br />'\n-        s += '<input type=\"submit\" value=\"Re-schedule selected\">\\n'\n-        s += '</form>\\n'\n-        s += \"</td>\\n\"\n-\n-        # not scheduled\n-        s += \"<td valign='top'>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += '<select name=\"add_pending_domains\" multiple=\"multiple\">\\n'\n-        for domain in sorted(self.not_scheduled):\n-            s += \"<option>%s</option>\\n\" % domain\n-        s += '</select><br>\\n'\n-        s += '<br />'\n-        s += '<input type=\"submit\" value=\"Schedule selected\">\\n'\n-        s += '</form>\\n'\n-        s += \"</td>\\n\"\n-\n-        s += \"</tr>\\n\"\n-        s += \"<tr>\\n\"\n-\n-        s += \"<td>&nbsp;</td>\\n\"\n-\n-        s += \"<td>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += \"<textarea name='bulk_remove_domains' rows='10' cols='25'></textarea>\\n\"\n-        s += '<br>\\n'\n-        s += '<input type=\"submit\" value=\"Bulk remove domains\">\\n'\n-        s += '</form>\\n'\n-        s += '</td>\\n'\n-\n-        s += \"<td>&nbsp;</td>\\n\"\n-\n-        s += \"<td>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += \"<textarea name='bulk_schedule_domains' rows='10' cols='25'></textarea>\\n\"\n-        s += '<br>\\n'\n-        s += '<input type=\"submit\" value=\"Bulk schedule domains\">\\n'\n-        s += '</form>\\n'\n-        s += \"</td>\\n\"\n-\n-        s += \"</tr>\\n\"\n-\n-        s += \"</table>\\n\"\n-\n-        if wc_request.args:\n-            s += changes\n-\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return s\n-\n-    def webconsole_control(self, wc_request):\n-        args = wc_request.args\n-        s = \"<hr />\\n\"\n-        if \"reloadspiders\" in args:\n-            scrapymanager.reload_spiders()\n-            s += \"<p><b>Spiders reloaded</b></p>\\n\"\n-            return s\n-\n-        if \"stop_running_domains\" in args:\n-            s += \"<p>\"\n-            s += \"Stopped spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"stop_running_domains\"])\n-            for domain in args[\"stop_running_domains\"]:\n-                scrapyengine.close_domain(domain)\n-            s += \"</p>\"\n-        if \"remove_pending_domains\" in args:\n-            removed = []\n-            for domain in args[\"remove_pending_domains\"]:\n-                if scrapyengine.scheduler.remove_pending_domain(domain):\n-                    removed.append(domain)\n-            if removed:\n-                s += \"<p>\"\n-                s += \"Removed scheduled spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"remove_pending_domains\"])\n-                s += \"</p>\"\n-        if \"bulk_remove_domains\" in args:\n-            scheduled = []\n-            to_remove = set([d.strip() for d in args[\"bulk_remove_domains\"][0].split(\"\\n\")])\n-            removed = set([d for d in scrapyengine.scheduler.pending_domains if d in to_remove])\n-            scrapyengine.scheduler.pending_domains = [d for d in execengine.scheduler.pending_domains if d not in removed]\n-            if removed:\n-                s += \"<p>\"\n-                s += \"Removed: <ul><li>%s</li></ul>\" % \"</li><li>\".join(removed)\n-                s += \"</p>\"\n-                s += \"<p>\"\n-                s += \"Not removed: <ul><li>%s</li></ul>\" % \"</li><li>\".join(to_remove - removed)\n-                s += \"</p>\"\n-        if \"add_pending_domains\" in args:\n-            for domain in args[\"add_pending_domains\"]:\n-                if domain not in scrapyengine.scheduler.pending_domains_count:\n-                    scrapymanager.crawl(domain)\n-            s += \"<p>\"\n-            s += \"Scheduled spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"add_pending_domains\"])\n-            s += \"</p>\"\n-        if \"bulk_schedule_domains\" in args:\n-            to_schedule = set([d.strip() for d in args[\"bulk_schedule_domains\"][0].split(\"\\n\")])\n-            scheduled = set()\n-            for domain in to_schedule:\n-                if domain in enabled_domains and domain not in scrapyengine.scheduler.pending_domains_count:\n-                    scrapymanager.crawl(domain)\n-                    scheduled.add(domain)\n-            if scheduled:\n-                s += \"<p>\"\n-                s += \"Scheduled: <ul><li>%s</li></ul>\" % \"</li><li>\".join(scheduled)\n-                s += \"</p>\"\n-                s += \"<p>\"\n-                s += \"Not scheduled: <ul><li>%s</li></ul>\" % \"</li><li>\".join(to_schedule - scheduled)\n-                s += \"</p>\"\n-        if \"rerun_finished_domains\" in args:\n-            for domain in args[\"rerun_finished_domains\"]:\n-                if domain not in scrapyengine.scheduler.pending_domains_count:\n-                    scrapymanager.crawl(domain)\n-                self.finished.remove(domain)\n-            s += \"<p>\"\n-            s += \"Re-scheduled finished spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"rerun_finished_domains\"])\n-            s += \"</p>\"\n-\n-        return s\n-        \n-    def webconsole_discover_module(self):\n-        return self\n\n@@ -1,78 +0,0 @@\n-\"\"\"\n-Download handlers for different schemes\n-\"\"\"\n-from __future__ import with_statement\n-\n-import os\n-import urlparse\n-\n-from twisted.web.client import HTTPClientFactory\n-from twisted.internet import defer, reactor\n-from twisted.web import error as web_error\n-\n-from scrapy.core import signals\n-from scrapy.http import Request, Response, Headers\n-from scrapy.core.exceptions import UsageError, HttpException\n-from scrapy.utils.defer import defer_succeed\n-from scrapy.conf import settings\n-\n-def download_any(request, spider):\n-    u = urlparse.urlparse(request.url)\n-    if u.scheme == 'file':\n-        return download_file(request, spider)\n-    elif u.scheme in ('http', 'https'):\n-        return download_http(request, spider)\n-    else:\n-        raise UsageError(\"Unsupported scheme '%s' in URL: <%s>\" % (u.scheme, request.url))\n-\n-def download_http(request, spider):\n-    \"\"\"This functions handles http/https downloads\"\"\"\n-    url = urlparse.urldefrag(request.url)[0]\n-    \n-    agent = request.headers.get('user-agent', settings.get('USER_AGENT'))\n-    request.headers.pop('user-agent', None)  # remove user-agent if already present\n-    factory = HTTPClientFactory(url=str(url), # never pass unicode urls to twisted\n-                                method=request.method,\n-                                postdata=request.body,\n-                                headers=request.headers,\n-                                agent=agent,\n-                                cookies=request.cookies,\n-                                timeout=getattr(spider, \"download_timeout\", None) or settings.getint('DOWNLOAD_TIMEOUT'),\n-                                followRedirect=False)\n-\n-    def _response(body):\n-        body = body or ''\n-        status = factory.status\n-        headers = Headers(factory.response_headers)\n-        r = Response(domain=spider.domain_name, url=request.url, headers=headers, status=status, body=body)\n-        signals.send_catch_log(signal=signals.request_uploaded, sender='download_http', request=request, spider=spider)\n-        signals.send_catch_log(signal=signals.response_downloaded, sender='download_http', response=r, spider=spider)\n-        return r\n-\n-    def _on_success(body):\n-        return _response(body)\n-\n-    def _on_error(_failure):\n-        ex = _failure.value\n-        if isinstance(ex, web_error.Error): # HttpException\n-            raise HttpException(ex.status, ex.message, _response(ex.response))\n-        return _failure\n-\n-    factory.noisy = False\n-    factory.deferred.addCallbacks(_on_success, _on_error)\n-\n-    u = urlparse.urlparse(request.url)\n-    if u.scheme == 'https' :\n-        from twisted.internet import ssl\n-        contextFactory = ssl.ClientContextFactory()\n-        reactor.connectSSL(u.hostname, u.port or 443, factory, contextFactory)\n-    else:\n-        reactor.connectTCP(u.hostname, u.port or 80, factory)\n-    return factory.deferred\n-\n-def download_file(request, spider) :\n-    \"\"\"Return a deferred for a file download.\"\"\"\n-    filepath = request.url.split(\"file://\")[1]\n-    with open(filepath) as f:\n-        response = Response(domain=spider.domain_name, url=request.url, body=f.read())\n-    return defer_succeed(response)\n\n@@ -1,196 +0,0 @@\n-\"\"\"\n-Download web pages using asynchronous IO\n-\"\"\"\n-\n-import datetime\n-\n-from twisted.internet import reactor, defer\n-\n-from scrapy.core.exceptions import IgnoreRequest\n-from scrapy.spider import spiders\n-from scrapy.core.downloader.handlers import download_any\n-from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n-from scrapy import log\n-from scrapy.conf import settings\n-from scrapy.utils.defer import chain_deferred, mustbe_deferred\n-\n-\n-class SiteDetails(object):\n-    \"\"\"This is a simple data record that encapsulates the details we hold on\n-    each domain which we are scraping.\n-    \"\"\"\n-    def __init__(self, download_delay=None, max_concurrent_requests=2):\n-        self.download_delay = download_delay\n-        self.max_concurrent_requests = max_concurrent_requests if not download_delay else 1\n-\n-        self.queue = []\n-        self.active = set()\n-        self.downloading = set()\n-        self.closed = False\n-        self.lastseen = None\n-\n-    def is_idle(self):\n-        return not (self.active or self.downloading)\n-\n-    def capacity(self):\n-        return self.max_concurrent_requests - len(self.downloading)\n-\n-\n-class Downloader(object):\n-    \"\"\"Maintain many concurrent downloads and provide an HTTP abstraction\n-    We will have a limited number of connections per domain and scrape many domains in\n-    parallel.\n-\n-    request(..) should be called to request resources using http, https or file\n-    protocols.\n-    \"\"\"\n-\n-    def __init__(self) :\n-        self.sites = {}\n-        self.middleware = DownloaderMiddlewareManager()\n-        self.middleware.download_function = self.enqueue\n-        self.download_function = download_any\n-\n-    def fetch(self, request, spider):\n-        \"\"\" Main method to use to request a download\n-\n-        This method includes middleware mangling. Middleware can returns a\n-        Response object, then request never reach downloader queue, and it will\n-        not be downloaded from site.\n-        \"\"\"\n-        domain = spider.domain_name\n-        site = self.sites[domain]\n-        if not site or site.closed:\n-            raise IgnoreRequest('Unable to fetch (domain already closed): %s' % request)\n-\n-        site.active.add(request)\n-        def _deactivate(_):\n-            site.active.remove(request)\n-            return _\n-        dwld = self.middleware.download(request, spider)\n-        dwld.addBoth(_deactivate)\n-        return dwld\n-\n-    def enqueue(self, request, spider):\n-        \"\"\" Enqueue a Request for a effective download from site\n-        \"\"\"\n-        domain = spider.domain_name\n-        site = self.sites.get(domain)\n-        if not site or site.closed:\n-            raise IgnoreRequest('Trying to enqueue %s from closed site %s' % (request, domain))\n-\n-        deferred = defer.Deferred()\n-        site.queue.append((request, deferred))\n-        self.process_queue(spider)\n-        return deferred\n-\n-    def process_queue(self, spider):\n-        \"\"\" Effective download requests from site queue\n-        \"\"\"\n-        domain = spider.domain_name\n-        site = self.sites.get(domain)\n-        if not site:\n-            return\n-\n-        # download delay handling\n-        now = datetime.datetime.now()\n-        if site.download_delay and site.lastseen:\n-            delta = now - site.lastseen\n-            penalty = site.download_delay - delta.seconds\n-            if penalty > 0:\n-                reactor.callLater(penalty, self.process_queue, spider=spider)\n-                return\n-        site.lastseen = now\n-\n-        while site.queue and site.capacity()>0:\n-            request, deferred = site.queue.pop(0)\n-            self._download(request, spider, deferred)\n-\n-        if site.closed and site.is_idle():\n-            # XXX: Remove scrapyengine reference\n-            del self.sites[domain]\n-            from scrapy.core.engine import scrapyengine\n-            scrapyengine.closed_domain(domain=domain)\n-\n-    def _download(self, request, spider, deferred):\n-        log.msg('Activating %s' % request.traceinfo(), log.TRACE)\n-        domain = spider.domain_name\n-        site = self.sites.get(domain)\n-        site.downloading.add(request)\n-\n-        def _remove(result):\n-            log.msg('Deactivating %s' % request.traceinfo(), log.TRACE)\n-            site.downloading.remove(request)\n-            return result\n-\n-        def _finish(result):\n-            self.process_queue(spider)\n-\n-        dwld = mustbe_deferred(self.download_function, request, spider)\n-        dwld.addBoth(_remove)\n-        chain_deferred(dwld, deferred)\n-        dwld.addBoth(_finish)\n-\n-    def open_domain(self, domain):\n-        \"\"\"Allocate resources to begin processing a domain\"\"\"\n-        spider = spiders.fromdomain(domain)\n-        if domain in self.sites: # reopen\n-            self.sites[domain].closed = False\n-            return\n-\n-        # Instanciate site specific handling based on info provided by spider\n-        delay = getattr(spider, 'download_delay', None) or settings.getint(\"DOWNLOAD_DELAY\", 0)\n-        maxcr = getattr(spider, 'max_concurrent_requests', settings.getint('REQUESTS_PER_DOMAIN'))\n-        site = SiteDetails(download_delay=delay, max_concurrent_requests=maxcr)\n-        self.sites[domain] = site\n-\n-    def close_domain(self, domain):\n-        \"\"\"Free any resources associated with the given domain\"\"\"\n-        log.msg(\"Downloader closing domain %s\" % domain, log.TRACE, domain=domain)\n-        site = self.sites.get(domain)\n-        if site:\n-            site.closed = True\n-            spider = spiders.fromdomain(domain)\n-            self.process_queue(spider)\n-        else:\n-            log.msg('Domain %s already closed' % domain, log.TRACE, domain=domain)\n-\n-    # Most of the following functions must be reviewed to decide if are really needed\n-    def domain_is_open(self, domain):\n-        return domain in self.sites\n-\n-    def lastseen(self, domain):\n-        if domain in self.sites:\n-            return self.sites[domain].lastseen\n-\n-    def outstanding(self, domain):\n-        \"\"\"The number of outstanding requests for a domain\n-        This includes both active requests and pending requests.\n-        \"\"\"\n-        site = self.sites.get(domain)\n-        if site:\n-            return len(site.active) + len(site.queue)\n-\n-    def domain_is_idle(self, domain):\n-        return not self.outstanding(domain)\n-\n-    def request_queue(self, domain):\n-        site = self.sites.get(domain)\n-        return site.queue if site else []\n-\n-    def active_requests(self, domain):\n-        site = self.sites.get(domain)\n-        return site.active if site else []\n-\n-    def has_capacity(self):\n-        \"\"\"Does the downloader have capacity to handle more domains\"\"\"\n-        return len(self.sites) < settings.getint('CONCURRENT_DOMAINS')\n-\n-    def is_idle(self):\n-        return not self.sites\n-\n-    # deprecated\n-    def clear_requests(self, domain):\n-        log.msg(\"Downloader clearing request for domain %s\" % domain, log.TRACE, domain=domain)\n-        self.sites[domain].queue = []\n-\n\n@@ -1,173 +0,0 @@\n-\"\"\"\n-request-response middleware extension\n-\"\"\"\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.http import Request, Response\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class\n-from scrapy.utils.defer import mustbe_deferred\n-from scrapy.core.downloader.handlers import download_any\n-from scrapy.conf import settings\n-\n-class DownloaderMiddlewareManager(object):\n-    \"\"\"Request-Response Middleware Manager\n-\n-    Middleware is a framework of hooks into Scrapy's request/response\n-    processing.  It's a light, low-level \"spider\" system for globally altering\n-    Scrapy's input and/or output.\n-\n-    Middleware is heavily based on Django middleware system, at the point that\n-    it tries to mimic Django middleware behaviour. For Scrapy, the Django's\n-    view function has the same meaning of the final download handler function\n-    to use for the request's url.\n-\n-    To activate a middleware component, add it to the DOWNLOADER_MIDDLEWARES list\n-    in your Scrapy settings.  In DOWNLOADER_MIDDLEWARES, each middleware component\n-    is represented by a string: the full Python path to the middleware's class\n-    name. For example:\n-\n-    DOWNLOADER_MIDDLEWARES = (\n-            'scrapy.contrib.middleware.common.SpiderMiddleware',\n-            'scrapy.contrib.middleware.common.CommonMiddleware',\n-            'scrapy.contrib.middleware.redirect.RedirectMiddleware',\n-            'scrapy.contrib.middleware.cache.CacheMiddleware',\n-    )\n-\n-    Writing your own middleware is easy. Each middleware component is a single\n-    Python class that defines one or more of the following methods:\n-\n-\n-    process_request(self, request, spider)\n-\n-        `request` is a Request object.\n-        `spider` is a BaseSpider object\n-\n-        This method is called in each request until scrapy decides which\n-        download function to use.\n-\n-        process_request() should return either None, Response or Request.\n-\n-        If returns None, Scrapy will continue processing this request,\n-        executing any other middleware and, then, the appropiate download\n-        function.\n-\n-        If returns a Response object, Scrapy won't bother calling ANY other\n-        request or exception middleware, or the appropiate download function;\n-        it'll return that Response. Response middleware is always called on\n-        every response.\n-\n-        If returns a Request object, returned request is used to instruct a\n-        redirection. Redirection is handled inside middleware scope, and\n-        original request don't finish until redirected request is completed.\n-\n-\n-    process_response(self, request, response, spider):\n-\n-        `request` is a Request object\n-        `response` is a Response object\n-        `spider` is a BaseSpider object\n-\n-        process_response MUST return a Response object. It could alter the given\n-        response, or it could create a brand-new Response.\n-\n-\n-    process_exception(self, request, exception, spider)\n-\n-        `request` is a Request object.\n-        `exception` is an Exception object\n-        `spider` is a BaseSpider object\n-\n-        Scrapy calls process_exception() when a download handler or\n-        process_request middleware raises an exception.\n-\n-        process_exception() should return either None, Response or Request object.\n-\n-        if it returns None, Scrapy will continue processing this exception,\n-        executing any other exception middleware, until no middleware left and\n-        default exception handling kicks in.\n-\n-        If it returns a Response object, the response middleware kicks in, and\n-        won't bother calling ANY other exception middleware.\n-\n-        If it returns a Request object, returned request is used to instruct a\n-        immediate redirection. Redirection is handled inside middleware scope,\n-        and original request don't finish until redirected request is\n-        completed. This stop process_exception middleware as returning\n-        Response does.\n-\n-    \"\"\"\n-    def __init__(self):\n-        self.loaded = False\n-        self.request_middleware = []\n-        self.response_middleware = []\n-        self.exception_middleware = []\n-        self.load()\n-        self.download_function = download_any\n-\n-    def _add_middleware(self, mw):\n-        if hasattr(mw, 'process_request'):\n-            self.request_middleware.append(mw.process_request)\n-        if hasattr(mw, 'process_response'):\n-            self.response_middleware.insert(0, mw.process_response)\n-        if hasattr(mw, 'process_exception'):\n-            self.exception_middleware.insert(0, mw.process_exception)\n-\n-    def load(self):\n-        \"\"\"Load middleware defined in settings module\n-        \"\"\"\n-        mws = []\n-        for mwpath in settings.getlist('DOWNLOADER_MIDDLEWARES') or ():\n-            cls = load_class(mwpath)\n-            if cls:\n-                try:\n-                    mw = cls()\n-                    self._add_middleware(mw)\n-                    mws.append(mw)\n-                except NotConfigured:\n-                    pass\n-        log.msg(\"Enabled downloader middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]))\n-        self.loaded = True\n-\n-    def download(self, request, spider):\n-        def process_request(request):\n-            for method in self.request_middleware:\n-                response = method(request=request, spider=spider)\n-                assert response is None or isinstance(response, (Response, Request)), \\\n-                        'Middleware %s.process_request must return None, Response or Request, got %s' % \\\n-                        (method.im_self.__class__.__name__, response.__class__.__name__)\n-                if response:\n-                    return response\n-            return self.download_function(request=request, spider=spider)\n-\n-        def process_response(response):\n-            assert response is not None, 'Received None in process_response'\n-            if isinstance(response, Request):\n-                return response\n-\n-            for method in self.response_middleware:\n-                response = method(request=request, response=response, spider=spider)\n-                assert isinstance(response, (Response, Request)), \\\n-                    'Middleware %s.process_response must return Response or Request, got %s' % \\\n-                    (method.im_self.__class__.__name__, type(response))\n-                if isinstance(response, Request):\n-                    signals.send_catch_log(signal=signals.response_received, sender=self.__class__, response=response, spider=spider)\n-                    return response\n-            signals.send_catch_log(signal=signals.response_received, sender=self.__class__, response=response, spider=spider)\n-            return response\n-\n-        def process_exception(_failure):\n-            exception = _failure.value\n-            for method in self.exception_middleware:\n-                response = method(request=request, exception=exception, spider=spider)\n-                assert response is None or isinstance(response, (Response, Request)), \\\n-                    'Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n-                    (method.im_self.__class__.__name__, type(response))\n-                if response:\n-                    return response\n-            return _failure\n-\n-        deferred = mustbe_deferred(process_request, request)\n-        deferred.addErrback(process_exception)\n-        deferred.addCallback(process_response)\n-        return deferred\n\n@@ -1,549 +0,0 @@\n-\"\"\"\n-Core execution engine for the web crawling framework. This controls the \n-scheduler, downloader and spiders.\n-\"\"\"\n-from datetime import datetime\n-\n-from twisted.internet import defer, reactor, task\n-from twisted.python.failure import Failure\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.core.scheduler import Scheduler\n-from scrapy.core.downloader import Downloader\n-from scrapy.http import Response, Request\n-from scrapy.core.exceptions import IgnoreRequest, HttpException, DontCloseDomain\n-from scrapy.item import ScrapedItem\n-from scrapy.item.pipeline import ItemPipeline\n-from scrapy.spider import spiders\n-from scrapy.spider.middleware import SpiderMiddlewareManager\n-from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate\n-from scrapy.conf import settings\n-\n-\n-class ExecutionEngine(object):\n-    \"\"\"\n-    The Execution Engine controls execution of the scraping process.\n-\n-    The process begins with the _mainloop() method, which is called\n-    periodically to add more domains to scrape. It adds the first page for a\n-    domain to the scheduler by calling _schedule_page and calls\n-    process_scheduled_requests, which starts the scraping process for a domain.\n-\n-    The process_scheduled_requests method asks the scheduler for the next\n-    available page for a given domain and requests it from the downloader. The\n-    downloader will execute a callback function that was added in the\n-    _schedule_page method. This callback with process the output from the\n-    spider and then call process_scheduled_requests to continue the scraping\n-    process for that domain.\n-    \"\"\"\n-\n-    # back off downloading more pages if we exceed this backlog size\n-    DOWNLOADER_BACKLOG = 10\n-\n-    def __init__(self):\n-        self.configured = False\n-        self.keep_alive = False\n-        self.domain_close_delay = settings.getint('DOMAIN_CLOSE_DELAY') # seconds after an idle domain will be closed\n-        self.initializing = set() # domais in intialization state\n-        self.cancelled = set() # domains in cancelation state\n-        self.debug_mode = settings.getbool('ENGINE_DEBUG')\n-        self.tasks = []\n-        self.ports = []\n-        self.running = False\n-        self.paused = False\n-\n-    def configure(self, scheduler=None, downloader=None):\n-        \"\"\"\n-        Configure execution engine with the given scheduling policy and downloader.\n-        \"\"\"\n-        self.scheduler = scheduler or Scheduler()\n-        self.downloader = downloader or Downloader()\n-        self.spidermiddleware = SpiderMiddlewareManager()\n-        self._scraping = {}\n-        self.pipeline = ItemPipeline()\n-        # key dictionary of per domain lists of initial requests to scrape\n-        self.starters = {}\n-\n-        self.configured = True\n-\n-    def addtask(self, function, interval, args=None, kwargs=None, now=False):\n-        \"\"\"\n-        Adds a looping task. Use this instead of twisted task.LooopingCall to\n-        make sure the reactor is left in a clean state after the engine is\n-        stopped.\n-        \"\"\"\n-        if not args:\n-            args = []\n-        if not kwargs:\n-            kwargs = {}\n-        tsk = task.LoopingCall(function, *args, **kwargs)\n-        self.tasks.append((tsk, interval, now))\n-        if self.running:\n-            tsk.start(interval, now)\n-        return tsk\n-\n-    def removetask(self, tsk):\n-        \"\"\"Remove a looping task previously added with addtask() method\"\"\"\n-        self.tasks = [(t, i, n) for (t, i, n) in self.tasks if t is not tsk]\n-        if tsk.running:\n-            tsk.stop()\n-\n-    def listenTCP(self, *args, **kwargs):\n-        if self.running:\n-            self.ports.append(reactor.listenTCP(*args, **kwargs))\n-        else:\n-            self.ports.append((args, kwargs))\n-\n-    def clean_reactor(self):\n-        \"\"\"Leaves the reactor in a clean state by removing all pending tasks\n-        and listening ports. It can only be called when the engine is not\n-        running.\n-        \"\"\"\n-        if not self.running:\n-            for tsk, _, _ in self.tasks:\n-                if tsk.running:\n-                    tsk.stop()\n-            self.tasks = []\n-            for p in [p for p in self.ports if not isinstance(p, tuple)]:\n-                p.stopListening()\n-            self.ports = []\n-\n-    def start(self):\n-        \"\"\"Start the execution engine\"\"\"\n-        if not self.running:\n-            reactor.callWhenRunning(self._mainloop)\n-            self.start_time = datetime.now()\n-            signals.send_catch_log(signal=signals.engine_started, sender=self.__class__)\n-            self.addtask(self._mainloop, 5.0)\n-            for tsk, interval, now in self.tasks:\n-                tsk.start(interval, now)\n-            for args, kwargs in [t for t in self.ports if isinstance(t, tuple)]:\n-                reactor.listenTCP(*args, **kwargs)\n-            self.running = True\n-            reactor.run() # blocking call\n-\n-    def stop(self):\n-        \"\"\"Stop the execution engine\"\"\"\n-        if self.running:\n-            self.running = False\n-            for domain in self.open_domains:\n-                spider = spiders.fromdomain(domain)\n-                signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, status='cancelled')\n-            for tsk, _, _ in self.tasks: # stop looping calls\n-                if tsk.running:\n-                    tsk.stop()\n-            self.tasks = []\n-            for p in [p for p in self.ports if not isinstance(p, tuple)]:\n-                p.stopListening()\n-            if reactor.running:\n-                reactor.stop()\n-            signals.send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n-\n-    def pause(self):\n-        \"\"\"Pause the execution engine\"\"\"\n-        self.paused = True\n-\n-    def resume(self):\n-        \"\"\"Resume the execution engine\"\"\"\n-        self.paused = False\n-\n-    def is_idle(self):\n-        return self.scheduler.is_idle() and self.pipeline.is_idle() and self.downloader.is_idle() and not self._scraping\n-\n-    def next_domain(self):\n-        domain = self.scheduler.next_domain()\n-        if domain:\n-            spider = spiders.fromdomain(domain)\n-            self.open_domain(domain, spider)\n-        return domain\n-\n-    def next_request(self, spider, breakloop=True):\n-        \"\"\"Scrape the next request for the domain passed.\n-\n-        The next request to be scraped is retrieved from the scheduler and\n-        requested from the downloader.\n-\n-        The domain is closed if there are no more pages to scrape.\n-        \"\"\"\n-        if self.paused:\n-            return reactor.callLater(5, self.next_request, spider)\n-\n-        if breakloop:\n-            # delaying make reentrant call to next_request safe\n-            return reactor.callLater(0, self.next_request, spider, breakloop=False)\n-\n-        domain = spider.domain_name\n-\n-        # check that the engine is still running and domain is open\n-        if not self.running:\n-            return\n-\n-        # outstanding requests in downloader queue\n-        outstanding = self.downloader.outstanding(domain)\n-        if outstanding > self.DOWNLOADER_BACKLOG:\n-            return\n-\n-        # Next pending request from scheduler\n-        request, deferred = self.scheduler.next_request(domain)\n-        if request:\n-            try:\n-                dwld = self.download(request, spider)\n-            except IgnoreRequest, ex:\n-                log.msg(ex.message, log.WARNING, domain=domain)\n-            except Exception, ex:\n-                log.exc(\"Bug in download code: %s\" % request, domain=domain)\n-                self._domain_idle(domain)\n-            else:\n-                chain_deferred(dwld, deferred)\n-        else:\n-            if self.domain_is_idle(domain):\n-                self._domain_idle(domain)\n-\n-    def domain_is_idle(self, domain):\n-        scraping = self._scraping.get(domain)\n-        pending = self.scheduler.domain_has_pending(domain)\n-        downloading = not self.downloader.domain_is_idle(domain)\n-        haspipe = not self.pipeline.domain_is_idle(domain)\n-        oninit = domain in self.initializing\n-        if pending or downloading or haspipe or oninit or scraping:\n-            return False\n-        elif self.domain_close_delay:\n-            lastseen = self.downloader.lastseen(domain)\n-            if not lastseen: # domain not yet started\n-                return False\n-            now = datetime.now()\n-            delta = now - lastseen\n-            return (delta.seconds >= self.domain_close_delay)\n-        else:\n-            return True\n-\n-    def domain_is_open(self, domain):\n-        return domain in self.downloader.sites\n-\n-    @property\n-    def open_domains(self):\n-        return self.downloader.sites.keys()\n-\n-    def crawl(self, request, spider, priority=1, domain_priority=1):\n-        domain = spider.domain_name\n-\n-        def _process(response):\n-            assert isinstance(response, (Response, Exception))\n-\n-            def _onpipelinefinish(pipe_result, item):\n-                # _ can only be an item or a DropItem failure, since other\n-                # failures are caught in ItemPipeine (item/pipeline.py)\n-                if isinstance(pipe_result, Failure):\n-                    signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, item=item, spider=spider, response=response, exception=pipe_result.value)\n-                else:\n-                    signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, item=item, spider=spider, response=response, pipe_output=pipe_result)\n-                self.next_request(spider)\n-\n-            def _onsuccess_per_item(item):\n-                if isinstance(item, ScrapedItem):\n-                    log.msg(\"Scraped %s in <%s>\" % (item, request.url), log.DEBUG, domain=domain)\n-                    signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, item=item, spider=spider, response=response)\n-                    piped = self.pipeline.pipe(item, spider, response) # TODO: remove response\n-                    piped.addBoth(_onpipelinefinish, item)\n-                elif isinstance(item, Request):\n-                    signals.send_catch_log(signal=signals.request_received, sender=self.__class__, request=item, spider=spider, response=response)\n-                    self.crawl(request=item, spider=spider, priority=priority)\n-                else:\n-                    log.msg('Garbage found in spider output while processing %s, got type %s' % (request, type(item)), log.TRACE, domain=domain)\n-\n-            class _ResultContainer(object):\n-                def append(self, item):\n-                    _onsuccess_per_item(item)\n-\n-            def _onsuccess(result):\n-                return deferred_degenerate(result, _ResultContainer())\n-\n-            def _onerror(_failure):\n-                if not isinstance(_failure.value, IgnoreRequest):\n-                    referer = None if not isinstance(response, Response) else response.request.headers.get('Referer', None)\n-                    log.msg(\"Error while spider was processing <%s> from <%s>: %s\" % (request.url, referer, _failure), log.ERROR, domain=domain)\n-\n-            def _bugtrap(_failure):\n-                log.msg('FRAMEWORK BUG processing %s: %s' % (request, _failure), log.ERROR, domain=domain)\n-\n-\n-            scd = self.scrape(request, response, spider)\n-            scd.addCallbacks(_onsuccess, _onerror)\n-            scd.addErrback(_bugtrap)\n-\n-            self._scraping[domain].add(response)\n-            scd.addBoth(lambda _: self._scraping[domain].remove(response))\n-            return scd\n-\n-        def _cleanfailure(_failure):\n-            ex = _failure.value\n-            if not isinstance(ex, IgnoreRequest):\n-                log.msg(\"Unknown error propagated in %s: %s\" % (request, _failure), log.ERROR, domain=domain)\n-            request.deferred.addErrback(lambda _:None)\n-            request.deferred.errback(_failure) #TODO: merge into spider middleware.\n-\n-        schd = self.schedule(request, spider, priority, domain_priority)\n-        schd.addCallbacks(_process, _cleanfailure)\n-        return schd\n-\n-    def scrape(self, request, response, spider):\n-        return self.spidermiddleware.scrape(request, response, spider)\n-\n-    def schedule(self, request, spider, priority=1, domain_priority=1):\n-        domain = spider.domain_name\n-        if not self.scheduler.domain_is_open(domain):\n-            if self.debug_mode: \n-                log.msg('Scheduling %s (delayed)' % request.traceinfo(), log.TRACE)\n-            return self._add_starter(request, spider, domain_priority)\n-        if self.debug_mode: \n-            log.msg('Scheduling %s (now)' % request.traceinfo(), log.TRACE)\n-        schd = self.scheduler.enqueue_request(domain, request, priority)\n-        self.next_request(spider)\n-        return schd\n-\n-    def _mainloop(self):\n-        \"\"\"Add more domains to be scraped if the downloader has the capacity.\n-\n-        If there is nothing else scheduled then stop the execution engine.\n-        \"\"\"\n-        if not self.running or self.paused:\n-            return\n-\n-        # main domain starter loop\n-        while self.running and self.downloader.has_capacity():\n-            if not self.next_domain():\n-                return self._stop_if_idle()\n-\n-        # purge idle domains - (domain_close_delay support)\n-        if self.domain_close_delay:\n-            for domain in self.downloader.sites:\n-                if self.domain_is_idle(domain):\n-                    self._domain_idle(domain)\n-\n-    def _add_starter(self, request, spider, domain_priority):\n-        domain = spider.domain_name\n-        if not self.scheduler.is_pending(domain):\n-            self.scheduler.add_domain(domain, priority=domain_priority)\n-            self.starters[domain] = []\n-        deferred = defer.Deferred()\n-        self.starters[domain] += [(request, deferred)]\n-        return deferred\n-\n-    def _run_starters(self, spider):\n-        domain = spider.domain_name\n-        starters = self.starters.get(domain, [])\n-        while starters:\n-            request, deferred = starters.pop(0)\n-            schd = self.schedule(request, spider)\n-            chain_deferred(schd, deferred)\n-        del self.starters[domain]\n-\n-    def download(self, request, spider):\n-        log.msg('Downloading %s' % request.traceinfo(), log.TRACE)\n-        domain = spider.domain_name\n-\n-        def _on_success(response):\n-            \"\"\"handle the result of a page download\"\"\"\n-            assert isinstance(response, (Response, Request))\n-            log.msg(\"Requested %s\" % request.traceinfo(), level=log.TRACE, domain=domain)\n-            if isinstance(response, Response):\n-                response.request = request # tie request to obtained response\n-                cached = 'cached' if response.cached else 'live'\n-                log.msg(\"Crawled %s <%s> from <%s>\" % (cached, response.url, request.headers.get('referer')), level=log.DEBUG, domain=domain)\n-                return response\n-            elif isinstance(response, Request):\n-                redirected = response # proper alias\n-                schd = self.schedule(redirected, spider, priority=4)\n-                chain_deferred(schd, redirected.deferred)\n-                return schd\n-\n-        def _on_error(_failure):\n-            \"\"\"handle an error processing a page\"\"\"\n-            ex = _failure.value\n-            if isinstance(ex, IgnoreRequest):\n-                log.msg(_failure.getErrorMessage(), level=log.DEBUG, domain=domain)\n-                return _failure\n-            referer = request.headers.get('Referer', None)\n-            errmsg = str(ex) if isinstance(ex, HttpException) else str(_failure)\n-            log.msg(\"Downloading <%s> from <%s>: %s\" % (request.url, referer, errmsg), log.ERROR, domain=domain)\n-            return Failure(IgnoreRequest(str(ex)))\n-\n-        def _on_complete(_):\n-            self.next_request(spider)\n-\n-        dwld = self.downloader.fetch(request, spider)\n-        dwld.addCallbacks(_on_success, _on_error)\n-        deferred = defer.Deferred()\n-        chain_deferred(dwld, deferred)\n-        dwld.addBoth(_on_complete)\n-        return deferred\n-\n-    def initialize(self, spider):\n-        domain = spider.domain_name\n-        if not hasattr(spider, 'init_domain'):\n-            return defer_succeed(True)\n-\n-        def _initialize(req):\n-            if isinstance(req, Request):\n-                _response = None\n-                def _referer(response):\n-                    req.deferred.addCallback(_setreferer, response)\n-                    return response\n-\n-                def _setreferer(result, response):\n-                    if isinstance(result, Request):\n-                        result.headers.setdefault('Referer', response.url)\n-                    return result\n-\n-                def _onerror(_failure):\n-                    ex = _failure.value\n-                    if isinstance(ex, IgnoreRequest):\n-                        log.msg(ex.message, log.DEBUG, domain=domain)\n-                    else:\n-                        return _failure\n-\n-                schd = self.schedule(req, spider)\n-                schd.addCallback(_referer)\n-                chain_deferred(schd, req.deferred)\n-                schd.addErrback(_onerror)\n-                schd.addBoth(_initialize)\n-                return schd\n-            return req\n-\n-        def _bugtrap(_failure):\n-            log.msg(\"Bug in %s init_domain code: %s\" % (domain, _failure), log.ERROR, domain=domain)\n-\n-        def _state(state):\n-            self.initializing.remove(domain)\n-            if state is True:\n-                log.msg('Succeded initialization for %s' % domain, log.INFO, domain=domain)\n-            else:\n-                log.msg('Failed initialization for %s' % domain, log.INFO, domain=domain)\n-            return state\n-\n-        log.msg('Started initialization for %s' % domain, log.INFO, domain=domain)\n-        self.initializing.add(domain)\n-        req = spider.init_domain()\n-        deferred = mustbe_deferred(_initialize, req)\n-        deferred.addErrback(_bugtrap)\n-        deferred.addCallback(_state)\n-        return deferred\n-\n-    def open_domain(self, domain, spider=None):\n-        log.msg(\"Domain opened\", domain=domain)\n-        spider = spider or spiders.fromdomain(domain)\n-\n-        self.scheduler.open_domain(domain)\n-        self.downloader.open_domain(domain)\n-        self.pipeline.open_domain(domain)\n-        self._scraping[domain] = set()\n-        signals.send_catch_log(signals.domain_open, sender=self.__class__, domain=domain, spider=spider)\n-\n-        # init_domain\n-        dfd = self.initialize(spider)\n-        def _state(state):\n-            if state is True:\n-                signals.send_catch_log(signals.domain_opened, sender=self.__class__, domain=domain, spider=spider)\n-                self._run_starters(spider)\n-            else:\n-                self._domain_idle(domain)\n-        dfd.addCallback(_state)\n-\n-    def close_domain(self, domain):\n-        \"\"\"Close (cancel) domain and clear all its outstanding requests\"\"\"\n-        if domain not in self.cancelled:\n-            self.cancelled.add(domain)\n-            self._close_domain(domain)\n-\n-    def _close_domain(self, domain):\n-        self.downloader.close_domain(domain)\n-\n-    def _domain_idle(self, domain):\n-        \"\"\"\n-        Called when a domain gets idle. This function is called when there are no\n-        remaining pages to download or scheduled. It can be called multiple\n-        times. If the some extensions raises a DontCloseDomain exception the\n-        domain won't be closed and this function is garanteed to be called\n-        again (at least once) for this domain.\n-        \"\"\"\n-        # we get a callback from the downloader which completes closing\n-        #log.msg(\"Finishing scraping %s\" % domain, domain=domain)\n-        #from traceback import print_stack; print_stack()\n-\n-        spider = spiders.fromdomain(domain)\n-        try:\n-            dispatcher.send(signal=signals.domain_idle, sender=self.__class__, domain=domain, spider=spider)\n-        except DontCloseDomain:\n-            return\n-        except:\n-            log.exc(\"Exception catched on domain_idle signal dispatch\")\n-\n-        if self.domain_is_idle(domain):\n-            self._close_domain(domain)\n-\n-    def _stop_if_idle(self):\n-        \"\"\"Call the stop method if the system has no outstanding tasks. \"\"\"\n-        if self.is_idle() and not self.keep_alive:\n-            self.stop()\n-\n-    def closed_domain(self, domain):\n-        \"\"\"\n-        This function is called after the domain has been closed, and throws\n-        the domain_closed signal which is meant to be used for cleaning up\n-        purposes. In contrast to domain_idle, this function is called only\n-        ONCE for each domain run.\n-        \"\"\" \n-        spider = spiders.fromdomain(domain) \n-        self.scheduler.close_domain(domain)\n-        self.pipeline.close_domain(domain)\n-        del self._scraping[domain]\n-        status = 'cancelled' if domain in self.cancelled else 'finished'\n-        signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, status=status)\n-        log.msg(\"Domain closed (%s)\" % status, domain=domain) \n-        self.cancelled.discard(domain)\n-        self._mainloop()\n-\n-    def getstatus(self):\n-        \"\"\"\n-        Return a report of the current engine status\n-        \"\"\"\n-        s = \"Execution engine status\\n\\n\"\n-\n-        global_tests = [\n-            \"datetime.now()-self.start_time\", \n-            \"self.is_idle()\", \n-            \"self.scheduler.is_idle()\",\n-            \"len(self.scheduler.pending_domains_count)\",\n-            \"self.downloader.is_idle()\",\n-            \"len(self.downloader.sites)\",\n-            \"self.downloader.has_capacity()\",\n-            \"self.pipeline.is_idle()\",\n-            \"len(self.pipeline.domaininfo)\",\n-            ]\n-        domain_tests = [\n-            \"self.domain_is_idle(domain)\",\n-            \"self.scheduler.domain_has_pending(domain)\",\n-            \"len(self.scheduler.pending_requests[domain])\",\n-            \"self.downloader.outstanding(domain)\",\n-            \"len(self.downloader.request_queue(domain))\",\n-            \"len(self.downloader.active_requests(domain))\",\n-            \"self.pipeline.domain_is_idle(domain)\",\n-            \"len(self.pipeline.domaininfo[domain])\",\n-            ]\n-\n-        for test in global_tests:\n-            s += \"%-47s : %s\\n\" % (test, eval(test))\n-        s += \"\\n\"\n-        for domain in self.downloader.sites:\n-            s += \"%s\\n\" % domain\n-            for test in domain_tests:\n-                s += \"  %-45s : %s\\n\" % (test, eval(test))\n-\n-        return s\n-\n-    def st(self): # shortcut for printing engine status (useful in telnet console)\n-        print self.getstatus()\n-\n-scrapyengine = ExecutionEngine()\n\n@@ -1,47 +0,0 @@\n-\"\"\"\n-Scrapy core exceptions\n-\"\"\"\n-\n-# Internal\n-\n-class UsageError(Exception):\n-    \"\"\"Incorrect usage of the core API\"\"\"\n-    pass\n-\n-class NotConfigured(Exception):\n-    \"\"\"Indicates a missing configuration situation\"\"\"\n-    pass\n-\n-# HTTP and crawling\n-\n-class IgnoreRequest(Exception):\n-    \"\"\"Indicates a decision was made not to process a request\"\"\"\n-    pass\n-\n-class DontCloseDomain(Exception):\n-    \"\"\"Request the domain not to be closed yet\"\"\"\n-    pass\n-\n-class HttpException(Exception):\n-    def __init__(self, status, message, response):\n-        if not message:\n-            from twisted.web import http\n-            message = http.responses.get(int(status))\n-\n-        self.status = status\n-        self.message = message\n-        self.response = response\n-        Exception.__init__(self, status, message, response)\n-\n-    def __str__(self):\n-        return '%s %s' % (self.status, self.message)\n-\n-# Items\n-\n-class DropItem(Exception):\n-    \"\"\"Drop item from the item pipeline\"\"\"\n-    pass\n-\n-class NotSupported(Exception):\n-    \"\"\"Indicates a feature or method is not supported\"\"\"\n-    pass\n\\ No newline at end of file\n\n@@ -1,149 +0,0 @@\n-import signal\n-\n-from scrapy.extension import extensions\n-from scrapy import log\n-from scrapy.http import Request\n-from scrapy.core.engine import scrapyengine\n-from scrapy.spider import spiders\n-from scrapy.utils.misc import load_class\n-from scrapy.utils.url import is_url\n-from scrapy.conf import settings\n-\n-class ExecutionManager(object):\n-    \"\"\"Process a list of sites or urls.\n-\n-    This class should be used in a main for process a list of sites/urls.\n-\n-    It extracts products and could be used to store results in a database or\n-    just for testing spiders.\n-    \"\"\"\n-    def __init__(self):\n-        self.interrupted = False\n-        self.configured = False\n-\n-    def configure(self, *args, **opts):\n-        self._install_signals()\n-\n-        extensions.load()\n-        log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()))\n-\n-        scheduler = load_class(settings['SCHEDULER'])()\n-\n-        scrapyengine.configure(scheduler=scheduler)\n-\n-        self.prioritizer_class = load_class(settings['PRIORITIZER'])\n-\n-        requests = self._parse_args(args)\n-        self.priorities = self.prioritizer_class(requests.keys())\n-        \n-    def crawl(self, *args):\n-        \"\"\"Schedule the given args for crawling. args is a list of urls or domains\"\"\"\n-\n-        requests = self._parse_args(args)\n-        # schedule initial requets to be scraped at engine start\n-        for domain in requests or ():\n-            spider = spiders.fromdomain(domain) \n-            priority = self.priorities.get_priority(domain) \n-            for request in requests[domain]:\n-                scrapyengine.crawl(request, spider, domain_priority=priority)\n-\n-    def runonce(self, *args, **opts):\n-        \"\"\"Run the engine until it finishes scraping all domains and then exit\"\"\"\n-        self.configure(*args, **opts)\n-        self.crawl(*args)\n-        scrapyengine.start()\n-\n-    def start(self, **opts):\n-        \"\"\"Start the scrapy server, without scheduling any domains\"\"\"\n-        self.configure(**opts)\n-        scrapyengine.keep_alive = True\n-        scrapyengine.start()# blocking call\n-        self.stop()\n-\n-    def stop(self):\n-        \"\"\"Stop the scrapy server, shutting down the execution engine\"\"\"\n-        self.interrupted = True\n-        scrapyengine.stop()\n-        log.log_level = -999 # disable logging\n-        signal.signal(signal.SIGTERM, signal.SIG_IGN)\n-        signal.signal(signal.SIGINT, signal.SIG_IGN)\n-        if hasattr(signal, \"SIGBREAK\"):\n-            signal.signal(signal.SIGBREAK, signal.SIG_IGN)\n-\n-    def reload_spiders(self):\n-        \"\"\"\n-        Reload all enabled spiders except for the ones that are currently\n-        running.\n-\n-        \"\"\"\n-        spiders.reload(skip_domains=scrapyengine.open_domains)\n-        # reload priorities for the new domains\n-        self.priorities = self.prioritizer_class(spiders.asdict(include_disabled=False).keys())\n-\n-    def _install_signals(self):\n-        def sig_handler_terminate(signalinfo, param):\n-            log.msg('Received shutdown request, waiting for deferreds to finish...', log.INFO)\n-            self.stop()\n-\n-        signal.signal(signal.SIGTERM, sig_handler_terminate)\n-        # only handle SIGINT if there isn't already a handler (e.g. for Pdb)\n-        if signal.getsignal(signal.SIGINT) == signal.default_int_handler:\n-            signal.signal(signal.SIGINT, sig_handler_terminate)\n-        # Catch Ctrl-Break in windows\n-        if hasattr(signal, \"SIGBREAK\"):\n-            signal.signal(signal.SIGBREAK, sig_handler_terminate)\n-\n-    def _parse_args(self, args):\n-        \"\"\" Parse crawl arguments and return a dict domains -> [requests] \"\"\"\n-        if not args:\n-            args = [p.domain_name for p in spiders.enabled]\n-\n-        requests, urls, sites = set(), set(), set()\n-        for a in args:\n-            if isinstance(a, Request):\n-                requests.add(a)\n-            elif is_url(a):\n-                urls.add(a)\n-            else:\n-                sites.add(a)\n-\n-        perdomain = {}\n-        def _add(domain, request):\n-            if domain not in perdomain:\n-                perdomain[domain] = []\n-            perdomain[domain] += [request]\n-\n-        # sites\n-        for domain in sites:\n-            spider = spiders.fromdomain(domain)\n-            if not spider:\n-                log.msg('Could not found spider for %s' % domain, log.ERROR)\n-                continue\n-            for url in self._start_urls(spider):\n-                request = Request(url, callback=spider.parse, dont_filter=True)\n-                _add(domain, request)\n-        # urls\n-        for url in urls:\n-            spider = spiders.fromurl(url)\n-            if spider:\n-                request = Request(url=url, callback=spider.parse, dont_filter=True)\n-                _add(spider.domain_name, request)\n-            else:\n-                log.msg('Could not found spider for <%s>' % url, log.ERROR)\n-\n-        # requests\n-        for request in requests:\n-            if request.domain:\n-                spider = spiders.fromdomain(request.domain)\n-            else:\n-                spider = spiders.fromurl(request.url)\n-            if not spider:\n-                log.msg('Could not found spider for %s' % request, log.ERROR)\n-                continue\n-            _add(spider.domain_name, request)\n-        return perdomain\n-\n-    def _start_urls(self, spider):\n-        return spider.start_urls if hasattr(spider.start_urls, '__iter__') else [spider.start_urls]\n-        \n-scrapymanager = ExecutionManager()\n\n@@ -1,3 +0,0 @@\n-from scrapy.core.scheduler.schedulers import Scheduler\n-from scrapy.core.scheduler.filter import GroupFilter\n-from scrapy.core.scheduler.store import MemoryStore\n\n@@ -1,19 +0,0 @@\n-class GroupFilter(dict):\n-    \"\"\"Filter groups of keys\"\"\"\n-    def open(self, group):\n-        self[group] = set()\n-\n-    def close(self, group):\n-        del self[group]\n-\n-    def add(self, group, key):\n-        \"\"\"Add a key to the group if an equivalent key has not already been added.\n-        This method will return true if the key was added and false otherwise.\n-        \"\"\"\n-        if key not in self[group]:\n-            self[group].add(key)\n-            return True\n-        return False\n-\n-    def has(self, group, key):\n-        return key in self[group]\n\n@@ -1,173 +0,0 @@\n-\"\"\"\n-The Scrapy Scheduler\n-\"\"\"\n-\n-from twisted.internet import defer\n-\n-from scrapy.core.scheduler.filter import GroupFilter\n-from scrapy import log\n-from scrapy.core.exceptions import IgnoreRequest\n-from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n-from scrapy.utils.defer import defer_fail\n-from scrapy.conf import settings\n-\n-\n-class Scheduler(object) :\n-    \"\"\"\n-    The scheduler decides what to scrape, how fast, and in what order.\n-    The scheduler schedules websites and pages to be scraped. Individual\n-    web pages that are to be scraped are batched up into a \"run\" for a website.\n-    As the domain is being scraped, pages that are discovered are added to the\n-    scheduler. The scheduler must not allow the same page to be requested\n-    multiple times within the same batch.\n-\n-    Typical usage:\n-\n-    * next_availble_domain() called to find out when there is something to do\n-    * begin_domain() called to commence scraping a website\n-    * enqueue_request() called multiple times when new links found\n-    * next_request() called multiple times when there is capacity to process urls\n-    * close_domain() called when there are no more pages or upon error\n-\n-    Note a couple things:\n-    1) The order in which you get back the list of pages to scrape is not\n-       necesarily the order you put them in.\n-    2) A canonical URL is calculated for each url for each domain to check that\n-       it is unique, however the actual url passed in is returned when\n-       get_next_page is called.\n-       This is for two main reasons:\n-        * To be nice to the screen scraped site just incase the specific\n-          format of the url is significant.\n-        * To take advantage of any caching that uses the URL/URI as a key\n-\n-    all_domains contains the names of all domains that are to be scheduled.\n-\n-    Two crawling orders are available by default, which can be set with the\n-    SCHEDULER_ORDER settings:\n-\n-    * BFO - breath-first order (default). Consumes more memory than DFO but reaches \n-            most relevant pages faster.\n-    * DFO - depth-first order. Consumes less memory than BFO but usually takes\n-            longer to reach the most relevant pages.\n-    \"\"\"\n-\n-    def __init__(self):\n-        self.pending_domains_count = {}\n-        self.domains_queue = PriorityQueue()\n-        self.pending_requests = {}\n-        self.groupfilter = GroupFilter()\n-        self.dfo = settings.get('SCHEDULER_ORDER', '').upper() == 'DFO'\n-\n-    def domain_is_open(self, domain):\n-        return domain in self.pending_requests\n-\n-    def is_pending(self, domain):\n-        return domain in self.pending_domains_count\n-\n-    def domain_has_pending(self, domain):\n-        if domain in self.pending_requests:\n-            return not self.pending_requests[domain].empty()\n-\n-    def next_domain(self) :\n-        \"\"\"\n-        Return next domain available to scrape and remove it from available domains queue\n-        \"\"\"\n-        if self.pending_domains_count:\n-            priority, domain = self.domains_queue.get_nowait()\n-            if self.pending_domains_count[domain] == 1:\n-                del self.pending_domains_count[domain]\n-            else:\n-                self.pending_domains_count[domain] -= 1\n-            return domain\n-        return None\n-\n-    def add_domain(self, domain, priority=1):\n-        \"\"\"\n-        This functions schedules a new domain to be scraped, with the given\n-        priority. It doesn't check if the domain is already scheduled.  A\n-        domain can be scheduled twice, either with the same or with different\n-        priority.\n-        \"\"\"\n-        self.domains_queue.put(domain, priority=priority)\n-        if domain not in self.pending_domains_count:\n-            self.pending_domains_count[domain] = 1\n-        else:\n-            self.pending_domains_count[domain] += 1\n-\n-    def open_domain(self, domain):\n-        \"\"\" \n-        Allocates resources for maintaining a schedule for domain.\n-        \"\"\"\n-        if self.dfo:\n-            self.pending_requests[domain] = PriorityStack()\n-        else:\n-            self.pending_requests[domain] = PriorityQueue()\n-\n-        self.groupfilter.open(domain)\n-\n-    def enqueue_request(self, domain, request, priority=1):\n-        \"\"\"\n-        Add a page to be scraped for a domain that is currently being scraped.\n-        \"\"\"\n-        requestid = request.fingerprint()\n-        added = self.groupfilter.add(domain, requestid)\n-\n-        if request.dont_filter or added:\n-            deferred = defer.Deferred()\n-            self.pending_requests[domain].put((request, deferred), priority)\n-            return deferred\n-        else:\n-            return defer_fail(IgnoreRequest('Skipped (already visited): %s' % request))\n-\n-    def request_seen(self, domain, request):\n-        \"\"\"\n-        Returns True if the given Request was scheduled before for the given\n-        domain\n-        \"\"\"\n-        return self.groupfilter.has(domain, request.fingerprint())\n-\n-    def next_request(self, domain):\n-        \"\"\"\n-        Get the next request to be scraped.\n-\n-        None should be returned if there are no more request pending for the domain passed.\n-        \"\"\"\n-        pending_list = self.pending_requests.get(domain)\n-        if pending_list and not pending_list.empty():\n-            return pending_list.get_nowait()[1]\n-        else:\n-            return (None, None)\n-\n-    def close_domain(self, domain) :\n-        \"\"\"\n-        Called once we are finished scraping a domain. The scheduler will\n-        free any resources associated with the domain.\n-        \"\"\"\n-        try :\n-            del self.pending_requests[domain]\n-        except Exception, inst:\n-            msg = \"Could not clear pending pages for domain %s, %s\" % (domain, inst)\n-            log.msg(msg, level=log.WARNING)\n-\n-        try :\n-            self.groupfilter.close(domain)\n-        except Exception, inst:\n-            msg = \"Could not clear url filter for domain %s, %s\" % (domain, inst)\n-            log.msg(msg, level=log.WARNING)\n-\n-    def remove_pending_domain(self, domain):\n-        \"\"\"\n-        Remove a pending domain not yet started. If the domain was enqueued\n-        several times, all those instances are removed.\n-\n-        Returns the number of times the domain was enqueued. 0 if domains was\n-        not pending.\n-        \n-        If domain is open (not pending) it is not removed and returns None. You\n-        need to call close_domain for open domains.\n-        \"\"\"\n-        if not self.domain_is_open(domain):\n-            return self.pending_domains_count.pop(domain, 0)\n-\n-    def is_idle(self):\n-        return not self.pending_requests\n\n@@ -1,73 +0,0 @@\n-\"\"\"\n-Scrapy core signals\n-\"\"\"\n-from pydispatch import dispatcher\n-\n-from scrapy import log\n-\n-# After the execution engine has started\n-# args: None\n-engine_started = object()\n-\n-# After the execution engine has stopped\n-# args: None\n-engine_stopped = object()\n-\n-# Before opening a new domain for crawling\n-# args: domain, spider\n-domain_open = object()\n-\n-# After a domain has been opened for crawling\n-# args: domain, spider\n-domain_opened = object()\n-\n-# When a domain has no remaining requests to process\n-# args: domain, spider\n-domain_idle = object()\n-\n-# After a domain has been closed\n-# args: domain, spider, status\n-# status is a string and its possible values are: \"finished\" or \"cancelled\"\n-domain_closed = object()\n-\n-# domain has been initialized (successful or not)\n-# args: domain, spider, status\n-domain_initialized = object()\n-\n-# New request received from spiders\n-# args: request, spider, response\n-# response is the response (fed to the spider) which generated the request\n-request_received = object()\n-\n-# When request is sent in the downloader\n-# args: request, spider\n-request_uploaded = object()\n-\n-# When new response is received (by the engine) from the downloader (middleware)\n-# args: response, spider\n-response_received = object()\n-\n-# When response arrives from the downloader\n-# args: response, spider\n-response_downloaded = object()\n-\n-# After item is returned from spiders\n-# args: item, spider, response\n-item_scraped = object()\n-\n-# After item is processed by pipeline and pass all its stages\n-# args: item, spider, response, pipe_output\n-item_passed = object()\n-\n-# After item is dropped by pipeline\n-# args: item, spider, response, exception\n-item_dropped = object()\n-\n-def send_catch_log(signal, sender=None, **kwargs):\n-    \"\"\"\n-    Send a signal and log any exceptions raised by its listeners\n-    \"\"\"\n-    try:\n-        dispatcher.send(signal=signal, sender=sender, **kwargs)\n-    except:\n-        log.exc(\"Exception catched on signal dispatch\")\n\n@@ -1,39 +0,0 @@\n-\"\"\"\n-This module contains the ExtensionManager  which takes care of loading and\n-keeping track of all enabled extensions. It also contains an instantiated\n-ExtensionManager (extensions) to be used as singleton.\n-\"\"\"\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class\n-from scrapy import log\n-from scrapy.conf import settings\n-\n-class ExtensionManager(object):\n-\n-    def __init__(self):\n-        self.loaded = False\n-        self.enabled = {}\n-        self.disabled = {}\n-\n-    def load(self):\n-        \"\"\"\n-        Load enabled extensions in settings module\n-        \"\"\"\n-\n-        self.loaded = False\n-        self.enabled.clear()\n-        self.disabled.clear()\n-        for extension_path in settings.getlist('EXTENSIONS'):\n-            try:\n-                cls = load_class(extension_path)\n-                self.enabled[cls.__name__] = cls()\n-            except NotConfigured, e:\n-                self.disabled[cls.__name__] = extension_path\n-                if e.args:\n-                    log.msg(e)\n-        self.loaded = True\n-\n-    def reload(self):\n-        self.load()\n-\n-extensions = ExtensionManager()\n\n@@ -1,11 +0,0 @@\n-\"\"\"\n-Module containing all HTTP related classes\n-\n-Use this module (instead of the more specific ones) when importing Headers,\n-Request, Response, ResponseBody and Url outside this module.\n-\"\"\"\n-\n-from scrapy.http.url import Url\n-from scrapy.http.headers import Headers\n-from scrapy.http.request import Request\n-from scrapy.http.response import Response, ResponseBody\n\n@@ -1,97 +0,0 @@\n-from scrapy.utils.datatypes import CaselessDict\n-\n-def headers_raw_to_dict(headers_raw):\n-    \"\"\"\n-    Convert raw headers (single multi-line string)\n-    to the dictionary.\n-    \n-    For example:\n-    >>> headers_raw_to_dict(\"Content-type: text/html\\\\n\\\\rAccept: gzip\\\\n\\\\n\")\n-    {'Content-type': ['text/html'], 'Accept': ['gzip']}\n-    \n-    Incorrect input:\n-    >>> headers_raw_to_dict(\"Content-typt gzip\\\\n\\\\n\")\n-    {}\n-    \n-    Argument is None:\n-    >>> headers_raw_to_dict(None)\n-    \"\"\"\n-    if headers_raw is None:\n-        return None \n-    return dict([\n-        (header_item[0].strip(), [header_item[1].strip()]) \n-        for header_item \n-        in [\n-            header.split(':', 1) \n-            for header \n-            in headers_raw.splitlines()] \n-        if len(header_item) == 2])\n-        \n-def headers_dict_to_raw(headers_dict):\n-    \"\"\"\n-    Returns a raw HTTP headers representation of headers\n-    \n-    For example:\n-    >>> headers_dict_to_raw({'Content-type': 'text/html', 'Accept': 'gzip'})\n-    'Content-type: text/html\\\\r\\\\nAccept: gzip'\n-    >>> from twisted.python.util import InsensitiveDict\n-    >>> td = InsensitiveDict({'Content-type': ['text/html'], 'Accept': ['gzip']})\n-    >>> headers_dict_to_raw(td)\n-    'Content-type: text/html\\\\r\\\\nAccept: gzip'\n-    \n-    Argument is None:\n-    >>> headers_dict_to_raw(None)\n-    \n-    \"\"\"\n-    if headers_dict is None:\n-        return None\n-    raw_lines = []\n-    for key, value in headers_dict.items():\n-        if isinstance(value, (str, unicode)):\n-            raw_lines.append(\"%s: %s\" % (key, value))\n-        elif isinstance(value, (list, tuple)):\n-            for v in value:\n-                raw_lines.append(\"%s: %s\" % (key, v))\n-    return '\\r\\n'.join(raw_lines)\n-\n-class Headers(CaselessDict):\n-    def __init__(self, dictorstr=None, fromdict=None, fromstr=None, encoding='utf-8'):\n-        self.encoding = encoding\n-\n-        if dictorstr is not None:\n-            if isinstance(dictorstr, dict):\n-                d = dictorstr\n-            elif isinstance(dictorstr, basestring):\n-                d = headers_raw_to_dict(dictorstr)\n-        elif fromdict is not None:\n-            d = fromdict\n-        elif fromstr is not None:\n-            d = headers_raw_to_dict(fromstr)\n-        else:\n-            d = {}\n-\n-        # can't use CaselessDict.__init__(self, d) because it doesn't call __setitem__\n-        for k,v in d.iteritems(): \n-            self.__setitem__(k.lower(), v) \n-\n-    def normkey(self, key):\n-        return key.title()\n-\n-    def __setitem__(self, key, value):\n-        \"\"\"Headers must not be unicode\"\"\"\n-        if isinstance(key, unicode):\n-            key = key.encode(self.encoding)\n-        if isinstance(value, unicode):\n-            value = value.encode(self.encoding)\n-        super(Headers, self).__setitem__(key, value)\n-\n-    def tostring(self):\n-        return headers_dict_to_raw(self)\n-\n-    def rawsize(self):\n-        \"\"\"Estimated size of raw HTTP headers, in bytes\"\"\"\n-        # For each header line you have 4 extra chars: \": \" and CRLF\n-        return sum([len(k)+len(v)+4 for k, v in self.iteritems()])\n-\n-    def to_string(self):\n-        return headers_dict_to_raw(self)\n\n@@ -1,188 +0,0 @@\n-import urllib\n-import warnings\n-import hashlib\n-from copy import copy\n-from base64 import urlsafe_b64encode\n-\n-from twisted.internet import defer\n-\n-from scrapy.http.url import Url\n-from scrapy.http.headers import Headers\n-from scrapy.utils.url import safe_url_string\n-from scrapy.utils.c14n import canonicalize\n-from scrapy.utils.defer import chain_deferred\n-\n-class Request(object):\n-    def __init__(self, url, callback=None, context=None, method=None, body=None, headers=None, cookies=None,\n-            referer=None, url_encoding='utf-8', link_text='', http_user='', http_pass='', dont_filter=None, \n-            fingerprint_params=None, domain=None):\n-\n-        self.encoding = url_encoding  # this one has to be set first\n-        self.set_url(url)\n-\n-        # method\n-        if method is None and body is not None:\n-            method = 'POST' # backwards compatibility\n-        self.method = method.upper() if method else 'GET'\n-        assert isinstance(self.method, basestring), 'Request method argument must be str or unicode, got %s: %s' % (type(method), method)\n-\n-        # body\n-        if isinstance(body, dict):\n-            body = urllib.urlencode(body)\n-        self.body = body\n-\n-        # callback / deferred\n-        if callable(callback):\n-            callback = defer.Deferred().addCallback(callback)\n-        self.deferred = callback or defer.Deferred()\n-\n-        # request cookies\n-        self.cookies = cookies or {}\n-        # request headers\n-        self.headers = Headers(headers or {}, encoding=url_encoding)\n-        # persistent context across requests\n-        self.context = context or {}\n-        # dont_filter be filtered by scheduler\n-        self.dont_filter = dont_filter\n-        # fingerprint parameters\n-        self.fingerprint_params = fingerprint_params or {}\n-        self._fingerprint = None\n-        # shortcut for setting referer\n-        if referer is not None:\n-            self.headers['referer'] = referer\n-        # http auth\n-        if http_user or http_pass:\n-            self.httpauth(http_user, http_pass)\n-        self.depth = 0\n-        self.link_text = link_text\n-        #allows to directly specify the spider for the request\n-        self.domain = domain\n-        \n-    def append_callback(self, callback, *args, **kwargs):\n-        if isinstance(callback, defer.Deferred):\n-            return chain_deferred(self.deferred, callback)\n-        return self.deferred.addCallback(callback, *args, **kwargs)\n-\n-    def prepend_callback(self, func, *args, **kwargs):\n-        if callable(func):\n-            func = defer.Deferred().addCallback(func, *args, **kwargs)\n-        assert isinstance(func, defer.Deferred), 'prepend_callback expects a callable or defer.Deferred instance, got %s' % type(func)\n-        self.deferred = chain_deferred(func, self.deferred)\n-        return self.deferred\n-\n-    def set_url(self, url):\n-        assert isinstance(url, basestring), 'Request url argument must be str or unicode, got %s:' % type(url).__name__\n-        decoded_url = url if isinstance(url, unicode) else url.decode(self.encoding)\n-        self._url = Url(safe_url_string(decoded_url, self.encoding))\n-        self._fingerprint = None # invalidate cached fingerprint\n-    url = property(lambda x: x._url, set_url)\n-\n-    def httpauth(self, http_user, http_pass):\n-        if not http_user:\n-            http_user = ''\n-        if not http_pass:\n-            http_pass = ''\n-        self.headers['Authorization'] = 'Basic ' + urlsafe_b64encode(\"%s:%s\" % (http_user, http_pass))\n-\n-    def __getitem__(self, key):\n-        warnings.warn(\"Request tuples have been deprecated, Request objects are now used. Index mappings: 0 -> r.url, 1 -> r.body, 2 -> r.deferred, 3 -> r.options \", DeprecationWarning)\n-        return (self.url, self.body, self.deferred, {})[key]\n-\n-    def __str__(self):\n-        if self.method != 'GET':\n-            return \"<(%s) %s>\" % (self.method, self.url)\n-        return \"<%s>\" % self.url\n-\n-    def __len__(self):\n-        \"\"\"Return raw HTTP request size\"\"\"\n-        return len(self.to_string())\n-\n-    def traceinfo(self):\n-        fp = self.fingerprint()\n-        version = '%s..%s' % (fp[:4], fp[-4:])\n-        return \"<Request: %s %s (%s)>\" % (self.method, self.url, version)\n-\n-    def __repr__(self):\n-        d = {\n-            'method': self.method,\n-            'url': self.url,\n-            'headers': self.headers,\n-            'cookies': self.cookies,\n-            'body': self.body,\n-            'context': self.context\n-            }\n-        return \"%s(%s)\" % (self.__class__.__name__, repr(d))\n-\n-    def copy(self):\n-        \"\"\"Clone request except `context` attribute\"\"\"\n-        new = copy(self)\n-        for att in self.__dict__:\n-            if att not in ['context', 'url', 'deferred', '_fingerprint']:\n-                value = getattr(self, att)\n-                setattr(new, att, copy(value))\n-        new.deferred = defer.Deferred()\n-        new.context = self.context # requests shares same context dictionary\n-        new._fingerprint = None # reset fingerprint\n-        return new\n-\n-    def fingerprint(self):\n-        \"\"\"Returns unique resource fingerprint with caching support\"\"\"\n-        if not self._fingerprint or self.fingerprint_params:\n-            self.update_fingerprint()\n-        return self._fingerprint\n-\n-    def update_fingerprint(self):\n-        \"\"\"Update request fingerprint, based on its current data. A request\n-        fingerprint is a hash which uniquely identifies the HTTP resource\"\"\"\n-\n-        headers = {}\n-        if self.fingerprint_params:\n-            if 'tamperfunc' in self.fingerprint_params:\n-                tamperfunc = self.fingerprint_params['tamperfunc']\n-                assert callable(tamperfunc)\n-                req = tamperfunc(self.copy())\n-                assert isinstance(req, Request)\n-                try:\n-                    del req.fingerprint_params['tamperfunc']\n-                except KeyError:\n-                    pass\n-                return req.fingerprint()\n-\n-            if self.headers:\n-                if 'include_headers' in self.fingerprint_params:\n-                    keys = [k.lower() for k in self.fingerprint_params['include_headers']]\n-                    headers = dict([(k, v) for k, v in self.headers.items() if k.lower() in keys])\n-                elif 'exclude_headers' in self.fingerprint_params:\n-                    keys = [k.lower() for k in self.fingerprint_params['exclude_headers']]\n-                    headers = dict([(k, v) for k, v in self.headers.items() if k.lower() not in keys])\n-\n-        # fingerprint generation\n-        fp = hashlib.sha1()\n-        fp.update(canonicalize(self.url))\n-        fp.update(self.method)\n-\n-        if self.body and self.method in ['POST', 'PUT']:\n-            fp.update(self.body)\n-\n-        if headers:\n-            for k, v in sorted([(k.lower(), v) for k, v in headers.items()]):\n-                fp.update(k)\n-                fp.update(v)\n-\n-        self._fingerprint = fp.hexdigest()\n-\n-    def to_string(self):\n-        \"\"\" Return raw HTTP request representation (as string). This is\n-        provided only for reference since it's not the actual stream of bytes\n-        that will be send when performing the request (that's controlled by\n-        Twisted).\n-        \"\"\"\n-\n-        s  = \"%s %s HTTP/1.1\\r\\n\" % (self.method, self.url)\n-        s += \"Host: %s\\r\\n\" % self.url.hostname\n-        s += self.headers.to_string() + \"\\r\\n\"\n-        s += \"\\r\\n\"\n-        if self.body:\n-            s += self.body\n-            s += \"\\r\\n\"\n-        return s\n\n@@ -1,194 +0,0 @@\n-import re\n-import hashlib\n-import copy\n-\n-from BeautifulSoup import UnicodeDammit\n-\n-from scrapy.http.url import Url\n-from scrapy.http.headers import Headers\n-\n-from twisted.web import http\n-reason_phrases = http.RESPONSES \n-\n-class Response(object) :\n-    \"\"\"HTTP responses\n-\n-    Arguments:\n-        * Domain - the spider domain for the page\n-        * url - the final url for the resource\n-        * original_url - the url requested\n-        * headers - HTTP headers\n-        * status - HTTP status code\n-        * body - Body object containing the content of the response\n-    \"\"\"\n-    _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n-\n-    def __init__(self, domain, url, original_url=None, headers=None, status=200, body=None):\n-        self.domain = domain\n-        self.url = Url(url)\n-        self.original_url = Url(original_url) if original_url else url # different if redirected or escaped\n-        self.headers = Headers(headers or {})\n-        self.status = status\n-        if isinstance(body, str):\n-            self.body = ResponseBody(body, self.headers_encoding())\n-        else:\n-            self.body = body\n-        self.cached = False\n-        self.request = None # request which originated this response\n-\n-    def version(self):\n-        \"\"\"A hash of the contents of this response\"\"\"\n-        if not hasattr(self, '_version'):\n-            self._version = hashlib.sha1(self.body.to_string()).hexdigest()\n-        return self._version\n-\n-    def headers_encoding(self):\n-        content_type = self.headers.get('Content-Type')\n-        if content_type:\n-            encoding = self._ENCODING_RE.search(content_type[0])\n-            if encoding:\n-                return encoding.group(1)\n-\n-    def __repr__(self):\n-        return \"Response(domain=%s, url=%s, original_url=%s, headers=%s, status=%s, body=%s)\" % \\\n-                (repr(self.domain), repr(self.url), repr(self.original_url), repr(self.headers), repr(self.status), repr(self.body))\n-\n-    def __str__(self):\n-        version = '%s..%s' % (self.version()[:4], self.version()[-4:])\n-        return \"<Response: %s %s (%s)>\" % (self.status, self.url, version)\n-\n-    def __len__(self):\n-        \"\"\"Return raw HTTP response size\"\"\"\n-        return len(self.to_string())\n-\n-    def info(self):\n-        return \"<Response status=%s domain=%s url=%s headers=%s\" % (self.status, self.domain, self.url, self.headers)\n-\n-    def copy(self):\n-        \"\"\"Create a new Response based on the current one\"\"\"\n-        return self.replace()\n-\n-    def replace(self, **kw):\n-        \"\"\"Create a new Response with the same attributes except for those given new values.\n-\n-        Example: newresp = oldresp.replace(body=\"New body\")\n-        \"\"\"\n-        def sameheaders():\n-            return copy.deepcopy(self.headers)\n-        def samebody():\n-            return copy.deepcopy(self.body)\n-        newresp = Response(kw.get('domain', self.domain),\n-                           kw.get('url', self.url),\n-                           original_url=kw.get('original_url', self.original_url),\n-                           headers=kw.get('headers', sameheaders()),\n-                           status=kw.get('status', self.status),\n-                           body=kw.get('body', samebody()))\n-        return newresp\n-\n-    def to_string(self):\n-        \"\"\"\n-        Return raw HTTP response representation (as string). This is provided\n-        only for reference, since it's not the exact stream of bytes that was\n-        received (that's not exposed by Twisted).\n-        \"\"\"\n-\n-        s  = \"HTTP/1.1 %s %s\\r\\n\" % (self.status, reason_phrases[int(self.status)])\n-        s += self.headers.to_string() + \"\\r\\n\"\n-        s += \"\\r\\n\"\n-        if self.body:\n-            s += self.body.to_string()\n-            s += \"\\r\\n\"\n-        return s\n-\n-class ResponseBody(object):\n-    \"\"\"The body of an HTTP response\n-\n-    This handles conversion to unicode and various character encodings.\n-    \"\"\"\n-\n-    _template = r'''%s\\s*=\\s*[\"']?\\s*%s\\s*[\"']?'''\n-\n-    _httpequiv_re = _template % ('http-equiv', 'Content-Type')\n-    _content_re   = _template % ('content', r'(?P<mime>[^;]+);\\s*charset=(?P<charset>[\\w-]+)')\n-    _encoding_re  = _template % ('encoding', r'(?P<charset>[\\w-]+)')\n-\n-    XMLDECL_RE  = re.compile(r'<\\?xml\\s.*?%s' % _encoding_re, re.I)\n-\n-    METATAG_RE  = re.compile(r'<meta\\s+%s\\s+%s' % (_httpequiv_re, _content_re), re.I)\n-    METATAG_RE2 = re.compile(r'<meta\\s+%s\\s+%s' % (_content_re, _httpequiv_re), re.I)\n-\n-    def __init__(self, content, declared_encoding=None):\n-        self._content = content\n-        self._unicode_content = None\n-        self.declared_encoding = declared_encoding\n-        self._expected_encoding = None\n-        self._actual_encoding = None\n-\n-    def to_string(self, encoding=None):\n-        \"\"\"Get the body as a string. If an encoding is specified, the\n-        body will be encoded using that encoding.\n-        \"\"\"\n-        if encoding in (None, self.declared_encoding, self._actual_encoding):\n-            return self._content\n-        # should we cache this decode?\n-        return self.to_unicode().encode(encoding)\n-\n-    def to_unicode(self):\n-        \"\"\"Return body as unicode string\"\"\"\n-        if self._unicode_content:\n-            return self._unicode_content\n-        proposed = self.get_expected_encoding()\n-        dammit = UnicodeDammit(self._content, [proposed])\n-        self._actual_encoding = dammit.originalEncoding\n-        # FIXME sometimes dammit.unicode fails, even when it recognizes the encoding correctly\n-        self._unicode_content = dammit.unicode\n-        return self._unicode_content\n-\n-    def get_content(self):\n-        \"\"\"Return original content bytes\"\"\"\n-        return self._content\n-\n-    def get_declared_encoding(self):\n-        \"\"\"Get the value of the declared encoding passed to the\n-        constructor.\n-        \"\"\"\n-        return self.declared_encoding\n-\n-    def get_real_encoding(self):\n-        \"\"\"Get the real encoding, by trying first the expected and then the\n-        actual encoding.\n-        \"\"\"\n-        if self.get_expected_encoding():\n-            result = self.get_expected_encoding()\n-        else:\n-            self.to_unicode()\n-            result = self._actual_encoding\n-        return result\n-\n-    def get_expected_encoding(self):\n-        \"\"\"Get the expected encoding for the page. This is the declared\n-        encoding, or the meta tag encoding.\n-        \"\"\"\n-        if self._expected_encoding:\n-            return self._expected_encoding\n-        proposed = self.declared_encoding\n-        if not proposed:\n-            chunk = self._content[:5000]\n-            match = self.XMLDECL_RE.search(chunk) or self.METATAG_RE.search(chunk) or self.METATAG_RE2.search(chunk)\n-            if match:\n-                proposed = match.group(\"charset\")\n-        self._expected_encoding = proposed\n-        return proposed\n-\n-    def __repr__(self):\n-        return \"ResponseBody(content=%s, declared_encoding=%s)\" % (repr(self._content), repr(self.declared_encoding))\n-\n-    def __str__(self):\n-        return self.to_string()\n-\n-    def __unicode__(self):\n-        return self.to_unicode()\n-\n-    def __len__(self):\n-        return len(self._content)\n-\n\n@@ -1 +0,0 @@\n-from scrapy.item.models import ScrapedItem\n\n@@ -1,241 +0,0 @@\n-import re\n-\n-from traceback import format_exc\n-\n-from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n-from scrapy.utils.python import unique, flatten\n-from scrapy.utils.markup import replace_tags, remove_entities\n-from scrapy.utils.misc import extract_regex\n-\n-class AdaptorPipe:\n-    def __init__(self, adaptors_pipe=None):\n-        \"\"\"\n-        Receives a dictionary that maps attribute_name to a list of adaptor functions\n-        \"\"\"\n-        self.pipes = adaptors_pipe or {}\n-\n-    def append_adaptor(self, attrname, adaptor):\n-        \"\"\"\n-        Add an adaptor at the end of the provided attribute's pipeline\n-        \"\"\"\n-        if callable(adaptor):\n-            if self.pipes.get(attrname):\n-                self.pipes[attrname].append(adaptor)\n-            else:\n-                self.pipes[attrname] = [adaptor]\n-                \n-    def execute(self, attrname, value, debug=False):\n-        \"\"\"\n-        Execute pipeline for attribute name \"attrname\" and value \"value\".\n-        \"\"\"\n-        for function in self.pipes.get(attrname, []):\n-            try:\n-                if debug:\n-                    print \"  %07s | input >\" % function.func_name, repr(value)\n-                value = function(value)\n-                if debug:\n-                    print \"  %07s | output>\" % function.func_name, repr(value)\n-\n-            except Exception, e:\n-                print \"Error in '%s' adaptor. Traceback text:\" % function.func_name\n-                print format_exc()\n-                return\n-\n-        return value\n-\n-\n-############\n-# Adaptors #\n-############\n-def extract(location):\n-    \"\"\"\n-    This adaptor extracts a list of strings\n-    from 'location', which can be either a list (or tuple),\n-    or an XPathSelector.\n-    \n-    This function *always* returns a list.\n-    \"\"\"\n-    if not location:\n-        return []\n-    elif isinstance(location, XPathSelectorList):\n-        return flatten([extract(o) for o in location])\n-    elif isinstance(location, XPathSelector):\n-        return location.extract()\n-    elif isinstance(location, (list, tuple)):\n-        return flatten(location)\n-    elif isinstance(location, basestring):\n-        return [location]\n-\n-def _absolutize_links(rel_links, current_url, base_url):\n-    abs_links = []\n-    for link in rel_links:\n-        if link.startswith('/'):\n-            abs_links.append('%s%s' % (base_url, link))\n-        elif link.startswith('http://'):\n-            abs_links.append(link)\n-        else:\n-            abs_links.append('%s/%s' % (current_url, link))\n-    return abs_links\n-    \n-def extract_links(locations):\n-    \"\"\"\n-    This adaptor receives either an XPathSelector containing\n-    the desired locations for finding urls, or a tuple like (xpath, regexp)\n-    containing the xpath locations to look in, and a regular expression\n-    to parse those locations.\n-    \n-    In any case, this adaptor returns a list of absolute urls extracted.\n-    \"\"\"\n-    ret = []\n-    if locations:\n-        regexp = None\n-        if isinstance(locations, XPathSelector):\n-            locations = XPathSelectorList([locations])\n-        elif isinstance(locations, tuple):\n-            locations, regexp = locations\n-    \n-        if isinstance(locations, XPathSelectorList):\n-            if regexp:\n-                ret = locations.re(regexp)\n-            else:\n-                for selector in locations:\n-                    if selector.xmlNode.type == 'element':\n-                        if selector.xmlNode.name == 'a':\n-                            children = selector.x('child::*')\n-                            if len(children) > 1:\n-                                ret.extend(selector.x('.//@href'))\n-                                ret.extend(selector.x('.//@src'))\n-                            elif len(children) == 1 and children[0].xmlNode.name == 'img':\n-                                ret.extend(children.x('@src'))\n-                            else:\n-                                ret.extend(selector.x('@href'))\n-                        elif selector.xmlNode.name == 'img':\n-                            ret.extend(selector.x('@src'))\n-                        else:\n-                            ret.extend(selector.x('.//@href'))\n-                            ret.extend(selector.x('.//@src'))\n-                    elif selector.xmlNode.type == 'attribute' and selector.xmlNode.name in ['href', 'src']:\n-                        ret.append(selector)\n-                ret = [selector.extract() for selector in ret]\n-            current_url, base_url = re.search(r'((http://(?:www\\.)?[\\w\\d\\.-]+?)(?:/|$).*)', locations[0].response.url).groups()\n-            ret = _absolutize_links(ret, current_url, base_url)\n-    return ret\n-\n-def to_unicode(value):\n-    \"\"\"\n-    Receives a list of strings, converts\n-    it to unicode, and returns a new list.\n-    \"\"\"\n-    if isinstance(value, (list, tuple)):\n-        return [ unicode(v) for v in value ]\n-    else:\n-        raise TypeError('to_unicode adaptor must receive a list or a tuple.')\n-    \n-def regex(expr):\n-    \"\"\"\n-    This factory function returns a ready-to-use\n-    adaptor for the specified regular expression.\n-    This adaptor will accept either an XPathSelectorList\n-    or a list of strings, and will apply the provided regular\n-    expression to each of its members.\n-    \n-    This adaptor always returns a list of strings.\n-    \"\"\"\n-    def _regex(value):\n-        if isinstance(value, (XPathSelector, XPathSelectorList)):\n-            return value.re(expr)\n-        elif isinstance(value, list) and value:\n-            return flatten([extract_regex(expr, string, 'utf-8') for string in value])\n-        return value\n-    return _regex\n-\n-def unquote_all(value):\n-    \"\"\"\n-    Receives a list of strings, removes all of the\n-    entities the strings may have, and returns\n-    a new list\n-    \"\"\"\n-    return [ remove_entities(v) for v in value ]\n-\n-def unquote(value):\n-    \"\"\"\n-    Receives a list of strings, removes all of the entities\n-    the strings may have (except for &lt; and &amp;), and\n-    returns a new list\n-    \"\"\"\n-    return [ remove_entities(v, keep=['lt', 'amp']) for v in value ]\n-\n-def remove_tags(value):\n-    return [ replace_tags(v) for v in value ]\n-    \n-_remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n-def _remove_root(value):\n-    m = _remove_root_re.search(value)\n-    if m:\n-        value = m.group(1)\n-    return value\n-\n-def remove_root(value):\n-    return [ _remove_root(v) for v in value ]\n-    \n-_clean_spaces_re = re.compile(\"\\s+\", re.U)\n-def remove_multispaces(value):\n-    return [ _clean_spaces_re.sub(' ', v) for v in value ]\n-\n-def strip(value):\n-    return [ v.strip() for v in value ]\n-\n-def drop_empty_elements(value):\n-    return [ v for v in value if v ]\n-\n-def delist(value):\n-    return ' '.join(value)\n-    \n-\n-\n-#############\n-# Pipelines #\n-#############\n-\"\"\"\n-The following methods automatically generate adaptor pipelines\n-for some basic datatypes, according to the parameters you pass them.\n-\"\"\"\n-def single_pipeline(remove_root=True, remove_tags=True, do_unquote=True):\n-    pipe = [ extract,\n-             unique,\n-             to_unicode,\n-             drop_empty_elements,\n-             remove_multispaces,\n-             strip,\n-           ]\n-    if remove_root:\n-        pipe.insert(4, remove_root)\n-    if remove_tags:\n-        pipe.insert(5, remove_tags)\n-    if do_unquote:\n-        pipe.append(unquote)\n-    return pipe + [delist]\n-           \n-def url_pipeline():\n-    return [ extract_links,\n-             unique,\n-             to_unicode,\n-             drop_empty_elements,\n-           ]\n-               \n-def list_pipeline(do_extract=True):\n-    pipe = []\n-    if do_extract:\n-        pipe.append(extract)\n-    return pipe + [ unique,\n-                    to_unicode,\n-                    drop_empty_elements,\n-                    unquote,\n-                    remove_tags,\n-                    remove_root,\n-                    strip,\n-                  ]\n-                \n-def list_join_pipeline(delimiter='\\t'):\n-    return list_pipeline() + [delimiter.join]\n-\n\n@@ -1,24 +0,0 @@\n-from scrapy.item.adaptors import AdaptorPipe\n-\n-class ScrapedItem(object):\n-    \"\"\"\n-    This is the base class for all scraped items.\n-    'guid' attribute is required, and is an attribute\n-    that identifies uniquely the given scraped item.\n-    \"\"\"\n-\n-    def setadaptors(self, adaptors_pipe):\n-        \"\"\"\n-        Set adaptors to use for this item. Receives a dict of adaptors and\n-        returns the item itself.\n-        \"\"\"\n-        object.__setattr__(self, '_adaptors_pipe', AdaptorPipe(adaptors_pipe))\n-        return self\n-    \n-    def append_adaptor(self, attrname, adaptor):\n-        self._adaptors_pipe.append_adaptor(attrname, adaptor)\n-            \n-    def attribute(self, attrname, value):\n-        value = self._adaptors_pipe.execute(attrname, value)\n-        if not hasattr(self, attrname) or not getattr(self, attrname):\n-            setattr(self, attrname, value)\n\n@@ -1,101 +0,0 @@\n-from scrapy import log\n-from scrapy.core.exceptions import DropItem, NotConfigured\n-from scrapy.item import ScrapedItem\n-from scrapy.utils.misc import load_class\n-from scrapy.utils.defer import defer_succeed, mustbe_deferred\n-from scrapy.conf import settings\n-\n-class ItemPipeline(object):\n-\n-    def __init__(self):\n-        self.loaded = False\n-        self.pipeline = []\n-        self.domaininfo = {}\n-        self.load()\n-\n-    def load(self):\n-        \"\"\"\n-        Load pipelines stages defined in settings module\n-        \"\"\"\n-        for stage in settings.getlist('ITEM_PIPELINES') or ():\n-            cls = load_class(stage)\n-            if cls:\n-                try:\n-                    stageinstance = cls()\n-                    self.pipeline.append(stageinstance)\n-                except NotConfigured:\n-                    pass\n-        log.msg(\"Enabled item pipelines: %s\" % \", \".join([type(p).__name__ for p in self.pipeline]))\n-        self.loaded = True\n-\n-    def open_domain(self, domain):\n-        self.domaininfo[domain] = set()\n-        for pipe in self.pipeline:\n-            if hasattr(pipe, 'open_domain'):\n-                pipe.open_domain(domain)\n-\n-    def close_domain(self, domain):\n-        for pipe in self.pipeline:\n-            if hasattr(pipe, 'close_domain'):\n-                pipe.close_domain(domain)\n-        del self.domaininfo[domain]\n-\n-    def is_idle(self):\n-        return not self.domaininfo\n-\n-    def domain_is_idle(self, domain):\n-        return not self.domaininfo.get(domain)\n-\n-    def pipe(self, item, spider, response):\n-        \"\"\"\n-        item pipelines are instanceable classes that defines a `pipeline` method\n-        that takes ScrapedItem as input and returns ScrapedItem.\n-\n-        The output from one stage is the input of the next.\n-\n-        Raising DropItem stops pipeline.\n-\n-        This pipeline is configurable with the ITEM_PIPELINES setting\n-        \"\"\"\n-        if not self.pipeline:\n-            return defer_succeed(item)\n-\n-        domain = spider.domain_name\n-        pipeline = self.pipeline[:]\n-        current_stage = pipeline[0]\n-        info = self.domaininfo[domain]\n-        info.add(item)\n-\n-        def _next_stage(item):\n-            assert isinstance(item, ScrapedItem), 'Pipeline stages must return a ScrapedItem or raise DropItem'\n-            if not pipeline:\n-                return item\n-\n-            current_stage = pipeline.pop(0)\n-            log.msg(\"_%s_ Pipeline stage: %s\" % (item, type(current_stage).__name__), log.TRACE, domain=domain)\n-\n-            d = mustbe_deferred(current_stage.process_item, domain, response, item)\n-            d.addCallback(_next_stage)\n-            return d\n-\n-        def _ondrop(_failure):\n-            ex = _failure.value\n-            if isinstance(ex, DropItem):\n-                # TODO: current_stage is not working, check why\n-                #log.msg(\"%s: Dropped %s - %s\" % (type(current_stage).__name__, item, str(ex)), log.DEBUG, domain=domain)\n-                log.msg(\"Dropped %s - %s\" % (item, str(ex)), log.DEBUG, domain=domain)\n-                return _failure\n-            else:\n-                # TODO: current_stage is not working, check why\n-                #log.msg('%s: Error processing %s - %s' % (type(current_stage).__name__, item, _failure), log.ERROR, domain=domain)\n-                log.msg('Error processing %s - %s' % (item, _failure), log.ERROR, domain=domain)\n-\n-        def _pipeline_finished(_):\n-            log.msg(\"_%s_ Pipeline finished\" % item, log.TRACE, domain=domain)\n-            info.remove(item)\n-            return _\n-\n-        deferred = mustbe_deferred(_next_stage, item)\n-        deferred.addErrback(_ondrop)\n-        deferred.addBoth(_pipeline_finished)\n-        return deferred\n\n@@ -1,68 +0,0 @@\n-\"\"\"\n-LinkExtractor provides en efficient way to extract links from pages\n-\"\"\"\n-\n-from scrapy.utils.python import FixedSGMLParser\n-from scrapy.utils.url import urljoin_rfc as urljoin\n-\n-class LinkExtractor(FixedSGMLParser):\n-    \"\"\"LinkExtractor are used to extract links from web pages. They are\n-    instantiated and later \"applied\" to a Response using the extract_urls\n-    method which must receive a Response object and return a dict whoose keys\n-    are the (absolute) urls to follow, and its values any arbitrary data. In\n-    this case the values are the text of the hyperlink.\n-\n-    This is the base LinkExtractor class that provides enough basic\n-    functionality for extracting links to follow, but you could override this\n-    class or create a new one if you need some additional functionality. The\n-    only requisite is that the new (or overrided) class must provide a\n-    extract_urls method that receives a Response and returns a dict with the\n-    links to follow as its keys.\n-\n-    The constructor arguments are:\n-\n-    * tag (string or function)\n-      * a tag name which is used to search for links (defaults to \"a\")\n-      * a function which receives a tag name and returns whether to scan it\n-    * attr (string or function)\n-      * an attribute name which is used to search for links (defaults to \"href\")\n-      * a function which receives an attribute name and returns whether to scan it\n-    \"\"\"\n-\n-    def __init__(self, tag=\"a\", attr=\"href\"):\n-        FixedSGMLParser.__init__(self)\n-        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n-        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n-        self.inside_link = False\n-\n-    def extract_urls(self, response):\n-        self.reset()\n-        self.feed(response.body.to_string())\n-        self.close()\n-        \n-        base_url = self.base_url if self.base_url else response.url\n-        urls = {}\n-        for link, text in self.links.iteritems():\n-            urls[urljoin(base_url, link)] = text\n-        return urls\n-\n-    def reset(self):\n-        FixedSGMLParser.reset(self)\n-        self.links = {}\n-        self.base_url = None\n-\n-    def unknown_starttag(self, tag, attrs):\n-        if tag == 'base':\n-            self.base_url = dict(attrs).get('href')\n-        if self.scan_tag(tag):\n-            for attr, value in attrs:\n-                if self.scan_attr(attr):\n-                    self.links[value] = \"\"\n-                    self.inside_link = value\n-\n-    def unknown_endtag(self, tag):\n-        self.inside_link = False\n-\n-    def handle_data(self, data):\n-        if self.inside_link and not self.links.get(self.inside_link, None):\n-            self.links[self.inside_link] = data\n\n@@ -1,76 +0,0 @@\n-\"\"\"\n-This module provides some LinkExtractors, which extend that base LinkExtractor\n-(scrapy.link.LinkExtractor) with some useful features.\n-\n-\"\"\"\n-\n-import re\n-\n-from scrapy.link import LinkExtractor\n-from scrapy.utils.url import canonicalize_url, url_is_from_any_domain\n-from scrapy.utils.response import new_response_from_xpaths\n-from scrapy.utils.misc import dict_updatedefault\n-\n-_re_type = type(re.compile(\"\", 0))\n-\n-_matches = lambda url, regexs: any((r.search(url) for r in regexs))\n-_is_valid_url = lambda url: url.split('://', 1)[0] in set(['http', 'https', 'file'])\n-\n-class RegexLinkExtractor(LinkExtractor):\n-    \"\"\"RegexLinkExtractor implements extends the base LinkExtractor by\n-    providing several mechanisms to extract the links.\n-\n-    It's constructor parameters are:\n-\n-    allow - list of regexes that the (absolute urls) must match to be extracted\n-    deny - ignore urls that match any of these regexes\n-    allow_domains - only extract urls from these domains\n-    deny_domains - ignore urls from these dmoains\n-    tags - look for urls in this tags\n-    attrs - look for urls in this attrs\n-    canonicalize - canonicalize all extracted urls using scrapy.utils.url.canonicalize_url\n-\n-    Both 'allow' and 'deny' arguments can be a list of regexes strings or regex\n-    python objects (already compiled)\n-\n-    Url matching is always performed against the absolute urls, never the\n-    relative urls found in pages.\n-\n-    \"\"\"\n-    \n-    def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), \n-                 tags=('a', 'area'), attrs=('href'), canonicalize=True):\n-        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in allow]\n-        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in deny]\n-        self.allow_domains = set(allow_domains)\n-        self.deny_domains = set(deny_domains)\n-        self.restrict_xpaths = restrict_xpaths\n-        self.canonicalize = canonicalize\n-        tag_func = lambda x: x in tags\n-        attr_func = lambda x: x in attrs\n-        LinkExtractor.__init__(self, tag=tag_func, attr=attr_func)\n-\n-    def extract_urls(self, response):\n-        if self.restrict_xpaths:\n-            response = new_response_from_xpaths(response, self.restrict_xpaths)\n-\n-        url_text = LinkExtractor.extract_urls(self, response)\n-        urls = [u for u in url_text.iterkeys() if _is_valid_url(u)]\n-\n-        if self.allow_res:\n-            urls = [u for u in urls if _matches(u, self.allow_res)]\n-        if self.deny_res:\n-            urls = [u for u in urls if not _matches(u, self.deny_res)]\n-        if self.allow_domains:\n-            urls = [u for u in urls if url_is_from_any_domain(u, self.allow_domains)]\n-        if self.deny_domains:\n-            urls = [u for u in urls if not url_is_from_any_domain(u, self.deny_domains)]\n-\n-        res = {}\n-        if self.canonicalize:\n-            for u in urls:\n-                res[canonicalize_url(u)] = url_text[u]\n-        else:\n-            for u in urls:\n-                res[u] = url_text[u]\n-        return res\n\n@@ -1,57 +0,0 @@\n-\"\"\" \n-Crawler logging functionality\n-\"\"\"\n-import sys\n-\n-from twisted.python import log\n-\n-from scrapy.conf import settings\n- \n-# Logging levels\n-levels = {\n-    0: \"SILENT\",\n-    1: \"CRITICAL\",\n-    2: \"ERROR\",\n-    3: \"WARNING\",\n-    4: \"INFO\",\n-    5: \"DEBUG\",\n-    6: \"TRACE\",\n-}\n-\n-BOT_NAME = settings['BOT_NAME']\n-\n-# Set logging level module attributes\n-for v, k in levels.items():\n-    setattr(sys.modules[__name__], k, v)\n-\n-# default logging level\n-log_level = DEBUG\n-\n-started = False\n-\n-def start(logfile=None, loglevel=None, log_stdout=None):\n-    \"\"\" Init logging \"\"\"\n-    if started or not settings.getbool('LOG_ENABLED'):\n-        return\n-\n-    logfile = logfile or settings['LOGFILE']\n-    loglevel = loglevel or settings['LOGLEVEL']\n-    log_stdout = log_stdout or settings.getbool('LOG_STDOUT')\n-\n-    file = open(logfile, 'a') if logfile else sys.stderr\n-    level = int(getattr(sys.modules[__name__], loglevel)) if loglevel else DEBUG\n-    log.startLogging(file, setStdout=log_stdout)\n-    setattr(sys.modules[__name__], 'log_level', level)\n-    setattr(sys.modules[__name__], 'started', True)\n-\n-def msg(message, level=INFO, component=BOT_NAME, domain=None):\n-    \"\"\" Log message according to the level \"\"\"\n-    component = \"%s/%s\" % (BOT_NAME, domain) if domain else component\n-    if level <= log_level:\n-        log.msg(\"%s: %s\" % (levels[level], message), system=component)\n-\n-def exc(message, level=ERROR, component=BOT_NAME, domain=None):\n-    from traceback import format_exc\n-    message = message + '\\n' + format_exc()\n-    msg(message, level, component, domain)\n-\n\n@@ -1,54 +0,0 @@\n-import smtplib\n-\n-from email.MIMEMultipart import MIMEMultipart\n-from email.MIMEBase import MIMEBase\n-from email.MIMEText import MIMEText\n-from email.Utils import COMMASPACE, formatdate\n-from email import Encoders\n-\n-from scrapy import log\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-\n-class MailSender(object):\n-\n-    def __init__(self, smtphost=None, mailfrom=None):\n-        self.smtphost = smtphost if smtphost else settings['MAIL_HOST']\n-        self.mailfrom = mailfrom if mailfrom else settings['MAIL_FROM']\n-\n-        if not self.smtphost or not self.mailfrom:\n-            raise NotConfigured(\"MAIL_HOST and MAIL_FROM settings are required\")\n-\n-    def send(self, to, subject, body, cc=None, attachs=None):\n-        \"\"\"\n-        Send mail to the given recipients\n-        \n-        - to: must be a list of email recipients\n-        - attachs must be a list of tuples: (attach_name, mimetype, file_object)\n-        - body and subjet must be a string\n-        \"\"\"\n-\n-        msg = MIMEMultipart()\n-        msg['From'] = self.mailfrom\n-        msg['To'] = COMMASPACE.join(to)\n-        msg['Date'] = formatdate(localtime=True)\n-        msg['Subject'] = subject\n-        rcpts = to[:]\n-        if cc:\n-            rcpts.extend(cc)\n-            msg['Cc'] = COMMASPACE.join(cc)\n-\n-        msg.attach(MIMEText(body))\n-\n-        for attach_name, mimetype, f in (attachs or []):\n-            part = MIMEBase(*mimetype.split('/'))\n-            part.set_payload(f.read())\n-            Encoders.encode_base64(part)\n-            part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' % attach_name)\n-            msg.attach(part)\n-\n-        smtp = smtplib.SMTP(self.smtphost)\n-        smtp.sendmail(self.mailfrom, rcpts, msg.as_string())\n-        log.msg('Mail sent: To=%s Cc=%s Subject=\"%s\"' % (to, cc, subject))\n-        smtp.close()\n-\n\n@@ -1,69 +0,0 @@\n-import re\n-import socket\n-from datetime import datetime\n-\n-from twisted.internet import reactor\n-from twisted.web import server, resource\n-from pydispatch import dispatcher\n-\n-from scrapy.core.engine import scrapyengine\n-from scrapy.conf import settings\n-\n-# web management signals\n-webconsole_discover_module = object()\n-\n-urlpath_re = re.compile(r\"^/(\\w+)/\")\n-\n-def banner(module=None):\n-    s  = \"<html>\\n\"\n-    s += \"<head><title>Scrapy</title></head>\\n\"\n-    s += \"<body>\\n\"\n-    s += \"<h1><a href='/'>Scrapy web console</a></h1>\\n\"\n-    now = datetime.now()\n-    uptime = now - scrapyengine.start_time\n-    s += \"<p>Bot: <b>%s</b> | Host: <b>%s</b> | Uptime: <b>%s</b> | Time: <b>%s</b> </p>\\n\" % (settings['BOT_NAME'], socket.gethostname(), str(uptime), str(now))\n-    if module:\n-        s += \"<h2><a href='/%s/'>%s</a></h2>\\n\" % (module.webconsole_id, module.webconsole_name)\n-    return s\n-\n-class WebConsoleResource(resource.Resource):\n-    isLeaf = True\n-\n-    @property\n-    def modules(self):\n-        if not hasattr(self, '_modules'):\n-            self._modules = {}\n-            for _, obj in dispatcher.send(signal=webconsole_discover_module, sender=self.__class__):\n-                self._modules[obj.webconsole_id] = obj\n-        return self._modules\n-\n-    def render_GET(self, request):\n-        m = urlpath_re.search(request.path)\n-        if m:\n-            return self.modules[m.group(1)].webconsole_render(request)\n-        else:\n-            return self.module_list()\n-\n-    render_POST = render_GET\n-\n-    def module_list(self):\n-        s  = banner()\n-        s += \"<p>Available modules:</p>\\n\"\n-        s += \"<ul>\\n\"\n-        for name, obj in self.modules.iteritems():\n-            s += \"<li><a href='/%s/'>%s</a></li>\\n\" % (name, obj.webconsole_name)\n-        s += \"</ul>\\n\"\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-        return s\n-\n-class WebConsole(server.Site):\n-\n-    def __init__(self):\n-        if not settings.getbool('WEBCONSOLE_ENABLED'):\n-            return\n-\n-        server.Site.__init__(self, WebConsoleResource())\n-        self.noisy = False\n-        port = settings.getint('WEBCONSOLE_PORT')\n-        scrapyengine.listenTCP(port, self)\n\n@@ -1,45 +0,0 @@\n-\"\"\"\n-Monkey patches\n-\n-These are generally a bad idea.\n-\"\"\"\n-from twisted.web.client import HTTPClientFactory\n-\n-\n-# Extend limit for BeautifulSoup parsing loops\n-#import sys\n-#sys.setrecursionlimit(7400)\n-\n-def apply_patches():\n-    patch_HTTPClientFactory_gotHeaders()\n-\n-\n-# XXX: Monkeypatch for twisted.web.client-HTTPClientFactory\n-# HTTPClientFactory.gotHeaders dies when parsing malformed cookies,\n-# and the crawler is getting malformed cookies from this site.\n-_old_gotHeaders = HTTPClientFactory.gotHeaders\n-# Cookies format: http://www.ietf.org/rfc/rfc2109.txt\n-# I have choosen not to filter based on this, so we don't filter invalid\n-# values that could be managed correctly by twisted.\n-#_COOKIES = re.compile(r'^[^=;]+=[^=;]*(;[^=;]+=[^=;])*$')\n-\n-def _is_good_cookie(cookie):\n-    \"\"\" Check if a given cookie would make gotHeaders raise an exception \"\"\"\n-    cookparts = cookie.split(';')\n-    cook = cookparts[0]\n-    return len(cook.split('=', 1)) == 2\n-\n-def _new_gotHeaders(self, headers):\n-    \"\"\" Remove cookies that would make twisted raise an exception \"\"\"\n-    if headers.has_key('set-cookie'):\n-        cookies = [cookie for cookie in headers['set-cookie']\n-                   if _is_good_cookie(cookie)]\n-        if cookies:\n-            headers['set-cookie'] = cookies\n-        else:\n-            del headers['set-cookie']\n-\n-    return _old_gotHeaders(self, headers)\n-\n-def patch_HTTPClientFactory_gotHeaders():\n-    setattr(HTTPClientFactory, 'gotHeaders', _new_gotHeaders)\n\n@@ -1,187 +0,0 @@\n-from __future__ import with_statement\n-\n-import os\n-import cPickle as pickle\n-import shutil\n-import tempfile\n-import tarfile\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.core.manager import scrapymanager\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-\n-class Replay(object):\n-    \"\"\"\n-    This is the Replay extension, used to replay a crawling and see if the\n-    items scraped are the same.\n-    \"\"\"\n-\n-    def __init__(self, repfile, mode='play', usedir=False):\n-        \"\"\"\n-        Available modes: record, play, update\n-\n-        repfile can be either a dir (is usedir=True) or a tar.gz file (if\n-        usedir=False)\n-        \"\"\"\n-\n-        # XXX: this is ugly, and should be removed. but how?\n-        cachemw = 'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware'\n-        if not cachemw in settings['DOWNLOADER_MIDDLEWARES']:\n-            raise NotConfigured(\"Cache middleware must be enabled to use Replay\")\n-\n-        self.recording = mode == 'record'\n-        self.updating = mode == 'update'\n-        self.playing = mode == 'play'\n-\n-        self.options = {}\n-        self.scraped_old = {}\n-        self.scraped_new = {}\n-        self.passed_old = {}\n-        self.passed_new = {}\n-        self.responses_old = {}\n-        self.responses_new = {}\n-\n-        #self._opendb(repfile, usedir)\n-        self._load(repfile, usedir)\n-\n-        settings.overrides['CACHE2_DIR'] = self.cache2dir\n-        settings.overrides['CACHE2_EXPIRATION_SECS'] = -1\n-        settings.overrides['CACHE2_IGNORE_MISSING'] = self.playing or self.updating\n-        settings.overrides['CACHE2_SECTORIZE'] = False\n-\n-        dispatcher.connect(self.engine_started, signal=signals.engine_started)\n-        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n-        dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n-        dispatcher.connect(self.item_passed, signal=signals.item_passed)\n-        dispatcher.connect(self.response_received, signal=signals.response_received)\n-\n-\n-    def play(self, args=None, opts=None):\n-        if self.recording:\n-            raise ValueError(\"Replay.play() not available in record mode\")\n-\n-        args = args or self.options['args']\n-        opts = opts or self.options['opts']\n-        scrapymanager.runonce(*args, **opts)\n-\n-    def record(self, args=None, opts=None):\n-        self.recording = True\n-        self.options.clear()\n-        self.options['args'] = args or []\n-        self.options['opts'] = opts or {}\n-\n-        if os.path.exists(self.cache2dir):\n-            shutil.rmtree(self.cache2dir)\n-        else:\n-            os.mkdir(self.cache2dir)\n-\n-    def update(self, args=None, opts=None):\n-        self.updating = True\n-        if (opts['pages']):\n-            args = self.options['args']\n-            opts = self.options['opts']\n-            settings.overrides['CACHE2_EXPIRATION_SECS'] = 0            \n-            settings.overrides['CACHE2_IGNORE_MISSING'] = False\n-        else:            \n-            self.play(args, opts)\n-\n-    def engine_started(self):\n-        log.msg(\"Replay: recording session in %s\" % self.repfile)\n-\n-    def engine_stopped(self):\n-        if self.recording or self.updating:\n-            log.msg(\"Replay: recorded in %s: %d/%d scraped/passed items, %d received responses\" % \\\n-                (self.repfile, len(self.scraped_old), len(self.passed_old), len(self.responses_old)))\n-        self._save()\n-        self.cleanup()\n-\n-    def item_scraped(self, item, spider):\n-        if self.recording or self.updating:\n-            self.scraped_old[str(item.guid)] = item.copy()\n-        else:\n-            self.scraped_new[str(item.guid)] = item.copy()\n-\n-    def item_passed(self, item, spider):\n-        if self.recording or self.updating:\n-            self.passed_old[str(item.guid)] = item.copy()\n-        else:\n-            self.passed_new[str(item.guid)] = item.copy()\n-\n-    def response_received(self, response, spider):\n-        #key = response.request.fingerprint()\n-        key = response.version()\n-        if (self.recording or self.updating) and key:\n-            self.responses_old[key] = response.copy()\n-        elif key:\n-            self.responses_new[key] = response.copy()\n-\n-    def _load(self, repfile, usedir):\n-        if usedir:\n-            if os.path.isdir(repfile):\n-                replay_dir = repfile\n-            elif self.recording:\n-                os.makedirs(repfile)\n-                replay_dir = repfile\n-            else:\n-                raise IOError(\"No such dir: \" % repfile)\n-        elif os.path.exists(repfile) and (self.playing or self.updating):\n-            if tarfile.is_tarfile(repfile):\n-                replay_dir = tempfile.mkdtemp(prefix=\"replay-\")\n-                tar = tarfile.open(repfile)\n-                tar.extractall(replay_dir)\n-                tar.close()\n-            else:\n-                raise IOError(\"Wrong tarfile: %s\" % repfile)\n-        elif self.recording:\n-            replay_dir = tempfile.mkdtemp(prefix=\"replay-\")\n-        else:\n-            raise IOError(\"No such file: %s\" % repfile)\n-\n-        self.cache2dir = os.path.join(replay_dir, \"httpcache\")\n-        self.replay_dir = replay_dir\n-        self.repfile = repfile\n-        self.usedir = usedir\n-\n-        self.options_path = os.path.join(replay_dir, \"options.db\")\n-        self.scraped_path = os.path.join(replay_dir, \"items_scraped.db\")\n-        self.passed_path = os.path.join(replay_dir, \"items_passed.db\")\n-        self.responses_path = os.path.join(replay_dir, \"responses.db\")\n-\n-        if self.updating:\n-            with open(self.options_path, 'r') as f:\n-                self.options = pickle.load(f)\n-        if self.playing:\n-            with open(self.options_path, 'r') as f:\n-                self.options = pickle.load(f)\n-            with open(self.responses_path, 'r') as f:\n-                self.responses_old = pickle.load(f)\n-            with open(self.scraped_path, 'r') as f:\n-                self.scraped_old = pickle.load(f)\n-            with open(self.passed_path, 'r') as f:\n-                self.passed_old = pickle.load(f)\n-\n-    def _save(self):\n-        if self.recording or self.updating:\n-            with open(self.options_path, 'w') as f:\n-                pickle.dump(self.options, f)\n-            with open(self.responses_path, 'w') as f:\n-                pickle.dump(self.responses_old, f)\n-            with open(self.scraped_path, 'w') as f:\n-                pickle.dump(self.scraped_old, f)\n-            with open(self.passed_path, 'w') as f:\n-                pickle.dump(self.passed_old, f)\n-\n-            if not self.usedir:\n-                if self.recording or self.updating:\n-                    tar = tarfile.open(self.repfile, \"w:gz\")\n-                    for name in os.listdir(self.replay_dir):\n-                        tar.add(os.path.join(self.replay_dir, name), name)\n-                    tar.close()\n-\n-    def cleanup(self):\n-        if not self.usedir and os.path.exists(self.replay_dir):\n-            shutil.rmtree(self.replay_dir)\n\n@@ -1,140 +0,0 @@\n-\"\"\"\n-SpiderManager is the class which locates and manages all website-specific\n-spiders\n-\"\"\"\n-import sys\n-import os\n-import urlparse\n-\n-from twisted.plugin import getCache\n-\n-from scrapy.spider.models import ISpider\n-from scrapy import log\n-from scrapy.conf import settings\n-from scrapy.utils.url import url_is_from_spider\n-\n-class SpiderManager(object):\n-    \"\"\"Spider locator and manager\"\"\"\n-\n-    def __init__(self):\n-        self.loaded = False\n-        self.default_domain = None\n-        self.spider_modules = settings.getlist('SPIDER_MODULES')\n-        self.force_domain = None\n-\n-    @property\n-    def all(self):\n-        self._load_on_demand()\n-        return self._alldict.values()\n-\n-    @property\n-    def enabled(self):\n-        self._load_on_demand()\n-        return self._enableddict.values()\n-\n-    def fromdomain(self, domain_name, include_disabled=True): \n-        return self.asdict(include_disabled=include_disabled).get(domain_name)\n-\n-    def fromurl(self, url, include_disabled=True):\n-        self._load_on_demand()\n-        if self.force_domain:\n-            return self._alldict.get(self.force_domain)\n-        domain = urlparse.urlparse(url).hostname\n-        domain = str(domain).replace('www.', '')\n-        if domain:\n-            if domain in self._alldict:         # try first locating by domain\n-                return self._alldict[domain]\n-            else:                               # else search spider by spider\n-                plist = self.all if include_disabled else self.enabled\n-                for p in plist:\n-                    if url_is_from_spider(url, p):\n-                        return p\n-        return self._alldict.get(self.default_domain)\n-\n-    def asdict(self, include_disabled=True):\n-        self._load_on_demand()\n-        return self._alldict if include_disabled else self._enableddict\n-\n-    def _load_on_demand(self):\n-        if not self.loaded:\n-            self.load()\n-\n-    def _enabled_spiders(self):\n-        if settings['ENABLED_SPIDERS']:\n-            return set(settings['ENABLED_SPIDERS'])\n-        elif settings['ENABLED_SPIDERS_FILE']:\n-            if os.path.exists(settings['ENABLED_SPIDERS_FILE']):\n-                lines = open(settings['ENABLED_SPIDERS_FILE']).readlines()\n-                return set([l.strip() for l in lines if not l.strip().startswith('#')])\n-            else:\n-                return set()\n-        else:\n-            return set()\n-\n-    def load(self):\n-        self._invaliddict = {}\n-        self._alldict = {}\n-        self._enableddict = {}\n-        enabled_spiders = self._enabled_spiders()\n-\n-        modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n-        for module in modules:\n-            for spider in self._getspiders(ISpider, module):\n-                try:\n-                    ISpider.validateInvariants(spider) \n-                    self._alldict[spider.domain_name] = spider\n-                    if spider.domain_name in enabled_spiders:\n-                        self._enableddict[spider.domain_name] = spider\n-                except Exception, e:\n-                    self._invaliddict[spider.domain_name] = spider\n-                    # we can't use the log module here because it may not be available yet\n-                    print \"WARNING: Could not load spider %s: %s\" % (spider, e)\n-        self.loaded = True\n-\n-    def reload(self, skip_domains=None):\n-        \"\"\"\n-        Reload all enabled spiders.\n-\n-        This discovers any spiders added under the spiders module/packages,\n-        removes any spiders removed, updates all enabled spiders code and\n-        updates list of enabled spiders from ENABLED_SPIDERS_FILE or\n-        ENABLED_SPIDERS setting.\n-\n-        Disabled spiders are intentionally excluded to avoid\n-        syntax/initialization errors. Currently running spiders are also\n-        excluded to avoid inconsistent behaviours.\n-\n-        If skip_domains is passed those spiders won't be reloaded.\n-\n-        \"\"\"\n-        skip_domains = set(skip_domains or [])\n-        modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n-        for m in modules:\n-            reload(m)\n-        self.load()  # first call to update list of enabled spiders\n-        reloaded = 0\n-        pdict = self.asdict(include_disabled=False)\n-        for domain, spider in pdict.iteritems():\n-            if not domain in skip_domains:\n-                reload(sys.modules[spider.__module__])\n-                reloaded += 1\n-        self.load()  # second call to update spider instances\n-        log.msg(\"Reloaded %d/%d scrapy spiders\" % (reloaded, len(pdict)), level=log.DEBUG)\n-\n-    def _getspiders(self, interface, package):\n-        \"\"\"\n-        This is an override of twisted.plugin.getPlugin, because we're\n-        interested in catching exceptions thrown when loading spiders such as\n-        KeyboardInterrupt\n-        \"\"\"\n-\n-        try:\n-            allDropins = getCache(package)\n-            for dropin in allDropins.itervalues():\n-                for plugin in dropin.plugins:\n-                    adapted = interface(plugin, None)\n-                    if adapted is not None:\n-                        yield adapted\n-        except KeyboardInterrupt:\n-            sys.stderr.write(\"Interrupted while loading Scrapy spiders\\n\")\n-            sys.exit(2)\n\n@@ -1,122 +0,0 @@\n-\"\"\"\n-Spider middleware manager\n-\"\"\"\n-\n-\n-from twisted.python.failure import Failure\n-\n-from scrapy import log\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class\n-from scrapy.utils.defer import mustbe_deferred\n-from scrapy.conf import settings\n-\n-def _isiterable(possible_iterator):\n-    try:\n-        return iter(possible_iterator)\n-    except TypeError:\n-        return None\n-\n-class SpiderMiddlewareManager(object):\n-    def __init__(self, callback=None, errback=None):\n-        self.loaded = False\n-        self.spider_middleware = []\n-        self.result_middleware = []\n-        self.exception_middleware = []\n-        self.filter_middleware = []\n-        self.domaininfo = {}\n-\n-        self.callback = callback or self._callback\n-        self.errback = errback or self._errback\n-        self.load()\n-\n-    def _add_middleware(self, mw):\n-        if hasattr(mw, 'process_scrape'):\n-            self.spider_middleware.append(mw.process_scrape)\n-        if hasattr(mw, 'process_result'):\n-            self.result_middleware.insert(0, mw.process_result)\n-        if hasattr(mw, 'process_exception'):\n-            self.exception_middleware.insert(0, mw.process_exception)\n-        if hasattr(mw, 'filter'):\n-            self.filter_middleware.insert(0, mw.filter)\n-\n-    def load(self):\n-        \"\"\"Load middleware defined in settings module\"\"\"\n-        mws = []\n-        for mwpath in settings.getlist('SPIDER_MIDDLEWARES') or ():\n-            cls = load_class(mwpath)\n-            if cls:\n-                try:\n-                    mw = cls()\n-                    self._add_middleware(mw)\n-                    mws.append(mw)\n-                except NotConfigured:\n-                    pass\n-        log.msg(\"Enabled spider middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]))\n-        self.loaded = True\n-\n-    def scrape(self, request, response, spider):\n-        fname = lambda f:'%s.%s' % (f.im_self.__class__.__name__, f.im_func.__name__)\n-\n-        def process_scrape(response):\n-            for method in self.spider_middleware:\n-                result = method(response=response, spider=spider)\n-                assert result is None or _isiterable(result), \\\n-                    'Middleware %s must returns None or an iterable object, got %s ' % \\\n-                    (fname(method), type(result))\n-                if result is not None:\n-                    return result\n-            return self.call(request=request, response=response, spider=spider)\n-\n-\n-        def process_result(result):\n-            for method in self.result_middleware:\n-                result = method(response=response, result=result, spider=spider)\n-                assert _isiterable(result), \\\n-                    'Middleware %s must returns an iterable object, got %s ' % \\\n-                    (fname(method), type(result))\n-            return result\n-\n-        def process_exception(_failure):\n-            exception = _failure.value\n-            for method in self.exception_middleware:\n-                result = method(response=response, exception=exception, spider=spider)\n-                assert result is None or _isiterable(result), \\\n-                    'Middleware %s must returns None, or an iterable object, got %s ' % \\\n-                    (fname(method), type(result))\n-                if result is not None:\n-                    return result\n-            return _failure\n-\n-        dfd = mustbe_deferred(process_scrape, response)\n-        dfd.addErrback(process_exception)\n-        dfd.addCallback(process_result)\n-        return dfd\n-\n-    def call(self, request, response, spider):\n-        if isinstance(response, Exception):\n-            return self.errback(request, Failure(response), spider)\n-        return self.callback(request, response, spider)\n-\n-    def _callback(self, request, response, spider):\n-        request.deferred.callback(response)\n-        return request.deferred\n-\n-    def _errback(self, request, failure, spider):\n-        request.deferred.errback(failure)\n-        return request.deferred\n-\n-\n-class DummyMiddleware(object):\n-    def process_scrape(self, response, spider):\n-        pass\n-\n-    def process_result(self, response, result, spider):\n-        return result\n-\n-    def process_exception(self, response, exception, spider):\n-        pass\n-\n-    def filter(self, item):\n-        return True\n-\n\n@@ -1,89 +0,0 @@\n-\"\"\"\n-Base class for scrapy spiders\n-\"\"\"\n-from zope.interface import Interface, Attribute, invariant, implements\n-from twisted.plugin import IPlugin\n-\n-from scrapy.core.exceptions import UsageError\n-\n-def _valid_start_urls(obj):\n-    \"\"\"Check the start urls specified are valid\"\"\"\n-    if not obj.start_urls:\n-        raise UsageError(\"A start url is required\")\n-\n-def _valid_domain_name(obj):\n-    \"\"\"Check the domain name specified is valid\"\"\"\n-    if not obj.domain_name:\n-        raise UsageError(\"A site domain name is required\")\n-\n-def _valid_download_delay(obj):\n-    \"\"\"Check the download delay is valid, if specified\"\"\"\n-    delay = getattr(obj, 'download_delay', 0)\n-    if not type(delay) in (int, long, float):\n-        raise UsageError(\"download_delay must be numeric\")\n-    if float(delay) < 0.0:\n-        raise UsageError(\"download_delay must be positive\")\n-\n-class ISpider(Interface, IPlugin) :\n-    \"\"\"Interface to be implemented by site-specific web spiders\"\"\"\n-    \n-    start_urls = Attribute(\n-        \"\"\"A sequence of URLs to retrieve to initiate the spider for this \n-        site. A single URL may also be provided here.\"\"\")\n-    \n-    domain_name = Attribute(\n-         \"\"\"The domain name of the site to be scraped.\"\"\")\n-\n-    download_delay = Attribute(\n-         \"\"\"Optional delay in seconds to wait between web page downloads.\n-         Note that this delay does not apply to image downloads.\n-         A delay of less than a second can be specified.\"\"\")\n-\n-    user_agent = Attribute(\n-         \"\"\"Optional User-Agent to use for this domain\"\"\")\n-\n-    invariant(_valid_start_urls)\n-    invariant(_valid_domain_name)\n-    invariant(_valid_download_delay)\n-\n-    def parse(self, pagedata) :\n-        \"\"\"This is first called with the data corresponding to start_url. It\n-        must return a (possibly empty) sequence where each element is either:\n-         * A Request object for further processing.\n-         * An object that extends ScrapedItem (defined in scrapeditem module)\n-         * or None (this will be ignored)\n-\n-        When a Request object is returned, the Request is scheduled, then\n-        downloaded and finally its results is handled to the Request callback.\n-        That callback must behave the same way as this function.\n-\n-        When a ScrapedItem is returned, it is passed to the transformation pipeline\n-        and finally the destination systems are updated.\n-\n-        The simplest way to use this is to have a method in your class for each\n-        page type. So each function knows the layout and how to extract data\n-        for a single page (or set of similar pages). A typical class might work\n-        like:\n-        * parse() parses the landing page and returns Requests\n-          for a category() function to parse category pages.\n-        * category() parses the category pages and returns links and callbacks\n-          for a item() function to parse item pages.\n-        * item() parses the item details page and returns objects that\n-          extend ScrapedItem\n-        \"\"\"\n-        pass\n-\n-    def init_domain(self):\n-        \"\"\"This is first called to initialize domain specific quirks, like \n-        session cookies or login stuff\n-        \"\"\"\n-        pass\n-\n-\n-class BaseSpider(object):\n-    \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n-    class.\"\"\"\n-\n-    implements(ISpider)\n-    domain_name = None\n-    extra_domain_names = []\n\n@@ -1,69 +0,0 @@\n-\"\"\"\n-Scrapy extension for collecting scraping stats\n-\"\"\"\n-import os\n-import getpass\n-import socket\n-import datetime\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy.stats import stats\n-from scrapy.conf import settings\n-\n-class CoreStats(object):\n-    \"\"\"Scrapy core stats collector\"\"\"\n-\n-    def __init__(self):\n-        stats.setpath('_envinfo/user', getpass.getuser())\n-        stats.setpath('_envinfo/host', socket.gethostname())\n-        stats.setpath('_envinfo/logfile', settings['LOGFILE'])\n-        stats.setpath('_envinfo/pid', os.getpid())\n-\n-        dispatcher.connect(self.stats_domain_open, signal=stats.domain_open)\n-        dispatcher.connect(self.stats_domain_closing, signal=stats.domain_closing)\n-        dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n-        dispatcher.connect(self.item_passed, signal=signals.item_passed)\n-        dispatcher.connect(self.item_dropped, signal=signals.item_dropped)\n-        dispatcher.connect(self.response_downloaded, signal=signals.response_downloaded)\n-        dispatcher.connect(self.request_uploaded, signal=signals.request_uploaded)\n-\n-    def stats_domain_open(self, domain, spider):\n-        stats.setpath('%s/start_time' % domain, datetime.datetime.now())\n-        stats.setpath('%s/envinfo' % domain, stats.getpath('_envinfo'))\n-        stats.incpath('_global/domain_count/opened')\n-\n-    def stats_domain_closing(self, domain, spider, status):\n-        stats.setpath('%s/finish_time' % domain, datetime.datetime.now())\n-        stats.setpath('%s/finish_status' % domain, 'OK' if status == 'finished' else status)\n-        stats.incpath('_global/domain_count/%s' % status)\n-\n-    def item_scraped(self, item, spider):\n-        stats.incpath('%s/item_scraped_count' % spider.domain_name)\n-        stats.incpath('_global/item_scraped_count')\n-\n-    def item_passed(self, item, spider, pipe_output):\n-        stats.incpath('%s/item_passed_count' % spider.domain_name)\n-        stats.incpath('_global/item_passed_count')\n-\n-    def item_dropped(self, item, spider, exception):\n-        reason = exception.__class__.__name__\n-        stats.incpath('%s/item_dropped_count' % spider.domain_name)\n-        stats.incpath('%s/item_dropped_reasons_count/%s' % (spider.domain_name, reason))\n-        stats.incpath('_global/item_dropped_count')\n-\n-    def response_downloaded(self, response, spider):\n-        stats.incpath('%s/response_count' % spider.domain_name)\n-        stats.incpath('%s/response_status_count/%s' % (spider.domain_name, response.status))\n-        stats.incpath('_global/response_downloaded_count')\n-\n-        reslen = len(response)\n-        stats.incpath('%s/transfer/downloaded_bytes' % spider.domain_name, reslen)\n-        stats.incpath('_global/transfer/downloaded_bytes', reslen)\n-\n-    def request_uploaded(self, request, spider):\n-        reqlen = len(request)\n-        stats.incpath('%s/transfer/uploaded_bytes' % spider.domain_name, reqlen)\n-        stats.incpath('_global/transfer/uploaded_bytes', reqlen)\n-        \n\n@@ -1 +0,0 @@\n-\"\"\" \"\"\"\n\\ No newline at end of file\n\n@@ -1,84 +0,0 @@\n-# -*- coding: utf8 -*-\n-import unittest\n-from scrapy.item import adaptors\n-from scrapy.xpath.selector import XmlXPathSelector\n-from scrapy.http import Response, ResponseBody\n-\n-class AdaptorsTestCase(unittest.TestCase):\n-    def test_extract(self):\n-        sample_xsel = XmlXPathSelector(text='<xml id=\"2\"><tag1>foo<tag2>bar</tag2></tag1><tag3 value=\"mytag\">test</tag3></xml>')\n-        self.assertEqual(adaptors.extract(sample_xsel.x('/')),\n-                         ['<xml id=\"2\"><tag1>foo<tag2>bar</tag2></tag1><tag3 value=\"mytag\">test</tag3></xml>'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('xml/*')),\n-                         ['<tag1>foo<tag2>bar</tag2></tag1>', '<tag3 value=\"mytag\">test</tag3>'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('xml/@id')), ['2'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1')), ['<tag1>foo<tag2>bar</tag2></tag1>'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1//text()')),\n-                         ['foo', 'bar'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//text()')),\n-                         ['foo', 'bar', 'test'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag3/@value')), ['mytag'])\n-        \n-    def test_extract_links(self):\n-        test_data = \"\"\"<html><body>\n-                         <div>\n-                           <a href=\"lala1.html\">lala1</a>\n-                           <a href=\"/lala2.html\">lala2</a>\n-                           <a href=\"http://foobar.com/lala3.html\">lala3</a>\n-                           <a href=\"lala4.html\"><img src=\"lala4.jpg\" /></a>\n-                           <a onclick=\"javascript: opensomething('/my_html1.html');\">something1</a>\n-                           <a onclick=\"javascript: opensomething('my_html2.html');\">something2</a>\n-                         </div>\n-                       </body></html>\"\"\"\n-        sample_response = Response('foobar.com', 'http://foobar.com/dummy', body=ResponseBody(test_data))\n-        sample_xsel = XmlXPathSelector(sample_response)\n-        self.assertEqual(adaptors.extract_links(sample_xsel.x('//@href')),\n-                         [u'http://foobar.com/dummy/lala1.html', u'http://foobar.com/lala2.html',\n-                          u'http://foobar.com/lala3.html', u'http://foobar.com/dummy/lala4.html'])\n-        self.assertEqual(adaptors.extract_links(sample_xsel.x('//a')),\n-                         [u'http://foobar.com/dummy/lala1.html', u'http://foobar.com/lala2.html',\n-                          u'http://foobar.com/lala3.html', u'http://foobar.com/dummy/lala4.jpg'])\n-        self.assertEqual(adaptors.extract_links((sample_xsel.x('//a[@onclick]'), r'opensomething\\(\\'(.*?)\\'\\)')),\n-                         [u'http://foobar.com/my_html1.html', u'http://foobar.com/dummy/my_html2.html'])\n-        \n-    def test_to_unicode(self):\n-        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'luluñ', 1, 'áé']),\n-                         [u'lala', u'lele', u'lulu\\xf1', u'1', u'\\xe1\\xe9'])\n-        \n-    def test_regex(self):\n-        func = adaptors.regex(r'href=\"(.*?)\"')\n-        self.assertEqual(func(['<a href=\"lala.com\">dsa</a><a href=\"pepe.co.uk\"></a>',\n-                               '<a href=\"das.biz\">href=\"lelelel.net\"</a>']),\n-                         ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])\n-        \n-    def test_unquote_all(self):\n-        self.assertEqual(adaptors.unquote_all([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n-        \n-    def test_unquote(self):\n-        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n-        \n-    def test_remove_tags(self):\n-        test_data = ['<a href=\"lala\">adsaas<br /></a>', '<div id=\"1\"><table>dsadasf</table></div>']\n-        self.assertEqual(adaptors.remove_tags(test_data), ['adsaas', 'dsadasf'])\n-        \n-    def test_remove_root(self):\n-        self.assertEqual(adaptors.remove_root(['<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>']),\n-                         ['lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br />'])\n-        \n-    def test_remove_multispaces(self):\n-        self.assertEqual(adaptors.remove_multispaces(['  hello,  whats     up?', 'testing testingtesting      testing']),\n-                         [' hello, whats up?', 'testing testingtesting testing'])\n-        \n-    def test_strip(self):\n-        self.assertEqual(adaptors.strip([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),\n-                         ['hi there, sweety ;D', 'I CAN HAZ TEST??'])\n-        \n-    def test_drop_empty_elements(self):\n-        self.assertEqual(adaptors.drop_empty_elements([1, 2, None, 5, None, 6, None, 'hi']),\n-                         [1, 2, 5, 6, 'hi'])\n-        \n-    def test_delist(self):\n-        self.assertEqual(adaptors.delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n-                         'hi there fellas. this is my test.')\n-        \n-    \n\n@@ -1,45 +0,0 @@\n-import os\n-from unittest import TestCase, main\n-from scrapy.http import Response, ResponseBody\n-from scrapy.utils.decompressor import Decompressor\n-\n-class ScrapyDecompressTest(TestCase):\n-    uncompressed_body = ''\n-    test_responses = {}\n-    decompressor = Decompressor()\n-    \n-    def setUp(self):\n-        self.datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'compressed')\n-        formats = ['tar', 'xml.bz2', 'xml.gz', 'zip']\n-        uncompressed_fd = open(os.path.join(self.datadir, 'feed-sample1.xml'), 'r')\n-        self.uncompressed_body = uncompressed_fd.read()\n-        uncompressed_fd.close()\n-        \n-        for format in formats:\n-            fd = open(os.path.join(self.datadir, 'feed-sample1.' + format), 'r')\n-            body = ResponseBody(fd.read())\n-            fd.close()\n-            self.test_responses[format] = Response('foo.com', 'http://foo.com/bar', body=body)\n-    \n-    def test_tar(self):\n-        ret = self.decompressor.extract(self.test_responses['tar'])\n-        if ret:\n-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n-        \n-    def test_zip(self):\n-        ret = self.decompressor.extract(self.test_responses['zip'])\n-        if ret:\n-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n-    \n-    def test_gz(self):\n-        ret = self.decompressor.extract(self.test_responses['xml.gz'])\n-        if ret:\n-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n-\n-    def test_bz2(self):\n-        ret = self.decompressor.extract(self.test_responses['xml.bz2'])\n-        if ret:\n-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n-        \n-if __name__ == '__main__':\n-    main()\n\n@@ -1,9 +0,0 @@\n-import sys\n-from unittest import TestCase, main\n-\n-class DefaultEncodingTest(TestCase):\n-    def test_defaultencoding(self):\n-        self.assertEqual(sys.getdefaultencoding(), 'utf-8')\n-\n-if __name__ == \"__main__\":\n-    main()\n\n@@ -1,197 +0,0 @@\n-\"\"\"\n-Scrapy engine tests\n-\"\"\"\n-\n-import sys\n-import os\n-import urlparse\n-import unittest\n-\n-from twisted.internet import reactor\n-from twisted.web import server, resource, static, util\n-\n-#class TestResource(resource.Resource):\n-#    isLeaf = True\n-#\n-#    def render_GET(self, request):\n-#        return \"hello world!\"\n-\n-def start_test_site():\n-    root_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"sample_data\", \"test_site\")\n-    r = static.File(root_dir)\n-#    r.putChild(\"test\", TestResource())\n-    r.putChild(\"redirect\", util.Redirect(\"/redirected\"))\n-    r.putChild(\"redirected\", static.Data(\"Redirected here\", \"text/plain\"))\n-\n-    port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n-    return port\n-\n-\n-class CrawlingSession(object):\n-\n-    def __init__(self):\n-        self.domain = 'scrapytest.org'\n-        self.spider = None\n-        self.respplug = []\n-        self.reqplug = []\n-        self.itemresp = []\n-        self.signals_catched = {}\n-        self.wasrun = False\n-\n-    def run(self):\n-        self.port = start_test_site()\n-        self.portno = self.port.getHost().port\n-\n-        from scrapy.spider import spiders\n-        spiders.spider_modules = ['scrapy.tests.test_spiders']\n-        spiders.reload()\n-\n-        self.spider = spiders.fromdomain(self.domain)\n-        if self.spider:\n-            self.spider.start_urls = [\n-                self.geturl(\"/\"),\n-                self.geturl(\"/redirect\"),\n-                ]\n-\n-            from scrapy.core import signals\n-            from scrapy.core.manager import scrapymanager\n-            from scrapy.core.engine import scrapyengine\n-            from pydispatch import dispatcher\n-\n-            dispatcher.connect(self.record_signal, signals.engine_started)\n-            dispatcher.connect(self.record_signal, signals.engine_stopped)\n-            dispatcher.connect(self.record_signal, signals.domain_opened)\n-            dispatcher.connect(self.record_signal, signals.domain_idle)\n-            dispatcher.connect(self.record_signal, signals.domain_closed)\n-            dispatcher.connect(self.item_scraped, signals.item_scraped)\n-            dispatcher.connect(self.request_received, signals.request_received)\n-            dispatcher.connect(self.response_downloaded, signals.response_downloaded)\n-\n-            scrapymanager.runonce(self.domain)\n-            self.port.stopListening()\n-            self.wasrun = True\n-\n-    def geturl(self, path):\n-        return \"http://localhost:%s%s\" % (self.portno, path)\n-\n-    def getpath(self, url):\n-        u = urlparse.urlparse(url)\n-        return u.path\n-\n-    def item_scraped(self, item, spider, response):\n-        self.itemresp.append((item, response))\n-\n-    def request_received(self, request, spider):\n-        self.reqplug.append((request, spider))\n-\n-    def response_downloaded(self, response, spider):\n-        self.respplug.append((response, spider))\n-\n-    def record_signal(self, *args, **kwargs):\n-        \"\"\"Record a signal and its parameters\"\"\"\n-        signalargs = kwargs.copy()\n-        sig = signalargs.pop('signal')\n-        signalargs.pop('sender', None)\n-        self.signals_catched[sig] = signalargs\n-\n-session = CrawlingSession()\n-\n-\n-class EngineTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        if not session.wasrun:\n-            session.run()\n-\n-            # disable extensions that cause problems with tests (probably\n-            # because they leave the reactor in an unclean state)\n-            from scrapy.conf import settings\n-            settings.overrides['CLUSTER_MANAGER_ENABLED'] = 0\n-\n-    def test_spider_locator(self):\n-        \"\"\"\n-        Check the spider is loaded and located properly via the SpiderLocator\n-        \"\"\"\n-        assert session.spider is not None\n-        self.assertEqual(session.spider.domain_name, session.domain)\n-\n-    def test_visited_urls(self):\n-        \"\"\"\n-        Make sure certain URls were actually visited\n-        \"\"\"\n-        # expected urls that should be visited\n-        must_be_visited = [\"/\", \"/redirect\", \"/redirected\", \n-                           \"/item1.html\", \"/item2.html\", \"/item999.html\"]\n-\n-        urls_visited = set([rp[0].url for rp in session.respplug])\n-        urls_expected = set([session.geturl(p) for p in must_be_visited])\n-        assert urls_expected <= urls_visited, \"URLs not visited: %s\" % list(urls_expected - urls_visited)\n-\n-    def test_requests_received(self):\n-        \"\"\"\n-        Check requests received\n-        \"\"\"\n-        # 3 requests should be received from the spider. start_urls and redirects don't count\n-        self.assertEqual(3, len(session.reqplug))\n-\n-        paths_expected = ['/item999.html', '/item2.html', '/item1.html']\n-\n-        urls_requested = set([rq[0].url for rq in session.reqplug])\n-        urls_expected = set([session.geturl(p) for p in paths_expected])\n-        assert urls_expected <= urls_requested\n-\n-    def test_responses_downloaded(self):\n-        \"\"\"\n-        Check responses downloaded\n-        \"\"\"\n-        # response tests\n-        self.assertEqual(6, len(session.respplug))\n-\n-        for response, spider in session.respplug:\n-            if session.getpath(response.url) == '/item999.html':\n-                self.assertEqual('404', response.status)\n-            if session.getpath(response.url) == '/redirect':\n-                self.assertEqual('302', response.status)\n-            self.assertEqual(response.domain, spider.domain_name)\n-\n-    def test_item_data(self):\n-        \"\"\"\n-        Check item data\n-        \"\"\"\n-        # item tests\n-        self.assertEqual(2, len(session.itemresp))\n-        for item, response in session.itemresp:\n-            self.assertEqual(item.url, response.url)\n-            if 'item1.html' in item.url:\n-                self.assertEqual('Item 1 name', item.name)\n-                self.assertEqual('100', item.price)\n-            if 'item2.html' in item.url:\n-                self.assertEqual('Item 2 name', item.name)\n-                self.assertEqual('200', item.price)\n-\n-    def test_signals(self):\n-        \"\"\"\n-        Check signals were sent properly\n-        \"\"\"\n-        from scrapy.core import signals\n-\n-        assert signals.engine_started in session.signals_catched\n-        assert signals.engine_stopped in session.signals_catched\n-        assert signals.domain_opened in session.signals_catched\n-        assert signals.domain_idle in session.signals_catched\n-        assert signals.domain_closed in session.signals_catched\n-\n-        self.assertEqual({'domain': session.domain, 'spider': session.spider},\n-                         session.signals_catched[signals.domain_opened])\n-        self.assertEqual({'domain': session.domain, 'spider': session.spider},\n-                         session.signals_catched[signals.domain_idle])\n-        self.assertEqual({'domain': session.domain, 'spider': session.spider, 'status': 'finished'},\n-                         session.signals_catched[signals.domain_closed])\n-\n-if __name__ == \"__main__\":\n-    if len(sys.argv) > 1 and sys.argv[1] == 'runserver':\n-        port = start_test_site()\n-        print \"Test server running at http://localhost:%d/ - hit Ctrl-C to finish.\" % port.getHost().port\n-        reactor.run()\n-    else:\n-        unittest.main()\n\n@@ -1,230 +0,0 @@\n-import unittest\n-from scrapy.http import Request, Headers\n-from scrapy.core.scheduler import GroupFilter\n-\n-class RequestTest(unittest.TestCase):\n-    \"\"\" c14n comparison functions \"\"\"\n-\n-    def test_groupfilter(self):\n-        k1 = Request(url=\"http://www.scrapy.org/path?key=val\").fingerprint()\n-        k2 = Request(url=\"http://www.scrapy.org/path?key=val&key=val\").fingerprint()\n-        self.assertEqual(k1, k2)\n-\n-        f = GroupFilter()\n-        f.open(\"mygroup\")\n-        self.assertTrue(f.add(\"mygroup\", k1))\n-        self.assertFalse(f.add(\"mygroup\", k1))\n-        self.assertFalse(f.add(\"mygroup\", k2))\n-\n-        f.open('anothergroup')\n-        self.assertTrue(f.add(\"anothergroup\", k1))\n-        self.assertFalse(f.add(\"anothergroup\", k1))\n-        self.assertFalse(f.add(\"anothergroup\", k2))\n-\n-        f.close('mygroup')\n-        f.open('mygroup')\n-        self.assertTrue(f.add(\"mygroup\", k2))\n-        self.assertFalse(f.add(\"mygroup\", k1))\n-\n-    def test_headers(self):\n-        # Different ways of setting headers attribute\n-        url = 'http://www.scrapy.org'\n-        headers = {'Accept':'gzip', 'Custom-Header':'nothing to tell you'}\n-        r = Request(url=url, headers=headers)\n-        p = Request(url=url, headers=r.headers)\n-\n-        self.assertEqual(r.headers, p.headers)\n-        self.assertFalse(r.headers is headers)\n-        self.assertFalse(p.headers is r.headers)\n-\n-        # headers must not be unicode\n-        h = Headers({'key1': u'val1', u'key2': 'val2'})\n-        h[u'newkey'] = u'newval'\n-        for k,v in h.iteritems():\n-            self.assert_(isinstance(k, str))\n-            self.assert_(isinstance(v, str))\n-\n-        r = Request(url=\"http://www.example.org/1\", referer=u\"http://www.example.org\")\n-        self.assert_(isinstance(r.headers['referer'], str))\n-\n-    def test_eq(self):\n-        url = 'http://www.scrapy.org'\n-        r1 = Request(url=url)\n-        r2 = Request(url=url)\n-        self.assertNotEqual(r1, r2)\n-\n-        set_ = set()\n-        set_.add(r1)\n-        set_.add(r2)\n-        self.assertEqual(len(set_), 2)\n-\n-    def test_fingerprint(self):\n-        url = 'http://www.scrapy.org'\n-        r = Request(url=url)\n-        urlhash = r.fingerprint()\n-\n-        # fingerprint including all initials headers\n-        r.fingerprint_params['exclude_headers'] = []\n-        fullhash = r.fingerprint()\n-        del r.fingerprint_params['exclude_headers']\n-\n-        # all headers are excluded from fingerprint by default\n-        r.headers['Accept'] = 'application/json'\n-        accepthash = r.fingerprint()\n-        r.headers['User-Agent'] = 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.8) Gecko/20071004 Iceweasel/2.0.0.8 (Debian-2.0.0.8-1)'\n-        useragenthash = r.fingerprint()\n-        self.assertEqual(urlhash, accepthash)\n-        self.assertEqual(urlhash, useragenthash)\n-        self.assertEqual(accepthash, useragenthash)\n-\n-        # take User-Agent header in count as specified in `include_headers`\n-        r.fingerprint_params['include_headers'] = ['user-agent']\n-        useragenthash = r.fingerprint()\n-        self.assertNotEqual(useragenthash, urlhash)\n-\n-        # include_headers has precedence over exclude_headers\n-        r.fingerprint_params['exclude_headers'] = ['user-agent', 'Accept']\n-        self.assertEqual(useragenthash, r.fingerprint())\n-\n-        del r.fingerprint_params['include_headers']\n-        self.assertNotEqual(useragenthash, r.fingerprint()) # exclude_headers is excluding 'User-Agent' from hash\n-        self.assertEqual(fullhash, r.fingerprint()) # all headers previously seted was excluded\n-\n-        # set more headers\n-        r.headers['Accept-Language'] = 'en-us,en;q=0.5'\n-        r.headers['SESSIONID'] = 'an ugly session id header'\n-        self.assertNotEqual(urlhash, r.fingerprint())\n-\n-        # force emtpy include_headers (ignore exclude_headers)\n-        r.fingerprint_params['include_headers'] = []\n-        self.assertEqual(urlhash, r.fingerprint())\n-\n-        # Tamper Function\n-        r.fingerprint_params['tamperfunc'] = lambda req: Request(url=req.url)\n-        self.assertEqual(urlhash, r.fingerprint())\n-\n-        # Compare request with None vs {} header\n-        r = Request(url=url, headers=None)\n-        o = Request(url=url, headers={})\n-        self.assertEqual(o.fingerprint(), r.fingerprint())\n-        self.assertNotEqual(r, o)\n-\n-        # Different ways of setting headers attribute\n-        headers = {'Accept':'gzip', 'Custom-Header':'nothing to tell you'}\n-        r = Request(url=url, headers=headers)\n-        p = Request(url=url, headers=r.headers)\n-        o = Request(url=url)\n-        o.headers = headers\n-        self.assertEqual(r.fingerprint(), o.fingerprint())\n-        self.assertEqual(r.fingerprint(), p.fingerprint())\n-        self.assertEqual(o.fingerprint(), p.fingerprint())\n-        self.assertNotEqual(r, o)\n-        self.assertNotEqual(r, p)\n-        self.assertNotEqual(o, p)\n-\n-        # same url, implicit method\n-        r1 = Request(url=url)\n-        r2 = Request(url=url, method='GET')\n-        self.assertEqual(r1.fingerprint(), r2.fingerprint())\n-\n-        # same url, different method\n-        r3 = Request(url=url, method='POST')\n-        self.assertNotEqual(r1.fingerprint(), r3.fingerprint())\n-\n-        # implicit POST method\n-        r3.body = ''\n-        r4 = Request(url=url, body='')\n-        self.assertEqual(r3.fingerprint(), r4.fingerprint())\n-\n-        # body is not important in GET or DELETE\n-        r1 = Request(url=url, method='get', body='data')\n-        r2 = Request(url=url, method='get')\n-        self.assertEqual(r1.fingerprint(), r2.fingerprint())\n-\n-        r1 = Request(url=url, method='delete', body='data')\n-        r2 = Request(url=url, method='delete')\n-        self.assertEqual(r1.fingerprint(), r2.fingerprint())\n-\n-        # no body by default\n-        r1 = Request(url=url, method='POST')\n-        r2 = Request(url=url, method='POST', body=None)\n-        self.assertEqual(r1.fingerprint(), r2.fingerprint())\n-\n-        # empty body is equal to None body\n-        r3 = Request(url=url, method='POST', body='')\n-        self.assertEqual(r1.fingerprint(), r3.fingerprint())\n-\n-    def test_insensitive_request_fingerprints(self):\n-        url = 'http://www.scrapy.org'\n-        fp = {'include_headers':['Accept','Content-Type']}\n-        r1a = Request(url=url.lower())\n-        r1b = Request(url=url.upper())\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get')\n-        r1b = Request(url=url.upper(), method='GET')\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-\n-        r1c = Request(url=url.upper(), method='GET', body='this is not important')\n-        self.assertEqual(r1b.fingerprint(), r1c.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get', fingerprint_params=fp)\n-        r1b = Request(url=url.upper(), method='GET', fingerprint_params=fp)\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get', headers={'ACCEPT':'Black'})\n-        r1b = Request(url=url.upper(), method='GET', headers={'ACCEPT':'Black'})\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get', fingerprint_params=fp, headers={'ACCEPT':'Black'})\n-        r1b = Request(url=url.upper(), method='GET', fingerprint_params=fp, headers={'ACCEPT':'Black'})\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get', fingerprint_params=fp, headers={'ACCEPT':'Black'})\n-        r1b = Request(url=url.upper(), method='GET', fingerprint_params=fp, headers={'Accept':'Black'})\n-        r1c = Request(url=url.upper(), method='gEt', fingerprint_params=fp, headers={'accept':'Black'})\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-        self.assertEqual(r1a.fingerprint(), r1c.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get', fingerprint_params=fp, headers={'ACCEPT':'Black', 'content-type':'application/json'})\n-        r1b = Request(url=url.upper(), method='GET', fingerprint_params=fp, headers={'Accept':'Black', 'Content-Type':'application/json'})\n-        r1c = Request(url=url.upper(), method='Get', fingerprint_params=fp, headers={'accepT':'Black', 'CONTENT-TYPE':'application/json'})\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-        self.assertEqual(r1a.fingerprint(), r1c.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='Post', fingerprint_params=fp, headers={'ACCEPT':'Black', 'content-type':'application/json'})\n-        r1b = Request(url=url.upper(), method='POST', fingerprint_params=fp, headers={'Accept':'Black', 'Content-Type':'application/json'})\n-        r1c = Request(url=url.upper(), method='posT', fingerprint_params=fp, headers={'accepT':'Black', 'CONTENT-TYPE':'application/json'})\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-        self.assertEqual(r1a.fingerprint(), r1c.fingerprint())\n-\n-    def test_url(self):\n-        \"\"\"Request url tests\"\"\"\n-        r = Request(url=\"http://www.scrapy.org/path\")\n-        self.assertEqual(r.url, \"http://www.scrapy.org/path\")\n-\n-        # url quoting on attribute assign\n-        r.url = \"http://www.scrapy.org/blank%20space\"\n-        self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n-        r.url = \"http://www.scrapy.org/blank space\"\n-        self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n-\n-        # url quoting on creation\n-        r = Request(url=\"http://www.scrapy.org/blank%20space\")\n-        self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n-        r = Request(url=\"http://www.scrapy.org/blank space\")\n-        self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n-\n-        # url coercion to string\n-        r.url = u\"http://www.scrapy.org/test\"\n-        self.assert_(isinstance(r.url, str))\n-\n-        # url encoding\n-        r = Request(url=u\"http://www.scrapy.org/price/\\xa3\", url_encoding=\"utf-8\")\n-        self.assert_(isinstance(r.url, str))\n-        self.assertEqual(r.url, \"http://www.scrapy.org/price/%C2%A3\")\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n-\n-\n\n@@ -1,20 +0,0 @@\n-# -*- coding: utf-8 -*-\n-from unittest import TestCase, main\n-import libxml2\n-from scrapy.http import ResponseBody\n-\n-class ResponseTest(TestCase):\n-    def test_responsebodyencoding(self):\n-        string = 'кириллический текст'\n-        unicode_string = unicode(string, 'utf-8')\n-        body_cp1251 = unicode_string.encode('cp1251')\n-        body = ResponseBody(body_cp1251, 'cp1251')\n-        self.assertEqual(body.to_unicode(), unicode_string)\n-        self.assertEqual(isinstance(body.to_unicode(), unicode), True)\n-        self.assertEqual(body.to_string('utf-8'), string)\n-        self.assertEqual(isinstance(body.to_string('utf-8'), str), True)\n-        self.assertEqual(body.to_string(), body_cp1251)\n-        self.assertEqual(isinstance(body.to_string('utf-8'), str), True)\n-\n-if __name__ == \"__main__\":\n-    main()\n\n@@ -1,28 +0,0 @@\n-import unittest\n-from scrapy.http import Url\n-\n-class UrlClassTest(unittest.TestCase):\n-\n-    def test_url_attributes(self):\n-        u = Url(\"http://scrapy.org/wiki/info\")\n-        self.assertEqual(\"scrapy.org\", u.hostname)\n-        self.assertEqual(\"/wiki/info\", u.path)\n-        self.assertEqual(None, u.username)\n-        self.assertEqual(None, u.password)\n-\n-        u = Url(\"http://someuser:somepass@example.com/ticket/query?owner=pablo\")\n-        self.assertEqual(\"someuser\", u.username)\n-        self.assertEqual(\"somepass\", u.password)\n-        self.assertEqual(\"example.com\", u.hostname)\n-        self.assertEqual(\"/ticket/query\", u.path)\n-        self.assertEqual(\"owner=pablo\", u.query)\n-\n-        u = Url(\"http://example.com/somepage.html#fragment-1\")\n-        self.assertEqual(\"fragment-1\", u.fragment)\n-\n-        u = Url(\"file:///home/pablo/file.txt\")\n-        self.assertEqual(\"/home/pablo/file.txt\", u.path)\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n-\n\n@@ -1,43 +0,0 @@\n-from unittest import TestCase, main\n-import libxml2\n-from scrapy.http import ResponseBody,  Response\n-\n-class Libxml2Test(TestCase):\n-    def setUp(self):\n-        libxml2.debugMemory(1)\n-\n-    def tearDown(self):\n-        libxml2.cleanupParser()\n-        leaked_bytes = libxml2.debugMemory(0)\n-        assert leaked_bytes == 0, \"libxml2 memory leak detected: %d bytes\" % leaked_bytes\n-\n-    def test_xpath(self):\n-        #this test will fail in version 2.6.27 but passes on 2.6.29+\n-        html = \"<td>1<b>2</b>3</td>\"\n-        node = libxml2.htmlParseDoc(html, 'utf-8')\n-        result = [str(r) for r in node.xpathEval('//text()')]\n-        self.assertEquals(result, ['1', '2', '3'])\n-        node.freeDoc()\n-\n-class ResponseLibxml2DocTest(TestCase):\n-    def setUp(self):\n-        libxml2.debugMemory(1)\n-\n-    def tearDown(self):\n-        libxml2.cleanupParser()\n-        leaked_bytes = libxml2.debugMemory(0)\n-        assert leaked_bytes == 0, \"libxml2 memory leak detected: %d bytes\" % leaked_bytes\n-\n-    def test_getlibxml2doc(self):\n-        # test to simulate '\\x00' char in body of html page\n-        #this method don't should raise TypeError Exception\n-        from scrapy.core.manager import scrapymanager\n-        scrapymanager.configure()\n-\n-        self.body_content = 'test problematic \\x00 body'\n-        self.problematic_body = ResponseBody(self.body_content, 'utf-8')\n-        response = Response('example.com', 'http://example.com/catalog/product/blabla-123', body=self.problematic_body)\n-        response.getlibxml2doc()\n-\n-if __name__ == \"__main__\":\n-    main()\n\n@@ -1,35 +0,0 @@\n-import unittest\n-\n-from scrapy.http import Response\n-from scrapy.link import LinkExtractor\n-\n-class LinkExtractorTestCase(unittest.TestCase):\n-    def test_basic(self):\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        <p><a href=\"/about.html\">About us</a></p>\n-        <img src=\"/logo.png\" alt=\"Company logo (not a link)\" />\n-        <p><a href=\"../othercat.html\">Other category</a></p>\n-        <p><a href=\"/\" /></p>\n-        </body></html>\"\"\"\n-        response = Response(\"example.org\", \"http://example.org/somepage/index.html\", body=html)\n-\n-        lx = LinkExtractor()  # default: tag=a, attr=href\n-        self.assertEqual(lx.extract_urls(response), \n-                         {'http://example.org/somepage/item/12.html': 'Item 12', \n-                          'http://example.org/about.html': 'About us', \n-                          'http://example.org/othercat.html': 'Other category', \n-                          'http://example.org/': ''})\n-\n-    def test_base_url(self):\n-        html = \"\"\"<html><head><title>Page title<title><base href=\"http://otherdomain.com/base/\" />\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        </body></html>\"\"\"\n-        response = Response(\"example.org\", \"http://example.org/somepage/index.html\", body=html)\n-\n-        lx = LinkExtractor()  # default: tag=a, attr=href\n-        self.assertEqual(lx.extract_urls(response), \n-                         {'http://otherdomain.com/base/item/12.html': 'Item 12'})\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,31 +0,0 @@\n-import unittest\n-from scrapy.contrib.pipeline.images import thumbnail_name, image_path\n-\n-class ImagesPipelineTestCase(unittest.TestCase):\n-    def test_image_path(self):\n-        self.assertEqual(image_path(\"https://dev.mydeco.com/mydeco.gif\"),\n-                         'dev.mydeco.com/mydeco.gif')\n-        self.assertEqual(image_path(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\"),\n-                         'www.maddiebrown.co.uk/catalogue-items/image_54642_12175_95307.jpg')\n-        self.assertEqual(image_path(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\"),\n-                         'dev.mydeco.com/two/dirs/with spaces+signs.gif')\n-        self.assertEqual(image_path(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"),\n-                         'www.dfsonline.co.uk/4507be485f38b0da8a0be9eb2e1dfab8a19223f2')\n-        self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532/\"),\n-                         'www.dorma.co.uk/images/product_details/2532.jpg')\n-        self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532\"),\n-                         'www.dorma.co.uk/images/product_details/2532')\n-\n-    def test_thumbnail_name(self):\n-        name = '50'\n-        self.assertEqual(thumbnail_name(\"/tmp/foo.jpg\", name),  \n-                         '/tmp/foo_50.jpg')\n-        self.assertEqual(thumbnail_name(\"foo.png\", name),\n-                         'foo_50.jpg')\n-        self.assertEqual(thumbnail_name(\"/tmp/foo\", name),\n-                         '/tmp/foo_50.jpg')\n-        self.assertEqual(thumbnail_name(\"/tmp/some.name/foo\", name),\n-                         '/tmp/some.name/foo_50.jpg')\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,22 +0,0 @@\n-import unittest\n-from scrapy.utils.serialization import serialize, unserialize, serialize_funcs\n-\n-class SerializationTest(unittest.TestCase):\n-    def test_string_serialization(self):\n-        \"\"\"Test simple string serialization\"\"\"\n-\n-        s = \"string to serialize\"\n-        for format in serialize_funcs.iterkeys():\n-            self.assertEqual(s, unserialize(serialize(s)))\n-\n-    def test_dict_serialization(self):\n-        \"\"\"Test dict serialization\"\"\"\n-\n-        # using only string keys/values since it's the only data type that\n-        # works with all serializers\n-        d = {'one': 'item one', 'two': 'item two'}\n-        for format in serialize_funcs.iterkeys():\n-            self.assertEqual(d, unserialize(serialize(d)))\n-        \n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,41 +0,0 @@\n-\"\"\"\n-This is a spider for the unittest sample site.\n-\n-See scrapy/tests/test_engine.py for more info.\n-\"\"\"\n-import re\n-\n-from scrapy.spider import BaseSpider\n-from scrapy.item import ScrapedItem\n-from scrapy.link import LinkExtractor\n-from scrapy.http import Request\n-\n-class TestSpider(BaseSpider):\n-    domain_name = \"scrapytest.org\"\n-    extra_domain_names = [\"localhost\"]\n-    start_urls = ['http://localhost']\n-\n-    itemurl_re = re.compile(\"item\\d+.html\")\n-    name_re = re.compile(\"<h1>(.*?)</h1>\", re.M)\n-    price_re = re.compile(\">Price: \\$(.*?)<\", re.M)\n-\n-    def parse(self, response):\n-        xlink = LinkExtractor()\n-        itemre = re.compile(self.itemurl_re)\n-        for url in xlink.extract_urls(response):\n-            if itemre.search(url):\n-                yield Request(url=url, callback=self.parse_item)\n-\n-    def parse_item(self, response):\n-        item = ScrapedItem()\n-        m = self.name_re.search(response.body.to_string())\n-        if m:\n-            item.name = m.group(1)\n-        item.url = response.url\n-        m = self.price_re.search(response.body.to_string())\n-        if m:\n-            item.price = m.group(1)\n-        return [item]\n-        \n-\n-SPIDER = TestSpider()\n\n@@ -1,76 +0,0 @@\n-from datetime import datetime, timedelta\n-from time import sleep\n-\n-from twisted.trial import unittest\n-from scrapy.conf import settings\n-\n-\n-class ProductComparisonTestCase(unittest.TestCase):\n-    \"\"\" Test product comparison functions \"\"\"\n-    \n-    def setUp(self):\n-        self.test_db = settings.get('TEST_SCRAPING_DB')\n-        if not self.test_db:\n-            raise unittest.SkipTest(\"Missing TEST_SCRAPING_DB setting\")\n-\n-        try:\n-            import MySQLdb\n-            from scrapy.utils.db import mysql_connect\n-            mysql_connect(self.test_db)\n-        except ImportError:\n-            raise unittest.SkipTest(\"MySQLdb module not available\")\n-        except MySQLdb.OperationalError:\n-            raise unittest.SkipTest(\"Test database not available at: %s\" % self.test_db)\n-\n-    def test_domaindatahistory(self):\n-        from scrapy.store.db import DomainDataHistory\n-        from time import sleep\n-\n-        ddh = DomainDataHistory(self.test_db, 'domain_data_history')\n-        c = ddh.mysql_conn.cursor()\n-        c.execute(\"DELETE FROM domain_data_history\")\n-        ddh.mysql_conn.commit()\n-\n-        def now_nomicro():\n-            now = datetime.now()\n-            return now - timedelta(microseconds=now.microsecond)\n-\n-        assert hasattr(ddh.get('scrapy.org'), '__iter__')\n-        self.assertEqual(list(ddh.get('scrapy.org')), [])\n-\n-        self.assertEqual(ddh.domain_count(), 0)\n-\n-        now = now_nomicro()\n-        ddh.put('scrapy.org', 'value', timestamp=now)\n-        self.assertEqual(list(ddh.get('scrapy.org')), [(now, 'value')])\n-        self.assertEqual(list(ddh.get('scrapy2.org')), [])\n-\n-        sleep(1)\n-        now2 = now_nomicro()\n-        ddh.put('scrapy.org', 'newvalue', timestamp=now2)\n-        self.assertEqual(list(ddh.getall('scrapy.org')), [(now2, 'newvalue'), (now, 'value')])\n-\n-        self.assertEqual(ddh.getlast('scrapy.org'), (now2, 'newvalue'))\n-        self.assertEqual(ddh.getlast('scrapy.org', offset=1), (now, 'value'))\n-        self.assertEqual(ddh.getlast('scrapy2.org'), None)\n-\n-        ddh.remove('scrapy.org')\n-        self.assertEqual(list(ddh.get('scrapy.org')), [])\n-\n-        now3 = now_nomicro()\n-        d1 = {'name': 'John', 'surname': 'Doe'}\n-        ddh.put('scrapy.org', d1, timestamp=now3)\n-        self.assertEqual(list(ddh.getall('scrapy.org')), [(now3, d1)])\n-\n-        # get path support\n-        self.assertEqual(ddh.getlast('scrapy.org', path='name'), (now3, 'John'))\n-        # behaviour for non existant paths\n-        self.assertEqual(ddh.getlast('scrapy.org', path='name2'), (now3, None))\n-\n-        self.assertEqual(ddh.domain_count(), 1)\n-\n-        self.assertEqual(list(ddh.getlast_alldomains()),\n-                        [('scrapy.org', now3, {'surname': 'Doe', 'name': 'John'})])\n-        \n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,26 +0,0 @@\n-import unittest\n-from scrapy.utils.datatypes import PriorityQueue\n-\n-class DatatypesTestCase(unittest.TestCase):\n-\n-    def test_priority_queue(self):\n-        pq = PriorityQueue()\n-\n-        pq.put('b', priority=1)\n-        pq.put('a', priority=1)\n-        pq.put('c', priority=1)\n-        pq.put('z', priority=0)\n-        pq.put('d', priority=2)\n-\n-        v = []\n-        p = []\n-        while not pq.empty():\n-            priority, value = pq.get()\n-            v.append(value)\n-            p.append(priority)\n-\n-        self.assertEqual(v, ['z', 'b', 'a', 'c', 'd'])\n-        self.assertEqual(p, [0, 1, 1, 1, 2])\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,113 +0,0 @@\n-import unittest\n-\n-from scrapy.utils.markup import remove_entities, replace_tags, remove_comments\n-from scrapy.utils.markup import remove_tags_with_content, remove_escape_chars, remove_tags\n-\n-class UtilsMarkupTest(unittest.TestCase):\n-\n-    def test_remove_entities(self):\n-        # make sure it always return uncode\n-        assert isinstance(remove_entities('no entities'), unicode)\n-        assert isinstance(remove_entities('Price: &pound;100!'),  unicode)\n-\n-        # regular conversions\n-        self.assertEqual(remove_entities(u'As low as &#163;100!'),\n-                         u'As low as \\xa3100!')\n-        self.assertEqual(remove_entities('As low as &pound;100!'),\n-                         u'As low as \\xa3100!')\n-        self.assertEqual(remove_entities('redirectTo=search&searchtext=MR0221Y&aff=buyat&affsrc=d_data&cm_mmc=buyat-_-ELECTRICAL & SEASONAL-_-MR0221Y-_-9-carat gold &frac12;oz solid crucifix pendant'),\n-                         u'redirectTo=search&searchtext=MR0221Y&aff=buyat&affsrc=d_data&cm_mmc=buyat-_-ELECTRICAL & SEASONAL-_-MR0221Y-_-9-carat gold \\xbdoz solid crucifix pendant')\n-        # keep some entities\n-        self.assertEqual(remove_entities('<b>Low &lt; High &amp; Medium &pound; six</b>', keep=['lt', 'amp']),\n-                         u'<b>Low &lt; High &amp; Medium \\xa3 six</b>')\n-\n-        # illegal entities\n-        self.assertEqual(remove_entities('a &lt; b &illegal; c &#12345678; six', remove_illegal=False),\n-                         u'a < b &illegal; c &#12345678; six')\n-        self.assertEqual(remove_entities('a &lt; b &illegal; c &#12345678; six', remove_illegal=True),\n-                         u'a < b  c  six')\n-\n-\n-\n-    def test_replace_tags(self):\n-        # make sure it always return uncode\n-        assert isinstance(replace_tags('no entities'), unicode)\n-\n-        self.assertEqual(replace_tags(u'This text contains <a>some tag</a>'),\n-                         u'This text contains some tag')\n-\n-        self.assertEqual(replace_tags('This text is very im<b>port</b>ant', ' '),\n-                         u'This text is very im port ant')\n-\n-        # multiline tags\n-        self.assertEqual(replace_tags('Click <a class=\"one\"\\r\\n href=\"url\">here</a>'),\n-                         u'Click here')\n-\n-    def test_remove_comments(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_comments('without comments'), unicode)\n-        assert isinstance(remove_comments('<!-- with comments -->'), unicode)\n-\n-        # text without comments \n-        self.assertEqual(remove_comments(u'text without comments'), u'text without comments')\n-\n-        # text with comments\n-        self.assertEqual(remove_comments(u'<!--text with comments-->'), u'')\n-        self.assertEqual(remove_comments(u'Hello<!--World-->'),u'Hello')\n-\n-    def test_remove_tags(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_tags('no tags'), unicode)\n-        assert isinstance(remove_tags('no tags', which_ones=('p',)), unicode)\n-        assert isinstance(remove_tags('<p>one tag</p>'), unicode)\n-        assert isinstance(remove_tags('<p>one tag</p>', which_ones=('p')), unicode)\n-        assert isinstance(remove_tags('<a>link</a>', which_ones=('b',)), unicode)\n-\n-        # text without tags\n-        self.assertEqual(remove_tags(u'no tags'), u'no tags')\n-        self.assertEqual(remove_tags(u'no tags', which_ones=('p','b',)), u'no tags')\n-\n-        # text with tags\n-        self.assertEqual(remove_tags(u'<p>one p tag</p>'), u'one p tag')\n-        self.assertEqual(remove_tags(u'<p>one p tag</p>', which_ones=('b',)), u'<p>one p tag</p>')\n-\n-        self.assertEqual(remove_tags(u'<b>not will removed</b><i>i will removed</i>', which_ones=('i',)),\n-                         u'<b>not will removed</b>i will removed')\n-\n-        # text with tags and attributes\n-        self.assertEqual(remove_tags(u'<p align=\"center\" class=\"one\">texty</p>'), u'texty')\n-        self.assertEqual(remove_tags(u'<p align=\"center\" class=\"one\">texty</p>', which_ones=('b',)),\n-                         u'<p align=\"center\" class=\"one\">texty</p>')\n-\n-    def test_remove_tags_with_content(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_tags_with_content('no tags'), unicode)\n-        assert isinstance(remove_tags_with_content('no tags', which_ones=('p',)), unicode)\n-        assert isinstance(remove_tags_with_content('<p>one tag</p>', which_ones=('p',)), unicode)\n-        assert isinstance(remove_tags_with_content('<a>link</a>', which_ones=('b',)), unicode)\n-\n-        # text without tags\n-        self.assertEqual(remove_tags_with_content(u'no tags'), u'no tags')\n-        self.assertEqual(remove_tags_with_content(u'no tags', which_ones=('p','b',)), u'no tags')\n-\n-        # text with tags\n-        self.assertEqual(remove_tags_with_content(u'<p>one p tag</p>'), u'<p>one p tag</p>')\n-        self.assertEqual(remove_tags_with_content(u'<p>one p tag</p>', which_ones=('p',)), u'')\n-\n-        self.assertEqual(remove_tags_with_content(u'<b>not will removed</b><i>i will removed</i>', which_ones=('i',)),\n-                         u'<b>not will removed</b>')\n-\n-    def test_remove_escape_chars(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_escape_chars('no ec'), unicode)\n-        assert isinstance(remove_escape_chars('no ec', which_ones=('\\n','\\t',)), unicode)\n-\n-        # text without escape chars\n-        self.assertEqual(remove_escape_chars(u'no ec'), u'no ec')\n-        self.assertEqual(remove_escape_chars(u'no ec', which_ones=('\\n',)), u'no ec')\n-\n-        # text with escape chars\n-        self.assertEqual(remove_escape_chars(u'escape\\n\\n'), u'escape')\n-        self.assertEqual(remove_escape_chars(u'escape\\n', which_ones=('\\t',)), u'escape\\n')\n-        self.assertEqual(remove_escape_chars(u'escape\\tchars\\n', which_ones=('\\t')), 'escapechars\\n')\n-\n\n@@ -1,151 +0,0 @@\n-import unittest\n-from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url\n-\n-class UrlUtilsTest(unittest.TestCase):\n-\n-    def test_url_is_from_any_domain(self):\n-        url = 'http://www.wheele-bin-art.co.uk/get/product/123'\n-        self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.co.uk']))\n-        self.assertFalse(url_is_from_any_domain(url, ['art.co.uk']))\n-\n-        url = 'http://wheele-bin-art.co.uk/get/product/123'\n-        self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.co.uk']))\n-        self.assertFalse(url_is_from_any_domain(url, ['art.co.uk']))\n-\n-        url = 'javascript:%20document.orderform_2581_1190810811.mode.value=%27add%27;%20javascript:%20document.orderform_2581_1190810811.submit%28%29'\n-        self.assertFalse(url_is_from_any_domain(url, ['testdomain.com']))\n-        self.assertFalse(url_is_from_any_domain(url+'.testdomain.com', ['testdomain.com']))\n-\n-    def test_safe_url_string(self):\n-        # Motoko Kusanagi (Cyborg from Ghost in the Shell)\n-        motoko = u'\\u8349\\u8599 \\u7d20\\u5b50'\n-        self.assertEqual(safe_url_string(motoko),  # note the %20 for space\n-                        '%E8%8D%89%E8%96%99%20%E7%B4%A0%E5%AD%90')\n-        self.assertEqual(safe_url_string(motoko),\n-                         safe_url_string(safe_url_string(motoko)))\n-        self.assertEqual(safe_url_string(u'\\xa9'), # copyright symbol\n-                         '%C2%A9')\n-        self.assertEqual(safe_url_string(u'\\xa9', 'iso-8859-1'),\n-                         '%A9')\n-        self.assertEqual(safe_url_string(\"http://www.scrapy.org/\"),\n-                        'http://www.scrapy.org/')\n-\n-        alessi = u'/ecommerce/oggetto/Te \\xf2/tea-strainer/1273'\n-\n-        self.assertEqual(safe_url_string(alessi),\n-                         '/ecommerce/oggetto/Te%20%C3%B2/tea-strainer/1273')\n-\n-        self.assertEqual(safe_url_string(\"http://www.example.com/test?p(29)url(http://www.another.net/page)\"),\n-                                         \"http://www.example.com/test?p(29)url(http://www.another.net/page)\")\n-        self.assertEqual(safe_url_string(\"http://www.example.com/Brochures_&_Paint_Cards&PageSize=200\"),\n-                                         \"http://www.example.com/Brochures_&_Paint_Cards&PageSize=200\")\n-                  \n-    def test_safe_download_url(self):\n-        self.assertEqual(safe_download_url('http://www.scrapy.org/../'),\n-                         'http://www.scrapy.org/')\n-        self.assertEqual(safe_download_url('http://www.scrapy.org/../../images/../image'),\n-                         'http://www.scrapy.org/image')\n-        self.assertEqual(safe_download_url('http://www.scrapy.org/dir/'),\n-                         'http://www.scrapy.org/dir/')\n-\n-    def test_url_query_parameter(self):\n-        self.assertEqual(url_query_parameter(\"product.html?id=200&foo=bar\", \"id\"),\n-                         '200')\n-        self.assertEqual(url_query_parameter(\"product.html?id=200&foo=bar\", \"notthere\", \"mydefault\"),\n-                         'mydefault')\n-        self.assertEqual(url_query_parameter(\"product.html?id=\", \"id\"),\n-                         None)\n-        self.assertEqual(url_query_parameter(\"product.html?id=\", \"id\", keep_blank_values=1),\n-                         '')\n-\n-    def test_add_or_replace_parameter(self):\n-        url = 'http://domain/test'\n-        self.assertEqual(add_or_replace_parameter(url, 'arg', 'v'),\n-                         'http://domain/test?arg=v')\n-        url = 'http://domain/test?arg1=v1&arg2=v2&arg3=v3'\n-        self.assertEqual(add_or_replace_parameter(url, 'arg4', 'v4'),\n-                         'http://domain/test?arg1=v1&arg2=v2&arg3=v3&arg4=v4')\n-        self.assertEqual(add_or_replace_parameter(url, 'arg3', 'nv3'),\n-                         'http://domain/test?arg1=v1&arg2=v2&arg3=nv3')\n-        url = 'http://domain/test?arg1=v1'\n-        self.assertEqual(add_or_replace_parameter(url, 'arg2', 'v2', sep=';'),\n-                         'http://domain/test?arg1=v1;arg2=v2')\n-        self.assertEqual(add_or_replace_parameter(\"http://domain/moreInfo.asp?prodID=\", 'prodID', '20'),\n-                         'http://domain/moreInfo.asp?prodID=20')\n-\n-    def test_url_query_cleaner(self):\n-        self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", 'id'),\n-                         'product.html?id=200')\n-        self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id', 'name']),\n-                         'product.html?id=200&name=wired')\n-\n-    def test_canonicalize_url(self):\n-        # no query arguments\n-        self.assertEqual(canonicalize_url(\"http://www.example.com\"),\n-                         \"http://www.example.com\")\n-\n-        # typical usage\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?a=1&b=2&c=3\"),\n-                                          \"http://www.example.com/do?a=1&b=2&c=3\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?c=1&b=2&a=3\"),\n-                                          \"http://www.example.com/do?a=3&b=2&c=1\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?&a=1\"),\n-                                          \"http://www.example.com/do?a=1\")\n-        \n-        # sorting by argument values\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?c=3&b=5&b=2&a=50\"),\n-                         \"http://www.example.com/do?a=50&b=2&b=5&c=3\")\n-\n-        # using keep_blank_values\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&a=2\"),\n-                                          \"http://www.example.com/do?a=2\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&a=2\", keep_blank_values=True),\n-                                          \"http://www.example.com/do?a=2&b=\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&c&a=2\"),\n-                                          \"http://www.example.com/do?a=2\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&c&a=2\", keep_blank_values=True),\n-                                          \"http://www.example.com/do?a=2&b=&c=\")\n-                        \n-        # spaces\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?q=a space&a=1\"),\n-                                          \"http://www.example.com/do?a=1&q=a+space\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?q=a+space&a=1\"),\n-                                          \"http://www.example.com/do?a=1&q=a+space\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?q=a%20space&a=1\"),\n-                                          \"http://www.example.com/do?a=1&q=a+space\")\n-\n-        # normalize percent-encoding case (in paths)\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/a%a3do\"),\n-                                          \"http://www.example.com/a%A3do\"),\n-        # normalize percent-encoding case (in query arguments)\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?k=b%a3\"),\n-                                          \"http://www.example.com/do?k=b%A3\")\n-\n-        # non-ASCII percent-encoding in paths\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/a do?a=1\"),\n-                                          \"http://www.example.com/a%20do?a=1\"),\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/a %20do?a=1\"),\n-                                          \"http://www.example.com/a%20%20do?a=1\"),\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/a do\\xc2\\xa3.html?a=1\"),\n-                                          \"http://www.example.com/a%20do%C2%A3.html?a=1\")\n-        # non-ASCII percent-encoding in query arguments\n-        self.assertEqual(canonicalize_url(u\"http://www.example.com/do?price=\\xa3500&a=5&z=3\"),\n-                                          u\"http://www.example.com/do?a=5&price=%C2%A3500&z=3\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?price=\\xc2\\xa3500&a=5&z=3\"),\n-                                          \"http://www.example.com/do?a=5&price=%C2%A3500&z=3\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?price(\\xc2\\xa3)=500&a=1\"),\n-                                          \"http://www.example.com/do?a=1&price%28%C2%A3%29=500\")\n-\n-        # urls containing auth and ports\n-        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com:81/do?now=1\"),\n-                                          u\"http://user:pass@www.example.com:81/do?now=1\")\n-\n-        # remove fragments\n-        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\"),\n-                                          u\"http://user:pass@www.example.com/do?a=1\")\n-        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\", keep_fragments=True),\n-                                          u\"http://user:pass@www.example.com/do?a=1#frag\")\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n-\n\n@@ -1,37 +0,0 @@\n-import os\n-import unittest\n-\n-from scrapy.utils.xml import xpathselector_iternodes\n-from scrapy.http import Response\n-\n-class UtilsXmlTestCase(unittest.TestCase):\n-\n-    def test_iterator(self):\n-        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\n-  <product id=\"001\">\n-    <type>Type 1</type>\n-    <name>Name 1</name>\n-  </product>\n-  <product id=\"002\">\n-    <type>Type 2</type>\n-    <name>Name 2</name>\n-  </product>\n-</products>\n-        \"\"\"\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        attrs = []\n-        for x in xpathselector_iternodes(response, 'product'):\n-            attrs.append((x.x(\"@id\").extract(), x.x(\"name/text()\").extract(), x.x(\"./type/text()\").extract()))\n-\n-        self.assertEqual(attrs, \n-                         [(['001'], ['Name 1'], ['Type 1']), (['002'], ['Name 2'], ['Type 2'])])\n-\n-    def test_iterator_text(self):\n-        body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n-        \n-        self.assertEqual([x.x(\"text()\").extract() for x in xpathselector_iternodes(body, 'product')],\n-                         [[u'one'], [u'two']])\n-\n-if __name__ == \"__main__\":\n-    unittest.main()   \n\n@@ -1,225 +0,0 @@\n-import re\n-import unittest\n-\n-import libxml2\n-\n-from scrapy.http import Response\n-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n-\n-class XPathTestCase(unittest.TestCase):\n-\n-    def setUp(self):\n-        libxml2.debugMemory(1)\n-\n-    def tearDown(self):\n-        libxml2.cleanupParser()\n-        leaked_bytes = libxml2.debugMemory(0) \n-        assert leaked_bytes == 0, \"libxml2 memory leak detected: %d bytes\" % leaked_bytes\n-\n-    def test_selector_simple(self):\n-        \"\"\"Simple selector tests\"\"\"\n-        body = \"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        xpath = HtmlXPathSelector(response)\n-\n-        xl = xpath.x('//input')\n-        self.assertEqual(2, len(xl))\n-        for x in xl:\n-            assert isinstance(x, HtmlXPathSelector)\n-\n-        self.assertEqual(xpath.x('//input').extract(),\n-                         [x.extract() for x in xpath.x('//input')])\n-\n-        self.assertEqual([x.extract() for x in xpath.x(\"//input[@name='a']/@name\")],\n-                         [u'a'])\n-        self.assertEqual([x.extract() for x in xpath.x(\"number(concat(//input[@name='a']/@value, //input[@name='b']/@value))\")],\n-                         [u'12.0'])\n-\n-        self.assertEqual(xpath.x(\"concat('xpath', 'rules')\").extract(),\n-                         [u'xpathrules'])\n-        self.assertEqual([x.extract() for x in xpath.x(\"concat(//input[@name='a']/@value, //input[@name='b']/@value)\")],\n-                         [u'12'])\n-\n-    def test_selector_same_type(self):\n-        \"\"\"Test XPathSelector returning the same type in x() method\"\"\"\n-        text = '<p>test<p>'\n-        assert isinstance(XmlXPathSelector(text=text).x(\"//p\")[0],\n-                          XmlXPathSelector)\n-        assert isinstance(HtmlXPathSelector(text=text).x(\"//p\")[0], \n-                          HtmlXPathSelector)\n-\n-    def test_selector_xml_html(self):\n-        \"\"\"Test that XML and HTML XPathSelector's behave differently\"\"\"\n-\n-        # some text which is parsed differently by XML and HTML flavors\n-        text = '<div><img src=\"a.jpg\"><p>Hello</div>'\n-\n-        self.assertEqual(XmlXPathSelector(text=text).x(\"//div\").extract(),\n-                         [u'<div><img src=\"a.jpg\"><p>Hello</p></img></div>'])\n-\n-        self.assertEqual(HtmlXPathSelector(text=text).x(\"//div\").extract(),\n-                         [u'<div><img src=\"a.jpg\"><p>Hello</p></div>'])\n-\n-    def test_selector_nested(self):\n-        \"\"\"Nested selector tests\"\"\"\n-        body = \"\"\"<body>\n-                    <div class='one'>\n-                      <ul>\n-                        <li>one</li><li>two</li>\n-                      </ul>\n-                    </div>\n-                    <div class='two'>\n-                      <ul>\n-                        <li>four</li><li>five</li><li>six</li>\n-                      </ul>\n-                    </div>\n-                  </body>\"\"\"\n-\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        x = HtmlXPathSelector(response)\n-\n-        divtwo = x.x('//div[@class=\"two\"]')\n-        self.assertEqual(divtwo.x(\"//li\").extract(),\n-                         [\"<li>one</li>\", \"<li>two</li>\", \"<li>four</li>\", \"<li>five</li>\", \"<li>six</li>\"])\n-        self.assertEqual(divtwo.x(\"./ul/li\").extract(),\n-                         [\"<li>four</li>\", \"<li>five</li>\", \"<li>six</li>\"])\n-        self.assertEqual(divtwo.x(\".//li\").extract(),\n-                         [\"<li>four</li>\", \"<li>five</li>\", \"<li>six</li>\"])\n-        self.assertEqual(divtwo.x(\"./li\").extract(),\n-                         [])\n-\n-    def test_selector_re(self):\n-        body = \"\"\"<div>Name: Mary\n-                    <ul>\n-                      <li>Name: John</li>\n-                      <li>Age: 10</li>\n-                      <li>Name: Paul</li>\n-                      <li>Age: 20</li>\n-                    </ul>\n-                    Age: 20\n-                  </div>\n-\n-               \"\"\"\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        x = HtmlXPathSelector(response)\n-\n-        name_re = re.compile(\"Name: (\\w+)\")\n-        self.assertEqual(x.x(\"//ul/li\").re(name_re),\n-                         [\"John\", \"Paul\"])\n-        self.assertEqual(x.x(\"//ul/li\").re(\"Age: (\\d+)\"),\n-                         [\"10\", \"20\"])\n-\n-    def test_selector_over_text(self):\n-        hxs = HtmlXPathSelector(text='<root>lala</root>')\n-        self.assertEqual(hxs.extract(),\n-                         u'<html><body><root>lala</root></body></html>')\n-\n-        xxs = XmlXPathSelector(text='<root>lala</root>')\n-        self.assertEqual(xxs.extract(),\n-                         u'<root>lala</root>')\n-\n-        xxs = XmlXPathSelector(text='<root>lala</root>')\n-        self.assertEqual(xxs.x('.').extract(),\n-                         [u'<root>lala</root>'])\n-\n-\n-    def test_selector_namespaces_simple(self):\n-        body = \"\"\"\n-        <test xmlns:somens=\"http://scrapy.org\">\n-           <somens:a id=\"foo\"/>\n-           <a id=\"bar\">found</a>\n-        </test>\n-        \"\"\"\n-\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        x = XmlXPathSelector(response)\n-        \n-        x.register_namespace(\"somens\", \"http://scrapy.org\")\n-        self.assertEqual(x.x(\"//somens:a\").extract(), \n-                         ['<somens:a id=\"foo\"/>'])\n-\n-\n-    def test_selector_namespaces_multiple(self):\n-        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<BrowseNode xmlns=\"http://webservices.amazon.com/AWSECommerceService/2005-10-05\"\n-            xmlns:b=\"http://somens.com\"\n-            xmlns:p=\"http://www.scrapy.org/product\" >\n-    <b:Operation>hello</b:Operation>\n-    <TestTag b:att=\"value\"><Other>value</Other></TestTag>\n-    <p:SecondTestTag><material/><price>90</price><p:name>Dried Rose</p:name></p:SecondTestTag>\n-</BrowseNode>\n-        \"\"\"\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        x = XmlXPathSelector(response)\n-\n-        x.register_namespace(\"xmlns\", \"http://webservices.amazon.com/AWSECommerceService/2005-10-05\")\n-        x.register_namespace(\"p\", \"http://www.scrapy.org/product\")\n-        x.register_namespace(\"b\", \"http://somens.com\")\n-        self.assertEqual(len(x.x(\"//xmlns:TestTag\")), 1)\n-        self.assertEqual(x.x(\"//b:Operation/text()\").extract()[0], 'hello')\n-        self.assertEqual(x.x(\"//xmlns:TestTag/@b:att\").extract()[0], 'value')\n-        self.assertEqual(x.x(\"//p:SecondTestTag/xmlns:price/text()\").extract()[0], '90')\n-        self.assertEqual(x.x(\"//p:SecondTestTag\").x(\"./xmlns:price/text()\")[0].extract(), '90')\n-        self.assertEqual(x.x(\"//p:SecondTestTag/xmlns:material\").extract()[0], '<material/>')\n-\n-    def test_http_header_encoding_precedence(self):\n-        # u'\\xa3'     = pound symbol in unicode\n-        # u'\\xc2\\xa3' = pound symbol in utf-8\n-        # u'\\xa3'     = pound symbol in latin-1 (iso-8859-1)\n-\n-        meta = u'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">'\n-        head = u'<head>' + meta + u'</head>'\n-        body_content = u'<span id=\"blank\">\\xa3</span>'\n-        body = u'<body>' + body_content + u'</body>'\n-        html = u'<html>' + head + body + u'</html>'\n-        encoding = 'utf-8'\n-        html_utf8 = html.encode(encoding)\n-\n-        headers = {'Content-Type': ['text/html; charset=utf-8']}\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", headers=headers, body=html_utf8)\n-        x = HtmlXPathSelector(response)\n-        self.assertEquals(x.x(\"//span[@id='blank']/text()\").extract(),\n-                          [u'\\xa3'])\n-\n-    def test_null_bytes(self):\n-        hxs = HtmlXPathSelector(text='<root>la\\x00la</root>')\n-        self.assertEqual(hxs.extract(),\n-                         u'<html><body><root>lala</root></body></html>')\n-\n-        xxs = XmlXPathSelector(text='<root>la\\x00la</root>')\n-        self.assertEqual(xxs.extract(),\n-                         u'<root>lala</root>')\n-\n-    def test_unquote(self):\n-        xmldoc = '\\n'.join((\n-            '<root>',\n-            '  lala',\n-            '  <node>',\n-            '    blabla&amp;more<!--comment-->a<b>test</b>oh',\n-            '    <![CDATA[lalalal&ppppp<b>PPPP</b>ppp&amp;la]]>',\n-            '  </node>',\n-            '  pff',\n-            '</root>'))\n-        xxs = XmlXPathSelector(text=xmldoc)\n-\n-        self.assertEqual(xxs.extract_unquoted(), u'')\n-\n-        self.assertEqual(xxs.x('/root').extract_unquoted(), [u''])\n-        self.assertEqual(xxs.x('/root/text()').extract_unquoted(), [\n-            u'\\n  lala\\n  ',\n-            u'\\n  pff\\n'])\n-\n-        self.assertEqual(xxs.x('//*').extract_unquoted(), [u'', u'', u''])\n-        self.assertEqual(xxs.x('//text()').extract_unquoted(), [\n-            u'\\n  lala\\n  ',\n-            u'\\n    blabla&more',\n-            u'a',\n-            u'test',\n-            u'oh\\n    ',\n-            u'lalalal&ppppp<b>PPPP</b>ppp&amp;la',\n-            u'\\n  ',\n-            u'\\n  pff\\n'])\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,458 +0,0 @@\n-\"\"\"\n-This module contains data types used by Scrapy which are not included in the\n-Python Standard Library.\n-\n-This module must not depend on any module outside the Standard Library.\n-\"\"\"\n-\n-import copy\n-import gzip\n-import Queue\n-import bisect\n-from cStringIO import StringIO\n-\n-class MergeDict(object):\n-    \"\"\"\n-    A simple class for creating new \"virtual\" dictionaries that actualy look\n-    up values in more than one dictionary, passed in the constructor.\n-    \"\"\"\n-    def __init__(self, *dicts):\n-        self.dicts = dicts\n-\n-    def __getitem__(self, key):\n-        for dict_ in self.dicts:\n-            try:\n-                return dict_[key]\n-            except KeyError:\n-                pass\n-        raise KeyError\n-\n-    def __copy__(self):\n-        return self.__class__(*self.dicts)\n-\n-    def get(self, key, default=None):\n-        try:\n-            return self[key]\n-        except KeyError:\n-            return default\n-\n-    def getlist(self, key):\n-        for dict in self.dicts:\n-            try:\n-                return dict.getlist(key)\n-            except KeyError:\n-                pass\n-        raise KeyError\n-\n-    def items(self):\n-        item_list = []\n-        for dict in self.dicts:\n-            item_list.extend(dict.items())\n-        return item_list\n-\n-    def has_key(self, key):\n-        for dict in self.dicts:\n-            if key in dict:\n-                return True\n-        return False\n-\n-    __contains__ = has_key\n-\n-    def copy(self):\n-        \"\"\" returns a copy of this object\"\"\"\n-        return self.__copy__()\n-\n-class SortedDict(dict):\n-    \"A dictionary that keeps its keys in the order in which they're inserted.\"\n-    def __init__(self, data=None):\n-        if data is None: \n-            data = {}\n-        dict.__init__(self, data)\n-        self.keyOrder = data.keys()\n-\n-    def __setitem__(self, key, value):\n-        dict.__setitem__(self, key, value)\n-        if key not in self.keyOrder:\n-            self.keyOrder.append(key)\n-\n-    def __delitem__(self, key):\n-        dict.__delitem__(self, key)\n-        self.keyOrder.remove(key)\n-\n-    def __iter__(self):\n-        for k in self.keyOrder:\n-            yield k\n-\n-    def items(self):\n-        return zip(self.keyOrder, self.values())\n-\n-    def iteritems(self):\n-        for key in self.keyOrder:\n-            yield key, dict.__getitem__(self, key)\n-\n-    def keys(self):\n-        return self.keyOrder[:]\n-\n-    def iterkeys(self):\n-        return iter(self.keyOrder)\n-\n-    def values(self):\n-        return [dict.__getitem__(self, k) for k in self.keyOrder]\n-\n-    def itervalues(self):\n-        for key in self.keyOrder:\n-            yield dict.__getitem__(self, key)\n-\n-    def update(self, dict):\n-        for k, v in dict.items():\n-            self.__setitem__(k, v)\n-\n-    def setdefault(self, key, default):\n-        if key not in self.keyOrder:\n-            self.keyOrder.append(key)\n-        return dict.setdefault(self, key, default)\n-\n-    def value_for_index(self, index):\n-        \"Returns the value of the item at the given zero-based index.\"\n-        return self[self.keyOrder[index]]\n-\n-    def insert(self, index, key, value):\n-        \"Inserts the key, value pair before the item with the given index.\"\n-        if key in self.keyOrder:\n-            n = self.keyOrder.index(key)\n-            del self.keyOrder[n]\n-            if n < index: \n-                index -= 1\n-        self.keyOrder.insert(index, key)\n-        dict.__setitem__(self, key, value)\n-\n-    def copy(self):\n-        \"Returns a copy of this object.\"\n-        # This way of initializing the copy means it works for subclasses, too.\n-        obj = self.__class__(self)\n-        obj.keyOrder = self.keyOrder\n-        return obj\n-\n-    def __repr__(self):\n-        \"\"\"\n-        Replaces the normal dict.__repr__ with a version that returns the keys\n-        in their sorted order.\n-        \"\"\"\n-        return '{%s}' % ', '.join(['%r: %r' % (k, v) for k, v in self.items()])\n-\n-class MultiValueDictKeyError(KeyError):\n-    pass\n-\n-class MultiValueDict(dict):\n-    \"\"\"\n-    A subclass of dictionary customized to handle multiple values for the same key.\n-\n-    >>> d = MultiValueDict({'name': ['Adrian', 'Simon'], 'position': ['Developer']})\n-    >>> d['name']\n-    'Simon'\n-    >>> d.getlist('name')\n-    ['Adrian', 'Simon']\n-    >>> d.get('lastname', 'nonexistent')\n-    'nonexistent'\n-    >>> d.setlist('lastname', ['Holovaty', 'Willison'])\n-\n-    This class exists to solve the irritating problem raised by cgi.parse_qs,\n-    which returns a list for every key, even though most Web forms submit\n-    single name-value pairs.\n-    \"\"\"\n-    def __init__(self, key_to_list_mapping=()):\n-        dict.__init__(self, key_to_list_mapping)\n-\n-    def __repr__(self):\n-        return \"<%s: %s>\" % (self.__class__.__name__, dict.__repr__(self))\n-\n-    def __getitem__(self, key):\n-        \"\"\"\n-        Returns the last data value for this key, or [] if it's an empty list;\n-        raises KeyError if not found.\n-        \"\"\"\n-        try:\n-            list_ = dict.__getitem__(self, key)\n-        except KeyError:\n-            raise MultiValueDictKeyError, \"Key %r not found in %r\" % (key, self)\n-        try:\n-            return list_[-1]\n-        except IndexError:\n-            return []\n-\n-    def __setitem__(self, key, value):\n-        dict.__setitem__(self, key, [value])\n-\n-    def __copy__(self):\n-        return self.__class__(dict.items(self))\n-\n-    def __deepcopy__(self, memo=None):\n-        if memo is None: \n-            memo = {}\n-        result = self.__class__()\n-        memo[id(self)] = result\n-        for key, value in dict.items(self):\n-            dict.__setitem__(result, copy.deepcopy(key, memo), copy.deepcopy(value, memo))\n-        return result\n-\n-    def get(self, key, default=None):\n-        \"Returns the default value if the requested data doesn't exist\"\n-        try:\n-            val = self[key]\n-        except KeyError:\n-            return default\n-        if val == []:\n-            return default\n-        return val\n-\n-    def getlist(self, key):\n-        \"Returns an empty list if the requested data doesn't exist\"\n-        try:\n-            return dict.__getitem__(self, key)\n-        except KeyError:\n-            return []\n-\n-    def setlist(self, key, list_):\n-        dict.__setitem__(self, key, list_)\n-\n-    def setdefault(self, key, default=None):\n-        if key not in self:\n-            self[key] = default\n-        return self[key]\n-\n-    def setlistdefault(self, key, default_list=()):\n-        if key not in self:\n-            self.setlist(key, default_list)\n-        return self.getlist(key)\n-\n-    def appendlist(self, key, value):\n-        \"Appends an item to the internal list associated with key\"\n-        self.setlistdefault(key, [])\n-        dict.__setitem__(self, key, self.getlist(key) + [value])\n-\n-    def items(self):\n-        \"\"\"\n-        Returns a list of (key, value) pairs, where value is the last item in\n-        the list associated with the key.\n-        \"\"\"\n-        return [(key, self[key]) for key in self.keys()]\n-\n-    def lists(self):\n-        \"Returns a list of (key, list) pairs.\"\n-        return dict.items(self)\n-\n-    def values(self):\n-        \"Returns a list of the last value on every key list.\"\n-        return [self[key] for key in self.keys()]\n-\n-    def copy(self):\n-        \"Returns a copy of this object.\"\n-        return self.__deepcopy__()\n-\n-    def update(self, *args, **kwargs):\n-        \"update() extends rather than replaces existing key lists. Also accepts keyword args.\"\n-        if len(args) > 1:\n-            raise TypeError, \"update expected at most 1 arguments, got %d\" % len(args)\n-        if args:\n-            other_dict = args[0]\n-            if isinstance(other_dict, MultiValueDict):\n-                for key, value_list in other_dict.lists():\n-                    self.setlistdefault(key, []).extend(value_list)\n-            else:\n-                try:\n-                    for key, value in other_dict.items():\n-                        self.setlistdefault(key, []).append(value)\n-                except TypeError:\n-                    raise ValueError, \"MultiValueDict.update() takes either a MultiValueDict or dictionary\"\n-        for key, value in kwargs.iteritems():\n-            self.setlistdefault(key, []).append(value)\n-\n-class DotExpandedDict(dict):\n-    \"\"\"\n-    A special dictionary constructor that takes a dictionary in which the keys\n-    may contain dots to specify inner dictionaries. It's confusing, but this\n-    example should make sense.\n-\n-    >>> d = DotExpandedDict({'person.1.firstname': ['Simon'], \\\n-            'person.1.lastname': ['Willison'], \\\n-            'person.2.firstname': ['Adrian'], \\\n-            'person.2.lastname': ['Holovaty']})\n-    >>> d\n-    {'person': {'1': {'lastname': ['Willison'], 'firstname': ['Simon']}, '2': {'lastname': ['Holovaty'], 'firstname': ['Adrian']}}}\n-    >>> d['person']\n-    {'1': {'lastname': ['Willison'], 'firstname': ['Simon']}, '2': {'lastname': ['Holovaty'], 'firstname': ['Adrian']}}\n-    >>> d['person']['1']\n-    {'lastname': ['Willison'], 'firstname': ['Simon']}\n-\n-    # Gotcha: Results are unpredictable if the dots are \"uneven\":\n-    >>> DotExpandedDict({'c.1': 2, 'c.2': 3, 'c': 1})\n-    {'c': 1}\n-    \"\"\"\n-    def __init__(self, key_to_list_mapping):\n-        for k, v in key_to_list_mapping.items():\n-            current = self\n-            bits = k.split('.')\n-            for bit in bits[:-1]:\n-                current = current.setdefault(bit, {})\n-            # Now assign value to current position\n-            try:\n-                current[bits[-1]] = v\n-            except TypeError: # Special-case if current isn't a dict.\n-                current = {bits[-1] : v}\n-\n-class FileDict(dict):\n-    \"\"\"\n-    A dictionary used to hold uploaded file contents. The only special feature\n-    here is that repr() of this object won't dump the entire contents of the\n-    file to the output. A handy safeguard for a large file upload.\n-    \"\"\"\n-    def __repr__(self):\n-        if 'content' in self:\n-            d = dict(self, content='<omitted>')\n-            return dict.__repr__(d)\n-        return dict.__repr__(self)\n-\n-class Sitemap(object):\n-    \"\"\"Sitemap class is used to build a map of the traversed pages\"\"\"\n-\n-    def __init__(self):\n-        self._nodes = {}\n-        self._roots = []\n-\n-    def add_node(self, url, parent_url):\n-        if not url in self._nodes:\n-            parent = self._nodes.get(parent_url, None)\n-            node = SiteNode(url)\n-            self._nodes[url] = node\n-            if parent:\n-                parent.add_child(node)\n-            else:\n-                self._roots.append(node)\n-\n-    def add_item(self, url, item):\n-        if url in self._nodes:\n-            self._nodes[url].itemnames.append(str(item))\n-    \n-    def to_string(self):\n-        s = ''.join([n.to_string(0) for n in self._roots])\n-        return s\n-\n-class SiteNode(object):\n-    \"\"\"Class to represent a site node (page, image or any other file)\"\"\"\n-\n-    def __init__(self, url):\n-        self.url = url\n-        self.itemnames = []\n-        self.children = []\n-        self.parent = None\n-\n-    def add_child(self, node):\n-        self.children.append(node)\n-        node.parent = self\n-\n-    def to_string(self, level=0):\n-        s = \"%s%s\\n\" % ('  '*level, self.url)\n-        if self.itemnames:\n-            for n in self.itemnames:\n-                s += \"%sScraped: %s\\n\" % ('  '*(level+1), n)\n-        for node in self.children:\n-            s += node.to_string(level+1)\n-        return s\n-\n-class CaselessDict(dict):\n-    def __init__(self, other=None):\n-        if other:\n-            # Doesn't do keyword args\n-            if isinstance(other, dict):\n-                for k, v in other.items():\n-                    dict.__setitem__(self, self.normkey(k), v)\n-            else:\n-                for k, v in other:\n-                    dict.__setitem__(self, self.normkey(k), v)\n-\n-    def __getitem__(self, key):\n-        return dict.__getitem__(self, self.normkey(key))\n-\n-    def __setitem__(self, key, value):\n-        dict.__setitem__(self, self.normkey(key), value)\n-\n-    def __delitem__(self, key):\n-        dict.__delitem__(self, self.normkey(key))\n-\n-    def __contains__(self, key):\n-        return dict.__contains__(self, self.normkey(key))\n-\n-    def normkey(self, key):\n-        return key.lower()\n-\n-    def has_key(self, key):\n-        return dict.has_key(self, self.normkey(key))\n-\n-    def get(self, key, def_val=None):\n-        return dict.get(self, self.normkey(key), def_val)\n-\n-    def setdefault(self, key, def_val=None):\n-        return dict.setdefault(self, self.normkey(key), def_val)\n-\n-    def update(self, other):\n-        for k, v in other.items():\n-            dict.__setitem__(self, self.normkey(k), v)\n-\n-    def fromkeys(self, iterable, value=None):\n-        d = CaselessDict()\n-        for k in iterable:\n-            dict.__setitem__(d, self.normkey(k), value)\n-        return d\n-\n-    def pop(self, key, def_val=None):\n-        return dict.pop(self, self.normkey(key), def_val)\n-\n-\n-class PriorityQueue(Queue.Queue):\n-\n-    def __len__(self):\n-        return self.qsize()\n-\n-    def _init(self, maxsize):\n-        self.maxsize = maxsize\n-        # Python 2.5 uses collections.deque, but we can't because\n-        # we need insert(pos, item) for our priority stuff\n-        self.queue = []\n-\n-    def put(self, item, priority=0, block=True, timeout=None):\n-        \"\"\"Puts an item onto the queue with a numeric priority (default is zero).\n-        \n-        Note that we are \"shadowing\" the original Queue.Queue put() method here.\n-        \"\"\"\n-        Queue.Queue.put(self, (priority, item), block, timeout)\n-\n-    def _put(self, item):\n-        \"\"\"Override of the Queue._put to support prioritisation.\"\"\"\n-        # Priorities must be integers!\n-        priority = int(item[0])\n-\n-        # Using a tuple (priority+1,) finds us the correct insertion\n-        # position to maintain the existing ordering.\n-        self.queue.insert(bisect.bisect_left(self.queue, (priority+1,)), item)\n-\n-    def _get(self):\n-        \"\"\"Override of Queue._get().  Strips the priority.\"\"\"\n-        return self.queue.pop(0)\n-\n-\n-class PriorityStack(PriorityQueue):\n-    def _put(self, item):\n-        priority = int(item[0])\n-        self.queue.insert(bisect.bisect_left(self.queue, (priority,)), item)\n-\n-class gzStringIO:\n-    \"\"\"a file like object, similar to StringIO, but gzip-compressed.\"\"\"\n-    def __init__(self, data, compress_level = 9, filename = \"\"):\n-        self._s = StringIO()\n-        g = gzip.GzipFile(filename, \"wb\", compress_level, self._s)\n-        g.write(data)\n-        g.flush()\n-        g.close()\n-    def read(self):\n-        return self._s.getvalue()\n-\n\n@@ -1,39 +0,0 @@\n-\"\"\"\n-Function for dealing with databases\n-\"\"\"\n-import re\n-from scrapy.conf import settings\n-from scrapy import log\n-from scrapy.core.engine import scrapyengine\n-\n-def mysql_connect(db_uri, **kwargs):\n-    \"\"\"\n-    Connects to a MySQL DB given a mysql URI\n-    \"\"\"\n-    import MySQLdb\n-\n-    if not db_uri or not db_uri.startswith('mysql://'):\n-        raise Exception(\"Incorrect MySQL URI: %s\" % db_uri)\n-    m = re.search(r\"mysql:\\/\\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/]+)/(?P<db>.*)$\", db_uri)\n-    if m:\n-        d = m.groupdict()\n-        if d['passwd'] is None:\n-            del(d['passwd'])\n-\n-        d.update(settings.get(\"MYSQL_CONNECTION_SETTINGS\"))\n-        d.update(kwargs)\n-        \n-        dcopy = d.copy()\n-        if dcopy.get(\"passwd\"):\n-            dcopy[\"passwd\"] = \"********\"\n-        log.msg(\"Connecting db with settings %s\" % dcopy )\n-        \n-        conn = MySQLdb.connect(**d)\n-        \n-        #this is to maintain active the connection\n-        def _ping():\n-            log.msg(\"Pinging connection to %s/%s\" % (d.get('host'), d.get('db')) )\n-            conn.ping()\n-        scrapyengine.addtask(_ping, settings.getint(\"MYSQL_CONNECTION_PING_PERIOD\", 600))\n-        \n-        return conn\n\n@@ -1,72 +0,0 @@\n-\"\"\"\n-Utility for autodetecting and decompressing responses\n-\"\"\"\n-\n-import zipfile\n-import tarfile\n-import gzip\n-import bz2\n-from scrapy.http import ResponseBody\n-try:\n-    from cStringIO import StringIO\n-except:\n-    from StringIO import StringIO\n-\n-class Decompressor(object):\n-    class ArchiveIsEmpty(Exception):\n-        pass\n-    \n-    def __init__(self):\n-        self.decompressors = {'tar': self.is_tar, 'zip': self.is_zip,\n-                              'gz': self.is_gzip, 'bz2': self.is_bzip2}        \n-    \n-    def is_tar(self, response):\n-        try:\n-            tar_file = tarfile.open(name='tar.tmp', fileobj=self.archive)\n-        except tarfile.ReadError:\n-            return False\n-        if tar_file.members:\n-            return response.replace(body=ResponseBody(tar_file.extractfile(tar_file.members[0]).read()))\n-        else:\n-            raise self.ArchiveIsEmpty\n-    \n-    def is_zip(self, response):\n-        try:\n-            zip_file = zipfile.ZipFile(self.archive)\n-        except zipfile.BadZipfile:\n-            return False\n-        namelist = zip_file.namelist()\n-        if namelist:\n-            return response.replace(body=ResponseBody(zip_file.read(namelist[0])))\n-        else:\n-            raise self.ArchiveIsEmpty\n-            \n-    def is_gzip(self, response):\n-        try:\n-            gzip_file = gzip.GzipFile(fileobj=self.archive)\n-            decompressed_body = gzip_file.read()\n-        except IOError:\n-            return False\n-        return response.replace(body=decompressed_body)\n-            \n-    def is_bzip2(self, response):\n-        try:\n-            decompressed_body = bz2.decompress(self.body)\n-        except IOError:\n-            return False\n-        return response.replace(body=ResponseBody(decompressed_body))\n-    \n-    def extract_winfo(self, response):\n-        self.body = response.body.to_string()\n-        self.archive = StringIO()\n-        self.archive.write(self.body)\n-        \n-        for decompressor in self.decompressors.keys():\n-            self.archive.seek(0)\n-            new_response = self.decompressors[decompressor](response)\n-            if new_response:\n-                return new_response, decompressor\n-        return response, ''\n-        \n-    def extract(self, response):\n-        return self.extract_winfo(response)[0]\n\n@@ -1,81 +0,0 @@\n-\"\"\"\n-Helper functions for dealing with Twisted deferreds\n-\"\"\"\n-\n-from twisted.internet import defer, reactor\n-from twisted.python import failure\n-\n-def defer_fail(_failure):\n-    \"\"\"same as twsited.internet.defer.fail, but delay calling errback \"\"\"\n-    d = defer.Deferred()\n-    reactor.callLater(0, d.errback, _failure)\n-    return d\n-\n-def defer_succeed(result):\n-    \"\"\"same as twsited.internet.defer.succed, but delay calling callback\"\"\"\n-    d = defer.Deferred()\n-    reactor.callLater(0, d.callback, result)\n-    return d\n-\n-def defer_result(result):\n-    if isinstance(result, defer.Deferred):\n-        return result\n-    elif isinstance(result, failure.Failure):\n-        return defer_fail(result)\n-    else:\n-        return defer_succeed(result)\n-\n-def mustbe_deferred(f, *args, **kw):\n-    \"\"\"same as twisted.internet.defer.maybeDeferred, but delay calling callback/errback\"\"\"\n-    try:\n-        result = f(*args, **kw)\n-    except:\n-        return defer_fail(failure.Failure())\n-    else:\n-        return defer_result(result)\n-\n-def chain_deferred(d1, d2):\n-    if callable(d2):\n-        d2 = lambda_deferred(d2)\n-\n-    def _pause(_):\n-        d2.pause()\n-        reactor.callLater(0, d2.unpause)\n-        return _\n-\n-    def _reclaim(_):\n-        return d2\n-\n-    #d1.addBoth(_pause) ## needs more debugging before reenable it\n-    d1.chainDeferred(d2)\n-    d1.addBoth(_reclaim)\n-    return d1\n-\n-def lambda_deferred(func):\n-    deferred = defer.Deferred()\n-    def _success(res):\n-        d = func()\n-        d.callback(res)\n-        return d\n-    def _fail(res):\n-        d = func()\n-        d.errback(res)\n-        return d\n-    return deferred.addCallbacks(_success, _fail)\n-\n-def deferred_degenerate(generator, container=None, next_delay=0):\n-    generator = iter(generator or [])\n-    deferred = defer.Deferred()\n-    container = container or []\n-    def _next():\n-        try:\n-            container.append(generator.next())\n-        except StopIteration:\n-            reactor.callLater(0, deferred.callback, container)\n-        except:\n-            reactor.callLater(0, deferred.errback, failure.Failure())\n-        else:\n-            reactor.callLater(next_delay, _next)\n-    _next()\n-    return deferred\n-\n\n@@ -1,23 +0,0 @@\n-import csv\n-\n-from scrapy.http import Response\n-from scrapy import log\n-\n-def csv_iter(response, delimiter=None, headers=None):\n-    if delimiter:\n-        csv_r = csv.reader(response.body.to_unicode().split('\\n'), delimiter=delimiter)\n-    else:\n-        csv_r = csv.reader(response.body.to_unicode().split('\\n'))\n-\n-    if not headers:\n-        headers = csv_r.next()\n-\n-    while True:\n-        node = csv_r.next()\n-\n-        if len(node) != len(headers):\n-            log.msg(\"ignoring node %d (length: %d, should be: %d)\" % (csv_r.line_num, len(node), len(headers)), log.WARNING)\n-            continue\n-\n-        yield dict(zip(headers, node))\n-\n\n@@ -1,100 +0,0 @@\n-\"\"\"\n-Functions for dealing with markup text\n-\"\"\"\n-\n-import re\n-import htmlentitydefs\n-\n-_ent_re = re.compile(r'&(#?)([^&;]+);')\n-_tag_re = re.compile(r'<[a-zA-Z\\/!].*?>', re.DOTALL)\n-\n-def remove_entities(text, keep=(), remove_illegal=True):\n-    \"\"\"Remove entities from the given text.\n-\n-    'text' can be a unicode string or a regular string encoded as 'utf-8'\n-\n-    If 'keep' is passed (with a list of entity names) those entities will\n-    be kept (they won't be removed).\n-\n-    It supports both numeric (&#nnnn;) and named (&nbsp; &gt;) entities.\n-\n-    If remove_illegal is True, entities that can't be converted are removed.\n-    If remove_illegal is False, entities that can't be converted are kept \"as\n-    is\". For more information see the tests.\n-\n-    Always returns a unicode string (with the entities removed).\n-    \"\"\"\n-\n-    def convert_entity(m):\n-        if m.group(1) == '#':\n-            try:\n-                return unichr(int(m.group(2)))\n-            except ValueError:\n-                if remove_illegal:\n-                    return u''\n-                else:\n-                    return u'&#%s;' % m.group(2)\n-        try:\n-            if m.group(2) in keep:\n-                return '&%s;' % m.group(2)\n-            else:\n-                return unichr(htmlentitydefs.name2codepoint[m.group(2)])\n-        except KeyError:\n-            if remove_illegal:\n-                return u''\n-            else:\n-                return u'&%s;' % m.group(2)\n-\n-    return _ent_re.sub(convert_entity, text.decode('utf-8'))\n-\n-def has_entities(text):\n-    return bool(_ent_re.search(text))\n-\n-def replace_tags(text, token=''):\n-    \"\"\"Replace all markup tags found in the given text by the given token. By\n-    default token is a null string so it just remove all tags.\n-\n-    'text' can be a unicode string or a regular string encoded as 'utf-8'\n-\n-    Always returns a unicode string.\n-    \"\"\"\n-    return _tag_re.sub(token, text.decode('utf-8'))\n-\n-\n-def remove_comments(text):\n-    \"\"\" Remove HTML Comments. \"\"\"\n-    return re.sub('<!--.*?-->', u'', text.decode('utf-8'), re.DOTALL)\n-      \n-def remove_tags(text, which_ones=()):\n-    \"\"\" Remove HTML Tags only. \n-\n-        which_ones -- is a tuple of which tags we want to remove.\n-                      if is empty remove all tags.\n-    \"\"\"\n-    if len(which_ones) > 0:\n-        tags = [ '<%s>|<%s .*?>|</%s>' % (tag,tag,tag) for tag in which_ones ]\n-        reg_exp_remove_tags = '|'.join(tags)\n-    else:\n-        reg_exp_remove_tags = '<.*?>'\n-    re_tags = re.compile(reg_exp_remove_tags, re.DOTALL)\n-    return re_tags.sub(u'', text.decode('utf-8'))\n-\n-def remove_tags_with_content(text, which_ones=()):\n-    \"\"\" Remove tags and its content.\n-        \n-        which_ones -- is a tuple of which tags with its content we want to remove.\n-                      if is empty do nothing.\n-    \"\"\"\n-    tags = [ '<%s.*?</%s>' % (tag,tag) for tag in which_ones ]\n-    re_tags_remove = re.compile('|'.join(tags), re.DOTALL)\n-    return re_tags_remove.sub(u'', text.decode('utf-8'))\n-\n-def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n-    \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n-\n-        which_ones -- is a tuple of which escape chars we want to remove.\n-                      By default removes \\n, \\t, \\r.\n-    \"\"\"\n-    re_escape_chars = re.compile('[%s]' % ''.join(which_ones))\n-    return re_escape_chars.sub(u'', text.decode('utf-8'))\n-\n\n@@ -1,97 +0,0 @@\n-\"\"\"\n-Auxiliary functions which doesn't fit anywhere else\n-\"\"\"\n-import re\n-\n-from twisted.internet import defer\n-\n-from scrapy.core.exceptions import UsageError\n-from scrapy.utils.python import flatten\n-from scrapy.utils.markup import remove_entities\n-from scrapy.utils.defer import defer_succeed\n-\n-def dict_updatedefault(D, E, **F):\n-    \"\"\"\n-    updatedefault(D, E, **F) -> None.\n-\n-    Update D from E and F: for k in E: D.setdefault(k, E[k])\n-    (if E has keys else: for (k, v) in E: D.setdefault(k, v))\n-    then: for k in F: D.setdefault(k, F[k])\n-    \"\"\"\n-    for k in E:\n-        if isinstance(k, tuple):\n-            k, v = k\n-        else:\n-            v = E[k]\n-        D.setdefault(k, v)\n-\n-    for k in F:\n-        D.setdefault(k, F[k])\n-\n-def memoize(cache, hash):\n-    def decorator(func):\n-        def wrapper(*args, **kwargs):\n-            key = hash(*args, **kwargs)\n-            if key in cache:\n-                return defer_succeed(cache[key])\n-\n-            def _store(_):\n-                cache[key] = _\n-                return _\n-\n-            result = func(*args, **kwargs)\n-            if isinstance(result, defer.Deferred):\n-                return result.addBoth(_store)\n-            cache[key] = result\n-            return result\n-        return wrapper\n-    return decorator\n-\n-def stats_getpath(dict_, path, default=None):\n-    for key in path.split('/'):\n-        if key in dict_:\n-            dict_ = dict_[key]\n-        else:\n-            return default\n-    return dict_\n-\n-def load_class(class_path):\n-    \"\"\"Load a class given its absolute class path, and return it without\n-    instantiating it\"\"\"\n-    try:\n-        dot = class_path.rindex('.')\n-    except ValueError:\n-        raise UsageError, '%s isn\\'t a module' % class_path\n-    module, classname = class_path[:dot], class_path[dot+1:]\n-    try:\n-        mod = __import__(module, {}, {}, [''])\n-    except ImportError, e:\n-        raise UsageError, 'Error importing %s: \"%s\"' % (module, e)\n-    try:\n-        cls = getattr(mod, classname)\n-    except AttributeError:\n-        raise UsageError, 'module \"%s\" does not define a \"%s\" class' % (module, classname)\n-\n-    return cls\n-\n-def extract_regex(regex, text, encoding):\n-    \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n-    \n-    * if the regex contains a named group called \"extract\" that will be returned\n-    * if the regex contains multiple numbered groups, all those will be returned (flattened)\n-    * if the regex doesn't contain any group the entire regex matching is returned\n-    \"\"\"\n-\n-    if isinstance(regex, basestring):\n-        regex = re.compile(regex)\n-\n-    try:\n-        strings = [regex.search(text).group('extract')]   # named group\n-    except:\n-        strings = regex.findall(text)    # full regex or numbered groups\n-    strings = flatten(strings)\n-\n-    if isinstance(text, unicode):\n-        return [remove_entities(s, keep=['lt', 'amp']) for s in strings]\n-    else:\n-        return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n\n@@ -1,55 +0,0 @@\n-\"\"\"\n-This module contains essential stuff that should've come with Python itself ;)\n-\"\"\"\n-\n-from sgmllib import SGMLParser\n-\n-class FixedSGMLParser(SGMLParser):\n-    \"\"\"The SGMLParser that comes with Python has a bug in the convert_charref()\n-    method. This is the same class with the bug fixed\"\"\"\n-\n-    def convert_charref(self, name):\n-        \"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n-        try:\n-            n = int(name)\n-        except ValueError:\n-            return\n-        if not 0 <= n <= 127 : # ASCII ends at 127, not 255\n-            return\n-        return self.convert_codepoint(n)\n-\n-\n-def flatten(x):\n-    \"\"\"flatten(sequence) -> list\n-\n-    Returns a single, flat list which contains all elements retrieved\n-    from the sequence and all recursively contained sub-sequences\n-    (iterables).\n-\n-    Examples:\n-    >>> [1, 2, [3,4], (5,6)]\n-    [1, 2, [3, 4], (5, 6)]\n-    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n-    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\"\"\"\n-\n-    result = []\n-    for el in x:\n-        if hasattr(el, \"__iter__\"):\n-            result.extend(flatten(el))\n-        else:\n-            result.append(el)\n-    return result\n-\n-\n-def unique(list_):\n-    \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n-    seen = {}\n-    result = []\n-    for item in list_:\n-        if item in seen: \n-            continue\n-        seen[item] = 1\n-        result.append(item)\n-    return result\n-\n-\n\n@@ -1,15 +0,0 @@\n-\"\"\"\n-This module provides some useful functions for working with\n-scrapy.http.Response objects\n-\"\"\"\n-\n-from scrapy.xpath import XPathSelector\n-\n-def new_response_from_xpaths(response, xpaths):\n-    \"\"\"Return a new response constructed by applying the given xpaths to the\n-    original response body\n-    \"\"\"\n-    xs = XPathSelector(response)\n-    parts = [''.join([n for n in xs.x(x).extract()]) for x in xpaths]\n-    new_body = ''.join(parts)\n-    return response.replace(body=new_body)\n\n@@ -1,147 +0,0 @@\n-\"\"\"\n-This module contains general purpose URL functions not found in the standard\n-library.\n-\"\"\"\n-\n-import re\n-import urlparse\n-import urllib\n-import posixpath\n-import cgi\n-\n-def url_is_from_any_domain(url, domains):\n-    \"\"\"Return True if the url belongs to the given domain\"\"\"\n-    host = urlparse.urlparse(url).hostname\n-\n-    if host:\n-        return any(((host == d) or (host.endswith('.%s' % d)) for d in domains))\n-    else:\n-        return False\n-\n-def url_is_from_spider(url, spider):\n-    \"\"\"Return True if the url belongs to the given spider\"\"\"\n-    domains = [spider.domain_name] + spider.extra_domain_names\n-    return url_is_from_any_domain(url, domains)\n-\n-def urljoin_rfc(base, ref):\n-    \"\"\"\n-    Fixed urlparse.urljoin version that handles\n-    relative query string as RFC states.\n-    \"\"\"\n-    if ref.startswith('?'):\n-        fpart = urlparse.urlsplit(str(base))[2].rsplit('/', 1)[-1]\n-        ref = ''.join([fpart, ref])\n-    # convert ref to a string. This should already\n-    # be the case, however, many spiders do not convert.\n-    return urlparse.urljoin(base, str(ref))\n-\n-\n-_reserved = ';/?:@&=+$|,#' # RFC 2396 (Generic Syntax)\n-_unreserved_marks = \"-_.!~*'()\" #RFC 2396 sec 2.3\n-_safe_chars = urllib.always_safe + '%' + _reserved + _unreserved_marks\n-\n-def safe_url_string(url, use_encoding='utf8'):\n-    \"\"\"Convert a unicode (or utf8 string) object into a legal URL.\n-\n-    Illegal characters are escaped (RFC-3986)\n-\n-    It is safe to call this function multiple times. Do not pass this\n-    function strings in encodings other than utf8.\n-\n-    The use_encoding argument is the encoding to use to determine the numerical\n-    values in the escaping. For urls on html pages, you should use the original\n-    encoding of that page.\n-    \"\"\"\n-    s = url.encode(use_encoding)\n-    return urllib.quote(s,  _safe_chars)\n-\n-\n-_parent_dirs = re.compile(r'/?(\\.\\./)+')\n-\n-def safe_download_url(url):\n-    \"\"\" Make a url for download. This will call safe_url_string\n-    and then strip the fragment, if one exists. The path will\n-    be normalised.\n-\n-    If the path is outside the document root, it will be changed\n-    to be within the document root.\n-    \"\"\"\n-    safe_url = safe_url_string(url)\n-    scheme, netloc, path, query, _ = urlparse.urlsplit(safe_url)\n-    if path:\n-        path = _parent_dirs.sub('', posixpath.normpath(path))\n-        if url.endswith('/') and not path.endswith('/'):\n-            path += '/'\n-    else:\n-        path = '/'\n-    return urlparse.urlunsplit((scheme, netloc, path, query, ''))\n-\n-def is_url(text):\n-    return text.partition(\"://\")[0] in ('file', 'http', 'https')\n-\n-def url_query_parameter(url, parameter, default=None, keep_blank_values=0):\n-    \"\"\"Return the value of a url parameter, given the url and parameter name\"\"\"\n-    queryparams = cgi.parse_qs(urlparse.urlsplit(str(url))[3], keep_blank_values=keep_blank_values)\n-    return queryparams.get(parameter, [default])[0] \n-\n-def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='='):\n-    \"\"\"Clean url arguments leaving only those passed in the parameterlist\"\"\"\n-    try:\n-        base, query = url.split('?', 1)\n-        parameters = [pair.split(kvsep, 1) for pair in query.split(sep)]\n-    except:\n-        base = url\n-        query = \"\"\n-        parameters = []\n-    \n-    # unique parameters while keeping order\n-    unique = {}\n-    querylist = []\n-    for pair in parameters: \n-        k = pair[0]\n-        if not unique.get(k):\n-            querylist += [pair]\n-            unique[k] = 1\n-    \n-    query = sep.join([kvsep.join(pair) for pair in querylist if pair[0] in parameterlist])\n-    return '?'.join([base, query])\n-    \n-def add_or_replace_parameter(url, name, new_value, sep='&'):\n-    \"\"\"Add or remove a parameter to a given url\"\"\"\n-    def has_querystring(url):\n-        _, _, _, query, _ = urlparse.urlsplit(url)\n-        return bool(query)\n-\n-    parameter = url_query_parameter(url, name, keep_blank_values=1)\n-    if parameter is None:\n-        if has_querystring(url):\n-            next_url = url + sep + name + '=' + new_value\n-        else:\n-            next_url = url + '?' + name + '=' + new_value\n-    else:\n-        next_url = url.replace(name+'='+parameter,\n-                               name+'='+new_value)\n-    return next_url\n-\n-def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n-    \"\"\"Canonicalize url by applying the following procedures:\n-\n-    - sort query arguments, first by key, then by value\n-    - percent encode paths and query arguments. non-ASCII characters are\n-      percent-encoded using UTF-8 (RFC-3986)\n-    - normalize all spaces (in query arguments) '+' (plus symbol)\n-    - normalize percent encodings case (%2f -> %2F)\n-    - remove query arguments with blank values (unless keep_blank_values is True)\n-    - remove fragments (unless keep_fragments is True)\n-\n-    For examples see the tests in scrapy.tests.test_utils_url\n-    \"\"\"\n-\n-    parts = list(urlparse.urlparse(url))\n-    keyvals = cgi.parse_qsl(parts[4], keep_blank_values)\n-    keyvals.sort()\n-    parts[2] = urllib.quote(urllib.unquote(parts[2]))\n-    parts[4] = urllib.urlencode(keyvals)\n-    if not keep_fragments:\n-        parts[5] = ''\n-    return urlparse.urlunparse(parts)\n\n@@ -1,28 +0,0 @@\n-import re\n-\n-from scrapy.xpath import XmlXPathSelector\n-from scrapy.http import Response\n-\n-def xpathselector_iternodes(obj, nodename):\n-    \"\"\"Return a iterator of XPathSelector's over all nodes of a XML document,\n-       given tha name of the node to iterate. Useful for parsing XML feeds.\n-\n-    obj can be:\n-    - a Response object\n-    - a unicode string\n-    - a string encoded as utf-8\n-    \"\"\"\n-    \n-    assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n-\n-    if isinstance(obj, Response):\n-        text = obj.body.to_unicode()\n-    elif isinstance(obj, str):\n-        text = obj.decode('utf-8')\n-    else:\n-        text = obj\n-\n-    r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n-    for match in r.finditer(text):\n-        nodetext = match.group()\n-        yield XmlXPathSelector(text=nodetext).x('/' + nodename)[0]\n\n@@ -1,31 +0,0 @@\n-\"\"\"\n-This module provides functions for generating libxml2 documents (xmlDoc).\n-\n-Constructors must receive a Response object and return a xmlDoc object.\n-\"\"\"\n-\n-import libxml2\n-\n-xml_parser_options = libxml2.XML_PARSE_RECOVER + \\\n-                     libxml2.XML_PARSE_NOERROR + \\\n-                     libxml2.XML_PARSE_NOWARNING\n-\n-html_parser_options = libxml2.HTML_PARSE_RECOVER + \\\n-                      libxml2.HTML_PARSE_NOERROR + \\\n-                      libxml2.HTML_PARSE_NOWARNING\n-\n-def xmlDoc_from_html(response):\n-    \"\"\"Return libxml2 doc for HTMLs\"\"\"\n-    try:\n-        lxdoc = libxml2.htmlReadDoc(response.body.to_string('utf-8'), response.url, 'utf-8', html_parser_options)\n-    except TypeError:  # libxml2 doesn't parse text with null bytes\n-        lxdoc = libxml2.htmlReadDoc(response.body.to_string('utf-8').replace(\"\\x00\", \"\"), response.url, 'utf-8', html_parser_options)\n-    return lxdoc\n-\n-def xmlDoc_from_xml(response):\n-    \"\"\"Return libxml2 doc for XMLs\"\"\"\n-    try:\n-        lxdoc = libxml2.readDoc(response.body.to_string('utf-8'), response.url, 'utf-8', xml_parser_options)\n-    except TypeError:  # libxml2 doesn't parse text with null bytes\n-        lxdoc = libxml2.readDoc(response.body.to_string('utf-8').replace(\"\\x00\", \"\"), response.url, 'utf-8', xml_parser_options)\n-    return lxdoc\n\n@@ -1,20 +0,0 @@\n-\"\"\"\n-The ResponseLibxml2 extension causes the Response objects to grow a new method\n-(\"getlibxml2doc\") which returns a (cached) libxml2 document of itself.\n-\"\"\"\n-\n-from scrapy.http import Response\n-from scrapy.xpath.document import Libxml2Document\n-from scrapy.xpath.constructors import xmlDoc_from_html\n-\n-class ResponseLibxml2(object):\n-    def __init__(self):\n-        setattr(Response, 'getlibxml2doc', getlibxml2doc)\n-\n-def getlibxml2doc(response, constructor=xmlDoc_from_html):\n-    attr = '_lx2doc_%s' % constructor.__name__\n-    if not hasattr(response, attr):\n-        lx2doc = Libxml2Document(response, constructor=constructor)\n-        setattr(response, attr, lx2doc)\n-    return getattr(response, attr)\n-\n\n@@ -1,133 +0,0 @@\n-import libxml2\n-\n-from scrapy.http import Response\n-from scrapy.xpath.extension import Libxml2Document\n-from scrapy.xpath.constructors import xmlDoc_from_html, xmlDoc_from_xml\n-from scrapy.utils.python import flatten\n-from scrapy.utils.misc import extract_regex\n-\n-class XPathSelector(object):\n-    \"\"\"The XPathSelector class provides a convenient way for selecting document\n-    parts using XPaths and regexs, with support for nested queries.\n-\n-    Although this is not an abstract class, you usually instantiate one of its\n-    children:\n-    \n-    - XmlXPathSelector (for XML content)\n-    - HtmlXPathSelector (for HTML content)\n-    \"\"\"\n-\n-    def __init__(self, response=None, text=None, node=None, parent=None, expr=None, constructor=xmlDoc_from_html):\n-        if parent:\n-            self.doc = parent.doc\n-            self.xmlNode = node\n-        elif response:\n-            try:\n-                self.doc = response.getlibxml2doc(constructor=constructor)  # try with cached version first\n-            except AttributeError:\n-                self.doc = Libxml2Document(response, constructor=constructor)\n-            self.xmlNode = self.doc.xmlDoc\n-        elif text:\n-            response = Response(domain=None, url=None, body=str(text))\n-            self.doc = Libxml2Document(response, constructor=constructor)\n-            self.xmlNode = self.doc.xmlDoc\n-        self.expr = expr\n-        self.response = response\n-\n-    def x(self, xpath):\n-        \"\"\"Perform the given XPath query on the current XPathSelector and\n-        return a XPathSelectorList of the result\"\"\"\n-        if hasattr(self.xmlNode, 'xpathEval'):\n-            self.doc.xpathContext.setContextNode(self.xmlNode)\n-            xpath_result = self.doc.xpathContext.xpathEval(xpath)\n-            cls = type(self)\n-            if hasattr(xpath_result, '__iter__'):\n-                return XPathSelectorList([cls(node=node, parent=self, expr=xpath, response=self.response)\n-                                          for node in xpath_result])\n-            else:\n-                return XPathSelectorList([cls(node=xpath_result, parent=self, expr=xpath, response=self.response)])\n-        else:\n-            return XPathSelectorList([])\n-\n-    def re(self, regex):\n-        \"\"\"Return a list of unicode strings by applying the regex over all\n-        current XPath selections, and flattening the results\"\"\"\n-        return extract_regex(regex, self.extract(), 'utf-8')\n-\n-    def extract(self):\n-        \"\"\"Return a unicode string of the content referenced by the XPathSelector\"\"\"\n-        if isinstance(self.xmlNode, basestring):\n-            text = unicode(self.xmlNode, 'utf-8', errors='ignore')\n-        elif hasattr(self.xmlNode, 'serialize'):\n-            if isinstance(self.xmlNode, libxml2.xmlDoc):\n-                data = self.xmlNode.getRootElement().serialize('utf-8')\n-                text = unicode(data, 'utf-8', errors='ignore') if data else u''\n-            elif isinstance(self.xmlNode, libxml2.xmlAttr): \n-                # serialization doesn't work sometimes for xmlAttr types\n-                text = unicode(self.xmlNode.content, errors='ignore')\n-            else:\n-                data = self.xmlNode.serialize('utf-8')\n-                text = unicode(data, 'utf-8', errors='ignore') if data else u''\n-        else:\n-            try:\n-                text = unicode(self.xmlNode, errors='ignore')\n-            except TypeError:  # catched when self.xmlNode is a float - see tests\n-                text = unicode(self.xmlNode)\n-        return text\n-\n-    def extract_unquoted(self):\n-        \"\"\"Get unescaped contents from the text node (no entities, no CDATA)\"\"\"\n-        if self.x('self::text()'):\n-            return unicode(self.xmlNode.getContent(), errors='ignore')\n-        else:\n-            return u''\n-\n-    def register_namespace(self, prefix, uri):\n-        \"\"\"Register namespace so that it can be used in XPath queries\"\"\"\n-        self.doc.xpathContext.xpathRegisterNs(prefix, uri)\n-\n-    def __nonzero__(self):\n-        return bool(self.extract())\n-\n-    def __str__(self):\n-        return \"<%s (%s) xpath=%s>\" % (type(self).__name__, getattr(self.xmlNode, 'name', type(self.xmlNode).__name__), self.expr)\n-\n-    __repr__ = __str__\n-\n-\n-class XPathSelectorList(list):\n-    \"\"\"List of XPathSelector objects\"\"\"\n-\n-    def __getslice__(self, i, j):\n-        return XPathSelectorList(list.__getslice__(self, i, j))\n-\n-    def x(self, xpath):\n-        \"\"\"Perform the given XPath query on each XPathSelector of the list and\n-        return a new (flattened) XPathSelectorList of the results\"\"\"\n-        return XPathSelectorList(flatten([x.x(xpath) for x in self]))\n-\n-    def re(self, regex):\n-        \"\"\"Perform the re() method on each XPathSelector of the list, and\n-        return the result as a flattened list of unicode strings\"\"\"\n-        return flatten([x.re(regex) for x in self])\n-\n-    def extract(self):\n-        \"\"\"Return a list of unicode strings with the content referenced by each\n-        XPathSelector of the list\"\"\"\n-        return [x.extract() if isinstance(x, XPathSelector) else x for x in self]\n-\n-    def extract_unquoted(self):\n-        return [x.extract_unquoted() if isinstance(x, XPathSelector) else x for x in self]\n-\n-\n-class XmlXPathSelector(XPathSelector):\n-    \"\"\"XPathSelector for XML content\"\"\"\n-    def __init__(self, *args, **kwargs):\n-        kwargs['constructor'] = xmlDoc_from_xml\n-        XPathSelector.__init__(self, *args, **kwargs)\n-\n-class HtmlXPathSelector(XPathSelector):\n-    \"\"\"XPathSelector for HTML content\"\"\"\n-    def __init__(self, *args, **kwargs):\n-        kwargs['constructor'] = xmlDoc_from_html\n-        XPathSelector.__init__(self, *args, **kwargs)\n\n@@ -1,26 +0,0 @@\n-from setuptools import setup, find_packages\n-import os, os.path, glob\n-\n-def findfiles(pattern, base='.'):\n-    matches = []\n-    for root, _, _ in os.walk(base):\n-        matches.extend(glob.glob(os.path.join(root, pattern)))\n-    return matches\n-\n-\n-setup(\n-    name = 'scrapy',\n-    version = '0.8',\n-    description = '',\n-    long_description = '',\n-    author = '',\n-    author_email = '',\n-    license = '',\n-    url = 'http://scrapy.org',\n-\n-    packages = find_packages(),\n-    package_data = {\n-        'scrapy': ['templates/*.tmpl'],\n-    },\n-    scripts = ['scrapy/bin/scrapy-admin.py'],\n-)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@@ -1 +0,0 @@\n-from scrapy.command.models import ScrapyCommand\n\n@@ -1,28 +0,0 @@\n-import sys, pprint\n-\n-from scrapy.command import ScrapyCommand\n-from scrapy.fetcher import fetch\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options] <url>\"\n-\n-    def short_desc(self):\n-        return \"Download a URL using the Scrapy downloader\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-s\", \"--source\", dest=\"source\", action=\"store_true\", help=\"output HTTP body only\")\n-        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", help=\"output HTTP headers only\")\n-\n-    def run(self, args, opts):\n-        if not args:\n-            print \"A URL is required\"\n-            return\n-\n-        responses = fetch(args)\n-        if responses:\n-            if opts.headers:\n-                pprint.pprint(responses[0].headers)\n-            else:\n-                sys.stdout.write(str(responses[0].body))\n\n@@ -1,26 +0,0 @@\n-from scrapy.command import ScrapyCommand, cmdline\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"<command>\"\n-    \n-    def short_desc(self):\n-        return \"Provides extended help for the given command\"\n-\n-    def run(self, args, opts):\n-        if not args:\n-            return False\n-\n-        commands = cmdline.builtin_commands_dict()\n-        commands.update(cmdline.custom_commands_dict())\n-\n-        cmdname = args[0]\n-        if cmdname in commands:\n-            cmd = commands[cmdname]\n-            help = getattr(cmd, 'help', None) or getattr(cmd, 'long_desc', None)\n-            print \"%s: %s\" % (cmdname, cmd.short_desc())\n-            print \"usage: %s %s\" % (cmdname, cmd.syntax())\n-            print\n-            print help()\n-        else:\n-            print \"Unknown command: %s\" % cmdname\n\n@@ -1,16 +0,0 @@\n-from scrapy.command import ScrapyCommand\n-from scrapy.spider import spiders\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"\"\n-    \n-    def short_desc(self):\n-        return \"List available spiders (both enabled and disabled)\"\n-\n-    def run(self, args, opts):\n-        spiders_dict = spiders.asdict()\n-        for n, p in spiders_dict.items():\n-            disabled = \"disabled\" if getattr(p, 'disabled', False) else \"enabled\"\n-            print \"%-30s %-30s %s\" % (n, p.__class__.__name__, disabled)\n-        print \"Total spiders: %d\" % len(spiders_dict)\n\n@@ -1,46 +0,0 @@\n-import re\n-\n-from scrapy.command import ScrapyCommand\n-\n-log_crawled_re = re.compile(r'^(.*?) \\[\\w+/(.*?)\\].*Crawled <(.*?)> from <(.*?)>$')\n-log_scraped_re = re.compile('^(.*?) \\[\\w+/(.*?)\\].*Scraped (.*) in <(.*?)>$')\n-log_opendomain_re = re.compile('^(.*?) \\[\\w+.*? Started scraping (.*)$')\n-log_closedomain_re = re.compile('^(.*?) \\[\\w+.*? Finished scraping (.*)$')\n-log_debug_re = re.compile(r'\\[\\w+/(.*?)\\] DEBUG')\n-log_info_re = re.compile(r'\\[\\w+/(.*?)\\] INFO')\n-log_warning_re = re.compile(r'\\[\\w+/(.*?)\\] WARNING')\n-log_error_re = re.compile(r'\\[\\w+/(.*?)\\] ERROR')\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options] <logfile>\"\n-\n-    def short_desc(self):\n-        return \"Several tools to perform scrapy log analysis\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--sitemap\", dest=\"sitemap\", action=\"store_true\", help=\"show sitemap based on given log file\")\n-\n-    def sitemap(self, logfile):\n-        from scrapy.utils.datatypes import Sitemap\n-\n-        sm = Sitemap()\n-        for l in open(logfile).readlines():\n-            m = log_crawled_re.search(l.strip())\n-            if m:\n-                sm.add_node(m.group(3), m.group(4))\n-            m = log_scraped_re.search(l.strip())\n-            if m:\n-                sm.add_item(m.group(4), m.group(3))\n-        print sm.to_string()\n-\n-    def run(self, args, opts):\n-        if not args:\n-            print \"A log file is required\"\n-            return\n-\n-        if opts.sitemap:\n-            self.sitemap(args[0])\n-        else:\n-            print \"No analysis method specified\"\n\n@@ -1,16 +0,0 @@\n-from scrapy.command import ScrapyCommand\n-from scrapy.core.manager import scrapymanager\n-\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options]\"\n-\n-    def short_desc(self):\n-        return \"Start the Scrapy server\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-\n-    def run(self, args, opts):\n-        scrapymanager.start(*args, **opts.__dict__)\n\n@@ -1 +0,0 @@\n-LOG_STDOUT = True\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n@@ -1,2 +0,0 @@\n-LOG_ENABLED = False\n-\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n\n\n@@ -1,13 +0,0 @@\n-from scrapy.core.exceptions import HttpException\n-\n-class ErrorPagesMiddleware(object):\n-    \"\"\"This middleware allows the spiders to receive error (non 200) responses,\n-    the same way the receive normal responses\"\"\"\n-\n-    def process_exception(self, request, exception, spider):\n-        if isinstance(exception, HttpException):\n-            statuses = getattr(spider, 'handle_httpstatus_list', None)\n-            httpstatus = exception.response.status\n-            if statuses and httpstatus in statuses:\n-                return exception.response\n-\n\n@@ -1,26 +0,0 @@\n-\"\"\"\n-Extensions to override scrapy settings with per-group settings according to the\n-group the spider belongs to. It only overrides the settings when running the\n-crawl command with *only one domain as argument*.\n-\"\"\"\n-\n-from scrapy.conf import settings\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.command.cmdline import command_executed\n-\n-class GroupSettings(object):\n-\n-    def __init__(self):\n-        if not settings.getbool(\"GROUPSETTINGS_ENABLED\"):\n-            raise NotConfigured\n-\n-        if command_executed and command_executed['name'] == 'crawl':\n-            mod = __import__(settings['GROUPSETTINGS_MODULE'], {}, {}, [''])\n-            args = command_executed['args']\n-            if len(args) == 1 and not args[0].startswith('http://'):\n-                domain = args[0]\n-                settings.overrides.update(mod.default_settings)\n-                for group, domains in mod.group_spiders.iteritems():\n-                    if domain in domains:\n-                        settings.overrides.update(mod.group_settings.get(group, {}))\n-        \n\n@@ -1 +0,0 @@\n-from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline\n\n\n@@ -1,34 +0,0 @@\n-import time\n-\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.store.db import DomainDataHistory\n-from scrapy.conf import settings\n-\n-class LessScrapedPrioritizer(object):\n-    \"\"\"\n-    A spider prioritizer based on these few simple rules:\n-    1. if spider was never scraped, it has top priority\n-    2. if spider was scraped before, then the less recently the spider\n-       has been scraped, the more priority it has\n-    \"\"\"\n-    def __init__(self, elements):\n-        if not settings['SCRAPING_DB']:\n-            raise NotConfigured(\"SCRAPING_DB setting is required\")\n-\n-        self.ddh = DomainDataHistory(settings['SCRAPING_DB'], 'domain_data_history')\n-        domains_to_scrape = set(elements)\n-\n-        self.priorities = {}\n-        \n-        for domain in domains_to_scrape:\n-            stat = self.ddh.getlast(domain, path=\"start_time\")\n-            if stat and stat[1]:\n-                last_started = stat[1]\n-                # spider is the timestamp of last start time\n-                self.priorities[domain] = time.mktime(last_started.timetuple())  \n-            else:\n-                # if domain was never scraped, it has top priority\n-                self.priorities[domain] = 1\n-\n-    def get_priority(self, element):\n-        return self.priorities[element]\n\n\n\n\n@@ -1,60 +0,0 @@\n-\"\"\"\n-SpiderProfiler is an extension that hooks itself into every Request callback\n-returned from spiders to measure the processing time and memory allocation\n-caused by spiders code.\n-\n-The results are collected using the StatsCollector.\n-\n-This extension introduces a big impact on crawling performance, so enable only\n-when needed.\n-\"\"\"\n-\n-import datetime\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.extension import extensions\n-from scrapy.core import signals\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.stats import stats\n-from scrapy.conf import settings\n-\n-class SpiderProfiler(object):\n-    \n-    def __init__(self):\n-        if not settings.getbool('SPIDERPROFILER_ENABLED'):\n-            raise NotConfigured\n-        dispatcher.connect(self._request_received, signals.request_received)\n-        dispatcher.connect(self._engine_started, signals.engine_started)\n-\n-    def _engine_started(self):\n-        self.memusage = extensions.enabled.get('MemoryUsage', None)\n-\n-    def _request_received(self, request, spider):\n-        old_cbs = request.deferred.callbacks[0]\n-        new_cbs = ((self._profiled_callback(old_cbs[0][0], spider), old_cbs[0][1], old_cbs[0][2]), old_cbs[1])\n-        request.deferred.callbacks[0] = new_cbs\n-\n-    def _profiled_callback(self, function, spider):\n-        def new_callback(*args, **kwargs):\n-            tbefore = datetime.datetime.now()\n-            mbefore = self._memusage()\n-            r = function(*args, **kwargs)\n-            tafter = datetime.datetime.now()\n-            mafter = self._memusage()\n-            ct = tafter-tbefore\n-            tcc = stats.getpath('%s/profiling/total_callback_time' % spider.domain_name, datetime.timedelta(0))\n-            sct = stats.getpath('%s/profiling/slowest_callback_time' % spider.domain_name, datetime.timedelta(0))\n-            stats.setpath('%s/profiling/total_callback_time' % spider.domain_name, tcc+ct)\n-            if ct > sct:\n-                stats.setpath('%s/profiling/slowest_callback_time' % spider.domain_name, ct)\n-                stats.setpath('%s/profiling/slowest_callback_name' % spider.domain_name, function.__name__)\n-                stats.setpath('%s/profiling/slowest_callback_url' % spider.domain_name, args[0].url)\n-            if self.memusage:\n-                tma = stats.getpath('%s/profiling/total_mem_allocated_in_callbacks' % spider.domain_name, 0)\n-                stats.setpath('%s/profiling/total_mem_allocated_in_callbacks' % spider.domain_name, tma+mafter-mbefore)\n-            return r\n-        return new_callback\n-\n-    def _memusage(self):\n-        return self.memusage.virtual if self.memusage else 0.0\n\n@@ -1,19 +0,0 @@\n-\"\"\"\n-Reload spider modules once they are finished scraping\n-\n-This is to release any resources held on to by scraping spiders.\n-\"\"\"\n-import sys\n-from pydispatch import dispatcher\n-from scrapy.core import signals\n-from scrapy import log\n-\n-class SpiderReloader(object):\n-\n-    def __init__(self):\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n-\n-    def domain_closed(self, domain, spider):\n-        module = spider.__module__\n-        log.msg(\"reloading module %s\" % module, domain=domain)\n-        reload(sys.modules[module])\n\n\n\n@@ -1,115 +0,0 @@\n-\"\"\"A django alike request-response model\n-\n-most of this code is borrowed from django\n-\"\"\"\n-\n-from Cookie import SimpleCookie\n-from scrapy.utils.datatypes import MultiValueDict, CaselessDict\n-\n-\n-def build_httprequest(twistedrequest):\n-    \"\"\"Translate twisted request object to a django request approach\"\"\"\n-    request = HttpRequest()\n-    request.path = twistedrequest.path\n-    request.method = twistedrequest.method.upper()\n-    request.COOKIES = SimpleCookie(twistedrequest.received_cookies)\n-    request.HEADERS = Headers(twistedrequest.received_headers)\n-    request.ARGS = MultiValueDict(twistedrequest.args)\n-    request.FILES = {} # not yet supported\n-    request.content = twistedrequest.content\n-    request.twistedrequest = twistedrequest\n-    return request\n-\n-\n-class HttpRequest(object):\n-    def __init__(self):\n-        self.path = ''\n-        self.method = None\n-        self.COOKIES = {}\n-        self.HEADERS = {}\n-        self.ARGS = {}\n-        self.FILES = {}\n-\n-\n-class HttpResponse(object):\n-    status_code = 200\n-\n-    def __init__(self, content='', status=None, content_type=None):\n-        content_type = content_type or \"text/html; charset=utf-8\"\n-        self._headers = {'content-type': content_type}\n-        self.content = content\n-        self.cookies = SimpleCookie()\n-        self.status_code = status\n-\n-    def __str__(self):\n-        \"Full HTTP message, including headers\"\n-        return '\\n'.join(['%s: %s' % (key, value)\n-            for key, value in self._headers.items()]) \\\n-            + '\\n\\n' + self.content\n-\n-    def __setitem__(self, header, value):\n-        self._headers[header.lower()] = value\n-\n-    def __delitem__(self, header):\n-        try:\n-            del self._headers[header.lower()]\n-        except KeyError:\n-            pass\n-\n-    def __getitem__(self, header):\n-        return self._headers[header.lower()]\n-\n-    def has_header(self, header):\n-        \"Case-insensitive check for a header\"\n-        return self._headers.has_key(header.lower())\n-\n-    __contains__ = has_header\n-\n-    def items(self):\n-        return self._headers.items()\n-\n-    def get(self, header, alternate):\n-        return self._headers.get(header, alternate)\n-\n-    def set_cookie(self, key, value='', max_age=None, expires=None, path='/', domain=None, secure=None):\n-        self.cookies[key] = value\n-        for var in ('max_age', 'path', 'domain', 'secure', 'expires'):\n-            val = locals()[var]\n-            if val is not None:\n-                self.cookies[key][var.replace('_', '-')] = val\n-\n-    def delete_cookie(self, key, path='/', domain=None):\n-        self.cookies[key] = ''\n-        if path is not None:\n-            self.cookies[key]['path'] = path\n-        if domain is not None:\n-            self.cookies[key]['domain'] = domain\n-        self.cookies[key]['expires'] = 0\n-        self.cookies[key]['max-age'] = 0\n-\n-\n-class Headers(CaselessDict):\n-    def __init__(self, source=None, encoding='utf-8'):\n-        self.encoding = encoding\n-\n-        if getattr(source, 'iteritems', None):\n-            d = source.iteritems()\n-        else:\n-            d = source # best effort\n-\n-        # can't use CaselessDict.__init__(self, d) because it doesn't call __setitem__\n-        for k,v in d:\n-            self.__setitem__(k.lower(), v) \n-\n-    def normkey(self, key):\n-        return key.title() # 'Content-Type' styles headers\n-\n-    def __setitem__(self, key, value):\n-        \"\"\"Headers must not be unicode\"\"\"\n-        if isinstance(key, unicode):\n-            key = key.encode(self.encoding)\n-        if isinstance(value, unicode):\n-            value = value.encode(self.encoding)\n-        super(Headers, self).__setitem__(key, value)\n-\n-\n\n@@ -1,99 +0,0 @@\n-from functools import wraps\n-\n-from scrapy.utils.defer import mustbe_deferred\n-from scrapy.utils.serialization import serialize as _serialize, unserialize as _unserialize\n-\n-from .http import HttpResponse\n-\n-JSON_CONTENT_TYPES = ('application/json',)\n-\n-serialize = lambda x: _serialize(x, 'json')\n-unserialize = lambda x: _unserialize(x, 'json')\n-\n-class JsonException(Exception):\n-    pass\n-\n-class JsonResponse(HttpResponse):\n-    def __init__(self, content=None, callback=None, serialize=serialize, *args, **kwargs):\n-        content = serialize(content)\n-        if callback: # JSONP support\n-            status, content = 200, '%s(%s)' % (callback, content)\n-        kwargs.setdefault('content_type', 'application/x-javascript')\n-        HttpResponse.__init__(self, content=content, *args, **kwargs)\n-\n-class JsonResponseAccepted(JsonResponse):\n-    status_code = 202\n-\n-class JsonResponseNoContent(JsonResponse):\n-    status_code = 204\n-\n-class JsonResponseNotModified(JsonResponse):\n-    status_code = 304\n-\n-class JsonResponseBadRequest(JsonResponse):\n-    status_code = 400\n-\n-class JsonResponseUnauthorized(JsonResponse):\n-    status_code = 401\n-\n-class JsonResponseForbidden(JsonResponse):\n-    status_code = 403\n-\n-class JsonResponseNotFound(JsonResponse):\n-    status_code = 404\n-\n-class JsonResponseInternalServerError(JsonResponse):\n-    status_code = 500\n-\n-class JsonResponseNotImplemented(JsonResponse):\n-    status_code = 501\n-\n-\n-def json(func):\n-    \"\"\" Decorator to wrap a json prepared view and return a JsonResponse\n-\n-    if content-type is application/json, sets request.JSON to unserialized request body.\n-    in case of unserialization failure, returns JsonResponseBadRequest()\n-\n-    if returned data from func is a dictionary, serialize it and returns JsonResponse()\n-    \"\"\"\n-    if not hasattr(func, '__call__'):\n-        raise TypeError('The argument should be a callable')\n-\n-    @wraps(func)\n-    def wrapper(request, *args, **kwargs):\n-        json_callback = request.ARGS.get('callback') # JSONP support\n-        request.method = method = _x_http_method_override(request)\n-        request.content_type = ct = content_type(request)\n-        request.JSON = None\n-\n-        if method in ('POST', 'PUT'):\n-            if ct in JSON_CONTENT_TYPES:\n-                body = request.content.read()\n-                try:\n-                    request.JSON = unserialize(body)\n-                except Exception, e:\n-                    return JsonResponseBadRequest('Invalid json: %s' % e )\n-\n-        def _onsuccess(response):\n-            if not isinstance(response, HttpResponse):\n-                return JsonResponse(response, json_callback) # best effort\n-            return response\n-\n-        ret = mustbe_deferred(func, request, *args, **kwargs)\n-        ret.addCallback(_onsuccess)\n-        return ret\n-    return wrapper\n-\n-def content_type(request):\n-    ct = request.HEADERS.get('content-type','')\n-    return ct.split(';')[0].strip()\n-\n-def _x_http_method_override(request):\n-    \"\"\" support for X-Http-Method-Override hack\n-\n-    some clients does not support methods others than GET and POST, that clients\n-    has a chance to set an extra header to indicate intended method.\n-    \"\"\"\n-    return request.HEADERS.get('x-http-method-override', request.method).upper()\n-\n\n@@ -1,89 +0,0 @@\n-import re\n-import urllib\n-import hashlib\n-from twisted.internet import defer\n-\n-from scrapy.core.engine import scrapyengine\n-from scrapy.spider import spiders\n-from scrapy.http import Request\n-from scrapy.item import ScrapedItem\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-from scrapy.utils.misc import memoize\n-from lrucache import LRUCache\n-\n-from .site import WebSite, WebResource\n-from .http import HttpResponse\n-from .json import JsonResponse\n-\n-JSONCALLBACK_RE = '^[a-zA-Z][a-zA-Z_.-]*$'\n-CACHESIZE = settings.get('WS_CACHESIZE', 20)\n-\n-\n-def _urlhash(request):\n-    h = hashlib.sha1()\n-    for a in sorted(request.ARGS):\n-        h.update(request.ARGS[a])\n-    return h.hexdigest()\n-\n-\n-@memoize(cache=LRUCache(CACHESIZE), hash=_urlhash)\n-def url_to_guid(httprequest):\n-    url = httprequest.ARGS.get('url')\n-    if not url:\n-        return HttpResponse('Bad Request', 400)\n-    url = urllib.unquote(url)\n-\n-    jsoncb = httprequest.ARGS.get('callback')\n-    if jsoncb and not re.match(JSONCALLBACK_RE, jsoncb):\n-        return HttpResponse('Bad callback argument', 400)\n-\n-    def _response(guids=(), message=None):\n-        content = {\n-            'guids': list(guids),\n-            'domain': getattr(spider, 'domain_name', None),\n-            'message': message,\n-        }\n-        return JsonResponse(content=content, callback=jsoncb)\n-\n-    spider = spiders.fromurl(url)\n-    if not spider:\n-        return _response(message='No crawler found for site')\n-\n-    if httprequest.ARGS.get('dontcrawl'):\n-        return _response()\n-\n-\n-    def _on_error(_failure):\n-        return _response(message='Error downloading url from site')\n-\n-    def _on_success(pagedata):\n-        try:\n-            items = spider.identify(pagedata)\n-        except Exception, ex:\n-            return _response(message='Error processing url')\n-\n-        guids = [i.guid for i in items if isinstance(i, ScrapedItem)]\n-        return _response(guids=guids)\n-\n-    deferred = defer.Deferred().addCallbacks(_on_success, _on_error)\n-    request = Request(url=url, callback=deferred, dont_filter=True)\n-    schd = scrapyengine.schedule(request, spider)\n-    schd.chainDeferred(deferred)\n-    return deferred\n-\n-\n-urlmapping = (\n-    ('^ws/tools/url_to_guid/$', url_to_guid),\n-)\n-\n-\n-class UrlToGuidService(WebSite):\n-    def __init__(self):\n-        if not settings.getbool('WS_ENABLED'):\n-            raise NotConfigured\n-\n-        port = settings.getint('WS_PORT') or 8088\n-        timeout = settings.getint('WS_TIMEOUT') or 15 # seconds\n-        resource = WebResource(urlmapping, timeout=timeout)\n-        WebSite.__init__(self, port=port, resource=resource)\n\n@@ -1,133 +0,0 @@\n-\"\"\"Twisted website object as django\n-\n-################################################################################\n-## Simple Usage example:\n-\n-from twisted.internet import reactor\n-from scrapy.contrib.web.http import WebSite, HttpResponse\n-\n-def helloword(request):\n-   return HttpResponse('Hello World!')\n-\n-def hello(request, name):\n-    return HttpResponse('Hello %s' % name)\n-\n-urls = (\n-        ('^hello/(?P<name>\\w+)/$', hello),\n-        ('^$', helloword),\n-        )\n-\n-\n-resource = WebResource(urls)\n-site = WebSite(port=8081, resource=resource)\n-reactor.run()\n-\n-# now go to http://localhost:8081/\n-\n-\n-\n-################################################################################\n-## Complex usage example:\n-\n-from twisted.internet import reactor, defer\n-from scrapy.contrib.web.http import WebSite, HttpResponse\n-\n-def delayed(request):\n-    def _callback(result):\n-        return HttpResponse('Heavy task completed: %s' % result)\n-\n-    def _errback(_failure):\n-        return HttpResponse('Internal Server Error: %s' % _failure, status=500)\n-\n-    def heavytask(_):\n-        import random\n-        assert random.randint(0,1), \"Exception found processing request\"\n-        return _\n-\n-    d = defer.Deferred().addCallback(heavytask)\n-    d.addCallbacks(_callback, _errback)\n-    reactor.callLater(1, d.callback, \"Well done\")\n-    return d\n-\n-urls = (('^delayed/$', delayed),)\n-\n-resource = WebResource(urls)\n-site = WebSite(port=8081, resource=resource)\n-reactor.run()\n-\n-\"\"\"\n-\n-import re\n-\n-from twisted.web import server, resource\n-from twisted.internet import reactor\n-from scrapy.utils.defer import mustbe_deferred\n-\n-from .http import HttpResponse, build_httprequest\n-\n-\n-def urlresolver(urls, path):\n-    \"\"\"Simple path to view mapper\"\"\"\n-    path = path.lstrip('/')\n-    for pathre, view in urls:\n-        m = re.search(pathre, path)\n-        if m:\n-            kwargs = m.groupdict()\n-            args = () if kwargs else m.groups()\n-            return view, args, kwargs\n-    return None, (), {}\n-\n-\n-class WebSite(server.Site):\n-    def __init__(self, port=None, *args, **kwargs):\n-        server.Site.__init__(self, *args, **kwargs)\n-        if port:\n-            self.bind(port)\n-\n-    def bind(self, port):\n-        from scrapy.core.engine import scrapyengine\n-        scrapyengine.listenTCP(port, self)\n-\n-\n-class WebResource(resource.Resource):\n-    \"\"\"Translate twisted web approach to django alike way\"\"\"\n-    isLeaf = True\n-    debug = True\n-\n-    def __init__(self, urls, timeout=3, urlresolver=urlresolver):\n-        resource.Resource.__init__(self)\n-        self.urlresolver = urlresolver\n-        self.timeout = timeout\n-        self.urls = urls\n-\n-    def render(self, twistedrequest):\n-        httprequest = build_httprequest(twistedrequest)\n-\n-        def _send_response(response):\n-            assert isinstance(response, HttpResponse), 'view should return a HttpResponse object'\n-            twistedrequest.setResponseCode(response.status_code or 200)\n-            for key, val in response.items():\n-                twistedrequest.setHeader(key, response[key])\n-            twistedrequest.write(response.content)\n-            twistedrequest.finish()\n-\n-        def _on_error(_failure):\n-            content = _failure.getTraceback() if self.debug else 'Internal Error'\n-            response = HttpResponse(content=str(_failure), status=500)\n-            return _send_response(response)\n-\n-        view, args, kwargs = self.urlresolver(self.urls, httprequest.path)\n-        if not view:\n-            response = HttpResponse(content='Not Found', status=404)\n-            _send_response(response)\n-            return server.NOT_DONE_YET\n-\n-        deferred = mustbe_deferred(view, httprequest, *args, **kwargs)\n-        deferred.addCallback(_send_response)\n-        deferred.addErrback(_on_error)\n-        if not deferred.timeoutCall:\n-            deferred.setTimeout(self.timeout)\n-\n-        return server.NOT_DONE_YET\n-\n-\n\n@@ -1,55 +0,0 @@\n-from functools import partial\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.serialization import serialize\n-from scrapy.conf import settings\n-\n-from .site import WebSite,  WebResource\n-from .json import JsonResponse\n-\n-STATS_WSPORT = settings.getint('STATS_WSPORT', 8089)\n-STATS_WSTIMEOUT = settings.getint('STATS_WSTIMEOUT', 15)\n-\n-jsonserialize = partial(serialize, format='json')\n-\n-\n-def stats(request):\n-    service = request.ARGS.get('service', 'getstats')\n-    if service == 'getstats':\n-        domain = request.ARGS.get('domain')\n-        count = request.ARGS.get('count', 1)\n-        if not domain:\n-            path = request.ARGS.get('path')\n-            offset = request.ARGS.get('offset')\n-            order = request.ARGS.get('order', 'domain')\n-            olist = request.ARGS.get('olist', 'ASC')\n-\n-    from scrapy.store.db import DomainDataHistory\n-    ddh = DomainDataHistory(settings['SCRAPING_DB'], 'domain_data_history')\n-\n-    if service == 'countdomains':\n-        stats = [ddh.domain_count()]\n-    elif domain:\n-        stats = list(ddh.get(domain, count=int(count)))\n-    else:\n-        stats = list(ddh.getlast_alldomains(count, offset, order, olist, path=path))\n-\n-    content = {'data': stats if stats else \"No stats found\"}\n-    return JsonResponse(content=content, serialize=jsonserialize)\n-\n-\n-urlmapping = (\n-        ('^ws/tools/stats/$', stats),\n-        )\n-\n-\n-class StatsService(WebSite):\n-    def __init__(self):\n-        if not settings['WS_ENABLED']:\n-            raise NotConfigured\n-\n-        if not settings['SCRAPING_DB']:\n-            print \"SCRAPING_DB setting is required for the stats web service\"\n-            raise NotConfigured\n-\n-        resource = WebResource(urlmapping, timeout=STATS_WSTIMEOUT)\n-        WebSite.__init__(self, port=STATS_WSPORT, resource=resource)\n\n\n@@ -1,27 +0,0 @@\n-\"\"\"\n-Scheduler information module for Scrapy webconsole\n-\"\"\"\n-from pydispatch import dispatcher\n-from scrapy.core.engine import scrapyengine\n-from scrapy.management.web import banner\n-\n-class EngineStatus(object):\n-    webconsole_id = 'enginestatus'\n-    webconsole_name = 'Engine status'\n-\n-    def __init__(self):\n-        from scrapy.management.web import webconsole_discover_module\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_render(self, wc_request):\n-        s = banner(self)\n-        s += \"<pre><code>\\n\"\n-        s += scrapyengine.getstatus()\n-        s += \"</pre></code>\\n\"\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return s\n-\n-    def webconsole_discover_module(self):\n-        return self\n\n@@ -1,80 +0,0 @@\n-\"\"\"\n-Extension for persistent statistics by domain\n-\"\"\"\n-import re\n-import pprint\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.spider import spiders\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.management.web import banner\n-from scrapy.conf import settings\n-\n-class SpiderStats(object):\n-    webconsole_id = 'spiderstats'\n-    webconsole_name = 'Spider stats (all-time)'\n-\n-    stats_mainpage = [\n-        ('item_scraped_count', \"Items scraped <br>(last run)\"),\n-        ('item_passed_count', \"Items passed <br>(last run)\"),\n-        ('response_count', \"Pages crawled <br>(last run)\"),\n-        ('start_time', \"Start time <br>(last run)\"),\n-        ('finish_time', \"Finish time <br>(last run)\"),\n-        ('finish_status', \"Finish status<br>(last run)\"),\n-        ]\n-\n-    PATH_RE = re.compile(\"/spiderstats/([^/]+)\")\n-    \n-    def __init__(self):\n-        if not settings['SCRAPING_DB']:\n-            raise NotConfigured(\"SpiderStats: Requires SCRAPING_DB setting\")\n-\n-        from scrapy.store.db import DomainDataHistory\n-        self.ddh = DomainDataHistory(settings['SCRAPING_DB'], 'domain_data_history')\n-\n-        from scrapy.management.web import webconsole_discover_module\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_render(self, wc_request):\n-        m = self.PATH_RE.search(wc_request.path)\n-        if m:\n-            return self.render_domain(m.group(1))\n-        else:\n-            return self.render_main()\n-\n-    def render_main(self):\n-        s = banner(self)\n-        s += \"<table border='1'>\\n\"\n-        s += \"<tr>\\n\"\n-        s += \"<th>Domain</th>\"\n-        for path, title in self.stats_mainpage:\n-            s += \"<th>%s</th>\" % title\n-        s += \"</tr>\\n\"\n-        for domain in spiders.asdict().keys():\n-            s += \"<tr>\\n\"\n-            s += \"<td><a href='%s'>%s</a></td>\" % (domain, domain)\n-            for path, title in self.stats_mainpage:\n-                stat = self.ddh.getlast(domain, path=path)\n-                value = stat[1] if stat else \"None\"\n-                s += \"<td>%s</td>\" % value\n-            s += \"</tr>\\n\"\n-        s += \"</table>\\n\"\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return str(s)\n-\n-    def render_domain(self, domain):\n-        s = banner(self)\n-        stats = self.ddh.getall(domain)\n-        s += \"<pre>\\n\"\n-        s += pprint.pformat(list(stats))\n-        s += \"</pre>\\n\"\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return str(s)\n-        \n-    def webconsole_discover_module(self):\n-        return self\n\n@@ -1,26 +0,0 @@\n-import pprint\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.stats import stats\n-from scrapy.management.web import banner, webconsole_discover_module\n-\n-class StatsDump(object):\n-    webconsole_id = 'stats'\n-    webconsole_name = 'StatsCollector dump'\n-\n-    def __init__(self):\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_render(self, wc_request):\n-        s = banner(self)\n-        s += \"<pre><code>\\n\"\n-        s += pprint.pformat(stats)\n-        s += \"</pre></code>\\n\"\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return str(s)\n-\n-    def webconsole_discover_module(self):\n-        return self\n\n\n@@ -1,3 +0,0 @@\n-from scrapy.contrib.history.history import ItemHistory\n-from scrapy.contrib.history.scheduler import RulesScheduler\n-from scrapy.contrib.history.store import SQLHistoryStore\n\n@@ -1,162 +0,0 @@\n-\"\"\"\n-History management\n-\"\"\"\n-import re\n-from datetime import datetime\n-\n-import MySQLdb\n-\n-class History(object):\n-    \"\"\"Base class for tracking different kinds of histories\"\"\"\n-    \n-    def __init__(self, db_uri):\n-        self.db_uri = db_uri\n-        self._mysql_conn = None\n-\n-    def connect(self):\n-        \"\"\"\n-        Connect to PDB and open mysql connect to PRODUCT_DB\n-        \"\"\"\n-        m = re.search(r\"mysql:\\/\\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/]+)/(?P<db>.*)$\", self.db_uri)\n-        if m:\n-            d = m.groupdict()\n-            if d['passwd'] is None:\n-                del(d['passwd'])\n-\n-            d['charset'] = \"utf8\"\n-            self._mysql_conn = MySQLdb.connect(**d)\n-\n-    def get_mysql_conn(self):\n-        if self._mysql_conn is None:\n-            self.connect()\n-        return self._mysql_conn\n-    mysql_conn = property(get_mysql_conn)\n-\n-\n-class ItemHistory(History):\n-    \"\"\"\n-    Class instance registers item guids, and versions with check_item method.\n-    It also gives access to stored ItemTicket and ItemVersion objects.\n-    \"\"\"\n-    NEW = 0\n-    UPDATE = 1\n-    DUPLICATE = 2\n-    \n-    def check_item(self, domain, item):\n-        \"\"\"\n-        Store item's guid and version to the database along\n-        with date and time of last occurence.\n-        Return:\n-         * ItemHistory.NEW       - if guid hasn't been met\n-         * ItemHistory.UPDATE    - if history already contains guid, but versions doesn't match\n-         * ItemHistory.DUPLICATE - both guid and version are not new\n-        \"\"\"\n-        version = item.version\n-\n-        c = self.mysql_conn.cursor(MySQLdb.cursors.DictCursor)\n-\n-        def add_version(version):\n-            insert = \"INSERT INTO version (guid, version, seen) VALUES (%s,%s,%s)\"\n-            c.execute(insert, (item.guid, version, datetime.now()))\n-\n-        select = \"SELECT * FROM ticket WHERE guid=%s\"\n-        c.execute(select, item.guid)\n-        r = c.fetchone()\n-        if r:\n-            select = \"SELECT * FROM version WHERE version=%s\"\n-            if c.execute(select, version):\n-                update = \"UPDATE version SET seen=%s WHERE version=%s\"\n-                c.execute(update, (datetime.now(), version))\n-                self.mysql_conn.commit()\n-                return ItemHistory.DUPLICATE\n-            else:\n-                add_version(version)\n-                self.mysql_conn.commit()\n-                return ItemHistory.UPDATE\n-        else:\n-            insert = \"INSERT INTO ticket (guid, domain, url, url_hash) VALUES (%s,%s,%s,%s)\"\n-            c.execute(insert, (item.guid, domain, item.url, hash(item.url)))\n-            add_version(version)\n-            self.mysql_conn.commit()\n-            return ItemHistory.NEW\n-        \n-    def get_ticket(self, guid):\n-        \"\"\"\n-        Return ItemTicket object for guid. \n-        ItemVersion objects can be accessed via 'versions' list.\n-        \"\"\"\n-        c = self.mysql_conn.cursor(MySQLdb.cursors.DictCursor)\n-        select = \"SELECT * FROM ticket WHERE guid=%s\"\n-        c.execute(select, guid)\n-        ticket = c.fetchone()\n-        if not ticket:\n-            raise Exception(\"Item ticket with guid = '%s' not found\" % guid)\n-        ticket['versions'] = []\n-        select = \"SELECT * FROM version WHERE guid=%s\"\n-        c.execute(select, guid)\n-        for version in c.fetchall():\n-            ticket['versions'].append(version)\n-        return ticket\n-\n-    def delete_ticket(self, guid):\n-        \"\"\"Delete item ticket and associated versions from DB\"\"\"\n-        c = self.mysql_conn.cursor()\n-        delete = \"DELETE FROM ticket WHERE guid=%s\"\n-        c.execute(delete, guid)\n-        self.mysql_conn.commit()\n-\n-\n-class URLHistory(History):\n-    \"\"\"\n-    Access URL status and history for the scraping engine\n-    \n-    This is degsigned to have an instance per domain where typically\n-    a call will be made to get_url_status, followed by either\n-    update_checked or record_version.\n-    \"\"\"\n-    \n-    def get_url_status(self, urlkey):\n-        \"\"\"\n-        Get the url status (url, last_version, last_checked), \n-        or None if the url data has not been seen before\n-        \"\"\"\n-        c = self.mysql_conn.cursor(MySQLdb.cursors.DictCursor)\n-        select = \"SELECT * FROM url_status WHERE url_hash=%s\"\n-        c.execute(select, urlkey)\n-        r = c.fetchone()\n-        return (r['url'], r['last_version'], r['last_checked']) if r else None\n-\n-    def record_version(self, urlkey, url, parent_key, version, postdata_hash=None):\n-        \"\"\"\n-        Record a version of a page and update the last checked time. \n-\n-        If the same version (or None) is passed, the last checked time is still updated.\n-        \"\"\"\n-        now = datetime.now()\n-        c = self.mysql_conn.cursor(MySQLdb.cursors.DictCursor)\n-        select = \"SELECT * FROM url_status WHERE url_hash=%s\"\n-        c.execute(select, urlkey)\n-        r = c.fetchone()\n-\n-        if not r:\n-            insert = \"INSERT INTO url_status (url_hash, url, parent_hash, last_version, last_checked) VALUES (%s,%s,%s,%s,%s)\"\n-            c.execute(insert, (urlkey, url, parent_key, version, now))\n-        else:\n-            update = \"UPDATE url_status SET last_version=%s, last_checked=%s WHERE url_hash=%s\"\n-            c.execute(update, (version, now, urlkey))\n-        self.mysql_conn.commit()\n-\n-        last_version = r['last_version'] if r else None\n-        if version and version != last_version:\n-            if not c.execute(\"SELECT url_hash FROM url_history WHERE version=%s\", version):\n-                insert = \"INSERT INTO url_history (url_hash, version, postdata_hash, created) VALUES (%s,%s,%s,%s)\"\n-                c.execute(insert, (urlkey, version, postdata_hash, now))\n-                self.mysql_conn.commit()\n-\n-    def get_version_info(self, version):\n-        \"\"\"Simple accessor method\"\"\"\n-        c = self.mysql_conn.cursor(MySQLdb.cursors.DictCursor)\n-        select = \"SELECT * FROM url_history WHERE version=%s\"\n-        c.execute(select, version)\n-        r = c.fetchone()\n-        return (r['url_hash'], r['created']) if r else None\n\n@@ -1,47 +0,0 @@\n-\"\"\"\n-SQLHistoryStore\n-\n-Persistent history storage using relational database\n-\"\"\"\n-from scrapy.contrib.history.history import URLHistory\n-from scrapy import log\n-from scrapy.conf import settings\n-\n-class SQLHistoryStore(object) :\n-    \"\"\"Implementation of a data store that stores information in a relation \n-    database\n-\n-    This maintains a URLHistory object per site. That means each domain\n-    has it's own session and is isolated from the others.\n-    \"\"\"\n-    \n-    def __init__(self):\n-        self._store = {}\n-        self._dbinfo = settings['SCRAPING_DB']\n-        self._debug = settings['DEBUG_SQL_HISTORY_STORE']\n-   \n-    def open(self, site):\n-        self._store[site] = URLHistory(self._dbinfo)\n-\n-    def close_site(self, site):\n-        self._store[site].close()\n-        del self._store[site]\n-    \n-    def store(self, site, key, url, parent=None, version=None, post_version=None):\n-        history = self._store[site]\n-        if self._debug:\n-            log.msg(\"record_version(key=%s, url=%s, parent=%s, version=%s, post_version=%s)\" % \n-                    (key, url, parent, version, post_version), domain=site, level=log.DEBUG)\n-        history.record_version(key, url, parent, version, post_version)\n-\n-    def has_site(self, site):\n-        return site in self._store\n-\n-    def status(self, site, key):\n-        if site in self._store:\n-            history = self._store[site]\n-            return history.get_url_status(key)\n-\n-    def version_info(self, site, version):\n-        history = self._store[site]\n-        return history.get_version_info(version)\n\n@@ -1,3 +0,0 @@\n-\"\"\"\n-Scrapy core library classes and functions.\n-\"\"\"\n\n@@ -1 +0,0 @@\n-from scrapy.core.downloader.manager import Downloader\n\n@@ -1,45 +0,0 @@\n-\"\"\"\n-The Prioritizer is a class which receives a list of elements and prioritizes\n-it. It's used for defining the order in which domains are to be scraped. \n-\n-A Prioritizer basically consists of a class which receives a list of elements\n-in its constructs and contains only one method: get_priority() which returns\n-the priority of the given element. The element passed to get_priority() must\n-exists in the list of elements passed in the constructor.\n-\n-This module contains several basic prioritizers.\n-\n-For more advanced prioritizers see: scrapy.contrib.prioritizers\n-\"\"\"\n-import random\n-\n-class NullPrioritizer(object):\n-    \"\"\"\n-    This prioritizer always return the same priority (1)\n-    \"\"\"\n-    def __init__(self, elements):\n-        pass\n-\n-    def get_priority(self, element):\n-        return 1\n-\n-class RandomPrioritizer(object):\n-    \"\"\"\n-    This prioritizer always return a random priority\n-    \"\"\"\n-    def __init__(self, elements):\n-        self.count = len(elements)\n-\n-    def get_priority(self, element):\n-        return random.randrange(0, self.count)\n-\n-class AlphabeticPrioritizer(object):\n-    \"\"\"\n-    This prioritizer priotizes based on the alphabetic order of the element\n-    \"\"\"\n-    def __init__(self, elements):\n-        self.elements = elements[:]\n-        self.elements.sort()\n-\n-    def get_priority(self, element):\n-        return self.elements.index(element)\n\n@@ -1,35 +0,0 @@\n-\"\"\"\n-The datastore module contains implementations of data storage engines for the\n-crawling process. These are used to track metrics on each site and on each\n-page visited.\n-\"\"\"\n-from datetime import datetime\n-\n-class MemoryStore(object) :\n-    \"\"\"Simple implementation of a data store. This is useful if no persistent \n-    history is required (e.g. unit testing or development) and provides a \n-    simpler reference implementation.\n-    \"\"\"\n-    def __init__(self) :\n-        \"\"\"The store will just be a dict with an entry for each site, that \n-        entry will contain dicts and lists of data.\n-        \"\"\"\n-        self._store = {}\n-\n-    def open(self, site):\n-        self._store[site] = {}\n-\n-    def close_site(self, site):\n-        del self._store[site]\n-\n-    def store(self, site, key, url, parent=None, version=None, post_version=None):\n-        checked = datetime.now()\n-        self._store[key] = (version, checked)\n-\n-    def status(self, site, key):\n-        \"\"\"Get the version and last checked time for a key.\n-\n-        this will be changed later to support checking (lazily) the last modified \n-        time and perhaps other statistics needed by the scheduling algorithms\n-        \"\"\"\n-        return self._store.get(key)\n\n@@ -1,51 +0,0 @@\n-import urlparse\n-\n-from scrapy.spider import spiders\n-from scrapy.http import Request\n-from scrapy.core.manager import scrapymanager\n-from scrapy.spider import BaseSpider\n-\n-def fetch(urls, perdomain=False):\n-    \"\"\"Download a set of urls. This is starts a reactor and is suitable\n-    for calling from a main method - not for calling from within the \n-    framework.\n-\n-    It will return a list of Response objects for all pages successfully\n-    downloaded.\n-    \"\"\"\n-    bigbucket = {} # big bucket to store response objects (per domain)\n-\n-    def _add(response, domain):\n-        if not domain in bigbucket:\n-            bigbucket[domain] = []\n-        bigbucket[domain] += [response]\n-\n-    # url clasification by domain\n-    requestdata = set()\n-    for url in urls:\n-        domain = get_or_create_spider(url).domain_name\n-        request = Request(url=url, callback=lambda r: _add(r, domain), dont_filter=True)\n-        requestdata.add(request)\n-\n-    scrapymanager.runonce(*requestdata)\n-\n-    # returns bigbucket as dict or flatted\n-    if perdomain:\n-        return bigbucket\n-\n-    flatbucket = []\n-    for domain, responses in bigbucket.iteritems():\n-        flatbucket.extend(responses)\n-    return flatbucket\n-\n-\n-def get_or_create_spider(url):\n-    # dirty hack to allow downloading pages from unknown domains\n-    spider = spiders.fromurl(url)\n-    if not spider:\n-        domain = str(urlparse.urlparse(url).hostname or spiders.default_domain)\n-        spider = BaseSpider()\n-        spider.domain_name = domain\n-        spiders._alldict[domain] = spider\n-    return spider\n-\n\n@@ -1,57 +0,0 @@\n-\"\"\"\n-The Url class is similar to the urlparse.ParseResult class (it has the same\n-attributes) with the following differences:\n-\n-- it inherits from str\n-- it's lazy, so it only parses the url when needed\n-\"\"\"\n-\n-import urlparse\n-\n-class Url(str):\n-\n-    @property\n-    def parsedurl(self):\n-        if not hasattr(self, '_parsedurl'):\n-            self._parsedurl = urlparse.urlparse(self)\n-        return self._parsedurl\n-\n-    @property\n-    def scheme(self):\n-        return self.parsedurl.scheme\n-\n-    @property\n-    def netloc(self):\n-        return self.parsedurl.netloc\n-\n-    @property\n-    def path(self):\n-        return self.parsedurl.path\n-\n-    @property\n-    def params(self):\n-        return self.parsedurl.params\n-\n-    @property\n-    def query(self):\n-        return self.parsedurl.query\n-\n-    @property\n-    def fragment(self):\n-        return self.parsedurl.fragment\n-\n-    @property\n-    def username(self):\n-        return self.parsedurl.username\n-\n-    @property\n-    def password(self):\n-        return self.parsedurl.password\n-\n-    @property\n-    def hostname(self):\n-        return self.parsedurl.hostname\n-\n-    @property\n-    def port(self):\n-        return self.parsedurl.port\n\n\n@@ -1,42 +0,0 @@\n-import pprint\n-\n-from twisted.internet import reactor\n-from twisted.conch import manhole, telnet\n-from twisted.conch.insults import insults\n-from twisted.internet import protocol\n-\n-from scrapy.extension import extensions\n-from scrapy.core.manager import scrapymanager\n-from scrapy.core.engine import scrapyengine\n-from scrapy.stats import stats\n-from scrapy.conf import settings\n-\n-try:\n-    import guppy\n-    hpy = guppy.hpy()\n-except ImportError:\n-    hpy = None\n-\n-telnet_namespace = {'ee': scrapyengine,\n-                    'em': scrapymanager,\n-                    'ex': extensions.enabled,\n-                    'st': stats,\n-                    'h': hpy,\n-                    'p': pprint.pprint}  # useful shortcut for debugging\n-\n-def makeProtocol():\n-    return telnet.TelnetTransport(telnet.TelnetBootstrapProtocol,\n-                                  insults.ServerProtocol,\n-                                  manhole.Manhole, telnet_namespace)\n-\n-class TelnetConsole(protocol.ServerFactory):\n-\n-    def __init__(self):\n-        if not settings.getbool('TELNETCONSOLE_ENABLED'):\n-            return\n-\n-        #protocol.ServerFactory.__init__(self)\n-        self.protocol = makeProtocol\n-        self.noisy = False\n-        port = settings.getint('TELNETCONSOLE_PORT')\n-        scrapyengine.listenTCP(port, self)\n\n\n@@ -1,4 +0,0 @@\n-from scrapy.spider.models import BaseSpider\n-from scrapy.spider.manager import SpiderManager\n-\n-spiders = SpiderManager()\n\n@@ -1,3 +0,0 @@\n-from scrapy.stats.statscollector import StatsCollector\n-\n-stats = StatsCollector()\n\n@@ -1,77 +0,0 @@\n-\"\"\"\n-Scrapy extension for collecting scraping stats\n-\"\"\"\n-import pprint\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.utils.misc import stats_getpath\n-from scrapy.conf import settings\n-\n-class StatsCollector(dict):\n-    # signal sent after a domain is opened for stats collection and its resorces have been allocated. args: domain\n-    domain_open = object()\n-    # signal sent before a domain is closed, and before the stats are persisted. it can be catched to add additional stats. args: domain\n-    domain_closing = object()\n-    # signal sent after a domain is closed and its resources have been freed.  args: domain\n-    domain_closed = object()\n-\n-    def __init__(self, enabled=None):\n-        self.db = None\n-        self.debug = settings.getbool('STATS_DEBUG')\n-        self.enabled = enabled if enabled is not None else settings.getbool('STATS_ENABLED')\n-        self.cleanup = settings.getbool('STATS_CLEANUP')\n-\n-        if self.enabled:\n-            if settings['SCRAPING_DB']:\n-                from scrapy.store.db import DomainDataHistory\n-                self.db = DomainDataHistory(settings['SCRAPING_DB'], table_name='domain_data_history')\n-\n-            dispatcher.connect(self._domain_open, signal=signals.domain_open)\n-            dispatcher.connect(self._domain_closed, signal=signals.domain_closed)\n-        else:\n-            self.setpath = lambda *args, **kwargs: None\n-            self.getpath = lambda path, default=None: default\n-            self.incpath = lambda *args, **kwargs: None\n-            self.delpath = lambda *args, **kwargs: None\n-\n-    def setpath(self, path, value):\n-        d = self\n-        keys = path.split('/')\n-        for key in keys[:-1]:\n-            if key not in d:\n-                d[key] = {}\n-            d = d[key]\n-        d[keys[-1]] = value\n-\n-    def getpath(self, path, default=None):\n-        return stats_getpath(self, path, default)\n-\n-    def delpath(self, path):\n-        d = self\n-        keys = path.split('/')\n-        for key in keys[:-1]:\n-            if key in d:\n-                d = d[key]\n-            else:\n-                return\n-        del d[keys[-1]]\n-\n-    def incpath(self, path, value_diff=1):\n-        curvalue = self.getpath(path) or 0\n-        self.setpath(path, curvalue + value_diff)\n-\n-    def _domain_open(self, domain, spider):\n-        dispatcher.send(signal=self.domain_open, sender=self.__class__, domain=domain, spider=spider)\n-\n-    def _domain_closed(self, domain, spider, status):\n-        dispatcher.send(signal=self.domain_closing, sender=self.__class__, domain=domain, spider=spider, status=status)\n-        if self.debug:\n-            log.msg(pprint.pformat(self[domain]), domain=domain, level=log.DEBUG)\n-        if self.db:\n-            self.db.put(domain, self[domain])\n-        if self.cleanup:\n-            del self[domain]\n-        dispatcher.send(signal=self.domain_closed, sender=self.__class__, domain=domain, spider=spider, status=status)\n\n\n@@ -1,130 +0,0 @@\n-\"\"\"\n-Store implementations using a MySQL DB\n-\"\"\"\n-import cPickle as pickle\n-from datetime import datetime\n-\n-from scrapy.utils.misc import stats_getpath\n-from scrapy.utils.db import mysql_connect\n-\n-class DomainDataHistory(object):\n-    \"\"\"\n-    This is a store for domain data, with history.\n-    \"\"\"\n-\n-    def __init__(self, db_uri, table_name):\n-        self.db_uri = db_uri\n-        self._mysql_conn = None\n-        self._table = table_name\n-\n-    def get_mysql_conn(self):\n-        if self._mysql_conn is None:\n-            self._mysql_conn = mysql_connect(self.db_uri, use_unicode=False)\n-        return self._mysql_conn\n-    mysql_conn = property(get_mysql_conn)\n-\n-    def get(self, domain, count=1, offset=0, stored_after=None, stored_before=None, path=None):\n-        \"\"\"\n-        Get the last records stored for the given domain. \n-        If count is given that number of records is retunred, otherwise all records.\n-        If stored_{after,before} is given the results are restricted to that time interval.\n-        The result is a list of tuples: (timestamp, data)\n-        If path is given it only returns stats of that given path (only for dict objects like stats)\n-        \"\"\"\n-        sqlsuf = \"\"\n-        if count is not None:\n-            sqlsuf += \"LIMIT %d \" % count\n-        if offset:\n-            sqlsuf += \"OFFSET %d \" % offset\n-        filter = \"\"\n-        if stored_after:\n-            filter += \"AND stored>='%s'\" % stored_after.strftime(\"%Y-%m-%d %H:%M:%S\")\n-        if stored_before:\n-            filter += \"AND stored<='%s'\" % stored_before.strftime(\"%Y-%m-%d %H:%M:%S\")\n-\n-        c = self.mysql_conn.cursor()\n-        select = \"SELECT stored,data FROM %s WHERE domain=%%s %s ORDER BY stored DESC %s\" % (self._table, filter, sqlsuf)\n-        c.execute(select, domain)\n-        for stored, datas in c:\n-            obj = pickle.loads(datas)\n-            if path and isinstance(obj, dict):\n-                yield stored, stats_getpath(obj, path)\n-            else:\n-                yield stored, obj\n-\n-    def getlast(self, domain, offset=0, path=None):\n-        \"\"\"\n-        Get the Nth last data stored for the given domain\n-        If offset=0 get the last data stored\n-        If offset=1 get the previous data stored\n-        ...and so on\n-        \"\"\"\n-        datal = list(self.get(domain, count=1, offset=offset, path=path))\n-        if datal:\n-            return datal[0]\n-        \n-    def getall(self, domain, path=None):\n-        \"\"\"\n-        Get all data stored for the given domain as a list of tuples:\n-        (stored, timestamp)\n-        \"\"\"\n-        return self.get(domain, count=None, path=path)\n-\n-    def getlast_alldomains(self, count=None, offset=None, order='domain', olist='ASC', path=None):\n-        \"\"\"\n-        Get all last data for all domains as a list of tuples, unless\n-        data is sliced by count and offset values.\n-        Ordered by domain is the default.\n-        TODO: This could be merged with: get(domain=None, ..., +order)\n-\n-        Return a list of tuples (domain, timestamp, data)\n-        \"\"\"\n-        sqlsuf = \"\"\n-        if count is not None:\n-            sqlsuf += \"LIMIT %s \" % count\n-        if offset:\n-            sqlsuf += \"OFFSET %s \" % offset\n-        \n-        c = self.mysql_conn.cursor()\n-        select = \"SELECT domain, MAX(stored) as stored, data \\\n-                  FROM ( \\\n-                    SELECT * FROM %s ORDER BY stored DESC \\\n-                  ) AS inner_ordered \\\n-                  GROUP BY domain \\\n-                  ORDER BY %s %s %s\" % (self._table, order, olist, sqlsuf)\n-        c.execute(select)\n-        for domain, stored, datas in c:\n-            obj = pickle.loads(datas)\n-            if path and isinstance(obj, dict):\n-                yield domain, stored, stats_getpath(obj, path)\n-            else:\n-                yield domain, stored, obj\n-\n-    def domain_count(self):\n-        \"\"\"\n-        Return the number of domains stored in the database\n-        \"\"\"\n-        c = self.mysql_conn.cursor()\n-        c.execute(\"SELECT COUNT(DISTINCT(domain)) FROM %s\" % self._table)\n-        return c.fetchone()[0]\n-\n-    def put(self, domain, data, timestamp=None):\n-        \"\"\"\n-        Store data in history using the given timestamp (or now if omitted).\n-        data can be any pickable object\n-        \"\"\"\n-        stored = timestamp if timestamp else datetime.now()\n-        datas = pickle.dumps(data)\n-\n-        c = self.mysql_conn.cursor()\n-        c.execute(\"INSERT INTO %s (domain,stored,data) VALUES (%%s,%%s,%%s)\" % self._table, (domain, stored, datas))\n-        self.mysql_conn.commit()\n-\n-    def remove(self, domain):\n-        \"\"\"\n-        Remove all data stored for the given domain\n-        \"\"\"\n-        c = self.mysql_conn.cursor()\n-        c.execute(\"DELETE FROM %s WHERE domain=%%s\" % self._table, domain)\n-        self.mysql_conn.commit()\n-\n\n\n@@ -1 +0,0 @@\n-# Place here all your scrapy spiders\n\n@@ -1,28 +0,0 @@\n-import unittest\n-from scrapy.utils.c14n import canonicalize\n-\n-class C14nTest(unittest.TestCase):\n-    \"\"\" c14n comparison functions \"\"\"\n-        \n-    def test_canonicalize(self):\n-        \"\"\"Test URL canonicalization function\"\"\"\n-        urls = [\n-            ('http://www.maddiebrown.co.uk//cgi-bin//html_parser.cgi?web_page_id=12&template_file=catalogue_list_items_template&category-id=1000070&category-type=1&page=3&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2',\n-            'http://www.maddiebrown.co.uk//cgi-bin//html_parser.cgi?web_page_id=12&template_file=catalogue_list_items_template&category-id=1000070&category-type=1&page=3&page=2'),\n-             \n-\n-            ('http://www.mfi.co.uk/mfi/productinfo.asp?CT=/1010/YourHomeOffice&CT=/1025/YourHomeOffice/Packages',\n-             'http://www.mfi.co.uk/mfi/productinfo.asp?CT=/1010/YourHomeOffice&CT=/1025/YourHomeOffice/Packages'),\n-\n-            ('http://www.test-nice-url.com/index.html',\n-             'http://www.test-nice-url.com/index.html'),\n-\n-            ('http://www.homebase.co.uk/webapp/wcs/stores/servlet/ProductDisplay?storeId=20001&langId=-1&catalogId=10701&productId=729482&Trail=C$cip=50704&categoryId=50704',\n-             'http://www.homebase.co.uk/webapp/wcs/stores/servlet/ProductDisplay?storeId=20001&langId=-1&catalogId=10701&productId=729482&Trail=C$cip=50704&categoryId=50704'),\n-        ]\n-        for origurl, c14nurl in urls:\n-            self.assertEqual(canonicalize(origurl), c14nurl)\n-        \n-if __name__ == \"__main__\":\n-    unittest.main()\n-        \n\n@@ -1,16 +0,0 @@\n-from unittest import TestCase, main\n-\n-class ScrapyUtilsTest(TestCase):\n-    def test_openssl(self):\n-        try:\n-            module = __import__('OpenSSL', {}, {}, [''])\n-        except ImportError, ex:\n-            return # no openssl installed\n-\n-        required_version = '0.6'\n-        if hasattr(module, '__version__'):\n-            for cur, req in zip(module.__version__.split('.'), required_version.split('.')):\n-                self.assertFalse(cur < req, \"module %s >= %s required\" % ('OpenSSL', required_version))\n-\n-if __name__ == \"__main__\":\n-    main()\n\n@@ -1,24 +0,0 @@\n-from twisted.trial import unittest\n-\n-class SpidermonkeyTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        try:\n-            from spidermonkey import Runtime\n-            r = Runtime()\n-            self.cx = r.new_context()\n-        except ImportError:\n-            raise unittest.SkipTest(\"Spidermonkey C library not available\")\n-\n-    def test_spidermonkey(self):\n-        \"\"\"Spidermonkey basic functionality tests\"\"\"\n-\n-        self.cx.eval_script(\"var item0={'price':50}\")\n-        self.cx.eval_script(\"var item1={'price':20}\")\n-        self.assertEqual(self.cx.eval_script(\"item0\"), {'price': 50})\n-        self.assertEqual(self.cx.eval_script(\"item1\"), {'price': 20})\n-        self.cx.eval_script(\"var itemArray=new Array(item0, item1)\")\n-        self.assertEqual(self.cx.eval_script(\"itemArray\"), [{'price': 50}, {'price':20}])\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n\n@@ -1,52 +0,0 @@\n-from unittest import TestCase, main\n-from scrapy.conf import settings\n-from scrapy.stats.statscollector import StatsCollector\n-\n-class StatsTest(TestCase):\n-    def test_stats(self):\n-        stats = StatsCollector(enabled=False)\n-        self.assertEqual(stats, {})\n-        self.assertEqual(stats.getpath('anything'), None)\n-        self.assertEqual(stats.getpath('anything', 'default'), 'default')\n-        stats.setpath('test', 'value')\n-        self.assertEqual(stats, {})\n-\n-        stats = StatsCollector(enabled=True)\n-        self.assertEqual(stats, {})\n-        self.assertEqual(stats.getpath('anything'), None)\n-        self.assertEqual(stats.getpath('anything', 'default'), 'default')\n-        stats.setpath('test', 'value')\n-        self.assertEqual(stats, {'test': 'value'})\n-        stats.setpath('test2', 23)\n-        self.assertEqual(stats, {'test': 'value', 'test2': 23})\n-        stats.setpath('one/two', 'val2')\n-        self.assertEqual(stats, {'test': 'value', 'test2': 23, 'one': {'two': 'val2'}})\n-        self.assertEqual(stats.getpath('one/two'), 'val2')\n-        self.assertEqual(stats.getpath('one/three'), None)\n-        self.assertEqual(stats.getpath('one/three', 'four'), 'four')\n-        self.assertEqual(stats.getpath('one'), {'two': 'val2'})\n-\n-        # nodes must contain either data or other nodes, but not both!\n-        self.assertRaises(TypeError, stats.setpath, 'one/two/three', 22)\n-\n-        stats.delpath('test2')\n-        self.assertEqual(stats, {'test': 'value', 'one': {'two': 'val2'}})\n-        stats.delpath('one/two')\n-        self.assertEqual(stats, {'test': 'value', 'one': {}})\n-        stats.delpath('one')\n-        self.assertEqual(stats, {'test': 'value'})\n-\n-        stats.setpath('one/other/three', 20)\n-        self.assertEqual(stats.getpath('one/other/three'), 20)\n-        stats.incpath('one/other/three')\n-        self.assertEqual(stats.getpath('one/other/three'), 21)\n-        stats.incpath('one/other/three', 4)\n-        self.assertEqual(stats.getpath('one/other/three'), 25)\n-\n-        stats.incpath('one/newnode', 1)\n-        self.assertEqual(stats.getpath('one/newnode'), 1)\n-        stats.incpath('one/newnode', -1)\n-        self.assertEqual(stats.getpath('one/newnode'), 0)\n-\n-if __name__ == \"__main__\":\n-    main()\n\n\n@@ -1,110 +0,0 @@\n-\"\"\"\n-Canonicalization routine for URLs\n-\"\"\"\n-import re\n-import urlparse\n-\n-_hexmatcher = re.compile(\"\\%[0-9A-Fa-f]{2}\")\n-\n-def get_canonized_port(port):\n-    if port and port != 80:\n-        cport = \":\" + str(port)\n-    else:\n-        cport = \"\"\n-    return cport\n-\n-def create_params_dict(params):\n-    param_dict = {}\n-    for param in params.split(\"&\"):\n-        nvpair = param.split(\"=\")\n-        if len(nvpair) >= 2:\n-            param_dict[nvpair[0]] = '='.join(nvpair[1:])\n-        else:\n-            param_dict[nvpair[0]] = None\n-    return param_dict\n-    \n-def create_unique_params_list(params):\n-    \"\"\"\n-    This function build a query string from unique params only.\n-    Unique params are params with unique key=>value pair. So param=qwe and param=123\n-    are diffrent and a query string will be param=qwe&param=123.\n-    If input params will contain pairs like this param=123 and param=123 a query\n-    string will be just param=123\n-    We have to do this because some ASP sites uses the same parameters names in the URLs,\n-    but with diffrent values. Such queries are interpreted as arrays in the ASP framework.\n-    \"\"\"\n-    param_list = []\n-    if params:\n-        for param in params.split(\"&\"):\n-            nvpair = param.split(\"=\", 1)\n-            if len(nvpair) >= 2:\n-                param_list.append(nvpair[0] + '=' + nvpair[1])\n-            else:\n-                param_list.append(nvpair[0] + '=')\n-        if param_list:\n-            p_list = []\n-            # select only unique pairs, it is possible to use set() for this but change order of pairs...\n-            for param in param_list:\n-                if param not in p_list:\n-                    p_list.append(param)\n-            return '&'.join(p_list)\n-    return ''\n-\n-def convert_escaped(string_to_convert):\n-    hexvalues = _hexmatcher.findall(string_to_convert)\n-    for hexvalue in hexvalues:\n-        string_to_convert = string_to_convert.replace(\n-                hexvalue, chr(int(hexvalue[1:len(hexvalue)],16)))\n-    return string_to_convert\n-\n-def remove_dirs(path):\n-    dirs = path.split(\"/\")\n-    dirs.reverse()\n-    stripped_dirs = []\n-    ellipsis = 0\n-    for dir in dirs:\n-        if dir == \"..\" :\n-            ellipsis += 1\n-        elif dir == \".\" :\n-            pass\n-        else:\n-            if ellipsis > 0 :\n-                ellipsis -= 1\n-            else:\n-                stripped_dirs.append(dir)\n-    stripped_dirs.reverse()\n-    return \"/\".join(stripped_dirs)\n-\n-def canonicalize(url):\n-    \"\"\"\n-    This method c14ns the url we are passed by doing the following\n-    1) lowercasing the hostname and scheme\n-    2) removing the port if it is 80\n-    3) making the query params unique and alphabetized\n-    4) removing url encoding\n-    5) Removing any ./\n-    6) Removing and ../ and their preceding directories\n-    \"\"\"\n-\n-    # Remove the fragment marker, even if it's url encoded\n-    defragmented_url = urlparse.urldefrag(url)[0]\n-\n-    # This automatically converts the scheme and hostname to lower case \n-    parsed = urlparse.urlparse(defragmented_url)\n-\n-    # Get the port in the correct format \n-    cport = get_canonized_port(parsed.port)\n-    \n-    query = create_unique_params_list(parsed.query)\n-\n-    # Remove the url encoded params from the path and query string\n-    query = convert_escaped(query)\n-    if query :\n-        query = \"?\" + query\n-    path = convert_escaped(parsed.path)\n-\n-    # Remove the ../ and the previous directories and remove the ./\n-    path = remove_dirs(path)\n-\n-    # Now put it all back together\n-    return \"%s://%s%s%s%s\" % (parsed.scheme, parsed.hostname, cport, path, query)\n\n@@ -1,49 +0,0 @@\n-\"\"\"\n-Helper functions for formatting and pretty printing some objects\n-\"\"\"\n-import sys\n-import pprint as pypprint\n-\n-from scrapy.item import ScrapedItem\n-from scrapy.http import Request, Response\n-\n-nocolour = False\n-\n-def colorize(text):\n-    if nocolour or not sys.stdout.isatty():\n-        return text\n-    try:\n-        from pygments import highlight\n-        from pygments.formatters import TerminalFormatter\n-        from pygments.lexers import PythonLexer\n-        return highlight(text, PythonLexer(), TerminalFormatter())\n-    except ImportError:\n-        return text\n-\n-def pformat_dictobj(obj):\n-    clsname = obj.__class__.__name__\n-    return \"%s(%s)\\n\" % (clsname, colorize(pypprint.pformat(obj.__dict__)))\n-\n-def pprint_dictobj(obj):\n-    print pformat_dictobj(obj)\n-\n-def pformat(obj, *args, **kwargs):\n-    \"\"\"\n-    Wrapper which autodetects the object type and uses the proper formatting\n-    function\n-    \"\"\"\n-\n-    if isinstance(obj, (list, tuple)):\n-        return \"\".join([pformat(i, *args, **kwargs) for i in obj])\n-    elif isinstance(obj, (ScrapedItem, Request, Response)):\n-        return pformat_dictobj(obj)\n-    else:\n-        return colorize(pypprint.pformat(obj))\n-\n-def pprint(obj, *args, **kwargs):\n-    \"\"\"\n-    Wrapper which autodetects the object type and uses the proper printing\n-    function\n-    \"\"\"\n-\n-    print pformat(obj, *args, **kwargs)\n\n@@ -1,74 +0,0 @@\n-\"\"\"\n-This module provides a universal API for different serialization formats.\n-\n-Keep in mind that not all formats support all types for serialization, and some\n-formats (like pprint) may be unsafe for unserialization (like pprint) but it\n-may still very convenient for serialization.\n-\"\"\"\n-\n-import datetime\n-import decimal\n-import cPickle as pickle\n-import pprint\n-\n-import simplejson\n-\n-class ScrapyJSONEncoder(simplejson.JSONEncoder):\n-    \"\"\"\n-    JSONEncoder subclass that knows how to encode date/time and decimal types.\n-    \"\"\"\n-\n-    DATE_FORMAT = \"%Y-%m-%d\"\n-    TIME_FORMAT = \"%H:%M:%S\"\n-\n-    def default(self, o):\n-        if isinstance(o, datetime.datetime):\n-            return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n-        elif isinstance(o, datetime.date):\n-            return o.strftime(self.DATE_FORMAT)\n-        elif isinstance(o, datetime.time):\n-            return o.strftime(self.TIME_FORMAT)\n-        elif isinstance(o, decimal.Decimal):\n-            return str(o)\n-        else:\n-            return super(ScrapyJSONEncoder, self).default(o)\n-\n-serialize_funcs = {\n-    'json': lambda obj: simplejson.dumps(obj, cls=ScrapyJSONEncoder),\n-    'pprint': lambda obj: pprint.pformat(obj),\n-    'pickle': lambda obj: pickle.dumps(obj),\n-}\n-\n-unserialize_funcs = {\n-    'json': lambda text: simplejson.loads(text),\n-    'pprint': lambda text: eval(text),\n-    'pickle': lambda text: pickle.loads(text),\n-}\n-\n-def serialize(obj, format='pickle'):\n-    \"\"\"\n-    Main entrance point for serialization.\n-\n-    Supported formats: pickle, json, pprint\n-    \"\"\"\n-    try:\n-        func = serialize_funcs[format]\n-        return func(obj)\n-    except KeyError:\n-        raise TypeError(\"Unknown serialization format: %s\" % format)\n-\n-def unserialize(text, format='pickle'):\n-    \"\"\"\n-    Main entrance point for unserialization.\n-    \"\"\"\n-    try:\n-        func = unserialize_funcs[format]\n-        return func(text)\n-    except KeyError:\n-        raise TypeError(\"Unknown serialization format: %s\" % format)\n-\n-def parse_jsondatetime(text):\n-    if isinstance(text, basestring):\n-        return datetime.datetime.strptime(text, \"%s %s\" % \\\n-               (ScrapyJSONEncoder.DATE_FORMAT, ScrapyJSONEncoder.TIME_FORMAT))\n-\n\n@@ -1,1931 +0,0 @@\n-\"\"\"Beautiful Soup\n-Elixir and Tonic\n-\"The Screen-Scraper's Friend\"\n-http://www.crummy.com/software/BeautifulSoup/\n-\n-Beautiful Soup parses a (possibly invalid) XML or HTML document into a\n-tree representation. It provides methods and Pythonic idioms that make\n-it easy to navigate, search, and modify the tree.\n-\n-A well-formed XML/HTML document yields a well-formed data\n-structure. An ill-formed XML/HTML document yields a correspondingly\n-ill-formed data structure. If your document is only locally\n-well-formed, you can use this library to find and process the\n-well-formed part of it.\n-\n-Beautiful Soup works with Python 2.2 and up. It has no external\n-dependencies, but you'll have more success at converting data to UTF-8\n-if you also install these three packages:\n-\n-* chardet, for auto-detecting character encodings\n-  http://chardet.feedparser.org/\n-* cjkcodecs and iconv_codec, which add more encodings to the ones supported\n-  by stock Python.\n-  http://cjkpython.i18n.org/\n-\n-Beautiful Soup defines classes for two main parsing strategies:\n-\n- * BeautifulStoneSoup, for parsing XML, SGML, or your domain-specific\n-   language that kind of looks like XML.\n-\n- * BeautifulSoup, for parsing run-of-the-mill HTML code, be it valid\n-   or invalid. This class has web browser-like heuristics for\n-   obtaining a sensible parse tree in the face of common HTML errors.\n-\n-Beautiful Soup also defines a class (UnicodeDammit) for autodetecting\n-the encoding of an HTML or XML document, and converting it to\n-Unicode. Much of this code is taken from Mark Pilgrim's Universal Feed Parser.\n-\n-For more than you ever wanted to know about Beautiful Soup, see the\n-documentation:\n-http://www.crummy.com/software/BeautifulSoup/documentation.html\n-\n-Here, have some legalese:\n-\n-Copyright (c) 2004-2007, Leonard Richardson\n-\n-All rights reserved.\n-\n-Redistribution and use in source and binary forms, with or without\n-modification, are permitted provided that the following conditions are\n-met:\n-\n-  * Redistributions of source code must retain the above copyright\n-    notice, this list of conditions and the following disclaimer.\n-\n-  * Redistributions in binary form must reproduce the above\n-    copyright notice, this list of conditions and the following\n-    disclaimer in the documentation and/or other materials provided\n-    with the distribution.\n-\n-  * Neither the name of the the Beautiful Soup Consortium and All\n-    Night Kosher Bakery nor the names of its contributors may be\n-    used to endorse or promote products derived from this software\n-    without specific prior written permission.\n-\n-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n-\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n-LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n-A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n-CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n-EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n-PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n-PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n-LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n-NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, DAMMIT.\n-\n-\"\"\"\n-from __future__ import generators\n-\n-__author__ = \"Leonard Richardson (leonardr@segfault.org)\"\n-__version__ = \"3.0.6\"\n-__copyright__ = \"Copyright (c) 2004-2008 Leonard Richardson\"\n-__license__ = \"New-style BSD\"\n-\n-from sgmllib import SGMLParser, SGMLParseError\n-import codecs\n-import types\n-import re\n-import sgmllib\n-try:\n-  from htmlentitydefs import name2codepoint\n-except ImportError:\n-  name2codepoint = {}\n-\n-#This hack makes Beautiful Soup able to parse XML with namespaces\n-sgmllib.tagfind = re.compile('[a-zA-Z][-_.:a-zA-Z0-9]*')\n-\n-DEFAULT_OUTPUT_ENCODING = \"utf-8\"\n-\n-# First, the classes that represent markup elements.\n-\n-class PageElement:\n-    \"\"\"Contains the navigational information for some part of the page\n-    (either a tag or a piece of text)\"\"\"\n-\n-    def setup(self, parent=None, previous=None):\n-        \"\"\"Sets up the initial relations between this element and\n-        other elements.\"\"\"\n-        self.parent = parent\n-        self.previous = previous\n-        self.next = None\n-        self.previousSibling = None\n-        self.nextSibling = None\n-        if self.parent and self.parent.contents:\n-            self.previousSibling = self.parent.contents[-1]\n-            self.previousSibling.nextSibling = self\n-\n-    def replaceWith(self, replaceWith):\n-        oldParent = self.parent\n-        myIndex = self.parent.contents.index(self)\n-        if hasattr(replaceWith, 'parent') and replaceWith.parent == self.parent:\n-            # We're replacing this element with one of its siblings.\n-            index = self.parent.contents.index(replaceWith)\n-            if index and index < myIndex:\n-                # Furthermore, it comes before this element. That\n-                # means that when we extract it, the index of this\n-                # element will change.\n-                myIndex = myIndex - 1\n-        self.extract()\n-        oldParent.insert(myIndex, replaceWith)\n-\n-    def extract(self):\n-        \"\"\"Destructively rips this element out of the tree.\"\"\"\n-        if self.parent:\n-            try:\n-                self.parent.contents.remove(self)\n-            except ValueError:\n-                pass\n-\n-        #Find the two elements that would be next to each other if\n-        #this element (and any children) hadn't been parsed. Connect\n-        #the two.\n-        lastChild = self._lastRecursiveChild()\n-        nextElement = lastChild.next\n-\n-        if self.previous:\n-            self.previous.next = nextElement\n-        if nextElement:\n-            nextElement.previous = self.previous\n-        self.previous = None\n-        lastChild.next = None\n-\n-        self.parent = None\n-        if self.previousSibling:\n-            self.previousSibling.nextSibling = self.nextSibling\n-        if self.nextSibling:\n-            self.nextSibling.previousSibling = self.previousSibling\n-        self.previousSibling = self.nextSibling = None\n-        return self\n-\n-    def _lastRecursiveChild(self):\n-        \"Finds the last element beneath this object to be parsed.\"\n-        lastChild = self\n-        while hasattr(lastChild, 'contents') and lastChild.contents:\n-            lastChild = lastChild.contents[-1]\n-        return lastChild\n-\n-    def insert(self, position, newChild):\n-        if (isinstance(newChild, basestring)\n-            or isinstance(newChild, unicode)) \\\n-            and not isinstance(newChild, NavigableString):\n-            newChild = NavigableString(newChild)\n-\n-        position =  min(position, len(self.contents))\n-        if hasattr(newChild, 'parent') and newChild.parent != None:\n-            # We're 'inserting' an element that's already one\n-            # of this object's children.\n-            if newChild.parent == self:\n-                index = self.find(newChild)\n-                if index and index < position:\n-                    # Furthermore we're moving it further down the\n-                    # list of this object's children. That means that\n-                    # when we extract this element, our target index\n-                    # will jump down one.\n-                    position = position - 1\n-            newChild.extract()\n-\n-        newChild.parent = self\n-        previousChild = None\n-        if position == 0:\n-            newChild.previousSibling = None\n-            newChild.previous = self\n-        else:\n-            previousChild = self.contents[position-1]\n-            newChild.previousSibling = previousChild\n-            newChild.previousSibling.nextSibling = newChild\n-            newChild.previous = previousChild._lastRecursiveChild()\n-        if newChild.previous:\n-            newChild.previous.next = newChild\n-\n-        newChildsLastElement = newChild._lastRecursiveChild()\n-\n-        if position >= len(self.contents):\n-            newChild.nextSibling = None\n-\n-            parent = self\n-            parentsNextSibling = None\n-            while not parentsNextSibling:\n-                parentsNextSibling = parent.nextSibling\n-                parent = parent.parent\n-                if not parent: # This is the last element in the document.\n-                    break\n-            if parentsNextSibling:\n-                newChildsLastElement.next = parentsNextSibling\n-            else:\n-                newChildsLastElement.next = None\n-        else:\n-            nextChild = self.contents[position]\n-            newChild.nextSibling = nextChild\n-            if newChild.nextSibling:\n-                newChild.nextSibling.previousSibling = newChild\n-            newChildsLastElement.next = nextChild\n-\n-        if newChildsLastElement.next:\n-            newChildsLastElement.next.previous = newChildsLastElement\n-        self.contents.insert(position, newChild)\n-\n-    def append(self, tag):\n-        \"\"\"Appends the given tag to the contents of this tag.\"\"\"\n-        self.insert(len(self.contents), tag)\n-\n-    def findNext(self, name=None, attrs={}, text=None, **kwargs):\n-        \"\"\"Returns the first item that matches the given criteria and\n-        appears after this Tag in the document.\"\"\"\n-        return self._findOne(self.findAllNext, name, attrs, text, **kwargs)\n-\n-    def findAllNext(self, name=None, attrs={}, text=None, limit=None,\n-                    **kwargs):\n-        \"\"\"Returns all items that match the given criteria and appear\n-        after this Tag in the document.\"\"\"\n-        return self._findAll(name, attrs, text, limit, self.nextGenerator,\n-                             **kwargs)\n-\n-    def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n-        \"\"\"Returns the closest sibling to this Tag that matches the\n-        given criteria and appears after this Tag in the document.\"\"\"\n-        return self._findOne(self.findNextSiblings, name, attrs, text,\n-                             **kwargs)\n-\n-    def findNextSiblings(self, name=None, attrs={}, text=None, limit=None,\n-                         **kwargs):\n-        \"\"\"Returns the siblings of this Tag that match the given\n-        criteria and appear after this Tag in the document.\"\"\"\n-        return self._findAll(name, attrs, text, limit,\n-                             self.nextSiblingGenerator, **kwargs)\n-    fetchNextSiblings = findNextSiblings # Compatibility with pre-3.x\n-\n-    def findPrevious(self, name=None, attrs={}, text=None, **kwargs):\n-        \"\"\"Returns the first item that matches the given criteria and\n-        appears before this Tag in the document.\"\"\"\n-        return self._findOne(self.findAllPrevious, name, attrs, text, **kwargs)\n-\n-    def findAllPrevious(self, name=None, attrs={}, text=None, limit=None,\n-                        **kwargs):\n-        \"\"\"Returns all items that match the given criteria and appear\n-        before this Tag in the document.\"\"\"\n-        return self._findAll(name, attrs, text, limit, self.previousGenerator,\n-                           **kwargs)\n-    fetchPrevious = findAllPrevious # Compatibility with pre-3.x\n-\n-    def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n-        \"\"\"Returns the closest sibling to this Tag that matches the\n-        given criteria and appears before this Tag in the document.\"\"\"\n-        return self._findOne(self.findPreviousSiblings, name, attrs, text,\n-                             **kwargs)\n-\n-    def findPreviousSiblings(self, name=None, attrs={}, text=None,\n-                             limit=None, **kwargs):\n-        \"\"\"Returns the siblings of this Tag that match the given\n-        criteria and appear before this Tag in the document.\"\"\"\n-        return self._findAll(name, attrs, text, limit,\n-                             self.previousSiblingGenerator, **kwargs)\n-    fetchPreviousSiblings = findPreviousSiblings # Compatibility with pre-3.x\n-\n-    def findParent(self, name=None, attrs={}, **kwargs):\n-        \"\"\"Returns the closest parent of this Tag that matches the given\n-        criteria.\"\"\"\n-        # NOTE: We can't use _findOne because findParents takes a different\n-        # set of arguments.\n-        r = None\n-        l = self.findParents(name, attrs, 1)\n-        if l:\n-            r = l[0]\n-        return r\n-\n-    def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n-        \"\"\"Returns the parents of this Tag that match the given\n-        criteria.\"\"\"\n-\n-        return self._findAll(name, attrs, None, limit, self.parentGenerator,\n-                             **kwargs)\n-    fetchParents = findParents # Compatibility with pre-3.x\n-\n-    #These methods do the real heavy lifting.\n-\n-    def _findOne(self, method, name, attrs, text, **kwargs):\n-        r = None\n-        l = method(name, attrs, text, 1, **kwargs)\n-        if l:\n-            r = l[0]\n-        return r\n-\n-    def _findAll(self, name, attrs, text, limit, generator, **kwargs):\n-        \"Iterates over a generator looking for things that match.\"\n-\n-        if isinstance(name, SoupStrainer):\n-            strainer = name\n-        else:\n-            # Build a SoupStrainer\n-            strainer = SoupStrainer(name, attrs, text, **kwargs)\n-        results = ResultSet(strainer)\n-        g = generator()\n-        while True:\n-            try:\n-                i = g.next()\n-            except StopIteration:\n-                break\n-            if i:\n-                found = strainer.search(i)\n-                if found:\n-                    results.append(found)\n-                    if limit and len(results) >= limit:\n-                        break\n-        return results\n-\n-    #These Generators can be used to navigate starting from both\n-    #NavigableStrings and Tags.\n-    def nextGenerator(self):\n-        i = self\n-        while i:\n-            i = i.next\n-            yield i\n-\n-    def nextSiblingGenerator(self):\n-        i = self\n-        while i:\n-            i = i.nextSibling\n-            yield i\n-\n-    def previousGenerator(self):\n-        i = self\n-        while i:\n-            i = i.previous\n-            yield i\n-\n-    def previousSiblingGenerator(self):\n-        i = self\n-        while i:\n-            i = i.previousSibling\n-            yield i\n-\n-    def parentGenerator(self):\n-        i = self\n-        while i:\n-            i = i.parent\n-            yield i\n-\n-    # Utility methods\n-    def substituteEncoding(self, str, encoding=None):\n-        encoding = encoding or \"utf-8\"\n-        return str.replace(\"%SOUP-ENCODING%\", encoding)\n-\n-    def toEncoding(self, s, encoding=None):\n-        \"\"\"Encodes an object to a string in some encoding, or to Unicode.\n-        .\"\"\"\n-        if isinstance(s, unicode):\n-            if encoding:\n-                s = s.encode(encoding)\n-        elif isinstance(s, str):\n-            if encoding:\n-                s = s.encode(encoding)\n-            else:\n-                s = unicode(s)\n-        else:\n-            if encoding:\n-                s  = self.toEncoding(str(s), encoding)\n-            else:\n-                s = unicode(s)\n-        return s\n-\n-class NavigableString(unicode, PageElement):\n-\n-    def __getnewargs__(self):\n-        return (NavigableString.__str__(self),)\n-\n-    def __getattr__(self, attr):\n-        \"\"\"text.string gives you text. This is for backwards\n-        compatibility for Navigable*String, but for CData* it lets you\n-        get the string without the CData wrapper.\"\"\"\n-        if attr == 'string':\n-            return self\n-        else:\n-            raise AttributeError, \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, attr)\n-\n-    def __unicode__(self):\n-        return str(self).decode(DEFAULT_OUTPUT_ENCODING)\n-\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        if encoding:\n-            return self.encode(encoding)\n-        else:\n-            return self\n-\n-class CData(NavigableString):\n-\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        return \"<![CDATA[%s]]>\" % NavigableString.__str__(self, encoding)\n-\n-class ProcessingInstruction(NavigableString):\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        output = self\n-        if \"%SOUP-ENCODING%\" in output:\n-            output = self.substituteEncoding(output, encoding)\n-        return \"<?%s?>\" % self.toEncoding(output, encoding)\n-\n-class Comment(NavigableString):\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        return \"<!--%s-->\" % NavigableString.__str__(self, encoding)\n-\n-class Declaration(NavigableString):\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        return \"<!%s>\" % NavigableString.__str__(self, encoding)\n-\n-class Tag(PageElement):\n-\n-    \"\"\"Represents a found HTML tag with its attributes and contents.\"\"\"\n-\n-    def _invert(h):\n-        \"Cheap function to invert a hash.\"\n-        i = {}\n-        for k,v in h.items():\n-            i[v] = k\n-        return i\n-\n-    XML_ENTITIES_TO_SPECIAL_CHARS = { \"apos\" : \"'\",\n-                                      \"quot\" : '\"',\n-                                      \"amp\" : \"&\",\n-                                      \"lt\" : \"<\",\n-                                      \"gt\" : \">\" }\n-\n-    XML_SPECIAL_CHARS_TO_ENTITIES = _invert(XML_ENTITIES_TO_SPECIAL_CHARS)\n-\n-    def _convertEntities(self, match):\n-        \"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\n-        entities with the appropriate Unicode characters. If HTML\n-        entities are being converted, any unrecognized entities are\n-        escaped.\"\"\"\n-        x = match.group(1)\n-        if self.convertHTMLEntities and x in name2codepoint:\n-            return unichr(name2codepoint[x])\n-        elif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n-            if self.convertXMLEntities:\n-                return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n-            else:\n-                return u'&%s;' % x\n-        elif len(x) > 0 and x[0] == '#':\n-            # Handle numeric entities\n-            if len(x) > 1 and x[1] == 'x':\n-                return unichr(int(x[2:], 16))\n-            else:\n-                return unichr(int(x[1:]))\n-\n-        elif self.escapeUnrecognizedEntities:\n-            return u'&amp;%s;' % x\n-        else:\n-            return u'&%s;' % x\n-\n-    def __init__(self, parser, name, attrs=None, parent=None,\n-                 previous=None):\n-        \"Basic constructor.\"\n-\n-        # We don't actually store the parser object: that lets extracted\n-        # chunks be garbage-collected\n-        self.parserClass = parser.__class__\n-        self.isSelfClosing = parser.isSelfClosingTag(name)\n-        self.name = name\n-        if attrs == None:\n-            attrs = []\n-        self.attrs = attrs\n-        self.contents = []\n-        self.setup(parent, previous)\n-        self.hidden = False\n-        self.containsSubstitutions = False\n-        self.convertHTMLEntities = parser.convertHTMLEntities\n-        self.convertXMLEntities = parser.convertXMLEntities\n-        self.escapeUnrecognizedEntities = parser.escapeUnrecognizedEntities\n-\n-        # Convert any HTML, XML, or numeric entities in the attribute values.\n-        convert = lambda(k, val): (k,\n-                                   re.sub(\"&(#\\d+|#x[0-9a-fA-F]+|\\w+);\",\n-                                          self._convertEntities,\n-                                          val))\n-        self.attrs = map(convert, self.attrs)\n-\n-    def get(self, key, default=None):\n-        \"\"\"Returns the value of the 'key' attribute for the tag, or\n-        the value given for 'default' if it doesn't have that\n-        attribute.\"\"\"\n-        return self._getAttrMap().get(key, default)\n-\n-    def has_key(self, key):\n-        return self._getAttrMap().has_key(key)\n-\n-    def __getitem__(self, key):\n-        \"\"\"tag[key] returns the value of the 'key' attribute for the tag,\n-        and throws an exception if it's not there.\"\"\"\n-        return self._getAttrMap()[key]\n-\n-    def __iter__(self):\n-        \"Iterating over a tag iterates over its contents.\"\n-        return iter(self.contents)\n-\n-    def __len__(self):\n-        \"The length of a tag is the length of its list of contents.\"\n-        return len(self.contents)\n-\n-    def __contains__(self, x):\n-        return x in self.contents\n-\n-    def __nonzero__(self):\n-        \"A tag is non-None even if it has no contents.\"\n-        return True\n-\n-    def __setitem__(self, key, value):\n-        \"\"\"Setting tag[key] sets the value of the 'key' attribute for the\n-        tag.\"\"\"\n-        self._getAttrMap()\n-        self.attrMap[key] = value\n-        found = False\n-        for i in range(0, len(self.attrs)):\n-            if self.attrs[i][0] == key:\n-                self.attrs[i] = (key, value)\n-                found = True\n-        if not found:\n-            self.attrs.append((key, value))\n-        self._getAttrMap()[key] = value\n-\n-    def __delitem__(self, key):\n-        \"Deleting tag[key] deletes all 'key' attributes for the tag.\"\n-        for item in self.attrs:\n-            if item[0] == key:\n-                self.attrs.remove(item)\n-                #We don't break because bad HTML can define the same\n-                #attribute multiple times.\n-            self._getAttrMap()\n-            if self.attrMap.has_key(key):\n-                del self.attrMap[key]\n-\n-    def __call__(self, *args, **kwargs):\n-        \"\"\"Calling a tag like a function is the same as calling its\n-        findAll() method. Eg. tag('a') returns a list of all the A tags\n-        found within this tag.\"\"\"\n-        return apply(self.findAll, args, kwargs)\n-\n-    def __getattr__(self, tag):\n-        #print \"Getattr %s.%s\" % (self.__class__, tag)\n-        if len(tag) > 3 and tag.rfind('Tag') == len(tag)-3:\n-            return self.find(tag[:-3])\n-        elif tag.find('__') != 0:\n-            return self.find(tag)\n-        raise AttributeError, \"'%s' object has no attribute '%s'\" % (self.__class__, tag)\n-\n-    def __eq__(self, other):\n-        \"\"\"Returns true iff this tag has the same name, the same attributes,\n-        and the same contents (recursively) as the given tag.\n-\n-        NOTE: right now this will return false if two tags have the\n-        same attributes in a different order. Should this be fixed?\"\"\"\n-        if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n-            return False\n-        for i in range(0, len(self.contents)):\n-            if self.contents[i] != other.contents[i]:\n-                return False\n-        return True\n-\n-    def __ne__(self, other):\n-        \"\"\"Returns true iff this tag is not identical to the other tag,\n-        as defined in __eq__.\"\"\"\n-        return not self == other\n-\n-    def __repr__(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        \"\"\"Renders this tag as a string.\"\"\"\n-        return self.__str__(encoding)\n-\n-    def __unicode__(self):\n-        return self.__str__(None)\n-\n-    BARE_AMPERSAND_OR_BRACKET = re.compile(\"([<>]|\"\n-                                           + \"&(?!#\\d+;|#x[0-9a-fA-F]+;|\\w+;)\"\n-                                           + \")\")\n-\n-    def _sub_entity(self, x):\n-        \"\"\"Used with a regular expression to substitute the\n-        appropriate XML entity for an XML special character.\"\"\"\n-        return \"&\" + self.XML_SPECIAL_CHARS_TO_ENTITIES[x.group(0)[0]] + \";\"\n-\n-    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING,\n-                prettyPrint=False, indentLevel=0):\n-        \"\"\"Returns a string or Unicode representation of this tag and\n-        its contents. To get Unicode, pass None for encoding.\n-\n-        NOTE: since Python's HTML parser consumes whitespace, this\n-        method is not certain to reproduce the whitespace present in\n-        the original string.\"\"\"\n-\n-        encodedName = self.toEncoding(self.name, encoding)\n-\n-        attrs = []\n-        if self.attrs:\n-            for key, val in self.attrs:\n-                fmt = '%s=\"%s\"'\n-                if isString(val):\n-                    if self.containsSubstitutions and '%SOUP-ENCODING%' in val:\n-                        val = self.substituteEncoding(val, encoding)\n-\n-                    # The attribute value either:\n-                    #\n-                    # * Contains no embedded double quotes or single quotes.\n-                    #   No problem: we enclose it in double quotes.\n-                    # * Contains embedded single quotes. No problem:\n-                    #   double quotes work here too.\n-                    # * Contains embedded double quotes. No problem:\n-                    #   we enclose it in single quotes.\n-                    # * Embeds both single _and_ double quotes. This\n-                    #   can't happen naturally, but it can happen if\n-                    #   you modify an attribute value after parsing\n-                    #   the document. Now we have a bit of a\n-                    #   problem. We solve it by enclosing the\n-                    #   attribute in single quotes, and escaping any\n-                    #   embedded single quotes to XML entities.\n-                    if '\"' in val:\n-                        fmt = \"%s='%s'\"\n-                        if \"'\" in val:\n-                            # TODO: replace with apos when\n-                            # appropriate.\n-                            val = val.replace(\"'\", \"&squot;\")\n-\n-                    # Now we're okay w/r/t quotes. But the attribute\n-                    # value might also contain angle brackets, or\n-                    # ampersands that aren't part of entities. We need\n-                    # to escape those to XML entities too.\n-                    val = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, val)\n-\n-                attrs.append(fmt % (self.toEncoding(key, encoding),\n-                                    self.toEncoding(val, encoding)))\n-        close = ''\n-        closeTag = ''\n-        if self.isSelfClosing:\n-            close = ' /'\n-        else:\n-            closeTag = '</%s>' % encodedName\n-\n-        indentTag, indentContents = 0, 0\n-        if prettyPrint:\n-            indentTag = indentLevel\n-            space = (' ' * (indentTag-1))\n-            indentContents = indentTag + 1\n-        contents = self.renderContents(encoding, prettyPrint, indentContents)\n-        if self.hidden:\n-            s = contents\n-        else:\n-            s = []\n-            attributeString = ''\n-            if attrs:\n-                attributeString = ' ' + ' '.join(attrs)\n-            if prettyPrint:\n-                s.append(space)\n-            s.append('<%s%s%s>' % (encodedName, attributeString, close))\n-            if prettyPrint:\n-                s.append(\"\\n\")\n-            s.append(contents)\n-            if prettyPrint and contents and contents[-1] != \"\\n\":\n-                s.append(\"\\n\")\n-            if prettyPrint and closeTag:\n-                s.append(space)\n-            s.append(closeTag)\n-            if prettyPrint and closeTag and self.nextSibling:\n-                s.append(\"\\n\")\n-            s = ''.join(s)\n-        return s\n-\n-    def decompose(self):\n-        \"\"\"Recursively destroys the contents of this tree.\"\"\"\n-        contents = [i for i in self.contents]\n-        for i in contents:\n-            if isinstance(i, Tag):\n-                i.decompose()\n-            else:\n-                i.extract()\n-        self.extract()\n-\n-    def prettify(self, encoding=DEFAULT_OUTPUT_ENCODING):\n-        return self.__str__(encoding, True)\n-\n-    def renderContents(self, encoding=DEFAULT_OUTPUT_ENCODING,\n-                       prettyPrint=False, indentLevel=0):\n-        \"\"\"Renders the contents of this tag as a string in the given\n-        encoding. If encoding is None, returns a Unicode string..\"\"\"\n-        s=[]\n-        for c in self:\n-            text = None\n-            if isinstance(c, NavigableString):\n-                text = c.__str__(encoding)\n-            elif isinstance(c, Tag):\n-                s.append(c.__str__(encoding, prettyPrint, indentLevel))\n-            if text and prettyPrint:\n-                text = text.strip()\n-            if text:\n-                if prettyPrint:\n-                    s.append(\" \" * (indentLevel-1))\n-                s.append(text)\n-                if prettyPrint:\n-                    s.append(\"\\n\")\n-        return ''.join(s)\n-\n-    #Soup methods\n-\n-    def find(self, name=None, attrs={}, recursive=True, text=None,\n-             **kwargs):\n-        \"\"\"Return only the first child of this Tag matching the given\n-        criteria.\"\"\"\n-        r = None\n-        l = self.findAll(name, attrs, recursive, text, 1, **kwargs)\n-        if l:\n-            r = l[0]\n-        return r\n-    findChild = find\n-\n-    def findAll(self, name=None, attrs={}, recursive=True, text=None,\n-                limit=None, **kwargs):\n-        \"\"\"Extracts a list of Tag objects that match the given\n-        criteria.  You can specify the name of the Tag and any\n-        attributes you want the Tag to have.\n-\n-        The value of a key-value pair in the 'attrs' map can be a\n-        string, a list of strings, a regular expression object, or a\n-        callable that takes a string and returns whether or not the\n-        string matches for some custom definition of 'matches'. The\n-        same is true of the tag name.\"\"\"\n-        generator = self.recursiveChildGenerator\n-        if not recursive:\n-            generator = self.childGenerator\n-        return self._findAll(name, attrs, text, limit, generator, **kwargs)\n-    findChildren = findAll\n-\n-    # Pre-3.x compatibility methods\n-    first = find\n-    fetch = findAll\n-\n-    def fetchText(self, text=None, recursive=True, limit=None):\n-        return self.findAll(text=text, recursive=recursive, limit=limit)\n-\n-    def firstText(self, text=None, recursive=True):\n-        return self.find(text=text, recursive=recursive)\n-\n-    #Private methods\n-\n-    def _getAttrMap(self):\n-        \"\"\"Initializes a map representation of this tag's attributes,\n-        if not already initialized.\"\"\"\n-        if not getattr(self, 'attrMap'):\n-            self.attrMap = {}\n-            for (key, value) in self.attrs:\n-                self.attrMap[key] = value\n-        return self.attrMap\n-\n-    #Generator methods\n-    def childGenerator(self):\n-        for i in range(0, len(self.contents)):\n-            yield self.contents[i]\n-        raise StopIteration\n-\n-    def recursiveChildGenerator(self):\n-        stack = [(self, 0)]\n-        while stack:\n-            tag, start = stack.pop()\n-            if isinstance(tag, Tag):\n-                for i in range(start, len(tag.contents)):\n-                    a = tag.contents[i]\n-                    yield a\n-                    if isinstance(a, Tag) and tag.contents:\n-                        if i < len(tag.contents) - 1:\n-                            stack.append((tag, i+1))\n-                        stack.append((a, 0))\n-                        break\n-        raise StopIteration\n-\n-# Next, a couple classes to represent queries and their results.\n-class SoupStrainer:\n-    \"\"\"Encapsulates a number of ways of matching a markup element (tag or\n-    text).\"\"\"\n-\n-    def __init__(self, name=None, attrs={}, text=None, **kwargs):\n-        self.name = name\n-        if isString(attrs):\n-            kwargs['class'] = attrs\n-            attrs = None\n-        if kwargs:\n-            if attrs:\n-                attrs = attrs.copy()\n-                attrs.update(kwargs)\n-            else:\n-                attrs = kwargs\n-        self.attrs = attrs\n-        self.text = text\n-\n-    def __str__(self):\n-        if self.text:\n-            return self.text\n-        else:\n-            return \"%s|%s\" % (self.name, self.attrs)\n-\n-    def searchTag(self, markupName=None, markupAttrs={}):\n-        found = None\n-        markup = None\n-        if isinstance(markupName, Tag):\n-            markup = markupName\n-            markupAttrs = markup\n-        callFunctionWithTagData = callable(self.name) \\\n-                                and not isinstance(markupName, Tag)\n-\n-        if (not self.name) \\\n-               or callFunctionWithTagData \\\n-               or (markup and self._matches(markup, self.name)) \\\n-               or (not markup and self._matches(markupName, self.name)):\n-            if callFunctionWithTagData:\n-                match = self.name(markupName, markupAttrs)\n-            else:\n-                match = True\n-                markupAttrMap = None\n-                for attr, matchAgainst in self.attrs.items():\n-                    if not markupAttrMap:\n-                         if hasattr(markupAttrs, 'get'):\n-                            markupAttrMap = markupAttrs\n-                         else:\n-                            markupAttrMap = {}\n-                            for k,v in markupAttrs:\n-                                markupAttrMap[k] = v\n-                    attrValue = markupAttrMap.get(attr)\n-                    if not self._matches(attrValue, matchAgainst):\n-                        match = False\n-                        break\n-            if match:\n-                if markup:\n-                    found = markup\n-                else:\n-                    found = markupName\n-        return found\n-\n-    def search(self, markup):\n-        #print 'looking for %s in %s' % (self, markup)\n-        found = None\n-        # If given a list of items, scan it for a text element that\n-        # matches.\n-        if isList(markup) and not isinstance(markup, Tag):\n-            for element in markup:\n-                if isinstance(element, NavigableString) \\\n-                       and self.search(element):\n-                    found = element\n-                    break\n-        # If it's a Tag, make sure its name or attributes match.\n-        # Don't bother with Tags if we're searching for text.\n-        elif isinstance(markup, Tag):\n-            if not self.text:\n-                found = self.searchTag(markup)\n-        # If it's text, make sure the text matches.\n-        elif isinstance(markup, NavigableString) or \\\n-                 isString(markup):\n-            if self._matches(markup, self.text):\n-                found = markup\n-        else:\n-            raise Exception, \"I don't know how to match against a %s\" \\\n-                  % markup.__class__\n-        return found\n-\n-    def _matches(self, markup, matchAgainst):\n-        #print \"Matching %s against %s\" % (markup, matchAgainst)\n-        result = False\n-        if matchAgainst == True and type(matchAgainst) == types.BooleanType:\n-            result = markup != None\n-        elif callable(matchAgainst):\n-            result = matchAgainst(markup)\n-        else:\n-            #Custom match methods take the tag as an argument, but all\n-            #other ways of matching match the tag name as a string.\n-            if isinstance(markup, Tag):\n-                markup = markup.name\n-            if markup and not isString(markup):\n-                markup = unicode(markup)\n-            #Now we know that chunk is either a string, or None.\n-            if hasattr(matchAgainst, 'match'):\n-                # It's a regexp object.\n-                result = markup and matchAgainst.search(markup)\n-            elif isList(matchAgainst):\n-                result = markup in matchAgainst\n-            elif hasattr(matchAgainst, 'items'):\n-                result = markup.has_key(matchAgainst)\n-            elif matchAgainst and isString(markup):\n-                if isinstance(markup, unicode):\n-                    matchAgainst = unicode(matchAgainst)\n-                else:\n-                    matchAgainst = str(matchAgainst)\n-\n-            if not result:\n-                result = matchAgainst == markup\n-        return result\n-\n-class ResultSet(list):\n-    \"\"\"A ResultSet is just a list that keeps track of the SoupStrainer\n-    that created it.\"\"\"\n-    def __init__(self, source):\n-        list.__init__([])\n-        self.source = source\n-\n-# Now, some helper functions.\n-\n-def isList(l):\n-    \"\"\"Convenience method that works with all 2.x versions of Python\n-    to determine whether or not something is listlike.\"\"\"\n-    return hasattr(l, '__iter__') \\\n-           or (type(l) in (types.ListType, types.TupleType))\n-\n-def isString(s):\n-    \"\"\"Convenience method that works with all 2.x versions of Python\n-    to determine whether or not something is stringlike.\"\"\"\n-    try:\n-        return isinstance(s, unicode) or isinstance(s, basestring)\n-    except NameError:\n-        return isinstance(s, str)\n-\n-def buildTagMap(default, *args):\n-    \"\"\"Turns a list of maps, lists, or scalars into a single map.\n-    Used to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\n-    NESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n-    built = {}\n-    for portion in args:\n-        if hasattr(portion, 'items'):\n-            #It's a map. Merge it.\n-            for k,v in portion.items():\n-                built[k] = v\n-        elif isList(portion):\n-            #It's a list. Map each item to the default.\n-            for k in portion:\n-                built[k] = default\n-        else:\n-            #It's a scalar. Map it to the default.\n-            built[portion] = default\n-    return built\n-\n-# Now, the parser classes.\n-\n-class BeautifulStoneSoup(Tag, SGMLParser):\n-\n-    \"\"\"This class contains the basic parser and search code. It defines\n-    a parser that knows nothing about tag behavior except for the\n-    following:\n-\n-      You can't close a tag without closing all the tags it encloses.\n-      That is, \"<foo><bar></foo>\" actually means\n-      \"<foo><bar></bar></foo>\".\n-\n-    [Another possible explanation is \"<foo><bar /></foo>\", but since\n-    this class defines no SELF_CLOSING_TAGS, it will never use that\n-    explanation.]\n-\n-    This class is useful for parsing XML or made-up markup languages,\n-    or when BeautifulSoup makes an assumption counter to what you were\n-    expecting.\"\"\"\n-\n-    SELF_CLOSING_TAGS = {}\n-    NESTABLE_TAGS = {}\n-    RESET_NESTING_TAGS = {}\n-    QUOTE_TAGS = {}\n-\n-    MARKUP_MASSAGE = [(re.compile('(<[^<>]*)/>'),\n-                       lambda x: x.group(1) + ' />'),\n-                      (re.compile('<!\\s+([^<>]*)>'),\n-                       lambda x: '<!' + x.group(1) + '>')\n-                      ]\n-\n-    ROOT_TAG_NAME = u'[document]'\n-\n-    HTML_ENTITIES = \"html\"\n-    XML_ENTITIES = \"xml\"\n-    XHTML_ENTITIES = \"xhtml\"\n-    # TODO: This only exists for backwards-compatibility\n-    ALL_ENTITIES = XHTML_ENTITIES\n-\n-    # Used when determining whether a text node is all whitespace and\n-    # can be replaced with a single space. A text node that contains\n-    # fancy Unicode spaces (usually non-breaking) should be left\n-    # alone.\n-    STRIP_ASCII_SPACES = { 9: None, 10: None, 12: None, 13: None, 32: None, }\n-\n-    def __init__(self, markup=\"\", parseOnlyThese=None, fromEncoding=None,\n-                 markupMassage=True, smartQuotesTo=XML_ENTITIES,\n-                 convertEntities=None, selfClosingTags=None):\n-        \"\"\"The Soup object is initialized as the 'root tag', and the\n-        provided markup (which can be a string or a file-like object)\n-        is fed into the underlying parser.\n-\n-        sgmllib will process most bad HTML, and the BeautifulSoup\n-        class has some tricks for dealing with some HTML that kills\n-        sgmllib, but Beautiful Soup can nonetheless choke or lose data\n-        if your data uses self-closing tags or declarations\n-        incorrectly.\n-\n-        By default, Beautiful Soup uses regexes to sanitize input,\n-        avoiding the vast majority of these problems. If the problems\n-        don't apply to you, pass in False for markupMassage, and\n-        you'll get better performance.\n-\n-        The default parser massage techniques fix the two most common\n-        instances of invalid HTML that choke sgmllib:\n-\n-         <br/> (No space between name of closing tag and tag close)\n-         <! --Comment--> (Extraneous whitespace in declaration)\n-\n-        You can pass in a custom list of (RE object, replace method)\n-        tuples to get Beautiful Soup to scrub your input the way you\n-        want.\"\"\"\n-\n-        self.parseOnlyThese = parseOnlyThese\n-        self.fromEncoding = fromEncoding\n-        self.smartQuotesTo = smartQuotesTo\n-        self.convertEntities = convertEntities\n-        # Set the rules for how we'll deal with the entities we\n-        # encounter\n-        if self.convertEntities:\n-            # It doesn't make sense to convert encoded characters to\n-            # entities even while you're converting entities to Unicode.\n-            # Just convert it all to Unicode.\n-            self.smartQuotesTo = None\n-            if convertEntities == self.HTML_ENTITIES:\n-                self.convertXMLEntities = False\n-                self.convertHTMLEntities = True\n-                self.escapeUnrecognizedEntities = True\n-            elif convertEntities == self.XHTML_ENTITIES:\n-                self.convertXMLEntities = True\n-                self.convertHTMLEntities = True\n-                self.escapeUnrecognizedEntities = False\n-            elif convertEntities == self.XML_ENTITIES:\n-                self.convertXMLEntities = True\n-                self.convertHTMLEntities = False\n-                self.escapeUnrecognizedEntities = False\n-        else:\n-            self.convertXMLEntities = False\n-            self.convertHTMLEntities = False\n-            self.escapeUnrecognizedEntities = False\n-\n-        self.instanceSelfClosingTags = buildTagMap(None, selfClosingTags)\n-        SGMLParser.__init__(self)\n-\n-        if hasattr(markup, 'read'):        # It's a file-type object.\n-            markup = markup.read()\n-        self.markup = markup\n-        self.markupMassage = markupMassage\n-        try:\n-            self._feed()\n-        except StopParsing:\n-            pass\n-        self.markup = None                 # The markup can now be GCed\n-\n-    def convert_charref(self, name):\n-        \"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n-        try:\n-            n = int(name)\n-        except ValueError:\n-            return\n-        if not 0 <= n <= 127 : # ASCII ends at 127, not 255\n-            return\n-        return self.convert_codepoint(n)\n-\n-    def _feed(self, inDocumentEncoding=None):\n-        # Convert the document to Unicode.\n-        markup = self.markup\n-        if isinstance(markup, unicode):\n-            if not hasattr(self, 'originalEncoding'):\n-                self.originalEncoding = None\n-        else:\n-            dammit = UnicodeDammit\\\n-                     (markup, [self.fromEncoding, inDocumentEncoding],\n-                      smartQuotesTo=self.smartQuotesTo)\n-            markup = dammit.unicode\n-            self.originalEncoding = dammit.originalEncoding\n-        if markup:\n-            if self.markupMassage:\n-                if not isList(self.markupMassage):\n-                    self.markupMassage = self.MARKUP_MASSAGE\n-                for fix, m in self.markupMassage:\n-                    markup = fix.sub(m, markup)\n-                # TODO: We get rid of markupMassage so that the\n-                # soup object can be deepcopied later on. Some\n-                # Python installations can't copy regexes. If anyone\n-                # was relying on the existence of markupMassage, this\n-                # might cause problems.\n-                del(self.markupMassage)\n-        self.reset()\n-\n-        SGMLParser.feed(self, markup)\n-        # Close out any unfinished strings and close all the open tags.\n-        self.endData()\n-        while self.currentTag.name != self.ROOT_TAG_NAME:\n-            self.popTag()\n-\n-    def __getattr__(self, methodName):\n-        \"\"\"This method routes method call requests to either the SGMLParser\n-        superclass or the Tag superclass, depending on the method name.\"\"\"\n-        #print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n-\n-        if methodName.find('start_') == 0 or methodName.find('end_') == 0 \\\n-               or methodName.find('do_') == 0:\n-            return SGMLParser.__getattr__(self, methodName)\n-        elif methodName.find('__') != 0:\n-            return Tag.__getattr__(self, methodName)\n-        else:\n-            raise AttributeError\n-\n-    def isSelfClosingTag(self, name):\n-        \"\"\"Returns true iff the given string is the name of a\n-        self-closing tag according to this parser.\"\"\"\n-        return self.SELF_CLOSING_TAGS.has_key(name) \\\n-               or self.instanceSelfClosingTags.has_key(name)\n-\n-    def reset(self):\n-        Tag.__init__(self, self, self.ROOT_TAG_NAME)\n-        self.hidden = 1\n-        SGMLParser.reset(self)\n-        self.currentData = []\n-        self.currentTag = None\n-        self.tagStack = []\n-        self.quoteStack = []\n-        self.pushTag(self)\n-\n-    def popTag(self):\n-        tag = self.tagStack.pop()\n-        # Tags with just one string-owning child get the child as a\n-        # 'string' property, so that soup.tag.string is shorthand for\n-        # soup.tag.contents[0]\n-        if len(self.currentTag.contents) == 1 and \\\n-           isinstance(self.currentTag.contents[0], NavigableString):\n-            self.currentTag.string = self.currentTag.contents[0]\n-\n-        #print \"Pop\", tag.name\n-        if self.tagStack:\n-            self.currentTag = self.tagStack[-1]\n-        return self.currentTag\n-\n-    def pushTag(self, tag):\n-        #print \"Push\", tag.name\n-        if self.currentTag:\n-            self.currentTag.contents.append(tag)\n-        self.tagStack.append(tag)\n-        self.currentTag = self.tagStack[-1]\n-\n-    def endData(self, containerClass=NavigableString):\n-        if self.currentData:\n-            currentData = ''.join(self.currentData)\n-            if not currentData.translate(self.STRIP_ASCII_SPACES):\n-                if '\\n' in currentData:\n-                    currentData = '\\n'\n-                else:\n-                    currentData = ' '\n-            self.currentData = []\n-            if self.parseOnlyThese and len(self.tagStack) <= 1 and \\\n-                   (not self.parseOnlyThese.text or \\\n-                    not self.parseOnlyThese.search(currentData)):\n-                return\n-            o = containerClass(currentData)\n-            o.setup(self.currentTag, self.previous)\n-            if self.previous:\n-                self.previous.next = o\n-            self.previous = o\n-            self.currentTag.contents.append(o)\n-\n-\n-    def _popToTag(self, name, inclusivePop=True):\n-        \"\"\"Pops the tag stack up to and including the most recent\n-        instance of the given tag. If inclusivePop is false, pops the tag\n-        stack up to but *not* including the most recent instqance of\n-        the given tag.\"\"\"\n-        #print \"Popping to %s\" % name\n-        if name == self.ROOT_TAG_NAME:\n-            return\n-\n-        numPops = 0\n-        mostRecentTag = None\n-        for i in range(len(self.tagStack)-1, 0, -1):\n-            if name == self.tagStack[i].name:\n-                numPops = len(self.tagStack)-i\n-                break\n-        if not inclusivePop:\n-            numPops = numPops - 1\n-\n-        for i in range(0, numPops):\n-            mostRecentTag = self.popTag()\n-        return mostRecentTag\n-\n-    def _smartPop(self, name):\n-\n-        \"\"\"We need to pop up to the previous tag of this type, unless\n-        one of this tag's nesting reset triggers comes between this\n-        tag and the previous tag of this type, OR unless this tag is a\n-        generic nesting trigger and another generic nesting trigger\n-        comes between this tag and the previous tag of this type.\n-\n-        Examples:\n-         <p>Foo<b>Bar *<p>* should pop to 'p', not 'b'.\n-         <p>Foo<table>Bar *<p>* should pop to 'table', not 'p'.\n-         <p>Foo<table><tr>Bar *<p>* should pop to 'tr', not 'p'.\n-\n-         <li><ul><li> *<li>* should pop to 'ul', not the first 'li'.\n-         <tr><table><tr> *<tr>* should pop to 'table', not the first 'tr'\n-         <td><tr><td> *<td>* should pop to 'tr', not the first 'td'\n-        \"\"\"\n-\n-        nestingResetTriggers = self.NESTABLE_TAGS.get(name)\n-        isNestable = nestingResetTriggers != None\n-        isResetNesting = self.RESET_NESTING_TAGS.has_key(name)\n-        popTo = None\n-        inclusive = True\n-        for i in range(len(self.tagStack)-1, 0, -1):\n-            p = self.tagStack[i]\n-            if (not p or p.name == name) and not isNestable:\n-                #Non-nestable tags get popped to the top or to their\n-                #last occurance.\n-                popTo = name\n-                break\n-            if (nestingResetTriggers != None\n-                and p.name in nestingResetTriggers) \\\n-                or (nestingResetTriggers == None and isResetNesting\n-                    and self.RESET_NESTING_TAGS.has_key(p.name)):\n-\n-                #If we encounter one of the nesting reset triggers\n-                #peculiar to this tag, or we encounter another tag\n-                #that causes nesting to reset, pop up to but not\n-                #including that tag.\n-                popTo = p.name\n-                inclusive = False\n-                break\n-            p = p.parent\n-        if popTo:\n-            self._popToTag(popTo, inclusive)\n-\n-    def unknown_starttag(self, name, attrs, selfClosing=0):\n-        #print \"Start tag %s: %s\" % (name, attrs)\n-        if self.quoteStack:\n-            #This is not a real tag.\n-            #print \"<%s> is not real!\" % name\n-            attrs = ''.join(map(lambda(x, y): ' %s=\"%s\"' % (x, y), attrs))\n-            self.handle_data('<%s%s>' % (name, attrs))\n-            return\n-        self.endData()\n-\n-        if not self.isSelfClosingTag(name) and not selfClosing:\n-            self._smartPop(name)\n-\n-        if self.parseOnlyThese and len(self.tagStack) <= 1 \\\n-               and (self.parseOnlyThese.text or not self.parseOnlyThese.searchTag(name, attrs)):\n-            return\n-\n-        tag = Tag(self, name, attrs, self.currentTag, self.previous)\n-        if self.previous:\n-            self.previous.next = tag\n-        self.previous = tag\n-        self.pushTag(tag)\n-        if selfClosing or self.isSelfClosingTag(name):\n-            self.popTag()\n-        if name in self.QUOTE_TAGS:\n-            #print \"Beginning quote (%s)\" % name\n-            self.quoteStack.append(name)\n-            self.literal = 1\n-        return tag\n-\n-    def unknown_endtag(self, name):\n-        #print \"End tag %s\" % name\n-        if self.quoteStack and self.quoteStack[-1] != name:\n-            #This is not a real end tag.\n-            #print \"</%s> is not real!\" % name\n-            self.handle_data('</%s>' % name)\n-            return\n-        self.endData()\n-        self._popToTag(name)\n-        if self.quoteStack and self.quoteStack[-1] == name:\n-            self.quoteStack.pop()\n-            self.literal = (len(self.quoteStack) > 0)\n-\n-    def handle_data(self, data):\n-        self.currentData.append(data)\n-\n-    def _toStringSubclass(self, text, subclass):\n-        \"\"\"Adds a certain piece of text to the tree as a NavigableString\n-        subclass.\"\"\"\n-        self.endData()\n-        self.handle_data(text)\n-        self.endData(subclass)\n-\n-    def handle_pi(self, text):\n-        \"\"\"Handle a processing instruction as a ProcessingInstruction\n-        object, possibly one with a %SOUP-ENCODING% slot into which an\n-        encoding will be plugged later.\"\"\"\n-        if text[:3] == \"xml\":\n-            text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\n-        self._toStringSubclass(text, ProcessingInstruction)\n-\n-    def handle_comment(self, text):\n-        \"Handle comments as Comment objects.\"\n-        self._toStringSubclass(text, Comment)\n-\n-    def handle_charref(self, ref):\n-        \"Handle character references as data.\"\n-        if self.convertEntities:\n-            data = unichr(int(ref))\n-        else:\n-            data = '&#%s;' % ref\n-        self.handle_data(data)\n-\n-    def handle_entityref(self, ref):\n-        \"\"\"Handle entity references as data, possibly converting known\n-        HTML and/or XML entity references to the corresponding Unicode\n-        characters.\"\"\"\n-        data = None\n-        if self.convertHTMLEntities:\n-            try:\n-                data = unichr(name2codepoint[ref])\n-            except KeyError:\n-                pass\n-\n-        if not data and self.convertXMLEntities:\n-                data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n-\n-        if not data and self.convertHTMLEntities and \\\n-            not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n-                # TODO: We've got a problem here. We're told this is\n-                # an entity reference, but it's not an XML entity\n-                # reference or an HTML entity reference. Nonetheless,\n-                # the logical thing to do is to pass it through as an\n-                # unrecognized entity reference.\n-                #\n-                # Except: when the input is \"&carol;\" this function\n-                # will be called with input \"carol\". When the input is\n-                # \"AT&T\", this function will be called with input\n-                # \"T\". We have no way of knowing whether a semicolon\n-                # was present originally, so we don't know whether\n-                # this is an unknown entity or just a misplaced\n-                # ampersand.\n-                #\n-                # The more common case is a misplaced ampersand, so I\n-                # escape the ampersand and omit the trailing semicolon.\n-                data = \"&amp;%s\" % ref\n-        if not data:\n-            # This case is different from the one above, because we\n-            # haven't already gone through a supposedly comprehensive\n-            # mapping of entities to Unicode characters. We might not\n-            # have gone through any mapping at all. So the chances are\n-            # very high that this is a real entity, and not a\n-            # misplaced ampersand.\n-            data = \"&%s;\" % ref\n-        self.handle_data(data)\n-\n-    def handle_decl(self, data):\n-        \"Handle DOCTYPEs and the like as Declaration objects.\"\n-        self._toStringSubclass(data, Declaration)\n-\n-    def parse_declaration(self, i):\n-        \"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\n-        declaration as a CData object.\"\"\"\n-        j = None\n-        if self.rawdata[i:i+9] == '<![CDATA[':\n-             k = self.rawdata.find(']]>', i)\n-             if k == -1:\n-                 k = len(self.rawdata)\n-             data = self.rawdata[i+9:k]\n-             j = k+3\n-             self._toStringSubclass(data, CData)\n-        else:\n-            try:\n-                j = SGMLParser.parse_declaration(self, i)\n-            except SGMLParseError:\n-                toHandle = self.rawdata[i:]\n-                self.handle_data(toHandle)\n-                j = i + len(toHandle)\n-        return j\n-\n-class BeautifulSoup(BeautifulStoneSoup):\n-\n-    \"\"\"This parser knows the following facts about HTML:\n-\n-    * Some tags have no closing tag and should be interpreted as being\n-      closed as soon as they are encountered.\n-\n-    * The text inside some tags (ie. 'script') may contain tags which\n-      are not really part of the document and which should be parsed\n-      as text, not tags. If you want to parse the text as tags, you can\n-      always fetch it and parse it explicitly.\n-\n-    * Tag nesting rules:\n-\n-      Most tags can't be nested at all. For instance, the occurance of\n-      a <p> tag should implicitly close the previous <p> tag.\n-\n-       <p>Para1<p>Para2\n-        should be transformed into:\n-       <p>Para1</p><p>Para2\n-\n-      Some tags can be nested arbitrarily. For instance, the occurance\n-      of a <blockquote> tag should _not_ implicitly close the previous\n-      <blockquote> tag.\n-\n-       Alice said: <blockquote>Bob said: <blockquote>Blah\n-        should NOT be transformed into:\n-       Alice said: <blockquote>Bob said: </blockquote><blockquote>Blah\n-\n-      Some tags can be nested, but the nesting is reset by the\n-      interposition of other tags. For instance, a <tr> tag should\n-      implicitly close the previous <tr> tag within the same <table>,\n-      but not close a <tr> tag in another table.\n-\n-       <table><tr>Blah<tr>Blah\n-        should be transformed into:\n-       <table><tr>Blah</tr><tr>Blah\n-        but,\n-       <tr>Blah<table><tr>Blah\n-        should NOT be transformed into\n-       <tr>Blah<table></tr><tr>Blah\n-\n-    Differing assumptions about tag nesting rules are a major source\n-    of problems with the BeautifulSoup class. If BeautifulSoup is not\n-    treating as nestable a tag your page author treats as nestable,\n-    try ICantBelieveItsBeautifulSoup, MinimalSoup, or\n-    BeautifulStoneSoup before writing your own subclass.\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        if not kwargs.has_key('smartQuotesTo'):\n-            kwargs['smartQuotesTo'] = self.HTML_ENTITIES\n-        BeautifulStoneSoup.__init__(self, *args, **kwargs)\n-\n-    SELF_CLOSING_TAGS = buildTagMap(None,\n-                                    ['br' , 'hr', 'input', 'img', 'meta',\n-                                    'spacer', 'link', 'frame', 'base'])\n-\n-    QUOTE_TAGS = {'script' : None, 'textarea' : None}\n-\n-    #According to the HTML standard, each of these inline tags can\n-    #contain another tag of the same type. Furthermore, it's common\n-    #to actually use these tags this way.\n-    NESTABLE_INLINE_TAGS = ['span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',\n-                            'center']\n-\n-    #According to the HTML standard, these block tags can contain\n-    #another tag of the same type. Furthermore, it's common\n-    #to actually use these tags this way.\n-    NESTABLE_BLOCK_TAGS = ['blockquote', 'div', 'fieldset', 'ins', 'del']\n-\n-    #Lists can contain other lists, but there are restrictions.\n-    NESTABLE_LIST_TAGS = { 'ol' : [],\n-                           'ul' : [],\n-                           'li' : ['ul', 'ol'],\n-                           'dl' : [],\n-                           'dd' : ['dl'],\n-                           'dt' : ['dl'] }\n-\n-    #Tables can contain other tables, but there are restrictions.\n-    NESTABLE_TABLE_TAGS = {'table' : [],\n-                           'tr' : ['table', 'tbody', 'tfoot', 'thead'],\n-                           'td' : ['tr'],\n-                           'th' : ['tr'],\n-                           'thead' : ['table'],\n-                           'tbody' : ['table'],\n-                           'tfoot' : ['table'],\n-                           }\n-\n-    NON_NESTABLE_BLOCK_TAGS = ['address', 'form', 'p', 'pre']\n-\n-    #If one of these tags is encountered, all tags up to the next tag of\n-    #this type are popped.\n-    RESET_NESTING_TAGS = buildTagMap(None, NESTABLE_BLOCK_TAGS, 'noscript',\n-                                     NON_NESTABLE_BLOCK_TAGS,\n-                                     NESTABLE_LIST_TAGS,\n-                                     NESTABLE_TABLE_TAGS)\n-\n-    NESTABLE_TAGS = buildTagMap([], NESTABLE_INLINE_TAGS, NESTABLE_BLOCK_TAGS,\n-                                NESTABLE_LIST_TAGS, NESTABLE_TABLE_TAGS)\n-\n-    # Used to detect the charset in a META tag; see start_meta\n-    CHARSET_RE = re.compile(\"((^|;)\\s*charset=)([^;]*)\")\n-\n-    def start_meta(self, attrs):\n-        \"\"\"Beautiful Soup can detect a charset included in a META tag,\n-        try to convert the document to that charset, and re-parse the\n-        document from the beginning.\"\"\"\n-        httpEquiv = None\n-        contentType = None\n-        contentTypeIndex = None\n-        tagNeedsEncodingSubstitution = False\n-\n-        for i in range(0, len(attrs)):\n-            key, value = attrs[i]\n-            key = key.lower()\n-            if key == 'http-equiv':\n-                httpEquiv = value\n-            elif key == 'content':\n-                contentType = value\n-                contentTypeIndex = i\n-\n-        if httpEquiv and contentType: # It's an interesting meta tag.\n-            match = self.CHARSET_RE.search(contentType)\n-            if match:\n-                if getattr(self, 'declaredHTMLEncoding') or \\\n-                       (self.originalEncoding == self.fromEncoding):\n-                    # This is our second pass through the document, or\n-                    # else an encoding was specified explicitly and it\n-                    # worked. Rewrite the meta tag.\n-                    newAttr = self.CHARSET_RE.sub\\\n-                              (lambda(match):match.group(1) +\n-                               \"%SOUP-ENCODING%\", contentType)\n-                    attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n-                                               newAttr)\n-                    tagNeedsEncodingSubstitution = True\n-                else:\n-                    # This is our first pass through the document.\n-                    # Go through it again with the new information.\n-                    newCharset = match.group(3)\n-                    if newCharset and newCharset != self.originalEncoding:\n-                        self.declaredHTMLEncoding = newCharset\n-                        self._feed(self.declaredHTMLEncoding)\n-                        raise StopParsing\n-        tag = self.unknown_starttag(\"meta\", attrs)\n-        if tag and tagNeedsEncodingSubstitution:\n-            tag.containsSubstitutions = True\n-\n-class StopParsing(Exception):\n-    pass\n-\n-class ICantBelieveItsBeautifulSoup(BeautifulSoup):\n-\n-    \"\"\"The BeautifulSoup class is oriented towards skipping over\n-    common HTML errors like unclosed tags. However, sometimes it makes\n-    errors of its own. For instance, consider this fragment:\n-\n-     <b>Foo<b>Bar</b></b>\n-\n-    This is perfectly valid (if bizarre) HTML. However, the\n-    BeautifulSoup class will implicitly close the first b tag when it\n-    encounters the second 'b'. It will think the author wrote\n-    \"<b>Foo<b>Bar\", and didn't close the first 'b' tag, because\n-    there's no real-world reason to bold something that's already\n-    bold. When it encounters '</b></b>' it will close two more 'b'\n-    tags, for a grand total of three tags closed instead of two. This\n-    can throw off the rest of your document structure. The same is\n-    true of a number of other tags, listed below.\n-\n-    It's much more common for someone to forget to close a 'b' tag\n-    than to actually use nested 'b' tags, and the BeautifulSoup class\n-    handles the common case. This class handles the not-co-common\n-    case: where you can't believe someone wrote what they did, but\n-    it's valid HTML and BeautifulSoup screwed up by assuming it\n-    wouldn't be.\"\"\"\n-\n-    I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS = \\\n-     ['em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',\n-      'cite', 'code', 'dfn', 'kbd', 'samp', 'strong', 'var', 'b',\n-      'big']\n-\n-    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ['noscript']\n-\n-    NESTABLE_TAGS = buildTagMap([], BeautifulSoup.NESTABLE_TAGS,\n-                                I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS,\n-                                I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS)\n-\n-class MinimalSoup(BeautifulSoup):\n-    \"\"\"The MinimalSoup class is for parsing HTML that contains\n-    pathologically bad markup. It makes no assumptions about tag\n-    nesting, but it does know which tags are self-closing, that\n-    <script> tags contain Javascript and should not be parsed, that\n-    META tags may contain encoding information, and so on.\n-\n-    This also makes it better for subclassing than BeautifulStoneSoup\n-    or BeautifulSoup.\"\"\"\n-\n-    RESET_NESTING_TAGS = buildTagMap('noscript')\n-    NESTABLE_TAGS = {}\n-\n-class BeautifulSOAP(BeautifulStoneSoup):\n-    \"\"\"This class will push a tag with only a single string child into\n-    the tag's parent as an attribute. The attribute's name is the tag\n-    name, and the value is the string child. An example should give\n-    the flavor of the change:\n-\n-    <foo><bar>baz</bar></foo>\n-     =>\n-    <foo bar=\"baz\"><bar>baz</bar></foo>\n-\n-    You can then access fooTag['bar'] instead of fooTag.barTag.string.\n-\n-    This is, of course, useful for scraping structures that tend to\n-    use subelements instead of attributes, such as SOAP messages. Note\n-    that it modifies its input, so don't print the modified version\n-    out.\n-\n-    I'm not sure how many people really want to use this class; let me\n-    know if you do. Mainly I like the name.\"\"\"\n-\n-    def popTag(self):\n-        if len(self.tagStack) > 1:\n-            tag = self.tagStack[-1]\n-            parent = self.tagStack[-2]\n-            parent._getAttrMap()\n-            if (isinstance(tag, Tag) and len(tag.contents) == 1 and\n-                isinstance(tag.contents[0], NavigableString) and\n-                not parent.attrMap.has_key(tag.name)):\n-                parent[tag.name] = tag.contents[0]\n-        BeautifulStoneSoup.popTag(self)\n-\n-#Enterprise class names! It has come to our attention that some people\n-#think the names of the Beautiful Soup parser classes are too silly\n-#and \"unprofessional\" for use in enterprise screen-scraping. We feel\n-#your pain! For such-minded folk, the Beautiful Soup Consortium And\n-#All-Night Kosher Bakery recommends renaming this file to\n-#\"RobustParser.py\" (or, in cases of extreme enterprisiness,\n-#\"RobustParserBeanInterface.class\") and using the following\n-#enterprise-friendly class aliases:\n-class RobustXMLParser(BeautifulStoneSoup):\n-    pass\n-class RobustHTMLParser(BeautifulSoup):\n-    pass\n-class RobustWackAssHTMLParser(ICantBelieveItsBeautifulSoup):\n-    pass\n-class RobustInsanelyWackAssHTMLParser(MinimalSoup):\n-    pass\n-class SimplifyingSOAPParser(BeautifulSOAP):\n-    pass\n-\n-######################################################\n-#\n-# Bonus library: Unicode, Dammit\n-#\n-# This class forces XML data into a standard format (usually to UTF-8\n-# or Unicode).  It is heavily based on code from Mark Pilgrim's\n-# Universal Feed Parser. It does not rewrite the XML or HTML to\n-# reflect a new encoding: that happens in BeautifulStoneSoup.handle_pi\n-# (XML) and BeautifulSoup.start_meta (HTML).\n-\n-# Autodetects character encodings.\n-# Download from http://chardet.feedparser.org/\n-try:\n-    import chardet\n-#    import chardet.constants\n-#    chardet.constants._debug = 1\n-except ImportError:\n-    chardet = None\n-\n-# cjkcodecs and iconv_codec make Python know about more character encodings.\n-# Both are available from http://cjkpython.i18n.org/\n-# They're built in if you use Python 2.4.\n-try:\n-    import cjkcodecs.aliases\n-except ImportError:\n-    pass\n-try:\n-    import iconv_codec\n-except ImportError:\n-    pass\n-\n-class UnicodeDammit:\n-    \"\"\"A class for detecting the encoding of a *ML document and\n-    converting it to a Unicode string. If the source encoding is\n-    windows-1252, can replace MS smart quotes with their HTML or XML\n-    equivalents.\"\"\"\n-\n-    # This dictionary maps commonly seen values for \"charset\" in HTML\n-    # meta tags to the corresponding Python codec names. It only covers\n-    # values that aren't in Python's aliases and can't be determined\n-    # by the heuristics in find_codec.\n-    CHARSET_ALIASES = { \"macintosh\" : \"mac-roman\",\n-                        \"x-sjis\" : \"shift-jis\" }\n-\n-    def __init__(self, markup, overrideEncodings=[],\n-                 smartQuotesTo='xml'):\n-        self.markup, documentEncoding, sniffedEncoding = \\\n-                     self._detectEncoding(markup)\n-        self.smartQuotesTo = smartQuotesTo\n-        self.triedEncodings = []\n-        if markup == '' or isinstance(markup, unicode):\n-            self.originalEncoding = None\n-            self.unicode = unicode(markup)\n-            return\n-\n-        u = None\n-        for proposedEncoding in overrideEncodings:\n-            u = self._convertFrom(proposedEncoding)\n-            if u: break\n-        if not u:\n-            for proposedEncoding in (documentEncoding, sniffedEncoding):\n-                u = self._convertFrom(proposedEncoding)\n-                if u: break\n-\n-        # If no luck and we have auto-detection library, try that:\n-        if not u and chardet and not isinstance(self.markup, unicode):\n-            u = self._convertFrom(chardet.detect(self.markup)['encoding'])\n-\n-        # As a last resort, try utf-8 and windows-1252:\n-        if not u:\n-            for proposed_encoding in (\"utf-8\", \"windows-1252\"):\n-                u = self._convertFrom(proposed_encoding)\n-                if u: break\n-        self.unicode = u\n-        if not u: self.originalEncoding = None\n-\n-    def _subMSChar(self, orig):\n-        \"\"\"Changes a MS smart quote character to an XML or HTML\n-        entity.\"\"\"\n-        sub = self.MS_CHARS.get(orig)\n-        if type(sub) == types.TupleType:\n-            if self.smartQuotesTo == 'xml':\n-                sub = '&#x%s;' % sub[1]\n-            else:\n-                sub = '&%s;' % sub[0]\n-        return sub\n-\n-    def _convertFrom(self, proposed):\n-        proposed = self.find_codec(proposed)\n-        if not proposed or proposed in self.triedEncodings:\n-            return None\n-        self.triedEncodings.append(proposed)\n-        markup = self.markup\n-\n-        # Convert smart quotes to HTML if coming from an encoding\n-        # that might have them.\n-        if self.smartQuotesTo and proposed.lower() in(\"windows-1252\",\n-                                                      \"iso-8859-1\",\n-                                                      \"iso-8859-2\"):\n-            markup = re.compile(\"([\\x80-\\x9f])\").sub \\\n-                     (lambda(x): self._subMSChar(x.group(1)),\n-                      markup)\n-\n-        try:\n-            # print \"Trying to convert document to %s\" % proposed\n-            u = self._toUnicode(markup, proposed)\n-            self.markup = u\n-            self.originalEncoding = proposed\n-        except Exception, e:\n-            # print \"That didn't work!\"\n-            # print e\n-            return None\n-        #print \"Correct encoding: %s\" % proposed\n-        return self.markup\n-\n-    def _toUnicode(self, data, encoding):\n-        '''Given a string and its encoding, decodes the string into Unicode.\n-        %encoding is a string recognized by encodings.aliases'''\n-\n-        # strip Byte Order Mark (if present)\n-        if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') \\\n-               and (data[2:4] != '\\x00\\x00'):\n-            encoding = 'utf-16be'\n-            data = data[2:]\n-        elif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') \\\n-                 and (data[2:4] != '\\x00\\x00'):\n-            encoding = 'utf-16le'\n-            data = data[2:]\n-        elif data[:3] == '\\xef\\xbb\\xbf':\n-            encoding = 'utf-8'\n-            data = data[3:]\n-        elif data[:4] == '\\x00\\x00\\xfe\\xff':\n-            encoding = 'utf-32be'\n-            data = data[4:]\n-        elif data[:4] == '\\xff\\xfe\\x00\\x00':\n-            encoding = 'utf-32le'\n-            data = data[4:]\n-        newdata = unicode(data, encoding)\n-        return newdata\n-\n-    def _detectEncoding(self, xml_data):\n-        \"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n-        xml_encoding = sniffed_xml_encoding = None\n-        try:\n-            if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n-                # EBCDIC\n-                xml_data = self._ebcdic_to_ascii(xml_data)\n-            elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n-                # UTF-16BE\n-                sniffed_xml_encoding = 'utf-16be'\n-                xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n-            elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n-                     and (xml_data[2:4] != '\\x00\\x00'):\n-                # UTF-16BE with BOM\n-                sniffed_xml_encoding = 'utf-16be'\n-                xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n-            elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n-                # UTF-16LE\n-                sniffed_xml_encoding = 'utf-16le'\n-                xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n-            elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n-                     (xml_data[2:4] != '\\x00\\x00'):\n-                # UTF-16LE with BOM\n-                sniffed_xml_encoding = 'utf-16le'\n-                xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n-            elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n-                # UTF-32BE\n-                sniffed_xml_encoding = 'utf-32be'\n-                xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n-            elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n-                # UTF-32LE\n-                sniffed_xml_encoding = 'utf-32le'\n-                xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n-            elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n-                # UTF-32BE with BOM\n-                sniffed_xml_encoding = 'utf-32be'\n-                xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n-            elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n-                # UTF-32LE with BOM\n-                sniffed_xml_encoding = 'utf-32le'\n-                xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n-            elif xml_data[:3] == '\\xef\\xbb\\xbf':\n-                # UTF-8 with BOM\n-                sniffed_xml_encoding = 'utf-8'\n-                xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n-            else:\n-                sniffed_xml_encoding = 'ascii'\n-                pass\n-            xml_encoding_match = re.compile \\\n-                                 ('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>')\\\n-                                 .match(xml_data)\n-        except:\n-            xml_encoding_match = None\n-        if xml_encoding_match:\n-            xml_encoding = xml_encoding_match.groups()[0].lower()\n-            if sniffed_xml_encoding and \\\n-               (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n-                                 'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n-                                 'utf-16', 'utf-32', 'utf_16', 'utf_32',\n-                                 'utf16', 'u16')):\n-                xml_encoding = sniffed_xml_encoding\n-        return xml_data, xml_encoding, sniffed_xml_encoding\n-\n-\n-    def find_codec(self, charset):\n-        return self._codec(self.CHARSET_ALIASES.get(charset, charset)) \\\n-               or (charset and self._codec(charset.replace(\"-\", \"\"))) \\\n-               or (charset and self._codec(charset.replace(\"-\", \"_\"))) \\\n-               or charset\n-\n-    def _codec(self, charset):\n-        if not charset: return charset\n-        codec = None\n-        try:\n-            codecs.lookup(charset)\n-            codec = charset\n-        except (LookupError, ValueError):\n-            pass\n-        return codec\n-\n-    EBCDIC_TO_ASCII_MAP = None\n-    def _ebcdic_to_ascii(self, s):\n-        c = self.__class__\n-        if not c.EBCDIC_TO_ASCII_MAP:\n-            emap = (0,1,2,3,156,9,134,127,151,141,142,11,12,13,14,15,\n-                    16,17,18,19,157,133,8,135,24,25,146,143,28,29,30,31,\n-                    128,129,130,131,132,10,23,27,136,137,138,139,140,5,6,7,\n-                    144,145,22,147,148,149,150,4,152,153,154,155,20,21,158,26,\n-                    32,160,161,162,163,164,165,166,167,168,91,46,60,40,43,33,\n-                    38,169,170,171,172,173,174,175,176,177,93,36,42,41,59,94,\n-                    45,47,178,179,180,181,182,183,184,185,124,44,37,95,62,63,\n-                    186,187,188,189,190,191,192,193,194,96,58,35,64,39,61,34,\n-                    195,97,98,99,100,101,102,103,104,105,196,197,198,199,200,\n-                    201,202,106,107,108,109,110,111,112,113,114,203,204,205,\n-                    206,207,208,209,126,115,116,117,118,119,120,121,122,210,\n-                    211,212,213,214,215,216,217,218,219,220,221,222,223,224,\n-                    225,226,227,228,229,230,231,123,65,66,67,68,69,70,71,72,\n-                    73,232,233,234,235,236,237,125,74,75,76,77,78,79,80,81,\n-                    82,238,239,240,241,242,243,92,159,83,84,85,86,87,88,89,\n-                    90,244,245,246,247,248,249,48,49,50,51,52,53,54,55,56,57,\n-                    250,251,252,253,254,255)\n-            import string\n-            c.EBCDIC_TO_ASCII_MAP = string.maketrans( \\\n-            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))\n-        return s.translate(c.EBCDIC_TO_ASCII_MAP)\n-\n-    MS_CHARS = { '\\x80' : ('euro', '20AC'),\n-                 '\\x81' : ' ',\n-                 '\\x82' : ('sbquo', '201A'),\n-                 '\\x83' : ('fnof', '192'),\n-                 '\\x84' : ('bdquo', '201E'),\n-                 '\\x85' : ('hellip', '2026'),\n-                 '\\x86' : ('dagger', '2020'),\n-                 '\\x87' : ('Dagger', '2021'),\n-                 '\\x88' : ('circ', '2C6'),\n-                 '\\x89' : ('permil', '2030'),\n-                 '\\x8A' : ('Scaron', '160'),\n-                 '\\x8B' : ('lsaquo', '2039'),\n-                 '\\x8C' : ('OElig', '152'),\n-                 '\\x8D' : '?',\n-                 '\\x8E' : ('#x17D', '17D'),\n-                 '\\x8F' : '?',\n-                 '\\x90' : '?',\n-                 '\\x91' : ('lsquo', '2018'),\n-                 '\\x92' : ('rsquo', '2019'),\n-                 '\\x93' : ('ldquo', '201C'),\n-                 '\\x94' : ('rdquo', '201D'),\n-                 '\\x95' : ('bull', '2022'),\n-                 '\\x96' : ('ndash', '2013'),\n-                 '\\x97' : ('mdash', '2014'),\n-                 '\\x98' : ('tilde', '2DC'),\n-                 '\\x99' : ('trade', '2122'),\n-                 '\\x9a' : ('scaron', '161'),\n-                 '\\x9b' : ('rsaquo', '203A'),\n-                 '\\x9c' : ('oelig', '153'),\n-                 '\\x9d' : '?',\n-                 '\\x9e' : ('#x17E', '17E'),\n-                 '\\x9f' : ('Yuml', ''),}\n-\n-#######################################################################\n-\n-\n-#By default, act as an HTML pretty-printer.\n-if __name__ == '__main__':\n-    import sys\n-    soup = BeautifulSoup(sys.stdin.read())\n-    print soup.prettify()\n\n@@ -1 +0,0 @@\n-\"\"\"This directory contains third party modules used by Scrapy\"\"\"\n\n@@ -1,213 +0,0 @@\n-# lrucache.py -- a simple LRU (Least-Recently-Used) cache class\n-\n-# Copyright 2004 Evan Prodromou <evan@bad.dynu.ca>\n-# Licensed under the Academic Free License 2.1\n-\n-# arch-tag: LRU cache main module\n-\n-\"\"\"a simple LRU (Least-Recently-Used) cache module\n-\n-This module provides very simple LRU (Least-Recently-Used) cache\n-functionality.\n-\n-An *in-memory cache* is useful for storing the results of an\n-'expensive' process (one that takes a lot of time or resources) for\n-later re-use. Typical examples are accessing data from the filesystem,\n-a database, or a network location. If you know you'll need to re-read\n-the data again, it can help to keep it in a cache.\n-\n-You *can* use a Python dictionary as a cache for some purposes.\n-However, if the results you're caching are large, or you have a lot of\n-possible results, this can be impractical memory-wise.\n-\n-An *LRU cache*, on the other hand, only keeps _some_ of the results in\n-memory, which keeps you from overusing resources. The cache is bounded\n-by a maximum size; if you try to add more values to the cache, it will\n-automatically discard the values that you haven't read or written to\n-in the longest time. In other words, the least-recently-used items are\n-discarded. [1]_\n-\n-.. [1]: 'Discarded' here means 'removed from the cache'.\n-\n-\"\"\"\n-\n-from __future__ import generators\n-import time\n-from heapq import heappush, heappop, heapify\n-\n-__version__ = \"0.2\"\n-__all__ = ['CacheKeyError', 'LRUCache', 'DEFAULT_SIZE']\n-__docformat__ = 'reStructuredText en'\n-\n-DEFAULT_SIZE = 16\n-\"\"\"Default size of a new LRUCache object, if no 'size' argument is given.\"\"\"\n-\n-class CacheKeyError(KeyError):\n-    \"\"\"Error raised when cache requests fail\n-    \n-    When a cache record is accessed which no longer exists (or never did),\n-    this error is raised. To avoid it, you may want to check for the existence\n-    of a cache record before reading or deleting it.\"\"\"\n-    pass\n-\n-class LRUCache(object):\n-    \"\"\"Least-Recently-Used (LRU) cache.\n-    \n-    Instances of this class provide a least-recently-used (LRU) cache. They\n-    emulate a Python mapping type. You can use an LRU cache more or less like\n-    a Python dictionary, with the exception that objects you put into the\n-    cache may be discarded before you take them out.\n-    \n-    Some example usage::\n-\t\n-    cache = LRUCache(32) # new cache\n-    cache['foo'] = get_file_contents('foo') # or whatever\n-\t\n-    if 'foo' in cache: # if it's still in cache...\n-\t    # use cached version\n-        contents = cache['foo']\n-    else:\n-\t    # recalculate\n-        contents = get_file_contents('foo')\n-\t    # store in cache for next time\n-        cache['foo'] = contents\n-\n-    print cache.size # Maximum size\n-\t\n-    print len(cache) # 0 <= len(cache) <= cache.size\n-\t\n-    cache.size = 10 # Auto-shrink on size assignment\n-\t\n-    for i in range(50): # note: larger than cache size\n-        cache[i] = i\n-\t    \n-    if 0 not in cache: print 'Zero was discarded.'\n-\n-    if 42 in cache:\n-        del cache[42] # Manual deletion\n-\n-    for j in cache:   # iterate (in LRU order)\n-        print j, cache[j] # iterator produces keys, not values\n-    \"\"\"\n-    \n-    class __Node(object):\n-        \"\"\"Record of a cached value. Not for public consumption.\"\"\"\n-        \n-        def __init__(self, key, obj, timestamp):\n-            object.__init__(self)\n-            self.key = key\n-            self.obj = obj\n-            self.atime = timestamp\n-            self.mtime = self.atime\n-\t    \n-        def __cmp__(self, other):\n-            return cmp(self.atime, other.atime)\n-\n-        def __repr__(self):\n-            return \"<%s %s => %s (%s)>\" % \\\n-                   (self.__class__, self.key, self.obj, \\\n-                    time.asctime(time.localtime(self.atime)))\n-\n-    def __init__(self, size=DEFAULT_SIZE):\n-        # Check arguments\n-        if size <= 0:\n-            raise ValueError, size\n-        elif type(size) is not type(0):\n-            raise TypeError, size\n-        object.__init__(self)\t\n-        self.__heap = []\n-        self.__dict = {}\n-        self.size = size\n-        \"\"\"Maximum size of the cache.\n-        If more than 'size' elements are added to the cache,\n-        the least-recently-used ones will be discarded.\"\"\"\n-\t\n-    def __len__(self):\n-        return len(self.__heap)\n-    \n-    def __contains__(self, key):\n-        return self.__dict.has_key(key)\n-    \n-    def __setitem__(self, key, obj):\n-        if self.__dict.has_key(key):\n-            node = self.__dict[key]\n-            node.obj = obj\n-            node.atime = time.time()\n-            node.mtime = node.atime\n-            heapify(self.__heap)\n-        else:\n-            # size may have been reset, so we loop\n-            while len(self.__heap) >= self.size:\n-                lru = heappop(self.__heap)\n-                del self.__dict[lru.key]\n-            node = self.__Node(key, obj, time.time())\n-            self.__dict[key] = node\n-            heappush(self.__heap, node)\n-\t\n-    def __getitem__(self, key):\n-        if not self.__dict.has_key(key):\n-            raise CacheKeyError(key)\n-        else:\n-            node = self.__dict[key]\n-            node.atime = time.time()\n-            heapify(self.__heap)\n-            return node.obj\n-\t\n-    def __delitem__(self, key):\n-        if not self.__dict.has_key(key):\n-            raise CacheKeyError(key)\n-        else:\n-            node = self.__dict[key]\n-            del self.__dict[key]\n-            self.__heap.remove(node)\n-            heapify(self.__heap)\n-            return node.obj\n-\n-    def __iter__(self):\n-        copy = self.__heap[:]\n-        while len(copy) > 0:\n-            node = heappop(copy)\n-            yield node.key\n-        raise StopIteration\n-\n-    def __setattr__(self, name, value):\n-        object.__setattr__(self, name, value)\n-        # automagically shrink heap on resize\n-        if name == 'size':\n-            while len(self.__heap) > value:\n-                lru = heappop(self.__heap)\n-                del self.__dict[lru.key]\n-\t    \n-    def __repr__(self):\n-        return \"<%s (%d elements)>\" % (str(self.__class__), len(self.__heap))\n-\n-    def mtime(self, key):\n-        \"\"\"Return the last modification time for the cache record with key.\n-        May be useful for cache instances where the stored values can get\n-        'stale', such as caching file or network resource contents.\"\"\"\n-        if not self.__dict.has_key(key):\n-            raise CacheKeyError(key)\n-        else:\n-            node = self.__dict[key]\n-            return node.mtime\n-\n-if __name__ == \"__main__\":\n-    cache = LRUCache(25)\n-    print cache\n-    for i in range(50):\n-        cache[i] = str(i)\n-    print cache\n-    if 46 in cache:\n-        del cache[46]\n-    print cache\n-    cache.size = 10\n-    print cache\n-    cache[46] = '46'\n-    print cache\n-    print len(cache)\n-    for c in cache:\n-        print c\n-    print cache\n-    print cache.mtime(46)\n-    for c in cache:\n-        print c\n\n@@ -1,119 +0,0 @@\n-# lsprofcalltree.py: lsprof output which is readable by kcachegrind\n-# David Allouche\n-# Jp Calderone & Itamar Shtull-Trauring\n-# Johan Dahlin\n-\n-import optparse\n-import os\n-import sys\n-\n-try:\n-    import cProfile\n-except ImportError:\n-    raise SystemExit(\"This script requires cProfile from Python 2.5\")\n-\n-def label(code):\n-    if isinstance(code, str):\n-        return ('~', 0, code)    # built-in functions ('~' sorts at the end)\n-    else:\n-        return '%s %s:%d' % (code.co_name,\n-                             code.co_filename,\n-                             code.co_firstlineno)\n-\n-class KCacheGrind(object):\n-    def __init__(self, profiler):\n-        self.data = profiler.getstats()\n-        self.out_file = None\n-\n-    def output(self, out_file):\n-        self.out_file = out_file\n-        print >> out_file, 'events: Ticks'\n-        self._print_summary()\n-        for entry in self.data:\n-            self._entry(entry)\n-\n-    def _print_summary(self):\n-        max_cost = 0\n-        for entry in self.data:\n-            totaltime = int(entry.totaltime * 1000)\n-            max_cost = max(max_cost, totaltime)\n-        print >> self.out_file, 'summary: %d' % (max_cost,)\n-\n-    def _entry(self, entry):\n-        out_file = self.out_file\n-\n-        code = entry.code\n-        #print >> out_file, 'ob=%s' % (code.co_filename,)\n-        if isinstance(code, str):\n-            print >> out_file, 'fi=~'\n-        else:\n-            print >> out_file, 'fi=%s' % (code.co_filename,)\n-        print >> out_file, 'fn=%s' % (label(code),)\n-\n-        inlinetime = int(entry.inlinetime * 1000)\n-        if isinstance(code, str):\n-            print >> out_file, '0 ', inlinetime\n-        else:\n-            print >> out_file, '%d %d' % (code.co_firstlineno, inlinetime)\n-\n-        # recursive calls are counted in entry.calls\n-        if entry.calls:\n-            calls = entry.calls\n-        else:\n-            calls = []\n-\n-        if isinstance(code, str):\n-            lineno = 0\n-        else:\n-            lineno = code.co_firstlineno\n-\n-        for subentry in calls:\n-            self._subentry(lineno, subentry)\n-        print >> out_file\n-\n-    def _subentry(self, lineno, subentry):\n-        out_file = self.out_file\n-        code = subentry.code\n-        #print >> out_file, 'cob=%s' % (code.co_filename,)\n-        print >> out_file, 'cfn=%s' % (label(code),)\n-        if isinstance(code, str):\n-            print >> out_file, 'cfi=~'\n-            print >> out_file, 'calls=%d 0' % (subentry.callcount,)\n-        else:\n-            print >> out_file, 'cfi=%s' % (code.co_filename,)\n-            print >> out_file, 'calls=%d %d' % (\n-                subentry.callcount, code.co_firstlineno)\n-\n-        totaltime = int(subentry.totaltime * 1000)\n-        print >> out_file, '%d %d' % (lineno, totaltime)\n-\n-def main(args):\n-    usage = \"%s [-o output_file_path] scriptfile [arg] ...\"\n-    parser = optparse.OptionParser(usage=usage % sys.argv[0])\n-    parser.allow_interspersed_args = False\n-    parser.add_option('-o', '--outfile', dest=\"outfile\",\n-                      help=\"Save stats to <outfile>\", default=None)\n-\n-    if not sys.argv[1:]:\n-        parser.print_usage()\n-        sys.exit(2)\n-\n-    options, args = parser.parse_args()\n-\n-    if not options.outfile:\n-        options.outfile = '%s.log' % os.path.basename(args[0])\n-\n-    sys.argv[:] = args\n-\n-    prof = cProfile.Profile()\n-    try:\n-        try:\n-            prof = prof.run('execfile(%r)' % (sys.argv[0],))\n-        except SystemExit:\n-            pass\n-    finally:\n-        kg = KCacheGrind(prof)\n-        kg.output(file(options.outfile, 'w'))\n-\n-if __name__ == '__main__':\n-    sys.exit(main(sys.argv))\n\n@@ -1,6 +0,0 @@\n-\"\"\"Multi-consumer multi-producer dispatching mechanism\n-\"\"\"\n-__version__ = \"2.0.0\"\n-__author__ = \"Patrick K. O'Brien\"\n-__license__ = \"BSD-style, see license.txt for details\"\n-\n\n@@ -1,497 +0,0 @@\n-\"\"\"Multiple-producer-multiple-consumer signal-dispatching\n-\n-dispatcher is the core of the PyDispatcher system,\n-providing the primary API and the core logic for the\n-system.\n-\n-Module attributes of note:\n-\n-\tAny -- Singleton used to signal either \"Any Sender\" or\n-\t\t\"Any Signal\".  See documentation of the _Any class.\n-\tAnonymous -- Singleton used to signal \"Anonymous Sender\"\n-\t\tSee documentation of the _Anonymous class.\n-\n-Internal attributes:\n-\tWEAKREF_TYPES -- tuple of types/classes which represent\n-\t\tweak references to receivers, and thus must be de-\n-\t\treferenced on retrieval to retrieve the callable\n-\t\tobject\n-\tconnections -- { senderkey (id) : { signal : [receivers...]}}\n-\tsenders -- { senderkey (id) : weakref(sender) }\n-\t\tused for cleaning up sender references on sender\n-\t\tdeletion\n-\tsendersBack -- { receiverkey (id) : [senderkey (id)...] }\n-\t\tused for cleaning up receiver references on receiver\n-\t\tdeletion, (considerably speeds up the cleanup process\n-\t\tvs. the original code.)\n-\"\"\"\n-from __future__ import generators\n-import types, weakref\n-from pydispatch import saferef, robustapply, errors\n-\n-__author__ = \"Patrick K. O'Brien <pobrien@orbtech.com>\"\n-__cvsid__ = \"$Id$\"\n-__version__ = \"$Revision: 1.1.1.1 $\"[11:-2]\n-\n-try:\n-\tTrue\n-except NameError:\n-\tTrue = 1==1\n-\tFalse = 1==0\n-\n-class _Parameter:\n-\t\"\"\"Used to represent default parameter values.\"\"\"\n-\tdef __repr__(self):\n-\t\treturn self.__class__.__name__\n-\n-class _Any(_Parameter):\n-\t\"\"\"Singleton used to signal either \"Any Sender\" or \"Any Signal\"\n-\n-\tThe Any object can be used with connect, disconnect,\n-\tsend, or sendExact to signal that the parameter given\n-\tAny should react to all senders/signals, not just\n-\ta particular sender/signal.\n-\t\"\"\"\n-Any = _Any()\n-\n-class _Anonymous(_Parameter):\n-\t\"\"\"Singleton used to signal \"Anonymous Sender\"\n-\n-\tThe Anonymous object is used to signal that the sender\n-\tof a message is not specified (as distinct from being\n-\t\"any sender\").  Registering callbacks for Anonymous\n-\twill only receive messages sent without senders.  Sending\n-\twith anonymous will only send messages to those receivers\n-\tregistered for Any or Anonymous.\n-\n-\tNote:\n-\t\tThe default sender for connect is Any, while the\n-\t\tdefault sender for send is Anonymous.  This has\n-\t\tthe effect that if you do not specify any senders\n-\t\tin either function then all messages are routed\n-\t\tas though there was a single sender (Anonymous)\n-\t\tbeing used everywhere.\n-\t\"\"\"\n-Anonymous = _Anonymous()\n-\n-WEAKREF_TYPES = (weakref.ReferenceType, saferef.BoundMethodWeakref)\n-\n-connections = {}\n-senders = {}\n-sendersBack = {}\n-\n-\n-def connect(receiver, signal=Any, sender=Any, weak=True):\n-\t\"\"\"Connect receiver to sender for signal\n-\n-\treceiver -- a callable Python object which is to receive\n-\t\tmessages/signals/events.  Receivers must be hashable\n-\t\tobjects.\n-\n-\t\tif weak is True, then receiver must be weak-referencable\n-\t\t(more precisely saferef.safeRef() must be able to create\n-\t\ta reference to the receiver).\n-\t\n-\t\tReceivers are fairly flexible in their specification,\n-\t\tas the machinery in the robustApply module takes care\n-\t\tof most of the details regarding figuring out appropriate\n-\t\tsubsets of the sent arguments to apply to a given\n-\t\treceiver.\n-\n-\t\tNote:\n-\t\t\tif receiver is itself a weak reference (a callable),\n-\t\t\tit will be de-referenced by the system's machinery,\n-\t\t\tso *generally* weak references are not suitable as\n-\t\t\treceivers, though some use might be found for the\n-\t\t\tfacility whereby a higher-level library passes in\n-\t\t\tpre-weakrefed receiver references.\n-\n-\tsignal -- the signal to which the receiver should respond\n-\t\n-\t\tif Any, receiver will receive any signal from the\n-\t\tindicated sender (which might also be Any, but is not\n-\t\tnecessarily Any).\n-\t\t\n-\t\tOtherwise must be a hashable Python object other than\n-\t\tNone (DispatcherError raised on None).\n-\t\t\n-\tsender -- the sender to which the receiver should respond\n-\t\n-\t\tif Any, receiver will receive the indicated signals\n-\t\tfrom any sender.\n-\t\t\n-\t\tif Anonymous, receiver will only receive indicated\n-\t\tsignals from send/sendExact which do not specify a\n-\t\tsender, or specify Anonymous explicitly as the sender.\n-\n-\t\tOtherwise can be any python object.\n-\t\t\n-\tweak -- whether to use weak references to the receiver\n-\t\tBy default, the module will attempt to use weak\n-\t\treferences to the receiver objects.  If this parameter\n-\t\tis false, then strong references will be used.\n-\n-\treturns None, may raise DispatcherTypeError\n-\t\"\"\"\n-\tif signal is None:\n-\t\traise errors.DispatcherTypeError(\n-\t\t\t'Signal cannot be None (receiver=%r sender=%r)'%( receiver,sender)\n-\t\t)\n-\tif weak:\n-\t\treceiver = saferef.safeRef(receiver, onDelete=_removeReceiver)\n-\tsenderkey = id(sender)\n-\tif connections.has_key(senderkey):\n-\t\tsignals = connections[senderkey]\n-\telse:\n-\t\tconnections[senderkey] = signals = {}\n-\t# Keep track of senders for cleanup.\n-\t# Is Anonymous something we want to clean up?\n-\tif sender not in (None, Anonymous, Any):\n-\t\tdef remove(object, senderkey=senderkey):\n-\t\t\t_removeSender(senderkey=senderkey)\n-\t\t# Skip objects that can not be weakly referenced, which means\n-\t\t# they won't be automatically cleaned up, but that's too bad.\n-\t\ttry:\n-\t\t\tweakSender = weakref.ref(sender, remove)\n-\t\t\tsenders[senderkey] = weakSender\n-\t\texcept:\n-\t\t\tpass\n-\t\t\n-\treceiverID = id(receiver)\n-\t# get current set, remove any current references to\n-\t# this receiver in the set, including back-references\n-\tif signals.has_key(signal):\n-\t\treceivers = signals[signal]\n-\t\t_removeOldBackRefs(senderkey, signal, receiver, receivers)\n-\telse:\n-\t\treceivers = signals[signal] = []\n-\ttry:\n-\t\tcurrent = sendersBack.get( receiverID )\n-\t\tif current is None:\n-\t\t\tsendersBack[ receiverID ] = current = []\n-\t\tif senderkey not in current:\n-\t\t\tcurrent.append(senderkey)\n-\texcept:\n-\t\tpass\n-\n-\treceivers.append(receiver)\n-\n-\n-\n-def disconnect(receiver, signal=Any, sender=Any, weak=True):\n-\t\"\"\"Disconnect receiver from sender for signal\n-\n-\treceiver -- the registered receiver to disconnect\n-\tsignal -- the registered signal to disconnect\n-\tsender -- the registered sender to disconnect\n-\tweak -- the weakref state to disconnect\n-\n-\tdisconnect reverses the process of connect,\n-\tthe semantics for the individual elements are\n-\tlogically equivalent to a tuple of\n-\t(receiver, signal, sender, weak) used as a key\n-\tto be deleted from the internal routing tables.\n-\t(The actual process is slightly more complex\n-\tbut the semantics are basically the same).\n-\n-\tNote:\n-\t\tUsing disconnect is not required to cleanup\n-\t\trouting when an object is deleted, the framework\n-\t\twill remove routes for deleted objects\n-\t\tautomatically.  It's only necessary to disconnect\n-\t\tif you want to stop routing to a live object.\n-\t\t\n-\treturns None, may raise DispatcherTypeError or\n-\t\tDispatcherKeyError\n-\t\"\"\"\n-\tif signal is None:\n-\t\traise errors.DispatcherTypeError(\n-\t\t\t'Signal cannot be None (receiver=%r sender=%r)'%( receiver,sender)\n-\t\t)\n-\tif weak: receiver = saferef.safeRef(receiver)\n-\tsenderkey = id(sender)\n-\ttry:\n-\t\tsignals = connections[senderkey]\n-\t\treceivers = signals[signal]\n-\texcept KeyError:\n-\t\traise errors.DispatcherKeyError(\n-\t\t\t\"\"\"No receivers found for signal %r from sender %r\"\"\" %(\n-\t\t\t\tsignal,\n-\t\t\t\tsender\n-\t\t\t)\n-\t\t)\n-\ttry:\n-\t\t# also removes from receivers\n-\t\t_removeOldBackRefs(senderkey, signal, receiver, receivers)\n-\texcept ValueError:\n-\t\traise errors.DispatcherKeyError(\n-\t\t\t\"\"\"No connection to receiver %s for signal %s from sender %s\"\"\" %(\n-\t\t\t\treceiver,\n-\t\t\t\tsignal,\n-\t\t\t\tsender\n-\t\t\t)\n-\t\t)\n-\t_cleanupConnections(senderkey, signal)\n-\n-def getReceivers( sender = Any, signal = Any ):\n-\t\"\"\"Get list of receivers from global tables\n-\n-\tThis utility function allows you to retrieve the\n-\traw list of receivers from the connections table\n-\tfor the given sender and signal pair.\n-\n-\tNote:\n-\t\tthere is no guarantee that this is the actual list\n-\t\tstored in the connections table, so the value\n-\t\tshould be treated as a simple iterable/truth value\n-\t\trather than, for instance a list to which you\n-\t\tmight append new records.\n-\n-\tNormally you would use liveReceivers( getReceivers( ...))\n-\tto retrieve the actual receiver objects as an iterable\n-\tobject.\n-\t\"\"\"\n-\ttry:\n-\t\treturn connections[id(sender)][signal]\n-\texcept KeyError:\n-\t\treturn []\n-\n-def liveReceivers(receivers):\n-\t\"\"\"Filter sequence of receivers to get resolved, live receivers\n-\n-\tThis is a generator which will iterate over\n-\tthe passed sequence, checking for weak references\n-\tand resolving them, then returning all live\n-\treceivers.\n-\t\"\"\"\n-\tfor receiver in receivers:\n-\t\tif isinstance( receiver, WEAKREF_TYPES):\n-\t\t\t# Dereference the weak reference.\n-\t\t\treceiver = receiver()\n-\t\t\tif receiver is not None:\n-\t\t\t\tyield receiver\n-\t\telse:\n-\t\t\tyield receiver\n-\n-\n-\n-def getAllReceivers( sender = Any, signal = Any ):\n-\t\"\"\"Get list of all receivers from global tables\n-\n-\tThis gets all receivers which should receive\n-\tthe given signal from sender, each receiver should\n-\tbe produced only once by the resulting generator\n-\t\"\"\"\n-\treceivers = {}\n-\tfor set in (\n-\t\t# Get receivers that receive *this* signal from *this* sender.\n-\t\tgetReceivers( sender, signal ),\n-\t\t# Add receivers that receive *any* signal from *this* sender.\n-\t\tgetReceivers( sender, Any ),\n-\t\t# Add receivers that receive *this* signal from *any* sender.\n-\t\tgetReceivers( Any, signal ),\n-\t\t# Add receivers that receive *any* signal from *any* sender.\n-\t\tgetReceivers( Any, Any ),\n-\t):\n-\t\tfor receiver in set:\n-\t\t\tif receiver: # filter out dead instance-method weakrefs\n-\t\t\t\ttry:\n-\t\t\t\t\tif not receivers.has_key( receiver ):\n-\t\t\t\t\t\treceivers[receiver] = 1\n-\t\t\t\t\t\tyield receiver\n-\t\t\t\texcept TypeError:\n-\t\t\t\t\t# dead weakrefs raise TypeError on hash...\n-\t\t\t\t\tpass\n-\n-def send(signal=Any, sender=Anonymous, *arguments, **named):\n-\t\"\"\"Send signal from sender to all connected receivers.\n-\t\n-\tsignal -- (hashable) signal value, see connect for details\n-\n-\tsender -- the sender of the signal\n-\t\n-\t\tif Any, only receivers registered for Any will receive\n-\t\tthe message.\n-\n-\t\tif Anonymous, only receivers registered to receive\n-\t\tmessages from Anonymous or Any will receive the message\n-\n-\t\tOtherwise can be any python object (normally one\n-\t\tregistered with a connect if you actually want\n-\t\tsomething to occur).\n-\n-\targuments -- positional arguments which will be passed to\n-\t\t*all* receivers. Note that this may raise TypeErrors\n-\t\tif the receivers do not allow the particular arguments.\n-\t\tNote also that arguments are applied before named\n-\t\targuments, so they should be used with care.\n-\n-\tnamed -- named arguments which will be filtered according\n-\t\tto the parameters of the receivers to only provide those\n-\t\tacceptable to the receiver.\n-\n-\tReturn a list of tuple pairs [(receiver, response), ... ]\n-\n-\tif any receiver raises an error, the error propagates back\n-\tthrough send, terminating the dispatch loop, so it is quite\n-\tpossible to not have all receivers called if a raises an\n-\terror.\n-\t\"\"\"\n-\t# Call each receiver with whatever arguments it can accept.\n-\t# Return a list of tuple pairs [(receiver, response), ... ].\n-\tresponses = []\n-\tfor receiver in liveReceivers(getAllReceivers(sender, signal)):\n-\t\tresponse = robustapply.robustApply(\n-\t\t\treceiver,\n-\t\t\tsignal=signal,\n-\t\t\tsender=sender,\n-\t\t\t*arguments,\n-\t\t\t**named\n-\t\t)\n-\t\tresponses.append((receiver, response))\n-\treturn responses\n-def sendExact( signal=Any, sender=Anonymous, *arguments, **named ):\n-\t\"\"\"Send signal only to those receivers registered for exact message\n-\n-\tsendExact allows for avoiding Any/Anonymous registered\n-\thandlers, sending only to those receivers explicitly\n-\tregistered for a particular signal on a particular\n-\tsender.\n-\t\"\"\"\n-\tresponses = []\n-\tfor receiver in liveReceivers(getReceivers(sender, signal)):\n-\t\tresponse = robustapply.robustApply(\n-\t\t\treceiver,\n-\t\t\tsignal=signal,\n-\t\t\tsender=sender,\n-\t\t\t*arguments,\n-\t\t\t**named\n-\t\t)\n-\t\tresponses.append((receiver, response))\n-\treturn responses\n-\t\n-\n-def _removeReceiver(receiver):\n-\t\"\"\"Remove receiver from connections.\"\"\"\n-\tif not sendersBack:\n-\t\t# During module cleanup the mapping will be replaced with None\n-\t\treturn False\n-\tbackKey = id(receiver)\n-\tfor senderkey in sendersBack.get(backKey,()):\n-\t\ttry:\n-\t\t\tsignals = connections[senderkey].keys()\n-\t\texcept KeyError,err:\n-\t\t\tpass\n-\t\telse:\n-\t\t\tfor signal in signals:\n-\t\t\t\ttry:\n-\t\t\t\t\treceivers = connections[senderkey][signal]\n-\t\t\t\texcept KeyError:\n-\t\t\t\t\tpass\n-\t\t\t\telse:\n-\t\t\t\t\ttry:\n-\t\t\t\t\t\treceivers.remove( receiver )\n-\t\t\t\t\texcept Exception, err:\n-\t\t\t\t\t\tpass\n-\t\t\t\t_cleanupConnections(senderkey, signal)\n-\ttry:\n-\t\tdel sendersBack[ backKey ]\n-\texcept KeyError:\n-\t\tpass\n-\t\t\t\n-def _cleanupConnections(senderkey, signal):\n-\t\"\"\"Delete any empty signals for senderkey. Delete senderkey if empty.\"\"\"\n-\ttry:\n-\t\treceivers = connections[senderkey][signal]\n-\texcept:\n-\t\tpass\n-\telse:\n-\t\tif not receivers:\n-\t\t\t# No more connected receivers. Therefore, remove the signal.\n-\t\t\ttry:\n-\t\t\t\tsignals = connections[senderkey]\n-\t\t\texcept KeyError:\n-\t\t\t\tpass\n-\t\t\telse:\n-\t\t\t\tdel signals[signal]\n-\t\t\t\tif not signals:\n-\t\t\t\t\t# No more signal connections. Therefore, remove the sender.\n-\t\t\t\t\t_removeSender(senderkey)\n-\n-def _removeSender(senderkey):\n-\t\"\"\"Remove senderkey from connections.\"\"\"\n-\t_removeBackrefs(senderkey)\n-\ttry:\n-\t\tdel connections[senderkey]\n-\texcept KeyError:\n-\t\tpass\n-\t# Senderkey will only be in senders dictionary if sender \n-\t# could be weakly referenced.\n-\ttry: \n-\t\tdel senders[senderkey]\n-\texcept: \n-\t\tpass\n-\n-\n-def _removeBackrefs( senderkey):\n-\t\"\"\"Remove all back-references to this senderkey\"\"\"\n-\ttry:\n-\t\tsignals = connections[senderkey]\n-\texcept KeyError:\n-\t\tsignals = None\n-\telse:\n-\t\titems = signals.items()\n-\t\tdef allReceivers( ):\n-\t\t\tfor signal,set in items:\n-\t\t\t\tfor item in set:\n-\t\t\t\t\tyield item\n-\t\tfor receiver in allReceivers():\n-\t\t\t_killBackref( receiver, senderkey )\n-\n-def _removeOldBackRefs(senderkey, signal, receiver, receivers):\n-\t\"\"\"Kill old sendersBack references from receiver\n-\n-\tThis guards against multiple registration of the same\n-\treceiver for a given signal and sender leaking memory\n-\tas old back reference records build up.\n-\n-\tAlso removes old receiver instance from receivers\n-\t\"\"\"\n-\ttry:\n-\t\tindex = receivers.index(receiver)\n-\t\t# need to scan back references here and remove senderkey\n-\texcept ValueError:\n-\t\treturn False\n-\telse:\n-\t\toldReceiver = receivers[index]\n-\t\tdel receivers[index]\n-\t\tfound = 0\n-\t\tsignals = connections.get(signal)\n-\t\tif signals is not None:\n-\t\t\tfor sig,recs in connections.get(signal,{}).iteritems():\n-\t\t\t\tif sig != signal:\n-\t\t\t\t\tfor rec in recs:\n-\t\t\t\t\t\tif rec is oldReceiver:\n-\t\t\t\t\t\t\tfound = 1\n-\t\t\t\t\t\t\tbreak\n-\t\tif not found:\n-\t\t\t_killBackref( oldReceiver, senderkey )\n-\t\t\treturn True\n-\t\treturn False\n-\t\t\n-\t\t\n-def _killBackref( receiver, senderkey ):\n-\t\"\"\"Do the actual removal of back reference from receiver to senderkey\"\"\"\n-\treceiverkey = id(receiver)\n-\tset = sendersBack.get( receiverkey, () )\n-\twhile senderkey in set:\n-\t\ttry:\n-\t\t\tset.remove( senderkey )\n-\t\texcept:\n-\t\t\tbreak\n-\tif not set:\n-\t\ttry:\n-\t\t\tdel sendersBack[ receiverkey ]\n-\t\texcept KeyError:\n-\t\t\tpass\n-\treturn True\n\n@@ -1,10 +0,0 @@\n-\"\"\"Error types for dispatcher mechanism\n-\"\"\"\n-\n-class DispatcherError(Exception):\n-\t\"\"\"Base class for all Dispatcher errors\"\"\"\n-class DispatcherKeyError(KeyError, DispatcherError):\n-\t\"\"\"Error raised when unknown (sender,signal) set specified\"\"\"\n-class DispatcherTypeError(TypeError, DispatcherError):\n-\t\"\"\"Error raised when inappropriate signal-type specified (None)\"\"\"\n-\n\n@@ -1,57 +0,0 @@\n-\"\"\"Module implementing error-catching version of send (sendRobust)\"\"\"\n-from pydispatch.dispatcher import Any, Anonymous, liveReceivers, getAllReceivers\n-from pydispatch.robustapply import robustApply\n-\n-def sendRobust(\n-\tsignal=Any, \n-\tsender=Anonymous, \n-\t*arguments, **named\n-):\n-\t\"\"\"Send signal from sender to all connected receivers catching errors\n-\t\n-\tsignal -- (hashable) signal value, see connect for details\n-\n-\tsender -- the sender of the signal\n-\t\n-\t\tif Any, only receivers registered for Any will receive\n-\t\tthe message.\n-\n-\t\tif Anonymous, only receivers registered to receive\n-\t\tmessages from Anonymous or Any will receive the message\n-\n-\t\tOtherwise can be any python object (normally one\n-\t\tregistered with a connect if you actually want\n-\t\tsomething to occur).\n-\n-\targuments -- positional arguments which will be passed to\n-\t\t*all* receivers. Note that this may raise TypeErrors\n-\t\tif the receivers do not allow the particular arguments.\n-\t\tNote also that arguments are applied before named\n-\t\targuments, so they should be used with care.\n-\n-\tnamed -- named arguments which will be filtered according\n-\t\tto the parameters of the receivers to only provide those\n-\t\tacceptable to the receiver.\n-\n-\tReturn a list of tuple pairs [(receiver, response), ... ]\n-\n-\tif any receiver raises an error (specifically any subclass of Exception),\n-\tthe error instance is returned as the result for that receiver.\n-\t\"\"\"\n-\t# Call each receiver with whatever arguments it can accept.\n-\t# Return a list of tuple pairs [(receiver, response), ... ].\n-\tresponses = []\n-\tfor receiver in liveReceivers(getAllReceivers(sender, signal)):\n-\t\ttry:\n-\t\t\tresponse = robustApply(\n-\t\t\t\treceiver,\n-\t\t\t\tsignal=signal,\n-\t\t\t\tsender=sender,\n-\t\t\t\t*arguments,\n-\t\t\t\t**named\n-\t\t\t)\n-\t\texcept Exception, err:\n-\t\t\tresponses.append((receiver, err))\n-\t\telse:\n-\t\t\tresponses.append((receiver, response))\n-\treturn responses\n\n@@ -1,49 +0,0 @@\n-\"\"\"Robust apply mechanism\n-\n-Provides a function \"call\", which can sort out\n-what arguments a given callable object can take,\n-and subset the given arguments to match only\n-those which are acceptable.\n-\"\"\"\n-\n-def function( receiver ):\n-\t\"\"\"Get function-like callable object for given receiver\n-\n-\treturns (function_or_method, codeObject, fromMethod)\n-\n-\tIf fromMethod is true, then the callable already\n-\thas its first argument bound\n-\t\"\"\"\n-\tif hasattr(receiver, '__call__'):\n-\t\t# receiver is a class instance; assume it is callable.\n-\t\t# Reassign receiver to the actual method that will be called.\n-\t\tif hasattr( receiver.__call__, 'im_func') or hasattr( receiver.__call__, 'im_code'):\n-\t\t\treceiver = receiver.__call__\n-\tif hasattr( receiver, 'im_func' ):\n-\t\t# an instance-method...\n-\t\treturn receiver, receiver.im_func.func_code, 1\n-\telif not hasattr( receiver, 'func_code'):\n-\t\traise ValueError('unknown reciever type %s %s'%(receiver, type(receiver)))\n-\treturn receiver, receiver.func_code, 0\n-\n-def robustApply(receiver, *arguments, **named):\n-\t\"\"\"Call receiver with arguments and an appropriate subset of named\n-\t\"\"\"\n-\treceiver, codeObject, startIndex = function( receiver )\n-\tacceptable = codeObject.co_varnames[startIndex+len(arguments):codeObject.co_argcount]\n-\tfor name in codeObject.co_varnames[startIndex:startIndex+len(arguments)]:\n-\t\tif named.has_key( name ):\n-\t\t\traise TypeError(\n-\t\t\t\t\"\"\"Argument %r specified both positionally and as a keyword for calling %r\"\"\"% (\n-\t\t\t\t\tname, receiver,\n-\t\t\t\t)\n-\t\t\t)\n-\tif not (codeObject.co_flags & 8):\n-\t\t# fc does not have a **kwds type parameter, therefore \n-\t\t# remove unacceptable arguments.\n-\t\tfor arg in named.keys():\n-\t\t\tif arg not in acceptable:\n-\t\t\t\tdel named[arg]\n-\treturn receiver(*arguments, **named)\n-\n-\t\t\t\n\\ No newline at end of file\n\n@@ -1,165 +0,0 @@\n-\"\"\"Refactored \"safe reference\" from dispatcher.py\"\"\"\n-import weakref, traceback\n-\n-def safeRef(target, onDelete = None):\n-\t\"\"\"Return a *safe* weak reference to a callable target\n-\n-\ttarget -- the object to be weakly referenced, if it's a\n-\t\tbound method reference, will create a BoundMethodWeakref,\n-\t\totherwise creates a simple weakref.\n-\tonDelete -- if provided, will have a hard reference stored\n-\t\tto the callable to be called after the safe reference\n-\t\tgoes out of scope with the reference object, (either a\n-\t\tweakref or a BoundMethodWeakref) as argument.\n-\t\"\"\"\n-\tif hasattr(target, 'im_self'):\n-\t\tif target.im_self is not None:\n-\t\t\t# Turn a bound method into a BoundMethodWeakref instance.\n-\t\t\t# Keep track of these instances for lookup by disconnect().\n-\t\t\tassert hasattr(target, 'im_func'), \"\"\"safeRef target %r has im_self, but no im_func, don't know how to create reference\"\"\"%( target,)\n-\t\t\treference = BoundMethodWeakref(\n-\t\t\t\ttarget=target,\n-\t\t\t\tonDelete=onDelete\n-\t\t\t)\n-\t\t\treturn reference\n-\tif callable(onDelete):\n-\t\treturn weakref.ref(target, onDelete)\n-\telse:\n-\t\treturn weakref.ref( target )\n-\n-class BoundMethodWeakref(object):\n-\t\"\"\"'Safe' and reusable weak references to instance methods\n-\n-\tBoundMethodWeakref objects provide a mechanism for\n-\treferencing a bound method without requiring that the\n-\tmethod object itself (which is normally a transient\n-\tobject) is kept alive.  Instead, the BoundMethodWeakref\n-\tobject keeps weak references to both the object and the\n-\tfunction which together define the instance method.\n-\n-\tAttributes:\n-\t\tkey -- the identity key for the reference, calculated\n-\t\t\tby the class's calculateKey method applied to the\n-\t\t\ttarget instance method\n-\t\tdeletionMethods -- sequence of callable objects taking\n-\t\t\tsingle argument, a reference to this object which\n-\t\t\twill be called when *either* the target object or\n-\t\t\ttarget function is garbage collected (i.e. when\n-\t\t\tthis object becomes invalid).  These are specified\n-\t\t\tas the onDelete parameters of safeRef calls.\n-\t\tweakSelf -- weak reference to the target object\n-\t\tweakFunc -- weak reference to the target function\n-\n-\tClass Attributes:\n-\t\t_allInstances -- class attribute pointing to all live\n-\t\t\tBoundMethodWeakref objects indexed by the class's\n-\t\t\tcalculateKey(target) method applied to the target\n-\t\t\tobjects.  This weak value dictionary is used to\n-\t\t\tshort-circuit creation so that multiple references\n-\t\t\tto the same (object, function) pair produce the\n-\t\t\tsame BoundMethodWeakref instance.\n-\n-\t\"\"\"\n-\t_allInstances = weakref.WeakValueDictionary()\n-\tdef __new__( cls, target, onDelete=None, *arguments,**named ):\n-\t\t\"\"\"Create new instance or return current instance\n-\n-\t\tBasically this method of construction allows us to\n-\t\tshort-circuit creation of references to already-\n-\t\treferenced instance methods.  The key corresponding\n-\t\tto the target is calculated, and if there is already\n-\t\tan existing reference, that is returned, with its\n-\t\tdeletionMethods attribute updated.  Otherwise the\n-\t\tnew instance is created and registered in the table\n-\t\tof already-referenced methods.\n-\t\t\"\"\"\n-\t\tkey = cls.calculateKey(target)\n-\t\tcurrent =cls._allInstances.get(key)\n-\t\tif current is not None:\n-\t\t\tcurrent.deletionMethods.append( onDelete)\n-\t\t\treturn current\n-\t\telse:\n-\t\t\tbase = super( BoundMethodWeakref, cls).__new__( cls )\n-\t\t\tcls._allInstances[key] = base\n-\t\t\tbase.__init__( target, onDelete, *arguments,**named)\n-\t\t\treturn base\n-\tdef __init__(self, target, onDelete=None):\n-\t\t\"\"\"Return a weak-reference-like instance for a bound method\n-\n-\t\ttarget -- the instance-method target for the weak\n-\t\t\treference, must have im_self and im_func attributes\n-\t\t\tand be reconstructable via:\n-\t\t\t\ttarget.im_func.__get__( target.im_self )\n-\t\t\twhich is true of built-in instance methods.\n-\t\tonDelete -- optional callback which will be called\n-\t\t\twhen this weak reference ceases to be valid\n-\t\t\t(i.e. either the object or the function is garbage\n-\t\t\tcollected).  Should take a single argument,\n-\t\t\twhich will be passed a pointer to this object.\n-\t\t\"\"\"\n-\t\tdef remove(weak, self=self):\n-\t\t\t\"\"\"Set self.isDead to true when method or instance is destroyed\"\"\"\n-\t\t\tmethods = self.deletionMethods[:]\n-\t\t\tdel self.deletionMethods[:]\n-\t\t\ttry:\n-\t\t\t\tdel self.__class__._allInstances[ self.key ]\n-\t\t\texcept KeyError:\n-\t\t\t\tpass\n-\t\t\tfor function in methods:\n-\t\t\t\ttry:\n-\t\t\t\t\tif callable( function ):\n-\t\t\t\t\t\tfunction( self )\n-\t\t\t\texcept Exception, e:\n-\t\t\t\t\ttry:\n-\t\t\t\t\t\ttraceback.print_exc()\n-\t\t\t\t\texcept AttributeError, err:\n-\t\t\t\t\t\tprint '''Exception during saferef %s cleanup function %s: %s'''%(\n-\t\t\t\t\t\t\tself, function, e\n-\t\t\t\t\t\t)\n-\t\tself.deletionMethods = [onDelete]\n-\t\tself.key = self.calculateKey( target )\n-\t\tself.weakSelf = weakref.ref(target.im_self, remove)\n-\t\tself.weakFunc = weakref.ref(target.im_func, remove)\n-\t\tself.selfName = str(target.im_self)\n-\t\tself.funcName = str(target.im_func.__name__)\n-\tdef calculateKey( cls, target ):\n-\t\t\"\"\"Calculate the reference key for this reference\n-\n-\t\tCurrently this is a two-tuple of the id()'s of the\n-\t\ttarget object and the target function respectively.\n-\t\t\"\"\"\n-\t\treturn (id(target.im_self),id(target.im_func))\n-\tcalculateKey = classmethod( calculateKey )\n-\tdef __str__(self):\n-\t\t\"\"\"Give a friendly representation of the object\"\"\"\n-\t\treturn \"\"\"%s( %s.%s )\"\"\"%(\n-\t\t\tself.__class__.__name__,\n-\t\t\tself.selfName,\n-\t\t\tself.funcName,\n-\t\t)\n-\t__repr__ = __str__\n-\tdef __nonzero__( self ):\n-\t\t\"\"\"Whether we are still a valid reference\"\"\"\n-\t\treturn self() is not None\n-\tdef __cmp__( self, other ):\n-\t\t\"\"\"Compare with another reference\"\"\"\n-\t\tif not isinstance (other,self.__class__):\n-\t\t\treturn cmp( self.__class__, type(other) )\n-\t\treturn cmp( self.key, other.key)\n-\tdef __call__(self):\n-\t\t\"\"\"Return a strong reference to the bound method\n-\n-\t\tIf the target cannot be retrieved, then will\n-\t\treturn None, otherwise returns a bound instance\n-\t\tmethod for our object and function.\n-\n-\t\tNote:\n-\t\t\tYou may call this method any number of times,\n-\t\t\tas it does not invalidate the reference.\n-\t\t\"\"\"\n-\t\ttarget = self.weakSelf()\n-\t\tif target is not None:\n-\t\t\tfunction = self.weakFunc()\n-\t\t\tif function is not None:\n-\t\t\t\treturn function.__get__(target)\n-\t\treturn None\n\n@@ -1 +0,0 @@\n-from spidermonkey import *\n\\ No newline at end of file\n\n@@ -1,2 +0,0 @@\n-#defines order in which spidermonkey bin library will be searched\n-try_libs = [\"libmozjs.so.0d\", \"libjs.so\", \"libjs.so.0\"]\n\\ No newline at end of file\n\n@@ -1,2254 +0,0 @@\n-# this is a ctypes-based python wrapper for mozilla spidermonkey\n-\n-# ************************************************************************\n-# This is an alpha release.  It has many known faults, and interfaces will\n-# change.\n-\n-# Note that code listed with JavaScript error messages can be the WRONG\n-# CODE!  Don't take it seriously.\n-# ************************************************************************\n-\n-# spidermonkey 0.0.2a: Python / JavaScript bridge.\n-# Copyright (C) 2003  John J. Lee <jjl@pobox.com>\n-# Copyright (C) 2006  Leonard Ritter <paniq@paniq.org>\n-\n-# Partly derived from Spidermonkey.xs, Copyright (C) 2001, Claes\n-#  Jacobsson, All Rights Reserved (Perl Artistic License).\n-# Partly derived from PerlConnect (part of spidermonkey) Copyright (C)\n-#  1998 Netscape Communications Corporation (GPL).\n-#\n-# This program is free software; you can redistribute it and/or modify\n-# it under the terms of the GNU General Public License as published by\n-# the Free Software Foundation; either version 2 of the License, or\n-# (at your option) any later version.\n-#\n-# This program is distributed in the hope that it will be useful,\n-# but WITHOUT ANY WARRANTY; without even the implied warranty of\n-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-# GNU General Public License for more details.\n-#\n-# You should have received a copy of the GNU General Public License\n-# along with this program; if not, write to the Free Software\n-# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n-\n-\n-# Security constraints:\n-#  Can't add attributes to Python objects.\n-#  *Can* add JS properties to JS proxies of Python objects, as long as it\n-#   won't hide a Python attribute of the same name.\n-#  Can't assign to Python methods.\n-#  Can't access any Python attribute starting with \"_\".\n-#  Also, bound Python objects can of course implement their own\n-#   protection, as the Window class does.\n-\n-# XXX\n-# Does JS expect some exceptions on reading a non-existent property?\n-#  Currently, this code has the effect of returning undef in that case,\n-#  which seems odd.  Look in O'Reilly book / spidermonkey code.\n-# Exception propagation.  IIRC, I'm waiting for except * to be fixed in\n-#  Pyrex.\n-# Code listings on JS errors are wrong.\n-# Review code\n-#  Replace Object with Py_INCREF and just keeping python object in JS\n-#   object (need to think first, though, to check this will work).\n-#  API call return values and bad function arguments!\n-#  Memory management.\n-#   Check strcpy's, malloc's / free's / Destroy*'s.\n-#   Look for and fix any memory leaks.\n-#  To prevent people crashing module, would be nice to make some things\n-#   private.  Ask on Pyrex list.\n-#  Make use of JS API security model?\n-#  GC issues (see spidermonkey GC tips page).\n-#  Threading issues (see GC tips again).\n-\n-from ctypes import *\n-\n-# predefines\n-size_t = c_ulong\n-_IO_FILE = c_ulong\n-__ssize_t = c_int\n-ssize_t = c_uint\n-\n-JSWord = c_long\n-jsword = JSWord\n-jsval = jsword\n-\n-class JSObject(Structure):\n-\t_fields_ = [\n-\t]\n-class ObjectJSVALUnion(Union):\n-\t_fields_ = [\n-\t\t('jsval', jsval),\n-\t\t('obj', POINTER(JSObject)),\n-\t]\n-\n-def OBJECT_TO_JSVAL(obj):\n-\tobjectJSVALUnion = ObjectJSVALUnion()\n-\tobjectJSVALUnion.obj = obj\n-\treturn objectJSVALUnion.jsval\n-\n-# defines\n-EOF = (-1)\n-FILENAME_MAX = 4096\n-FOPEN_MAX = 16\n-def JS_BIT(n):\n-\treturn 1 << n\n-def JS_BITMASK(n):\n-\treturn JS_BIT(n) - 1\n-def JSVAL_SETTAG(v,t):\n-\tif type(v) == c_long:\n-\t\tv = v.value\n-\treturn v | t\n-def BOOLEAN_TO_JSVAL(b):\n-\treturn JSVAL_SETTAG(b << JSVAL_TAGBITS, JSVAL_BOOLEAN)\n-def JSVAL_INT_POW2(n):\n-\treturn 1 << n\n-def INT_TO_JSVAL(i):\n-\treturn (i << 1) | JSVAL_INT\n-def JSVAL_IS_PRIMITIVE(v):\n-\treturn ((not JSVAL_IS_OBJECT(v)) or JSVAL_IS_NULL(v))\n-def JSVAL_IS_OBJECT(v):\n-\treturn (JSVAL_TAG(v) == JSVAL_OBJECT)\n-def JSVAL_IS_INT(v):\n-\tif type(v) == c_long:\n-\t\tv = v.value\n-\treturn ((v & JSVAL_INT) and (v != JSVAL_VOID))\n-def JSVAL_TO_INT(v):\n-\tif type(v) == c_long:\n-\t\tv = v.value\n-\treturn v >> 1\n-def JSVAL_IS_DOUBLE(v):\n-\treturn (JSVAL_TAG(v) == JSVAL_DOUBLE)\n-def JSVAL_TO_DOUBLE(v):\n-\treturn cast(JSVAL_TO_GCTHING(v),POINTER(jsdouble))\n-def JSVAL_IS_BOOLEAN(v):\n-\treturn (JSVAL_TAG(v) == JSVAL_BOOLEAN)\n-def JSVAL_TO_BOOLEAN(v):\n-\tif type(v) == c_long:\n-\t\tv = v.value\n-\treturn v >> JSVAL_TAGBITS\n-def JSVAL_IS_STRING(v):\n-\treturn JSVAL_TAG(v) == JSVAL_STRING\n-def JSVAL_TO_STRING(v):\n-\treturn cast(JSVAL_TO_GCTHING(v),POINTER(JSString))\n-def STRING_TO_JSVAL(str):\n-\treturn JSVAL_SETTAG(cast(addressof(str),POINTER(jsval)).contents, JSVAL_STRING)\n-def DOUBLE_TO_JSVAL(dp):\n-\treturn JSVAL_SETTAG(cast(addressof(dp),POINTER(jsval)).contents, JSVAL_DOUBLE)\n-def JSVAL_TO_GCTHING(v):\n-\treturn cast(JSVAL_CLRTAG(v),c_void_p)\n-def JSVAL_CLRTAG(v):\n-\tif type(v) == c_long:\n-\t\tv = v.value\n-\treturn v & (~JSVAL_TAGMASK)\n-def JSVAL_TAG(v):\n-\tif type(v) == c_long:\n-\t\tv = v.value\n-\treturn v & JSVAL_TAGMASK\n-def JSVAL_IS_NULL(v):\n-\tif type(v) == c_long:\n-\t\tv = v.value\n-\treturn v == JSVAL_NULL\n-def JSVAL_IS_VOID(v):\n-\tif type(v) == c_long:\n-\t\tv = v.value\n-\treturn v == JSVAL_VOID\n-def JSVAL_TO_OBJECT(v):\n-\treturn cast(JSVAL_TO_GCTHING(v), POINTER(JSObject))\n-JSCLASS_HAS_PRIVATE = (1<<0)\n-JSCLASS_NEW_ENUMERATE = (1<<1)\n-JSCLASS_NEW_RESOLVE = (1<<2)\n-JSCLASS_NEW_RESOLVE_GETS_START = (1<<5)\n-JSCLASS_NO_OPTIONAL_MEMBERS = 0,0,0,0,0,0,0,0\n-JSCLASS_PRIVATE_IS_NSISUPPORTS = (1<<3)\n-JSCLASS_RESERVED_SLOTS_WIDTH = 8\n-JSCLASS_RESERVED_SLOTS_MASK = JS_BITMASK(JSCLASS_RESERVED_SLOTS_WIDTH)\n-JSCLASS_RESERVED_SLOTS_SHIFT = 8\n-JSCLASS_SHARE_ALL_PROPERTIES = (1<<4)\n-JSFUN_BOUND_METHOD = 0x40\n-JSFUN_FLAGS_MASK = 0xf8\n-JSFUN_HEAVYWEIGHT = 0x80\n-JSFUN_LAMBDA = 0x08\n-JSOPTION_COMPILE_N_GO = JS_BIT(4)\n-JSOPTION_PRIVATE_IS_NSISUPPORTS = JS_BIT(3)\n-JSOPTION_STRICT = JS_BIT(0)\n-JSOPTION_VAROBJFIX = JS_BIT(2)\n-JSOPTION_WERROR = JS_BIT(1)\n-JSPROP_ENUMERATE = 0x01\n-JSPROP_EXPORTED = 0x08\n-JSPROP_GETTER = 0x10\n-JSPROP_INDEX = 0x80\n-JSPROP_PERMANENT = 0x04\n-JSPROP_READONLY = 0x02\n-JSPROP_SETTER = 0x20\n-JSPROP_SHARED = 0x40\n-JSREG_FOLD = 0x01\n-JSREG_GLOB = 0x02\n-JSREG_MULTILINE = 0x04\n-JSREPORT_ERROR = 0x0\n-JSREPORT_EXCEPTION = 0x2\n-JSREPORT_STRICT = 0x4\n-JSREPORT_WARNING = 0x1\n-JSRESOLVE_ASSIGNING = 0x02\n-JSRESOLVE_QUALIFIED = 0x01\n-JSVAL_TAGBITS = 3\n-JSVAL_ALIGN = JS_BIT(JSVAL_TAGBITS)\n-JSVAL_BOOLEAN = 0x6\n-JSVAL_DOUBLE = 0x2\n-JS_FALSE = 0\n-JSVAL_FALSE = BOOLEAN_TO_JSVAL(JS_FALSE)\n-JSVAL_INT = 0x1\n-JSVAL_INT_BITS = 31\n-JSVAL_INT_MAX = (JSVAL_INT_POW2(30) - 1)\n-JSVAL_INT_MIN = (1 - JSVAL_INT_POW2(30))\n-JSVAL_NULL = 0\n-JSVAL_OBJECT = 0x0\n-JSVAL_ONE = INT_TO_JSVAL(1)\n-JSVAL_STRING = 0x4\n-JSVAL_TAGMASK = JS_BITMASK(JSVAL_TAGBITS)\n-JS_TRUE = 1\n-JSVAL_TRUE = BOOLEAN_TO_JSVAL(JS_TRUE)\n-JSVAL_VOID = INT_TO_JSVAL(0 - JSVAL_INT_POW2(30))\n-JSVAL_ZERO = INT_TO_JSVAL(0)\n-JS_DONT_PRETTY_PRINT = (0x8000)\n-JS_MAP_GCROOT_NEXT = 0\n-JS_MAP_GCROOT_REMOVE = 2\n-JS_MAP_GCROOT_STOP = 1\n-SEEK_CUR = 1\n-SEEK_END = 2\n-SEEK_SET = 0\n-TMP_MAX = 238328\n-\n-# using file \"/usr/include/smjs/jslong.h\"\n-# using file \"/usr/include/smjs/jscompat.h\"\n-# using file \"/usr/include/smjs/jspubtd.h\"\n-# using file \"/usr/include/smjs/jsapi.h\"\n-JSUintn = c_uint\n-\n-uintN = JSUintn\n-\n-class JSErrorFormatString(Structure):\n-\t_fields_ = [\n-\t\t('format', c_char_p),\n-\t\t('argCount', uintN),\n-\t]\n-\n-JSErrorFormatString = JSErrorFormatString\n-\n-class JSContext(Structure):\n-\t_fields_ = [\n-\t]\n-\n-JSContext = JSContext\n-\n-JSIntn = c_int\n-\n-JSBool = JSIntn\n-\n-JSInt32 = c_int\n-\n-int32 = JSInt32\n-\n-jsrefcount = int32\n-\n-class JSPrincipals(Structure):\n-\t_fields_ = [\n-\t\t('codebase', c_char_p),\n-\t\t('getPrincipalArray', CFUNCTYPE(c_void_p,POINTER(JSContext),POINTER('JSPrincipals'))),\n-\t\t('globalPrivilegesEnabled', CFUNCTYPE(JSBool,POINTER(JSContext),POINTER('JSPrincipals'))),\n-\t\t('refcount', jsrefcount),\n-\t\t('destroy', CFUNCTYPE(None,POINTER(JSContext),POINTER('JSPrincipals'))),\n-\t]\n-\n-JSPrincipals = JSPrincipals\n-\n-jsid = jsword\n-\n-class JSProperty(Structure):\n-\t_fields_ = [\n-\t\t('id', jsid),\n-\t]\n-\n-JSProperty = JSProperty\n-\n-class JSObjectMap(Structure):\n-\t_fields_ = [\n-\t]\n-\n-JSObjectMap = JSObjectMap\n-\n-class JSXDRState(Structure):\n-\t_fields_ = [\n-\t]\n-\n-JSXDRState = JSXDRState\n-\n-class JSObjectOps(Structure):\n-\t_fields_ = []\n-\n-JSUint32 = c_uint\n-\n-uint32 = JSUint32\n-\n-JSObject = JSObject\n-\n-JSPropertyOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval))\n-\n-JSEnumerateOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject))\n-\n-JSResolveOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long)\n-\n-# enumeration JSType\n-JSTYPE_VOID = 0\n-JSTYPE_OBJECT = 1\n-JSTYPE_FUNCTION = 2\n-JSTYPE_STRING = 3\n-JSTYPE_NUMBER = 4\n-JSTYPE_BOOLEAN = 5\n-JSTYPE_LIMIT = 6\n-\n-JSConvertOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_int,POINTER(jsval))\n-\n-JSFinalizeOp = CFUNCTYPE(None,POINTER(JSContext),POINTER(JSObject))\n-\n-class JSClass(Structure):\n-\t_fields_ = []\n-\n-JSGetObjectOps = CFUNCTYPE(POINTER(JSObjectOps),POINTER(JSContext),POINTER(JSClass))\n-\n-# enumeration JSAccessMode\n-JSACC_PROTO = 0\n-JSACC_PARENT = 1\n-JSACC_IMPORT = 2\n-JSACC_WATCH = 3\n-JSACC_READ = 4\n-JSACC_WRITE = 8\n-JSACC_LIMIT = 9\n-\n-JSCheckAccessOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,c_int,POINTER(jsval))\n-\n-JSNative = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_uint,POINTER(jsval),POINTER(jsval))\n-\n-JSXDRObjectOp = CFUNCTYPE(JSBool,POINTER(JSXDRState),POINTER(POINTER(JSObject)))\n-\n-JSHasInstanceOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(JSBool))\n-\n-JSMarkOp = CFUNCTYPE(uint32,POINTER(JSContext),POINTER(JSObject),c_void_p)\n-\n-class JSClass(Structure):\n-\t_fields_ = [\n-\t\t('name', c_char_p),\n-\t\t('flags', uint32),\n-\t\t('addProperty', JSPropertyOp),\n-\t\t('delProperty', JSPropertyOp),\n-\t\t('getProperty', JSPropertyOp),\n-\t\t('setProperty', JSPropertyOp),\n-\t\t('enumerate', JSEnumerateOp),\n-\t\t('resolve', JSResolveOp),\n-\t\t('convert', JSConvertOp),\n-\t\t('finalize', JSFinalizeOp),\n-\t\t('getObjectOps', JSGetObjectOps),\n-\t\t('checkAccess', JSCheckAccessOp),\n-\t\t('call', JSNative),\n-\t\t('construct', JSNative),\n-\t\t('xdrObject', JSXDRObjectOp),\n-\t\t('hasInstance', JSHasInstanceOp),\n-\t\t('mark', JSMarkOp),\n-\t\t('spare', jsword),\n-\t]\n-\n-JSClass = JSClass\n-\n-JSNewObjectMapOp = CFUNCTYPE(POINTER(JSObjectMap),POINTER(JSContext),c_int,POINTER(JSObjectOps),POINTER(JSClass),POINTER(JSObject))\n-\n-JSObjectMapOp = CFUNCTYPE(None,POINTER(JSContext),POINTER(JSObjectMap))\n-\n-JSLookupPropOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(POINTER(JSObject)),POINTER(POINTER(JSProperty)))\n-\n-JSDefinePropOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,c_long,CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)),CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)),c_uint,POINTER(POINTER(JSProperty)))\n-\n-JSPropertyIdOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval))\n-\n-JSAttributesOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(JSProperty),POINTER(uintN))\n-\n-# enumeration JSIterateOp\n-JSENUMERATE_INIT = 0\n-JSENUMERATE_NEXT = 1\n-JSENUMERATE_DESTROY = 2\n-\n-JSNewEnumerateOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_int,POINTER(jsval),POINTER(jsid))\n-\n-JSCheckAccessIdOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,c_int,POINTER(jsval),POINTER(uintN))\n-\n-JSObjectOp = CFUNCTYPE(POINTER(JSObject),POINTER(JSContext),POINTER(JSObject))\n-\n-JSPropertyRefOp = CFUNCTYPE(None,POINTER(JSContext),POINTER(JSObject),POINTER(JSProperty))\n-\n-JSSetObjectSlotOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_uint,POINTER(JSObject))\n-\n-JSGetRequiredSlotOp = CFUNCTYPE(jsval,POINTER(JSContext),POINTER(JSObject),c_uint)\n-\n-JSSetRequiredSlotOp = CFUNCTYPE(None,POINTER(JSContext),POINTER(JSObject),c_uint,c_long)\n-\n-class JSObjectOps(Structure):\n-\t_fields_ = [\n-\t\t('newObjectMap', JSNewObjectMapOp),\n-\t\t('destroyObjectMap', JSObjectMapOp),\n-\t\t('lookupProperty', JSLookupPropOp),\n-\t\t('defineProperty', JSDefinePropOp),\n-\t\t('getProperty', JSPropertyIdOp),\n-\t\t('setProperty', JSPropertyIdOp),\n-\t\t('getAttributes', JSAttributesOp),\n-\t\t('setAttributes', JSAttributesOp),\n-\t\t('deleteProperty', JSPropertyIdOp),\n-\t\t('defaultValue', JSConvertOp),\n-\t\t('enumerate', JSNewEnumerateOp),\n-\t\t('checkAccess', JSCheckAccessIdOp),\n-\t\t('thisObject', JSObjectOp),\n-\t\t('dropProperty', JSPropertyRefOp),\n-\t\t('call', JSNative),\n-\t\t('construct', JSNative),\n-\t\t('xdrObject', JSXDRObjectOp),\n-\t\t('hasInstance', JSHasInstanceOp),\n-\t\t('setProto', JSSetObjectSlotOp),\n-\t\t('setParent', JSSetObjectSlotOp),\n-\t\t('mark', JSMarkOp),\n-\t\t('clear', JSFinalizeOp),\n-\t\t('getRequiredSlot', JSGetRequiredSlotOp),\n-\t\t('setRequiredSlot', JSSetRequiredSlotOp),\n-\t]\n-\n-JSObjectOps = JSObjectOps\n-\n-JSUint16 = c_ushort\n-\n-uint16 = JSUint16\n-\n-jschar = uint16\n-\n-class JSErrorReport(Structure):\n-\t_fields_ = [\n-\t\t('filename', c_char_p),\n-\t\t('lineno', uintN),\n-\t\t('linebuf', c_char_p),\n-\t\t('tokenptr', c_char_p),\n-\t\t('uclinebuf', POINTER(jschar)),\n-\t\t('uctokenptr', POINTER(jschar)),\n-\t\t('flags', uintN),\n-\t\t('errorNumber', uintN),\n-\t\t('ucmessage', POINTER(jschar)),\n-\t\t('messageArgs', POINTER(POINTER(jschar))),\n-\t]\n-\n-class JSString(Structure):\n-\t_fields_ = [\n-\t]\n-\n-JSString = JSString\n-\n-JSLocaleToUpperCase = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSString),POINTER(jsval))\n-\n-JSLocaleToLowerCase = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSString),POINTER(jsval))\n-\n-JSLocaleCompare = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSString),POINTER(JSString),POINTER(jsval))\n-\n-class JSLocaleCallbacks(Structure):\n-\t_fields_ = [\n-\t\t('localeToUpperCase', JSLocaleToUpperCase),\n-\t\t('localeToLowerCase', JSLocaleToLowerCase),\n-\t\t('localeCompare', JSLocaleCompare),\n-\t]\n-\n-JSUint8 = c_ubyte\n-\n-uint8 = JSUint8\n-\n-class JSFunctionSpec(Structure):\n-\t_fields_ = [\n-\t\t('name', c_char_p),\n-\t\t('call', JSNative),\n-\t\t('nargs', uint8),\n-\t\t('flags', uint8),\n-\t\t('extra', uint16),\n-\t]\n-\n-JSInt8 = c_char\n-\n-int8 = JSInt8\n-\n-class JSPropertySpec(Structure):\n-\t_fields_ = [\n-\t\t('name', c_char_p),\n-\t\t('tinyid', int8),\n-\t\t('flags', uint8),\n-\t\t('getter', JSPropertyOp),\n-\t\t('setter', JSPropertyOp),\n-\t]\n-\n-JSFloat64 = c_double\n-\n-float64 = JSFloat64\n-\n-jsdouble = float64\n-\n-class JSConstDoubleSpec(Structure):\n-\t_fields_ = [\n-\t\t('dval', jsdouble),\n-\t\t('name', c_char_p),\n-\t\t('flags', uint8),\n-\t\t('spare', uint8*3),\n-\t]\n-\n-jsint = int32\n-\n-class JSIdArray(Structure):\n-\t_fields_ = [\n-\t\t('length', jsint),\n-\t\t('vector', jsid*1),\n-\t]\n-\n-intN = JSIntn\n-\n-JSErrorReport = JSErrorReport\n-\n-class JSScript(Structure):\n-\t_fields_ = [\n-\t]\n-\n-JSScript = JSScript\n-\n-# enumeration JSGCStatus\n-JSGC_BEGIN = 0\n-JSGC_END = 1\n-JSGC_MARK_END = 2\n-JSGC_FINALIZE_END = 3\n-\n-JSConstDoubleSpec = JSConstDoubleSpec\n-\n-JSPropertySpec = JSPropertySpec\n-\n-jsuint = uint32\n-\n-JSIdArray = JSIdArray\n-\n-class JSRuntime(Structure):\n-\t_fields_ = [\n-\t]\n-\n-JSRuntime = JSRuntime\n-\n-JSFunctionSpec = JSFunctionSpec\n-\n-class JSFunction(Structure):\n-\t_fields_ = [\n-\t]\n-\n-JSFunction = JSFunction\n-\n-JSLocaleCallbacks = JSLocaleCallbacks\n-\n-class JSExceptionState(Structure):\n-\t_fields_ = [\n-\t]\n-\n-JSExceptionState = JSExceptionState\n-\n-JSInt64 = c_longlong\n-\n-from sm_settings import try_libs\n-for lib in try_libs:\n-    try:\n-        libsmjs = CDLL(lib)\n-        break\n-    except OSError:\n-        if lib == try_libs[-1]:\n-            raise ImportError(\"The spidermonkey C library is not installed\")\n-\n-JSLL_MaxInt = libsmjs.JSLL_MaxInt\n-JSLL_MaxInt.restype = JSInt64\n-JSLL_MaxInt.argtypes = []\n-\n-JSLL_MinInt = libsmjs.JSLL_MinInt\n-JSLL_MinInt.restype = JSInt64\n-JSLL_MinInt.argtypes = []\n-\n-JSLL_Zero = libsmjs.JSLL_Zero\n-JSLL_Zero.restype = JSInt64\n-JSLL_Zero.argtypes = []\n-\n-JSUword = c_ulong\n-\n-jsuword = JSUword\n-\n-float32 = c_float\n-\n-# enumeration Js-1.7.0.tar.gzSVersion\n-JSVERSION_1_0 = 100\n-JSVERSION_1_1 = 110\n-JSVERSION_1_2 = 120\n-JSVERSION_1_3 = 130\n-JSVERSION_1_4 = 140\n-JSVERSION_ECMA_3 = 148\n-JSVERSION_1_5 = 150\n-JSVERSION_DEFAULT = 0\n-JSVERSION_UNKNOWN = -1\n-\n-JSVersion = c_int\n-\n-JSType = c_int\n-\n-JSAccessMode = c_int\n-\n-JSIterateOp = c_int\n-\n-JSTaskState = JSRuntime\n-\n-JSNewResolveOp = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,c_uint,POINTER(POINTER(JSObject)))\n-\n-JSStringFinalizeOp = CFUNCTYPE(None,POINTER(JSContext),POINTER(JSString))\n-\n-JSGCStatus = c_int\n-\n-JSGCCallback = CFUNCTYPE(JSBool,POINTER(JSContext),c_int)\n-\n-JSBranchCallback = CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSScript))\n-\n-JSErrorReporter = CFUNCTYPE(None,POINTER(JSContext),c_char_p,POINTER(JSErrorReport))\n-\n-JSErrorCallback = CFUNCTYPE(POINTER(JSErrorFormatString),c_void_p,c_char_p,c_uint)\n-\n-JSPrincipalsTranscoder = CFUNCTYPE(JSBool,POINTER(JSXDRState),POINTER(POINTER(JSPrincipals)))\n-\n-JSObjectPrincipalsFinder = CFUNCTYPE(POINTER(JSPrincipals),POINTER(JSContext),POINTER(JSObject))\n-\n-int64 = JSInt64\n-\n-JS_Now = libsmjs.JS_Now\n-JS_Now.restype = int64\n-JS_Now.argtypes = []\n-\n-JS_GetNaNValue = libsmjs.JS_GetNaNValue\n-JS_GetNaNValue.restype = jsval\n-JS_GetNaNValue.argtypes = [POINTER(JSContext)]\n-\n-JS_GetNegativeInfinityValue = libsmjs.JS_GetNegativeInfinityValue\n-JS_GetNegativeInfinityValue.restype = jsval\n-JS_GetNegativeInfinityValue.argtypes = [POINTER(JSContext)]\n-\n-JS_GetPositiveInfinityValue = libsmjs.JS_GetPositiveInfinityValue\n-JS_GetPositiveInfinityValue.restype = jsval\n-JS_GetPositiveInfinityValue.argtypes = [POINTER(JSContext)]\n-\n-JS_GetEmptyStringValue = libsmjs.JS_GetEmptyStringValue\n-JS_GetEmptyStringValue.restype = jsval\n-JS_GetEmptyStringValue.argtypes = [POINTER(JSContext)]\n-\n-JS_ConvertArguments = libsmjs.JS_ConvertArguments\n-JS_ConvertArguments.restype = JSBool\n-JS_ConvertArguments.argtypes = [POINTER(JSContext),c_uint,POINTER(jsval),c_char_p]\n-\n-JS_PushArguments = libsmjs.JS_PushArguments\n-JS_PushArguments.restype = POINTER(jsval)\n-JS_PushArguments.argtypes = [POINTER(JSContext),POINTER(c_void_p),c_char_p]\n-\n-JS_PopArguments = libsmjs.JS_PopArguments\n-JS_PopArguments.restype = None\n-JS_PopArguments.argtypes = [POINTER(JSContext),c_void_p]\n-\n-JS_ConvertValue = libsmjs.JS_ConvertValue\n-JS_ConvertValue.restype = JSBool\n-JS_ConvertValue.argtypes = [POINTER(JSContext),c_long,c_int,POINTER(jsval)]\n-\n-JS_ValueToObject = libsmjs.JS_ValueToObject\n-JS_ValueToObject.restype = JSBool\n-JS_ValueToObject.argtypes = [POINTER(JSContext),c_long,POINTER(POINTER(JSObject))]\n-\n-JS_ValueToFunction = libsmjs.JS_ValueToFunction\n-JS_ValueToFunction.restype = POINTER(JSFunction)\n-JS_ValueToFunction.argtypes = [POINTER(JSContext),c_long]\n-\n-JS_ValueToConstructor = libsmjs.JS_ValueToConstructor\n-JS_ValueToConstructor.restype = POINTER(JSFunction)\n-JS_ValueToConstructor.argtypes = [POINTER(JSContext),c_long]\n-\n-JS_ValueToString = libsmjs.JS_ValueToString\n-JS_ValueToString.restype = POINTER(JSString)\n-JS_ValueToString.argtypes = [POINTER(JSContext),c_long]\n-\n-JS_ValueToNumber = libsmjs.JS_ValueToNumber\n-JS_ValueToNumber.restype = JSBool\n-JS_ValueToNumber.argtypes = [POINTER(JSContext),c_long,POINTER(jsdouble)]\n-\n-JS_ValueToECMAInt32 = libsmjs.JS_ValueToECMAInt32\n-JS_ValueToECMAInt32.restype = JSBool\n-JS_ValueToECMAInt32.argtypes = [POINTER(JSContext),c_long,POINTER(int32)]\n-\n-JS_ValueToECMAUint32 = libsmjs.JS_ValueToECMAUint32\n-JS_ValueToECMAUint32.restype = JSBool\n-JS_ValueToECMAUint32.argtypes = [POINTER(JSContext),c_long,POINTER(uint32)]\n-\n-JS_ValueToInt32 = libsmjs.JS_ValueToInt32\n-JS_ValueToInt32.restype = JSBool\n-JS_ValueToInt32.argtypes = [POINTER(JSContext),c_long,POINTER(int32)]\n-\n-JS_ValueToUint16 = libsmjs.JS_ValueToUint16\n-JS_ValueToUint16.restype = JSBool\n-JS_ValueToUint16.argtypes = [POINTER(JSContext),c_long,POINTER(uint16)]\n-\n-JS_ValueToBoolean = libsmjs.JS_ValueToBoolean\n-JS_ValueToBoolean.restype = JSBool\n-JS_ValueToBoolean.argtypes = [POINTER(JSContext),c_long,POINTER(JSBool)]\n-\n-JS_TypeOfValue = libsmjs.JS_TypeOfValue\n-JS_TypeOfValue.restype = JSType\n-JS_TypeOfValue.argtypes = [POINTER(JSContext),c_long]\n-\n-JS_GetTypeName = libsmjs.JS_GetTypeName\n-JS_GetTypeName.restype = c_char_p\n-JS_GetTypeName.argtypes = [POINTER(JSContext),c_int]\n-\n-JS_Init = libsmjs.JS_Init\n-JS_Init.restype = POINTER(JSRuntime)\n-JS_Init.argtypes = [c_uint]\n-\n-JS_Finish = libsmjs.JS_Finish\n-JS_Finish.restype = None\n-JS_Finish.argtypes = [POINTER(JSRuntime)]\n-\n-JS_ShutDown = libsmjs.JS_ShutDown\n-JS_ShutDown.restype = None\n-JS_ShutDown.argtypes = []\n-\n-JS_GetRuntimePrivate = libsmjs.JS_GetRuntimePrivate\n-JS_GetRuntimePrivate.restype = c_void_p\n-JS_GetRuntimePrivate.argtypes = [POINTER(JSRuntime)]\n-\n-JS_SetRuntimePrivate = libsmjs.JS_SetRuntimePrivate\n-JS_SetRuntimePrivate.restype = None\n-JS_SetRuntimePrivate.argtypes = [POINTER(JSRuntime),c_void_p]\n-\n-JS_Lock = libsmjs.JS_Lock\n-JS_Lock.restype = None\n-JS_Lock.argtypes = [POINTER(JSRuntime)]\n-\n-JS_Unlock = libsmjs.JS_Unlock\n-JS_Unlock.restype = None\n-JS_Unlock.argtypes = [POINTER(JSRuntime)]\n-\n-JS_NewContext = libsmjs.JS_NewContext\n-JS_NewContext.restype = POINTER(JSContext)\n-JS_NewContext.argtypes = [POINTER(JSRuntime),c_uint]\n-\n-JS_DestroyContext = libsmjs.JS_DestroyContext\n-JS_DestroyContext.restype = None\n-JS_DestroyContext.argtypes = [POINTER(JSContext)]\n-\n-JS_DestroyContextNoGC = libsmjs.JS_DestroyContextNoGC\n-JS_DestroyContextNoGC.restype = None\n-JS_DestroyContextNoGC.argtypes = [POINTER(JSContext)]\n-\n-JS_DestroyContextMaybeGC = libsmjs.JS_DestroyContextMaybeGC\n-JS_DestroyContextMaybeGC.restype = None\n-JS_DestroyContextMaybeGC.argtypes = [POINTER(JSContext)]\n-\n-JS_GetContextPrivate = libsmjs.JS_GetContextPrivate\n-JS_GetContextPrivate.restype = c_void_p\n-JS_GetContextPrivate.argtypes = [POINTER(JSContext)]\n-\n-JS_SetContextPrivate = libsmjs.JS_SetContextPrivate\n-JS_SetContextPrivate.restype = None\n-JS_SetContextPrivate.argtypes = [POINTER(JSContext),c_void_p]\n-\n-JS_GetRuntime = libsmjs.JS_GetRuntime\n-JS_GetRuntime.restype = POINTER(JSRuntime)\n-JS_GetRuntime.argtypes = [POINTER(JSContext)]\n-\n-JS_ContextIterator = libsmjs.JS_ContextIterator\n-JS_ContextIterator.restype = POINTER(JSContext)\n-JS_ContextIterator.argtypes = [POINTER(JSRuntime),POINTER(POINTER(JSContext))]\n-\n-JS_GetVersion = libsmjs.JS_GetVersion\n-JS_GetVersion.restype = JSVersion\n-JS_GetVersion.argtypes = [POINTER(JSContext)]\n-\n-JS_SetVersion = libsmjs.JS_SetVersion\n-JS_SetVersion.restype = JSVersion\n-JS_SetVersion.argtypes = [POINTER(JSContext),c_int]\n-\n-JS_VersionToString = libsmjs.JS_VersionToString\n-JS_VersionToString.restype = c_char_p\n-JS_VersionToString.argtypes = [c_int]\n-\n-JS_StringToVersion = libsmjs.JS_StringToVersion\n-JS_StringToVersion.restype = JSVersion\n-JS_StringToVersion.argtypes = [c_char_p]\n-\n-JS_GetOptions = libsmjs.JS_GetOptions\n-JS_GetOptions.restype = uint32\n-JS_GetOptions.argtypes = [POINTER(JSContext)]\n-\n-JS_SetOptions = libsmjs.JS_SetOptions\n-JS_SetOptions.restype = uint32\n-JS_SetOptions.argtypes = [POINTER(JSContext),c_uint]\n-\n-JS_ToggleOptions = libsmjs.JS_ToggleOptions\n-JS_ToggleOptions.restype = uint32\n-JS_ToggleOptions.argtypes = [POINTER(JSContext),c_uint]\n-\n-JS_GetImplementationVersion = libsmjs.JS_GetImplementationVersion\n-JS_GetImplementationVersion.restype = c_char_p\n-JS_GetImplementationVersion.argtypes = []\n-\n-JS_GetGlobalObject = libsmjs.JS_GetGlobalObject\n-JS_GetGlobalObject.restype = POINTER(JSObject)\n-JS_GetGlobalObject.argtypes = [POINTER(JSContext)]\n-\n-JS_SetGlobalObject = libsmjs.JS_SetGlobalObject\n-JS_SetGlobalObject.restype = None\n-JS_SetGlobalObject.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_InitStandardClasses = libsmjs.JS_InitStandardClasses\n-JS_InitStandardClasses.restype = JSBool\n-JS_InitStandardClasses.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_ResolveStandardClass = libsmjs.JS_ResolveStandardClass\n-JS_ResolveStandardClass.restype = JSBool\n-JS_ResolveStandardClass.argtypes = [POINTER(JSContext),POINTER(JSObject),c_long,POINTER(JSBool)]\n-\n-JS_EnumerateStandardClasses = libsmjs.JS_EnumerateStandardClasses\n-JS_EnumerateStandardClasses.restype = JSBool\n-JS_EnumerateStandardClasses.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_GetScopeChain = libsmjs.JS_GetScopeChain\n-JS_GetScopeChain.restype = POINTER(JSObject)\n-JS_GetScopeChain.argtypes = [POINTER(JSContext)]\n-\n-JS_malloc = libsmjs.JS_malloc\n-JS_malloc.restype = c_void_p\n-JS_malloc.argtypes = [POINTER(JSContext),c_uint]\n-\n-JS_realloc = libsmjs.JS_realloc\n-JS_realloc.restype = c_void_p\n-JS_realloc.argtypes = [POINTER(JSContext),c_void_p,c_uint]\n-\n-JS_free = libsmjs.JS_free\n-JS_free.restype = None\n-JS_free.argtypes = [POINTER(JSContext),c_void_p]\n-\n-JS_strdup = libsmjs.JS_strdup\n-JS_strdup.restype = c_char_p\n-JS_strdup.argtypes = [POINTER(JSContext),c_char_p]\n-\n-JS_NewDouble = libsmjs.JS_NewDouble\n-JS_NewDouble.restype = POINTER(jsdouble)\n-JS_NewDouble.argtypes = [POINTER(JSContext),c_double]\n-\n-JS_NewDoubleValue = libsmjs.JS_NewDoubleValue\n-JS_NewDoubleValue.restype = JSBool\n-JS_NewDoubleValue.argtypes = [POINTER(JSContext),c_double,POINTER(jsval)]\n-\n-JS_NewNumberValue = libsmjs.JS_NewNumberValue\n-JS_NewNumberValue.restype = JSBool\n-JS_NewNumberValue.argtypes = [POINTER(JSContext),c_double,POINTER(jsval)]\n-\n-JS_AddRoot = libsmjs.JS_AddRoot\n-JS_AddRoot.restype = JSBool\n-JS_AddRoot.argtypes = [POINTER(JSContext),c_void_p]\n-\n-JS_AddNamedRoot = libsmjs.JS_AddNamedRoot\n-JS_AddNamedRoot.restype = JSBool\n-JS_AddNamedRoot.argtypes = [POINTER(JSContext),c_void_p,c_char_p]\n-\n-JS_AddNamedRootRT = libsmjs.JS_AddNamedRootRT\n-JS_AddNamedRootRT.restype = JSBool\n-JS_AddNamedRootRT.argtypes = [POINTER(JSRuntime),c_void_p,c_char_p]\n-\n-JS_RemoveRoot = libsmjs.JS_RemoveRoot\n-JS_RemoveRoot.restype = JSBool\n-JS_RemoveRoot.argtypes = [POINTER(JSContext),c_void_p]\n-\n-JS_RemoveRootRT = libsmjs.JS_RemoveRootRT\n-JS_RemoveRootRT.restype = JSBool\n-JS_RemoveRootRT.argtypes = [POINTER(JSRuntime),c_void_p]\n-\n-JS_ClearNewbornRoots = libsmjs.JS_ClearNewbornRoots\n-JS_ClearNewbornRoots.restype = None\n-JS_ClearNewbornRoots.argtypes = [POINTER(JSContext)]\n-\n-JSGCRootMapFun = CFUNCTYPE(intN,c_void_p,c_char_p,c_void_p)\n-\n-JS_MapGCRoots = libsmjs.JS_MapGCRoots\n-JS_MapGCRoots.restype = uint32\n-JS_MapGCRoots.argtypes = [POINTER(JSRuntime),CFUNCTYPE(intN,c_void_p,c_char_p,c_void_p),c_void_p]\n-\n-JS_LockGCThing = libsmjs.JS_LockGCThing\n-JS_LockGCThing.restype = JSBool\n-JS_LockGCThing.argtypes = [POINTER(JSContext),c_void_p]\n-\n-JS_LockGCThingRT = libsmjs.JS_LockGCThingRT\n-JS_LockGCThingRT.restype = JSBool\n-JS_LockGCThingRT.argtypes = [POINTER(JSRuntime),c_void_p]\n-\n-JS_UnlockGCThing = libsmjs.JS_UnlockGCThing\n-JS_UnlockGCThing.restype = JSBool\n-JS_UnlockGCThing.argtypes = [POINTER(JSContext),c_void_p]\n-\n-JS_UnlockGCThingRT = libsmjs.JS_UnlockGCThingRT\n-JS_UnlockGCThingRT.restype = JSBool\n-JS_UnlockGCThingRT.argtypes = [POINTER(JSRuntime),c_void_p]\n-\n-JS_MarkGCThing = libsmjs.JS_MarkGCThing\n-JS_MarkGCThing.restype = None\n-JS_MarkGCThing.argtypes = [POINTER(JSContext),c_void_p,c_char_p,c_void_p]\n-\n-JS_GC = libsmjs.JS_GC\n-JS_GC.restype = None\n-JS_GC.argtypes = [POINTER(JSContext)]\n-\n-JS_MaybeGC = libsmjs.JS_MaybeGC\n-JS_MaybeGC.restype = None\n-JS_MaybeGC.argtypes = [POINTER(JSContext)]\n-\n-JS_SetGCCallback = libsmjs.JS_SetGCCallback\n-JS_SetGCCallback.restype = JSGCCallback\n-JS_SetGCCallback.argtypes = [POINTER(JSContext),CFUNCTYPE(JSBool,POINTER(JSContext),c_int)]\n-\n-JS_SetGCCallbackRT = libsmjs.JS_SetGCCallbackRT\n-JS_SetGCCallbackRT.restype = JSGCCallback\n-JS_SetGCCallbackRT.argtypes = [POINTER(JSRuntime),CFUNCTYPE(JSBool,POINTER(JSContext),c_int)]\n-\n-JS_IsAboutToBeFinalized = libsmjs.JS_IsAboutToBeFinalized\n-JS_IsAboutToBeFinalized.restype = JSBool\n-JS_IsAboutToBeFinalized.argtypes = [POINTER(JSContext),c_void_p]\n-\n-JS_AddExternalStringFinalizer = libsmjs.JS_AddExternalStringFinalizer\n-JS_AddExternalStringFinalizer.restype = intN\n-JS_AddExternalStringFinalizer.argtypes = [CFUNCTYPE(None,POINTER(JSContext),POINTER(JSString))]\n-\n-JS_RemoveExternalStringFinalizer = libsmjs.JS_RemoveExternalStringFinalizer\n-JS_RemoveExternalStringFinalizer.restype = intN\n-JS_RemoveExternalStringFinalizer.argtypes = [CFUNCTYPE(None,POINTER(JSContext),POINTER(JSString))]\n-\n-JS_NewExternalString = libsmjs.JS_NewExternalString\n-JS_NewExternalString.restype = POINTER(JSString)\n-JS_NewExternalString.argtypes = [POINTER(JSContext),POINTER(jschar),c_uint,c_int]\n-\n-JS_GetExternalStringGCType = libsmjs.JS_GetExternalStringGCType\n-JS_GetExternalStringGCType.restype = intN\n-JS_GetExternalStringGCType.argtypes = [POINTER(JSRuntime),POINTER(JSString)]\n-\n-JS_SetThreadStackLimit = libsmjs.JS_SetThreadStackLimit\n-JS_SetThreadStackLimit.restype = None\n-JS_SetThreadStackLimit.argtypes = [POINTER(JSContext),c_ulong]\n-\n-JS_DestroyIdArray = libsmjs.JS_DestroyIdArray\n-JS_DestroyIdArray.restype = None\n-JS_DestroyIdArray.argtypes = [POINTER(JSContext),POINTER(JSIdArray)]\n-\n-JS_ValueToId = libsmjs.JS_ValueToId\n-JS_ValueToId.restype = JSBool\n-JS_ValueToId.argtypes = [POINTER(JSContext),c_long,POINTER(jsid)]\n-\n-JS_IdToValue = libsmjs.JS_IdToValue\n-JS_IdToValue.restype = JSBool\n-JS_IdToValue.argtypes = [POINTER(JSContext),c_long,POINTER(jsval)]\n-\n-JS_PropertyStub = JSPropertyOp(libsmjs.JS_PropertyStub)\n-#JS_PropertyStub.restype = JSBool\n-#JS_PropertyStub.argtypes = [POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)]\n-\n-JS_EnumerateStub = JSEnumerateOp(libsmjs.JS_EnumerateStub)\n-#JS_EnumerateStub.restype = JSBool\n-#JS_EnumerateStub.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_ResolveStub = JSResolveOp(libsmjs.JS_ResolveStub)\n-#JS_ResolveStub.restype = JSBool\n-#JS_ResolveStub.argtypes = [POINTER(JSContext),POINTER(JSObject),c_long]\n-\n-JS_ConvertStub = JSConvertOp(libsmjs.JS_ConvertStub)\n-#JS_ConvertStub.restype = JSBool\n-#JS_ConvertStub.argtypes = [POINTER(JSContext),POINTER(JSObject),c_int,POINTER(jsval)]\n-\n-JS_FinalizeStub = JSFinalizeOp(libsmjs.JS_FinalizeStub)\n-#JS_FinalizeStub.restype = None\n-#JS_FinalizeStub.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_InitClass = libsmjs.JS_InitClass\n-JS_InitClass.restype = POINTER(JSObject)\n-JS_InitClass.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSObject),POINTER(JSClass),CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_uint,POINTER(jsval),POINTER(jsval)),c_uint,POINTER(JSPropertySpec),POINTER(JSFunctionSpec),POINTER(JSPropertySpec),POINTER(JSFunctionSpec)]\n-\n-JS_GetClass = libsmjs.JS_GetClass\n-JS_GetClass.restype = POINTER(JSClass)\n-JS_GetClass.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-#JS_GetClass.argtypes = [POINTER(JSObject)]\n-\n-JS_InstanceOf = libsmjs.JS_InstanceOf\n-JS_InstanceOf.restype = JSBool\n-JS_InstanceOf.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSClass),POINTER(jsval)]\n-\n-JS_GetPrivate = libsmjs.JS_GetPrivate\n-JS_GetPrivate.restype = c_void_p\n-JS_GetPrivate.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_SetPrivate = libsmjs.JS_SetPrivate\n-JS_SetPrivate.restype = JSBool\n-JS_SetPrivate.argtypes = [POINTER(JSContext),POINTER(JSObject),c_void_p]\n-\n-JS_GetInstancePrivate = libsmjs.JS_GetInstancePrivate\n-JS_GetInstancePrivate.restype = c_void_p\n-JS_GetInstancePrivate.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSClass),POINTER(jsval)]\n-\n-JS_GetPrototype = libsmjs.JS_GetPrototype\n-JS_GetPrototype.restype = POINTER(JSObject)\n-JS_GetPrototype.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_SetPrototype = libsmjs.JS_SetPrototype\n-JS_SetPrototype.restype = JSBool\n-JS_SetPrototype.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSObject)]\n-\n-JS_GetParent = libsmjs.JS_GetParent\n-JS_GetParent.restype = POINTER(JSObject)\n-JS_GetParent.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_SetParent = libsmjs.JS_SetParent\n-JS_SetParent.restype = JSBool\n-JS_SetParent.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSObject)]\n-\n-JS_GetConstructor = libsmjs.JS_GetConstructor\n-JS_GetConstructor.restype = POINTER(JSObject)\n-JS_GetConstructor.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_GetObjectId = libsmjs.JS_GetObjectId\n-JS_GetObjectId.restype = JSBool\n-JS_GetObjectId.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jsid)]\n-\n-JS_NewObject = libsmjs.JS_NewObject\n-JS_NewObject.restype = POINTER(JSObject)\n-JS_NewObject.argtypes = [POINTER(JSContext),POINTER(JSClass),POINTER(JSObject),POINTER(JSObject)]\n-\n-JS_SealObject = libsmjs.JS_SealObject\n-JS_SealObject.restype = JSBool\n-JS_SealObject.argtypes = [POINTER(JSContext),POINTER(JSObject),c_int]\n-\n-JS_ConstructObject = libsmjs.JS_ConstructObject\n-JS_ConstructObject.restype = POINTER(JSObject)\n-JS_ConstructObject.argtypes = [POINTER(JSContext),POINTER(JSClass),POINTER(JSObject),POINTER(JSObject)]\n-\n-JS_ConstructObjectWithArguments = libsmjs.JS_ConstructObjectWithArguments\n-JS_ConstructObjectWithArguments.restype = POINTER(JSObject)\n-JS_ConstructObjectWithArguments.argtypes = [POINTER(JSContext),POINTER(JSClass),POINTER(JSObject),POINTER(JSObject),c_uint,POINTER(jsval)]\n-\n-JS_DefineObject = libsmjs.JS_DefineObject\n-JS_DefineObject.restype = POINTER(JSObject)\n-JS_DefineObject.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,POINTER(JSClass),POINTER(JSObject),c_uint]\n-\n-JS_DefineConstDoubles = libsmjs.JS_DefineConstDoubles\n-JS_DefineConstDoubles.restype = JSBool\n-JS_DefineConstDoubles.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSConstDoubleSpec)]\n-\n-JS_DefineProperties = libsmjs.JS_DefineProperties\n-JS_DefineProperties.restype = JSBool\n-JS_DefineProperties.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSPropertySpec)]\n-\n-JS_DefineProperty = libsmjs.JS_DefineProperty\n-JS_DefineProperty.restype = JSBool\n-#JS_DefineProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_long,CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)),CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)),c_uint]\n-JS_DefineProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_long,c_void_p,c_void_p,c_uint]\n-\n-JS_GetPropertyAttributes = libsmjs.JS_GetPropertyAttributes\n-JS_GetPropertyAttributes.restype = JSBool\n-JS_GetPropertyAttributes.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,POINTER(uintN),POINTER(JSBool)]\n-\n-JS_SetPropertyAttributes = libsmjs.JS_SetPropertyAttributes\n-JS_SetPropertyAttributes.restype = JSBool\n-JS_SetPropertyAttributes.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_uint,POINTER(JSBool)]\n-\n-JS_DefinePropertyWithTinyId = libsmjs.JS_DefinePropertyWithTinyId\n-JS_DefinePropertyWithTinyId.restype = JSBool\n-JS_DefinePropertyWithTinyId.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_char,c_long,CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)),CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)),c_uint]\n-\n-JS_AliasProperty = libsmjs.JS_AliasProperty\n-JS_AliasProperty.restype = JSBool\n-JS_AliasProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_char_p]\n-\n-JS_LookupProperty = libsmjs.JS_LookupProperty\n-JS_LookupProperty.restype = JSBool\n-JS_LookupProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,POINTER(jsval)]\n-\n-JS_GetProperty = libsmjs.JS_GetProperty\n-JS_GetProperty.restype = JSBool\n-JS_GetProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,POINTER(jsval)]\n-\n-JS_SetProperty = libsmjs.JS_SetProperty\n-JS_SetProperty.restype = JSBool\n-JS_SetProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,POINTER(jsval)]\n-\n-JS_DeleteProperty = libsmjs.JS_DeleteProperty\n-JS_DeleteProperty.restype = JSBool\n-JS_DeleteProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p]\n-\n-JS_DeleteProperty2 = libsmjs.JS_DeleteProperty2\n-JS_DeleteProperty2.restype = JSBool\n-JS_DeleteProperty2.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,POINTER(jsval)]\n-\n-JS_DefineUCProperty = libsmjs.JS_DefineUCProperty\n-JS_DefineUCProperty.restype = JSBool\n-JS_DefineUCProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jschar),c_uint,c_long,CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)),CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)),c_uint]\n-\n-JS_GetUCPropertyAttributes = libsmjs.JS_GetUCPropertyAttributes\n-JS_GetUCPropertyAttributes.restype = JSBool\n-JS_GetUCPropertyAttributes.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jschar),c_uint,POINTER(uintN),POINTER(JSBool)]\n-\n-JS_SetUCPropertyAttributes = libsmjs.JS_SetUCPropertyAttributes\n-JS_SetUCPropertyAttributes.restype = JSBool\n-JS_SetUCPropertyAttributes.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jschar),c_uint,c_uint,POINTER(JSBool)]\n-\n-JS_DefineUCPropertyWithTinyId = libsmjs.JS_DefineUCPropertyWithTinyId\n-JS_DefineUCPropertyWithTinyId.restype = JSBool\n-JS_DefineUCPropertyWithTinyId.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jschar),c_uint,c_char,c_long,CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)),CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,POINTER(jsval)),c_uint]\n-\n-JS_LookupUCProperty = libsmjs.JS_LookupUCProperty\n-JS_LookupUCProperty.restype = JSBool\n-JS_LookupUCProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jschar),c_uint,POINTER(jsval)]\n-\n-JS_GetUCProperty = libsmjs.JS_GetUCProperty\n-JS_GetUCProperty.restype = JSBool\n-JS_GetUCProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jschar),c_uint,POINTER(jsval)]\n-\n-JS_SetUCProperty = libsmjs.JS_SetUCProperty\n-JS_SetUCProperty.restype = JSBool\n-JS_SetUCProperty.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jschar),c_uint,POINTER(jsval)]\n-\n-JS_DeleteUCProperty2 = libsmjs.JS_DeleteUCProperty2\n-JS_DeleteUCProperty2.restype = JSBool\n-JS_DeleteUCProperty2.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jschar),c_uint,POINTER(jsval)]\n-\n-JS_NewArrayObject = libsmjs.JS_NewArrayObject\n-JS_NewArrayObject.restype = POINTER(JSObject)\n-JS_NewArrayObject.argtypes = [POINTER(JSContext),c_int,POINTER(jsval)]\n-\n-JS_IsArrayObject = libsmjs.JS_IsArrayObject\n-JS_IsArrayObject.restype = JSBool\n-JS_IsArrayObject.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_GetArrayLength = libsmjs.JS_GetArrayLength\n-JS_GetArrayLength.restype = JSBool\n-JS_GetArrayLength.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jsuint)]\n-\n-JS_SetArrayLength = libsmjs.JS_SetArrayLength\n-JS_SetArrayLength.restype = JSBool\n-JS_SetArrayLength.argtypes = [POINTER(JSContext),POINTER(JSObject),c_uint]\n-\n-JS_HasArrayLength = libsmjs.JS_HasArrayLength\n-JS_HasArrayLength.restype = JSBool\n-JS_HasArrayLength.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jsuint)]\n-\n-JS_DefineElement = libsmjs.JS_DefineElement\n-JS_DefineElement.restype = JSBool\n-#JS_DefineElement.argtypes = [POINTER(JSContext),POINTER(JSObject),c_int,c_long,JSPropertyOp,JSPropertyOp,c_uint]\n-JS_DefineElement.argtypes = [POINTER(JSContext),POINTER(JSObject),c_int,c_long,c_void_p,c_void_p,c_uint]\n-\n-JS_AliasElement = libsmjs.JS_AliasElement\n-JS_AliasElement.restype = JSBool\n-JS_AliasElement.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_int]\n-\n-JS_LookupElement = libsmjs.JS_LookupElement\n-JS_LookupElement.restype = JSBool\n-JS_LookupElement.argtypes = [POINTER(JSContext),POINTER(JSObject),c_int,POINTER(jsval)]\n-\n-JS_GetElement = libsmjs.JS_GetElement\n-JS_GetElement.restype = JSBool\n-JS_GetElement.argtypes = [POINTER(JSContext),POINTER(JSObject),c_int,POINTER(jsval)]\n-\n-JS_SetElement = libsmjs.JS_SetElement\n-JS_SetElement.restype = JSBool\n-JS_SetElement.argtypes = [POINTER(JSContext),POINTER(JSObject),c_int,POINTER(jsval)]\n-\n-JS_DeleteElement = libsmjs.JS_DeleteElement\n-JS_DeleteElement.restype = JSBool\n-JS_DeleteElement.argtypes = [POINTER(JSContext),POINTER(JSObject),c_int]\n-\n-JS_DeleteElement2 = libsmjs.JS_DeleteElement2\n-JS_DeleteElement2.restype = JSBool\n-JS_DeleteElement2.argtypes = [POINTER(JSContext),POINTER(JSObject),c_int,POINTER(jsval)]\n-\n-JS_ClearScope = libsmjs.JS_ClearScope\n-JS_ClearScope.restype = None\n-JS_ClearScope.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_Enumerate = libsmjs.JS_Enumerate\n-JS_Enumerate.restype = POINTER(JSIdArray)\n-JS_Enumerate.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_CheckAccess = libsmjs.JS_CheckAccess\n-JS_CheckAccess.restype = JSBool\n-JS_CheckAccess.argtypes = [POINTER(JSContext),POINTER(JSObject),c_long,c_int,POINTER(jsval),POINTER(uintN)]\n-\n-JS_SetCheckObjectAccessCallback = libsmjs.JS_SetCheckObjectAccessCallback\n-JS_SetCheckObjectAccessCallback.restype = JSCheckAccessOp\n-JS_SetCheckObjectAccessCallback.argtypes = [POINTER(JSRuntime),CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_long,c_int,POINTER(jsval))]\n-\n-JS_GetReservedSlot = libsmjs.JS_GetReservedSlot\n-JS_GetReservedSlot.restype = JSBool\n-JS_GetReservedSlot.argtypes = [POINTER(JSContext),POINTER(JSObject),c_uint,POINTER(jsval)]\n-\n-JS_SetReservedSlot = libsmjs.JS_SetReservedSlot\n-JS_SetReservedSlot.restype = JSBool\n-JS_SetReservedSlot.argtypes = [POINTER(JSContext),POINTER(JSObject),c_uint,c_long]\n-\n-JS_SetPrincipalsTranscoder = libsmjs.JS_SetPrincipalsTranscoder\n-JS_SetPrincipalsTranscoder.restype = JSPrincipalsTranscoder\n-JS_SetPrincipalsTranscoder.argtypes = [POINTER(JSRuntime),CFUNCTYPE(JSBool,POINTER(JSXDRState),POINTER(POINTER(JSPrincipals)))]\n-\n-JS_SetObjectPrincipalsFinder = libsmjs.JS_SetObjectPrincipalsFinder\n-JS_SetObjectPrincipalsFinder.restype = JSObjectPrincipalsFinder\n-JS_SetObjectPrincipalsFinder.argtypes = [POINTER(JSContext),CFUNCTYPE(POINTER(JSPrincipals),POINTER(JSContext),POINTER(JSObject))]\n-\n-JS_NewFunction = libsmjs.JS_NewFunction\n-JS_NewFunction.restype = POINTER(JSFunction)\n-JS_NewFunction.argtypes = [POINTER(JSContext),CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_uint,POINTER(jsval),POINTER(jsval)),c_uint,c_uint,POINTER(JSObject),c_char_p]\n-\n-JS_GetFunctionObject = libsmjs.JS_GetFunctionObject\n-JS_GetFunctionObject.restype = POINTER(JSObject)\n-JS_GetFunctionObject.argtypes = [POINTER(JSFunction)]\n-\n-JS_GetFunctionName = libsmjs.JS_GetFunctionName\n-JS_GetFunctionName.restype = c_char_p\n-JS_GetFunctionName.argtypes = [POINTER(JSFunction)]\n-\n-JS_GetFunctionId = libsmjs.JS_GetFunctionId\n-JS_GetFunctionId.restype = POINTER(JSString)\n-JS_GetFunctionId.argtypes = [POINTER(JSFunction)]\n-\n-JS_GetFunctionFlags = libsmjs.JS_GetFunctionFlags\n-JS_GetFunctionFlags.restype = uintN\n-JS_GetFunctionFlags.argtypes = [POINTER(JSFunction)]\n-\n-JS_ObjectIsFunction = libsmjs.JS_ObjectIsFunction\n-JS_ObjectIsFunction.restype = JSBool\n-JS_ObjectIsFunction.argtypes = [POINTER(JSContext),POINTER(JSObject)]\n-\n-JS_DefineFunctions = libsmjs.JS_DefineFunctions\n-JS_DefineFunctions.restype = JSBool\n-JS_DefineFunctions.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSFunctionSpec)]\n-\n-JS_DefineFunction = libsmjs.JS_DefineFunction\n-JS_DefineFunction.restype = POINTER(JSFunction)\n-JS_DefineFunction.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSObject),c_uint,POINTER(jsval),POINTER(jsval)),c_uint,c_uint]\n-\n-JS_CloneFunctionObject = libsmjs.JS_CloneFunctionObject\n-JS_CloneFunctionObject.restype = POINTER(JSObject)\n-JS_CloneFunctionObject.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSObject)]\n-\n-JS_BufferIsCompilableUnit = libsmjs.JS_BufferIsCompilableUnit\n-JS_BufferIsCompilableUnit.restype = JSBool\n-JS_BufferIsCompilableUnit.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_uint]\n-\n-JS_CompileScript = libsmjs.JS_CompileScript\n-JS_CompileScript.restype = POINTER(JSScript)\n-JS_CompileScript.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_uint,c_char_p,c_uint]\n-\n-JS_CompileScriptForPrincipals = libsmjs.JS_CompileScriptForPrincipals\n-JS_CompileScriptForPrincipals.restype = POINTER(JSScript)\n-JS_CompileScriptForPrincipals.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSPrincipals),c_char_p,c_uint,c_char_p,c_uint]\n-\n-JS_CompileUCScript = libsmjs.JS_CompileUCScript\n-JS_CompileUCScript.restype = POINTER(JSScript)\n-JS_CompileUCScript.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jschar),c_uint,c_char_p,c_uint]\n-\n-JS_CompileUCScriptForPrincipals = libsmjs.JS_CompileUCScriptForPrincipals\n-JS_CompileUCScriptForPrincipals.restype = POINTER(JSScript)\n-JS_CompileUCScriptForPrincipals.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSPrincipals),POINTER(jschar),c_uint,c_char_p,c_uint]\n-\n-JS_CompileFile = libsmjs.JS_CompileFile\n-JS_CompileFile.restype = POINTER(JSScript)\n-JS_CompileFile.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p]\n-\n-class _IO_marker(Structure):\n-\t_fields_ = [\n-\t\t('_next', c_void_p),\n-\t\t('_sbuf', POINTER(_IO_FILE)),\n-\t\t('_pos', c_int),\n-\t]\n-\n-t__off_t = c_long\n-\n-_IO_lock_t = None\n-\n-t__quad_t = c_longlong\n-\n-t__off64_t = t__quad_t\n-\n-class _IO_FILE(Structure):\n-\t_fields_ = [\n-\t\t('_flags', c_int),\n-\t\t('_IO_read_ptr', c_char_p),\n-\t\t('_IO_read_end', c_char_p),\n-\t\t('_IO_read_base', c_char_p),\n-\t\t('_IO_write_base', c_char_p),\n-\t\t('_IO_write_ptr', c_char_p),\n-\t\t('_IO_write_end', c_char_p),\n-\t\t('_IO_buf_base', c_char_p),\n-\t\t('_IO_buf_end', c_char_p),\n-\t\t('_IO_save_base', c_char_p),\n-\t\t('_IO_backup_base', c_char_p),\n-\t\t('_IO_save_end', c_char_p),\n-\t\t('_markers', POINTER(_IO_marker)),\n-\t\t('_chain', c_void_p),\n-\t\t('_fileno', c_int),\n-\t\t('_flags2', c_int),\n-\t\t('_old_offset', t__off_t),\n-\t\t('_cur_column', c_ushort),\n-\t\t('_vtable_offset', c_char),\n-\t\t('_shortbuf', c_char*1),\n-\t\t('_lock', POINTER(_IO_lock_t)),\n-\t\t('_offset', t__off64_t),\n-\t\t('__pad1', c_void_p),\n-\t\t('__pad2', c_void_p),\n-\t\t('_mode', c_int),\n-\t\t('_unused2', c_char*52),\n-\t]\n-\n-FILE = _IO_FILE\n-\n-JS_CompileFileHandle = libsmjs.JS_CompileFileHandle\n-JS_CompileFileHandle.restype = POINTER(JSScript)\n-JS_CompileFileHandle.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,POINTER(FILE)]\n-\n-JS_CompileFileHandleForPrincipals = libsmjs.JS_CompileFileHandleForPrincipals\n-JS_CompileFileHandleForPrincipals.restype = POINTER(JSScript)\n-JS_CompileFileHandleForPrincipals.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,POINTER(FILE),POINTER(JSPrincipals)]\n-\n-JS_NewScriptObject = libsmjs.JS_NewScriptObject\n-JS_NewScriptObject.restype = POINTER(JSObject)\n-JS_NewScriptObject.argtypes = [POINTER(JSContext),POINTER(JSScript)]\n-\n-JS_GetScriptObject = libsmjs.JS_GetScriptObject\n-JS_GetScriptObject.restype = POINTER(JSObject)\n-JS_GetScriptObject.argtypes = [POINTER(JSScript)]\n-\n-JS_DestroyScript = libsmjs.JS_DestroyScript\n-JS_DestroyScript.restype = None\n-JS_DestroyScript.argtypes = [POINTER(JSContext),POINTER(JSScript)]\n-\n-JS_CompileFunction = libsmjs.JS_CompileFunction\n-JS_CompileFunction.restype = POINTER(JSFunction)\n-JS_CompileFunction.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_uint,POINTER(c_char_p),c_char_p,c_uint,c_char_p,c_uint]\n-\n-JS_CompileFunctionForPrincipals = libsmjs.JS_CompileFunctionForPrincipals\n-JS_CompileFunctionForPrincipals.restype = POINTER(JSFunction)\n-JS_CompileFunctionForPrincipals.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSPrincipals),c_char_p,c_uint,POINTER(c_char_p),c_char_p,c_uint,c_char_p,c_uint]\n-\n-JS_CompileUCFunction = libsmjs.JS_CompileUCFunction\n-JS_CompileUCFunction.restype = POINTER(JSFunction)\n-JS_CompileUCFunction.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_uint,POINTER(c_char_p),POINTER(jschar),c_uint,c_char_p,c_uint]\n-\n-JS_CompileUCFunctionForPrincipals = libsmjs.JS_CompileUCFunctionForPrincipals\n-JS_CompileUCFunctionForPrincipals.restype = POINTER(JSFunction)\n-JS_CompileUCFunctionForPrincipals.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSPrincipals),c_char_p,c_uint,POINTER(c_char_p),POINTER(jschar),c_uint,c_char_p,c_uint]\n-\n-JS_DecompileScript = libsmjs.JS_DecompileScript\n-JS_DecompileScript.restype = POINTER(JSString)\n-JS_DecompileScript.argtypes = [POINTER(JSContext),POINTER(JSScript),c_char_p,c_uint]\n-\n-JS_DecompileFunction = libsmjs.JS_DecompileFunction\n-JS_DecompileFunction.restype = POINTER(JSString)\n-JS_DecompileFunction.argtypes = [POINTER(JSContext),POINTER(JSFunction),c_uint]\n-\n-JS_DecompileFunctionBody = libsmjs.JS_DecompileFunctionBody\n-JS_DecompileFunctionBody.restype = POINTER(JSString)\n-JS_DecompileFunctionBody.argtypes = [POINTER(JSContext),POINTER(JSFunction),c_uint]\n-\n-JS_ExecuteScript = libsmjs.JS_ExecuteScript\n-JS_ExecuteScript.restype = JSBool\n-JS_ExecuteScript.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSScript),POINTER(jsval)]\n-\n-# enumeration JSExecPart\n-JSEXEC_PROLOG = 0\n-JSEXEC_MAIN = 1\n-\n-JSExecPart = c_int\n-\n-JS_ExecuteScriptPart = libsmjs.JS_ExecuteScriptPart\n-JS_ExecuteScriptPart.restype = JSBool\n-JS_ExecuteScriptPart.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSScript),c_int,POINTER(jsval)]\n-\n-JS_EvaluateScript = libsmjs.JS_EvaluateScript\n-JS_EvaluateScript.restype = JSBool\n-JS_EvaluateScript.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_uint,c_char_p,c_uint,POINTER(jsval)]\n-\n-JS_EvaluateScriptForPrincipals = libsmjs.JS_EvaluateScriptForPrincipals\n-JS_EvaluateScriptForPrincipals.restype = JSBool\n-JS_EvaluateScriptForPrincipals.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSPrincipals),c_char_p,c_uint,c_char_p,c_uint,POINTER(jsval)]\n-\n-JS_EvaluateUCScript = libsmjs.JS_EvaluateUCScript\n-JS_EvaluateUCScript.restype = JSBool\n-JS_EvaluateUCScript.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(jschar),c_uint,c_char_p,c_uint,POINTER(jsval)]\n-\n-JS_EvaluateUCScriptForPrincipals = libsmjs.JS_EvaluateUCScriptForPrincipals\n-JS_EvaluateUCScriptForPrincipals.restype = JSBool\n-JS_EvaluateUCScriptForPrincipals.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSPrincipals),POINTER(jschar),c_uint,c_char_p,c_uint,POINTER(jsval)]\n-\n-JS_CallFunction = libsmjs.JS_CallFunction\n-JS_CallFunction.restype = JSBool\n-JS_CallFunction.argtypes = [POINTER(JSContext),POINTER(JSObject),POINTER(JSFunction),c_uint,POINTER(jsval),POINTER(jsval)]\n-\n-JS_CallFunctionName = libsmjs.JS_CallFunctionName\n-JS_CallFunctionName.restype = JSBool\n-JS_CallFunctionName.argtypes = [POINTER(JSContext),POINTER(JSObject),c_char_p,c_uint,POINTER(jsval),POINTER(jsval)]\n-\n-JS_CallFunctionValue = libsmjs.JS_CallFunctionValue\n-JS_CallFunctionValue.restype = JSBool\n-JS_CallFunctionValue.argtypes = [POINTER(JSContext),POINTER(JSObject),c_long,c_uint,POINTER(jsval),POINTER(jsval)]\n-\n-JS_SetBranchCallback = libsmjs.JS_SetBranchCallback\n-JS_SetBranchCallback.restype = JSBranchCallback\n-JS_SetBranchCallback.argtypes = [POINTER(JSContext),CFUNCTYPE(JSBool,POINTER(JSContext),POINTER(JSScript))]\n-\n-JS_IsRunning = libsmjs.JS_IsRunning\n-JS_IsRunning.restype = JSBool\n-JS_IsRunning.argtypes = [POINTER(JSContext)]\n-\n-JS_IsConstructing = libsmjs.JS_IsConstructing\n-JS_IsConstructing.restype = JSBool\n-JS_IsConstructing.argtypes = [POINTER(JSContext)]\n-\n-JS_IsAssigning = libsmjs.JS_IsAssigning\n-JS_IsAssigning.restype = JSBool\n-JS_IsAssigning.argtypes = [POINTER(JSContext)]\n-\n-JS_SetCallReturnValue2 = libsmjs.JS_SetCallReturnValue2\n-JS_SetCallReturnValue2.restype = None\n-JS_SetCallReturnValue2.argtypes = [POINTER(JSContext),c_long]\n-\n-JS_NewString = libsmjs.JS_NewString\n-JS_NewString.restype = POINTER(JSString)\n-JS_NewString.argtypes = [POINTER(JSContext),c_char_p,c_uint]\n-\n-JS_NewStringCopyN = libsmjs.JS_NewStringCopyN\n-JS_NewStringCopyN.restype = POINTER(JSString)\n-JS_NewStringCopyN.argtypes = [POINTER(JSContext),c_char_p,c_uint]\n-\n-JS_NewStringCopyZ = libsmjs.JS_NewStringCopyZ\n-JS_NewStringCopyZ.restype = POINTER(JSString)\n-JS_NewStringCopyZ.argtypes = [POINTER(JSContext),c_char_p]\n-\n-JS_InternString = libsmjs.JS_InternString\n-JS_InternString.restype = POINTER(JSString)\n-JS_InternString.argtypes = [POINTER(JSContext),c_char_p]\n-\n-JS_NewUCString = libsmjs.JS_NewUCString\n-JS_NewUCString.restype = POINTER(JSString)\n-JS_NewUCString.argtypes = [POINTER(JSContext),POINTER(jschar),c_uint]\n-\n-JS_NewUCStringCopyN = libsmjs.JS_NewUCStringCopyN\n-JS_NewUCStringCopyN.restype = POINTER(JSString)\n-JS_NewUCStringCopyN.argtypes = [POINTER(JSContext),POINTER(jschar),c_uint]\n-\n-JS_NewUCStringCopyZ = libsmjs.JS_NewUCStringCopyZ\n-JS_NewUCStringCopyZ.restype = POINTER(JSString)\n-JS_NewUCStringCopyZ.argtypes = [POINTER(JSContext),POINTER(jschar)]\n-\n-JS_InternUCStringN = libsmjs.JS_InternUCStringN\n-JS_InternUCStringN.restype = POINTER(JSString)\n-JS_InternUCStringN.argtypes = [POINTER(JSContext),POINTER(jschar),c_uint]\n-\n-JS_InternUCString = libsmjs.JS_InternUCString\n-JS_InternUCString.restype = POINTER(JSString)\n-JS_InternUCString.argtypes = [POINTER(JSContext),POINTER(jschar)]\n-\n-JS_GetStringBytes = libsmjs.JS_GetStringBytes\n-JS_GetStringBytes.restype = c_char_p\n-JS_GetStringBytes.argtypes = [POINTER(JSString)]\n-\n-JS_GetStringChars = libsmjs.JS_GetStringChars\n-JS_GetStringChars.restype = POINTER(jschar)\n-JS_GetStringChars.argtypes = [POINTER(JSString)]\n-\n-JS_GetStringLength = libsmjs.JS_GetStringLength\n-JS_GetStringLength.restype = size_t\n-JS_GetStringLength.argtypes = [POINTER(JSString)]\n-\n-JS_CompareStrings = libsmjs.JS_CompareStrings\n-JS_CompareStrings.restype = intN\n-JS_CompareStrings.argtypes = [POINTER(JSString),POINTER(JSString)]\n-\n-JS_NewGrowableString = libsmjs.JS_NewGrowableString\n-JS_NewGrowableString.restype = POINTER(JSString)\n-JS_NewGrowableString.argtypes = [POINTER(JSContext),POINTER(jschar),c_uint]\n-\n-JS_NewDependentString = libsmjs.JS_NewDependentString\n-JS_NewDependentString.restype = POINTER(JSString)\n-JS_NewDependentString.argtypes = [POINTER(JSContext),POINTER(JSString),c_uint,c_uint]\n-\n-JS_ConcatStrings = libsmjs.JS_ConcatStrings\n-JS_ConcatStrings.restype = POINTER(JSString)\n-JS_ConcatStrings.argtypes = [POINTER(JSContext),POINTER(JSString),POINTER(JSString)]\n-\n-JS_UndependString = libsmjs.JS_UndependString\n-JS_UndependString.restype = POINTER(jschar)\n-JS_UndependString.argtypes = [POINTER(JSContext),POINTER(JSString)]\n-\n-JS_MakeStringImmutable = libsmjs.JS_MakeStringImmutable\n-JS_MakeStringImmutable.restype = JSBool\n-JS_MakeStringImmutable.argtypes = [POINTER(JSContext),POINTER(JSString)]\n-\n-JS_SetLocaleCallbacks = libsmjs.JS_SetLocaleCallbacks\n-JS_SetLocaleCallbacks.restype = None\n-JS_SetLocaleCallbacks.argtypes = [POINTER(JSContext),POINTER(JSLocaleCallbacks)]\n-\n-JS_GetLocaleCallbacks = libsmjs.JS_GetLocaleCallbacks\n-JS_GetLocaleCallbacks.restype = POINTER(JSLocaleCallbacks)\n-JS_GetLocaleCallbacks.argtypes = [POINTER(JSContext)]\n-\n-JS_ReportError = libsmjs.JS_ReportError\n-JS_ReportError.restype = None\n-JS_ReportError.argtypes = [POINTER(JSContext),c_char_p]\n-\n-JS_ReportErrorNumber = libsmjs.JS_ReportErrorNumber\n-JS_ReportErrorNumber.restype = None\n-JS_ReportErrorNumber.argtypes = [POINTER(JSContext),CFUNCTYPE(POINTER(JSErrorFormatString),c_void_p,c_char_p,c_uint),c_void_p,c_uint]\n-\n-JS_ReportErrorNumberUC = libsmjs.JS_ReportErrorNumberUC\n-JS_ReportErrorNumberUC.restype = None\n-JS_ReportErrorNumberUC.argtypes = [POINTER(JSContext),CFUNCTYPE(POINTER(JSErrorFormatString),c_void_p,c_char_p,c_uint),c_void_p,c_uint]\n-\n-JS_ReportWarning = libsmjs.JS_ReportWarning\n-JS_ReportWarning.restype = JSBool\n-JS_ReportWarning.argtypes = [POINTER(JSContext),c_char_p]\n-\n-JS_ReportErrorFlagsAndNumber = libsmjs.JS_ReportErrorFlagsAndNumber\n-JS_ReportErrorFlagsAndNumber.restype = JSBool\n-JS_ReportErrorFlagsAndNumber.argtypes = [POINTER(JSContext),c_uint,CFUNCTYPE(POINTER(JSErrorFormatString),c_void_p,c_char_p,c_uint),c_void_p,c_uint]\n-\n-JS_ReportErrorFlagsAndNumberUC = libsmjs.JS_ReportErrorFlagsAndNumberUC\n-JS_ReportErrorFlagsAndNumberUC.restype = JSBool\n-JS_ReportErrorFlagsAndNumberUC.argtypes = [POINTER(JSContext),c_uint,CFUNCTYPE(POINTER(JSErrorFormatString),c_void_p,c_char_p,c_uint),c_void_p,c_uint]\n-\n-JS_ReportOutOfMemory = libsmjs.JS_ReportOutOfMemory\n-JS_ReportOutOfMemory.restype = None\n-JS_ReportOutOfMemory.argtypes = [POINTER(JSContext)]\n-\n-JS_SetErrorReporter = libsmjs.JS_SetErrorReporter\n-JS_SetErrorReporter.restype = JSErrorReporter\n-JS_SetErrorReporter.argtypes = [POINTER(JSContext),CFUNCTYPE(None,POINTER(JSContext),c_char_p,POINTER(JSErrorReport))]\n-\n-JS_NewRegExpObject = libsmjs.JS_NewRegExpObject\n-JS_NewRegExpObject.restype = POINTER(JSObject)\n-JS_NewRegExpObject.argtypes = [POINTER(JSContext),c_char_p,c_uint,c_uint]\n-\n-JS_NewUCRegExpObject = libsmjs.JS_NewUCRegExpObject\n-JS_NewUCRegExpObject.restype = POINTER(JSObject)\n-JS_NewUCRegExpObject.argtypes = [POINTER(JSContext),POINTER(jschar),c_uint,c_uint]\n-\n-JS_SetRegExpInput = libsmjs.JS_SetRegExpInput\n-JS_SetRegExpInput.restype = None\n-JS_SetRegExpInput.argtypes = [POINTER(JSContext),POINTER(JSString),c_int]\n-\n-JS_ClearRegExpStatics = libsmjs.JS_ClearRegExpStatics\n-JS_ClearRegExpStatics.restype = None\n-JS_ClearRegExpStatics.argtypes = [POINTER(JSContext)]\n-\n-JS_ClearRegExpRoots = libsmjs.JS_ClearRegExpRoots\n-JS_ClearRegExpRoots.restype = None\n-JS_ClearRegExpRoots.argtypes = [POINTER(JSContext)]\n-\n-JS_IsExceptionPending = libsmjs.JS_IsExceptionPending\n-JS_IsExceptionPending.restype = JSBool\n-JS_IsExceptionPending.argtypes = [POINTER(JSContext)]\n-\n-JS_GetPendingException = libsmjs.JS_GetPendingException\n-JS_GetPendingException.restype = JSBool\n-JS_GetPendingException.argtypes = [POINTER(JSContext),POINTER(jsval)]\n-\n-JS_SetPendingException = libsmjs.JS_SetPendingException\n-JS_SetPendingException.restype = None\n-JS_SetPendingException.argtypes = [POINTER(JSContext),c_long]\n-\n-JS_ClearPendingException = libsmjs.JS_ClearPendingException\n-JS_ClearPendingException.restype = None\n-JS_ClearPendingException.argtypes = [POINTER(JSContext)]\n-\n-JS_SaveExceptionState = libsmjs.JS_SaveExceptionState\n-JS_SaveExceptionState.restype = POINTER(JSExceptionState)\n-JS_SaveExceptionState.argtypes = [POINTER(JSContext)]\n-\n-JS_RestoreExceptionState = libsmjs.JS_RestoreExceptionState\n-JS_RestoreExceptionState.restype = None\n-JS_RestoreExceptionState.argtypes = [POINTER(JSContext),POINTER(JSExceptionState)]\n-\n-JS_DropExceptionState = libsmjs.JS_DropExceptionState\n-JS_DropExceptionState.restype = None\n-JS_DropExceptionState.argtypes = [POINTER(JSContext),POINTER(JSExceptionState)]\n-\n-JS_ErrorFromException = libsmjs.JS_ErrorFromException\n-JS_ErrorFromException.restype = POINTER(JSErrorReport)\n-JS_ErrorFromException.argtypes = [POINTER(JSContext),c_long]\n-\n-JSACC_TYPEMASK = (JSACC_WRITE - 1)\n-JSLL_MAXINT = JSLL_MaxInt()\n-JSLL_MININT = JSLL_MinInt()\n-JSLL_ZERO = JSLL_Zero()\n-\n-_tabs = 0\n-\n-def _log_call(func, *args, **kargs):\n-\tglobal _tabs\n-\t_tabs += 1\n-\ttry:\n-\t\tprint '\\t'*_tabs + 'CALL {',func,args,kargs,'}'\n-\t\tr = func(*args,**kargs)\n-\t\tprint '\\t'*_tabs + \"RETURNED\",r\n-\tfinally:\n-\t\t_tabs += 0\n-\treturn r\n-\n-def log_call(func):\n-\treturn lambda *args, **kargs: _log_call(func,*args,**kargs)\n-\n-class JSError(Exception):\n-\tpass\n-\n-def dict_from_JShash(context, hash):\n-\t#~ cdef JSIdArray *prop_arr\n-\t#~ cdef int i\n-\t#~ cdef jsval jskey\n-\t#~ cdef jsval jsvalue\n-\t#~ cdef JSObject *obj\n-\n-\t\n-\tcx = context.cx\n-\n-\tprop_arr = JS_Enumerate(cx, hash).contents\n-\n-\tjskey = jsval()\n-\tjsvalue = jsval()\n-\n-\td = {}\n-\t\n-\tfor i in range(prop_arr.length):\n-\t\tvector = cast(addressof(prop_arr.vector),POINTER(c_long))\n-\t\tJS_IdToValue(cx, (vector)[i], byref(jskey))\n-\n-\t\tif JSVAL_IS_STRING(jskey):\n-\t\t\tkey = JS_GetStringBytes(JSVAL_TO_STRING(jskey))\n-\t\t\tJS_GetProperty(cx, hash, key, byref(jsvalue))\n-\t\telif JSVAL_IS_INT(jskey):\n-\t\t\tkey = JSVAL_TO_INT(jskey)\n-\t\t\tJS_GetElement(cx, hash, key, byref(jsvalue))\n-\t\telse:\n-\t\t\tassert False, \"can't happen\"\n-\n-\t\tif JSVAL_IS_PRIMITIVE(jsvalue):\n-\t\t\td[key] = Py_from_JSprimitive(jsvalue)\n-\t\telse:\n-\t\t\tif JSVAL_IS_OBJECT(jsvalue):\n-\t\t\t\tobj = JSVAL_TO_OBJECT(jsvalue)\n-\t\t\t\tif JS_IsArrayObject(cx, obj):\n-\t\t\t\t\td[key] = list_from_JSarray(context, obj)\n-\t\t\t\telse:\n-\t\t\t\t\td[key] = dict_from_JShash(context, obj)\n-\n-\tJS_DestroyIdArray(cx, prop_arr)\n-\n-\treturn d\n-\n-def list_from_JSarray(context, array):\n-\t#~ cdef int nr_elems, i\n-\t#~ cdef jsval elem\n-\t#~ cdef JSObject *jsobj\n-\n-\tnr_elems = c_uint()\t\n-\telem = jsval()\n-\n-\tcx = context.cx\n-\tJS_GetArrayLength(cx, array, byref(nr_elems))\n-\tnr_elems = nr_elems.value\n-\n-\tl = [None]*nr_elems\n-\n-\tfor i in range(nr_elems):\n-\t\tJS_GetElement(cx, array, i, byref(elem))\n-\n-\t\tif JSVAL_IS_PRIMITIVE(elem):\n-\t\t\tl[i] = Py_from_JSprimitive(elem)\n-\t\telif JSVAL_IS_OBJECT(elem):\n-\t\t\tjsobj = JSVAL_TO_OBJECT(elem)\n-\t\t\tif JS_IsArrayObject(cx, jsobj):\n-\t\t\t\tl[i] = list_from_JSarray(context, jsobj)\n-\t\t\telse:\n-\t\t\t\tl[i] = dict_from_JShash(context, jsobj)\n-\n-\treturn l\n-\n-def Py_from_JSprimitive(v):\n-\t# JS_NULL is null, JS_VOID is undefined\n-\tif JSVAL_IS_NULL(v) or JSVAL_IS_VOID(v):\n-\t\treturn None\n-\telif JSVAL_IS_INT(v):\n-\t\treturn JSVAL_TO_INT(v)\n-\telif JSVAL_IS_DOUBLE(v):\n-\t\treturn JSVAL_TO_DOUBLE(v)[0]\n-\telif JSVAL_IS_STRING(v):\n-\t\treturn JS_GetStringBytes(JSVAL_TO_STRING(v))\n-\telif JSVAL_IS_BOOLEAN(v):\n-\t\treturn bool(JSVAL_TO_BOOLEAN(v))\n-\telse:\n-\t\traise SystemError(\"unknown primitive type\")\n-\n-def Py_from_JS(context, v):\n-\t# Convert JavaScript value to equivalent Python value.\n-\t#~ cdef JSObject *object\n-\t#~ cdef ProxyObject proxy_obj\n-\t#~ cdef Context context\n-\n-\tif JSVAL_IS_PRIMITIVE(v):\n-\t\treturn Py_from_JSprimitive(v)\n-\telse:\n-\t\tif JSVAL_IS_OBJECT(v):\n-\t\t\tobject = JSVAL_TO_OBJECT(v)\n-\n-\t\t\tif JS_IsArrayObject(context.cx, object):\n-\t\t\t\treturn list_from_JSarray(context, object)\n-\t\t\telse:\n-\t\t\t\ttry:\n-\t\t\t\t\tproxy_obj = context.get_object(object)\n-\t\t\t\texcept ValueError:\n-\t\t\t\t\treturn dict_from_JShash(context, object)\n-\t\t\t\telse:\n-\t\t\t\t\treturn proxy_obj.obj\n-\n-def isinteger(x):\n-\treturn bool(compat_isinstance(x, (IntType, LongType)))\n-\n-def isfloat(x):\n-\treturn bool(compat_isinstance(x, FloatType))\n-\n-def isstringlike(x):\n-\treturn bool(compat_isinstance(x, StringTypes))\n-\n-def issequence(x):\n-\treturn bool(compat_isinstance(x, (ListType, TupleType)))\n-\n-def ismapping(x):\n-\treturn bool(compat_isinstance(x, DictType))\n-\n-def js_classname(pobj):\n-\t# class or instance?\n-\ttry:\n-\t\tklass = pobj.__class__\n-\texcept AttributeError:\n-\t\tklass = pobj\n-\n-\ttry:\n-\t\tname = klass.js_name\n-\texcept AttributeError:\n-\t\tname = klass.__name__\n-\n-\tif not isstringlike(name):\n-\t\traise AttributeError(\"%s js_name attribute is not string-like\" % klass)\n-\treturn name\n-\n-def JS_from_Py(context, parent, py_obj):\n-\t# Convert Python value to equivalent JavaScript value.\n-\t#~ cdef JSObject *jsobj\n-\t#~ cdef JSString *s\n-\n-\t#~ cdef jsval elem\n-\t#~ cdef jsval rval\n-\n-\t#~ cdef int i\n-\t#~ cdef int nr_elems\n-\t#~ cdef jsval *elems\n-\t#~ cdef JSObject *arr_obj\n-\n-\t#~ cdef Context context\n-\t#~ cdef ProxyClass proxy_class\n-\t#~ cdef ProxyObject proxy_obj\n-\n-\trval = jsval()\n-\tcx = context.cx\n-\n-\tif py_obj is None:\n-\t\treturn JSVAL_VOID\n-\telif isinteger(py_obj):\n-\t\treturn INT_TO_JSVAL(py_obj)\n-\telif isfloat(py_obj):\n-\t\td = JS_NewDouble(cx, py_obj)\n-\t\tif d == None:\n-\t\t\traise SystemError(\"can't create new double\")\n-\t\treturn DOUBLE_TO_JSVAL(d)\n-\telif isstringlike(py_obj):\n-\t\ts = JS_NewStringCopyN(cx, py_obj, len(py_obj))\n-\t\tif s == None:\n-\t\t\traise SystemError(\"can't create new string\")\n-\t\treturn STRING_TO_JSVAL(s)\n-\telif ismapping(py_obj):\n-\t\tjsobj = JS_NewObject(cx, None, None, None)\n-\n-\t\tif jsobj == None:\n-\t\t\traise SystemError(\"can't create new object\")\n-\t\telse:\n-\t\t\t# assign properties\n-\t\t\tfor key, value in py_obj.iteritems():\n-\t\t\t\telem = JS_from_Py(context, parent, value)\n-\t\t\t\tok = JS_DefineProperty(cx, jsobj, key, elem, None, None, JSPROP_ENUMERATE)\n-\t\t\t\tif not ok:\n-\t\t\t\t\traise SystemError(\"can't define property\")\n-\n-\t\t\treturn OBJECT_TO_JSVAL(jsobj)\n-\n-\telif issequence(py_obj):\n-\t\tarr_obj = JS_NewArrayObject(cx, 0, None)\n-\n-\t\tif arr_obj == None:\n-\t\t\traise SystemError(\"can't create new array object\")\n-\t\telse:\n-\t\t\tfor i in range(len(py_obj)):\n-\t\t\t\telem = JS_from_Py(context, parent, py_obj[i])\n-\t\t\t\tok = JS_DefineElement(cx, arr_obj, i, elem, None, None, JSPROP_ENUMERATE)\n-\t\t\t\tif not ok:\n-\t\t\t\t\traise SystemError(\"can't define element\")\n-\n-\t\treturn OBJECT_TO_JSVAL(arr_obj)\n-\telif compat_isinstance(py_obj, MethodType):\n-\t\t# XXX leak? -- calls JS_DefineFunction every time a method\n-\t\t#  is called\n-\t\treturn context.new_method(py_obj)\n-\telif compat_isinstance(py_obj, (FunctionType, LambdaType)):\n-\t\t# XXX implement me?\n-\t\treturn JSVAL_VOID\n-\telse:\n-\t\t# If we get here, py_obj is probably a Python class or a class instance.\n-\t\t# XXXX could problems be caused if some weird object such a module or\n-\t\t#  regexp or code object were, eg., returned by a Python function?\n-\t\t\n-\t\t# is object already bound to a JS proxy?...\n-\t\ttry:\n-\t\t\tproxy_obj = context.get_object_from_py(py_obj)\n-\t\texcept ValueError:\n-\t\t\t# ...no, so create and register a new JS proxy\n-\t\t\tproxy_class = context.get_class(js_classname(py_obj))\n-\t\t\t# XXXX I have *no idea* if using globj as the proto here is\n-\t\t\t#  correct!  If I don't put globj in here, JS does a get_property\n-\t\t\t#  on an object that hasn't been registered with the Context --\n-\t\t\t#  dunno why.\t\t\t\n-\t\t\tjsobj = proxy_class.new_object(parent=parent,py_obj = py_obj) # JS_NewObject(cx, proxy_class.jsc, context.globj, parent)\n-\t\t\tif jsobj == None:\n-\t\t\t\traise SystemError(\"couldn't look up or create new object\")\t\t\t\n-\t\telse:\n-\t\t\t# ...yes, use existing proxy\n-\t\t\tjsobj = proxy_obj.jsobj\n-\t\treturn OBJECT_TO_JSVAL(jsobj)\n-\n-class Runtime:\n-\t\n-\tdef __init__(self, memsize = (8*1024*1024), max_context = 32):\n-\n-\t\tself.rt = JS_Init(memsize)\n-\t\tself._cxs = []\n-                self._max_context = max_context\n-\n-\tdef __dealloc__(self):\n-\t\tJS_Finish(self.rt)\n-\t\t\n-\tdef new_context(self, globj = None, stacksize = 8192):\n-\t\tcontext = Context(globj, self, stacksize)\n-\t\tself._cxs.append(context)\n-                if len(self._cxs) > self._max_context:\n-                    old_cx = self._cxs.pop(0)\n-                    old_cx.alive = False\n-                    JS_DestroyContext(old_cx.cx)\n-\t\tcontext.initialize()\n-\t\treturn context\n-\n-import sys, inspect\n-\n-class ProxyObject:\n-\tdef __init__(self, obj, jsobj = None):\n-\t\tself.obj = obj\n-\t\tself.jsobj = jsobj\n-\n-def compat_isinstance(obj, tuple_or_obj):\n-\tif type(tuple_or_obj) == TupleType:\n-\t\tfor otherobj in tuple_or_obj:\n-\t\t\tif isinstance(obj, otherobj):\n-\t\t\t\treturn True\n-\t\treturn False\n-\treturn isinstance(obj, tuple_or_obj)\n-\n-from types import *\n-\n-class ProxyClass:\n-\tdef __init__(self, context, classobject, bind_constructor, is_global, flags):\n-\t\tself.context = context\n-\t\tname = js_classname(classobject)\n-\t\tself.classobject = classobject\n-\t\tself.jsclass = JSClass()\n-\t\tself.jsclass.name = name\n-\t\tself.jsclass.flags = flags\n-\t\tself.jsclass.addProperty = JS_PropertyStub\n-\t\tself.jsclass.delProperty = JS_PropertyStub\n-\t\tself.jsclass.getProperty = self.context._get_property\n-\t\tself.jsclass.setProperty = self.context._set_property\n-\t\tself.jsclass.enumerate = JS_EnumerateStub\n-\t\tif is_global:\n-\t\t\tself._resolve_global = JSResolveOp(self.resolve_global)\n-\t\t\tself.jsclass.resolve = self._resolve_global\n-\t\telse:\n-\t\t\tself.jsclass.resolve = JS_ResolveStub\n-\t\tself.jsclass.convert = JS_ConvertStub\n-\t\tself.jsclass.finalize = JS_FinalizeStub\n-\t\t\n-\t\tif bind_constructor:\n-\t\t\tself._constructor_cb = JSNative(self.constructor_cb)\n-\t\t\tres = JS_InitClass(context.cx, context._global, None, self.jsclass, self._constructor_cb, 0, None, None, None, None)\n-\t\t\tassert res, \"couldn't initialise JavaScript proxy class\"\n-\t\t\t\n-\tdef resolve_global(self, cx, obj, id):\n-\t\t#~ cdef Context context\n-\t\t#~ cdef ProxyObject proxy_obj\n-\t\t#~ cdef object thing\n-\t\t#~ cdef object key\n-\n-\t\ttry:\t\t\t\n-\t\t\tproxy_obj = self.context.get_object(obj)\n-\t\t\tthing = proxy_obj.obj\n-\t\t\tkey = Py_from_JS(self.context, id)\n-\t\t\tif type(key) == str and hasattr(thing, key):\n-\t\t\t\tattr = getattr(thing, key)\n-\t\t\t\tif compat_isinstance(attr, MethodType):\n-\t\t\t\t\tself.context.bind_callable(key, attr)\n-\t\t\t\telse:\n-\t\t\t\t\tself.context.bind_attribute(key, thing, key)\n-\t\t\treturn JS_TRUE\n-\t\texcept:\n-\t\t\treturn self.context.report()\n-\t\treturn JS_FALSE\n-\t\t\t\t\n-\tdef constructor_cb(self, cx, obj, argc, argv, rval):\n-\t\t#~ cdef ProxyClass proxy_class\n-\t\t#~ cdef Context context\n-\t\t#~ cdef char *fname\n-\t\t#~ cdef JSFunction *func\n-\t\t#~ cdef int arg\n-\t\t#~ cdef ProxyObject object\n-\t\tfunc = JS_ValueToFunction(cx, argv[-2])\n-\t\tif not func:\n-\t\t\tmsg = \"couldn't get JS constructor\"\n-\t\t\tJS_ReportError(cx, msg)\n-\t\t\treturn JS_FALSE\n-\t\tfname = JS_GetFunctionName(func)\n-\t\ttry:\n-\t\t\targs = []\n-\t\t\tfor arg in range(argc):\n-\t\t\t\targs.append(Py_from_JS(cx, argv[arg]))\n-\t\t\tif hasattr(self.classobject, \"js_constructor\"):\n-\t\t\t\tpy_rval = self.classobject.js_constructor[0](self.context, *args)\n-\t\t\telse:\n-\t\t\t\tpy_rval = self.classobject(*args)\n-\t\t\tself.context.register_object(py_rval, obj)\n-\t\texcept:\n-\t\t\tself.context.report()\n-\n-\t\treturn JS_TRUE\n-\n-\tdef new_object(self, parent = None, py_obj = None):\n-\t\tproto = self.context._global\n-\t\tjsobj = JS_NewObject(self.context.cx, byref(self.jsclass), proto, parent)\n-\t\tif py_obj:\n-\t\t\tself.context.register_object(py_obj, jsobj)\n-\t\treturn jsobj\n-\n-stderr = sys.stderr\n-\n-class DestroyedContext(Exception):\n-    def __str__(self):\n-        return \"Context Memory has been released previously by Runtime Class sanity checks.\"\n-\n-class Context:\n-\tcx = None\n-\t\n-\tclass global_class:\n-\t\tjs_name = 'global'\n-\t\t\n-\t_global = None\n-\t\n-\tdef __init__(self, globj, runtime, stacksize):\n-\t\tself.cx = JS_NewContext(runtime.rt, stacksize)\n-                self.alive = True\n-\t\tself.classes = []\n-\t\tself.objects = []\n-\t\tself.funcs = []\n-\t\t#self.runtime = runtime\n-\t\tself._get_property = JSPropertyOp(self.get_property)\n-\t\tself._set_property = JSPropertyOp(self.set_property)\n-\t\tself._bound_method_cb = JSNative(self.bound_method_cb)\n-\t\tif not globj:\n-\t\t\tglobj = self.global_class()\n-\t\tglobal_class = self.bind_class(globj.__class__, bind_constructor = False, is_global = True)\n-\t\tself._global = global_class.new_object(py_obj = globj)\n-\t\tassert self._global\n-\t\t\n-\tdef get_object_from_py(self, py_obj):\n-\t\tfor proxy_obj in self.objects:\n-\t\t\tif proxy_obj.obj == py_obj:\n-\t\t\t\treturn proxy_obj\n-\t\traise ValueError, py_obj\n-\t\t\n-\tdef get_object(self, jsobj):\n-\t\tfor proxy_obj in self.objects:\n-\t\t\tif addressof(proxy_obj.jsobj.contents) == addressof(jsobj.contents):\n-\t\t\t\treturn proxy_obj\n-\t\traise ValueError, jsobj.contents\n-\t\t\n-\tdef get_class(self, name):\n-\t\t#cdef ProxyClass cl\n-\t\tfor cl in self.classes:\n-\t\t\tif cl.jsclass.name == name:\n-\t\t\t\treturn cl\n-\t\traise ValueError(\"no class named '%s' is bound\" % name)\n-\t\t\n-\tdef get_global(self, name):\n-\t\t\"\"\"Only works with undotted names.  Use eval_script for anything\n-\t\telse.\"\"\"\n-\t\t# XXXX probably best to get rid of this -- eval_script does the job\n-\t\t#cdef jsval jsv\n-\t\tif not isstringlike(name):\n-\t\t\traise TypeError(\"name must be string-like\")\n-\t\tjsv = jsval()\n-\t\tif not JS_GetProperty(self.cx, self._global, name, byref(jsv)):\n-\t\t\traise SystemError(\"can't get JavaScript property\")\n-\t\tval = Py_from_JS(self, jsv)\n-\t\tif val is None:\n-\t\t\traise ValueError(\"no global named '%s'\" % name)\n-\t\t\t##         # XXXX why does this hang??\n-\t\t\t##             raise AttributeError(\"%s instance has no attribute '%s'\" %\n-\t\t\t##                                  (self.__class__.__name__, name))\n-\t\telse:\n-\t\t\treturn val\n-\n-\tdef register_object(self, obj, jsobj):\t\t\n-\t\tself.objects.append(ProxyObject(obj, jsobj))\n-\t\t\n-\tdef report(self):\n-\t\timport traceback\n-\t\tself.report_error(traceback.format_exc())\n-\t\t\n-\tdef report_error(self, msg, *args):\n-\t\tJS_ReportError(self.cx, msg, *args)\n-\t\t\n-\tdef error_reporter(self, cx, text, er):\n-\t\tprint >> stderr, text\n-\t\t\n-\t#def __del__(self):\n-\t\t#JS_DestroyContext(self.cx)\n-\n-\tdef new_method(self, py_method):\n-\t\t#~ cdef JSFunction *method\n-\t\t#~ cdef JSObject *method_obj\n-\n-\t\t# Method (hence its func_name attribute) will go away only when the\n-\t\t# context is GC'd by Python, because the Python function object is\n-\t\t# stored in Context extension type instance.  XXX erm, no it isn't:\n-\t\t# the class instance is kept in the ProxyClass, but the instance's\n-\t\t# function is not.  Hmm...  XXX also, JS objects exist in runtime,\n-\t\t# not in context!\n-\t\tmethod = JS_NewFunction(self.cx, self._bound_method_cb, 0, 0, None, py_method.func_name)\n-\t\tmethod_obj = JS_GetFunctionObject(method)\n-\t\treturn OBJECT_TO_JSVAL(method_obj)\n-\t\t\n-\tdef get_property(self, cx, obj, id, vp):\t\t\n-\t\t#~ cdef Context context\n-\t\t#~ cdef ProxyObject proxy_obj\n-\t\t#~ cdef object thing\n-\t\t#~ cdef object key\n-\n-\t\ttry:\n-\t\t\tkey = Py_from_JS(self, id)\n-\t\t\tproxy_obj = self.get_object(obj)\n-\t\t\tthing = proxy_obj.obj\n-\t\t\tif type(key) == IntType:\n-\t\t\t\ttry:\n-\t\t\t\t\tattr = thing[key]\n-\t\t\t\texcept:\n-\t\t\t\t\tpass\n-\t\t\t\telse:\n-\t\t\t\t\tvp[0] = JS_from_Py(self, obj, attr)\n-\t\t\telif type(key) == StringType:\n-\t\t\t\tif not key.startswith(\"_\"):\n-\t\t\t\t\ttry:\n-\t\t\t\t\t\tattr = getattr(thing, key)\n-\t\t\t\t\texcept:\n-\t\t\t\t\t\tpass\n-\t\t\t\t\telse:\n-\t\t\t\t\t\tvp[0] = JS_from_Py(self, obj, attr)\n-\t\t\telse:\n-\t\t\t\t#in original version: assert False\n-                                return JS_FALSE\n-\t\t\treturn JS_TRUE\n-\t\texcept:\n-\t\t\tself.report()\n-\t\treturn JS_FALSE\n-\t\t\n-\tdef set_property(self, cx, obj, id, vp):\n-\t\t#~ cdef Context context\n-\t\t#~ cdef ProxyObject proxy_obj\n-\t\t#~ cdef object thing\n-\t\t#~ cdef object key\n-\t\t#~ cdef object value\n-\t\t#~ cdef object attr\n-\n-\t\ttry:\n-\t\t\tproxy_obj = self.get_object(obj)\n-\n-\t\t\tthing = proxy_obj.obj\n-\t\t\tkey = Py_from_JS(self, id)\n-\t\t\tvalue = Py_from_JS(self, vp[0])\n-\t\t\tif type(key) == IntType:\n-\t\t\t\ttry:\n-\t\t\t\t\tthing[key] = value\n-\t\t\t\texcept:\n-\t\t\t\t\tpass\n-\t\t\telif type(key) == StringType:\n-\t\t\t\tif hasattr(thing, key) and not key.startswith(\"_\"):\n-\t\t\t\t\tattr = getattr(thing, key)\n-\t\t\t\t\tif not callable(attr):\n-\t\t\t\t\t\ttry:\n-\t\t\t\t\t\t\tsetattr(thing, key, value)\n-\t\t\t\t\t\texcept:\n-\t\t\t\t\t\t\tpass\n-\t\t\telse:\n-\t\t\t\tassert False\n-\t\t\treturn JS_TRUE\n-\t\texcept:\n-\t\t\treturn self.report()\n-\n-\t\treturn JS_FALSE\n-\t\t\n-\tdef bound_method_cb(self, cx, obj, argc, argv, rval):\n-\t\t#~ cdef Context context\n-\t\t#~ cdef JSFunction *func\n-\t\t#~ cdef int arg\n-\t\t#~ cdef JSClass *jsclass\n-\t\t#~ cdef ProxyObject proxy_obj\n-\n-\t\tfunc = JS_ValueToFunction(cx, argv[-2])\n-\t\tmethod_name = JS_GetFunctionName(func)\n-\t\tjsclass = JS_GetClass(self.cx, obj)\n-\n-\t\ttry:\n-\t\t\tproxy_obj = self.get_object(obj)\n-\n-\t\t\tpy_args = []\n-\t\t\tfor arg in range(argc):\n-\t\t\t\tpy_args.append(Py_from_JS(self, argv[arg]))\n-\t\t\tmeth = getattr(proxy_obj.obj, method_name)\n-\t\t\tpy_rval = meth(*py_args)\n-\t\t\trval[0] = JS_from_Py(self, obj, py_rval)\n-\t\texcept:\n-\t\t\treturn self.report()\n-\t\treturn JS_TRUE\n-\n-\tdef get_callback_fn(self, name):\n-\t\tfor cbname, fn in self.funcs:\n-\t\t\tif cbname == name:\n-\t\t\t\treturn fn\n-\t\traise ValueError(\"no callback function named '%s' is bound\" % name)\n-\t\t\t\n-\tdef function_cb(self, cx, obj, argc, argv, rval):\n-\t\t#~ cdef JSFunction *jsfunc\n-\t\t#~ cdef int i\n-\t\t#~ cdef char *name\n-\n-\t\t# XXX is this argv[-2] documented anywhere??\n-\t\tjsfunc = JS_ValueToFunction(cx, argv[-2])\n-\t\tif not jsfunc:\n-\t\t\treturn JS_FALSE\n-\t\tname = JS_GetFunctionName(jsfunc)\n-\t\tif not name:\n-\t\t\treturn JS_FALSE\n-\n-\t\ttry:\n-\t\t\tcallback = self.get_callback_fn(name)\n-\t\t\targs = []\n-\t\t\tfor i in range(argc):\n-\t\t\t\targs.append(Py_from_JS(self, argv[i]))\n-\t\t\tpyrval = callback(*args)\n-\n-\t\t\t# XXX shouldn't NULL be the JSObject if it's a Python class instance?\n-\t\t\trval[0] = JS_from_Py(self, None, pyrval)\n-\t\texcept:\n-\t\t\treturn self.report()\n-\n-\t\treturn JS_TRUE\n-\t\t\n-\tdef bind_callable(self, name, function):\n-\t\tif not callable(function):\n-\t\t\traise ValueError(\"not a callable object\")\n-\t\tself._function_cb = JSNative(self.function_cb)\n-\t\tself.funcs.append((name, function))\n-\t\tJS_DefineFunction(self.cx, self._global, name, self._function_cb, 0, 0)\n-\n-\tdef bind_attribute(self, name, obj, attr_name):\n-\t\t#cdef jsval initial_value\n-\t\tif not isstringlike(name):\n-\t\t\traise TypeError(\"name must be string-like\")\n-\t\tif not isstringlike(attr_name):\n-\t\t\traise TypeError(\"name must be string-like\")\n-\t\tinitial_value = JS_from_Py(self, None, getattr(obj, name))\n-\t\tJS_DefineProperty(self.cx, self._global, name, initial_value, cast(self._get_property,c_void_p), cast(self._set_property,c_void_p), 0)\n-\n-\tdef bind_class(self, classobject, bind_constructor = True, is_global = False, flags = 0):\n-\t\tif not inspect.isclass(classobject): raise TypeError(\"klass must be a class\")\n-\t\tif not isinteger(flags): raise TypeError(\"flags must be an integer\")\n-\t\t\n-\t\tproxy_class = ProxyClass(self, classobject, bind_constructor, is_global, flags)\n-\t\tself.classes.append(proxy_class)\n-\t\treturn proxy_class\n-\t\t\n-\tdef bind_object(self, name, obj):\n-\t\t#~ cdef JSObject *jsobj\n-\t\t#~ cdef ProxyClass proxy_class\n-\n-\t\tif not isstringlike(name):\n-\t\t\traise TypeError(\"name must be string-like\")\n-\n-\t\tproxy_class = self.get_class(js_classname(obj))\n-\t\tjsobj = JS_DefineObject(self.cx, self._global, name, proxy_class.jsclass, None, JSPROP_READONLY)\n-\t\tif jsobj:\n-\t\t\tself.register_object(obj, jsobj)\n-\t\telse:\n-\t\t\traise ValueError(\"failed to bind Python object %s\" % obj)\n-\n-\tdef initialize(self):\n-\t\tJS_InitStandardClasses(self.cx, self._global)\n-\t\tself._error_reporter = JSErrorReporter(self.error_reporter)\n-\t\tJS_SetErrorReporter(self.cx, self._error_reporter)\n-\t\t\t\n-\tdef eval_script(self, script):\n-\t\tif not isstringlike(script):\n-\t\t\traise TypeError(\"name must be string-like\")\n-\t\tif not self.alive:\n-                    raise DestroyedContext\n-\t\trval = jsval()\n-\t\tif not JS_EvaluateScript(self.cx, self._global, script, len(script), 'Python', 0, byref(rval)):\n-\t\t\traise JSError, \"Failed to execute script.\"\n-\t\t\n-\t\tretval = Py_from_JS(self, rval)\n-\t\tJS_MaybeGC(self.cx)\n-\t\treturn retval\n-\n-if __name__ == '__main__':\n-\trt = Runtime()\n-\tcx = rt.new_context()\n-\n-\tclass foo:\n-\t\tdef test(self):\n-\t\t\treturn 1.0\n-\n-\tcx.bind_class(foo, bind_constructor=True)\n-\tf = cx.eval_script(\"\"\"var f = new foo();\n-\tf.test(); // script return value\n-\t\"\"\")\n-\tprint f\n\n@@ -1,13 +0,0 @@\n-\"\"\"\n-The scrapy.xpath module provides useful classes for parsing HTML and XML\n-documents using XPath. It requires libxml2 and its python bindings.\n-\n-This parent module exports the classes most commonly used when building\n-spiders, for convenience.\n-\n-* XPath - a simple class to represent a XPath expression\n-* XPathSelector - to extract data using XPaths (parses the entire response)\n-\"\"\"\n-\n-from scrapy.xpath.types import XPath\n-from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n\n@@ -1,22 +0,0 @@\n-\"\"\"\n-This module contains a simple class (Libxml2Document) to wrap libxml2 documents\n-(xmlDoc) for proper garbage collection.\n-\"\"\"\n-\n-from scrapy.xpath.constructors import xmlDoc_from_html\n-\n-class Libxml2Document(object):\n-\n-    def __init__(self, response, constructor=xmlDoc_from_html):\n-        self.xmlDoc = constructor(response)\n-        self.xpathContext = self.xmlDoc.xpathNewContext()\n-\n-    def __del__(self):\n-        if hasattr(self, 'xmlDoc'):\n-            self.xmlDoc.freeDoc()\n-        if hasattr(self, 'xpathContext'):\n-            self.xpathContext.xpathFreeContext()\n-\n-    def __str__(self):\n-        return \"<Libxml2Document %s>\" % self.xmlDoc.name\n-\n\n@@ -1,11 +0,0 @@\n-class XPath(object):\n-    \"\"\"A XPath expression\"\"\"\n-\n-    def __init__(self, xpath_expr):\n-        self.expr = xpath_expr\n-\n-    def __repr__(self):\n-        return \"XPath(%s)\" % repr(self.expr)\n-\n-    def __str__(self):\n-        return self.expr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@@ -1,9 +0,0 @@\n-from django.conf.urls.defaults import *\n-\n-from scrapyorg.article.views import *\n-\n-\n-urlpatterns = patterns('',\n-    (r\"^$\", render_template, { \"path\": \"home\" }),\n-    (r\"(?P<path>.*)/\", render_template),\n-)\n\n@@ -1,21 +0,0 @@\n-from os.path import join\n-\n-from django.template import TemplateDoesNotExist\n-from django.template.context import RequestContext\n-from django.shortcuts import render_to_response\n-from django.http import Http404\n-\n-\n-ARTICLES_TEMPLATES_DIR = \"articles\"\n-\n-\n-def render_template(request, path):\n-    if not path.endswith(\".html\"):\n-        path = path + \".html\"\n-    path = join(ARTICLES_TEMPLATES_DIR, path)\n-\n-    try:\n-        c = RequestContext(request)\n-        return render_to_response(path, context_instance=c)\n-    except TemplateDoesNotExist, e:\n-        raise Http404(\"Article does not exists\")\n\n\n@@ -1,17 +0,0 @@\n-from django.contrib import admin\n-from scrapyorg.blog.models import *\n-\n-\n-class CategoryAdmin(admin.ModelAdmin):\n-    prepopulated_fields = {'slug': ('title',)}\n-\n-admin.site.register(Category, CategoryAdmin)\n-\n-\n-class PostAdmin(admin.ModelAdmin):\n-    list_display  = ('title', 'publish', 'status')\n-    list_filter   = ('publish', 'categories', 'status')\n-    search_fields = ('title', 'body')\n-    prepopulated_fields = {'slug': ('title',)}\n-\n-admin.site.register(Post, PostAdmin)\n\\ No newline at end of file\n\n@@ -1,42 +0,0 @@\n-from django.contrib.syndication.feeds import FeedDoesNotExist\n-from django.core.exceptions import ObjectDoesNotExist\n-from django.contrib.sites.models import Site\n-from django.contrib.syndication.feeds import Feed\n-from django.core.urlresolvers import reverse\n-from scrapyorg.blog.models import Post, Category\n-\n-\n-class BlogPostsFeed(Feed):\n-    _site = Site.objects.get_current()\n-    title = '%s feed' % _site.name\n-    description = '%s posts feed.' % _site.name\n-\n-    def link(self):\n-        return reverse('blog_index')\n-\n-    def items(self):\n-        return Post.objects.published()[:10]\n-\n-    def item_pubdate(self, obj):\n-        return obj.publish\n-\n-\n-class BlogPostsByCategory(Feed):\n-    _site = Site.objects.get_current()\n-    title = '%s posts category feed' % _site.name\n-    \n-    def get_object(self, bits):\n-        if len(bits) != 1:\n-            raise ObjectDoesNotExist\n-        return Category.objects.get(slug__exact=bits[0])\n-\n-    def link(self, obj):\n-        if not obj:\n-            raise FeedDoesNotExist\n-        return obj.get_absolute_url()\n-\n-    def description(self, obj):\n-        return \"Posts recently categorized as %s\" % obj.title\n-    \n-    def items(self, obj):\n-        return obj.post_set.published()[:10]\n\\ No newline at end of file\n\n@@ -1,9 +0,0 @@\n-from django.db.models import Manager\n-import datetime\n-\n-\n-class PublicManager(Manager):\n-    \"\"\"Returns published posts that are not in the future.\"\"\"\n-    \n-    def published(self):\n-        return self.get_query_set().filter(status__gte=2, publish__lte=datetime.datetime.now())\n\\ No newline at end of file\n\n@@ -1,80 +0,0 @@\n-from django.db import models\n-from django.utils.translation import ugettext_lazy as _\n-from django.db.models import permalink\n-from django.contrib.auth.models import User\n-#from tagging.fields import TagField\n-from scrapyorg.blog.managers import PublicManager\n-\n-#import tagging\n-\n-\n-class Category(models.Model):\n-    \"\"\"Category model.\"\"\"\n-    title       = models.CharField(_('title'), max_length=100)\n-    slug        = models.SlugField(_('slug'), unique=True)\n-\n-    class Meta:\n-        verbose_name = _('category')\n-        verbose_name_plural = _('categories')\n-        db_table = 'blog_categories'\n-        ordering = ('title',)\n-\n-    class Admin:\n-        pass\n-\n-    def __unicode__(self):\n-        return u'%s' % self.title\n-\n-    @permalink\n-    def get_absolute_url(self):\n-        return ('blog_category_detail', None, {'slug': self.slug})\n-\n-\n-class Post(models.Model):\n-    \"\"\"Post model.\"\"\"\n-    STATUS_CHOICES = (\n-        (1, _('Draft')),\n-        (2, _('Public')),\n-    )\n-    title           = models.CharField(_('title'), max_length=200)\n-    slug            = models.SlugField(_('slug'), unique_for_date='publish')\n-    author          = models.ForeignKey(User, blank=True, null=True)\n-    body            = models.TextField(_('body'))\n-    tease           = models.TextField(_('tease'), blank=True)\n-    status          = models.IntegerField(_('status'), choices=STATUS_CHOICES, default=2)\n-    publish         = models.DateTimeField(_('publish'))\n-    created         = models.DateTimeField(_('created'), auto_now_add=True)\n-    modified        = models.DateTimeField(_('modified'), auto_now=True)\n-    categories      = models.ManyToManyField(Category, blank=True)\n-#    tags            = TagField()\n-    objects         = PublicManager()\n-\n-    class Meta:\n-        verbose_name = _('post')\n-        verbose_name_plural = _('posts')\n-        db_table  = 'blog_posts'\n-        ordering  = ('-publish',)\n-        get_latest_by = 'publish'\n-\n-    class Admin:\n-        list_display  = ('title', 'publish', 'status')\n-        list_filter   = ('publish', 'categories', 'status')\n-        search_fields = ('title', 'body')\n-\n-    def __unicode__(self):\n-        return u'%s' % self.title\n-\n-    @permalink\n-    def get_absolute_url(self):\n-        return ('blog_detail', None, {\n-            'year': self.publish.year,\n-            'month': self.publish.strftime('%b').lower(),\n-            'day': self.publish.day,\n-            'slug': self.slug\n-        })\n-    \n-    def get_previous_post(self):\n-        return self.get_previous_by_publish(status__gte=2)\n-    \n-    def get_next_post(self):\n-        return self.get_next_by_publish(status__gte=2)\n\n@@ -1,13 +0,0 @@\n-from django.contrib.sitemaps import Sitemap\n-from scrapyorg.blog.models import Post\n-\n-\n-class BlogSitemap(Sitemap):\n-    changefreq = \"never\"\n-    priority = 0.5\n-\n-    def items(self):\n-        return Post.objects.published()\n-\n-        def lastmod(self, obj):\n-            return obj.publish\n\\ No newline at end of file\n\n\n@@ -1,103 +0,0 @@\n-from django import template\n-from django.conf import settings\n-from django.db import models\n-\n-import re\n-\n-Post = models.get_model('blog', 'post')\n-Category = models.get_model('blog', 'category')\n-\n-register = template.Library()\n-\n-\n-class LatestPosts(template.Node):\n-    def __init__(self, limit, var_name):\n-        self.limit = limit\n-        self.var_name = var_name\n-\n-    def render(self, context):\n-        posts = Post.objects.published()[:int(self.limit)]\n-        if posts and (int(self.limit) == 1):\n-            context[self.var_name] = posts[0]\n-        else:\n-            context[self.var_name] = posts\n-        return ''\n-\n-@register.tag\n-def get_latest_posts(parser, token):\n-    \"\"\"\n-    Gets any number of latest posts and stores them in a varable.\n-\n-    Syntax::\n-\n-        {% get_latest_posts [limit] as [var_name] %}\n-\n-    Example usage::\n-\n-        {% get_latest_posts 10 as latest_post_list %}\n-    \"\"\"\n-    try:\n-        tag_name, arg = token.contents.split(None, 1)\n-    except ValueError:\n-        raise template.TemplateSyntaxError, \"%s tag requires arguments\" % token.contents.split()[0]\n-    m = re.search(r'(.*?) as (\\w+)', arg)\n-    if not m:\n-        raise template.TemplateSyntaxError, \"%s tag had invalid arguments\" % tag_name\n-    format_string, var_name = m.groups()\n-    return LatestPosts(format_string, var_name)\n-\n-\n-class BlogCategories(template.Node):\n-    def __init__(self, var_name):\n-        self.var_name = var_name\n-\n-    def render(self, context):\n-        categories = Category.objects.all()\n-        context[self.var_name] = categories\n-        return ''\n-\n-@register.tag\n-def get_blog_categories(parser, token):\n-    \"\"\"\n-    Gets all blog categories.\n-\n-    Syntax::\n-\n-        {% get_blog_categories as [var_name] %}\n-\n-    Example usage::\n-\n-        {% get_blog_categories as category_list %}\n-    \"\"\"\n-    try:\n-        tag_name, arg = token.contents.split(None, 1)\n-    except ValueError:\n-        raise template.TemplateSyntaxError, \"%s tag requires arguments\" % token.contents.split()[0]\n-    m = re.search(r'as (\\w+)', arg)\n-    if not m:\n-        raise template.TemplateSyntaxError, \"%s tag had invalid arguments\" % tag_name\n-    var_name = m.groups()[0]\n-    return BlogCategories(var_name)\n-\n-\n-@register.filter\n-def get_links(value):\n-    \"\"\"\n-    Extracts links from a ``Post`` body and returns a list.\n-\n-    Template Syntax::\n-\n-        {{ post.body|markdown:\"safe\"|get_links }}\n-\n-    \"\"\"\n-    try:\n-        try:\n-            from BeautifulSoup import BeautifulSoup\n-        except ImportError:\n-            from beautifulsoup import BeautifulSoup\n-        soup = BeautifulSoup(value)\n-        return soup.findAll('a')\n-    except ImportError:\n-        if settings.DEBUG:\n-            raise template.TemplateSyntaxError, \"Error in 'get_links' filter: BeautifulSoup isn't installed.\"\n-    return value\n\n@@ -1,66 +0,0 @@\n-\"\"\"\n->>> from django.test import Client\n->>> from scrapyorg.blog.models import Post, Category\n->>> import datetime\n->>> from django.core.urlresolvers import reverse\n->>> client = Client()\n-\n->>> category = Category(title='Django', slug='django')\n->>> category.save()\n->>> category2 = Category(title='Rails', slug='rails')\n->>> category2.save()\n-\n->>> post = Post(title='DJ Ango', slug='dj-ango', body='Yo DJ! Turn that music up!', status=2, publish=datetime.datetime(2008,5,5,16,20))\n->>> post.save()\n-\n->>> post2 = Post(title='Where my grails at?', slug='where', body='I Can haz Holy plez?', status=2, publish=datetime.datetime(2008,4,2,11,11))\n->>> post2.save()\n-\n->>> post.categories.add(category)\n->>> post2.categories.add(category2)\n-\n->>> response = client.get(reverse('blog_index'))\n->>> response.context[-1]['object_list']\n-[<Post: DJ Ango>, <Post: Where my grails at?>]\n->>> response.status_code\n-200\n-\n->>> response = client.get(reverse('blog_category_list'))\n->>> response.context[-1]['object_list']\n-[<Category: Django>, <Category: Rails>]\n->>> response.status_code\n-200\n-\n->>> response = client.get(category.get_absolute_url())\n->>> response.context[-1]['object_list']\n-[<Post: DJ Ango>]\n->>> response.status_code\n-200\n-\n->>> response = client.get(post.get_absolute_url())\n->>> response.context[-1]['object']\n-<Post: DJ Ango>\n->>> response.status_code\n-200\n-\n->>> response = client.get(reverse('blog_search'), {'q': 'DJ'})\n->>> response.context[-1]['object_list']\n-[<Post: DJ Ango>]\n->>> response.status_code\n-200\n->>> response = client.get(reverse('blog_search'), {'q': 'Holy'})\n->>> response.context[-1]['object_list']\n-[<Post: Where my grails at?>]\n->>> response.status_code\n-200\n->>> response = client.get(reverse('blog_search'), {'q': ''})\n->>> response.context[-1]['message']\n-'Search term was too vague. Please try again.'\n-\n->>> response = client.get(reverse('blog_detail', args=[2008, 'apr', 2, 'where']))\n->>> response.context[-1]['object']\n-<Post: Where my grails at?>\n->>> response.status_code\n-200\n-\"\"\"\n-\n\n@@ -1,41 +0,0 @@\n-from django.conf.urls.defaults import *\n-from scrapyorg.blog import views as blog_views\n-\n-\n-urlpatterns = patterns('',\n-    url(r'^(?P<year>\\d{4})/(?P<month>\\w{3})/(?P<day>\\d{1,2})/(?P<slug>[-\\w]+)/$',\n-        view=blog_views.post_detail,\n-        name='blog_detail'),\n-\n-    url(r'^(?P<year>\\d{4})/(?P<month>\\w{3})/(?P<day>\\d{1,2})/$',\n-        view=blog_views.post_archive_day,\n-        name='blog_archive_day'),\n-\n-    url(r'^(?P<year>\\d{4})/(?P<month>\\w{3})/$',\n-        view=blog_views.post_archive_month,\n-        name='blog_archive_month'),\n-\n-    url(r'^(?P<year>\\d{4})/$',\n-        view=blog_views.post_archive_year,\n-        name='blog_archive_year'),\n-\n-    url(r'^categories/(?P<slug>[-\\w]+)/$',\n-        view=blog_views.category_detail,\n-        name='blog_category_detail'),\n-\n-    url (r'^categories/$',\n-        view=blog_views.category_list,\n-        name='blog_category_list'),\n-\n-    url (r'^search/$',\n-        view=blog_views.search,\n-        name='blog_search'),\n-\n-    url(r'^page/(?P<page>\\w)/$',\n-        view=blog_views.post_list,\n-        name='blog_index_paginated'),\n-\n-    url(r'^$',\n-        view=blog_views.post_list,\n-        name='blog_index'),\n-)\n\\ No newline at end of file\n\n@@ -1,160 +0,0 @@\n-from django.shortcuts import render_to_response, get_object_or_404\n-from django.template import RequestContext\n-from django.http import Http404\n-from django.views.generic import date_based, list_detail\n-from django.db.models import Q\n-from scrapyorg.blog.models import *\n-\n-import datetime\n-import re\n-\n-\n-def post_list(request, page=0, **kwargs):\n-    return list_detail.object_list(\n-        request,\n-        queryset = Post.objects.published(),\n-        paginate_by = 20,\n-        page = page,\n-        **kwargs\n-    )\n-post_list.__doc__ = list_detail.object_list.__doc__\n-\n-\n-def post_archive_year(request, year, **kwargs):\n-    return date_based.archive_year(\n-        request,\n-        year = year,\n-        date_field = 'publish',\n-        queryset = Post.objects.published(),\n-        make_object_list = True,\n-        **kwargs\n-    )\n-post_archive_year.__doc__ = date_based.archive_year.__doc__\n-\n-\n-def post_archive_month(request, year, month, **kwargs):\n-    return date_based.archive_month(\n-        request,\n-        year = year,\n-        month = month,\n-        date_field = 'publish',\n-        queryset = Post.objects.published(),\n-        **kwargs\n-    )\n-post_archive_month.__doc__ = date_based.archive_month.__doc__\n-\n-\n-def post_archive_day(request, year, month, day, **kwargs):\n-    return date_based.archive_day(\n-        request,\n-        year = year,\n-        month = month,\n-        day = day,\n-        date_field = 'publish',\n-        queryset = Post.objects.published(),\n-        **kwargs\n-    )\n-post_archive_day.__doc__ = date_based.archive_day.__doc__\n-\n-\n-def post_detail(request, slug, year, month, day, **kwargs):\n-    return date_based.object_detail(\n-        request,\n-        year = year,\n-        month = month,\n-        day = day,\n-        date_field = 'publish',\n-        slug = slug,\n-        queryset = Post.objects.published(),\n-        **kwargs\n-    )\n-post_detail.__doc__ = date_based.object_detail.__doc__\n-\n-\n-def category_list(request, template_name = 'blog/category_list.html', **kwargs):\n-    \"\"\"\n-    Category list\n-\n-    Template: ``blog/category_list.html``\n-    Context:\n-        object_list\n-            List of categories.\n-    \"\"\"\n-    return list_detail.object_list(\n-        request,\n-        queryset = Category.objects.all(),\n-        template_name = template_name,\n-        **kwargs\n-    )\n-\n-def category_detail(request, slug, template_name = 'blog/category_detail.html', **kwargs):\n-    \"\"\"\n-    Category detail\n-\n-    Template: ``blog/category_detail.html``\n-    Context:\n-        object_list\n-            List of posts specific to the given category.\n-        category\n-            Given category.\n-    \"\"\"\n-    category = get_object_or_404(Category, slug__iexact=slug)\n-\n-    return list_detail.object_list(\n-        request,\n-        queryset = category.post_set.published(),\n-        extra_context = {'category': category},\n-        template_name = template_name,\n-        **kwargs\n-    )\n-\n-\n-# Stop Words courtesy of http://www.dcs.gla.ac.uk/idom/ir_resources/linguistic_utils/stop_words\n-STOP_WORDS = r\"\"\"\\b(a|about|above|across|after|afterwards|again|against|all|almost|alone|along|already|also|\n-although|always|am|among|amongst|amoungst|amount|an|and|another|any|anyhow|anyone|anything|anyway|anywhere|are|\n-around|as|at|back|be|became|because|become|becomes|becoming|been|before|beforehand|behind|being|below|beside|\n-besides|between|beyond|bill|both|bottom|but|by|call|can|cannot|cant|co|computer|con|could|couldnt|cry|de|describe|\n-detail|do|done|down|due|during|each|eg|eight|either|eleven|else|elsewhere|empty|enough|etc|even|ever|every|everyone|\n-everything|everywhere|except|few|fifteen|fify|fill|find|fire|first|five|for|former|formerly|forty|found|four|from|\n-front|full|further|get|give|go|had|has|hasnt|have|he|hence|her|here|hereafter|hereby|herein|hereupon|hers|herself|\n-him|himself|his|how|however|hundred|i|ie|if|in|inc|indeed|interest|into|is|it|its|itself|keep|last|latter|latterly|\n-least|less|ltd|made|many|may|me|meanwhile|might|mill|mine|more|moreover|most|mostly|move|much|must|my|myself|name|\n-namely|neither|never|nevertheless|next|nine|no|nobody|none|noone|nor|not|nothing|now|nowhere|of|off|often|on|once|\n-one|only|onto|or|other|others|otherwise|our|ours|ourselves|out|over|own|part|per|perhaps|please|put|rather|re|same|\n-see|seem|seemed|seeming|seems|serious|several|she|should|show|side|since|sincere|six|sixty|so|some|somehow|someone|\n-something|sometime|sometimes|somewhere|still|such|system|take|ten|than|that|the|their|them|themselves|then|thence|\n-there|thereafter|thereby|therefore|therein|thereupon|these|they|thick|thin|third|this|those|though|three|through|\n-throughout|thru|thus|to|together|too|top|toward|towards|twelve|twenty|two|un|under|until|up|upon|us|very|via|was|\n-we|well|were|what|whatever|when|whence|whenever|where|whereafter|whereas|whereby|wherein|whereupon|wherever|whether|\n-which|while|whither|who|whoever|whole|whom|whose|why|will|with|within|without|would|yet|you|your|yours|yourself|\n-yourselves)\\b\"\"\"\n-\n-\n-def search(request, template_name='blog/post_search.html'):\n-    \"\"\"\n-    Search for blog posts.\n-\n-    This template will allow you to setup a simple search form that will try to return results based on\n-    given search strings. The queries will be put through a stop words filter to remove words like\n-    'the', 'a', or 'have' to help imporve the result set.\n-\n-    Template: ``blog/post_search.html``\n-    Context:\n-        object_list\n-            List of blog posts that match given search term(s).\n-        search_term\n-            Given search term.\n-    \"\"\"\n-    context = {}\n-    if request.GET:\n-        stop_word_list = re.compile(STOP_WORDS, re.IGNORECASE)\n-        search_term = '%s' % request.GET['q']\n-        cleaned_search_term = stop_word_list.sub('', search_term)\n-        cleaned_search_term = cleaned_search_term.strip()\n-        if len(cleaned_search_term) != 0:\n-            post_list = Post.objects.published().filter(Q(body__icontains=cleaned_search_term) | Q(tags__icontains=cleaned_search_term) | Q(categories__title__icontains=cleaned_search_term))\n-            context = {'object_list': post_list, 'search_term':search_term}\n-        else:\n-            message = 'Search term was too vague. Please try again.'\n-            context = {'message':message}\n-    return render_to_response(template_name, context, context_instance=RequestContext(request))\n\n\n@@ -1,3 +0,0 @@\n-from django.db import models\n-\n-# Create your models here.\n\n@@ -1,20 +0,0 @@\n-from django.conf import settings\n-from django.conf.urls.defaults import *\n-\n-from scrapyorg.docs.views import document\n-from scrapyorg.docs.views import images\n-\n-\n-urlpatterns = patterns('',\n-    url(r'^$', document, {'url': ''}),\n-    url(r'^(?P<url>[\\w./-]*)/$', document),\n-    url(r'^_images/(?P<path>.*)$', images),\n-)\n-\n-if settings.DEBUG: # devel\n-    urlpatterns += patterns('',         \n-        (r'^%s/(?P<path>.*)$' % settings.MEDIA_URL[1:],\n-          'django.views.static.serve',\n-          {'document_root': settings.MEDIA_ROOT}),\n-    )\n-\n\n@@ -1,33 +0,0 @@\n-import cPickle as pickle\n-import os\n-\n-import django.views.static\n-from django.conf import settings\n-from django.http import Http404\n-from django.shortcuts import render_to_response\n-from django.template import RequestContext\n-\n-\n-def document(request, url):\n-    docroot = settings.DOC_PICKLE_ROOT\n-\n-    if os.path.exists(os.path.join(docroot, url, 'index.fpickle')):\n-        docpath = os.path.join(docroot, url, 'index.fpickle')\n-    elif os.path.exists(os.path.join(docroot, url + '.fpickle')):\n-        docpath = os.path.join(docroot, url + '.fpickle')\n-    else:\n-        raise Http404(\"'%s' does not exist\" % url)\n-\n-    docfile = open(docpath, 'rb')\n-    doc = pickle.load(docfile)\n-\n-    return render_to_response('docs/doc.html', {'doc': doc},\n-                              context_instance=RequestContext(request))\n-\n-\n-def images(request, path):\n-    return django.views.static.serve(\n-\t    request,\n-        document_root = os.path.join(settings.DOC_PICKLE_ROOT, '_images'),\n-        path = path,\n-    )\n\n\n@@ -1,11 +0,0 @@\n-from django.contrib import admin\n-\n-from scrapyorg.download.models import DownloadLink\n-\n-\n-class DownloadLinkAdmin(admin.ModelAdmin):\n-    list_display = (\"text\", \"address\", \"public_link\", \"created\")\n-    list_filter = (\"public\", \"created\")\n-    search_fields = (\"address\", \"text\")\n-\n-admin.site.register(DownloadLink, DownloadLinkAdmin)\n\n@@ -1,51 +0,0 @@\n-from datetime import datetime\n-\n-from django.db import models\n-from django.utils.translation import ugettext_lazy as _\n-\n-\n-class DownloadLink(models.Model):\n-    description = models.CharField(_(\"description\"), max_length=512,\n-                                   blank=True, default='')\n-    address = models.CharField(_(\"address\"), max_length=1024, blank=False)\n-    text = models.CharField(_(\"link text\"), max_length=512, blank=False,\n-                            default=_(\"download\"))\n-    public = models.BooleanField(_(\"public\"), blank=False, default=True)\n-\n-    # automatic dates\n-    created = models.DateTimeField(_(\"created\"), editable=False)\n-    updated = models.DateTimeField(_(\"updated\"), editable=False)\n-\n-    def toggle_public(self):\n-        self.public = not self.public\n-        self.save()\n-\n-    def save(self):\n-        if not self.id:\n-            self.created = datetime.now()\n-        self.updated = datetime.now()\n-        super(DownloadLink, self).save()\n-\n-    def __unicode__(self):\n-        return self.address\n-\n-    # ugly, but django-admin isn't very versatile right now\n-    def public_link(self):\n-        img_url = \"/media/img/admin/icon-%s.gif\" % \\\n-                  (self.public and \"yes\" or \"no\")\n-        html = _('<img alt=\"%s\" src=\"' + img_url + '\"/> ' \\\n-                 '(<a href=\"%s/toggle/\">Toggle</a>)')\n-        return html % (_(str(self.public)), self.id)\n-\n-    public_link.short_description = u\"public\"\n-    public_link.allow_tags = True\n-\n-#     class Admin:\n-#         list_display = (\"text\", \"address\", \"public_link\", \"created\")\n-#         list_filter = (\"public\", \"created\")\n-#         save_on_top = True\n-\n-    class Meta:\n-        verbose_name = _(\"download link\")\n-        verbose_name_plural = _(\"download links\")\n-        ordering = [\"-created\",]\n\n\n@@ -1,39 +0,0 @@\n-from django import template\n-\n-from scrapyorg.lib.templatetags import *\n-\n-from scrapyorg.download.models import DownloadLink\n-\n-\n-register = template.Library()\n-\n-\n-@register.tag(name=\"load_download_links\")\n-def do_load_links(parser, token):\n-    syntax_msg = 'Syntax: %s \"COUNT\" as \"VAR_NAME\"'\n-\n-    try:\n-        splited = token.split_contents()\n-        if len(splited) == 4:\n-            tag, count, _as, var_name = splited\n-        elif len(splited) == 3:\n-            tag, _as, var_name = splited\n-            count = None\n-\n-        if count and not is_string(count) or not is_string(var_name):\n-            raise_syntax(syntax_msg % tag)\n-        count = count and int(unquoute(count)) or count\n-    except:\n-        raise_syntax(syntax_msg % token.split_contents()[0])\n-    return LoadLinksNode(count, unquoute(var_name))\n-\n-\n-class LoadLinksNode(template.Node):\n-    def __init__(self, count, var_name):\n-        self.count = count\n-        self.var_name = var_name\n-\n-    def render(self, context):\n-        links = DownloadLink.objects.filter(public=True)[:self.count]\n-        context[self.var_name] = links\n-        return ''\n\n@@ -1,8 +0,0 @@\n-from django.conf.urls.defaults import *\n-\n-from scrapyorg.download.views import *\n-\n-\n-urlpatterns = patterns('',\n-    (r\"^(?P<link_id>\\d+)/toggle/$\", toggle_public),\n-)\n\n@@ -1,12 +0,0 @@\n-from django.shortcuts import get_object_or_404\n-from django.http import HttpResponseRedirect\n-from django.contrib.admin.views.decorators import staff_member_required\n-\n-from scrapyorg.download.models import DownloadLink\n-\n-\n-@staff_member_required\n-def toggle_public(request, link_id):\n-    link = get_object_or_404(DownloadLink, pk=link_id)\n-    link.toggle_public()\n-    return HttpResponseRedirect(\"/admin/download/downloadlink/\")\n\n\n@@ -1,8 +0,0 @@\n-from django import template\n-\n-\n-is_string    = lambda val: val[0] in (\"'\", '\"') and val[0] == val[-1]\n-unquoute     = lambda val: val[1:-1]\n-\n-def raise_syntax(msg):\n-   raise template.TemplateSyntaxError, msg\n\n@@ -1,11 +0,0 @@\n-#!/usr/bin/env python\n-from django.core.management import execute_manager\n-try:\n-    import settings # Assumed to be in the same directory.\n-except ImportError:\n-    import sys\n-    sys.stderr.write(\"Error: Can't find the file 'settings.py' in the directory containing %r. It appears you've customized things.\\nYou'll have to run django-admin.py, passing it your settings module.\\n(If the file settings.py does indeed exist, it's causing an ImportError somehow.)\\n\" % __file__)\n-    sys.exit(1)\n-\n-if __name__ == \"__main__\":\n-    execute_manager(settings)\n\n@@ -1,98 +0,0 @@\n-# -*- coding: utf-8 -*-\n-# Django settings for scrapy project.\n-\n-#from os.path import abspath, dirname, basename, join\n-import os\n-\n-PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n-\n-ADMINS = (\n-    # ('Your Name', 'your_email@domain.com'),\n-)\n-\n-MANAGERS = ADMINS\n-\n-DEBUG=False\n-\n-DEFAULT_CHARSET = \"utf-8\"\n-\n-# Local time zone for this installation. Choices can be found here:\n-# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name\n-# although not all choices may be avilable on all operating systems.\n-# If running in a Windows environment this must be set to the same as your\n-# system time zone.\n-TIME_ZONE = 'America/Montevideo'\n-\n-# Language code for this installation. All choices can be found here:\n-# http://www.i18nguy.com/unicode/language-identifiers.html\n-LANGUAGE_CODE = 'en-us'\n-\n-SITE_ID = 1\n-\n-# If you set this to False, Django will make some optimizations so as not\n-# to load the internationalization machinery.\n-# USE_I18N = True\n-\n-# Absolute path to the directory that holds media.\n-# Example: \"/home/media/media.lawrence.com/\"\n-MEDIA_ROOT = os.path.join(PROJECT_ROOT, \"static\")\n-\n-# URL that handles the media served from MEDIA_ROOT. Make sure to use a\n-# trailing slash if there is a path component (optional in other cases).\n-# Examples: \"http://media.lawrence.com\", \"http://example.com/media/\"\n-MEDIA_URL = '/site-media'\n-\n-# URL prefix for admin media -- CSS, JavaScript and images. Make sure to use a\n-# trailing slash.\n-# Examples: \"http://foo.com/media/\", \"/media/\".\n-ADMIN_MEDIA_PREFIX = '/media/'\n-\n-# Make this unique, and don't share it with anybody.\n-SECRET_KEY = 'fq04ss$#1h=m=39sh4vvph+76i5u716z1-x5$$9xn7sb6y4-di'\n-\n-# List of callables that know how to import templates from various sources.\n-TEMPLATE_LOADERS = (\n-    'django.template.loaders.filesystem.load_template_source',\n-    'django.template.loaders.app_directories.load_template_source',\n-#     'django.template.loaders.eggs.load_template_source',\n-)\n-\n-MIDDLEWARE_CLASSES = (\n-    'django.middleware.common.CommonMiddleware',\n-    'django.contrib.sessions.middleware.SessionMiddleware',\n-    'django.contrib.auth.middleware.AuthenticationMiddleware',\n-    'django.middleware.doc.XViewMiddleware',\n-)\n-\n-ROOT_URLCONF = 'scrapyorg.urls'\n-\n-TEMPLATE_DIRS = (\n-    os.path.join(PROJECT_ROOT, \"templates\"),\n-)\n-\n-INSTALLED_APPS = (\n-    'django.contrib.auth',\n-    'django.contrib.contenttypes',\n-    'django.contrib.sessions',\n-    'django.contrib.sites',\n-    'django.contrib.admin',\n-    'django.contrib.markup',\n-    'scrapyorg.article',\n-    'scrapyorg.download',\n-    'scrapyorg.docs',\n-    'scrapyorg.blog',\n-)\n-\n-DOC_PICKLE_ROOT = os.path.join(PROJECT_ROOT, 'docs', 'pickle')\n-\n-# Override previous settings with values in local_settings.py settings file.\n-try:\n-    from local_settings import *\n-except ImportError:\n-    debug_msg =\"Can't find local_settings.py, using default settings.\"\n-    try:\n-        from mod_python import apache\n-        apache.log_error(\"%s\" % debug_msg, apache.APLOG_NOTICE)\n-    except ImportError:\n-        import sys\n-        sys.stderr.write(\"%s\\n\" % debug_msg)\n\n@@ -1,37 +0,0 @@\n-from django.conf import settings\n-from django.conf.urls.defaults import *\n-from django.contrib import admin\n-\n-\n-admin.autodiscover()\n-\n-\n-urlpatterns = patterns('',\n-    # admin\n-    url(r\"^admin/download/downloadlink/\", include(\"scrapyorg.download.urls\")),\n-    url(r'^admin/(.*)', admin.site.root),\n-\n-    # pablo: disabled documentation until we improve its usability. for now,\n-    # documentation will be served at http://doc.scrapy.org using static html\n-    # files generated by Sphinx, and a small notice landing page will be shown\n-    # in scrapy.org/doc\n-    # docs\n-#    url(r\"^docs/\", include(\"scrapyorg.docs.urls\")),\n-    url(r\"docs/.*\", 'django.views.generic.simple.redirect_to', {'url': '/doc/'}),\n-    # news\n-    url(r\"^news/\", include(\"scrapyorg.blog.urls\")),\n-)\n-\n-\n-if settings.DEBUG: # devel\n-    urlpatterns += patterns('',         \n-        (r'^%s/(?P<path>.*)$' % settings.MEDIA_URL[1:],\n-          'django.views.static.serve',\n-          {'document_root': settings.MEDIA_ROOT}),\n-    )\n-    \n-# last resort, it's an article\n-urlpatterns += patterns('',         \n-    url(r\"\", include(\"scrapyorg.article.urls\")),\n-)\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
