{"custom_id": "keras#914d976801c5d2323cdc87b902a5342637e86dc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 4365 | Contributors (this commit): 47 | Commits (past 90d): 32 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1171,7 +1171,7 @@ def normalize_batch_in_training(x, gamma, beta,\n                 target_shape.append(1)\n             else:\n                 target_shape.append(tf.shape(x)[axis])\n-        target_shape = tf.stack(target_shape)\n+        target_shape = stack(target_shape)\n \n         broadcast_mean = tf.reshape(mean, target_shape)\n         broadcast_var = tf.reshape(var, target_shape)\n@@ -1303,7 +1303,7 @@ def repeat(x, n):\n     '''\n     assert ndim(x) == 2\n     x = tf.expand_dims(x, 1)\n-    pattern = tf.stack([1, n, 1])\n+    pattern = stack([1, n, 1])\n     return tf.tile(x, pattern)\n \n \n@@ -1340,7 +1340,7 @@ def batch_flatten(x):\n     '''Turn a n-D tensor into a 2D tensor where\n     the first dimension is conserved.\n     '''\n-    x = tf.reshape(x, tf.stack([-1, prod(shape(x)[1:])]))\n+    x = tf.reshape(x, stack([-1, prod(shape(x)[1:])]))\n     return x\n \n \n@@ -1448,7 +1448,10 @@ def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='default'):\n \n \n def stack(x):\n+    try:\n         return tf.stack(x)\n+    except AttributeError:\n+        return tf.pack(x)\n \n \n def one_hot(indices, nb_classes):\n@@ -1692,12 +1695,12 @@ def rnn(step_function, inputs, initial_states,\n         successive_states = []\n         successive_outputs = []\n \n-        input_list = tf.unstack(inputs)\n+        input_list = tf.unpack(inputs)\n         if go_backwards:\n             input_list.reverse()\n \n         if mask is not None:\n-            mask_list = tf.unstack(mask)\n+            mask_list = tf.unpack(mask)\n             if go_backwards:\n                 mask_list.reverse()\n \n@@ -1715,7 +1718,7 @@ def rnn(step_function, inputs, initial_states,\n                 # it just repeats the mask along its second dimension\n                 # n times.\n                 tiled_mask_t = tf.tile(mask_t,\n-                                       tf.stack([1, tf.shape(output)[1]]))\n+                                       stack([1, tf.shape(output)[1]]))\n \n                 if len(successive_outputs) == 0:\n                     prev_output = zeros_like(output)\n@@ -1728,7 +1731,7 @@ def rnn(step_function, inputs, initial_states,\n                 for state, new_state in zip(states, new_states):\n                     # (see earlier comment for tile explanation)\n                     tiled_mask_t = tf.tile(mask_t,\n-                                           tf.stack([1, tf.shape(new_state)[1]]))\n+                                           stack([1, tf.shape(new_state)[1]]))\n                     return_states.append(tf.where(tiled_mask_t,\n                                                   new_state,\n                                                   state))\n@@ -1737,7 +1740,7 @@ def rnn(step_function, inputs, initial_states,\n                 successive_states.append(states)\n                 last_output = successive_outputs[-1]\n                 new_states = successive_states[-1]\n-                outputs = tf.stack(successive_outputs)\n+                outputs = stack(successive_outputs)\n         else:\n             for input in input_list:\n                 output, states = step_function(input, states + constants)\n@@ -1745,7 +1748,7 @@ def rnn(step_function, inputs, initial_states,\n                 successive_states.append(states)\n             last_output = successive_outputs[-1]\n             new_states = successive_states[-1]\n-            outputs = tf.stack(successive_outputs)\n+            outputs = stack(successive_outputs)\n \n     else:\n         if go_backwards:\n@@ -1791,7 +1794,7 @@ def rnn(step_function, inputs, initial_states,\n                 for state, new_state in zip(states, new_states):\n                     new_state.set_shape(state.get_shape())\n                 tiled_mask_t = tf.tile(mask_t,\n-                                       tf.stack([1, tf.shape(output)[1]]))\n+                                       stack([1, tf.shape(output)[1]]))\n                 output = tf.where(tiled_mask_t, output, states[0])\n                 new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\n                 output_ta_t = output_ta_t.write(time, output)\n@@ -2397,8 +2400,8 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n     # undocumented feature soon to be made public\n     from tensorflow.python.ops import functional_ops\n     label_shape = tf.shape(labels)\n-    num_batches_tns = tf.stack([label_shape[0]])\n-    max_num_labels_tns = tf.stack([label_shape[1]])\n+    num_batches_tns = stack([label_shape[0]])\n+    max_num_labels_tns = stack([label_shape[1]])\n \n     def range_less_than(previous_state, current_input):\n         return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30fa61d4576d103872a28ea8ae2281100a470388", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 4379 | Contributors (this commit): 47 | Commits (past 90d): 33 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -1707,7 +1707,7 @@ def rnn(step_function, inputs, initial_states,\n             for input, mask_t in zip(input_list, mask_list):\n                 output, new_states = step_function(input, states + constants)\n \n-                # tf.where needs its condition tensor\n+                # tf.select needs its condition tensor\n                 # to be the same shape as its two\n                 # result tensors, but in our case\n                 # the condition (mask) tensor is\n@@ -1725,14 +1725,14 @@ def rnn(step_function, inputs, initial_states,\n                 else:\n                     prev_output = successive_outputs[-1]\n \n-                output = tf.where(tiled_mask_t, output, prev_output)\n+                output = tf.select(tiled_mask_t, output, prev_output)\n \n                 return_states = []\n                 for state, new_state in zip(states, new_states):\n                     # (see earlier comment for tile explanation)\n                     tiled_mask_t = tf.tile(mask_t,\n                                            stack([1, tf.shape(new_state)[1]]))\n-                    return_states.append(tf.where(tiled_mask_t,\n+                    return_states.append(tf.select(tiled_mask_t,\n                                                    new_state,\n                                                    state))\n                 states = return_states\n@@ -1795,8 +1795,8 @@ def rnn(step_function, inputs, initial_states,\n                     new_state.set_shape(state.get_shape())\n                 tiled_mask_t = tf.tile(mask_t,\n                                        stack([1, tf.shape(output)[1]]))\n-                output = tf.where(tiled_mask_t, output, states[0])\n-                new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\n+                output = tf.select(tiled_mask_t, output, states[0])\n+                new_states = [tf.select(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\n                 output_ta_t = output_ta_t.write(time, output)\n                 return (time + 1, output_ta_t) + tuple(new_states)\n         else:\n@@ -1921,7 +1921,7 @@ def elu(x, alpha=1.):\n     if alpha == 1:\n         return res\n     else:\n-        return tf.where(x > 0, res, alpha * res)\n+        return tf.select(x > 0, res, alpha * res)\n \n \n def softmax(x):\n@@ -2384,7 +2384,7 @@ def random_uniform(shape, low=0.0, high=1.0, dtype=_FLOATX, seed=None):\n def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):\n     if seed is None:\n         seed = np.random.randint(10e6)\n-    return tf.where(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n+    return tf.select(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n                      tf.ones(shape, dtype=dtype),\n                      tf.zeros(shape, dtype=dtype))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#79406f111bfe2041b9d288a0b152daf6447235ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 117 | Lines Deleted: 34 | Files Changed: 3 | Hunks: 47 | Methods Changed: 54 | Complexity Δ (Sum/Max): 24/9 | Churn Δ: 151 | Churn Cumulative: 8816 | Contributors (this commit): 67 | Commits (past 90d): 65 | Contributors (cumulative): 116 | DMM Complexity: 0.746031746031746\n\nDIFF:\n@@ -12,7 +12,7 @@ import numpy as np\n import os\n import copy\n import warnings\n-from .common import _FLOATX, _EPSILON, image_dim_ordering, reset_uids\n+from .common import floatx, _EPSILON, image_dim_ordering, reset_uids\n py_all = all\n \n # INTERNAL UTILS\n@@ -207,7 +207,7 @@ def to_dense(tensor):\n         return tensor\n \n \n-def variable(value, dtype=_FLOATX, name=None):\n+def variable(value, dtype=None, name=None):\n     '''Instantiates a variable and returns it.\n \n     # Arguments\n@@ -232,6 +232,8 @@ def variable(value, dtype=_FLOATX, name=None):\n                [ 3.,  4.]])\n     ```\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     if hasattr(value, 'tocoo'):\n         sparse_coo = value.tocoo()\n         indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),\n@@ -271,7 +273,7 @@ def _initialize_variables():\n             sess.run(tf.initialize_variables(uninitialized_variables))\n \n \n-def placeholder(shape=None, ndim=None, dtype=_FLOATX, sparse=False, name=None):\n+def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n     '''Instantiates a placeholder tensor and returns it.\n \n     # Arguments\n@@ -296,6 +298,8 @@ def placeholder(shape=None, ndim=None, dtype=_FLOATX, sparse=False, name=None):\n         <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>\n     ```\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     if not shape:\n         if ndim:\n             shape = tuple([None for _ in range(ndim)])\n@@ -448,7 +452,7 @@ def eval(x):\n     return to_dense(x).eval(session=get_session())\n \n \n-def zeros(shape, dtype=_FLOATX, name=None):\n+def zeros(shape, dtype=None, name=None):\n     '''Instantiates an all-zeros variable and returns it.\n \n     # Arguments\n@@ -469,13 +473,15 @@ def zeros(shape, dtype=_FLOATX, name=None):\n                [ 0.,  0.,  0.,  0.]], dtype=float32)\n     ```\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     return variable(tf.constant_initializer(0., dtype=tf_dtype)(shape),\n                     dtype, name)\n \n \n-def ones(shape, dtype=_FLOATX, name=None):\n+def ones(shape, dtype=None, name=None):\n     '''Instantiates an all-ones tensor variable and returns it.\n \n     # Arguments\n@@ -498,13 +504,15 @@ def ones(shape, dtype=_FLOATX, name=None):\n                [ 1.,  1.,  1.,  1.]], dtype=float32)\n     ```\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     return variable(tf.constant_initializer(1., dtype=tf_dtype)(shape),\n                     dtype, name)\n \n \n-def eye(size, dtype=_FLOATX, name=None):\n+def eye(size, dtype=None, name=None):\n     '''Instantiate an identity matrix and returns it.\n \n     # Arguments\n@@ -577,7 +585,7 @@ def ones_like(x, name=None):\n     return tf.ones_like(x, name=name)\n \n \n-def random_uniform_variable(shape, low, high, dtype=_FLOATX,\n+def random_uniform_variable(shape, low, high, dtype=None,\n                             name=None, seed=None):\n     '''Instantiates an Keras variable filled with\n     samples drawn from a uniform distribution and returns it.\n@@ -609,6 +617,8 @@ def random_uniform_variable(shape, low, high, dtype=_FLOATX,\n                [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)\n     ```\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     if seed is None:\n@@ -619,7 +629,7 @@ def random_uniform_variable(shape, low, high, dtype=_FLOATX,\n     return variable(value, dtype=dtype, name=name)\n \n \n-def random_normal_variable(shape, mean, scale, dtype=_FLOATX,\n+def random_normal_variable(shape, mean, scale, dtype=None,\n                            name=None, seed=None):\n     '''Instantiates an Keras variable filled with\n     samples drawn from a normal distribution and returns it.\n@@ -651,6 +661,8 @@ def random_normal_variable(shape, mean, scale, dtype=_FLOATX,\n                [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)\n     ```\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     if seed is None:\n@@ -963,7 +975,7 @@ def var(x, axis=None, keepdims=False):\n     '''\n     axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n-        x = tf.cast(x, _FLOATX)\n+        x = tf.cast(x, floatx())\n     m = tf.reduce_mean(x, reduction_indices=axis, keep_dims=True)\n     devs_squared = tf.square(x - m)\n     return tf.reduce_mean(devs_squared,\n@@ -982,7 +994,7 @@ def mean(x, axis=None, keepdims=False):\n     '''\n     axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n-        x = tf.cast(x, _FLOATX)\n+        x = tf.cast(x, floatx())\n     return tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n@@ -2073,7 +2085,7 @@ def _preprocess_deconv_output_shape(shape, dim_ordering):\n \n \n def _preprocess_conv2d_input(x, dim_ordering):\n-    if _FLOATX == 'float64':\n+    if dtype(x) == 'float64':\n         x = tf.cast(x, 'float32')\n     if dim_ordering == 'th':\n         # TF uses the last dimension as channel dimension,\n@@ -2085,7 +2097,7 @@ def _preprocess_conv2d_input(x, dim_ordering):\n \n \n def _preprocess_conv3d_input(x, dim_ordering):\n-    if _FLOATX == 'float64':\n+    if dtype(x) == 'float64':\n         x = tf.cast(x, 'float32')\n     if dim_ordering == 'th':\n         # TF uses the last dimension as channel dimension,\n@@ -2097,7 +2109,7 @@ def _preprocess_conv3d_input(x, dim_ordering):\n \n \n def _preprocess_conv2d_kernel(kernel, dim_ordering):\n-    if _FLOATX == 'float64':\n+    if dtype(kernel) == 'float64':\n         kernel = tf.cast(kernel, 'float32')\n     if dim_ordering == 'th':\n         # TF uses the last dimension as channel dimension,\n@@ -2109,7 +2121,7 @@ def _preprocess_conv2d_kernel(kernel, dim_ordering):\n \n \n def _preprocess_conv3d_kernel(kernel, dim_ordering):\n-    if _FLOATX == 'float64':\n+    if dtype(kernel) == 'float64':\n         kernel = tf.cast(kernel, 'float32')\n     if dim_ordering == 'th':\n         # TF uses the last dimension as channel dimension,\n@@ -2134,7 +2146,7 @@ def _postprocess_conv2d_output(x, dim_ordering):\n     if dim_ordering == 'th':\n         x = tf.transpose(x, (0, 3, 1, 2))\n \n-    if _FLOATX == 'float64':\n+    if floatx() == 'float64':\n         x = tf.cast(x, 'float64')\n     return x\n \n@@ -2143,7 +2155,7 @@ def _postprocess_conv3d_output(x, dim_ordering):\n     if dim_ordering == 'th':\n         x = tf.transpose(x, (0, 4, 1, 2, 3))\n \n-    if _FLOATX == 'float64':\n+    if floatx() == 'float64':\n         x = tf.cast(x, 'float64')\n     return x\n \n@@ -2158,13 +2170,14 @@ def conv1d(x, kernel, stride=1, border_mode='valid',\n         border_mode: string, \"same\" or \"valid\".\n     '''\n     # pre-process dtype\n-    if _FLOATX == 'float64':\n+    x_dtype = dtype(x)\n+    if x_dtype == 'float64':\n         x = tf.cast(x, 'float32')\n         kernel = tf.cast(kernel, 'float32')\n     padding = _preprocess_border_mode(border_mode)\n     x = tf.nn.conv1d(x, kernel, stride, padding=padding)\n     # post-process dtype\n-    if _FLOATX == 'float64':\n+    if x_dtype == 'float64':\n         x = tf.cast(x, 'float64')\n     return x\n \n@@ -2367,21 +2380,27 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n \n # RANDOMNESS\n \n-def random_normal(shape, mean=0.0, std=1.0, dtype=_FLOATX, seed=None):\n+def random_normal(shape, mean=0.0, std=1.0, dtype=None, seed=None):\n+    if dtype is None:\n+        dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(10e6)\n     return tf.random_normal(shape, mean=mean, stddev=std,\n                             dtype=dtype, seed=seed)\n \n \n-def random_uniform(shape, low=0.0, high=1.0, dtype=_FLOATX, seed=None):\n+def random_uniform(shape, low=0.0, high=1.0, dtype=None, seed=None):\n+    if dtype is None:\n+        dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(10e6)\n     return tf.random_uniform(shape, minval=low, maxval=high,\n                              dtype=dtype, seed=seed)\n \n \n-def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):\n+def random_binomial(shape, p=0.0, dtype=None, seed=None):\n+    if dtype is None:\n+        dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(10e6)\n     return tf.select(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n\n@@ -14,7 +14,7 @@ except ImportError:\n     from theano.sandbox.softsign import softsign as T_softsign\n import inspect\n import numpy as np\n-from .common import _FLOATX, _EPSILON, image_dim_ordering\n+from .common import _FLOATX, floatx, _EPSILON, image_dim_ordering\n py_all = all\n \n \n@@ -56,9 +56,11 @@ def to_dense(tensor):\n         return tensor\n \n \n-def variable(value, dtype=_FLOATX, name=None):\n+def variable(value, dtype=None, name=None):\n     '''Instantiates a variable.\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     if hasattr(value, 'tocoo'):\n         _assert_sparse_module()\n         return th_sparse_module.as_sparse_variable(value)\n@@ -67,9 +69,11 @@ def variable(value, dtype=_FLOATX, name=None):\n         return theano.shared(value=value, name=name, strict=False)\n \n \n-def placeholder(shape=None, ndim=None, dtype=_FLOATX, sparse=False, name=None):\n+def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n     '''Instantiate an input data placeholder variable.\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     if shape is None and ndim is None:\n         raise ValueError('Specify either a shape or ndim value.')\n     if shape is not None:\n@@ -111,21 +115,27 @@ def eval(x):\n     return to_dense(x).eval()\n \n \n-def zeros(shape, dtype=_FLOATX, name=None):\n+def zeros(shape, dtype=None, name=None):\n     '''Instantiates an all-zeros variable.\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     return variable(np.zeros(shape), dtype, name)\n \n \n-def ones(shape, dtype=_FLOATX, name=None):\n+def ones(shape, dtype=None, name=None):\n     '''Instantiates an all-ones variable.\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     return variable(np.ones(shape), dtype, name)\n \n \n-def eye(size, dtype=_FLOATX, name=None):\n+def eye(size, dtype=None, name=None):\n     '''Instantiates an identity matrix.\n     '''\n+    if dtype is None:\n+        dtype = floatx()\n     return variable(np.eye(size), dtype, name)\n \n \n@@ -137,12 +147,12 @@ def zeros_like(x, name=None):\n     return T.zeros_like(x)\n \n \n-def random_uniform_variable(shape, low, high, dtype=_FLOATX, name=None):\n+def random_uniform_variable(shape, low, high, dtype=None, name=None):\n     return variable(np.random.uniform(low=low, high=high, size=shape),\n                     dtype=dtype, name=name)\n \n \n-def random_normal_variable(shape, mean, scale, dtype=_FLOATX, name=None):\n+def random_normal_variable(shape, mean, scale, dtype=None, name=None):\n     return variable(np.random.normal(loc=0.0, scale=scale, size=shape),\n                     dtype=dtype, name=name)\n \n@@ -284,7 +294,7 @@ def mean(x, axis=None, keepdims=False):\n     dtype = None\n     # bool is available since theano v0.9dev\n     if 'int' in x.dtype or x.dtype == 'bool':\n-        dtype = _FLOATX\n+        dtype = floatx()\n     return T.mean(x, axis=axis, keepdims=keepdims, dtype=dtype)\n \n \n@@ -1799,21 +1809,27 @@ def _old_theano_pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n # RANDOMNESS\n \n \n-def random_normal(shape, mean=0.0, std=1.0, dtype=_FLOATX, seed=None):\n+def random_normal(shape, mean=0.0, std=1.0, dtype=None, seed=None):\n+    if dtype is None:\n+        dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(1, 10e6)\n     rng = RandomStreams(seed=seed)\n     return rng.normal(size=shape, avg=mean, std=std, dtype=dtype)\n \n \n-def random_uniform(shape, low=0.0, high=1.0, dtype=_FLOATX, seed=None):\n+def random_uniform(shape, low=0.0, high=1.0, dtype=None, seed=None):\n+    if dtype is None:\n+        dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(1, 10e6)\n     rng = RandomStreams(seed=seed)\n     return rng.uniform(shape, low=low, high=high, dtype=dtype)\n \n \n-def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):\n+def random_binomial(shape, p=0.0, dtype=None, seed=None):\n+    if dtype is None:\n+        dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(1, 10e6)\n     rng = RandomStreams(seed=seed)\n\n@@ -3,11 +3,19 @@ from numpy.testing import assert_allclose\n import numpy as np\n import scipy.sparse as sparse\n \n-from keras.backend import theano_backend as KTH\n+from keras import backend as K\n+from keras.backend import theano_backend as KTH, floatx, set_floatx, variable\n from keras.backend import tensorflow_backend as KTF\n from keras.utils.np_utils import convert_kernel\n \n \n+def check_dtype(var, dtype):\n+    if K._BACKEND == 'theano':\n+        assert var.dtype == dtype\n+    else:\n+        assert var.dtype.name == '%s_ref' % dtype\n+\n+\n def check_single_tensor_operation(function_name, input_shape, **kwargs):\n     val = np.random.random(input_shape) - 0.5\n     xth = KTH.variable(val)\n@@ -930,6 +938,46 @@ class TestBackend(object):\n                 t = backend.arange(10, dtype=dtype)\n                 assert backend.dtype(t) == dtype\n \n+    def test_setfloatx_incorrect_values(self):\n+        # Keep track of the old value\n+        old_floatx = floatx()\n+        # Try some incorrect values\n+        initial = floatx()\n+        for value in ['', 'beerfloat', 123]:\n+            with pytest.raises(Exception):\n+                set_floatx(value)\n+        assert floatx() == initial\n+        # Restore old value\n+        set_floatx(old_floatx)\n+\n+    def test_setfloatx_correct_values(self):\n+        # Keep track of the old value\n+        old_floatx = floatx()\n+        # Check correct values\n+        for value in ['float16', 'float32', 'float64']:\n+            set_floatx(value)\n+            assert floatx() == value\n+        # Restore old value\n+        set_floatx(old_floatx)\n+\n+    def test_set_floatx(self):\n+        \"\"\"\n+        Make sure that changes to the global floatx are effectively\n+        taken into account by the backend.\n+        \"\"\"\n+        # Keep track of the old value\n+        old_floatx = floatx()\n+\n+        set_floatx('float16')\n+        var = variable([10])\n+        check_dtype(var, 'float16')\n+\n+        set_floatx('float64')\n+        var = variable([10])\n+        check_dtype(var, 'float64')\n+\n+        # Restore old value\n+        set_floatx(old_floatx)\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f0e0d6c385d75090350eabbe27c124c767c00f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 114 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 25 | Methods Changed: 13 | Complexity Δ (Sum/Max): 18/15 | Churn Δ: 131 | Churn Cumulative: 4465 | Contributors (this commit): 59 | Commits (past 90d): 28 | Contributors (cumulative): 67 | DMM Complexity: 1.0\n\nDIFF:\n@@ -831,6 +831,9 @@ def reverse(x, axes):\n     return x[slices]\n \n \n+def pattern_broadcast(x, broatcastable):\n+    return T.patternbroadcast(x, broatcastable)\n+\n # VALUE MANIPULATION\n \n \n\n@@ -52,18 +52,37 @@ class PReLU(Layer):\n     # Arguments\n         init: initialization function for the weights.\n         weights: initial weights, as a list of a single Numpy array.\n+        shared_axes: the axes along which to share learnable\n+            parameters for the activation function.\n+            For example, if the incoming feature maps\n+            are from a 2D convolution\n+            with output shape `(batch, height, width, channels)`,\n+            and you wish to share parameters across space\n+            so that each filter only has one set of parameters,\n+            set `shared_axes=[1, 2]`.\n \n     # References\n         - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](http://arxiv.org/pdf/1502.01852v1.pdf)\n     '''\n-    def __init__(self, init='zero', weights=None, **kwargs):\n+    def __init__(self, init='zero', weights=None, shared_axes=None, **kwargs):\n         self.supports_masking = True\n         self.init = initializations.get(init)\n         self.initial_weights = weights\n+        if type(shared_axes) is not list and type(shared_axes) is not tuple:\n+            self.shared_axes = [shared_axes]\n+        else:\n+            self.shared_axes = list(shared_axes)\n         super(PReLU, self).__init__(**kwargs)\n \n     def build(self, input_shape):\n-        self.alphas = self.init(input_shape[1:],\n+        param_shape = list(input_shape[1:])\n+        self.param_broadcast = [False] * len(param_shape)\n+        if self.shared_axes[0] is not None:\n+            for i in self.shared_axes:\n+                param_shape[i] = 1\n+                self.param_broadcast[i] = True\n+\n+        self.alphas = self.init(param_shape,\n                                 name='{}_alphas'.format(self.name))\n         self.trainable_weights = [self.alphas]\n \n@@ -73,6 +92,9 @@ class PReLU(Layer):\n \n     def call(self, x, mask=None):\n         pos = K.relu(x)\n+        if K.backend() == 'theano':\n+            neg = K.pattern_broadcast(self.alphas, self.param_broadcast) * (x - abs(x)) * 0.5\n+        else:\n             neg = self.alphas * (x - abs(x)) * 0.5\n         return pos + neg\n \n@@ -131,23 +153,41 @@ class ParametricSoftplus(Layer):\n         alpha_init: float. Initial value of the alpha weights.\n         beta_init: float. Initial values of the beta weights.\n         weights: initial weights, as a list of 2 numpy arrays.\n+        shared_axes: the axes along which to share learnable\n+            parameters for the activation function.\n+            For example, if the incoming feature maps\n+            are from a 2D convolution\n+            with output shape `(batch, height, width, channels)`,\n+            and you wish to share parameters across space\n+            so that each filter only has one set of parameters,\n+            set `shared_axes=[1, 2]`.\n \n     # References\n         - [Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143)\n     '''\n     def __init__(self, alpha_init=0.2, beta_init=5.0,\n-                 weights=None, **kwargs):\n+                 weights=None, shared_axes=None, **kwargs):\n         self.supports_masking = True\n         self.alpha_init = K.cast_to_floatx(alpha_init)\n         self.beta_init = K.cast_to_floatx(beta_init)\n         self.initial_weights = weights\n+        if type(shared_axes) is not list and type(shared_axes) is not tuple:\n+            self.shared_axes = [shared_axes]\n+        else:\n+            self.shared_axes = list(shared_axes)\n         super(ParametricSoftplus, self).__init__(**kwargs)\n \n     def build(self, input_shape):\n-        input_shape = input_shape[1:]\n-        self.alphas = K.variable(self.alpha_init * np.ones(input_shape),\n+        param_shape = list(input_shape[1:])\n+        self.param_broadcast = [False] * len(param_shape)\n+        if self.shared_axes[0] is not None:\n+            for i in self.shared_axes:\n+                param_shape[i] = 1\n+                self.param_broadcast[i] = True\n+\n+        self.alphas = K.variable(self.alpha_init * np.ones(param_shape),\n                                  name='{}_alphas'.format(self.name))\n-        self.betas = K.variable(self.beta_init * np.ones(input_shape),\n+        self.betas = K.variable(self.beta_init * np.ones(param_shape),\n                                 name='{}_betas'.format(self.name))\n         self.trainable_weights = [self.alphas, self.betas]\n \n@@ -156,6 +196,9 @@ class ParametricSoftplus(Layer):\n             del self.initial_weights\n \n     def call(self, x, mask=None):\n+        if K.backend() == 'theano':\n+            return K.softplus(K.pattern_broadcast(self.betas, self.param_broadcast) * x) * K.pattern_broadcast(self.alphas, self.param_broadcast)\n+        else:\n             return K.softplus(self.betas * x) * self.alphas\n \n     def get_config(self):\n@@ -214,34 +257,51 @@ class SReLU(Layer):\n         a_left_init: initialization function for the left part slope\n         t_right_init: initialization function for the right part intercept\n         a_right_init: initialization function for the right part slope\n+        shared_axes: the axes along which to share learnable\n+            parameters for the activation function.\n+            For example, if the incoming feature maps\n+            are from a 2D convolution\n+            with output shape `(batch, height, width, channels)`,\n+            and you wish to share parameters across space\n+            so that each filter only has one set of parameters,\n+            set `shared_axes=[1, 2]`.\n \n     # References\n         - [Deep Learning with S-shaped Rectified Linear Activation Units](http://arxiv.org/abs/1512.07030)\n     '''\n     def __init__(self, t_left_init='zero', a_left_init='glorot_uniform',\n-                 t_right_init='glorot_uniform', a_right_init='one', **kwargs):\n+                 t_right_init='glorot_uniform', a_right_init='one', shared_axes=None, **kwargs):\n         self.supports_masking = True\n         self.t_left_init = t_left_init\n         self.a_left_init = a_left_init\n         self.t_right_init = t_right_init\n         self.a_right_init = a_right_init\n+        if type(shared_axes) is not list and type(shared_axes) is not tuple:\n+            self.shared_axes = [shared_axes]\n+        else:\n+            self.shared_axes = list(shared_axes)\n         super(SReLU, self).__init__(**kwargs)\n \n     def build(self, input_shape):\n-        input_shape = input_shape[1:]\n+        param_shape = list(input_shape[1:])\n+        self.param_broadcast = [False] * len(param_shape)\n+        if self.shared_axes[0] is not None:\n+            for i in self.shared_axes:\n+                param_shape[i] = 1\n+                self.param_broadcast[i] = True\n \n         t_left_init = initializations.get(self.t_left_init)\n         a_left_init = initializations.get(self.a_left_init)\n         t_right_init = initializations.get(self.t_right_init)\n         a_right_init = initializations.get(self.a_right_init)\n \n-        self.t_left = t_left_init(input_shape,\n+        self.t_left = t_left_init(param_shape,\n                                   name='{}_t_left'.format(self.name))\n-        self.a_left = a_left_init(input_shape,\n+        self.a_left = a_left_init(param_shape,\n                                   name='{}_a_left'.format(self.name))\n-        self.t_right = t_right_init(input_shape,\n+        self.t_right = t_right_init(param_shape,\n                                     name='{}_t_right'.format(self.name))\n-        self.a_right = a_right_init(input_shape,\n+        self.a_right = a_right_init(param_shape,\n                                     name='{}_a_right'.format(self.name))\n         # ensure the the right part is always to the right of the left\n         self.t_right_actual = self.t_left + abs(self.t_right)\n@@ -249,10 +309,21 @@ class SReLU(Layer):\n                                   self.t_right, self.a_right]\n \n     def call(self, x, mask=None):\n-        Y_left_and_center = self.t_left + K.relu(x - self.t_left,\n-                                                 self.a_left,\n-                                                 self.t_right_actual - self.t_left)\n-        Y_right = K.relu(x - self.t_right_actual) * self.a_right\n+        if K.backend() == 'theano':\n+            t_left = K.pattern_broadcast(self.t_left, self.param_broadcast)\n+            a_left = K.pattern_broadcast(self.a_left, self.param_broadcast)\n+            a_right = K.pattern_broadcast(self.a_right, self.param_broadcast)\n+            t_right_actual = K.pattern_broadcast(self.t_right_actual, self.param_broadcast)\n+        else:\n+            t_left = self.t_left\n+            a_left = self.a_left\n+            a_right = self.a_right\n+            t_right_actual = self.t_right_actual\n+\n+        Y_left_and_center = t_left + K.relu(x - t_left,\n+                                            a_left,\n+                                            t_right_actual - t_left)\n+        Y_right = K.relu(x - t_right_actual) * a_right\n         return Y_left_and_center + Y_right\n \n     def get_config(self):\n\n@@ -17,6 +17,13 @@ def test_prelu():\n                input_shape=(2, 3, 4))\n \n \n+@keras_test\n+def test_prelu_share():\n+    from keras.layers.advanced_activations import PReLU\n+    layer_test(PReLU, kwargs={'shared_axes': 1},\n+               input_shape=(2, 3, 4))\n+\n+\n @keras_test\n def test_elu():\n     from keras.layers.advanced_activations import ELU\n@@ -28,13 +35,22 @@ def test_elu():\n @keras_test\n def test_parametric_softplus():\n     from keras.layers.advanced_activations import ParametricSoftplus\n-    for alpha in [0., .5, -1.]:\n     layer_test(ParametricSoftplus,\n                kwargs={'alpha_init': 1.,\n                        'beta_init': -1},\n                input_shape=(2, 3, 4))\n \n \n+@keras_test\n+def test_parametric_softplus_share():\n+    from keras.layers.advanced_activations import ParametricSoftplus\n+    layer_test(ParametricSoftplus,\n+               kwargs={'shared_axes': 1,\n+                       'alpha_init': 1.,\n+                       'beta_init': -1},\n+               input_shape=(2, 3, 4))\n+\n+\n @keras_test\n def test_thresholded_relu():\n     from keras.layers.advanced_activations import ThresholdedReLU\n@@ -49,5 +65,12 @@ def test_srelu():\n                input_shape=(2, 3, 4))\n \n \n+@keras_test\n+def test_srelu_share():\n+    from keras.layers.advanced_activations import SReLU\n+    layer_test(SReLU, kwargs={'shared_axes': 1},\n+               input_shape=(2, 3, 4))\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1de4d7cfba0914f59f21ac7c91c6710eef8f9a00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 5601 | Contributors (this commit): 30 | Commits (past 90d): 16 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1649,8 +1649,7 @@ class Cropping1D(Layer):\n                 input_shape[2])\n \n     def call(self, x, mask=None):\n-        input_shape = self.input_spec[0].shape\n-        return x[:, self.cropping[0]:input_shape[1]-self.cropping[1], :]\n+        return x[:, self.cropping[0]:-self.cropping[1], :]\n \n     def get_config(self):\n         config = {'cropping': self.cropping}\n@@ -1731,16 +1730,15 @@ class Cropping2D(Layer):\n             raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n     def call(self, x, mask=None):\n-        input_shape = self.input_spec[0].shape\n         if self.dim_ordering == 'th':\n             return x[:,\n                      :,\n-                     self.cropping[0][0]:input_shape[2]-self.cropping[0][1],\n-                     self.cropping[1][0]:input_shape[3]-self.cropping[1][1]]\n+                     self.cropping[0][0]:-self.cropping[0][1],\n+                     self.cropping[1][0]:-self.cropping[1][1]]\n         elif self.dim_ordering == 'tf':\n             return x[:,\n-                     self.cropping[0][0]:input_shape[1]-self.cropping[0][1],\n-                     self.cropping[1][0]:input_shape[2]-self.cropping[1][1],\n+                     self.cropping[0][0]:-self.cropping[0][1],\n+                     self.cropping[1][0]:-self.cropping[1][1],\n                      :]\n \n     def get_config(self):\n@@ -1819,18 +1817,17 @@ class Cropping3D(Layer):\n             raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n     def call(self, x, mask=None):\n-        input_shape = self.input_spec[0].shape\n         if self.dim_ordering == 'th':\n             return x[:,\n                      :,\n-                     self.cropping[0][0]:input_shape[2]-self.cropping[0][1],\n-                     self.cropping[1][0]:input_shape[3]-self.cropping[1][1],\n-                     self.cropping[2][0]:input_shape[4]-self.cropping[2][1]]\n+                     self.cropping[0][0]:-self.cropping[0][1],\n+                     self.cropping[1][0]:-self.cropping[1][1],\n+                     self.cropping[2][0]:-self.cropping[2][1]]\n         elif self.dim_ordering == 'tf':\n             return x[:,\n-                     self.cropping[0][0]:input_shape[1]-self.cropping[0][1],\n-                     self.cropping[1][0]:input_shape[2]-self.cropping[1][1],\n-                     self.cropping[2][0]:input_shape[3]-self.cropping[2][1],\n+                     self.cropping[0][0]:-self.cropping[0][1],\n+                     self.cropping[1][0]:-self.cropping[1][1],\n+                     self.cropping[2][0]:-self.cropping[2][1],\n                      :]\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#766572b5b8c0617e1261809f1916d29bb770a769", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 33 | Churn Cumulative: 3162 | Contributors (this commit): 48 | Commits (past 90d): 26 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,16 +57,27 @@ def to_dense(tensor):\n \n \n def variable(value, dtype=None, name=None):\n-    '''Instantiates a variable.\n+    '''Instantiates a variable and returns it.\n+\n+    # Arguments\n+        value: Numpy array, initial value of the tensor.\n+        dtype: Tensor type.\n+        name: Optional name string for the tensor.\n+\n+    # Returns\n+        A variable instance (with Keras metadata included).\n     '''\n     if dtype is None:\n         dtype = floatx()\n     if hasattr(value, 'tocoo'):\n         _assert_sparse_module()\n-        return th_sparse_module.as_sparse_variable(value)\n+        variable = th_sparse_module.as_sparse_variable(value)\n     else:\n         value = np.asarray(value, dtype=dtype)\n-        return theano.shared(value=value, name=name, strict=False)\n+        variable = theano.shared(value=value, name=name, strict=False)\n+    variable._keras_shape = value.shape\n+    variable._uses_learning_phase = False\n+    return variable\n \n \n def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n@@ -101,6 +112,22 @@ def shape(x):\n     return x.shape\n \n \n+def int_shape(x):\n+    '''Returns the shape of a Keras tensor or a Keras variable as a tuple of\n+    integers or None entries.\n+\n+    # Arguments\n+        x: Tensor or variable.\n+\n+    # Returns\n+        A tuple of integers (or None entries).\n+    '''\n+    if hasattr(x, '_keras_shape'):\n+        return x._keras_shape\n+    else:\n+        raise Exception('Not a Keras tensor:', x)\n+\n+\n def ndim(x):\n     return x.ndim\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#18e5b75f67ed640ff207ae52b425e9e3c0c293be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 44 | Files Changed: 2 | Hunks: 21 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 135 | Churn Cumulative: 12188 | Contributors (this commit): 59 | Commits (past 90d): 11 | Contributors (cumulative): 68 | DMM Complexity: 1.0\n\nDIFF:\n@@ -290,8 +290,8 @@ class Reshape(Layer):\n         '''Find and replace a single missing dimension in an output shape\n         given an input shape.\n \n-        A near direct port of the internal Numpy function _fix_unknown_dimension\n-        in numpy/core/src/multiarray/shape.c\n+        A near direct port of the internal Numpy function\n+        _fix_unknown_dimension in numpy/core/src/multiarray/shape.c\n \n         # Arguments\n             input_shape: shape of array being reshaped\n@@ -332,7 +332,8 @@ class Reshape(Layer):\n         return tuple(output_shape)\n \n     def get_output_shape_for(self, input_shape):\n-        return (input_shape[0],) + self._fix_unknown_dimension(input_shape[1:], self.target_shape)\n+        return (input_shape[0],) + self._fix_unknown_dimension(input_shape[1:],\n+                                                               self.target_shape)\n \n     def call(self, x, mask=None):\n         # In case the target shape is not fully defined,\n@@ -415,7 +416,9 @@ class Flatten(Layer):\n \n     ```python\n         model = Sequential()\n-        model.add(Convolution2D(64, 3, 3, border_mode='same', input_shape=(3, 32, 32)))\n+        model.add(Convolution2D(64, 3, 3,\n+                                border_mode='same',\n+                                input_shape=(3, 32, 32)))\n         # now: model.output_shape == (None, 64, 32, 32)\n \n         model.add(Flatten())\n@@ -509,7 +512,8 @@ class Lambda(Layer):\n             shape[-1] *= 2\n             return tuple(shape)\n \n-        model.add(Lambda(antirectifier, output_shape=antirectifier_output_shape))\n+        model.add(Lambda(antirectifier,\n+                         output_shape=antirectifier_output_shape))\n     ```\n \n     # Arguments\n@@ -553,7 +557,7 @@ class Lambda(Layer):\n \n     def get_output_shape_for(self, input_shape):\n         if self._output_shape is None:\n-            # if TensorFlow, we can infer the output shape directly:\n+            # With TensorFlow, we can infer the output shape directly:\n             if K.backend() == 'tensorflow':\n                 if isinstance(input_shape, list):\n                     xs = [K.placeholder(shape=shape) for shape in input_shape]\n@@ -565,7 +569,7 @@ class Lambda(Layer):\n                     return [K.int_shape(x_elem) for x_elem in x]\n                 else:\n                     return K.int_shape(x)\n-            # otherwise, we default to the input shape\n+            # Otherwise, we default to the input shape.\n             warnings.warn('`output_shape` argument not specified for layer {} '\n                           'and cannot be automatically inferred with the Theano backend. '\n                           'Defaulting to output shape `{}` (same as input shape). '\n@@ -686,16 +690,21 @@ class Dense(Layer):\n             (eg. maxnorm, nonneg), applied to the main weights matrix.\n         b_constraint: instance of the [constraints](../constraints.md) module,\n             applied to the bias.\n-        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n-        input_dim: dimensionality of the input (integer).\n-            This argument (or alternatively, the keyword argument `input_shape`)\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n+        input_dim: dimensionality of the input (integer). This argument\n+            (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n \n     # Input shape\n-        2D tensor with shape: `(nb_samples, input_dim)`.\n+        nD tensor with shape: `(nb_samples, ..., input_dim)`.\n+        The most common situation would be\n+        a 2D input with shape `(nb_samples, input_dim)`.\n \n     # Output shape\n-        2D tensor with shape: `(nb_samples, output_dim)`.\n+        nD tensor with shape: `(nb_samples, ..., output_dim)`.\n+        For instance, for a 2D input with shape `(nb_samples, input_dim)`,\n+        the output would have shape `(nb_samples, output_dim)`.\n     '''\n     def __init__(self, output_dim, init='glorot_uniform',\n                  activation=None, weights=None,\n@@ -716,17 +725,18 @@ class Dense(Layer):\n \n         self.bias = bias\n         self.initial_weights = weights\n-        self.input_spec = [InputSpec(ndim=2)]\n+        self.input_spec = [InputSpec(ndim='2+')]\n \n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_dim,)\n         super(Dense, self).__init__(**kwargs)\n \n     def build(self, input_shape):\n-        assert len(input_shape) == 2\n-        input_dim = input_shape[1]\n+        assert len(input_shape) >= 2\n+        input_dim = input_shape[-1]\n+        self.input_dim = input_dim\n         self.input_spec = [InputSpec(dtype=K.floatx(),\n-                                     shape=(None, input_dim))]\n+                                     ndim='2+')]\n \n         self.W = self.add_weight((input_dim, self.output_dim),\n                                  initializer=self.init,\n@@ -754,8 +764,11 @@ class Dense(Layer):\n         return self.activation(output)\n \n     def get_output_shape_for(self, input_shape):\n-        assert input_shape and len(input_shape) == 2\n-        return (input_shape[0], self.output_dim)\n+        assert input_shape and len(input_shape) >= 2\n+        assert input_shape[-1] and input_shape[-1] == self.input_dim\n+        output_shape = list(input_shape)\n+        output_shape[-1] = self.output_dim\n+        return tuple(output_shape)\n \n     def get_config(self):\n         config = {'output_dim': self.output_dim,\n@@ -838,9 +851,10 @@ class MaxoutDense(Layer):\n             (eg. maxnorm, nonneg), applied to the main weights matrix.\n         b_constraint: instance of the [constraints](../constraints.md) module,\n             applied to the bias.\n-        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n-        input_dim: dimensionality of the input (integer).\n-            This argument (or alternatively, the keyword argument `input_shape`)\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n+        input_dim: dimensionality of the input (integer). This argument\n+            (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n \n     # Input shape\n@@ -852,11 +866,18 @@ class MaxoutDense(Layer):\n     # References\n         - [Maxout Networks](http://arxiv.org/pdf/1302.4389.pdf)\n     '''\n-    def __init__(self, output_dim, nb_feature=4,\n-                 init='glorot_uniform', weights=None,\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None,\n-                 bias=True, input_dim=None, **kwargs):\n+    def __init__(self, output_dim,\n+                 nb_feature=4,\n+                 init='glorot_uniform',\n+                 weights=None,\n+                 W_regularizer=None,\n+                 b_regularizer=None,\n+                 activity_regularizer=None,\n+                 W_constraint=None,\n+                 b_constraint=None,\n+                 bias=True,\n+                 input_dim=None,\n+                 **kwargs):\n         self.output_dim = output_dim\n         self.nb_feature = nb_feature\n         self.init = initializations.get(init)\n@@ -957,9 +978,10 @@ class Highway(Layer):\n             (eg. maxnorm, nonneg), applied to the main weights matrix.\n         b_constraint: instance of the [constraints](../constraints.md) module,\n             applied to the bias.\n-        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n-        input_dim: dimensionality of the input (integer).\n-            This argument (or alternatively, the keyword argument `input_shape`)\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n+        input_dim: dimensionality of the input (integer). This argument\n+            (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n \n     # Input shape\n@@ -971,11 +993,19 @@ class Highway(Layer):\n     # References\n         - [Highway Networks](http://arxiv.org/pdf/1505.00387v2.pdf)\n     '''\n-    def __init__(self, init='glorot_uniform', transform_bias=-2,\n-                 activation=None, weights=None,\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None,\n-                 bias=True, input_dim=None, **kwargs):\n+    def __init__(self,\n+                 init='glorot_uniform',\n+                 transform_bias=-2,\n+                 activation=None,\n+                 weights=None,\n+                 W_regularizer=None,\n+                 b_regularizer=None,\n+                 activity_regularizer=None,\n+                 W_constraint=None,\n+                 b_constraint=None,\n+                 bias=True,\n+                 input_dim=None,\n+                 **kwargs):\n         self.init = initializations.get(init)\n         self.transform_bias = transform_bias\n         self.activation = activations.get(activation)\n@@ -1094,21 +1124,31 @@ class TimeDistributedDense(Layer):\n             (eg. maxnorm, nonneg), applied to the main weights matrix.\n         b_constraint: instance of the [constraints](../constraints.md) module,\n             applied to the bias.\n-        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n-        input_dim: dimensionality of the input (integer).\n-            This argument (or alternatively, the keyword argument `input_shape`)\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n+        input_dim: dimensionality of the input (integer). This argument\n+            (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n         input_length: length of inputs sequences\n             (integer, or None for variable-length sequences).\n     '''\n \n     def __init__(self, output_dim,\n-                 init='glorot_uniform', activation=None, weights=None,\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None,\n-                 bias=True, input_dim=None, input_length=None, **kwargs):\n-        warnings.warn('TimeDistributedDense is deprecated, '\n-                      'please use TimeDistributed(Dense(...)) instead.')\n+                 init='glorot_uniform',\n+                 activation=None,\n+                 weights=None,\n+                 W_regularizer=None,\n+                 b_regularizer=None,\n+                 activity_regularizer=None,\n+                 W_constraint=None,\n+                 b_constraint=None,\n+                 bias=True,\n+                 input_dim=None,\n+                 input_length=None,\n+                 **kwargs):\n+        warnings.warn('`TimeDistributedDense` is deprecated, '\n+                      'And will be removed on May 1st, 2017. '\n+                      'Please use a `Dense` layer instead.')\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n@@ -1162,7 +1202,6 @@ class TimeDistributedDense(Layer):\n         input_shape = self.input_spec[0].shape\n         # x has shape (samples, timesteps, input_dim)\n         input_length = input_shape[1]\n-        # Note: input_length should always be provided when using tensorflow backend.\n         if not input_length:\n             if hasattr(K, 'int_shape'):\n                 input_length = K.int_shape(x)[1]\n\n@@ -325,6 +325,14 @@ def test_dense():\n                kwargs={'output_dim': 3},\n                input_shape=(3, 2))\n \n+    layer_test(core.Dense,\n+               kwargs={'output_dim': 3},\n+               input_shape=(3, 4, 2))\n+\n+    layer_test(core.Dense,\n+               kwargs={'output_dim': 3},\n+               input_shape=(3, 4, 5, 2))\n+\n     layer_test(core.Dense,\n                kwargs={'output_dim': 3,\n                        'W_regularizer': regularizers.l2(0.01),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
