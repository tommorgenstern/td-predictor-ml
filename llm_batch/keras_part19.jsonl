{"custom_id": "keras#560cb94519ad5a5eed3adc323a9e1ad3cf0690e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 79 | Files Changed: 1 | Hunks: 26 | Methods Changed: 10 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 148 | Churn Cumulative: 704 | Contributors (this commit): 4 | Commits (past 90d): 18 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -109,128 +109,113 @@ class BaseLogger(Callback):\n             print('Epoch %d' % epoch)\n             self.progbar = Progbar(target=self.params['nb_sample'], \\\n                 verbose=self.verbose)\n-        self.current = 0\n-        self.tot_loss = 0.\n-        self.tot_acc = 0.\n+        self.seen = 0\n+        self.totals = {}\n \n     def on_batch_begin(self, batch, logs={}):\n-        if self.current < self.params['nb_sample']:\n+        if self.seen < self.params['nb_sample']:\n             self.log_values = []\n \n     def on_batch_end(self, batch, logs={}):\n         batch_size = logs.get('size', 0)\n-        self.current += batch_size\n+        self.seen += batch_size\n+\n+        for k, v in logs.items():\n+            self.log_values.append((k, v))\n+            if k in self.totals:\n+                self.totals[k] += v * batch_size\n+            else:\n+                self.totals[k] = v * batch_size\n \n-        loss = logs.get('loss')\n-        self.log_values.append(('loss', loss))\n-        self.tot_loss += loss * batch_size\n-        if self.params['show_accuracy']:\n-            accuracy = logs.get('accuracy')\n-            self.log_values.append(('acc.', accuracy))\n-            self.tot_acc += accuracy * batch_size\n         # skip progbar update for the last batch; will be handled by on_epoch_end\n-        if self.verbose and self.current < self.params['nb_sample']:\n-            self.progbar.update(self.current, self.log_values)\n+        if self.verbose and self.seen < self.params['nb_sample']:\n+            self.progbar.update(self.seen, self.log_values)\n \n     def on_epoch_end(self, epoch, logs={}):\n-        self.log_values.append(('loss', self.tot_loss / self.current))\n-        if self.params['show_accuracy']:\n-            self.log_values.append(('acc.', self.tot_acc / self.current))\n-        if self.params['do_validation']:\n-            val_loss = logs.get('val_loss')\n-            self.log_values.append(('val. loss', val_loss))\n-            if self.params['show_accuracy']:\n-                val_acc = logs.get('val_accuracy')\n-                self.log_values.append(('val. acc.', val_acc))\n-        self.progbar.update(self.current, self.log_values)\n+        for k, v in self.totals.items():\n+            self.log_values.append((k, self.totals[k] / self.seen))\n+                \n+        for k, v in logs.items():\n+            self.log_values.append((k, v))\n+        self.progbar.update(self.seen, self.log_values)\n \n \n class History(Callback):\n \n     def on_train_begin(self, logs={}):\n         self.epoch = []\n-        self.loss = []\n-        if self.params['show_accuracy']:\n-            self.accuracy = []\n-        if self.params['do_validation']:\n-            self.validation_loss = []\n-            if self.params['show_accuracy']:\n-                self.validation_accuracy = []\n+        self.history = {}\n \n     def on_epoch_begin(self, epoch, logs={}):\n         self.seen = 0\n-        self.tot_loss = 0.\n-        self.tot_accuracy = 0.\n+        self.totals = {}\n \n     def on_batch_end(self, batch, logs={}):\n         batch_size = logs.get('size', 0)\n         self.seen += batch_size\n-        self.tot_loss += logs.get('loss', 0.) * batch_size\n-        if self.params['show_accuracy']:\n-            self.tot_accuracy += logs.get('accuracy', 0.) * batch_size\n+        for k, v in logs.items():\n+            if k in self.totals:\n+                self.totals[k] += v * batch_size\n+            else:\n+                self.totals[k] = v * batch_size\n \n     def on_epoch_end(self, epoch, logs={}):\n-        val_loss = logs.get('val_loss')\n-        val_acc = logs.get('val_accuracy')\n         self.epoch.append(epoch)\n-        self.loss.append(self.tot_loss / self.seen)\n-        if self.params['show_accuracy']:\n-            self.accuracy.append(self.tot_accuracy / self.seen)\n-        if self.params['do_validation']:\n-            self.validation_loss.append(val_loss)\n-            if self.params['show_accuracy']:\n-                self.validation_accuracy.append(val_acc)\n+        for k, v in self.totals.items():\n+            self.history[k] = v / self.seen\n+\n+        for k, v in logs.items():\n+            self.history[k] = v\n \n \n class ModelCheckpoint(Callback):\n-    def __init__(self, filepath, verbose=0, save_best_only=False):\n+    def __init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False):\n         super(Callback, self).__init__()\n         \n+        self.monitor\n         self.verbose = verbose\n         self.filepath = filepath\n         self.save_best_only = save_best_only\n-        self.loss = []\n-        self.best_loss = np.Inf\n-        self.val_loss = []\n-        self.best_val_loss = np.Inf\n+        self.best = np.Inf\n \n     def on_epoch_end(self, epoch, logs={}):\n-        if self.save_best_only and self.params['do_validation']:\n-            cur_val_loss = logs.get('val_loss')\n-            self.val_loss.append(cur_val_loss)\n-            if cur_val_loss < self.best_val_loss:\n+        if self.save_best_only:\n+            current = logs.get(self.monitor)\n+            if current is None:\n+                warnings.warn(\"Can save best model only with %s available, skipping.\" % (self.monitor), RuntimeWarning)\n+            else:\n+                if current < self.best:\n                     if self.verbose > 0:\n-                    print(\"Epoch %05d: validation loss improved from %0.5f to %0.5f, saving model to %s\"\n-                        % (epoch, self.best_val_loss, cur_val_loss, self.filepath))\n-                self.best_val_loss = cur_val_loss\n+                        print(\"Epoch %05d: %s improved from %0.5f to %0.5f, saving model to %s\"\n+                            % (epoch, self.monitor, self.best, current, self.filepath))\n+                    self.best = current\n                     self.model.save_weights(self.filepath, overwrite=True)\n                 else:\n                     if self.verbose > 0:\n-                    print(\"Epoch %05d: validation loss did not improve\" % (epoch))\n-        elif self.save_best_only and not self.params['do_validation']:\n-            warnings.warn(\"Can save best model only with validation data, skipping\", RuntimeWarning)\n-        elif not self.save_best_only:\n+                        print(\"Epoch %05d: %s did not improve\" % (epoch, self.monitor))\n+        else:\n             if self.verbose > 0:\n                 print(\"Epoch %05d: saving model to %s\" % (epoch, self.filepath))\n             self.model.save_weights(self.filepath, overwrite=True)\n \n \n class EarlyStopping(Callback):\n-    def __init__(self, patience=0, verbose=0):\n+    def __init__(self, monitor='val_loss', patience=0, verbose=0):\n         super(Callback, self).__init__()\n \n+        self.monitor = monitor\n         self.patience = patience\n         self.verbose = verbose\n-        self.best_val_loss = np.Inf\n+        self.best = np.Inf\n         self.wait = 0\n \n     def on_epoch_end(self, epoch, logs={}):\n-        if not self.params['do_validation']:\n-            warnings.warn(\"Early stopping requires validation data!\", RuntimeWarning)\n+        current = logs.get(self.monitor)\n+        if current is None:\n+            warnings.warn(\"Early stopping requires %s available!\" % (self.monitor), RuntimeWarning)\n  \n-        cur_val_loss = logs.get('val_loss')\n-        if cur_val_loss < self.best_val_loss:\n-            self.best_val_loss = cur_val_loss\n+        if current < self.best:\n+            self.best = current\n             self.wait = 0\n         else:\n             if self.wait >= self.patience:\n@@ -243,24 +228,29 @@ class EarlyStopping(Callback):\n class RemoteMonitor(Callback):\n     def __init__(self, root='http://localhost:9000'):\n         self.root = root\n-        self.seen = 0\n-        self.tot_loss = 0.\n-        self.tot_accuracy = 0.\n \n     def on_epoch_begin(self, epoch, logs={}):\n         self.seen = 0\n-        self.tot_loss = 0.\n-        self.tot_accuracy = 0.\n+        self.totals = {}\n+        \n \n     def on_batch_end(self, batch, logs={}):\n         batch_size = logs.get('size', 0)\n         self.seen += batch_size\n-        self.tot_loss += logs.get('loss', 0.) * batch_size\n-        if self.params['show_accuracy']:\n-            self.tot_accuracy += logs.get('accuracy', 0.) * batch_size\n+        for k, v in logs.items():\n+            if k in self.totals:\n+                self.totals[k] += v * batch_size\n+            else:\n+                self.totals[k] = v * batch_size\n \n     def on_epoch_end(self, epoch, logs={}):\n         import requests\n-        logs['epoch'] = epoch\n-        logs['loss'] = self.tot_loss / self.seen\n-        r = requests.post(self.root + '/publish/epoch/end/', {'data':json.dumps(logs)})\n+        send = {}\n+        send['epoch'] = epoch\n+\n+        for k, v in self.totals.items():\n+            send[k] = v / self.seen\n+        for k, v in self.logs:\n+            send[k] = v\n+\n+        r = requests.post(self.root + '/publish/epoch/end/', {'data':json.dumps(send)})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24df6a9f9bda51784ce3d41bc23830eae72a118e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -20/0 | Churn Δ: 3 | Churn Cumulative: 1880 | Contributors (this commit): 17 | Commits (past 90d): 78 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -152,7 +152,8 @@ class Model(object):\n     def test(self, X, y, accuracy=False):\n         X = standardize_X(X)\n         y = standardize_y(y)\n-        ins = X + [y]\n+        sample_weight = np.ones(y.shape[:-1] + (1,))\n+        ins = X + [y, sample_weight]\n         if accuracy:\n             return self._test_with_acc(*ins)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3af4d2fd33d56ef0efb7a43dbf4ed4a3855bf27e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 74 | Churn Cumulative: 74 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,74 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337) # for reproducibility\n+\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation\n+from keras.initializations import normal, identity\n+from keras.layers.recurrent import SimpleRNN\n+from keras.optimizers import RMSprop\n+from keras.utils import np_utils\n+\n+'''\n+    This is a variant of IRNN experiment with sequential MNIST in\n+    \"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units \"\n+    by Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton\n+\n+    arXiv:1504.00941v2 [cs.NE] 7 Apr 201\n+    http://arxiv.org/pdf/1504.00941v2.pdf\n+\n+    Optimizer is replaced with RMSprop which give more stable and steady\n+    improvement.\n+\n+    Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_irnn_rmsprop.py\n+\n+    Reaches to 80% train/test accuracy and 0.55 train/test loss after 70 epochs\n+    (it's still underfitting at that point, though).\n+\n+    20-25 minuts per epoch on a GRID K520 GPU.\n+'''\n+\n+batch_size = 16\n+nb_classes = 10\n+nb_epochs = 100000\n+hidden_units = 100\n+\n+learning_rate = 1e-6\n+clip_norm = 1.0\n+BPTT_trancate = 28*28\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape(X_train.shape[0], -1, 1)\n+X_test = X_test.reshape(X_test.shape[0], -1, 1)\n+X_train = X_train.astype(\"float32\")\n+X_test = X_test.astype(\"float32\")\n+X_train /= 255\n+X_test /= 255\n+print('X_train shape:', X_train.shape)\n+print(X_train.shape[0], 'train samples')\n+print(X_test.shape[0], 'test samples')\n+\n+# convert class vectors to binary class matrices\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+\n+model = Sequential()\n+model.add(SimpleRNN(input_dim=1, output_dim=hidden_units,\n+                    init=lambda shape: normal(shape, scale=0.001),\n+                    inner_init=lambda shape: identity(shape, scale=1.0),\n+                    activation='relu', truncate_gradient=BPTT_trancate))\n+model.add(Dense(hidden_units, nb_classes))\n+model.add(Activation('softmax'))\n+rmsprop = RMSprop(lr=learning_rate)\n+model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n+\n+model.fit(X_train, Y_train, batch_size=16, nb_epoch=nb_epochs,\n+          show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n+\n+score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n+print('Test score:', score[0])\n+print('Test accuracy:', score[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#89ad3c726cb416f25adad78795be393956fed4bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 76 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ from keras.utils import np_utils\n \n batch_size = 16\n nb_classes = 10\n-nb_epochs = 100000\n+nb_epochs = 200\n hidden_units = 100\n \n learning_rate = 1e-6\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ae4c9c8df82e0c7bc606d7beb523867b946f748", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 78 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -27,7 +27,7 @@ from keras.utils import np_utils\n     Reaches to 80% train/test accuracy and 0.55 train/test loss after 70 epochs\n     (it's still underfitting at that point, though).\n \n-    20-25 minuts per epoch on a GRID K520 GPU.\n+    About 15 minuts per epoch on a GRID K520 GPU.\n '''\n \n batch_size = 16\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#278618281c1f603e3815ee15a08dbff64fbc4ea6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1215 | Contributors (this commit): 11 | Commits (past 90d): 58 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,7 +20,7 @@ class Layer(object):\n         self.params = []\n \n     def connect(self, layer):\n-        if not hasattr(self, \"get_output_mask\") and layer.get_output_mask() is not None:\n+        if not self.supports_masked_input() and layer.get_output_mask() is not None:\n             raise Exception(\"Attached non-masking layer to layer with masked output\")\n         self.previous = layer\n \n@@ -140,6 +140,12 @@ class Merge(object):\n     def input(self):\n         return self.get_input()\n \n+    def supports_masked_input(self):\n+        return False\n+\n+    def get_output_mask(self, train=None):\n+        return None\n+\n     def get_weights(self):\n         weights = []\n         for m in self.models:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#42497d9fda537b5abc70088bfd3656fc92b5c9ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 171 | Contributors (this commit): 4 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class Embedding(Layer):\n     def get_output_mask(self, train=None):\n         X = self.get_input(train)\n         if not self.mask_zero:\n-            return T.ones_like(X)\n+            return None\n         else:\n             return T.ones_like(X) * (1 - T.eq(X,0))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9fd2c8a8c2639fe8663f753e278a5948fc21f56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 1235 | Contributors (this commit): 11 | Commits (past 90d): 59 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -34,9 +34,25 @@ class Layer(object):\n             return self.input\n \n     def supports_masked_input(self):\n+        ''' Whether or not this layer respects the output mask of its previous layer in its calculations. If you try\n+        to attach a layer that does *not* support masked_input to a layer that gives a non-None output_mask() that is\n+        an error'''\n         return False\n \n     def get_output_mask(self, train=None):\n+        '''\n+        For some models (such as RNNs) you want a way of being able to mark some output data-points as\n+        \"masked\", so they are not used in future calculations. In such a model, get_output_mask() should return a mask\n+        of one less dimension than get_output() (so if get_output is (nb_samples, nb_timesteps, nb_dimensions), then the mask\n+        is (nb_samples, nb_timesteps), with a one for every unmasked datapoint, and a zero for every masked one.\n+\n+        If there is *no* masking then it shall return None. For instance if you attach an Activation layer (they support masking)\n+        to a layer with an output_mask, then that Activation shall also have an output_mask. If you attach it to a layer with no\n+        such mask, then the Activation's get_output_mask shall return None.\n+\n+        Some layers have an output_mask even if their input is unmasked, notably Embedding which can turn the entry \"0\" into\n+        a mask.\n+        '''\n         return None\n \n     def set_weights(self, weights):\n@@ -76,6 +92,10 @@ class Layer(object):\n         return self.params, regularizers, consts\n \n class MaskedLayer(Layer):\n+    '''\n+    If your layer trivially supports masking (by simply copying the input mask to the output), then subclass MaskedLayer\n+    instead of Layer, and make sure that you incorporate the input mask into your calculation of get_output()\n+    '''\n     def supports_masked_input(self):\n         return True\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24be9eb88b899b3cc8ed0470bb4f72e33060698e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 52 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,7 +22,7 @@ class NonNeg(Constraint):\n         p *= T.ge(p, 0)\n         return p\n \n-def UnitNorm(Constraint):\n+class UnitNorm(Constraint):\n     def __call__(self, p):\n         return e / T.sqrt(T.sum(e**2, axis=-1, keepdims=True))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#12a5c6fe46bee6ffa4a54522f464a2c52647822c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 41 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -7,12 +7,13 @@ from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n from keras.initializations import normal, identity\n-from keras.layers.recurrent import SimpleRNN\n+from keras.layers.recurrent import SimpleRNN, LSTM\n from keras.optimizers import RMSprop\n from keras.utils import np_utils\n \n '''\n-    This is a variant of IRNN experiment with sequential MNIST in\n+    This is a reproduction of the IRNN experiment \n+    with pixel-by-pixel sequential MNIST in\n     \"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units \"\n     by Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton\n \n@@ -22,24 +23,20 @@ from keras.utils import np_utils\n     Optimizer is replaced with RMSprop which give more stable and steady\n     improvement.\n \n-    Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_irnn_rmsprop.py\n-\n-    Reaches to 80% train/test accuracy and 0.55 train/test loss after 70 epochs\n+    0.80 train/test accuracy and 0.55 train/test loss after 70 epochs\n     (it's still underfitting at that point, though).\n-\n-    About 15 minuts per epoch on a GRID K520 GPU.\n '''\n \n-batch_size = 16\n+batch_size = 32\n nb_classes = 10\n nb_epochs = 200\n hidden_units = 100\n \n learning_rate = 1e-6\n clip_norm = 1.0\n-BPTT_trancate = 28*28\n+BPTT_truncate = 28*28\n \n-# the data, shuffled and split between tran and test sets\n+# the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n X_train = X_train.reshape(X_train.shape[0], -1, 1)\n@@ -56,11 +53,12 @@ print(X_test.shape[0], 'test samples')\n Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n+print('Evaluate IRNN...')\n model = Sequential()\n model.add(SimpleRNN(input_dim=1, output_dim=hidden_units,\n                     init=lambda shape: normal(shape, scale=0.001),\n                     inner_init=lambda shape: identity(shape, scale=1.0),\n-                    activation='relu', truncate_gradient=BPTT_trancate))\n+                    activation='relu', truncate_gradient=BPTT_truncate))\n model.add(Dense(hidden_units, nb_classes))\n model.add(Activation('softmax'))\n rmsprop = RMSprop(lr=learning_rate)\n@@ -69,6 +67,21 @@ model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n model.fit(X_train, Y_train, batch_size=16, nb_epoch=nb_epochs,\n           show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n \n-score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n-print('Test score:', score[0])\n-print('Test accuracy:', score[1])\n+scores = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n+print('IRNN test score:', scores[0])\n+print('IRNN test accuracy:', scores[1])\n+\n+print('Compare to LSTM...')\n+model = Sequential()\n+model.add(LSTM(1, hidden_units))\n+model.add(Dense(hidden_units, nb_classes))\n+model.add(Activation('softmax'))\n+rmsprop = RMSprop(lr=learning_rate)\n+model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n+\n+model.fit(X_train, Y_train, batch_size=16, nb_epoch=nb_epochs,\n+          show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n+\n+scores = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n+print('LSTM test score:', scores[0])\n+print('LSTM test accuracy:', scores[1])\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a808ae6ff89290a65802769e925610ba123a99f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 20 | Churn Cumulative: 20 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,20 @@\n+import unittest\n+import numpy as np\n+from theano import tensor as T\n+\n+class TestConstraints(unittest.TestCase):\n+    def setUp(self):\n+        self.some_values = [0.1,0.5,3,8,1e-7]\n+        self.example_array = np.random.random((100,100))*100. - 50.\n+        self.example_array[0,0] = 0. # 0 could possibly cause trouble\n+\n+    def test_maxnorm(self):\n+        from keras.constraints import maxnorm\n+\n+        for m in self.some_values:\n+            norm_instance = maxnorm(m)\n+            normed = norm_instance(self.example_array)\n+            assert(np.all(normed.eval() < m))\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b48e39aafd9ef413216444a6bfb97e867aa40e1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 20 | Churn Cumulative: 40 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,5 +16,25 @@ class TestConstraints(unittest.TestCase):\n             normed = norm_instance(self.example_array)\n             assert(np.all(normed.eval() < m))\n \n+    def test_nonneg(self):\n+        from keras.constraints import nonneg\n+\n+        normed = nonneg(self.example_array)\n+        assert(np.all(np.min(normed.eval(),axis=1) == 0.))\n+\n+    def test_identity(self):\n+        from keras.constraints import identity\n+\n+        normed = identity(self.example_array)\n+        assert(np.all(normed == self.example_array))\n+\n+    def test_unitnorm(self):\n+        from keras.constraints import unitnorm\n+\n+        normed = unitnorm(self.example_array)\n+        self.assertAlmostEqual(\n+            np.max(np.abs(np.sqrt(np.sum(normed.eval()**2,axis=1))-1.))\n+            ,0.)\n+\n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4956ed9e9719d7faf7c0a72dd17b1b7164c95021", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 42 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,7 @@ import unittest\n import numpy as np\n from theano import tensor as T\n \n+\n class TestConstraints(unittest.TestCase):\n     def setUp(self):\n         self.some_values = [0.1, 0.5, 3, 8, 1e-7]\n@@ -36,5 +37,6 @@ class TestConstraints(unittest.TestCase):\n             np.max(np.abs(np.sqrt(np.sum(normed.eval() ** 2, axis=1)) - 1.))\n             , 0.)\n \n+\n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d59f2519fa91ce3c6e68845e8d4ec55f469eb8a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 62 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,13 @@\n import unittest\n import numpy as np\n+from numpy.testing import assert_allclose\n from theano import tensor as T\n \n \n class TestConstraints(unittest.TestCase):\n     def setUp(self):\n         self.some_values = [0.1, 0.5, 3, 8, 1e-7]\n+        np.random.seed(3537)\n         self.example_array = np.random.random((100, 100)) * 100. - 50.\n         self.example_array[0, 0] = 0.  # 0 could possibly cause trouble\n \n@@ -17,6 +19,14 @@ class TestConstraints(unittest.TestCase):\n             normed = norm_instance(self.example_array)\n             assert (np.all(normed.eval() < m))\n \n+        # a more explicit example\n+        norm_instance = maxnorm(2.0)\n+        x = np.array([[0, 0, 0], [1.0, 0, 0], [3, 0, 0], [3, 3, 3]]).T\n+        x_normed_target = np.array([[0, 0, 0], [1.0, 0, 0], [2.0, 0, 0], [2./np.sqrt(3), 2./np.sqrt(3), 2./np.sqrt(3)]]).T\n+        x_normed_actual = norm_instance(x).eval()\n+        assert_allclose(x_normed_actual, x_normed_target)\n+\n+\n     def test_nonneg(self):\n         from keras.constraints import nonneg\n \n@@ -32,10 +42,12 @@ class TestConstraints(unittest.TestCase):\n     def test_unitnorm(self):\n         from keras.constraints import unitnorm\n \n-        normed = unitnorm(self.example_array)\n-        self.assertAlmostEqual(\n-            np.max(np.abs(np.sqrt(np.sum(normed.eval() ** 2, axis=1)) - 1.))\n-            , 0.)\n+        normalized = unitnorm(self.example_array)\n+\n+        norm_of_normalized = np.sqrt(np.sum(normalized.eval()**2))\n+        difference = norm_of_normalized - 1. #in the unit norm constraint, it should be equal to 1.\n+        largest_difference = np.max(np.abs(difference))\n+        self.assertAlmostEqual(largest_difference, 0.)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f184db8c53f547a3d48460e4992b12a5a87a2683", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 74 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,6 @@ class TestConstraints(unittest.TestCase):\n         x_normed_actual = norm_instance(x).eval()\n         assert_allclose(x_normed_actual, x_normed_target)\n \n-\n     def test_nonneg(self):\n         from keras.constraints import nonneg\n \n@@ -39,6 +38,17 @@ class TestConstraints(unittest.TestCase):\n         normed = identity(self.example_array)\n         assert (np.all(normed == self.example_array))\n \n+    def test_identity_oddballs(self):\n+        \"\"\"\n+        test the identity constraint on some more exotic input.\n+        this does not need to pass for the desired real life behaviour,\n+        but it should in the current implementation.\n+        \"\"\"\n+        from keras.constraints import identity\n+\n+        oddball_examples = [\"Hello\", [1], -1, None]\n+        assert(oddball_examples == identity(oddball_examples))\n+\n     def test_unitnorm(self):\n         from keras.constraints import unitnorm\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#514ac06c2e76e0ccf03e252629ba7ab57c805265", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 77 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -54,11 +54,10 @@ class TestConstraints(unittest.TestCase):\n \n         normalized = unitnorm(self.example_array)\n \n-        norm_of_normalized = np.sqrt(np.sum(normalized.eval()**2))\n+        norm_of_normalized = np.sqrt(np.sum(normalized.eval()**2, axis=1))\n         difference = norm_of_normalized - 1. #in the unit norm constraint, it should be equal to 1.\n         largest_difference = np.max(np.abs(difference))\n         self.assertAlmostEqual(largest_difference, 0.)\n \n-\n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#acef25270349905926730a47cbdf4380ece5ad94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 91 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,13 +29,17 @@ class TestConstraints(unittest.TestCase):\n     def test_nonneg(self):\n         from keras.constraints import nonneg\n \n-        normed = nonneg(self.example_array)\n+        nonneg_instance = nonneg()\n+\n+        normed = nonneg_instance(self.example_array)\n         assert (np.all(np.min(normed.eval(), axis=1) == 0.))\n \n     def test_identity(self):\n         from keras.constraints import identity\n \n-        normed = identity(self.example_array)\n+        identity_instance = identity()\n+\n+        normed = identity_instance(self.example_array)\n         assert (np.all(normed == self.example_array))\n \n     def test_identity_oddballs(self):\n@@ -45,14 +49,16 @@ class TestConstraints(unittest.TestCase):\n         but it should in the current implementation.\n         \"\"\"\n         from keras.constraints import identity\n+        identity_instance = identity()\n \n         oddball_examples = [\"Hello\", [1], -1, None]\n-        assert(oddball_examples == identity(oddball_examples))\n+        assert(oddball_examples == identity_instance(oddball_examples))\n \n     def test_unitnorm(self):\n         from keras.constraints import unitnorm\n+        unitnorm_instance = unitnorm()\n \n-        normalized = unitnorm(self.example_array)\n+        normalized = unitnorm_instance(self.example_array)\n \n         norm_of_normalized = np.sqrt(np.sum(normalized.eval()**2, axis=1))\n         difference = norm_of_normalized - 1. #in the unit norm constraint, it should be equal to 1.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bba53793057c0cdf068e67d17ae90e39b90ae24c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 56 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ class NonNeg(Constraint):\n \n class UnitNorm(Constraint):\n     def __call__(self, p):\n-        return e / T.sqrt(T.sum(e**2, axis=-1, keepdims=True))\n+        return p / T.sqrt(T.sum(p**2, axis=-1, keepdims=True))\n \n identity = Constraint\n maxnorm = MaxNorm\n\n@@ -14,7 +14,7 @@ class TestConcatenation(unittest.TestCase):\n \n     def test_unitnorm_constraint(self):\n         lookup = Sequential()\n-        lookup.add(Embedding(3, 2, weights=[self.W1], W_constraint=unitnorm))\n+        lookup.add(Embedding(3, 2, weights=[self.W1], W_constraint=unitnorm()))\n         lookup.add(Flatten())\n         lookup.add(Dense(2, 1))\n         lookup.add(Activation('sigmoid'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06d7c7dab271624f4f7f65819a3afeadabc3792b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 202 | Contributors (this commit): 4 | Commits (past 90d): 17 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,6 +28,8 @@ def categorical_crossentropy(y_true, y_pred):\n     return cce\n \n def binary_crossentropy(y_true, y_pred):\n+    if y_true.shape[-1] != 1:\n+        raise Exception(\"binary_crossentropy can only be used with scalar outputs.\")\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     bce = T.nnet.binary_crossentropy(y_pred, y_true)\n     return bce\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ee730496eecdfcc4bf22915bb349c5ddcbe9530c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 130 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 130 | Churn Cumulative: 130 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,130 @@\n+from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337)\n+from keras.utils.test_utils import get_test_data\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation, TimeDistributedDense, Flatten\n+from keras.layers.recurrent import GRU\n+from keras.layers.convolutional import Convolution2D\n+from keras.utils.np_utils import to_categorical\n+import unittest\n+\n+class TestRegularizers(unittest.TestCase):\n+    def test_vector_clf(self):\n+        nb_hidden = 10\n+\n+        print('vector classification data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(10,),\n+            classification=True, nb_class=2)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        y_train = to_categorical(y_train)\n+        y_test = to_categorical(y_test)\n+\n+        model = Sequential()\n+        model.add(Dense(X_train.shape[-1], nb_hidden))\n+        model.add(Activation('relu'))\n+        model.add(Dense(nb_hidden, y_train.shape[-1]))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n+        self.assertTrue(history.validation_accuracy[-1] > 0.9)\n+\n+    def test_vector_reg(self):\n+        nb_hidden = 10\n+        print('vector regression data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(10,), output_shape=(2,),\n+            classification=False)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        model = Sequential()\n+        model.add(Dense(X_train.shape[-1], nb_hidden))\n+        model.add(Activation('relu'))\n+        model.add(Dense(nb_hidden, y_train.shape[-1]))\n+        model.compile(loss='hinge', optimizer='rmsprop')\n+        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n+        self.assertTrue(history.validation_loss[-1] < 0.75)\n+\n+    def test_temporal_clf(self):\n+        print('temporal classification data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(5,10), \n+            classification=True, nb_class=2)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        y_train = to_categorical(y_train)\n+        y_test = to_categorical(y_test)\n+\n+        model = Sequential()\n+        model.add(GRU(X_train.shape[-1], y_train.shape[-1]))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n+        self.assertTrue(history.validation_accuracy[-1] > 0.9)\n+\n+    def test_temporal_reg(self):\n+        print('temporal regression data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(5, 10), output_shape=(2,),\n+            classification=False)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        model = Sequential()\n+        model.add(GRU(X_train.shape[-1], y_train.shape[-1]))\n+        model.compile(loss='hinge', optimizer='rmsprop')\n+        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n+        self.assertTrue(history.validation_loss[-1] < 0.75)\n+\n+\n+    def test_seq_to_seq(self):\n+        print('sequence to sequence data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(5, 10), output_shape=(5, 10),\n+            classification=False)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        model = Sequential()\n+        model.add(TimeDistributedDense(X_train.shape[-1], y_train.shape[-1]))\n+        model.compile(loss='hinge', optimizer='rmsprop')\n+        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n+        self.assertTrue(history.validation_loss[-1] < 0.75)\n+\n+\n+    def test_img_clf(self):\n+        print('image classification data:')\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(3, 32, 32), \n+            classification=True, nb_class=2)\n+        print('X_train:', X_train.shape)\n+        print('X_test:', X_test.shape)\n+        print('y_train:', y_train.shape)\n+        print('y_test:', y_test.shape)\n+\n+        y_train = to_categorical(y_train)\n+        y_test = to_categorical(y_test)\n+\n+        model = Sequential()\n+        model.add(Convolution2D(32, 3, 32, 32))\n+        model.add(Activation('relu'))\n+        model.add(Flatten())\n+        model.add(Dense(32, y_test.shape[-1]))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n+        self.assertTrue(history.validation_accuracy[-1] > 0.9)\n+\n+\n+if __name__ == '__main__':\n+    print('Test different types of classification and regression tasks')\n+    unittest.main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#940bd47bc968857da7c3f227ba1244300350f376", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 79 | Churn Cumulative: 79 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,79 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation\n+from keras.utils import np_utils\n+import numpy as np\n+import unittest\n+\n+nb_classes = 10\n+batch_size = 128\n+nb_epoch = 5\n+weighted_class = 9\n+standard_weight = 1\n+high_weight = 5\n+max_train_samples = 5000\n+max_test_samples = 1000\n+\n+np.random.seed(1337) # for reproducibility\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+X_train = X_train.reshape(60000, 784)[:max_train_samples]\n+X_test = X_test.reshape(10000, 784)[:max_test_samples]\n+X_train = X_train.astype(\"float32\") / 255\n+X_test = X_test.astype(\"float32\") / 255\n+\n+# convert class vectors to binary class matrices\n+y_train = y_train[:max_train_samples]\n+y_test = y_test[:max_test_samples]\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+test_ids = np.where(y_test == np.array(weighted_class))[0]\n+\n+def create_model():\n+    model = Sequential()\n+    model.add(Dense(784, 50))\n+    model.add(Activation('relu'))\n+    model.add(Dense(50, 10))\n+    model.add(Activation('softmax'))\n+    return model\n+\n+def test_weights(model, class_weight=None, sample_weight=None):\n+    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, \\\n+        class_weight=class_weight, sample_weight=sample_weight)\n+    score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n+    return score\n+\n+class TestConcatenation(unittest.TestCase):\n+\n+    def test_loss_weighting(self):\n+        class_weight = dict([(i, standard_weight) for i in range(nb_classes)])\n+        class_weight[weighted_class] = high_weight\n+\n+        sample_weight = np.ones((y_train.shape[0])) * standard_weight\n+        sample_weight[y_train == weighted_class] = high_weight\n+\n+        for loss in ['mae', 'mse', 'categorical_crossentropy']:\n+            print('loss:', loss)\n+            # no weights: reference point\n+            model = create_model()\n+            model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+            standard_score = test_weights(model)\n+            # test class_weight\n+            model = create_model()\n+            model.compile(loss=loss, optimizer='rmsprop')\n+            score = test_weights(model, class_weight=class_weight)\n+            print('score:', score, ' vs.', standard_score)\n+            self.assertTrue(score < standard_score)\n+            # test sample_weight\n+            model = create_model()\n+            model.compile(loss=loss, optimizer='rmsprop')\n+            score = test_weights(model, sample_weight=sample_weight)\n+            print('score:', score, ' vs.', standard_score)\n+            self.assertTrue(score < standard_score)\n+\n+if __name__ == '__main__':\n+    print('Test class_weight and sample_weight')\n+    unittest.main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0332b95cdf622bf3a6fbe66f5d4eda5ee482dd90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 204 | Contributors (this commit): 4 | Commits (past 90d): 18 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,8 +28,6 @@ def categorical_crossentropy(y_true, y_pred):\n     return cce\n \n def binary_crossentropy(y_true, y_pred):\n-    if y_true.shape[-1] != 1:\n-        raise Exception(\"binary_crossentropy can only be used with scalar outputs.\")\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     bce = T.nnet.binary_crossentropy(y_pred, y_true)\n     return bce\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
