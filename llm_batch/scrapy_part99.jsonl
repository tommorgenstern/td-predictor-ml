{"custom_id": "scrapy#67e42d1bfd339da5b161f515c7c77e26cf981803", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 14/15 | Churn Δ: 12 | Churn Cumulative: 523 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,8 +2,6 @@\n Base class for Scrapy commands\n \"\"\"\n \n-# TODO: move this module to scrapy/command.py for Scrapy 0.10\n-\n from __future__ import with_statement\n \n import os\n\n@@ -1,10 +0,0 @@\n-# TODO: remove this module for Scrapy 0.10\n-import warnings\n-\n-from scrapy import cmdline\n-\n-def execute():\n-    warnings.warn(\"scrapy.command.cmdline.execute() is deprecated, modify your \" \\\n-        \"project-ctl.py script to use scrapy.cmdline.execute() instead\", \\\n-        DeprecationWarning, stacklevel=2)\n-    cmdline.execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5139843c5c99d0050443f7e815d2aa672ddd491f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 521 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -92,7 +92,9 @@ class Scraper(object):\n             site.closing.callback(None)\n \n     def enqueue_scrape(self, response, request, spider):\n-        site = self.sites[spider]\n+        site = self.sites.get(spider, None)\n+        if site is None:\n+            return\n         dfd = site.add_response_request(response, request)\n         def finish_scraping(_):\n             site.finish_response(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2aa84073aba24d54d65747d8bdef62c93f8f1c0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 294 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,6 +38,18 @@ class ScrapyFileLogObserver(log.FileLogObserver):\n         log.FileLogObserver.__init__(self, f)\n \n     def emit(self, eventDict):\n+        ev = _adapt_eventdict(eventDict, self.level, self.encoding)\n+        if ev is not None:\n+            log.FileLogObserver.emit(self, ev)\n+\n+def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8'):\n+    \"\"\"Adapt Twisted log eventDict making it suitable for logging with a Scrapy\n+    log observer. It may return None to indicate that the event should be\n+    ignored by a Scrapy log observer.\n+\n+    `log_level` is the minimum level being logged, and `encoding` is the log\n+    encoding.\n+    \"\"\"\n     ev = eventDict.copy()\n     if ev['isError']:\n         ev.setdefault('logLevel', ERROR)\n@@ -45,7 +57,7 @@ class ScrapyFileLogObserver(log.FileLogObserver):\n     if ev.get('system') != 'scrapy' and not ev['isError']:\n         return\n     level = ev.get('logLevel')\n-        if level < self.level:\n+    if level < log_level:\n         return\n     spider = ev.get('spider')\n     if spider:\n@@ -53,15 +65,14 @@ class ScrapyFileLogObserver(log.FileLogObserver):\n     message = ev.get('message')\n     lvlname = level_names.get(level, 'NOLEVEL')\n     if message:\n-            message = [unicode_to_str(x, self.encoding) for x in message]\n+        message = [unicode_to_str(x, encoding) for x in message]\n         message[0] = \"%s: %s\" % (lvlname, message[0])\n     ev['message'] = message\n     why = ev.get('why')\n     if why:\n-            why = \"%s: %s\" % (lvlname, unicode_to_str(why, self.encoding))\n+        why = \"%s: %s\" % (lvlname, unicode_to_str(why, encoding))\n     ev['why'] = why\n-        log.FileLogObserver.emit(self, ev)\n-\n+    return ev\n \n def _get_log_level(level_name_or_id=None):\n     if level_name_or_id is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8de0dc3647b0e9e317303fe715b3f32b1df741b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 48 | Churn Cumulative: 1491 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,6 @@ from time import time\n \n from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n-from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import log\n from scrapy.stats import stats\n@@ -228,15 +227,13 @@ class ExecutionEngine(object):\n         next loop and this function is guaranteed to be called (at least) once\n         again for this spider.\n         \"\"\"\n-        try:\n-            dispatcher.send(signal=signals.spider_idle, sender=self.__class__, \\\n-                spider=spider)\n-        except DontCloseSpider:\n+        res = send_catch_log(signal=signals.spider_idle, sender=self.__class__, \\\n+            spider=spider, dont_log=DontCloseSpider)\n+        if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n+                for _, x in res):\n             reactor.callLater(5, self.next_request, spider)\n             return\n-        except Exception, e:\n-            log.msg(\"Exception caught on 'spider_idle' signal dispatch: %r\" % e, \\\n-                level=log.ERROR)\n+\n         if self.spider_is_idle(spider):\n             self.close_spider(spider, reason='finished')\n \n\n@@ -1,16 +1,29 @@\n \"\"\"Helper functinos for working with signals\"\"\"\n \n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.xlib.pydispatch.robust import sendRobust\n+from twisted.python.failure import Failure\n+\n+from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, getAllReceivers\n+from scrapy.xlib.pydispatch.robustapply import robustApply\n+\n from scrapy import log\n \n-def send_catch_log(*args, **kwargs):\n-    \"\"\"Same as dispatcher.robust.sendRobust but logs any exceptions raised by\n-    the signal handlers\n+def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n+    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n+    Failures instead of exceptions.\n     \"\"\"\n-    results = sendRobust(*args, **kwargs)\n-    for receiver, response in results:\n-        if isinstance(response, Exception):\n-            log.msg(\"Exception caught on signal dispatch: receiver=%r, \" \\\n-                \" exception=%r\" % (receiver, response), level=log.ERROR)\n-    return results\n+    dont_log = named.pop('dont_log', None)\n+    spider = named.get('spider', None)\n+    responses = []\n+    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n+        try:\n+            response = robustApply(receiver, signal=signal, sender=sender,\n+                *arguments, **named)\n+        except dont_log:\n+            result = Failure()\n+        except Exception:\n+            result = Failure()\n+            log.err(result, \"Signal handler error\", spider=spider)\n+        else:\n+            result = response\n+        responses.append((receiver, result))\n+    return responses\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb2e0de7dae90bd4563c12314c52d0fc55f50591", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 71 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,6 @@\n-import unittest\n-\n+from twisted.trial import unittest\n from twisted.python import log as txlog\n+from twisted.python.failure import Failure\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.utils.signal import send_catch_log\n@@ -36,13 +36,10 @@ class SignalUtilsTest(unittest.TestCase):\n         assert test_handler_check in handlers_called\n         assert log_received in handlers_called\n         self.assertEqual(result[0][0], test_handler_error)\n-        self.assert_(isinstance(result[0][1], Exception))\n+        self.assert_(isinstance(result[0][1], Failure))\n         self.assertEqual(result[1], (test_handler_check, \"OK\"))\n \n         txlog.removeObserver(log_received)\n+        self.flushLoggedErrors()\n         dispatcher.disconnect(test_handler_error, signal=test_signal)\n         dispatcher.disconnect(test_handler_check, signal=test_signal)\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d121898e20b2cd0b3f06ee9f0a3928377f0eb81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 82 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,7 +23,8 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n             result = Failure()\n         except Exception:\n             result = Failure()\n-            log.err(result, \"Signal handler error\", spider=spider)\n+            log.err(result, \"Error caught on signal handler: %s\" % receiver, \\\n+                spider=spider)\n         else:\n             result = response\n         responses.append((receiver, result))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5aa8e639574a2195ce5e110163f842cd69a71fde", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 11 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 2070 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,10 +19,10 @@ default_timeout = settings.getint('DOWNLOAD_TIMEOUT')\n \n def _create_factory(request, spider):\n     def _download_signals(response):\n-        send_catch_log(signal=signals.request_uploaded, \\\n-                sender='download_http', request=request, spider=spider)\n-        send_catch_log(signal=signals.response_downloaded, \\\n-                sender='download_http', response=response, spider=spider)\n+        send_catch_log(signal=signals.request_uploaded, request=request, \\\n+            spider=spider)\n+        send_catch_log(signal=signals.response_downloaded, response=response, \\\n+            spider=spider)\n         return response\n \n     timeout = getattr(spider, \"download_timeout\", None) or default_timeout\n\n@@ -78,9 +78,9 @@ class DownloaderMiddlewareManager(object):\n                     (method.im_self.__class__.__name__, type(response))\n                 if isinstance(response, Request):\n                     send_catch_log(signal=signals.response_received, \\\n-                        sender=self.__class__, response=response, spider=spider)\n+                        response=response, spider=spider)\n                     return response\n-            send_catch_log(signal=signals.response_received, sender=self.__class__, \\\n+            send_catch_log(signal=signals.response_received, \\\n                 response=response, spider=spider)\n             return response\n \n\n@@ -47,7 +47,7 @@ class ExecutionEngine(object):\n         \"\"\"Start the execution engine\"\"\"\n         assert not self.running, \"Engine already running\"\n         self.start_time = time()\n-        send_catch_log(signal=signals.engine_started, sender=self.__class__)\n+        send_catch_log(signal=signals.engine_started)\n         self.running = True\n \n     def stop(self):\n@@ -216,7 +216,7 @@ class ExecutionEngine(object):\n         self.downloader.open_spider(spider)\n         self.scraper.open_spider(spider)\n         stats.open_spider(spider)\n-        send_catch_log(signals.spider_opened, sender=self.__class__, spider=spider)\n+        send_catch_log(signals.spider_opened, spider=spider)\n         self.next_request(spider)\n \n     def _spider_idle(self, spider):\n@@ -227,7 +227,7 @@ class ExecutionEngine(object):\n         next loop and this function is guaranteed to be called (at least) once\n         again for this spider.\n         \"\"\"\n-        res = send_catch_log(signal=signals.spider_idle, sender=self.__class__, \\\n+        res = send_catch_log(signal=signals.spider_idle, \\\n             spider=spider, dont_log=DontCloseSpider)\n         if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n                 for _, x in res):\n@@ -264,8 +264,7 @@ class ExecutionEngine(object):\n     def _finish_closing_spider(self, spider):\n         \"\"\"This function is called after the spider has been closed\"\"\"\n         reason = self.closing.pop(spider, 'finished')\n-        send_catch_log(signal=signals.spider_closed, sender=self.__class__, \\\n-            spider=spider, reason=reason)\n+        send_catch_log(signal=signals.spider_closed, spider=spider, reason=reason)\n         call = self._next_request_calls.pop(spider, None)\n         if call and call.active():\n             call.cancel()\n@@ -280,4 +279,4 @@ class ExecutionEngine(object):\n         return dfd\n \n     def _finish_stopping_engine(self):\n-        send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n+        send_catch_log(signal=signals.engine_stopped)\n\n@@ -168,7 +168,7 @@ class Scraper(object):\n         elif isinstance(output, BaseItem):\n             log.msg(\"Scraped %s in <%s>\" % (output, request.url), level=log.DEBUG, \\\n                 spider=spider)\n-            send_catch_log(signal=signals.item_scraped, sender=self.__class__, \\\n+            send_catch_log(signal=signals.item_scraped, \\\n                 item=output, spider=spider, response=response)\n             self.sites[spider].itemproc_size += 1\n             # FIXME: this can't be called here because the stats spider may be\n@@ -207,13 +207,13 @@ class Scraper(object):\n             ex = output.value\n             if isinstance(ex, DropItem):\n                 log.msg(\"Dropped %s - %s\" % (item, str(ex)), level=log.WARNING, spider=spider)\n-                send_catch_log(signal=signals.item_dropped, sender=self.__class__, \\\n+                send_catch_log(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.msg('Error processing %s - %s' % (item, output), \\\n                     log.ERROR, spider=spider)\n         else:\n             log.msg(\"Passed %s\" % item, log.INFO, spider=spider)\n-            send_catch_log(signal=signals.item_passed, sender=self.__class__, \\\n+            send_catch_log(signal=signals.item_passed, \\\n                 item=item, spider=spider, output=output)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#355c615c2c0603fa22cf2e6123f5e2ae7f617b8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 61 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 61 | Churn Cumulative: 82 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,61 +0,0 @@\n-\"\"\"\n-SpiderProfiler is an extension that hooks itself into every Request callback\n-returned from spiders to measure the processing time and memory allocation\n-caused by spiders code.\n-\n-The results are collected using the StatsCollector.\n-\n-This extension introduces a big impact on crawling performance, so enable only\n-for debugging.\n-\"\"\"\n-\n-from time import time\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.memory import get_vmvalue_from_procfs\n-from scrapy.stats import stats\n-from scrapy.conf import settings\n-\n-class SpiderProfiler(object):\n-    \n-    def __init__(self):\n-        if not settings.getbool('SPIDERPROFILER_ENABLED'):\n-            raise NotConfigured\n-        try:\n-            get_vmvalue_from_procfs('VmSize')\n-        except RuntimeError:\n-            self._mem_tracking = False\n-        else:\n-            self._mem_tracking = True\n-        dispatcher.connect(self._request_received, signals.request_received)\n-\n-    def _request_received(self, request, spider):\n-        request.callback = self._profiled_callback(request.callback, spider)\n-\n-    def _profiled_callback(self, function, spider):\n-        def new_callback(*args, **kwargs):\n-            tbefore = time()\n-            mbefore = self._memusage()\n-            r = function(*args, **kwargs)\n-            mafter = self._memusage()\n-            ct = time() - tbefore\n-            tcc = stats.get_value('profiling/total_callback_time', 0, spider=spider)\n-            sct = stats.get_value('profiling/slowest_callback_time', 0, spider=spider)\n-            stats.set_value('profiling/total_callback_time', tcc+ct, spider=spider)\n-            if ct > sct:\n-                stats.set_value('profiling/slowest_callback_time', ct, spider=spider)\n-                stats.set_value('profiling/slowest_callback_name', function.__name__, \\\n-                    spider=spider)\n-                stats.set_value('profiling/slowest_callback_url', args[0].url, \\\n-                    spider=spider)\n-            if self._memusage:\n-                stats.inc_value('profiling/total_mem_allocated_in_callbacks', \\\n-                    count=mafter-mbefore, spider=spider)\n-            return r\n-        return new_callback\n-\n-    def _memusage(self):\n-        return get_vmvalue_from_procfs('VmSize') if self._mem_tracking else 0.0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c359a34d7da5f513c225b4984c8d9cd44d4d3885", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 77 | Files Changed: 40 | Hunks: 41 | Methods Changed: 4 | Complexity Δ (Sum/Max): 37/20 | Churn Δ: 159 | Churn Cumulative: 5570 | Contributors (this commit): 8 | Commits (past 90d): 76 | Contributors (cumulative): 97 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@\n # Don't forget to add your pipeline to the ITEM_PIPELINES setting\n # See: http://doc.scrapy.org/topics/item-pipeline.html\n \n-from scrapy.core.exceptions import DropItem\n+from scrapy.exceptions import DropItem\n \n class FilterWordsPipeline(object):\n     \"\"\"\n\n@@ -1,4 +1,4 @@\n-from scrapy.core.exceptions import DropItem\n+from scrapy.exceptions import DropItem\n \n class FilterWordsPipeline(object):\n     \"\"\"A pipeline for filtering out items which contain certain words in their\n\n@@ -8,7 +8,7 @@ import cPickle as pickle\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy.http import Headers\n-from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n+from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.core.downloader.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.http import headers_dict_to_raw, headers_raw_to_dict\n\n@@ -4,7 +4,7 @@ from urllib2 import _parse_proxy\n from urlparse import urlunparse\n \n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n \n \n class HttpProxyMiddleware(object):\n\n@@ -2,7 +2,7 @@ from scrapy import log\n from scrapy.http import HtmlResponse\n from scrapy.utils.url import urljoin_rfc\n from scrapy.utils.response import get_meta_refresh\n-from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest\n from scrapy.conf import settings\n \n \n\n@@ -10,7 +10,7 @@ from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.core.manager import scrapymanager\n-from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n+from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.conf import settings\n\n@@ -1,4 +1,4 @@\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.utils.request import request_httprepr\n from scrapy.utils.response import response_httprepr\n from scrapy.stats import stats\n\n@@ -28,7 +28,7 @@ import cPickle as pickle\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core.manager import scrapymanager\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.core import signals\n from scrapy.utils.response import response_httprepr\n from scrapy.stats import stats\n\n@@ -11,7 +11,7 @@ import libxml2\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n from scrapy import log\n\n@@ -14,7 +14,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy import log\n from scrapy.core.manager import scrapymanager\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n from scrapy.stats import stats\n\n@@ -5,7 +5,7 @@ See documentation in docs/item-pipeline.rst\n \"\"\"\n \n from scrapy import log\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.item import BaseItem\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import defer_succeed, mustbe_deferred\n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/item-pipeline.rst\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.contrib import exporter\n from scrapy.conf import settings\n \n\n@@ -21,7 +21,7 @@ from scrapy import log\n from scrapy.stats import stats\n from scrapy.utils.misc import md5sum\n from scrapy.core import signals\n-from scrapy.core.exceptions import DropItem, NotConfigured, IgnoreRequest\n+from scrapy.exceptions import DropItem, NotConfigured, IgnoreRequest\n from scrapy.contrib.pipeline.media import MediaPipeline\n from scrapy.conf import settings\n \n\n@@ -4,7 +4,7 @@ DuplicatesFilterMiddleware: Filter out already visited urls\n See documentation in docs/topics/scheduler-middleware.rst\n \"\"\"\n \n-from scrapy.core.exceptions import IgnoreRequest, NotConfigured\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n \n\n@@ -10,7 +10,7 @@ from collections import defaultdict\n \n from scrapy.core import signals\n from scrapy.core.manager import scrapymanager\n-from scrapy.core.exceptions import NotConfigured, DontCloseSpider\n+from scrapy.exceptions import NotConfigured, DontCloseSpider\n from scrapy.conf import settings\n \n \n\n@@ -3,7 +3,7 @@ HttpError Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n-from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest\n \n \n class HttpError(IgnoreRequest):\n\n@@ -8,7 +8,7 @@ from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.core.manager import scrapymanager\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.conf import settings\n from scrapy.http import Request\n from scrapy import log\n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/spider-middleware.rst\n \n from scrapy import log\n from scrapy.http import Request\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.conf import settings\n \n class UrlLengthMiddleware(object):\n\n@@ -10,7 +10,7 @@ from scrapy.item import BaseItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n from scrapy.selector import XmlXPathSelector, HtmlXPathSelector\n-from scrapy.core.exceptions import NotConfigured, NotSupported\n+from scrapy.exceptions import NotConfigured, NotSupported\n \n class XMLFeedSpider(InitSpider):\n     \"\"\"\n\n@@ -9,7 +9,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.stats import stats, signals\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n \n class StatsMailer(object):\n \n\n@@ -1,6 +1,6 @@\n \"\"\"Download handlers for different schemes\"\"\"\n \n-from scrapy.core.exceptions import NotSupported\n+from scrapy.exceptions import NotSupported\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.conf import settings\n from scrapy.utils.misc import load_object\n\n@@ -3,7 +3,7 @@\n from twisted.internet import reactor\n \n from scrapy.core import signals\n-from scrapy.core.exceptions import NotSupported\n+from scrapy.exceptions import NotSupported\n from scrapy.utils.signal import send_catch_log\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n\n@@ -8,7 +8,7 @@ from time import time\n from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n \n-from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest\n from scrapy.conf import settings\n from scrapy.utils.defer import mustbe_deferred\n from scrapy import log\n\n@@ -10,7 +10,7 @@ from scrapy.core import signals\n from scrapy.utils.signal import send_catch_log\n from scrapy import log\n from scrapy.http import Request, Response\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.conf import build_component_list\n\n@@ -15,7 +15,7 @@ from scrapy.conf import settings\n from scrapy.core import signals\n from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n-from scrapy.core.exceptions import IgnoreRequest, DontCloseSpider\n+from scrapy.exceptions import IgnoreRequest, DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.spider import spiders\n from scrapy.utils.misc import load_object\n\n@@ -1,40 +1,5 @@\n-\"\"\"\n-Scrapy core exceptions\n+from scrapy.exceptions import *\n \n-These exceptions are documented in docs/topics/exceptions.rst. Please don't add\n-new exceptions here without documenting them there.\n-\"\"\"\n-\n-from scrapy import log\n-\n-# Internal\n-\n-class NotConfigured(Exception):\n-    \"\"\"Indicates a missing configuration situation\"\"\"\n-    pass\n-\n-# HTTP and crawling\n-\n-class IgnoreRequest(Exception):\n-    \"\"\"Indicates a decision was made not to process a request\"\"\"\n-\n-    def __init__(self, msg='', level=log.ERROR):\n-        self.msg = msg\n-        self.level = level\n-\n-    def __str__(self):\n-        return self.msg\n-\n-class DontCloseSpider(Exception):\n-    \"\"\"Request the spider not to be closed yet\"\"\"\n-    pass\n-\n-# Items\n-\n-class DropItem(Exception):\n-    \"\"\"Drop item from the item pipeline\"\"\"\n-    pass\n-\n-class NotSupported(Exception):\n-    \"\"\"Indicates a feature or method is not supported\"\"\"\n-    pass\n+import warnings\n+warnings.warn(\"scrapy.core.exceptions is deprecated, use scrapy.exceptions instead\", \\\n+    DeprecationWarning, stacklevel=2)\n\n@@ -7,7 +7,7 @@ from twisted.python.failure import Failure\n \n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n from scrapy.core.schedulermw import SchedulerMiddlewareManager\n-from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest\n from scrapy.conf import settings\n \n class Scheduler(object):\n\n@@ -9,7 +9,7 @@ from collections import defaultdict\n from twisted.internet.defer import Deferred\n \n from scrapy import log\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.conf import build_component_list\n\n@@ -8,7 +8,7 @@ from scrapy.utils.defer import defer_result, defer_succeed, parallel\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log\n-from scrapy.core.exceptions import IgnoreRequest, DropItem\n+from scrapy.exceptions import IgnoreRequest, DropItem\n from scrapy.core import signals\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n\n@@ -8,7 +8,7 @@ docs/topics/spider-middleware.rst\n \n from scrapy import log\n from twisted.python.failure import Failure\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import mustbe_deferred\n\n@@ -0,0 +1,40 @@\n+\"\"\"\n+Scrapy core exceptions\n+\n+These exceptions are documented in docs/topics/exceptions.rst. Please don't add\n+new exceptions here without documenting them there.\n+\"\"\"\n+\n+from scrapy import log\n+\n+# Internal\n+\n+class NotConfigured(Exception):\n+    \"\"\"Indicates a missing configuration situation\"\"\"\n+    pass\n+\n+# HTTP and crawling\n+\n+class IgnoreRequest(Exception):\n+    \"\"\"Indicates a decision was made not to process a request\"\"\"\n+\n+    def __init__(self, msg='', level=log.ERROR):\n+        self.msg = msg\n+        self.level = level\n+\n+    def __str__(self):\n+        return self.msg\n+\n+class DontCloseSpider(Exception):\n+    \"\"\"Request the spider not to be closed yet\"\"\"\n+    pass\n+\n+# Items\n+\n+class DropItem(Exception):\n+    \"\"\"Drop item from the item pipeline\"\"\"\n+    pass\n+\n+class NotSupported(Exception):\n+    \"\"\"Indicates a feature or method is not supported\"\"\"\n+    pass\n\n@@ -3,7 +3,7 @@ This module contains the ExtensionManager  which takes care of loading and\n keeping track of all enabled extensions. It also contains an instantiated\n ExtensionManager (extensions) to be used as singleton.\n \"\"\"\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.conf import build_component_list\n from scrapy import log\n\n@@ -15,7 +15,7 @@ from twisted.internet import defer, reactor\n from twisted.mail.smtp import ESMTPSenderFactory\n \n from scrapy import log\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.conf import settings\n from scrapy.utils.signal import send_catch_log\n \n\n@@ -11,7 +11,7 @@ from twisted.conch.insults import insults\n from twisted.internet import reactor, protocol\n \n from scrapy.extension import extensions\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.core.manager import scrapymanager\n from scrapy.spider import spiders\n from scrapy.stats import stats\n\n@@ -4,7 +4,7 @@ from scrapy.http import Response, HtmlResponse, Request\n from scrapy.spider import BaseSpider\n from scrapy.contrib.downloadermiddleware.httpcache import FilesystemCacheStorage, HttpCacheMiddleware\n from scrapy.conf import Settings\n-from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest\n \n \n class HttpCacheMiddlewareTest(unittest.TestCase):\n\n@@ -3,7 +3,7 @@ import sys\n from twisted.trial.unittest import TestCase, SkipTest\n \n from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n from scrapy.spider import BaseSpider\n from scrapy.conf import settings\n\n@@ -2,7 +2,7 @@ import unittest\n \n from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware\n from scrapy.spider import BaseSpider\n-from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response, HtmlResponse, Headers\n \n class RedirectMiddlewareTest(unittest.TestCase):\n\n@@ -2,7 +2,7 @@ import unittest\n \n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n-from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest\n from scrapy.contrib.schedulermiddleware.duplicatesfilter import DuplicatesFilterMiddleware\n \n \n\n@@ -5,7 +5,7 @@ Helper functions for dealing with Twisted deferreds\n from twisted.internet import defer, reactor, task\n from twisted.python import failure\n \n-from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest\n \n def defer_fail(_failure):\n     \"\"\"Same as twisted.internet.defer.fail, but delay calling errback until\n\n@@ -7,7 +7,7 @@ See docs/topics/ws.rst\n from twisted.internet import reactor\n from twisted.web import server, resource, error\n \n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured\n from scrapy.utils.jsonrpc import jsonrpc_server_call\n from scrapy.utils.serialize import ScrapyJSONEncoder, ScrapyJSONDecoder\n from scrapy.utils.misc import load_object\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#784722774bf0f4425b6e14c82514edcfe76781b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 42 | Files Changed: 25 | Hunks: 27 | Methods Changed: 1 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 89 | Churn Cumulative: 4719 | Contributors (this commit): 8 | Commits (past 90d): 71 | Contributors (cumulative): 58 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.contrib.exporter import XmlItemExporter\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n-from scrapy.core import signals\n+from scrapy import signals\n \n def _import_file(filepath):\n     abspath = os.path.abspath(filepath)\n\n@@ -9,7 +9,7 @@ from collections import defaultdict\n from twisted.internet import reactor\n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.core.manager import scrapymanager\n from scrapy.conf import settings\n \n\n@@ -8,7 +8,7 @@ import datetime\n \n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.stats import stats\n from scrapy.stats.signals import stats_spider_opened, stats_spider_closing\n from scrapy.conf import settings\n\n@@ -3,7 +3,7 @@ from itertools import groupby\n from collections import defaultdict\n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n from scrapy.conf import settings\n\n@@ -5,7 +5,7 @@ See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n from scrapy.conf import settings\n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core import signals\n+from scrapy import signals\n \n \n class DefaultHeadersMiddleware(object):\n\n@@ -6,7 +6,7 @@ from time import time\n import cPickle as pickle\n \n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.http import Headers\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.core.downloader.responsetypes import responsetypes\n\n@@ -8,7 +8,7 @@ import robotparser\n \n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.core.manager import scrapymanager\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n\n@@ -29,7 +29,7 @@ from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core.manager import scrapymanager\n from scrapy.exceptions import NotConfigured\n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.utils.response import response_httprepr\n from scrapy.stats import stats\n from scrapy.http import Request\n\n@@ -10,7 +10,7 @@ import socket\n import libxml2\n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n\n@@ -11,7 +11,7 @@ from pprint import pformat\n from twisted.internet import task\n \n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy import log\n from scrapy.core.manager import scrapymanager\n from scrapy.exceptions import NotConfigured\n\n@@ -5,7 +5,7 @@ See documentation in docs/topics/item-pipeline.rst\n \"\"\"\n \n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.exceptions import NotConfigured\n from scrapy.contrib import exporter\n from scrapy.conf import settings\n\n@@ -20,7 +20,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy import log\n from scrapy.stats import stats\n from scrapy.utils.misc import md5sum\n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.exceptions import DropItem, NotConfigured, IgnoreRequest\n from scrapy.contrib.pipeline.media import MediaPipeline\n from scrapy.conf import settings\n\n@@ -3,7 +3,7 @@ from twisted.internet.defer import Deferred, DeferredList\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy import log\n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.core.manager import scrapymanager\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n\n@@ -5,7 +5,7 @@ from twisted.internet.base import ThreadedResolver\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.core import signals\n+from scrapy import signals\n \n \n class CachingResolver(object):\n\n@@ -8,7 +8,7 @@ from time import time\n from scrapy.xlib.pydispatch import dispatcher\n from collections import defaultdict\n \n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.core.manager import scrapymanager\n from scrapy.exceptions import NotConfigured, DontCloseSpider\n from scrapy.conf import settings\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/spider-middleware.rst\n import re\n \n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy import log\n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/spider-middleware.rst\n from itertools import imap\n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.core.manager import scrapymanager\n from scrapy.exceptions import NotConfigured\n from scrapy.conf import settings\n\n@@ -2,7 +2,7 @@\n \n from twisted.internet import reactor\n \n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.exceptions import NotSupported\n from scrapy.utils.signal import send_catch_log\n from scrapy.utils.misc import load_object\n\n@@ -6,7 +6,7 @@ docs/topics/downloader-middleware.rst\n \n \"\"\"\n \n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.utils.signal import send_catch_log\n from scrapy import log\n from scrapy.http import Request, Response\n\n@@ -12,7 +12,7 @@ from twisted.python.failure import Failure\n from scrapy import log\n from scrapy.stats import stats\n from scrapy.conf import settings\n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n from scrapy.exceptions import IgnoreRequest, DontCloseSpider\n\n@@ -9,7 +9,7 @@ from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log\n from scrapy.exceptions import IgnoreRequest, DropItem\n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n\n@@ -1,19 +1,5 @@\n-\"\"\"\n-Scrapy core signals\n+from scrapy.signals import *\n \n-These signals are documented in docs/topics/signals.rst. Please don't add new\n-signals here without documenting them there.\n-\"\"\"\n-\n-engine_started = object()\n-engine_stopped = object()\n-spider_opened = object()\n-spider_idle = object()\n-spider_closed = object()\n-request_received = object()\n-request_uploaded = object()\n-response_received = object()\n-response_downloaded = object()\n-item_scraped = object()\n-item_passed = object()\n-item_dropped = object()\n+import warnings\n+warnings.warn(\"scrapy.core.signals is deprecated, use scrapy.signals instead\", \\\n+    DeprecationWarning, stacklevel=2)\n\n@@ -0,0 +1,19 @@\n+\"\"\"\n+Scrapy signals\n+\n+These signals are documented in docs/topics/signals.rst. Please don't add new\n+signals here without documenting them there.\n+\"\"\"\n+\n+engine_started = object()\n+engine_stopped = object()\n+spider_opened = object()\n+spider_idle = object()\n+spider_closed = object()\n+request_received = object()\n+request_uploaded = object()\n+response_received = object()\n+response_downloaded = object()\n+item_scraped = object()\n+item_passed = object()\n+item_dropped = object()\n\n@@ -8,7 +8,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.stats.signals import stats_spider_opened, stats_spider_closing, \\\n     stats_spider_closed\n from scrapy.utils.signal import send_catch_log\n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy import log\n from scrapy.conf import settings\n \n\n@@ -7,7 +7,7 @@ import sys, os, re, urlparse, unittest\n from twisted.internet import reactor\n from twisted.web import server, resource, static, util\n \n-from scrapy.core import signals\n+from scrapy import signals\n from scrapy.core.manager import scrapymanager\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.tests import tests_datadir\n@@ -206,7 +206,7 @@ class EngineTest(unittest.TestCase):\n         \"\"\"\n         Check signals were sent properly\n         \"\"\"\n-        from scrapy.core import signals\n+        from scrapy import signals\n \n         assert signals.engine_started in session.signals_catched\n         assert signals.engine_stopped in session.signals_catched\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dedd6b2256fa2079cbbfaefbd9875084ac443389", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 171 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n from scrapy.exceptions import *\n \n import warnings\n-warnings.warn(\"scrapy.core.exceptions is deprecated, use scrapy.exceptions instead\", \\\n+warnings.warn(\"scrapy.core.exceptions is deprecated and will be removed in Scrapy 0.11, use scrapy.exceptions instead\", \\\n     DeprecationWarning, stacklevel=2)\n\n@@ -1,5 +1,5 @@\n from scrapy.signals import *\n \n import warnings\n-warnings.warn(\"scrapy.core.signals is deprecated, use scrapy.signals instead\", \\\n+warnings.warn(\"scrapy.core.signals is deprecated and will be removed in Scrapy 0.11, use scrapy.signals instead\", \\\n     DeprecationWarning, stacklevel=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2822db730fc986ff150e88ce714d8abcfc198020", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 226 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 8 | Methods Changed: 33 | Complexity Δ (Sum/Max): 55/21 | Churn Δ: 231 | Churn Cumulative: 414 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 0.8732394366197183\n\nDIFF:\n@@ -0,0 +1,63 @@\n+from collections import defaultdict\n+\n+from scrapy import log\n+from scrapy.exceptions import NotConfigured\n+from scrapy.utils.misc import load_object\n+from scrapy.utils.defer import process_parallel, process_chain, process_chain_both\n+\n+class MiddlewareManager(object):\n+    \"\"\"Base class for implementing middleware managers\"\"\"\n+\n+    component_name = 'foo middleware'\n+\n+    def __init__(self, *middlewares):\n+        self.middlewares = middlewares\n+        self.methods = defaultdict(list)\n+        for mw in middlewares:\n+            self._add_middleware(mw)\n+\n+    @classmethod\n+    def _get_mwlist_from_settings(cls, settings):\n+        return []\n+\n+    @classmethod\n+    def from_settings(cls, settings):\n+        mwlist = cls._get_mwlist_from_settings(settings)\n+        middlewares = []\n+        for clspath in mwlist:\n+            try:\n+                mwcls = load_object(clspath)\n+                if hasattr(mwcls, 'from_settings'):\n+                    mw = mwcls.from_settings(settings)\n+                else:\n+                    mw = mwcls()\n+                middlewares.append(mw)\n+            except NotConfigured, e:\n+                if e.args:\n+                    log.msg(e)\n+        enabled = [type(x).__name__ for x in middlewares]\n+        log.msg(\"Enabled %ss: %s\" % (cls.component_name, \",\".join(enabled)), \\\n+            level=log.DEBUG)\n+        return cls(*middlewares)\n+\n+    def _add_middleware(self, mw):\n+        if hasattr(mw, 'open_spider'):\n+            self.methods['open_spider'].append(mw.open_spider)\n+        if hasattr(mw, 'close_spider'):\n+            self.methods['close_spider'].insert(0, mw.close_spider)\n+\n+    def _process_parallel(self, methodname, obj, *args):\n+        return process_parallel(self.methods[methodname], obj, *args)\n+\n+    def _process_chain(self, methodname, obj, *args):\n+        return process_chain(self.methods[methodname], obj, *args)\n+\n+    def _process_chain_both(self, cb_methodname, eb_methodname, obj, *args):\n+        return process_chain_both(self.methods[cb_methodname], \\\n+            self.methods[eb_methodname], obj, *args)\n+\n+    def open_spider(self, spider):\n+        return self._process_parallel('open_spider', spider)\n+\n+    def close_spider(self, spider):\n+        return self._process_parallel('close_spider', spider)\n\n@@ -0,0 +1,84 @@\n+from twisted.trial import unittest\n+\n+from scrapy.conf import Settings\n+from scrapy.exceptions import NotConfigured\n+from scrapy.middleware import MiddlewareManager\n+\n+class M1(object):\n+\n+    def open_spider(self, spider):\n+        pass\n+\n+    def close_spider(self, spider):\n+        pass\n+\n+    def process(self, response, request, spider):\n+        pass\n+\n+class M2(object):\n+\n+    def open_spider(self, spider):\n+        pass\n+\n+    def close_spider(self, spider):\n+        pass\n+\n+    pass\n+\n+class M3(object):\n+\n+    def process(self, response, request, spider):\n+        pass\n+\n+\n+class MOff(object):\n+\n+    def open_spider(self, spider):\n+        pass\n+\n+    def close_spider(self, spider):\n+        pass\n+\n+    def __init__(self):\n+        raise NotConfigured\n+\n+\n+class TestMiddlewareManager(MiddlewareManager):\n+\n+    @classmethod\n+    def _get_mwlist_from_settings(cls, settings):\n+        return ['scrapy.tests.test_middleware.%s' % x for x in ['M1', 'MOff', 'M3']]\n+\n+    def _add_middleware(self, mw):\n+        super(TestMiddlewareManager, self)._add_middleware(mw)\n+        if hasattr(mw, 'process'):\n+            self.methods['process'].append(mw.process)\n+\n+class MiddlewareManagerTest(unittest.TestCase):\n+\n+    def test_init(self):\n+        m1, m2, m3 = M1(), M2(), M3()\n+        mwman = TestMiddlewareManager(m1, m2, m3)\n+        self.assertEqual(mwman.methods['open_spider'], [m1.open_spider, m2.open_spider])\n+        self.assertEqual(mwman.methods['close_spider'], [m2.close_spider, m1.close_spider])\n+        self.assertEqual(mwman.methods['process'], [m1.process, m3.process])\n+\n+    def test_methods(self):\n+        mwman = TestMiddlewareManager(M1(), M2(), M3())\n+        self.assertEqual([x.im_class for x in mwman.methods['open_spider']],\n+            [M1, M2])\n+        self.assertEqual([x.im_class for x in mwman.methods['close_spider']],\n+            [M2, M1])\n+        self.assertEqual([x.im_class for x in mwman.methods['process']],\n+            [M1, M3])\n+\n+    def test_enabled(self):\n+        m1, m2, m3 = M1(), M2(), M3()\n+        mwman = MiddlewareManager(m1, m2, m3)\n+        self.failUnlessEqual(mwman.middlewares, (m1, m2, m3))\n+\n+    def test_enabled_from_settings(self):\n+        settings = Settings()\n+        mwman = TestMiddlewareManager.from_settings(settings)\n+        classes = [x.__class__ for x in mwman.middlewares]\n+        self.failUnlessEqual(classes, [M1, M3])\n\n@@ -1,9 +1,9 @@\n-from itertools import imap\n from twisted.trial import unittest\n+from twisted.internet import reactor, defer\n+from twisted.python.failure import Failure\n \n-from twisted.internet import reactor\n-from twisted.internet.defer import Deferred\n-from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.defer import mustbe_deferred, process_chain, \\\n+    process_chain_both, process_parallel\n \n \n class MustbeDeferredTest(unittest.TestCase):\n@@ -22,7 +22,7 @@ class MustbeDeferredTest(unittest.TestCase):\n         steps = []\n         def _append(v):\n             steps.append(v)\n-            dfd = Deferred()\n+            dfd = defer.Deferred()\n             reactor.callLater(0, dfd.callback, steps)\n             return dfd\n \n@@ -31,3 +31,48 @@ class MustbeDeferredTest(unittest.TestCase):\n         steps.append(2) # add another value, that should be catched by assertEqual\n         return dfd\n \n+def cb1(value, arg1, arg2):\n+    return \"(cb1 %s %s %s)\" % (value, arg1, arg2)\n+def cb2(value, arg1, arg2):\n+    return defer.succeed(\"(cb2 %s %s %s)\" % (value, arg1, arg2))\n+def cb3(value, arg1, arg2):\n+    return \"(cb3 %s %s %s)\" % (value, arg1, arg2)\n+def cb_fail(value, arg1, arg2):\n+    return Failure(TypeError())\n+def eb1(failure, arg1, arg2):\n+    return \"(eb1 %s %s %s)\" % (failure.value.__class__.__name__, arg1, arg2)\n+\n+\n+class DeferUtilsTest(unittest.TestCase):\n+\n+    @defer.inlineCallbacks\n+    def test_process_chain(self):\n+        x = yield process_chain([cb1, cb2, cb3], 'res', 'v1', 'v2')\n+        self.assertEqual(x, \"(cb3 (cb2 (cb1 res v1 v2) v1 v2) v1 v2)\")\n+\n+        gotexc = False\n+        try:\n+            yield process_chain([cb1, cb_fail, cb3], 'res', 'v1', 'v2')\n+        except TypeError, e:\n+            gotexc = True\n+        self.failUnless(gotexc)\n+\n+    @defer.inlineCallbacks\n+    def test_process_chain_both(self):\n+        x = yield process_chain_both([cb_fail, cb2, cb3], [None, eb1, None], 'res', 'v1', 'v2')\n+        self.assertEqual(x, \"(cb3 (eb1 TypeError v1 v2) v1 v2)\")\n+\n+        fail = Failure(ZeroDivisionError())\n+        x = yield process_chain_both([eb1, cb2, cb3], [eb1, None, None], fail, 'v1', 'v2')\n+        self.assertEqual(x, \"(cb3 (cb2 (eb1 ZeroDivisionError v1 v2) v1 v2) v1 v2)\")\n+\n+    @defer.inlineCallbacks\n+    def test_process_parallel(self):\n+        x = yield process_parallel([cb1, cb2, cb3], 'res', 'v1', 'v2')\n+        self.assertEqual(x, ['(cb1 res v1 v2)', '(cb2 res v1 v2)', '(cb3 res v1 v2)'])\n+\n+    def test_process_parallel_failure(self):\n+        d = process_parallel([cb1, cb_fail, cb3], 'res', 'v1', 'v2')\n+        self.failUnlessFailure(d, TypeError)\n+        self.flushLoggedErrors()\n+        return d\n\n@@ -56,3 +56,32 @@ def parallel(iterable, count, callable, *args, **named):\n     coop = task.Cooperator()\n     work = (callable(elem, *args, **named) for elem in iterable)\n     return defer.DeferredList([coop.coiterate(work) for i in xrange(count)])\n+\n+def process_chain(callbacks, input, *a, **kw):\n+    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n+    d = defer.Deferred()\n+    for x in callbacks:\n+        d.addCallback(x, *a, **kw)\n+    d.callback(input)\n+    return d\n+\n+def process_chain_both(callbacks, errbacks, input, *a, **kw):\n+    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n+    d = defer.Deferred()\n+    for cb, eb in zip(callbacks, errbacks):\n+        d.addCallbacks(cb, eb, callbackArgs=a, callbackKeywords=kw,\n+            errbackArgs=a, errbackKeywords=kw)\n+    if isinstance(input, failure.Failure):\n+        d.errback(input)\n+    else:\n+        d.callback(input)\n+    return d\n+\n+def process_parallel(callbacks, input, *a, **kw):\n+    \"\"\"Return a Deferred with the output of all successful calls to the given\n+    callbacks\n+    \"\"\"\n+    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n+    d = defer.gatherResults(dfds)\n+    d.addErrback(lambda _: _.value.subFailure)\n+    return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
