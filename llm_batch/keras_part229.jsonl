{"custom_id": "keras#475def3da3a1a3e6f55f438fad06c0d691a204a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1363 | Contributors (this commit): 15 | Commits (past 90d): 16 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -318,7 +318,6 @@ class Sequence(object):\n         from skimage.io import imread\n         from skimage.transform import resize\n         import numpy as np\n-        import math\n \n         # Here, `x_set` is list of path to the images\n         # and `y_set` are the associated classes.\n@@ -330,7 +329,7 @@ class Sequence(object):\n                 self.batch_size = batch_size\n \n             def __len__(self):\n-                return math.ceil(len(self.x) / self.batch_size)\n+                return np.ceil(len(self.x) / float(self.batch_size))\n \n             def __getitem__(self, idx):\n                 batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1a7848772636e29231b23888b48f22e24d41c67a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 372 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -28,6 +28,7 @@ from ..layers import Concatenate\n from ..layers import Conv2D\n from ..layers import Dense\n from ..layers import GlobalAveragePooling2D\n+from ..layers import GlobalMaxPooling2D\n from ..layers import Input\n from ..layers import MaxPooling2D\n from ..layers import ZeroPadding2D\n@@ -217,9 +218,9 @@ def DenseNet(blocks,\n         x = Dense(classes, activation='softmax', name='fc1000')(x)\n     else:\n         if pooling == 'avg':\n-            x = AveragePooling2D(7, name='avg_pool')(x)\n+            x = GlobalAveragePooling2D(name='avg_pool')(x)\n         elif pooling == 'max':\n-            x = MaxPooling2D(7, name='max_pool')(x)\n+            x = GlobalMaxPooling2D(name='max_pool')(x)\n \n     # Ensure that the model takes into account\n     # any potential predecessors of `input_tensor`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f65a562d8266b769e16cc2cd14f0049720e8d81c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 319 | Files Changed: 1 | Hunks: 28 | Methods Changed: 54 | Complexity Δ (Sum/Max): -51/0 | Churn Δ: 420 | Churn Cumulative: 1443 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,4 @@\n import pytest\n-import numpy as np\n-import time\n import random\n import os\n from multiprocessing import Process, Queue\n@@ -13,8 +11,9 @@ from keras import backend as K\n \n \n pytestmark = pytest.mark.skipif(\n-    os.environ['CORE_CHANGED'] == 'False' and os.environ['APP_CHANGED'] == 'False',\n-    reason='runs only when the relevant files have been modified')\n+    os.environ.get('CORE_CHANGED', 'True') == 'False' and\n+    os.environ('APP_CHANGED', 'True') == 'False',\n+    reason='Runs only when the relevant files have been modified.')\n \n \n DENSENET_LIST = [(applications.DenseNet121, 1024),\n@@ -24,12 +23,15 @@ NASNET_LIST = [(applications.NASNetMobile, 1056),\n                (applications.NASNetLarge, 4032)]\n \n \n-def clean_run(model_fn):\n+def _get_output_shape(model_fn):\n     if K.backend() == 'cntk':\n-        # Create model in a subprocess so that the memory consumed by InceptionResNetV2 will be\n-        # released back to the system after this test (to deal with OOM error on CNTK backend)\n-        # TODO: remove the use of multiprocessing from these tests once a memory clearing mechanism\n-        # is implemented in the CNTK backend\n+        # Create model in a subprocess so that\n+        # the memory consumed by InceptionResNetV2 will be\n+        # released back to the system after this test\n+        # (to deal with OOM error on CNTK backend).\n+        # TODO: remove the use of multiprocessing from these tests\n+        # once a memory clearing mechanism\n+        # is implemented in the CNTK backend.\n         def target(queue):\n             model = model_fn()\n             queue.put(model.output_shape)\n@@ -37,8 +39,10 @@ def clean_run(model_fn):\n         p = Process(target=target, args=(queue,))\n         p.start()\n         p.join()\n-        # The error in a subprocess won't propagate to the main process, so we check if the model\n-        # is successfully created by checking if the output shape has been put into the queue\n+        # The error in a subprocess won't propagate\n+        # to the main process, so we check if the model\n+        # is successfully created by checking if the output shape\n+        # has been put into the queue\n         assert not queue.empty(), 'Model creation failed.'\n         return queue.get_nowait()\n     else:\n@@ -47,350 +51,123 @@ def clean_run(model_fn):\n \n \n @keras_test\n-def test_resnet50():\n-    model = applications.ResNet50(weights=None)\n-    assert model.output_shape == (None, 1000)\n+def _test_application_basic(app, last_dim=1000):\n+    output_shape = _get_output_shape(lambda: app(weights=None))\n+    assert output_shape == (None, last_dim)\n \n \n @keras_test\n-def test_resnet50_notop():\n-    model = applications.ResNet50(weights=None, include_top=False)\n-    assert model.output_shape == (None, None, None, 2048)\n+def _test_application_notop(app, last_dim):\n+    output_shape = _get_output_shape(\n+        lambda: app(weights=None, include_top=False))\n+    assert output_shape == (None, None, None, last_dim)\n \n \n @keras_test\n-def test_resnet50_variable_input_channels():\n-    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n-    model = applications.ResNet50(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 2048)\n+def _test_application_variable_input_channels(app, last_dim):\n+    if K.image_data_format() == 'channels_first':\n+        input_shape = (1, None, None)\n+    else:\n+        input_shape = (None, None, 1)\n+    output_shape = _get_output_shape(\n+        lambda: app(weights=None, include_top=False, input_shape=input_shape))\n+    assert output_shape == (None, None, None, last_dim)\n \n-    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n-    model = applications.ResNet50(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 2048)\n+    if K.image_data_format() == 'channels_first':\n+        input_shape = (4, None, None)\n+    else:\n+        input_shape = (None, None, 4)\n+    output_shape = _get_output_shape(\n+        lambda: app(weights=None, include_top=False, input_shape=input_shape))\n+    assert output_shape == (None, None, None, last_dim)\n \n \n @keras_test\n-def test_resnet50_pooling():\n-    model = applications.ResNet50(weights=None,\n+def _test_app_pooling(app, last_dim):\n+    output_shape = _get_output_shape(\n+        lambda: app(weights=None,\n                     include_top=False,\n-                                  pooling='avg')\n-    assert model.output_shape == (None, 2048)\n+                    pooling=random.choice(['avg', 'max'])))\n+    assert output_shape == (None, last_dim)\n \n \n-@keras_test\n-def test_vgg16():\n-    model = applications.VGG16(weights=None)\n-    assert model.output_shape == (None, 1000)\n+def test_resnet50():\n+    app = applications.ResNet50\n+    last_dim = 2048\n+    _test_application_basic(app)\n+    _test_application_notop(app, last_dim)\n+    _test_application_variable_input_channels(app, last_dim)\n+    _test_app_pooling(app, last_dim)\n \n \n-@keras_test\n-def test_vgg16_notop():\n-    model = applications.VGG16(weights=None, include_top=False)\n-    assert model.output_shape == (None, None, None, 512)\n+def test_vgg():\n+    app = random.choice([applications.VGG16, applications.VGG19])\n+    last_dim = 512\n+    _test_application_basic(app)\n+    _test_application_notop(app, last_dim)\n+    _test_application_variable_input_channels(app, last_dim)\n+    _test_app_pooling(app, last_dim)\n \n \n-@keras_test\n-def test_vgg16_variable_input_channels():\n-    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n-    model = applications.VGG16(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 512)\n-\n-    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n-    model = applications.VGG16(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 512)\n-\n-\n-@keras_test\n-def test_vgg16_pooling():\n-    model = applications.VGG16(weights=None, include_top=False, pooling='avg')\n-    assert model.output_shape == (None, 512)\n-\n-\n-@keras_test\n-def test_vgg19():\n-    model = applications.VGG19(weights=None)\n-    assert model.output_shape == (None, 1000)\n-\n-\n-@keras_test\n-def test_vgg19_notop():\n-    model = applications.VGG19(weights=None, include_top=False)\n-    assert model.output_shape == (None, None, None, 512)\n-\n-\n-@keras_test\n-def test_vgg19_variable_input_channels():\n-    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n-    model = applications.VGG19(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 512)\n-\n-    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n-    model = applications.VGG19(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 512)\n-\n-\n-@keras_test\n-def test_vgg19_notop_specified_input_shape():\n-    input_shape = (3, 300, 300) if K.image_data_format() == 'channels_first' else (300, 300, 3)\n-    model = applications.VGG19(weights=None, include_top=False, input_shape=input_shape)\n-    output_shape = (None, 512, 9, 9) if K.image_data_format() == 'channels_first' else (None, 9, 9, 512)\n-    assert model.output_shape == output_shape\n-\n-\n-@keras_test\n-def test_vgg19_pooling():\n-    model = applications.VGG16(weights=None, include_top=False, pooling='avg')\n-    assert model.output_shape == (None, 512)\n-\n-\n-@keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason='Requires TensorFlow backend')\n def test_xception():\n-    model = applications.Xception(weights=None)\n-    assert model.output_shape == (None, 1000)\n+    app = applications.Xception\n+    last_dim = 2048\n+    _test_application_basic(app)\n+    _test_application_notop(app, last_dim)\n+    _test_application_variable_input_channels(app, last_dim)\n+    _test_app_pooling(app, last_dim)\n \n \n-@keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-def test_xception_notop():\n-    model = applications.Xception(weights=None, include_top=False)\n-    assert model.output_shape == (None, None, None, 2048)\n-\n-\n-@keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-def test_xception_pooling():\n-    model = applications.Xception(weights=None, include_top=False, pooling='avg')\n-    assert model.output_shape == (None, 2048)\n-\n-\n-@keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-def test_xception_variable_input_channels():\n-    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n-    model = applications.Xception(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 2048)\n-\n-    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n-    model = applications.Xception(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 2048)\n-\n-\n-@keras_test\n def test_inceptionv3():\n-    model = applications.InceptionV3(weights=None)\n-    assert model.output_shape == (None, 1000)\n+    app = applications.InceptionV3\n+    last_dim = 2048\n+    _test_application_basic(app)\n+    _test_application_notop(app, last_dim)\n+    _test_app_pooling(app, last_dim)\n+\n+    if K.backend() != 'cntk':\n+        # CNTK does not support dynamic padding.\n+        _test_application_variable_input_channels(app, last_dim)\n \n \n-@keras_test\n-def test_inceptionv3_notop():\n-    model = applications.InceptionV3(weights=None, include_top=False)\n-    assert model.output_shape == (None, None, None, 2048)\n-\n-\n-@keras_test\n-def test_inceptionv3_pooling():\n-    model = applications.InceptionV3(weights=None, include_top=False, pooling='avg')\n-    assert model.output_shape == (None, 2048)\n-\n-\n-@keras_test\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='cntk does not support padding with non-concrete dimension')\n-def test_inceptionv3_variable_input_channels():\n-    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n-    model = applications.InceptionV3(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 2048)\n-\n-    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n-    model = applications.InceptionV3(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 2048)\n-\n-\n-@keras_test\n def test_inceptionresnetv2():\n-    def model_fn():\n-        return applications.InceptionResNetV2(weights=None)\n-    output_shape = clean_run(model_fn)\n-    assert output_shape == (None, 1000)\n+    app = applications.InceptionResNetV2\n+    last_dim = 1536\n+    _test_application_basic(app)\n+    _test_application_notop(app, last_dim)\n+    _test_application_variable_input_channels(app, last_dim)\n+    _test_app_pooling(app, last_dim)\n \n \n-@keras_test\n-def test_inceptionresnetv2_notop():\n-    def model_fn():\n-        return applications.InceptionResNetV2(weights=None, include_top=False)\n-    output_shape = clean_run(model_fn)\n-    if K.image_data_format() == 'channels_first':\n-        assert output_shape == (None, 1536, None, None)\n-    else:\n-        assert output_shape == (None, None, None, 1536)\n-\n-\n-@keras_test\n-def test_inceptionresnetv2_pooling():\n-    def model_fn():\n-        return applications.InceptionResNetV2(weights=None, include_top=False, pooling='avg')\n-    output_shape = clean_run(model_fn)\n-    assert output_shape == (None, 1536)\n-\n-\n-@keras_test\n-def test_inceptionresnetv2_variable_input_channels():\n-    def model_fn(input_shape):\n-        return applications.InceptionResNetV2(weights=None, include_top=False, input_shape=input_shape)\n-    output_shape = clean_run(lambda: model_fn((None, None, 1)))\n-    assert output_shape == (None, None, None, 1536)\n-\n-    output_shape = clean_run(lambda: model_fn((None, None, 4)))\n-    assert output_shape == (None, None, None, 1536)\n-\n-\n-@keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason='MobileNets are supported only on TensorFlow')\n def test_mobilenet():\n-    model = applications.MobileNet(weights=None)\n-    assert model.output_shape == (None, 1000)\n+    app = applications.MobileNet\n+    last_dim = 1024\n+    _test_application_basic(app)\n+    _test_application_notop(app, last_dim)\n+    _test_application_variable_input_channels(app, last_dim)\n+    _test_app_pooling(app, last_dim)\n \n \n-@keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='MobileNets are supported only on TensorFlow')\n-def test_mobilenet_no_top():\n-    model = applications.MobileNet(weights=None, include_top=False)\n-    assert model.output_shape == (None, None, None, 1024)\n-\n-\n-@keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='MobileNets are supported only on TensorFlow')\n-def test_mobilenet_pooling():\n-    model = applications.MobileNet(weights=None, include_top=False, pooling='avg')\n-    assert model.output_shape == (None, 1024)\n-\n-\n-@keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='MobileNets are supported only on TensorFlow')\n-def test_mobilenet_variable_input_channels():\n-    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n-    model = applications.MobileNet(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 1024)\n-\n-    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n-    model = applications.MobileNet(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, 1024)\n-\n-\n-@keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='MobileNets are supported only on TensorFlow')\n-def test_mobilenet_image_size():\n-    valid_image_sizes = [128, 160, 192, 224]\n-    for size in valid_image_sizes:\n-        input_shape = (size, size, 3) if K.image_data_format() == 'channels_last' else (3, size, size)\n-        model = applications.MobileNet(input_shape=input_shape, weights='imagenet', include_top=True)\n-        assert model.input_shape == (None,) + input_shape\n-\n-    invalid_image_shape = (112, 112, 3) if K.image_data_format() == 'channels_last' else (3, 112, 112)\n-    with pytest.raises(ValueError):\n-        model = applications.MobileNet(input_shape=invalid_image_shape, weights='imagenet', include_top=True)\n-\n-\n-@keras_test\n def test_densenet():\n-    random.seed(time.time())\n-    fun, _ = random.choice(DENSENET_LIST)\n-\n-    def model_fn():\n-        return fun(weights=None)\n-    output_shape = clean_run(model_fn)\n-    assert output_shape == (None, 1000)\n+    app, last_dim = random.choice(DENSENET_LIST)\n+    _test_application_basic(app)\n+    _test_application_notop(app, last_dim)\n+    _test_application_variable_input_channels(app, last_dim)\n+    _test_app_pooling(app, last_dim)\n \n \n-@keras_test\n-def test_densenet_no_top():\n-    random.seed(time.time())\n-    fun, dim = random.choice(DENSENET_LIST)\n-\n-    def model_fn():\n-        return fun(weights=None, include_top=False)\n-    output_shape = clean_run(model_fn)\n-    assert output_shape == (None, None, None, dim)\n-\n-\n-@keras_test\n-def test_densenet_pooling():\n-    random.seed(time.time())\n-    fun, dim = random.choice(DENSENET_LIST)\n-\n-    def model_fn():\n-        return fun(weights=None, include_top=False, pooling='avg')\n-    output_shape = clean_run(model_fn)\n-    assert output_shape == (None, None, None, dim)\n-\n-\n-@keras_test\n-def test_densenet_variable_input_channels():\n-    random.seed(time.time())\n-    fun, dim = random.choice(DENSENET_LIST)\n-\n-    def model_fn(input_shape):\n-        return fun(weights=None, include_top=False, input_shape=input_shape)\n-    output_shape = clean_run(lambda: model_fn((None, None, 1)))\n-    assert output_shape == (None, None, None, dim)\n-\n-    output_shape = clean_run(lambda: model_fn((None, None, 4)))\n-    assert output_shape == (None, None, None, dim)\n-\n-\n-@keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason='NASNets are supported only on TensorFlow')\n def test_nasnet():\n-    random.seed(time.time())\n-    fun, _ = random.choice(NASNET_LIST)\n-    model = fun(weights=None)\n-    assert model.output_shape == (None, 1000)\n-\n-\n-@keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='NASNets are supported only on TensorFlow')\n-def test_nasnet_no_top():\n-    random.seed(time.time())\n-    fun, dim = random.choice(NASNET_LIST)\n-    model = fun(weights=None, include_top=False)\n-    assert model.output_shape == (None, None, None, dim)\n-\n-\n-@keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='NASNets are supported only on TensorFlow')\n-def test_nasnet_pooling():\n-    random.seed(time.time())\n-    fun, dim = random.choice(NASNET_LIST)\n-    model = fun(weights=None, include_top=False, pooling='avg')\n-    assert model.output_shape == (None, dim)\n-\n-\n-@keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='NASNets are supported only on TensorFlow')\n-def test_nasnet_variable_input_channels():\n-    random.seed(time.time())\n-    fun, dim = random.choice(NASNET_LIST)\n-    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n-    model = fun(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, dim)\n-\n-    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n-    model = fun(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, dim)\n+    app, last_dim = random.choice(NASNET_LIST)\n+    _test_application_basic(app)\n+    _test_application_notop(app, last_dim)\n+    _test_application_variable_input_channels(app, last_dim)\n+    _test_app_pooling(app, last_dim)\n \n \n @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n@@ -402,7 +179,8 @@ def test_depthwise_conv_2d():\n     num_row = 7\n     num_col = 6\n \n-    with CustomObjectScope({'relu6': applications.mobilenet.relu6,\n+    with CustomObjectScope(\n+        {'relu6': applications.mobilenet.relu6,\n          'DepthwiseConv2D': applications.mobilenet.DepthwiseConv2D}):\n         for padding in _convolution_paddings:\n             for strides in [(1, 1), (2, 2)]:\n@@ -415,7 +193,10 @@ def test_depthwise_conv_2d():\n                                        'padding': padding,\n                                        'strides': strides,\n                                        'depth_multiplier': multiplier},\n-                               input_shape=(num_samples, num_row, num_col, stack_size))\n+                               input_shape=(num_samples,\n+                                            num_row,\n+                                            num_col,\n+                                            stack_size))\n \n         layer_test(applications.mobilenet.DepthwiseConv2D,\n                    kwargs={'kernel_size': 3,\n@@ -432,7 +213,8 @@ def test_depthwise_conv_2d():\n \n         # Test invalid use case\n         with pytest.raises(ValueError):\n-            model = Sequential([applications.mobilenet.DepthwiseConv2D(kernel_size=3,\n+            Sequential([applications.mobilenet.DepthwiseConv2D(\n+                kernel_size=3,\n                 padding=padding,\n                 batch_input_shape=(None, None, 5, None))])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b300bfb198cb1c4967b912009e18ada1affa8c38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1445 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,7 @@ from keras import backend as K\n \n pytestmark = pytest.mark.skipif(\n     os.environ.get('CORE_CHANGED', 'True') == 'False' and\n-    os.environ('APP_CHANGED', 'True') == 'False',\n+    os.environ.get('APP_CHANGED', 'True') == 'False',\n     reason='Runs only when the relevant files have been modified.')\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f353f09c065222313991be3ef5bd6e19c4307aac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 22 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 3085 | Contributors (this commit): 20 | Commits (past 90d): 7 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -255,27 +255,6 @@ class TestBackend(object):\n         check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)},\n                                          'squeeze', {'axis': 2},\n                                          (4, 3, 1, 1), BACKENDS)\n-        check_single_tensor_operation('temporal_padding',\n-                                      (4, 3, 3),\n-                                      BACKENDS)\n-        check_single_tensor_operation('temporal_padding',\n-                                      (4, 3, 3),\n-                                      BACKENDS,\n-                                      padding=(2, 2))\n-        check_single_tensor_operation('spatial_2d_padding',\n-                                      (4, 4, 3, 3),\n-                                      BACKENDS)\n-        check_single_tensor_operation('spatial_2d_padding',\n-                                      (4, 4, 3, 3),\n-                                      BACKENDS,\n-                                      padding=((2, 2), (2, 2)))\n-        check_single_tensor_operation('spatial_3d_padding',\n-                                      (4, 4, 3, 3, 3),\n-                                      BACKENDS)\n-        check_single_tensor_operation('spatial_3d_padding',\n-                                      (4, 4, 3, 3, 3),\n-                                      BACKENDS,\n-                                      padding=((2, 2), (2, 2), (2, 2)))\n \n     def test_none_shape_operations(self):\n         # Test shape inference when input\n@@ -991,8 +970,10 @@ class TestBackend(object):\n                                  data_format='channels_middle')\n \n     def test_temporal_padding(self):\n+        check_single_tensor_operation('temporal_padding', (4, 3, 3),\n+                                      BACKENDS)\n         check_single_tensor_operation('temporal_padding', (2, 3, 4),\n-                                      BACKENDS, padding=(2, 2))\n+                                      BACKENDS, padding=(1, 2))\n \n     def test_spatial_2d_padding(self):\n         padding = ((1, 2), (2, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67b823aeeba62f6895e00424524b0e1cd19e561a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 84 | Churn Cumulative: 313 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,6 +42,7 @@ import tensorflow as tf\n import keras\n from keras import backend as K\n from keras import layers\n+from keras.callbacks import Callback\n \n from tensorflow.contrib.learn.python.learn.datasets import mnist\n \n@@ -52,6 +53,53 @@ if K.backend() != 'tensorflow':\n                        'are not supported on other platforms.')\n \n \n+class EvaluateInputTensor(Callback):\n+    \"\"\" Validate a model which does not expect external numpy data during training.\n+\n+    Keras does not expect external numpy data at training time, and thus cannot\n+    accept numpy arrays for validation when all of a Keras Model's\n+    `Input(input_tensor)` layers are provided an  `input_tensor` parameter,\n+    and the call to `Model.compile(target_tensors)` defines all `target_tensors`.\n+    Instead, create a second model for validation which is also configured\n+    with input tensors and add it to the `EvaluateInputTensor` callback\n+    to perform validation.\n+\n+    It is recommended that this callback be the first in the list of callbacks\n+    because it defines the validation variables required by many other callbacks,\n+    and Callbacks are made in order.\n+\n+    # Arguments\n+        model: Keras model on which to call model.evaluate().\n+        steps: Integer or `None`.\n+            Total number of steps (batches of samples)\n+            before declaring the evaluation round finished.\n+            Ignored with the default value of `None`.\n+    \"\"\"\n+\n+    def __init__(self, model, steps, metrics_prefix='val', verbose=1):\n+        # parameter of callbacks passed during initialization\n+        # pass evalation mode directly\n+        super(EvaluateInputTensor, self).__init__()\n+        self.val_model = model\n+        self.num_steps = steps\n+        self.verbose = verbose\n+        self.metrics_prefix = metrics_prefix\n+\n+    def on_epoch_end(self, epoch, logs={}):\n+        self.val_model.set_weights(self.model.get_weights())\n+        results = self.val_model.evaluate(None, None, steps=int(self.num_steps),\n+                                          verbose=self.verbose)\n+        metrics_str = '\\n'\n+        for result, name in zip(results, self.val_model.metrics_names):\n+            metric_name = self.metrics_prefix + '_' + name\n+            logs[metric_name] = result\n+            if self.verbose > 0:\n+                metrics_str = metrics_str + metric_name + ': ' + str(result) + ' '\n+\n+        if self.verbose > 0:\n+            print(metrics_str)\n+\n+\n def cnn_layers(x_train_input):\n     x = layers.Conv2D(32, (3, 3),\n                       activation='relu', padding='valid')(x_train_input)\n@@ -68,9 +116,9 @@ def cnn_layers(x_train_input):\n \n sess = K.get_session()\n \n-batch_size = 128\n+batch_size = 100\n batch_shape = (batch_size, 28, 28, 1)\n-steps_per_epoch = 469\n+steps_per_epoch = 600\n epochs = 5\n num_classes = 10\n \n@@ -123,12 +171,42 @@ train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n                     target_tensors=[y_train_batch])\n train_model.summary()\n \n+x_test_batch, y_test_batch = tf.train.batch(\n+    tensors=[data.test.images, data.test.labels.astype(np.int32)],\n+    batch_size=batch_size,\n+    capacity=capacity,\n+    enqueue_many=enqueue_many,\n+    num_threads=8)\n+\n+# Create a separate test model\n+# to perform validation during training\n+x_test_batch = tf.cast(x_test_batch, tf.float32)\n+x_test_batch = tf.reshape(x_test_batch, shape=batch_shape)\n+\n+y_test_batch = tf.cast(y_test_batch, tf.int32)\n+y_test_batch = tf.one_hot(y_test_batch, num_classes)\n+\n+x_test_batch_shape = x_test_batch.get_shape().as_list()\n+y_test_batch_shape = y_test_batch.get_shape().as_list()\n+\n+test_model_input = layers.Input(tensor=x_test_batch)\n+test_model_output = cnn_layers(test_model_input)\n+test_model = keras.models.Model(inputs=test_model_input, outputs=test_model_output)\n+\n+# Pass the target tensor `y_test_batch` to `compile`\n+# via the `target_tensors` keyword argument:\n+test_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n+                   loss='categorical_crossentropy',\n+                   metrics=['accuracy'],\n+                   target_tensors=[y_test_batch])\n+\n # Fit the model using data from the TFRecord data tensors.\n coord = tf.train.Coordinator()\n threads = tf.train.start_queue_runners(sess, coord)\n \n train_model.fit(epochs=epochs,\n-                steps_per_epoch=steps_per_epoch)\n+                steps_per_epoch=steps_per_epoch,\n+                callbacks=[EvaluateInputTensor(test_model, steps=100)])\n \n # Save the model weights.\n train_model.save_weights('saved_wt.h5')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4f609860c8e16a12197a941ff8ad704870d95e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 3087 | Contributors (this commit): 20 | Commits (past 90d): 8 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -62,7 +62,7 @@ def parse_shape_or_val(shape_or_val):\n     if isinstance(shape_or_val, np.ndarray):\n         return shape_or_val.shape, shape_or_val\n     else:\n-        return shape_or_val, np.random.random(shape_or_val) - 0.5\n+        return shape_or_val, np.random.random(shape_or_val).astype(np.float32) - 0.5\n \n \n def assert_list_pairwise(z_list, shape=True, allclose=True, itself=False, atol=1e-05):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#abe06d593391d25e463970e8618fc0a0f9279c45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 5 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 26393 | Contributors (this commit): 208 | Commits (past 90d): 99 | Contributors (cumulative): 269 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-'''Trains a denoising autoenconder on MNIST dataset.\n+'''Trains a denoising autoencoder on MNIST dataset.\n \n Denoising is one of the classic applications of autoencoders.\n The denoising process removes unwanted noise that corrupted the\n\n@@ -18,7 +18,7 @@ The below table describes the performance on ImageNet 2012:\n |   NASNet-A (6 @ 4032)  |   82.7 %  |   96.2 %  |      23.8 B    |    88.9    |\n --------------------------------------------------------------------------------\n \n-Weights obtained from the official Tensorflow repository found at\n+Weights obtained from the official TensorFlow repository found at\n https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet\n \n # References\n@@ -145,7 +145,7 @@ def NASNet(input_shape=None,\n             backend that does not support separable convolutions.\n     '''\n     if K.backend() != 'tensorflow':\n-        raise RuntimeError('Only Tensorflow backend is currently supported, '\n+        raise RuntimeError('Only TensorFlow backend is currently supported, '\n                            'as other backends do not support '\n                            'separable convolution.')\n \n\n@@ -272,7 +272,7 @@ def _get_available_gpus():\n def _has_nchw_support():\n     \"\"\"Check whether the current scope supports NCHW ops.\n \n-    Tensorflow does not support NCHW on CPU. Therefore we check if we are not explicitly put on\n+    TensorFlow does not support NCHW on CPU. Therefore we check if we are not explicitly put on\n     CPU, and have GPUs available. In this case there will be soft-placing on the GPU device.\n \n     # Returns\n\n@@ -425,7 +425,7 @@ def _weighted_masked_objective(fn):\n         # score_array has ndim >= 2\n         score_array = fn(y_true, y_pred)\n         if mask is not None:\n-            # Cast the mask to floatX to avoid float64 upcasting in theano\n+            # Cast the mask to floatX to avoid float64 upcasting in Theano\n             mask = K.cast(mask, K.floatx())\n             # mask should have the same shape as score_array\n             score_array *= mask\n\n@@ -152,7 +152,7 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n                     for i, (w, val) in enumerate(zip(symbolic_weights,\n                                                      weight_values)):\n                         # Default values of symbolic_weights is /variable\n-                        # for theano and cntk\n+                        # for Theano and CNTK\n                         if K.backend() == 'theano' or K.backend() == 'cntk':\n                             if hasattr(w, 'name'):\n                                 if w.name.split('/')[-1] == 'variable':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ab8070c77b047f8c519bd97cb38c33533639224", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1606 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -88,7 +88,7 @@ class AveragePooling1D(_Pooling1D):\n     \"\"\"Average pooling for temporal data.\n \n     # Arguments\n-        pool_size: Integer, size of the max pooling windows.\n+        pool_size: Integer, size of the average pooling windows.\n         strides: Integer, or None. Factor by which to downscale.\n             E.g. 2 will halve the input.\n             If None, it will default to `pool_size`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#632d811f2f65bce1806559bf3eede37e517afb6d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 54 | Files Changed: 1 | Hunks: 20 | Methods Changed: 14 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 102 | Churn Cumulative: 3189 | Contributors (this commit): 20 | Commits (past 90d): 9 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -288,8 +288,7 @@ class TestBackend(object):\n                     x = K.placeholder(shape=shape)\n                     y = K.repeat_elements(x, reps, axis=rep_axis)\n                     assert y._keras_shape == tuple(shape)\n-                if K.backend() == 'tensorflow':\n-                    assert y._keras_shape == tuple(y.get_shape().as_list())\n+                    assert y._keras_shape == K.int_shape(y)\n \n     def test_tile(self):\n         shape = (3, 4)\n@@ -302,7 +301,7 @@ class TestBackend(object):\n         if K.backend() == 'theano':\n             x = K.placeholder(shape=(None, 4))\n             n = 2\n-            y = KTH.tile(x, n)\n+            y = K.tile(x, n)\n             assert y._keras_shape == (None, 8)\n             n = (4, 3)\n             y = K.tile(x, n)\n@@ -522,7 +521,7 @@ class TestBackend(object):\n         W_o_val = np.random.random((output_dim, output_dim)).astype(np.float32)\n         np_mask = np.random.randint(2, size=(num_samples, timesteps))\n \n-        def rnn_step_fn(input_dim, output_dim, k):\n+        def rnn_step_fn(k):\n             W_i = k.variable(W_i_val)\n             W_o = k.variable(W_o_val)\n \n@@ -540,7 +539,7 @@ class TestBackend(object):\n         state_list = [[], [], [], [], [], []]\n \n         for k in BACKENDS:\n-            rnn_fn = rnn_step_fn(input_dim, output_dim, k)\n+            rnn_fn = rnn_step_fn(k)\n             inputs = k.variable(input_val)\n             initial_states = [k.variable(init_state_val)]\n             mask = k.variable(np_mask)\n@@ -590,17 +589,18 @@ class TestBackend(object):\n             assert_allclose(b_s, b_u_s, atol=1e-04)\n \n         for m_l, u_m_l, k in zip(last_output_list[4], last_output_list[5], BACKENDS):\n-            # skip this compare on tensorflow\n-            if k != KTF:\n+            if k == KTF:\n+                m_l = m_l * np.expand_dims(np_mask[:, -1], -1)\n+                u_m_l = u_m_l * np.expand_dims(np_mask[:, -1], -1)\n             assert_allclose(m_l, u_m_l, atol=1e-04)\n \n         for m_o, u_m_o, k in zip(outputs_list[4], outputs_list[5], BACKENDS):\n-            # skip this compare on tensorflow\n-            if k != KTF:\n+            if k == KTF:\n+                m_o = m_o * np.expand_dims(np_mask, -1)\n+                u_m_o = u_m_o * np.expand_dims(np_mask, -1)\n             assert_allclose(m_o, u_m_o, atol=1e-04)\n \n         for m_s, u_m_s, k in zip(state_list[4], state_list[5], BACKENDS):\n-            if k != KTF:\n             assert_allclose(m_s, u_m_s, atol=1e-04)\n \n     def test_rnn_no_states(self):\n@@ -612,12 +612,12 @@ class TestBackend(object):\n         input_val = np.random.random((32, timesteps, input_dim))\n         W_i_val = np.random.random((input_dim, output_dim))\n \n-        def rnn_step_fn(input_dim, output_dim, K):\n-            W_i = K.variable(W_i_val)\n+        def rnn_step_fn(k):\n+            W_i = k.variable(W_i_val)\n \n             def step_function(x, states):\n                 assert len(states) == 0\n-                output = K.dot(x, W_i)\n+                output = k.dot(x, W_i)\n                 return output, []\n \n             return step_function\n@@ -627,7 +627,7 @@ class TestBackend(object):\n         outputs_list = []\n \n         for k in BACKENDS:\n-            rnn_fn = rnn_step_fn(input_dim, output_dim, k)\n+            rnn_fn = rnn_step_fn(k)\n             inputs = k.variable(input_val)\n             initial_states = []\n             last_output, outputs, new_states = k.rnn(rnn_fn, inputs,\n@@ -1061,18 +1061,19 @@ class TestBackend(object):\n             assert zth.shape == ztf.shape\n             assert zth.shape == zc.shape\n \n-    def test_ctc(self):\n-        # simplified version of TensorFlow's test\n-\n-        label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n-        input_lens = np.expand_dims(np.asarray([5, 5]), 1)  # number of timesteps\n-\n     # the Theano and TensorFlow CTC code use different methods to ensure\n     # numerical stability.  The Theano code subtracts out the max\n     # before the final log, so the results are different but scale\n     # identically and still train properly\n-        loss_log_probs_tf = [3.34211, 5.42262]\n-        loss_log_probs_th = [1.73308, 3.81351]\n+    @pytest.mark.parametrize('k,ref', [\n+        (KTF, [3.34211, 5.42262]),\n+        (KTH, [1.73308, 3.81351]),\n+    ], ids=['TensorFlow', 'Theano'])\n+    def test_ctc(self, k, ref):\n+        # simplified version of TensorFlow's test\n+\n+        label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n+        input_lens = np.expand_dims(np.asarray([5, 5]), 1)  # number of timesteps\n \n         # dimensions are batch x time x categories\n         labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n@@ -1089,19 +1090,12 @@ class TestBackend(object):\n               [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]],\n             dtype=np.float32)\n \n-        labels_tf = KTF.variable(labels, dtype=\"int32\")\n-        inputs_tf = KTF.variable(inputs, dtype=\"float32\")\n-        input_lens_tf = KTF.variable(input_lens, dtype=\"int32\")\n-        label_lens_tf = KTF.variable(label_lens, dtype=\"int32\")\n-        res = KTF.eval(KTF.ctc_batch_cost(labels_tf, inputs_tf, input_lens_tf, label_lens_tf))\n-        assert_allclose(res[:, 0], loss_log_probs_tf, atol=1e-05)\n-\n-        labels_th = KTH.variable(labels, dtype=\"int32\")\n-        inputs_th = KTH.variable(inputs, dtype=\"float32\")\n-        input_lens_th = KTH.variable(input_lens, dtype=\"int32\")\n-        label_lens_th = KTH.variable(label_lens, dtype=\"int32\")\n-        res = KTH.eval(KTH.ctc_batch_cost(labels_th, inputs_th, input_lens_th, label_lens_th))\n-        assert_allclose(res[0, :], loss_log_probs_th, atol=1e-05)\n+        k_labels = k.variable(labels, dtype=\"int32\")\n+        k_inputs = k.variable(inputs, dtype=\"float32\")\n+        k_input_lens = k.variable(input_lens, dtype=\"int32\")\n+        k_label_lens = k.variable(label_lens, dtype=\"int32\")\n+        res = k.eval(k.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n+        assert_allclose(res[:, 0] if k == KTF else res[0, :], ref, atol=1e-05)\n \n     '''only tensorflow tested, need special handle'''\n \n@@ -1242,10 +1236,10 @@ class TestBackend(object):\n             # Theano has some dependency issues for sparse\n             backends.append(KTH)\n \n-        for K in backends:\n-            t_W = K.variable(W)\n-            k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n-            k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n+        for k in backends:\n+            t_W = k.variable(W)\n+            k_s = k.eval(k.dot(k.variable(x_sparse), t_W))\n+            k_d = k.eval(k.dot(k.variable(x_dense), t_W))\n \n             assert k_s.shape == k_d.shape\n             assert_allclose(k_s, k_d, atol=1e-05)\n@@ -1272,13 +1266,13 @@ class TestBackend(object):\n             # Theano has some dependency issues for sparse\n             backends.append(KTH)\n \n-        for K in backends:\n-            k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n-            assert K.is_sparse(k_s)\n+        for k in backends:\n+            k_s = k.concatenate([k.variable(x_sparse_1), k.variable(x_sparse_2)])\n+            assert k.is_sparse(k_s)\n \n-            k_s_d = K.eval(k_s)\n+            k_s_d = k.eval(k_s)\n \n-            k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n+            k_d = k.eval(k.concatenate([k.variable(x_dense_1), k.variable(x_dense_2)]))\n \n             assert k_s_d.shape == k_d.shape\n             assert_allclose(k_s_d, k_d, atol=1e-05)\n@@ -1348,18 +1342,18 @@ class TestBackend(object):\n                 assert np.array_equal(a_list[i], a_list[i + 1])\n \n         for dtype in ('int32', 'int64', 'float32', 'float64'):\n-            for backend in [KTH, KTF]:\n-                t = backend.arange(10, dtype=dtype)\n-                assert backend.dtype(t) == dtype\n+            for k in [KTH, KTF]:\n+                t = k.arange(10, dtype=dtype)\n+                assert k.dtype(t) == dtype\n \n-        for backend in [KTH, KTF]:\n-            start = backend.constant(1, dtype='int32')\n-            t = backend.arange(start)\n-            assert len(backend.eval(t)) == 1\n+        for k in [KTH, KTF]:\n+            start = k.constant(1, dtype='int32')\n+            t = k.arange(start)\n+            assert len(k.eval(t)) == 1\n \n-            start = backend.constant(-1, dtype='int32')\n-            t = backend.arange(start)\n-            assert len(backend.eval(t)) == 0\n+            start = k.constant(-1, dtype='int32')\n+            t = k.arange(start)\n+            assert len(k.eval(t)) == 0\n \n     def test_in_train_phase(self):\n         for training in [True, False]:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#05fe6076a117a184781c2c2dce087189995bf4d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 15 | Files Changed: 5 | Hunks: 14 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 23113 | Contributors (this commit): 96 | Commits (past 90d): 36 | Contributors (cumulative): 127 | DMM Complexity: None\n\nDIFF:\n@@ -553,7 +553,7 @@ def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n             and width and height should be no smaller than 32.\n             E.g. `(224, 224, 3)` would be one valid value.\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         alpha: controls the width of the network.\n             - If `alpha` < 1.0, proportionally decreases the number\n                 of filters in each layer.\n@@ -612,7 +612,7 @@ def _depthwise_conv_block(inputs, pointwise_conv_filters, alpha,\n             (with `channels_last` data format) or\n             (channels, rows, cols) (with `channels_first` data format).\n         pointwise_conv_filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the pointwise convolution).\n+            (i.e. the number of output filters in the pointwise convolution).\n         alpha: controls the width of the network.\n             - If `alpha` < 1.0, proportionally decreases the number\n                 of filters in each layer.\n\n@@ -40,7 +40,7 @@ class _Conv(Layer):\n         rank: An integer, the rank of the convolution,\n             e.g. \"2\" for 2D convolution.\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of n integers, specifying the\n             dimensions of the convolution window.\n         strides: An integer or tuple/list of n integers,\n@@ -252,7 +252,7 @@ class Conv1D(_Conv):\n \n     # Arguments\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of a single integer,\n             specifying the length of the 1D convolution window.\n         strides: An integer or tuple/list of a single integer,\n@@ -361,7 +361,7 @@ class Conv2D(_Conv):\n \n     # Arguments\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of 2 integers, specifying the\n             width and height of the 2D convolution window.\n             Can be a single integer to specify the same value for\n@@ -486,7 +486,7 @@ class Conv3D(_Conv):\n \n     # Arguments\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of 3 integers, specifying the\n             depth, height and width of the 3D convolution window.\n             Can be a single integer to specify the same value for\n@@ -1059,7 +1059,7 @@ class _SeparableConv(_Conv):\n         rank: An integer, the rank of the convolution,\n             e.g. \"2\" for 2D convolution.\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of 2 integers, specifying the\n             width and height of the 2D convolution window.\n             Can be a single integer to specify the same value for\n@@ -1272,7 +1272,7 @@ class SeparableConv1D(_SeparableConv):\n \n     # Arguments\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of single integer,\n             specifying the length of the 1D convolution window.\n         strides: An integer or tuple/list of single integer,\n@@ -1397,7 +1397,7 @@ class SeparableConv2D(_SeparableConv):\n \n     # Arguments\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of 2 integers, specifying the\n             width and height of the 2D convolution window.\n             Can be a single integer to specify the same value for\n\n@@ -25,7 +25,7 @@ class ConvRecurrent2D(Recurrent):\n \n     # Arguments\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of n integers, specifying the\n             dimensions of the convolution window.\n         strides: An integer or tuple/list of n integers,\n@@ -174,7 +174,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n \n     # Arguments\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of n integers, specifying the\n             dimensions of the convolution window.\n         strides: An integer or tuple/list of n integers,\n\n@@ -38,7 +38,7 @@ class LocallyConnected1D(Layer):\n \n     # Arguments\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of a single integer,\n             specifying the length of the 1D convolution window.\n         strides: An integer or tuple/list of a single integer,\n@@ -202,7 +202,7 @@ class LocallyConnected2D(Layer):\n \n     # Arguments\n         filters: Integer, the dimensionality of the output space\n-            (i.e. the number output of filters in the convolution).\n+            (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of 2 integers, specifying the\n             width and height of the 2D convolution window.\n             Can be a single integer to specify the same value for\n\n@@ -2141,8 +2141,8 @@ class LSTM(RNN):\n \n \n def _generate_dropout_ones(inputs, dims):\n-    # Currently, CTNK can't instantiate `ones` with symbolic shapes.\n-    # Will update workaround once CTNK supports it.\n+    # Currently, CNTK can't instantiate `ones` with symbolic shapes.\n+    # Will update workaround once CNTK supports it.\n     if K.backend() == 'cntk':\n         ones = K.ones_like(K.reshape(inputs[:, 0], (-1, 1)))\n         return K.tile(ones, (1, dims))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
