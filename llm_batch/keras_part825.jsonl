{"custom_id": "keras#c7b9ac1743e3d863ac65a291224a31fe2defeca9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 354 | Lines Deleted: 60 | Files Changed: 3 | Hunks: 89 | Methods Changed: 44 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 414 | Churn Cumulative: 608 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import inspect\n \n from keras.src.api_export import keras_export\n from keras.src.losses.loss import Loss\n+from keras.src.losses.losses import CTC\n from keras.src.losses.losses import BinaryCrossentropy\n from keras.src.losses.losses import BinaryFocalCrossentropy\n from keras.src.losses.losses import CategoricalCrossentropy\n@@ -71,6 +72,8 @@ ALL_OBJECTS = {\n     # Image segmentation\n     Dice,\n     Tversky,\n+    # Sequence\n+    CTC,\n     # Probabilistic\n     kl_divergence,\n     poisson,\n@@ -94,6 +97,8 @@ ALL_OBJECTS = {\n     # Image segmentation\n     dice,\n     tversky,\n+    # Sequence\n+    ctc,\n }\n \n ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n\n@@ -11,9 +11,14 @@ from keras.src.utils.numerical_utils import normalize\n \n class LossFunctionWrapper(Loss):\n     def __init__(\n-        self, fn, reduction=\"sum_over_batch_size\", name=None, **kwargs\n+        self,\n+        fn,\n+        reduction=\"sum_over_batch_size\",\n+        name=None,\n+        dtype=None,\n+        **kwargs,\n     ):\n-        super().__init__(reduction=reduction, name=name)\n+        super().__init__(name=name, reduction=reduction, dtype=dtype)\n         self.fn = fn\n         self._fn_kwargs = kwargs\n \n@@ -22,10 +27,10 @@ class LossFunctionWrapper(Loss):\n         return self.fn(y_true, y_pred, **self._fn_kwargs)\n \n     def get_config(self):\n-        base_config = super().get_config()\n-        config = {\"fn\": serialization_lib.serialize_keras_object(self.fn)}\n+        config = super().get_config()\n+        config.update({\"fn\": serialization_lib.serialize_keras_object(self.fn)})\n         config.update(serialization_lib.serialize_keras_object(self._fn_kwargs))\n-        return {**base_config, **config}\n+        return config\n \n     @classmethod\n     def from_config(cls, config):\n@@ -49,12 +54,21 @@ class MeanSquaredError(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n     def __init__(\n-        self, reduction=\"sum_over_batch_size\", name=\"mean_squared_error\"\n+        self,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"mean_squared_error\",\n+        dtype=None,\n     ):\n-        super().__init__(mean_squared_error, reduction=reduction, name=name)\n+        super().__init__(\n+            mean_squared_error, name=name, reduction=reduction, dtype=dtype\n+        )\n \n     def get_config(self):\n         return Loss.get_config(self)\n@@ -75,12 +89,21 @@ class MeanAbsoluteError(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n     def __init__(\n-        self, reduction=\"sum_over_batch_size\", name=\"mean_absolute_error\"\n+        self,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"mean_absolute_error\",\n+        dtype=None,\n     ):\n-        super().__init__(mean_absolute_error, reduction=reduction, name=name)\n+        super().__init__(\n+            mean_absolute_error, name=name, reduction=reduction, dtype=dtype\n+        )\n \n     def get_config(self):\n         return Loss.get_config(self)\n@@ -101,15 +124,23 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n     def __init__(\n         self,\n         reduction=\"sum_over_batch_size\",\n         name=\"mean_absolute_percentage_error\",\n+        dtype=None,\n     ):\n         super().__init__(\n-            mean_absolute_percentage_error, reduction=reduction, name=name\n+            mean_absolute_percentage_error,\n+            name=name,\n+            reduction=reduction,\n+            dtype=dtype,\n         )\n \n     def get_config(self):\n@@ -131,15 +162,23 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n     def __init__(\n         self,\n         reduction=\"sum_over_batch_size\",\n         name=\"mean_squared_logarithmic_error\",\n+        dtype=None,\n     ):\n         super().__init__(\n-            mean_squared_logarithmic_error, reduction=reduction, name=name\n+            mean_squared_logarithmic_error,\n+            name=name,\n+            reduction=reduction,\n+            dtype=dtype,\n         )\n \n     def get_config(self):\n@@ -170,6 +209,10 @@ class CosineSimilarity(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n     def __init__(\n@@ -177,9 +220,14 @@ class CosineSimilarity(LossFunctionWrapper):\n         axis=-1,\n         reduction=\"sum_over_batch_size\",\n         name=\"cosine_similarity\",\n+        dtype=None,\n     ):\n         super().__init__(\n-            cosine_similarity, reduction=reduction, name=name, axis=axis\n+            cosine_similarity,\n+            name=name,\n+            reduction=reduction,\n+            dtype=dtype,\n+            axis=axis,\n         )\n \n     def get_config(self):\n@@ -210,6 +258,10 @@ class Huber(LossFunctionWrapper):\n             `\"sum_over_batch_size\"` or `None`. Defaults to\n             `\"sum_over_batch_size\"`.\n         name: Optional name for the instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n     def __init__(\n@@ -217,8 +269,11 @@ class Huber(LossFunctionWrapper):\n         delta=1.0,\n         reduction=\"sum_over_batch_size\",\n         name=\"huber_loss\",\n+        dtype=None,\n     ):\n-        super().__init__(huber, name=name, reduction=reduction, delta=delta)\n+        super().__init__(\n+            huber, name=name, reduction=reduction, dtype=dtype, delta=delta\n+        )\n \n     def get_config(self):\n         return Loss.get_config(self)\n@@ -241,10 +296,16 @@ class LogCosh(LossFunctionWrapper):\n             `\"sum_over_batch_size\"` or `None`. Defaults to\n             `\"sum_over_batch_size\"`.\n         name: Optional name for the instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n-    def __init__(self, reduction=\"sum_over_batch_size\", name=\"log_cosh\"):\n-        super().__init__(log_cosh, name=name, reduction=reduction)\n+    def __init__(\n+        self, reduction=\"sum_over_batch_size\", name=\"log_cosh\", dtype=None\n+    ):\n+        super().__init__(log_cosh, name=name, reduction=reduction, dtype=dtype)\n \n     def get_config(self):\n         return Loss.get_config(self)\n@@ -268,10 +329,16 @@ class Hinge(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n-    def __init__(self, reduction=\"sum_over_batch_size\", name=\"hinge\"):\n-        super().__init__(hinge, reduction=reduction, name=name)\n+    def __init__(\n+        self, reduction=\"sum_over_batch_size\", name=\"hinge\", dtype=None\n+    ):\n+        super().__init__(hinge, name=name, reduction=reduction, dtype=dtype)\n \n     def get_config(self):\n         return Loss.get_config(self)\n@@ -295,10 +362,18 @@ class SquaredHinge(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n-    def __init__(self, reduction=\"sum_over_batch_size\", name=\"squared_hinge\"):\n-        super().__init__(squared_hinge, reduction=reduction, name=name)\n+    def __init__(\n+        self, reduction=\"sum_over_batch_size\", name=\"squared_hinge\", dtype=None\n+    ):\n+        super().__init__(\n+            squared_hinge, name=name, reduction=reduction, dtype=dtype\n+        )\n \n     def get_config(self):\n         return Loss.get_config(self)\n@@ -321,12 +396,21 @@ class CategoricalHinge(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n     def __init__(\n-        self, reduction=\"sum_over_batch_size\", name=\"categorical_hinge\"\n+        self,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"categorical_hinge\",\n+        dtype=None,\n     ):\n-        super().__init__(categorical_hinge, reduction=reduction, name=name)\n+        super().__init__(\n+            categorical_hinge, name=name, reduction=reduction, dtype=dtype\n+        )\n \n     def get_config(self):\n         return Loss.get_config(self)\n@@ -351,10 +435,18 @@ class KLDivergence(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n-    def __init__(self, reduction=\"sum_over_batch_size\", name=\"kl_divergence\"):\n-        super().__init__(kl_divergence, reduction=reduction, name=name)\n+    def __init__(\n+        self, reduction=\"sum_over_batch_size\", name=\"kl_divergence\", dtype=None\n+    ):\n+        super().__init__(\n+            kl_divergence, name=name, reduction=reduction, dtype=dtype\n+        )\n \n     def get_config(self):\n         return Loss.get_config(self)\n@@ -375,10 +467,16 @@ class Poisson(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n-    def __init__(self, reduction=\"sum_over_batch_size\", name=\"poisson\"):\n-        super().__init__(poisson, reduction=reduction, name=name)\n+    def __init__(\n+        self, reduction=\"sum_over_batch_size\", name=\"poisson\", dtype=None\n+    ):\n+        super().__init__(poisson, name=name, reduction=reduction, dtype=dtype)\n \n     def get_config(self):\n         return Loss.get_config(self)\n@@ -413,6 +511,10 @@ class BinaryCrossentropy(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n \n     Examples:\n \n@@ -473,11 +575,13 @@ class BinaryCrossentropy(LossFunctionWrapper):\n         axis=-1,\n         reduction=\"sum_over_batch_size\",\n         name=\"binary_crossentropy\",\n+        dtype=None,\n     ):\n         super().__init__(\n             binary_crossentropy,\n             name=name,\n             reduction=reduction,\n+            dtype=dtype,\n             from_logits=from_logits,\n             label_smoothing=label_smoothing,\n             axis=axis,\n@@ -543,6 +647,10 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n \n     Examples:\n \n@@ -638,14 +746,16 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n         axis=-1,\n         reduction=\"sum_over_batch_size\",\n         name=\"binary_focal_crossentropy\",\n+        dtype=None,\n     ):\n         super().__init__(\n             binary_focal_crossentropy,\n+            name=name,\n+            reduction=reduction,\n+            dtype=dtype,\n             apply_class_balancing=apply_class_balancing,\n             alpha=alpha,\n             gamma=gamma,\n-            name=name,\n-            reduction=reduction,\n             from_logits=from_logits,\n             label_smoothing=label_smoothing,\n             axis=axis,\n@@ -694,6 +804,10 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n \n     Examples:\n \n@@ -737,11 +851,13 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n         axis=-1,\n         reduction=\"sum_over_batch_size\",\n         name=\"categorical_crossentropy\",\n+        dtype=None,\n     ):\n         super().__init__(\n             categorical_crossentropy,\n             name=name,\n             reduction=reduction,\n+            dtype=dtype,\n             from_logits=from_logits,\n             label_smoothing=label_smoothing,\n             axis=axis,\n@@ -825,6 +941,10 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n \n     Examples:\n \n@@ -870,14 +990,16 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n         axis=-1,\n         reduction=\"sum_over_batch_size\",\n         name=\"categorical_focal_crossentropy\",\n+        dtype=None,\n     ):\n         \"\"\"Initializes `CategoricalFocalCrossentropy` instance.\"\"\"\n         super().__init__(\n             categorical_focal_crossentropy,\n-            alpha=alpha,\n-            gamma=gamma,\n             name=name,\n             reduction=reduction,\n+            dtype=dtype,\n+            alpha=alpha,\n+            gamma=gamma,\n             from_logits=from_logits,\n             label_smoothing=label_smoothing,\n             axis=axis,\n@@ -923,6 +1045,10 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n \n     Examples:\n \n@@ -963,11 +1089,13 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n         ignore_class=None,\n         reduction=\"sum_over_batch_size\",\n         name=\"sparse_categorical_crossentropy\",\n+        dtype=None,\n     ):\n         super().__init__(\n             sparse_categorical_crossentropy,\n             name=name,\n             reduction=reduction,\n+            dtype=dtype,\n             from_logits=from_logits,\n             ignore_class=ignore_class,\n         )\n@@ -1235,7 +1363,7 @@ def mean_absolute_percentage_error(y_true, y_pred):\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n-    epsilon = ops.convert_to_tensor(backend.epsilon())\n+    epsilon = ops.convert_to_tensor(backend.epsilon(), dtype=y_pred.dtype)\n     y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     diff = ops.abs((y_true - y_pred) / ops.maximum(ops.abs(y_true), epsilon))\n     return 100.0 * ops.mean(diff, axis=-1)\n@@ -1364,7 +1492,7 @@ def huber(y_true, y_pred, delta=1.0):\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n     y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n-    delta = ops.convert_to_tensor(delta)\n+    delta = ops.convert_to_tensor(delta, dtype=y_pred.dtype)\n     error = ops.subtract(y_pred, y_true)\n     abs_error = ops.abs(error)\n     half = ops.convert_to_tensor(0.5, dtype=abs_error.dtype)\n@@ -1511,7 +1639,7 @@ def poisson(y_true, y_pred):\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n-    epsilon = ops.convert_to_tensor(backend.epsilon())\n+    epsilon = ops.convert_to_tensor(backend.epsilon(), dtype=y_pred.dtype)\n     return ops.mean(y_pred - y_true * ops.log(y_pred + epsilon), axis=-1)\n \n \n@@ -1885,24 +2013,23 @@ class CTC(LossFunctionWrapper):\n     \"\"\"CTC (Connectionist Temporal Classification) loss.\n \n     Args:\n-        y_true: A tensor of shape `(batch_size, target_max_length)` containing\n-            the true labels in integer format. `0` always represents\n-            the blank/mask index and should not be used for classes.\n-        y_pred: A tensor of shape `(batch_size, output_max_length, num_classes)`\n-            containing logits (the output of your model).\n-            They should *not* be normalized via softmax.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n     \"\"\"\n \n     def __init__(\n         self,\n         reduction=\"sum_over_batch_size\",\n         name=\"ctc\",\n+        dtype=None,\n     ):\n-        super().__init__(\n-            ctc,\n-            name=name,\n-            reduction=reduction,\n-        )\n+        super().__init__(ctc, name=name, reduction=reduction, dtype=dtype)\n \n     def get_config(self):\n         return {\n@@ -1959,8 +2086,16 @@ class Dice(LossFunctionWrapper):\n     ```\n \n     Args:\n-        y_true: tensor of true targets.\n-        y_pred: tensor of predicted targets.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+        axis: Tuple for which dimensions the loss is calculated. Defaults to\n+            `None`.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n \n     Returns:\n         Dice loss value.\n@@ -1996,11 +2131,13 @@ class Dice(LossFunctionWrapper):\n         reduction=\"sum_over_batch_size\",\n         name=\"dice\",\n         axis=None,\n+        dtype=None,\n     ):\n         super().__init__(\n             dice,\n             name=name,\n             reduction=reduction,\n+            dtype=dtype,\n             axis=axis,\n         )\n         self.axis = axis\n@@ -2058,10 +2195,18 @@ class Tversky(LossFunctionWrapper):\n     Dice Loss.\n \n     Args:\n-        y_true: tensor of true targets.\n-        y_pred: tensor of predicted targets.\n-        alpha: coefficient controlling incidence of false positives.\n-        beta: coefficient controlling incidence of false negatives.\n+        alpha: The coefficient controlling incidence of false positives.\n+            Defaults to `0.5`.\n+        beta: The coefficient controlling incidence of false negatives.\n+            Defaults to `0.5`.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`).\n \n     Returns:\n         Tversky loss value.\n@@ -2077,13 +2222,15 @@ class Tversky(LossFunctionWrapper):\n         beta=0.5,\n         reduction=\"sum_over_batch_size\",\n         name=\"tversky\",\n+        dtype=None,\n     ):\n         super().__init__(\n             tversky,\n-            alpha=alpha,\n-            beta=beta,\n             name=name,\n             reduction=reduction,\n+            dtype=dtype,\n+            alpha=alpha,\n+            beta=beta,\n         )\n         self.alpha = alpha\n         self.beta = beta\n\n@@ -78,6 +78,13 @@ class MeanSquaredErrorTest(testing.TestCase):\n         loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n         self.assertAlmostEqual(loss, 227.69998)\n \n+    def test_dtype_arg(self):\n+        mse_obj = losses.MeanSquaredError(dtype=\"bfloat16\")\n+        y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n+        y_pred = np.array([[4, 8, 12], [8, 1, 3]], dtype=\"float32\")\n+        loss = mse_obj(y_true, y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class MeanAbsoluteErrorTest(testing.TestCase):\n     def test_config(self):\n@@ -146,6 +153,13 @@ class MeanAbsoluteErrorTest(testing.TestCase):\n         loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n         self.assertAlmostEqual(loss, 25.29999)\n \n+    def test_dtype_arg(self):\n+        mae_obj = losses.MeanAbsoluteError(dtype=\"bfloat16\")\n+        y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n+        y_pred = np.array([[4, 8, 12], [8, 1, 3]], dtype=\"float32\")\n+        loss = mae_obj(y_true, y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class MeanAbsolutePercentageErrorTest(testing.TestCase):\n     def test_config(self):\n@@ -207,6 +221,13 @@ class MeanAbsolutePercentageErrorTest(testing.TestCase):\n         loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n         self.assertAlmostEqual(loss, [621.8518, 352.6666])\n \n+    def test_dtype_arg(self):\n+        mape_obj = losses.MeanAbsolutePercentageError(dtype=\"bfloat16\")\n+        y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n+        y_pred = np.array([[4, 8, 12], [8, 1, 3]], dtype=\"float32\")\n+        loss = mape_obj(y_true, y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class MeanSquaredLogarithmicErrorTest(testing.TestCase):\n     def test_config(self):\n@@ -255,6 +276,13 @@ class MeanSquaredLogarithmicErrorTest(testing.TestCase):\n         loss = msle_obj(y_true, y_pred, sample_weight=0)\n         self.assertAlmostEqual(loss, 0.0, 3)\n \n+    def test_dtype_arg(self):\n+        msle_obj = losses.MeanSquaredLogarithmicError(dtype=\"bfloat16\")\n+        y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n+        y_pred = np.array([[4, 8, 12], [8, 1, 3]], dtype=\"float32\")\n+        loss = msle_obj(y_true, y_pred, sample_weight=2.3)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class HingeTest(testing.TestCase):\n     def test_unweighted(self):\n@@ -309,6 +337,13 @@ class HingeTest(testing.TestCase):\n         loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n         self.assertEqual(loss, 0.0)\n \n+    def test_dtype_arg(self):\n+        hinge_obj = losses.Hinge(dtype=\"bfloat16\")\n+        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n+        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n+        loss = hinge_obj(y_true, y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class SquaredHingeTest(testing.TestCase):\n     def test_unweighted(self):\n@@ -363,6 +398,13 @@ class SquaredHingeTest(testing.TestCase):\n         loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n         self.assertEqual(loss, 0.0)\n \n+    def test_dtype_arg(self):\n+        hinge_obj = losses.SquaredHinge(dtype=\"bfloat16\")\n+        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n+        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n+        loss = hinge_obj(y_true, y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class CategoricalHingeTest(testing.TestCase):\n     def test_unweighted(self):\n@@ -417,6 +459,13 @@ class CategoricalHingeTest(testing.TestCase):\n         loss = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n         self.assertEqual(loss, 0.0)\n \n+    def test_dtype_arg(self):\n+        hinge_obj = losses.CategoricalHinge(dtype=\"bfloat16\")\n+        y_true = np.array([[0.0, 1.0], [0.0, 0.0]])\n+        y_pred = np.array([[0.6, 0.4], [0.4, 0.6]])\n+        loss = hinge_obj(y_true, y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class CosineSimilarityTest(testing.TestCase):\n     def l2_norm(self, x, axis):\n@@ -499,6 +548,12 @@ class CosineSimilarityTest(testing.TestCase):\n         expected_loss = -np.mean(self.expected_loss)\n         self.assertAlmostEqual(loss, expected_loss, 3)\n \n+    def test_dtype_arg(self):\n+        self.setup()\n+        cosine_obj = losses.CosineSimilarity(dtype=\"bfloat16\")\n+        loss = cosine_obj(self.y_true, self.y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class HuberLossTest(testing.TestCase):\n     def huber_loss(self, y_true, y_pred, delta=1.0):\n@@ -605,11 +660,11 @@ class HuberLossTest(testing.TestCase):\n         )\n         self.assertAlmostEqual(loss, actual_loss, 3)\n \n-    def test_loss_with_non_default_dtype(self):\n-        # Test case for GitHub issue:\n-        # https://github.com/tensorflow/tensorflow/issues/39004\n-        # TODO\n-        pass\n+    def test_dtype_arg(self):\n+        self.setup()\n+        h_obj = losses.Huber(dtype=\"bfloat16\")\n+        loss = h_obj(self.y_true, self.y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n \n \n class LogCoshTest(testing.TestCase):\n@@ -702,6 +757,12 @@ class LogCoshTest(testing.TestCase):\n         )\n         self.assertAlmostEqual(loss, 0.0, 3)\n \n+    def test_dtype_arg(self):\n+        self.setup()\n+        logcosh_obj = losses.LogCosh(dtype=\"bfloat16\")\n+        loss = logcosh_obj(self.y_true, self.y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class KLDivergenceTest(testing.TestCase):\n     def setup(self):\n@@ -783,6 +844,12 @@ class KLDivergenceTest(testing.TestCase):\n         loss = k_obj(self.y_true, self.y_pred, sample_weight=0)\n         self.assertAlmostEqual(loss, 0.0, 3)\n \n+    def test_dtype_arg(self):\n+        self.setup()\n+        k_obj = losses.KLDivergence(dtype=\"bfloat16\")\n+        loss = k_obj(self.y_true, self.y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class PoissonTest(testing.TestCase):\n     def setup(self):\n@@ -870,6 +937,12 @@ class PoissonTest(testing.TestCase):\n         loss = poisson_obj(self.y_true, self.y_pred, sample_weight=0)\n         self.assertAlmostEqual(loss, 0.0, 3)\n \n+    def test_dtype_arg(self):\n+        self.setup()\n+        poisson_obj = losses.Poisson(dtype=\"bfloat16\")\n+        loss = poisson_obj(self.y_true, self.y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class BinaryCrossentropyTest(testing.TestCase):\n     def test_config(self):\n@@ -974,6 +1047,19 @@ class BinaryCrossentropyTest(testing.TestCase):\n         with self.assertRaisesRegex(ValueError, \"must have the same shape\"):\n             cce_obj(y_true, y_pred)\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"torch\",\n+        reason=\"Torch doesn't support bfloat16 for BinaryCrossentropy\",\n+    )\n+    def test_dtype_arg(self):\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=\"float32\")\n+        y_pred = np.array(\n+            [[0.9, 0.1, 0.2], [0.3, 0.8, 0.1], [0.1, 0.2, 0.7]], dtype=\"float32\"\n+        )\n+        bce_obj = losses.BinaryCrossentropy(dtype=\"bfloat16\")\n+        loss = bce_obj(y_true, y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class CategoricalCrossentropyTest(testing.TestCase):\n     def test_config(self):\n@@ -1089,6 +1175,16 @@ class CategoricalCrossentropyTest(testing.TestCase):\n         with self.assertRaisesRegex(ValueError, \"must have the same shape\"):\n             cce_obj(y_true, y_pred)\n \n+    def test_dtype_arg(self):\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=\"int64\")\n+        y_pred = np.array(\n+            [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]],\n+            dtype=\"float32\",\n+        )\n+        cce_obj = losses.CategoricalCrossentropy(dtype=\"bfloat16\")\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class SparseCategoricalCrossentropyTest(testing.TestCase):\n     def test_config(self):\n@@ -1189,6 +1285,16 @@ class SparseCategoricalCrossentropyTest(testing.TestCase):\n         loss = cce_obj(y_true, logits)\n         self.assertAllClose([[0.0, 1.480129]], loss)\n \n+    def test_dtype_arg(self):\n+        y_true = np.array([[0], [1], [2]], dtype=\"int64\")\n+        y_pred = np.array(\n+            [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]],\n+            dtype=\"float32\",\n+        )\n+        cce_obj = losses.SparseCategoricalCrossentropy(dtype=\"bfloat16\")\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class BinaryFocalCrossentropyTest(testing.TestCase):\n     def test_config(self):\n@@ -1282,6 +1388,19 @@ class BinaryFocalCrossentropyTest(testing.TestCase):\n         loss = obj(y_true, y_pred)\n         self.assertAllClose(loss, (0.515547, 0.020513))\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"torch\",\n+        reason=\"Torch doesn't support bfloat16 for BinaryFocalCrossentropy\",\n+    )\n+    def test_dtype_arg(self):\n+        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape(\n+            [2, 2]\n+        )\n+        obj = losses.BinaryFocalCrossentropy(dtype=\"bfloat16\")\n+        loss = obj(y_true, y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class CategoricalFocalCrossentropyTest(testing.TestCase):\n     def test_config(self):\n@@ -1381,21 +1500,32 @@ class CategoricalFocalCrossentropyTest(testing.TestCase):\n         expected_value = 0.06685\n         self.assertAlmostEqual(loss, expected_value, 3)\n \n+    def test_dtype_arg(self):\n+        logits = np.array([[4.9, -0.5, 2.05]])\n+        y_true = np.array([[1, 0, 0]])\n+        cce_obj = losses.CategoricalFocalCrossentropy(\n+            from_logits=True, dtype=\"bfloat16\"\n+        )\n+        loss = cce_obj(y_true, logits)\n+        self.assertDType(loss, \"bfloat16\")\n+\n \n class CTCTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(losses.CTC(name=\"myctc\"))\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"numpy\",\n-        reason=\"Numpy does not support CTC loss\",\n-    )\n     def test_correctness(self):\n         logits = (np.arange(24).reshape((2, 4, 3)).astype(\"float32\") - 12) / 100\n         y_true = np.array(([[1, 2, 1, 0], [1, 2, 0, 2]]))\n         output = losses.CTC()(y_true, logits)\n         self.assertAllClose(output, 2.448645)\n \n+    def test_dtype_arg(self):\n+        logits = (np.arange(24).reshape((2, 4, 3)).astype(\"float32\") - 12) / 100\n+        y_true = np.array(([[1, 2, 1, 0], [1, 2, 0, 2]]))\n+        output = losses.CTC(dtype=\"bfloat16\")(y_true, logits)\n+        self.assertDType(output, \"bfloat16\")\n+\n \n class DiceTest(testing.TestCase):\n     def test_config(self):\n@@ -1427,6 +1557,12 @@ class DiceTest(testing.TestCase):\n         output = losses.Dice(axis=(1, 2, 3), reduction=None)(y_true, y_pred)\n         self.assertAllClose(output, [0.5, 0.75757575])\n \n+    def test_dtype_arg(self):\n+        y_true = np.array(([[1, 2], [1, 2]]))\n+        y_pred = np.array(([[4, 1], [6, 1]]))\n+        output = losses.Dice(dtype=\"bfloat16\")(y_true, y_pred)\n+        self.assertDType(output, \"bfloat16\")\n+\n \n class TverskyTest(testing.TestCase):\n     def test_config(self):\n@@ -1463,3 +1599,9 @@ class TverskyTest(testing.TestCase):\n         )\n         output = losses.Tversky(alpha=0.2, beta=0.8)(y_true, y_pred)\n         self.assertAllClose(output, 0.7916667)\n+\n+    def test_dtype_arg(self):\n+        y_true = np.array(([[1, 2], [1, 2]]))\n+        y_pred = np.array(([[4, 1], [6, 1]]))\n+        output = losses.Tversky(dtype=\"bfloat16\")(y_true, y_pred)\n+        self.assertDType(output, \"bfloat16\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#453350220918e5b59c563d289688ab58b71a87ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 756 | Lines Deleted: 139 | Files Changed: 14 | Hunks: 69 | Methods Changed: 69 | Complexity Δ (Sum/Max): 81/32 | Churn Δ: 895 | Churn Cumulative: 2755 | Contributors (this commit): 7 | Commits (past 90d): 76 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,3 +11,4 @@ from keras.src.dtype_policies.dtype_policy import DTypePolicy\n from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n from keras.src.dtype_policies.dtype_policy import QuantizedDTypePolicy\n from keras.src.dtype_policies.dtype_policy import QuantizedFloat8DTypePolicy\n+from keras.src.dtype_policies.dtype_policy_map import DTypePolicyMap\n\n@@ -11,3 +11,4 @@ from keras.src.dtype_policies.dtype_policy import DTypePolicy\n from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n from keras.src.dtype_policies.dtype_policy import QuantizedDTypePolicy\n from keras.src.dtype_policies.dtype_policy import QuantizedFloat8DTypePolicy\n+from keras.src.dtype_policies.dtype_policy_map import DTypePolicyMap\n\n@@ -6,12 +6,14 @@ from keras.src.dtype_policies.dtype_policy import DTypePolicy\n from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n from keras.src.dtype_policies.dtype_policy import QuantizedDTypePolicy\n from keras.src.dtype_policies.dtype_policy import QuantizedFloat8DTypePolicy\n+from keras.src.dtype_policies.dtype_policy_map import DTypePolicyMap\n \n ALL_OBJECTS = {\n     DTypePolicy,\n     FloatDTypePolicy,\n     QuantizedDTypePolicy,\n     QuantizedFloat8DTypePolicy,\n+    DTypePolicyMap,\n }\n ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n \n\n@@ -173,7 +173,19 @@ class DTypePolicy:\n         return cls(**config)\n \n     def __repr__(self):\n-        return f'<FloatDTypePolicy \"{self._name}\">'\n+        class_name = self.__class__.__name__\n+        if class_name == \"DTypePolicy\":\n+            class_name = \"FloatDTypePolicy\"\n+        return f'<{class_name} \"{self._name}\">'\n+\n+    def __eq__(self, other):\n+        if self.__class__ in (DTypePolicy, FloatDTypePolicy):\n+            if type(other) not in (DTypePolicy, FloatDTypePolicy):\n+                return False\n+        else:\n+            if type(other) is not self.__class__:\n+                return False\n+        return self._name == other._name\n \n     def _should_cast(self, x, autocast, dtype):\n         x_dtype = backend.standardize_dtype(x.dtype)\n@@ -217,8 +229,13 @@ class QuantizedDTypePolicy(DTypePolicy):\n         \"\"\"\n         return self._quantization_mode\n \n-    def __repr__(self):\n-        return f'<QuantizedDTypePolicy \"{self._name}\">'\n+    def __eq__(self, other):\n+        if super().__eq__(other) is False:\n+            return False\n+        return (\n+            self._quantization_mode == other._quantization_mode\n+            and self._source_name == other._source_name\n+        )\n \n     def get_config(self):\n         return {\n@@ -261,8 +278,10 @@ class QuantizedFloat8DTypePolicy(QuantizedDTypePolicy):\n         \"\"\"\n         return self._amax_history_length\n \n-    def __repr__(self):\n-        return f'<QuantizedFloat8DTypePolicy \"{self._name}\">'\n+    def __eq__(self, other):\n+        if super().__eq__(other) is False:\n+            return False\n+        return self._amax_history_length == other._amax_history_length\n \n     def get_config(self):\n         config = super().get_config()\n\n@@ -0,0 +1,212 @@\n+import re\n+from collections.abc import MutableMapping\n+\n+from keras.src import dtype_policies\n+from keras.src.api_export import keras_export\n+from keras.src.dtype_policies import DTypePolicy\n+\n+\n+@keras_export([\"keras.dtype_policies.DTypePolicyMap\"])\n+class DTypePolicyMap(DTypePolicy, MutableMapping):\n+    \"\"\"Dict-like object mapping layer paths to `DTypePolicy` instances.\n+\n+    `DTypePolicyMap` can be used in `get_config` in layers and subclasses to\n+    support a complex configurations of dtype policies.\n+\n+    For example, we can modify `get_config` in `layers.MultiHeadAttention` as\n+    follows to support the mixing of dtype policies, such as quantization.\n+\n+    ```python\n+    @keras.saving.register_keras_serializable(\"MyPackage\")\n+    class MyMultiHeadAttention(keras.layers.MultiHeadAttention):\n+        def get_config(self):\n+            config = super().get_config()\n+            dtype_policy_map = dtype_policies.DTypePolicyMap()\n+            for layer in self._flatten_layers():\n+                if layer.dtype_policy.is_quantized:\n+                    dtype_policy_map[layer.path] = layer.dtype_policy\n+            if len(dtype_policy_map) > 0:\n+                config.update({\"dtype\": dtype_policy_map})\n+            return config\n+    ```\n+\n+    Internally, `DTypePolicyMap` uses a string as a key and a `DTypePolicy`\n+    as the value. Typically, the key used for querying is the `Layer.path`.\n+    However, it is also possible to set a regex as the key. See the docstring of\n+    `get` for more details.\n+\n+    See below for a usage example. You can define the naming schema\n+    of the `DTypePolicy`, and then retrieve the corresponding `DTypePolicy`\n+    instance.\n+\n+    ```python\n+    dtype_policy_map = DTypePolicyMap()\n+    dtype_policy_map[\"layer/dense_0\"] = FloatDTypePolicy(\"bfloat16\")\n+    dtype_policy_map[\"layer/dense_1\"] = QuantizedDTypePolicy(\"int8\", \"bfloat16\")\n+\n+    policy_0 = dtype_policy_map[\"layer/dense_0\"]\n+    policy_1 = dtype_policy_map[\"layer/dense_1\"]\n+    policy_2 = dtype_policy_map[\"layer/dense_2\"]  # No hit\n+    assert policy_0 == FloatDTypePolicy(\"bfloat16\")\n+    assert policy_1 == QuantizedDTypePolicy(\"int8\", \"bfloat16\")\n+    assert policy_2 == keras.config.dtype_policy()\n+    ```\n+\n+    Args:\n+        default_policy: An optional `DTypePolicy` instance specifying the\n+            default dtype policy. If not specified, the value will default to\n+            `keras.config.dtype_policy()`.\n+        policy_map: An optional dict that maps string to `DTypePolicy`\n+            instances. Defaults to `None`\n+    \"\"\"\n+\n+    def __init__(self, default_policy=None, policy_map=None):\n+        if isinstance(default_policy, DTypePolicyMap):\n+            raise ValueError(\"`default_policy` cannot be a `DTypePolicyMap`.\")\n+        if policy_map is not None and not isinstance(policy_map, dict):\n+            raise TypeError(\n+                \"If specified, `policy_map` must be a dict. \"\n+                f\"Received: policy_map={policy_map} of type {type(policy_map)}\"\n+            )\n+        self._default_policy_arg = default_policy\n+        self._default_policy = dtype_policies.get(default_policy)\n+        self._policy_map = policy_map or dict()\n+\n+    @property\n+    def name(self):\n+        return \"map_\" + self.default_policy._name\n+\n+    @property\n+    def default_policy(self):\n+        \"\"\"The default dtype policy.\n+\n+        If `default_policy` is not specified in the constructor, this property\n+        will be `keras.config.dtype_policy()`.\n+        \"\"\"\n+        return dtype_policies.get(self._default_policy)\n+\n+    @property\n+    def is_quantized(self):\n+        return self.default_policy._is_quantized\n+\n+    @property\n+    def variable_dtype(self):\n+        return self.default_policy.variable_dtype\n+\n+    @property\n+    def compute_dtype(self):\n+        return self.default_policy.compute_dtype\n+\n+    def __getitem__(self, key):\n+        \"\"\"Retrieves the corresponding `DTypePolicy` by the string key.\n+\n+        When there isn't an exact match, all the existing keys in the map\n+        will be treated as a regex and map against the input key again. When\n+        there are multiple matches for the regex, an `ValueError` will be\n+        raised. Returns `self.default_policy` if there isn't any match found.\n+\n+        Args:\n+            key: String key to query a `DTypePolicy`.\n+\n+        Returns:\n+            Corresponding `DTypePolicy` based on the query.\n+        \"\"\"\n+        if key in self._policy_map:\n+            return self._policy_map[key]\n+\n+        matching_keys = []\n+        for k in self._policy_map:\n+            if re.search(k, key):\n+                matching_keys.append(k)\n+        if len(matching_keys) > 1:\n+            raise ValueError(\n+                f\"Path '{key}' matches multiple dtype policy \"\n+                f\"specification keys: {matching_keys}. Please make \"\n+                \"sure each path only matches at most \"\n+                \"one dtype policy specification key in the DTypePolicyMap.\"\n+            )\n+        elif len(matching_keys) == 1:\n+            return self._policy_map[matching_keys[0]]\n+        return self.default_policy\n+\n+    def __setitem__(self, key, policy):\n+        \"\"\"Insert `DTypePolicy` to the `DTypePolicyMap`.\n+\n+        Args:\n+            key: String key for the `DTypePolicy`.\n+            policy: The `DTypePolicy`.\n+        \"\"\"\n+        if key in self._policy_map:\n+            raise ValueError(\n+                f\"{key} already exist in the DTypePolicyMap with \"\n+                f\"value {self._policy_map[key]}. Please make sure to \"\n+                \"not use duplicated keys.\"\n+            )\n+        try:\n+            policy = dtype_policies.get(policy)\n+        except Exception:\n+            raise ValueError(\n+                \"Cannot interpret the assigned value by \"\n+                \"`keras.dtype_policies.get`. \"\n+                f\"Received: {policy} of type {type(policy)}\"\n+            )\n+        self._policy_map[key] = policy\n+\n+    def __delitem__(self, key):\n+        # Let the dict to handle the key missing error\n+        return self._policy_map.pop(key)\n+\n+    def __contains__(self, key):\n+        return key in self._policy_map\n+\n+    def get_config(self):\n+        from keras.src.saving import serialization_lib\n+\n+        policy_map = self._policy_map\n+        if self._default_policy_arg is None:\n+            # `default_policy=None` enables us to defer to\n+            # `keras.config.dtype_policy()` during loading.\n+            # To support this feature, we can set `_name` and `_source_name` to\n+            # `None` in `FloatDTypePolicy` and `QuantizedDTypePolicy`,\n+            # respectively.\n+            for policy in policy_map.values():\n+                if isinstance(policy, dtype_policies.QuantizedDTypePolicy):\n+                    policy._name = None\n+                    policy._source_name = None\n+                elif isinstance(policy, dtype_policies.DTypePolicy):\n+                    policy._name = None\n+        return {\n+            \"default_policy\": self._default_policy_arg,\n+            \"policy_map\": serialization_lib.serialize_keras_object(policy_map),\n+        }\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        from keras.src.saving import serialization_lib\n+\n+        config = config.copy()\n+        config[\"policy_map\"] = serialization_lib.deserialize_keras_object(\n+            config[\"policy_map\"], custom_objects=custom_objects\n+        )\n+        return cls(**config)\n+\n+    def __len__(self):\n+        return len(self._policy_map)\n+\n+    def __iter__(self):\n+        return iter(self._policy_map)\n+\n+    def __repr__(self):\n+        default_policy = (\n+            self._default_policy.name\n+            if self._default_policy is not None\n+            else None\n+        )\n+        mapping = []\n+        for k, v in self._policy_map.items():\n+            mapping.append((k, v.name))\n+        return (\n+            f\"<DTypePolicyMap at {hex(id(self))} \"\n+            f\"default_policy={default_policy}, \"\n+            f\"mapping={mapping}>\"\n+        )\n\n@@ -0,0 +1,347 @@\n+import numpy as np\n+import pytest\n+\n+from keras.src import dtype_policies\n+from keras.src import layers\n+from keras.src import models\n+from keras.src import saving\n+from keras.src import testing\n+from keras.src.dtype_policies.dtype_policy import dtype_policy\n+from keras.src.dtype_policies.dtype_policy import set_dtype_policy\n+from keras.src.dtype_policies.dtype_policy_map import DTypePolicyMap\n+\n+\n+class DTypePolicyMapTest(testing.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        self._global_dtype_policy = dtype_policy()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        set_dtype_policy(self._global_dtype_policy)\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_basic_usage(self):\n+        # Create a subclass that might contain mixing dtype policies for\n+        # sublayers.\n+        # It is important to ensure that `dtype` is passed to sublayers and\n+        # that each sublayer has a unique `name`.\n+        @saving.register_keras_serializable()\n+        class Subclass(layers.Layer):\n+            def __init__(self, dtype=None, name=\"subclass\", **kwargs):\n+                super().__init__(dtype=dtype, name=name, **kwargs)\n+                self.dense = layers.Dense(8, dtype=dtype, name=f\"{name}_dense\")\n+                self.bn = layers.BatchNormalization(\n+                    dtype=dtype, name=f\"{name}_bn\"\n+                )\n+                self.relu = layers.ReLU(dtype=dtype, name=f\"{name}_relu\")\n+\n+            def call(self, inputs, training=None):\n+                return self.relu(self.bn(self.dense(inputs), training=training))\n+\n+            def get_config(self):\n+                # Typically, we only need to record the quantized policy for\n+                # `DTypePolicyMap`\n+                config = super().get_config()\n+                dtype_policy_map = DTypePolicyMap()\n+                for layer in self._flatten_layers():\n+                    if layer.quantization_mode is not None:\n+                        dtype_policy_map[layer.path] = layer.dtype_policy\n+                if len(dtype_policy_map) > 0:\n+                    config.update({\"dtype\": dtype_policy_map})\n+                return config\n+\n+        # Instantiate the model\n+        inputs = layers.Input([4])\n+        outputs = Subclass()(inputs)\n+        model = models.Model(inputs, outputs)\n+\n+        # Quantize the model to make mixing of dtype policies in sublayers\n+        model.quantize(\"int8\")\n+        for layer in model._flatten_layers():\n+            if isinstance(layer, layers.Dense):\n+                self.assertEqual(\n+                    layer.dtype_policy,\n+                    dtype_policies.QuantizedDTypePolicy(\"int8\"),\n+                )\n+            elif isinstance(layer, layers.BatchNormalization):\n+                self.assertEqual(\n+                    layer.dtype_policy, dtype_policies.FloatDTypePolicy()\n+                )\n+            elif isinstance(layer, layers.ReLU):\n+                self.assertEqual(\n+                    layer.dtype_policy, dtype_policies.FloatDTypePolicy()\n+                )\n+\n+        # Verify the output after saving and loading\n+        x = np.random.uniform(size=[16, 4])\n+        temp_dir = self.get_temp_dir()\n+        y = model(x, training=False)\n+        model.save(f\"{temp_dir}/model.keras\")\n+        reloaded_model = saving.load_model(f\"{temp_dir}/model.keras\")\n+        reloaded_y = reloaded_model(x, training=False)\n+        self.assertAllClose(y, reloaded_y)\n+\n+    def test_add(self):\n+        dtype_policy_map = DTypePolicyMap()\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            \"bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n+            \"int8\", \"mixed_bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_2\"] = (\n+            dtype_policies.QuantizedFloat8DTypePolicy(\"float8\", \"mixed_float16\")\n+        )\n+\n+        self.assertLen(dtype_policy_map, 3)\n+\n+        policy = dtype_policy_map[\"layer/dense_0\"]\n+        self.assertIsInstance(policy, dtype_policies.FloatDTypePolicy)\n+        self.assertEqual(policy.name, \"bfloat16\")\n+\n+        policy = dtype_policy_map[\"layer/dense_1\"]\n+        self.assertIsInstance(policy, dtype_policies.QuantizedDTypePolicy)\n+        self.assertEqual(policy._source_name, \"mixed_bfloat16\")\n+        self.assertEqual(policy.quantization_mode, \"int8\")\n+\n+        policy = dtype_policy_map[\"layer/dense_2\"]\n+        self.assertIsInstance(policy, dtype_policies.QuantizedFloat8DTypePolicy)\n+        self.assertEqual(policy._source_name, \"mixed_float16\")\n+        self.assertEqual(policy.quantization_mode, \"float8\")\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"layer/dense_0 already exist in the DTypePolicyMap\"\n+        ):\n+            dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+                \"float32\"\n+            )\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Cannot interpret the assigned value.\"\n+        ):\n+            dtype_policy_map[\"layer/dense_3\"] = 123\n+\n+    def test_get(self):\n+        dtype_policy_map = DTypePolicyMap()\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            \"bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n+            \"int8\", \"mixed_bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_2\"] = (\n+            dtype_policies.QuantizedFloat8DTypePolicy(\"float8\", \"mixed_float16\")\n+        )\n+\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_0\"],\n+            dtype_policies.FloatDTypePolicy(\"bfloat16\"),\n+        )\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_1\"],\n+            dtype_policies.QuantizedDTypePolicy(\"int8\", \"mixed_bfloat16\"),\n+        )\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_2\"],\n+            dtype_policies.QuantizedFloat8DTypePolicy(\n+                \"float8\", \"mixed_float16\"\n+            ),\n+        )\n+\n+        self.assertNotEqual(\n+            dtype_policy_map[\"layer/dense_2\"],\n+            dtype_policies.QuantizedFloat8DTypePolicy(\"float8\", \"bfloat16\"),\n+        )\n+\n+        # No hit\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/batch_normalization\"],\n+            dtype_policy_map.default_policy,\n+        )\n+\n+        # It will cause a ValueError in the case of one-to-many.\n+        dtype_policy_map[\"dense\"] = dtype_policies.FloatDTypePolicy(\"float32\")\n+        dtype_policy_map[\"dense_1\"] = dtype_policies.FloatDTypePolicy(\"float32\")\n+        with self.assertRaisesRegex(\n+            ValueError, \"Path 'dense_10' matches multiple dtype policy\"\n+        ):\n+            dtype_policy_map[\"dense_10\"]\n+\n+    def test_delete(self):\n+        dtype_policy_map = DTypePolicyMap()\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            \"bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n+            \"int8\", \"mixed_bfloat16\"\n+        )\n+\n+        self.assertEqual(\n+            dtype_policy_map.pop(\"layer/dense_0\"),\n+            dtype_policies.FloatDTypePolicy(\"bfloat16\"),\n+        )\n+        with self.assertRaises(KeyError):\n+            dtype_policy_map.pop(\"layer/dense_0\")\n+\n+        # Test `del`, causing no hit\n+        del dtype_policy_map[\"layer/dense_1\"]\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_1\"], dtype_policy_map.default_policy\n+        )\n+\n+        self.assertLen(dtype_policy_map, 0)\n+\n+    def test_len(self):\n+        dtype_policy_map = DTypePolicyMap()\n+        self.assertLen(dtype_policy_map, 0)\n+\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            \"bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n+            \"int8\", \"mixed_bfloat16\"\n+        )\n+        self.assertLen(dtype_policy_map, 2)\n+\n+    def test_iter(self):\n+        dtype_policy_map = DTypePolicyMap()\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            \"bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n+            \"int8\", \"mixed_bfloat16\"\n+        )\n+\n+        self.assertEqual(\n+            list(dtype_policy_map.keys()), [\"layer/dense_0\", \"layer/dense_1\"]\n+        )\n+\n+        keys = []\n+        values = []\n+        for k, v in dtype_policy_map.items():\n+            keys.append(k)\n+            values.append(v)\n+        self.assertEqual(keys, [\"layer/dense_0\", \"layer/dense_1\"])\n+        self.assertEqual(\n+            values,\n+            [\n+                dtype_policies.FloatDTypePolicy(\"bfloat16\"),\n+                dtype_policies.QuantizedDTypePolicy(\"int8\", \"mixed_bfloat16\"),\n+            ],\n+        )\n+\n+    def test_in(self):\n+        dtype_policy_map = DTypePolicyMap()\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            \"bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n+            \"int8\", \"mixed_bfloat16\"\n+        )\n+\n+        self.assertTrue(\"layer/dense_0\" in dtype_policy_map)\n+        self.assertTrue(\"layer/dense_1\" in dtype_policy_map)\n+        self.assertFalse(\"layer/dense_2\" in dtype_policy_map)\n+\n+    def test_default_policy(self):\n+        # Test default_policy is set to `\"float32\"`\n+        dtype_policy_map = DTypePolicyMap(default_policy=\"mixed_bfloat16\")\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            \"mixed_bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n+            \"int8\", \"mixed_bfloat16\"\n+        )\n+        config = dtype_policy_map.get_config()\n+        dtype_policy_map = DTypePolicyMap.from_config(config)\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_0\"],\n+            dtype_policies.FloatDTypePolicy(\"mixed_bfloat16\"),\n+        )\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_1\"],\n+            dtype_policies.QuantizedDTypePolicy(\"int8\", \"mixed_bfloat16\"),\n+        )\n+        # No hit, defers to `dtype_policy_map.default_policy`\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_2\"], dtype_policy_map.default_policy\n+        )\n+\n+        # Test that default_policy defers to `keras.config.dtype_policy()`\n+        # during loading\n+        set_dtype_policy(\"bfloat16\")\n+        dtype_policy_map = DTypePolicyMap()\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            \"mixed_bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n+            \"int8\", \"mixed_bfloat16\"\n+        )\n+        config = dtype_policy_map.get_config()\n+        dtype_policy_map = DTypePolicyMap.from_config(config)\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_0\"],\n+            dtype_policies.FloatDTypePolicy(\"bfloat16\"),\n+        )\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_1\"],\n+            dtype_policies.QuantizedDTypePolicy(\"int8\", \"bfloat16\"),\n+        )\n+        # No hit, defers to `dtype_policy_map.default_policy` which is\n+        # `keras.config.dtype_policy()`\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_2\"], dtype_policy_map.default_policy\n+        )\n+        self.assertEqual(\n+            dtype_policy_map[\"layer/dense_2\"], dtype_policies.get(\"bfloat16\")\n+        )\n+\n+    def test_serialization(self):\n+        dtype_policy_map = DTypePolicyMap(default_policy=\"mixed_bfloat16\")\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            \"mixed_bfloat16\"\n+        )\n+        dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n+            \"int8\", \"mixed_bfloat16\"\n+        )\n+\n+        config = dtype_policies.serialize(dtype_policy_map)\n+        reloaded_dtype_policy_map = dtype_policies.deserialize(config)\n+        self.assertEqual(\n+            dtype_policy_map.default_policy,\n+            reloaded_dtype_policy_map.default_policy,\n+        )\n+        for k, v in dtype_policy_map.items():\n+            self.assertEqual(reloaded_dtype_policy_map[k], v)\n+\n+        # Test that config remains intact during deserialization\n+        config = dtype_policy_map.get_config()\n+        original_config = config.copy()\n+        DTypePolicyMap.from_config(config)\n+        self.assertDictEqual(config, original_config)\n+\n+    def test_repr(self):\n+        dtype_policy_map = DTypePolicyMap()\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            \"mixed_bfloat16\"\n+        )\n+        repr_str = repr(dtype_policy_map)\n+        self.assertTrue(\"DTypePolicyMap\" in repr_str)\n+        self.assertTrue(\"default_policy\" in repr_str)\n+        self.assertTrue(\n+            \"mapping=[('layer/dense_0', 'mixed_bfloat16')]\" in repr_str\n+        )\n+\n+    def test_invalid_policy_map(self):\n+        with self.assertRaisesRegex(\n+            TypeError, \"If specified, `policy_map` must be a dict.\"\n+        ):\n+            DTypePolicyMap(policy_map=123)\n+\n+        with self.assertRaisesRegex(\n+            TypeError, \"If specified, `policy_map` must be a dict.\"\n+        ):\n+            DTypePolicyMap(\n+                policy_map=dtype_policies.FloatDTypePolicy(\"mixed_bfloat16\")\n+            )\n\n@@ -225,6 +225,19 @@ class FloatDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n             repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n         )\n \n+    def test_eq(self):\n+        policy = DTypePolicy(\"mixed_bfloat16\")\n+\n+        # Test True\n+        self.assertEqual(policy, DTypePolicy(\"mixed_bfloat16\"))\n+        self.assertEqual(policy, FloatDTypePolicy(\"mixed_bfloat16\"))\n+\n+        # Test False\n+        self.assertNotEqual(policy, \"mixed_float16\")\n+        self.assertNotEqual(\n+            policy, QuantizedDTypePolicy(\"int8\", \"mixed_bfloat16\")\n+        )\n+\n \n class QuantizedDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n     def setUp(self):\n@@ -494,6 +507,19 @@ class QuantizedDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n             policy.amax_history_length, reloaded_policy.amax_history_length\n         )\n \n+    def test_eq(self):\n+        policy = QuantizedDTypePolicy(\"int8\", \"mixed_bfloat16\")\n+\n+        # Test True\n+        self.assertEqual(policy, QuantizedDTypePolicy(\"int8\", \"mixed_bfloat16\"))\n+\n+        # Test False\n+        self.assertNotEqual(policy, \"mixed_bfloat16\")\n+        self.assertNotEqual(policy, DTypePolicy(\"mixed_bfloat16\"))\n+        self.assertNotEqual(\n+            policy, QuantizedFloat8DTypePolicy(\"float8\", \"mixed_bfloat16\")\n+        )\n+\n     @parameterized.named_parameters(\n         (\"int8_from_mixed_bfloat16\", \"int8_from_mixed_bfloat16\"),\n         (\"float8_from_mixed_bfloat16\", \"float8_from_mixed_bfloat16\"),\n\n@@ -6,6 +6,7 @@ from absl.testing import parameterized\n \n from keras.src import backend\n from keras.src import constraints\n+from keras.src import dtype_policies\n from keras.src import initializers\n from keras.src import layers\n from keras.src import models\n@@ -378,3 +379,26 @@ class MultiHeadAttentionTest(testing.TestCase, parameterized.TestCase):\n         self.assertLen(out, 2)\n         self.assertEqual(symbolic_out[0].shape, out[0].shape)\n         self.assertEqual(symbolic_out[1].shape, out[1].shape)\n+\n+    def test_dtype_policy_map(self):\n+        quantized_policy = dtype_policies.QuantizedDTypePolicy(\n+            \"int8\", \"float32\"\n+        )\n+        policy_map = dtype_policies.DTypePolicyMap()\n+\n+        # Preset the quantized policy\n+        policy_map[\"mha/query\"] = quantized_policy\n+        policy_map[\"mha/key\"] = quantized_policy\n+        policy_map[\"mha/value\"] = quantized_policy\n+        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n+        key = np.array([[[0.0, 1.0], [1.0, 0.0]]])\n+        value = np.array([[[1.0, 2.0], [3.0, 4.0]]])\n+        layer = layers.MultiHeadAttention(\n+            num_heads=3, key_dim=8, use_bias=False, dtype=policy_map, name=\"mha\"\n+        )\n+        layer.build(query.shape, key.shape, value.shape)\n+\n+        # Sublayers should be quantized\n+        self.assertDType(layer._query_dense._kernel, \"int8\")\n+        self.assertDType(layer._key_dense._kernel, \"int8\")\n+        self.assertDType(layer._value_dense._kernel, \"int8\")\n\n@@ -101,13 +101,9 @@ class Dense(Layer):\n \n     def build(self, input_shape):\n         input_dim = input_shape[-1]\n-        # We use `self._dtype_policy` to check to avoid issues in torch dynamo\n-        is_quantized = self._dtype_policy.is_quantized\n-        if is_quantized:\n-            self.quantized_build(\n-                input_shape, mode=self.dtype_policy.quantization_mode\n-            )\n-        if not is_quantized or self.dtype_policy.quantization_mode != \"int8\":\n+        if self.quantization_mode:\n+            self.quantized_build(input_shape, mode=self.quantization_mode)\n+        if self.quantization_mode != \"int8\":\n             # If the layer is quantized to int8, `self._kernel` will be added\n             # in `self._int8_build`. Therefore, we skip it here.\n             self._kernel = self.add_weight(\n@@ -203,11 +199,10 @@ class Dense(Layer):\n         target_variables = [kernel_value]\n         if self.use_bias:\n             target_variables.append(self.bias)\n-        if self.dtype_policy.is_quantized:\n-            mode = self.dtype_policy.quantization_mode\n-            if mode == \"int8\":\n+        if self.quantization_mode is not None:\n+            if self.quantization_mode == \"int8\":\n                 target_variables.append(kernel_scale)\n-            elif mode == \"float8\":\n+            elif self.quantization_mode == \"float8\":\n                 target_variables.append(self.inputs_scale)\n                 target_variables.append(self.inputs_amax_history)\n                 target_variables.append(self.kernel_scale)\n@@ -215,9 +210,7 @@ class Dense(Layer):\n                 target_variables.append(self.outputs_grad_scale)\n                 target_variables.append(self.outputs_grad_amax_history)\n             else:\n-                raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-                )\n+                raise self._quantization_mode_error(self.quantization_mode)\n         for i, variable in enumerate(target_variables):\n             store[str(i)] = variable\n \n@@ -232,11 +225,10 @@ class Dense(Layer):\n         target_variables = [self._kernel]\n         if self.use_bias:\n             target_variables.append(self.bias)\n-        if self.dtype_policy.is_quantized:\n-            mode = self.dtype_policy.quantization_mode\n-            if mode == \"int8\":\n+        if self.quantization_mode is not None:\n+            if self.quantization_mode == \"int8\":\n                 target_variables.append(self.kernel_scale)\n-            elif mode == \"float8\":\n+            elif self.quantization_mode == \"float8\":\n                 target_variables.append(self.inputs_scale)\n                 target_variables.append(self.inputs_amax_history)\n                 target_variables.append(self.kernel_scale)\n@@ -244,9 +236,7 @@ class Dense(Layer):\n                 target_variables.append(self.outputs_grad_scale)\n                 target_variables.append(self.outputs_grad_amax_history)\n             else:\n-                raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-                )\n+                raise self._quantization_mode_error(self.quantization_mode)\n         for i, variable in enumerate(target_variables):\n             variable.assign(store[str(i)])\n         if self.lora_enabled:\n@@ -310,10 +300,11 @@ class Dense(Layer):\n \n     # Quantization-related (int8 and float8) methods\n \n-    QUANTIZATION_MODE_ERROR_TEMPLATE = (\n-        f\"Invalid quantization mode. Expected one of \"\n+    def _quantization_mode_error(self, mode):\n+        return NotImplementedError(\n+            \"Invalid quantization mode. Expected one of \"\n             f\"{dtype_policies.QUANTIZATION_MODES}. \"\n-        \"Received: quantization_mode={mode}\"\n+            f\"Received: quantization_mode={mode}\"\n         )\n \n     def quantized_build(self, input_shape, mode):\n@@ -324,9 +315,7 @@ class Dense(Layer):\n         elif mode == \"float8\":\n             self._float8_build()\n         else:\n-            raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-            )\n+            raise self._quantization_mode_error(mode)\n \n     def _int8_build(\n         self,\n@@ -402,15 +391,12 @@ class Dense(Layer):\n         self._is_quantized = True\n \n     def quantized_call(self, inputs, training=None):\n-        if self.dtype_policy.quantization_mode == \"int8\":\n+        if self.quantization_mode == \"int8\":\n             return self._int8_call(inputs)\n-        elif self.dtype_policy.quantization_mode == \"float8\":\n+        elif self.quantization_mode == \"float8\":\n             return self._float8_call(inputs, training=training)\n         else:\n-            mode = self.dtype_policy.quantization_mode\n-            raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-            )\n+            raise self._quantization_mode_error(self.quantization_mode)\n \n     def _int8_call(self, inputs):\n         @ops.custom_gradient\n@@ -569,17 +555,13 @@ class Dense(Layer):\n         elif mode == \"float8\":\n             self._float8_build()\n         else:\n-            raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-            )\n+            raise self._quantization_mode_error(mode)\n         self._tracker.lock()\n \n         # Set new dtype policy\n         if not self.dtype_policy.is_quantized:\n-            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n-            # We set the internal `self._dtype_policy` instead of using the\n-            # setter to avoid double `quantize` call\n-            self._dtype_policy = dtype_policies.get(quantized_dtype)\n+            policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n+            self.dtype_policy = policy\n \n         # Release memory manually because sometimes the backend doesn't\n         gc.collect()\n\n@@ -156,12 +156,9 @@ class EinsumDense(Layer):\n         # `self._int8_build` needs `self.input_spec`\n         self.input_spec = InputSpec(ndim=len(input_shape))\n         # We use `self._dtype_policy` to check to avoid issues in torch dynamo\n-        is_quantized = self._dtype_policy.is_quantized\n-        if is_quantized:\n-            self.quantized_build(\n-                input_shape, mode=self.dtype_policy.quantization_mode\n-            )\n-        if not is_quantized or self.dtype_policy.quantization_mode != \"int8\":\n+        if self.quantization_mode is not None:\n+            self.quantized_build(input_shape, mode=self.quantization_mode)\n+        if self.quantization_mode != \"int8\":\n             # If the layer is quantized to int8, `self._kernel` will be added\n             # in `self._int8_build`. Therefore, we skip it here.\n             self._kernel = self.add_weight(\n@@ -258,11 +255,10 @@ class EinsumDense(Layer):\n         target_variables = [kernel_value]\n         if self.bias is not None:\n             target_variables.append(self.bias)\n-        if self.dtype_policy.is_quantized:\n-            mode = self.dtype_policy.quantization_mode\n-            if mode == \"int8\":\n+        if self.quantization_mode is not None:\n+            if self.quantization_mode == \"int8\":\n                 target_variables.append(kernel_scale)\n-            elif mode == \"float8\":\n+            elif self.quantization_mode == \"float8\":\n                 target_variables.append(self.inputs_scale)\n                 target_variables.append(self.inputs_amax_history)\n                 target_variables.append(self.kernel_scale)\n@@ -270,9 +266,7 @@ class EinsumDense(Layer):\n                 target_variables.append(self.outputs_grad_scale)\n                 target_variables.append(self.outputs_grad_amax_history)\n             else:\n-                raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-                )\n+                raise self._quantization_mode_error(self.quantization_mode)\n         for i, variable in enumerate(target_variables):\n             store[str(i)] = variable\n \n@@ -287,11 +281,10 @@ class EinsumDense(Layer):\n         target_variables = [self._kernel]\n         if self.bias is not None:\n             target_variables.append(self.bias)\n-        if self.dtype_policy.is_quantized:\n-            mode = self.dtype_policy.quantization_mode\n-            if mode == \"int8\":\n+        if self.quantization_mode is not None:\n+            if self.quantization_mode == \"int8\":\n                 target_variables.append(self.kernel_scale)\n-            elif mode == \"float8\":\n+            elif self.quantization_mode == \"float8\":\n                 target_variables.append(self.inputs_scale)\n                 target_variables.append(self.inputs_amax_history)\n                 target_variables.append(self.kernel_scale)\n@@ -299,9 +292,7 @@ class EinsumDense(Layer):\n                 target_variables.append(self.outputs_grad_scale)\n                 target_variables.append(self.outputs_grad_amax_history)\n             else:\n-                raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-                )\n+                raise self._quantization_mode_error(self.quantization_mode)\n         for i, variable in enumerate(target_variables):\n             variable.assign(store[str(i)])\n         if self.lora_enabled:\n@@ -369,10 +360,11 @@ class EinsumDense(Layer):\n \n     # Quantization-related (int8 and float8) methods\n \n-    QUANTIZATION_MODE_ERROR_TEMPLATE = (\n-        f\"Invalid quantization mode. Expected one of \"\n+    def _quantization_mode_error(self, mode):\n+        return NotImplementedError(\n+            \"Invalid quantization mode. Expected one of \"\n             f\"{dtype_policies.QUANTIZATION_MODES}. \"\n-        \"Received: quantization_mode={mode}\"\n+            f\"Received: quantization_mode={mode}\"\n         )\n \n     def quantized_build(self, input_shape, mode):\n@@ -388,9 +380,7 @@ class EinsumDense(Layer):\n         elif mode == \"float8\":\n             self._float8_build()\n         else:\n-            raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-            )\n+            raise self._quantization_mode_error(mode)\n \n     def _int8_build(\n         self,\n@@ -488,15 +478,12 @@ class EinsumDense(Layer):\n         self._is_quantized = True\n \n     def quantized_call(self, inputs, training=None):\n-        if self.dtype_policy.quantization_mode == \"int8\":\n+        if self.quantization_mode == \"int8\":\n             return self._int8_call(inputs)\n-        elif self.dtype_policy.quantization_mode == \"float8\":\n+        elif self.quantization_mode == \"float8\":\n             return self._float8_call(inputs, training=training)\n         else:\n-            mode = self.dtype_policy.quantization_mode\n-            raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-            )\n+            raise self._quantization_mode_error(self.quantization_mode)\n \n     def _int8_call(self, inputs):\n         @ops.custom_gradient\n@@ -706,17 +693,13 @@ class EinsumDense(Layer):\n         elif mode == \"float8\":\n             self._float8_build()\n         else:\n-            raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-            )\n+            raise self._quantization_mode_error(mode)\n         self._tracker.lock()\n \n         # Set new dtype policy\n         if not self.dtype_policy.is_quantized:\n-            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n-            # We set the internal `self._dtype_policy` instead of using the\n-            # setter to avoid double `quantize` call\n-            self._dtype_policy = dtype_policies.get(quantized_dtype)\n+            policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n+            self.dtype_policy = policy\n \n         # Release memory manually because sometimes the backend doesn't\n         gc.collect()\n\n@@ -111,15 +111,9 @@ class Embedding(Layer):\n     def build(self, input_shape=None):\n         if self.built:\n             return\n-        # We use `self._dtype_policy` to check to avoid issues in torch dynamo\n-        is_quantized = isinstance(\n-            self._dtype_policy, dtype_policies.QuantizedDTypePolicy\n-        )\n-        if is_quantized:\n-            self.quantized_build(\n-                input_shape, mode=self.dtype_policy.quantization_mode\n-            )\n-        if not is_quantized or self.dtype_policy.quantization_mode != \"int8\":\n+        if self.quantization_mode is not None:\n+            self.quantized_build(input_shape, mode=self.quantization_mode)\n+        if self.quantization_mode != \"int8\":\n             self._embeddings = self.add_weight(\n                 shape=(self.input_dim, self.output_dim),\n                 initializer=self.embeddings_initializer,\n@@ -200,14 +194,11 @@ class Embedding(Layer):\n             self._get_embeddings_with_merged_lora()\n         )\n         target_variables = [embeddings_value]\n-        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n-            mode = self.dtype_policy.quantization_mode\n-            if mode == \"int8\":\n+        if self.quantization_mode is not None:\n+            if self.quantization_mode == \"int8\":\n                 target_variables.append(embeddings_scale)\n             else:\n-                raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-                )\n+                raise self._quantization_mode_error(self.quantization_mode)\n         for i, variable in enumerate(target_variables):\n             store[str(i)] = variable\n \n@@ -220,14 +211,11 @@ class Embedding(Layer):\n         # The keys of the `store` will be saved as determined because the\n         # default ordering will change after quantization\n         target_variables = [self._embeddings]\n-        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n-            mode = self.dtype_policy.quantization_mode\n-            if mode == \"int8\":\n+        if self.quantization_mode is not None:\n+            if self.quantization_mode == \"int8\":\n                 target_variables.append(self.embeddings_scale)\n             else:\n-                raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-                )\n+                raise self._quantization_mode_error(self.quantization_mode)\n         for i, variable in enumerate(target_variables):\n             variable.assign(store[str(i)])\n         if self.lora_enabled:\n@@ -297,18 +285,17 @@ class Embedding(Layer):\n \n     \"\"\"Quantization-related (int8) methods\"\"\"\n \n-    QUANTIZATION_MODE_ERROR_TEMPLATE = (\n+    def _quantization_mode_error(self, mode):\n+        return NotImplementedError(\n             \"Invalid quantization mode. Expected 'int8'. \"\n-        \"Received: quantization_mode={mode}\"\n+            f\"Received: quantization_mode={mode}\"\n         )\n \n     def quantized_build(self, input_shape, mode):\n         if mode == \"int8\":\n             self._int8_build()\n         else:\n-            raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-            )\n+            raise self._quantization_mode_error(mode)\n \n     def _int8_build(\n         self,\n@@ -334,13 +321,10 @@ class Embedding(Layer):\n         self._is_quantized = True\n \n     def quantized_call(self, inputs):\n-        if self.dtype_policy.quantization_mode == \"int8\":\n+        if self.quantization_mode == \"int8\":\n             return self._int8_call(inputs)\n         else:\n-            mode = self.dtype_policy.quantization_mode\n-            raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-            )\n+            raise self._quantization_mode_error(self.quantization_mode)\n \n     def _int8_call(self, inputs):\n         # We cannot update quantized self._embeddings, so the custom gradient is\n@@ -388,25 +372,19 @@ class Embedding(Layer):\n                 lambda shape, dtype: embeddings_scale,\n             )\n         else:\n-            raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n-            )\n+            raise self._quantization_mode_error(mode)\n         self._tracker.lock()\n \n         # Set new dtype policy\n-        if not isinstance(\n-            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n-        ):\n-            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n-            # We set the internal `self._dtype_policy` instead of using the\n-            # setter to avoid double `quantize` call\n-            self._dtype_policy = dtype_policies.get(quantized_dtype)\n+        if not self.dtype_policy.is_quantized:\n+            policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n+            self.dtype_policy = policy\n \n         # Release memory manually because sometimes the backend doesn't\n         gc.collect()\n \n     def _get_embeddings_with_merged_lora(self):\n-        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+        if self.dtype_policy.is_quantized:\n             embeddings_value = self._embeddings\n             embeddings_scale = self.embeddings_scale\n             if self.lora_enabled:\n\n@@ -33,6 +33,7 @@ from keras.src.backend import KerasTensor\n from keras.src.backend.common import global_state\n from keras.src.backend.common.name_scope import current_path\n from keras.src.distribution import distribution_lib\n+from keras.src.dtype_policies import DTypePolicyMap\n from keras.src.layers import input_spec\n from keras.src.metrics.metric import Metric\n from keras.src.ops.operation import Operation\n@@ -220,6 +221,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         @wraps(original_build_method)\n         def build_wrapper(*args, **kwargs):\n             with obj._open_name_scope():\n+                obj._path = current_path()\n                 original_build_method(*args, **kwargs)\n             # Record build config.\n             signature = inspect.signature(original_build_method)\n@@ -266,6 +268,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n                 f\"passed to {self.__class__.__name__}: {kwargs}\"\n             )\n \n+        self._path = None  # Will be determined in `build_wrapper`\n         self.built = False\n         self.autocast = autocast\n         self._input_spec = None\n@@ -346,6 +349,14 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n             # Reset attribute tracking (TF-specific)\n             self._self_setattr_tracking = _self_setattr_tracking\n \n+    @property\n+    def path(self):\n+        \"\"\"The path of the layer.\n+\n+        If the layer has not been built yet, it will be `None`.\n+        \"\"\"\n+        return self._path\n+\n     @property\n     def input_spec(self):\n         return self._input_spec\n@@ -683,10 +694,16 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n \n     @dtype_policy.setter\n     def dtype_policy(self, value):\n-        self._dtype_policy = dtype_policies.get(value)\n-        if isinstance(self._dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+        policy = dtype_policies.get(value)\n+        if isinstance(self._dtype_policy, DTypePolicyMap) and self.path:\n+            if self.path in self._dtype_policy:\n+                del self._dtype_policy[self.path]\n+            self._dtype_policy[self.path] = policy\n+        else:\n+            self._dtype_policy = policy\n+        if policy.is_quantized and not getattr(self, \"_is_quantized\", False):\n             if self.built:\n-                self.quantize(self._dtype_policy.quantization_mode)\n+                self.quantize(policy.quantization_mode)\n \n     @property\n     def dtype(self):\n@@ -696,17 +713,34 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n     @property\n     def compute_dtype(self):\n         \"\"\"The dtype of the computations performed by the layer.\"\"\"\n-        return self.dtype_policy.compute_dtype\n+        if isinstance(self._dtype_policy, DTypePolicyMap) and self.path:\n+            policy = self._dtype_policy[self.path]\n+        else:\n+            policy = self._dtype_policy\n+        return policy.compute_dtype\n \n     @property\n     def variable_dtype(self):\n         \"\"\"The dtype of the state (weights) of the layer.\"\"\"\n-        return self.dtype_policy.variable_dtype\n+        if isinstance(self._dtype_policy, DTypePolicyMap) and self.path:\n+            policy = self._dtype_policy[self.path]\n+        else:\n+            policy = self._dtype_policy\n+        return policy.variable_dtype\n+\n+    @property\n+    def quantization_mode(self):\n+        \"\"\"The quantization_mode, or None if the layer is not quantized.\"\"\"\n+        if isinstance(self._dtype_policy, DTypePolicyMap) and self.path:\n+            policy = self._dtype_policy[self.path]\n+        else:\n+            policy = self._dtype_policy\n+        return policy.quantization_mode if policy.is_quantized else None\n \n     @property\n     def input_dtype(self):\n         \"\"\"The dtype layer inputs should be converted to.\"\"\"\n-        return self.dtype_policy.compute_dtype\n+        return self.compute_dtype\n \n     @property\n     def supports_masking(self):\n@@ -991,9 +1025,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         with backend.StatelessScope(\n             state_mapping=mapping, collect_losses=return_losses\n         ) as scope:\n-            if isinstance(\n-                self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n-            ):\n+            if self.dtype_policy.is_quantized:\n                 outputs = self.quantized_call(*args, **kwargs)\n             else:\n                 outputs = self.call(*args, **kwargs)\n\n@@ -976,6 +976,18 @@ class LayerTest(testing.TestCase):\n         self.assertEqual(layer.dtype_policy.name, \"mixed_float16\")\n         self.assertEqual(layer.dtype_policy.compute_dtype, \"float16\")\n         self.assertEqual(layer.dtype_policy.variable_dtype, \"float32\")\n+        # Set with DTypePolicyMap\n+        dtype_policy_map = dtype_policies.DTypePolicyMap()\n+        layer = layers.Dense(2, dtype=dtype_policy_map)\n+        layer.build([None, 1])\n+        layer.dtype_policy = \"mixed_bfloat16\"\n+        self.assertIsInstance(\n+            layer._dtype_policy, dtype_policies.DTypePolicyMap\n+        )\n+        self.assertEqual(\n+            layer._dtype_policy[layer.path],\n+            dtype_policies.FloatDTypePolicy(\"mixed_bfloat16\"),\n+        )\n \n     def test_pickle_layer(self):\n         layer = layers.Dense(2)\n\n@@ -35,9 +35,7 @@ class Operation:\n             if any_symbolic_tensors(args, kwargs):\n                 call_fn = self.symbolic_call\n             else:\n-                if isinstance(\n-                    self._dtype_policy, dtype_policies.QuantizedDTypePolicy\n-                ):\n+                if getattr(self, \"quantization_mode\", None) is not None:\n                     call_fn = self.quantized_call\n                 else:\n                     call_fn = self.call\n@@ -50,7 +48,7 @@ class Operation:\n         # Plain flow.\n         if any_symbolic_tensors(args, kwargs):\n             return self.symbolic_call(*args, **kwargs)\n-        if isinstance(self._dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+        if getattr(self, \"quantization_mode\", None) is not None:\n             return self.quantized_call(*args, **kwargs)\n         else:\n             return self.call(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
