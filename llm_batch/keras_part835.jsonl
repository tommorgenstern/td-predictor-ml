{"custom_id": "keras#25391fef9b44c807030f79ee008e11b6520739cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 48 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -166,6 +166,12 @@ class BatchNormalization(Layer):\n         self.gamma_constraint = constraints.get(gamma_constraint)\n         self.supports_masking = True\n \n+        self.gamma = None\n+        self.beta = None\n+        self.moving_mean = None\n+        self.moving_variance = None\n+        self._reduction_axes = None\n+\n     def build(self, input_shape):\n         shape = (input_shape[self.axis],)\n         if self.scale:\n@@ -202,9 +208,11 @@ class BatchNormalization(Layer):\n             trainable=False,\n             autocast=False,\n         )\n+\n         self.input_spec = InputSpec(\n             ndim=len(input_shape), axes={self.axis: input_shape[self.axis]}\n         )\n+\n         reduction_axes = list(range(len(input_shape)))\n         del reduction_axes[self.axis]\n         self._reduction_axes = reduction_axes\n@@ -230,10 +238,12 @@ class BatchNormalization(Layer):\n             # out BN for mixed precision.\n             inputs = ops.cast(inputs, \"float32\")\n \n-        if training and self.trainable:\n-            mean, variance = self._moments(inputs, mask)\n         moving_mean = ops.cast(self.moving_mean, inputs.dtype)\n         moving_variance = ops.cast(self.moving_variance, inputs.dtype)\n+\n+        if training and self.trainable:\n+            mean, variance = self._moments(inputs, mask)\n+\n             self.moving_mean.assign(\n                 moving_mean * self.momentum + mean * (1.0 - self.momentum)\n             )\n@@ -242,8 +252,6 @@ class BatchNormalization(Layer):\n                 + variance * (1.0 - self.momentum)\n             )\n         else:\n-            moving_mean = ops.cast(self.moving_mean, inputs.dtype)\n-            moving_variance = ops.cast(self.moving_variance, inputs.dtype)\n             mean = moving_mean\n             variance = moving_variance\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58a6e42990451f13d77036c9fa7770416fc462f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 28 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -7,10 +7,10 @@ class Conv2D(BaseConv):\n     \"\"\"2D convolution layer.\n \n     This layer creates a convolution kernel that is convolved with the layer\n-    input over a single spatial (or temporal) dimension to produce a tensor of\n-    outputs. If `use_bias` is True, a bias vector is created and added to the\n-    outputs. Finally, if `activation` is not `None`, it is applied to the\n-    outputs as well.\n+    input over a 2D spatial (or temporal) dimension (height and width) to \n+    produce a tensor of outputs. If `use_bias` is True, a bias vector is created \n+    and added to the outputs. Finally, if `activation` is not `None`, it is \n+    applied to the outputs as well.\n \n     Args:\n         filters: int, the dimension of the output space (the number of filters\n\n@@ -7,10 +7,10 @@ class Conv3D(BaseConv):\n     \"\"\"3D convolution layer.\n \n     This layer creates a convolution kernel that is convolved with the layer\n-    input over a single spatial (or temporal) dimension to produce a tensor of\n-    outputs. If `use_bias` is True, a bias vector is created and added to the\n-    outputs. Finally, if `activation` is not `None`, it is applied to the\n-    outputs as well.\n+    input over a 3D spatial (or temporal) dimension (width,height and depth) to \n+    produce a tensor of outputs. If `use_bias` is True, a bias vector is created \n+    and added to the outputs. Finally, if `activation` is not `None`, it is \n+    applied to the outputs as well.\n \n     Args:\n         filters: int, the dimension of the output space (the number of filters\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#913194e4252ecb2cabf60ea9d983b427906afd8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 27 | Churn Cumulative: 551 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -159,11 +159,10 @@ class KerasVariable:\n                 )\n         else:\n             if callable(initializer):\n-                shape = self._validate_shape(shape)\n-                value = initializer(shape, dtype=dtype)\n+                self._shape = self._validate_shape(shape)\n+                self._initialize_with_initializer(initializer)\n             else:\n-                value = initializer\n-            self._initialize(value)\n+                self._initialize(initializer)\n                 self._shape = tuple(self._value.shape)\n         self._ndim = len(self._shape)\n \n@@ -178,8 +177,8 @@ class KerasVariable:\n                 \"Make sure that all variables are initialized \"\n                 \"before you start using your layer/model objects.\"\n             )\n-        value = self._initializer(self._shape, dtype=self._dtype)\n-        self._initialize(value)\n+        self._initialize_with_initializer(self._initializer)\n+        self._initializer = None\n \n     def _validate_shape(self, shape):\n         shape = standardize_shape(shape)\n@@ -335,6 +334,10 @@ class KerasVariable:\n     def _initialize(self, value):\n         raise NotImplementedError\n \n+    def _initialize_with_initializer(self, initializer):\n+        value = initializer(self._shape, dtype=self._dtype)\n+        self._initialize(value)\n+\n     def _convert_to_tensor(self, value, dtype=None):\n         raise NotImplementedError\n \n\n@@ -37,6 +37,14 @@ class Variable(\n             value, dtype=self._dtype, trainable=self.trainable, name=self.name\n         )\n \n+    def _initialize_with_initializer(self, initializer):\n+        self._value = tf.Variable(\n+            lambda: initializer(self._shape, dtype=self._dtype),\n+            dtype=self._dtype,\n+            trainable=self.trainable,\n+            name=self.name,\n+        )\n+\n     def _deferred_initialize(self):\n         if self._value is not None:\n             raise ValueError(f\"Variable {self.path} is already initialized.\")\n@@ -49,8 +57,8 @@ class Variable(\n                 \"before you start using your layer/model objects.\"\n             )\n         with tf.init_scope():\n-            value = self._initializer(self._shape, dtype=self._dtype)\n-            self._initialize(value)\n+            self._initialize_with_initializer(self._initializer)\n+            self._initializer = None\n \n     def _direct_assign(self, value):\n         self._value.assign(tf.cast(value, self._value.dtype))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#70b53a563da3fbb68514500fe11086488bbf309e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 119 | Lines Deleted: 105 | Files Changed: 3 | Hunks: 17 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 224 | Churn Cumulative: 342 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 11 | DMM Complexity: 0.6896551724137931\n\nDIFF:\n@@ -106,7 +106,7 @@ def distribute_tensor(tensor, layout):\n         return global_value\n \n \n-def distribute_data_input(inputs, layout):\n+def distribute_data_input(per_process_batch, layout):\n     \"\"\"Distribute the input data with the corresponding layout.\n \n     Note that the inputs here is a local worker batch. Within the local worker,\n@@ -118,72 +118,64 @@ def distribute_data_input(inputs, layout):\n             `jax.sharding.Sharding` instance.\n \n     Returns:\n-        Distributed inputs thats been properly put to local devices.\n+        A global batch distributed according to `layout`.\n     \"\"\"\n     if not isinstance(layout, jax.sharding.Sharding):\n         layout = _to_jax_layout(layout)\n-    if layout.is_fully_addressable:\n-        return jax.device_put(inputs, layout)\n \n-    # We need the jax mesh information to determine how to place the data\n-    # on to each of the worker.\n-    jax_mesh = layout.mesh\n-    mesh_rank = len(jax_mesh.shape)\n-    per_process_batch_size = inputs.shape[0]\n-    if mesh_rank == 1:\n-        # This is data parallel mesh only. We will split the full data\n-        # across the batch dim.\n-        num_split = jax.local_device_count()\n-        per_replica_batch_size = per_process_batch_size // num_split\n+    mesh_shape = list(layout.mesh.shape.values())\n+    mesh_batch_dim_size = mesh_shape[0]\n+    num_model_replicas_total = mesh_batch_dim_size\n+    mesh_model_dim_size = mesh_shape[1] if len(mesh_shape) > 1 else 1\n+    num_model_replicas_per_process = num_model_replicas_total / num_processes()\n+    per_process_batch_size = per_process_batch.shape[0]\n+\n+    if num_model_replicas_per_process >= 1:\n+        # If there are more than one model replicas per process, we need to\n+        # further shard the data to each of the model replicas.\n+        per_replica_batch_size = int(\n+            per_process_batch_size // num_model_replicas_per_process\n+        )\n         if per_process_batch_size % per_replica_batch_size != 0:\n             raise ValueError(\n-                f\"The local batch size {per_process_batch_size} is not\"\n-                \"divisible by the number of local replicas \"\n-                f\"{num_split}\"\n+                \"`per_process_batch_size` should be divisible by `\"\n+                \"per_replica_batch_size`. \"\n+                f\"per_process_batch_size={per_process_batch_size} and \"\n+                f\"per_replica_batch_size = {per_replica_batch_size}\"\n             )\n-        global_batch_size = per_process_batch_size * jax.process_count()\n-        per_replica_batches = np.split(inputs, num_split, axis=0)\n-    elif mesh_rank == 2:\n-        # Data+Model parallel\n-        # In this case, we need to check if the mesh batch dim shape is large\n-        # than number of local devices, so that we can decide whether a split\n-        # is needed for the data, or a repeat/copy of the data is needed for\n-        # each of the device.\n-        # TODO(scottzhu): The mesh batch dim name is not available here, since\n-        # we only have jax Mesh. We assume the first dim is for batch, and\n-        # second dim is for model for now.\n-        mesh_batch_dim_size = list(jax_mesh.shape.values())[0]\n-        local_device_count = jax.local_device_count()\n-        if mesh_batch_dim_size < local_device_count:\n-            # No split needed, we only need to repeat here.\n-            global_batch_size = per_process_batch_size\n-            per_replica_batches = [inputs for _ in range(local_device_count)]\n-        else:\n-            # Note that global batch size is not simply per_process_batch_size *\n-            # num_process. It actually depends on the model dim size.\n-            global_batch_size = per_process_batch_size * (\n-                mesh_batch_dim_size // local_device_count\n+        per_replica_batches = np.split(\n+            per_process_batch, num_model_replicas_per_process\n         )\n-            per_replica_batches = jax.numpy.split(\n-                inputs, local_device_count, axis=0\n-            )\n-    else:\n-        raise ValueError(\n-            \"Only 1D or 2D mesh is supported at the moment. \"\n-            f\"Received mesh shape = {jax_mesh.shape}\"\n-        )\n-\n-    global_shape = (global_batch_size,) + inputs.shape[1:]\n-    global_batch_array = jax.make_array_from_single_device_arrays(\n-        global_shape,\n-        layout,\n-        arrays=[\n+        # Replicate data along the model_dim.\n+        per_device_batches = [\n+            per_replica_batch\n+            for per_replica_batch in per_replica_batches\n+            for _ in range(mesh_model_dim_size)\n+        ]\n+        batches_on_devices = [\n             jax.device_put(batch, device)\n             for batch, device in zip(\n-                per_replica_batches, layout.addressable_devices\n+                per_device_batches, layout.addressable_devices\n             )\n-        ],\n+        ]\n+    else:\n+        # If there are less than one model replicas per process, we need to\n+        # replicate the data to each of the model replicas. No further data\n+        # sharding is needed.\n+        per_replica_batch_size = per_process_batch_size\n+        batches_on_devices = [\n+            jax.device_put(per_process_batch, device)\n+            for device in layout.addressable_devices\n+        ]\n+\n+    global_batch_size = per_replica_batch_size * num_model_replicas_total\n+    global_batch_shape = (global_batch_size,) + per_process_batch.shape[1:]\n+    global_batch_array = jax.make_array_from_single_device_arrays(\n+        shape=global_batch_shape,\n+        sharding=layout,\n+        arrays=batches_on_devices,\n     )\n+\n     return global_batch_array\n \n \n\n@@ -323,6 +323,37 @@ class JaxDistributionLibTest(testing.TestCase):\n             )\n         )\n \n+    def test_distribute_data_input(self):\n+        per_process_batch = jax.numpy.arange(24).reshape(\n+            6, 4\n+        )  # Example input array\n+        devices = jax.devices()[:4]  # Simulate 4 devices\n+        batch_dim_size, model_dim_size = 2, 2\n+        mesh = jax.sharding.Mesh(\n+            np.array(devices).reshape(batch_dim_size, model_dim_size),\n+            axis_names=[\"batch\", \"model\"],\n+        )\n+        layout = jax.sharding.NamedSharding(\n+            mesh, jax.sharding.PartitionSpec(\"batch\", None)\n+        )\n+\n+        result = backend_dlib.distribute_data_input(per_process_batch, layout)\n+\n+        # Check the shape of the global batch array\n+        self.assertEqual(\n+            result.shape, (6, 4)\n+        )  # (per_replica_batch_size * num_model_replicas_total, 4)\n+\n+        # Check the sharding of the global batch array\n+        self.assertEqual(len(result.addressable_shards), len(devices))\n+        # Since batch_dim_size=2, there are 2 model replicas so there is one\n+        # replication of data for model replica #1 and another replication of\n+        # data for model replica #2. Within each model replica, the data is\n+        # sharded to two shards. Therefore, each shard has 1/2 of\n+        # per_process_batch.\n+        for shard in result.addressable_shards:\n+            self.assertEqual(shard.data.shape, (3, 4))\n+\n \n class ShardingCaptureLayer(layers.Layer):\n     def __init__(self, **kwargs):\n\n@@ -628,63 +628,54 @@ class ModelParallel(Distribution):\n                 \"the input dataset, e.g via `dataset.batch(batch_size)`\"\n             )\n \n-        # We need to compute the the per-worker batch size.\n-        # Imagine we have 4 workers, and each worker has 2 devices. The device\n-        # ID will be w{worker_id}_{device_id} from global perspective, eg\n-        # 'w0_0', 'w0_1', 'w1_0', etc.\n-        # For a mesh with global shape {'batch': 4, 'model': 2}, we would like\n-        # the mesh to be like below:\n-        #         batch ->\n-        #         --------------------------\n-        # model   | w0_0, w0_1, w1_0, w1_1 |\n-        #   v     | w0_2, w0_3, w1_2, w1_3 |\n-        #         --------------------------\n-        # In this case, the batch dim will be split across 2 workers.\n-        #\n-        # In the case that global batch dim is smaller than the worker size, eg\n-        # {'batch': 1, 'model': 8}, then the mesh will be like below:\n-        #         batch ->\n-        #         --------\n-        # model   | w0_0 |\n-        #         | w0_1 |\n-        #         | w1_0 |\n-        #         | w1_1 |\n-        #         | w2_0 |\n-        #         | w2_1 |\n-        #         | w3_0 |\n-        #         | w3_1 |\n-        #         --------\n-        # And the data batch will be fully replicated for each of the worker.\n+        # We need to compute the per-process/worker/host batch size.\n+        # This will depend on how many model replicas we have on each process.\n+        # Note that this might be smaller than one if model replicas are sharded\n+        # across multiple processes.\n         mesh_batch_dim_index = self.device_mesh.axis_names.index(\n             self._batch_dim_name\n         )\n-        mesh_batch_dim_size = self.device_mesh.shape[mesh_batch_dim_index]\n-        # TODO(scottzhu): local device count can be a field of the mesh.\n-        local_device_count = (\n-            np.prod(self.device_mesh.shape) // self._num_process\n-        )\n-        if mesh_batch_dim_size < local_device_count:\n-            # No sharding is needed in this case. The worker will have the\n+        num_model_replicas = self.device_mesh.shape[mesh_batch_dim_index]\n+        if num_model_replicas == 1:\n+            # No sharding is needed in this case. Each process will have the\n             # global batch size, and data from the iterator will need to be\n-            # replicated for model dimension.\n+            # replicated across all processes.\n             return dataset.prefetch(tf.data.AUTOTUNE)\n-        else:\n-            if mesh_batch_dim_size % local_device_count != 0:\n+        num_model_replicas_per_process = num_model_replicas / self._num_process\n+        if num_model_replicas_per_process >= 1:\n+            # Each process will have one or more full model replicas. Data will\n+            # be sharded across all processes without replication.\n+            if global_batch_size % self._num_process != 0:\n                 raise ValueError(\n-                    \"The Batch dimension of the mesh is not compatible \"\n-                    \"with the local worker device count. Mesh batch \"\n-                    f\"dim = {mesh_batch_dim_size} and local device \"\n-                    f\"count = {local_device_count}\"\n+                    \"Global batch size must be divisible by the number of \"\n+                    f\"processes. `global_batch_size`={global_batch_size} and \"\n+                    f\"`num_process`={self._num_process}\"\n                 )\n-            num_shards = mesh_batch_dim_size // local_device_count\n-            per_worker_batch_size = global_batch_size // num_shards\n-\n-            distributed_dataset = dataset.rebatch(per_worker_batch_size)\n-            distributed_dataset = tf_data_distribute._AutoShardDataset(\n-                distributed_dataset,\n-                num_workers=num_shards,\n-                index=self._process_id % num_shards,\n-                num_replicas=num_shards,\n+            per_process_batch_size = global_batch_size // self._num_process\n+            distributed_dataset = dataset.rebatch(per_process_batch_size)\n+            distributed_dataset = distributed_dataset.shard(\n+                num_shards=self._num_process,\n+                index=self._process_id,\n+            )\n+            return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n+        else:\n+            # Model replicas are sharded across multiple processes. Data will be\n+            # sharded across model replicas, and replicated across processes\n+            # within the same model replica.\n+            if global_batch_size % num_model_replicas != 0:\n+                raise ValueError(\n+                    \"Global batch size must be divisible by the number of \"\n+                    f\"replicas. `global_batch_size`={global_batch_size} and \"\n+                    f\"`num_model_replicas`={num_model_replicas}\"\n+                )\n+            per_process_batch_size = global_batch_size // num_model_replicas\n+            distributed_dataset = dataset.rebatch(per_process_batch_size)\n+            processes_per_replica = self._num_process // num_model_replicas\n+            # TODO: Figure out what the convention is for data sharding id.\n+            data_shard_id = self._process_id % processes_per_replica\n+            distributed_dataset = distributed_dataset.shard(\n+                num_shards=num_model_replicas,\n+                index=data_shard_id,\n             )\n             return distributed_dataset.prefetch(tf.data.AUTOTUNE)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1a00e64621d6d4c1e6d1c5c8e7fb7b36f12fb57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 29 | Churn Cumulative: 72 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,10 +84,12 @@ def abs_max_quantize(\n             scale, dtype=original_dtype\n         )\n \n+    inputs = ops.convert_to_tensor(inputs)\n     scale = ops.divide(\n         value_range[1],\n         ops.add(ops.max(ops.abs(inputs), axis=axis, keepdims=True), epsilon),\n     )\n+    scale = ops.cast(scale, backend.standardize_dtype(inputs.dtype))\n     outputs = ops.multiply(inputs, scale)\n     outputs = ops.clip(ops.round(outputs), value_range[0], value_range[1])\n     outputs = ops.cast(outputs, dtype)\n\n@@ -16,11 +16,13 @@ class QuantizersTest(testing.TestCase):\n             quantizers.get(\"typo\")\n \n     def test_abs_max_quantizer(self):\n-        values = random.uniform([3, 4, 5], minval=-1, maxval=1)\n+        values = random.uniform([3, 4, 5], minval=-1, maxval=1, dtype=\"float32\")\n         quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n \n         # Test quantizing\n         quantized_values, scale = quantizer(values)\n+        self.assertDType(quantized_values, \"int8\")\n+        self.assertDType(scale, \"float32\")\n         self.assertEqual(tuple(quantized_values.shape), (3, 4, 5))\n         self.assertEqual(tuple(scale.shape), (3, 4, 1))\n         self.assertLessEqual(ops.max(quantized_values), 127)\n@@ -36,6 +38,29 @@ class QuantizersTest(testing.TestCase):\n         # Test serialization\n         self.run_class_serialization_test(quantizer)\n \n+        # Test bfloat16 & float16 dtype\n+        values = random.uniform(\n+            [3, 4, 5], minval=-1, maxval=1, dtype=\"bfloat16\"\n+        )\n+        quantized_values, scale = quantizer(values)\n+        self.assertDType(quantized_values, \"int8\")\n+        self.assertDType(scale, \"bfloat16\")\n+        values = random.uniform([3, 4, 5], minval=-1, maxval=1, dtype=\"float16\")\n+        quantized_values, scale = quantizer(values)\n+        self.assertDType(quantized_values, \"int8\")\n+        self.assertDType(scale, \"float16\")\n+\n+    def test_abs_max_quantizer_to_numpy(self):\n+        values = random.uniform([3, 4, 5], minval=-1, maxval=1, dtype=\"float32\")\n+        quantized_values, scale = quantizers.abs_max_quantize(\n+            values, axis=-1, to_numpy=True\n+        )\n+        ref_quantized_values, ref_scale = quantizers.abs_max_quantize(\n+            values, axis=-1\n+        )\n+        self.assertAllClose(quantized_values, ref_quantized_values)\n+        self.assertAllClose(scale, ref_scale)\n+\n     def test_compute_float8_scale(self):\n         amax = 3.0\n         scale = 4.0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8b331412a89cbeaf07cc94fa5b55b9e5b6bf626", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 35 | Churn Cumulative: 180 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,7 +46,7 @@ class InputLayer(Layer):\n         if shape is not None:\n             shape = backend.standardize_shape(shape)\n             batch_shape = (batch_size,) + shape\n-        self.batch_shape = tuple(batch_shape)\n+        self._batch_shape = backend.standardize_shape(batch_shape)\n         self._dtype = backend.standardize_dtype(dtype)\n \n         self.sparse = bool(sparse)\n@@ -75,6 +75,10 @@ class InputLayer(Layer):\n     def call(self):\n         return\n \n+    @property\n+    def batch_shape(self):\n+        return self._batch_shape\n+\n     @property\n     def dtype(self):\n         return self._dtype\n@@ -124,6 +128,8 @@ def Input(\n             be passed into the input - they will be densified with a default\n             value of 0. This feature is only supported with the TensorFlow\n             backend. Defaults to `False`.\n+        batch_shape: Optional shape tuple (tuple of integers or `None` objects),\n+            including the batch size.\n         name: Optional name string for the layer.\n             Should be unique in a model (do not reuse the same name twice).\n             It will be autogenerated if it isn't provided.\n\n@@ -5,6 +5,7 @@ import typing\n from keras.src import tree\n from keras.src.api_export import keras_export\n from keras.src.backend.common import global_state\n+from keras.src.backend.common import standardize_shape\n from keras.src.layers.core.input_layer import InputLayer\n from keras.src.layers.layer import Layer\n from keras.src.legacy.saving import saving_utils\n@@ -152,13 +153,9 @@ class Sequential(Model):\n         return \"Sequential\"\n \n     def build(self, input_shape=None):\n-        if not isinstance(input_shape, (tuple, list)):\n-            # Do not attempt to build if the model does not have a single\n-            # input tensor.\n-            return\n-        if input_shape and not (\n-            isinstance(input_shape[0], int) or input_shape[0] is None\n-        ):\n+        try:\n+            input_shape = standardize_shape(input_shape)\n+        except:\n             # Do not attempt to build if the model does not have a single\n             # input tensor.\n             return\n\n@@ -5,6 +5,7 @@ import pytest\n \n from keras.src import backend\n from keras.src import layers\n+from keras.src import saving\n from keras.src import testing\n from keras.src.layers.core.input_layer import Input\n from keras.src.models.functional import Functional\n@@ -304,6 +305,21 @@ class SequentialTest(testing.TestCase):\n         )\n         self.assertLen(revived.layers, 1)\n \n+    def test_serialization_with_lambda_layer(self):\n+        # https://github.com/keras-team/keras/issues/20074\n+        inputs = np.random.random(size=(1, 10, 4)).astype(\"float32\")\n+        CONV_WIDTH = 3\n+        model = Sequential([layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :])])\n+        outputs = model(inputs)\n+\n+        temp = self.get_temp_dir()\n+        save_path = f\"{temp}/model.keras\"\n+        model.save(save_path)\n+        revived = saving.load_model(save_path, safe_mode=False)\n+        revived_outputs = revived(inputs)\n+        self.assertLen(revived.layers, 1)\n+        self.assertAllClose(revived_outputs, outputs)\n+\n     def test_functional_properties(self):\n         model = Sequential(name=\"seq\")\n         inputs = Input(shape=(2,))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16dd8b861f4a88da9ec780c2c4b829938d1f78c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 43 | Churn Cumulative: 55 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n from keras.src import backend\n from keras.src.api_export import keras_export\n from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.src.saving import serialization_lib\n \n \n @keras_export(\"keras.layers.Rescaling\")\n@@ -55,9 +56,23 @@ class Rescaling(TFDataLayer):\n         return input_shape\n \n     def get_config(self):\n-        base_config = super().get_config()\n-        config = {\n-            \"scale\": self.scale,\n-            \"offset\": self.offset,\n+        config = super().get_config()\n+        config.update(\n+            {\n+                # `scale` and `offset` might be numpy array.\n+                \"scale\": serialization_lib.serialize_keras_object(self.scale),\n+                \"offset\": serialization_lib.serialize_keras_object(self.offset),\n             }\n-        return {**base_config, **config}\n+        )\n+        return config\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        config = config.copy()\n+        config[\"scale\"] = serialization_lib.deserialize_keras_object(\n+            config[\"scale\"], custom_objects=custom_objects\n+        )\n+        config[\"offset\"] = serialization_lib.deserialize_keras_object(\n+            config[\"offset\"], custom_objects=custom_objects\n+        )\n+        return cls(**config)\n\n@@ -84,3 +84,21 @@ class RescalingTest(testing.TestCase):\n         x = np.random.random((2, 3, 10, 10)) * 255\n         layer(x)\n         backend.set_image_data_format(config)\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_numpy_args(self):\n+        # https://github.com/keras-team/keras/issues/20072\n+        self.run_layer_test(\n+            layers.Rescaling,\n+            init_kwargs={\n+                \"scale\": np.array(1.0 / 255.0),\n+                \"offset\": np.array(0.5),\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0869c750587a09d021365780e541d2d59e87a22a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 135 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -124,8 +124,7 @@ def distribute_data_input(per_process_batch, layout):\n         layout = _to_jax_layout(layout)\n \n     mesh_shape = list(layout.mesh.shape.values())\n-    mesh_batch_dim_size = mesh_shape[0]\n-    num_model_replicas_total = mesh_batch_dim_size\n+    num_model_replicas_total = mesh_shape[0]  # batch dimension of the mesh\n     mesh_model_dim_size = mesh_shape[1] if len(mesh_shape) > 1 else 1\n     num_model_replicas_per_process = num_model_replicas_total / num_processes()\n     per_process_batch_size = per_process_batch.shape[0]\n@@ -133,6 +132,15 @@ def distribute_data_input(per_process_batch, layout):\n     if num_model_replicas_per_process >= 1:\n         # If there are more than one model replicas per process, we need to\n         # further shard the data to each of the model replicas.\n+        if num_model_replicas_total % num_processes() != 0:\n+            raise ValueError(\n+                \"If there is more than one replicas per process, batch \"\n+                \"dimension of the mesh should be divisible \"\n+                \"by the number of processes. \"\n+                f\"Batch dimension size is {num_model_replicas_total} and \"\n+                f\"number of processes is {num_processes()}.\"\n+            )\n+\n         per_replica_batch_size = int(\n             per_process_batch_size // num_model_replicas_per_process\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6026fda252bfe377c9abe73e2bd073e88368013d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 145 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -130,15 +130,15 @@ def distribute_data_input(per_process_batch, layout):\n     per_process_batch_size = per_process_batch.shape[0]\n \n     if num_model_replicas_per_process >= 1:\n-        # If there are more than one model replicas per process, we need to\n+        # If there is more than one model replica per process, we need to\n         # further shard the data to each of the model replicas.\n         if num_model_replicas_total % num_processes() != 0:\n             raise ValueError(\n-                \"If there is more than one replicas per process, batch \"\n+                \"If there is more than one replica per process, the batch \"\n                 \"dimension of the mesh should be divisible \"\n-                \"by the number of processes. \"\n-                f\"Batch dimension size is {num_model_replicas_total} and \"\n-                f\"number of processes is {num_processes()}.\"\n+                \"by the number of processes. Here, \"\n+                f\"batch dimension = {num_model_replicas_total}, while \"\n+                f\"number of processes = {num_processes()}\"\n             )\n \n         per_replica_batch_size = int(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#445da52a125332906a9fbb09d7191ec49c03d9fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 72 | Churn Cumulative: 197 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 10 | DMM Complexity: 0.7796610169491526\n\nDIFF:\n@@ -490,6 +490,10 @@ class FeatureSpace(Layer):\n             if isinstance(preprocessor, layers.Normalization):\n                 if preprocessor.input_mean is not None:\n                     continue\n+            # Special case: a TextVectorization layer with provided vocabulary.\n+            elif isinstance(preprocessor, layers.TextVectorization):\n+                if preprocessor._has_input_vocabulary:\n+                    continue\n             if hasattr(preprocessor, \"adapt\"):\n                 adaptable_preprocessors.append(name)\n         return adaptable_preprocessors\n\n@@ -454,15 +454,77 @@ class FeatureSpaceTest(testing.TestCase):\n     def test_no_adapt(self):\n         data = {\n             \"int_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n+            \"text_1\": [\n+                \"This is\",\n+                \"not just\",\n+                \"an example\",\n+                \"of random words.\",\n+                \"these are\",\n+                \"some words\",\n+                \"in\",\n+                \"a random\",\n+                \"example.\",\n+                \"Bye!\",\n+            ],\n+            \"float_1\": [\n+                -1.2,\n+                0.0,\n+                2.4,\n+                1.2,\n+                15.0,\n+                -100.0,\n+                23.1,\n+                3.12,\n+                0.1,\n+                -0.01,\n+            ],\n         }\n+        cls = feature_space.FeatureSpace\n+        # Pre-defined vocabulary. No need to adapt.\n+        tv_vocab = [\n+            \"this\",\n+            \"is\",\n+            \"just\",\n+            \"an\",\n+            \"example\",\n+            \"with\",\n+            \"some\",\n+            \"words\",\n+        ]\n+        tv_with_vocab = layers.TextVectorization(\n+            vocabulary=tv_vocab, output_mode=\"int\", output_sequence_length=3\n+        )\n+\n+        # Pre-defined mean and variance. No need to adapt.\n+        mean, variance = 12.0, 5.0\n+        normalization = layers.Normalization(mean=mean, variance=variance)\n+\n         fs = feature_space.FeatureSpace(\n             {\n                 \"int_1\": \"integer_hashed\",\n+                \"text_1\": cls.feature(\n+                    dtype=\"string\",\n+                    preprocessor=tv_with_vocab,\n+                    output_mode=\"int\",\n+                ),\n+                \"float_1\": cls.feature(\n+                    dtype=\"float32\",\n+                    preprocessor=normalization,\n+                    output_mode=\"float\",\n+                ),\n             },\n-            output_mode=\"concat\",\n+            output_mode=\"dict\",\n         )\n+\n         out = fs(data)\n-        self.assertEqual(tuple(out.shape), (10, 32))\n+        float_out = ops.divide(\n+            ops.convert_to_tensor(data[\"float_1\"]) - mean, ops.sqrt(variance)\n+        )\n+        float_out = ops.reshape(float_out, (10, -1))\n+\n+        self.assertEqual(tuple(out[\"int_1\"].shape), (10, 32))\n+        self.assertEqual(tuple(out[\"text_1\"].shape), (10, 3))\n+        self.assertAllEqual(out[\"float_1\"], float_out)\n \n     @pytest.mark.skipif(\n         backend.backend() in (\"numpy\", \"torch\"),\n\n@@ -330,7 +330,7 @@ class TextVectorization(Layer):\n         self._encoding = encoding\n \n         # We save this hidden option to persist the fact\n-        # that we have have a non-adaptable layer with a\n+        # that we have a non-adaptable layer with a\n         # manually set vocab.\n         self._has_input_vocabulary = kwargs.pop(\n             \"has_input_vocabulary\", (vocabulary is not None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#473a8ec02cd862b846dd17545e4463240919da30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 48 | Churn Cumulative: 78 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -149,6 +149,7 @@ class ConvLSTMCell(Layer, DropoutRNNCell):\n \n         self.dropout = min(1.0, max(0.0, dropout))\n         self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n+        self.dropout_mask_count = 4\n         self.input_spec = InputSpec(ndim=rank + 2)\n         self.state_size = -1  # Custom, defined in methods\n \n@@ -233,19 +234,25 @@ class ConvLSTMCell(Layer, DropoutRNNCell):\n         h_tm1 = states[0]  # previous memory state\n         c_tm1 = states[1]  # previous carry state\n \n-        # dp_mask = self.get_dropout_mask(inputs)\n-        # rec_dp_mask = self.get_recurrent_dropout_mask(h_tm1)\n-\n-        # if training and 0.0 < self.dropout < 1.0:\n-        #     inputs *= dp_mask\n-        # if training and 0.0 < self.recurrent_dropout < 1.0:\n-        #     h_tm1 *= rec_dp_mask\n-\n+        if training and 0.0 < self.dropout < 1.0:\n+            dp_mask = self.get_dropout_mask(inputs)\n+            inputs_i = inputs * dp_mask[0]\n+            inputs_f = inputs * dp_mask[1]\n+            inputs_c = inputs * dp_mask[2]\n+            inputs_o = inputs * dp_mask[3]\n+        else:\n             inputs_i = inputs\n             inputs_f = inputs\n             inputs_c = inputs\n             inputs_o = inputs\n \n+        if training and 0.0 < self.recurrent_dropout < 1.0:\n+            rec_dp_mask = self.get_recurrent_dropout_mask(h_tm1)\n+            h_tm1_i = h_tm1 * rec_dp_mask[0]\n+            h_tm1_f = h_tm1 * rec_dp_mask[1]\n+            h_tm1_c = h_tm1 * rec_dp_mask[2]\n+            h_tm1_o = h_tm1 * rec_dp_mask[3]\n+        else:\n             h_tm1_i = h_tm1\n             h_tm1_f = h_tm1\n             h_tm1_c = h_tm1\n\n@@ -19,13 +19,27 @@ class DropoutRNNCell:\n     all incoming steps, so that the same mask is used for every step.\n     \"\"\"\n \n+    def _create_dropout_mask(self, step_input, dropout_rate):\n+        count = getattr(self, \"dropout_mask_count\", None)\n+        ones = ops.ones_like(step_input)\n+        if count is None:\n+            return backend.random.dropout(\n+                ones, rate=dropout_rate, seed=self.seed_generator\n+            )\n+        else:\n+            return [\n+                backend.random.dropout(\n+                    ones, rate=dropout_rate, seed=self.seed_generator\n+                )\n+                for _ in range(count)\n+            ]\n+\n     def get_dropout_mask(self, step_input):\n         if not hasattr(self, \"_dropout_mask\"):\n             self._dropout_mask = None\n         if self._dropout_mask is None and self.dropout > 0:\n-            ones = ops.ones_like(step_input)\n-            self._dropout_mask = backend.random.dropout(\n-                ones, rate=self.dropout, seed=self.seed_generator\n+            self._dropout_mask = self._create_dropout_mask(\n+                step_input, self.dropout\n             )\n         return self._dropout_mask\n \n@@ -33,9 +47,8 @@ class DropoutRNNCell:\n         if not hasattr(self, \"_recurrent_dropout_mask\"):\n             self._recurrent_dropout_mask = None\n         if self._recurrent_dropout_mask is None and self.recurrent_dropout > 0:\n-            ones = ops.ones_like(step_input)\n-            self._recurrent_dropout_mask = backend.random.dropout(\n-                ones, rate=self.recurrent_dropout, seed=self.seed_generator\n+            self._recurrent_dropout_mask = self._create_dropout_mask(\n+                step_input, self.recurrent_dropout\n             )\n         return self._recurrent_dropout_mask\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fc8c5ef19b672419b0b8f5b0d00aa003d97e6fb6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 40 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -247,6 +247,8 @@ class TorchTrainer(base_trainer.Trainer):\n             # do training behavior in case the user did not use `self.training`\n             # when implementing a custom layer with torch layers.\n             self.train()\n+\n+            logs = {}\n             for step, data in epoch_iterator.enumerate_epoch():\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6abd865eaa1af31d8fc60a573992a9e75f570bb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 105 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -524,7 +524,7 @@ class FeatureSpaceTest(testing.TestCase):\n \n         self.assertEqual(tuple(out[\"int_1\"].shape), (10, 32))\n         self.assertEqual(tuple(out[\"text_1\"].shape), (10, 3))\n-        self.assertAllEqual(out[\"float_1\"], float_out)\n+        self.assertAllClose(out[\"float_1\"], float_out, atol=1e-3)\n \n     @pytest.mark.skipif(\n         backend.backend() in (\"numpy\", \"torch\"),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b45be337c6156d90f220beba9f68eeb2e52e2b0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 22 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ class StringLookup(IndexLookup):\n \n     This layer translates a set of arbitrary strings into integer output via a\n     table-based vocabulary lookup. This layer will perform no splitting or\n-    transformation of input strings. For a layer than can split and tokenize\n+    transformation of input strings. For a layer that can split and tokenize\n     natural language, see the `keras.layers.TextVectorization` layer.\n \n     The vocabulary for the layer must be either supplied on construction or\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#933579d3c4a585f236982d05d3a74921f9567415", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 26 | Churn Cumulative: 117 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -284,7 +284,7 @@ def print_summary(\n         if not sequential_like:\n             fields.append(get_connections(layer))\n         if show_trainable:\n-            if layer.weights:\n+            if hasattr(layer, \"weights\") and len(layer.weights) > 0:\n                 fields.append(\n                     bold_text(\"Y\", color=34)\n                     if layer.trainable\n\n@@ -4,6 +4,7 @@ from absl.testing import parameterized\n \n from keras.src import layers\n from keras.src import models\n+from keras.src import ops\n from keras.src import testing\n from keras.src.utils import summary_utils\n \n@@ -74,3 +75,26 @@ class SummaryUtilsTest(testing.TestCase, parameterized.TestCase):\n         self.assertIn(\"Total params: 22\", summary_content)\n         self.assertIn(\"Trainable params: 22\", summary_content)\n         self.assertIn(\"Non-trainable params: 0\", summary_content)\n+\n+    def test_print_model_summary_op_as_layer(self):\n+        inputs = layers.Input((2,))\n+        x = layers.Dense(4)(inputs)\n+        outputs = ops.mean(x)\n+        model = models.Model(inputs, outputs)\n+\n+        summary_content = []\n+\n+        def print_to_variable(text, line_break=False):\n+            summary_content.append(text)\n+\n+        summary_utils.print_summary(\n+            model, print_fn=print_to_variable, show_trainable=True\n+        )\n+        summary_content = \"\\n\".join(summary_content)\n+        self.assertIn(\"(None, 4)\", summary_content)  # dense\n+        self.assertIn(\"Y\", summary_content)  # dense\n+        self.assertIn(\"()\", summary_content)  # mean\n+        self.assertIn(\"-\", summary_content)  # mean\n+        self.assertIn(\"Total params: 12\", summary_content)\n+        self.assertIn(\"Trainable params: 12\", summary_content)\n+        self.assertIn(\"Non-trainable params: 0\", summary_content)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04ed73e6f9b0a7f835a973ee2ceb3262278e8afb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 339 | Contributors (this commit): 10 | Commits (past 90d): 16 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -554,7 +554,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self.make_test_function()\n         self.stop_evaluating = False\n         callbacks.on_test_begin()\n-        logs = None\n+        logs = {}\n         self.reset_metrics()\n \n         self._jax_state_synced = True\n\n@@ -276,7 +276,7 @@ class NumpyTrainer(base_trainer.Trainer):\n         self.make_test_function()\n         self.stop_evaluating = False\n         callbacks.on_test_begin()\n-        logs = None\n+        logs = {}\n         self.reset_metrics()\n         for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_test_batch_begin(step)\n\n@@ -309,7 +309,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         self.make_train_function()\n         callbacks.on_train_begin()\n         training_logs = None\n-        logs = None\n+        logs = {}\n         initial_epoch = self._initial_epoch or initial_epoch\n         for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n@@ -425,7 +425,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         self.make_test_function()\n         self.stop_evaluating = False\n         callbacks.on_test_begin()\n-        logs = None\n+        logs = {}\n         self.reset_metrics()\n         with epoch_iterator.catch_stop_iteration():\n             for step, iterator in epoch_iterator.enumerate_epoch():\n\n@@ -366,7 +366,7 @@ class TorchTrainer(base_trainer.Trainer):\n         self.make_test_function()\n         self.stop_evaluating = False\n         callbacks.on_test_begin()\n-        logs = None\n+        logs = {}\n         self.reset_metrics()\n         for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_test_batch_begin(step)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
