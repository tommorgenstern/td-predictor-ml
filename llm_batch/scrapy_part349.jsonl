{"custom_id": "scrapy#d47c732ae9ebda84c689563048919923ddb17383", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 5 | Churn Cumulative: 2379 | Contributors (this commit): 35 | Commits (past 90d): 18 | Contributors (cumulative): 57 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,7 @@ Files Pipeline\n \n See documentation in topics/media-pipeline.rst\n \"\"\"\n+import base64\n import functools\n import hashlib\n import logging\n@@ -228,7 +229,7 @@ class GCSFilesStore:\n     def stat_file(self, path, info):\n         def _onsuccess(blob):\n             if blob:\n-                checksum = blob.md5_hash\n+                checksum = base64.b64decode(blob.md5_hash).hex()\n                 last_modified = time.mktime(blob.updated.timetuple())\n                 return {\"checksum\": checksum, \"last_modified\": last_modified}\n             return {}\n\n@@ -601,7 +601,7 @@ class TestGCSFilesStore(unittest.TestCase):\n         s = yield store.stat_file(path, info=None)\n         self.assertIn(\"last_modified\", s)\n         self.assertIn(\"checksum\", s)\n-        self.assertEqual(s[\"checksum\"], \"zc2oVgXkbQr2EQdSdw3OPA==\")\n+        self.assertEqual(s[\"checksum\"], \"cdcda85605e46d0af6110752770dce3c\")\n         u = urlparse(uri)\n         content, acl, blob = get_gcs_content_and_delete(u.hostname, u.path[1:] + path)\n         self.assertEqual(content, data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f0c2fae5e18c448bd1791920500c976d44fc321", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 2059 | Contributors (this commit): 20 | Commits (past 90d): 14 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n \"\"\"\n Link extractor based on lxml.html\n \"\"\"\n-import operator\n import logging\n+import operator\n from functools import partial\n from urllib.parse import urljoin, urlparse\n \n@@ -94,8 +94,8 @@ class LxmlParserLinkExtractor:\n             try:\n                 url = safe_url_string(url, encoding=response_encoding)\n             except ValueError:\n-                logger.debug(f\"Skipping extraction of bad link {url}\")\n-                continue  # Disregard badly formatted urls\n+                logger.debug(f\"Skipping extraction of link with bad URL {url!r}\")\n+                continue\n \n             # to fix relative links after process_value\n             url = urljoin(response_url, url)\n\n@@ -1,7 +1,10 @@\n import pickle\n import re\n import unittest\n-import sys \n+\n+from packaging.version import Version\n+from pytest import mark\n+from w3lib import __version__ as w3lib_version\n \n from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.link import Link\n@@ -817,13 +820,16 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n     def test_restrict_xpaths_with_html_entities(self):\n         super().test_restrict_xpaths_with_html_entities()\n \n-    @unittest.skipIf(\n-        sys.version_info < (3, 8),\n-        reason=\"some library for python 3.7 so is less strict so bad links like htis don't crash scrapy\",\n+    @mark.skipif(\n+        Version(w3lib_version) < Version(\"2.0.0\"),\n+        reason=(\n+            \"Before w3lib 2.0.0, w3lib.url.safe_url_string would not complain \"\n+            \"about an invalid port value.\"\n+        ),\n     )\n     def test_skip_bad_links(self):\n         html = b\"\"\"\n-        <a href=\"http://Some wierd html : http://example.com/like_this\">Why would you do this?</a>\n+        <a href=\"http://example.org:non-port\">Why would you do this?</a>\n         <a href=\"http://example.org/item2.html\">Good Link</a>\n         <a href=\"http://example.org/item3.html\">Good Link 2</a>\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2a31974ffc06412a5ab8d87fe070c98cd9c856b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 33 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 62 | Churn Cumulative: 1792 | Contributors (this commit): 34 | Commits (past 90d): 6 | Contributors (cumulative): 49 | DMM Complexity: 0.5\n\nDIFF:\n@@ -174,33 +174,33 @@ def binary_is_text(data):\n \n \n def get_func_args(func, stripself=False):\n-    \"\"\"Return the argument name list of a callable\"\"\"\n-    if inspect.isfunction(func):\n-        spec = inspect.getfullargspec(func)\n-        func_args = spec.args + spec.kwonlyargs\n-    elif inspect.isclass(func):\n-        return get_func_args(func.__init__, True)\n-    elif inspect.ismethod(func):\n-        return get_func_args(func.__func__, True)\n-    elif inspect.ismethoddescriptor(func):\n-        return []\n-    elif isinstance(func, partial):\n-        return [\n-            x\n-            for x in get_func_args(func.func)[len(func.args) :]\n-            if not (func.keywords and x in func.keywords)\n-        ]\n-    elif hasattr(func, \"__call__\"):\n-        if inspect.isroutine(func):\n-            return []\n-        if getattr(func, \"__name__\", None) == \"__call__\":\n-            return []\n-        return get_func_args(func.__call__, True)\n+    \"\"\"Return the argument name list of a callable object\"\"\"\n+    if not callable(func):\n+        raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n+\n+    args = []\n+    try:\n+        sig = inspect.signature(func)\n+    except ValueError:\n+        return args\n+\n+    if isinstance(func, partial):\n+        partial_args = func.args\n+        partial_kw = func.keywords\n+\n+        for name, param in sig.parameters.items():\n+            if param.name in partial_args:\n+                continue\n+            if partial_kw and param.name in partial_kw:\n+                continue\n+            args.append(name)\n     else:\n-        raise TypeError(f\"{type(func)} is not callable\")\n-    if stripself:\n-        func_args.pop(0)\n-    return func_args\n+        for name in sig.parameters.keys():\n+            args.append(name)\n+\n+    if stripself and args and args[0] == \"self\":\n+        args = args[1:]\n+    return args\n \n \n def get_spec(func):\n\n@@ -235,20 +235,16 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.assertEqual(get_func_args(partial_f3), [\"c\"])\n         self.assertEqual(get_func_args(cal), [\"a\", \"b\", \"c\"])\n         self.assertEqual(get_func_args(object), [])\n+        self.assertEqual(get_func_args(str.split, stripself=True), [\"sep\", \"maxsplit\"])\n+        self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n \n         if platform.python_implementation() == \"CPython\":\n-            # TODO: how do we fix this to return the actual argument names?\n-            self.assertEqual(get_func_args(str.split), [])\n-            self.assertEqual(get_func_args(\" \".join), [])\n+            # doesn't work on CPython: https://bugs.python.org/issue42785\n             self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n         elif platform.python_implementation() == \"PyPy\":\n-            self.assertEqual(\n-                get_func_args(str.split, stripself=True), [\"sep\", \"maxsplit\"]\n-            )\n             self.assertEqual(\n                 get_func_args(operator.itemgetter(2), stripself=True), [\"obj\"]\n             )\n-            self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#441ac196e4151765fa424af59f3938e72b8434c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 14/8 | Churn Δ: 95 | Churn Cumulative: 1819 | Contributors (this commit): 19 | Commits (past 90d): 4 | Contributors (cumulative): 26 | DMM Complexity: 0.6835443037974683\n\nDIFF:\n@@ -327,3 +327,34 @@ def _get_method(obj, name):\n         return getattr(obj, name)\n     except AttributeError:\n         raise ValueError(f\"Method {name!r} not found in: {obj}\")\n+\n+\n+def request_to_curl(request: Request) -> str:\n+    \"\"\"\n+    Converts a :class:`~scrapy.Request` object to a curl command.\n+\n+    :param :class:`~scrapy.Request`: Request object to be converted\n+    :return: string containing the curl command\n+    \"\"\"\n+    method = request.method\n+\n+    data = f\"--data-raw '{request.body.decode('utf-8')}'\" if request.body else \"\"\n+\n+    headers = \" \".join(\n+        f\"-H '{k.decode()}: {v[0].decode()}'\" for k, v in request.headers.items()\n+    )\n+\n+    url = request.url\n+    cookies = \"\"\n+    if request.cookies:\n+        if isinstance(request.cookies, dict):\n+            cookie = \"; \".join(f\"{k}={v}\" for k, v in request.cookies.items())\n+            cookies = f\"--cookie '{cookie}'\"\n+        elif isinstance(request.cookies, list):\n+            cookie = \"; \".join(\n+                f\"{list(c.keys())[0]}={list(c.values())[0]}\" for c in request.cookies\n+            )\n+            cookies = f\"--cookie '{cookie}'\"\n+\n+    curl_cmd = f\"curl -X {method} {url} {data} {headers} {cookies}\".strip()\n+    return \" \".join(curl_cmd.split())\n\n@@ -1,3 +1,4 @@\n+import json\n import unittest\n import warnings\n from hashlib import sha1\n@@ -18,6 +19,7 @@ from scrapy.utils.request import (\n     request_authenticate,\n     request_fingerprint,\n     request_httprepr,\n+    request_to_curl,\n )\n from scrapy.utils.test import get_crawler\n \n@@ -666,5 +668,67 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n         self.assertEqual(fingerprint, settings[\"FINGERPRINT\"])\n \n \n+class RequestToCurlTest(unittest.TestCase):\n+    def _test_request(self, request_object, expected_curl_command):\n+        curl_command = request_to_curl(request_object)\n+        self.assertEqual(curl_command, expected_curl_command)\n+\n+    def test_get(self):\n+        request_object = Request(\"https://www.example.com\")\n+        expected_curl_command = \"curl -X GET https://www.example.com\"\n+        self._test_request(request_object, expected_curl_command)\n+\n+    def test_post(self):\n+        request_object = Request(\n+            \"https://www.httpbin.org/post\",\n+            method=\"POST\",\n+            body=json.dumps({\"foo\": \"bar\"}),\n+        )\n+        expected_curl_command = (\n+            'curl -X POST https://www.httpbin.org/post --data-raw \\'{\"foo\": \"bar\"}\\''\n+        )\n+        self._test_request(request_object, expected_curl_command)\n+\n+    def test_headers(self):\n+        request_object = Request(\n+            \"https://www.httpbin.org/post\",\n+            method=\"POST\",\n+            headers={\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"},\n+            body=json.dumps({\"foo\": \"bar\"}),\n+        )\n+        expected_curl_command = (\n+            \"curl -X POST https://www.httpbin.org/post\"\n+            ' --data-raw \\'{\"foo\": \"bar\"}\\''\n+            \" -H 'Content-Type: application/json' -H 'Accept: application/json'\"\n+        )\n+        self._test_request(request_object, expected_curl_command)\n+\n+    def test_cookies_dict(self):\n+        request_object = Request(\n+            \"https://www.httpbin.org/post\",\n+            method=\"POST\",\n+            cookies={\"foo\": \"bar\"},\n+            body=json.dumps({\"foo\": \"bar\"}),\n+        )\n+        expected_curl_command = (\n+            \"curl -X POST https://www.httpbin.org/post\"\n+            \" --data-raw '{\\\"foo\\\": \\\"bar\\\"}' --cookie 'foo=bar'\"\n+        )\n+        self._test_request(request_object, expected_curl_command)\n+\n+    def test_cookies_list(self):\n+        request_object = Request(\n+            \"https://www.httpbin.org/post\",\n+            method=\"POST\",\n+            cookies=[{\"foo\": \"bar\"}],\n+            body=json.dumps({\"foo\": \"bar\"}),\n+        )\n+        expected_curl_command = (\n+            \"curl -X POST https://www.httpbin.org/post\"\n+            \" --data-raw '{\\\"foo\\\": \\\"bar\\\"}' --cookie 'foo=bar'\"\n+        )\n+        self._test_request(request_object, expected_curl_command)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#02f3e8d413ccdd6a3f0b5828a9cd94e1fb3662b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 206 | Lines Deleted: 120 | Files Changed: 8 | Hunks: 88 | Methods Changed: 86 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 326 | Churn Cumulative: 4473 | Contributors (this commit): 54 | Commits (past 90d): 29 | Contributors (cumulative): 124 | DMM Complexity: 0.9122807017543859\n\nDIFF:\n@@ -2,45 +2,52 @@ import random\n from collections import deque\n from datetime import datetime\n from time import time\n+from typing import TYPE_CHECKING, Any, Deque, Dict, Set, Tuple, cast\n \n-from twisted.internet import defer, task\n+from twisted.internet import task\n from twisted.internet.defer import Deferred\n \n from scrapy import Request, Spider, signals\n from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n+from scrapy.http import Response\n from scrapy.resolver import dnscache\n+from scrapy.settings import BaseSettings\n+from scrapy.signalmanager import SignalManager\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.httpobj import urlparse_cached\n \n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n \n class Slot:\n     \"\"\"Downloader slot\"\"\"\n \n-    def __init__(self, concurrency, delay, randomize_delay):\n-        self.concurrency = concurrency\n-        self.delay = delay\n-        self.randomize_delay = randomize_delay\n+    def __init__(self, concurrency: int, delay: float, randomize_delay: bool):\n+        self.concurrency: int = concurrency\n+        self.delay: float = delay\n+        self.randomize_delay: bool = randomize_delay\n \n-        self.active = set()\n-        self.queue = deque()\n-        self.transferring = set()\n-        self.lastseen = 0\n+        self.active: Set[Request] = set()\n+        self.queue: Deque[Tuple[Request, Deferred]] = deque()\n+        self.transferring: Set[Request] = set()\n+        self.lastseen: float = 0\n         self.latercall = None\n \n-    def free_transfer_slots(self):\n+    def free_transfer_slots(self) -> int:\n         return self.concurrency - len(self.transferring)\n \n-    def download_delay(self):\n+    def download_delay(self) -> float:\n         if self.randomize_delay:\n             return random.uniform(0.5 * self.delay, 1.5 * self.delay)\n         return self.delay\n \n-    def close(self):\n+    def close(self) -> None:\n         if self.latercall and self.latercall.active():\n             self.latercall.cancel()\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         cls_name = self.__class__.__name__\n         return (\n             f\"{cls_name}(concurrency={self.concurrency!r}, \"\n@@ -48,7 +55,7 @@ class Slot:\n             f\"randomize_delay={self.randomize_delay!r})\"\n         )\n \n-    def __str__(self):\n+    def __str__(self) -> str:\n         return (\n             f\"<downloader.Slot concurrency={self.concurrency!r} \"\n             f\"delay={self.delay:.2f} randomize_delay={self.randomize_delay!r} \"\n@@ -58,8 +65,10 @@ class Slot:\n         )\n \n \n-def _get_concurrency_delay(concurrency, spider, settings):\n-    delay = settings.getfloat(\"DOWNLOAD_DELAY\")\n+def _get_concurrency_delay(\n+    concurrency: int, spider: Spider, settings: BaseSettings\n+) -> Tuple[int, float]:\n+    delay: float = settings.getfloat(\"DOWNLOAD_DELAY\")\n     if hasattr(spider, \"download_delay\"):\n         delay = spider.download_delay\n \n@@ -72,23 +81,29 @@ def _get_concurrency_delay(concurrency, spider, settings):\n class Downloader:\n     DOWNLOAD_SLOT = \"download_slot\"\n \n-    def __init__(self, crawler):\n-        self.settings = crawler.settings\n-        self.signals = crawler.signals\n-        self.slots = {}\n-        self.active = set()\n-        self.handlers = DownloadHandlers(crawler)\n-        self.total_concurrency = self.settings.getint(\"CONCURRENT_REQUESTS\")\n-        self.domain_concurrency = self.settings.getint(\"CONCURRENT_REQUESTS_PER_DOMAIN\")\n-        self.ip_concurrency = self.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\")\n-        self.randomize_delay = self.settings.getbool(\"RANDOMIZE_DOWNLOAD_DELAY\")\n-        self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n-        self._slot_gc_loop = task.LoopingCall(self._slot_gc)\n+    def __init__(self, crawler: \"Crawler\"):\n+        self.settings: BaseSettings = crawler.settings\n+        self.signals: SignalManager = crawler.signals\n+        self.slots: Dict[str, Slot] = {}\n+        self.active: Set[Request] = set()\n+        self.handlers: DownloadHandlers = DownloadHandlers(crawler)\n+        self.total_concurrency: int = self.settings.getint(\"CONCURRENT_REQUESTS\")\n+        self.domain_concurrency: int = self.settings.getint(\n+            \"CONCURRENT_REQUESTS_PER_DOMAIN\"\n+        )\n+        self.ip_concurrency: int = self.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\")\n+        self.randomize_delay: bool = self.settings.getbool(\"RANDOMIZE_DOWNLOAD_DELAY\")\n+        self.middleware: DownloaderMiddlewareManager = (\n+            DownloaderMiddlewareManager.from_crawler(crawler)\n+        )\n+        self._slot_gc_loop: task.LoopingCall = task.LoopingCall(self._slot_gc)\n         self._slot_gc_loop.start(60)\n-        self.per_slot_settings = self.settings.getdict(\"DOWNLOAD_SLOTS\", {})\n+        self.per_slot_settings: Dict[str, Dict[str, Any]] = self.settings.getdict(\n+            \"DOWNLOAD_SLOTS\", {}\n+        )\n \n     def fetch(self, request: Request, spider: Spider) -> Deferred:\n-        def _deactivate(response):\n+        def _deactivate(response: Response) -> Response:\n             self.active.remove(request)\n             return response\n \n@@ -99,7 +114,7 @@ class Downloader:\n     def needs_backout(self) -> bool:\n         return len(self.active) >= self.total_concurrency\n \n-    def _get_slot(self, request, spider):\n+    def _get_slot(self, request: Request, spider: Spider) -> Tuple[str, Slot]:\n         key = self._get_slot_key(request, spider)\n         if key not in self.slots:\n             slot_settings = self.per_slot_settings.get(key, {})\n@@ -117,9 +132,9 @@ class Downloader:\n \n         return key, self.slots[key]\n \n-    def _get_slot_key(self, request, spider):\n+    def _get_slot_key(self, request: Request, spider: Spider) -> str:\n         if self.DOWNLOAD_SLOT in request.meta:\n-            return request.meta[self.DOWNLOAD_SLOT]\n+            return cast(str, request.meta[self.DOWNLOAD_SLOT])\n \n         key = urlparse_cached(request).hostname or \"\"\n         if self.ip_concurrency:\n@@ -127,11 +142,11 @@ class Downloader:\n \n         return key\n \n-    def _enqueue_request(self, request, spider):\n+    def _enqueue_request(self, request: Request, spider: Spider) -> Deferred:\n         key, slot = self._get_slot(request, spider)\n         request.meta[self.DOWNLOAD_SLOT] = key\n \n-        def _deactivate(response):\n+        def _deactivate(response: Response) -> Response:\n             slot.active.remove(request)\n             return response\n \n@@ -139,12 +154,12 @@ class Downloader:\n         self.signals.send_catch_log(\n             signal=signals.request_reached_downloader, request=request, spider=spider\n         )\n-        deferred = defer.Deferred().addBoth(_deactivate)\n+        deferred = Deferred().addBoth(_deactivate)\n         slot.queue.append((request, deferred))\n         self._process_queue(spider, slot)\n         return deferred\n \n-    def _process_queue(self, spider, slot):\n+    def _process_queue(self, spider: Spider, slot: Slot) -> None:\n         from twisted.internet import reactor\n \n         if slot.latercall and slot.latercall.active():\n@@ -172,7 +187,7 @@ class Downloader:\n                 self._process_queue(spider, slot)\n                 break\n \n-    def _download(self, slot, request, spider):\n+    def _download(self, slot: Slot, request: Request, spider: Spider) -> Deferred:\n         # The order is very important for the following deferreds. Do not change!\n \n         # 1. Create the download deferred\n@@ -180,7 +195,7 @@ class Downloader:\n \n         # 2. Notify response_downloaded listeners about the recent download\n         # before querying queue for next request\n-        def _downloaded(response):\n+        def _downloaded(response: Response) -> Response:\n             self.signals.send_catch_log(\n                 signal=signals.response_downloaded,\n                 response=response,\n@@ -197,7 +212,7 @@ class Downloader:\n         # middleware itself)\n         slot.transferring.add(request)\n \n-        def finish_transferring(_):\n+        def finish_transferring(_: Any) -> Any:\n             slot.transferring.remove(request)\n             self._process_queue(spider, slot)\n             self.signals.send_catch_log(\n\n@@ -1,4 +1,5 @@\n import warnings\n+from typing import TYPE_CHECKING, Any, List, Optional\n \n from OpenSSL import SSL\n from twisted.internet._sslverify import _setAcceptableProtocols\n@@ -18,8 +19,12 @@ from scrapy.core.downloader.tls import (\n     ScrapyClientTLSOptions,\n     openssl_methods,\n )\n+from scrapy.settings import BaseSettings\n from scrapy.utils.misc import create_instance, load_object\n \n+if TYPE_CHECKING:\n+    from twisted.internet._sslverify import ClientTLSOptions\n+\n \n @implementer(IPolicyForHTTPS)\n class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n@@ -35,25 +40,34 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n \n     def __init__(\n         self,\n-        method=SSL.SSLv23_METHOD,\n-        tls_verbose_logging=False,\n-        tls_ciphers=None,\n-        *args,\n-        **kwargs,\n+        method: int = SSL.SSLv23_METHOD,\n+        tls_verbose_logging: bool = False,\n+        tls_ciphers: Optional[str] = None,\n+        *args: Any,\n+        **kwargs: Any,\n     ):\n         super().__init__(*args, **kwargs)\n-        self._ssl_method = method\n-        self.tls_verbose_logging = tls_verbose_logging\n+        self._ssl_method: int = method\n+        self.tls_verbose_logging: bool = tls_verbose_logging\n+        self.tls_ciphers: AcceptableCiphers\n         if tls_ciphers:\n             self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(tls_ciphers)\n         else:\n             self.tls_ciphers = DEFAULT_CIPHERS\n \n     @classmethod\n-    def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):\n-        tls_verbose_logging = settings.getbool(\"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\")\n-        tls_ciphers = settings[\"DOWNLOADER_CLIENT_TLS_CIPHERS\"]\n-        return cls(\n+    def from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n+    ):\n+        tls_verbose_logging: bool = settings.getbool(\n+            \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\n+        )\n+        tls_ciphers: Optional[str] = settings[\"DOWNLOADER_CLIENT_TLS_CIPHERS\"]\n+        return cls(  # type: ignore[misc]\n             method=method,\n             tls_verbose_logging=tls_verbose_logging,\n             tls_ciphers=tls_ciphers,\n@@ -61,7 +75,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n             **kwargs,\n         )\n \n-    def getCertificateOptions(self):\n+    def getCertificateOptions(self) -> CertificateOptions:\n         # setting verify=True will require you to provide CAs\n         # to verify against; in other words: it's not that simple\n \n@@ -82,12 +96,12 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n \n     # kept for old-style HTTP/1.0 downloader context twisted calls,\n     # e.g. connectSSL()\n-    def getContext(self, hostname=None, port=None):\n+    def getContext(self, hostname: Any = None, port: Any = None) -> SSL.Context:\n         ctx = self.getCertificateOptions().getContext()\n         ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT\n         return ctx\n \n-    def creatorForNetloc(self, hostname, port):\n+    def creatorForNetloc(self, hostname: bytes, port: int) -> \"ClientTLSOptions\":\n         return ScrapyClientTLSOptions(\n             hostname.decode(\"ascii\"),\n             self.getContext(),\n@@ -114,7 +128,7 @@ class BrowserLikeContextFactory(ScrapyClientContextFactory):\n     ``SSLv23_METHOD``) which allows TLS protocol negotiation.\n     \"\"\"\n \n-    def creatorForNetloc(self, hostname, port):\n+    def creatorForNetloc(self, hostname: bytes, port: int) -> \"ClientTLSOptions\":\n         # trustRoot set to platformTrust() will use the platform's root CAs.\n         #\n         # This means that a website like https://www.cacert.org will be rejected\n@@ -133,13 +147,15 @@ class AcceptableProtocolsContextFactory:\n     negotiation.\n     \"\"\"\n \n-    def __init__(self, context_factory, acceptable_protocols):\n+    def __init__(self, context_factory: Any, acceptable_protocols: List[bytes]):\n         verifyObject(IPolicyForHTTPS, context_factory)\n-        self._wrapped_context_factory = context_factory\n-        self._acceptable_protocols = acceptable_protocols\n+        self._wrapped_context_factory: Any = context_factory\n+        self._acceptable_protocols: List[bytes] = acceptable_protocols\n \n-    def creatorForNetloc(self, hostname, port):\n-        options = self._wrapped_context_factory.creatorForNetloc(hostname, port)\n+    def creatorForNetloc(self, hostname: bytes, port: int) -> \"ClientTLSOptions\":\n+        options: \"ClientTLSOptions\" = self._wrapped_context_factory.creatorForNetloc(\n+            hostname, port\n+        )\n         _setAcceptableProtocols(options._ctx, self._acceptable_protocols)\n         return options\n \n\n@@ -1,25 +1,32 @@\n \"\"\"Download handlers for different schemes\"\"\"\n \n import logging\n+from typing import TYPE_CHECKING, Any, Callable, Dict, Generator, Union, cast\n \n from twisted.internet import defer\n+from twisted.internet.defer import Deferred\n \n-from scrapy import signals\n+from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured, NotSupported\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import without_none_values\n \n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n logger = logging.getLogger(__name__)\n \n \n class DownloadHandlers:\n-    def __init__(self, crawler):\n-        self._crawler = crawler\n-        self._schemes = {}  # stores acceptable schemes on instancing\n-        self._handlers = {}  # stores instanced handlers for schemes\n-        self._notconfigured = {}  # remembers failed handlers\n-        handlers = without_none_values(\n+    def __init__(self, crawler: \"Crawler\"):\n+        self._crawler: \"Crawler\" = crawler\n+        self._schemes: Dict[\n+            str, Union[str, Callable]\n+        ] = {}  # stores acceptable schemes on instancing\n+        self._handlers: Dict[str, Any] = {}  # stores instanced handlers for schemes\n+        self._notconfigured: Dict[str, str] = {}  # remembers failed handlers\n+        handlers: Dict[str, Union[str, Callable]] = without_none_values(\n             crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")\n         )\n         for scheme, clspath in handlers.items():\n@@ -28,7 +35,7 @@ class DownloadHandlers:\n \n         crawler.signals.connect(self._close, signals.engine_stopped)\n \n-    def _get_handler(self, scheme):\n+    def _get_handler(self, scheme: str) -> Any:\n         \"\"\"Lazy-load the downloadhandler for a scheme\n         only on the first request for that scheme.\n         \"\"\"\n@@ -42,7 +49,7 @@ class DownloadHandlers:\n \n         return self._load_handler(scheme)\n \n-    def _load_handler(self, scheme, skip_lazy=False):\n+    def _load_handler(self, scheme: str, skip_lazy: bool = False) -> Any:\n         path = self._schemes[scheme]\n         try:\n             dhcls = load_object(path)\n@@ -69,17 +76,17 @@ class DownloadHandlers:\n             self._handlers[scheme] = dh\n             return dh\n \n-    def download_request(self, request, spider):\n+    def download_request(self, request: Request, spider: Spider) -> Deferred:\n         scheme = urlparse_cached(request).scheme\n         handler = self._get_handler(scheme)\n         if not handler:\n             raise NotSupported(\n                 f\"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}\"\n             )\n-        return handler.download_request(request, spider)\n+        return cast(Deferred, handler.download_request(request, spider))\n \n     @defer.inlineCallbacks\n-    def _close(self, *_a, **_kw):\n+    def _close(self, *_a: Any, **_kw: Any) -> Generator[Deferred, Any, None]:\n         for dh in self._handlers.values():\n             if hasattr(dh, \"close\"):\n                 yield dh.close()\n\n@@ -3,15 +3,16 @@ Downloader Middleware manager\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n-from typing import Callable, Union, cast\n+from typing import Any, Callable, Generator, List, Union, cast\n \n-from twisted.internet import defer\n+from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.python.failure import Failure\n \n from scrapy import Spider\n from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Request, Response\n from scrapy.middleware import MiddlewareManager\n+from scrapy.settings import BaseSettings\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import deferred_from_coro, mustbe_deferred\n \n@@ -20,10 +21,10 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n     component_name = \"downloader middleware\"\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings):\n+    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> List[Any]:\n         return build_component_list(settings.getwithbase(\"DOWNLOADER_MIDDLEWARES\"))\n \n-    def _add_middleware(self, mw):\n+    def _add_middleware(self, mw: Any) -> None:\n         if hasattr(mw, \"process_request\"):\n             self.methods[\"process_request\"].append(mw.process_request)\n         if hasattr(mw, \"process_response\"):\n@@ -31,9 +32,11 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         if hasattr(mw, \"process_exception\"):\n             self.methods[\"process_exception\"].appendleft(mw.process_exception)\n \n-    def download(self, download_func: Callable, request: Request, spider: Spider):\n-        @defer.inlineCallbacks\n-        def process_request(request: Request):\n+    def download(\n+        self, download_func: Callable, request: Request, spider: Spider\n+    ) -> Deferred:\n+        @inlineCallbacks\n+        def process_request(request: Request) -> Generator[Deferred, Any, Any]:\n             for method in self.methods[\"process_request\"]:\n                 method = cast(Callable, method)\n                 response = yield deferred_from_coro(\n@@ -50,8 +53,10 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                     return response\n             return (yield download_func(request=request, spider=spider))\n \n-        @defer.inlineCallbacks\n-        def process_response(response: Union[Response, Request]):\n+        @inlineCallbacks\n+        def process_response(\n+            response: Union[Response, Request]\n+        ) -> Generator[Deferred, Any, Union[Response, Request]]:\n             if response is None:\n                 raise TypeError(\"Received None in process_response\")\n             elif isinstance(response, Request):\n@@ -71,8 +76,10 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                     return response\n             return response\n \n-        @defer.inlineCallbacks\n-        def process_exception(failure: Failure):\n+        @inlineCallbacks\n+        def process_exception(\n+            failure: Failure,\n+        ) -> Generator[Deferred, Any, Union[Failure, Response, Request]]:\n             exception = failure.value\n             for method in self.methods[\"process_exception\"]:\n                 method = cast(Callable, method)\n\n@@ -1,4 +1,5 @@\n import logging\n+from typing import Any, Dict\n \n from OpenSSL import SSL\n from service_identity.exceptions import CertificateError\n@@ -20,7 +21,7 @@ METHOD_TLSv11 = \"TLSv1.1\"\n METHOD_TLSv12 = \"TLSv1.2\"\n \n \n-openssl_methods = {\n+openssl_methods: Dict[str, int] = {\n     METHOD_TLS: SSL.SSLv23_METHOD,  # protocol negotiation (recommended)\n     METHOD_TLSv10: SSL.TLSv1_METHOD,  # TLS 1.0 only\n     METHOD_TLSv11: SSL.TLSv1_1_METHOD,  # TLS 1.1 only\n@@ -39,11 +40,13 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n     logging warnings. Also, HTTPS connection parameters logging is added.\n     \"\"\"\n \n-    def __init__(self, hostname, ctx, verbose_logging=False):\n+    def __init__(self, hostname: str, ctx: SSL.Context, verbose_logging: bool = False):\n         super().__init__(hostname, ctx)\n-        self.verbose_logging = verbose_logging\n+        self.verbose_logging: bool = verbose_logging\n \n-    def _identityVerifyingInfoCallback(self, connection, where, ret):\n+    def _identityVerifyingInfoCallback(\n+        self, connection: SSL.Connection, where: int, ret: Any\n+    ) -> None:\n         if where & SSL.SSL_CB_HANDSHAKE_START:\n             connection.set_tlsext_host_name(self._hostnameBytes)\n         elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n@@ -55,6 +58,7 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n                     connection.get_cipher_name(),\n                 )\n                 server_cert = connection.get_peer_certificate()\n+                if server_cert:\n                     logger.debug(\n                         'SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                         x509name_to_string(server_cert.get_issuer()),\n@@ -82,4 +86,6 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n                 )\n \n \n-DEFAULT_CIPHERS = AcceptableCiphers.fromOpenSSLCipherString(\"DEFAULT\")\n+DEFAULT_CIPHERS: AcceptableCiphers = AcceptableCiphers.fromOpenSSLCipherString(\n+    \"DEFAULT\"\n+)\n\n@@ -1,22 +1,25 @@\n import re\n from time import time\n-from urllib.parse import urldefrag, urlparse, urlunparse\n+from typing import Optional, Tuple\n+from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n from twisted.internet import defer\n from twisted.internet.protocol import ClientFactory\n from twisted.web.http import HTTPClient\n \n+from scrapy import Request\n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n-def _parsed_url_args(parsed):\n+def _parsed_url_args(parsed: ParseResult) -> Tuple[bytes, bytes, bytes, int, bytes]:\n     # Assume parsed is urlparse-d from Request.url,\n     # which was passed via safe_url_string and is ascii-only.\n-    path = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n-    path = to_bytes(path, encoding=\"ascii\")\n+    path_str = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n+    path = to_bytes(path_str, encoding=\"ascii\")\n+    assert parsed.hostname is not None\n     host = to_bytes(parsed.hostname, encoding=\"ascii\")\n     port = parsed.port\n     scheme = to_bytes(parsed.scheme, encoding=\"ascii\")\n@@ -26,7 +29,7 @@ def _parsed_url_args(parsed):\n     return scheme, netloc, host, port, path\n \n \n-def _parse(url):\n+def _parse(url: str) -> Tuple[bytes, bytes, bytes, int, bytes]:\n     \"\"\"Return tuple of (scheme, netloc, host, port, path),\n     all in bytes except for port which is int.\n     Assume url is from Request.url, which was passed via safe_url_string\n@@ -132,17 +135,19 @@ class ScrapyHTTPClientFactory(ClientFactory):\n             self.scheme, _, self.host, self.port, _ = _parse(proxy)\n             self.path = self.url\n \n-    def __init__(self, request, timeout=180):\n-        self._url = urldefrag(request.url)[0]\n+    def __init__(self, request: Request, timeout: float = 180):\n+        self._url: str = urldefrag(request.url)[0]\n         # converting to bytes to comply to Twisted interface\n-        self.url = to_bytes(self._url, encoding=\"ascii\")\n-        self.method = to_bytes(request.method, encoding=\"ascii\")\n-        self.body = request.body or None\n-        self.headers = Headers(request.headers)\n-        self.response_headers = None\n-        self.timeout = request.meta.get(\"download_timeout\") or timeout\n-        self.start_time = time()\n-        self.deferred = defer.Deferred().addCallback(self._build_response, request)\n+        self.url: bytes = to_bytes(self._url, encoding=\"ascii\")\n+        self.method: bytes = to_bytes(request.method, encoding=\"ascii\")\n+        self.body: Optional[bytes] = request.body or None\n+        self.headers: Headers = Headers(request.headers)\n+        self.response_headers: Optional[Headers] = None\n+        self.timeout: float = request.meta.get(\"download_timeout\") or timeout\n+        self.start_time: float = time()\n+        self.deferred: defer.Deferred = defer.Deferred().addCallback(\n+            self._build_response, request\n+        )\n \n         # Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected\n         # to have _disconnectedDeferred. See Twisted r32329.\n@@ -150,7 +155,7 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         # needed to add the callback _waitForDisconnect.\n         # Specifically this avoids the AttributeError exception when\n         # clientConnectionFailed method is called.\n-        self._disconnectedDeferred = defer.Deferred()\n+        self._disconnectedDeferred: defer.Deferred = defer.Deferred()\n \n         self._set_connection_attributes(request)\n \n@@ -166,8 +171,8 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         elif self.method == b\"POST\":\n             self.headers[\"Content-Length\"] = 0\n \n-    def __repr__(self):\n-        return f\"<{self.__class__.__name__}: {self.url}>\"\n+    def __repr__(self) -> str:\n+        return f\"<{self.__class__.__name__}: {self._url}>\"\n \n     def _cancelTimeout(self, result, timeoutCall):\n         if timeoutCall.active():\n\n@@ -8,7 +8,16 @@ import sys\n import weakref\n from functools import partial, wraps\n from itertools import chain\n-from typing import Any, AsyncGenerator, AsyncIterable, Iterable, Union\n+from typing import (\n+    Any,\n+    AsyncGenerator,\n+    AsyncIterable,\n+    Iterable,\n+    Mapping,\n+    Optional,\n+    Union,\n+    overload,\n+)\n \n from scrapy.utils.asyncgen import as_async_generator\n \n@@ -82,7 +91,9 @@ def unique(list_, key=lambda x: x):\n     return result\n \n \n-def to_unicode(text, encoding=None, errors=\"strict\"):\n+def to_unicode(\n+    text: Union[str, bytes], encoding: Optional[str] = None, errors: str = \"strict\"\n+) -> str:\n     \"\"\"Return the unicode representation of a bytes object ``text``. If\n     ``text`` is already an unicode object, return it as-is.\"\"\"\n     if isinstance(text, str):\n@@ -97,7 +108,9 @@ def to_unicode(text, encoding=None, errors=\"strict\"):\n     return text.decode(encoding, errors)\n \n \n-def to_bytes(text, encoding=None, errors=\"strict\"):\n+def to_bytes(\n+    text: Union[str, bytes], encoding: Optional[str] = None, errors: str = \"strict\"\n+) -> bytes:\n     \"\"\"Return the binary representation of ``text``. If ``text``\n     is already a bytes object, return it as-is.\"\"\"\n     if isinstance(text, bytes):\n@@ -160,11 +173,12 @@ def memoizemethod_noargs(method):\n     return new_method\n \n \n-_BINARYCHARS = {to_bytes(chr(i)) for i in range(32)} - {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n-_BINARYCHARS |= {ord(ch) for ch in _BINARYCHARS}\n+_BINARYCHARS = {\n+    i for i in range(32) if to_bytes(chr(i)) not in {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n+}\n \n \n-def binary_is_text(data):\n+def binary_is_text(data: bytes) -> bool:\n     \"\"\"Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n     does not contain unprintable control characters.\n     \"\"\"\n@@ -258,6 +272,16 @@ def equal_attributes(obj1, obj2, attributes):\n     return True\n \n \n+@overload\n+def without_none_values(iterable: Mapping) -> dict:\n+    ...\n+\n+\n+@overload\n+def without_none_values(iterable: Iterable) -> Iterable:\n+    ...\n+\n+\n def without_none_values(iterable):\n     \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n \n\n@@ -1,24 +1,28 @@\n+from typing import Any, Optional, cast\n+\n import OpenSSL._util as pyOpenSSLutil\n import OpenSSL.SSL\n+import OpenSSL.version\n+from OpenSSL.crypto import X509Name\n \n from scrapy.utils.python import to_unicode\n \n \n-def ffi_buf_to_string(buf):\n+def ffi_buf_to_string(buf: Any) -> str:\n     return to_unicode(pyOpenSSLutil.ffi.string(buf))\n \n \n-def x509name_to_string(x509name):\n+def x509name_to_string(x509name: X509Name) -> str:\n     # from OpenSSL.crypto.X509Name.__repr__\n-    result_buffer = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n+    result_buffer: Any = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n     pyOpenSSLutil.lib.X509_NAME_oneline(\n-        x509name._name, result_buffer, len(result_buffer)\n+        x509name._name, result_buffer, len(result_buffer)  # type: ignore[attr-defined]\n     )\n \n     return ffi_buf_to_string(result_buffer)\n \n \n-def get_temp_key_info(ssl_object):\n+def get_temp_key_info(ssl_object: Any) -> Optional[str]:\n     # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n     if not hasattr(pyOpenSSLutil.lib, \"SSL_get_server_tmp_key\"):\n         # removed in cryptography 40.0.0\n@@ -53,8 +57,10 @@ def get_temp_key_info(ssl_object):\n     return \", \".join(key_info)\n \n \n-def get_openssl_version():\n-    system_openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION).decode(\n-        \"ascii\", errors=\"replace\"\n+def get_openssl_version() -> str:\n+    # https://github.com/python/typeshed/issues/10024\n+    system_openssl_bytes = cast(\n+        bytes, OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\n     )\n+    system_openssl = system_openssl_bytes.decode(\"ascii\", errors=\"replace\")\n     return f\"{OpenSSL.version.__version__} ({system_openssl})\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f5447f3b4ca2c91a07bfdd5acad9661061b8bbd7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 228 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import argparse\n+import re\n import warnings\n from http.cookies import SimpleCookie\n from shlex import split\n@@ -7,6 +8,15 @@ from urllib.parse import urlparse\n from w3lib.http import basic_auth_header\n \n \n+class DataAction(argparse.Action):\n+    def __call__(self, parser, namespace, values, option_string=None):\n+        value = str(values).encode(\"utf-8\").decode(\"utf-8\")\n+        if items := re.findall(r\"{.+}\", value):\n+            value = items[0]\n+\n+        setattr(namespace, self.dest, value)\n+\n+\n class CurlParser(argparse.ArgumentParser):\n     def error(self, message):\n         error_msg = f\"There was an error parsing the curl command: {message}\"\n@@ -17,7 +27,7 @@ curl_parser = CurlParser()\n curl_parser.add_argument(\"url\")\n curl_parser.add_argument(\"-H\", \"--header\", dest=\"headers\", action=\"append\")\n curl_parser.add_argument(\"-X\", \"--request\", dest=\"method\")\n-curl_parser.add_argument(\"-d\", \"--data\", \"--data-raw\", dest=\"data\")\n+curl_parser.add_argument(\"-d\", \"--data\", \"--data-raw\", dest=\"data\", action=DataAction)\n curl_parser.add_argument(\"-u\", \"--user\", dest=\"auth\")\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#88c58a8c9ca6f8149779bfc92d00499a613e5fb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 305 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -154,6 +154,15 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n         }\n         self._test_command(curl_command, expected_result)\n \n+    def test_post_data_raw_with_string_prefix(self):\n+        curl_command = \"curl 'https://www.example.org/' --data-raw $'{\\\"$filters\\\":\\\"Filter\\u0021\\\"}'\"\n+        expected_result = {\n+            \"method\": \"POST\",\n+            \"url\": \"https://www.example.org/\",\n+            \"body\": '{\"$filters\":\"Filter!\"}',\n+        }\n+        self._test_command(curl_command, expected_result)\n+\n     def test_explicit_get_with_data(self):\n         curl_command = \"curl httpbin.org/anything -X GET --data asdf\"\n         expected_result = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#69f96b9e96b1399f68cc61db0fe2d2b6cff5484d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 230 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ from w3lib.http import basic_auth_header\n class DataAction(argparse.Action):\n     def __call__(self, parser, namespace, values, option_string=None):\n         value = str(values).encode(\"utf-8\").decode(\"utf-8\")\n-        if items := re.findall(r\"{.+}\", value):\n+        if items := re.findall(r\"\\$(.+)\", value):\n             value = items[0]\n \n         setattr(namespace, self.dest, value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3209eac14f430f9cba522c12a615111bcabaecd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 235 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -11,9 +11,8 @@ from w3lib.http import basic_auth_header\n class DataAction(argparse.Action):\n     def __call__(self, parser, namespace, values, option_string=None):\n         value = str(values).encode(\"utf-8\").decode(\"utf-8\")\n-        if items := re.findall(r\"\\$(.+)\", value):\n-            value = items[0]\n-\n+        items = re.findall(r\"\\$(.+)\", value)\n+        value = items[0] if items else value\n         setattr(namespace, self.dest, value)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e1814faf836757933afec4c7c394f43f34c3567", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 238 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,8 +11,7 @@ from w3lib.http import basic_auth_header\n class DataAction(argparse.Action):\n     def __call__(self, parser, namespace, values, option_string=None):\n         value = str(values).encode(\"utf-8\").decode(\"utf-8\")\n-        items = re.findall(r\"\\$(.+)\", value)\n-        value = items[0] if items else value\n+        value = value[1::] if re.match(r\"^\\$(.+)\", value) else value\n         setattr(namespace, self.dest, value)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
