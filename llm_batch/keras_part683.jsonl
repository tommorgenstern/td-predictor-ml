{"custom_id": "keras#cb4e28ac9385bfb8a239fbfb543bdaf01e723a7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 233 | Lines Deleted: 21 | Files Changed: 9 | Hunks: 28 | Methods Changed: 22 | Complexity Δ (Sum/Max): 33/10 | Churn Δ: 254 | Churn Cumulative: 8952 | Contributors (this commit): 14 | Commits (past 90d): 102 | Contributors (cumulative): 45 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from keras_core.backend.common.backend_utils import (\n     compute_conv_transpose_padding,\n )\n from keras_core.backend.config import epsilon\n+from keras_core.backend.jax.core import cast\n from keras_core.backend.jax.core import convert_to_tensor\n \n \n@@ -391,6 +392,15 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n     return jnn.one_hot(x, num_classes, axis=axis, dtype=dtype)\n \n \n+def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    reduction_axis = 1 if len(x.shape) > 1 else 0\n+    outputs = jnp.max(\n+        one_hot(cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype),\n+        axis=reduction_axis,\n+    )\n+    return outputs\n+\n+\n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     target = jnp.array(target)\n     output = jnp.array(output)\n\n@@ -8,6 +8,7 @@ from keras_core.backend.common.backend_utils import (\n     compute_conv_transpose_padding,\n )\n from keras_core.backend.config import epsilon\n+from keras_core.backend.numpy.core import cast\n from keras_core.backend.numpy.core import is_tensor\n \n \n@@ -443,6 +444,15 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n     return categorical\n \n \n+def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    reduction_axis = 1 if len(x.shape) > 1 else 0\n+    outputs = np.max(\n+        one_hot(cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype),\n+        axis=reduction_axis,\n+    )\n+    return outputs\n+\n+\n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     target = np.array(target)\n     output = np.array(output)\n\n@@ -7,6 +7,7 @@ from keras_core.backend.common.backend_utils import (\n     compute_conv_transpose_output_shape,\n )\n from keras_core.backend.config import epsilon\n+from keras_core.backend.tensorflow.core import cast\n \n \n def relu(x):\n@@ -420,6 +421,15 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n     return tf.one_hot(x, num_classes, axis=axis, dtype=dtype)\n \n \n+def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    reduction_axis = 1 if len(x.shape) > 1 else 0\n+    outputs = tf.reduce_max(\n+        one_hot(cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype),\n+        axis=reduction_axis,\n+    )\n+    return outputs\n+\n+\n def _get_logits(output, from_logits, op_type, fn_name):\n     \"\"\"Retrieves logits tensor from maybe-softmax or maybe-sigmoid tensor.\"\"\"\n     output_ = output\n\n@@ -7,6 +7,7 @@ from keras_core.backend.common.backend_utils import (\n     compute_conv_transpose_padding,\n )\n from keras_core.backend.config import epsilon\n+from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import convert_to_tensor\n from keras_core.backend.torch.core import get_device\n from keras_core.backend.torch.numpy import expand_dims\n@@ -536,6 +537,15 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n     return output\n \n \n+def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    reduction_axis = 1 if len(x.shape) > 1 else 0\n+    outputs = torch.amax(\n+        one_hot(cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype),\n+        dim=reduction_axis,\n+    )\n+    return outputs\n+\n+\n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     target = convert_to_tensor(target)\n     output = convert_to_tensor(output)\n\n@@ -51,6 +51,21 @@ class Concatenate(Merge):\n         shape_set = set()\n \n         for i in range(len(reduced_inputs_shapes)):\n+            # Convert self.axis to positive axis for each input\n+            # in case self.axis is a negative number\n+            concat_axis = self.axis % len(reduced_inputs_shapes[i])\n+            #  Skip batch axis.\n+            for axis, axis_value in enumerate(reduced_inputs_shapes[i][1:],\n+                                              start=1):\n+                # Remove squeezable axes (axes with value of 1)\n+                # if not in the axis that will be used for concatenation\n+                # otherwise leave it.\n+                # This approach allows building the layer,\n+                # but if tensor shapes are not the same when\n+                # calling, an exception will be raised.\n+                if axis != concat_axis and axis_value == 1:\n+                    del reduced_inputs_shapes[i][axis]\n+\n             del reduced_inputs_shapes[i][self.axis]\n             shape_set.add(tuple(reduced_inputs_shapes[i]))\n \n\n@@ -1,12 +1,11 @@\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras_core.utils import backend_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.CategoryEncoding\")\n-class CategoryEncoding(Layer):\n+class CategoryEncoding(TFDataLayer):\n     \"\"\"A preprocessing layer which encodes integer features.\n \n     This layer provides options for condensing data into a categorical encoding\n@@ -84,17 +83,12 @@ class CategoryEncoding(Layer):\n     \"\"\"\n \n     def __init__(self, num_tokens=None, output_mode=\"multi_hot\", **kwargs):\n-        if not tf.available:\n-            raise ImportError(\n-                \"Layer CategoryEncoding requires TensorFlow. \"\n-                \"Install it via `pip install tensorflow`.\"\n-            )\n-\n         super().__init__(**kwargs)\n \n         # Support deprecated names for output_modes.\n         if output_mode == \"binary\":\n             output_mode = \"multi_hot\"\n+\n         # 'output_mode' must be one of (\"count\", \"one_hot\", \"multi_hot\")\n         if output_mode not in (\"count\", \"one_hot\", \"multi_hot\"):\n             raise ValueError(f\"Unknown arg for output_mode: {output_mode}\")\n@@ -111,17 +105,35 @@ class CategoryEncoding(Layer):\n             )\n         self.num_tokens = num_tokens\n         self.output_mode = output_mode\n-\n-        self.layer = tf.keras.layers.CategoryEncoding(\n-            num_tokens=num_tokens,\n-            output_mode=output_mode,\n-            **kwargs,\n-        )\n         self._allow_non_tensor_positional_args = True\n         self._convert_input_args = False\n \n+    def _count(self, inputs, axis=-1):\n+        reduction_axis = 1 if len(inputs.shape) > 1 else 0\n+        outputs = self.backend.numpy.sum(\n+            self.backend.nn.one_hot(\n+                inputs, self.num_tokens, axis=axis, dtype=self.dtype\n+            ),\n+            axis=reduction_axis,\n+        )\n+        return outputs\n+\n+    def _encode(self, inputs):\n+        if self.output_mode == \"multi_hot\":\n+            outputs = self.backend.nn.multi_hot(\n+                inputs, self.num_tokens, dtype=self.dtype\n+            )\n+        elif self.output_mode == \"one_hot\":\n+            outputs = self.backend.nn.one_hot(\n+                inputs, self.num_tokens, dtype=self.dtype\n+            )\n+        elif self.output_mode == \"count\":\n+            outputs = self._count(inputs)\n+\n+        return outputs\n+\n     def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+        return tuple(input_shape + (self.num_tokens,))\n \n     def get_config(self):\n         config = {\n@@ -132,9 +144,10 @@ class CategoryEncoding(Layer):\n         return {**base_config, **config}\n \n     def call(self, inputs):\n-        outputs = self.layer.call(inputs)\n+        outputs = self._encode(inputs)\n+\n         if (\n-            backend.backend() != \"tensorflow\"\n+            self.backend._backend != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n         ):\n             outputs = backend.convert_to_tensor(outputs)\n\n@@ -6,9 +6,19 @@ from keras_core import testing\n \n \n class CategoryEncodingTest(testing.TestCase):\n-    # TODO: test coverage is insufficient.\n-    # Must test > 1 batch size, unbatch cases separately\n     def test_count_output(self):\n+        input_array = np.array([1, 2, 3, 1])\n+        expected_output = np.array([0, 2, 1, 1, 0, 0])\n+\n+        num_tokens = 6\n+        expected_output_shape = (num_tokens,)\n+\n+        layer = layers.CategoryEncoding(num_tokens=6, output_mode=\"count\")\n+        int_data = layer(input_array)\n+        self.assertEqual(expected_output_shape, int_data.shape)\n+        self.assertAllClose(int_data, expected_output)\n+\n+    def test_batched_count_output(self):\n         input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n         expected_output = np.array([[0, 2, 1, 1, 0, 0], [2, 1, 0, 1, 0, 0]])\n \n@@ -34,6 +44,20 @@ class CategoryEncodingTest(testing.TestCase):\n         self.assertAllClose(expected_output, output_data)\n         self.assertEqual(expected_output_shape, output_data.shape)\n \n+    def test_batched_multi_hot(self):\n+        input_data = np.array([[3, 2, 0, 1], [3, 2, 0, 1]])\n+        expected_output = np.array([[1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 0, 0]])\n+        num_tokens = 6\n+        expected_output_shape = (2, num_tokens)\n+\n+        # Test call on layer directly.\n+        layer = layers.CategoryEncoding(\n+            num_tokens=num_tokens, output_mode=\"multi_hot\"\n+        )\n+        output_data = layer(input_data)\n+        self.assertAllClose(expected_output, output_data)\n+        self.assertEqual(expected_output_shape, output_data.shape)\n+\n     def test_one_hot(self):\n         input_data = np.array([3, 2, 0, 1])\n         expected_output = np.array(\n@@ -55,8 +79,39 @@ class CategoryEncodingTest(testing.TestCase):\n         self.assertAllClose(expected_output, output_data)\n         self.assertEqual(expected_output_shape, output_data.shape)\n \n+    def test_batched_one_hot(self):\n+        input_data = np.array([[3, 2, 0, 1], [3, 2, 0, 1]])\n+        expected_output = np.array(\n+            [\n+                [\n+                    [0, 0, 0, 1],\n+                    [0, 0, 1, 0],\n+                    [1, 0, 0, 0],\n+                    [0, 1, 0, 0],\n+                ],\n+                [\n+                    [0, 0, 0, 1],\n+                    [0, 0, 1, 0],\n+                    [1, 0, 0, 0],\n+                    [0, 1, 0, 0],\n+                ],\n+            ]\n+        )\n+        num_tokens = 4\n+        expected_output_shape = (2, num_tokens, num_tokens)\n+\n+        # Test call on layer directly.\n+        layer = layers.CategoryEncoding(\n+            num_tokens=num_tokens, output_mode=\"one_hot\"\n+        )\n+        output_data = layer(input_data)\n+        self.assertAllClose(expected_output, output_data)\n+        self.assertEqual(expected_output_shape, output_data.shape)\n+\n     def test_tf_data_compatibility(self):\n-        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"one_hot\")\n+        layer = layers.CategoryEncoding(\n+            num_tokens=4, output_mode=\"one_hot\", dtype=\"int32\"\n+        )\n         input_data = np.array([3, 2, 0, 1])\n         expected_output = np.array(\n             [\n\n@@ -1198,3 +1198,51 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     return backend.nn.sparse_categorical_crossentropy(\n         target, output, from_logits=from_logits, axis=axis\n     )\n+\n+\n+class MultiHot(Operation):\n+    def __init__(self, num_tokens=None, axis=-1, dtype=None, name=None):\n+        super().__init__(name)\n+        self.num_tokens = num_tokens\n+        self.axis = axis\n+        self.dtype = dtype or backend.floatx()\n+\n+    def call(self, inputs):\n+        return backend.nn.multi_hot(\n+            inputs,\n+            num_classes=self.num_tokens,\n+            axis=self.axis,\n+            dtype=self.dtype,\n+        )\n+\n+    def compute_output_spec(self, inputs):\n+        x_shape = list(getattr(inputs, \"shape\", []))\n+        if self.axis == -1:\n+            x_shape.append(self.num_tokens)\n+        elif self.axis >= 0 and self.axis < len(x_shape):\n+            x_shape.insert(self.axis, self.num_tokens)\n+        else:\n+            raise ValueError(\n+                f\"axis must be -1 or between [0, {len(inputs.shape)}), but \"\n+                f\"received {self.axis}.\"\n+            )\n+\n+        if len(x_shape) == 2:\n+            x_shape = [x_shape[-1]]\n+        else:\n+            x_shape = [x_shape[0]] + x_shape[2:]\n+\n+        return KerasTensor(x_shape, dtype=inputs.dtype)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.ops.multi_hot\",\n+        \"keras_core.ops.nn.multi_hot\",\n+    ]\n+)\n+def multi_hot(inputs, num_tokens, axis=-1, dtype=None):\n+    if any_symbolic_tensors((inputs,)):\n+        return MultiHot(num_tokens, axis, dtype).symbolic_call(inputs)\n+\n+    return backend.nn.multi_hot(inputs, num_tokens, axis, dtype)\n\n@@ -108,6 +108,19 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n             (None, 4, None, 3),\n         )\n \n+    def test_multi_hot(self):\n+        x = KerasTensor([None, 3, 1])\n+        self.assertEqual(knn.multi_hot(x, 5).shape, (None, 1, 5))\n+        self.assertEqual(knn.multi_hot(x, 5, 1).shape, (None, 3, 1))\n+        self.assertEqual(knn.multi_hot(x, 5, 2).shape, (None, 5, 1))\n+\n+    @parameterized.product(dtype=[\"float32\", \"int32\"])\n+    def test_multi_hot_dtype(self, dtype):\n+        # dtype tests\n+        x = np.arange(5)\n+        out = knn.multi_hot(x, 5, axis=0, dtype=dtype)\n+        self.assertEqual(backend.standardize_dtype(out.dtype), dtype)\n+\n     def test_conv(self):\n         # Test 1D conv.\n         inputs_1d = KerasTensor([None, 20, 3])\n@@ -551,6 +564,18 @@ class NNOpsStaticShapeTest(testing.TestCase):\n             (2, 21, 21, 5),\n         )\n \n+    def test_batched_and_unbatched_inputs_multi_hot(self):\n+        x = KerasTensor([2, 3, 1])\n+        unbatched_input = KerasTensor(\n+            [\n+                5,\n+            ]\n+        )\n+        self.assertEqual(knn.multi_hot(unbatched_input, 5, -1).shape, (5,))\n+        self.assertEqual(knn.multi_hot(x, 5).shape, (2, 1, 5))\n+        self.assertEqual(knn.multi_hot(x, 5, 1).shape, (2, 3, 1))\n+        self.assertEqual(knn.multi_hot(x, 5, 2).shape, (2, 5, 1))\n+\n     def test_one_hot(self):\n         x = KerasTensor([2, 3, 1])\n         self.assertEqual(knn.one_hot(x, 5).shape, (2, 3, 1, 5))\n@@ -1115,3 +1140,19 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             target, output, from_logits=True\n         )\n         self.assertAllClose(result, [0.001822, 0.000459, 0.169846])\n+\n+    def test_multi_hot(self):\n+        # Test 1D multi-hot.\n+        indices_1d = np.array([0, 1, 2, 3])\n+        expected_output_1d = np.array([1, 1, 1, 1])\n+        self.assertAllClose(knn.multi_hot(indices_1d, 4), expected_output_1d)\n+\n+        # Test 2D multi-hot.\n+        indices_2d = np.array([[0, 1], [2, 3]])\n+        expected_output_2d = np.array([[1, 1, 0, 0], [0, 0, 1, 1]])\n+        self.assertAllClose(knn.multi_hot(indices_2d, 4), expected_output_2d)\n+\n+        # Test 1D multi-hot with negative inputs\n+        indices_1d = np.array([0, -1, -1, 3])\n+        expected_output_1d = np.array([1, 0, 0, 1])\n+        self.assertAllClose(knn.multi_hot(indices_1d, 4), expected_output_1d)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07e72f477d8a5350107f247a87d8a8b431b9288c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 127 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 127 | Churn Cumulative: 127 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,127 @@\n+import tree\n+\n+from keras_core import testing\n+from keras_core.backend import KerasTensor\n+from keras_core.ops.symbolic_arguments import SymbolicArguments\n+\n+\n+class SymbolicArgumentsTest(testing.TestCase):\n+    # Testing multiple args and empty kwargs\n+    def test_args(self):\n+        shape = (2, 3, 4)\n+        a = KerasTensor(shape=shape)\n+        b = KerasTensor(shape=shape)\n+        args = SymbolicArguments(\n+            (\n+                a,\n+                b,\n+            ),\n+            {},\n+        )\n+\n+        self.assertEqual(args.keras_tensors, [a, b])\n+        self.assertEqual(args._flat_arguments, [a, b])\n+        self.assertEqual(args._single_positional_tensor, None)\n+\n+    # Testing single arg and single position tensor\n+    def test_args_single_arg(self):\n+        shape = (2, 3, 4)\n+        a = KerasTensor(shape=shape)\n+        args = SymbolicArguments((a))\n+\n+        self.assertEqual(args.keras_tensors, [a])\n+        self.assertEqual(args._flat_arguments, [a])\n+        self.assertEqual(len(args.kwargs), 0)\n+        self.assertEqual(isinstance(args.args[0], KerasTensor), True)\n+        self.assertEqual(args._single_positional_tensor, a)\n+\n+    # Testing kwargs\n+    def test_kwargs(self):\n+        shape = (2, 3, 4)\n+        a = KerasTensor(shape=shape)\n+        b = KerasTensor(shape=shape)\n+        c = KerasTensor(shape=shape)\n+        args = SymbolicArguments(\n+            (\n+                a,\n+                b,\n+            ),\n+            {1: c},\n+        )\n+\n+        self.assertEqual(args.keras_tensors, [a, b, c])\n+        self.assertEqual(args._flat_arguments, [a, b, c])\n+        self.assertEqual(args._single_positional_tensor, None)\n+\n+    # Testing conversion function with args and kwargs\n+    def test_conversion_fn(self):\n+        shape = (2, 3, 4)\n+        a = KerasTensor(shape=shape)\n+        b = KerasTensor(shape=shape)\n+        c = KerasTensor(shape=shape)\n+        sym_args = SymbolicArguments(\n+            (\n+                a,\n+                b,\n+            ),\n+            {1: c},\n+        )\n+\n+        (value, _) = sym_args.convert(lambda x: x**2)\n+        args1 = value[0][0]\n+        args2 = value[0][1]\n+\n+        self.assertIsInstance(args1, KerasTensor)\n+        self.assertNotEqual(args1, a)\n+        self.assertNotEqual(args2, b)\n+\n+        mapped_value = tree.map_structure(lambda x: x**2, a)\n+        self.assertEqual(mapped_value.shape, args1.shape)\n+        self.assertEqual(mapped_value.dtype, args1.dtype)\n+\n+    # Testing fill in function with single args only\n+    def test_fill_in_single_arg(self):\n+        shape = (2, 3, 4)\n+        a = KerasTensor(shape=shape)\n+\n+        tensor_dict = {id(a): 3}\n+        sym_args = SymbolicArguments((a))\n+\n+        # Call the method to be tested\n+        result, _ = sym_args.fill_in(tensor_dict)\n+\n+        self.assertEqual(result, (3,))\n+\n+    # Testing fill in function with multiple args\n+    def test_fill_in_multiple_arg(self):\n+        shape = (2, 3, 4)\n+        a = KerasTensor(shape=shape)\n+        b = KerasTensor(shape=shape)\n+\n+        tensor_dict = {id(b): 2}\n+        sym_args = SymbolicArguments((a, b))\n+\n+        # Call the method to be tested\n+        result, _ = sym_args.fill_in(tensor_dict)\n+\n+        self.assertEqual(result, ((a, 2),))\n+\n+    # Testing fill in function for args and kwargs\n+    def test_fill_in(self):\n+        shape1 = (2, 3, 4)\n+        shape2 = (3, 2, 4)\n+        a = KerasTensor(shape=shape1)\n+        b = KerasTensor(shape=shape2)\n+        c = KerasTensor(shape=shape2)\n+        dictionary = {id(a): 3, id(c): 2}\n+        sym_args = SymbolicArguments(\n+            (\n+                a,\n+                b,\n+            ),\n+            {1: c},\n+        )\n+\n+        (values, _) = sym_args.fill_in(dictionary)\n+\n+        self.assertEqual(values, ((3, b), {1: 2}))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb95017e6a7f02e77f408a9def0d0f4d225f75f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 327 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 11 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 327 | Churn Cumulative: 327 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,327 @@\n+\"\"\"\n+Title: Multiclass semantic segmentation using DeepLabV3+\n+Author: [Soumik Rakshit](http://github.com/soumik12345)\n+Converted to Keras Core: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 2021/08/31\n+Last modified: 2023/07/19\n+Description: Implement DeepLabV3+ architecture for Multi-class Semantic Segmentation.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+Semantic segmentation, with the goal to assign semantic labels to every pixel in an image,\n+is an essential computer vision task. In this example, we implement\n+the **DeepLabV3+** model for multi-class semantic segmentation, a fully-convolutional\n+architecture that performs well on semantic segmentation benchmarks.\n+\n+### References:\n+\n+- [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/pdf/1802.02611.pdf)\n+- [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)\n+- [DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://arxiv.org/abs/1606.00915)\n+\"\"\"\n+\n+\"\"\"\n+## Downloading the data\n+\n+We will use the [Crowd Instance-level Human Parsing Dataset](https://arxiv.org/abs/1811.12596)\n+for training our model. The Crowd Instance-level Human Parsing (CIHP) dataset has 38,280 diverse human images.\n+Each image in CIHP is labeled with pixel-wise annotations for 20 categories, as well as instance-level identification.\n+This dataset can be used for the \"human part segmentation\" task.\n+\"\"\"\n+\n+\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import ops\n+\n+import cv2\n+import numpy as np\n+from glob import glob\n+from scipy.io import loadmat\n+import matplotlib.pyplot as plt\n+from tensorflow import image as tf_image\n+from tensorflow import data as tf_data\n+from tensorflow import io as tf_io\n+\n+\"\"\"shell\n+gdown \"1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz&confirm=t\"\n+unzip -q instance-level-human-parsing.zip\n+\"\"\"\n+\n+\"\"\"\n+## Creating a TensorFlow Dataset\n+\n+Training on the entire CIHP dataset with 38,280 images takes a lot of time, hence we will be using\n+a smaller subset of 200 images for training our model in this example.\n+\"\"\"\n+\n+IMAGE_SIZE = 512\n+BATCH_SIZE = 4\n+NUM_CLASSES = 20\n+DATA_DIR = \"./instance-level_human_parsing/instance-level_human_parsing/Training\"\n+NUM_TRAIN_IMAGES = 1000\n+NUM_VAL_IMAGES = 50\n+\n+train_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[:NUM_TRAIN_IMAGES]\n+train_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[:NUM_TRAIN_IMAGES]\n+val_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[\n+    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n+]\n+val_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[\n+    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n+]\n+\n+\n+def read_image(image_path, mask=False):\n+    image = tf_io.read_file(image_path)\n+    if mask:\n+        image = tf_image.decode_png(image, channels=1)\n+        image.set_shape([None, None, 1])\n+        image = tf_image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n+    else:\n+        image = tf_image.decode_png(image, channels=3)\n+        image.set_shape([None, None, 3])\n+        image = tf_image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n+    return image\n+\n+\n+def load_data(image_list, mask_list):\n+    image = read_image(image_list)\n+    mask = read_image(mask_list, mask=True)\n+    return image, mask\n+\n+\n+def data_generator(image_list, mask_list):\n+    dataset = tf_data.Dataset.from_tensor_slices((image_list, mask_list))\n+    dataset = dataset.map(load_data, num_parallel_calls=tf_data.AUTOTUNE)\n+    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n+    return dataset\n+\n+\n+train_dataset = data_generator(train_images, train_masks)\n+val_dataset = data_generator(val_images, val_masks)\n+\n+print(\"Train Dataset:\", train_dataset)\n+print(\"Val Dataset:\", val_dataset)\n+\n+\"\"\"\n+## Building the DeepLabV3+ model\n+\n+DeepLabv3+ extends DeepLabv3 by adding an encoder-decoder structure. The encoder module\n+processes multiscale contextual information by applying dilated convolution at multiple\n+scales, while the decoder module refines the segmentation results along object boundaries.\n+\n+![](https://github.com/lattice-ai/DeepLabV3-Plus/raw/master/assets/deeplabv3_plus_diagram.png)\n+\n+**Dilated convolution:** With dilated convolution, as we go deeper in the network, we can keep the\n+stride constant but with larger field-of-view without increasing the number of parameters\n+or the amount of computation. Besides, it enables larger output feature maps, which is\n+useful for semantic segmentation.\n+\n+The reason for using **Dilated Spatial Pyramid Pooling** is that it was shown that as the\n+sampling rate becomes larger, the number of valid filter weights (i.e., weights that\n+are applied to the valid feature region, instead of padded zeros) becomes smaller.\n+\"\"\"\n+\n+\n+def convolution_block(\n+    block_input,\n+    num_filters=256,\n+    kernel_size=3,\n+    dilation_rate=1,\n+    padding=\"same\",\n+    use_bias=False,\n+):\n+    x = layers.Conv2D(\n+        num_filters,\n+        kernel_size=kernel_size,\n+        dilation_rate=dilation_rate,\n+        padding=\"same\",\n+        use_bias=use_bias,\n+        kernel_initializer=keras.initializers.HeNormal(),\n+    )(block_input)\n+    x = layers.BatchNormalization()(x)\n+    return ops.nn.relu(x)\n+\n+\n+def DilatedSpatialPyramidPooling(dspp_input):\n+    dims = dspp_input.shape\n+    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n+    x = convolution_block(x, kernel_size=1, use_bias=True)\n+    out_pool = layers.UpSampling2D(\n+        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]),\n+        interpolation=\"bilinear\",\n+    )(x)\n+\n+    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n+    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n+    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n+    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n+\n+    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n+    output = convolution_block(x, kernel_size=1)\n+    return output\n+\n+\n+\"\"\"\n+The encoder features are first bilinearly upsampled by a factor 4, and then\n+concatenated with the corresponding low-level features from the network backbone that\n+have the same spatial resolution. For this example, we\n+use a ResNet50 pretrained on ImageNet as the backbone model, and we use\n+the low-level features from the `conv4_block6_2_relu` block of the backbone.\n+\"\"\"\n+\n+\n+def DeeplabV3Plus(image_size, num_classes):\n+    model_input = keras.Input(shape=(image_size, image_size, 3))\n+    preprocessed = keras.applications.resnet50.preprocess_input(model_input)\n+    resnet50 = keras.applications.ResNet50(\n+        weights=\"imagenet\", include_top=False, input_tensor=preprocessed\n+    )\n+    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n+    x = DilatedSpatialPyramidPooling(x)\n+\n+    input_a = layers.UpSampling2D(\n+        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n+        interpolation=\"bilinear\",\n+    )(x)\n+    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n+    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n+\n+    x = layers.Concatenate(axis=-1)([input_a, input_b])\n+    x = convolution_block(x)\n+    x = convolution_block(x)\n+    x = layers.UpSampling2D(\n+        size=(image_size // x.shape[1], image_size // x.shape[2]),\n+        interpolation=\"bilinear\",\n+    )(x)\n+    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n+    return keras.Model(inputs=model_input, outputs=model_output)\n+\n+\n+model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n+model.summary()\n+\n+\"\"\"\n+## Training\n+\n+We train the model using sparse categorical crossentropy as the loss function, and\n+Adam as the optimizer.\n+\"\"\"\n+\n+loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n+model.compile(\n+    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n+    loss=loss,\n+    metrics=[\"accuracy\"],\n+)\n+\n+history = model.fit(train_dataset, validation_data=val_dataset, epochs=25)\n+\n+plt.plot(history.history[\"loss\"])\n+plt.title(\"Training Loss\")\n+plt.ylabel(\"loss\")\n+plt.xlabel(\"epoch\")\n+plt.show()\n+\n+plt.plot(history.history[\"accuracy\"])\n+plt.title(\"Training Accuracy\")\n+plt.ylabel(\"accuracy\")\n+plt.xlabel(\"epoch\")\n+plt.show()\n+\n+plt.plot(history.history[\"val_loss\"])\n+plt.title(\"Validation Loss\")\n+plt.ylabel(\"val_loss\")\n+plt.xlabel(\"epoch\")\n+plt.show()\n+\n+plt.plot(history.history[\"val_accuracy\"])\n+plt.title(\"Validation Accuracy\")\n+plt.ylabel(\"val_accuracy\")\n+plt.xlabel(\"epoch\")\n+plt.show()\n+\n+\"\"\"\n+## Inference using Colormap Overlay\n+\n+The raw predictions from the model represent a one-hot encoded tensor of shape `(N, 512, 512, 20)`\n+where each one of the 20 channels is a binary mask corresponding to a predicted label.\n+In order to visualize the results, we plot them as RGB segmentation masks where each pixel\n+is represented by a unique color corresponding to the particular label predicted. We can easily\n+find the color corresponding to each label from the `human_colormap.mat` file provided as part\n+of the dataset. We would also plot an overlay of the RGB segmentation mask on the input image as\n+this further helps us to identify the different categories present in the image more intuitively.\n+\"\"\"\n+\n+# Loading the Colormap\n+colormap = loadmat(\n+    \"./instance-level_human_parsing/instance-level_human_parsing/human_colormap.mat\"\n+)[\"colormap\"]\n+colormap = colormap * 100\n+colormap = colormap.astype(np.uint8)\n+\n+\n+def infer(model, image_tensor):\n+    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n+    predictions = np.squeeze(predictions)\n+    predictions = np.argmax(predictions, axis=2)\n+    return predictions\n+\n+\n+def decode_segmentation_masks(mask, colormap, n_classes):\n+    r = np.zeros_like(mask).astype(np.uint8)\n+    g = np.zeros_like(mask).astype(np.uint8)\n+    b = np.zeros_like(mask).astype(np.uint8)\n+    for l in range(0, n_classes):\n+        idx = mask == l\n+        r[idx] = colormap[l, 0]\n+        g[idx] = colormap[l, 1]\n+        b[idx] = colormap[l, 2]\n+    rgb = np.stack([r, g, b], axis=2)\n+    return rgb\n+\n+\n+def get_overlay(image, colored_mask):\n+    image = keras.utils.array_to_img(image)\n+    image = np.array(image).astype(np.uint8)\n+    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n+    return overlay\n+\n+\n+def plot_samples_matplotlib(display_list, figsize=(5, 3)):\n+    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n+    for i in range(len(display_list)):\n+        if display_list[i].shape[-1] == 3:\n+            axes[i].imshow(keras.utils.array_to_img(display_list[i]))\n+        else:\n+            axes[i].imshow(display_list[i])\n+    plt.show()\n+\n+\n+def plot_predictions(images_list, colormap, model):\n+    for image_file in images_list:\n+        image_tensor = read_image(image_file)\n+        prediction_mask = infer(image_tensor=image_tensor, model=model)\n+        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 20)\n+        overlay = get_overlay(image_tensor, prediction_colormap)\n+        plot_samples_matplotlib(\n+            [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n+        )\n+\n+\n+\"\"\"\n+### Inference on Train Images\n+\"\"\"\n+\n+plot_predictions(train_images[:4], colormap, model=model)\n+\n+\"\"\"\n+### Inference on Validation Images\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/deeplabv3p-resnet50)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Human-Part-Segmentation).\n+\"\"\"\n+\n+plot_predictions(val_images[:4], colormap, model=model)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0e0cc4009fd37b4c5ee057aaf6be0b9b25c2fe7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9d4de0ac24c5842112e057a21bf14d90b61bcc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 30 | Files Changed: 12 | Hunks: 26 | Methods Changed: 23 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 67 | Churn Cumulative: 15240 | Contributors (this commit): 22 | Commits (past 90d): 394 | Contributors (cumulative): 102 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ class JAXTrainer(base_trainer.Trainer):\n     ):\n         \"\"\"This method is stateless and is intended for use with jax.grad.\"\"\"\n         kwargs = {}\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             kwargs[\"training\"] = training\n         y_pred, non_trainable_variables, losses = self.stateless_call(\n             trainable_variables,\n@@ -167,7 +167,7 @@ class JAXTrainer(base_trainer.Trainer):\n     def predict_step(self, state, data):\n         trainable_variables, non_trainable_variables = state\n         kwargs = {}\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             kwargs[\"training\"] = False\n \n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n\n@@ -48,7 +48,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n         # Forward pass\n         with tf.GradientTape() as tape:\n-            if self._call_has_training_arg():\n+            if self._call_has_training_arg:\n                 y_pred = self(x, training=True)\n             else:\n                 y_pred = self(x)\n@@ -71,7 +71,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n     def test_step(self, data):\n         x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n@@ -83,7 +83,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n     def predict_step(self, data):\n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n\n@@ -126,9 +126,12 @@ def _get_concrete_noise_shape(inputs, noise_shape):\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n+    if noise_shape is not None:\n         keep_prob = 1.0 - rate\n         noise_shape = _get_concrete_noise_shape(inputs, noise_shape)\n-    keep_prob_matrix = torch.full(noise_shape, keep_prob, device=get_device())\n+        keep_prob_matrix = torch.full(\n+            noise_shape, keep_prob, device=get_device()\n+        )\n         generator = torch_seed_generator(seed)\n \n         # Do not use generator during symbolic execution.\n@@ -140,5 +143,12 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n         mask = mask.bool()\n         mask = torch.broadcast_to(mask, inputs.shape)\n         return torch.where(\n-        mask, inputs / keep_prob, torch.zeros_like(inputs, dtype=inputs.dtype)\n+            mask,\n+            inputs / keep_prob,\n+            torch.zeros_like(inputs, dtype=inputs.dtype),\n+        )\n+    # Fast path, unseeded (since torch doesn't support seeding dropout!!!!)\n+    # Using the above implementation is possible, but much slower.\n+    return torch.nn.functional.dropout(\n+        inputs, p=rate, training=True, inplace=False\n     )\n\n@@ -27,7 +27,7 @@ class TorchTrainer(base_trainer.Trainer):\n         x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n \n         # Compute predictions\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             y_pred = self(x, training=True)\n         else:\n             y_pred = self(x)\n@@ -64,7 +64,7 @@ class TorchTrainer(base_trainer.Trainer):\n             y,\n             sample_weight,\n         ) = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n@@ -76,7 +76,7 @@ class TorchTrainer(base_trainer.Trainer):\n \n     def predict_step(self, data):\n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n\n@@ -250,9 +250,12 @@ class Layer(BackendLayer, Operation):\n         self._losses = []\n         self._loss_ids = set()\n \n-        self._call_signature_parameters = [\n+        call_signature_parameters = [\n             p.name for p in inspect.signature(self.call).parameters.values()\n         ]\n+        self._call_has_training_arg = \"training\" in call_signature_parameters\n+        self._call_has_mask_arg = \"mask\" in call_signature_parameters\n+\n         self._supports_masking = not utils.is_default(self.compute_mask)\n         # Whether to automatically convert (+ auto-cast) inputs to `call()`.\n         self._convert_input_args = True\n@@ -693,7 +696,7 @@ class Layer(BackendLayer, Operation):\n                 # Get signature default value\n                 training = call_spec.arguments_dict.get(\"training\", None)\n         call_context.training = training\n-        if self._call_has_training_arg() and training is not None:\n+        if self._call_has_training_arg and training is not None:\n             # Only populate arg if it has a concrete value\n             kwargs[\"training\"] = training\n \n@@ -1191,12 +1194,6 @@ class Layer(BackendLayer, Operation):\n                 self.input_spec, arg_0, layer_name=self.name\n             )\n \n-    def _call_has_training_arg(self):\n-        return \"training\" in self._call_signature_parameters\n-\n-    def _call_has_mask_arg(self):\n-        return \"mask\" in self._call_signature_parameters\n-\n     def _get_call_context(self):\n         \"\"\"Returns currently active `CallContext`.\"\"\"\n         layer_call_ctx = global_state.get_global_attribute(\"current_call_ctx\")\n\n@@ -141,7 +141,7 @@ class LayerTest(testing.TestCase):\n                 self.seed_gen = backend.random.SeedGenerator(seed=1337)\n \n             def call(self, x):\n-                return backend.random.dropout(x, rate=0.5, seed=self.seed_gen)\n+                return x * backend.random.normal(x.shape, seed=self.seed_gen)\n \n         layer = RNGLayer()\n         self.assertEqual(layer.variables, [layer.seed_gen.state])\n@@ -158,8 +158,8 @@ class LayerTest(testing.TestCase):\n                 self.seed_gens.append(backend.random.SeedGenerator(seed=10))\n \n             def call(self, x):\n-                x = backend.random.dropout(x, rate=0.5, seed=self.seed_gens[0])\n-                x = backend.random.dropout(x, rate=0.5, seed=self.seed_gens[1])\n+                x = x * backend.random.normal(x.shape, seed=self.seed_gens[0])\n+                x = x * backend.random.normal(x.shape, seed=self.seed_gens[1])\n                 return x\n \n         layer = RNGListLayer()\n\n@@ -198,9 +198,9 @@ class Bidirectional(Wrapper):\n         training=None,\n     ):\n         kwargs = {}\n-        if self.forward_layer._call_has_training_arg():\n+        if self.forward_layer._call_has_training_arg:\n             kwargs[\"training\"] = training\n-        if self.forward_layer._call_has_mask_arg():\n+        if self.forward_layer._call_has_mask_arg:\n             kwargs[\"mask\"] = mask\n \n         if initial_state is not None:\n\n@@ -323,7 +323,7 @@ class RNN(Layer):\n \n     def inner_loop(self, sequences, initial_state, mask, training=False):\n         cell_kwargs = {}\n-        if isinstance(self.cell, Layer) and self.cell._call_has_training_arg():\n+        if isinstance(self.cell, Layer) and self.cell._call_has_training_arg:\n             cell_kwargs[\"training\"] = training\n \n         def step(inputs, states):\n\n@@ -90,7 +90,7 @@ class StackedRNNCells(Layer):\n         new_states = []\n         for cell, states in zip(self.cells, states):\n             states = list(states) if tree.is_nested(states) else [states]\n-            if isinstance(cell, Layer) and cell._call_has_training_arg():\n+            if isinstance(cell, Layer) and cell._call_has_training_arg:\n                 kwargs[\"training\"] = training\n             else:\n                 kwargs.pop(\"training\", None)\n\n@@ -95,9 +95,9 @@ class TimeDistributed(Wrapper):\n \n         def step_function(i):\n             kwargs = {}\n-            if self.layer._call_has_mask_arg() and mask is not None:\n+            if self.layer._call_has_mask_arg and mask is not None:\n                 kwargs[\"mask\"] = mask[i]\n-            if self.layer._call_has_training_arg():\n+            if self.layer._call_has_training_arg:\n                 kwargs[\"training\"] = training\n             return self.layer.call(inputs[i], **kwargs)\n \n\n@@ -542,7 +542,7 @@ def operation_fn(operation, training):\n     def call(*args, **kwargs):\n         if (\n             hasattr(operation, \"_call_has_training_arg\")\n-            and operation._call_has_training_arg()\n+            and operation._call_has_training_arg\n             and training is not None\n         ):\n             kwargs[\"training\"] = training\n\n@@ -183,9 +183,9 @@ class Sequential(Model):\n             # end of each iteration `inputs` is set to `outputs` to prepare for\n             # the next layer.\n             kwargs = {}\n-            if layer._call_has_mask_arg():\n+            if layer._call_has_mask_arg:\n                 kwargs[\"mask\"] = mask\n-            if layer._call_has_training_arg() and training is not None:\n+            if layer._call_has_training_arg and training is not None:\n                 kwargs[\"training\"] = training\n             outputs = layer(inputs, **kwargs)\n             inputs = outputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#18c8114ac456c9c8a96830ea741be104441b4fbf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 590 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -354,7 +354,7 @@ class Amax(Operation):\n @keras_core_export([\"keras_core.ops.amax\", \"keras_core.ops.numpy.amax\"])\n def amax(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n-        return All(axis=axis, keepdims=keepdims).symbolic_call(x)\n+        return Amax(axis=axis, keepdims=keepdims).symbolic_call(x)\n     return backend.numpy.amax(x, axis=axis, keepdims=keepdims)\n \n \n@@ -379,7 +379,7 @@ class Amin(Operation):\n @keras_core_export([\"keras_core.ops.amin\", \"keras_core.ops.numpy.amin\"])\n def amin(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n-        return All(axis=axis, keepdims=keepdims).symbolic_call(x)\n+        return Amin(axis=axis, keepdims=keepdims).symbolic_call(x)\n     return backend.numpy.amin(x, axis=axis, keepdims=keepdims)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be95d85049908a09a9babba14dccb359cf1b89f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 271 | Lines Deleted: 98 | Files Changed: 10 | Hunks: 82 | Methods Changed: 36 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 369 | Churn Cumulative: 8046 | Contributors (this commit): 8 | Commits (past 90d): 32 | Contributors (cumulative): 24 | DMM Complexity: 0.9829059829059829\n\nDIFF:\n@@ -157,7 +157,12 @@ devices = mesh_utils.create_device_mesh((8,))\n # data will be split along the batch axis\n data_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh\n # naming axes of the sharded partition\n-data_sharding = NamedSharding(data_mesh,P(\"batch\",),)\n+data_sharding = NamedSharding(\n+    data_mesh,\n+    P(\n+        \"batch\",\n+    ),\n+)\n # all variables will be replicated on all devices\n var_mesh = Mesh(devices, axis_names=(\"_\"))\n # in NamedSharding, axes that are not mentioned are replicated (all axes here)\n\n@@ -40,6 +40,7 @@ installed `ffmpg`.\n \"\"\"\n \n import os\n+\n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import shutil\n@@ -53,7 +54,9 @@ from IPython.display import display, Audio\n \n # Get the data from https://www.kaggle.com/kongaevans/speaker-recognition-dataset/download\n # and save it to the 'Downloads' folder in your HOME directory\n-DATASET_ROOT = os.path.join(os.path.expanduser(\"~\"), \"Downloads/16000_pcm_speeches\")\n+DATASET_ROOT = os.path.join(\n+    os.path.expanduser(\"~\"), \"Downloads/16000_pcm_speeches\"\n+)\n \n # The folders in which we will put the audio samples and the noise samples\n AUDIO_SUBFOLDER = \"audio\"\n@@ -265,8 +268,12 @@ def add_noise(audio, noises=None, scale=0.5):\n         noise = tf.gather(noises, tf_rnd, axis=0)\n \n         # Get the amplitude proportion between the audio and the noise\n-        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n-        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n+        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(\n+            noise, axis=1\n+        )\n+        prop = tf.repeat(\n+            tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1\n+        )\n \n         # Adding the rescaled noise to audio\n         audio = audio + noise * prop * scale\n@@ -316,7 +323,9 @@ for label, name in enumerate(class_names):\n     labels += [label] * len(speaker_sample_paths)\n \n print(\n-    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n+    \"Found {} files belonging to {} classes.\".format(\n+        len(audio_paths), len(class_names)\n+    )\n )\n \n # Shuffle\n@@ -337,9 +346,9 @@ valid_labels = labels[-num_val_samples:]\n \n # Create 2 datasets, one for training and the other for validation\n train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n-train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n-    BATCH_SIZE\n-)\n+train_ds = train_ds.shuffle(\n+    buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED\n+).batch(BATCH_SIZE)\n \n valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n@@ -393,7 +402,9 @@ def build_model(input_shape, num_classes):\n     x = keras.layers.Dense(256, activation=\"relu\")(x)\n     x = keras.layers.Dense(128, activation=\"relu\")(x)\n \n-    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n+    outputs = keras.layers.Dense(\n+        num_classes, activation=\"softmax\", name=\"output\"\n+    )(x)\n \n     return keras.models.Model(inputs=inputs, outputs=outputs)\n \n@@ -404,7 +415,9 @@ model.summary()\n \n # Compile the model using Adam's default learning rate\n model.compile(\n-    optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n+    optimizer=\"Adam\",\n+    loss=\"sparse_categorical_crossentropy\",\n+    metrics=[\"accuracy\"],\n )\n \n # Add callbacks:\n@@ -412,7 +425,9 @@ model.compile(\n # 'ModelCheckPoint' to always keep the model that has the best val_accuracy\n model_save_filename = \"model.keras\"\n \n-earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n+earlystopping_cb = keras.callbacks.EarlyStopping(\n+    patience=10, restore_best_weights=True\n+)\n mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n     model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n )\n\n@@ -98,6 +98,7 @@ class_names = [\n \"\"\"\n \n import os\n+\n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import io\n@@ -118,7 +119,9 @@ from IPython.display import Audio\n keras.utils.set_random_seed(SEED)\n \n # Where to download the dataset\n-DATASET_DESTINATION = os.path.join(CACHE_DIR if CACHE_DIR else \"~/.keras/\", \"datasets\")\n+DATASET_DESTINATION = os.path.join(\n+    CACHE_DIR if CACHE_DIR else \"~/.keras/\", \"datasets\"\n+)\n \n \"\"\"\n ## Yamnet Model\n@@ -182,7 +185,9 @@ We will ignore the other two.\n \"\"\"\n \n dataframe = pd.read_csv(\n-    line_index_file, names=[\"id\", \"filename\", \"transcript\"], usecols=[\"filename\"]\n+    line_index_file,\n+    names=[\"id\", \"filename\", \"transcript\"],\n+    usecols=[\"filename\"],\n )\n dataframe.head()\n \n@@ -206,20 +211,26 @@ accent.\n # - Shuffle samples\n def preprocess_dataframe(dataframe):\n     # Remove leading space in filename column\n-    dataframe[\"filename\"] = dataframe.apply(lambda row: row[\"filename\"].strip(), axis=1)\n+    dataframe[\"filename\"] = dataframe.apply(\n+        lambda row: row[\"filename\"].strip(), axis=1\n+    )\n \n     # Create gender agnostic labels based on the filename first 2 letters\n     dataframe[\"label\"] = dataframe.apply(\n-        lambda row: gender_agnostic_categories.index(row[\"filename\"][:2]), axis=1\n+        lambda row: gender_agnostic_categories.index(row[\"filename\"][:2]),\n+        axis=1,\n     )\n \n     # Add the file path to the name\n     dataframe[\"filename\"] = dataframe.apply(\n-        lambda row: os.path.join(DATASET_DESTINATION, row[\"filename\"] + \".wav\"), axis=1\n+        lambda row: os.path.join(DATASET_DESTINATION, row[\"filename\"] + \".wav\"),\n+        axis=1,\n     )\n \n     # Shuffle the samples\n-    dataframe = dataframe.sample(frac=1, random_state=SEED).reset_index(drop=True)\n+    dataframe = dataframe.sample(frac=1, random_state=SEED).reset_index(\n+        drop=True\n+    )\n \n     return dataframe\n \n@@ -272,12 +283,16 @@ def load_16k_audio_wav(filename):\n     file_content = tf.io.read_file(filename)\n \n     # Decode audio wave\n-    audio_wav, sample_rate = tf.audio.decode_wav(file_content, desired_channels=1)\n+    audio_wav, sample_rate = tf.audio.decode_wav(\n+        file_content, desired_channels=1\n+    )\n     audio_wav = tf.squeeze(audio_wav, axis=-1)\n     sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n \n     # Resample to 16k\n-    audio_wav = tfio.audio.resample(audio_wav, rate_in=sample_rate, rate_out=16000)\n+    audio_wav = tfio.audio.resample(\n+        audio_wav, rate_in=sample_rate, rate_out=16000\n+    )\n \n     return audio_wav\n \n@@ -297,7 +312,9 @@ def filepath_to_embeddings(filename, label):\n     labels = tf.repeat(label, embeddings_num)\n \n     # Change labels for time-slots that are not speech into a new category 'other'\n-    labels = tf.where(tf.argmax(scores, axis=1) == 0, label, len(class_names) - 1)\n+    labels = tf.where(\n+        tf.argmax(scores, axis=1) == 0, label, len(class_names) - 1\n+    )\n \n     # Using one-hot in order to use AUC\n     return (embeddings, tf.one_hot(labels, len(class_names)))\n@@ -350,11 +367,13 @@ def build_and_compile_model():\n     x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_4\")(x)\n     x = keras.layers.Dropout(0.2, name=\"dropout_4\")(x)\n \n-    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\", name=\"ouput\")(\n-        x\n-    )\n+    outputs = keras.layers.Dense(\n+        len(class_names), activation=\"softmax\", name=\"ouput\"\n+    )(x)\n \n-    model = keras.Model(inputs=inputs, outputs=outputs, name=\"accent_recognition\")\n+    model = keras.Model(\n+        inputs=inputs, outputs=outputs, name=\"accent_recognition\"\n+    )\n \n     model.compile(\n         optimizer=keras.optimizers.Adam(learning_rate=1.9644e-5),\n@@ -529,7 +548,11 @@ confusion_mtx = tf.math.confusion_matrix(\n # Plot the confusion matrix\n plt.figure(figsize=(10, 8))\n sns.heatmap(\n-    confusion_mtx, xticklabels=class_names, yticklabels=class_names, annot=True, fmt=\"g\"\n+    confusion_mtx,\n+    xticklabels=class_names,\n+    yticklabels=class_names,\n+    annot=True,\n+    fmt=\"g\",\n )\n plt.xlabel(\"Prediction\")\n plt.ylabel(\"Label\")\n@@ -614,7 +637,9 @@ yamnet_class_names = yamnet_class_names_from_csv(\n \n def calculate_number_of_non_speech(scores):\n     number_of_non_speech = tf.math.reduce_sum(\n-        tf.where(tf.math.argmax(scores, axis=1, output_type=tf.int32) != 0, 1, 0)\n+        tf.where(\n+            tf.math.argmax(scores, axis=1, output_type=tf.int32) != 0, 1, 0\n+        )\n     )\n \n     return number_of_non_speech\n@@ -674,7 +699,10 @@ plt.xlim([0, len(audio_wav)])\n # Plot the log-mel spectrogram (returned by the model).\n plt.subplot(3, 1, 2)\n plt.imshow(\n-    mel_spectrogram.numpy().T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\"\n+    mel_spectrogram.numpy().T,\n+    aspect=\"auto\",\n+    interpolation=\"nearest\",\n+    origin=\"lower\",\n )\n \n # Plot and label the model output scores for the top-scoring classes.\n\n@@ -41,7 +41,8 @@ remains upright. The agent, therefore, must learn to keep the pole from falling\n \"\"\"\n \n import os\n-os.environ['KERAS_BACKEND'] = 'tensorflow'\n+\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import keras_core as keras\n from keras_core import layers\n@@ -57,7 +58,9 @@ gamma = 0.99  # Discount factor for past rewards\n max_steps_per_episode = 10000\n env = gym.make(\"CartPole-v1\", new_step_api=True)  # Create the environment\n env.reset(seed=seed)\n-eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n+eps = np.finfo(\n+    np.float32\n+).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n \n \"\"\"\n ## Implement Actor Critic network\n\n@@ -40,6 +40,7 @@ code snippets from another example,\n \"\"\"\n \n import os\n+\n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n from keras_core import layers\n@@ -145,7 +146,11 @@ class CCTTokenizer(layers.Layer):\n         # are flattened to form sequences.\n         reshaped = tf.reshape(\n             outputs,\n-            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n+            (\n+                -1,\n+                tf.shape(outputs)[1] * tf.shape(outputs)[2],\n+                tf.shape(outputs)[-1],\n+            ),\n         )\n         return reshaped\n \n@@ -166,6 +171,7 @@ class CCTTokenizer(layers.Layer):\n         else:\n             return None\n \n+\n \"\"\"\n ## Sequence Pooling\n Another recipe introduced in CCT is attention pooling or sequence pooling. In ViT, only\n@@ -173,6 +179,7 @@ the feature map corresponding to the class token is pooled and is then used for\n subsequent classification task (or any other downstream task).\n \"\"\"\n \n+\n class SequencePooling(layers.Layer):\n     def __init__(self):\n         super().__init__()\n@@ -253,8 +260,6 @@ this example, we do classification).\n \"\"\"\n \n \n-\n-\n def create_cct_model(\n     image_size=image_size,\n     input_shape=input_shape,\n\n@@ -104,22 +104,34 @@ def random_crop(low_image, enhanced_image):\n def load_data(low_light_image_path, enhanced_image_path):\n     low_light_image = read_image(low_light_image_path)\n     enhanced_image = read_image(enhanced_image_path)\n-    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n+    low_light_image, enhanced_image = random_crop(\n+        low_light_image, enhanced_image\n+    )\n     return low_light_image, enhanced_image\n \n \n def get_dataset(low_light_images, enhanced_images):\n-    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n+    dataset = tf.data.Dataset.from_tensor_slices(\n+        (low_light_images, enhanced_images)\n+    )\n     dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n     dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n     return dataset\n \n \n-train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n-train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[:MAX_TRAIN_IMAGES]\n+train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[\n+    :MAX_TRAIN_IMAGES\n+]\n+train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[\n+    :MAX_TRAIN_IMAGES\n+]\n \n-val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n-val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[MAX_TRAIN_IMAGES:]\n+val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[\n+    MAX_TRAIN_IMAGES:\n+]\n+val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[\n+    MAX_TRAIN_IMAGES:\n+]\n \n test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n test_enhanced_images = sorted(glob(\"./lol_dataset/eval15/high/*\"))\n@@ -238,6 +250,7 @@ then used to rescale the input feature map.\n ![](https://i.imgur.com/Dl0IwQs.png)\n \"\"\"\n \n+\n class ChannelPooling(layers.Layer):\n     def __init__(self, axis=-1, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -248,9 +261,7 @@ class ChannelPooling(layers.Layer):\n         average_pooling = tf.expand_dims(\n             tf.reduce_mean(inputs, axis=-1), axis=-1\n         )\n-        max_pooling = tf.expand_dims(\n-            tf.reduce_max(inputs, axis=-1), axis=-1\n-        )\n+        max_pooling = tf.expand_dims(tf.reduce_max(inputs, axis=-1), axis=-1)\n         return self.concat([average_pooling, max_pooling])\n \n     def get_config(self):\n@@ -288,7 +299,9 @@ def dual_attention_unit_block(input_tensor):\n     )\n     channel_attention = channel_attention_block(feature_map)\n     spatial_attention = spatial_attention_block(feature_map)\n-    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n+    concatenation = layers.Concatenate(axis=-1)(\n+        [channel_attention, spatial_attention]\n+    )\n     concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n     return layers.Add()([input_tensor, concatenation])\n \n@@ -315,9 +328,9 @@ Residual Block.\n \n def down_sampling_module(input_tensor):\n     channels = list(input_tensor.shape)[-1]\n-    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n-        input_tensor\n-    )\n+    main_branch = layers.Conv2D(\n+        channels, kernel_size=(1, 1), activation=\"relu\"\n+    )(input_tensor)\n     main_branch = layers.Conv2D(\n         channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n     )(main_branch)\n@@ -330,9 +343,9 @@ def down_sampling_module(input_tensor):\n \n def up_sampling_module(input_tensor):\n     channels = list(input_tensor.shape)[-1]\n-    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n-        input_tensor\n-    )\n+    main_branch = layers.Conv2D(\n+        channels, kernel_size=(1, 1), activation=\"relu\"\n+    )(input_tensor)\n     main_branch = layers.Conv2D(\n         channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n     )(main_branch)\n@@ -360,7 +373,9 @@ def multi_scale_residual_block(input_tensor, channels):\n         up_sampling_module(up_sampling_module(level3_dau)),\n     )\n     level2_skff = selective_kernel_feature_fusion(\n-        down_sampling_module(level1_dau), level2_dau, up_sampling_module(level3_dau)\n+        down_sampling_module(level1_dau),\n+        level2_dau,\n+        up_sampling_module(level3_dau),\n     )\n     level3_skff = selective_kernel_feature_fusion(\n         down_sampling_module(down_sampling_module(level1_dau)),\n@@ -374,7 +389,9 @@ def multi_scale_residual_block(input_tensor, channels):\n         up_sampling_module(dual_attention_unit_block(level3_skff))\n     )\n     # SKFF 2\n-    skff_ = selective_kernel_feature_fusion(level1_dau_2, level2_dau_2, level3_dau_2)\n+    skff_ = selective_kernel_feature_fusion(\n+        level1_dau_2, level2_dau_2, level3_dau_2\n+    )\n     conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n     return layers.Add()([input_tensor, conv])\n \n@@ -385,7 +402,9 @@ def multi_scale_residual_block(input_tensor, channels):\n \n \n def recursive_residual_group(input_tensor, num_mrb, channels):\n-    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n+    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n+        input_tensor\n+    )\n     for _ in range(num_mrb):\n         conv1 = multi_scale_residual_block(conv1, channels)\n     conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n@@ -394,7 +413,9 @@ def recursive_residual_group(input_tensor, num_mrb, channels):\n \n def mirnet_model(num_rrg, num_mrb, channels):\n     input_tensor = keras.Input(shape=[None, None, 3])\n-    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n+    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n+        input_tensor\n+    )\n     for _ in range(num_rrg):\n         x1 = recursive_residual_group(x1, num_mrb, channels)\n     conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n@@ -425,7 +446,9 @@ def peak_signal_noise_ratio(y_true, y_pred):\n \n optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n model.compile(\n-    optimizer=optimizer, loss=charbonnier_loss, metrics=[peak_signal_noise_ratio]\n+    optimizer=optimizer,\n+    loss=charbonnier_loss,\n+    metrics=[peak_signal_noise_ratio],\n )\n \n history = model.fit(\n@@ -444,6 +467,7 @@ history = model.fit(\n     ],\n )\n \n+\n def plot_history(value):\n     plt.plot(history.history[value], label=f\"train_{value}\")\n     plt.plot(history.history[f\"val_{value}\"], label=f\"val_{value}\")\n\n@@ -33,6 +33,7 @@ In this example, we minimally implement the paper with close alignement to the a\n \"\"\"\n \n import os\n+\n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import numpy as np\n@@ -43,7 +44,6 @@ import keras_core as keras\n from keras_core import layers\n \n \n-\n # Setting seed for reproducibiltiy\n SEED = 42\n keras.utils.set_random_seed(SEED)\n@@ -105,7 +105,9 @@ print(f\"Testing samples: {len(x_test)}\")\n \n AUTO = tf.data.AUTOTUNE\n train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n-train_ds = train_ds.shuffle(config.buffer_size).batch(config.batch_size).prefetch(AUTO)\n+train_ds = (\n+    train_ds.shuffle(config.buffer_size).batch(config.batch_size).prefetch(AUTO)\n+)\n \n val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n val_ds = val_ds.batch(config.batch_size).prefetch(AUTO)\n@@ -136,7 +138,9 @@ def get_augmentation_model():\n     \"\"\"Build the data augmentation model.\"\"\"\n     data_augmentation = keras.Sequential(\n         [\n-            layers.Resizing(config.input_shape[0] + 20, config.input_shape[0] + 20),\n+            layers.Resizing(\n+                config.input_shape[0] + 20, config.input_shape[0] + 20\n+            ),\n             layers.RandomCrop(config.image_size, config.image_size),\n             layers.RandomFlip(\"horizontal\"),\n             layers.Rescaling(1 / 255.0),\n@@ -394,7 +398,9 @@ class ShiftViTBlock(layers.Layer):\n \n     def call(self, x, training=False):\n         # Split the feature maps\n-        x_splits = tf.split(x, num_or_size_splits=self.C // self.num_div, axis=-1)\n+        x_splits = tf.split(\n+            x, num_or_size_splits=self.C // self.num_div, axis=-1\n+        )\n \n         # Shift the feature maps\n         x_splits[0] = self.get_shift_pad(x_splits[0], mode=\"left\")\n@@ -407,7 +413,9 @@ class ShiftViTBlock(layers.Layer):\n \n         # Add the residual connection\n         shortcut = x\n-        x = shortcut + self.drop_path(self.mlp(self.layer_norm(x)), training=training)\n+        x = shortcut + self.drop_path(\n+            self.mlp(self.layer_norm(x)), training=training\n+        )\n         return x\n \n \n@@ -448,7 +456,11 @@ class PatchMerging(layers.Layer):\n     def build(self, input_shape):\n         filters = 2 * input_shape[-1]\n         self.reduction = layers.Conv2D(\n-            filters=filters, kernel_size=2, strides=2, padding=\"same\", use_bias=False\n+            filters=filters,\n+            kernel_size=2,\n+            strides=2,\n+            padding=\"same\",\n+            use_bias=False,\n         )\n         self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n \n@@ -515,7 +527,9 @@ class StackedShiftBlocks(layers.Layer):\n         dpr = [\n             x\n             for x in np.linspace(\n-                start=0, stop=self.stochastic_depth_rate, num=self.num_shift_blocks\n+                start=0,\n+                stop=self.stochastic_depth_rate,\n+                num=self.num_shift_blocks,\n             )\n         ]\n \n@@ -659,7 +673,9 @@ class ShiftViTModel(keras.Model):\n             self.patch_projection.trainable_variables,\n             self.global_avg_pool.trainable_variables,\n         ]\n-        train_vars = train_vars + [stage.trainable_variables for stage in self.stages]\n+        train_vars = train_vars + [\n+            stage.trainable_variables for stage in self.stages\n+        ]\n \n         # Optimize the gradients.\n         grads = tape.gradient(total_loss, train_vars)\n\n@@ -88,8 +88,12 @@ def data_generator(low_light_images):\n     return dataset\n \n \n-train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n-val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n+train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[\n+    :MAX_TRAIN_IMAGES\n+]\n+val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[\n+    MAX_TRAIN_IMAGES:\n+]\n test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n \n \n@@ -163,9 +167,9 @@ def build_dce_net():\n         32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n     )(int_con2)\n     int_con3 = layers.Concatenate(axis=-1)([conv6, conv1])\n-    x_r = layers.Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(\n-        int_con3\n-    )\n+    x_r = layers.Conv2D(\n+        24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\"\n+    )(int_con3)\n     return keras.Model(inputs=input_img, outputs=x_r)\n \n \n@@ -186,7 +190,11 @@ enhanced image.\n \n def color_constancy_loss(x):\n     mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n-    mr, mg, mb = mean_rgb[:, :, :, 0], mean_rgb[:, :, :, 1], mean_rgb[:, :, :, 2]\n+    mr, mg, mb = (\n+        mean_rgb[:, :, :, 0],\n+        mean_rgb[:, :, :, 1],\n+        mean_rgb[:, :, :, 2],\n+    )\n     d_rg = tf.square(mr - mg)\n     d_rb = tf.square(mr - mb)\n     d_gb = tf.square(mb - mg)\n@@ -266,29 +274,47 @@ class SpatialConsistencyLoss(keras.losses.Loss):\n         )\n \n         d_original_left = tf.nn.conv2d(\n-            original_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+            original_pool,\n+            self.left_kernel,\n+            strides=[1, 1, 1, 1],\n+            padding=\"SAME\",\n         )\n         d_original_right = tf.nn.conv2d(\n-            original_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+            original_pool,\n+            self.right_kernel,\n+            strides=[1, 1, 1, 1],\n+            padding=\"SAME\",\n         )\n         d_original_up = tf.nn.conv2d(\n             original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n         )\n         d_original_down = tf.nn.conv2d(\n-            original_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+            original_pool,\n+            self.down_kernel,\n+            strides=[1, 1, 1, 1],\n+            padding=\"SAME\",\n         )\n \n         d_enhanced_left = tf.nn.conv2d(\n-            enhanced_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+            enhanced_pool,\n+            self.left_kernel,\n+            strides=[1, 1, 1, 1],\n+            padding=\"SAME\",\n         )\n         d_enhanced_right = tf.nn.conv2d(\n-            enhanced_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+            enhanced_pool,\n+            self.right_kernel,\n+            strides=[1, 1, 1, 1],\n+            padding=\"SAME\",\n         )\n         d_enhanced_up = tf.nn.conv2d(\n             enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n         )\n         d_enhanced_down = tf.nn.conv2d(\n-            enhanced_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+            enhanced_pool,\n+            self.down_kernel,\n+            strides=[1, 1, 1, 1],\n+            padding=\"SAME\",\n         )\n \n         d_left = tf.square(d_original_left - d_enhanced_left)\n@@ -315,9 +341,15 @@ class ZeroDCE(keras.Model):\n         self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n         self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n         self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n-        self.illumination_smoothness_loss_tracker = keras.metrics.Mean(name=\"illumination_smoothness_loss\")\n-        self.spatial_constancy_loss_tracker = keras.metrics.Mean(name=\"spatial_constancy_loss\")\n-        self.color_constancy_loss_tracker = keras.metrics.Mean(name=\"color_constancy_loss\")\n+        self.illumination_smoothness_loss_tracker = keras.metrics.Mean(\n+            name=\"illumination_smoothness_loss\"\n+        )\n+        self.spatial_constancy_loss_tracker = keras.metrics.Mean(\n+            name=\"spatial_constancy_loss\"\n+        )\n+        self.color_constancy_loss_tracker = keras.metrics.Mean(\n+            name=\"color_constancy_loss\"\n+        )\n         self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")\n \n     @property\n@@ -359,7 +391,9 @@ class ZeroDCE(keras.Model):\n         loss_spatial_constancy = tf.reduce_mean(\n             self.spatial_constancy_loss(enhanced_image, data)\n         )\n-        loss_color_constancy = 5 * tf.reduce_mean(color_constancy_loss(enhanced_image))\n+        loss_color_constancy = 5 * tf.reduce_mean(\n+            color_constancy_loss(enhanced_image)\n+        )\n         loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n         total_loss = (\n             loss_illumination\n@@ -384,12 +418,20 @@ class ZeroDCE(keras.Model):\n         gradients = tape.gradient(\n             losses[\"total_loss\"], self.dce_model.trainable_weights\n         )\n-        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n+        self.optimizer.apply_gradients(\n+            zip(gradients, self.dce_model.trainable_weights)\n+        )\n \n         self.total_loss_tracker.update_state(losses[\"total_loss\"])\n-        self.illumination_smoothness_loss_tracker.update_state(losses[\"illumination_smoothness_loss\"])\n-        self.spatial_constancy_loss_tracker.update_state(losses[\"spatial_constancy_loss\"])\n-        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n+        self.illumination_smoothness_loss_tracker.update_state(\n+            losses[\"illumination_smoothness_loss\"]\n+        )\n+        self.spatial_constancy_loss_tracker.update_state(\n+            losses[\"spatial_constancy_loss\"]\n+        )\n+        self.color_constancy_loss_tracker.update_state(\n+            losses[\"color_constancy_loss\"]\n+        )\n         self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n \n         return {metric.name: metric.result() for metric in self.metrics}\n@@ -399,20 +441,33 @@ class ZeroDCE(keras.Model):\n         losses = self.compute_losses(data, output)\n \n         self.total_loss_tracker.update_state(losses[\"total_loss\"])\n-        self.illumination_smoothness_loss_tracker.update_state(losses[\"illumination_smoothness_loss\"])\n-        self.spatial_constancy_loss_tracker.update_state(losses[\"spatial_constancy_loss\"])\n-        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n+        self.illumination_smoothness_loss_tracker.update_state(\n+            losses[\"illumination_smoothness_loss\"]\n+        )\n+        self.spatial_constancy_loss_tracker.update_state(\n+            losses[\"spatial_constancy_loss\"]\n+        )\n+        self.color_constancy_loss_tracker.update_state(\n+            losses[\"color_constancy_loss\"]\n+        )\n         self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n \n         return {metric.name: metric.result() for metric in self.metrics}\n \n-    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n+    def save_weights(\n+        self, filepath, overwrite=True, save_format=None, options=None\n+    ):\n         \"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\n         self.dce_model.save_weights(\n-            filepath, overwrite=overwrite, save_format=save_format, options=options\n+            filepath,\n+            overwrite=overwrite,\n+            save_format=save_format,\n+            options=options,\n         )\n \n-    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n+    def load_weights(\n+        self, filepath, by_name=False, skip_mismatch=False, options=None\n+    ):\n         \"\"\"While loading the weights, we simply load the weights of the DCE-Net\"\"\"\n         self.dce_model.load_weights(\n             filepath=filepath,\n@@ -428,7 +483,9 @@ class ZeroDCE(keras.Model):\n \n zero_dce_model = ZeroDCE()\n zero_dce_model.compile(learning_rate=1e-4)\n-history = zero_dce_model.fit(train_dataset, validation_data=val_dataset, epochs=100)\n+history = zero_dce_model.fit(\n+    train_dataset, validation_data=val_dataset, epochs=100\n+)\n \n \n def plot_result(item):\n\n@@ -60,12 +60,18 @@ a smaller subset of 200 images for training our model in this example.\n IMAGE_SIZE = 512\n BATCH_SIZE = 4\n NUM_CLASSES = 20\n-DATA_DIR = \"./instance-level_human_parsing/instance-level_human_parsing/Training\"\n+DATA_DIR = (\n+    \"./instance-level_human_parsing/instance-level_human_parsing/Training\"\n+)\n NUM_TRAIN_IMAGES = 1000\n NUM_VAL_IMAGES = 50\n \n-train_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[:NUM_TRAIN_IMAGES]\n-train_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[:NUM_TRAIN_IMAGES]\n+train_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[\n+    :NUM_TRAIN_IMAGES\n+]\n+train_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[\n+    :NUM_TRAIN_IMAGES\n+]\n val_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[\n     NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n ]\n@@ -197,7 +203,9 @@ def DeeplabV3Plus(image_size, num_classes):\n         size=(image_size // x.shape[1], image_size // x.shape[2]),\n         interpolation=\"bilinear\",\n     )(x)\n-    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n+    model_output = layers.Conv2D(\n+        num_classes, kernel_size=(1, 1), padding=\"same\"\n+    )(x)\n     return keras.Model(inputs=model_input, outputs=model_output)\n \n \n@@ -305,7 +313,9 @@ def plot_predictions(images_list, colormap, model):\n     for image_file in images_list:\n         image_tensor = read_image(image_file)\n         prediction_mask = infer(image_tensor=image_tensor, model=model)\n-        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 20)\n+        prediction_colormap = decode_segmentation_masks(\n+            prediction_mask, colormap, 20\n+        )\n         overlay = get_overlay(image_tensor, prediction_colormap)\n         plot_samples_matplotlib(\n             [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n\n@@ -219,7 +219,9 @@ crucial to make TokenLearner work, especially when we are significantly reducing\n \n def token_learner(inputs, number_of_tokens=NUM_TOKENS):\n     # Layer normalize the inputs.\n-    x = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(inputs)  # (B, H, W, C)\n+    x = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(\n+        inputs\n+    )  # (B, H, W, C)\n \n     # Applying Conv2D => Reshape => Permute\n     # The reshape and permute is done to help with the next steps of\n@@ -267,7 +269,9 @@ def token_learner(inputs, number_of_tokens=NUM_TOKENS):\n \n     # Reshape the input to align it with the output of the conv block.\n     num_filters = inputs.shape[-1]\n-    inputs = layers.Reshape((1, -1, num_filters))(inputs)  # inputs == (B, 1, H*W, C)\n+    inputs = layers.Reshape((1, -1, num_filters))(\n+        inputs\n+    )  # inputs == (B, 1, H*W, C)\n \n     # Element-Wise multiplication of the attention maps and the inputs\n     attended_inputs = (\n@@ -312,7 +316,9 @@ def transformer(encoded_patches):\n \"\"\"\n \n \n-def create_vit_classifier(use_token_learner=True, token_learner_units=NUM_TOKENS):\n+def create_vit_classifier(\n+    use_token_learner=True, token_learner_units=NUM_TOKENS\n+):\n     inputs = layers.Input(shape=INPUT_SHAPE)  # (B, H, W, C)\n \n     # Augment data.\n@@ -356,7 +362,9 @@ def create_vit_classifier(use_token_learner=True, token_learner_units=NUM_TOKENS\n             )  # (B, num_tokens, c)\n \n     # Layer normalization and Global average pooling.\n-    representation = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n+    representation = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(\n+        encoded_patches\n+    )\n     representation = layers.GlobalAvgPool1D()(representation)\n \n     # Classify outputs.\n@@ -391,7 +399,9 @@ def run_experiment(model):\n         loss=\"sparse_categorical_crossentropy\",\n         metrics=[\n             keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n-            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n+            keras.metrics.SparseTopKCategoricalAccuracy(\n+                5, name=\"top-5-accuracy\"\n+            ),\n         ],\n     )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
