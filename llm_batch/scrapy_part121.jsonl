{"custom_id": "scrapy#84dee1f77fb12e170d151d5e9c615a4cc3252939", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 61 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -64,13 +64,6 @@ def request_authenticate(request, username, password):\n     \"\"\"\n     request.headers['Authorization'] = basic_auth_header(username, password)\n \n-def request_info(request):\n-    \"\"\"Return a short string with request info including method, url and\n-    fingeprint. Mainly used for debugging\n-    \"\"\"\n-    fp = request_fingerprint(request)\n-    return \"<Request: %s %s (%s..)>\" % (request.method, request.url, fp[:8])\n-\n def request_httprepr(request):\n     \"\"\"Return the raw HTTP representation (as string) of the given request.\n     This is provided only for reference since it's not the actual stream of\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8a5c08a6bcde96938ed92ee4996aafb6655c3688", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 23 | Churn Cumulative: 985 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -143,11 +143,20 @@ class XmlItemExporter(BaseItemExporter):\n \n class CsvItemExporter(BaseItemExporter):\n \n-    def __init__(self, file, include_headers_line=True, **kwargs):\n+    def __init__(self, file, include_headers_line=True, join_multivalued=',', **kwargs):\n         self._configure(kwargs, dont_fail=True)\n         self.include_headers_line = include_headers_line\n         self.csv_writer = csv.writer(file, **kwargs)\n         self._headers_not_written = True\n+        self._join_multivalued = join_multivalued\n+\n+    def _to_str_if_unicode(self, value):\n+        if isinstance(value, (list, tuple)):\n+            try:\n+                value = self._join_multivalued.join(value)\n+            except TypeError: # list in value may not contain strings\n+                pass\n+        return super(CsvItemExporter, self)._to_str_if_unicode(value)\n \n     def export_item(self, item):\n         if self._headers_not_written:\n\n@@ -127,6 +127,18 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         ie.finish_exporting()\n         self.assertEqual(output.getvalue(), '22,John\\xc2\\xa3\\r\\n')\n \n+    def test_join_multivalue(self):\n+        class TestItem2(Item):\n+            name = Field()\n+            friends = Field()\n+\n+        i = TestItem2(name='John', friends=['Mary', 'Paul'])\n+        output = StringIO()\n+        ie = CsvItemExporter(output, include_headers_line=False)\n+        ie.start_exporting()\n+        ie.export_item(i)\n+        ie.finish_exporting()\n+        self.assertEqual(output.getvalue(), '\"Mary,Paul\",John\\r\\n')\n \n class XmlItemExporterTest(BaseItemExporterTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ee2c94e933474656f47c475c6e5b2ef86d80472", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 30 | Churn Cumulative: 56 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import os\n from collections import defaultdict\n from scrapy.xlib.pydispatch import dispatcher\n \n@@ -28,7 +29,7 @@ class CookiesMiddleware(object):\n         # set Cookie header\n         request.headers.pop('Cookie', None)\n         jar.add_cookie_header(request)\n-        self._debug_cookie(request)\n+        self._debug_cookie(request, spider)\n \n     def process_response(self, request, response, spider):\n         if 'dont_merge_cookies' in request.meta:\n@@ -37,31 +38,28 @@ class CookiesMiddleware(object):\n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n         jar = self.jars[spider]\n         jar.extract_cookies(response, request)\n-        self._debug_set_cookie(response)\n+        self._debug_set_cookie(response, spider)\n \n         return response\n \n     def spider_closed(self, spider):\n         self.jars.pop(spider, None)\n \n-    def _debug_cookie(self, request):\n-        \"\"\"log Cookie header for request\"\"\"\n+    def _debug_cookie(self, request, spider):\n         if self.debug:\n-            c = request.headers.get('Cookie')\n-            c = c and [p.split('=')[0] for p in c.split(';')]\n-            log.msg('Cookie: %s for %s' % (c, request.url), level=log.DEBUG)\n+            cl = request.headers.getlist('Cookie')\n+            if cl:\n+                msg = \"Sending cookies to: %s\" % request + os.linesep\n+                msg += os.linesep.join(\"Cookie: %s\" % c for c in cl)\n+                log.msg(msg, spider=spider, level=log.DEBUG)\n \n-    def _debug_set_cookie(self, response):\n-        \"\"\"log Set-Cookies headers but exclude cookie values\"\"\"\n+    def _debug_set_cookie(self, response, spider):\n         if self.debug:\n             cl = response.headers.getlist('Set-Cookie')\n-            res = []\n-            for c in cl:\n-                kv, tail = c.split(';', 1)\n-                k = kv.split('=', 1)[0]\n-                res.append('%s %s' % (k, tail))\n-            log.msg('Set-Cookie: %s from %s' % (res, response.url))\n-\n+            if cl:\n+                msg = \"Received cookies from: %s\" % response + os.linesep\n+                msg += os.linesep.join(\"Set-Cookie: %s\" % c for c in cl)\n+                log.msg(msg, spider=spider, level=log.DEBUG)\n \n     def _get_request_cookies(self, jar, request):\n         headers = {'Set-Cookie': ['%s=%s;' % (k, v) for k, v in request.cookies.iteritems()]}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f262a198cb0af8e4262d5007a7c1e8f8d9fe205", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 45 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -30,9 +30,11 @@ from scrapy.conf import settings\n \n class RetryMiddleware(object):\n \n+    # IOError is raised by the HttpCompression middleware when trying to\n+    # decompress an empty response\n     EXCEPTIONS_TO_RETRY = (ServerTimeoutError, UserTimeoutError, DNSLookupError,\n                            ConnectionRefusedError, ConnectionDone, ConnectError,\n-                           ConnectionLost, PartialDownloadError)\n+                           ConnectionLost, PartialDownloadError, IOError)\n \n     def __init__(self):\n         self.max_retry_times = settings.getint('RETRY_TIMES')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b10f4fae3534b0b3e25cbb3e4d3770ff0501913b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 861 | Files Changed: 10 | Hunks: 31 | Methods Changed: 49 | Complexity Δ (Sum/Max): -99/0 | Churn Δ: 887 | Churn Cumulative: 1940 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 32 | DMM Complexity: 0.099644128113879\n\nDIFF:\n@@ -1,13 +0,0 @@\n-import unittest\n-from scrapy.utils.http import basic_auth_header\n-\n-__doctests__ = ['scrapy.utils.http']\n-\n-class UtilsHttpTestCase(unittest.TestCase):\n-\n-    def test_basic_auth_header(self):\n-        self.assertEqual('Basic c29tZXVzZXI6c29tZXBhc3M=',\n-                basic_auth_header('someuser', 'somepass'))\n-        # Check url unsafe encoded header\n-        self.assertEqual('Basic c29tZXVzZXI6QDx5dTk-Jm8_UQ==',\n-            basic_auth_header('someuser', '@<yu9>&o?Q'))\n\n@@ -1,156 +0,0 @@\n-# -*- coding: utf-8 -*-\n-import unittest\n-\n-from scrapy.utils.markup import remove_entities, replace_tags, remove_comments\n-from scrapy.utils.markup import remove_tags_with_content, replace_escape_chars, remove_tags\n-from scrapy.utils.markup import unquote_markup\n-\n-class UtilsMarkupTest(unittest.TestCase):\n-\n-    def test_remove_entities(self):\n-        # make sure it always return uncode\n-        assert isinstance(remove_entities('no entities'), unicode)\n-        assert isinstance(remove_entities('Price: &pound;100!'),  unicode)\n-\n-        # regular conversions\n-        self.assertEqual(remove_entities(u'As low as &#163;100!'),\n-                         u'As low as \\xa3100!')\n-        self.assertEqual(remove_entities('As low as &pound;100!'),\n-                         u'As low as \\xa3100!')\n-        self.assertEqual(remove_entities('redirectTo=search&searchtext=MR0221Y&aff=buyat&affsrc=d_data&cm_mmc=buyat-_-ELECTRICAL & SEASONAL-_-MR0221Y-_-9-carat gold &frac12;oz solid crucifix pendant'),\n-                         u'redirectTo=search&searchtext=MR0221Y&aff=buyat&affsrc=d_data&cm_mmc=buyat-_-ELECTRICAL & SEASONAL-_-MR0221Y-_-9-carat gold \\xbdoz solid crucifix pendant')\n-        # keep some entities\n-        self.assertEqual(remove_entities('<b>Low &lt; High &amp; Medium &pound; six</b>', keep=['lt', 'amp']),\n-                         u'<b>Low &lt; High &amp; Medium \\xa3 six</b>')\n-\n-        # illegal entities\n-        self.assertEqual(remove_entities('a &lt; b &illegal; c &#12345678; six', remove_illegal=False),\n-                         u'a < b &illegal; c &#12345678; six')\n-        self.assertEqual(remove_entities('a &lt; b &illegal; c &#12345678; six', remove_illegal=True),\n-                         u'a < b  c  six')\n-        self.assertEqual(remove_entities('x&#x2264;y'), u'x\\u2264y')\n-\n-        # check browser hack for numeric character references in the 80-9F range\n-        self.assertEqual(remove_entities('x&#153;y', encoding='cp1252'), u'x\\u2122y')\n-\n-        # encoding\n-        self.assertEqual(remove_entities('x\\x99&#153;&#8482;y', encoding='cp1252'), \\\n-                         u'x\\u2122\\u2122\\u2122y')\n-\n-    def test_replace_tags(self):\n-        # make sure it always return uncode\n-        assert isinstance(replace_tags('no entities'), unicode)\n-\n-        self.assertEqual(replace_tags(u'This text contains <a>some tag</a>'),\n-                         u'This text contains some tag')\n-\n-        self.assertEqual(replace_tags('This text is very im<b>port</b>ant', ' '),\n-                         u'This text is very im port ant')\n-\n-        # multiline tags\n-        self.assertEqual(replace_tags('Click <a class=\"one\"\\r\\n href=\"url\">here</a>'),\n-                         u'Click here')\n-\n-    def test_remove_comments(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_comments('without comments'), unicode)\n-        assert isinstance(remove_comments('<!-- with comments -->'), unicode)\n-\n-        # text without comments \n-        self.assertEqual(remove_comments(u'text without comments'), u'text without comments')\n-\n-        # text with comments\n-        self.assertEqual(remove_comments(u'<!--text with comments-->'), u'')\n-        self.assertEqual(remove_comments(u'Hello<!--World-->'),u'Hello')\n-\n-    def test_remove_tags(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_tags('no tags'), unicode)\n-        assert isinstance(remove_tags('no tags', which_ones=('p',)), unicode)\n-        assert isinstance(remove_tags('<p>one tag</p>'), unicode)\n-        assert isinstance(remove_tags('<p>one tag</p>', which_ones=('p')), unicode)\n-        assert isinstance(remove_tags('<a>link</a>', which_ones=('b',)), unicode)\n-\n-        # text without tags\n-        self.assertEqual(remove_tags(u'no tags'), u'no tags')\n-        self.assertEqual(remove_tags(u'no tags', which_ones=('p','b',)), u'no tags')\n-\n-        # text with tags\n-        self.assertEqual(remove_tags(u'<p>one p tag</p>'), u'one p tag')\n-        self.assertEqual(remove_tags(u'<p>one p tag</p>', which_ones=('b',)), u'<p>one p tag</p>')\n-\n-        self.assertEqual(remove_tags(u'<b>not will removed</b><i>i will removed</i>', which_ones=('i',)),\n-                         u'<b>not will removed</b>i will removed')\n-\n-        # text with tags and attributes\n-        self.assertEqual(remove_tags(u'<p align=\"center\" class=\"one\">texty</p>'), u'texty')\n-        self.assertEqual(remove_tags(u'<p align=\"center\" class=\"one\">texty</p>', which_ones=('b',)),\n-                         u'<p align=\"center\" class=\"one\">texty</p>')\n-\n-        # text with empty tags\n-        self.assertEqual(remove_tags(u'a<br />b<br/>c'), u'abc')\n-        self.assertEqual(remove_tags(u'a<br />b<br/>c', which_ones=('br',)), u'abc')\n-\n-        # test keep arg\n-        self.assertEqual(remove_tags(u'<p>a<br />b<br/>c</p>', keep=('br',)), u'a<br />b<br/>c')\n-        self.assertEqual(remove_tags(u'<p>a<br />b<br/>c</p>', keep=('p',)), u'<p>abc</p>')\n-        self.assertEqual(remove_tags(u'<p>a<br />b<br/>c</p>', keep=('p','br','div')), u'<p>a<br />b<br/>c</p>')\n-\n-    def test_remove_tags_with_content(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_tags_with_content('no tags'), unicode)\n-        assert isinstance(remove_tags_with_content('no tags', which_ones=('p',)), unicode)\n-        assert isinstance(remove_tags_with_content('<p>one tag</p>', which_ones=('p',)), unicode)\n-        assert isinstance(remove_tags_with_content('<a>link</a>', which_ones=('b',)), unicode)\n-\n-        # text without tags\n-        self.assertEqual(remove_tags_with_content(u'no tags'), u'no tags')\n-        self.assertEqual(remove_tags_with_content(u'no tags', which_ones=('p','b',)), u'no tags')\n-\n-        # text with tags\n-        self.assertEqual(remove_tags_with_content(u'<p>one p tag</p>'), u'<p>one p tag</p>')\n-        self.assertEqual(remove_tags_with_content(u'<p>one p tag</p>', which_ones=('p',)), u'')\n-\n-        self.assertEqual(remove_tags_with_content(u'<b>not will removed</b><i>i will removed</i>', which_ones=('i',)),\n-                         u'<b>not will removed</b>')\n-\n-        # text with empty tags\n-        self.assertEqual(remove_tags_with_content(u'<br/>a<br />', which_ones=('br',)), u'a')\n-\n-    def test_replace_escape_chars(self):\n-        # make sure it always return unicode\n-        assert isinstance(replace_escape_chars('no ec'), unicode)\n-        assert isinstance(replace_escape_chars('no ec', replace_by='str'), unicode)\n-        assert isinstance(replace_escape_chars('no ec', which_ones=('\\n','\\t',)), unicode)\n-\n-        # text without escape chars\n-        self.assertEqual(replace_escape_chars(u'no ec'), u'no ec')\n-        self.assertEqual(replace_escape_chars(u'no ec', which_ones=('\\n',)), u'no ec')\n-\n-        # text with escape chars\n-        self.assertEqual(replace_escape_chars(u'escape\\n\\n'), u'escape')\n-        self.assertEqual(replace_escape_chars(u'escape\\n', which_ones=('\\t',)), u'escape\\n')\n-        self.assertEqual(replace_escape_chars(u'escape\\tchars\\n', which_ones=('\\t')), 'escapechars\\n')\n-        self.assertEqual(replace_escape_chars(u'escape\\tchars\\n', replace_by=' '), 'escape chars ')\n-        self.assertEqual(replace_escape_chars(u'escape\\tchars\\n', replace_by=u'\\xa3'), u'escape\\xa3chars\\xa3')\n-        self.assertEqual(replace_escape_chars(u'escape\\tchars\\n', replace_by='\\xc2\\xa3'), u'escape\\xa3chars\\xa3')\n-\n-    def test_unquote_markup(self):\n-        sample_txt1 = u\"\"\"<node1>hi, this is sample text with entities: &amp; &copy;\n-<![CDATA[although this is inside a cdata! &amp; &quot;]]></node1>\"\"\"\n-        sample_txt2 = u'<node2>blah&amp;blah<![CDATA[blahblahblah!&pound;]]>moreblah&lt;&gt;</node2>'\n-        sample_txt3 = u'something&pound;&amp;more<node3><![CDATA[things, stuff, and such]]>what&quot;ever</node3><node4'\n-\n-        # make sure it always return unicode\n-        assert isinstance(unquote_markup(sample_txt1.encode('latin-1')), unicode)\n-        assert isinstance(unquote_markup(sample_txt2), unicode)\n-\n-        self.assertEqual(unquote_markup(sample_txt1), u\"\"\"<node1>hi, this is sample text with entities: & \\xa9\n-although this is inside a cdata! &amp; &quot;</node1>\"\"\")\n-\n-        self.assertEqual(unquote_markup(sample_txt2), u'<node2>blah&blahblahblahblah!&pound;moreblah<></node2>')\n-\n-        self.assertEqual(unquote_markup(sample_txt1 + sample_txt2), u\"\"\"<node1>hi, this is sample text with entities: & \\xa9\n-although this is inside a cdata! &amp; &quot;</node1><node2>blah&blahblahblahblah!&pound;moreblah<></node2>\"\"\")\n-\n-        self.assertEqual(unquote_markup(sample_txt3), u'something\\xa3&more<node3>things, stuff, and suchwhat\"ever</node3><node4')\n\n@@ -31,108 +31,6 @@ class ResponseUtilsTest(unittest.TestCase):\n         self.assertTrue(isinstance(body_or_str(u'text', unicode=False), str))\n         self.assertTrue(isinstance(body_or_str(u'text', unicode=True), unicode))\n \n-    def test_get_base_url(self):\n-        response = HtmlResponse(url='https://example.org', body=\"\"\"\\\n-            <html>\\\n-            <head><title>Dummy</title><base href='http://example.org/something' /></head>\\\n-            <body>blahablsdfsal&amp;</body>\\\n-            </html>\"\"\")\n-        self.assertEqual(get_base_url(response), 'http://example.org/something')\n-\n-        # relative url with absolute path\n-        response = HtmlResponse(url='https://example.org', body=\"\"\"\\\n-            <html>\\\n-            <head><title>Dummy</title><base href='/absolutepath' /></head>\\\n-            <body>blahablsdfsal&amp;</body>\\\n-            </html>\"\"\")\n-        self.assertEqual(get_base_url(response), 'https://example.org/absolutepath')\n-\n-        # no scheme url\n-        response = HtmlResponse(url='https://example.org', body=\"\"\"\\\n-            <html>\\\n-            <head><title>Dummy</title><base href='//noscheme.com/path' /></head>\\\n-            <body>blahablsdfsal&amp;</body>\\\n-            </html>\"\"\")\n-        self.assertEqual(get_base_url(response), 'https://noscheme.com/path')\n-\n-    def test_get_meta_refresh(self):\n-        body = \"\"\"\n-            <html>\n-            <head><title>Dummy</title><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n-            <body>blahablsdfsal&amp;</body>\n-            </html>\"\"\"\n-        response = TextResponse(url='http://example.org', body=body)\n-        self.assertEqual(get_meta_refresh(response), (5, 'http://example.org/newpage'))\n-\n-        # refresh without url should return (None, None)\n-        body = \"\"\"<meta http-equiv=\"refresh\" content=\"5\" />\"\"\"\n-        response = TextResponse(url='http://example.org', body=body)\n-        self.assertEqual(get_meta_refresh(response), (None, None))\n-\n-        body = \"\"\"<meta http-equiv=\"refresh\" content=\"5;\n-            url=http://example.org/newpage\" /></head>\"\"\"\n-        response = TextResponse(url='http://example.org', body=body)\n-        self.assertEqual(get_meta_refresh(response), (5, 'http://example.org/newpage'))\n-\n-        # meta refresh in multiple lines\n-        body = \"\"\"<html><head>\n-               <META\n-               HTTP-EQUIV=\"Refresh\"\n-               CONTENT=\"1; URL=http://example.org/newpage\">\"\"\"\n-        response = TextResponse(url='http://example.org', body=body)\n-        self.assertEqual(get_meta_refresh(response), (1, 'http://example.org/newpage'))\n-\n-        # entities in the redirect url\n-        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=&#39;http://www.example.com/other&#39;\">\"\"\"\n-        response = TextResponse(url='http://example.com', body=body)\n-        self.assertEqual(get_meta_refresh(response), (3, 'http://www.example.com/other'))\n-\n-        # relative redirects\n-        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=other.html\">\"\"\"\n-        response = TextResponse(url='http://example.com/page/this.html', body=body)\n-        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/page/other.html'))\n-\n-        # non-standard encodings (utf-16)\n-        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/redirect\">\"\"\"\n-        body = body.decode('ascii').encode('utf-16')\n-        response = TextResponse(url='http://example.com', body=body, encoding='utf-16')\n-        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/redirect'))\n-\n-        # non-ascii chars in the url (utf8)\n-        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/to\\xc2\\xa3\">\"\"\"\n-        response = TextResponse(url='http://example.com', body=body, encoding='utf-8')\n-        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/to%C2%A3'))\n-\n-        # non-ascii chars in the url (latin1)\n-        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/to\\xa3\">\"\"\"\n-        response = TextResponse(url='http://example.com', body=body, encoding='latin1')\n-        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/to%C2%A3'))\n-\n-        # responses without refresh tag should return None None\n-        response = TextResponse(url='http://example.org')\n-        self.assertEqual(get_meta_refresh(response), (None, None))\n-        response = TextResponse(url='http://example.org')\n-        self.assertEqual(get_meta_refresh(response), (None, None))\n-\n-        # html commented meta refresh header must not directed\n-        body = \"\"\"<!--<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/\">-->\"\"\"\n-        response = TextResponse(url='http://example.com', body=body)\n-        self.assertEqual(get_meta_refresh(response), (None, None))\n-\n-        # html comments must not interfere with uncommented meta refresh header\n-        body = \"\"\"<!-- commented --><meta http-equiv=\"refresh\" content=\"3; url=http://example.com/\">-->\"\"\"\n-        response = TextResponse(url='http://example.com', body=body)\n-        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/'))\n-\n-        # float refresh intervals\n-        body = \"\"\"<meta http-equiv=\"refresh\" content=\".1;URL=index.html\" />\"\"\"\n-        response = TextResponse(url='http://example.com', body=body)\n-        self.assertEqual(get_meta_refresh(response), (0.1, 'http://example.com/index.html'))\n-\n-        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3.1;URL=index.html\" />\"\"\"\n-        response = TextResponse(url='http://example.com', body=body)\n-        self.assertEqual(get_meta_refresh(response), (3.1, 'http://example.com/index.html'))\n-\n     def test_response_httprepr(self):\n         r1 = Response(\"http://www.example.com\")\n         self.assertEqual(response_httprepr(r1), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n\n@@ -1,9 +1,6 @@\n-import os\n import unittest\n from scrapy.spider import BaseSpider\n-from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, \\\n-    url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url, \\\n-    urljoin_rfc, url_is_from_spider, file_uri_to_path, path_to_file_uri, any_to_uri\n+from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url\n \n class UrlUtilsTest(unittest.TestCase):\n \n@@ -55,144 +52,6 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))\n \n-    def test_urljoin_rfc(self):\n-        self.assertEqual(urljoin_rfc('http://example.com/some/path', 'newpath/test'),\n-                                     'http://example.com/some/newpath/test')\n-        self.assertEqual(urljoin_rfc('http://example.com/some/path/a.jpg', '../key/other'),\n-                                     'http://example.com/some/key/other')\n-        u = urljoin_rfc(u'http://example.com/lolo/\\xa3/lele', u'lala/\\xa3')\n-        self.assertEqual(u, 'http://example.com/lolo/\\xc2\\xa3/lala/\\xc2\\xa3')\n-        assert isinstance(u, str)\n-        u = urljoin_rfc(u'http://example.com/lolo/\\xa3/lele', 'lala/\\xa3', encoding='latin-1')\n-        self.assertEqual(u, 'http://example.com/lolo/\\xa3/lala/\\xa3')\n-        assert isinstance(u, str)\n-        u = urljoin_rfc('http://example.com/lolo/\\xa3/lele', 'lala/\\xa3')\n-        self.assertEqual(u, 'http://example.com/lolo/\\xa3/lala/\\xa3')\n-        assert isinstance(u, str)\n-\n-    def test_safe_url_string(self):\n-        # Motoko Kusanagi (Cyborg from Ghost in the Shell)\n-        motoko = u'\\u8349\\u8599 \\u7d20\\u5b50'\n-        self.assertEqual(safe_url_string(motoko),  # note the %20 for space\n-                        '%E8%8D%89%E8%96%99%20%E7%B4%A0%E5%AD%90')\n-        self.assertEqual(safe_url_string(motoko),\n-                         safe_url_string(safe_url_string(motoko)))\n-        self.assertEqual(safe_url_string(u'\\xa9'), # copyright symbol\n-                         '%C2%A9')\n-        self.assertEqual(safe_url_string(u'\\xa9', 'iso-8859-1'),\n-                         '%A9')\n-        self.assertEqual(safe_url_string(\"http://www.scrapy.org/\"),\n-                        'http://www.scrapy.org/')\n-\n-        alessi = u'/ecommerce/oggetto/Te \\xf2/tea-strainer/1273'\n-\n-        self.assertEqual(safe_url_string(alessi),\n-                         '/ecommerce/oggetto/Te%20%C3%B2/tea-strainer/1273')\n-\n-        self.assertEqual(safe_url_string(\"http://www.example.com/test?p(29)url(http://www.another.net/page)\"),\n-                                         \"http://www.example.com/test?p(29)url(http://www.another.net/page)\")\n-        self.assertEqual(safe_url_string(\"http://www.example.com/Brochures_&_Paint_Cards&PageSize=200\"),\n-                                         \"http://www.example.com/Brochures_&_Paint_Cards&PageSize=200\")\n-\n-        safeurl = safe_url_string(u\"http://www.example.com/\\xa3\", encoding='latin-1')\n-        self.assert_(isinstance(safeurl, str))\n-        self.assertEqual(safeurl, \"http://www.example.com/%A3\")\n-\n-        safeurl = safe_url_string(u\"http://www.example.com/\\xa3\", encoding='utf-8')\n-        self.assert_(isinstance(safeurl, str))\n-        self.assertEqual(safeurl, \"http://www.example.com/%C2%A3\")\n-\n-    def test_safe_download_url(self):\n-        self.assertEqual(safe_download_url('http://www.scrapy.org/../'),\n-                         'http://www.scrapy.org/')\n-        self.assertEqual(safe_download_url('http://www.scrapy.org/../../images/../image'),\n-                         'http://www.scrapy.org/image')\n-        self.assertEqual(safe_download_url('http://www.scrapy.org/dir/'),\n-                         'http://www.scrapy.org/dir/')\n-\n-    def test_url_query_parameter(self):\n-        self.assertEqual(url_query_parameter(\"product.html?id=200&foo=bar\", \"id\"),\n-                         '200')\n-        self.assertEqual(url_query_parameter(\"product.html?id=200&foo=bar\", \"notthere\", \"mydefault\"),\n-                         'mydefault')\n-        self.assertEqual(url_query_parameter(\"product.html?id=\", \"id\"),\n-                         None)\n-        self.assertEqual(url_query_parameter(\"product.html?id=\", \"id\", keep_blank_values=1),\n-                         '')\n-\n-    def test_url_query_parameter_2(self):\n-        \"\"\"\n-        This problem was seen several times in the feeds. Sometime affiliate URLs contains\n-        nested encoded affiliate URL with direct URL as parameters. For example:\n-        aff_url1 = 'http://www.tkqlhce.com/click-2590032-10294381?url=http%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FArgosCreateReferral%3FstoreId%3D10001%26langId%3D-1%26referrer%3DCOJUN%26params%3Dadref%253DGarden+and+DIY-%3EGarden+furniture-%3EChildren%26%2339%3Bs+garden+furniture%26referredURL%3Dhttp%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FProductDisplay%253FstoreId%253D10001%2526catalogId%253D1500001501%2526productId%253D1500357023%2526langId%253D-1'\n-        the typical code to extract needed URL from it is:\n-        aff_url2 = url_query_parameter(aff_url1, 'url')\n-        after this aff2_url is:\n-        'http://www.argos.co.uk/webapp/wcs/stores/servlet/ArgosCreateReferral?storeId=10001&langId=-1&referrer=COJUN&params=adref%3DGarden and DIY->Garden furniture->Children&#39;s gardenfurniture&referredURL=http://www.argos.co.uk/webapp/wcs/stores/servlet/ProductDisplay%3FstoreId%3D10001%26catalogId%3D1500001501%26productId%3D1500357023%26langId%3D-1'\n-        the direct URL extraction is\n-        url = url_query_parameter(aff_url2, 'referredURL')\n-        but this will not work, because aff_url2 contains &#39; (comma sign encoded in the feed)\n-        and the URL extraction will fail, current workaround was made in the spider,\n-        just a replace for &#39; to %27\n-        \"\"\"\n-        return # FIXME: this test should pass but currently doesnt\n-        # correct case\n-        aff_url1 = \"http://www.anrdoezrs.net/click-2590032-10294381?url=http%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FArgosCreateReferral%3FstoreId%3D10001%26langId%3D-1%26referrer%3DCOJUN%26params%3Dadref%253DGarden+and+DIY-%3EGarden+furniture-%3EGarden+table+and+chair+sets%26referredURL%3Dhttp%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FProductDisplay%253FstoreId%253D10001%2526catalogId%253D1500001501%2526productId%253D1500357199%2526langId%253D-1\"\n-        aff_url2 = url_query_parameter(aff_url1, 'url')\n-        self.assertEqual(aff_url2, \"http://www.argos.co.uk/webapp/wcs/stores/servlet/ArgosCreateReferral?storeId=10001&langId=-1&referrer=COJUN&params=adref%3DGarden and DIY->Garden furniture->Garden table and chair sets&referredURL=http://www.argos.co.uk/webapp/wcs/stores/servlet/ProductDisplay%3FstoreId%3D10001%26catalogId%3D1500001501%26productId%3D1500357199%26langId%3D-1\")\n-        prod_url = url_query_parameter(aff_url2, 'referredURL')\n-        self.assertEqual(prod_url, \"http://www.argos.co.uk/webapp/wcs/stores/servlet/ProductDisplay?storeId=10001&catalogId=1500001501&productId=1500357199&langId=-1\")\n-        # weird case\n-        aff_url1 = \"http://www.tkqlhce.com/click-2590032-10294381?url=http%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FArgosCreateReferral%3FstoreId%3D10001%26langId%3D-1%26referrer%3DCOJUN%26params%3Dadref%253DGarden+and+DIY-%3EGarden+furniture-%3EChildren%26%2339%3Bs+garden+furniture%26referredURL%3Dhttp%3A%2F%2Fwww.argos.co.uk%2Fwebapp%2Fwcs%2Fstores%2Fservlet%2FProductDisplay%253FstoreId%253D10001%2526catalogId%253D1500001501%2526productId%253D1500357023%2526langId%253D-1\"\n-        aff_url2 = url_query_parameter(aff_url1, 'url')\n-        self.assertEqual(aff_url2, \"http://www.argos.co.uk/webapp/wcs/stores/servlet/ArgosCreateReferral?storeId=10001&langId=-1&referrer=COJUN&params=adref%3DGarden and DIY->Garden furniture->Children&#39;s garden furniture&referredURL=http://www.argos.co.uk/webapp/wcs/stores/servlet/ProductDisplay%3FstoreId%3D10001%26catalogId%3D1500001501%26productId%3D1500357023%26langId%3D-1\")\n-        prod_url = url_query_parameter(aff_url2, 'referredURL')\n-        # fails, prod_url is None now\n-        self.assertEqual(prod_url, \"http://www.argos.co.uk/webapp/wcs/stores/servlet/ProductDisplay?storeId=10001&catalogId=1500001501&productId=1500357023&langId=-1\")\n-\n-    def test_add_or_replace_parameter(self):\n-        url = 'http://domain/test'\n-        self.assertEqual(add_or_replace_parameter(url, 'arg', 'v'),\n-                         'http://domain/test?arg=v')\n-        url = 'http://domain/test?arg1=v1&arg2=v2&arg3=v3'\n-        self.assertEqual(add_or_replace_parameter(url, 'arg4', 'v4'),\n-                         'http://domain/test?arg1=v1&arg2=v2&arg3=v3&arg4=v4')\n-        self.assertEqual(add_or_replace_parameter(url, 'arg3', 'nv3'),\n-                         'http://domain/test?arg1=v1&arg2=v2&arg3=nv3')\n-        url = 'http://domain/test?arg1=v1'\n-        self.assertEqual(add_or_replace_parameter(url, 'arg2', 'v2', sep=';'),\n-                         'http://domain/test?arg1=v1;arg2=v2')\n-        self.assertEqual(add_or_replace_parameter(\"http://domain/moreInfo.asp?prodID=\", 'prodID', '20'),\n-                         'http://domain/moreInfo.asp?prodID=20')\n-        url = 'http://rmc-offers.co.uk/productlist.asp?BCat=2%2C60&CatID=60'\n-        self.assertEqual(add_or_replace_parameter(url, 'BCat', 'newvalue', url_is_quoted=True),\n-                         'http://rmc-offers.co.uk/productlist.asp?BCat=newvalue&CatID=60')\n-        url = 'http://rmc-offers.co.uk/productlist.asp?BCat=2,60&CatID=60'\n-        self.assertEqual(add_or_replace_parameter(url, 'BCat', 'newvalue'),\n-                         'http://rmc-offers.co.uk/productlist.asp?BCat=newvalue&CatID=60')\n-\n-    def test_url_query_cleaner(self):\n-        self.assertEqual('product.html?id=200',\n-                url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id']))\n-        self.assertEqual('product.html?id=200',\n-                url_query_cleaner(\"product.html?&id=200&&foo=bar&name=wired\", ['id']))\n-        self.assertEqual('product.html',\n-                url_query_cleaner(\"product.html?foo=bar&name=wired\", ['id']))\n-        self.assertEqual('product.html?id=200&name=wired',\n-                url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id', 'name']))\n-        self.assertEqual('product.html?id',\n-                url_query_cleaner(\"product.html?id&other=3&novalue=\", ['id']))\n-        self.assertEqual('product.html?d=1&d=2&d=3',\n-                url_query_cleaner(\"product.html?d=1&e=b&d=2&d=3&other=other\", ['d'], unique=False))\n-        self.assertEqual('product.html?id=200&foo=bar',\n-                url_query_cleaner(\"product.html?id=200&foo=bar&name=wired#id20\", ['id', 'foo']))\n-        self.assertEqual('product.html?foo=bar&name=wired',\n-                url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id'], remove=True))\n-        self.assertEqual('product.html?name=wired',\n-                url_query_cleaner(\"product.html?id=2&foo=bar&name=wired\", ['id', 'foo'], remove=True))\n-        self.assertEqual('product.html?foo=bar&name=wired',\n-                url_query_cleaner(\"product.html?id=2&foo=bar&name=wired\", ['id', 'footo'], remove=True))\n-\n     def test_canonicalize_url(self):\n         # simplest case\n         self.assertEqual(canonicalize_url(\"http://www.example.com\"),\n@@ -283,50 +142,6 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(\"http://www.EXAMPLE.com\"),\n                                           \"http://www.example.com\")\n \n-    def test_path_to_file_uri(self):\n-        if os.name == 'nt':\n-            self.assertEqual(path_to_file_uri(\"C:\\\\windows\\clock.avi\"),\n-                             \"file:///C:/windows/clock.avi\")\n-        else:\n-            self.assertEqual(path_to_file_uri(\"/some/path.txt\"),\n-                             \"file:///some/path.txt\")\n-\n-        fn = \"test.txt\"\n-        x = path_to_file_uri(fn)\n-        self.assert_(x.startswith('file:///'))\n-        self.assertEqual(file_uri_to_path(x).lower(), os.path.abspath(fn).lower())\n-\n-    def test_file_uri_to_path(self):\n-        if os.name == 'nt':\n-            self.assertEqual(file_uri_to_path(\"file:///C:/windows/clock.avi\"),\n-                             \"C:\\\\windows\\clock.avi\")\n-            uri = \"file:///C:/windows/clock.avi\"\n-            uri2 = path_to_file_uri(file_uri_to_path(uri))\n-            self.assertEqual(uri, uri2)\n-        else:\n-            self.assertEqual(file_uri_to_path(\"file:///path/to/test.txt\"),\n-                             \"/path/to/test.txt\")\n-            self.assertEqual(file_uri_to_path(\"/path/to/test.txt\"),\n-                             \"/path/to/test.txt\")\n-            uri = \"file:///path/to/test.txt\"\n-            uri2 = path_to_file_uri(file_uri_to_path(uri))\n-            self.assertEqual(uri, uri2)\n-\n-        self.assertEqual(file_uri_to_path(\"test.txt\"),\n-                         \"test.txt\")\n-\n-    def test_any_to_uri(self):\n-        if os.name == 'nt':\n-            self.assertEqual(any_to_uri(\"C:\\\\windows\\clock.avi\"),\n-                             \"file:///C:/windows/clock.avi\")\n-        else:\n-            self.assertEqual(any_to_uri(\"/some/path.txt\"),\n-                             \"file:///some/path.txt\")\n-        self.assertEqual(any_to_uri(\"file:///some/path.txt\"),\n-                         \"file:///some/path.txt\")\n-        self.assertEqual(any_to_uri(\"http://www.example.com/some/path.txt\"),\n-                         \"http://www.example.com/some/path.txt\")\n-\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,61 +1,7 @@\n-from base64 import urlsafe_b64encode\n-\n-def headers_raw_to_dict(headers_raw):\n \"\"\"\n-    Convert raw headers (single multi-line string)\n-    to the dictionary.\n+Transitional module for moving to the w3lib library.\n \n-    For example:\n-    >>> headers_raw_to_dict(\"Content-type: text/html\\\\n\\\\rAccept: gzip\\\\n\\\\n\")\n-    {'Content-type': ['text/html'], 'Accept': ['gzip']}\n-\n-    Incorrect input:\n-    >>> headers_raw_to_dict(\"Content-typt gzip\\\\n\\\\n\")\n-    {}\n-\n-    Argument is None:\n-    >>> headers_raw_to_dict(None)\n+For new code, always import from w3lib.http instead of this module\n \"\"\"\n-    if headers_raw is None:\n-        return None\n-    return dict([\n-        (header_item[0].strip(), [header_item[1].strip()])\n-        for header_item\n-        in [\n-            header.split(':', 1)\n-            for header\n-            in headers_raw.splitlines()]\n-        if len(header_item) == 2])\n \n-\n-def headers_dict_to_raw(headers_dict):\n-    \"\"\"\n-    Returns a raw HTTP headers representation of headers\n-\n-    For example:\n-    >>> headers_dict_to_raw({'Content-type': 'text/html', 'Accept': 'gzip'})\n-    'Content-type: text/html\\\\r\\\\nAccept: gzip'\n-    >>> from twisted.python.util import InsensitiveDict\n-    >>> td = InsensitiveDict({'Content-type': ['text/html'], 'Accept': ['gzip']})\n-    >>> headers_dict_to_raw(td)\n-    'Content-type: text/html\\\\r\\\\nAccept: gzip'\n-\n-    Argument is None:\n-    >>> headers_dict_to_raw(None)\n-\n-    \"\"\"\n-    if headers_dict is None:\n-        return None\n-    raw_lines = []\n-    for key, value in headers_dict.items():\n-        if isinstance(value, (str, unicode)):\n-            raw_lines.append(\"%s: %s\" % (key, value))\n-        elif isinstance(value, (list, tuple)):\n-            for v in value:\n-                raw_lines.append(\"%s: %s\" % (key, v))\n-    return '\\r\\n'.join(raw_lines)\n-\n-\n-def basic_auth_header(username, password):\n-    \"\"\"Return `Authorization` header for HTTP Basic Access Authentication (RFC 2617)\"\"\"\n-    return 'Basic ' + urlsafe_b64encode(\"%s:%s\" % (username, password))\n+from w3lib.http import *\n\n@@ -1,165 +1,7 @@\n \"\"\"\n-Functions for dealing with markup text\n+Transitional module for moving to the w3lib library.\n+\n+For new code, always import from w3lib.html instead of this module\n \"\"\"\n \n-import re\n-from htmlentitydefs import name2codepoint\n-\n-from scrapy.utils.python import str_to_unicode\n-\n-_ent_re = re.compile(r'&(#?(x?))([^&;\\s]+);')\n-_tag_re = re.compile(r'<[a-zA-Z\\/!].*?>', re.DOTALL)\n-\n-def remove_entities(text, keep=(), remove_illegal=True, encoding='utf-8'):\n-    \"\"\"Remove entities from the given text.\n-\n-    'text' can be a unicode string or a regular string encoded in the given\n-    `encoding` (which defaults to 'utf-8').\n-\n-    If 'keep' is passed (with a list of entity names) those entities will\n-    be kept (they won't be removed).\n-\n-    It supports both numeric (&#nnnn; and &#hhhh;) and named (&nbsp; &gt;)\n-    entities.\n-\n-    If remove_illegal is True, entities that can't be converted are removed.\n-    If remove_illegal is False, entities that can't be converted are kept \"as\n-    is\". For more information see the tests.\n-\n-    Always returns a unicode string (with the entities removed).\n-    \"\"\"\n-\n-    def convert_entity(m):\n-        entity_body = m.group(3)\n-        if m.group(1):\n-            try:\n-                if m.group(2):\n-                    number = int(entity_body, 16)\n-                else:\n-                    number = int(entity_body, 10)\n-                # Numeric character references in the 80-9F range are typically\n-                # interpreted by browsers as representing the characters mapped\n-                # to bytes 80-9F in the Windows-1252 encoding. For more info\n-                # see: http://en.wikipedia.org/wiki/Character_encodings_in_HTML\n-                if 0x80 <= number <= 0x9f:\n-                    return chr(number).decode('cp1252')\n-            except ValueError:\n-                number = None\n-        else:\n-            if entity_body in keep:\n-                return m.group(0)\n-            else:\n-                number = name2codepoint.get(entity_body)\n-        if number is not None:\n-            try:\n-                return unichr(number)\n-            except ValueError:\n-                pass\n-\n-        return u'' if remove_illegal else m.group(0)\n-\n-    return _ent_re.sub(convert_entity, str_to_unicode(text, encoding))\n-\n-def has_entities(text, encoding=None):\n-    return bool(_ent_re.search(str_to_unicode(text, encoding)))\n-\n-def replace_tags(text, token='', encoding=None):\n-    \"\"\"Replace all markup tags found in the given text by the given token. By\n-    default token is a null string so it just remove all tags.\n-\n-    'text' can be a unicode string or a regular string encoded as 'utf-8'\n-\n-    Always returns a unicode string.\n-    \"\"\"\n-    return _tag_re.sub(token, str_to_unicode(text, encoding))\n-\n-\n-def remove_comments(text, encoding=None):\n-    \"\"\" Remove HTML Comments. \"\"\"\n-    return re.sub('<!--.*?-->', u'', str_to_unicode(text, encoding), re.DOTALL)\n-      \n-def remove_tags(text, which_ones=(), keep=(), encoding=None):\n-    \"\"\" Remove HTML Tags only. \n-\n-        which_ones and keep are both tuples, there are four cases:\n-\n-        which_ones, keep (1 - not empty, 0 - empty)\n-        1, 0 - remove all tags in which_ones\n-        0, 1 - remove all tags except the ones in keep\n-        0, 0 - remove all tags\n-        1, 1 - not allowd\n-    \"\"\"\n-\n-    assert not (which_ones and keep), 'which_ones and keep can not be given at the same time'\n-\n-    def will_remove(tag):\n-        if which_ones:\n-            return tag in which_ones\n-        else:\n-            return tag not in keep\n-\n-    def remove_tag(m):\n-        tag = m.group(1)\n-        return u'' if will_remove(tag) else m.group(0)\n-\n-    regex = '</?([^ >/]+).*?>'\n-    retags = re.compile(regex, re.DOTALL | re.IGNORECASE)\n-\n-    return retags.sub(remove_tag, str_to_unicode(text, encoding))\n-\n-def remove_tags_with_content(text, which_ones=(), encoding=None):\n-    \"\"\" Remove tags and its content.\n-        \n-        which_ones -- is a tuple of which tags with its content we want to remove.\n-                      if is empty do nothing.\n-    \"\"\"\n-    text = str_to_unicode(text, encoding)\n-    if which_ones:\n-        tags = '|'.join([r'<%s.*?</%s>|<%s\\s*/>' % (tag, tag, tag) for tag in which_ones])\n-        retags = re.compile(tags, re.DOTALL | re.IGNORECASE)\n-        text = retags.sub(u'', text)\n-    return text\n-    \n-\n-def replace_escape_chars(text, which_ones=('\\n', '\\t', '\\r'), replace_by=u'', \\\n-        encoding=None):\n-    \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n-\n-        which_ones -- is a tuple of which escape chars we want to remove.\n-                      By default removes \\n, \\t, \\r.\n-\n-        replace_by -- text to replace the escape chars for.\n-                      It defaults to '', so the escape chars are removed.\n-    \"\"\"\n-    for ec in which_ones:\n-        text = text.replace(ec, str_to_unicode(replace_by, encoding))\n-    return str_to_unicode(text, encoding)\n-\n-def unquote_markup(text, keep=(), remove_illegal=True, encoding=None):\n-    \"\"\"\n-    This function receives markup as a text (always a unicode string or a utf-8 encoded string) and does the following:\n-     - removes entities (except the ones in 'keep') from any part of it that it's not inside a CDATA\n-     - searches for CDATAs and extracts their text (if any) without modifying it.\n-     - removes the found CDATAs\n-    \"\"\"\n-    _cdata_re = re.compile(r'((?P<cdata_s><!\\[CDATA\\[)(?P<cdata_d>.*?)(?P<cdata_e>\\]\\]>))', re.DOTALL)\n-\n-    def _get_fragments(txt, pattern):\n-        offset = 0\n-        for match in pattern.finditer(txt):\n-            match_s, match_e = match.span(1)\n-            yield txt[offset:match_s]\n-            yield match\n-            offset = match_e\n-        yield txt[offset:]\n-\n-    text = str_to_unicode(text, encoding)\n-    ret_text = u''\n-    for fragment in _get_fragments(text, _cdata_re):\n-        if isinstance(fragment, basestring):\n-            # it's not a CDATA (so we try to remove its entities)\n-            ret_text += remove_entities(fragment, keep=keep, remove_illegal=remove_illegal)\n-        else:\n-            # it's a CDATA (so we just extract its content)\n-            ret_text += fragment.group('cdata_d')\n-    return ret_text\n+from w3lib.html import *\n\n@@ -1,34 +1,7 @@\n-from cStringIO import StringIO\n+\"\"\"\n+Transitional module for moving to the w3lib library.\n \n-def encode_multipart(data):\n-    \"\"\"Encode the given data to be used in a multipart HTTP POST. Data is a\n-    where keys are the field name, and values are either strings or tuples\n-    (filename, content) for file uploads.\n-\n-    This code is based on distutils.command.upload\n+For new code, always import from w3lib.form instead of this module\n \"\"\"\n \n-    # Build up the MIME payload for the POST data\n-    boundary = '--------------GHSKFJDLGDS7543FJKLFHRE75642756743254'\n-    sep_boundary = '\\r\\n--' + boundary\n-    end_boundary = sep_boundary + '--'\n-    body = StringIO()\n-    for key, value in data.items():\n-        # handle multiple entries for the same name\n-        if type(value) != type([]):\n-            value = [value]\n-        for value in value:\n-            if type(value) is tuple:\n-                fn = '; filename=\"%s\"' % value[0]\n-                value = value[1]\n-            else:\n-                fn = \"\"\n-\n-            body.write(sep_boundary)\n-            body.write('\\r\\nContent-Disposition: form-data; name=\"%s\"' % key)\n-            body.write(fn)\n-            body.write(\"\\r\\n\\r\\n\")\n-            body.write(value)\n-    body.write(end_boundary)\n-    body.write(\"\\r\\n\")\n-    return body.getvalue(), boundary\n+from w3lib.form import *\n\n@@ -4,16 +4,14 @@ scrapy.http.Response objects\n \"\"\"\n \n import os\n-import re\n import weakref\n import webbrowser\n import tempfile\n \n from twisted.web import http\n from twisted.web.http import RESPONSES\n+from w3lib import html\n \n-from scrapy.utils.markup import remove_entities, remove_comments\n-from scrapy.utils.url import safe_url_string, urljoin_rfc\n from scrapy.xlib.BeautifulSoup import BeautifulSoup\n from scrapy.http import Response, HtmlResponse\n \n@@ -27,37 +25,22 @@ def body_or_str(obj, unicode=True):\n     else:\n         return obj if unicode else obj.encode('utf-8')\n \n-BASEURL_RE = re.compile(ur'<base\\s+href\\s*=\\s*[\\\"\\']\\s*([^\\\"\\'\\s]+)\\s*[\\\"\\']', re.I)\n _baseurl_cache = weakref.WeakKeyDictionary()\n def get_base_url(response):\n-    \"\"\" Return the base url of the given response used to resolve relative links. \"\"\"\n+    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n     if response not in _baseurl_cache:\n-        match = BASEURL_RE.search(response.body_as_unicode()[0:4096])\n-        _baseurl_cache[response] = urljoin_rfc(response.url, match.group(1)) if match else response.url\n+        text = response.body_as_unicode()[0:4096]\n+        _baseurl_cache[response] = html.get_base_url(text, response.url, \\\n+            response.encoding)\n     return _baseurl_cache[response]\n \n-META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\\s*=\\s*(?P<quote>[\"\\'])(?P<int>(\\d*\\.)?\\d+)\\s*;\\s*url=(?P<url>.*?)(?P=quote)', \\\n-    re.DOTALL | re.IGNORECASE)\n _metaref_cache = weakref.WeakKeyDictionary()\n def get_meta_refresh(response):\n-    \"\"\"Parse the http-equiv parameter of the HTML meta element from the given\n-    response and return a tuple (interval, url) where interval is an integer\n-    containing the delay in seconds (or zero if not present) and url is a\n-    string with the absolute url to redirect.\n-\n-    If no meta redirect is found, (None, None) is returned.\n-    \"\"\"\n+    \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n-        body_chunk = remove_comments(remove_entities(response.body_as_unicode()[0:4096]))\n-        match = META_REFRESH_RE.search(body_chunk)\n-        if match:\n-            interval = float(match.group('int'))\n-            url = safe_url_string(match.group('url').strip(' \"\\''))\n-            url = urljoin_rfc(response.url, url)\n-            _metaref_cache[response] = (interval, url)\n-        else:\n-            _metaref_cache[response] = (None, None)\n-        #_metaref_cache[response] = match.groups() if match else (None, None)\n+        text = response.body_as_unicode()[0:4096]\n+        _metaref_cache[response] = html.get_meta_refresh(text, response.url, \\\n+            response.encoding)\n     return _metaref_cache[response]\n \n _beautifulsoup_cache = weakref.WeakKeyDictionary()\n\n@@ -1,6 +1,9 @@\n \"\"\"\n This module contains general purpose URL functions not found in the standard\n library.\n+\n+Some of the functions that used to be imported from this module have been moved\n+to the w3lib.url module. Always import those from there instead.\n \"\"\"\n \n import os\n@@ -10,6 +13,7 @@ import urllib\n import posixpath\n import cgi\n \n+from w3lib.url import *\n from scrapy.utils.python import unicode_to_str\n \n def url_is_from_any_domain(url, domains):\n@@ -26,109 +30,6 @@ def url_is_from_spider(url, spider):\n     return url_is_from_any_domain(url, [spider.name] + \\\n         getattr(spider, 'allowed_domains', []))\n \n-def urljoin_rfc(base, ref, encoding='utf-8'):\n-    \"\"\"Same as urlparse.urljoin but supports unicode values in base and ref\n-    parameters (in which case they will be converted to str using the given\n-    encoding).\n-\n-    Always returns a str.\n-    \"\"\"\n-    return urlparse.urljoin(unicode_to_str(base, encoding), \\\n-        unicode_to_str(ref, encoding))\n-\n-_reserved = ';/?:@&=+$|,#' # RFC 3986 (Generic Syntax)\n-_unreserved_marks = \"-_.!~*'()\" # RFC 3986 sec 2.3\n-_safe_chars = urllib.always_safe + '%' + _reserved + _unreserved_marks\n-\n-def safe_url_string(url, encoding='utf8'):\n-    \"\"\"Convert the given url into a legal URL by escaping unsafe characters\n-    according to RFC-3986.\n-\n-    If a unicode url is given, it is first converted to str using the given\n-    encoding (which defaults to 'utf-8'). When passing a encoding, you should\n-    use the encoding of the original page (the page from which the url was\n-    extracted from).\n-\n-    Calling this function on an already \"safe\" url will return the url\n-    unmodified.\n-\n-    Always returns a str.\n-    \"\"\"\n-    s = unicode_to_str(url, encoding)\n-    return urllib.quote(s,  _safe_chars)\n-\n-\n-_parent_dirs = re.compile(r'/?(\\.\\./)+')\n-\n-def safe_download_url(url):\n-    \"\"\" Make a url for download. This will call safe_url_string\n-    and then strip the fragment, if one exists. The path will\n-    be normalised.\n-\n-    If the path is outside the document root, it will be changed\n-    to be within the document root.\n-    \"\"\"\n-    safe_url = safe_url_string(url)\n-    scheme, netloc, path, query, _ = urlparse.urlsplit(safe_url)\n-    if path:\n-        path = _parent_dirs.sub('', posixpath.normpath(path))\n-        if url.endswith('/') and not path.endswith('/'):\n-            path += '/'\n-    else:\n-        path = '/'\n-    return urlparse.urlunsplit((scheme, netloc, path, query, ''))\n-\n-def is_url(text):\n-    return text.partition(\"://\")[0] in ('file', 'http', 'https')\n-\n-def url_query_parameter(url, parameter, default=None, keep_blank_values=0):\n-    \"\"\"Return the value of a url parameter, given the url and parameter name\"\"\"\n-    queryparams = cgi.parse_qs(urlparse.urlsplit(str(url))[3], \\\n-        keep_blank_values=keep_blank_values)\n-    return queryparams.get(parameter, [default])[0]\n-\n-def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='=', remove=False, unique=True):\n-    \"\"\"Clean url arguments leaving only those passed in the parameterlist keeping order\n-\n-    If remove is True, leave only those not in parameterlist.\n-    If unique is False, do not remove duplicated keys\n-    \"\"\"\n-    url = urlparse.urldefrag(url)[0]\n-    base, _, query = url.partition('?')\n-    seen = set()\n-    querylist = []\n-    for ksv in query.split(sep):\n-        k, _, _ = ksv.partition(kvsep)\n-        if unique and k in seen:\n-            continue\n-        elif remove and k in parameterlist:\n-            continue\n-        elif not remove and k not in parameterlist:\n-            continue\n-        else:\n-            querylist.append(ksv)\n-            seen.add(k)\n-    return '?'.join([base, sep.join(querylist)]) if querylist else base\n-\n-def add_or_replace_parameter(url, name, new_value, sep='&', url_is_quoted=False):\n-    \"\"\"Add or remove a parameter to a given url\"\"\"\n-    def has_querystring(url):\n-        _, _, _, query, _ = urlparse.urlsplit(url)\n-        return bool(query)\n-\n-    parameter = url_query_parameter(url, name, keep_blank_values=1)\n-    if url_is_quoted:\n-        parameter = urllib.quote(parameter)\n-    if parameter is None:\n-        if has_querystring(url):\n-            next_url = url + sep + name + '=' + new_value\n-        else:\n-            next_url = url + '?' + name + '=' + new_value\n-    else:\n-        next_url = url.replace(name+'='+parameter,\n-                               name+'='+new_value)\n-    return next_url\n-\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n         encoding=None):\n     \"\"\"Canonicalize the given url by applying the following procedures:\n@@ -155,27 +56,3 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n     path = safe_url_string(urllib.unquote(path))\n     fragment = '' if not keep_fragments else fragment\n     return urlparse.urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n-\n-def path_to_file_uri(path):\n-    \"\"\"Convert local filesystem path to legal File URIs as described in:\n-    http://en.wikipedia.org/wiki/File_URI_scheme\n-    \"\"\"\n-    x = urllib.pathname2url(os.path.abspath(path))\n-    if os.name == 'nt':\n-        x = x.replace('|', ':') # http://bugs.python.org/issue5861\n-    return 'file:///%s' % x.lstrip('/')\n-\n-def file_uri_to_path(uri):\n-    \"\"\"Convert File URI to local filesystem path according to:\n-    http://en.wikipedia.org/wiki/File_URI_scheme\n-    \"\"\"\n-    return urllib.url2pathname(urlparse.urlparse(uri).path)\n-\n-def any_to_uri(uri_or_path):\n-    \"\"\"If given a path name, return its File URI, otherwise return it\n-    unmodified\n-    \"\"\"\n-    if os.path.splitdrive(uri_or_path)[0]:\n-        return path_to_file_uri(uri_or_path)\n-    u = urlparse.urlparse(uri_or_path)\n-    return uri_or_path if u.scheme else path_to_file_uri(uri_or_path)\n\n@@ -120,7 +120,7 @@ setup_args = {\n \n try:\n     from setuptools import setup\n-    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml']\n+    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml', 'w3lib']\n     if sys.version_info < (2, 6):\n         setup_args['install_requires'] += ['simplejson']\n except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
