{"custom_id": "scrapy#fcc8d73840070943a853a392c7e098cdd70b7df5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3748 | Files Changed: 15 | Hunks: 15 | Methods Changed: 151 | Complexity Δ (Sum/Max): -493/0 | Churn Δ: 3748 | Churn Cumulative: 8638 | Contributors (this commit): 5 | Commits (past 90d): 30 | Contributors (cumulative): 37 | DMM Complexity: 0.41469013006885996\n\nDIFF:\n@@ -1,12 +0,0 @@\n-\"\"\"\n-This contrib implements an automatic extraction library based on an Instance\n-Based Learning (IBL) algorithm, as described in the following papers:\n-\n-    A hierarchical approach to wrapper induction\n-    http://portal.acm.org/citation.cfm?id=301191 \n-\n-    Extracting web data using instance based learning\n-    http://portal.acm.org/citation.cfm?id=1265174 \n-\n-This code requires the numpy library.\n-\"\"\"\n\n@@ -1,59 +0,0 @@\n-\"\"\"\n-Extended types for IBL extraction\n-\"\"\"\n-from itertools import chain \n-\n-from scrapy.contrib.ibl.extractors import text, html\n-\n-class FieldDescriptor(object):\n-    \"\"\"description of a scraped attribute\"\"\"\n-    __slots__ = ('name', 'description', 'extractor', 'required')\n-\n-    def __init__(self, name, description, extractor=text, required=False):\n-        self.name = name\n-        self.description = description\n-        self.extractor = extractor\n-        self.required = required\n-    \n-    @classmethod\n-    def from_field(cls, name, field):\n-        return cls(name, field.get('description'), \\\n-            field.get('ibl_extractor', text), field.get('required', False))\n-\n-    def __str__(self):\n-        return \"FieldDescriptor(%s)\" % self.name\n-\n-class ItemDescriptor(object):\n-    \"\"\"Simple auto scraping item descriptor. \n-\n-    This used to describe type-specific operations and may be overridden where\n-    necessary.\n-    \"\"\"\n-\n-    def __init__(self, name, description, attribute_descriptors):\n-        self.name = name\n-        self.attribute_map = dict((d.name, d) for d in attribute_descriptors)\n-        self._required_attributes = [d.name for d in attribute_descriptors \\\n-                if d.required]\n-\n-    @classmethod\n-    def from_item(cls, name, description, item):\n-        a = [FieldDescriptor.from_field(n, f) for n, f in item.fields.items()]\n-        return cls(name, description, a)\n-\n-    def validated(self, data):\n-        \"\"\"Only return the items in the data that are valid\"\"\"\n-        return [d for d in data if self._item_validates(d)]\n-\n-    def _item_validates(self, item):\n-        \"\"\"simply checks that all mandatory attributes are present\"\"\"\n-        variant_attrs = set(chain(*\n-            [v.keys() for v in item.get('variants', [])]))\n-        return all([(name in item or name in variant_attrs) \\\n-                for name in self._required_attributes])\n-\n-    def get_required_attributes(self):\n-        return self._required_attributes\n-    \n-    def __str__(self):\n-        return \"ItemDescriptor(%s)\" % self.name\n\n@@ -1,96 +0,0 @@\n-\"\"\"\n-IBL module\n-\n-This contains an extraction algorithm based on the paper Extracting Web Data\n-Using Instance-Based Learning by Yanhong Zhai and Bing Liu.\n-\n-It defines the InstanceBasedLearningExtractor class, which implements this\n-extraction algorithm.\n-\n-Main departures from the original algorithm:\n-    * there is no limit in prefix or suffix size\n-    * we have \"attribute adaptors\" that allow generic post processing and may\n-      affect the extraction process. For example, a price field may require a \n-      numeric value to be present.\n-    * tags can be inserted to extract regions not wrapped by html tags. These\n-      regions are then identified using the longest unique character prefix and\n-      suffix.\n-\"\"\"\n-from operator import itemgetter\n-from .regionextract import build_extraction_tree\n-from .pageparsing import parse_template, parse_extraction_page\n-from .pageobjects import TokenDict\n-\n-class InstanceBasedLearningExtractor(object):\n-    \"\"\"Implementation of the instance based learning algorithm to \n-    extract data from web pages.\n-    \"\"\"\n-\n-    def __init__(self, templates, type_descriptor=None, trace=False):\n-        \"\"\"Initialise this extractor\n-\n-        templates should contain a sequence of strings, each containing \n-        annotated html that will be used as templates for extraction.\n-\n-        Tags surrounding areas to be extracted must contain a \n-        'data-scrapy-annotate' attribute and the value must be the name\n-        of the attribute. If the tag was inserted and was not present in the \n-        original page, the data-scrapy-generated attribute must be present.\n-        \n-        type_descriptor may contain a type descriptor describing the item\n-        to be extracted.\n-        \n-        if trace is true, the returned extracted data will have a 'trace'\n-        property that contains a trace of the extraction execution.\n-        \"\"\"\n-        self.token_dict = TokenDict()\n-        parsed_plus_templates = [(parse_template(self.token_dict, t), t) for t in templates]\n-        parsed_plus_epages = [(p, parse_extraction_page(self.token_dict, t)) for p, t \\\n-               in parsed_plus_templates if _annotation_count(p)]\n-        parsed_templates = map(itemgetter(0), parsed_plus_epages)\n-        \n-        # templates with more attributes are considered first\n-        sorted_templates = sorted(parsed_templates, key=_annotation_count, reverse=True)\n-        self.extraction_trees = [build_extraction_tree(t, type_descriptor, \n-            trace) for t in sorted_templates]\n-        self.validated = type_descriptor.validated if type_descriptor else \\\n-                self._filter_not_none\n-\n-    def extract(self, html, pref_template_id=None, useone=False):\n-        \"\"\"extract data from an html page\n-        \n-        If pref_template_url is specified, the template with that url will be \n-        used first.\n-        if useone is True and no data was extracted, no additional template will\n-        be tried. If False and no data was extracted, try with rest of item templates\n-        \"\"\"\n-        extraction_page = parse_extraction_page(self.token_dict, html)\n-        if pref_template_id is not None:\n-            if useone:\n-                extraction_trees = [x for x in self.extraction_trees if x.template.id == pref_template_id]\n-            else:\n-                extraction_trees = sorted(self.extraction_trees, \n-                    key=lambda x: x.template.id != pref_template_id)\n-        else:\n-            extraction_trees = self.extraction_trees\n-\n-        for extraction_tree in extraction_trees:\n-            extracted = extraction_tree.extract(extraction_page)\n-            correctly_extracted = self.validated(extracted)\n-            extra_required = extraction_tree.template.extra_required_attrs\n-            correctly_extracted = [c for c in correctly_extracted if \\\n-                extra_required.intersection(c.keys()) == extra_required ]\n-            if len(correctly_extracted) > 0:\n-                return correctly_extracted, extraction_tree.template\n-        return None, None\n-\n-    def __str__(self):\n-        return \"InstanceBasedLearningExtractor[\\n%s\\n]\" % \\\n-                (',\\n'.join(map(str, self.extraction_trees)))\n-    \n-    @staticmethod\n-    def _filter_not_none(items):\n-        return [d for d in items if d is not None]\n-\n-def _annotation_count(template):\n-    return len(template.annotations)\n\n@@ -1,231 +0,0 @@\n-\"\"\"\n-Page objects\n-\n-This module contains objects representing pages and parts of pages (e.g. tokens\n-and annotations) used in the instance based learning algorithm.\n-\"\"\"\n-from itertools import chain\n-from numpy import array, ndarray\n-\n-from scrapy.contrib.ibl.htmlpage import HtmlTagType, HtmlPageRegion\n-\n-class TokenType(HtmlTagType):\n-    \"\"\"constants for token types\"\"\"\n-    WORD = 0\n-\n-class TokenDict(object):\n-    \"\"\"Mapping from parse tokens to integers\n-    \n-    >>> d = TokenDict()\n-    >>> d.tokenid('i')\n-    0\n-    >>> d.tokenid('b')\n-    1\n-    >>> d.tokenid('i')\n-    0\n-\n-    Tokens can be searched for by id\n-    >>> d.find_token(1)\n-    'b'\n-\n-    The lower 24 bits store the token reference and the higher bits the type.\n-    \"\"\"\n-    \n-    def __init__(self):\n-        self.token_ids = {}\n-\n-    def tokenid(self, token, token_type=TokenType.WORD):\n-        \"\"\"create an integer id from the token and token type passed\"\"\"\n-        tid = self.token_ids.setdefault(token, len(self.token_ids))\n-        return tid | (token_type << 24)\n-    \n-    @staticmethod\n-    def token_type(token):\n-        \"\"\"extract the token type from the token id passed\"\"\"\n-        return token >> 24\n-\n-    def find_token(self, tid):\n-        \"\"\"Search for a tag with the given ID\n-\n-        This is O(N) and is only intended for debugging\n-        \"\"\"\n-        tid &= 0xFFFFFF\n-        if tid >= len(self.token_ids) or tid < 0:\n-            raise ValueError(\"tag id %s out of range\" % tid)\n-\n-        for (token, token_id) in self.token_ids.items():\n-            if token_id == tid:\n-                return token\n-        assert False, \"token dictionary is corrupt\"\n-\n-    def token_string(self, tid):\n-        \"\"\"create a string representation of a token\n-\n-        This is O(N).\n-        \"\"\"\n-        templates = [\"%s\", \"<%s>\", \"</%s>\", \"<%s/>\"]\n-        return templates[tid >> 24] % self.find_token(tid)\n-\n-class PageRegion(object):\n-    \"\"\"A region in a page, defined by a start and end index\"\"\"\n-\n-    __slots__ = ('start_index', 'end_index')\n-    \n-    def __init__(self, start, end):\n-        self.start_index = start\n-        self.end_index = end\n-        \n-    def __str__(self):\n-        return \"%s(%s, %s)\" % (self.__class__.__name__, self.start_index,\n-                self.end_index)\n-    \n-    def __repr__(self):\n-        return str(self)\n-\n-class FragmentedHtmlPageRegion(HtmlPageRegion):\n-    \"\"\"An HtmlPageRegion consisting of possibly non-contiguous sub-regions\"\"\"\n-    def __new__(cls, htmlpage, regions):\n-        text = u''.join(regions)\n-        return HtmlPageRegion.__new__(cls, htmlpage, text)\n-\n-    def __init__(self, htmlpage, regions):\n-        self.htmlpage = htmlpage\n-        self.regions = regions\n-    \n-    @property\n-    def parsed_fragments(self):\n-        return chain(*(r.parsed_fragments for r in self.regions))\n-\n-class Page(object):\n-    \"\"\"Basic representation of a page. This consists of a reference to a\n-    dictionary of tokens and an array of raw token ids\n-    \"\"\"\n-\n-    __slots__ = ('token_dict', 'page_tokens')\n-\n-    def __init__(self, token_dict, page_tokens):\n-        self.token_dict = token_dict\n-        # use a numpy array becuase we can index/slice easily and efficiently\n-        if not isinstance(page_tokens, ndarray):\n-            page_tokens = array(page_tokens)\n-        self.page_tokens = page_tokens\n-\n-class TemplatePage(Page):\n-    __slots__ = ('annotations', 'id', 'ignored_regions', 'extra_required_attrs')\n-\n-    def __init__(self, token_dict, page_tokens, annotations, template_id=None, \\\n-            ignored_regions=None, extra_required=None):\n-        Page.__init__(self, token_dict, page_tokens)\n-        # ensure order is the same as start tag order in the original page\n-        annotations = sorted(annotations, key=lambda x: x.end_index, reverse=True)\n-        self.annotations = sorted(annotations, key=lambda x: x.start_index)\n-        self.id = template_id\n-        self.ignored_regions = [i if isinstance(i, PageRegion) else PageRegion(*i) \\\n-            for i in (ignored_regions or [])]\n-        self.extra_required_attrs = set(extra_required or [])\n-\n-    def __str__(self):\n-        summary = []\n-        for index, token in enumerate(self.page_tokens):\n-            text = \"%s: %s\" % (index, self.token_dict.find_token(token))\n-            summary.append(text)\n-        return \"TemplatePage\\n============\\nTokens: (index, token)\\n%s\\nAnnotations: %s\\n\" % \\\n-                ('\\n'.join(summary), '\\n'.join(map(str, self.annotations)))\n-\n-class ExtractionPage(Page):\n-    \"\"\"Parsed data belonging to a web page upon which we wish to perform\n-    extraction.\n-    \"\"\"\n-    __slots__ = ('htmlpage', 'token_page_indexes')\n-\n-    def __init__(self, htmlpage, token_dict, page_tokens, token_page_indexes):\n-        \"\"\"Construct a new ExtractionPage\n-\n-        Arguments:\n-            `htmlpage`: The source HtmlPage\n-            `token_dict`: Token Dictionary used for tokenization\n-            `page_tokens': array of page tokens for matching\n-            `token_page_indexes`: indexes of each token in the parsed htmlpage\n-        \"\"\"\n-        Page.__init__(self, token_dict, page_tokens)\n-        self.htmlpage = htmlpage\n-        self.token_page_indexes = token_page_indexes\n-    \n-    def htmlpage_region(self, start_token_index, end_token_index):\n-        \"\"\"The region in the HtmlPage corresonding to the area defined by\n-        the start_token_index and the end_token_index\n-\n-        This includes the tokens at the specified indexes\n-        \"\"\"\n-        start = self.token_page_indexes[start_token_index]\n-        end = self.token_page_indexes[end_token_index]\n-        return self.htmlpage.subregion(start, end)\n-\n-    def htmlpage_region_inside(self, start_token_index, end_token_index):\n-        \"\"\"The region in the HtmlPage corresonding to the area between \n-        the start_token_index and the end_token_index. \n-\n-        This excludes the tokens at the specified indexes\n-        \"\"\"\n-        start = self.token_page_indexes[start_token_index] + 1\n-        end = self.token_page_indexes[end_token_index] - 1\n-        return self.htmlpage.subregion(start, end)\n-    \n-    def htmlpage_tag(self, token_index):\n-        \"\"\"The HtmlPage tag at corresponding to the token at token_index\"\"\"\n-        return self.htmlpage.parsed_body[self.token_page_indexes[token_index]]\n-\n-    def __str__(self):\n-        summary = []\n-        for token, tindex in zip(self.page_tokens, self.token_page_indexes):\n-            text = \"%s page[%s]: %s\" % (self.token_dict.find_token(token), \n-                tindex, self.htmlpage.parsed_body[tindex])\n-            summary.append(text)\n-        return \"ExtractionPage\\n==============\\nTokens: %s\\n\\nRaw text: %s\\n\\n\" \\\n-                % ('\\n'.join(summary), self.htmlpage.body)\n-\n-class AnnotationText(object):\n-    __slots__ = ('start_text', 'follow_text')\n-\n-    def __init__(self, start_text=None, follow_text=None):\n-        self.start_text = start_text\n-        self.follow_text = follow_text\n-\n-    def __str__(self):\n-        return \"AnnotationText(%s..%s)\" % \\\n-                (repr(self.start_text), repr(self.follow_text))\n-\n-\n-class AnnotationTag(PageRegion):\n-    \"\"\"A tag that annotates part of the document\n-\n-    It has the following properties:\n-        start_index - index of the token for the opening tag\n-        end_index - index of the token for the closing tag\n-        surrounds_attribute - the attribute name surrounded by this tag\n-        tag_attributes - list of (tag attribute, extracted attribute) tuples\n-                         for each item to be extracted from a tag attribute\n-        annotation_text - text prefix and suffix for the attribute to be extracted\n-        metadata - dict with annotation data not used by IBL extractor\n-    \"\"\"\n-    __slots__ = ('surrounds_attribute', 'start_index', 'end_index',\n-            'tag_attributes', 'annotation_text', 'variant_id', \n-            'metadata')\n-    \n-    def __init__(self, start_index, end_index, surrounds_attribute=None, \n-            annotation_text=None, tag_attributes=None, variant_id=None):\n-        PageRegion.__init__(self, start_index, end_index)\n-        self.surrounds_attribute = surrounds_attribute\n-        self.annotation_text = annotation_text\n-        self.tag_attributes = tag_attributes or []\n-        self.variant_id = variant_id\n-        self.metadata = {}\n-\n-    def __str__(self):\n-        return \"AnnotationTag(%s)\" % \", \".join(\n-                [\"%s=%s\" % (s, getattr(self, s)) \\\n-                for s in self.__slots__ if getattr(self, s)])\n-\n-    def __repr__(self):\n-        return str(self)\n-\n\n@@ -1,310 +0,0 @@\n-\"\"\"\n-Page parsing\n-\n-Parsing of web pages for extraction task.\n-\"\"\"\n-from collections import defaultdict\n-from numpy import array\n-\n-from scrapy.utils.py26 import json\n-\n-from scrapy.contrib.ibl.htmlpage import HtmlTagType, HtmlTag, HtmlPage\n-from scrapy.contrib.ibl.extraction.pageobjects import (AnnotationTag,\n-    TemplatePage, ExtractionPage, AnnotationText, TokenDict, FragmentedHtmlPageRegion)\n-\n-def parse_strings(template_html, extraction_html):\n-    \"\"\"Create a template and extraction page from raw strings\n-\n-    this is useful for testing purposes\n-    \"\"\"\n-    t = TokenDict()\n-    template_page = HtmlPage(body=template_html)\n-    extraction_page = HtmlPage(body=extraction_html)\n-    return (parse_template(t, template_page), \n-            parse_extraction_page(t, extraction_page))\n-\n-def parse_template(token_dict, template_html):\n-    \"\"\"Create an TemplatePage object by parsing the annotated html\"\"\"\n-    parser = TemplatePageParser(token_dict)\n-    parser.feed(template_html)\n-    return parser.to_template()\n-\n-def parse_extraction_page(token_dict, page_html):\n-    \"\"\"Create an ExtractionPage object by parsing the html\"\"\"\n-    parser = ExtractionPageParser(token_dict)\n-    parser.feed(page_html)\n-    return parser.to_extraction_page()\n-\n-class InstanceLearningParser(object):\n-    \"\"\"Base parser for instance based learning algorithm\n-    \n-    This does not require correct HTML and the parsing method should not alter\n-    the original tag order. It is important that parsing results do not vary.\n-    \"\"\"\n-    def __init__(self, token_dict):\n-        self.token_dict = token_dict\n-        self.token_list = []\n-    \n-    def _add_token(self, token, token_type, start, end):\n-        tid = self.token_dict.tokenid(token, token_type)\n-        self.token_list.append(tid)\n-\n-    def feed(self, html_page):\n-        self.html_page = html_page\n-        self.previous_element_class = None\n-        for index, data in enumerate(html_page.parsed_body):\n-            if isinstance(data, HtmlTag):\n-                self._add_token(data.tag, data.tag_type, data.start, data.end)\n-                self.handle_tag(data, index)\n-            else:\n-                self.handle_data(data, index)\n-            self.previous_element_class = data.__class__\n-\n-    def handle_data(self, html_data_fragment, index):\n-        pass\n-\n-    def handle_tag(self, html_tag, index):\n-        pass\n-\n-_END_UNPAIREDTAG_TAGS = [\"form\", \"div\", \"p\", \"table\", \"tr\", \"td\"]\n-\n-class TemplatePageParser(InstanceLearningParser):\n-    \"\"\"Template parsing for instance based learning algorithm\"\"\"\n-\n-    def __init__(self, token_dict):\n-        InstanceLearningParser.__init__(self, token_dict)\n-        self.annotations = []\n-        self.ignored_regions = []\n-        self.extra_required_attrs = []\n-        self.ignored_tag_stacks = defaultdict(list)\n-        # tag names that have not been completed\n-        self.labelled_tag_stacks = defaultdict(list)\n-        self.replacement_stacks = defaultdict(list)\n-        self.unpairedtag_stack = []\n-        self.variant_stack = []\n-        self.prev_data = None\n-        self.last_text_region = None\n-        self.next_tag_index = 0\n-\n-    def handle_tag(self, html_tag, index):\n-        if self.last_text_region:\n-            self._process_text('')\n-        \n-        if html_tag.tag_type == HtmlTagType.OPEN_TAG:\n-            self._handle_open_tag(html_tag)\n-        elif html_tag.tag_type == HtmlTagType.CLOSE_TAG:\n-            self._handle_close_tag(html_tag)\n-        else:\n-            # the tag is not paired, it can contain only attribute annotations\n-            self._handle_unpaired_tag(html_tag)\n-    \n-    @staticmethod\n-    def _read_template_annotation(html_tag):\n-        template_attr = html_tag.attributes.get('data-scrapy-annotate')\n-        if template_attr is None:\n-            return None\n-        unescaped = template_attr.replace('&quot;', '\"')\n-        return json.loads(unescaped)\n-    \n-    @staticmethod\n-    def _read_bool_template_attribute(html_tag, attribute):\n-        return html_tag.attributes.get(\"data-scrapy-\" + attribute) == \"true\"\n-    \n-    def _close_unpaired_tag(self):\n-        self.unpairedtag_stack[0].end_index = self.next_tag_index\n-        self.unpairedtag_stack = []\n-\n-    def _handle_unpaired_tag(self, html_tag):\n-        if self._read_bool_template_attribute(html_tag, \"ignore\") and html_tag.tag == \"img\":\n-            self.ignored_regions.append((self.next_tag_index, self.next_tag_index + 1))\n-        elif self._read_bool_template_attribute(html_tag, \"ignore-beneath\"):\n-            self.ignored_regions.append((self.next_tag_index, None))\n-        jannotation = self._read_template_annotation(html_tag)\n-        if jannotation:\n-            if self.unpairedtag_stack:\n-                self._close_unpaired_tag()\n-                \n-            annotation = AnnotationTag(self.next_tag_index, self.next_tag_index + 1)\n-            attribute_annotations = jannotation.pop('annotations', {}).items()\n-            for extract_attribute, tag_value in attribute_annotations:\n-                if extract_attribute == 'content':\n-                    annotation.surrounds_attribute = tag_value\n-                    self.unpairedtag_stack.append(annotation)\n-                else:\n-                    annotation.tag_attributes.append((extract_attribute, tag_value))\n-            self.annotations.append(annotation)\n-\n-            self.extra_required_attrs.extend(jannotation.pop('required', []))\n-            annotation.metadata = jannotation\n-\n-        self.next_tag_index += 1\n-\n-    def _handle_open_tag(self, html_tag):\n-        if self._read_bool_template_attribute(html_tag, \"ignore\"):\n-            if html_tag.tag == \"img\":\n-                self.ignored_regions.append((self.next_tag_index, self.next_tag_index + 1))\n-            else:\n-                self.ignored_regions.append((self.next_tag_index, None))\n-                self.ignored_tag_stacks[html_tag.tag].append(html_tag)\n-                \n-        elif self.ignored_tag_stacks.get(html_tag.tag):\n-            self.ignored_tag_stacks[html_tag.tag].append(None)\n-        if self._read_bool_template_attribute(html_tag, \"ignore-beneath\"):\n-            self.ignored_regions.append((self.next_tag_index, None))\n-        \n-        replacement = html_tag.attributes.pop(\"data-scrapy-replacement\", None)\n-        if replacement:\n-            self.token_list.pop()\n-            self._add_token(replacement, html_tag.tag_type, html_tag.start, html_tag.end)\n-            self.replacement_stacks[html_tag.tag].append(replacement)\n-        elif html_tag.tag in self.replacement_stacks:\n-            self.replacement_stacks[html_tag.tag].append(None)\n-\n-        if self.unpairedtag_stack:\n-            if html_tag.tag in _END_UNPAIREDTAG_TAGS:\n-                self._close_unpaired_tag()\n-            else:\n-                self.unpairedtag_stack.append(html_tag.tag)\n-            \n-        # can't be a p inside another p. Also, an open p element closes\n-        # a previous open p element.\n-        if html_tag.tag == \"p\" and html_tag.tag in self.labelled_tag_stacks:\n-            annotation = self.labelled_tag_stacks.pop(html_tag.tag)[0]\n-            annotation.end_index = self.next_tag_index\n-            self.annotations.append(annotation)\n-                \n-        jannotation = self._read_template_annotation(html_tag)\n-        if not jannotation:\n-            if html_tag.tag in self.labelled_tag_stacks:\n-                # add this tag to the stack to match correct end tag\n-                self.labelled_tag_stacks[html_tag.tag].append(None)\n-            self.next_tag_index += 1\n-            return\n-        \n-        annotation = AnnotationTag(self.next_tag_index, None)\n-        if jannotation.pop('generated', False):\n-            self.token_list.pop()\n-            annotation.start_index -= 1\n-            if self.previous_element_class == HtmlTag:\n-                annotation.annotation_text = AnnotationText('')\n-            else:\n-                annotation.annotation_text = AnnotationText(self.prev_data)\n-            if self._read_bool_template_attribute(html_tag, \"ignore\") \\\n-                    or self._read_bool_template_attribute(html_tag, \"ignore-beneath\"):\n-                ignored = self.ignored_regions.pop()\n-                self.ignored_regions.append((ignored[0]-1, ignored[1]))\n-                \n-        self.extra_required_attrs.extend(jannotation.pop('required', []))\n-        \n-        attribute_annotations = jannotation.pop('annotations', {}).items()\n-        for extract_attribute, tag_value in attribute_annotations:\n-            if extract_attribute == 'content':\n-                annotation.surrounds_attribute = tag_value\n-            else:\n-                annotation.tag_attributes.append((extract_attribute, tag_value))\n- \n-        variant_id = jannotation.pop('variant', 0)\n-        if variant_id > 0:\n-            if annotation.surrounds_attribute is not None:\n-                self.variant_stack.append(variant_id)\n-            else:\n-                annotation.variant_id = variant_id\n-       \n-        annotation.metadata = jannotation\n-\n-        if annotation.annotation_text is None:\n-            self.next_tag_index += 1\n-        if self.variant_stack and annotation.variant_id is None:\n-            variant_id = self.variant_stack[-1]\n-            if variant_id == '0':\n-                variant_id = None\n-            annotation.variant_id = variant_id\n-        \n-        # look for a closing tag if the content is important\n-        if annotation.surrounds_attribute:\n-            self.labelled_tag_stacks[html_tag.tag].append(annotation)\n-        else:\n-            annotation.end_index = annotation.start_index + 1\n-            self.annotations.append(annotation)\n-\n-    def _handle_close_tag(self, html_tag):\n-        \n-        if self.unpairedtag_stack:\n-            if html_tag.tag == self.unpairedtag_stack[-1]:\n-                self.unpairedtag_stack.pop()\n-            else:\n-                self._close_unpaired_tag()\n-        ignored_tags = self.ignored_tag_stacks.get(html_tag.tag)\n-        if ignored_tags is not None:\n-            tag = ignored_tags.pop()\n-            if isinstance(tag, HtmlTag):\n-                for i in range(-1, -len(self.ignored_regions) - 1, -1):\n-                    if self.ignored_regions[i][1] is None:\n-                        self.ignored_regions[i] = (self.ignored_regions[i][0], self.next_tag_index)\n-                        break\n-            if len(ignored_tags) == 0:\n-                del self.ignored_tag_stacks[html_tag.tag]\n-\n-        if html_tag.tag in self.replacement_stacks:\n-            replacement = self.replacement_stacks[html_tag.tag].pop()\n-            if replacement:\n-                self.token_list.pop()\n-                self._add_token(replacement, html_tag.tag_type, html_tag.start, html_tag.end)\n-            if len(self.replacement_stacks[html_tag.tag]) == 0:\n-                del self.replacement_stacks[html_tag.tag]\n-\n-        labelled_tags = self.labelled_tag_stacks.get(html_tag.tag)\n-        if labelled_tags is None:\n-            self.next_tag_index += 1\n-            return\n-        annotation = labelled_tags.pop()\n-        if annotation is None:\n-            self.next_tag_index += 1\n-        else:\n-            annotation.end_index = self.next_tag_index\n-            self.annotations.append(annotation)\n-            if annotation.annotation_text is not None:\n-                self.token_list.pop()\n-                self.last_text_region = annotation\n-            else:\n-                self.next_tag_index += 1\n-            if len(labelled_tags) == 0:\n-                del self.labelled_tag_stacks[html_tag.tag]\n-            if annotation.variant_id and self.variant_stack:\n-                prev = self.variant_stack.pop()\n-                if prev != annotation.variant_id:\n-                    raise ValueError(\"unbalanced variant annotation tags\")\n-                    \n-    def handle_data(self, html_data_fragment, index):\n-        fragment_text = self.html_page.fragment_data(html_data_fragment)\n-        self._process_text(fragment_text)\n-\n-    def _process_text(self, text):\n-        if self.last_text_region is not None:\n-            self.last_text_region.annotation_text.follow_text = text\n-            self.last_text_region = None\n-        self.prev_data = text\n-\n-    def to_template(self):\n-        \"\"\"create a TemplatePage from the data fed to this parser\"\"\"\n-        return TemplatePage(self.token_dict, self.token_list, self.annotations,\n-                self.html_page.page_id, self.ignored_regions, self.extra_required_attrs)\n-\n-class ExtractionPageParser(InstanceLearningParser):\n-    \"\"\"Parse an HTML page for extraction using the instance based learning\n-    algorithm\n-\n-    This needs to extract the tokens in a similar way to LabelledPageParser,\n-    it needs to also maintain a mapping from token index to the original content\n-    so that once regions are identified, the original content can be extracted.\n-    \"\"\"\n-    def __init__(self, token_dict):\n-        InstanceLearningParser.__init__(self, token_dict)\n-        self._page_token_indexes = []\n-\n-    def handle_tag(self, html_tag, index):\n-        self._page_token_indexes.append(index)\n-    \n-    def to_extraction_page(self):\n-        return ExtractionPage(self.html_page, self.token_dict, array(self.token_list), \n-                self._page_token_indexes)\n\n@@ -1,627 +0,0 @@\n-\"\"\"\n-Region Extract\n-\n-Custom extraction for regions in a document\n-\"\"\"\n-import re\n-import operator\n-import copy\n-import pprint\n-import cStringIO\n-from itertools import groupby, izip, starmap\n-\n-from numpy import array\n-\n-from scrapy.contrib.ibl.descriptor import FieldDescriptor\n-from scrapy.contrib.ibl.htmlpage import HtmlPageRegion\n-from scrapy.contrib.ibl.extraction.similarity import (similar_region,\n-    longest_unique_subsequence, common_prefix)\n-from scrapy.contrib.ibl.extraction.pageobjects import (AnnotationTag,\n-    PageRegion, FragmentedHtmlPageRegion)\n-\n-def build_extraction_tree(template, type_descriptor, trace=True):\n-    \"\"\"Build a tree of region extractors corresponding to the \n-    template\n-    \"\"\"\n-    attribute_map = type_descriptor.attribute_map if type_descriptor else None\n-    extractors = BasicTypeExtractor.create(template.annotations, attribute_map)\n-    if trace:\n-        extractors = TraceExtractor.apply(template, extractors)\n-    for cls in (AdjacentVariantExtractor, RepeatedDataExtractor, AdjacentVariantExtractor, RepeatedDataExtractor,\n-            RecordExtractor):\n-        extractors = cls.apply(template, extractors)\n-        if trace:\n-            extractors = TraceExtractor.apply(template, extractors)\n-\n-    return TemplatePageExtractor(template, extractors)\n-\n-_EXTRACT_HTML = lambda x: x\n-_DEFAULT_DESCRIPTOR = FieldDescriptor('none', None)\n-\n-def _labelled(obj):\n-    \"\"\"\n-    Returns labelled element of the object (extractor or labelled region)\n-    \"\"\"\n-    return getattr(obj, 'annotation', obj)\n-\n-def _compose(f, g):\n-    \"\"\"given unary functions f and g, return a function that computes f(g(x))\n-    \"\"\"\n-    def _exec(x):\n-        ret = g(x)\n-        return f(ret) if ret is not None else None\n-    return _exec\n-\n-class BasicTypeExtractor(object):\n-    \"\"\"The BasicTypeExtractor extracts single attributes corresponding to \n-    annotations.\n-    \n-    For example:\n-    >>> from scrapy.contrib.ibl.extraction.pageparsing import parse_strings\n-    >>> template, page = parse_strings( \\\n-        u'<h1 data-scrapy-annotate=\"{&quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">x</h1>', u'<h1> a name</h1>')\n-    >>> ex = BasicTypeExtractor(template.annotations[0])\n-    >>> ex.extract(page, 0, 1, None)\n-    [(u'name', u' a name')]\n-\n-    It supports attribute descriptors\n-    >>> descriptor = FieldDescriptor('name', None, lambda x: x.strip())\n-    >>> ex = BasicTypeExtractor(template.annotations[0], {'name': descriptor})\n-    >>> ex.extract(page, 0, 1, None)\n-    [(u'name', u'a name')]\n-    \n-    It supports ignoring regions\n-    >>> template, page = parse_strings(\\\n-        u'<div data-scrapy-annotate=\"{&quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">x<b> xx</b></div>',\\\n-        u'<div>a name<b> id-9</b></div>')\n-    >>> ex = BasicTypeExtractor(template.annotations[0])\n-    >>> ex.extract(page, 0, 3, [PageRegion(1, 2)])\n-    [(u'name', u'a name')]\n-    \"\"\"\n-\n-    def __init__(self, annotation, attribute_descriptors=None):\n-        self.annotation = annotation\n-        if attribute_descriptors is None:\n-            attribute_descriptors = {}\n-\n-        if annotation.surrounds_attribute:\n-            descriptor = attribute_descriptors.get(annotation.surrounds_attribute)\n-            if descriptor:\n-                self.content_validate = descriptor.extractor\n-            else:\n-                self.content_validate = _EXTRACT_HTML\n-            self.extract = self._extract_content\n-\n-        if annotation.tag_attributes:\n-            self.tag_data = []\n-            for (tag_attr, extraction_attr) in annotation.tag_attributes:\n-                descriptor = attribute_descriptors.get(extraction_attr)\n-                extractf = descriptor.extractor if descriptor else _EXTRACT_HTML\n-                self.tag_data.append((extractf, tag_attr, extraction_attr))\n-\n-            self.extract = self._extract_both if \\\n-                    annotation.surrounds_attribute else self._extract_attribute\n-        \n-    def _extract_both(self, page, start_index, end_index, ignored_regions=None):\n-        return self._extract_content(page, start_index, end_index, ignored_regions) + \\\n-            self._extract_attribute(page, start_index, end_index, ignored_regions)\n-\n-    def _extract_content(self, extraction_page, start_index, end_index, ignored_regions=None):\n-        # extract content between annotation indexes\n-        if not ignored_regions:\n-            region = extraction_page.htmlpage_region_inside(start_index, end_index)\n-        else:\n-            # assumes ignored_regions are completely contained within start and end index\n-            assert (start_index <= ignored_regions[0].start_index and \n-                end_index >= ignored_regions[-1].end_index)\n-            starts = [start_index] + [i.end_index for i in ignored_regions]\n-            ends = [i.start_index for i in ignored_regions]\n-            if starts[-1] is not None:\n-                ends.append(end_index)\n-            included_regions = izip(starts, ends)\n-            if ends[0] is None:\n-                included_regions.next()\n-            regions = starmap(extraction_page.htmlpage_region_inside, included_regions)\n-            region = FragmentedHtmlPageRegion(extraction_page.htmlpage, list(regions))\n-        validated = self.content_validate(region)\n-        return [(self.annotation.surrounds_attribute, validated)] if validated else []\n-    \n-    def _extract_attribute(self, extraction_page, start_index, end_index, ignored_regions=None):\n-        data = []\n-        for (f, ta, ea) in self.tag_data:\n-            tag_value = extraction_page.htmlpage_tag(start_index).attributes.get(ta)\n-            if tag_value:\n-                region = HtmlPageRegion(extraction_page.htmlpage, tag_value)\n-                extracted = f(region)\n-                if extracted is not None:\n-                    data.append((ea, extracted))\n-        return data\n-\n-    @classmethod\n-    def create(cls, annotations, attribute_descriptors=None):\n-        \"\"\"Create a list of basic extractors from the given annotations\n-        and attribute descriptors\n-        \"\"\"\n-        if attribute_descriptors is None:\n-            attribute_descriptors = {}\n-        return [cls._create_basic_extractor(annotation, attribute_descriptors) \\\n-            for annotation in annotations \\\n-            if annotation.surrounds_attribute or annotation.tag_attributes]\n-    \n-    @staticmethod\n-    def _create_basic_extractor(annotation, attribute_descriptors):\n-        \"\"\"Create a basic type extractor for the annotation\"\"\"\n-        text_region = annotation.annotation_text\n-        if text_region is not None:\n-            region_extract = TextRegionDataExtractor(text_region.start_text, \n-                text_region.follow_text).extract\n-            # copy attribute_descriptors and add the text extractor\n-            descriptor_copy = dict(attribute_descriptors)\n-            attr_descr = descriptor_copy.get(annotation.surrounds_attribute, \n-                    _DEFAULT_DESCRIPTOR)\n-            attr_descr = copy.copy(attr_descr)\n-            attr_descr.extractor = _compose(attr_descr.extractor, region_extract)\n-            descriptor_copy[annotation.surrounds_attribute] = attr_descr\n-            attribute_descriptors = descriptor_copy\n-        return BasicTypeExtractor(annotation, attribute_descriptors)\n-    \n-    def extracted_item(self):\n-        \"\"\"key used to identify the item extracted\"\"\"\n-        return (self.annotation.surrounds_attribute, self.annotation.tag_attributes)\n-    \n-    def __repr__(self):\n-        return str(self)\n-\n-    def __str__(self):\n-        messages = ['BasicTypeExtractor(']\n-        if self.annotation.surrounds_attribute:\n-            messages.append(self.annotation.surrounds_attribute)\n-            if self.content_validate != _EXTRACT_HTML:\n-                messages += [', extracted with \\'', \n-                        self.content_validate.__name__, '\\'']\n-        \n-        if self.annotation.tag_attributes:\n-            if self.annotation.surrounds_attribute:\n-                messages.append(';')\n-            for (f, ta, ea) in self.tag_data:\n-                messages += [ea, ': tag attribute \"', ta, '\"']\n-                if f != _EXTRACT_HTML:\n-                    messages += [', validated by ', str(f)]\n-        messages.append(\", template[%s:%s])\" % \\\n-                (self.annotation.start_index, self.annotation.end_index))\n-        return ''.join(messages)\n-\n-class RepeatedDataExtractor(object):\n-    \"\"\"Data extractor for handling repeated data\"\"\"\n-\n-    def __init__(self, prefix, suffix, extractors):\n-        self.prefix = array(prefix)\n-        self.suffix = array(suffix)\n-        self.extractor = copy.copy(extractors[0])\n-        self.annotation = copy.copy(self.extractor.annotation)\n-        self.annotation.end_index = extractors[-1].annotation.end_index\n-\n-    def extract(self, page, start_index, end_index, ignored_regions):\n-        \"\"\"repeatedly find regions bounded by the repeated \n-        prefix and suffix and extract them\n-        \"\"\"\n-        prefixlen = len(self.prefix)\n-        suffixlen = len(self.suffix)\n-        index = max(0, start_index - prefixlen)\n-        max_index = min(len(page.page_tokens) - suffixlen, end_index + len(self.suffix))\n-        max_start_index = max_index - prefixlen\n-        extracted = []\n-        while index <= max_start_index:\n-            prefix_end = index + prefixlen\n-            if (page.page_tokens[index:prefix_end] == self.prefix).all():\n-                for peek in xrange(prefix_end, max_index):\n-                    if (page.page_tokens[peek:peek + suffixlen] \\\n-                            == self.suffix).all():\n-                        extracted += self.extractor.extract(page, \n-                                prefix_end - 1, peek, ignored_regions)\n-                        index = max(peek, index + 1)\n-                        break\n-                else:\n-                    break\n-            else:\n-                index += 1\n-        return extracted\n-\n-    @staticmethod\n-    def apply(template, extractors):\n-        tokens = template.page_tokens\n-        output_extractors = []\n-        group_key = lambda x: x.extracted_item()\n-        for extr_key, extraction_group in groupby(extractors, group_key):\n-            extraction_group = list(extraction_group)\n-            if extr_key is None or len(extraction_group) == 1:\n-                output_extractors += extraction_group\n-                continue\n-\n-            separating_tokens = [ \\\n-                tokens[x.annotation.end_index:y.annotation.start_index+1] \\\n-                for (x, y) in zip(extraction_group[:-1], extraction_group[1:])]\n-            \n-            # calculate the common prefix\n-            group_start = extraction_group[0].annotation.start_index\n-            prefix_start = max(0, group_start - len(separating_tokens[0]))\n-            first_prefix = tokens[prefix_start:group_start+1]\n-            prefixes = [first_prefix] + separating_tokens\n-            prefix_pattern = list(reversed(\n-                common_prefix(*map(reversed, prefixes))))\n-            \n-            # calculate the common suffix\n-            group_end = extraction_group[-1].annotation.end_index\n-            last_suffix = tokens[group_end:group_end + \\\n-                    len(separating_tokens[-1])]\n-            suffixes = separating_tokens + [last_suffix]\n-            suffix_pattern = common_prefix(*suffixes)\n-            \n-            # create a repeated data extractor, if there is a suitable \n-            # prefix and suffix. (TODO: tune this heuristic)\n-            matchlen = len(prefix_pattern) + len(suffix_pattern)\n-            if matchlen >= len(separating_tokens):\n-                group_extractor = RepeatedDataExtractor(prefix_pattern, \n-                    suffix_pattern, extraction_group)\n-                output_extractors.append(group_extractor)\n-            else:\n-                output_extractors += extraction_group\n-        return output_extractors\n-    \n-    def extracted_item(self):\n-        \"\"\"key used to identify the item extracted\"\"\"\n-        return self.extractor.extracted_item()\n-    \n-    def __repr__(self):\n-        return \"Repeat(%r)\" % self.extractor\n-\n-    def __str__(self):\n-        return \"Repeat(%s)\" % self.extractor\n-\n-class TransposedDataExtractor(object):\n-    \"\"\" \"\"\"\n-    pass\n-\n-_namef = operator.itemgetter(0)\n-_valuef = operator.itemgetter(1)\n-def _attrs2dict(attributes):\n-    \"\"\"convert a list of attributes (name, value) tuples\n-    into a dict of lists. \n-\n-    For example:\n-    >>> l = [('name', 'sofa'), ('colour', 'red'), ('colour', 'green')]\n-    >>> _attrs2dict(l) == {'name': ['sofa'], 'colour': ['red', 'green']}\n-    True\n-    \"\"\"\n-    grouped_data = groupby(sorted(attributes, key=_namef), _namef)\n-    return dict((name, map(_valuef, data)) for (name, data)  in grouped_data)\n-\n-class RecordExtractor(object):\n-    \"\"\"The RecordExtractor will extract records given annotations.\n-    \n-    It looks for a similar region in the target document, using the ibl\n-    similarity algorithm. The annotations are partitioned by the first similar\n-    region found and searched recursively.\n-\n-    Records are represented as dicts mapping attribute names to lists\n-    containing their values.\n-    \n-    For example:\n-    >>> from scrapy.contrib.ibl.extraction.pageparsing import parse_strings\n-    >>> template, page = parse_strings( \\\n-            u'<h1 data-scrapy-annotate=\"{&quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">x</h1>' + \\\n-            u'<p data-scrapy-annotate=\"{&quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">y</p>', \\\n-            u'<h1>name</h1> <p>description</p>')\n-    >>> basic_extractors = map(BasicTypeExtractor, template.annotations)\n-    >>> ex = RecordExtractor.apply(template, basic_extractors)[0]\n-    >>> ex.extract(page)\n-    [{u'description': [u'description'], u'name': [u'name']}]\n-    \"\"\"\n-    \n-    def __init__(self, extractors, template_tokens):\n-        \"\"\"Construct a RecordExtractor for the given annotations and their\n-        corresponding region extractors\n-        \"\"\"\n-        self.extractors = extractors\n-        self.template_tokens = template_tokens\n-        self.template_ignored_regions = []\n-        start_index = min(e.annotation.start_index for e in extractors)\n-        end_index = max(e.annotation.end_index for e in extractors)\n-        self.annotation = AnnotationTag(start_index, end_index)\n-    \n-    def extract(self, page, start_index=0, end_index=None, ignored_regions=None):\n-        \"\"\"extract data from an extraction page\n-        \n-        The region in the page to be extracted from may be specified using\n-        start_index and end_index\n-        \"\"\"\n-        if ignored_regions is None:\n-            ignored_regions = []\n-        region_elements = sorted(self.extractors + ignored_regions, key=lambda x: _labelled(x).start_index)\n-        _, _, attributes = self._doextract(page, region_elements, start_index, \n-                end_index)\n-        # collect variant data, maintaining the order of variants\n-        variant_ids = []; variants = {}; items = []\n-        for k, v in attributes:\n-            if isinstance(k, int):\n-                if k in variants:\n-                    variants[k] += v\n-                else:\n-                    variant_ids.append(k)\n-                    variants[k] = v\n-            else:\n-                items.append((k, v))\n-        \n-        variant_records = [('variants', _attrs2dict(variants[vid])) \\\n-                for vid in variant_ids]\n-        items += variant_records\n-        return [_attrs2dict(items)]\n-    \n-    def _doextract(self, page, region_elements, start_index, end_index, nested_regions=None, ignored_regions=None):\n-        \"\"\"Carry out extraction of records using the given annotations\n-        in the page tokens bounded by start_index and end_index\n-        \"\"\"\n-        # reorder extractors leaving nested ones for the end and separating\n-        # ignore regions\n-        nested_regions = nested_regions or []\n-        ignored_regions = ignored_regions or []\n-        first_region, following_regions = region_elements[0], region_elements[1:]\n-        while following_regions and _labelled(following_regions[0]).start_index \\\n-                < _labelled(first_region).end_index:\n-            region = following_regions.pop(0)\n-            labelled = _labelled(region)\n-            if isinstance(labelled, AnnotationTag) or (nested_regions and \\\n-                    _labelled(nested_regions[-1]).start_index < labelled.start_index \\\n-                    < _labelled(nested_regions[-1]).end_index):\n-                nested_regions.append(region)\n-            else:\n-                ignored_regions.append(region)\n-        extracted_data = []\n-        # end_index is inclusive, but similar_region treats it as exclusive\n-        end_region = None if end_index is None else end_index + 1\n-        labelled = _labelled(first_region)\n-        score, pindex, sindex = \\\n-            similar_region(page.page_tokens, self.template_tokens,\n-                labelled, start_index, end_region)\n-        if score > 0:\n-            if isinstance(labelled, AnnotationTag):\n-                similar_ignored_regions = []\n-                start = pindex\n-                for i in ignored_regions:\n-                    s, p, e = similar_region(page.page_tokens, self.template_tokens, \\\n-                              i, start, sindex)\n-                    if s > 0:\n-                        similar_ignored_regions.append(PageRegion(p, e))\n-                        start = e or start\n-                extracted_data = first_region.extract(page, pindex, sindex, similar_ignored_regions)\n-                if extracted_data:\n-                    if first_region.annotation.variant_id:\n-                        extracted_data = [(first_region.annotation.variant_id, extracted_data)]\n-            \n-            if nested_regions:\n-                _, _, nested_data = self._doextract(page, nested_regions, pindex, sindex)\n-                extracted_data += nested_data\n-            if following_regions:\n-                _, _, following_data = self._doextract(page, following_regions, sindex or start_index, end_index)\n-                extracted_data += following_data\n-        \n-        elif following_regions:\n-            end_index, _, following_data = self._doextract(page, following_regions, start_index, end_index)\n-            if end_index is not None:\n-                pindex, sindex, extracted_data = self._doextract(page, [first_region], start_index, end_index - 1, nested_regions, ignored_regions)\n-                extracted_data += following_data\n-        elif nested_regions:\n-            _, _, nested_data = self._doextract(page, nested_regions, start_index, end_index)\n-            extracted_data += nested_data\n-        return pindex, sindex, extracted_data\n-                \n-    @classmethod\n-    def apply(cls, template, extractors):\n-        return [cls(extractors, template.page_tokens)]\n-    \n-    def extracted_item(self):\n-        return [self.__class__.__name__] + \\\n-                sorted(e.extracted_item() for e in self.extractors)\n-    \n-    def __repr__(self):\n-        return str(self)\n-\n-    def __str__(self):\n-        stream = cStringIO.StringIO()\n-        pprint.pprint(self.extractors, stream)\n-        stream.seek(0)\n-        template_data = stream.read()\n-        if template_data:\n-            return \"%s[\\n%s\\n]\" % (self.__class__.__name__, template_data)\n-        return \"%s[none]\" % (self.__class__.__name__)\n-\n-class AdjacentVariantExtractor(RecordExtractor):\n-    \"\"\"Extractor for variants\n-\n-    This simply extends the RecordExtractor to output data in a \"variants\"\n-    attribute.\n-\n-    The \"apply\" method will only apply to variants whose items are all adjacent and \n-    it will appear as one record so that it can be handled by the RepeatedDataExtractor. \n-    \"\"\"\n-\n-    def extract(self, page, start_index=0, end_index=None, ignored_regions=None):\n-        records = RecordExtractor.extract(self, page, start_index, end_index, ignored_regions)\n-        return [('variants', r['variants'][0]) for r in records if r]\n-    \n-    @classmethod\n-    def apply(cls, template, extractors):\n-        adjacent_variants = set([])\n-        variantf = lambda x: x.annotation.variant_id\n-        for vid, egroup in groupby(extractors, variantf):\n-            if not vid:\n-                continue\n-            if vid in adjacent_variants:\n-                adjacent_variants.remove(vid)\n-            else:\n-                adjacent_variants.add(vid)\n-        new_extractors = []\n-        for variant, group_seq in groupby(extractors, variantf):\n-            group_seq = list(group_seq)\n-            if variant in adjacent_variants:\n-                record_extractor = AdjacentVariantExtractor(group_seq, template.page_tokens)\n-                new_extractors.append(record_extractor)\n-            else:\n-                new_extractors += group_seq\n-        return new_extractors\n-\n-    def __repr__(self):\n-        return str(self)\n-\n-class TraceExtractor(object):\n-    \"\"\"Extractor that wraps other extractors and prints an execution\n-    trace of the extraction process to aid debugging\n-    \"\"\"\n-\n-    def __init__(self, traced, template):\n-        self.traced = traced\n-        self.annotation = traced.annotation\n-        tstart = traced.annotation.start_index\n-        tend = traced.annotation.end_index\n-        self.tprefix = \" \".join([template.token_dict.token_string(t)\n-            for t in template.page_tokens[tstart-4:tstart+1]])\n-        self.tsuffix = \" \".join([template.token_dict.token_string(t)\n-            for t in template.page_tokens[tend:tend+5]])\n-    \n-    def summarize_trace(self, page, start, end, ret):\n-        text_start = page.htmlpage.parsed_body[page.token_page_indexes[start]].start\n-        text_end = page.htmlpage.parsed_body[page.token_page_indexes[end or -1]].end\n-        page_snippet = \"(...%s)%s(%s...)\" % (\n-                page.htmlpage.body[text_start-50:text_start].replace('\\n', ' '), \n-                page.htmlpage.body[text_start:text_end], \n-                page.htmlpage.body[text_end:text_end+50].replace('\\n', ' '))\n-        pre_summary = \"\\nstart %s page[%s:%s]\\n\" % (self.traced.__class__.__name__, start, end)\n-        post_summary = \"\"\"\n-%s page[%s:%s] \n-\n-html\n-%s\n-\n-annotation\n-...%s\n-%s\n-%s...\n-\n-extracted\n-%s\n-        \"\"\" % (self.traced.__class__.__name__, start, end, page_snippet, \n-                self.tprefix, self.annotation, self.tsuffix, [r for r in ret if 'trace' not in r])\n-        return pre_summary, post_summary\n-\n-    def extract(self, page, start, end, ignored_regions):\n-        ret = self.traced.extract(page, start, end, ignored_regions)\n-        if not ret:\n-            return []\n-\n-        # handle records by inserting a trace and combining with variant traces\n-        if len(ret) == 1 and isinstance(ret[0], dict):\n-            item = ret[0]\n-            trace = item.pop('trace', [])\n-            variants = item.get('variants', ())\n-            for variant in variants:\n-                trace += variant.pop('trace', [])\n-            pre_summary, post_summary = self.summarize_trace(page, start, end, ret)\n-            item['trace'] = [pre_summary] + trace + [post_summary]\n-            return ret\n-        \n-        pre_summary, post_summary = self.summarize_trace(page, start, end, ret)\n-        return [('trace', pre_summary)] + ret + [('trace', post_summary)]\n-    \n-    @staticmethod\n-    def apply(template, extractors):\n-        output = []\n-        for extractor in extractors:\n-            if not isinstance(extractor, TraceExtractor):\n-                extractor = TraceExtractor(extractor, template)\n-            output.append(extractor)\n-        return output\n-    \n-    def extracted_item(self):\n-        return self.traced.extracted_item()\n-\n-    def __repr__(self):\n-        return \"Trace(%s)\" % repr(self.traced)\n-\n-class TemplatePageExtractor(object):\n-    \"\"\"Top level extractor for a template page\"\"\"\n-\n-    def __init__(self, template, extractors):\n-        # fixme: handle multiple items per page\n-        self.extractor = extractors[0]\n-        self.template = template\n-\n-    def extract(self, page, start_index=0, end_index=None):\n-        return self.extractor.extract(page, start_index, end_index, self.template.ignored_regions)\n-    \n-    def __repr__(self):\n-        return repr(self.extractor)\n-\n-    def __str__(self):\n-        return str(self.extractor)\n-\n-# Based on nltk's WordPunctTokenizer\n-_tokenize = re.compile(r'\\w+|[^\\w\\s]+', re.UNICODE | re.MULTILINE | re.DOTALL).findall\n-\n-class TextRegionDataExtractor(object):\n-    \"\"\"Data Extractor for extracting text fragments from an annotation page\n-    fragment or string. It extracts based on the longest unique prefix and\n-    suffix.\n-\n-    for example:\n-    >>> extractor = TextRegionDataExtractor('designed by ', '.')\n-    >>> extractor.extract_text(\"by Marc Newson.\")\n-    'Marc Newson'\n-\n-    Both prefix and suffix are optional:\n-    >>> extractor = TextRegionDataExtractor('designed by ')\n-    >>> extractor.extract_text(\"by Marc Newson.\")\n-    'Marc Newson.'\n-    >>> extractor = TextRegionDataExtractor(suffix='.')\n-    >>> extractor.extract_text(\"by Marc Newson.\")\n-    'by Marc Newson'\n-\n-    It requires a minimum match of at least one word or punctuation character:\n-    >>> extractor = TextRegionDataExtractor('designed by')\n-    >>> extractor.extract_text(\"y Marc Newson.\") is None\n-    True\n-    \"\"\"\n-    def __init__(self, prefix=None, suffix=None):\n-        self.prefix = (prefix or '')[::-1]\n-        self.suffix = suffix or ''\n-        self.minprefix = self.minmatch(self.prefix)\n-        self.minsuffix = self.minmatch(self.suffix)\n- \n-    @staticmethod\n-    def minmatch(matchstring):\n-        \"\"\"the minimum number of characters that should match in order\n-        to consider it a match for that string.\n-\n-        This uses the last word of punctuation character\n-        \"\"\"\n-        tokens = _tokenize(matchstring or '')\n-        return len(tokens[0]) if tokens else 0\n-\n-    def extract(self, region):\n-        \"\"\"Extract a region from the region passed\"\"\"\n-        text = self.extract_text(region)\n-        return HtmlPageRegion(region.htmlpage, text) if text else None\n-\n-    def extract_text(self, text):\n-        \"\"\"Extract a substring from the text\"\"\"\n-        pref_index = 0\n-        if self.minprefix > 0:\n-            rev_idx, plen = longest_unique_subsequence(text[::-1], self.prefix)\n-            if plen < self.minprefix:\n-                return None\n-            pref_index = -rev_idx\n-        if self.minsuffix == 0:\n-            return text[pref_index:]\n-        sidx, slen = longest_unique_subsequence(text[pref_index:], self.suffix)\n-        if slen < self.minsuffix:\n-            return None\n-        return text[pref_index:pref_index + sidx]\n\n@@ -1,136 +0,0 @@\n-\"\"\"\n-Similarity calculation for Instance based extraction algorithm.\n-\"\"\"\n-from itertools import izip, count\n-from operator import itemgetter\n-from heapq import nlargest\n-\n-def common_prefix_length(a, b):\n-    \"\"\"Calculate the length of the common prefix in both sequences passed.\n-    \n-    For example, the common prefix in this example is [1, 3]\n-    >>> common_prefix_length([1, 3, 4], [1, 3, 5, 1])\n-    2\n-    \n-    If there is no common prefix, 0 is returned\n-    >>> common_prefix_length([1], [])\n-    0\n-    \"\"\"\n-    i = -1\n-    for i, x, y in izip(count(), a, b):\n-        if x != y:\n-            return i\n-    return i + 1\n-\n-def common_prefix(*sequences):\n-    \"\"\"determine the common prefix of all sequences passed\n-    \n-    For example:\n-    >>> common_prefix('abcdef', 'abc', 'abac')\n-    ['a', 'b']\n-    \"\"\"\n-    prefix = []\n-    for sample in izip(*sequences):\n-        first = sample[0]\n-        if all(x == first for x in sample[1:]):\n-            prefix.append(first)\n-        else:\n-            break\n-    return prefix\n-\n-def longest_unique_subsequence(to_search, subsequence, range_start=0, \n-        range_end=None):\n-    \"\"\"Find the longest unique subsequence of items in a list or array.  This\n-    searches the to_search list or array looking for the longest overlapping\n-    match with subsequence. If the largest match is unique (there is no other\n-    match of equivalent length), the index and length of match is returned.  If\n-    there is no match, (None, None) is returned.\n-\n-    Please see section 3.2 of Extracting Web Data Using Instance-Based\n-    Learning by Yanhong Zhai and Bing Liu\n-\n-    For example, the longest match occurs at index 2 and has length 3\n-    >>> to_search = [6, 3, 2, 4, 3, 2, 5]\n-    >>> longest_unique_subsequence(to_search, [2, 4, 3])\n-    (2, 3)\n-    \n-    When there are two equally long subsequences, it does not generate a match\n-    >>> longest_unique_subsequence(to_search, [3, 2])\n-    (None, None)\n-\n-    range_start and range_end specify a range in which the match must begin\n-    >>> longest_unique_subsequence(to_search, [3, 2], 3)\n-    (4, 2)\n-    >>> longest_unique_subsequence(to_search, [3, 2], 0, 2)\n-    (1, 2)\n-    \"\"\"\n-    startval = subsequence[0]\n-    if range_end is None:\n-        range_end = len(to_search)\n-    \n-    # the comparison to startval ensures only matches of length >= 1 and \n-    # reduces the number of calls to the common_length function\n-    matches = ((i, common_prefix_length(to_search[i:], subsequence)) \\\n-        for i in xrange(range_start, range_end) if startval == to_search[i])\n-    best2 = nlargest(2, matches, key=itemgetter(1))\n-    # if there is a single unique best match, return that\n-    if len(best2) == 1 or len(best2) == 2 and best2[0][1] != best2[1][1]:\n-        return best2[0]\n-    return None, None\n-\n-def similar_region(extracted_tokens, template_tokens, labelled_region, \n-        range_start=0, range_end=None):\n-    \"\"\"Given a labelled section in a template, identify a similar region\n-    in the extracted tokens.\n-\n-    The start and end index of the similar region in the extracted tokens\n-    is returned.\n-\n-    This will return a tuple containing:\n-    (match score, start index, end index)\n-    where match score is the sum of the length of the matching prefix and \n-    suffix. If there is no unique match, (0, None, None) will be returned.\n-\n-    start_index and end_index specify a range in which the match must begin\n-    \"\"\"\n-    data_length = len(extracted_tokens)\n-    if range_end is None:\n-        range_end = data_length\n-    # calculate the prefix score by finding a longest subsequence in \n-    # reverse order\n-    reverse_prefix = template_tokens[labelled_region.start_index::-1]\n-    reverse_tokens = extracted_tokens[::-1]\n-    (rpi, pscore) = longest_unique_subsequence(reverse_tokens, reverse_prefix,\n-            data_length - range_end, data_length - range_start)\n-\n-    # None means nothing exracted. Index 0 means there cannot be a suffix.\n-    if not rpi:\n-        return 0, None, None\n-    \n-    # convert to an index from the start instead of in reverse\n-    prefix_index = len(extracted_tokens) - rpi - 1\n-\n-    if labelled_region.end_index is None:\n-        return pscore, prefix_index, None\n-\n-    suffix = template_tokens[labelled_region.end_index:]\n-\n-    # if it's not a paired tag, use the best match between prefix & suffix\n-    if labelled_region.start_index == labelled_region.end_index:\n-        (match_index, sscore) = longest_unique_subsequence(extracted_tokens,\n-            suffix, prefix_index, range_end)\n-        if match_index == prefix_index:\n-            return (pscore + sscore, prefix_index, match_index)\n-        elif pscore > sscore:\n-            return pscore, prefix_index, prefix_index\n-        elif sscore > pscore:\n-            return sscore, match_index, match_index\n-        return 0, None, None\n-\n-    # calculate the suffix match on the tokens following the prefix. We could\n-    # consider the whole page and require a good match.\n-    (match_index, sscore) = longest_unique_subsequence(extracted_tokens,\n-            suffix, prefix_index + 1, range_end)\n-    if match_index is None:\n-        return 0, None, None\n-    return (pscore + sscore, prefix_index, match_index)\n\n@@ -1,325 +0,0 @@\n-\"\"\"\n-Extractors for attributes\n-\"\"\"\n-import re\n-import urlparse\n-\n-from scrapy.utils.markup import remove_entities, remove_comments\n-from scrapy.utils.url import safe_url_string\n-from scrapy.contrib.ibl.htmlpage import HtmlTag, HtmlTagType\n-\n-#FIXME: the use of \".\" needs to be localized\n-_NUMERIC_ENTITIES = re.compile(\"&#([0-9]+)(?:;|\\s)\", re.U)\n-_PRICE_NUMBER_RE = re.compile('(?:^|[^a-zA-Z0-9])(\\d+(?:\\.\\d+)?)(?:$|[^a-zA-Z0-9])')\n-_NUMBER_RE = re.compile('(\\d+(?:\\.\\d+)?)')\n-\n-_IMAGES = (\n-    'mng', 'pct', 'bmp', 'gif', 'jpg', 'jpeg', 'png', 'pst', 'psp', 'tif',\n-    'tiff', 'ai', 'drw', 'dxf', 'eps', 'ps', 'svg',\n-)\n-\n-_IMAGES_TYPES = '|'.join(_IMAGES)\n-_CSS_IMAGERE = re.compile(\"background(?:-image)?\\s*:\\s*url\\((.*?)\\)\", re.I)\n-_BASE_PATH_RE = \"/?(?:[^/]+/)*(?:.+%s)\"\n-_IMAGE_PATH_RE = re.compile(_BASE_PATH_RE % '\\.(?:%s)' % _IMAGES_TYPES, re.I)\n-_GENERIC_PATH_RE = re.compile(_BASE_PATH_RE % '', re.I)\n-_WS = re.compile(\"\\s+\", re.U)\n-\n-# tags to keep (only for attributes with markup)\n-_TAGS_TO_KEEP = frozenset(['br', 'p', 'big', 'em', 'small', 'strong', 'sub', \n-    'sup', 'ins', 'del', 'code', 'kbd', 'samp', 'tt', 'var', 'pre', 'listing',\n-    'plaintext', 'abbr', 'acronym', 'address', 'bdo', 'blockquote', 'q', \n-    'cite', 'dfn', 'table', 'tr', 'th', 'td', 'tbody', 'ul', 'ol', 'li', 'dl',\n-    'dd', 'dt'])\n-\n-# tag names to be replaced by other tag names (overrides tags_to_keep)\n-_TAGS_TO_REPLACE = {\n-    'h1': 'strong',\n-    'h2': 'strong',\n-    'h3': 'strong',\n-    'h4': 'strong',\n-    'h5': 'strong',\n-    'h6': 'strong',\n-    'b' : 'strong',\n-    'i' : 'em',\n-}\n-\n-# tags whoose content will be completely removed (recursively)\n-# (overrides tags_to_keep and tags_to_replace)\n-_TAGS_TO_PURGE = ('script', 'img', 'input')\n-\n-def htmlregion(text):\n-    \"\"\"convenience function to make an html region from text.\n-    This is useful for testing\n-    \"\"\"\n-    from scrapy.contrib.ibl.htmlpage import HtmlPage\n-    return HtmlPage(body=text).subregion()\n-\n-def notags(region, tag_replace=u' '):\n-    \"\"\"Removes all html tags\"\"\"\n-    fragments = getattr(region, 'parsed_fragments', None)\n-    if fragments is None:\n-        return region\n-    page = region.htmlpage\n-    data = [page.fragment_data(f) for f in fragments if not isinstance(f, HtmlTag)]\n-    return tag_replace.join(data)\n-\n-def text(region):\n-    \"\"\"Converts HTML to text. There is no attempt at formatting other than\n-    removing excessive whitespace,\n-    \n-    For example:\n-    >>> t = lambda s: text(htmlregion(s))\n-    >>> t(u'<h1>test</h1>')\n-    u'test'\n-    \n-    Leading and trailing whitespace are removed\n-    >>> t(u'<h1> test</h1> ')\n-    u'test'\n-    \n-    Comments are removed\n-    >>> t(u'test <!-- this is a comment --> me')\n-    u'test me'\n-    \n-    Text between script tags is ignored\n-    >>> t(u\"scripts are<script>n't</script> ignored\")\n-    u'scripts are ignored'\n-    \n-    HTML entities are converted to text\n-    >>> t(u\"only &pound;42\")\n-    u'only \\\\xa342'\n-    \"\"\"\n-    chunks = _process_markup(region, \n-        lambda text: remove_entities(text, encoding=region.htmlpage.encoding),\n-        lambda tag: u' '\n-    )\n-    text = u''.join(chunks)\n-    return _WS.sub(u' ', text).strip()\n-\n-def safehtml(region, allowed_tags=_TAGS_TO_KEEP, replace_tags=_TAGS_TO_REPLACE):\n-    \"\"\"Creates an HTML subset, using a whitelist of HTML tags.\n-\n-    The HTML generated is safe for display on a website,without escaping and\n-    should not cause formatting problems.\n-    \n-    Allowed_tags is a set of tags that are allowed and replace_tags is a mapping of \n-    tags to alternative tags to substitute.\n-\n-    For example:\n-    >>> t = lambda s: safehtml(htmlregion(s))\n-    >>> t(u'<strong>test <blink>test</blink></strong>')\n-    u'<strong>test test</strong>'\n-    \n-    Some tags, like script, are completely removed\n-    >>> t(u'<script>test </script>test')\n-    u'test'\n-\n-    replace_tags define tags that are converted. By default all headers, bold and indenting\n-    are converted to strong and em.\n-    >>> t(u'<h2>header</h2> test <b>bold</b> <i>indent</i>')\n-    u'<strong>header</strong> test <strong>bold</strong> <em>indent</em>'\n-\n-    Comments are stripped, but entities are not converted\n-    >>> t(u'<!-- comment --> only &pound;42')\n-    u'only &pound;42'\n-    \n-    Paired tags are closed\n-    >>> t(u'<p>test')\n-    u'<p>test</p>'\n-\n-    >>> t(u'<p>test <i><br/><b>test</p>')\n-    u'<p>test <em><br/><strong>test</strong></em></p>'\n-\n-    \"\"\"\n-    tagstack = []\n-    def _process_tag(tag):\n-        tagstr = replace_tags.get(tag.tag, tag.tag)\n-        if tagstr not in allowed_tags:\n-            return\n-        if tag.tag_type == HtmlTagType.OPEN_TAG:\n-            tagstack.append(tagstr)\n-            return u\"<%s>\" % tagstr\n-        elif tag.tag_type == HtmlTagType.CLOSE_TAG:\n-            try:\n-                last = tagstack.pop()\n-                # common case of matching tag\n-                if last == tagstr:\n-                    return u\"</%s>\" % last\n-                # output all preceeding tags (if present)\n-                revtags = tagstack[::-1]\n-                tindex = revtags.index(tagstr)\n-                del tagstack[-tindex-1:]\n-                return u\"</%s></%s>\" % (last, u\"></\".join(revtags[:tindex+1]))\n-            except (ValueError, IndexError):\n-                # popped from empty stack or failed to find the tag\n-                pass \n-        else:\n-            assert tag.tag_type == HtmlTagType.UNPAIRED_TAG, \"unrecognised tag type\"\n-            return u\"<%s/>\" % tag.tag\n-    chunks = list(_process_markup(region, lambda text: text, _process_tag)) + \\\n-        [\"</%s>\" % t for t in reversed(tagstack)]\n-    return u''.join(chunks).strip()\n-\n-def _process_markup(region, textf, tagf):\n-    fragments = getattr(region, 'parsed_fragments', None)\n-    if fragments is None:\n-        yield textf(region)\n-        return\n-    fiter = iter(fragments)\n-    for fragment in fiter:\n-        if isinstance(fragment, HtmlTag):\n-            # skip forward to closing script tags\n-            tag = fragment.tag\n-            if tag in _TAGS_TO_PURGE:\n-                # if opening, keep going until closed\n-                if fragment.tag_type == HtmlTagType.OPEN_TAG:\n-                    for probe in fiter:\n-                        if isinstance(probe, HtmlTag) and \\\n-                            probe.tag == tag and \\\n-                            probe.tag_type == HtmlTagType.CLOSE_TAG:\n-                            break\n-            else:\n-                output = tagf(fragment)\n-                if output:\n-                    yield output\n-        else:\n-            text = region.htmlpage.fragment_data(fragment)\n-            text = remove_comments(text)\n-            text = textf(text)\n-            if text:\n-                yield text\n-\n-def html(pageregion):\n-    \"\"\"A page region is already html, so this is the identity function\"\"\"\n-    return pageregion\n-\n-def contains_any_numbers(txt):\n-    \"\"\"text that must contain at least one number\n-    >>> contains_any_numbers('foo')\n-    >>> contains_any_numbers('$67 at 15% discount')\n-    '$67 at 15% discount'\n-    \"\"\"\n-    if _NUMBER_RE.search(txt) is not None:\n-        return txt\n-\n-def contains_prices(txt):\n-    \"\"\"text must contain a number that is not joined to text\"\"\"\n-    if _PRICE_NUMBER_RE.findall(txt) is not None:\n-        return txt\n-\n-def contains_numbers(txt, count=1):\n-    \"\"\"Must contain a certain amount of numbers\n-    \n-    >>> contains_numbers('foo', 2)\n-    >>> contains_numbers('this 1 has 2 numbers', 2)\n-    'this 1 has 2 numbers'\n-    \"\"\"\n-    numbers = _NUMBER_RE.findall(txt)\n-    if len(numbers) == count:\n-        return txt\n-\n-def extract_number(txt):\n-    \"\"\"Extract a numeric value.\n-    \n-    This will fail if more than one numeric value is present.\n-\n-    >>> extract_number('  45.3')\n-    '45.3'\n-    >>> extract_number('  45.3, 7')\n-\n-    It will handle unescaped entities:\n-    >>> extract_number(u'&#163;129&#46;99')\n-    u'129.99'\n-    \"\"\"\n-    txt = _NUMERIC_ENTITIES.sub(lambda m: unichr(int(m.groups()[0])), txt)\n-    numbers = _NUMBER_RE.findall(txt)\n-    if len(numbers) == 1:\n-        return numbers[0]\n-    \n-def url(txt):\n-    \"\"\"convert text to a url\n-    \n-    this is quite conservative, since relative urls are supported\n-    \"\"\"\n-    txt = txt.strip(\"\\t\\r\\n '\\\"\")\n-    if txt:\n-        return txt\n-\n-def image_url(txt):\n-    \"\"\"convert text to a url\n-    \n-    this is quite conservative, since relative urls are supported\n-    Example:\n-\n-        >>> image_url('')\n-\n-        >>> image_url('   ')\n-\n-        >>> image_url(' \\\\n\\\\n  ')\n-\n-        >>> image_url('foo-bar.jpg')\n-        ['foo-bar.jpg']\n-        >>> image_url('/images/main_logo12.gif')\n-        ['/images/main_logo12.gif']\n-        >>> image_url(\"http://www.image.com/image.jpg\")\n-        ['http://www.image.com/image.jpg']\n-        >>> image_url(\"http://www.domain.com/path1/path2/path3/image.jpg\")\n-        ['http://www.domain.com/path1/path2/path3/image.jpg']\n-        >>> image_url(\"/path1/path2/path3/image.jpg\")\n-        ['/path1/path2/path3/image.jpg']\n-        >>> image_url(\"path1/path2/image.jpg\")\n-        ['path1/path2/image.jpg']\n-        >>> image_url(\"background-image : url(http://www.site.com/path1/path2/image.jpg)\")\n-        ['http://www.site.com/path1/path2/image.jpg']\n-        >>> image_url(\"background-image : url('http://www.site.com/path1/path2/image.jpg')\")\n-        ['http://www.site.com/path1/path2/image.jpg']\n-        >>> image_url('background-image : url(\"http://www.site.com/path1/path2/image.jpg\")')\n-        ['http://www.site.com/path1/path2/image.jpg']\n-        >>> image_url(\"background : url(http://www.site.com/path1/path2/image.jpg)\")\n-        ['http://www.site.com/path1/path2/image.jpg']\n-        >>> image_url(\"background : url('http://www.site.com/path1/path2/image.jpg')\")\n-        ['http://www.site.com/path1/path2/image.jpg']\n-        >>> image_url('background : url(\"http://www.site.com/path1/path2/image.jpg\")')\n-        ['http://www.site.com/path1/path2/image.jpg']\n-        >>> image_url('/getimage.php?image=totalgardens/outbbq2_400.jpg&type=prod&resizeto=350')\n-        ['/getimage.php?image=totalgardens/outbbq2_400.jpg&type=prod&resizeto=350']\n-        >>> image_url('http://www.site.com/getimage.php?image=totalgardens/outbbq2_400.jpg&type=prod&resizeto=350')\n-        ['http://www.site.com/getimage.php?image=totalgardens/outbbq2_400.jpg&type=prod&resizeto=350']\n-        >>> image_url('http://s7d4.scene7.com/is/image/Kohler/jaa03267?hei=425&wid=457&op_usm=2,1,2,1&qlt=80')\n-        ['http://s7d4.scene7.com/is/image/Kohler/jaa03267?hei=425&wid=457&op_usm=2,1,2,1&qlt=80']\n-        >>> image_url('../image.aspx?thumb=true&amp;boxSize=175&amp;img=Unknoportrait[1].jpg')\n-        ['../image.aspx?thumb=true&boxSize=175&img=Unknoportrait%5B1%5D.jpg']\n-        >>> image_url('http://www.sundancecatalog.com/mgen/catalog/test.ms?args=%2245932|MERIDIAN+PENDANT|.jpg%22&is=336,336,0xffffff')\n-        ['http://www.sundancecatalog.com/mgen/catalog/test.ms?args=%2245932|MERIDIAN+PENDANT|.jpg%22&is=336,336,0xffffff']\n-        >>> image_url('http://www.site.com/image.php')\n-        ['http://www.site.com/image.php']\n-        >>> image_url('background-image:URL(http://s7d5.scene7.com/is/image/wasserstrom/165133?wid=227&hei=227&amp;defaultImage=noimage_wasserstrom)')\n-        ['http://s7d5.scene7.com/is/image/wasserstrom/165133?wid=227&hei=227&defaultImage=noimage_wasserstrom']\n-\n-    \"\"\"\n-    imgurl = extract_image_url(txt)\n-    return [safe_url_string(remove_entities(url(imgurl)))] if imgurl else None\n-\n-def extract_image_url(txt):\n-    txt = url(txt)\n-    imgurl = None\n-    if txt:\n-        # check if the text is style content\n-        m = _CSS_IMAGERE.search(txt)\n-        txt = m.groups()[0] if m else txt\n-        parsed = urlparse.urlparse(txt)\n-        path = None\n-        m = _IMAGE_PATH_RE.search(parsed.path)\n-        if m:\n-            path = m.group()\n-        elif parsed.query:\n-            m = _GENERIC_PATH_RE.search(parsed.path)\n-            if m:\n-                path = m.group()\n-        if path is not None:\n-            parsed = list(parsed)\n-            parsed[2] = path\n-            imgurl = urlparse.urlunparse(parsed)\n-        if not imgurl:\n-            imgurl = txt\n-    return imgurl\n\n@@ -1,224 +0,0 @@\n-\"\"\"\n-htmlpage\n-\n-Container objects for representing html pages and their parts in the IBL\n-system. This encapsulates page related information and prevents parsing\n-multiple times.\n-\"\"\"\n-import re\n-import hashlib\n-\n-from scrapy.utils.python import str_to_unicode\n-\n-def create_page_from_jsonpage(jsonpage, body_key):\n-    \"\"\"Create an HtmlPage object from a dict object conforming to the schema\n-    for a page\n-\n-    `body_key` is the key where the body is stored and can be either 'body'\n-    (original page with annotations - if any) or 'original_body' (original\n-    page, always). Classification typically uses 'original_body' to avoid\n-    confusing the classifier with annotated pages, while extraction uses 'body'\n-    to pass the annotated pages.\n-    \"\"\"\n-    url = jsonpage['url']\n-    headers = jsonpage.get('headers')\n-    body = str_to_unicode(jsonpage[body_key])\n-    page_id = jsonpage.get('page_id')\n-    return HtmlPage(url, headers, body, page_id)\n-\n-class HtmlPage(object):\n-    \"\"\"HtmlPage\n-\n-    This is a parsed HTML page. It contains the page headers, url, raw body and parsed \n-    body.\n-\n-    The parsed body is a list of HtmlDataFragment objects.\n-    \"\"\"\n-    def __init__(self, url=None, headers=None, body=None, page_id=None, encoding='utf-8'):\n-        assert isinstance(body, unicode), \"unicode expected, got: %s\" % type(body).__name__\n-        self.headers = headers or {}\n-        self.body = body\n-        self.url = url or u''\n-        self.encoding = encoding\n-        if page_id is None and url:\n-            self.page_id = hashlib.sha1(url).hexdigest()\n-        else:\n-            self.page_id = page_id \n-    \n-    @classmethod\n-    def from_response(cls, response, page_id=None):\n-        \"\"\"Create an HtmlPage from a scrapy response\"\"\"\n-        return HtmlPage(response.url, response.headers,\n-            response.body_as_unicode(), page_id, response.encoding)\n-\n-    def _set_body(self, body):\n-        self._body = body\n-        self.parsed_body = list(parse_html(body))\n-        \n-    body = property(lambda x: x._body, _set_body, doc=\"raw html for the page\")\n-    \n-    def subregion(self, start=0, end=None):\n-        \"\"\"HtmlPageRegion constructed from the start and end index (inclusive)\n-        into the parsed page\n-        \"\"\"\n-        return HtmlPageParsedRegion(self, start, end)\n-\n-    def fragment_data(self, data_fragment):\n-        \"\"\"portion of the body corresponding to the HtmlDataFragment\"\"\"\n-        return self.body[data_fragment.start:data_fragment.end]\n-\n-class HtmlPageRegion(unicode):\n-    \"\"\"A Region of an HtmlPage that has been extracted\n-    \"\"\"\n-    def __new__(cls, htmlpage, data):\n-        return unicode.__new__(cls, data)\n-\n-    def __init__(self, htmlpage, data):\n-        \"\"\"Construct a new HtmlPageRegion object.\n-\n-        htmlpage is the original page and data is the raw html\n-        \"\"\"\n-        self.htmlpage = htmlpage\n-    \n-class HtmlPageParsedRegion(HtmlPageRegion):\n-    \"\"\"A region of an HtmlPage that has been extracted\n-\n-    This has a parsed_fragments property that contains the parsed html \n-    fragments contained within this region\n-    \"\"\"\n-    def __new__(cls, htmlpage, start_index, end_index):\n-        text_start = htmlpage.parsed_body[start_index].start\n-        text_end = htmlpage.parsed_body[end_index or -1].end\n-        text = htmlpage.body[text_start:text_end]\n-        return HtmlPageRegion.__new__(cls, htmlpage, text)\n-\n-    def __init__(self, htmlpage, start_index, end_index):\n-        self.htmlpage = htmlpage\n-        self.start_index = start_index\n-        self.end_index = end_index\n-\n-    @property\n-    def parsed_fragments(self):\n-        \"\"\"HtmlDataFragment or HtmlTag objects for this parsed region\"\"\"\n-        end = self.end_index + 1 if self.end_index is not None else None\n-        return self.htmlpage.parsed_body[self.start_index:end]\n-\n-class HtmlTagType(object):\n-    OPEN_TAG = 1\n-    CLOSE_TAG = 2 \n-    UNPAIRED_TAG = 3\n-\n-class HtmlDataFragment(object):\n-    __slots__ = ('start', 'end')\n-    \n-    def __init__(self, start, end):\n-        self.start = start\n-        self.end = end\n-        \n-    def __str__(self):\n-        return \"<HtmlDataFragment [%s:%s]>\" % (self.start, self.end)\n-\n-    def __repr__(self):\n-        return str(self)\n-    \n-class HtmlTag(HtmlDataFragment):\n-    __slots__ = ('tag_type', 'tag', 'attributes')\n-\n-    def __init__(self, tag_type, tag, attributes, start, end):\n-        HtmlDataFragment.__init__(self, start, end)\n-        self.tag_type = tag_type\n-        self.tag = tag\n-        self.attributes = attributes\n-\n-    def __str__(self):\n-        return \"<HtmlTag tag='%s' attributes={%s} [%s:%s]>\" % (self.tag, ', '.join(sorted\\\n-                ([\"%s: %s\" % (k, repr(v)) for k, v in self.attributes.items()])), self.start, self.end)\n-    \n-    def __repr__(self):\n-        return str(self)\n-\n-_ATTR = \"((?:[^=/<>\\s]|/(?!>))+)(?:\\s*=(?:\\s*\\\"(.*?)\\\"|\\s*'(.*?)'|([^>\\s]+))?)?\"\n-_TAG = \"<(\\/?)(\\w+(?::\\w+)?)((?:\\s*\" + _ATTR + \")+\\s*|\\s*)(\\/?)>?\"\n-_DOCTYPE = r\"<!DOCTYPE.*?>\"\n-_SCRIPT = \"(<script.*?>)(.*?)(</script.*?>)\"\n-_COMMENT = \"(<!--.*?-->)\"\n-\n-_ATTR_REGEXP = re.compile(_ATTR, re.I | re.DOTALL)\n-_HTML_REGEXP = re.compile(\"%s|%s|%s\" % (_COMMENT, _SCRIPT, _TAG), re.I | re.DOTALL)\n-_DOCTYPE_REGEXP = re.compile(\"(?:%s)\" % _DOCTYPE)\n-_COMMENT_REGEXP = re.compile(_COMMENT, re.DOTALL)\n-\n-def parse_html(text):\n-    \"\"\"Higher level html parser. Calls lower level parsers and joins sucesive\n-    HtmlDataFragment elements in a single one.\n-    \"\"\"\n-    # If have doctype remove it.\n-    start_pos = 0\n-    match = _DOCTYPE_REGEXP.match(text)\n-    if match:\n-        start_pos = match.end()\n-    prev_end = start_pos\n-    for match in _HTML_REGEXP.finditer(text, start_pos):\n-        start = match.start()\n-        end = match.end()\n-            \n-        if start > prev_end:\n-            yield HtmlDataFragment(prev_end, start)\n-\n-        if match.groups()[0] is not None: # comment\n-            yield HtmlDataFragment(start, end)\n-        elif match.groups()[1] is not None: # <script>...</script>\n-            for e in _parse_script(match):\n-                yield e\n-        else: # tag\n-            yield _parse_tag(match)\n-        prev_end = end\n-    textlen = len(text)\n-    if prev_end < textlen:\n-        yield HtmlDataFragment(prev_end, textlen)\n-\n-def _parse_script(match):\n-    \"\"\"parse a <script>...</script> region matched by _HTML_REGEXP\"\"\"\n-    open_text, content, close_text = match.groups()[1:4]\n-\n-    open_tag = _parse_tag(_HTML_REGEXP.match(open_text))\n-    open_tag.start = match.start()\n-    open_tag.end = match.start() + len(open_text)\n-\n-    close_tag = _parse_tag(_HTML_REGEXP.match(close_text))\n-    close_tag.start = match.end() - len(close_text)\n-    close_tag.end = match.end()\n-    \n-    yield open_tag\n-    if open_tag.end < close_tag.start:\n-        start_pos = 0\n-        for m in _COMMENT_REGEXP.finditer(content):\n-            if m.start() > start_pos:\n-                yield HtmlDataFragment(open_tag.end + start_pos, open_tag.end + m.start())\n-            yield HtmlDataFragment(open_tag.end + m.start(), open_tag.end + m.end())\n-            start_pos = m.end()\n-        if open_tag.end + start_pos < close_tag.start:\n-            yield HtmlDataFragment(open_tag.end + start_pos, close_tag.start)\n-    yield close_tag\n-\n-def _parse_tag(match):\n-    \"\"\"\n-    parse a tag matched by _HTML_REGEXP\n-    \"\"\"\n-    data = match.groups()\n-    closing, tag, attr_text = data[4:7]\n-    # if tag is None then the match is a comment\n-    if tag is not None:\n-        unpaired = data[-1]\n-        if closing:\n-            tag_type = HtmlTagType.CLOSE_TAG\n-        elif unpaired:\n-            tag_type = HtmlTagType.UNPAIRED_TAG\n-        else:\n-            tag_type = HtmlTagType.OPEN_TAG\n-        attributes = []\n-        for attr_match in _ATTR_REGEXP.findall(attr_text):\n-            name = attr_match[0].lower()\n-            values = [v for v in attr_match[1:] if v]\n-            attributes.append((name, values[0] if values else None))\n-        return HtmlTag(tag_type, tag.lower(), dict(attributes), match.start(), match.end())\n\n@@ -1,2 +0,0 @@\n-import sys\n-path = sys.modules[__name__].__path__[0]\n\n@@ -1,959 +0,0 @@\n-\"\"\"\n-tests for page parsing\n-\n-Page parsing effectiveness is measured through the evaluation system. These\n-tests should focus on specific bits of functionality work correctly.\n-\"\"\"\n-from twisted.trial.unittest import TestCase, SkipTest\n-from scrapy.contrib.ibl.htmlpage import HtmlPage\n-from scrapy.contrib.ibl.descriptor import (FieldDescriptor as A, \n-        ItemDescriptor)\n-from scrapy.contrib.ibl.extractors import (contains_any_numbers,\n-        image_url, html, notags)\n-\n-try:\n-    import numpy\n-    __doctests__ = ['scrapy.contrib.ibl.extraction.%s' % x for x in \\\n-        ['regionextract', 'similarity', 'pageobjects']]\n-except ImportError:\n-    numpy = None\n-\n-if numpy:\n-    from scrapy.contrib.ibl.extraction import InstanceBasedLearningExtractor\n-\n-# simple page with all features\n-\n-ANNOTATED_PAGE1 = u\"\"\"\n-<html>\n-<h1>COMPANY - <ins \n-    data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true, \n-    &quot;annotations&quot;: {&quot;content&quot;: &quot;title&quot;}}\" \n->Item Title</ins></h1>\n-<p>introduction</p>\n-<div>\n-<img data-scrapy-annotate=\"{&quot;variant&quot;: 0, \n-    &quot;annotations&quot;: {&quot;src&quot;: &quot;image_url&quot;}}\"\n-    src=\"img.jpg\"/>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, \n-    &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-This is such a nice item<br/> Everybody likes it.\n-</p>\n-<br/>\n-</div>\n-<p>click here for other items</p>\n-</html>\n-\"\"\"\n-\n-EXTRACT_PAGE1 = u\"\"\"\n-<html>\n-<h1>Scrapy - Nice Product</h1>\n-<p>introduction</p>\n-<div>\n-<img src=\"nice_product.jpg\" alt=\"a nice product image\"/>\n-<p>wonderful product</p>\n-<br/>\n-</div>\n-</html>\n-\"\"\"\n-\n-# single tag with multiple items extracted\n-ANNOTATED_PAGE2 = u\"\"\"\n-<a href=\"http://example.com/xxx\" title=\"xxx\"\n-    data-scrapy-annotate=\"{&quot;variant&quot;: 0, \n-    &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;, \n-        &quot;href&quot;: &quot;image_url&quot;, &quot;title&quot;: &quot;name&quot;}}\"\n->xx</a>\n-xxx\n-</a>\n-\"\"\"\n-EXTRACT_PAGE2 = u\"\"\"<a href='http://example.com/product1.jpg' \n-    title=\"product 1\">product 1 is great</a>\"\"\"\n-\n-# matching must match the second attribute in order to find the first\n-ANNOTATED_PAGE3 = u\"\"\"\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, \n-    &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">xx</p>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, \n-    &quot;annotations&quot;: {&quot;content&quot;: &quot;delivery&quot;}}\">xx</div>\n-\"\"\"\n-EXTRACT_PAGE3 = u\"\"\"\n-<p>description</p>\n-<div>delivery</div>\n-<p>this is not the description</p>\n-\"\"\"\n-\n-# test inferring repeated elements\n-ANNOTATED_PAGE4 = u\"\"\"\n-<ul>\n-<li data-scrapy-annotate=\"{&quot;variant&quot;: 0, \n-    &quot;annotations&quot;: {&quot;content&quot;: &quot;features&quot;}}\">feature1</li>\n-<li data-scrapy-annotate=\"{&quot;variant&quot;: 0, \n-    &quot;annotations&quot;: {&quot;content&quot;: &quot;features&quot;}}\">feature2</li>\n-</ul>\n-\"\"\"\n-\n-EXTRACT_PAGE4 = u\"\"\"\n-<ul>\n-<li>feature1</li> ignore this\n-<li>feature2</li>\n-<li>feature3</li>\n-</ul>\n-\"\"\"\n-\n-# test variant handling with identical repeated variant\n-ANNOTATED_PAGE5 =  u\"\"\"\n-<p data-scrapy-annotate=\"{&quot;annotations&quot;:\n-    {&quot;content&quot;: &quot;description&quot;}}\">description</p>\n-<table>\n-<tr>\n-<td data-scrapy-annotate=\"{&quot;variant&quot;: 1, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;colour&quot;}}\" >colour 1</td>\n-<td data-scrapy-annotate=\"{&quot;variant&quot;: 1, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;price&quot;}}\" >price 1</td>\n-</tr>\n-<tr>\n-<td data-scrapy-annotate=\"{&quot;variant&quot;: 2, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;colour&quot;}}\" >colour 2</td>\n-<td data-scrapy-annotate=\"{&quot;variant&quot;: 2, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;price&quot;}}\" >price 2</td>\n-</tr>\n-</table>\n-\"\"\"\n-\n-EXTRACT_PAGE5 = u\"\"\"\n-<p>description</p>\n-<table>\n-<tr>\n-<td>colour 1</td>\n-<td>price 1</td>\n-</tr>\n-<tr>\n-<td>colour 2</td>\n-<td>price 2</td>\n-</tr>\n-<tr>\n-<td>colour 3</td>\n-<td>price 3</td>\n-</tr>\n-</table>\n-\"\"\"\n-\n-# test variant handling with irregular structure and some non-variant\n-# attributes\n-ANNOTATED_PAGE6 =  u\"\"\"\n-<p data-scrapy-annotate=\"{&quot;annotations&quot;:\n-    {&quot;content&quot;: &quot;description&quot;}}\">description</p>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 1, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;name&quot;}}\">name 1</p>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 3, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;name&quot;}}\" >name 3</div>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 2, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;name&quot;}}\" >name 2</p>\n-\"\"\"\n-EXTRACT_PAGE6 =  u\"\"\"\n-<p>description</p>\n-<p>name 1</p>\n-<div>name 3</div>\n-<p>name 2</p>\n-\"\"\"\n-\n-# test repeating variants at the table column level\n-ANNOTATED_PAGE7 =  u\"\"\"\n-<table>\n-<tr>\n-<td data-scrapy-annotate=\"{&quot;variant&quot;: 1, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;colour&quot;}}\" >colour 1</td>\n-<td data-scrapy-annotate=\"{&quot;variant&quot;: 2, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;colour&quot;}}\" >colour 2</td>\n-</tr>\n-<tr>\n-<td data-scrapy-annotate=\"{&quot;variant&quot;: 2, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;price&quot;}}\" >price 1</td>\n-<td data-scrapy-annotate=\"{&quot;variant&quot;: 2, &quot;annotations&quot;: \n-    {&quot;content&quot;: &quot;price&quot;}}\" >price 2</td>\n-</tr>\n-</table>\n-\"\"\"\n-\n-EXTRACT_PAGE7 = u\"\"\"\n-<table>\n-<tr>\n-<td>colour 1</td>\n-<td>colour 2</td>\n-<td>colour 3</td>\n-</tr>\n-<tr>\n-<td>price 1</td>\n-<td>price 2</td>\n-<td>price 3</td>\n-</tr>\n-</table>\n-\"\"\"\n-\n-ANNOTATED_PAGE8 = u\"\"\"\n-<html><body>\n-<h1>A product</h1>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-<p>XXXX XXXX xxxxx</p>\n-<div data-scrapy-ignore=\"true\">\n-<img scr=\"image.jpg\" /><br/><a link=\"back.html\">Click here to go back</a>\n-</div>\n-</div>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">\n-10.00<p data-scrapy-ignore=\"true\"> 13</p>\n-</div>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE8 = u\"\"\"\n-<html><body>\n-<h1>A product</h1>\n-<div>\n-<p>A very nice product for all intelligent people</p>\n-<div>\n-<img scr=\"image.jpg\" /><br/><a link=\"back.html\">Click here to go back</a>\n-</div>\n-</div>\n-<div>\n-12.00<p> ID 15</p>\n-(VAT exc.)</div>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE9 = ANNOTATED_PAGE8\n-\n-EXTRACT_PAGE9 = u\"\"\"\n-<html><body>\n-<img src=\"logo.jpg\" />\n-<h1>A product</h1>\n-<div>\n-<p>A very nice product for all intelligent people</p>\n-<div>\n-<img scr=\"image.jpg\" /><br/><a link=\"back.html\">Click here to go back</a>\n-</div>\n-</div>\n-<div>\n-12.00<p> ID 16</p>\n-(VAT exc.)</div>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE11 = u\"\"\"\n-<html><body>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-<ins data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true,\n-    &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">\n-SL342\n-</ins>\n-<br/>\n-Nice product for ladies\n-<br/><ins data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true,\n-     &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">\n-&pound;85.00\n-</ins>\n-</p>\n-<ins data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true,\n-     &quot;annotations&quot;: {&quot;content&quot;: &quot;price_before_discount&quot;}}\">\n-&pound;100.00\n-</ins>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE11 = u\"\"\"\n-<html><body>\n-<p>\n-SL342\n-<br/>\n-Nice product for ladies\n-<br/>\n-&pound;85.00\n-</p>\n-&pound;100.00\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE12 = u\"\"\"\n-<html><body>\n-<h1 data-scrapy-ignore-beneath=\"true\">A product</h1>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-<p>XXXX XXXX xxxxx</p>\n-<div data-scrapy-ignore-beneath=\"true\">\n-<img scr=\"image.jpg\" /><br/><a link=\"back.html\">Click here to go back</a>\n-</div>\n-<div>\n-10.00<p> 13</p>\n-</div>\n-</div>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE12a = u\"\"\"\n-<html><body>\n-<h1>A product</h1>\n-<div>\n-<p>A very nice product for all intelligent people</p>\n-<div>\n-<img scr=\"image.jpg\" /><br/><a link=\"back.html\">Click here to go back</a>\n-</div>\n-<div>\n-12.00<p> ID 15</p>\n-(VAT exc.)\n-</div></div>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE12b = u\"\"\"\n-<html><body>\n-<h1>A product</h1>\n-<div>\n-<p>A very nice product for all intelligent people</p>\n-<div>\n-<img scr=\"image.jpg\" /><br/><a link=\"back.html\">Click here to go back</a>\n-</div>\n-<div>\n-12.00<p> ID 15</p>\n-(VAT exc.)\n-</div>\n-<ul>\n-Features\n-<li>Feature A</li>\n-<li>Feature B</li>\n-</ul>\n-</div>\n-</body></html>\n-\"\"\"\n-\n-# Ex1: nested annotation with token sequence replica outside exterior annotation\n-# and a possible sequence pattern can be extracted only with\n-# correct handling of nested annotations\n-ANNOTATED_PAGE13a = u\"\"\"\n-<html><body>\n-<span>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-<hr/>\n-<h3 data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">A product</h3>\n-<b data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$50.00</b>\n-This product is excelent. Buy it!\n-</p>\n-</span>\n-<span>\n-<p>\n-<h3>See other products:</h3>\n-<b>Product b</b>\n-</p>\n-</span>\n-<hr/>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE13a = u\"\"\"\n-<html><body>\n-<span>\n-<p>\n-<h3>A product</h3>\n-<b>$50.00</b>\n-This product is excelent. Buy it!\n-<hr/>\n-</p>\n-</span>\n-<span>\n-<p>\n-<h3>See other products:</h3>\n-<b>Product B</b>\n-</p>\n-</span>\n-</body></html>\n-\"\"\"\n-\n-# Ex2: annotation with token sequence replica inside a previous nested annotation\n-# and a possible sequence pattern can be extracted only with\n-# correct handling of nested annotations\n-ANNOTATED_PAGE13b = u\"\"\"\n-<html><body>\n-<span>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-<h3 data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">A product</h3>\n-<b>Previous price: $50.00</b>\n-This product is excelent. Buy it!\n-</p>\n-</span>\n-<span>\n-<p>\n-<h3>Save 10%!!</h3>\n-<b data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$45.00</b>\n-</p>\n-</span>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE13b = u\"\"\"\n-<html><body>\n-<span>\n-<p>\n-<h3>A product</h3>\n-<b>$50.00</b>\n-This product is excelent. Buy it!\n-</p>\n-</span>\n-<span>\n-<hr/>\n-<p>\n-<h3>Save 10%!!</h3>\n-<b>$45.00</b>\n-</p>\n-</span>\n-<hr/>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE14 = u\"\"\"\n-<html><body>\n-<b data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\"></b>\n-<p data-scrapy-ignore=\"true\"></p>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE14 = u\"\"\"\n-<html><body>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE15 = u\"\"\"\n-<html><body>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;short_description&quot;}}\">Short\n-<div data-scrapy-ignore=\"true\" data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;site_id&quot;}}\">892342</div>\n-</div>\n-<hr/>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">Description\n-<b data-scrapy-ignore=\"true\" data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">90.00</b>\n-</p>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE15 = u\"\"\"\n-<html><body>\n-<hr/>\n-<p>Description\n-<b>80.00</b>\n-</p>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE16 = u\"\"\"\n-<html><body>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-Description\n-<p data-scrapy-ignore=\"true\" data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">\n-name</p>\n-<p data-scrapy-ignore=\"true\" data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">\n-80.00</p>\n-</div>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE16 = u\"\"\"\n-<html><body>\n-<p>product name</p>\n-<p>90.00</p>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE17 = u\"\"\"\n-<html><body>\n-<span>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-This product is excelent. Buy it!\n-</p>\n-</span>\n-<table></table>\n-<img src=\"line.jpg\" data-scrapy-ignore-beneath=\"true\"/>\n-<span>\n-<h3>See other products:</h3>\n-<p>Product b\n-</p>\n-</span>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE17 = u\"\"\"\n-<html><body>\n-<span>\n-<p>\n-This product is excelent. Buy it!\n-</p>\n-</span>\n-<img src=\"line.jpg\"/>\n-<span>\n-<h3>See other products:</h3>\n-<p>Product B\n-</p>\n-</span>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE18 = u\"\"\"\n-<html><body>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-<ins data-scrapy-ignore=\"true\" data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;site_id&quot;}}\">Item Id</ins>\n-<br>\n-Description\n-</div>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE18 = u\"\"\"\n-<html><body>\n-<div>\n-Item Id\n-<br>\n-Description\n-</div>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE19 = u\"\"\"\n-<html><body>\n-<div>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">Product name</p>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">60.00</p>\n-<img data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}\" src=\"image.jpg\" />\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;required&quot;: [&quot;description&quot;], &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">description</p>\n-</div>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE19a = u\"\"\"\n-<html><body>\n-<div>\n-<p>Product name</p>\n-<p>60.00</p>\n-<img src=\"http://example.com/image.jpg\" />\n-<p>description</p>\n-</div>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE19b = u\"\"\"\n-<html><body>\n-<div>\n-<p>Range</p>\n-<p>from 20.00</p>\n-<img src=\"http://example.com/image1.jpg\" />\n-<p>\n-<br/>\n-</div>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE20 = u\"\"\"\n-<html><body>\n-<h1>Product Name</h1>\n-<img src=\"product.jpg\">\n-<br/>\n-<span><ins data-scrapy-annotate=\"{&quot;variant&quot;: 1, &quot;generated&quot;: true,                                              \n-&quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">Twin</ins>:</span> $<ins data-scrapy-annotate=\"{&quot;variant&quot;: 1, &quot;generated&quot;: true,\n-&quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">270</ins> - November 2010<br/>\n-<span><ins data-scrapy-annotate=\"{&quot;variant&quot;: 2, &quot;generated&quot;: true,\n-&quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">Queen</ins>:</span> $<ins data-scrapy-annotate=\"{&quot;variant&quot;: 2, &quot;generated&quot;: true,\n-&quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">330</ins> - In stock<br/>\n-<br/>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE20 = u\"\"\"\n-<html><body>\n-<h1>Product Name</h1>\n-<img src=\"product.jpg\">\n-<br/>\n-<span>Twin:</span> $270 - November 2010<br/>\n-<span>Queen:</span> $330 - Movember 2010<br/>\n-<br/>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE21 = u\"\"\"\n-<html><body>                                                                                                                                                                       \n-<img src=\"image.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}\">\n-<p>\n-<table>\n-\n-<tr><td><img src=\"swatch1.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\"></td></tr>\n-\n-<tr><td><img src=\"swatch2.jpg\"></td></tr>\n-\n-<tr><td><img src=\"swatch3.jpg\"></td></tr>\n-\n-<tr><td><img src=\"swatch4.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\"></td></tr>\n-\n-</table>\n-\n-<div data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;category&quot;}}\">tables</div>\n-\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE21 = u\"\"\"\n-<html><body>\n-<img src=\"image.jpg\">\n-<p>\n-<table>\n-\n-<tr><td><img src=\"swatch1.jpg\"></td></tr>\n-\n-<tr><td><img src=\"swatch2.jpg\"></td></tr>\n-\n-<tr><td><img src=\"swatch3.jpg\"></td></tr>\n-\n-<tr><td><img src=\"swatch4.jpg\"></td></tr>\n-\n-</table>\n-\n-<div>chairs</div>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE22 = u\"\"\"\n-<html><body>                                                                                                                                                                       \n-<img src=\"image.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}\">\n-<p>\n-<table>\n-\n-<tr><td>\n-<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 1</p>\n-<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$67</b>\n-<img src=\"swatch1.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\">\n-</td></tr>\n-\n-<tr><td>\n-<p>product 2</p>\n-<b>$70</b>\n-<img src=\"swatch2.jpg\">\n-</td></tr>\n-\n-<tr><td>\n-<p>product 3</p>\n-<b>$73</b>\n-<img src=\"swatch3.jpg\">\n-</td></tr>\n-\n-<tr><td>\n-<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 4</p>\n-<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$80</b>\n-<img src=\"swatch4.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\">\n-</td></tr>\n-\n-</table>\n-\n-<div data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;category&quot;}}\">tables</div>\n-\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE22 = u\"\"\"\n-<html><body>\n-<img src=\"image.jpg\">\n-<p>\n-<table>\n-\n-<tr><td>\n-<p>product 1</p>\n-<b>$70</b>\n-<img src=\"swatch1.jpg\">\n-</td></tr>\n-\n-<tr><td>\n-<p>product 2</p>\n-<b>$80</b>\n-<img src=\"swatch2.jpg\">\n-</td></tr>\n-\n-<tr><td>\n-<p>product 3</p>\n-<b>$90</b>\n-<img src=\"swatch3.jpg\">\n-</td></tr>\n-\n-<tr><td>\n-<p>product 4</p>\n-<b>$100</b>\n-<img src=\"swatch4.jpg\">\n-</td></tr>\n-\n-</table>\n-\n-<div>chairs</div>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE23 = u\"\"\"\n-<html><body>\n-<h4>Product</h4>\n-<table>\n-<tr><td>\n-<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">Variant 1<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\" data-scrapy-ignore=\"true\">560</b></p>\n-</td></tr>\n-<tr><td>\n-<p>Variant 2<b>570</b></p>\n-</td></tr>\n-<tr><td>\n-<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">Variant 3<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\" data-scrapy-ignore=\"true\">580</b></p>\n-</td></tr>\n-</table>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE23 = u\"\"\"\n-<html><body>\n-<h4>Product</h4>\n-<table>\n-<tr><td>\n-<p>Variant 1<b>300</b></p>\n-</td></tr>\n-<tr><td>\n-<p>Variant 2<b>320</b></p>\n-</td></tr>\n-<tr><td>\n-<p>Variant 3<b>340</b></p>\n-</td></tr>\n-</table>\n-</body></html>\n-\"\"\"\n-\n-DEFAULT_DESCRIPTOR = ItemDescriptor('test', \n-    'item test, removes tags from description attribute',\n-    [A('description', 'description field without tags', notags)])\n-\n-SAMPLE_DESCRIPTOR1 = ItemDescriptor('test', 'product test', [\n-    A('name', \"Product name\", required=True),\n-    A('price', \"Product price, including any discounts and tax or vat\", \n-        contains_any_numbers, True),    \n-    A('image_urls', \"URLs for one or more images\", image_url, True),\n-    A('description', \"The full description of the product\", html),\n-    ]\n-)\n-\n-# A list of (test name, [templates], page, extractors, expected_result)\n-TEST_DATA = [\n-    # extract from a similar page\n-    ('similar page extraction', [ANNOTATED_PAGE1], EXTRACT_PAGE1, DEFAULT_DESCRIPTOR,\n-        {u'title': [u'Nice Product'], u'description': [u'wonderful product'], \n-            u'image_url': [u'nice_product.jpg']}\n-    ),\n-    # strip the first 5 characters from the title\n-    ('extractor test', [ANNOTATED_PAGE1], EXTRACT_PAGE1,\n-        ItemDescriptor('test', 'product test', \n-            [A('title', \"something about a title\", lambda x: x[5:])]),\n-        {u'title': [u'Product'], u'description': [u'wonderful product'], \n-            u'image_url': [u'nice_product.jpg']}\n-    ),\n-    # compilicated tag (multiple attributes and annotation)\n-    ('multiple attributes and annotation', [ANNOTATED_PAGE2], EXTRACT_PAGE2, DEFAULT_DESCRIPTOR,\n-        {'name': [u'product 1'], 'image_url': [u'http://example.com/product1.jpg'],\n-            'description': [u'product 1 is great']}\n-    ),\n-    # can only work out correct placement by matching the second attribute first\n-    ('ambiguous description', [ANNOTATED_PAGE3], EXTRACT_PAGE3, DEFAULT_DESCRIPTOR,\n-        {'description': [u'description'], 'delivery': [u'delivery']}\n-    ),\n-    # infer a repeated structure\n-    ('repeated elements', [ANNOTATED_PAGE4], EXTRACT_PAGE4, DEFAULT_DESCRIPTOR,\n-        {'features': [u'feature1', u'feature2', u'feature3']}\n-    ),\n-    # identical variants with a repeated structure\n-    ('repeated identical variants', [ANNOTATED_PAGE5], EXTRACT_PAGE5, DEFAULT_DESCRIPTOR,\n-         {\n-             'description': [u'description'],\n-             'variants': [\n-                 {u'colour': [u'colour 1'], u'price': [u'price 1']}, \n-                 {u'colour': [u'colour 2'], u'price': [u'price 2']}, \n-                 {u'colour': [u'colour 3'], u'price': [u'price 3']} \n-             ]\n-         }\n-    ),\n-    # variants with an irregular structure\n-    ('irregular variants', [ANNOTATED_PAGE6], EXTRACT_PAGE6, DEFAULT_DESCRIPTOR,\n-         {\n-             'description': [u'description'],\n-             'variants': [\n-                 {u'name': [u'name 1']}, \n-                 {u'name': [u'name 3']}, \n-                 {u'name': [u'name 2']}\n-             ]\n-         }\n-    ),\n-\n-    # discovering repeated variants in table columns\n-#    ('variants in table columns', [ANNOTATED_PAGE7], EXTRACT_PAGE7, DEFAULT_DESCRIPTOR,\n-#         {'variants': [\n-#             {u'colour': [u'colour 1'], u'price': [u'price 1']}, \n-#             {u'colour': [u'colour 2'], u'price': [u'price 2']}, \n-#             {u'colour': [u'colour 3'], u'price': [u'price 3']}\n-#         ]}\n-#    ),\n-    \n-    \n-    # ignored regions\n-    (\n-    'ignored_regions', [ANNOTATED_PAGE8], EXTRACT_PAGE8, DEFAULT_DESCRIPTOR,\n-          {\n-             'description': [u'\\n A very nice product for all intelligent people \\n \\n'],\n-             'price': [u'\\n12.00\\n(VAT exc.)'],\n-          }\n-    ),\n-    # shifted ignored regions (detected by region similarity)\n-    (\n-    'shifted_ignored_regions', [ANNOTATED_PAGE9], EXTRACT_PAGE9, DEFAULT_DESCRIPTOR,\n-          {\n-             'description': [u'\\n A very nice product for all intelligent people \\n \\n'],\n-             'price': [u'\\n12.00\\n(VAT exc.)'],\n-          }\n-    ),\n-    (# special case with partial annotations\n-    'special_partial_annotation', [ANNOTATED_PAGE11], EXTRACT_PAGE11, DEFAULT_DESCRIPTOR,\n-          {\n-            'name': [u'SL342'],\n-            'description': ['\\nSL342\\n \\nNice product for ladies\\n \\n&pound;85.00\\n'],\n-            'price': [u'\\xa385.00'],\n-            'price_before_discount': [u'\\xa3100.00'],\n-          }\n-    ),\n-    (# with ignore-beneath feature\n-    'ignore-beneath', [ANNOTATED_PAGE12], EXTRACT_PAGE12a, DEFAULT_DESCRIPTOR,\n-          {\n-            'description': [u'\\n A very nice product for all intelligent people \\n'],\n-          }\n-    ),\n-    (# ignore-beneath with extra tags\n-    'ignore-beneath with extra tags', [ANNOTATED_PAGE12], EXTRACT_PAGE12b, DEFAULT_DESCRIPTOR,\n-          {\n-            'description': [u'\\n A very nice product for all intelligent people \\n'],\n-          }\n-    ),\n-    ('nested annotation with replica outside', [ANNOTATED_PAGE13a], EXTRACT_PAGE13a, DEFAULT_DESCRIPTOR,\n-          {'description': [u'\\n A product \\n $50.00 \\nThis product is excelent. Buy it!\\n \\n'],\n-           'price': [\"$50.00\"],\n-           'name': [u'A product']}\n-    ),\n-    ('outside annotation with nested replica', [ANNOTATED_PAGE13b], EXTRACT_PAGE13b, DEFAULT_DESCRIPTOR,\n-          {'description': [u'\\n A product \\n $50.00 \\nThis product is excelent. Buy it!\\n'],\n-           'price': [\"$45.00\"],\n-           'name': [u'A product']}\n-    ),\n-    ('consistency check', [ANNOTATED_PAGE14], EXTRACT_PAGE14, DEFAULT_DESCRIPTOR,\n-          {},\n-    ),\n-    ('consecutive nesting', [ANNOTATED_PAGE15], EXTRACT_PAGE15, DEFAULT_DESCRIPTOR,\n-          {'description': [u'Description\\n \\n'],\n-           'price': [u'80.00']},\n-    ),\n-    ('nested inside not found', [ANNOTATED_PAGE16], EXTRACT_PAGE16, DEFAULT_DESCRIPTOR,\n-          {'price': [u'90.00'],\n-           'name': [u'product name']},\n-    ),\n-    ('ignored region helps to find attributes', [ANNOTATED_PAGE17], EXTRACT_PAGE17, DEFAULT_DESCRIPTOR,\n-          {'description': [u'\\nThis product is excelent. Buy it!\\n']},\n-    ),\n-    ('ignored region in partial annotation', [ANNOTATED_PAGE18], EXTRACT_PAGE18, DEFAULT_DESCRIPTOR,\n-          {u'site_id': [u'Item Id'],\n-           u'description': [u'\\nDescription\\n']},\n-    ),\n-    ('extra required attribute product', [ANNOTATED_PAGE19], EXTRACT_PAGE19a,\n-         SAMPLE_DESCRIPTOR1,\n-         {u'price': [u'60.00'],\n-          u'description': [u'description'],\n-          u'image_urls': [['http://example.com/image.jpg']],\n-          u'name': [u'Product name']},\n-    ),\n-    ('extra required attribute no product', [ANNOTATED_PAGE19], EXTRACT_PAGE19b,\n-         SAMPLE_DESCRIPTOR1,\n-         None,\n-    ),\n-    ('repeated partial annotations with variants', [ANNOTATED_PAGE20], EXTRACT_PAGE20, DEFAULT_DESCRIPTOR,\n-            {u'variants': [\n-                {'price': ['270'], 'name': ['Twin']},\n-                {'price': ['330'], 'name': ['Queen']},\n-            ]},\n-    ),\n-    ('variants with swatches', [ANNOTATED_PAGE21], EXTRACT_PAGE21, DEFAULT_DESCRIPTOR,\n-            {u'category': [u'chairs'],\n-             u'image_urls': [u'image.jpg'],\n-             u'variants': [\n-                {'swatches': ['swatch1.jpg']},\n-                {'swatches': ['swatch2.jpg']},\n-                {'swatches': ['swatch3.jpg']},\n-                {'swatches': ['swatch4.jpg']},\n-             ]\n-            },\n-    ),\n-    ('variants with swatches complete', [ANNOTATED_PAGE22], EXTRACT_PAGE22, DEFAULT_DESCRIPTOR,\n-            {u'category': [u'chairs'],\n-             u'variants': [\n-                 {u'swatches': [u'swatch1.jpg'],\n-                  u'price': [u'$70'],\n-                  u'name': [u'product 1']},\n-                 {u'swatches': [u'swatch2.jpg'],\\\n-                  u'price': [u'$80'],\n-                  u'name': [u'product 2']},\n-                 {u'swatches': [u'swatch3.jpg'],\n-                  u'price': [u'$90'],\n-                  u'name': [u'product 3']},\n-                 {u'swatches': [u'swatch4.jpg'],\n-                  u'price': [u'$100'],\n-                  u'name': [u'product 4']}\n-             ],\n-             u'image_urls': [u'image.jpg']},\n-    ),\n-    ('repeated (variants) with ignore annotations', [ANNOTATED_PAGE23], EXTRACT_PAGE23, DEFAULT_DESCRIPTOR,\n-        {'variants': [\n-            {u'price': [u'300'], u'name': [u'Variant 1']},\n-            {u'price': [u'320'], u'name': [u'Variant 2']},\n-            {u'price': [u'340'], u'name': [u'Variant 3']}\n-            ]},\n-    ),\n-]\n-\n-class TestExtraction(TestCase):\n-\n-    def setUp(self):\n-        if not numpy:\n-            raise SkipTest(\"numpy not available\")\n-\n-    def _run_extraction(self, name, templates, page, extractors, expected_output):\n-        self.trace = None\n-        template_pages = [HtmlPage(None, {}, t) for t in templates]\n-        extractor = InstanceBasedLearningExtractor(template_pages, extractors, True)\n-        actual_output, _ = extractor.extract(HtmlPage(None, {}, page))\n-        if not actual_output:\n-            if expected_output is None:\n-                return\n-            assert False, \"failed to extract data for test '%s'\" % name\n-        actual_output = actual_output[0]\n-        self.trace = [\"Extractor:\\n%s\" % extractor] + actual_output.pop('trace', [])\n-        expected_names = set(expected_output.keys())\n-        actual_names = set(actual_output.keys())\n-        \n-        missing_in_output = filter(None, expected_names - actual_names)\n-        error = \"attributes '%s' were expected but were not present in test '%s'\" % \\\n-                (\"', '\".join(missing_in_output), name)\n-        assert len(missing_in_output) == 0, error\n-\n-        unexpected = actual_names - expected_names\n-        error = \"unexpected attributes %s in test '%s'\" % \\\n-                (', '.join(unexpected), name)\n-        assert len(unexpected) == 0, error\n-\n-        for k, v in expected_output.items():\n-            extracted = actual_output[k]\n-            assert v == extracted, \"in test '%s' for attribute '%s', \" \\\n-                \"expected value '%s' but got '%s'\" % (name, k, v, extracted)\n-\n-    def test_expected_outputs(self):\n-        try:\n-            for data in TEST_DATA:\n-                self._run_extraction(*data)\n-        except AssertionError:\n-            if self.trace:\n-                print \"Trace:\"\n-                for line in self.trace:\n-                    print \"\\n---\\n%s\" % line\n-            raise\n\n@@ -1,5 +0,0 @@\n-try:\n-    import numpy\n-    __doctests__ = ['scrapy.contrib.ibl.extractors']\n-except ImportError:\n-    pass\n\n@@ -1,155 +0,0 @@\n-\"\"\"\n-htmlpage.py tests\n-\"\"\"\n-import os\n-from unittest import TestCase\n-\n-from scrapy.utils.py26 import json\n-from scrapy.tests.test_contrib_ibl import path\n-from scrapy.contrib.ibl.htmlpage import parse_html, HtmlTag, HtmlDataFragment\n-from scrapy.tests.test_contrib_ibl.test_htmlpage_data import *\n-from scrapy.utils.python import unicode_to_str, str_to_unicode\n-\n-SAMPLES_FILE_PREFIX = os.path.join(path, \"samples/samples_htmlpage\")\n-\n-def _encode_element(el):\n-    \"\"\"\n-    jsonize parse element\n-    \"\"\"\n-    if isinstance(el, HtmlTag):\n-        return {\"tag\": el.tag, \"attributes\": el.attributes,\n-            \"start\": el.start, \"end\": el.end, \"tag_type\": el.tag_type}\n-    if isinstance(el, HtmlDataFragment):\n-        return {\"start\": el.start, \"end\": el.end}\n-    raise TypeError\n-\n-def _decode_element(dct):\n-    \"\"\"\n-    dejsonize parse element\n-    \"\"\"\n-    if \"tag\" in dct:\n-        return HtmlTag(dct[\"tag_type\"], dct[\"tag\"], \\\n-            dct[\"attributes\"], dct[\"start\"], dct[\"end\"])\n-    if \"start\" in dct:\n-        return HtmlDataFragment(dct[\"start\"], dct[\"end\"])\n-    return dct\n-\n-def add_sample(source):\n-    \"\"\"\n-    Method for adding samples to test samples file\n-    (use from console)\n-    \"\"\"\n-    count = 0\n-    while os.path.exists(\"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)):\n-        count += 1\n-    \n-    open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"wb\").write(unicode_to_str(source))\n-    parsed = list(parse_html(source))\n-    open(\"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count), \"wb\")\\\n-        .write(json.dumps(parsed, default=_encode_element, indent=8))\n-\n-class TestParseHtml(TestCase):\n-    \"\"\"Test for parse_html\"\"\"\n-    def _test_sample(self, source, expected_parsed, samplecount=None):\n-        parsed = parse_html(source)\n-        count_element = 0\n-        count_expected = 0\n-        for element in parsed:\n-            if type(element) == HtmlTag:\n-                count_element += 1\n-            expected = expected_parsed.pop(0)\n-            if type(expected) == HtmlTag:\n-                count_expected += 1\n-            element_text = source[element.start:element.end]\n-            expected_text = source[expected.start:expected.end]\n-            if element.start != expected.start or element.end != expected.end:\n-                errstring = \"[%s,%s] %s != [%s,%s] %s\" % (element.start, \\\n-                    element.end, element_text, expected.start, \\\n-                    expected.end, expected_text)\n-                if samplecount is not None:\n-                    errstring += \" (sample %d)\" % samplecount\n-                assert False, errstring\n-            if type(element) != type(expected):\n-                errstring = \"(%s) %s != (%s) %s for text\\n%s\" % (count_element, \\\n-                    repr(type(element)), count_expected, repr(type(expected)), element_text)\n-                if samplecount is not None:\n-                    errstring += \" (sample %d)\" % samplecount\n-                assert False, errstring\n-            if type(element) == HtmlTag:\n-                self.assertEqual(element.tag, expected.tag)\n-                self.assertEqual(element.attributes, expected.attributes)\n-                self.assertEqual(element.tag_type, expected.tag_type)\n-        if expected_parsed:\n-            errstring = \"Expected %s\" % repr(expected_parsed)\n-            if samplecount is not None:\n-                errstring += \" (sample %d)\" % samplecount\n-            assert False, errstring\n-\n-    def test_parse(self):\n-        \"\"\"simple parse_html test\"\"\"\n-        parsed = [_decode_element(d) for d in PARSED]\n-        sample = {\"source\": PAGE, \"parsed\": parsed}\n-        self._test_sample(PAGE, parsed)\n-        \n-    def test_site_samples(self):\n-        \"\"\"test parse_html from real cases\"\"\"\n-        count = 0\n-        fname = \"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)\n-        while os.path.exists(fname):\n-            source = str_to_unicode(open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"rb\").read())\n-            parsed = json.loads(str_to_unicode(open(fname, \"rb\").read()),\\\n-                    object_hook=_decode_element)\n-            self._test_sample(source, parsed, count)\n-            count += 1\n-            fname = \"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)\n- \n-    def test_bad(self):\n-        \"\"\"test parsing of bad html layout\"\"\"\n-        parsed = [_decode_element(d) for d in PARSED2]\n-        self._test_sample(PAGE2, parsed)\n-\n-    def test_comments(self):\n-        \"\"\"test parsing of tags inside comments\"\"\"\n-        parsed = [_decode_element(d) for d in PARSED3]\n-        self._test_sample(PAGE3, parsed)\n-\n-    def test_script_text(self):\n-        \"\"\"test parsing of tags inside scripts\"\"\"\n-        parsed = [_decode_element(d) for d in PARSED4]\n-        self._test_sample(PAGE4, parsed)\n-        \n-    def test_sucessive(self):\n-        \"\"\"test parsing of sucesive cleaned elements\"\"\"\n-        parsed = [_decode_element(d) for d in PARSED5]\n-        self._test_sample(PAGE5, parsed)\n-        \n-    def test_sucessive2(self):\n-        \"\"\"test parsing of sucesive cleaned elements (variant 2)\"\"\"\n-        parsed = [_decode_element(d) for d in PARSED6]\n-        self._test_sample(PAGE6, parsed)\n-    \n-    def test_special_cases(self):\n-        \"\"\"some special cases tests\"\"\"\n-        parsed = list(parse_html(\"<meta http-equiv='Pragma' content='no-cache' />\"))\n-        self.assertEqual(parsed[0].attributes, {'content': 'no-cache', 'http-equiv': 'Pragma'})\n-        parsed = list(parse_html(\"<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>\"))\n-        self.assertEqual(parsed[0].attributes, {'xmlns': 'http://www.w3.org/1999/xhtml', 'xml:lang': 'en', 'lang': 'en'})\n-        parsed = list(parse_html(\"<IMG SRC='http://images.play.com/banners/SAM550a.jpg' align='left' / hspace=5>\"))\n-        self.assertEqual(parsed[0].attributes, {'src': 'http://images.play.com/banners/SAM550a.jpg', \\\n-                                                'align': 'left', 'hspace': '5', '/': None})\n-\n-    def test_no_ending_body(self):\n-        \"\"\"Test case when no ending body nor html elements are present\"\"\"\n-        parsed = [_decode_element(d) for d in PARSED7]\n-        self._test_sample(PAGE7, parsed)\n-\n-    def test_malformed(self):\n-        \"\"\"Test parsing of some malformed cases\"\"\"\n-        parsed = [_decode_element(d) for d in PARSED8]\n-        self._test_sample(PAGE8, parsed)\n-\n-    def test_malformed2(self):\n-        \"\"\"Test case when attributes are not separated by space (still recognizable because of quotes)\"\"\"\n-        parsed = [_decode_element(d) for d in PARSED9]\n-        self._test_sample(PAGE9, parsed)\n-\n\n@@ -1,277 +0,0 @@\n-PAGE = u\"\"\"\n-<style id=\"scrapy-style\" type=\"text/css\">@import url(http://localhost:8000/as/site_media/clean.css);                           \n-</style>\n-<body>\n-<div class=\"scrapy-selected\" id=\"header\">\n-<img src=\"company_logo.jpg\" style=\"margin-left: 68px; padding-top:5px;\" alt=\"Logo\" width=\"530\" height=\"105\">\n-<div id=\"vertrule\">\n-<h1>COMPANY - <ins data-scrapy-annotate=\"{&quot;variant&quot;: &quot;0&quot;, &quot;generated&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;title&quot;}}\">Item Title</ins></h1>\n-<p>introduction</p>\n-<div>\n-<img src=\"/upload/img.jpg\" classid=\"\"\n-    data-scrapy-annotate=\"{&quot;variant&quot;: &quot;0&quot;, &quot;annotations&quot;: {&quot;image_url&quot;: &quot;src&quot;}}\"\n->\n-<p classid=\"\" data-scrapy-annotate=\"{&quot;variant&quot;: &quot;0&quot;, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\"\n->\n-This is such a nice item<br/> Everybody likes it.\n-</p>\n-<br></br>\n-</div>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: &quot;0&quot;, &quot;annotations&quot;: {&quot;content&quot;: &quot;features&quot;}}\"\n-class=\"\" >Power: 50W</p>\n-<!-- A comment --!>\n-<ul data-scrapy-replacement='select' class='product'>\n-<li data-scrapy-replacement='option'>Small</li>\n-<li data-scrapy-replacement='option'>Big</li>\n-</ul>\n-<p>click here for other items</p>\n-<h3>Louis Chair</h3>\n-<table class=\"rulet\" width=\"420\" cellpadding=\"0\" cellspacing=\"0\"><tbody>\n-<tr><td>Height</td>\n-<td><ins data-scrapy-annotate=\"{&quot;variant&quot;: &quot;0&quot;, &quot;generated&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">32.00</ins></td>\n-</tr><tbody></table>\n-<p onmouseover='xxx' class= style=\"my style\">\n-\"\"\"\n-\n-PARSED = [\n-{'start': 0, 'end': 1},\n-{'attributes': {'type': 'text/css', 'id': 'scrapy-style'}, 'tag': 'style', 'end': 42, 'start': 1, 'tag_type': 1},\n-{'start': 42, 'end': 129},\n-{'attributes': {}, 'tag': 'style', 'end': 137, 'start': 129, 'tag_type': 2},\n-{'start': 137, 'end': 138},\n-{'attributes': {}, 'tag': 'body', 'end': 144, 'start': 138, 'tag_type': 1},\n-{'start': 144, 'end': 145},\n-{'attributes': {'class': 'scrapy-selected', 'id': 'header'}, 'tag': 'div', 'end': 186, 'start': 145, 'tag_type': 1},\n-{'start': 186, 'end': 187},\n-{'attributes': {'src': 'company_logo.jpg', 'style': 'margin-left: 68px; padding-top:5px;', 'width': '530', 'alt': 'Logo', 'height': '105'}, 'tag': 'img', 'end': 295, 'start': 187, 'tag_type': 1},\n-{'start': 295, 'end': 296},\n-{'attributes': {'id': 'vertrule'}, 'tag': 'div', 'end': 315, 'start': 296, 'tag_type': 1},\n-{'start': 315, 'end': 316},\n-{'attributes': {}, 'tag': 'h1', 'end': 320, 'start': 316, 'tag_type': 1},\n-{'start': 320, 'end': 330},\n-{'attributes': {'data-scrapy-annotate': '{&quot;variant&quot;: &quot;0&quot;, &quot;generated&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;title&quot;}}'}, 'tag': 'ins', 'end': 491, 'start': 330, 'tag_type': 1},\n-{'start': 491, 'end': 501},\n-{'attributes': {}, 'tag': 'ins', 'end': 507, 'start': 501, 'tag_type': 2},\n-{'attributes': {}, 'tag': 'h1', 'end': 512, 'start': 507, 'tag_type': 2},\n-{'start': 512, 'end': 513},\n-{'attributes': {}, 'tag': 'p', 'end': 516, 'start': 513, 'tag_type': 1},\n-{'start': 516, 'end': 528},\n-{'attributes': {}, 'tag': 'p', 'end': 532, 'start': 528, 'tag_type': 2},\n-{'start': 532, 'end': 533},\n-{'attributes': {}, 'tag': 'div', 'end': 538, 'start': 533, 'tag_type': 1},\n-{'start': 538, 'end': 539},\n-{'attributes': {'classid': None, 'src': '/upload/img.jpg', 'data-scrapy-annotate': '{&quot;variant&quot;: &quot;0&quot;, &quot;annotations&quot;: {&quot;image_url&quot;: &quot;src&quot;}}'}, 'tag': 'img', 'end': 709, 'start': 539, 'tag_type': 1},\n-{'start': 709, 'end': 710},\n-{'attributes': {'classid': None, 'data-scrapy-annotate': '{&quot;variant&quot;: &quot;0&quot;, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}'}, 'tag': 'p', 'end': 858, 'start': 710, 'tag_type': 1},\n-{'start': 858, 'end': 883},\n-{'attributes': {}, 'tag': 'br', 'end': 888, 'start': 883, 'tag_type': 3},\n-{'start': 888, 'end': 909},\n-{'attributes': {}, 'tag': 'p', 'end': 913, 'start': 909, 'tag_type': 2},\n-{'start': 913, 'end': 914},\n-{'attributes': {}, 'tag': 'br', 'end': 918, 'start': 914, 'tag_type': 1},\n-{'attributes': {}, 'tag': 'br', 'end': 923, 'start': 918, 'tag_type': 2},\n-{'start': 923, 'end': 924},\n-{'attributes': {}, 'tag': 'div', 'end': 930, 'start': 924, 'tag_type': 2},\n-{'start': 930, 'end': 931},\n-{'attributes': {'data-scrapy-annotate': '{&quot;variant&quot;: &quot;0&quot;, &quot;annotations&quot;: {&quot;content&quot;: &quot;features&quot;}}', 'class': None}, 'tag': 'p', 'end': 1074, 'start': 931, 'tag_type': 1},\n-{'start': 1074, 'end': 1084},\n-{'attributes': {}, 'tag': 'p', 'end': 1088, 'start': 1084, 'tag_type': 2},\n-{'start': 1088, 'end': 1109},\n-{'attributes': {'data-scrapy-replacement': 'select', 'class': 'product'}, 'tag': 'ul', 'end': 1162, 'start': 1109, 'tag_type': 1},\n-{'start': 1162, 'end': 1163},\n-{'attributes': {'data-scrapy-replacement': 'option'}, 'tag': 'li', 'end': 1200, 'start': 1163, 'tag_type': 1},\n-{'start': 1200, 'end': 1205},\n-{'attributes': {}, 'tag': 'li', 'end': 1210, 'start': 1205, 'tag_type': 2},\n-{'start': 1210, 'end': 1211},\n-{'attributes': {'data-scrapy-replacement': 'option'}, 'tag': 'li', 'end': 1248, 'start': 1211, 'tag_type': 1},\n-{'start': 1248, 'end': 1251},\n-{'attributes': {}, 'tag': 'li', 'end': 1256, 'start': 1251, 'tag_type': 2},\n-{'start': 1256, 'end': 1257},\n-{'attributes': {}, 'tag': 'ul', 'end': 1262, 'start': 1257, 'tag_type': 2},\n-{'start': 1262, 'end': 1263},\n-{'attributes': {}, 'tag': 'p', 'end': 1266, 'start': 1263, 'tag_type': 1},\n-{'start': 1266, 'end': 1292},\n-{'attributes': {}, 'tag': 'p', 'end': 1296, 'start': 1292, 'tag_type': 2},\n-{'start': 1296, 'end': 1297},\n-{'attributes': {}, 'tag': 'h3', 'end': 1301, 'start': 1297, 'tag_type': 1},\n-{'start': 1301, 'end': 1312},\n-{'attributes': {}, 'tag': 'h3', 'end': 1317, 'start': 1312, 'tag_type': 2},\n-{'start': 1317, 'end': 1318},\n-{'attributes': {'cellpadding': '0', 'width': '420', 'cellspacing': '0', 'class': 'rulet'}, 'tag': 'table', 'end': 1383, 'start': 1318, 'tag_type': 1},\n-{'attributes': {}, 'tag': 'tbody', 'end': 1390, 'start': 1383, 'tag_type': 1},\n-{'start': 1390, 'end': 1391},\n-{'attributes': {}, 'tag': 'tr', 'end': 1395, 'start': 1391, 'tag_type': 1},\n-{'attributes': {}, 'tag': 'td', 'end': 1399, 'start': 1395, 'tag_type': 1},\n-{'start': 1399, 'end': 1405},\n-{'attributes': {}, 'tag': 'td', 'end': 1410, 'start': 1405, 'tag_type': 2},\n-{'start': 1410, 'end': 1411},\n-{'attributes': {}, 'tag': 'td', 'end': 1415, 'start': 1411, 'tag_type': 1},\n-{'attributes': {'data-scrapy-annotate': '{&quot;variant&quot;: &quot;0&quot;, &quot;generated&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}'}, 'tag': 'ins', 'end': 1576, 'start': 1415, 'tag_type': 1},\n-{'start': 1576, 'end': 1581},\n-{'attributes': {}, 'tag': 'ins', 'end': 1587, 'start': 1581, 'tag_type': 2},\n-{'attributes': {}, 'tag': 'td', 'end': 1592, 'start': 1587, 'tag_type': 2},\n-{'start': 1592, 'end': 1593},\n-{'attributes': {}, 'tag': 'tr', 'end': 1598, 'start': 1593, 'tag_type': 2},\n-{'attributes': {}, 'tag': 'tbody', 'end': 1605, 'start': 1598, 'tag_type': 1},\n-{'attributes': {}, 'tag': 'table', 'end': 1613, 'start': 1605, 'tag_type': 2},\n-{'start': 1613, 'end': 1614},\n-{'attributes': {'style': 'my style', 'onmouseover': 'xxx', 'class': None}, 'tag': 'p', 'end': 1659, 'start': 1614, 'tag_type': 1},\n-{'start': 1659, 'end': 1660},\n-]\n-\n-# for testing parsing of some invalid html code (but still managed by browsers)\n-PAGE2 = u\"\"\"\n-<html>\n-<body>\n-<p class=&#34;MsoNormal&#34; style=&#34;margin: 0cm 0cm 0pt&#34;><span lang=&#34;EN-GB&#34;>\n-Hello world!\n-</span>\n-</p>\n-</body>\n-</html>\n-\"\"\"\n-\n-PARSED2 = [\n- {'end': 1, 'start': 0},\n- {'attributes': {}, 'end': 7, 'start': 1, 'tag': u'html', 'tag_type': 1},\n- {'end': 8, 'start': 7},\n- {'attributes': {}, 'end': 14, 'start': 8, 'tag': u'body', 'tag_type': 1},\n- {'end': 15, 'start': 14},\n- {'attributes': {u'style': u'&#34;margin:', u'0pt&#34;': None, u'class': u'&#34;MsoNormal&#34;', u'0cm': None}, 'end': 80, 'start': 15, 'tag': u'p', 'tag_type': 1},\n- {'attributes': {u'lang': u'&#34;EN-GB&#34;'}, 'end': 107, 'start': 80, 'tag': u'span', 'tag_type': 1},\n- {'end': 121, 'start': 107},\n- {'attributes': {}, 'end': 128, 'start': 121, 'tag': u'span', 'tag_type': 2},\n- {'end': 129, 'start': 128},\n- {'attributes': {}, 'end': 133, 'start': 129, 'tag': u'p', 'tag_type': 2},\n- {'end': 134, 'start': 133},\n- {'attributes': {}, 'end': 141, 'start': 134, 'tag': u'body', 'tag_type': 2},\n- {'end': 142, 'start': 141},\n- {'attributes': {}, 'end': 149, 'start': 142, 'tag': u'html', 'tag_type': 2},\n- {'end': 150, 'start': 149},\n-]\n-\n-# for testing tags inside comments\n-PAGE3 = u\"\"\"<html><body><h1>Helloooo!!</h1><p>Did i say hello??</p><!--<p>\n-</p>--><script type=\"text/javascript\">bla<!--comment-->blabla</script></body></html>\"\"\"\n-\n-PARSED3 = [\n- {'attributes': {}, 'end': 6, 'start': 0, 'tag': u'html', 'tag_type': 1},\n- {'attributes': {}, 'end': 12, 'start': 6, 'tag': u'body', 'tag_type': 1},\n- {'attributes': {}, 'end': 16, 'start': 12, 'tag': u'h1', 'tag_type': 1},\n- {'end': 26, 'start': 16},\n- {'attributes': {}, 'end': 31, 'start': 26, 'tag': u'h1', 'tag_type': 2},\n- {'attributes': {}, 'end': 34, 'start': 31, 'tag': u'p', 'tag_type': 1},\n- {'end': 51, 'start': 34},\n- {'attributes': {}, 'end': 55, 'start': 51, 'tag': u'p', 'tag_type': 2},\n- {'end': 70, 'start': 55},\n- {'attributes': {u'type': u'text/javascript'}, 'end': 101, 'start': 70, 'tag': u'script', 'tag_type': 1},\n- {'end': 104, 'start': 101},\n- {'end': 118, 'start': 104},\n- {'end': 124, 'start': 118},\n- {'attributes': {}, 'end': 133, 'start': 124, 'tag': u'script', 'tag_type': 2},\n- {'attributes': {}, 'end': 140, 'start': 133, 'tag': u'body', 'tag_type': 2},\n- {'attributes': {}, 'end': 147, 'start': 140, 'tag': u'html', 'tag_type': 2}\n-]\n-\n-# for testing tags inside scripts\n-PAGE4 = u\"\"\"<html><body><h1>Konnichiwa!!</h1>hello<script type=\"text/javascript\">\\\n-doc.write(\"<img src=\" + base + \"product/\" + productid + \">\");\\\n-</script>hello again</body></html>\"\"\"\n-\n-PARSED4 = [\n- {'attributes': {}, 'end': 6, 'start': 0, 'tag': u'html', 'tag_type': 1},\n- {'attributes': {}, 'end': 12, 'start': 6, 'tag': u'body', 'tag_type': 1},\n- {'attributes': {}, 'end': 16, 'start': 12, 'tag': u'h1', 'tag_type': 1},\n- {'end': 28,'start': 16},\n- {'attributes': {}, 'end': 33, 'start': 28, 'tag': u'h1', 'tag_type': 2},\n- {'end': 38, 'start': 33},\n- {'attributes': {u'type': u'text/javascript'}, 'end': 69, 'start': 38, 'tag': u'script', 'tag_type': 1},\n- {'end': 130, 'start': 69},\n- {'attributes': {}, 'end': 139, 'start': 130, 'tag': u'script', 'tag_type': 2},\n- {'end': 150, 'start': 139},\n- {'attributes': {}, 'end': 157, 'start': 150, 'tag': u'body', 'tag_type': 2},\n- {'attributes': {}, 'end': 164, 'start': 157, 'tag': u'html', 'tag_type': 2},\n-]\n-\n-# Test sucessive cleaning elements\n-PAGE5 = u\"\"\"<html><body><script>hello</script><script>brb</script></body><!--commentA--><!--commentB--></html>\"\"\"\n-\n-PARSED5 = [\n- {'attributes': {}, 'end': 6, 'start': 0, 'tag': u'html', 'tag_type': 1},\n- {'attributes': {}, 'end': 12, 'start': 6, 'tag': u'body', 'tag_type': 1},\n- {'attributes': {}, 'end': 20, 'start': 12, 'tag': u'script', 'tag_type': 1},\n- {'end': 25, 'start': 20},\n- {'attributes': {}, 'end': 34, 'start': 25, 'tag': u'script', 'tag_type': 2},\n- {'attributes': {}, 'end': 42, 'start': 34, 'tag': u'script', 'tag_type': 1},\n- {'end': 45, 'start': 42},\n- {'attributes': {}, 'end': 54, 'start': 45, 'tag': u'script', 'tag_type': 2},\n- {'attributes': {}, 'end': 61, 'start': 54, 'tag': u'body', 'tag_type': 2},\n- {'end': 76, 'start': 61},\n- {'end': 91, 'start': 76},\n- {'attributes': {}, 'end': 98, 'start': 91, 'tag': u'html', 'tag_type': 2},\n-]\n- \n-# Test sucessive cleaning elements variant 2\n-PAGE6 = u\"\"\"<html><body><script>pss<!--comment-->pss</script>all<script>brb</script>\\n\\n</body></html>\"\"\"\n-\n-PARSED6 = [\n- {'attributes': {}, 'end': 6, 'start': 0, 'tag': u'html', 'tag_type': 1},\n- {'attributes': {}, 'end': 12, 'start': 6, 'tag': u'body', 'tag_type': 1},\n- {'attributes': {}, 'end': 20, 'start': 12, 'tag': u'script', 'tag_type': 1},\n- {'end': 23, 'start': 20},\n- {'end': 37, 'start': 23},\n- {'end': 40, 'start': 37},\n- {'attributes': {}, 'end': 49, 'start': 40, 'tag': u'script', 'tag_type': 2},\n- {'end': 52, 'start': 49},\n- {'attributes': {}, 'end': 60, 'start': 52, 'tag': u'script', 'tag_type': 1},\n- {'end': 63, 'start': 60},\n- {'attributes': {}, 'end': 72, 'start': 63, 'tag': u'script', 'tag_type': 2},\n- {'end': 74, 'start': 72},\n- {'attributes': {}, 'end': 81, 'start': 74, 'tag': u'body', 'tag_type': 2},\n- {'attributes': {}, 'end': 88, 'start': 81, 'tag': u'html', 'tag_type': 2},\n-]\n-\n-# Test source without ending body nor html\n-PAGE7 = u\"\"\"<html><body><p>veris in temporibus sub aprilis idibus</p><script>script code</script><!--comment-->\"\"\"\n-\n-PARSED7 = [\n-    {'attributes' : {}, 'end': 6, 'start': 0, 'tag': u'html', 'tag_type': 1},\n-    {'attributes': {}, 'end': 12, 'start': 6, 'tag': u'body', 'tag_type': 1},\n-    {'attributes': {}, 'end': 15, 'start': 12, 'tag': u'p', 'tag_type': 1},\n-    {'end': 53, 'start': 15},\n-    {'attributes': {}, 'end': 57, 'start': 53, 'tag': u'p', 'tag_type': 2},\n-    {'attributes' : {}, 'end': 65, 'start': 57, 'tag': u'script', 'tag_type': 1},\n-    {'end': 76, 'start': 65},\n-    {'attributes' : {}, 'end': 85, 'start': 76, 'tag': u'script', 'tag_type': 2},\n-    {'end': 99, 'start': 85},\n-]\n-\n-PAGE8 = u\"\"\"<a href=\"/overview.asp?id=277\"><img border=\"0\" src=\"/img/5200814311.jpg\" title=\\'Vinyl Cornice\\'</a></td><table width=\\'5\\'>\"\"\"\n-\n-PARSED8 = [\n-   {'attributes' : {u'href' : u\"/overview.asp?id=277\"}, 'end': 31, 'start': 0, 'tag': u'a', 'tag_type': 1},\n-   {'attributes' : {u'src' : u\"/img/5200814311.jpg\", u'border' : u\"0\", u'title': u'Vinyl Cornice'}, 'end': 94, 'start': 31, 'tag': u'img', 'tag_type': 1},\n-   {'attributes' : {}, 'end': 98, 'start': 94, 'tag': u'a', 'tag_type': 2},\n-   {'attributes' : {}, 'end': 103, 'start': 98, 'tag': u'td', 'tag_type': 2},\n-   {'attributes' : {u'width': u'5'}, 'end': 120, 'start': 103, 'tag': u'table', 'tag_type': 1}\n-]\n-\n-PAGE9 = u\"\"\"\\\n-<html>\\\n-<body>\\\n-<img width='230' height='150'src='/images/9589.jpg' >\\\n-<a href=\"/product/9589\">Click here</a>\\\n-</body>\\\n-</html>\\\n-\"\"\"\n-\n-PARSED9 = [\n-    {'attributes' : {}, 'end': 6, 'start': 0, 'tag': 'html', 'tag_type': 1},\n-    {'attributes' : {}, 'end': 12, 'start': 6, 'tag': 'body', 'tag_type': 1},\n-    {'attributes' : {'width': '230', 'height': '150', 'src': '/images/9589.jpg'}, 'end': 65, 'start': 12, 'tag': 'img', 'tag_type': 1},\n-    {'attributes' : {'href': '/product/9589'}, 'end': 89, 'start': 65, 'tag': 'a', 'tag_type': 1},\n-    {'end': 99, 'start': 89},\n-    {'attributes' : {}, 'end': 103, 'start': 99, 'tag': 'a', 'tag_type': 2},\n-    {'attributes' : {}, 'end': 110, 'start': 103, 'tag': 'body', 'tag_type': 2},\n-    {'attributes' : {}, 'end': 117, 'start': 110, 'tag': 'html', 'tag_type': 2},\n-]\n\n@@ -1,330 +0,0 @@\n-\"\"\"\n-Unit tests for pageparsing\n-\"\"\"\n-import os\n-from cStringIO import StringIO                                                                                                    \n-\n-from twisted.trial.unittest import TestCase, SkipTest\n-from scrapy.utils.python import str_to_unicode\n-from scrapy.utils.py26 import json\n-\n-from scrapy.contrib.ibl.htmlpage import HtmlPage\n-from scrapy.tests.test_contrib_ibl import path\n-\n-try:\n-    import numpy\n-except ImportError:\n-    numpy = None\n-\n-if numpy:\n-    from scrapy.contrib.ibl.extraction.pageparsing import (\n-        InstanceLearningParser, TemplatePageParser, ExtractionPageParser)\n-    from scrapy.contrib.ibl.extraction.pageobjects import TokenDict, TokenType\n-\n-\n-SIMPLE_PAGE = u\"\"\"\n-<html> <p some-attr=\"foo\">this is a test</p> </html>\n-\"\"\"\n-\n-LABELLED_PAGE1 = u\"\"\"\n-<html>\n-<h1 data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">Some Product</h1>\n-<p> some stuff</p>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-This is such a nice item<br/>\n-Everybody likes it.\n-</p>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\"/>\n-\\xa310.00\n-<br/>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;short_description&quot;}}\">\n-Old fashioned product\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;short_description&quot;}}\">\n-For exigent individuals\n-<p>click here for other items</p>\n-</html>\n-\"\"\"\n-\n-BROKEN_PAGE = u\"\"\"\n-<html> <p class=\"ruleb\"align=\"center\">html parser cannot parse this</p></html>\n-\"\"\"\n-\n-LABELLED_PAGE2 = u\"\"\"\n-<html><body>\n-<h1>A product</h1>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-<p>A very nice product for all intelligent people</p>\n-<div data-scrapy-ignore=\"true\">\n-<img scr=\"image.jpg\" /><br/><a link=\"back.html\">Click here to go back</a>\n-</div>\n-</div>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">\n-\\xa310.00<p data-scrapy-ignore=\"true\"> 13 <br></p>\n-</div>\n-<table data-scrapy-ignore=\"true\">\n-<tr><td data-scrapy-ignore=\"true\"></td></tr>\n-<tr></tr>\n-</table>\n-<img data-scrapy-ignore=\"true\" src=\"image2.jpg\"> \n-<img data-scrapy-ignore=\"true\" src=\"image3.jpg\" />\n-<img data-scrapy-ignore-beneath=\"true\" src=\"image2.jpg\">\n-<img data-scrapy-ignore-beneath=\"true\" src=\"image3.jpg\" />\n-</body></html>\n-\n-\"\"\"\n-\n-LABELLED_PAGE3 = u\"\"\"\n-<html><body>\n-<h1>A product</h1>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-<p>A very nice product for all intelligent people</p>\n-<div data-scrapy-ignore=\"true\">\n-<img scr=\"image.jpg\" /><br/><a link=\"back.html\">Click here to go back</a>\n-</div>\n-</div>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-\\xa310.00<p data-scrapy-ignore=\"true\"> 13 <br></p>\n-<table><tr>\n-<td>Description 1</td>\n-<td data-scrapy-ignore-beneath=\"true\">Description 2</td>\n-<td>Description 3</td>\n-<td>Description 4</td>\n-</tr></table>\n-</div>\n-</body></html>\n-\"\"\"\n-\n-LABELLED_PAGE4 = u\"\"\"\n-<html><body>\n-<h1>A product</h1>\n-<div>\n-<p>A very nice product for all intelligent people</p>\n-<div>\n-<img scr=\"image.jpg\" /><br/><a link=\"back.html\">Click here to go back</a>\n-</div>\n-</div>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-\\xa310.00<p data-scrapy-ignore=\"true\"> 13 <br></p>\n-<table><tr>\n-<td>Description 1</td>\n-<td data-scrapy-ignore-beneath=\"true\">Description 2</td>\n-<td>Description 3</td>\n-<td data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">\n-Price \\xa310.00</td>\n-</tr></table>\n-</div>\n-</body></html>\n-\"\"\"\n-\n-LABELLED_PAGE5 = u\"\"\"\n-<html><body>\n-<ul data-scrapy-replacement='select'>\n-<li data-scrapy-replacement='option'>Option A</li>\n-<li>Option I</li>\n-<li data-scrapy-replacement='option'>Option B</li>\n-</ul>\n-</body></html>\n-\"\"\"\n-\n-LABELLED_PAGE6 = u\"\"\"\n-<html><body>\n-Text A\n-<p><ins data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">\n-65.00</ins>pounds</p>\n-<p>Description: <ins data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-Text B</ins></p>\n-Text C\n-</body></html>\n-\"\"\"\n-\n-LABELLED_PAGE7 = u\"\"\"\n-<html><body>\n-<div data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-<ins data-scrapy-ignore=\"true\" data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;site_id&quot;}}\">Item Id</ins>\n-Description\n-</div>\n-</body></html>\n-\"\"\"\n-\n-LABELLED_PAGE8 = u\"\"\"\n-<html><body>\n-<div data-scrapy-annotate=\"{&quot;required&quot;: [&quot;description&quot;], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n-<ins data-scrapy-ignore=\"true\" data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;site_id&quot;}}\">Item Id</ins>\n-Description\n-</div>\n-</body></html>\n-\"\"\"\n-\n-LABELLED_PAGE9 = u\"\"\"\n-<html><body>\n-<img src=\"image.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}\">\n-<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 1</p>\n-<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$67</b>\n-<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 2</p>\n-<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$70</b>\n-<div data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;category&quot;}}\">tables</div>\n-</body></html>\n-\"\"\"\n-\n-LABELLED_PAGE10 = u\"\"\"\n-<html><body>\n-<img src=\"image.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}\">\n-<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 1</p>\n-<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$67</b>\n-<img src=\"swatch1.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\">\n-\n-<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 2</p>\n-<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$70</b>\n-<img src=\"swatch2.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\">\n-\n-<div data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;category&quot;}}\">tables</div>\n-</body></html>\n-\"\"\"\n-\n-def _parse_page(parser_class, pagetext):\n-    htmlpage = HtmlPage(None, {}, pagetext)\n-    parser = parser_class(TokenDict())\n-    parser.feed(htmlpage)\n-    return parser\n-\n-def _tags(pp, predicate):\n-    return [pp.token_dict.token_string(s) for s in pp.token_list \\\n-            if predicate(s)]\n-\n-class TestPageParsing(TestCase):\n-\n-    def setUp(self):\n-        if not numpy:\n-            raise SkipTest(\"numpy not available\")\n-\n-    def test_instance_parsing(self):\n-        pp = _parse_page(InstanceLearningParser, SIMPLE_PAGE)\n-        # all tags\n-        self.assertEqual(_tags(pp, bool), ['<html>', '<p>', '</p>', '</html>'])\n-\n-        # open/closing tag handling\n-        openp = lambda x: pp.token_dict.token_type(x) == TokenType.OPEN_TAG\n-        self.assertEqual(_tags(pp, openp), ['<html>', '<p>'])\n-        closep = lambda x: pp.token_dict.token_type(x) == TokenType.CLOSE_TAG\n-        self.assertEqual(_tags(pp, closep), ['</p>', '</html>'])\n-    \n-    def _validate_annotation(self, parser, lable_region, name, start_tag, end_tag):\n-        assert lable_region.surrounds_attribute == name\n-        start_token = parser.token_list[lable_region.start_index]\n-        assert parser.token_dict.token_string(start_token) == start_tag\n-        end_token = parser.token_list[lable_region.end_index]\n-        assert parser.token_dict.token_string(end_token) == end_tag\n-\n-    def test_template_parsing(self):\n-        lp = _parse_page(TemplatePageParser, LABELLED_PAGE1)\n-        self.assertEqual(len(lp.annotations), 5)\n-        self._validate_annotation(lp, lp.annotations[0], \n-                'name', '<h1>', '</h1>')\n-        \n-        # all tags were closed\n-        self.assertEqual(len(lp.labelled_tag_stacks), 0)\n-    \n-    def test_extraction_page_parsing(self):\n-        epp = _parse_page(ExtractionPageParser, SIMPLE_PAGE)\n-        ep = epp.to_extraction_page()\n-        assert len(ep.page_tokens) == 4\n-        assert ep.htmlpage.fragment_data(ep.htmlpage_tag(0)) == '<html>'\n-        assert ep.htmlpage.fragment_data(ep.htmlpage_tag(1)) == '<p some-attr=\"foo\">'\n-        \n-        assert ep.htmlpage_region_inside(1, 2) == 'this is a test'\n-        assert ep.htmlpage_region_inside(1, 3) == 'this is a test</p> '\n-\n-    def test_invalid_html(self):\n-        p = _parse_page(InstanceLearningParser, BROKEN_PAGE)\n-        assert p\n-        \n-    def test_ignore_region(self):\n-        \"\"\"Test ignored regions\"\"\"\n-        p = _parse_page(TemplatePageParser, LABELLED_PAGE2)\n-        self.assertEqual(p.ignored_regions, [(7,12),(15,17),(19,26),(21,22),(27,28),(28,29),(29,None),(30,None)])\n-        self.assertEqual(len(p.ignored_tag_stacks), 0)\n-\n-    def test_ignore_regions2(self):\n-        \"\"\"Test ignore-beneath regions\"\"\"\n-        p = _parse_page(TemplatePageParser, LABELLED_PAGE3)\n-        self.assertEqual(p.ignored_regions, [(7,12),(15,17),(22,None)])\n-        self.assertEqual(len(p.ignored_tag_stacks), 0)\n-        \n-    def test_ignore_regions3(self):\n-        \"\"\"Test ignore-beneath with annotation inside region\"\"\"\n-        p = _parse_page(TemplatePageParser, LABELLED_PAGE4)\n-        self.assertEqual(p.ignored_regions, [(15,17),(22,None)])\n-        self.assertEqual(len(p.ignored_tag_stacks), 0)\n-        \n-    def test_replacement(self):\n-        \"\"\"Test parsing of replacement tags\"\"\"\n-        p = _parse_page(TemplatePageParser, LABELLED_PAGE5)\n-        self.assertEqual(_tags(p, bool), ['<html>', '<body>', '<select>', '<option>',\n-                    '</option>', '<li>', '</li>', '<option>', '</option>', '</select>', '</body>', '</html>'])\n-        \n-    def test_partial(self):\n-        \"\"\"Test partial annotation parsing\"\"\"\n-        p = _parse_page(TemplatePageParser, LABELLED_PAGE6)\n-        text = p.annotations[0].annotation_text\n-        self.assertEqual(text.start_text, '')\n-        self.assertEqual(text.follow_text, 'pounds')\n-        text = p.annotations[1].annotation_text\n-        self.assertEqual(text.start_text, \"Description: \")\n-        self.assertEqual(text.follow_text, '')\n-        \n-    def test_ignored_partial(self):\n-        \"\"\"Test ignored region declared on partial annotation\"\"\"\n-        p = _parse_page(TemplatePageParser, LABELLED_PAGE7)\n-        self.assertEqual(p.ignored_regions, [(2, 3)])\n-        \n-    def test_extra_required(self):\n-        \"\"\"Test parsing of extra required attributes\"\"\"\n-        p = _parse_page(TemplatePageParser, LABELLED_PAGE8)\n-        self.assertEqual(p.extra_required_attrs, [\"description\"])\n-    \n-    def test_variants(self):\n-        \"\"\"Test parsing of variant annotations\"\"\"\n-        annotations = _parse_page(TemplatePageParser, LABELLED_PAGE9).annotations\n-        self.assertEqual(annotations[0].variant_id, None)\n-        self.assertEqual(annotations[1].variant_id, 1)\n-        self.assertEqual(annotations[2].variant_id, 1)\n-        self.assertEqual(annotations[3].variant_id, 2)\n-        self.assertEqual(annotations[4].variant_id, 2)\n-        self.assertEqual(annotations[5].variant_id, None)\n-\n-    def test_variants_in_attributes(self):\n-        \"\"\"Test parsing of variant annotations in attributes\"\"\"\n-        annotations = _parse_page(TemplatePageParser, LABELLED_PAGE10).annotations\n-        self.assertEqual(annotations[0].variant_id, None)\n-        self.assertEqual(annotations[1].variant_id, 1)\n-        self.assertEqual(annotations[2].variant_id, 1)\n-        self.assertEqual(annotations[3].variant_id, 1)\n-        self.assertEqual(annotations[4].variant_id, 2)\n-        self.assertEqual(annotations[5].variant_id, 2)\n-        self.assertEqual(annotations[6].variant_id, 2)\n-        self.assertEqual(annotations[7].variant_id, None)\n-\n-    def test_site_pages(self):\n-        \"\"\"\n-        Tests from real pages. More reliable and easy to build for more complicated structures\n-        \"\"\"\n-        SAMPLES_FILE_PREFIX = os.path.join(path, \"samples/samples_pageparsing\")\n-        count = 0\n-        fname = \"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)\n-        while os.path.exists(fname):\n-            source = str_to_unicode(open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"rb\").read())\n-            annotations = json.loads(str_to_unicode(open(fname, \"rb\").read()))\n-            template = HtmlPage(body=source)\n-            parser = TemplatePageParser(TokenDict())\n-            parser.feed(template)\n-            for annotation in parser.annotations:\n-                test_annotation = annotations.pop(0)\n-                for s in annotation.__slots__:\n-                    if s == \"tag_attributes\":\n-                        for pair in getattr(annotation, s):\n-                            self.assertEqual(list(pair), test_annotation[s].pop(0))\n-                    else:\n-                        self.assertEqual(getattr(annotation, s), test_annotation[s])\n-            self.assertEqual(annotations, [])\n-            count += 1\n-            fname = \"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
