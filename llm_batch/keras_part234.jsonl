{"custom_id": "keras#84f755259cf64cf38395ce5e5bafa36df9f979e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 1500 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -124,11 +124,8 @@ def test_inceptionv3():\n     last_dim = 2048\n     _test_application_basic(app)\n     _test_application_notop(app, last_dim)\n-    _test_app_pooling(app, last_dim)\n-\n-    if K.backend() != 'cntk':\n-        # CNTK does not support dynamic padding.\n     _test_application_variable_input_channels(app, last_dim)\n+    _test_app_pooling(app, last_dim)\n \n \n def test_inceptionresnetv2():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba204bc43847a62de8eb8a7941da24f1fe6ac2a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 72 | Churn Cumulative: 72 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,72 @@\n+import pytest\n+import numpy as np\n+from keras.utils import conv_utils\n+\n+\n+def test_normalize_tuple():\n+    assert conv_utils.normalize_tuple(5, 2, 'kernel_size') == (5, 5)\n+    assert conv_utils.normalize_tuple([7, 9], 2, 'kernel_size') == (7, 9)\n+\n+    with pytest.raises(ValueError):\n+        conv_utils.normalize_tuple(None, 2, 'kernel_size')\n+    with pytest.raises(ValueError):\n+        conv_utils.normalize_tuple([2, 3, 4], 2, 'kernel_size')\n+    with pytest.raises(ValueError):\n+        conv_utils.normalize_tuple(['str', 'impossible'], 2, 'kernel_size')\n+\n+\n+def test_invalid_data_format():\n+    with pytest.raises(ValueError):\n+        conv_utils.normalize_data_format('channels_middle')\n+\n+\n+def test_invalid_padding():\n+    with pytest.raises(ValueError):\n+        conv_utils.normalize_padding('diagonal')\n+\n+\n+def test_invalid_convert_kernel():\n+    with pytest.raises(ValueError):\n+        conv_utils.convert_kernel(np.zeros((10, 20)))\n+\n+\n+def test_conv_output_length():\n+    assert conv_utils.conv_output_length(None, 7, 'same', 1) is None\n+    assert conv_utils.conv_output_length(224, 7, 'same', 1) == 224\n+    assert conv_utils.conv_output_length(224, 7, 'same', 2) == 112\n+    assert conv_utils.conv_output_length(32, 5, 'valid', 1) == 28\n+    assert conv_utils.conv_output_length(32, 5, 'valid', 2) == 14\n+    assert conv_utils.conv_output_length(32, 5, 'causal', 1) == 32\n+    assert conv_utils.conv_output_length(32, 5, 'causal', 2) == 16\n+    assert conv_utils.conv_output_length(32, 5, 'full', 1) == 36\n+    assert conv_utils.conv_output_length(32, 5, 'full', 2) == 18\n+\n+    with pytest.raises(AssertionError):\n+        conv_utils.conv_output_length(32, 5, 'diagonal', 2)\n+\n+\n+def test_conv_input_length():\n+    assert conv_utils.conv_input_length(None, 7, 'same', 1) is None\n+    assert conv_utils.conv_input_length(112, 7, 'same', 1) == 112\n+    assert conv_utils.conv_input_length(112, 7, 'same', 2) == 223\n+    assert conv_utils.conv_input_length(28, 5, 'valid', 1) == 32\n+    assert conv_utils.conv_input_length(14, 5, 'valid', 2) == 31\n+    assert conv_utils.conv_input_length(36, 5, 'full', 1) == 32\n+    assert conv_utils.conv_input_length(18, 5, 'full', 2) == 31\n+\n+    with pytest.raises(AssertionError):\n+        conv_utils.conv_output_length(18, 5, 'diagonal', 2)\n+\n+\n+def test_deconv_length():\n+    assert conv_utils.deconv_length(None, 1, 7, 'same') is None\n+    assert conv_utils.deconv_length(224, 1, 7, 'same') == 224\n+    assert conv_utils.deconv_length(224, 2, 7, 'same') == 448\n+    assert conv_utils.deconv_length(32, 1, 5, 'valid') == 36\n+    assert conv_utils.deconv_length(32, 2, 5, 'valid') == 67\n+    assert conv_utils.deconv_length(32, 1, 5, 'full') == 28\n+    assert conv_utils.deconv_length(32, 2, 5, 'full') == 59\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#531a8eabe222c3831cc5b55bafc81d2aa36f0672", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 2220 | Contributors (this commit): 29 | Commits (past 90d): 11 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -106,6 +106,11 @@ class Optimizer(object):\n             ValueError: in case of incompatible weight shapes.\n         \"\"\"\n         params = self.weights\n+        if len(params) != len(weights):\n+            raise ValueError('Length of the specified weight list (' +\n+                             str(len(weights)) +\n+                             ') does not match the number of weights ' +\n+                             'of the optimizer (' + str(len(params)) + ')')\n         weight_value_tuples = []\n         param_values = K.batch_get_value(params)\n         for pv, p, w in zip(param_values, params, weights):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b9ba1b3fa99755910971bd75a87f3d6824d2be8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 234 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -101,7 +101,7 @@ y_val = labels[-num_validation_samples:]\n print('Preparing embedding matrix.')\n \n # prepare embedding matrix\n-num_words = min(MAX_NUM_WORDS, len(word_index))\n+num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n for word, i in word_index.items():\n     if i >= MAX_NUM_WORDS:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ba2076275fab79cf4b8c281ccce506b257cab21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 14 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 27374 | Contributors (this commit): 222 | Commits (past 90d): 87 | Contributors (cumulative): 294 | DMM Complexity: 0.0\n\nDIFF:\n@@ -252,7 +252,7 @@ def _is_current_explicit_device(device_type):\n     \"\"\"\n     device_type = device_type.upper()\n     if device_type not in ['CPU', 'GPU']:\n-        raise ValueError('device_type should be either \"CPU\" or \"GPU\".')\n+        raise ValueError('`device_type` should be either \"CPU\" or \"GPU\".')\n     device = _get_current_tf_device()\n     return (device is not None and device.device_type == device_type.upper())\n \n\n@@ -327,6 +327,7 @@ class Layer(object):\n     @staticmethod\n     def _node_key(layer, node_index):\n         \"\"\"Converts a layer and its index to a unique (immutable type) name.\n+\n         This function is used internally with `self._container_nodes`.\n \n         # Arguments\n@@ -1093,7 +1094,7 @@ class Layer(object):\n                                  'instead.')\n \n     def add_loss(self, losses, inputs=None):\n-        \"\"\"Add losses to the layer.\n+        \"\"\"Adds losses to the layer.\n \n         The loss may potentially be conditional on some inputs tensors,\n         for instance activity losses are conditional on the layer's inputs.\n@@ -1127,7 +1128,7 @@ class Layer(object):\n         self._per_input_losses[inputs_hash] += losses\n \n     def add_update(self, updates, inputs=None):\n-        \"\"\"Add updates to the layer.\n+        \"\"\"Adds updates to the layer.\n \n         The updates may potentially be conditional on some inputs tensors,\n         for instance batch norm updates are conditional on the layer's inputs.\n@@ -1270,7 +1271,7 @@ class Layer(object):\n         return cls(**config)\n \n     def count_params(self):\n-        \"\"\"Count the total number of scalars composing the weights.\n+        \"\"\"Counts the total number of scalars composing the weights.\n \n         # Returns\n             An integer count.\n@@ -1889,7 +1890,7 @@ class Container(Layer):\n \n     @property\n     def updates(self):\n-        \"\"\"Retrieve the model's updates.\n+        \"\"\"Retrieves the model's updates.\n \n         Will only include updates that are either\n         inconditional, or conditional on inputs to this model\n@@ -1918,7 +1919,7 @@ class Container(Layer):\n \n     @property\n     def losses(self):\n-        \"\"\"Retrieve the model's losses.\n+        \"\"\"Retrieves the model's losses.\n \n         Will only include losses that are either\n         inconditional, or conditional on inputs to this model\n@@ -2052,7 +2053,7 @@ class Container(Layer):\n         return specs\n \n     def call(self, inputs, mask=None):\n-        \"\"\"Call the model on new inputs.\n+        \"\"\"Calls the model on new inputs.\n \n         In this case `call` just reapplies\n         all ops in the graph to the new inputs\n@@ -2480,7 +2481,7 @@ class Container(Layer):\n                     layer(input_tensors, **kwargs)\n \n         def process_layer(layer_data):\n-            \"\"\"Deserialize a layer, then call it on appropriate inputs.\n+            \"\"\"Deserializes a layer, then call it on appropriate inputs.\n \n             # Arguments\n                 layer_data: layer config dict.\n@@ -2538,7 +2539,7 @@ class Container(Layer):\n         return cls(inputs=input_tensors, outputs=output_tensors, name=name)\n \n     def save(self, filepath, overwrite=True, include_optimizer=True):\n-        \"\"\"Save the model to a single HDF5 file.\n+        \"\"\"Saves the model to a single HDF5 file.\n \n         The savefile includes:\n             - The model architecture, allowing to re-instantiate the model.\n@@ -3286,7 +3287,7 @@ def _convert_rnn_weights(layer, weights):\n \n \n def _need_convert_kernel(original_backend):\n-    \"\"\"Check if conversion on kernel matrices is required during weight loading.\n+    \"\"\"Checks if conversion on kernel matrices is required during weight loading.\n \n     The convolution operation is implemented differently in different backends.\n     While TH implements convolution, TF and CNTK implement the correlation operation.\n\n@@ -2149,12 +2149,14 @@ class Model(Container):\n                 if val_gen:\n                     if workers > 0:\n                         if isinstance(validation_data, Sequence):\n-                            val_enqueuer = OrderedEnqueuer(validation_data,\n+                            val_enqueuer = OrderedEnqueuer(\n+                                validation_data,\n                                 use_multiprocessing=use_multiprocessing)\n                             if validation_steps is None:\n                                 validation_steps = len(validation_data)\n                         else:\n-                            val_enqueuer = GeneratorEnqueuer(validation_data,\n+                            val_enqueuer = GeneratorEnqueuer(\n+                                validation_data,\n                                 use_multiprocessing=use_multiprocessing,\n                                 wait_time=wait_time)\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n\n@@ -872,7 +872,7 @@ class NumpyArrayIterator(Iterator):\n                  data_format=None,\n                  save_to_dir=None, save_prefix='', save_format='png'):\n         if y is not None and len(x) != len(y):\n-            raise ValueError('x (images tensor) and y (labels) '\n+            raise ValueError('`x` (images tensor) and `y` (labels) '\n                              'should have the same length. '\n                              'Found: x.shape = %s, y.shape = %s' %\n                              (np.asarray(x).shape, np.asarray(y).shape))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#22c54e9615f91dda7bf791e85bec8cbc58638023", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff0690ab533b7b882301f8b836ccfda8fd4f7f21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 34 | Churn Cumulative: 1808 | Contributors (this commit): 19 | Commits (past 90d): 15 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -373,6 +373,11 @@ _SHARED_SEQUENCES = {}\n _SEQUENCE_COUNTER = None\n \n \n+def init_pool(seqs):\n+    global _SHARED_SEQUENCES\n+    _SHARED_SEQUENCES = seqs\n+\n+\n def get_index(uid, i):\n     \"\"\"Get the value from the Sequence `uid` at index `i`.\n \n@@ -506,9 +511,12 @@ class OrderedEnqueuer(SequenceEnqueuer):\n                 (when full, workers could block on `put()`)\n         \"\"\"\n         if self.use_multiprocessing:\n-            self.executor_fn = lambda: multiprocessing.Pool(workers)\n+            self.executor_fn = lambda seqs: multiprocessing.Pool(workers,\n+                                                                 initializer=init_pool,\n+                                                                 initargs=(seqs,))\n         else:\n-            self.executor_fn = lambda: ThreadPool(workers)\n+            # We do not need the init since it's threads.\n+            self.executor_fn = lambda _: ThreadPool(workers)\n         self.workers = workers\n         self.queue = queue.Queue(max_queue_size)\n         self.stop_signal = threading.Event()\n@@ -531,7 +539,7 @@ class OrderedEnqueuer(SequenceEnqueuer):\n             if self.shuffle:\n                 random.shuffle(sequence)\n \n-            with closing(self.executor_fn()) as executor:\n+            with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n                 for i in sequence:\n                     if self.stop_signal.is_set():\n                         return\n\n@@ -6,9 +6,10 @@ import tarfile\n import threading\n import zipfile\n from itertools import cycle\n-\n+import multiprocessing as mp\n import numpy as np\n import pytest\n+import six\n from six.moves.urllib.parse import urljoin\n from six.moves.urllib.request import pathname2url\n \n@@ -24,6 +25,19 @@ if sys.version_info < (3,):\n         return x.next()\n \n \n+def use_spawn(func):\n+    \"\"\"Decorator to test both Unix (fork) and Windows (spawn)\"\"\"\n+    @six.wraps(func)\n+    def wrapper(*args, **kwargs):\n+        out = func(*args, **kwargs)\n+        if sys.version_info > (3, 4):\n+            mp.set_start_method('spawn', force=True)\n+            func(*args, **kwargs)\n+            mp.set_start_method('fork', force=True)\n+        return out\n+    return wrapper\n+\n+\n @pytest.fixture\n def in_tmpdir(tmpdir):\n     \"\"\"Runs a function in a temporary directory.\n@@ -219,6 +233,7 @@ def test_ordered_enqueuer_threads_not_ordered():\n     enqueuer.stop()\n \n \n+@use_spawn\n def test_ordered_enqueuer_processes():\n     enqueuer = OrderedEnqueuer(DummySequence([3, 200, 200, 3]), use_multiprocessing=True)\n     enqueuer.start(3, 10)\n@@ -238,6 +253,7 @@ def test_ordered_enqueuer_fail_threads():\n         next(gen_output)\n \n \n+@use_spawn\n def test_on_epoch_end_processes():\n     enqueuer = OrderedEnqueuer(DummySequence([3, 200, 200, 3]), use_multiprocessing=True)\n     enqueuer.start(3, 10)\n@@ -249,6 +265,7 @@ def test_on_epoch_end_processes():\n     enqueuer.stop()\n \n \n+@use_spawn\n def test_context_switch():\n     enqueuer = OrderedEnqueuer(DummySequence([3, 200, 200, 3]), use_multiprocessing=True)\n     enqueuer2 = OrderedEnqueuer(DummySequence([3, 200, 200, 3], value=15), use_multiprocessing=True)\n@@ -293,6 +310,7 @@ def test_on_epoch_end_threads():\n     enqueuer.stop()\n \n \n+@use_spawn\n def test_ordered_enqueuer_fail_processes():\n     enqueuer = OrderedEnqueuer(FaultSequence(), use_multiprocessing=True)\n     enqueuer.start(3, 10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b329a17e194e6e945bda87e0707ccb97e481746", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 9371 | Contributors (this commit): 106 | Commits (past 90d): 25 | Contributors (cumulative): 106 | DMM Complexity: 0.0\n\nDIFF:\n@@ -163,8 +163,11 @@ def get_session():\n         A TensorFlow session.\n     \"\"\"\n     global _SESSION\n-    if tf.get_default_session() is not None:\n-        session = tf.get_default_session()\n+\n+    default_session = tf.get_default_session()\n+\n+    if default_session is not None:\n+        session = default_session\n     else:\n         if _SESSION is None:\n             if not os.environ.get('OMP_NUM_THREADS'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ed76638b62fb8f46b047dce737e813360fd4ec1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 189 | Lines Deleted: 42 | Files Changed: 2 | Hunks: 31 | Methods Changed: 18 | Complexity Δ (Sum/Max): 13/9 | Churn Δ: 231 | Churn Cumulative: 3318 | Contributors (this commit): 50 | Commits (past 90d): 19 | Contributors (cumulative): 57 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,6 @@ try:\n except ImportError:\n     pil_image = None\n \n-\n if pil_image is not None:\n     _PIL_INTERPOLATION_METHODS = {\n         'nearest': pil_image.NEAREST,\n@@ -417,6 +416,7 @@ class ImageDataGenerator(object):\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n+        validation_split: fraction of images reserved for validation (strictly between 0 and 1).\n     \"\"\"\n \n     def __init__(self,\n@@ -438,7 +438,8 @@ class ImageDataGenerator(object):\n                  vertical_flip=False,\n                  rescale=None,\n                  preprocessing_function=None,\n-                 data_format=None):\n+                 data_format=None,\n+                 validation_split=0.0):\n         if data_format is None:\n             data_format = K.image_data_format()\n         self.featurewise_center = featurewise_center\n@@ -473,6 +474,10 @@ class ImageDataGenerator(object):\n             self.channel_axis = 3\n             self.row_axis = 1\n             self.col_axis = 2\n+        if validation_split and not 0 < validation_split < 1:\n+            raise ValueError('`validation_split` must be strictly between 0 and 1. '\n+                             ' Received arg: ', validation_split)\n+        self._validation_split = validation_split\n \n         self.mean = None\n         self.std = None\n@@ -514,7 +519,7 @@ class ImageDataGenerator(object):\n                               '`samplewise_center`.')\n \n     def flow(self, x, y=None, batch_size=32, shuffle=True, seed=None,\n-             save_to_dir=None, save_prefix='', save_format='png'):\n+             save_to_dir=None, save_prefix='', save_format='png', subset=None):\n         return NumpyArrayIterator(\n             x, y, self,\n             batch_size=batch_size,\n@@ -523,7 +528,8 @@ class ImageDataGenerator(object):\n             data_format=self.data_format,\n             save_to_dir=save_to_dir,\n             save_prefix=save_prefix,\n-            save_format=save_format)\n+            save_format=save_format,\n+            subset=subset)\n \n     def flow_from_directory(self, directory,\n                             target_size=(256, 256), color_mode='rgb',\n@@ -533,6 +539,7 @@ class ImageDataGenerator(object):\n                             save_prefix='',\n                             save_format='png',\n                             follow_links=False,\n+                            subset=None,\n                             interpolation='nearest'):\n         return DirectoryIterator(\n             directory, self,\n@@ -544,6 +551,7 @@ class ImageDataGenerator(object):\n             save_prefix=save_prefix,\n             save_format=save_format,\n             follow_links=follow_links,\n+            subset=subset,\n             interpolation=interpolation)\n \n     def standardize(self, x):\n@@ -865,22 +873,36 @@ class NumpyArrayIterator(Iterator):\n             images (if `save_to_dir` is set).\n         save_format: Format to use for saving sample images\n             (if `save_to_dir` is set).\n+        subset: Subset of data (`\"training\"` or `\"validation\"`) if\n+            validation_split is set in ImageDataGenerator.\n     \"\"\"\n \n     def __init__(self, x, y, image_data_generator,\n                  batch_size=32, shuffle=False, seed=None,\n                  data_format=None,\n-                 save_to_dir=None, save_prefix='', save_format='png'):\n+                 save_to_dir=None, save_prefix='', save_format='png',\n+                 subset=None):\n         if y is not None and len(x) != len(y):\n             raise ValueError('`x` (images tensor) and `y` (labels) '\n                              'should have the same length. '\n                              'Found: x.shape = %s, y.shape = %s' %\n                              (np.asarray(x).shape, np.asarray(y).shape))\n-\n+        if subset is not None:\n+            if subset not in {'training', 'validation'}:\n+                raise ValueError('Invalid subset name:', subset,\n+                                 '; expected \"training\" or \"validation\".')\n+            split_idx = int(len(x) * image_data_generator._validation_split)\n+            if subset == 'validation':\n+                x = x[:split_idx]\n+                if y is not None:\n+                    y = y[:split_idx]\n+            else:\n+                x = x[split_idx:]\n+                if y is not None:\n+                    y = y[split_idx:]\n         if data_format is None:\n             data_format = K.image_data_format()\n         self.x = np.asarray(x, dtype=K.floatx())\n-\n         if self.x.ndim != 4:\n             raise ValueError('Input data in `NumpyArrayIterator` '\n                              'should have rank 4. You passed an array '\n@@ -940,7 +962,7 @@ class NumpyArrayIterator(Iterator):\n         return self._get_batches_of_transformed_samples(index_array)\n \n \n-def _count_valid_files_in_directory(directory, white_list_formats, follow_links):\n+def _iter_valid_files(directory, white_list_formats, follow_links):\n     \"\"\"Count files with extension in `white_list_formats` contained in directory.\n \n     # Arguments\n@@ -950,30 +972,49 @@ def _count_valid_files_in_directory(directory, white_list_formats, follow_links)\n             the files to be counted.\n         follow_links: boolean.\n \n-    # Returns\n-        the count of files with extension in `white_list_formats` contained in\n-        the directory.\n+    # Yields\n+        tuple of (root, filename) with extension in `white_list_formats`.\n     \"\"\"\n     def _recursive_list(subpath):\n         return sorted(os.walk(subpath, followlinks=follow_links), key=lambda x: x[0])\n \n-    samples = 0\n-    for _, _, files in _recursive_list(directory):\n-        for fname in files:\n-            is_valid = False\n+    for root, _, files in _recursive_list(directory):\n+        for fname in sorted(files):\n             for extension in white_list_formats:\n                 if fname.lower().endswith('.tiff'):\n                     warnings.warn('Using \\'.tiff\\' files with multiple bands will cause distortion. '\n                                   'Please verify your output.')\n                 if fname.lower().endswith('.' + extension):\n-                    is_valid = True\n-                    break\n-            if is_valid:\n-                samples += 1\n-    return samples\n+                    yield root, fname\n \n \n-def _list_valid_filenames_in_directory(directory, white_list_formats,\n+def _count_valid_files_in_directory(directory, white_list_formats, split, follow_links):\n+    \"\"\"Count files with extension in `white_list_formats` contained in directory.\n+\n+    # Arguments\n+        directory: absolute path to the directory\n+            containing files to be counted\n+        white_list_formats: set of strings containing allowed extensions for\n+            the files to be counted.\n+        split: tuple of floats (e.g. `(0.2, 0.6)`) to only take into\n+            account a certain fraction of files in each directory.\n+            E.g.: `segment=(0.6, 1.0)` would only account for last 40 percent\n+            of images in each directory.\n+        follow_links: boolean.\n+\n+    # Returns\n+        the count of files with extension in `white_list_formats` contained in\n+        the directory.\n+    \"\"\"\n+    num_files = len(list(_iter_valid_files(directory, white_list_formats, follow_links)))\n+    if split:\n+        start, stop = int(split[0] * num_files), int(split[1] * num_files)\n+    else:\n+        start, stop = 0, num_files\n+    return stop - start\n+\n+\n+def _list_valid_filenames_in_directory(directory, white_list_formats, split,\n                                        class_indices, follow_links):\n     \"\"\"List paths of files in `subdir` with extensions in `white_list_formats`.\n \n@@ -982,6 +1023,10 @@ def _list_valid_filenames_in_directory(directory, white_list_formats,\n             The directory name is used as class label and must be a key of `class_indices`.\n         white_list_formats: set of strings containing allowed extensions for\n             the files to be counted.\n+        split: tuple of floats (e.g. `(0.2, 0.6)`) to only take into\n+            account a certain fraction of files in each directory.\n+            E.g.: `segment=(0.6, 1.0)` would only account for last 40 percent\n+            of images in each directory.\n         class_indices: dictionary mapping a class name to its index.\n         follow_links: boolean.\n \n@@ -991,25 +1036,22 @@ def _list_valid_filenames_in_directory(directory, white_list_formats,\n             `directory`'s parent (e.g., if `directory` is \"dataset/class1\",\n             the filenames will be [\"class1/file1.jpg\", \"class1/file2.jpg\", ...]).\n     \"\"\"\n-    def _recursive_list(subpath):\n-        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda x: x[0])\n+    dirname = os.path.basename(directory)\n+    if split:\n+        num_files = len(list(_iter_valid_files(directory, white_list_formats, follow_links)))\n+        start, stop = int(split[0] * num_files), int(split[1] * num_files)\n+        valid_files = list(_iter_valid_files(directory, white_list_formats, follow_links))[start: stop]\n+    else:\n+        valid_files = _iter_valid_files(directory, white_list_formats, follow_links)\n \n     classes = []\n     filenames = []\n-    subdir = os.path.basename(directory)\n-    basedir = os.path.dirname(directory)\n-    for root, _, files in _recursive_list(directory):\n-        for fname in sorted(files):\n-            is_valid = False\n-            for extension in white_list_formats:\n-                if fname.lower().endswith('.' + extension):\n-                    is_valid = True\n-                    break\n-            if is_valid:\n-                classes.append(class_indices[subdir])\n-                # add filename relative to directory\n+    for root, fname in valid_files:\n+        classes.append(class_indices[dirname])\n         absolute_path = os.path.join(root, fname)\n-                filenames.append(os.path.relpath(absolute_path, basedir))\n+        relative_path = os.path.join(dirname, os.path.relpath(absolute_path, directory))\n+        filenames.append(relative_path)\n+\n     return classes, filenames\n \n \n@@ -1048,6 +1090,8 @@ class DirectoryIterator(Iterator):\n             images (if `save_to_dir` is set).\n         save_format: Format to use for saving sample images\n             (if `save_to_dir` is set).\n+        subset: Subset of data (`\"training\"` or `\"validation\"`) if\n+            validation_split is set in ImageDataGenerator.\n         interpolation: Interpolation method used to resample the image if the\n             target size is different from that of the loaded image.\n             Supported methods are \"nearest\", \"bilinear\", and \"bicubic\".\n@@ -1060,9 +1104,11 @@ class DirectoryIterator(Iterator):\n                  target_size=(256, 256), color_mode='rgb',\n                  classes=None, class_mode='categorical',\n                  batch_size=32, shuffle=True, seed=None,\n-                 data_format=None, save_to_dir=None,\n-                 save_prefix='', save_format='png',\n-                 follow_links=False, interpolation='nearest'):\n+                 data_format=None,\n+                 save_to_dir=None, save_prefix='', save_format='png',\n+                 follow_links=False,\n+                 subset=None,\n+                 interpolation='nearest'):\n         if data_format is None:\n             data_format = K.image_data_format()\n         self.directory = directory\n@@ -1096,6 +1142,19 @@ class DirectoryIterator(Iterator):\n         self.save_format = save_format\n         self.interpolation = interpolation\n \n+        if subset is not None:\n+            validation_split = self.image_data_generator._validation_split\n+            if subset == 'validation':\n+                split = (0, validation_split)\n+            elif subset == 'training':\n+                split = (validation_split, 1)\n+            else:\n+                raise ValueError('Invalid subset name: ', subset,\n+                                 '; expected \"training\" or \"validation\"')\n+        else:\n+            split = None\n+        self.subset = subset\n+\n         white_list_formats = {'png', 'jpg', 'jpeg', 'bmp', 'ppm', 'tif', 'tiff'}\n \n         # first, count the number of samples and classes\n@@ -1112,7 +1171,8 @@ class DirectoryIterator(Iterator):\n         pool = multiprocessing.pool.ThreadPool()\n         function_partial = partial(_count_valid_files_in_directory,\n                                    white_list_formats=white_list_formats,\n-                                   follow_links=follow_links)\n+                                   follow_links=follow_links,\n+                                   split=split)\n         self.samples = sum(pool.map(function_partial,\n                                     (os.path.join(directory, subdir)\n                                      for subdir in classes)))\n@@ -1127,13 +1187,14 @@ class DirectoryIterator(Iterator):\n         i = 0\n         for dirpath in (os.path.join(directory, subdir) for subdir in classes):\n             results.append(pool.apply_async(_list_valid_filenames_in_directory,\n-                                            (dirpath, white_list_formats,\n+                                            (dirpath, white_list_formats, split,\n                                              self.class_indices, follow_links)))\n         for res in results:\n             classes, filenames = res.get()\n             self.classes[i:i + len(classes)] = classes\n             self.filenames += filenames\n             i += len(classes)\n+\n         pool.close()\n         pool.join()\n         super(DirectoryIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n\n@@ -3,6 +3,8 @@ from keras.preprocessing import image\n from PIL import Image\n import numpy as np\n import os\n+import tempfile\n+import shutil\n \n \n class TestImage(object):\n@@ -90,6 +92,34 @@ class TestImage(object):\n             x2, y2 = seq[0]\n             assert list(y) != list(y2)\n \n+    def test_image_data_generator_with_validation_split(self):\n+        for test_images in self.all_test_images:\n+            img_list = []\n+            for im in test_images:\n+                img_list.append(image.img_to_array(im)[None, ...])\n+\n+            images = np.vstack(img_list)\n+            generator = image.ImageDataGenerator(validation_split=0.5)\n+            seq = generator.flow(images, np.arange(images.shape[0]),\n+                                 shuffle=False, batch_size=3,\n+                                 subset='validation')\n+            x, y = seq[0]\n+            assert list(y) == [0, 1, 2]\n+            seq = generator.flow(images, np.arange(images.shape[0]),\n+                                 shuffle=False, batch_size=3,\n+                                 subset='training')\n+            x2, y2 = seq[0]\n+            assert list(y2) == [4, 5, 6]\n+\n+            with pytest.raises(ValueError):\n+                generator.flow(images, np.arange(images.shape[0]),\n+                               shuffle=False, batch_size=3,\n+                               subset='foo')\n+\n+    def test_image_data_generator_with_split_value_error(self):\n+        with pytest.raises(ValueError):\n+            generator = image.ImageDataGenerator(validation_split=5)\n+\n     def test_image_data_generator_invalid_data(self):\n         generator = image.ImageDataGenerator(\n             featurewise_center=True,\n@@ -181,7 +211,7 @@ class TestImage(object):\n         # check number of classes and images\n         assert len(dir_iterator.class_indices) == num_classes\n         assert len(dir_iterator.classes) == count\n-        assert dir_iterator.filenames == sorted(filenames)\n+        assert set(dir_iterator.filenames) == set(filenames)\n \n         # Test invalid use cases\n         with pytest.raises(ValueError):\n@@ -228,6 +258,62 @@ class TestImage(object):\n         output_img[0][0][0] += 1\n         assert(input_img[0][0][0] != output_img[0][0][0])\n \n+    @pytest.mark.parametrize('validation_split,num_training', [\n+        (0.25, 12),\n+        (0.40, 10),\n+        (0.50, 8),\n+    ])\n+    def test_directory_iterator_with_validation_split(self, validation_split, num_training):\n+        num_classes = 2\n+        tmp_folder = tempfile.mkdtemp(prefix='test_images')\n+\n+        # create folders and subfolders\n+        paths = []\n+        for cl in range(num_classes):\n+            class_directory = 'class-{}'.format(cl)\n+            classpaths = [\n+                class_directory,\n+                os.path.join(class_directory, 'subfolder-1'),\n+                os.path.join(class_directory, 'subfolder-2'),\n+                os.path.join(class_directory, 'subfolder-1', 'sub-subfolder')\n+            ]\n+            for path in classpaths:\n+                os.mkdir(os.path.join(tmp_folder, path))\n+            paths.append(classpaths)\n+\n+        # save the images in the paths\n+        count = 0\n+        filenames = []\n+        for test_images in self.all_test_images:\n+            for im in test_images:\n+                # rotate image class\n+                im_class = count % num_classes\n+                # rotate subfolders\n+                classpaths = paths[im_class]\n+                filename = os.path.join(classpaths[count % len(classpaths)], 'image-{}.jpg'.format(count))\n+                filenames.append(filename)\n+                im.save(os.path.join(tmp_folder, filename))\n+                count += 1\n+\n+        # create iterator\n+        generator = image.ImageDataGenerator(validation_split=validation_split)\n+\n+        with pytest.raises(ValueError):\n+            generator.flow_from_directory(tmp_folder, subset='foo')\n+\n+        train_iterator = generator.flow_from_directory(tmp_folder, subset='training')\n+        assert train_iterator.samples == num_training\n+\n+        valid_iterator = generator.flow_from_directory(tmp_folder, subset='validation')\n+        assert valid_iterator.samples == count - num_training\n+\n+        # check number of classes and images\n+        assert len(train_iterator.class_indices) == num_classes\n+        assert len(train_iterator.classes) == num_training\n+        assert len(set(train_iterator.filenames) & set(filenames)) == num_training\n+\n+        shutil.rmtree(tmp_folder)\n+\n     def test_img_utils(self):\n         height, width = 10, 8\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c9b7d0dd8ffeb906c291a087c25c40fb8c7788e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 6 | Churn Cumulative: 9351 | Contributors (this commit): 51 | Commits (past 90d): 21 | Contributors (cumulative): 51 | DMM Complexity: None\n\nDIFF:\n@@ -1027,7 +1027,7 @@ class SimpleRNN(RNN):\n             warnings.warn('The `implementation` argument '\n                           'in `SimpleRNN` has been deprecated. '\n                           'Please remove it from your layer call.')\n-        if K.backend() == 'theano':\n+        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n             warnings.warn(\n                 'RNN dropout is no longer supported with the Theano backend '\n                 'due to technical limitations. '\n@@ -1583,7 +1583,7 @@ class GRU(RNN):\n             warnings.warn('`implementation=0` has been deprecated, '\n                           'and now defaults to `implementation=1`.'\n                           'Please update your layer call.')\n-        if K.backend() == 'theano':\n+        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n             warnings.warn(\n                 'RNN dropout is no longer supported with the Theano backend '\n                 'due to technical limitations. '\n@@ -2106,7 +2106,7 @@ class LSTM(RNN):\n             warnings.warn('`implementation=0` has been deprecated, '\n                           'and now defaults to `implementation=1`.'\n                           'Please update your layer call.')\n-        if K.backend() == 'theano':\n+        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n             warnings.warn(\n                 'RNN dropout is no longer supported with the Theano backend '\n                 'due to technical limitations. '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c2944e00121b6182ca7e0ee7d9e6537eb3e56e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 22 | Churn Cumulative: 3340 | Contributors (this commit): 51 | Commits (past 90d): 21 | Contributors (cumulative): 59 | DMM Complexity: 0.75\n\nDIFF:\n@@ -22,6 +22,7 @@ from .. import backend as K\n from ..utils.data_utils import Sequence\n \n try:\n+    from PIL import ImageEnhance\n     from PIL import Image as pil_image\n except ImportError:\n     pil_image = None\n@@ -184,6 +185,19 @@ def random_channel_shift(x, intensity, channel_axis=0):\n     return x\n \n \n+def random_brightness(x, brightness_range):\n+    if len(brightness_range) != 2:\n+        raise ValueError('`brightness_range should be tuple or list of two floats. '\n+                         'Received arg: ', brightness_range)\n+\n+    x = array_to_img(x)\n+    x = imgenhancer_Brightness = ImageEnhance.Brightness(x)\n+    u = np.random.uniform(brightness_range[0], brightness_range[1])\n+    x = imgenhancer_Brightness.enhance(u)\n+    x = img_to_array(x)\n+    return x\n+\n+\n def transform_matrix_offset_center(matrix, x, y):\n     o_x = float(x) / 2 + 0.5\n     o_y = float(y) / 2 + 0.5\n@@ -385,6 +399,7 @@ class ImageDataGenerator(object):\n         rotation_range: degrees (0 to 180).\n         width_shift_range: fraction of total width, if < 1, or pixels if >= 1.\n         height_shift_range: fraction of total height, if < 1, or pixels if >= 1.\n+        brightness_range: the range of brightness to apply\n         shear_range: shear intensity (shear angle in degrees).\n         zoom_range: amount of zoom. if scalar z, zoom will be randomly picked\n             in the range [1-z, 1+z]. A sequence of two can be passed instead\n@@ -429,6 +444,7 @@ class ImageDataGenerator(object):\n                  rotation_range=0.,\n                  width_shift_range=0.,\n                  height_shift_range=0.,\n+                 brightness_range=None,\n                  shear_range=0.,\n                  zoom_range=0.,\n                  channel_shift_range=0.,\n@@ -451,6 +467,7 @@ class ImageDataGenerator(object):\n         self.rotation_range = rotation_range\n         self.width_shift_range = width_shift_range\n         self.height_shift_range = height_shift_range\n+        self.brightness_range = brightness_range\n         self.shear_range = shear_range\n         self.zoom_range = zoom_range\n         self.channel_shift_range = channel_shift_range\n@@ -692,6 +709,9 @@ class ImageDataGenerator(object):\n             if np.random.random() < 0.5:\n                 x = flip_axis(x, img_row_axis)\n \n+        if self.brightness_range is not None:\n+            x = random_brightness(x, self.brightness_range)\n+\n         return x\n \n     def fit(self, x,\n\n@@ -48,6 +48,7 @@ class TestImage(object):\n                 shear_range=0.5,\n                 zoom_range=0.2,\n                 channel_shift_range=0.,\n+                brightness_range=(1, 5),\n                 fill_mode='nearest',\n                 cval=0.5,\n                 horizontal_flip=True,\n@@ -388,6 +389,7 @@ class TestImage(object):\n                 shear_range=0.5,\n                 zoom_range=0.2,\n                 channel_shift_range=0.,\n+                brightness_range=(1, 5),\n                 fill_mode='nearest',\n                 cval=0.5,\n                 horizontal_flip=True,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4f2e65c385d60fa87bb143c6c506cbe428895f44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 10 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 2710 | Contributors (this commit): 27 | Commits (past 90d): 12 | Contributors (cumulative): 63 | DMM Complexity: None\n\nDIFF:\n@@ -64,7 +64,7 @@ batch_size = 128\n num_classes = 10\n epochs = 40\n \n-# the data, shuffled and split between train and test sets\n+# the data, split between train and test sets\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n x_train = x_train.reshape(60000, 784)\n\n@@ -21,7 +21,7 @@ num_predictions = 20\n save_dir = os.path.join(os.getcwd(), 'saved_models')\n model_name = 'keras_cifar10_trained_model.h5'\n \n-# The data, shuffled and split between train and test sets:\n+# The data, split between train and test sets:\n (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n print('x_train shape:', x_train.shape)\n print(x_train.shape[0], 'train samples')\n\n@@ -112,7 +112,7 @@ num_predictions = 20\n save_dir = '/tmp/saved_models'\n model_name = 'keras_cifar10_trained_model.h5'\n \n-# The data, shuffled and split between train and test sets:\n+# The data, split between train and test sets:\n (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n print('x_train shape:', x_train.shape)\n print(x_train.shape[0], 'train samples')\n\n@@ -20,7 +20,7 @@ epochs = 12\n # input image dimensions\n img_rows, img_cols = 28, 28\n \n-# the data, shuffled and split between train and test sets\n+# the data, split between train and test sets\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n if K.image_data_format() == 'channels_first':\n\n@@ -47,7 +47,7 @@ epochs = 5\n row_hidden = 128\n col_hidden = 128\n \n-# The data, shuffled and split between train and test sets.\n+# The data, split between train and test sets.\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n # Reshapes data to 4D for Hierarchical RNN.\n\n@@ -31,7 +31,7 @@ hidden_units = 100\n learning_rate = 1e-6\n clip_norm = 1.0\n \n-# the data, shuffled and split between train and test sets\n+# the data, split between train and test sets\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n x_train = x_train.reshape(x_train.shape[0], -1, 1)\n\n@@ -17,7 +17,7 @@ batch_size = 128\n num_classes = 10\n epochs = 20\n \n-# the data, shuffled and split between train and test sets\n+# the data, split between train and test sets\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n x_train = x_train.reshape(60000, 784)\n\n@@ -91,7 +91,7 @@ def accuracy(y_true, y_pred):\n     return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n \n \n-# the data, shuffled and split between train and test sets\n+# the data, split between train and test sets\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n x_train = x_train.astype('float32')\n x_test = x_test.astype('float32')\n\n@@ -107,7 +107,7 @@ K.set_image_data_format('channels_first')\n # input image dimensions\r\n img_rows, img_cols = 28, 28\r\n \r\n-# the data, shuffled and split between train and test sets\r\n+# the data, split between train and test sets\r\n (x_train, _), (x_test, _) = mnist.load_data()\r\n \r\n x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n\n@@ -71,7 +71,7 @@ def train_model(model, train, test, num_classes):\n     print('Test accuracy:', score[1])\n \n \n-# the data, shuffled and split between train and test sets\n+# the data, split between train and test sets\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n # create two datasets one with digits below 5 and one with 5 and above\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06eaeebecfb73c23bfd531013ca172ee3bf5069c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 9/7 | Churn Δ: 74 | Churn Cumulative: 7145 | Contributors (this commit): 32 | Commits (past 90d): 24 | Contributors (cumulative): 48 | DMM Complexity: 0.12903225806451613\n\nDIFF:\n@@ -1512,7 +1512,40 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n \n def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                      padding='valid', data_format=None, dilation_rate=(1, 1)):\n-    raise NotImplementedError\n+    if data_format is None:\n+        data_format = image_data_format()\n+    if data_format not in {'channels_first', 'channels_last'}:\n+        raise ValueError('Unknown data_format ' + str(data_format))\n+\n+    x = _preprocess_conv2d_input(x, data_format)\n+    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n+    depthwise_kernel = C.reshape(C.transpose(depthwise_kernel, (1, 0, 2, 3)),\n+                                 (-1, 1) + depthwise_kernel.shape[2:])\n+    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n+    padding = _preprocess_border_mode(padding)\n+\n+    if dilation_rate == (1, 1):\n+        strides = (1,) + strides\n+        x = C.convolution(depthwise_kernel, x,\n+                          strides=strides,\n+                          auto_padding=[False, padding, padding],\n+                          groups=x.shape[0])\n+        x = C.convolution(pointwise_kernel, x,\n+                          strides=(1, 1, 1),\n+                          auto_padding=[False])\n+    else:\n+        if dilation_rate[0] != dilation_rate[1]:\n+            raise ValueError('CNTK Backend: non-square dilation_rate is '\n+                             'not supported.')\n+        if strides != (1, 1):\n+            raise ValueError('Invalid strides for dilated convolution')\n+        x = C.convolution(depthwise_kernel, x,\n+                          strides=dilation_rate[0],\n+                          auto_padding=[False, padding, padding])\n+        x = C.convolution(pointwise_kernel, x,\n+                          strides=(1, 1, 1),\n+                          auto_padding=[False])\n+    return _postprocess_conv2d_output(x, data_format)\n \n \n def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n\n@@ -58,6 +58,12 @@ def cntk_func_two_tensor(function_name, x_shape, y, **kwargs):\n         return KC.function([xc, yc], [output_cntk])\n \n \n+def cntk_func_three_tensor(function_name, x_shape, y, z, **kwargs):\n+    xc = KC.placeholder(x_shape)\n+    output_cntk = getattr(KC, function_name)(xc, KC.variable(y), KC.variable(z), **kwargs)\n+    return KC.function([xc], [output_cntk])\n+\n+\n def parse_shape_or_val(shape_or_val):\n     if isinstance(shape_or_val, np.ndarray):\n         return shape_or_val.shape, shape_or_val\n@@ -844,6 +850,35 @@ class TestBackend(object):\n             with pytest.raises(ValueError):\n                 k.conv3d(k.variable(xval), k.variable(kernel_val), data_format='channels_middle')\n \n+    def test_separable_conv2d(self):\n+        for (input_shape, data_format) in [((2, 3, 4, 5), 'channels_first'),\n+                                           ((2, 3, 5, 6), 'channels_first'),\n+                                           ((1, 6, 5, 3), 'channels_last')]:\n+            input_depth = input_shape[1] if data_format == 'channels_first' else input_shape[-1]\n+            _, x_val = parse_shape_or_val(input_shape)\n+            x_tf = KTF.variable(x_val)\n+            for kernel_shape in [(2, 2), (4, 3)]:\n+                for depth_multiplier in [1, 2]:\n+                    _, depthwise_val = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n+                    _, pointwise_val = parse_shape_or_val((1, 1) + (input_depth * depth_multiplier, 7))\n+\n+                    z_tf = KTF.eval(KTF.separable_conv2d(x_tf, KTF.variable(depthwise_val),\n+                                                         KTF.variable(pointwise_val),\n+                                                         data_format=data_format))\n+                    z_c = cntk_func_three_tensor('separable_conv2d', input_shape,\n+                                                 depthwise_val,\n+                                                 pointwise_val,\n+                                                 data_format=data_format)([x_val])[0]\n+                    assert_allclose(z_tf, z_c, 1e-3)\n+\n+        # Test invalid use cases\n+        for k in [KTF, KC]:\n+            with pytest.raises(ValueError):\n+                k.separable_conv2d(k.variable(x_val),\n+                                   k.variable(depthwise_val),\n+                                   k.variable(pointwise_val),\n+                                   data_format='channels_middle')\n+\n     @pytest.mark.parametrize('k', [KTF], ids=['TensorFlow'])\n     def test_depthwise_conv_2d(self, k):\n         for data_format in ['channels_first', 'channels_last']:\n\n@@ -279,7 +279,7 @@ def test_separable_conv_1d():\n                                                           batch_input_shape=(None, 5, None))])\n \n \n-@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n+@pytest.mark.skipif(K.backend() == 'theano', reason='Theano does not support it yet')\n @keras_test\n def test_separable_conv_2d():\n     num_samples = 2\n@@ -296,6 +296,8 @@ def test_separable_conv_2d():\n                         continue\n                     if dilation_rate != (1, 1) and strides != (1, 1):\n                         continue\n+                    if dilation_rate != (1, 1) and K.backend() == 'cntk':\n+                        continue\n \n                     layer_test(convolutional.SeparableConv2D,\n                                kwargs={'filters': filters,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
