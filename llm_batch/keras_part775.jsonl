{"custom_id": "keras#b1a1107ed4177d00230a1c6914a1cf574596fe2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 52 | Churn Cumulative: 584 | Contributors (this commit): 6 | Commits (past 90d): 21 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -163,3 +163,8 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n             grads,\n             accumulators,\n         )\n+\n+    def _clip_by_norm(self, values, axes=None):\n+        # We need to use TF-specific OP to support the case,\n+        # when `values` are `tf.IndexedSlices`.\n+        return tf.clip_by_norm(values, self.clipnorm, axes)\n\n@@ -84,3 +84,21 @@ class AdamTest(testing.TestCase):\n         x = keras.ops.zeros((1, 5))\n         y = keras.ops.zeros((1, 10))\n         model.fit(x, y)\n+\n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\",\n+        reason=\"The IndexedSlices test can only run with TF backend.\",\n+    )\n+    def test_clipnorm_indexed_slices(self):\n+        # https://github.com/keras-team/keras/issues/18985\n+        model = keras.Sequential(\n+            [\n+                keras.layers.Embedding(10, 4),\n+                keras.layers.Flatten(),\n+                keras.layers.Dense(2),\n+            ]\n+        )\n+        model.compile(optimizer=Adam(clipnorm=100), loss=\"mse\")\n+        x = keras.ops.ones((8, 5))\n+        y = keras.ops.zeros((8, 2))\n+        model.fit(x, y, verbose=0)\n\n@@ -589,7 +589,7 @@ class BaseOptimizer:\n                 if g is None:\n                     clipped_grads.append(g)\n                 else:\n-                    clipped_grads.append(clip_by_norm(g, self.clipnorm))\n+                    clipped_grads.append(self._clip_by_norm(g))\n             return clipped_grads\n \n         if self.global_clipnorm and self.global_clipnorm > 0:\n@@ -796,6 +796,19 @@ class BaseOptimizer:\n             value = self._tracker.track(value)\n         return super().__setattr__(name, value)\n \n+    def _clip_by_norm(self, values, axes=None):\n+        # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n+        l2sum = ops.sum(ops.square(values), axes, keepdims=True)\n+        pred = l2sum > 0\n+        # Two-tap tf.where trick to bypass NaN gradients\n+        l2sum_safe = ops.where(pred, l2sum, ops.ones_like(l2sum))\n+        l2norm = ops.where(pred, ops.sqrt(l2sum_safe), l2sum)\n+        intermediate = ops.multiply(values, self.clipnorm)\n+        values_clip = ops.convert_to_tensor(intermediate) / ops.maximum(\n+            l2norm, self.clipnorm\n+        )\n+        return values_clip\n+\n \n base_optimizer_keyword_args = \"\"\"name: String. The name to use\n             for momentum accumulator weights created by\n@@ -845,20 +858,6 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n \"\"\"\n \n \n-def clip_by_norm(values, clip_norm, axes=None):\n-    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n-    l2sum = ops.sum(values * values, axes, keepdims=True)\n-    pred = l2sum > 0\n-    # Two-tap tf.where trick to bypass NaN gradients\n-    l2sum_safe = ops.where(pred, l2sum, ops.ones_like(l2sum))\n-    l2norm = ops.where(pred, ops.sqrt(l2sum_safe), l2sum)\n-    intermediate = values * clip_norm\n-    values_clip = ops.convert_to_tensor(intermediate) / ops.maximum(\n-        l2norm, clip_norm\n-    )\n-    return values_clip\n-\n-\n def global_norm(value_list):\n     \"\"\"Computes the global norm of multiple tensors.\"\"\"\n     squared_norms = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d02be35a2e3279998e4b727547530f4c63a27ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 63 | Churn Cumulative: 7543 | Contributors (this commit): 7 | Commits (past 90d): 84 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1331,7 +1331,21 @@ def sort(x, axis=-1):\n \n \n def split(x, indices_or_sections, axis=0):\n-    return tfnp.split(x, indices_or_sections, axis=axis)\n+    if not isinstance(indices_or_sections, int):\n+        # `tf.split` requires `num_or_size_splits`, so we need to convert\n+        # `indices_or_sections` to the appropriate format.\n+        # The following implementation offers better compatibility for the\n+        # tensor argument `indices_or_sections` than original `tfnp.split`.\n+        total_size = x.shape[axis]\n+        indices_or_sections = convert_to_tensor(indices_or_sections)\n+        start_size = indices_or_sections[0:1]\n+        end_size = total_size - indices_or_sections[-1:]\n+        num_or_size_splits = tf.concat(\n+            [start_size, tfnp.diff(indices_or_sections), end_size], axis=0\n+        )\n+    else:\n+        num_or_size_splits = indices_or_sections\n+    return tf.split(x, num_or_size_splits, axis=axis)\n \n \n def stack(x, axis=0):\n\n@@ -1168,15 +1168,15 @@ def sort(x, axis=-1):\n def split(x, indices_or_sections, axis=0):\n     x = convert_to_tensor(x)\n     dim = x.shape[axis]\n-    if isinstance(indices_or_sections, (list, tuple)):\n-        idxs = convert_to_tensor(indices_or_sections)\n-        start_size = indices_or_sections[0]\n-        end_size = dim - indices_or_sections[-1]\n-        chunk_sizes = (\n-            [start_size]\n-            + torch.diff(idxs).type(torch.int).tolist()\n-            + [end_size]\n+    if not isinstance(indices_or_sections, int):\n+        indices_or_sections = convert_to_tensor(indices_or_sections)\n+        start_size = indices_or_sections[0:1]\n+        end_size = dim - indices_or_sections[-1:]\n+        chunk_sizes = torch.concat(\n+            [start_size, torch.diff(indices_or_sections), end_size], dim=0\n         )\n+        # torch.split doesn't support tensor input for `split_size_or_sections`\n+        chunk_sizes = chunk_sizes.tolist()\n     else:\n         if dim % indices_or_sections != 0:\n             raise ValueError(\n\n@@ -3740,6 +3740,37 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(len(knp.split(x, 2)), 2)\n         self.assertEqual(len(knp.Split(2)(x)), 2)\n \n+        # test indices_or_sections as tensor\n+        x = knp.array([[1, 2, 3], [3, 2, 1]])\n+        indices_or_sections = knp.array([1, 2])\n+        x_np = np.array([[1, 2, 3], [3, 2, 1]])\n+        indices_or_sections_np = np.array([1, 2])\n+        self.assertAllClose(\n+            knp.split(x, indices_or_sections, axis=1),\n+            np.split(x_np, indices_or_sections_np, axis=1),\n+        )\n+\n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\",\n+        reason=\"Only test tensorflow backend\",\n+    )\n+    def test_split_with_jit_in_tf(self):\n+        import tensorflow as tf\n+\n+        x = knp.array([[1, 2, 3], [3, 2, 1]])\n+        indices = knp.array([1, 2])\n+        x_np = np.array([[1, 2, 3], [3, 2, 1]])\n+        indices_np = np.array([1, 2])\n+\n+        @tf.function(jit_compile=True)\n+        def fn(x, indices, axis):\n+            return knp.split(x, indices, axis=axis)\n+\n+        self.assertAllClose(\n+            fn(x, indices, axis=1),\n+            np.split(x_np, indices_np, axis=1),\n+        )\n+\n     def test_sqrt(self):\n         x = np.array([[1, 4, 9], [16, 25, 36]], dtype=\"float32\")\n         ref_y = np.sqrt(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#167c62025105aa5016760869ce74c9987bce2dfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 77 | Churn Cumulative: 144 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -107,7 +107,6 @@ class Tracker:\n \n \n class TrackedList(list):\n-    # TODO: override item removal methods?\n     def __init__(self, values=None, tracker=None):\n         self.tracker = tracker\n         if tracker and values:\n@@ -137,9 +136,28 @@ class TrackedList(list):\n         except ValueError:\n             python_utils.remove_by_id(self, value)\n \n+    def pop(self, index=-1):\n+        if self.tracker:\n+            value = self[index]\n+            self.tracker.untrack(value)\n+            return super().pop(index)\n+        else:\n+            return super().pop(index)\n+\n+    def clear(self):\n+        if self.tracker:\n+            for value in self:\n+                self.tracker.untrack(value)\n+        super().clear()\n+\n+    def __delitem__(self, index):\n+        value = self[index]  # Get value before removing\n+        super().__delitem__(index)\n+        if self.tracker:\n+            self.tracker.untrack(value)\n+\n \n class TrackedDict(dict):\n-    # TODO: override item removal methods?\n     def __init__(self, values=None, tracker=None):\n         self.tracker = tracker\n         if tracker and values:\n@@ -156,9 +174,29 @@ class TrackedDict(dict):\n             mapping = {k: self.tracker.track(v) for k, v in mapping.items()}\n         super().update(mapping)\n \n+    def pop(self, key, default=None):\n+        if self.tracker:\n+            value = super().pop(key, default)\n+            if value is not default:\n+                self.tracker.untrack(value)\n+            return value\n+        else:\n+            return super().pop(key, default)\n+\n+    def popitem(self):\n+        key, value = super().popitem()\n+        if self.tracker:\n+            self.tracker.untrack(value)\n+        return key, value\n+\n+    def clear(self):\n+        if self.tracker:\n+            for value in self.values():\n+                self.tracker.untrack(value)\n+        super().clear()\n+\n \n class TrackedSet(set):\n-    # TODO: override item removal methods?\n     def __init__(self, values=None, tracker=None):\n         self.tracker = tracker\n         if tracker and values:\n@@ -179,3 +217,15 @@ class TrackedSet(set):\n         if self.tracker:\n             self.tracker.untrack(value)\n         super().remove(value)\n+\n+    def pop(self):\n+        value = super().pop()\n+        if self.tracker:\n+            self.tracker.untrack(value)\n+        return value\n+\n+    def clear(self):\n+        if self.tracker:\n+            for value in self:\n+                self.tracker.untrack(value)\n+        super().clear()\n\n@@ -33,3 +33,24 @@ class TrackingTest(testing.TestCase):\n         lst.remove(v2)\n         self.assertLen(lst, 2)\n         self.assertLen(tracked_variables, 0)\n+\n+        lst2 = tracking.TrackedList([], tracker)\n+        lst2.append(v1)\n+        lst2.append(None)\n+        lst2.append(v2)\n+        lst2.append(0)\n+\n+        popped_value = lst2.pop()\n+        self.assertEqual(popped_value, 0)\n+        self.assertLen(lst2, 3)\n+        self.assertLen(tracked_variables, 2)\n+\n+        lst2.clear()\n+        self.assertLen(lst2, 0)\n+        self.assertLen(tracked_variables, 0)\n+\n+        lst2.append(v1)\n+        lst2.append(v2)\n+        del lst2[0]\n+        self.assertLen(lst2, 1)\n+        self.assertLen(tracked_variables, 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9723c7937e49b497696131ec5f9cf6e4ab177271", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 15 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.0.2\"\n+__version__ = \"3.0.3\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#df10cb2bf412c9cac9da0f168d1ae5b35fc0696b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 35 | Files Changed: 4 | Hunks: 31 | Methods Changed: 21 | Complexity Δ (Sum/Max): 27/14 | Churn Δ: 144 | Churn Cumulative: 461 | Contributors (this commit): 8 | Commits (past 90d): 18 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import collections\n+import itertools\n from functools import partial\n \n import jax\n@@ -336,7 +338,7 @@ class JAXTrainer(base_trainer.Trainer):\n             ) = data_adapter_utils.unpack_x_y_sample_weight(validation_data)\n \n         # Create an iterator that yields batches for one epoch.\n-        epoch_iterator = EpochIterator(\n+        epoch_iterator = JAXEpochIterator(\n             x=x,\n             y=y,\n             sample_weight=sample_weight,\n@@ -378,7 +380,7 @@ class JAXTrainer(base_trainer.Trainer):\n             metrics_variables = [v.value for v in self.metrics_variables]\n \n             self._purge_model_variables()\n-            for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+            for step, data in epoch_iterator.enumerate_epoch():\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n \n@@ -389,7 +391,6 @@ class JAXTrainer(base_trainer.Trainer):\n                     optimizer_variables,\n                     metrics_variables,\n                 )\n-                data = self._distribute_data(data)\n                 logs, state = self.train_function(state, data)\n                 (\n                     trainable_variables,\n@@ -426,9 +427,9 @@ class JAXTrainer(base_trainer.Trainer):\n \n             # Run validation.\n             if validation_data and self._should_eval(epoch, validation_freq):\n-                # Create EpochIterator for evaluation and cache it.\n+                # Create JAXEpochIterator for evaluation and cache it.\n                 if getattr(self, \"_eval_epoch_iterator\", None) is None:\n-                    self._eval_epoch_iterator = EpochIterator(\n+                    self._eval_epoch_iterator = JAXEpochIterator(\n                         x=val_x,\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n@@ -493,7 +494,7 @@ class JAXTrainer(base_trainer.Trainer):\n             epoch_iterator = self._eval_epoch_iterator\n         else:\n             # Create an iterator that yields batches of input/target data.\n-            epoch_iterator = EpochIterator(\n+            epoch_iterator = JAXEpochIterator(\n                 x=x,\n                 y=y,\n                 sample_weight=sample_weight,\n@@ -531,7 +532,7 @@ class JAXTrainer(base_trainer.Trainer):\n         metrics_variables = [v.value for v in self.metrics_variables]\n \n         self._purge_model_variables(optimizer_variables=False)\n-        for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+        for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_test_batch_begin(step)\n \n             state = (\n@@ -539,7 +540,6 @@ class JAXTrainer(base_trainer.Trainer):\n                 non_trainable_variables,\n                 metrics_variables,\n             )\n-            data = self._distribute_data(data)\n             logs, state = self.test_function(state, data)\n             (\n                 trainable_variables,\n@@ -579,7 +579,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n         # Create an iterator that yields batches of input data.\n-        epoch_iterator = EpochIterator(\n+        epoch_iterator = JAXEpochIterator(\n             x=x,\n             batch_size=batch_size,\n             steps_per_epoch=steps,\n@@ -589,7 +589,7 @@ class JAXTrainer(base_trainer.Trainer):\n \n         if not all(layer.built for layer in self._flatten_layers()):\n             # Build the model on one batch of data.\n-            for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+            for _, data in epoch_iterator.enumerate_epoch():\n                 # Build model\n                 x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data[0])\n                 with backend.StatelessScope():\n@@ -634,9 +634,8 @@ class JAXTrainer(base_trainer.Trainer):\n         ]\n         state = (trainable_variables, non_trainable_variables)\n         outputs = None\n-        for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+        for step, x in epoch_iterator.enumerate_epoch():\n             callbacks.on_predict_batch_begin(step)\n-            x = self._distribute_data(x)\n             batch_outputs, state = self.predict_function(state, x)\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n@@ -666,7 +665,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 y, class_weight\n             )\n         data = (x, y, sample_weight)\n-        data = self._distribute_data(data)\n+        data = _distribute_data(data)\n \n         # Maybe build model\n         self._symbolic_build(data_batch=data)\n@@ -719,7 +718,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self._assert_compile_called(\"test_on_batch\")\n \n         data = (x, y, sample_weight)\n-        data = self._distribute_data(data)\n+        data = _distribute_data(data)\n         # Maybe build model\n         self._symbolic_build(data_batch=data)\n         self._record_training_state_sharding_spec()\n@@ -795,18 +794,6 @@ class JAXTrainer(base_trainer.Trainer):\n             for ref_v, v in zip(self.metrics_variables, metrics_variables):\n                 ref_v.assign(v)\n \n-    def _distribute_data(self, data):\n-        distribution = distribution_lib.distribution()\n-        if distribution is not None:\n-\n-            def distribute_single_value(d):\n-                layout = distribution.get_data_layout(d.shape)\n-                return jax_distribution_lib.distribute_data_input(d, layout)\n-\n-            return jax.tree_util.tree_map(distribute_single_value, data)\n-        else:\n-            return data\n-\n     def _record_training_state_sharding_spec(self):\n         self._trainable_variable_shardings = [\n             v.value.sharding for v in self.trainable_variables\n@@ -901,3 +888,49 @@ class JAXTrainer(base_trainer.Trainer):\n         if metric_variables:\n             for v in self.metrics_variables:\n                 v._value = None\n+\n+\n+def _distribute_data(data):\n+    distribution = distribution_lib.distribution()\n+    if distribution is not None:\n+\n+        def distribute_single_value(d):\n+            layout = distribution.get_data_layout(d.shape)\n+            return jax_distribution_lib.distribute_data_input(d, layout)\n+\n+        return jax.tree_util.tree_map(distribute_single_value, data)\n+    else:\n+        return jax.tree_util.tree_map(jax.device_put, data)\n+\n+\n+class JAXEpochIterator(EpochIterator):\n+    def _get_iterator(self, return_type=\"auto\"):\n+        if return_type in (\"np\", \"auto\"):\n+            # enable prefetching when using numpy_iterator\n+            return self._prefetch_numpy_iterator(super()._get_iterator(\"np\"))\n+        return super()._get_iterator(return_type)\n+\n+    def _prefetch_numpy_iterator(self, numpy_iterator):\n+        \"\"\"Shard and prefetch batches on device.\n+\n+        Most of the implementation has been borrowed from\n+        `flax.jax_utils.prefetch_to_device`\n+\n+        This utility takes an iterator and returns a new iterator which fills an\n+        on device prefetch buffer. Eager prefetching can improve the performance\n+        of training loops significantly by overlapping compute and data\n+        transfer.\n+        \"\"\"\n+        queue = collections.deque()\n+\n+        # If you're training on GPUs, 2 is generally the best choice because\n+        # this guarantees that you can overlap a training step on GPU with a\n+        # data prefetch step on CPU.\n+        def enqueue(n=2):\n+            for data in itertools.islice(numpy_iterator, n):\n+                queue.append(_distribute_data(data))\n+\n+        enqueue(n=2)  # TODO: should we make `n` configurable?\n+        while queue:\n+            yield queue.popleft()\n+            enqueue(1)\n\n@@ -1,3 +1,5 @@\n+import collections\n+import itertools\n import warnings\n \n import numpy as np\n@@ -8,8 +10,8 @@ from packaging.version import parse\n from keras import backend\n from keras import callbacks as callbacks_module\n from keras import optimizers as optimizers_module\n+from keras.backend.torch.core import get_device\n from keras.trainers import data_adapters\n-from keras.trainers import epoch_iterator\n from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n@@ -207,7 +209,7 @@ class TorchTrainer(base_trainer.Trainer):\n             ) = data_adapter_utils.unpack_x_y_sample_weight(validation_data)\n \n         # Create an iterator that yields batches for one epoch.\n-        epoch_iterator = EpochIterator(\n+        epoch_iterator = TorchEpochIterator(\n             x=x,\n             y=y,\n             sample_weight=sample_weight,\n@@ -263,9 +265,9 @@ class TorchTrainer(base_trainer.Trainer):\n \n             # Run validation.\n             if validation_data and self._should_eval(epoch, validation_freq):\n-                # Create EpochIterator for evaluation and cache it.\n+                # Create TorchEpochIterator for evaluation and cache it.\n                 if getattr(self, \"_eval_epoch_iterator\", None) is None:\n-                    self._eval_epoch_iterator = EpochIterator(\n+                    self._eval_epoch_iterator = TorchEpochIterator(\n                         x=val_x,\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n@@ -328,7 +330,7 @@ class TorchTrainer(base_trainer.Trainer):\n             epoch_iterator = self._eval_epoch_iterator\n         else:\n             # Create an iterator that yields batches of input/target data.\n-            epoch_iterator = EpochIterator(\n+            epoch_iterator = TorchEpochIterator(\n                 x=x,\n                 y=y,\n                 sample_weight=sample_weight,\n@@ -494,11 +496,44 @@ class TorchTrainer(base_trainer.Trainer):\n         return batch_outputs\n \n \n-class TorchEpochIterator(epoch_iterator.EpochIterator):\n+class TorchEpochIterator(EpochIterator):\n     def _get_iterator(self, return_type=\"auto\"):\n         if return_type == \"auto\" and isinstance(\n             self.data_adapter, data_adapters.TorchDataLoaderAdapter\n         ):\n             return self.data_adapter.get_torch_dataloader()\n-\n+        elif return_type in (\"np\", \"auto\"):\n+            # enable prefetching when using numpy_iterator\n+            return self._prefetch_numpy_iterator(super()._get_iterator(\"np\"))\n         return super()._get_iterator(return_type)\n+\n+    def _prefetch_numpy_data(self, data):\n+        def to_device(d):\n+            return torch.as_tensor(d, device=get_device())\n+\n+        return tree.map_structure(to_device, data)\n+\n+    def _prefetch_numpy_iterator(self, numpy_iterator):\n+        \"\"\"Prefetch batches on device.\n+\n+        The idea has been borrowed from\n+        `torchtnt.utils.data.CudaDataPrefetcher`\n+\n+        This utility takes an iterator and returns a new iterator which fills an\n+        on device prefetch buffer. Eager prefetching can improve the performance\n+        of training loops significantly by overlapping compute and data\n+        transfer.\n+        \"\"\"\n+        queue = collections.deque()\n+\n+        # If you're training on GPUs, 2 is generally the best choice because\n+        # this guarantees that you can overlap a training step on GPU with a\n+        # data prefetch step on CPU.\n+        def enqueue(n=2):\n+            for data in itertools.islice(numpy_iterator, n):\n+                queue.append(self._prefetch_numpy_data(data))\n+\n+        enqueue(n=2)  # TODO: should we make `n` configurable?\n+        while queue:\n+            yield queue.popleft()\n+            enqueue(1)\n\n@@ -1,3 +1,4 @@\n+import numpy as np\n import tree\n \n from keras.trainers.data_adapters import data_adapter_utils\n@@ -41,7 +42,8 @@ class TFDatasetAdapter(DataAdapter):\n         def convert_to_numpy(x):\n             if isinstance(x, tf.SparseTensor):\n                 x = tf.sparse.to_dense(x)\n-            return x.numpy()\n+            # shared memory using `np.asarray`\n+            return np.asarray(x)\n \n         for batch in self._dataset:\n             yield tree.map_structure(convert_to_numpy, batch)\n\n@@ -1,3 +1,4 @@\n+import numpy as np\n import tree\n \n from keras.trainers.data_adapters.data_adapter import DataAdapter\n@@ -22,7 +23,10 @@ class TorchDataLoaderAdapter(DataAdapter):\n \n     def get_numpy_iterator(self):\n         for batch in self._dataloader:\n-            yield tuple(tree.map_structure(lambda x: x.cpu().numpy(), batch))\n+            # shared memory using `np.asarray`\n+            yield tuple(\n+                tree.map_structure(lambda x: np.asarray(x.cpu()), batch)\n+            )\n \n     def get_torch_dataloader(self):\n         return self._dataloader\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e897fba1f553734dcd30a09d35784ea8626e472", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 29 | Files Changed: 4 | Hunks: 17 | Methods Changed: 6 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 74 | Churn Cumulative: 12187 | Contributors (this commit): 12 | Commits (past 90d): 74 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -809,17 +809,30 @@ def isclose(x1, x2):\n \n @sparse.densifying_unary(True)\n def isfinite(x):\n-    return tfnp.isfinite(x)\n+    # `tfnp.isfinite` requires `enable_numpy_behavior`, so we reimplement it.\n+    x = convert_to_tensor(x)\n+    dtype_as_dtype = tf.as_dtype(x.dtype)\n+    if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n+        return tf.ones(x.shape, tf.bool)\n+    return tf.math.is_finite(x)\n \n \n def isinf(x):\n-    # TODO: tfnp.isinf will get python bool when input is a scalar, so we\n-    # need the extra `convert_to_tensor`\n-    return convert_to_tensor(tfnp.isinf(x))\n+    # `tfnp.isinf` requires `enable_numpy_behavior`, so we reimplement it.\n+    x = convert_to_tensor(x)\n+    dtype_as_dtype = tf.as_dtype(x.dtype)\n+    if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n+        return tf.zeros(x.shape, tf.bool)\n+    return tf.math.is_inf(x)\n \n \n def isnan(x):\n-    return tfnp.isnan(x)\n+    # `tfnp.isnan` requires `enable_numpy_behavior`, so we reimplement it.\n+    x = convert_to_tensor(x)\n+    dtype_as_dtype = tf.as_dtype(x.dtype)\n+    if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n+        return tf.zeros(x.shape, tf.bool)\n+    return tf.math.is_nan(x)\n \n \n def less(x1, x2):\n@@ -1039,23 +1052,20 @@ def moveaxis(x, source, destination):\n \n def nan_to_num(x):\n     x = convert_to_tensor(x)\n-    dtype = standardize_dtype(x.dtype)\n \n-    # tf.bool doesn't support max and min\n-    if dtype == \"bool\":\n-        x = tf.where(tfnp.isnan(x), tf.constant(False, x.dtype), x)\n-        x = tf.where(tfnp.isinf(x) & (x > 0), tf.constant(True, x.dtype), x)\n-        x = tf.where(tfnp.isinf(x) & (x < 0), tf.constant(False, x.dtype), x)\n+    dtype = x.dtype\n+    dtype_as_dtype = tf.as_dtype(dtype)\n+    if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n         return x\n \n     # Replace NaN with 0\n-    x = tf.where(tfnp.isnan(x), tf.constant(0, x.dtype), x)\n+    x = tf.where(tf.math.is_nan(x), tf.constant(0, dtype), x)\n \n     # Replace positive infinitiy with dtype.max\n-    x = tf.where(tfnp.isinf(x) & (x > 0), tf.constant(x.dtype.max, x.dtype), x)\n+    x = tf.where(tf.math.is_inf(x) & (x > 0), tf.constant(dtype.max, dtype), x)\n \n     # Replace negative infinity with dtype.min\n-    x = tf.where(tfnp.isinf(x) & (x < 0), tf.constant(x.dtype.min, x.dtype), x)\n+    x = tf.where(tf.math.is_inf(x) & (x < 0), tf.constant(dtype.min, dtype), x)\n \n     return x\n \n@@ -1439,7 +1449,24 @@ def tensordot(x1, x2, axes=2):\n \n @sparse.elementwise_unary\n def round(x, decimals=0):\n-    return tfnp.round(x, decimals=decimals)\n+    # `tfnp.round` requires `enable_numpy_behavior`, so we reimplement it.\n+    if decimals == 0:\n+        return tf.round(x)\n+    x_dtype = x.dtype\n+    if tf.as_dtype(x_dtype).is_integer:\n+        # int\n+        if decimals > 0:\n+            return x\n+        # temporarilaly convert to floats\n+        factor = tf.cast(math.pow(10, decimals), config.floatx())\n+        x = tf.cast(x, config.floatx())\n+    else:\n+        # float\n+        factor = tf.cast(math.pow(10, decimals), x.dtype)\n+    x = tf.multiply(x, factor)\n+    x = tf.round(x)\n+    x = tf.divide(x, factor)\n+    return tf.cast(x, x_dtype)\n \n \n def tile(x, repeats):\n\n@@ -4,7 +4,6 @@ import numpy as np\n import pytest\n from absl import logging\n from absl.testing import parameterized\n-from tensorflow.python.ops.numpy_ops import np_config\n \n from keras import layers\n from keras import metrics\n@@ -13,10 +12,6 @@ from keras import ops\n from keras import testing\n from keras.metrics import metrics_utils\n \n-# TODO: remove reliance on this (or alternatively, turn it on by default).\n-# This is no longer needed with tf-nightly.\n-np_config.enable_numpy_behavior()\n-\n \n class FalsePositivesTest(testing.TestCase):\n     def test_config(self):\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n from absl.testing import parameterized\n-from tensorflow.python.ops.numpy_ops import np_config\n \n from keras import backend\n from keras import layers\n@@ -25,9 +24,6 @@ from keras.ops import nn as knn\n from keras.ops import numpy as knp\n from keras.testing.test_utils import named_product\n \n-# TODO: remove reliance on this (or alternatively, turn it on by default).\n-np_config.enable_numpy_behavior()\n-\n \n class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n     def test_relu(self):\n\n@@ -1,10 +1,10 @@\n import contextlib\n import itertools\n+import math\n \n import numpy as np\n import pytest\n from absl.testing import parameterized\n-from tensorflow.python.ops.numpy_ops import np_config\n \n import keras\n from keras import backend\n@@ -15,9 +15,6 @@ from keras.backend.common.variables import ALLOWED_DTYPES\n from keras.ops import numpy as knp\n from keras.testing.test_utils import named_product\n \n-# TODO: remove reliance on this (or alternatively, turn it on by default).\n-np_config.enable_numpy_behavior()\n-\n \n class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n     def test_add(self):\n@@ -4013,7 +4010,8 @@ def create_sparse_tensor(x, indices_from=None, start=0, delta=2):\n     if indices_from is not None:\n         indices = indices_from.indices\n     else:\n-        flat_indices = np.arange(start, x.size, delta)\n+        size = math.prod(x.shape)\n+        flat_indices = np.arange(start, size, delta)\n         indices = np.stack(np.where(np.ones_like(x)), axis=1)[flat_indices]\n \n     if backend.backend() == \"tensorflow\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86bd12ce703f0b0f90a33d34a1d88b2d0101d966", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 298 | Lines Deleted: 109 | Files Changed: 6 | Hunks: 109 | Methods Changed: 84 | Complexity Δ (Sum/Max): 30/18 | Churn Δ: 407 | Churn Cumulative: 10489 | Contributors (this commit): 7 | Commits (past 90d): 147 | Contributors (cumulative): 29 | DMM Complexity: 0.64\n\nDIFF:\n@@ -207,6 +207,9 @@ def argmin(x, axis=None):\n \n \n def argsort(x, axis=-1):\n+    x = convert_to_tensor(x)\n+    if x.ndim == 0:\n+        return jnp.argsort(x, axis=None)\n     return jnp.argsort(x, axis=axis)\n \n \n\n@@ -1037,6 +1037,15 @@ def eye(N, M=None, k=0, dtype=None):\n \n \n def floor_divide(x1, x2):\n+    if not isinstance(x1, (int, float)):\n+        x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n+        x2 = convert_to_tensor(x2)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)), getattr(x2, \"dtype\", type(x2))\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return np.floor_divide(x1, x2)\n \n \n\n@@ -27,7 +27,7 @@ def add(x1, x2):\n     )\n     x1 = convert_to_tensor(x1, dtype)\n     x2 = convert_to_tensor(x2, dtype)\n-    return tfnp.add(x1, x2)\n+    return tf.add(x1, x2)\n \n \n def bincount(x, weights=None, minlength=0):\n@@ -83,7 +83,7 @@ def einsum(subscripts, *operands, **kwargs):\n         dtypes_to_resolve.append(x.dtype)\n     result_dtype = dtypes.result_type(*dtypes_to_resolve)\n     compute_dtype = result_dtype\n-    # TODO: tfnp.einsum doesn't support integer dtype with gpu\n+    # TODO: tf.einsum doesn't support integer dtype with gpu\n     if \"int\" in compute_dtype:\n         compute_dtype = config.floatx()\n \n@@ -105,14 +105,14 @@ def subtract(x1, x2):\n     )\n     x1 = convert_to_tensor(x1, dtype)\n     x2 = convert_to_tensor(x2, dtype)\n-    return tfnp.subtract(x1, x2)\n+    return tf.subtract(x1, x2)\n \n \n def matmul(x1, x2):\n-    x1_shape = x1.shape\n-    x2_shape = x2.shape\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n+    x1_shape = x1.shape\n+    x2_shape = x2.shape\n     # TODO: GPU and XLA only support float types\n     compute_dtype = dtypes.result_type(x1.dtype, x2.dtype, float)\n     result_dtype = dtypes.result_type(x1.dtype, x2.dtype)\n@@ -218,7 +218,15 @@ def matmul(x1, x2):\n         output.set_shape(output_shape)\n         return output\n     else:\n-        return tf.cast(tfnp.matmul(x1, x2), result_dtype)\n+        if x1_shape.rank == 2 and x2_shape.rank == 2:\n+            output = tf.matmul(x1, x2)\n+        elif x2_shape.rank == 1:\n+            output = tf.tensordot(x1, x2, axes=1)\n+        elif x1_shape.rank == 1:\n+            output = tf.tensordot(x1, x2, axes=[[0], [-2]])\n+        else:\n+            output = tf.matmul(x1, x2)\n+        return tf.cast(output, result_dtype)\n \n \n @sparse.elementwise_binary_intersection\n@@ -233,7 +241,7 @@ def multiply(x1, x2):\n     )\n     x1 = convert_to_tensor(x1, dtype)\n     x2 = convert_to_tensor(x2, dtype)\n-    return tfnp.multiply(x1, x2)\n+    return tf.multiply(x1, x2)\n \n \n def mean(x, axis=None, keepdims=False):\n@@ -302,13 +310,15 @@ def mean(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n     ori_dtype = standardize_dtype(x.dtype)\n     compute_dtype = dtypes.result_type(x.dtype, \"float32\")\n-    # `tfnp.mean` does not handle low precision (e.g., float16) overflow\n+    # `tf.reduce_mean` does not handle low precision (e.g., float16) overflow\n     # correctly, so we compute with float32 and cast back to the original type.\n     if \"int\" in ori_dtype or ori_dtype == \"bool\":\n         result_dtype = compute_dtype\n     else:\n         result_dtype = ori_dtype\n-    output = tfnp.mean(x, axis=axis, keepdims=keepdims, dtype=compute_dtype)\n+    output = tf.reduce_mean(\n+        tf.cast(x, compute_dtype), axis=axis, keepdims=keepdims\n+    )\n     return tf.cast(output, result_dtype)\n \n \n@@ -349,28 +359,30 @@ def absolute(x):\n     dtype = standardize_dtype(x.dtype)\n     if \"uint\" in dtype or dtype == \"bool\":\n         return x\n-    return tfnp.absolute(x)\n+    return tf.abs(x)\n \n \n @sparse.elementwise_unary\n def abs(x):\n-    return tfnp.absolute(x)\n+    return absolute(x)\n \n \n def all(x, axis=None, keepdims=False):\n-    return tfnp.all(x, axis=axis, keepdims=keepdims)\n+    x = tf.cast(x, \"bool\")\n+    return tf.reduce_all(x, axis=axis, keepdims=keepdims)\n \n \n def any(x, axis=None, keepdims=False):\n-    return tfnp.any(x, axis=axis, keepdims=keepdims)\n+    x = tf.cast(x, \"bool\")\n+    return tf.reduce_any(x, axis=axis, keepdims=keepdims)\n \n \n def amax(x, axis=None, keepdims=False):\n-    return tfnp.amax(x, axis=axis, keepdims=keepdims)\n+    return max(x, axis=axis, keepdims=keepdims)\n \n \n def amin(x, axis=None, keepdims=False):\n-    return tfnp.amin(x, axis=axis, keepdims=keepdims)\n+    return min(x, axis=axis, keepdims=keepdims)\n \n \n def append(x1, x2, axis=None):\n@@ -379,7 +391,10 @@ def append(x1, x2, axis=None):\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.append(x1, x2, axis=axis)\n+    if axis is None:\n+        return tf.concat([tf.reshape(x1, [-1]), tf.reshape(x2, [-1])], axis=0)\n+    else:\n+        return tf.concat([x1, x2], axis=axis)\n \n \n def arange(start, stop=None, step=1, dtype=None):\n@@ -397,7 +412,7 @@ def arange(start, stop=None, step=1, dtype=None):\n     return tf.range(start, stop, delta=step, dtype=dtype)\n \n \n-@sparse.densifying_unary(0.5 * tfnp.pi)\n+@sparse.densifying_unary(0.5 * np.pi)\n def arccos(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -458,7 +473,7 @@ def arctan2(x1, x2):\n     dtype = dtypes.result_type(x1.dtype, x2.dtype, float)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.arctan2(x1, x2)\n+    return tf.math.atan2(x1, x2)\n \n \n @sparse.elementwise_unary\n@@ -473,18 +488,30 @@ def arctanh(x):\n \n \n def argmax(x, axis=None):\n-    return tf.cast(tfnp.argmax(x, axis=axis), dtype=\"int32\")\n+    if axis is None:\n+        x = tf.reshape(x, [-1])\n+    return tf.cast(tf.argmax(x, axis=axis), dtype=\"int32\")\n \n \n def argmin(x, axis=None):\n-    return tf.cast(tfnp.argmin(x, axis=axis), dtype=\"int32\")\n+    if axis is None:\n+        x = tf.reshape(x, [-1])\n+    return tf.cast(tf.argmin(x, axis=axis), dtype=\"int32\")\n \n \n def argsort(x, axis=-1):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"bool\":\n         x = tf.cast(x, \"uint8\")\n-    return tf.cast(tfnp.argsort(x, axis=axis), dtype=\"int32\")\n+\n+    x_shape = x.shape\n+    if x_shape.rank == 0:\n+        return tf.cast([0], \"int32\")\n+\n+    if axis is None:\n+        x = tf.reshape(x, [-1])\n+        axis = 0\n+    return tf.argsort(x, axis=axis)\n \n \n def array(x, dtype=None):\n@@ -515,7 +542,7 @@ def average(x, axis=None, weights=None):\n \n \n def broadcast_to(x, shape):\n-    return tfnp.broadcast_to(x, shape)\n+    return tf.broadcast_to(x, shape)\n \n \n @sparse.elementwise_unary\n@@ -553,17 +580,17 @@ def concatenate(xs, axis=0):\n     if len(dtype_set) > 1:\n         dtype = dtypes.result_type(*dtype_set)\n         xs = tf.nest.map_structure(lambda x: tf.cast(x, dtype), xs)\n-    return tfnp.concatenate(xs, axis=axis)\n+    return tf.concat(xs, axis=axis)\n \n \n @sparse.elementwise_unary\n def conjugate(x):\n-    return tfnp.conjugate(x)\n+    return tf.math.conj(x)\n \n \n @sparse.elementwise_unary\n def conj(x):\n-    return tfnp.conjugate(x)\n+    return tf.math.conj(x)\n \n \n @sparse.elementwise_unary\n@@ -614,17 +641,25 @@ def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n \n \n def cumprod(x, axis=None, dtype=None):\n-    dtype = dtypes.result_type(dtype or x.dtype)\n-    if dtype == \"bool\":\n-        dtype = \"int32\"\n-    return tfnp.cumprod(x, axis=axis, dtype=dtype)\n+    x = convert_to_tensor(x, dtype=dtype)\n+    # tf.math.cumprod doesn't support bool\n+    if standardize_dtype(x.dtype) == \"bool\":\n+        x = tf.cast(x, \"int32\")\n+    if axis is None:\n+        x = tf.reshape(x, [-1])\n+        axis = 0\n+    return tf.math.cumprod(x, axis=axis)\n \n \n def cumsum(x, axis=None, dtype=None):\n-    dtype = dtypes.result_type(dtype or x.dtype)\n-    if dtype == \"bool\":\n-        dtype = \"int32\"\n-    return tfnp.cumsum(x, axis=axis, dtype=dtype)\n+    x = convert_to_tensor(x, dtype=dtype)\n+    # tf.math.cumprod doesn't support bool\n+    if standardize_dtype(x.dtype) == \"bool\":\n+        x = tf.cast(x, \"int32\")\n+    if axis is None:\n+        x = tf.reshape(x, [-1])\n+        axis = 0\n+    return tf.math.cumsum(x, axis=axis)\n \n \n def diag(x, k=0):\n@@ -684,23 +719,30 @@ def dot(x, y):\n     compute_dtype = dtypes.result_type(result_dtype, float)\n     x = tf.cast(x, compute_dtype)\n     y = tf.cast(y, compute_dtype)\n-    return tf.cast(tfnp.dot(x, y), dtype=result_dtype)\n+\n+    x_shape = x.shape\n+    y_shape = y.shape\n+    if x_shape.rank == 0 or y_shape.rank == 0:\n+        output = x * y\n+    elif y_shape.rank == 1:\n+        output = tf.tensordot(x, y, axes=[[-1], [-1]])\n+    else:\n+        output = tf.tensordot(x, y, axes=[[-1], [-2]])\n+    return tf.cast(output, result_dtype)\n \n \n def empty(shape, dtype=None):\n     dtype = dtype or config.floatx()\n-    return tfnp.empty(shape, dtype=dtype)\n+    return tf.zeros(shape, dtype=dtype)\n \n \n def equal(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # tfnp handles the casting internally during comparision, but it lacks\n-    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.equal(x1, x2)\n+    return tf.equal(x1, x2)\n \n \n @sparse.densifying_unary(1)\n@@ -719,7 +761,7 @@ def expand_dims(x, axis):\n         output = tf.sparse.expand_dims(x, axis)\n         output.set_shape(compute_expand_dims_output_shape(x.shape, axis))\n         return output\n-    return tfnp.expand_dims(x, axis)\n+    return tf.expand_dims(x, axis)\n \n \n @sparse.elementwise_unary\n@@ -732,7 +774,10 @@ def expm1(x):\n \n \n def flip(x, axis=None):\n-    return tfnp.flip(x, axis=axis)\n+    x = convert_to_tensor(x)\n+    if axis is None:\n+        return tf.reverse(x, tf.range(tf.rank(x)))\n+    return tf.reverse(x, [axis])\n \n \n @sparse.elementwise_unary\n@@ -749,33 +794,33 @@ def floor(x):\n \n def full(shape, fill_value, dtype=None):\n     dtype = dtype or config.floatx()\n-    return tfnp.full(shape, fill_value, dtype=dtype)\n+    fill_value = convert_to_tensor(fill_value, dtype)\n+    return tf.broadcast_to(fill_value, shape)\n \n \n def full_like(x, fill_value, dtype=None):\n-    return tfnp.full_like(x, fill_value, dtype=dtype)\n+    x = convert_to_tensor(x)\n+    dtype = dtypes.result_type(dtype or x.dtype)\n+    fill_value = convert_to_tensor(fill_value, dtype)\n+    return tf.broadcast_to(fill_value, tf.shape(x))\n \n \n def greater(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # tfnp handles the casting internally during comparision, but it lacks\n-    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.greater(x1, x2)\n+    return tf.greater(x1, x2)\n \n \n def greater_equal(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # tfnp handles the casting internally during comparision, but it lacks\n-    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.greater_equal(x1, x2)\n+    return tf.greater_equal(x1, x2)\n \n \n def hstack(xs):\n@@ -783,28 +828,35 @@ def hstack(xs):\n     if len(dtype_set) > 1:\n         dtype = dtypes.result_type(*dtype_set)\n         xs = tf.nest.map_structure(lambda x: convert_to_tensor(x, dtype), xs)\n-    return tfnp.hstack(xs)\n+    rank = tf.rank(xs[0])\n+    return tf.cond(\n+        tf.equal(rank, 1),\n+        lambda: tf.concat(xs, axis=0),\n+        lambda: tf.concat(xs, axis=1),\n+    )\n \n \n def identity(n, dtype=None):\n-    dtype = dtype or config.floatx()\n-    return tfnp.identity(n, dtype=dtype)\n+    return eye(N=n, M=n, dtype=dtype)\n \n \n @sparse.elementwise_unary\n def imag(x):\n-    return tfnp.imag(x)\n+    return tf.math.imag(x)\n \n \n def isclose(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # tfnp handles the casting internally during comparision, but it lacks\n-    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.isclose(x1, x2)\n+    if \"float\" in dtype:\n+        # atol defaults to 1e-08\n+        # rtol defaults to 1e-05\n+        return tf.abs(x1 - x2) <= (1e-08 + 1e-05 * tf.abs(x2))\n+    else:\n+        return tf.equal(x1, x2)\n \n \n @sparse.densifying_unary(True)\n@@ -838,23 +890,19 @@ def isnan(x):\n def less(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # tfnp handles the casting internally during comparison, but it lacks\n-    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.less(x1, x2)\n+    return tf.less(x1, x2)\n \n \n def less_equal(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # tfnp handles the casting internally during comparision, but it lacks\n-    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.less_equal(x1, x2)\n+    return tf.less_equal(x1, x2)\n \n \n def linspace(\n@@ -878,7 +926,7 @@ def linspace(\n     )\n \n \n-@sparse.densifying_unary(-tfnp.inf)\n+@sparse.densifying_unary(-np.inf)\n def log(x):\n     x = convert_to_tensor(x)\n     dtype = (\n@@ -890,7 +938,7 @@ def log(x):\n     return tf.math.log(x)\n \n \n-@sparse.densifying_unary(-tfnp.inf)\n+@sparse.densifying_unary(-np.inf)\n def log10(x):\n     x = convert_to_tensor(x)\n     dtype = (\n@@ -914,7 +962,7 @@ def log1p(x):\n     return tf.math.log1p(x)\n \n \n-@sparse.densifying_unary(-tfnp.inf)\n+@sparse.densifying_unary(-np.inf)\n def log2(x):\n     x = convert_to_tensor(x)\n     dtype = (\n@@ -944,15 +992,20 @@ def logaddexp(x1, x2):\n \n \n def logical_and(x1, x2):\n-    return tfnp.logical_and(x1, x2)\n+    x1 = tf.cast(x1, \"bool\")\n+    x2 = tf.cast(x2, \"bool\")\n+    return tf.logical_and(x1, x2)\n \n \n def logical_not(x):\n-    return tfnp.logical_not(x)\n+    x = tf.cast(x, \"bool\")\n+    return tf.logical_not(x)\n \n \n def logical_or(x1, x2):\n-    return tfnp.logical_or(x1, x2)\n+    x1 = tf.cast(x1, \"bool\")\n+    x2 = tf.cast(x2, \"bool\")\n+    return tf.logical_or(x1, x2)\n \n \n def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n@@ -988,7 +1041,7 @@ def maximum(x1, x2):\n     )\n     x1 = convert_to_tensor(x1, dtype)\n     x2 = convert_to_tensor(x2, dtype)\n-    return tfnp.maximum(x1, x2)\n+    return tf.maximum(x1, x2)\n \n \n def median(x, axis=None, keepdims=False):\n@@ -996,10 +1049,11 @@ def median(x, axis=None, keepdims=False):\n \n \n def meshgrid(*x, indexing=\"xy\"):\n-    return tfnp.meshgrid(*x, indexing=indexing)\n+    return tf.meshgrid(*x, indexing=indexing)\n \n \n def min(x, axis=None, keepdims=False, initial=None):\n+    x = convert_to_tensor(x)\n     # The TensorFlow numpy API implementation doesn't support `initial` so we\n     # handle it manually here.\n     if initial is not None:\n@@ -1032,7 +1086,7 @@ def minimum(x1, x2):\n     )\n     x1 = convert_to_tensor(x1, dtype)\n     x2 = convert_to_tensor(x2, dtype)\n-    return tfnp.minimum(x1, x2)\n+    return tf.minimum(x1, x2)\n \n \n def mod(x1, x2):\n@@ -1043,7 +1097,7 @@ def mod(x1, x2):\n         dtype = \"int32\"\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.mod(x1, x2)\n+    return tf.math.mod(x1, x2)\n \n \n def moveaxis(x, source, destination):\n@@ -1071,12 +1125,15 @@ def nan_to_num(x):\n \n \n def ndim(x):\n-    return tfnp.ndim(x)\n+    x = convert_to_tensor(x)\n+    return x.ndim\n \n \n def nonzero(x):\n+    x = convert_to_tensor(x)\n+    result = tf.unstack(tf.where(tf.cast(x, \"bool\")), x.shape.rank, axis=1)\n     return tf.nest.map_structure(\n-        lambda indices: tf.cast(indices, \"int32\"), tfnp.nonzero(x)\n+        lambda indices: tf.cast(indices, \"int32\"), result\n     )\n \n \n@@ -1086,11 +1143,11 @@ def not_equal(x1, x2):\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.not_equal(x1, x2)\n+    return tf.not_equal(x1, x2)\n \n \n def ones_like(x, dtype=None):\n-    return tfnp.ones_like(x, dtype=dtype)\n+    return tf.ones_like(x, dtype=dtype)\n \n \n def zeros_like(x, dtype=None):\n@@ -1103,10 +1160,11 @@ def outer(x1, x2):\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.outer(x1, x2)\n+    return tf.reshape(x1, [-1, 1]) * tf.reshape(x2, [-1])\n \n \n def pad(x, pad_width, mode=\"constant\", constant_values=None):\n+    x = convert_to_tensor(x)\n     kwargs = {}\n     if constant_values is not None:\n         if mode != \"constant\":\n@@ -1116,7 +1174,8 @@ def pad(x, pad_width, mode=\"constant\", constant_values=None):\n                 f\"Received: mode={mode}\"\n             )\n         kwargs[\"constant_values\"] = constant_values\n-    return tfnp.pad(x, pad_width, mode=mode, **kwargs)\n+    pad_width = convert_to_tensor(pad_width, \"int32\")\n+    return tf.pad(x, pad_width, mode.upper(), **kwargs)\n \n \n def prod(x, axis=None, keepdims=False, dtype=None):\n@@ -1129,7 +1188,8 @@ def prod(x, axis=None, keepdims=False, dtype=None):\n             dtype = \"int32\"\n         elif dtype in (\"uint8\", \"uint16\"):\n             dtype = \"uint32\"\n-    return tfnp.prod(x, axis=axis, keepdims=keepdims, dtype=dtype)\n+        x = tf.cast(x, dtype)\n+    return tf.reduce_prod(x, axis=axis, keepdims=keepdims)\n \n \n def _quantile(x, q, axis=None, method=\"linear\", keepdims=False):\n@@ -1252,17 +1312,20 @@ def quantile(x, q, axis=None, method=\"linear\", keepdims=False):\n \n \n def ravel(x):\n-    return tfnp.ravel(x)\n+    x = convert_to_tensor(x)\n+    return tf.reshape(x, [-1])\n \n \n @sparse.elementwise_unary\n def real(x):\n-    return tfnp.real(x)\n+    x = convert_to_tensor(x)\n+    return tf.math.real(x)\n \n \n-@sparse.densifying_unary(tfnp.inf)\n+@sparse.densifying_unary(np.inf)\n def reciprocal(x):\n-    return tfnp.reciprocal(x)\n+    x = convert_to_tensor(x)\n+    return tf.math.reciprocal(x)\n \n \n def repeat(x, repeats, axis=None):\n@@ -1277,6 +1340,7 @@ def repeat(x, repeats, axis=None):\n \n \n def reshape(x, new_shape):\n+    x = convert_to_tensor(x)\n     if isinstance(x, tf.SparseTensor):\n         from keras.ops.operation_utils import compute_reshape_output_shape\n \n@@ -1286,7 +1350,7 @@ def reshape(x, new_shape):\n         output = tf.sparse.reshape(x, new_shape)\n         output.set_shape(output_shape)\n         return output\n-    return tfnp.reshape(x, new_shape)\n+    return tf.reshape(x, new_shape)\n \n \n def roll(x, shift, axis=None):\n@@ -1327,7 +1391,8 @@ def sinh(x):\n \n \n def size(x):\n-    return tfnp.size(x)\n+    x = convert_to_tensor(x)\n+    return tf.size(x)\n \n \n def sort(x, axis=-1):\n@@ -1363,7 +1428,7 @@ def stack(x, axis=0):\n     if len(dtype_set) > 1:\n         dtype = dtypes.result_type(*dtype_set)\n         x = tf.nest.map_structure(lambda a: convert_to_tensor(a, dtype), x)\n-    return tfnp.stack(x, axis=axis)\n+    return tf.stack(x, axis=axis)\n \n \n def std(x, axis=None, keepdims=False):\n@@ -1371,7 +1436,7 @@ def std(x, axis=None, keepdims=False):\n     ori_dtype = standardize_dtype(x.dtype)\n     if \"int\" in ori_dtype or ori_dtype == \"bool\":\n         x = tf.cast(x, config.floatx())\n-    return tfnp.std(x, axis=axis, keepdims=keepdims)\n+    return tf.math.reduce_std(x, axis=axis, keepdims=keepdims)\n \n \n def swapaxes(x, axis1, axis2):\n@@ -1440,11 +1505,11 @@ def tensordot(x1, x2, axes=2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n     result_dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    # TODO: tfnp.tensordot only supports float types\n+    # TODO: tf.tensordot only supports float types\n     compute_dtype = dtypes.result_type(result_dtype, float)\n     x1 = tf.cast(x1, compute_dtype)\n     x2 = tf.cast(x2, compute_dtype)\n-    return tf.cast(tfnp.tensordot(x1, x2, axes=axes), dtype=result_dtype)\n+    return tf.cast(tf.tensordot(x1, x2, axes=axes), dtype=result_dtype)\n \n \n @sparse.elementwise_unary\n@@ -1529,11 +1594,12 @@ def vdot(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n     result_dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    # TODO: tfnp.vdot only supports float types\n     compute_dtype = dtypes.result_type(result_dtype, float)\n     x1 = tf.cast(x1, compute_dtype)\n     x2 = tf.cast(x2, compute_dtype)\n-    return tf.cast(tfnp.vdot(x1, x2), result_dtype)\n+    x1 = tf.reshape(x1, [-1])\n+    x2 = tf.reshape(x2, [-1])\n+    return tf.cast(dot(x1, x2), result_dtype)\n \n \n def vstack(xs):\n@@ -1541,10 +1607,11 @@ def vstack(xs):\n     if len(dtype_set) > 1:\n         dtype = dtypes.result_type(*dtype_set)\n         xs = tf.nest.map_structure(lambda x: convert_to_tensor(x, dtype), xs)\n-    return tfnp.vstack(xs)\n+    return tf.concat(xs, axis=0)\n \n \n def where(condition, x1, x2):\n+    condition = tf.cast(condition, \"bool\")\n     if x1 is not None and x2 is not None:\n         if not isinstance(x1, (int, float)):\n             x1 = convert_to_tensor(x1)\n@@ -1556,7 +1623,13 @@ def where(condition, x1, x2):\n         )\n         x1 = convert_to_tensor(x1, dtype)\n         x2 = convert_to_tensor(x2, dtype)\n-    return tfnp.where(condition, x1, x2)\n+        return tf.where(condition, x1, x2)\n+    if x1 is None and x2 is None:\n+        return nonzero(condition)\n+    raise ValueError(\n+        \"`x1` and `x2` either both should be `None`\"\n+        \" or both should have non-None value.\"\n+    )\n \n \n @sparse.elementwise_division\n@@ -1572,7 +1645,7 @@ def divide(x1, x2):\n     )\n     x1 = convert_to_tensor(x1, dtype)\n     x2 = convert_to_tensor(x2, dtype)\n-    return tfnp.divide(x1, x2)\n+    return tf.divide(x1, x2)\n \n \n @sparse.elementwise_division\n@@ -1589,19 +1662,19 @@ def power(x1, x2):\n         getattr(x1, \"dtype\", type(x1)),\n         getattr(x2, \"dtype\", type(x2)),\n     )\n-    # TODO: tfnp.power doesn't support uint* types\n+    # TODO: tf.pow doesn't support uint* types\n     if \"uint\" in dtype:\n         x1 = convert_to_tensor(x1, \"int32\")\n         x2 = convert_to_tensor(x2, \"int32\")\n-        return tf.cast(tfnp.power(x1, x2), dtype)\n+        return tf.cast(tf.pow(x1, x2), dtype)\n     x1 = convert_to_tensor(x1, dtype)\n     x2 = convert_to_tensor(x2, dtype)\n-    return tfnp.power(x1, x2)\n+    return tf.pow(x1, x2)\n \n \n @sparse.elementwise_unary\n def negative(x):\n-    return tfnp.negative(x)\n+    return tf.negative(x)\n \n \n @sparse.elementwise_unary\n@@ -1609,7 +1682,7 @@ def square(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"bool\":\n         x = tf.cast(x, \"int32\")\n-    return tfnp.square(x)\n+    return tf.square(x)\n \n \n @sparse.elementwise_unary\n@@ -1641,7 +1714,7 @@ def squeeze(x, axis=None):\n                 gather_indices.append(i)\n         new_indices = tf.gather(x.indices, gather_indices, axis=1)\n         return tf.SparseTensor(new_indices, x.values, tuple(new_shape))\n-    return tfnp.squeeze(x, axis=axis)\n+    return tf.squeeze(x, axis=axis)\n \n \n def transpose(x, axes=None):\n@@ -1651,15 +1724,16 @@ def transpose(x, axes=None):\n         output = tf.sparse.transpose(x, perm=axes)\n         output.set_shape(compute_transpose_output_shape(x.shape, axes))\n         return output\n-    return tfnp.transpose(x, axes=axes)\n+    return tf.transpose(x, perm=axes)\n \n \n def var(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n     compute_dtype = dtypes.result_type(x.dtype, \"float32\")\n     result_dtype = dtypes.result_type(x.dtype, float)\n+    x = tf.cast(x, compute_dtype)\n     return tf.cast(\n-        tfnp.var(x, axis=axis, keepdims=keepdims, dtype=compute_dtype),\n+        tf.math.reduce_variance(x, axis=axis, keepdims=keepdims),\n         result_dtype,\n     )\n \n@@ -1682,8 +1756,20 @@ def eye(N, M=None, k=0, dtype=None):\n \n \n def floor_divide(x1, x2):\n-    return tfnp.floor_divide(x1, x2)\n+    if not isinstance(x1, (int, float)):\n+        x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n+        x2 = convert_to_tensor(x2)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n+    return tf.math.floordiv(x1, x2)\n \n \n def logical_xor(x1, x2):\n-    return tfnp.logical_xor(x1, x2)\n+    x1 = tf.cast(x1, \"bool\")\n+    x2 = tf.cast(x2, \"bool\")\n+    return tf.math.logical_xor(x1, x2)\n\n@@ -1450,8 +1450,15 @@ def eye(N, M=None, k=None, dtype=None):\n \n \n def floor_divide(x1, x2):\n-    x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n-    return torch.floor_divide(x1, x2)\n+    if not isinstance(x1, (int, float)):\n+        x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n+        x2 = convert_to_tensor(x2)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    return cast(torch.floor_divide(x1, x2), dtype)\n \n \n def logical_xor(x1, x2):\n\n@@ -5967,7 +5967,11 @@ class FloorDivide(Operation):\n         x1_shape = getattr(x1, \"shape\", [])\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(x1_shape, x2_shape)\n-        return KerasTensor(output_shape, dtype=x1.dtype)\n+        output_dtype = dtypes.result_type(\n+            getattr(x1, \"dtype\", type(x1)),\n+            getattr(x2, \"dtype\", type(x2)),\n+        )\n+        return KerasTensor(output_shape, dtype=output_dtype)\n \n \n @keras_export([\"keras.ops.floor_divide\", \"keras.ops.numpy.floor_divide\"])\n\n@@ -269,7 +269,7 @@ class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(knp.where(condition, x, y).shape, (2, None, 3))\n         self.assertEqual(knp.where(condition).shape, (2, None, 1))\n \n-    def test_floordiv(self):\n+    def test_floor_divide(self):\n         x = KerasTensor((None, 3))\n         y = KerasTensor((2, None))\n         self.assertEqual(knp.floor_divide(x, y).shape, (2, 3))\n@@ -796,7 +796,7 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(knp.where(condition, x, y).shape, (2, 3))\n         self.assertAllEqual(knp.where(condition).shape, (2, 3))\n \n-    def test_floordiv(self):\n+    def test_floor_divide(self):\n         x = KerasTensor((2, 3))\n         y = KerasTensor((2, 3))\n         self.assertEqual(knp.floor_divide(x, y).shape, (2, 3))\n@@ -1984,11 +1984,14 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n         x = np.ones([2, 3, 4, 5])\n         y = np.ones([2, 3, 5, 6])\n         z = np.ones([5, 6])\n+        p = np.ones([4])\n         self.assertAllClose(knp.matmul(x, y), np.matmul(x, y))\n         self.assertAllClose(knp.matmul(x, z), np.matmul(x, z))\n+        self.assertAllClose(knp.matmul(p, x), np.matmul(p, x))\n \n         self.assertAllClose(knp.Matmul()(x, y), np.matmul(x, y))\n         self.assertAllClose(knp.Matmul()(x, z), np.matmul(x, z))\n+        self.assertAllClose(knp.Matmul()(p, x), np.matmul(p, x))\n \n     @parameterized.named_parameters(\n         named_product(\n@@ -2570,6 +2573,11 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.where(x > 1), np.where(x > 1))\n         self.assertAllClose(knp.Where()(x > 1), np.where(x > 1))\n \n+        with self.assertRaisesRegexp(\n+            ValueError, \"`x1` and `x2` either both should be `None`\"\n+        ):\n+            knp.where(x > 1, x, None)\n+\n     def test_digitize(self):\n         x = np.array([0.0, 1.0, 3.0, 1.6])\n         bins = np.array([0.0, 3.0, 4.5, 7.0])\n@@ -2927,6 +2935,10 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.Argsort(axis=1)(x), np.argsort(x, axis=1))\n         self.assertAllClose(knp.Argsort(axis=None)(x), np.argsort(x, axis=None))\n \n+        x = np.array(1)  # rank == 0\n+        self.assertAllClose(knp.argsort(x), np.argsort(x))\n+        self.assertAllClose(knp.Argsort()(x), np.argsort(x))\n+\n     def test_array(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.array(x), np.array(x))\n@@ -3923,7 +3935,7 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.vstack([x, y]), np.vstack([x, y]))\n         self.assertAllClose(knp.Vstack()([x, y]), np.vstack([x, y]))\n \n-    def test_floordiv(self):\n+    def test_floor_divide(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         y = np.array([[4, 5, 6], [3, 2, 1]])\n         z = np.array([[[1, 2, 3], [3, 2, 1]]])\n@@ -5613,6 +5625,74 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(\n+        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+    )\n+    def test_floor_divide(self, dtypes):\n+        import jax.numpy as jnp\n+\n+        dtype1, dtype2 = dtypes\n+        x1 = knp.ones((), dtype=dtype1)\n+        x2 = knp.ones((), dtype=dtype2)\n+        x1_jax = jnp.ones((), dtype=dtype1)\n+        x2_jax = jnp.ones((), dtype=dtype2)\n+        expected_dtype = standardize_dtype(\n+            jnp.floor_divide(x1_jax, x2_jax).dtype\n+        )\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.floor_divide(x1, x2).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.FloorDivide().symbolic_call(x1, x2).dtype),\n+            expected_dtype,\n+        )\n+\n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_floor_divide_python_types(self, dtype):\n+        import jax.experimental\n+        import jax.numpy as jnp\n+\n+        # We have to disable x64 for jax since jnp.floor_divide doesn't respect\n+        # JAX_DEFAULT_DTYPE_BITS=32 in `./conftest.py`. We also need to downcast\n+        # the expected dtype from 64 bit to 32 bit when using jax backend.\n+        with jax.experimental.disable_x64():\n+            x = knp.ones((), dtype=dtype)\n+            x_jax = jnp.ones((), dtype=dtype)\n+\n+            # python int\n+            expected_dtype = standardize_dtype(jnp.floor_divide(x_jax, 1).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            elif dtype == \"int64\":\n+                expected_dtype = \"int64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.floor_divide(x, 1).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.FloorDivide().symbolic_call(x, 1).dtype, expected_dtype\n+            )\n+\n+            # python float\n+            expected_dtype = standardize_dtype(\n+                jnp.floor_divide(x_jax, 1.0).dtype\n+            )\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.floor_divide(x, 1.0).dtype),\n+                expected_dtype,\n+            )\n+            self.assertEqual(\n+                knp.FloorDivide().symbolic_call(x, 1.0).dtype, expected_dtype\n+            )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_full(self, dtype):\n         import jax.numpy as jnp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
