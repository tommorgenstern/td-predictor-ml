{"custom_id": "keras#8f1eda8188e4fc7aa3ff80a88151a87f9fbe2197", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 248 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ def deserialize_model_from_bytecode(serialized_model):\n         # Some custom objects (e.g. an activation in a Dense layer,\n         # serialized as a string by Dense.get_config()) will require\n         # a custom_object_scope.\n-        model = saving_lib.load_model(filepath)\n+        model = saving_lib.load_model(filepath, safe_mode=False)\n     except Exception as e:\n         raise e\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c98ea36b96f6036b1dec569e5e496aee06a44b29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 173 | Lines Deleted: 28 | Files Changed: 5 | Hunks: 48 | Methods Changed: 21 | Complexity Δ (Sum/Max): 25/14 | Churn Δ: 201 | Churn Cumulative: 14825 | Contributors (this commit): 28 | Commits (past 90d): 35 | Contributors (cumulative): 66 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1485,8 +1485,8 @@ class Model(training_lib.Model):\n             if not isinstance(self.optimizer, optimizer_v2.OptimizerV2):\n                 raise ValueError(\n                     '\"optimizer\" must be an instance of '\n-                    \"tf.keras.optimizers.Optimizer when a dype policy \"\n-                    \"with a loss scale  used, but got: %s. Using policy: \"\n+                    \"tf.keras.optimizers.legacy.Optimizer when a dype policy \"\n+                    \"with a loss scale is used, but got: %s. Using policy: \"\n                     \"%s\" % (self.optimizer, self._dtype_policy)\n                 )\n             self.optimizer = loss_scale_optimizer.LossScaleOptimizer(\n\n@@ -909,6 +909,9 @@ class LossScaleOptimizer(\n                     \"longer be deserialized\"\n                 )\n             config[\"inner_optimizer\"] = config.pop(\"optimizer\")\n+        if isinstance(config[\"inner_optimizer\"], optimizer_v2.OptimizerV2):\n+            inner_optimizer = config[\"inner_optimizer\"]\n+        else:\n             inner_optimizer = optimizers.deserialize(\n                 config[\"inner_optimizer\"],\n                 custom_objects=custom_objects,\n@@ -1356,6 +1359,9 @@ class LossScaleOptimizerV3(\n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n         config = config.copy()  # Make a copy, since we mutate config\n+        if isinstance(config[\"inner_optimizer\"], optimizer.Optimizer):\n+            inner_optimizer = config[\"inner_optimizer\"]\n+        else:\n             inner_optimizer = optimizers.deserialize(\n                 config[\"inner_optimizer\"],\n                 custom_objects=custom_objects,\n@@ -1372,6 +1378,13 @@ class LossScaleOptimizerV3(\n     def iterations(self, variable):\n         self._optimizer.iterations = variable\n \n+    @property\n+    def variables(self):\n+        return self._optimizer.variables\n+\n+    def build(self, var_list):\n+        return self._optimizer.build(var_list)\n+\n     @property\n     def learning_rate(self):\n         return self._optimizer.learning_rate\n\n@@ -30,7 +30,9 @@ from keras.optimizers import sgd as sgd_experimental\n from keras.optimizers.legacy import adam\n from keras.optimizers.legacy import gradient_descent\n from keras.optimizers.legacy import optimizer_v2\n+from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n # isort: off\n from tensorflow.python.framework import (\n@@ -1202,6 +1204,46 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertEqual(opt.dynamic_growth_steps, 3.0)\n         self.assertEqual(opt.inner_optimizer.my_attribute, 123)\n \n+    @test_utils.run_v2_only\n+    def testConvertToLegacyOptimizer(self):\n+        opt = sgd_experimental.SGD(1.0)\n+        opt = loss_scale_optimizer.BaseLossScaleOptimizer(opt)\n+        converted_opt = optimizers.convert_to_legacy_optimizer(opt)\n+        self.assertEqual(\n+            type(converted_opt), loss_scale_optimizer.LossScaleOptimizer\n+        )\n+\n+        reference_opt = gradient_descent.SGD(1.0)\n+        reference_opt = loss_scale_optimizer.BaseLossScaleOptimizer(\n+            reference_opt\n+        )\n+        self.assertEqual(converted_opt.get_config(), reference_opt.get_config())\n+\n+        # Test with a custom learning rate schedule\n+        class CustomLRSchedule(learning_rate_schedule.LearningRateSchedule):\n+            def __init__(self, initial_learning_rate):\n+                self.initial_learning_rate = initial_learning_rate\n+\n+            def __call__(self, step):\n+                step = tf.cast(step, tf.float32)\n+                return self.initial_learning_rate / (step + 1)\n+\n+            def get_config(self):\n+                return {\"initial_learning_rate\": self.initial_learning_rate}\n+\n+        opt = sgd_experimental.SGD(CustomLRSchedule(1.0))\n+        opt = loss_scale_optimizer.BaseLossScaleOptimizer(opt)\n+        converted_opt = optimizers.convert_to_legacy_optimizer(opt)\n+        self.assertEqual(\n+            type(converted_opt), loss_scale_optimizer.LossScaleOptimizer\n+        )\n+\n+        reference_opt = gradient_descent.SGD(CustomLRSchedule(1.0))\n+        reference_opt = loss_scale_optimizer.BaseLossScaleOptimizer(\n+            reference_opt\n+        )\n+        self.assertEqual(converted_opt.get_config(), reference_opt.get_config())\n+\n     @test_combinations.generate(opt_combinations_only())\n     def testUnsupportedStrategy(self, opt_cls):\n         strategy = tf.distribute.experimental.CentralStorageStrategy()\n\n@@ -41,6 +41,7 @@ from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import policy\n from keras.mixed_precision import test_util as mp_test_util\n from keras.optimizers import optimizer_v1\n+from keras.optimizers import sgd\n from keras.optimizers.legacy import gradient_descent\n from keras.saving import object_registration\n from keras.saving.legacy import save\n@@ -142,6 +143,13 @@ class KerasModelTest(test_combinations.TestCase):\n             \"save_format\": \"tf\",\n             \"use_regularizer\": True,\n         },\n+        {\n+            \"testcase_name\": \"saved_model_legacy_distribute\",\n+            \"strategy_fn\": create_mirrored_strategy,\n+            \"save_format\": \"tf\",\n+            \"use_regularizer\": True,\n+            \"use_legacy_optimizer\": True,\n+        },\n         {\n             \"testcase_name\": \"saved_model_input_spec_distribute\",\n             \"strategy_fn\": create_mirrored_strategy,\n@@ -155,6 +163,13 @@ class KerasModelTest(test_combinations.TestCase):\n             \"save_format\": \"h5\",\n             \"use_regularizer\": True,\n         },\n+        {\n+            \"testcase_name\": \"h5_legacy_distribute\",\n+            \"strategy_fn\": create_mirrored_strategy,\n+            \"save_format\": \"h5\",\n+            \"use_regularizer\": True,\n+            \"use_legacy_optimizer\": True,\n+        },\n     )\n     def test_model(\n         self,\n@@ -165,9 +180,13 @@ class KerasModelTest(test_combinations.TestCase):\n         get_config=False,\n         save_format=None,\n         use_input_spec=False,\n+        use_legacy_optimizer=False,\n     ):\n         self._skip_if_strategy_unsupported(strategy_fn)\n         self._skip_if_save_format_unsupported(save_format)\n+        if not tf.__internal__.tf2.enabled():\n+            # The non-legacy optimizer is only supported in TF2\n+            use_legacy_optimizer = True\n         if use_regularizer:\n             weight_regularizer = mp_test_util.IdentityRegularizer()\n             activity_regularizer = mp_test_util.ReduceSumRegularizer()\n@@ -209,10 +228,14 @@ class KerasModelTest(test_combinations.TestCase):\n                 # variable, the variable will not change. So this tests the\n                 # learning rate not applied to a float16 value, but instead the\n                 # float32 variable.\n-                opt = gradient_descent.SGD(2**-14)\n+                learning_rate = 2**-14\n+                if use_legacy_optimizer:\n+                    opt = gradient_descent.SGD(learning_rate)\n+                else:\n+                    opt = sgd.SGD(learning_rate)\n                 # Use a fixed loss scale, as this test will fail if gradients\n                 # are skipped for a step due to dynamic loss scaling.\n-                opt = loss_scale_optimizer.LossScaleOptimizer(\n+                opt = loss_scale_optimizer.BaseLossScaleOptimizer(\n                     opt, dynamic=False, initial_scale=8\n                 )\n                 model.compile(\n@@ -295,6 +318,8 @@ class KerasModelTest(test_combinations.TestCase):\n         },\n     )\n     def test_fixed_loss_scaling(self, strategy_fn):\n+        # The non-legacy optimizer is only supported in TF2\n+        use_legacy_optimizer = not tf.__internal__.tf2.enabled()\n         # Note: We do not test mixed precision in this method, only loss\n         # scaling.\n         loss_scale = 8.0\n@@ -320,8 +345,11 @@ class KerasModelTest(test_combinations.TestCase):\n                 del y_true\n                 return tf.reduce_mean(y_pred)\n \n+            if use_legacy_optimizer:\n                 opt = gradient_descent.SGD(1.0)\n-            opt = loss_scale_optimizer.LossScaleOptimizer(\n+            else:\n+                opt = sgd.SGD(1.0)\n+            opt = loss_scale_optimizer.BaseLossScaleOptimizer(\n                 opt, dynamic=False, initial_scale=loss_scale\n             )\n             model.compile(\n@@ -363,6 +391,8 @@ class KerasModelTest(test_combinations.TestCase):\n         if use_loss_scaling:\n             loss_scale = 8.0\n         learning_rate = 2**-14\n+        # The non-legacy optimizer is only supported in TF2\n+        use_legacy_optimizer = not tf.__internal__.tf2.enabled()\n \n         with strategy.scope():\n             with policy.policy_scope(policy.Policy(\"mixed_float16\")):\n@@ -405,9 +435,12 @@ class KerasModelTest(test_combinations.TestCase):\n                     del y_true\n                     return tf.reduce_mean(y_pred)\n \n+                if use_legacy_optimizer:\n                     opt = gradient_descent.SGD(learning_rate)\n+                else:\n+                    opt = sgd.SGD(learning_rate)\n                 if use_loss_scaling:\n-                    opt = loss_scale_optimizer.LossScaleOptimizer(\n+                    opt = loss_scale_optimizer.BaseLossScaleOptimizer(\n                         opt, dynamic=False, initial_scale=loss_scale\n                     )\n                 model.compile(\n@@ -452,8 +485,8 @@ class KerasModelTest(test_combinations.TestCase):\n         # gradients\n         have_nan_gradients = backend.variable(False, dtype=tf.bool)\n         with strategy.scope():\n-            opt = gradient_descent.SGD(1.0)\n-            opt = loss_scale_optimizer.LossScaleOptimizer(\n+            opt = sgd.SGD(1.0)\n+            opt = loss_scale_optimizer.BaseLossScaleOptimizer(\n                 opt, initial_scale=initial_loss_scale, dynamic_growth_steps=2\n             )\n             with policy.policy_scope(\"mixed_float16\"):\n@@ -542,12 +575,21 @@ class KerasModelTest(test_combinations.TestCase):\n         x = layers.Input(shape=(1,))\n         y = mp_test_util.MultiplyLayer()(x)\n \n+        # The non-legacy optimizer is only supported in TF2\n+        use_legacy_optimizer = (\n+            not tf.__internal__.tf2.enabled() or not tf.executing_eagerly()\n+        )\n+\n         with policy.policy_scope(\"mixed_float16\"):\n             # Test optimizer is automatically wrapped with LSO\n             model = models.Model(x, y)\n-            model.compile(gradient_descent.SGD(1.0), \"mse\")\n+            if use_legacy_optimizer:\n+                optimizer = gradient_descent.SGD(1.0)\n+            else:\n+                optimizer = sgd.SGD(1.0)\n+            model.compile(optimizer, \"mse\")\n             self.assertIsInstance(\n-                model.optimizer, loss_scale_optimizer.LossScaleOptimizer\n+                model.optimizer, loss_scale_optimizer.BaseLossScaleOptimizer\n             )\n             self.assertEqual(\n                 backend.get_value(model.optimizer.learning_rate), 1.0\n@@ -557,33 +599,40 @@ class KerasModelTest(test_combinations.TestCase):\n             model = models.Model(x, y)\n             model.compile(\"sgd\", \"mse\")\n             self.assertIsInstance(\n-                model.optimizer,\n-                (\n-                    loss_scale_optimizer.LossScaleOptimizer,\n-                    loss_scale_optimizer.LossScaleOptimizerV3,\n-                ),\n+                model.optimizer, loss_scale_optimizer.BaseLossScaleOptimizer\n             )\n \n             # Test if an LSO is passed, optimizer is not automatically wrapped\n             # with another LSO\n             model = models.Model(x, y)\n-            optimizer = loss_scale_optimizer.LossScaleOptimizer(\n-                gradient_descent.SGD(1.0), dynamic_growth_steps=2\n+            if use_legacy_optimizer:\n+                optimizer = gradient_descent.SGD(1.0)\n+            else:\n+                optimizer = sgd.SGD(1.0)\n+            optimizer = loss_scale_optimizer.BaseLossScaleOptimizer(\n+                optimizer, dynamic_growth_steps=2\n             )\n             model.compile(optimizer, \"mse\")\n             self.assertIsInstance(\n-                model.optimizer, loss_scale_optimizer.LossScaleOptimizer\n+                model.optimizer, loss_scale_optimizer.BaseLossScaleOptimizer\n             )\n             self.assertEqual(model.optimizer.dynamic_growth_steps, 2)\n \n         with policy.policy_scope(\"mixed_bfloat16\"):\n             # Test mixed_bfloat16 models are not automatically wrapped with LSO\n             model = models.Model(x, y)\n-            model.compile(gradient_descent.SGD(1.0), \"mse\")\n+            if use_legacy_optimizer:\n+                optimizer = gradient_descent.SGD(1.0)\n+            else:\n+                optimizer = sgd.SGD(1.0)\n+            model.compile(optimizer, \"mse\")\n             self.assertNotIsInstance(\n-                model.optimizer, loss_scale_optimizer.LossScaleOptimizer\n+                model.optimizer, loss_scale_optimizer.BaseLossScaleOptimizer\n+            )\n+            self.assertIsInstance(\n+                model.optimizer,\n+                gradient_descent.SGD if use_legacy_optimizer else sgd.SGD,\n             )\n-            self.assertIsInstance(model.optimizer, gradient_descent.SGD)\n \n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n@@ -664,6 +713,11 @@ class KerasModelTest(test_combinations.TestCase):\n             \"testcase_name\": \"distribute\",\n             \"strategy_fn\": create_mirrored_strategy,\n         },\n+        {\n+            \"testcase_name\": \"distribute_legacy\",\n+            \"strategy_fn\": create_mirrored_strategy,\n+            \"use_legacy_optimizer\": True,\n+        },\n         {\n             \"testcase_name\": \"different_var_name\",\n             \"strategy_fn\": default_strategy_fn,\n@@ -676,8 +730,11 @@ class KerasModelTest(test_combinations.TestCase):\n         },\n     )\n     def test_save_slot_variables_with_autocast_vars(\n-        self, strategy_fn, var_name=\"v\"\n+        self, strategy_fn, var_name=\"v\", use_legacy_optimizer=False\n     ):\n+        if not tf.__internal__.tf2.enabled():\n+            # The non-legacy optimizer is only supported in TF2\n+            use_legacy_optimizer = True\n         p = policy.Policy(\"mixed_float16\")\n         with strategy_fn().scope(), policy.policy_scope(p):\n             x = layers.Input(shape=(2,), batch_size=2)\n@@ -691,8 +748,11 @@ class KerasModelTest(test_combinations.TestCase):\n             )\n             y = layer(x)\n             model = models.Model(inputs=x, outputs=y)\n+            if use_legacy_optimizer:\n                 opt = gradient_descent.SGD(1.0, 1.0)\n-            opt = loss_scale_optimizer.LossScaleOptimizer(\n+            else:\n+                opt = sgd.SGD(1.0, 1.0)\n+            opt = loss_scale_optimizer.BaseLossScaleOptimizer(\n                 opt, dynamic=False, initial_scale=1\n             )\n             model.compile(\n@@ -701,17 +761,23 @@ class KerasModelTest(test_combinations.TestCase):\n                 run_eagerly=test_utils.should_run_eagerly(),\n             )\n \n+        def get_momentum_slot():\n+            if use_legacy_optimizer:\n+                return opt.get_slot(layer.v, \"momentum\")\n+            else:\n+                return opt.inner_optimizer.momentums[0]\n+\n         model.fit(np.ones((2, 2)), np.zeros((2, 2)), batch_size=2)\n         weights_file = os.path.join(self.get_temp_dir(), \"weights\")\n         model.save_weights(weights_file)\n-        saved_slot = backend.get_value(opt.get_slot(layer.v, \"momentum\"))\n+        saved_slot = backend.get_value(get_momentum_slot())\n \n         model.fit(np.ones((2, 2)), np.zeros((2, 2)), batch_size=2)\n-        new_slot = backend.get_value(opt.get_slot(layer.v, \"momentum\"))\n+        new_slot = backend.get_value(get_momentum_slot())\n         self.assertNotEqual(new_slot, saved_slot)\n \n         model.load_weights(weights_file)\n-        restored_slot = backend.get_value(opt.get_slot(layer.v, \"momentum\"))\n+        restored_slot = backend.get_value(get_momentum_slot())\n         self.assertEqual(restored_slot, saved_slot)\n \n     @test_combinations.run_all_keras_modes\n@@ -725,14 +791,20 @@ class KerasModelTest(test_combinations.TestCase):\n             # TODO(b/121381184): Enable running the test in this case.\n             return\n \n+        # The non-legacy optimizer is only supported in TF2\n+        use_legacy_optimizer = not tf.__internal__.tf2.enabled()\n+\n         # Create and run model.\n         with strategy.scope():\n             x = layers.Input(shape=(2,), batch_size=2, dtype=tf.float32)\n             y = mp_test_util.MultiplyLayer(assert_type=tf.float32)(x)\n             model = models.Model(inputs=x, outputs=y)\n \n+            if use_legacy_optimizer:\n                 opt = gradient_descent.SGD(1.0)\n-            opt = loss_scale_optimizer.LossScaleOptimizer(\n+            else:\n+                opt = sgd.SGD(1.0)\n+            opt = loss_scale_optimizer.BaseLossScaleOptimizer(\n                 opt, initial_scale=1.0, dynamic_growth_steps=2.0\n             )\n             model.compile(\n@@ -763,6 +835,7 @@ class KerasModelTest(test_combinations.TestCase):\n     def test_restore_old_loss_scale_checkpoint(self):\n         # Ensure a checkpoint from TF 2.2 can be loaded. The checkpoint format\n         # of LossScaleOptimizer changed, but old checkpoints can still be loaded\n+        # into the legacy optimizers.\n         opt = gradient_descent.SGD(0.1, momentum=0.1)\n         opt = loss_scale_optimizer.LossScaleOptimizer(opt)\n         model = sequential.Sequential(\n@@ -871,6 +944,8 @@ class KerasModelTest(test_combinations.TestCase):\n             y = mp_test_util.MultiplyLayer()(x)\n             model = models.Model(inputs=x, outputs=y)\n \n+            # Only test the legacy optimizer. The new optimizer does not\n+            # support saving optimizer weights.\n             opt = gradient_descent.SGD(1.0)\n             opt = loss_scale_optimizer.LossScaleOptimizer(\n                 opt, initial_scale=1.0, dynamic_growth_steps=2.0\n\n@@ -181,6 +181,12 @@ def convert_to_legacy_optimizer(optimizer):\n     Args:\n         optimizer: An instance of `tf.keras.optimizers.experimental.Optimizer`.\n     \"\"\"\n+    # loss_scale_optimizer has a direct dependency of optimizer, import here\n+    # rather than top to avoid the cyclic dependency.\n+    from keras.mixed_precision import (\n+        loss_scale_optimizer,\n+    )\n+\n     if not isinstance(optimizer, base_optimizer.Optimizer):\n         raise ValueError(\n             \"`convert_to_legacy_optimizer` should only be called \"\n@@ -200,9 +206,18 @@ def convert_to_legacy_optimizer(optimizer):\n     ]\n     for key in keys_to_remove:\n         config.pop(key, None)\n+\n+    if isinstance(optimizer, loss_scale_optimizer.LossScaleOptimizerV3):\n+        # For LossScaleOptimizers, recursively convert the inner optimizer\n+        config[\"inner_optimizer\"] = convert_to_legacy_optimizer(\n+            optimizer.inner_optimizer\n+        )\n+        if optimizer_name == \"lossscaleoptimizerv3\":\n+            optimizer_name = \"lossscaleoptimizer\"\n+\n     # Learning rate can be a custom LearningRateSchedule, which is stored as\n     # a dict in config, and cannot be deserialized.\n-    if isinstance(\n+    if hasattr(optimizer, \"_learning_rate\") and isinstance(\n         optimizer._learning_rate, learning_rate_schedule.LearningRateSchedule\n     ):\n         config[\"learning_rate\"] = optimizer._learning_rate\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ebc63f326ead418de329da379e9028a317f80841", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 6129 | Contributors (this commit): 32 | Commits (past 90d): 15 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,6 +18,7 @@\n \n import abc\n import functools\n+import warnings\n \n import tensorflow.compat.v2 as tf\n \n@@ -1959,6 +1960,14 @@ def categorical_crossentropy(\n     y_true = tf.cast(y_true, y_pred.dtype)\n     label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n \n+    if y_pred.shape[-1] == 1:\n+        warnings.warn(\n+            \"Recieved an one-dimensional output. \"\n+            \"Consider using binary crossentropy \"\n+            \"instead of categorical crossentropy \"\n+            \"if you have only 2 labels\"\n+        )\n+\n     def _smooth_labels():\n         num_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n         return y_true * (1.0 - label_smoothing) + (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16c70d32dc4632a3c65f445c69142d58db736d4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 14 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,11 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @register_keras_serializable()\n-@keras_export(\"keras.optimizers.experimental.Adafactor\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.Adafactor\",\n+    \"keras.optimizers.experimental.Adafactor\",\n+    v1=[],\n+)\n class Adafactor(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Adafactor algorithm.\n \n\n@@ -25,7 +25,9 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @register_keras_serializable()\n-@keras_export(\"keras.optimizers.experimental.AdamW\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.AdamW\", \"keras.optimizers.experimental.AdamW\", v1=[]\n+)\n class AdamW(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the AdamW algorithm.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b30853e39e8857f8deb61f7fcc28428ff053753", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 231 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 5 | DMM Complexity: 0.9166666666666666\n\nDIFF:\n@@ -369,7 +369,8 @@ def deserialize_keras_object(\n       The object described by the `config` dictionary.\n \n     \"\"\"\n-    safe_mode = in_safe_mode() or safe_mode\n+    safe_scope_arg = in_safe_mode()  # Enforces SafeModeScope\n+    safe_mode = safe_scope_arg if safe_scope_arg is not None else safe_mode\n \n     module_objects = kwargs.pop(\"module_objects\", None)\n     custom_objects = custom_objects or {}\n\n@@ -186,6 +186,18 @@ class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n         y2 = new_lmbda(x)\n         self.assertAllClose(y1, y2, atol=1e-5)\n \n+    def test_safe_mode_scope(self):\n+        lmbda = keras.layers.Lambda(lambda x: x**2)\n+        with serialization_lib.SafeModeScope(safe_mode=True):\n+            with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n+                self.roundtrip(lmbda)\n+        with serialization_lib.SafeModeScope(safe_mode=False):\n+            _, new_lmbda, _ = self.roundtrip(lmbda)\n+        x = tf.random.normal((2, 2))\n+        y1 = lmbda(x)\n+        y2 = new_lmbda(x)\n+        self.assertAllClose(y1, y2, atol=1e-5)\n+\n     def test_tensorspec(self):\n         inputs = keras.Input(type_spec=tf.TensorSpec((2, 2), tf.float32))\n         outputs = keras.layers.Dense(1)(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c981191da1d1053ca7b09ce8fde2807d603c9430", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 186 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -158,4 +158,5 @@ def get_custom_objects():\n         \"CustomLRSchedule\": CustomLRSchedule,\n         \"CustomModel\": CustomModel,\n         \"BinaryTruePositives\": BinaryTruePositives,\n+        \"custom_loss\": custom_loss,\n     }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb297a63ee109004e42e579062e86af5dd0ce186", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 945 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -370,13 +370,6 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n         # on models with normal variables, and vice versa.\n         return self._variable._gather_saveables_for_checkpoint()\n \n-    def _map_resources(self, save_options):\n-        # By delegating this method to the wrapped variable, SavedModel with\n-        # AutoCastVariables are identical to SavedModel with normal variables.\n-        obj_map, resource_map = self._variable._map_resources(save_options)\n-        obj_map[self] = obj_map[self._variable]\n-        return obj_map, resource_map\n-\n     def _export_to_saved_model_graph(\n         self, object_map, tensor_map, options, **kwargs\n     ):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#168c70b4cbd952329abcb49222fc409f41093688", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 319 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -53,9 +53,10 @@ class DepthwiseConv2D(DepthwiseConv):\n         specify the same value for all spatial dimensions.\n       strides: An integer or tuple/list of 2 integers, specifying the strides of\n         the convolution along the height and width. Can be a single integer to\n-        specify the same value for all spatial dimensions. Specifying any stride\n-        value != 1 is incompatible with specifying any `dilation_rate` value !=\n-        1.\n+        specify the same value for all spatial dimensions. Current implementation\n+        only supports equal length strides in row and column dimensions.\n+        Specifying any stride value != 1 is incompatible with specifying any \n+        `dilation_rate` value !=1.\n       padding: one of `'valid'` or `'same'` (case-insensitive). `\"valid\"` means\n         no padding. `\"same\"` results in padding with zeros evenly to the\n         left/right or up/down of the input such that output has the same\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2fa6f100123575d9427123b9e9ad84c92f8881b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 327 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -53,10 +53,10 @@ class DepthwiseConv2D(DepthwiseConv):\n         specify the same value for all spatial dimensions.\n       strides: An integer or tuple/list of 2 integers, specifying the strides of\n         the convolution along the height and width. Can be a single integer to\n-        specify the same value for all spatial dimensions. Current implementation\n-        only supports equal length strides in row and column dimensions.\n-        Specifying any stride value != 1 is incompatible with specifying any \n-        `dilation_rate` value !=1.\n+        specify the same value for all spatial dimensions. Current\n+        implementation only supports equal length strides in row and \n+        column dimensions. Specifying any stride value != 1 is incompatible\n+        with specifying any `dilation_rate` value !=1.\n       padding: one of `'valid'` or `'same'` (case-insensitive). `\"valid\"` means\n         no padding. `\"same\"` results in padding with zeros evenly to the\n         left/right or up/down of the input such that output has the same\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf12d744075791538392b0f60833da7d533994ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 1816 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -493,6 +493,7 @@ def index_directory(\n     shuffle=True,\n     seed=None,\n     follow_links=False,\n+    verbose=1,\n ):\n     \"\"\"Make list of all files in the subdirs of `directory`, with their labels.\n \n@@ -514,6 +515,8 @@ def index_directory(\n           If set to False, sorts the data in alphanumeric order.\n       seed: Optional random seed for shuffling.\n       follow_links: Whether to visits subdirectories pointed to by symlinks.\n+      verbose: 0 or 1. Verbosity mode.\n+          0 = silent, 1 = print how many files and classes were found.\n \n     Returns:\n       tuple (file_paths, labels, class_names).\n@@ -577,6 +580,7 @@ def index_directory(\n             labels[i : i + len(partial_labels)] = partial_labels\n             i += len(partial_labels)\n \n+    if verbose:\n         if labels is None:\n             print(f\"Found {len(filenames)} files.\")\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#459a4abd6b1898eb14b6d129578c6dca0d290543", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 221 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -40,6 +40,7 @@ def text_dataset_from_directory(\n     validation_split=None,\n     subset=None,\n     follow_links=False,\n+    verbose=1,\n ):\n     \"\"\"Generates a `tf.data.Dataset` from text files in a directory.\n \n@@ -105,6 +106,8 @@ def text_dataset_from_directory(\n           (the training and validation datasets respectively).\n       follow_links: Whether to visits subdirectories pointed to by symlinks.\n           Defaults to False.\n+      verbose: 0 or 1. Verbosity mode.\n+          0 = silent, 1 = print how many files and classes were found.\n \n     Returns:\n       A `tf.data.Dataset` object.\n@@ -163,6 +166,7 @@ def text_dataset_from_directory(\n         shuffle=shuffle,\n         seed=seed,\n         follow_links=follow_links,\n+        verbose=verbose,\n     )\n \n     if label_mode == \"binary\" and len(class_names) != 2:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6688c950a1d82cd499c65152bc2a191fd8c0128", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 287 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -47,6 +47,7 @@ def image_dataset_from_directory(\n     interpolation=\"bilinear\",\n     follow_links=False,\n     crop_to_aspect_ratio=False,\n+    verbose=1,\n     **kwargs,\n ):\n     \"\"\"Generates a `tf.data.Dataset` from image files in a directory.\n@@ -128,6 +129,8 @@ def image_dataset_from_directory(\n         largest possible window in the image (of size `image_size`) that matches\n         the target aspect ratio. By default (`crop_to_aspect_ratio=False`),\n         aspect ratio may not be preserved.\n+      verbose: 0 or 1. Verbosity mode.\n+          0 = silent, 1 = print how many files and classes were found.\n       **kwargs: Legacy keyword arguments.\n \n     Returns:\n@@ -215,6 +218,7 @@ def image_dataset_from_directory(\n         shuffle=shuffle,\n         seed=seed,\n         follow_links=follow_links,\n+        verbose=verbose,\n     )\n \n     if label_mode == \"binary\" and len(class_names) != 2:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cee460a3720ee2a277c290332d1ffbdcef77da92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 768 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -46,6 +46,7 @@ def audio_dataset_from_directory(\n     validation_split=None,\n     subset=None,\n     follow_links=False,\n+    verbose=1,\n ):\n     \"\"\"Generates a `tf.data.Dataset` from audio files in a directory.\n \n@@ -107,6 +108,8 @@ def audio_dataset_from_directory(\n         \"both\". Only used if `validation_split` is set.\n       follow_links: Whether to visits subdirectories pointed to by symlinks.\n         Defaults to False.\n+      verbose: 0 or 1. Verbosity mode.\n+          0 = silent, 1 = print how many files and classes were found.\n \n     Returns:\n       A `tf.data.Dataset` object.\n@@ -194,6 +197,7 @@ def audio_dataset_from_directory(\n         shuffle=shuffle,\n         seed=seed,\n         follow_links=follow_links,\n+        verbose=verbose,\n     )\n \n     if label_mode == \"binary\" and len(class_names) != 2:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2fc680dac8a4eb108077ed396bbdc98ebacbfbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1481 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -929,8 +929,10 @@ def no_ragged_support(inputs, layer_name):\n \n \n def is_split_variable(v):\n-    \"\"\"Returns True if `v` is a PartionedVariable or a ShardedVariable.\"\"\"\n-    return hasattr(v, \"_variable_list\") or hasattr(v, \"_variables\")\n+    \"\"\"Returns True if `v` is a PartitionedVariable or a ShardedVariable.\"\"\"\n+    return not {clz.__name__ for clz in v.__class__.__mro__}.isdisjoint(\n+        {\"PartitionedVariable\", \"ShardedVariable\"}\n+    )\n \n \n def has_weights(obj):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4511bb9ef456babc6f73bed3d07422c779ad17c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 846 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -326,7 +326,6 @@ class RNNCell(base_layer.Layer):\n             self._last_zero_state = (state_size, batch_size, dtype, output)\n         return output\n \n-    # TODO(b/134773139): Remove when contrib RNN cells implement `get_config`\n     def get_config(self):\n         return super().get_config()\n \n@@ -386,8 +385,7 @@ class LayerRNNCell(RNNCell):\n class BasicRNNCell(LayerRNNCell):\n     \"\"\"The most basic RNN cell.\n \n-    Note that this cell is not optimized for performance. Please use\n-    `tf.contrib.cudnn_rnn.CudnnRNNTanh` for better performance on GPU.\n+    Note that this cell is not optimized for performance.\n \n     Args:\n       num_units: int, The number of units in the RNN cell.\n@@ -424,9 +422,7 @@ class BasicRNNCell(LayerRNNCell):\n         _check_supported_dtypes(self.dtype)\n         if tf.executing_eagerly() and tf.config.list_logical_devices(\"GPU\"):\n             logging.warning(\n-                \"%s: Note that this cell is not optimized for performance. \"\n-                \"Please use tf.contrib.cudnn_rnn.CudnnRNNTanh for better \"\n-                \"performance on GPU.\",\n+                \"%s: Note that this cell is not optimized for performance.\",\n                 self,\n             )\n \n@@ -494,8 +490,8 @@ class GRUCell(LayerRNNCell):\n     \"\"\"Gated Recurrent Unit cell.\n \n     Note that this cell is not optimized for performance. Please use\n-    `tf.contrib.cudnn_rnn.CudnnGRU` for better performance on GPU, or\n-    `tf.contrib.rnn.GRUBlockCellV2` for better performance on CPU.\n+    `tf.compat.v1.keras.layers.CuDNNGRU` for better performance on GPU, or\n+    `tf.raw_ops.GRUBlockCell` for better performance on CPU.\n \n     Args:\n       num_units: int, The number of units in the GRU cell.\n@@ -542,7 +538,7 @@ class GRUCell(LayerRNNCell):\n         if tf.executing_eagerly() and tf.config.list_logical_devices(\"GPU\"):\n             logging.warning(\n                 \"%s: Note that this cell is not optimized for performance. \"\n-                \"Please use tf.contrib.cudnn_rnn.CudnnGRU for better \"\n+                \"Please use tf.compat.v1.keras.layers.CuDNNGRU for better \"\n                 \"performance on GPU.\",\n                 self,\n             )\n@@ -688,9 +684,8 @@ class BasicLSTMCell(LayerRNNCell):\n     that follows.\n \n     Note that this cell is not optimized for performance. Please use\n-    `tf.contrib.cudnn_rnn.CudnnLSTM` for better performance on GPU, or\n-    `tf.contrib.rnn.LSTMBlockCell` and `tf.contrib.rnn.LSTMBlockFusedCell` for\n-    better performance on CPU.\n+    `tf.compat.v1.keras.layers.CuDNNLSTM` for better performance on GPU, or\n+    `tf.raw_ops.LSTMBlockCell` for better performance on CPU.\n     \"\"\"\n \n     def __init__(\n@@ -749,7 +744,7 @@ class BasicLSTMCell(LayerRNNCell):\n         if tf.executing_eagerly() and tf.config.list_logical_devices(\"GPU\"):\n             logging.warning(\n                 \"%s: Note that this cell is not optimized for performance. \"\n-                \"Please use tf.contrib.cudnn_rnn.CudnnLSTM for better \"\n+                \"Please use tf.compat.v1.keras.layers.CuDNNLSTM for better \"\n                 \"performance on GPU.\",\n                 self,\n             )\n@@ -870,9 +865,8 @@ class LSTMCell(LayerRNNCell):\n     an optional projection layer.\n \n     Note that this cell is not optimized for performance. Please use\n-    `tf.contrib.cudnn_rnn.CudnnLSTM` for better performance on GPU, or\n-    `tf.contrib.rnn.LSTMBlockCell` and `tf.contrib.rnn.LSTMBlockFusedCell` for\n-    better performance on CPU.\n+    `tf.compat.v1.keras.layers.CuDNNLSTM` for better performance on GPU, or\n+    `tf.raw_ops.LSTMBlockCell` for better performance on CPU.\n     References:\n       Long short-term memory recurrent neural network architectures for large\n       scale acoustic modeling:\n@@ -975,7 +969,7 @@ class LSTMCell(LayerRNNCell):\n         if tf.executing_eagerly() and tf.config.list_logical_devices(\"GPU\"):\n             logging.warning(\n                 \"%s: Note that this cell is not optimized for performance. \"\n-                \"Please use tf.contrib.cudnn_rnn.CudnnLSTM for better \"\n+                \"Please use tf.compat.v1.keras.layers.CuDNNLSTM for better \"\n                 \"performance on GPU.\",\n                 self,\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#662584202be250a2f2507ac7207fb5755b4e29fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 18 | Churn Cumulative: 16015 | Contributors (this commit): 121 | Commits (past 90d): 21 | Contributors (cumulative): 139 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1511,6 +1511,11 @@ class ModelCheckpoint(Callback):\n             self.epochs_since_last_save = 0\n             filepath = self._get_file_path(epoch, batch, logs)\n \n+            # Create host directory if it doesn't exist.\n+            dirname = os.path.dirname(filepath)\n+            if dirname and not tf.io.gfile.exists(dirname):\n+                tf.io.gfile.makedirs(dirname)\n+\n             try:\n                 if self.save_best_only:\n                     current = logs.get(self.monitor)\n@@ -1791,12 +1796,13 @@ class BackupAndRestore(Callback):\n \n     Args:\n         backup_dir: String, path to store the checkpoint.\n-          e.g. backup_dir = os.path.join(working_dir, 'backup')\n+          e.g. `backup_dir = os.path.join(working_dir, 'backup')`.\n           This is the directory in which the system stores temporary files to\n           recover the model from jobs terminated unexpectedly. The directory\n-          cannot be reused elsewhere to store other files, e.g. by\n-          BackupAndRestore callback of another training, or by another callback\n-          (ModelCheckpoint) of the same training.\n+          cannot be reused elsewhere to store other files, e.g. by the\n+          `BackupAndRestore` callback of another training run,\n+          or by another callback\n+          (e.g. `ModelCheckpoint`) of the same training.\n         save_freq: `'epoch'`, integer, or `False`. When set to `'epoch'`\n           the callback saves the checkpoint at the end of each epoch.\n           When set to an integer, the callback saves the checkpoint every\n\n@@ -913,7 +913,9 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         temp_dir = self.get_temp_dir()\n         self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n \n-        filepath = os.path.join(temp_dir, \"checkpoint.h5\")\n+        # Save model to a subdir inside the temp_dir so we can test\n+        # automatic directory creation.\n+        filepath = os.path.join(temp_dir, \"subdir\", \"checkpoint.h5\")\n         (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n             train_samples=TRAIN_SAMPLES,\n             test_samples=TEST_SAMPLES,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e52c89c7d1bd52d1f0db0da86a72322ba72c1dc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 29 | Churn Cumulative: 9047 | Contributors (this commit): 21 | Commits (past 90d): 13 | Contributors (cumulative): 51 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,7 +23,9 @@ Reference:\n \n import tensorflow.compat.v2 as tf\n \n+import keras\n from keras import backend\n+from keras import layers as keras_layers\n from keras.applications import imagenet_utils\n from keras.engine import training\n from keras.layers import VersionAwareLayers\n@@ -319,6 +321,12 @@ def conv2d_bn(\n     return x\n \n \n+@keras.utils.register_keras_serializable()\n+class CustomScaleLayer(keras_layers.Layer):\n+    def call(self, x, up, scale):\n+        return x + up * scale\n+\n+\n def inception_resnet_block(x, scale, block_type, block_idx, activation=\"relu\"):\n     \"\"\"Adds an Inception-ResNet block.\n \n@@ -395,12 +403,7 @@ def inception_resnet_block(x, scale, block_type, block_idx, activation=\"relu\"):\n         name=block_name + \"_conv\",\n     )\n \n-    x = layers.Lambda(\n-        lambda inputs, scale: inputs[0] + inputs[1] * scale,\n-        output_shape=backend.int_shape(x)[1:],\n-        arguments={\"scale\": scale},\n-        name=block_name,\n-    )([x, up])\n+    x = CustomScaleLayer()(x, up, scale)\n     if activation is not None:\n         x = layers.Activation(activation, name=block_name + \"_ac\")(x)\n     return x\n\n@@ -897,13 +897,10 @@ class NetworkConstructionTest(test_combinations.TestCase):\n         # See https://github.com/keras-team/keras/issues/14838.\n         inp = input_layer_lib.Input(shape=[5], name=\"main_input\")\n \n-        zeros = layers.Lambda(tf.zeros_like, name=\"generate_zeros\")(inp)\n-        ones = layers.Lambda(tf.ones_like, name=\"generate_ones\")(inp)\n-\n         shared_layer = layers.Layer(name=\"shared\")\n \n-        ones_result = shared_layer(ones)\n-        zeros_result = shared_layer(zeros)\n+        ones_result = shared_layer(tf.ones_like(inp))\n+        zeros_result = shared_layer(tf.zeros_like(inp))\n         zeros_result = layers.Layer(name=\"blank\")(zeros_result)\n \n         m = training_lib.Model(\n\n@@ -21,6 +21,7 @@ from keras.engine import functional\n from keras.engine import input_layer as input_layer_lib\n from keras.layers import core\n from keras.saving.legacy import model_config\n+from keras.saving.serialization_lib import SafeModeScope\n from keras.testing_infra import test_combinations\n \n # isort: off\n@@ -406,6 +407,7 @@ class InputLayerTest(test_combinations.TestCase):\n             self.assertAllEqual(model(two_tensors), lambda_fn(two_tensors))\n \n             # Test serialization / deserialization\n+            with SafeModeScope(safe_mode=False):\n                 model = functional.Functional.from_config(model.get_config())\n                 self.assertAllEqual(model(two_tensors), lambda_fn(two_tensors))\n                 model = model_config.model_from_json(model.to_json())\n\n@@ -24,6 +24,7 @@ import keras\n from keras import initializers\n from keras.layers import core\n from keras.mixed_precision import policy\n+from keras.saving.serialization_lib import SafeModeScope\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n@@ -156,6 +157,7 @@ class LambdaLayerTest(test_combinations.TestCase):\n \n         ld = keras.layers.Lambda(f)\n         config = ld.get_config()\n+        with SafeModeScope(safe_mode=False):\n             ld = keras.layers.deserialize(\n                 {\"class_name\": \"Lambda\", \"config\": config}\n             )\n@@ -248,6 +250,7 @@ class LambdaLayerTest(test_combinations.TestCase):\n         layer(keras.backend.variable(np.ones((1, 1))))\n         config = layer.get_config()\n \n+        with SafeModeScope(safe_mode=False):\n             layer = keras.layers.deserialize(\n                 {\"class_name\": \"Lambda\", \"config\": config}\n             )\n\n@@ -45,6 +45,7 @@ from keras.optimizers import sgd\n from keras.optimizers.legacy import gradient_descent\n from keras.saving import object_registration\n from keras.saving.legacy import save\n+from keras.saving.serialization_lib import SafeModeScope\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n@@ -511,6 +512,7 @@ class KerasModelTest(test_combinations.TestCase):\n                 model = models.Model(inputs=x, outputs=y)\n                 if get_config:\n                     config = model.get_config()\n+                    with SafeModeScope(safe_mode=False):\n                         model = model.__class__.from_config(\n                             config,\n                             custom_objects={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
