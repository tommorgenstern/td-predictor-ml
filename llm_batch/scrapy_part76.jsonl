{"custom_id": "scrapy#49e11d34c2a395a062906df56ff0afe6222f03fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 28 | Churn Cumulative: 61 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import unittest\n from scrapy.http import Request\n-from scrapy.utils.request import request_fingerprint, request_authenticate, request_httprepr\n+from scrapy.utils.request import request_fingerprint, _fingerprint_cache, \\\n+    request_authenticate, request_httprepr\n \n class UtilsRequestTest(unittest.TestCase):\n \n@@ -15,7 +16,7 @@ class UtilsRequestTest(unittest.TestCase):\n         self.assertNotEqual(request_fingerprint(r1), request_fingerprint(r2))\n \n         # make sure caching is working\n-        self.assertEqual(request_fingerprint(r1), r1.cache['fingerprint'])\n+        self.assertEqual(request_fingerprint(r1), _fingerprint_cache[r1][None])\n \n         r1 = Request(\"http://www.example.com/members/offers.html\")\n         r2 = Request(\"http://www.example.com/members/offers.html\")\n\n@@ -4,12 +4,15 @@ scrapy.http.Request objects\n \"\"\"\n \n import hashlib\n+import weakref\n from base64 import urlsafe_b64encode\n \n from scrapy.utils.url import canonicalize_url\n from scrapy.utils.httpobj import urlparse_cached\n \n-def request_fingerprint(request, include_headers=()):\n+\n+_fingerprint_cache = weakref.WeakKeyDictionary()\n+def request_fingerprint(request, include_headers=None):\n     \"\"\"\n     Return the request fingerprint.\n     \n@@ -36,28 +39,22 @@ def request_fingerprint(request, include_headers=()):\n     include_headers argument, which is a list of Request headers to include.\n \n     \"\"\"\n-\n     if include_headers:\n-        include_headers = [h.lower() for h in sorted(include_headers)]\n-        cachekey = 'fingerprint' + '_'.join(include_headers)\n-    else:\n-        cachekey = 'fingerprint'\n-\n-    try:\n-        return request.cache[cachekey]\n-    except KeyError:\n+        include_headers = tuple([h.lower() for h in sorted(include_headers)])\n+    cache = _fingerprint_cache.setdefault(request, {})\n+    if include_headers not in cache:\n         fp = hashlib.sha1()\n         fp.update(request.method)\n         fp.update(canonicalize_url(request.url))\n         fp.update(request.body or '')\n+        if include_headers:\n             for hdr in include_headers:\n                 if hdr in request.headers:\n                     fp.update(hdr)\n                     for v in request.headers.getlist(hdr):\n                         fp.update(v)\n-        fphash = fp.hexdigest()\n-        request.cache[cachekey] = fphash\n-        return fphash\n+        cache[include_headers] = fp.hexdigest()\n+    return cache[include_headers]\n \n def request_authenticate(request, username, password):\n     \"\"\"Autenticate the given request (in place) using the HTTP basic access\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81832773ee8bdef4c2196d40723fa2d88306da91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 58 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -21,26 +21,25 @@ def body_or_str(obj, unicode=True):\n         return obj if unicode else obj.encode('utf-8')\n \n BASEURL_RE = re.compile(r'<base\\s+href\\s*=\\s*[\\\"\\']\\s*([^\\\"\\'\\s]+)\\s*[\\\"\\']', re.I)\n+_baseurl_cache = weakref.WeakKeyDictionary()\n def get_base_url(response):\n     \"\"\" Return the base url of the given response used to resolve relative links. \"\"\"\n-    # re is used instead of xpath for eficiency reasons. In a quick\n-    # benchmark using timeit we got (for 50 repetitions) 0.0017 seconds\n-    # using re and 0.7452 using xpath\n-    if 'base_url' not in response.cache:\n+    if response not in _baseurl_cache:\n         match = BASEURL_RE.search(response.body[0:4096])\n-        response.cache['base_url'] = match.group(1) if match else response.url\n-    return response.cache['base_url']\n+        _baseurl_cache[response] = match.group(1) if match else response.url\n+    return _baseurl_cache[response]\n \n META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\\d+);\\s*url=([^\"\\']+)', re.DOTALL | re.IGNORECASE)\n+_metaref_cache = weakref.WeakKeyDictionary()\n def get_meta_refresh(response):\n     \"\"\" Return a tuple of two strings containing the interval and url included\n     in the http-equiv parameter of the HTML meta element. If no url is included\n     (None, None) is returned [instead of (interval, None)]\n     \"\"\"\n-    if 'meta_refresh_url' not in response.cache:\n+    if response not in _metaref_cache:\n         match = META_REFRESH_RE.search(response.body[0:4096])\n-        response.cache['meta_refresh_url'] = match.groups() if match else (None, None)\n-    return response.cache['meta_refresh_url']\n+        _metaref_cache[response] = match.groups() if match else (None, None)\n+    return _metaref_cache[response]\n \n _beautifulsoup_cache = weakref.WeakKeyDictionary()\n def get_cached_beautifulsoup(response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b71de57b210cd3277b096b7124634be627386e8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 32 | Files Changed: 6 | Hunks: 20 | Methods Changed: 16 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 63 | Churn Cumulative: 146 | Contributors (this commit): 2 | Commits (past 90d): 25 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -26,7 +26,7 @@ class FormRequest(Request):\n \n     def __init__(self, *args, **kwargs):\n         formdata = kwargs.pop('formdata', None)\n-        Request.__init__(self, *args, **kwargs)\n+        super(FormRequest, self).__init__(*args, **kwargs)\n \n         if formdata:\n             items = formdata.iteritems() if isinstance(formdata, dict) else formdata\n\n@@ -12,6 +12,8 @@ from scrapy.http.request import Request\n \n class XmlRpcRequest(Request):\n \n+    __slots__ = ()\n+\n     def __init__(self, *args, **kwargs):\n         if 'body' not in kwargs:\n             params = kwargs.pop('params')\n@@ -24,5 +26,5 @@ class XmlRpcRequest(Request):\n         # xmlrpc query multiples times over the same url\n         kwargs.setdefault('dont_filter', True)\n \n-        Request.__init__(self, *args, **kwargs)\n+        super(XmlRpcRequest, self).__init__(*args, **kwargs)\n         self.headers.setdefault('Content-Type', 'text/xml')\n\n@@ -44,17 +44,20 @@ class Response(object_ref):\n         if isinstance(body, str):\n             self._body = body\n         elif isinstance(body, unicode):\n-            raise TypeError(\"Cannot assign a unicode body to a raw Response. Use TextResponse, HtmlResponse, etc\")\n+            raise TypeError(\"Cannot assign a unicode body to a raw Response. \" \\\n+                \"Use TextResponse, HtmlResponse, etc\")\n         elif body is None:\n             self._body = ''\n         else:\n-            raise TypeError(\"Response body must either str or unicode. Got: '%s'\" % type(body).__name__)\n+            raise TypeError(\"Response body must either str or unicode. Got: '%s'\" \\\n+                % type(body).__name__)\n \n     body = property(_get_body, _set_body)\n \n     def __repr__(self):\n         return \"%s(url=%s, headers=%s, status=%s, body=%s)\" % \\\n-                (type(self).__name__, repr(self.url), repr(self.headers), repr(self.status), repr(self.body))\n+            (type(self).__name__, repr(self.url), repr(self.headers), \\\n+            repr(self.status), repr(self.body))\n \n     def __str__(self):\n         flags = \"(%s) \" % \",\".join(self.flags) if self.flags else \"\"\n@@ -65,7 +68,8 @@ class Response(object_ref):\n         \"\"\"Return a copy of this Response\"\"\"\n         return self.replace()\n \n-    def replace(self, url=None, status=None, headers=None, body=None, meta=None, flags=None, cls=None, **kwargs):\n+    def replace(self, url=None, status=None, headers=None, body=None, meta=None, \\\n+            flags=None, cls=None, **kwargs):\n         \"\"\"Create a new Response with the same attributes except for those\n         given new values.\n         \"\"\"\n\n@@ -24,7 +24,7 @@ class HtmlResponse(TextResponse):\n     METATAG_RE2 = re.compile(r'<meta\\s+%s\\s+%s' % (_content_re, _httpequiv_re), re.I)\n \n     def body_encoding(self):\n-        return self._body_declared_encoding() or self._body_inferred_encoding()\n+        return self._body_declared_encoding() or super(HtmlResponse, self).body_encoding()\n \n     @memoizemethod('cache')\n     def _body_declared_encoding(self):\n\n@@ -16,27 +16,22 @@ class TextResponse(Response):\n \n     _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n \n-    __slots__ = ['_encoding']\n+    __slots__ = ['_encoding', '_body_inferred_encoding']\n \n-    def __init__(self, url, status=200, headers=None, body=None, meta=None, flags=None, encoding=None):\n+    def __init__(self, url, status=200, headers=None, body=None, meta=None, \\\n+            flags=None, encoding=None):\n         self._encoding = encoding\n-        if isinstance(body, unicode):\n-            if encoding is None:\n-                clsname = self.__class__.__name__\n-                raise TypeError(\"To instantiate a %s with unicode body you must specify the encoding\" % clsname)\n-            body = body.encode(encoding)\n-        Response.__init__(self, url, status, headers, body, meta, flags)\n+        self._body_inferred_encoding = None\n+        super(TextResponse, self).__init__(url, status, headers, body, meta, flags)\n \n-    def set_body(self, body):\n-        if isinstance(body, str):\n-            self._body = body\n-        elif isinstance(body, unicode):\n+    def _set_body(self, body):\n+        if isinstance(body, unicode):\n+            if self._encoding is None:\n+                raise TypeError(\"To instantiate a %s with unicode body you \" \\\n+                    \"must specify the encoding\" % self.__class__.__name__)\n             self._body = body.encode(self._encoding)\n-        elif body is None:\n-            self._body = None\n         else:\n-            raise TypeError(\"Request body must either str, unicode or None. Got: '%s'\" % type(body).__name__)\n-    body = property(lambda x: x._body, set_body)\n+            super(TextResponse, self)._set_body(body)\n \n     def replace(self, *args, **kwargs):\n         kwargs.setdefault('encoding', getattr(self, '_encoding', None))\n@@ -59,19 +54,17 @@ class TextResponse(Response):\n     @memoizemethod('cache')\n     def body_as_unicode(self):\n         \"\"\"Return body as unicode\"\"\"\n-        possible_encodings = (self._encoding, self.headers_encoding(), self._body_declared_encoding())\n+        possible_encodings = (self._encoding, self.headers_encoding(), \\\n+            self._body_declared_encoding())\n         dammit = UnicodeDammit(self.body, possible_encodings)\n-        self.cache['body_inferred_encoding'] = dammit.originalEncoding\n-        # XXX: sometimes dammit.unicode fails, even when it recognizes the encoding correctly\n+        self._body_inferred_encoding = dammit.originalEncoding\n         return dammit.unicode\n \n     def body_encoding(self):\n-        return self._body_inferred_encoding()\n-\n-    def _body_inferred_encoding(self):\n-        if 'body_inferred_encoding' not in self.cache:\n+        if self._body_inferred_encoding is None:\n             self.body_as_unicode()\n-        return self.cache['body_inferred_encoding']\n+        return self._body_inferred_encoding\n \n     def _body_declared_encoding(self):\n+        # implemented in subclasses (XmlResponse, HtmlResponse)\n         return None\n\n@@ -19,7 +19,7 @@ class XmlResponse(TextResponse):\n     XMLDECL_RE  = re.compile(r'<\\?xml\\s.*?%s' % _encoding_re, re.I)\n \n     def body_encoding(self):\n-        return self._body_declared_encoding() or self._body_inferred_encoding()\n+        return self._body_declared_encoding() or super(XmlResponse, self).body_encoding()\n \n     @memoizemethod('cache')\n     def _body_declared_encoding(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f2925420f01d3d86c2b8855683f80502da8aacf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 80 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,9 +55,15 @@ class Response(object_ref):\n     body = property(_get_body, _set_body)\n \n     def __repr__(self):\n-        return \"%s(url=%s, headers=%s, status=%s, body=%s)\" % \\\n-            (type(self).__name__, repr(self.url), repr(self.headers), \\\n-            repr(self.status), repr(self.body))\n+        d = {\n+            'status': self.status,\n+            'url': self.url,\n+            'headers': self.headers,\n+            'body': self.body,\n+            'meta': self.meta,\n+            'flags': self.flags,\n+        }\n+        return \"%s(%s)\" % (self.__class__.__name__, repr(d))\n \n     def __str__(self):\n         flags = \"(%s) \" % \",\".join(self.flags) if self.flags else \"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#79bf4c817ae8f48071407559ab669546843dcd64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 65 | Files Changed: 5 | Hunks: 15 | Methods Changed: 13 | Complexity Δ (Sum/Max): 54/48 | Churn Δ: 98 | Churn Cumulative: 159 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,7 +8,7 @@ See documentation in docs/topics/request-response.rst\n import re\n \n from scrapy.http.response.text import TextResponse\n-from scrapy.utils.python import memoizemethod\n+from scrapy.utils.python import memoizemethod_noargs\n \n class HtmlResponse(TextResponse):\n \n@@ -26,7 +26,7 @@ class HtmlResponse(TextResponse):\n     def body_encoding(self):\n         return self._body_declared_encoding() or super(HtmlResponse, self).body_encoding()\n \n-    @memoizemethod('cache')\n+    @memoizemethod_noargs\n     def _body_declared_encoding(self):\n         chunk = self.body[:5000]\n         match = self.METATAG_RE.search(chunk) or self.METATAG_RE2.search(chunk)\n\n@@ -10,7 +10,7 @@ import re\n from scrapy.xlib.BeautifulSoup import UnicodeDammit\n \n from scrapy.http.response import Response\n-from scrapy.utils.python import memoizemethod\n+from scrapy.utils.python import memoizemethod_noargs\n \n class TextResponse(Response):\n \n@@ -41,17 +41,15 @@ class TextResponse(Response):\n     def encoding(self):\n         return self._encoding or self.headers_encoding() or self.body_encoding()\n \n-    @memoizemethod('cache')\n-    def headers_encoding(self, headers=None):\n-        if headers is None:\n-            headers = self.headers\n-        content_type = headers.get('Content-Type')\n+    @memoizemethod_noargs\n+    def headers_encoding(self):\n+        content_type = self.headers.get('Content-Type')\n         if content_type:\n             encoding = self._ENCODING_RE.search(content_type)\n             if encoding:\n                 return encoding.group(1)\n \n-    @memoizemethod('cache')\n+    @memoizemethod_noargs\n     def body_as_unicode(self):\n         \"\"\"Return body as unicode\"\"\"\n         possible_encodings = (self._encoding, self.headers_encoding(), \\\n\n@@ -8,7 +8,7 @@ See documentation in docs/topics/request-response.rst\n import re\n \n from scrapy.http.response.text import TextResponse\n-from scrapy.utils.python import memoizemethod\n+from scrapy.utils.python import memoizemethod_noargs\n \n class XmlResponse(TextResponse):\n \n@@ -21,7 +21,7 @@ class XmlResponse(TextResponse):\n     def body_encoding(self):\n         return self._body_declared_encoding() or super(XmlResponse, self).body_encoding()\n \n-    @memoizemethod('cache')\n+    @memoizemethod_noargs\n     def _body_declared_encoding(self):\n         chunk = self.body[:5000]\n         match = self.XMLDECL_RE.search(chunk)\n\n@@ -1,6 +1,7 @@\n import unittest\n \n-from scrapy.utils.python import str_to_unicode, unicode_to_str, memoizemethod, isbinarytext\n+from scrapy.utils.python import str_to_unicode, unicode_to_str, \\\n+    memoizemethod_noargs, isbinarytext\n \n class UtilsPythonTestCase(unittest.TestCase):\n     def test_str_to_unicode(self):\n@@ -29,23 +30,24 @@ class UtilsPythonTestCase(unittest.TestCase):\n         # converting a strange object should raise TypeError\n         self.assertRaises(TypeError, unicode_to_str, unittest)\n \n-    def test_memoizemethod(self):\n+    def test_memoizemethod_noargs(self):\n         class A(object):\n             def __init__(self):\n                 self.cache = {}\n \n-            @memoizemethod('cache')\n-            def heavyfunc(self, arg1=None, arg2=None):\n-                return [arg1, arg2]\n+            @memoizemethod_noargs\n+            def cached(self):\n+                return object()\n+\n+            def noncached(self):\n+                return object()\n \n         a = A()\n-        one = a.heavyfunc()\n-        two = a.heavyfunc()\n-        three = a.heavyfunc('two')\n-        four = a.heavyfunc('two')\n+        one = a.cached()\n+        two = a.cached()\n+        three = a.noncached()\n         assert one is two\n         assert one is not three\n-        assert three is four\n \n     def test_isbinarytext(self):\n \n\n@@ -9,6 +9,8 @@ import re\n import os\n import fnmatch\n import inspect\n+import weakref\n+from functools import wraps\n from shutil import copy2, copystat\n from sgmllib import SGMLParser\n \n@@ -119,51 +121,17 @@ def re_rsearch(pattern, text, chunk_size=1024):\n             return (offset + matches[-1].span()[0], offset + matches[-1].span()[1])\n     return None\n \n-def memoizemethod(cacheattr):\n-    \"\"\"A memoize decorator for methods, which caches calls to instance methods\n-    into an attribute of the same instance (which must be dict). Calls with\n-    different arguments will be cached in different buckets.\n-\n-    This has the advantage that, when the instance is collected (by the garbage\n-    collector) the cache is collected as well.\n-\n-    Descriptors are required to implement this functionality. See why at:\n-    http://blog.ianbicking.org/2008/10/24/decorators-and-descriptors/\n-\n-    Example:\n-\n-    class A(object):\n-            def __init__(self):\n-            self.cache = {}\n-            @memoizemethod('cache')\n-        def calculate(self, arg1, arg2):\n-            # expensive code here\n-\n-    All calls to calculate() method of A instances will be cached in their\n-    cache (instance) attribute, which must be a dict.\n-\n+def memoizemethod_noargs(method):\n+    \"\"\"Decorator to cache the result of a method (without arguments) using a\n+    weak reference to its object\n     \"\"\"\n-\n-    class MemoizeMethod(object):\n-\n-        def __init__(self, function):\n-            self.function = function\n-\n-        def __get__(self, obj, objtype=None):\n-            if obj is None:\n-                return self\n-            new_func = self.function.__get__(obj, objtype)\n-            return self.__class__(new_func)\n-\n-        def __call__(self, *args, **kwargs):\n-            method = self.function\n-            cache = getattr(method.im_self, cacheattr)\n-            key = (method.im_func, tuple(args), frozenset(kwargs.items()))\n-            if key not in cache:\n-                cache[key] = method(*args, **kwargs)\n-            return cache[key]\n-\n-    return MemoizeMethod\n+    cache = weakref.WeakKeyDictionary()\n+    @wraps(method)\n+    def new_method(self, *args, **kwargs):\n+        if self not in cache:\n+            cache[self] = method(self, *args, **kwargs)\n+        return cache[self]\n+    return new_method\n \n _BINARYCHARS = set(map(chr, range(32))) - set([\"\\0\", \"\\t\", \"\\n\", \"\\r\"])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31693eb90fad58c5a1b0451c31661fabd0f56817", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 30 | Files Changed: 5 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 32 | Churn Cumulative: 285 | Contributors (this commit): 2 | Commits (past 90d): 28 | Contributors (cumulative): 10 | DMM Complexity: 0.045454545454545456\n\nDIFF:\n@@ -15,7 +15,7 @@ from scrapy.utils.trackref import object_ref\n \n class Request(object_ref):\n \n-    __slots__ = ['_encoding', 'method', '_url', '_body', '_meta', '_cache', \\\n+    __slots__ = ['_encoding', 'method', '_url', '_body', '_meta', \\\n         'dont_filter', 'headers', 'cookies', 'deferred', 'priority', \\\n         '__weakref__']\n \n@@ -38,7 +38,6 @@ class Request(object_ref):\n         self.dont_filter = dont_filter\n \n         self._meta = dict(meta) if meta else None\n-        self._cache = None\n \n     @property\n     def meta(self):\n@@ -46,12 +45,6 @@ class Request(object_ref):\n             self._meta = {}\n         return self._meta\n \n-    @property\n-    def cache(self):\n-        if self._cache is None:\n-            self._cache = {}\n-        return self._cache\n-\n     def _get_url(self):\n         return self._url\n \n\n@@ -13,7 +13,7 @@ from scrapy.utils.trackref import object_ref\n class Response(object_ref):\n \n     __slots__ = ['url', 'headers', 'status', '_body', 'request', '_meta', \\\n-        'flags', '_cache', '__weakref__']\n+        'flags', '__weakref__']\n \n     def __init__(self, url, status=200, headers=None, body='', meta=None, flags=None):\n         self.url = url\n@@ -23,7 +23,6 @@ class Response(object_ref):\n         self.request = None\n         self.flags = [] if flags is None else list(flags)\n         self._meta = dict(meta) if meta else None\n-        self._cache = None\n \n     @property\n     def meta(self):\n@@ -31,12 +30,6 @@ class Response(object_ref):\n             self._meta = {}\n         return self._meta\n \n-    @property\n-    def cache(self):\n-        if self._cache is None:\n-            self._cache = {}\n-        return self._cache\n-\n     def _get_body(self):\n         return self._body\n \n\n@@ -118,12 +118,8 @@ class RequestTest(unittest.TestCase):\n \n         r1 = Request(\"http://www.example.com\", callback=somecallback)\n         r1.meta['foo'] = 'bar'\n-        r1.cache['lala'] = 'lolo'\n         r2 = r1.copy()\n \n-        assert r1.cache\n-        assert not r2.cache\n-\n         assert r1.deferred is not r2.deferred\n \n         # make sure meta dict is shallow copied\n@@ -288,12 +284,8 @@ class XmlRpcRequestTest(unittest.TestCase):\n         r1 = XmlRpcRequest(\"http://www.example.com\", callback=somecallback,\n                 methodname='login', params=('username', 'password'))\n         r1.meta['foo'] = 'bar'\n-        r1.cache['lala'] = 'lolo'\n         r2 = r1.copy()\n \n-        assert r1.cache\n-        assert not r2.cache\n-\n         assert r1.deferred is not r2.deferred\n \n         # make sure meta dict is shallow copied\n\n@@ -46,15 +46,11 @@ class ResponseTest(unittest.TestCase):\n         r1 = Response(\"http://www.example.com\", body=\"Some body\")\n         r1.meta['foo'] = 'bar'\n         r1.flags.append('cached')\n-        r1.cache['lala'] = 'lolo'\n         r2 = r1.copy()\n \n         self.assertEqual(r1.status, r2.status)\n         self.assertEqual(r1.body, r2.body)\n \n-        assert r1.cache\n-        assert not r2.cache\n-\n         # make sure meta dict is shallow copied\n         assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n         self.assertEqual(r1.meta, r2.meta)\n\n@@ -32,8 +32,6 @@ class UtilsPythonTestCase(unittest.TestCase):\n \n     def test_memoizemethod_noargs(self):\n         class A(object):\n-            def __init__(self):\n-                self.cache = {}\n \n             @memoizemethod_noargs\n             def cached(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#74f706b3562a714b59e5c55bb53d86fd080ce88c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 38 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,8 +43,10 @@ class XMLFeedSpider(InitSpider):\n         \"\"\"\n         return response\n \n-    def parse_item(self, response, selector):\n+    def parse_node(self, response, selector):\n         \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n+        if hasattr(self, 'parse_item'): # backward compatibility\n+            return self.parse_item(response, selector)\n         raise NotImplemented\n         \n     def parse_nodes(self, response, nodes):\n@@ -56,7 +58,7 @@ class XMLFeedSpider(InitSpider):\n         \"\"\"\n \n         for selector in nodes:\n-            ret = self.parse_item(response, selector)\n+            ret = self.parse_node(response, selector)\n             if isinstance(ret, (BaseItem, Request)):\n                 ret = [ret]\n             if not isinstance(ret, (list, tuple)):\n@@ -65,8 +67,8 @@ class XMLFeedSpider(InitSpider):\n                 yield result_item\n \n     def parse(self, response):\n-        if not hasattr(self, 'parse_item'):\n-            raise NotConfigured('You must define parse_item method in order to scrape this XML feed')\n+        if not hasattr(self, 'parse_node'):\n+            raise NotConfigured('You must define parse_node method in order to scrape this XML feed')\n \n         response = self.adapt_response(response)\n         if self.iterator == 'iternodes':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8a074c9cb58555f2ce8acf5827bb4f2af760edef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 147 | Lines Deleted: 125 | Files Changed: 23 | Hunks: 49 | Methods Changed: 26 | Complexity Δ (Sum/Max): 11/7 | Churn Δ: 272 | Churn Cumulative: 1018 | Contributors (this commit): 5 | Commits (past 90d): 73 | Contributors (cumulative): 48 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,75 +0,0 @@\n-#!/usr/bin/env python\n-\"\"\"Scrapy admin script is used to create new scrapy projects and similar\n-tasks\"\"\"\n-import os\n-import string\n-from optparse import OptionParser\n-import re\n-\n-import scrapy\n-from scrapy.utils.template import render_templatefile, string_camelcase\n-from scrapy.utils.python import ignore_patterns, copytree\n-\n-usage = \"\"\"\n-scrapy-admin.py [options] [command]\n- \n-Available commands:\n-     \n-    startproject <project_name>\n-      Starts a new project with name 'project_name'\n-\"\"\"\n-\n-PROJECT_TEMPLATES_PATH = os.path.join(scrapy.__path__[0], 'templates/project')\n-\n-# This is the list of templatefile's path that are rendered *after copying* to\n-# project directory.\n-TEMPLATES = (\n-        'scrapy-ctl.py',\n-        '${project_name}/settings.py.tmpl',\n-        '${project_name}/items.py.tmpl',\n-        '${project_name}/pipelines.py.tmpl',\n-        )\n-\n-IGNORE = ignore_patterns('*.pyc', '.svn')\n-\n-def main():\n-    parser = OptionParser(usage=usage)\n-    opts, args = parser.parse_args()\n-\n-    if not args:\n-        parser.print_help()\n-        return\n-\n-    cmd = args[0]\n-    if cmd == \"startproject\":\n-        if len(args) >= 2:\n-            project_name = args[1]\n-            if not re.search(r'^[_a-zA-Z]\\w*$', project_name): # If it's not a valid directory name.\n-                # Provide a smart error message, depending on the error.\n-                if not re.search(r'^[_a-zA-Z]', project_name):\n-                    message = 'make sure the project_name begins with a letter or underscore'\n-                else:\n-                    message = 'use only numbers, letters and underscores'\n-                print \"scrapy-admin.py: %r is not a valid project name. Please %s.\" % (project_name, message)\n-            else:\n-                project_root_path = project_name\n-\n-                roottpl = os.path.join(PROJECT_TEMPLATES_PATH, 'root')\n-                copytree(roottpl, project_name, ignore=IGNORE)\n-\n-                moduletpl = os.path.join(PROJECT_TEMPLATES_PATH, 'module')\n-                copytree(moduletpl, '%s/%s' % (project_name, project_name),\n-                         ignore=IGNORE)\n-\n-                for path in TEMPLATES:\n-                    tplfile = os.path.join(project_root_path,\n-                            string.Template(path).substitute(project_name=project_name))\n-                    render_templatefile(tplfile, project_name=project_name,\n-                            ProjectName=string_camelcase(project_name))\n-        else:\n-            print \"scrapy-admin.py: missing project name\"\n-    else:\n-        print \"scrapy-admin.py: unknown command: %s\" % cmd\n-\n-if __name__ == '__main__':\n-    main()\n\n@@ -0,0 +1,4 @@\n+#!/usr/bin/env python\n+\n+from scrapy.command.cmdline import execute\n+execute()\n\n@@ -9,7 +9,7 @@ import scrapy\n from scrapy import log\n from scrapy.spider import spiders\n from scrapy.xlib import lsprofcalltree\n-from scrapy.conf import settings, SETTINGS_MODULE\n+from scrapy.conf import settings\n \n # This dict holds information about the executed command for later use\n command_executed = {}\n@@ -45,12 +45,12 @@ def get_command_name(argv):\n         if not arg.startswith('-'):\n             return arg\n \n-def usage(prog):\n+def usage():\n     s = \"Usage\\n\"\n     s += \"=====\\n\"\n-    s += \"%s <command> [options] [args]\\n\" % prog\n+    s += \"scrapy-ctl.py <command> [options] [args]\\n\"\n     s += \"  Run a command\\n\\n\"\n-    s += \"%s <command> -h\\n\" % prog\n+    s += \"scrapy-ctl.py <command> -h\\n\"\n     s += \"  Print command help and options\\n\\n\"\n     s += \"Available commands\\n\"\n     s += \"===================\\n\"\n@@ -72,14 +72,10 @@ def update_default_settings(module, cmdname):\n         if not k.startswith(\"_\"):\n             settings.defaults[k] = v\n \n-def execute():\n-    if not settings.settings_module:\n-        print \"Scrapy %s\\n\" % scrapy.__version__\n-        print \"Error: Cannot find %r module in python path\" % SETTINGS_MODULE\n-        sys.exit(1)\n-    execute_with_args(sys.argv)\n+def execute(argv=None):\n+    if argv is None:\n+        argv = sys.argv\n \n-def execute_with_args(argv):\n     cmds = get_commands_dict()\n \n     cmdname = get_command_name(argv)\n@@ -88,7 +84,7 @@ def execute_with_args(argv):\n \n     if not cmdname:\n         print \"Scrapy %s\\n\" % scrapy.__version__\n-        print usage(argv[0])\n+        print usage()\n         sys.exit(2)\n \n     parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \\\n@@ -119,6 +115,11 @@ def execute_with_args(argv):\n     command_executed['opts'] = opts.__dict__.copy()\n \n     cmd.process_options(args, opts)\n+    if cmd.requires_project and not settings.settings_module:\n+        print \"Error running: scrapy-ctl.py %s\\n\" % cmdname\n+        print \"Cannot find project settings module in python path: %s\" % \\\n+            settings.settings_module_path\n+        sys.exit(1)\n     spiders.load()\n     log.start()\n     ret = run_command(cmd, args, opts)\n\n@@ -8,6 +8,9 @@ from scrapy import log\n \n \n class Command(ScrapyCommand):\n+\n+    requires_project = True\n+\n     def syntax(self):\n         return \"[options] <domain|url> ...\"\n \n\n@@ -4,6 +4,9 @@ from scrapy.command import ScrapyCommand\n from scrapy.fetcher import fetch\n \n class Command(ScrapyCommand):\n+\n+    requires_project = False\n+\n     def syntax(self):\n         return \"[options] <url>\"\n \n\n@@ -24,6 +24,8 @@ def sanitize_module_name(module_name):\n \n class Command(ScrapyCommand):\n \n+    requires_project = True\n+\n     def syntax(self):\n         return \"[options] <spider_module_name> <spider_domain_name>\"\n \n\n@@ -1,6 +1,9 @@\n from scrapy.command import ScrapyCommand, cmdline\n \n class Command(ScrapyCommand):\n+\n+    requires_project = False\n+\n     def syntax(self):\n         return \"<command>\"\n     \n\n@@ -2,6 +2,9 @@ from scrapy.command import ScrapyCommand\n from scrapy.spider import spiders\n \n class Command(ScrapyCommand):\n+\n+    requires_project = True\n+\n     def short_desc(self):\n         return \"List available spiders\"\n \n\n@@ -7,6 +7,9 @@ from scrapy.utils import display\n from scrapy import log\n \n class Command(ScrapyCommand):\n+\n+    requires_project = True\n+\n     def syntax(self):\n         return \"[options] <url>\"\n \n\n@@ -21,6 +21,9 @@ from scrapy.http import Request\n from scrapy.fetcher import get_or_create_spider\n \n class Command(ScrapyCommand):\n+\n+    requires_project = False\n+\n     def syntax(self):\n         return \"[url]\"\n \n\n@@ -2,6 +2,9 @@ from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n \n class Command(ScrapyCommand):\n+\n+    requires_project = True\n+\n     def short_desc(self):\n         return \"Start the Scrapy manager but don't run any spider (idle mode)\"\n \n\n@@ -0,0 +1,62 @@\n+#!/usr/bin/env python\n+\n+import os\n+import string\n+import re\n+\n+import scrapy\n+from scrapy.command import ScrapyCommand\n+from scrapy.utils.template import render_templatefile, string_camelcase\n+from scrapy.utils.python import ignore_patterns, copytree\n+\n+PROJECT_TEMPLATES_PATH = os.path.join(scrapy.__path__[0], 'templates/project')\n+\n+# This is the list of templatefile's path that are rendered *after copying* to\n+# the new project directory.\n+TEMPLATES = (\n+    ('scrapy-ctl.py',),\n+    ('${project_name}', 'settings.py.tmpl'),\n+    ('${project_name}', 'items.py.tmpl'),\n+    ('${project_name}', 'pipelines.py.tmpl'),\n+)\n+\n+IGNORE = ignore_patterns('*.pyc', '.svn')\n+\n+class Command(ScrapyCommand):\n+\n+    requires_project = False\n+\n+    def syntax(self):\n+        return \"<project_name>\"\n+\n+    def short_desc(self):\n+        return \"Create new project with an initial project template\"\n+\n+    def run(self, args, opts):\n+        if len(args) != 1:\n+            return False\n+\n+        project_name = args[0]\n+        if not re.search(r'^[_a-zA-Z]\\w*$', project_name): # If it's not a valid directory name.\n+            # Provide a smart error message, depending on the error.\n+            if not re.search(r'^[a-zA-Z]', project_name):\n+                message = 'Project names must begin with a letter'\n+            else:\n+                message = 'Project names must contain only letters, numbers and underscores'\n+            print \"Invalid project name: %s\\n\\n%s\" % (project_name, message)\n+        else:\n+            project_root_path = project_name\n+\n+            roottpl = os.path.join(PROJECT_TEMPLATES_PATH, 'root')\n+            copytree(roottpl, project_name, ignore=IGNORE)\n+\n+            moduletpl = os.path.join(PROJECT_TEMPLATES_PATH, 'module')\n+            copytree(moduletpl, '%s/%s' % (project_name, project_name),\n+                ignore=IGNORE)\n+\n+            for paths in TEMPLATES:\n+                path = os.path.join(*paths)\n+                tplfile = os.path.join(project_root_path,\n+                    string.Template(path).substitute(project_name=project_name))\n+                render_templatefile(tplfile, project_name=project_name,\n+                    ProjectName=string_camelcase(project_name))\n\n@@ -11,6 +11,9 @@ from optparse import OptionGroup\n from scrapy.conf import settings\n \n class ScrapyCommand(object):\n+\n+    requires_project = False\n+\n     def syntax(self):\n         \"\"\"\n         Command syntax (preferably one-line). Do not include command name.\n@@ -61,12 +64,17 @@ class ScrapyCommand(object):\n             help=\"write lsprof profiling stats to FILE\")\n         group.add_option(\"--pidfile\", dest=\"pidfile\", metavar=\"FILE\", \\\n             help=\"write process ID to FILE\")\n-        group.add_option(\"--set\", dest=\"settings\", action=\"append\", \\\n+        group.add_option(\"--set\", dest=\"set\", action=\"append\", \\\n             metavar=\"SETTING=VALUE\", default=[], \\\n             help=\"set/override setting (may be repeated)\")\n+        group.add_option(\"--settings\", dest=\"settings\", metavar=\"MODULE\",\n+            help=\"python path to the Scrapy project settings\")\n         parser.add_option_group(group)\n         \n     def process_options(self, args, opts):\n+        if opts.settings:\n+            settings.set_settings_module(opts.settings)\n+\n         if opts.logfile:\n             settings.overrides['LOG_ENABLED'] = True\n             settings.overrides['LOG_FILE'] = opts.logfile\n@@ -90,7 +98,7 @@ class ScrapyCommand(object):\n             with open(opts.pidfile, \"w\") as f:\n                 f.write(str(os.getpid()))\n \n-        for setting in opts.settings:\n+        for setting in opts.set:\n             if '=' in setting:\n                 name, val = setting.split('=', 1)\n                 settings.overrides[name] = val\n\n@@ -7,7 +7,6 @@ See documentation in docs/topics/settings.rst\n import os\n import cPickle as pickle\n \n-SETTINGS_MODULE = os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings')\n SETTINGS_DISABLED = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n \n class Settings(object):\n@@ -19,17 +18,11 @@ class Settings(object):\n     global_defaults = None\n \n     def __init__(self):\n+        self.set_settings_module()\n         pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n         self.overrides = pickle.loads(pickled_settings) if pickled_settings else {}\n-        self.settings_module = self._import(SETTINGS_MODULE)\n         self.defaults = {}\n-        self.global_defaults = self._import('scrapy.conf.default_settings')\n-\n-    def _import(self, modulepath):\n-        try:\n-            return __import__(modulepath, {}, {}, [''])\n-        except ImportError:\n-            pass\n+        self.global_defaults = __import__('scrapy.conf.default_settings', {}, {}, [''])\n \n     def __getitem__(self, opt_name):\n         if not SETTINGS_DISABLED:\n@@ -43,6 +36,16 @@ class Settings(object):\n                 return self.defaults[opt_name]\n         return getattr(self.global_defaults, opt_name, None)\n \n+    def set_settings_module(self, settings_module_path=None):\n+        if settings_module_path is None:\n+            settings_module_path = os.environ.get('SCRAPYSETTINGS_MODULE', \\\n+                'scrapy_settings')\n+        self.settings_module_path = settings_module_path\n+        try:\n+            self.settings_module = __import__(settings_module_path, {}, {}, [''])\n+        except ImportError:\n+            self.settings_module = None\n+\n     def get(self, name, default=None):\n         return self[name] if self[name] is not None else default\n \n@@ -68,4 +71,7 @@ class Settings(object):\n         else:\n             return str(value).split(',')\n \n+    def __str__(self):\n+        return \"<Settings %r>\" % self.settings_module_path\n+\n settings = Settings()\n\n@@ -0,0 +1 @@\n+LOG_ENABLED = False\n\n@@ -19,8 +19,8 @@ class SpiderManager(object):\n     def __init__(self):\n         self.loaded = False\n         self.default_domain = None\n-        self.spider_modules = settings.getlist('SPIDER_MODULES')\n         self.force_domain = None\n+        self.spider_modules = None\n \n     def fromdomain(self, domain_name):\n         return self.asdict().get(domain_name)\n@@ -56,7 +56,10 @@ class SpiderManager(object):\n         if not self.loaded:\n             self.load()\n \n-    def load(self):\n+    def load(self, spider_modules=None):\n+        if spider_modules is None:\n+            spider_modules = settings.getlist('SPIDER_MODULES')\n+        self.spider_modules = spider_modules\n         self._invaliddict = {}\n         self._spiders = {}\n \n@@ -75,7 +78,7 @@ class SpiderManager(object):\n             # we can't use the log module here because it may not be available yet\n             print \"WARNING: Could not load spider %s: %s\" % (spider, e)\n \n-    def reload(self, skip_domains=None):\n+    def reload(self, spider_modules=None, skip_domains=None):\n         \"\"\"Reload spiders by trying to discover any spiders added under the\n         spiders module/packages, removes any spiders removed.\n \n@@ -91,7 +94,7 @@ class SpiderManager(object):\n             if not domain in skip_domains:\n                 reload(sys.modules[spider.__module__])\n                 reloaded += 1\n-        self.load()  # second call to update spider instances\n+        self.load(spider_modules=spider_modules)  # second call to update spider instances\n         log.msg(\"Reloaded %d/%d scrapy spiders\" % (reloaded, len(pdict)), level=log.DEBUG)\n \n     def _getspiders(self, interface, package):\n\n@@ -2,17 +2,15 @@ from __future__ import with_statement\n \n from unittest import TestCase\n \n-from scrapy.spider import spiders\n from scrapy.http import Response, Request\n+from scrapy.spider import BaseSpider\n from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware\n \n \n class CookiesMiddlewareTest(TestCase):\n \n     def setUp(self):\n-        spiders.spider_modules = ['scrapy.tests.test_spiders']\n-        spiders.reload()\n-        self.spider = spiders.fromdomain('scrapytest.org')\n+        self.spider = BaseSpider()\n         self.mw = CookiesMiddleware()\n \n     def tearDown(self):\n\n@@ -3,7 +3,7 @@ from __future__ import with_statement\n from unittest import TestCase\n from os.path import join, abspath, dirname\n \n-from scrapy.spider import spiders\n+from scrapy.spider import BaseSpider\n from scrapy.http import Response, Request\n from scrapy.contrib.downloadermiddleware.httpcompression import HttpCompressionMiddleware\n from scrapy.tests import tests_datadir\n@@ -20,9 +20,7 @@ FORMAT = {\n class HttpCompressionTest(TestCase):\n \n     def setUp(self):\n-        spiders.spider_modules = ['scrapy.tests.test_spiders']\n-        spiders.reload()\n-        self.spider = spiders.fromdomain('scrapytest.org')\n+        self.spider = BaseSpider()\n         self.mw = HttpCompressionMiddleware()\n \n     def _getresponse(self, coding):\n\n@@ -1,16 +1,14 @@\n import unittest\n \n from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware\n-from scrapy.spider import spiders\n+from scrapy.spider import BaseSpider\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.http import Request, Response, Headers\n \n class RedirectMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n-        spiders.spider_modules = ['scrapy.tests.test_spiders']\n-        spiders.reload()\n-        self.spider = spiders.fromdomain('scrapytest.org')\n+        self.spider = BaseSpider()\n         self.mw = RedirectMiddleware()\n \n     def test_priority_adjust(self):\n\n@@ -5,14 +5,12 @@ from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookup\n                                    ConnectionLost\n \n from scrapy.contrib.downloadermiddleware.retry import RetryMiddleware\n-from scrapy.spider import spiders\n+from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n \n class RetryTest(unittest.TestCase):\n     def setUp(self):\n-        spiders.spider_modules = ['scrapy.tests.test_spiders']\n-        spiders.reload()\n-        self.spider = spiders.fromdomain('scrapytest.org')\n+        self.spider = BaseSpider()\n         self.mw = RetryMiddleware()\n         self.mw.max_retry_times = 2\n \n\n@@ -1,6 +1,6 @@\n from unittest import TestCase\n \n-from scrapy.spider import spiders\n+from scrapy.spider import BaseSpider\n from scrapy.http import Request\n from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware\n from scrapy.conf import settings\n@@ -9,9 +9,7 @@ from scrapy.conf import settings\n class UserAgentMiddlewareTest(TestCase):\n \n     def setUp(self):\n-        spiders.spider_modules = ['scrapy.tests.test_spiders']\n-        spiders.reload()\n-        self.spider = spiders.fromdomain('scrapytest.org')\n+        self.spider = BaseSpider()\n         self.mw = UserAgentMiddleware()\n \n     def tearDown(self):\n\n@@ -45,8 +45,7 @@ class CrawlingSession(object):\n         self.portno = self.port.getHost().port\n \n         from scrapy.spider import spiders\n-        spiders.spider_modules = ['scrapy.tests.test_spiders']\n-        spiders.reload()\n+        spiders.load(['scrapy.tests.test_spiders'])\n \n         self.spider = spiders.fromdomain(self.domain)\n         if self.spider:\n\n@@ -101,5 +101,5 @@ setup(\n     packages = packages,\n     cmdclass = cmdclasses,\n     data_files = data_files,\n-    scripts = ['bin/scrapy-admin.py'],\n+    scripts = ['bin/scrapy-ctl.py'],\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9fbaef90fe4e4965a8ceebff7bdc7371f8361950", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 169 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,7 +18,7 @@ def sanitize_module_name(module_name):\n     prefixing it with a letter if it doesn't start with one\n     \"\"\"\n     module_name = module_name.replace('-', '_')\n-    if module_name[0] not in string.letters:\n+    if module_name[0] not in string.ascii_letters:\n         module_name = \"a\" + module_name\n     return module_name\n \n@@ -88,7 +88,7 @@ class Command(ScrapyCommand):\n         print \"  %s.%s\" % (spiders_module.__name__, module)\n \n     def _find_template(self, template):\n-        template_file = join(settings['TEMPLATES_DIR'], '%s.tmpl' % template)\n+        template_file = join(settings['TEMPLATES_DIR'], 'spiders', '%s.tmpl' % template)\n         if not exists(template_file):\n             template_file = join(SPIDER_TEMPLATES_PATH, '%s.tmpl' % template)\n             if not exists(template_file):\n@@ -99,10 +99,9 @@ class Command(ScrapyCommand):\n         return template_file\n \n     def _list_templates(self):\n-        files = set()\n+        files = set(listdir(SPIDER_TEMPLATES_PATH))\n         if exists(settings['TEMPLATES_DIR']):\n-            files.update(listdir(settings['TEMPLATES_DIR']))\n-        files.update(listdir(SPIDER_TEMPLATES_PATH))\n+            files.update(listdir(join(settings['TEMPLATES_DIR'], 'spiders')))\n \n         for filename in sorted(files):\n             if filename.endswith('.tmpl'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9bafa7325c0b4f285ab5e5e65a4707b8a040f63f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 60 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -24,9 +24,9 @@ class OverrideFieldPersonItem(BasePersonItem):\n     age = Field(default=1)\n \n \n-class DjangoItemTest(TestCase):\n+class DjangoItemTest(unittest.TestCase):\n     \n-    def setUp(\n+    def setUp(self):\n         if not django:\n             raise unittest.SkipTest(\"Django is not available\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9f1ed6376302d2412ce5bba118e67ce2998c08b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 50 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import sys\n import warnings\n from functools import wraps\n \n@@ -13,14 +14,7 @@ def deprecated(use_instead=None):\n             message = \"Call to deprecated function %s.\" % func.__name__\n             if use_instead:\n                 message += \" Use %s instead.\" % use_instead\n-\n-            warnings.warn_explicit(message,\n-                category=DeprecationWarning,\n-                filename=func.func_code.co_filename,\n-                lineno=func.func_code.co_firstlineno + 1,\n-                registry=func.func_globals.setdefault('__warningregistry__', \\\n-                                                      {}),\n-                module_globals=func.func_globals)\n+            warnings.warn(message, category=DeprecationWarning, stacklevel=2)\n             return func(*args, **kwargs)\n         return new_func\n     return wrapped\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
