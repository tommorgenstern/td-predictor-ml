{"custom_id": "scrapy#c025003da2c60a0a786c2ee158fe620402914273", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 48 | Churn Cumulative: 412 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ import os.path\n import time\n import logging\n from email.utils import parsedate_tz, mktime_tz\n+from ftplib import FTP\n from six.moves.urllib.parse import urlparse\n from collections import defaultdict\n import six\n@@ -31,6 +32,7 @@ from scrapy.utils.python import to_bytes\n from scrapy.utils.request import referer_str\n from scrapy.utils.boto import is_botocore\n from scrapy.utils.datatypes import CaselessDict\n+from scrapy.utils.ftp import ftp_makedirs_cwd\n \n logger = logging.getLogger(__name__)\n \n@@ -248,6 +250,42 @@ class GCSFilesStore(object):\n         )\n \n \n+class FTPFilesStore(object):\n+    \n+    def __init__(self, uri):\n+        assert uri.startswith('ftp://')\n+        u = urlparse(uri)\n+        self.ftp = FTP()\n+        self.ftp.connect(u.hostname, u.port or '21')\n+        self.ftp.login(u.username, u.password)\n+        self.basedir = u.path + '/'\n+        ftp_makedirs_cwd(self.ftp, self.basedir+'full')\n+        \n+    def persist_file(self, path, buf, info, meta=None, headers=None):\n+        buf.seek(0)\n+        filename = path.split('/')[1]\n+        return threads.deferToThread(\n+            self.ftp.storbinary,\n+            'STOR %s' % filename,\n+            buf\n+        )\n+            \n+    def stat_file(self, path, info):\n+        def _stat_file(path):\n+            try:\n+                last_modified = float(self.ftp.voidcmd(\"MDTM \" + self.basedir + '/' + path)[4:].strip())\n+                m = hashlib.md5()\n+                self.ftp.retrbinary('RETR %s' % self.basedir + path, m.update)\n+                return {'last_modified': last_modified, 'checksum': m.hexdigest()}\n+            # The file doesn't exist\n+            except Exception as e :\n+                return {} \n+        return threads.deferToThread(_stat_file, path)\n+    \n+    def close_connection(self):\n+        self.ftp.quit()\n+\n+\n class FilesPipeline(MediaPipeline):\n     \"\"\"Abstract pipeline that implement the file downloading\n \n@@ -274,6 +312,7 @@ class FilesPipeline(MediaPipeline):\n         'file': FSFilesStore,\n         's3': S3FilesStore,\n         'gs': GCSFilesStore,\n+        'ftp': FTPFilesStore\n     }\n     DEFAULT_FILES_URLS_FIELD = 'file_urls'\n     DEFAULT_FILES_RESULT_FIELD = 'files'\n@@ -284,7 +323,6 @@ class FilesPipeline(MediaPipeline):\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n-\n         cls_name = \"FilesPipeline\"\n         self.store = self._get_store(store_uri)\n         resolve = functools.partial(self._key_for_pipe,\n@@ -303,7 +341,6 @@ class FilesPipeline(MediaPipeline):\n         self.files_result_field = settings.get(\n             resolve('FILES_RESULT_FIELD'), self.FILES_RESULT_FIELD\n         )\n-\n         super(FilesPipeline, self).__init__(download_func=download_func, settings=settings)\n \n     @classmethod\n@@ -461,3 +498,10 @@ class FilesPipeline(MediaPipeline):\n         media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         media_ext = os.path.splitext(request.url)[1]\n         return 'full/%s%s' % (media_guid, media_ext)\n+    \n+    def close_spider(self, spider):\n+        try:\n+            self.store.close_connection()\n+        # If the store doesn't implement this function, pass\n+        except AttributeError:\n+            pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#00fe05e53611d5c61fb23cdd6674fd89c99fd69e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 14 | Churn Cumulative: 1087 | Contributors (this commit): 51 | Commits (past 90d): 10 | Contributors (cumulative): 65 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,6 +26,7 @@ class RobotsTxtMiddleware(object):\n         if not crawler.settings.getbool('ROBOTSTXT_OBEY'):\n             raise NotConfigured\n         self._default_useragent = crawler.settings.get('USER_AGENT', 'Scrapy')\n+        self._robotstxt_useragent = crawler.settings.get('ROBOTSTXT_USER_AGENT', None)\n         self.crawler = crawler\n         self._parsers = {}\n         self._parserimpl = load_object(crawler.settings.get('ROBOTSTXT_PARSER'))\n@@ -47,6 +48,9 @@ class RobotsTxtMiddleware(object):\n     def process_request_2(self, rp, request, spider):\n         if rp is None:\n             return\n+\n+        useragent = self._robotstxt_useragent\n+        if not useragent:\n             useragent = request.headers.get(b'User-Agent', self._default_useragent)\n         if not rp.allowed(request.url, useragent):\n             logger.debug(\"Forbidden by robots.txt: %(request)s\",\n\n@@ -246,6 +246,7 @@ RETRY_PRIORITY_ADJUST = -1\n \n ROBOTSTXT_OBEY = False\n ROBOTSTXT_PARSER = 'scrapy.robotstxt.PythonRobotParser'\n+ROBOTSTXT_USER_AGENT = None\n \n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'\n\n@@ -164,6 +164,15 @@ Disallow: /some/randome/page.html\n         d.addCallback(lambda _: self.assertFalse(mw_module_logger.error.called))\n         return d\n \n+    def test_robotstxt_user_agent_setting(self):\n+        crawler = self._get_successful_crawler()\n+        crawler.settings.set('ROBOTSTXT_USER_AGENT', 'Examplebot')\n+        crawler.settings.set('USER_AGENT', 'Mozilla/5.0 (X11; Linux x86_64)')\n+        middleware = RobotsTxtMiddleware(crawler)\n+        rp = mock.MagicMock(return_value=True)\n+        middleware.process_request_2(rp, Request('http://site.local/allowed'), None)\n+        rp.allowed.assert_called_once_with('http://site.local/allowed', 'Examplebot')\n+\n     def assertNotIgnored(self, request, middleware):\n         spider = None  # not actually used\n         dfd = maybeDeferred(middleware.process_request, request, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b1587ed1bc736f9fcc357ce425405adf5bd6d08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 445 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -252,18 +252,29 @@ class GCSFilesStore(object):\n \n class FTPFilesStore(object):\n \n+    FTP_USERNAME = None\n+    FTP_PASSWORD = None\n+    \n     def __init__(self, uri):\n         assert uri.startswith('ftp://')\n         u = urlparse(uri)\n         self.ftp = FTP()\n         self.ftp.connect(u.hostname, u.port or '21')\n-        self.ftp.login(u.username, u.password)\n-        self.basedir = u.path + '/'\n-        ftp_makedirs_cwd(self.ftp, self.basedir+'full')\n+        username = u.username or FTP_USERNAME\n+        password = u.password or FTP_PASSWORD\n+        self.ftp.login(username, password)\n+        self.basedir = u.path\n         \n     def persist_file(self, path, buf, info, meta=None, headers=None):\n         buf.seek(0)\n-        filename = path.split('/')[1]\n+        # If the path is like 'x/y/z.ext' the 'x/y' is rel_path and \n+        # 'z.ext' is file name\n+        # If path is only the file name 'z.ext', then rel_path is\n+        # the empty string and filename is 'z.ext'\n+        x = path.rsplit('/',1)\n+        rel_path, filename = ('/' + x[0], x[1]) if len(x) > 1 else ('', x[0])\n+        abs_path = self.basedir + rel_path\n+        ftp_makedirs_cwd(self.ftp, abs_path)\n         return threads.deferToThread(\n             self.ftp.storbinary,\n             'STOR %s' % filename,\n@@ -282,9 +293,6 @@ class FTPFilesStore(object):\n                 return {} \n         return threads.deferToThread(_stat_file, path)\n \n-    def close_connection(self):\n-        self.ftp.quit()\n-\n \n class FilesPipeline(MediaPipeline):\n     \"\"\"Abstract pipeline that implement the file downloading\n@@ -358,6 +366,10 @@ class FilesPipeline(MediaPipeline):\n         gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n         gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None\n \n+        ftp_store = cls.STORE_SCHEMES['ftp']\n+        ftp_store.FTP_USERNAME = settings['FTP_USER']           # Default is 'anonymous'\n+        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']       # Default is `guest`\n+        \n         store_uri = settings['FILES_STORE']\n         return cls(store_uri, settings=settings)\n \n@@ -498,10 +510,3 @@ class FilesPipeline(MediaPipeline):\n         media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         media_ext = os.path.splitext(request.url)[1]\n         return 'full/%s%s' % (media_guid, media_ext)\n\\ No newline at end of file\n-    \n-    def close_spider(self, spider):\n-        try:\n-            self.store.close_connection()\n-        # If the store doesn't implement this function, pass\n-        except AttributeError:\n-            pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0a5cb7745bc22b8e193c4b0e964b20e59ddc0bc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 449 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -260,8 +260,8 @@ class FTPFilesStore(object):\n         u = urlparse(uri)\n         self.ftp = FTP()\n         self.ftp.connect(u.hostname, u.port or '21')\n-        username = u.username or FTP_USERNAME\n-        password = u.password or FTP_PASSWORD\n+        username = u.username or self.FTP_USERNAME\n+        password = u.password or self.FTP_PASSWORD\n         self.ftp.login(username, password)\n         self.basedir = u.path\n         \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81ac1da3813c11a806aca845a5022e9964b03f80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 453 | Contributors (this commit): 14 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -263,7 +263,7 @@ class FTPFilesStore(object):\n         username = u.username or self.FTP_USERNAME\n         password = u.password or self.FTP_PASSWORD\n         self.ftp.login(username, password)\n-        self.basedir = u.path\n+        self.basedir = u.path.rstrip('/')\n         \n     def persist_file(self, path, buf, info, meta=None, headers=None):\n         buf.seek(0)\n@@ -272,7 +272,7 @@ class FTPFilesStore(object):\n         # If path is only the file name 'z.ext', then rel_path is\n         # the empty string and filename is 'z.ext'\n         x = path.rsplit('/',1)\n-        rel_path, filename = ('/' + x[0], x[1]) if len(x) > 1 else ('', x[0])\n+        rel_path, filename = ('/' + x[0].lstrip('/'), x[1]) if len(x) > 1 else ('', x[0])\n         abs_path = self.basedir + rel_path\n         ftp_makedirs_cwd(self.ftp, abs_path)\n         return threads.deferToThread(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#790bf9031229261639a5c457f6dd3498bdff8830", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 25 | Churn Cumulative: 478 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -258,14 +258,19 @@ class FTPFilesStore(object):\n     def __init__(self, uri):\n         assert uri.startswith('ftp://')\n         u = urlparse(uri)  \n-        self.ftp = FTP()\n-        self.ftp.connect(u.hostname, u.port or '21')\n-        username = u.username or self.FTP_USERNAME\n-        password = u.password or self.FTP_PASSWORD\n-        self.ftp.login(username, password)\n+        self.port = u.port\n+        self.host = u.hostname\n+        self.port = int(u.port or '21')\n+        self.username = u.username or self.FTP_USERNAME\n+        self.password = u.password or self.FTP_PASSWORD\n         self.basedir = u.path.rstrip('/')\n         \n     def persist_file(self, path, buf, info, meta=None, headers=None):\n+        \n+        def _persist_file(path, buf):\n+            ftp = FTP()\n+            ftp.connect(self.host, self.port)\n+            ftp.login(self.username, self.password)\n             buf.seek(0)\n             # If the path is like 'x/y/z.ext' the 'x/y' is rel_path and \n             # 'z.ext' is file name\n@@ -274,12 +279,10 @@ class FTPFilesStore(object):\n             x = path.rsplit('/',1)\n             rel_path, filename = ('/' + x[0].lstrip('/'), x[1]) if len(x) > 1 else ('', x[0])\n             abs_path = self.basedir + rel_path\n-        ftp_makedirs_cwd(self.ftp, abs_path)\n-        return threads.deferToThread(\n-            self.ftp.storbinary,\n-            'STOR %s' % filename,\n-            buf\n-        )\n+            ftp_makedirs_cwd(ftp, abs_path)\n+            ftp.storbinary('STOR %s' % filename, buf)\n+\n+        return threads.deferToThread(_persist_file, path, buf)\n             \n     def stat_file(self, path, info):\n         def _stat_file(path):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8c970c636eb37deeb5caddbe5069bfcc0a79015e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 480 | Contributors (this commit): 14 | Commits (past 90d): 7 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -260,7 +260,7 @@ class FTPFilesStore(object):\n         u = urlparse(uri)  \n         self.port = u.port\n         self.host = u.hostname\n-        self.port = int(u.port or '21')\n+        self.port = int(u.port or 21)\n         self.username = u.username or self.FTP_USERNAME\n         self.password = u.password or self.FTP_PASSWORD\n         self.basedir = u.path.rstrip('/')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd22b25ef4e4223aafc6e326058c6d62b1fbf13c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 75 | Churn Cumulative: 830 | Contributors (this commit): 26 | Commits (past 90d): 12 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,7 +19,7 @@ from twisted.internet import defer, threads\n from w3lib.url import file_uri_to_path\n \n from scrapy import signals\n-from scrapy.utils.ftp import ftp_makedirs_cwd\n+from scrapy.utils.ftp import ftp_makedirs_cwd, ftp_store_file\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.log import failure_to_exc_info\n@@ -174,16 +174,11 @@ class FTPFeedStorage(BlockingFeedStorage):\n         )\n \n     def _store_in_thread(self, file):\n-        file.seek(0)\n-        ftp = FTP()\n-        ftp.connect(self.host, self.port)\n-        ftp.login(self.username, self.password)\n-        if self.use_active_mode:\n-            ftp.set_pasv(False)\n-        dirname, filename = posixpath.split(self.path)\n-        ftp_makedirs_cwd(ftp, dirname)\n-        ftp.storbinary('STOR %s' % filename, file)\n-        ftp.quit()\n+        ftp_store_file(\n+            self.path, file, self.host,\n+            self.port, self.username, \n+            self.password, self.use_active_mode\n+        )\n \n \n class SpiderSlot(object):\n\n@@ -32,7 +32,7 @@ from scrapy.utils.python import to_bytes\n from scrapy.utils.request import referer_str\n from scrapy.utils.boto import is_botocore\n from scrapy.utils.datatypes import CaselessDict\n-from scrapy.utils.ftp import ftp_makedirs_cwd\n+from scrapy.utils.ftp import ftp_makedirs_cwd, ftp_store_file\n \n logger = logging.getLogger(__name__)\n \n@@ -254,6 +254,7 @@ class FTPFilesStore(object):\n \n     FTP_USERNAME = None\n     FTP_PASSWORD = None\n+    USE_ACTIVE_MODE = None\n     \n     def __init__(self, uri):\n         assert uri.startswith('ftp://')\n@@ -266,30 +267,25 @@ class FTPFilesStore(object):\n         self.basedir = u.path.rstrip('/')\n         \n     def persist_file(self, path, buf, info, meta=None, headers=None):     \n-        \n-        def _persist_file(path, buf):\n-            ftp = FTP()\n-            ftp.connect(self.host, self.port)\n-            ftp.login(self.username, self.password)\n-            buf.seek(0)\n-            # If the path is like 'x/y/z.ext' the 'x/y' is rel_path and \n-            # 'z.ext' is file name\n-            # If path is only the file name 'z.ext', then rel_path is\n-            # the empty string and filename is 'z.ext'\n-            x = path.rsplit('/',1)\n-            rel_path, filename = ('/' + x[0].lstrip('/'), x[1]) if len(x) > 1 else ('', x[0])\n-            abs_path = self.basedir + rel_path\n-            ftp_makedirs_cwd(ftp, abs_path)\n-            ftp.storbinary('STOR %s' % filename, buf)\n-\n-        return threads.deferToThread(_persist_file, path, buf)\n+        path = '%s/%s' % (self.basedir, path)\n+        return threads.deferToThread(\n+            ftp_store_file, path,buf,\n+            self.host, self.port,self.username,\n+            self.password, self.USE_ACTIVE_MODE\n+        )\n             \n     def stat_file(self, path, info):\n         def _stat_file(path):\n             try:\n-                last_modified = float(self.ftp.voidcmd(\"MDTM \" + self.basedir + '/' + path)[4:].strip())\n+                ftp = FTP()\n+                ftp.connect(self.host, self.port)\n+                ftp.login(self.username, self.password)\n+                if self.USE_ACTIVE_MODE:\n+                    ftp.set_pasv(False)\n+                file_path = \"%s/%s\" % (self.basedir, path)\n+                last_modified = float(ftp.voidcmd(\"MDTM %s\" % file_path)[4:].strip())\n                 m = hashlib.md5()\n-                self.ftp.retrbinary('RETR %s' % self.basedir + path, m.update)\n+                ftp.retrbinary('RETR %s' % file_path, m.update)\n                 return {'last_modified': last_modified, 'checksum': m.hexdigest()}\n             # The file doesn't exist\n             except Exception as e :\n@@ -372,6 +368,7 @@ class FilesPipeline(MediaPipeline):\n         ftp_store = cls.STORE_SCHEMES['ftp']\n         ftp_store.FTP_USERNAME = settings['FTP_USER']           # Default is 'anonymous'\n         ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']       # Default is `guest`\n+        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n         \n         store_uri = settings['FILES_STORE']\n         return cls(store_uri, settings=settings)\n\n@@ -1,4 +1,6 @@\n-from ftplib import error_perm\n+import posixpath\n+\n+from ftplib import error_perm, FTP\n from posixpath import dirname\n \n def ftp_makedirs_cwd(ftp, path, first_call=True):\n@@ -13,3 +15,20 @@ def ftp_makedirs_cwd(ftp, path, first_call=True):\n         ftp.mkd(path)\n         if first_call:\n             ftp.cwd(path)\n+\n+def ftp_store_file( \n+        path, file, host ,port,\n+        username, password, use_active_mode=False):\n+    \"\"\"Opens a FTP connection with passed credentials,sets current directory\n+    to the directory extracted from given path, then uploads the file to server\n+    \"\"\"\n+    ftp = FTP()\n+    ftp.connect(host, port)\n+    ftp.login(username, password)\n+    if use_active_mode:\n+        ftp.set_pasv(False)\n+    file.seek(0)\n+    dirname, filename = posixpath.split(path)\n+    ftp_makedirs_cwd(ftp, dirname)\n+    ftp.storbinary('STOR %s' % filename, file)\n+    ftp.quit()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2047124b3573d02ec60b13614f4e3dce85e71546", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 523 | Contributors (this commit): 14 | Commits (past 90d): 9 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -330,6 +330,7 @@ class FilesPipeline(MediaPipeline):\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n+\n         cls_name = \"FilesPipeline\"\n         self.store = self._get_store(store_uri)\n         resolve = functools.partial(self._key_for_pipe,\n@@ -348,6 +349,7 @@ class FilesPipeline(MediaPipeline):\n         self.files_result_field = settings.get(\n             resolve('FILES_RESULT_FIELD'), self.FILES_RESULT_FIELD\n         )\n+\n         super(FilesPipeline, self).__init__(download_func=download_func, settings=settings)\n \n     @classmethod\n@@ -366,8 +368,8 @@ class FilesPipeline(MediaPipeline):\n         gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None\n \n         ftp_store = cls.STORE_SCHEMES['ftp']\n-        ftp_store.FTP_USERNAME = settings['FTP_USER']           # Default is 'anonymous'\n-        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']       # Default is `guest`\n+        ftp_store.FTP_USERNAME = settings['FTP_USER']\n+        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']\n         ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n         \n         store_uri = settings['FILES_STORE']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97d2f717ae30f53282a9cffacc40a879989f05af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 400 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -99,6 +99,11 @@ class ImagesPipeline(FilesPipeline):\n         gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n         gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL'] or None\n \n+        ftp_store = cls.STORE_SCHEMES['ftp']\n+        ftp_store.FTP_USERNAME = settings['FTP_USER']\n+        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']\n+        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n+\n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri, settings=settings)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0fa384e80defaaac88b2f0d5449072f832611431", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 22 | Churn Cumulative: 420 | Contributors (this commit): 18 | Commits (past 90d): 6 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -252,6 +252,10 @@ coverage_ignore_pyobjects = [\n \n     # Private exception used by the command-line interface implementation.\n     r'^scrapy\\.exceptions\\.UsageError',\n+\n+    # Methods of BaseItemExporter subclasses are only documented in\n+    # BaseItemExporter.\n+    r'^scrapy\\.exporters\\.(?!BaseItemExporter\\b)\\w*?\\.',\n ]\n \n \n\n@@ -276,6 +276,13 @@ class PickleItemExporter(BaseItemExporter):\n \n \n class MarshalItemExporter(BaseItemExporter):\n+    \"\"\"Exports items in a Python-specific binary format (see\n+    :mod:`marshal`).\n+\n+    :param file: The file-like object to use for exporting the data. Its\n+                 ``write`` method should accept :class:`bytes` (a disk file\n+                 opened in binary mode, a :class:`~io.BytesIO` object, etc)\n+    \"\"\"\n \n     def __init__(self, file, **kwargs):\n         self._configure(kwargs)\n@@ -297,10 +304,13 @@ class PprintItemExporter(BaseItemExporter):\n \n \n class PythonItemExporter(BaseItemExporter):\n-    \"\"\"The idea behind this exporter is to have a mechanism to serialize items\n-    to built-in python types so any serialization library (like\n-    json, msgpack, binc, etc) can be used on top of it. Its main goal is to\n-    seamless support what BaseItemExporter does plus nested items.\n+    \"\"\"This is a base class for item exporters that extends\n+    :class:`BaseItemExporter` with support for nested items.\n+\n+    It serializes items to built-in Python types, so that any serialization\n+    library (e.g. :mod:`json` or msgpack_) can be used on top of it.\n+\n+    .. _msgpack: https://pypi.org/project/msgpack/\n     \"\"\"\n     def _configure(self, options, dont_fail=False):\n         self.binary = options.pop('binary', True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a7b949d6d8058cdc342dba683bc054d95ed6ccd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 79 | Churn Cumulative: 333 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,21 @@ from scrapy.utils.python import to_native_str, to_unicode\n \n logger = logging.getLogger(__name__)\n \n+def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n+    try:\n+        if to_native_str_type:\n+            robotstxt_body = to_native_str(robotstxt_body)\n+        else:\n+            robotstxt_body = robotstxt_body.decode('utf-8')\n+    except UnicodeDecodeError:\n+        # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n+        # Switch to 'allow all' state.\n+        logger.warning(\"Failure while parsing robots.txt. \"\n+                       \"File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.\",\n+                       exc_info=sys.exc_info(),\n+                       extra={'spider': spider})\n+        robotstxt_body = ''\n+    return robotstxt_body\n \n class RobotParser(with_metaclass(ABCMeta)):\n     @classmethod\n@@ -40,17 +55,7 @@ class PythonRobotParser(RobotParser):\n     def __init__(self, robotstxt_body, spider):\n         from six.moves.urllib_robotparser import RobotFileParser\n         self.spider = spider\n-        try:\n-            robotstxt_body = to_native_str(robotstxt_body)\n-        except UnicodeDecodeError:\n-            # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n-            # Switch to 'allow all' state.\n-            logger.warning(\"Failure while parsing robots.txt using %(parser)s.\"\n-                           \" File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.\",\n-                           {'parser': \"RobotFileParser\"},\n-                           exc_info=sys.exc_info(),\n-                           extra={'spider': self.spider})\n-            robotstxt_body = ''\n+        robotstxt_body = decode_robotstxt(robotstxt_body, spider, to_native_str_type=True)\n         self.rp = RobotFileParser()\n         self.rp.parse(robotstxt_body.splitlines())\n \n@@ -87,17 +92,7 @@ class RerpRobotParser(RobotParser):\n         from robotexclusionrulesparser import RobotExclusionRulesParser\n         self.spider = spider\n         self.rp = RobotExclusionRulesParser()\n-        try:\n-            robotstxt_body = robotstxt_body.decode('utf-8')\n-        except UnicodeDecodeError:\n-            # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n-            # Switch to 'allow all' state.\n-            logger.warning(\"Failure while parsing robots.txt using %(parser)s.\"\n-                           \" File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.\",\n-                           {'parser': \"RobotExclusionRulesParser\"},\n-                           exc_info=sys.exc_info(),\n-                           extra={'spider': self.spider})\n-            robotstxt_body = ''\n+        robotstxt_body = decode_robotstxt(robotstxt_body, spider)\n         self.rp.parse(robotstxt_body)\n \n     @classmethod\n@@ -110,3 +105,22 @@ class RerpRobotParser(RobotParser):\n         user_agent = to_unicode(user_agent)\n         url = to_unicode(url)\n         return self.rp.is_allowed(user_agent, url)\n+\n+\n+class ProtegoRobotParser(RobotParser):\n+    def __init__(self, robotstxt_body, spider):\n+        from protego import Protego\n+        self.spider = spider\n+        robotstxt_body = decode_robotstxt(robotstxt_body, spider)\n+        self.rp = Protego.parse(robotstxt_body)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, robotstxt_body):\n+        spider = None if not crawler else crawler.spider\n+        o = cls(robotstxt_body, spider)\n+        return o\n+\n+    def allowed(self, url, user_agent):\n+        user_agent = to_unicode(user_agent)\n+        url = to_unicode(url)\n+        return self.rp.can_fetch(url, user_agent)\n\n@@ -20,6 +20,13 @@ def rerp_available():\n         return False\n     return True\n \n+def protego_available():\n+    # check if protego parser is installed\n+    try:\n+        from protego import Protego\n+    except ImportError:\n+        return False\n+    return True\n \n class BaseRobotParserTest:\n     def _setUp(self, parser_cls):\n@@ -127,7 +134,7 @@ class ReppyRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n         super(ReppyRobotParserTest, self)._setUp(ReppyRobotParser)\n \n     def test_order_based_precedence(self):\n-        raise unittest.SkipTest(\"Rerp does not support order based directives precedence.\")\n+        raise unittest.SkipTest(\"Reppy does not support order based directives precedence.\")\n \n \n class RerpRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n@@ -140,3 +147,15 @@ class RerpRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n \n     def test_length_based_precedence(self):\n         raise unittest.SkipTest(\"Rerp does not support length based directives precedence.\")\n+\n+\n+class ProtegoRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n+    if not protego_available():\n+        skip = \"Protego parser is not installed\"\n+\n+    def setUp(self):\n+        from scrapy.robotstxt import ProtegoRobotParser\n+        super(ProtegoRobotParserTest, self)._setUp(ProtegoRobotParser)\n+\n+    def test_order_based_precedence(self):\n+        raise unittest.SkipTest(\"Protego does not support order based directives precedence.\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b6b76df0574d65e053eb2e0aba5dd757a4907c24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 1124 | Contributors (this commit): 50 | Commits (past 90d): 8 | Contributors (cumulative): 59 | DMM Complexity: 1.0\n\nDIFF:\n@@ -92,7 +92,7 @@ class ContractsManager(object):\n         @wraps(cb)\n         def cb_wrapper(response):\n             try:\n-                output = cb(response)\n+                output = cb(response, **request.cb_kwargs)\n                 output = list(iterate_spider_output(output))\n             except Exception:\n                 case = _create_testcase(method, 'callback')\n@@ -133,7 +133,7 @@ class Contract(object):\n                 else:\n                     results.addSuccess(self.testcase_pre)\n                 finally:\n-                    return list(iterate_spider_output(cb(response)))\n+                    return list(iterate_spider_output(cb(response, **request.cb_kwargs)))\n \n             request.callback = wrapper\n \n@@ -145,7 +145,7 @@ class Contract(object):\n \n             @wraps(cb)\n             def wrapper(response):\n-                output = list(iterate_spider_output(cb(response)))\n+                output = list(iterate_spider_output(cb(response, **request.cb_kwargs)))\n                 try:\n                     results.startTest(self.testcase_post)\n                     self.post_process(output)\n\n@@ -1,3 +1,5 @@\n+import json\n+\n from scrapy.item import BaseItem\n from scrapy.http import Request\n from scrapy.exceptions import ContractFail\n@@ -18,6 +20,20 @@ class UrlContract(Contract):\n         return args\n \n \n+class CallbackKeywordArgumentsContract(Contract):\n+    \"\"\" Contract to set the keyword arguments for the request.\n+        The value should be a JSON-encoded dictionary, e.g.:\n+\n+        @cb_kwargs {\"arg1\": \"some value\"}\n+    \"\"\"\n+\n+    name = 'cb_kwargs'\n+\n+    def adjust_request_args(self, args):\n+        args['cb_kwargs'] = json.loads(' '.join(self.args))\n+        return args\n+\n+\n class ReturnsContract(Contract):\n     \"\"\" Contract to check the output of a callback\n \n\n@@ -291,6 +291,7 @@ TELNETCONSOLE_PASSWORD = None\n SPIDER_CONTRACTS = {}\n SPIDER_CONTRACTS_BASE = {\n     'scrapy.contracts.default.UrlContract': 1,\n-    'scrapy.contracts.default.ReturnsContract': 2,\n-    'scrapy.contracts.default.ScrapesContract': 3,\n+    'scrapy.contracts.default.CallbackKeywordArgumentsContract': 2,\n+    'scrapy.contracts.default.ReturnsContract': 3,\n+    'scrapy.contracts.default.ScrapesContract': 4,\n }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97a7d775f7021e485a88d4becb363c6e49ca3bb4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 386 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -90,9 +90,9 @@ class ContractsManager(object):\n         cb = request.callback\n \n         @wraps(cb)\n-        def cb_wrapper(response):\n+        def cb_wrapper(response, **cb_kwargs):\n             try:\n-                output = cb(response, **request.cb_kwargs)\n+                output = cb(response, **cb_kwargs)\n                 output = list(iterate_spider_output(output))\n             except Exception:\n                 case = _create_testcase(method, 'callback')\n@@ -121,7 +121,7 @@ class Contract(object):\n             cb = request.callback\n \n             @wraps(cb)\n-            def wrapper(response):\n+            def wrapper(response, **cb_kwargs):\n                 try:\n                     results.startTest(self.testcase_pre)\n                     self.pre_process(response)\n@@ -133,7 +133,7 @@ class Contract(object):\n                 else:\n                     results.addSuccess(self.testcase_pre)\n                 finally:\n-                    return list(iterate_spider_output(cb(response, **request.cb_kwargs)))\n+                    return list(iterate_spider_output(cb(response, **cb_kwargs)))\n \n             request.callback = wrapper\n \n@@ -144,8 +144,8 @@ class Contract(object):\n             cb = request.callback\n \n             @wraps(cb)\n-            def wrapper(response):\n-                output = list(iterate_spider_output(cb(response, **request.cb_kwargs)))\n+            def wrapper(response, **cb_kwargs):\n+                output = list(iterate_spider_output(cb(response, **cb_kwargs)))\n                 try:\n                     results.startTest(self.testcase_post)\n                     self.post_process(output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#110bc92e6b9c9c3ce1775a9ca1487df42a80d219", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 22 | Churn Cumulative: 2003 | Contributors (this commit): 53 | Commits (past 90d): 13 | Contributors (cumulative): 82 | DMM Complexity: 1.0\n\nDIFF:\n@@ -160,7 +160,7 @@ class GCSFeedStorage(BlockingFeedStorage):\n         return cls(\n             uri,\n             crawler.settings['GCS_PROJECT_ID'],\n-            crawler.settings['FEED_STORAGE_GCS_ACL']\n+            crawler.settings['FEED_STORAGE_GCS_ACL'] or None\n         )\n \n     def _store_in_thread(self, file):\n\n@@ -159,7 +159,7 @@ FEED_EXPORTERS_BASE = {\n }\n FEED_EXPORT_INDENT = 0\n \n-FEED_STORAGE_GCS_ACL = None\n+FEED_STORAGE_GCS_ACL = ''\n \n FILES_STORE_S3_ACL = 'private'\n FILES_STORE_GCS_ACL = ''\n\n@@ -206,6 +206,24 @@ class GCSFeedStorageTest(unittest.TestCase):\n         assert storage.bucket_name == 'mybucket'\n         assert storage.blob_name == 'export.csv'\n \n+    @mock.patch('scrapy.conf.settings',\n+                new={'GCS_PROJECT_ID': 'conf_id', 'FEED_STORAGE_GCS_ACL': '' }, create=True)\n+    def test_parse_empty_acl(self):\n+        try:\n+            from google.cloud.storage import Client\n+        except ImportError:\n+            raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n+\n+        settings = {'GCS_PROJECT_ID': '123', 'FEED_STORAGE_GCS_ACL': '' }\n+        crawler = get_crawler(settings_dict=settings)\n+        storage = GCSFeedStorage.from_crawler(crawler, 'gs://mybucket/export.csv')\n+        assert storage.acl is None\n+\n+        settings = {'GCS_PROJECT_ID': '123', 'FEED_STORAGE_GCS_ACL': None }\n+        crawler = get_crawler(settings_dict=settings)\n+        storage = GCSFeedStorage.from_crawler(crawler, 'gs://mybucket/export.csv')\n+        assert storage.acl is None\n+\n     @defer.inlineCallbacks\n     def test_store(self):\n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb0bd2daef8d91e6386390975b2e75d1a93b3161", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 574 | Contributors (this commit): 48 | Commits (past 90d): 7 | Contributors (cumulative): 48 | DMM Complexity: None\n\nDIFF:\n@@ -291,7 +291,7 @@ TELNETCONSOLE_PASSWORD = None\n SPIDER_CONTRACTS = {}\n SPIDER_CONTRACTS_BASE = {\n     'scrapy.contracts.default.UrlContract': 1,\n-    'scrapy.contracts.default.CallbackKeywordArgumentsContract': 2,\n-    'scrapy.contracts.default.ReturnsContract': 3,\n-    'scrapy.contracts.default.ScrapesContract': 4,\n+    'scrapy.contracts.default.CallbackKeywordArgumentsContract': 1,\n+    'scrapy.contracts.default.ReturnsContract': 2,\n+    'scrapy.contracts.default.ScrapesContract': 3,\n }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ace2df3d140ac783c622ea5403876fe76af2bb72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 1340 | Contributors (this commit): 20 | Commits (past 90d): 6 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ from scrapy.http.headers import Headers\n from scrapy.http.request import Request\n from scrapy.http.request.form import FormRequest\n from scrapy.http.request.rpc import XmlRpcRequest\n-from scrapy.http.request.json_request import JSONRequest\n+from scrapy.http.request.json_request import JsonRequest\n \n from scrapy.http.response import Response\n from scrapy.http.response.html import HtmlResponse\n\n@@ -1,5 +1,5 @@\n \"\"\"\n-This module implements the JSONRequest class which is a more convenient class\n+This module implements the JsonRequest class which is a more convenient class\n (than Request) to generate JSON Requests.\n \n See documentation in docs/topics/request-response.rst\n@@ -10,9 +10,10 @@ import json\n import warnings\n \n from scrapy.http.request import Request\n+from scrapy.utils.deprecate import create_deprecated_class\n \n \n-class JSONRequest(Request):\n+class JsonRequest(Request):\n     def __init__(self, *args, **kwargs):\n         dumps_kwargs = copy.deepcopy(kwargs.pop('dumps_kwargs', {}))\n         dumps_kwargs.setdefault('sort_keys', True)\n@@ -31,7 +32,7 @@ class JSONRequest(Request):\n             if 'method' not in kwargs:\n                 kwargs['method'] = 'POST'\n \n-        super(JSONRequest, self).__init__(*args, **kwargs)\n+        super(JsonRequest, self).__init__(*args, **kwargs)\n         self.headers.setdefault('Content-Type', 'application/json')\n         self.headers.setdefault('Accept', 'application/json, text/javascript, */*; q=0.01')\n \n@@ -46,8 +47,11 @@ class JSONRequest(Request):\n         elif not body_passed and data_passed:\n             kwargs['body'] = self._dumps(data)\n \n-        return super(JSONRequest, self).replace(*args, **kwargs)\n+        return super(JsonRequest, self).replace(*args, **kwargs)\n \n     def _dumps(self, data):\n         \"\"\"Convert to JSON \"\"\"\n         return json.dumps(data, **self._dumps_kwargs)\n+\n+\n+JSONRequest = create_deprecated_class(\"JSONRequest\", JsonRequest)\n\n@@ -11,7 +11,7 @@ from six.moves.urllib.parse import urlparse, parse_qs, unquote\n if six.PY3:\n     from urllib.parse import unquote_to_bytes\n \n-from scrapy.http import Request, FormRequest, XmlRpcRequest, JSONRequest, Headers, HtmlResponse\n+from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n from scrapy.utils.python import to_bytes, to_native_str\n \n from tests import mock\n@@ -1246,14 +1246,14 @@ class XmlRpcRequestTest(RequestTest):\n         self._test_request(params=(u'pas£',), encoding='latin1')\n \n \n-class JSONRequestTest(RequestTest):\n-    request_class = JSONRequest\n+class JsonRequestTest(RequestTest):\n+    request_class = JsonRequest\n     default_method = 'GET'\n     default_headers = {b'Content-Type': [b'application/json'], b'Accept': [b'application/json, text/javascript, */*; q=0.01']}\n \n     def setUp(self):\n         warnings.simplefilter(\"always\")\n-        super(JSONRequestTest, self).setUp()\n+        super(JsonRequestTest, self).setUp()\n \n     def test_data(self):\n         r1 = self.request_class(url=\"http://www.example.com/\")\n@@ -1407,7 +1407,7 @@ class JSONRequestTest(RequestTest):\n \n     def tearDown(self):\n         warnings.resetwarnings()\n-        super(JSONRequestTest, self).tearDown()\n+        super(JsonRequestTest, self).tearDown()\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2828cb769f0618d3986cd12c2ef0f8143f31bfba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 157 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -256,6 +256,12 @@ coverage_ignore_pyobjects = [\n     # Methods of BaseItemExporter subclasses are only documented in\n     # BaseItemExporter.\n     r'^scrapy\\.exporters\\.(?!BaseItemExporter\\b)\\w*?\\.',\n+\n+    # Extension behavior is only modified through settings. Methods of\n+    # extension classes, as well as helper functions, are implementation\n+    # details that are not documented.\n+    r'^scrapy\\.extensions\\.[a-z]\\w*?\\.[A-Z]\\w*?\\.',  # methods\n+    r'^scrapy\\.extensions\\.[a-z]\\w*?\\.[a-z]',  # helper functions\n ]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b92b1146335f6b651ba8e6e56e579a6844ec3ebd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 78 | Churn Cumulative: 372 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,7 @@ from scrapy.item import Item, Field\n from scrapy.contracts import ContractsManager, Contract\n from scrapy.contracts.default import (\n     UrlContract,\n+    CallbackKeywordArgumentsContract,\n     ReturnsContract,\n     ScrapesContract,\n )\n@@ -70,6 +71,37 @@ class TestSpider(Spider):\n         \"\"\"\n         return TestItem(url=response.url)\n \n+    def returns_request_cb_kwargs(self, response, url):\n+        \"\"\" method which returns request\n+        @url https://example.org\n+        @cb_kwargs {\"url\": \"http://scrapy.org\"}\n+        @returns requests 1\n+        \"\"\"\n+        return Request(url, callback=self.returns_item_cb_kwargs)\n+\n+    def returns_item_cb_kwargs(self, response, name):\n+        \"\"\" method which returns item\n+        @url http://scrapy.org\n+        @cb_kwargs {\"name\": \"Scrapy\"}\n+        @returns items 1 1\n+        \"\"\"\n+        return TestItem(name=name, url=response.url)\n+\n+    def returns_item_cb_kwargs_error_unexpected_keyword(self, response):\n+        \"\"\" method which returns item\n+        @url http://scrapy.org\n+        @cb_kwargs {\"arg\": \"value\"}\n+        @returns items 1 1\n+        \"\"\"\n+        return TestItem(url=response.url)\n+\n+    def returns_item_cb_kwargs_error_missing_argument(self, response, arg):\n+        \"\"\" method which returns item\n+        @url http://scrapy.org\n+        @returns items 1 1\n+        \"\"\"\n+        return TestItem(url=response.url)\n+\n     def returns_dict_item(self, response):\n         \"\"\" method which returns item\n         @url http://scrapy.org\n@@ -172,6 +204,7 @@ class InheritsTestSpider(TestSpider):\n class ContractsManagerTest(unittest.TestCase):\n     contracts = [\n         UrlContract,\n+        CallbackKeywordArgumentsContract,\n         ReturnsContract,\n         ScrapesContract,\n         CustomFormContract,\n@@ -211,6 +244,51 @@ class ContractsManagerTest(unittest.TestCase):\n         request = self.conman.from_method(spider.parse_no_url, self.results)\n         self.assertEqual(request, None)\n \n+    def test_cb_kwargs(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+\n+        # extract contracts correctly\n+        contracts = self.conman.extract_contracts(spider.returns_request_cb_kwargs)\n+        self.assertEqual(len(contracts), 3)\n+        self.assertEqual(frozenset(type(x) for x in contracts),\n+                         frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]))\n+        \n+        contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs)\n+        self.assertEqual(len(contracts), 3)\n+        self.assertEqual(frozenset(type(x) for x in contracts),\n+                         frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]))\n+\n+        contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs_error_unexpected_keyword)\n+        self.assertEqual(len(contracts), 3)\n+        self.assertEqual(frozenset(type(x) for x in contracts),\n+                         frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]))\n+\n+        contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs_error_missing_argument)\n+        self.assertEqual(len(contracts), 2)\n+        self.assertEqual(frozenset(type(x) for x in contracts),\n+                         frozenset([UrlContract, ReturnsContract]))\n+\n+        # returns_request\n+        request = self.conman.from_method(spider.returns_request_cb_kwargs, self.results)\n+        request.callback(response, **request.cb_kwargs)\n+        self.should_succeed()\n+\n+        # returns_item\n+        request = self.conman.from_method(spider.returns_item_cb_kwargs, self.results)\n+        request.callback(response, **request.cb_kwargs)\n+        self.should_succeed()\n+\n+        # returns_item (error, callback doesn't take keyword arguments)\n+        request = self.conman.from_method(spider.returns_item_cb_kwargs_error_unexpected_keyword, self.results)\n+        request.callback(response, **request.cb_kwargs)\n+        self.should_error()\n+\n+        # returns_item (error, contract doesn't provide keyword arguments)\n+        request = self.conman.from_method(spider.returns_item_cb_kwargs_error_missing_argument, self.results)\n+        request.callback(response, **request.cb_kwargs)\n+        self.should_error()\n+\n     def test_returns(self):\n         spider = TestSpider()\n         response = ResponseMock()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
