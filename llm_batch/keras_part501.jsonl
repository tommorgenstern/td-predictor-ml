{"custom_id": "keras#2bbb2ea49609bde79bfcebbe242d0ec8eab1527f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 70 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1132,18 +1132,18 @@ def revive_custom_object(identifier, metadata):\n     }\n     parent_classes = revived_classes.get(identifier, None)\n \n+    class_name = tf.compat.as_str(metadata[\"class_name\"])\n     if parent_classes is not None:\n         parent_classes = revived_classes[identifier]\n-        revived_cls = type(\n-            tf.compat.as_str(metadata[\"class_name\"]), parent_classes, {}\n-        )\n+        revived_cls = type(class_name, parent_classes, {})\n         return revived_cls._init_from_metadata(metadata)\n     else:\n         raise ValueError(\n-            f\"Unable to restore custom object of type {identifier}. \"\n-            \"Please make sure that any custom layers are included in the \"\n-            \"`custom_objects` arg when calling `load_model()` and make sure \"\n-            \"that all layers implement `get_config` and `from_config`.\"\n+            f'Unable to restore custom object of class \"{class_name}\" '\n+            f\"(type {identifier}). Please make sure that this class is \"\n+            \"included in the `custom_objects` arg when calling `load_model()`. \"\n+            \"Also, check that the class implements `get_config` and \"\n+            f\"`from_config`.\\n\\nComplete metadata: {metadata}\"\n         )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b8240ece60fced29c0e47970219c064bed18ae83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1586 | Contributors (this commit): 22 | Commits (past 90d): 3 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -63,6 +63,7 @@ from keras.layers.core.activation import Activation\n from keras.layers.core.dense import Dense\n from keras.layers.core.einsum_dense import EinsumDense\n from keras.layers.core.embedding import Embedding\n+from keras.layers.core.identity import Identity\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n from keras.layers.core.tf_op_layer import ClassMethod\n\n@@ -18,6 +18,7 @@ from keras.layers.core.activation import Activation\n from keras.layers.core.dense import Dense\n from keras.layers.core.einsum_dense import EinsumDense\n from keras.layers.core.embedding import Embedding\n+from keras.layers.core.identity import Identity\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50e0b18ceeb370157f36a387115c5ae1a16fca47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 21 | Files Changed: 8 | Hunks: 20 | Methods Changed: 14 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 52 | Churn Cumulative: 6527 | Contributors (this commit): 13 | Commits (past 90d): 40 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3027,7 +3027,7 @@ class TestModelCapturesStrategy(tf.test.TestCase, parameterized.TestCase):\n                     keras.layers.Dense(1),\n                 ]\n             )\n-            model.compile(optimizer=\"adam\", loss=\"mse\")\n+            model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n             model.build([None, 1])  # create weights.\n             return model\n \n@@ -3041,11 +3041,12 @@ class TestModelCapturesStrategy(tf.test.TestCase, parameterized.TestCase):\n             model.load_weights(temp_dir)\n             if isinstance(model.optimizer, optimizer_base.Optimizer):\n                 model.optimizer.build(model.trainable_variables)\n-            self.assertNotEmpty(model.optimizer.variables())\n+                variables = model.optimizer.variables\n+            else:\n+                variables = model.optimizer.variables()\n+            self.assertNotEmpty(variables)\n             self.assertTrue(\n-                distributed_training_utils.is_distributed_variable(\n-                    model.optimizer.variables()[0]\n-                )\n+                distributed_training_utils.is_distributed_variable(variables[0])\n             )\n \n         with distribution.scope():\n@@ -3053,8 +3054,9 @@ class TestModelCapturesStrategy(tf.test.TestCase, parameterized.TestCase):\n         # create/restore slot variables outside of scope is fine.\n         model.load_weights(temp_dir)\n         if isinstance(model.optimizer, optimizer_base.Optimizer):\n-            # Experimental optimizer has to restore variables in scope.\n+            # V3 optimizer has to restore variables in scope.\n             return\n+        # From this point on, the optimizer must be a V2 optimizer.\n         self.assertNotEmpty(model.optimizer.variables())\n         self.assertTrue(\n             distributed_training_utils.is_distributed_variable(\n\n@@ -127,7 +127,7 @@ class OptimizersTest(test_util.DTensorBaseTest):\n \n         grads = tf.ones_like(variable_init_value)\n         optimizer.apply_gradients(zip([grads], [model_variable]))\n-        optimizer_variables = optimizer.variables()\n+        optimizer_variables = optimizer.variables\n \n         self.assertEqual(self.evaluate(optimizer.iterations), 1)\n \n\n@@ -770,9 +770,10 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n                 )\n         return cls(**config)\n \n+    @property\n     def variables(self):\n         \"\"\"Returns variables of this optimizer.\"\"\"\n-        return self._variables\n+        return CallableList(self._variables)\n \n     def set_weights(self, weights):\n         \"\"\"Set the weights of the optimizer.\n@@ -801,12 +802,12 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n \n     def _save_own_variables(self, store):\n         \"\"\"Get the state of this optimizer object.\"\"\"\n-        for i, variable in enumerate(self.variables()):\n+        for i, variable in enumerate(self.variables):\n             store[str(i)] = variable.numpy()\n \n     def _load_own_variables(self, store):\n         \"\"\"Set the state of this optimizer object.\"\"\"\n-        for i, variable in enumerate(self.variables()):\n+        for i, variable in enumerate(self.variables):\n             variable.assign(store[str(i)])\n \n \n@@ -1248,6 +1249,13 @@ class RestoredOptimizer(Optimizer):\n         )\n \n \n+class CallableList(list):\n+    \"\"\"Temporary shim to support both `opt.variables()` and `opt.variables`.\"\"\"\n+\n+    def __call__(self):\n+        return self\n+\n+\n # Register the optimizer for loading from saved_model purpose.\n tf.__internal__.saved_model.load.register_revived_type(\n     \"experimentalOptimizer\",\n\n@@ -86,7 +86,7 @@ class OptimizerPssTest(tf.test.TestCase, parameterized.TestCase):\n         return dataset_fn\n \n     def _verify_accumulators_updated(self, optimizer):\n-        variables = optimizer.variables()\n+        variables = optimizer.variables\n         for var in variables:\n             if \"iteration\" not in var.name and \"learning_rate\" not in var.name:\n                 # Find a variable not iteration or learning_rate, and verify its\n\n@@ -221,7 +221,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         optimizer = adam_new.Adam()\n         grads = tf.convert_to_tensor([[1.0, 2.0], [3.0, 4.0]])\n         optimizer.apply_gradients(zip([grads], [x]))\n-        optimizer_variables = optimizer.variables()\n+        optimizer_variables = optimizer.variables\n         all_names = [var._shared_name for var in optimizer_variables]\n         self.assertLen(optimizer_variables, 3)\n         self.assertCountEqual(\n@@ -240,10 +240,10 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         optimizer_1.apply_gradients(zip([grads], [x]))\n         optimizer_2 = adam_new.Adam()\n         with self.assertRaisesRegex(ValueError, \"You are calling*\"):\n-            optimizer_2.set_weights(optimizer_1.variables())\n+            optimizer_2.set_weights(optimizer_1.variables)\n         optimizer_2.build([x])\n-        optimizer_2.set_weights(optimizer_1.variables())\n-        self.assertAllClose(optimizer_1.variables(), optimizer_2.variables())\n+        optimizer_2.set_weights(optimizer_1.variables)\n+        self.assertAllClose(optimizer_1.variables, optimizer_2.variables)\n \n     def testSetLearningRate(self):\n         optimizer = adam_new.Adam(learning_rate=1.0)\n@@ -501,7 +501,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertEqual(type(optimizer), type(loaded_optimizer))\n         self.assertEqual(loaded_optimizer.learning_rate, 0.002)\n         self.assertEqual(loaded_optimizer.clipnorm, 0.1)\n-        self.assertAllClose(optimizer.variables(), loaded_optimizer.variables())\n+        self.assertAllClose(optimizer.variables, loaded_optimizer.variables)\n \n         # Save in Keras SavedModel format.\n         model.fit(x, y)\n@@ -514,7 +514,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertEqual(loaded_optimizer.learning_rate, 0.002)\n         self.assertEqual(loaded_optimizer.clipnorm, 0.1)\n         loaded_optimizer.build(loaded_model.trainable_variables)\n-        self.assertAllClose(optimizer.variables(), loaded_optimizer.variables())\n+        self.assertAllClose(optimizer.variables, loaded_optimizer.variables)\n \n     @parameterized.product(optimizer_fn=OPTIMIZER_FN)\n     def testSparseGradientsWorkAsExpected(self, optimizer_fn):\n\n@@ -458,7 +458,7 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n         # The optimizer variables are supposed to be the same (between original\n         # and loaded models).\n         for original_weights, loaded_weights in zip(\n-            model.optimizer.variables(), loaded_model.optimizer.variables()\n+            model.optimizer.variables, loaded_model.optimizer.variables\n         ):\n             np.testing.assert_allclose(original_weights, loaded_weights)\n \n\n@@ -665,7 +665,7 @@ def save_optimizer_weights_to_hdf5_group(hdf5_group, optimizer):\n         optimizer: optimizer instance.\n     \"\"\"\n     if isinstance(optimizer, optimizer_base.Optimizer):\n-        symbolic_weights = optimizer.variables()\n+        symbolic_weights = optimizer.variables\n     else:\n         symbolic_weights = getattr(optimizer, \"weights\")\n     if symbolic_weights:\n\n@@ -487,8 +487,8 @@ class TestWholeModelSaving(test_combinations.TestCase):\n             ):\n                 loaded_model.optimizer.build(loaded_model.trainable_variables)\n                 self.assertAllClose(\n-                    model.optimizer.variables(),\n-                    loaded_model.optimizer.variables(),\n+                    model.optimizer.variables,\n+                    loaded_model.optimizer.variables,\n                 )\n             else:\n                 self.assertAllClose(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5711bc6d3bbc1d9ece1f17badbbbbbd2451d9a9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 311 | Lines Deleted: 61 | Files Changed: 17 | Hunks: 50 | Methods Changed: 47 | Complexity Δ (Sum/Max): 93/68 | Churn Δ: 372 | Churn Cumulative: 28150 | Contributors (this commit): 143 | Commits (past 90d): 90 | Contributors (cumulative): 184 | DMM Complexity: 0.84\n\nDIFF:\n@@ -33,7 +33,6 @@ from keras.engine import input_spec\n from keras.engine import node as node_module\n from keras.engine import training as training_lib\n from keras.engine import training_utils\n-from keras.saving.experimental import saving_lib\n from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import json_utils\n from keras.saving.legacy.saved_model import network_serialization\n@@ -793,11 +792,6 @@ class Functional(training_lib.Model):\n             if argspec.varkw != \"kwargs\":\n                 for key in xtra_args - xtra_args.intersection(argspec.args[1:]):\n                     config.pop(key, None)\n-\n-        # Add compile config\n-        if saving_lib.saving_v3_enabled():\n-            if self._is_compiled and hasattr(self, \"_compile_config\"):\n-                config[\"compile_config\"] = self._compile_config.serialize()\n         return config\n \n     def get_weight_paths(self):\n\n@@ -837,8 +837,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"\n         metrics = []\n         if self._is_compiled:\n-            # TODO(omalleyt): Track `LossesContainer` and `MetricsContainer`\n-            # objects so that attr names are not load-bearing.\n             if self.compiled_loss is not None:\n                 metrics += self.compiled_loss.metrics\n             if self.compiled_metrics is not None:\n@@ -3074,7 +3072,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                         \"You can override this default behavior by defining a \"\n                         \"`from_config(cls, config)` class method to specify \"\n                         \"how to create an \"\n-                        f\"instance of {cls.__name__} from the config.\\n\\n\"\n+                        f\"instance of {cls.__name__} from its config.\\n\\n\"\n                         f\"Received config={config}\\n\\n\"\n                         f\"Error encountered during deserialization: {e}\"\n                     )\n@@ -3382,6 +3380,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             return\n         config = saving_lib.deserialize_keras_object(config)\n         self.compile(**config)\n+        if hasattr(self, \"optimizer\") and self.built:\n+            # Create optimizer variables.\n+            self.optimizer.build(self.trainable_variables)\n \n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def _set_save_spec(self, inputs, args=None, kwargs=None):\n@@ -3495,9 +3496,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             # Also make sure to exclude Model class itself which has build()\n             # defined.\n             raise ValueError(\n-                f\"Weights for model {self.name} have not yet been \"\n+                f\"Weights for model '{self.name}' have not yet been \"\n                 \"created. \"\n-                \"Weights are created when the Model is first called on \"\n+                \"Weights are created when the model is first called on \"\n                 \"inputs or `build()` is called with an `input_shape`.\"\n             )\n \n\n@@ -2,6 +2,7 @@ import tensorflow as tf\n from tensorflow import keras\n \n from keras.integration_test.models.input_spec import InputSpec\n+from keras.saving import serialization_lib\n \n IMG_SIZE = (64, 64)\n LATENT_DIM = 128\n@@ -75,6 +76,48 @@ class GAN(keras.Model):\n             \"g_loss\": self.g_loss_metric.result(),\n         }\n \n+    def get_config(self):\n+        return {\n+            \"discriminator\": self.discriminator,\n+            \"generator\": self.generator,\n+            \"latent_dim\": self.latent_dim,\n+        }\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        discriminator = serialization_lib.deserialize_keras_object(\n+            config[\"discriminator\"]\n+        )\n+        generator = serialization_lib.deserialize_keras_object(\n+            config[\"generator\"]\n+        )\n+        latent_dim = config[\"latent_dim\"]\n+        return cls(discriminator, generator, latent_dim)\n+\n+    def get_compile_config(self):\n+        return {\n+            \"loss_fn\": self.loss_fn,\n+            \"d_optimizer\": self.d_optimizer,\n+            \"g_optimizer\": self.g_optimizer,\n+            \"jit_compile\": self.jit_compile,\n+        }\n+\n+    def compile_from_config(self, config):\n+        loss_fn = serialization_lib.deserialize_keras_object(config[\"loss_fn\"])\n+        d_optimizer = serialization_lib.deserialize_keras_object(\n+            config[\"d_optimizer\"]\n+        )\n+        g_optimizer = serialization_lib.deserialize_keras_object(\n+            config[\"g_optimizer\"]\n+        )\n+        jit_compile = config[\"jit_compile\"]\n+        self.compile(\n+            loss_fn=loss_fn,\n+            d_optimizer=d_optimizer,\n+            g_optimizer=g_optimizer,\n+            jit_compile=jit_compile,\n+        )\n+\n \n def get_model(\n     build=False, compile=False, jit_compile=False, include_preprocessing=True\n\n@@ -42,21 +42,23 @@ class LinearA(keras.layers.Layer):\n \n \n class LinearB(keras.layers.Layer):\n-    \"\"\"Layer that tracks weights in a dict attribute.\"\"\"\n+    \"\"\"Layer that tracks weights in a dict attribute that gets updated later.\"\"\"\n \n-    def __init__(self, units=32, input_dim=32):\n-        super().__init__()\n+    def __init__(self, units=32, input_dim=32, **kwargs):\n+        super().__init__(**kwargs)\n         w_init = tf.random_normal_initializer()\n         b_init = tf.zeros_initializer()\n         self.state = {\n             \"kernel\": tf.Variable(\n                 initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n                 trainable=True,\n+                name=\"kernel\",\n             )\n         }\n         self.state[\"bias\"] = tf.Variable(\n             initial_value=b_init(shape=(units,), dtype=\"float32\"),\n             trainable=True,\n+            name=\"bias\",\n         )\n \n     def call(self, inputs):\n@@ -66,8 +68,8 @@ class LinearB(keras.layers.Layer):\n class LinearC(keras.layers.Layer):\n     \"\"\"Layer that creates weights in call().\"\"\"\n \n-    def __init__(self, units=32, input_dim=32):\n-        super().__init__()\n+    def __init__(self, units=32, input_dim=32, **kwargs):\n+        super().__init__(**kwargs)\n         self._custom_built = False\n         self.units = units\n         self.input_dim = input_dim\n@@ -89,8 +91,10 @@ class LinearC(keras.layers.Layer):\n class BatchNorm(keras.layers.Layer):\n     \"\"\"Layer with different training/test behavior and non-trainable updates.\"\"\"\n \n-    def __init__(self, scale=True, center=True, epsilon=1e-6, momentum=0.9):\n-        super().__init__()\n+    def __init__(\n+        self, scale=True, center=True, epsilon=1e-6, momentum=0.9, **kwargs\n+    ):\n+        super().__init__(**kwargs)\n         self.scale = scale\n         self.center = center\n         self.epsilon = epsilon\n@@ -122,7 +126,7 @@ class BatchNorm(keras.layers.Layer):\n \n \n class FunctionalSubclassModel(keras.Model):\n-    def __init__(self):\n+    def __init__(self, **kwargs):\n         inputs = keras.Input((INPUT_DIM,))\n         x = inputs\n         x = LinearA(32, INPUT_DIM)(x, x)\n@@ -130,7 +134,7 @@ class FunctionalSubclassModel(keras.Model):\n         x = LinearC(32, 32)(x)\n         x = BatchNorm()(x)\n         outputs = keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n-        super().__init__(inputs, outputs)\n+        super().__init__(inputs, outputs, **kwargs)\n \n \n def get_model(\n\n@@ -309,3 +309,7 @@ def get_model(\n             \"adam\", loss=\"categorical_crossentropy\", jit_compile=jit_compile\n         )\n     return model\n+\n+\n+def get_custom_objects():\n+    return {}\n\n@@ -29,8 +29,8 @@ def get_input_preprocessor():\n \n \n class Linear(keras.layers.Layer):\n-    def __init__(self, units=32):\n-        super().__init__()\n+    def __init__(self, units=32, name=None):\n+        super().__init__(name=name)\n         self.units = units\n \n     def build(self, input_shape):\n@@ -38,9 +38,13 @@ class Linear(keras.layers.Layer):\n             shape=(input_shape[-1], self.units),\n             initializer=\"random_normal\",\n             trainable=True,\n+            name=\"w\",\n         )\n         self.b = self.add_weight(\n-            shape=(self.units,), initializer=\"random_normal\", trainable=True\n+            shape=(self.units,),\n+            initializer=\"random_normal\",\n+            trainable=True,\n+            name=\"b\",\n         )\n \n     def call(self, inputs):\n@@ -66,7 +70,7 @@ class BinaryTruePositives(tf.keras.metrics.Metric):\n     def result(self):\n         return self.true_positives\n \n-    def reset_states(self):\n+    def reset_state(self):\n         self.true_positives.assign(0)\n \n \n@@ -76,20 +80,14 @@ class CustomModel(keras.Model):\n         self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n         self.btp_metric = BinaryTruePositives(name=\"mae\")\n \n-        self.linear_1 = Linear(32)\n-        self.linear_2 = Linear(NUM_CLASSES)\n+        self.linear_1 = Linear(32, name=\"linear_1\")\n+        self.linear_2 = Linear(NUM_CLASSES, name=\"linear_2\")\n \n     def call(self, inputs, training=False):\n         x = self.linear_1(inputs)\n         x = self.linear_2(x)\n         return x\n \n-    def compile(self, optimizer, loss, jit_compile=False):\n-        self.optimizer = optimizer\n-        self.loss = loss\n-        self.jit_compile = jit_compile\n-        self._is_compiled = True\n-\n     def train_step(self, data):\n         x, y = data\n         with tf.GradientTape() as tape:\n@@ -129,6 +127,11 @@ class CustomLRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n     def __call__(self, step):\n         return self.initial_learning_rate / tf.cast(step + 1, \"float32\")\n \n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+        }\n+\n \n def custom_loss(y_true, y_pred):\n     return keras.losses.mse(y_true, y_pred)\n@@ -138,6 +141,8 @@ def get_model(\n     build=False, compile=False, jit_compile=False, include_preprocessing=True\n ):\n     model = CustomModel()\n+    if build:\n+        model(tf.zeros((1, INPUT_DIM)))\n     if compile:\n         model.compile(\n             optimizer=keras.optimizers.Adam(CustomLRSchedule(0.1)),\n\n@@ -77,4 +77,4 @@ def get_model(\n \n \n def get_custom_objects():\n-    return None\n+    return {}\n\n@@ -81,4 +81,4 @@ def get_model(\n \n \n def get_custom_objects():\n-    return None\n+    return {}\n\n@@ -6,6 +6,7 @@ import tensorflow as tf\n from tensorflow import keras\n \n from keras.integration_test.models.input_spec import InputSpec\n+from keras.saving import serialization_lib\n \n NUM_CLASSES = 10\n IMG_SIZE = (224, 224)\n@@ -121,6 +122,22 @@ class RetinaNet(keras.Model):\n         box_outputs = tf.concat(box_outputs, axis=1)\n         return tf.concat([box_outputs, cls_outputs], axis=-1)\n \n+    def get_config(self):\n+        return {\n+            \"num_classes\": self.num_classes,\n+            \"backbone\": self.fpn.backbone,\n+        }\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        backbone = serialization_lib.deserialize_keras_object(\n+            config.pop(\"backbone\")\n+        )\n+        num_classes = config[\"num_classes\"]\n+        retinanet = cls(num_classes=num_classes, backbone=backbone)\n+        retinanet(tf.zeros((1, 32, 32, 3)))  # Build model\n+        return retinanet\n+\n \n class RetinaNetBoxLoss(keras.losses.Loss):\n     def __init__(self, delta):\n@@ -138,6 +155,9 @@ class RetinaNetBoxLoss(keras.losses.Loss):\n         )\n         return tf.reduce_sum(loss, axis=-1)\n \n+    def get_config(self):\n+        return {\"delta\": self._delta}\n+\n \n class RetinaNetClassificationLoss(keras.losses.Loss):\n     def __init__(self, alpha, gamma):\n@@ -157,6 +177,9 @@ class RetinaNetClassificationLoss(keras.losses.Loss):\n         loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n         return tf.reduce_sum(loss, axis=-1)\n \n+    def get_config(self):\n+        return {\"alpha\": self._alpha, \"gamma\": self._gamma}\n+\n \n class RetinaNetLoss(keras.losses.Loss):\n     def __init__(self, num_classes=80, alpha=0.25, gamma=2.0, delta=1.0):\n@@ -164,6 +187,9 @@ class RetinaNetLoss(keras.losses.Loss):\n         self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n         self._box_loss = RetinaNetBoxLoss(delta)\n         self._num_classes = num_classes\n+        self._alpha = alpha\n+        self._gamma = gamma\n+        self._delta = delta\n \n     def call(self, y_true, y_pred):\n         y_pred = tf.cast(y_pred, dtype=tf.float32)\n@@ -193,6 +219,14 @@ class RetinaNetLoss(keras.losses.Loss):\n         loss = clf_loss + box_loss\n         return loss\n \n+    def get_config(self):\n+        return {\n+            \"num_classes\": self._num_classes,\n+            \"alpha\": self._alpha,\n+            \"gamma\": self._gamma,\n+            \"delta\": self._delta,\n+        }\n+\n \n def get_model(\n     build=False, compile=False, jit_compile=False, include_preprocessing=True\n\n@@ -54,10 +54,7 @@ def get_input_preprocessor():\n \n \n def encode_numerical_feature(feature, name, dataset):\n-    normalizer = keras.layers.Normalization()\n-    feature_ds = dataset.map(lambda x: x[name])\n-    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n-    normalizer.adapt(feature_ds)\n+    normalizer = keras.layers.Normalization(mean=[1.0], variance=[2.0])\n     encoded_feature = normalizer(feature)\n     return encoded_feature\n \n@@ -100,4 +97,4 @@ def get_model(\n \n \n def get_custom_objects():\n-    return None\n+    return {}\n\n@@ -26,6 +26,8 @@ def get_model(\n             keras.layers.Dense(1),\n         ]\n     )\n+    if build:\n+        model.build((None, TIMESTEPS, 1))\n     if compile:\n         model.compile(\n             optimizer=keras.optimizers.Adam(),\n@@ -36,4 +38,4 @@ def get_model(\n \n \n def get_custom_objects():\n-    return None\n+    return {}\n\n@@ -7,6 +7,7 @@ import tensorflow as tf\n from tensorflow import keras\n \n from keras.integration_test.models.input_spec import InputSpec\n+from keras.saving import serialization_lib\n \n IMG_SIZE = (28, 28)\n LATENT_DIM = 64\n@@ -70,6 +71,24 @@ class VAE(keras.Model):\n             \"kl_loss\": self.kl_loss_tracker.result(),\n         }\n \n+    def get_config(self):\n+        base_config = super().get_config()\n+        return {\n+            \"encoder\": self.encoder,\n+            \"decoder\": self.decoder,\n+            **base_config,\n+        }\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        encoder = serialization_lib.deserialize_keras_object(\n+            config.pop(\"encoder\")\n+        )\n+        decoder = serialization_lib.deserialize_keras_object(\n+            config.pop(\"decoder\")\n+        )\n+        return cls(encoder, decoder, **config)\n+\n \n def get_data_spec(batch_size):\n     return InputSpec((batch_size,) + IMG_SIZE + (1,))\n\n@@ -0,0 +1,130 @@\n+\"\"\"Test Model.fit across a diverse range of models.\"\"\"\n+\n+import os\n+\n+import tensorflow.compat.v2 as tf\n+from absl.testing import parameterized\n+\n+from keras.integration_test.models import bert\n+from keras.integration_test.models import dcgan\n+from keras.integration_test.models import edge_case_model\n+from keras.integration_test.models import input_spec\n+from keras.integration_test.models import low_level_model\n+from keras.integration_test.models import mini_unet\n+from keras.integration_test.models import mini_xception\n+from keras.integration_test.models import retinanet\n+from keras.integration_test.models import structured_data_classification\n+from keras.integration_test.models import text_classification\n+from keras.integration_test.models import timeseries_forecasting\n+from keras.integration_test.models import vae\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n+\n+\n+def get_dataset(data_specs, batch_size):\n+    values = tf.nest.map_structure(input_spec.spec_to_value, data_specs)\n+    dataset = (\n+        tf.data.Dataset.from_tensor_slices(values)\n+        .prefetch(batch_size * 2)\n+        .batch(batch_size)\n+    )\n+    return dataset\n+\n+\n+@test_utils.run_v2_only\n+class SavingV3Test(test_combinations.TestCase):\n+    @parameterized.named_parameters(\n+        (\"bert\", bert),\n+        (\"edge_case_model\", edge_case_model),\n+        # (\"efficientnet_v2\", efficientnet_v2),  # Too expensive to run on CI\n+        (\"low_level_model\", low_level_model),\n+        (\"mini_unet\", mini_unet),\n+        (\"mini_xception\", mini_xception),\n+        (\"retinanet\", retinanet),\n+        (\"structured_data_classification\", structured_data_classification),\n+        (\"text_classification\", text_classification),\n+        (\"timeseries_forecasting\", timeseries_forecasting),\n+    )\n+    def test_saving_v3(self, module):\n+        batch_size = 2\n+        data_specs = module.get_data_spec(batch_size * 2)\n+        dataset = get_dataset(data_specs, batch_size)\n+        for batch in dataset.take(1):\n+            pass\n+        if isinstance(batch, tuple):\n+            batch = batch[0]\n+\n+        model = module.get_model(\n+            build=True,\n+            compile=True,\n+            jit_compile=False,\n+            include_preprocessing=True,\n+        )\n+        model.fit(dataset, epochs=1, steps_per_epoch=1)\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), f\"{module.__name__}.keras\"\n+        )\n+        model.save(temp_filepath, save_format=\"keras_v3\")\n+        with tf.keras.utils.custom_object_scope(module.get_custom_objects()):\n+            new_model = tf.keras.models.load_model(temp_filepath)\n+\n+        # Test model weights\n+        self.assertIs(new_model.__class__, model.__class__)\n+        self.assertEqual(len(model.get_weights()), len(new_model.get_weights()))\n+        for w1, w2 in zip(model.get_weights(), new_model.get_weights()):\n+            if w1.dtype == \"object\":\n+                self.assertEqual(str(w1), str(w2))\n+            else:\n+                self.assertAllClose(w1, w2, atol=1e-6)\n+\n+        # Test forward pass\n+        self.assertAllClose(new_model(batch), model(batch), atol=1e-6)\n+\n+        # Test optimizer state\n+        if hasattr(model, \"optimizer\"):\n+            self.assertEqual(\n+                len(model.optimizer.variables()),\n+                len(new_model.optimizer.variables()),\n+            )\n+            for v1, v2 in zip(\n+                model.optimizer.variables(), new_model.optimizer.variables()\n+            ):\n+                self.assertAllClose(v1.numpy(), v2.numpy(), atol=1e-6)\n+\n+        # Test training still works\n+        new_model.fit(dataset, epochs=1, steps_per_epoch=1)\n+\n+    @parameterized.named_parameters((\"dcgan\", dcgan), (\"vae\", vae))\n+    def test_saving_v3_no_call(self, module):\n+        batch_size = 2\n+        data_specs = module.get_data_spec(batch_size * 2)\n+        dataset = get_dataset(data_specs, batch_size)\n+\n+        model = module.get_model(\n+            build=True,\n+            compile=True,\n+            jit_compile=False,\n+            include_preprocessing=True,\n+        )\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), f\"{module.__name__}.keras\"\n+        )\n+        model.save(temp_filepath, save_format=\"keras_v3\")\n+        with tf.keras.utils.custom_object_scope(module.get_custom_objects()):\n+            new_model = tf.keras.models.load_model(temp_filepath)\n+\n+        # Test model weights\n+        self.assertIs(new_model.__class__, model.__class__)\n+        self.assertEqual(len(model.get_weights()), len(new_model.get_weights()))\n+        for w1, w2 in zip(model.get_weights(), new_model.get_weights()):\n+            if w1.dtype == \"object\":\n+                self.assertEqual(str(w1), str(w2))\n+            else:\n+                self.assertAllClose(w1, w2, atol=1e-6)\n+\n+        # Test training still works\n+        new_model.fit(dataset, epochs=1, steps_per_epoch=1)\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n@@ -219,7 +219,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n                 \"update different parts of the model separately. Please call \"\n                 \"`optimizer.build(variables)` with the full list of trainable \"\n                 \"variables before the training loop or use legacy optimizer \"\n-                \"`tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\n+                f\"`tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\n             )\n         self.update_step(gradient, variable)\n \n@@ -807,6 +807,19 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n \n     def _load_own_variables(self, store):\n         \"\"\"Set the state of this optimizer object.\"\"\"\n+        if len(store.keys()) != len(self.variables):\n+            msg = (\n+                f\"Skipping variable loading for optimizer '{self.name}', \"\n+                f\"because it has {len(self.variables)} variables whereas \"\n+                f\"the saved optimizer has {len(store.keys())} variables. \"\n+            )\n+            if len(self.variables) == 0:\n+                msg += (\n+                    \"This is likely because the optimizer has not been \"\n+                    \"called/built yet.\"\n+                )\n+            logging.warning(msg)\n+            return\n         for i, variable in enumerate(self.variables):\n             variable.assign(store[str(i)])\n \n\n@@ -73,13 +73,15 @@ class LearningRateSchedule:\n     @abc.abstractmethod\n     def __call__(self, step):\n         raise NotImplementedError(\n-            \"Learning rate schedule must override __call__\"\n+            f\"Learning rate schedule '{self.__class__.__name__}' \"\n+            \"must override `__call__(self, step)`.\"\n         )\n \n     @abc.abstractmethod\n     def get_config(self):\n         raise NotImplementedError(\n-            \"Learning rate schedule must override get_config\"\n+            f\"Learning rate schedule '{self.__class__.__name__}' \"\n+            \"must override `get_config()` in order to be serializable.\"\n         )\n \n     @classmethod\n\n@@ -342,6 +342,18 @@ def _write_to_zip_recursively(zipfile_to_save, system_path, zip_path):\n             )\n \n \n+def _walk_trackable(trackable):\n+    for child_attr in dir(trackable):\n+        if child_attr.startswith(\"__\") or child_attr in ATTR_SKIPLIST:\n+            continue\n+        try:\n+            child_obj = getattr(trackable, child_attr)\n+        except Exception:\n+            # Avoid raising the exception when visiting the attributes.\n+            continue\n+        yield child_attr, child_obj\n+\n+\n def _save_state(\n     trackable, weights_store, assets_store, inner_path, visited_trackables\n ):\n@@ -358,14 +370,7 @@ def _save_state(\n     visited_trackables.add(id(trackable))\n \n     # Recursively save state of children trackables (layers, optimizers, etc.)\n-    for child_attr in dir(trackable):\n-        if child_attr.startswith(\"__\") or child_attr in ATTR_SKIPLIST:\n-            continue\n-        try:\n-            child_obj = getattr(trackable, child_attr)\n-        except Exception:\n-            # Avoid raising the exception when visiting the attributes.\n-            continue\n+    for child_attr, child_obj in _walk_trackable(trackable):\n         if _is_keras_trackable(child_obj):\n             _save_state(\n                 child_obj,\n@@ -398,14 +403,7 @@ def _load_state(\n     visited_trackables.add(id(trackable))\n \n     # Recursively load states for Keras trackables such as layers/optimizers.\n-    for child_attr in dir(trackable):\n-        if child_attr.startswith(\"__\") or child_attr in ATTR_SKIPLIST:\n-            continue\n-        try:\n-            child_obj = getattr(trackable, child_attr)\n-        except Exception:\n-            # Avoid raising exceptions when visiting attributes.\n-            continue\n+    for child_attr, child_obj in _walk_trackable(trackable):\n         if _is_keras_trackable(child_obj):\n             _load_state(\n                 child_obj,\n\n@@ -167,9 +167,13 @@ def serialize_keras_object(obj):\n         \"registered_name\": registered_name,\n     }\n     if hasattr(obj, \"get_build_config\"):\n-        config[\"build_config\"] = obj.get_build_config()\n+        build_config = obj.get_build_config()\n+        if build_config is not None:\n+            config[\"build_config\"] = serialize_dict(build_config)\n     if hasattr(obj, \"get_compile_config\"):\n-        config[\"compile_config\"] = obj.get_compile_config()\n+        compile_config = obj.get_compile_config()\n+        if compile_config is not None:\n+            config[\"compile_config\"] = serialize_dict(compile_config)\n     record_object_after_serialization(obj, config)\n     return config\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c53a700183edcb434c26129855154ab9cc082a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 2766 | Contributors (this commit): 17 | Commits (past 90d): 7 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -764,17 +764,17 @@ class Functional(training_lib.Model):\n \n     @generic_utils.default\n     def get_config(self):\n-        if saved_model_utils.in_tf_saved_model_scope():\n-            # SavedModel special case: need to preserve legacy (potentially\n-            # incorrect) behavior.\n-            config = super().get_config()\n-            return copy.deepcopy(get_network_config(self, config=config))\n-\n         # Prepare base arguments\n         config = {\n             \"name\": self.name,\n             \"trainable\": self.trainable,\n         }\n+\n+        if saved_model_utils.in_tf_saved_model_scope():\n+            # SavedModel special case: need to preserve legacy (potentially\n+            # incorrect) behavior.\n+            return copy.deepcopy(get_network_config(self, config=config))\n+\n         # Check whether the class has a constructor compatible with a Functional\n         # model or if it has a custom constructor.\n         if has_functional_like_constructor(self.__class__):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#52a2ba7c6ea04a631c4ab14ce219a62471a0575f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 38 | Files Changed: 3 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 38 | Churn Cumulative: 1660 | Contributors (this commit): 23 | Commits (past 90d): 7 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -63,7 +63,6 @@ from keras.layers.core.activation import Activation\n from keras.layers.core.dense import Dense\n from keras.layers.core.einsum_dense import EinsumDense\n from keras.layers.core.embedding import Embedding\n-from keras.layers.core.identity import Identity\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n from keras.layers.core.tf_op_layer import ClassMethod\n\n@@ -18,7 +18,6 @@ from keras.layers.core.activation import Activation\n from keras.layers.core.dense import Dense\n from keras.layers.core.einsum_dense import EinsumDense\n from keras.layers.core.embedding import Embedding\n-from keras.layers.core.identity import Identity\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n \n\n@@ -1,36 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Contains the Identity layer.\"\"\"\n-\n-from keras.engine.base_layer import Layer\n-\n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export(\"keras.layers.Identity\")\n-class Identity(Layer):\n-    \"\"\"Identity layer.\n-\n-    This layer should be used as a placeholder when no operation is to be\n-    performed. The layer is argument insensitive, and returns its `inputs`\n-    argument as output.\n-\n-    Args:\n-        name: Optional name for the layer instance.\n-    \"\"\"\n-\n-    def call(self, inputs):\n-        return inputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2fd719d00ae7b26ed80b3345c5eb0264685e95aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 106 | Churn Cumulative: 23002 | Contributors (this commit): 48 | Commits (past 90d): 28 | Contributors (cumulative): 91 | DMM Complexity: 0.0\n\nDIFF:\n@@ -294,19 +294,74 @@ class LossesContainerTest(test_combinations.TestCase):\n         self.assertIsInstance(total_loss, tf.Tensor)\n         self.assertEqual(total_loss.dtype, tf.float64)\n \n+    @test_combinations.generate(\n+        test_combinations.combine(\n+            input_type=[\"dense\", \"masked\", \"ragged\"],\n+            reduction=[\"auto\", \"sum\"],\n+            use_sample_weights=[True, False],\n+        ),\n+    )\n+    def test_loss_consistency(self, input_type, reduction, use_sample_weights):\n+        y_p = tf.ragged.constant(\n+            [[[1], [1], [1]], [[1], [1]]], dtype=tf.float32\n+        )\n+        y_t = tf.ragged.constant(\n+            [[[1], [0], [0]], [[1], [1]]], dtype=tf.float32\n+        )\n+\n+        if input_type == \"masked\":\n+            mask = tf.ones_like(y_p).to_tensor()\n+            y_p = y_p.to_tensor()\n+            y_t = y_t.to_tensor()\n+            y_p._keras_mask = mask\n+        elif input_type == \"dense\":\n+            y_p = y_p.to_tensor()\n+            y_t = y_t.to_tensor()\n+\n+        if input_type == \"dense\":\n+            count = 6\n+        else:\n+            count = 5\n+\n+        if use_sample_weights:\n+            wrong = 4\n+            maybe_sample_weight = {\n+                \"sample_weight\": tf.constant([[2], [1]], dtype=tf.float32)\n+            }\n+        else:\n+            wrong = 2\n+            maybe_sample_weight = {}\n+\n+        expected = wrong\n+        if reduction != \"sum\":\n+            expected /= count\n+\n+        loss_obj = losses_mod.MeanAbsoluteError(reduction=reduction)\n+\n+        result = loss_obj(y_t, y_p, **maybe_sample_weight)\n+        self.assertAlmostEqual(result.numpy(), expected)\n+\n+        container = compile_utils.LossesContainer(loss_obj)\n+        container_result = container(y_t, y_p, **maybe_sample_weight)\n+        self.assertAlmostEqual(container_result.numpy(), expected)\n+\n     def test_loss_masking(self):\n         loss_container = compile_utils.LossesContainer(\"mae\")\n         y_p = tf.constant([[[1], [1]], [[0], [0]]], dtype=tf.float32)\n         y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)\n+        # Reduction is \"sum_over_batch_size\" that's not the literal batch size,\n+        # but the number of elements being summed: The number of valid\n+        # emlements. So since the mask has two valid items, the number of\n+        # elements is 2.\n         y_p._keras_mask = tf.constant([[1, 0], [1, 0]], dtype=tf.float32)\n \n         total_loss = loss_container(y_t, y_p)\n-        self.assertAlmostEqual(total_loss.numpy(), 0.25)  # sum over batch size\n+        self.assertAlmostEqual(total_loss.numpy(), 0.5)  # sum over num valid\n \n         self.assertLen(loss_container.metrics, 1)\n         loss_metric = loss_container.metrics[0]\n         self.assertEqual(loss_metric.name, \"loss\")\n-        self.assertAlmostEqual(loss_metric.result().numpy(), 0.25)\n+        self.assertAlmostEqual(loss_metric.result().numpy(), 0.5)\n \n     def test_loss_sample_weight(self):\n         loss_container = compile_utils.LossesContainer(\"mae\")\n@@ -331,13 +386,13 @@ class LossesContainerTest(test_combinations.TestCase):\n         y_p._keras_mask = tf.constant([[1, 0], [1, 0]], dtype=tf.float32)\n \n         total_loss = loss_container(y_t, y_p, sample_weight=sw)\n-        # (0 * .2 + 1 * .5) / 4\n-        self.assertAlmostEqual(total_loss.numpy(), 0.125)  # sum over batch size\n+        # (0 * .2 + 1 * .5) / 2\n+        self.assertAlmostEqual(total_loss.numpy(), 0.25)  # sum over num valid\n \n         self.assertLen(loss_container.metrics, 1)\n         loss_metric = loss_container.metrics[0]\n         self.assertEqual(loss_metric.name, \"loss\")\n-        self.assertAlmostEqual(loss_metric.result().numpy(), 0.125)\n+        self.assertAlmostEqual(loss_metric.result().numpy(), 0.25)\n \n     def test_custom_loss_callables(self):\n         def custom_loss_fn(y_true, y_pred):\n\n@@ -3871,15 +3871,25 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n         )\n \n         # verify that masking is applied.\n-        x = np.array([[[1], [1]], [[1], [1]], [[0], [0]]])\n+        x = np.array(\n+            # third row is masked\n+            [[[1], [1]], [[1], [1]], [[0], [0]]]\n+        )\n         y = np.array([[[1], [1]], [[0], [1]], [[1], [1]]])\n-        scores = model.train_on_batch(x, y)\n-        self.assertArrayNear(scores, [0.25, 0.75], 0.1)\n+\n+        scores = model.test_on_batch(x, y)\n+        self.assertArrayNear(scores, [0.25, 0.75], 0.0001)\n \n         # verify that masking is combined with sample weights.\n         w = np.array([3, 2, 4])\n+        scores = model.test_on_batch(x, y, sample_weight=w)\n+        self.assertArrayNear(scores, [0.5, 0.8], 0.0001)\n+\n+        scores = model.train_on_batch(x, y)\n+        self.assertArrayNear(scores, [0.25, 0.75], 0.0001)\n+\n         scores = model.train_on_batch(x, y, sample_weight=w)\n-        self.assertArrayNear(scores, [0.3328, 0.8], 0.001)\n+        self.assertArrayNear(scores, [0.5 - 0.001037, 0.8], 0.0001)\n \n     @test_combinations.run_all_keras_modes\n     def test_add_metric_with_tensor_on_model(self):\n\n@@ -1756,10 +1756,15 @@ class Model(training_lib.Model):\n                             ) = losses_utils.squeeze_or_expand_dimensions(\n                                 mask, sample_weight=sample_weight\n                             )\n-                            sample_weight *= mask\n \n                     if hasattr(loss_fn, \"reduction\"):\n                         per_sample_losses = loss_fn.call(y_true, y_pred)\n+                        sample_weight = losses_utils.apply_valid_mask(\n+                            per_sample_losses,\n+                            sample_weight,\n+                            mask,\n+                            loss_fn.reduction,\n+                        )\n                         weighted_losses = losses_utils.compute_weighted_loss(\n                             per_sample_losses,\n                             sample_weight=sample_weight,\n\n@@ -148,8 +148,21 @@ class Loss:\n                 call_fn = tf.__internal__.autograph.tf_convert(\n                     self.call, tf.__internal__.autograph.control_status_ctx()\n                 )\n+\n             losses = call_fn(y_true, y_pred)\n-            mask = losses_utils.get_mask(losses)\n+\n+            in_mask = losses_utils.get_mask(y_pred)\n+            out_mask = losses_utils.get_mask(losses)\n+\n+            if in_mask is not None and out_mask is not None:\n+                mask = in_mask & out_mask\n+            elif in_mask is not None:\n+                mask = in_mask\n+            elif out_mask is not None:\n+                mask = out_mask\n+            else:\n+                mask = None\n+\n             reduction = self._get_reduction()\n             sample_weight = losses_utils.apply_valid_mask(\n                 losses, sample_weight, mask, reduction\n\n@@ -405,6 +405,7 @@ def apply_mask(y_p, sw, mask):\n     if mask is not None:\n         mask = tf.cast(mask, y_p.dtype)\n         if sw is not None:\n+            sw = tf.cast(sw, mask.dtype)\n             mask, _, sw = squeeze_or_expand_dimensions(mask, sample_weight=sw)\n             sw *= mask\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
