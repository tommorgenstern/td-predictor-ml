{"custom_id": "keras#6f15a44f40af0f4d7984a4dbc4415d183ad332d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 154 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -70,7 +70,9 @@ class DicretizationTest(testing.TestCase):\n \n     def test_tf_data_compatibility(self):\n         # With fixed bins\n-        layer = layers.Discretization(bin_boundaries=[0.0, 0.35, 0.5, 1.0], dtype=\"float32\")\n+        layer = layers.Discretization(\n+            bin_boundaries=[0.0, 0.35, 0.5, 1.0], dtype=\"float32\"\n+        )\n         x = np.array([[-1.0, 0.0, 0.1, 0.2, 0.4, 0.5, 1.0, 1.2, 0.98]])\n         self.assertAllClose(layer(x), np.array([[0, 1, 1, 1, 2, 3, 4, 4, 3]]))\n         ds = tf_data.Dataset.from_tensor_slices(x).batch(1).map(layer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#34081e1cb9cff079bf0c226b431baeee73ac19d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 552 | Lines Deleted: 703 | Files Changed: 12 | Hunks: 46 | Methods Changed: 49 | Complexity Δ (Sum/Max): -12/4 | Churn Δ: 1255 | Churn Cumulative: 10910 | Contributors (this commit): 22 | Commits (past 90d): 112 | Contributors (cumulative): 78 | DMM Complexity: 0.06181015452538632\n\nDIFF:\n@@ -0,0 +1,285 @@\n+\"\"\"\n+Title: Fine-tuning a pre-trained TorchVision Model\n+Author: [Ayush Thakur](https://twitter.com/ayushthakur0), [Soumik Rakshit](https://twitter.com/soumikRakshit96)\n+Date created: 2023/09/18\n+Last modified: 2023/09/18\n+Description: Fine-tuning a pre-trained Torch model from TorchVision for image\n+classification using Keras.\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+[TorchVision](https://pytorch.org/vision/stable/index.html) is a library part of the\n+[PyTorch](http://pytorch.org/) project that consists of popular datasets, model\n+architectures, and common image transformations for computer vision. This example\n+demonstrates how we can perform transfer learning for image classification using a\n+pre-trained backbone model from TorchVision on the [Imagenette\n+dataset](https://github.com/fastai/imagenette) using KerasCore. We will also demonstrate\n+the compatibility of KerasCore with an input system consisting of [Torch Datasets and\n+Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n+\n+### References:\n+\n+- [Customizing what happens in `fit()` with\n+PyTorch](https://keras.io/keras_core/guides/custom_train_step_in_torch/)\n+- [PyTorch Datasets and\n+Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n+- [Transfer learning for Computer Vision using\n+PyTorch](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n+- [Fine-tuning a TorchVision Model using Keras\n+](https://wandb.ai/ml-colabs/keras-torch/reports/Fine-tuning-a-TorchVision-Model-using-Keras--Vmlldzo1NDE5NDE1)\n+\n+## Setup\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"torch\"\n+\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+import torchvision\n+from torchvision import datasets, models, transforms\n+\n+import keras_core as keras\n+from keras_core.layers import TorchModuleWrapper\n+\n+\"\"\"\n+## Define the Hyperparameters\n+\"\"\"\n+\n+batch_size = 32\n+image_size = 224\n+initial_learning_rate = 1e-3\n+num_epochs = 5\n+\n+\"\"\"\n+## Creating the Torch Datasets and Dataloaders\n+\n+In this example, we would train an image classification model on the [Imagenette\n+dataset](https://github.com/fastai/imagenette). Imagenette is a subset of 10 easily\n+classified classes from [Imagenet](https://www.image-net.org/) (tench, English springer,\n+cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball,\n+parachute).\n+\"\"\"\n+\n+# Fetch the imagenette dataset\n+data_dir = keras.utils.get_file(\n+    fname=\"imagenette2-320.tgz\",\n+    origin=\"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\",\n+    extract=True,\n+)\n+data_dir = data_dir.replace(\".tgz\", \"\")\n+\n+\"\"\"\n+Next, we define pre-processing and augmentation transforms from TorchVision for the train\n+and validation sets.\n+\"\"\"\n+\n+data_transforms = {\n+    \"train\": transforms.Compose(\n+        [\n+            transforms.RandomResizedCrop(image_size),\n+            transforms.RandomHorizontalFlip(),\n+            transforms.ToTensor(),\n+            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n+        ]\n+    ),\n+    \"val\": transforms.Compose(\n+        [\n+            transforms.Resize(256),\n+            transforms.CenterCrop(image_size),\n+            transforms.ToTensor(),\n+            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n+        ]\n+    ),\n+}\n+\n+\"\"\"\n+Finally, we will use TorchVision and the\n+[`torch.utils.data`](https://pytorch.org/docs/stable/data.html) packages for creating the\n+dataloaders for trainig and validation.\n+\"\"\"\n+\n+# Define the train and validation datasets\n+image_datasets = {\n+    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n+    for x in [\"train\", \"val\"]\n+}\n+\n+# Define the torch dataloaders corresponding to the train and validation dataset\n+dataloaders = {\n+    x: torch.utils.data.DataLoader(\n+        image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4\n+    )\n+    for x in [\"train\", \"val\"]\n+}\n+dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\"]}\n+class_names = image_datasets[\"train\"].classes\n+\n+# Specify the global device\n+device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+\n+\"\"\"\n+Let us visualize a few samples from the training dataloader.\n+\"\"\"\n+\n+plt.figure(figsize=(10, 10))\n+sample_images, sample_labels = next(iter(dataloaders[\"train\"]))\n+sample_images = sample_images.numpy()\n+sample_labels = sample_labels.numpy()\n+for idx in range(9):\n+    ax = plt.subplot(3, 3, idx + 1)\n+    image = sample_images[idx].transpose((1, 2, 0))\n+    mean = np.array([0.485, 0.456, 0.406])\n+    std = np.array([0.229, 0.224, 0.225])\n+    image = std * image + mean\n+    image = np.clip(image, 0, 1)\n+    plt.imshow(image)\n+    plt.title(\"Ground Truth Label: \" + class_names[int(sample_labels[idx])])\n+    plt.axis(\"off\")\n+\n+\"\"\"\n+## The Image Classification Model\n+\"\"\"\n+\n+\"\"\"\n+We typically define a model in PyTorch using\n+[`torch.nn.Module`s](https://pytorch.org/docs/stable/notes/modules.html) which act as the\n+building blocks of stateful computation. Let us define the ResNet18 model from the\n+TorchVision package as a `torch.nn.Module` pre-trained on the [Imagenet1K\n+dataset](https://huggingface.co/datasets/imagenet-1k).\n+\"\"\"\n+\n+# Define the pre-trained resnet18 module from TorchVision\n+resnet_18 = models.resnet18(weights=\"IMAGENET1K_V1\")\n+\n+# We set the classification head of the pre-trained ResNet18\n+# module to an identity module\n+resnet_18.fc = nn.Identity()\n+\n+\"\"\"\n+Even though Keras supports PyTorch as a backend, it does not mean that we can nest torch\n+modules inside a [`keras_core.Model`](https://keras.io/keras_core/api/models/), because\n+trainable variables inside a Keras Model is tracked exclusively via [Keras\n+Layers](https://keras.io/keras_core/api/layers/).\n+\n+KerasCore provides us with a feature called `TorchModuleWrapper` which enables us to do\n+exactly this. The `TorchModuleWrapper` is a Keras Layer that accepts a torch module and\n+tracks its trainable variables, essentially converting the torch module into a Keras\n+Layer. This enables us to put any torch modules inside a Keras Model and train them with\n+a single `model.fit()`!\n+\"\"\"\n+\n+# We set the trainable ResNet18 backbone to be a Keras Layer\n+# using `TorchModuleWrapper`\n+backbone = TorchModuleWrapper(resnet_18)\n+\n+# We set this to `False` if you want to freeze the backbone\n+backbone.trainable = True\n+\n+\"\"\"\n+Now, we will build a Keras functional model with the backbone layer.\n+\"\"\"\n+\n+inputs = keras.Input(shape=(3, image_size, image_size))\n+x = backbone(inputs)\n+x = keras.layers.Dropout(0.5)(x)\n+x = keras.layers.Dense(len(class_names))(x)\n+outputs = keras.activations.softmax(x, axis=1)\n+model = keras.Model(inputs, outputs, name=\"ResNet18_Classifier\")\n+\n+model.summary()\n+\n+# Create exponential decay learning rate scheduler\n+decay_steps = num_epochs * len(dataloaders[\"train\"]) // batch_size\n+lr_scheduler = keras.optimizers.schedules.ExponentialDecay(\n+    initial_learning_rate=initial_learning_rate,\n+    decay_steps=decay_steps,\n+    decay_rate=0.1,\n+)\n+\n+# Compile the model\n+model.compile(\n+    loss=\"sparse_categorical_crossentropy\",\n+    optimizer=keras.optimizers.Adam(lr_scheduler),\n+    metrics=[\"accuracy\"],\n+)\n+\n+# Define the backend-agnostic WandB callbacks for KerasCore\n+callbacks = [\n+    # Save best model checkpoints\n+    keras.callbacks.ModelCheckpoint(\n+        filepath=\"model.weights.h5\",\n+        monitor=\"val_loss\",\n+        save_best_only=True,\n+        save_weights_only=True,\n+    )\n+]\n+\n+# Train the model by calling model.fit\n+history = model.fit(\n+    dataloaders[\"train\"],\n+    validation_data=dataloaders[\"val\"],\n+    epochs=num_epochs,\n+    callbacks=callbacks,\n+)\n+\n+\n+def plot_history(item):\n+    plt.plot(history.history[item], label=item)\n+    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n+    plt.xlabel(\"Epochs\")\n+    plt.ylabel(item)\n+    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n+    plt.legend()\n+    plt.grid()\n+    plt.show()\n+\n+\n+plot_history(\"loss\")\n+plot_history(\"accuracy\")\n+\n+\"\"\"\n+## Evaluation and Inference\n+\n+Now, we let us load the best model weights checkpoint and evaluate the model.\n+\"\"\"\n+\n+model.load_weights(\"model.weights.h5\")\n+_, val_accuracy = model.evaluate(dataloaders[\"val\"])\n+print(\"Best Validation Accuracy:\", val_accuracy)\n+\n+\"\"\"\n+Finally, let us visualize the some predictions of the model\n+\"\"\"\n+\n+plt.figure(figsize=(10, 10))\n+sample_images, sample_labels = next(iter(dataloaders[\"train\"]))\n+\n+# We perform inference and detach the predicted probabilities from the Torch\n+# computation graph with a tensor that does not require gradient computation.\n+sample_pred_probas = model(sample_images.to(\"cuda\")).detach()\n+sample_pred_logits = keras.ops.argmax(sample_pred_probas, axis=1)\n+sample_pred_logits = sample_pred_logits.to(\"cpu\").numpy()\n+\n+sample_images = sample_images.numpy()\n+sample_labels = sample_labels.numpy()\n+\n+for idx in range(9):\n+    ax = plt.subplot(3, 3, idx + 1)\n+    image = sample_images[idx].transpose((1, 2, 0))\n+    mean = np.array([0.485, 0.456, 0.406])\n+    std = np.array([0.229, 0.224, 0.225])\n+    image = std * image + mean\n+    image = np.clip(image, 0, 1)\n+    plt.imshow(image)\n+    title = \"Ground Truth Label: \" + class_names[int(sample_labels[idx])]\n+    title += \"\\nPredicted Label: \" + class_names[int(sample_pred_logits[idx])]\n+    plt.title(title)\n+    plt.axis(\"off\")\n\n@@ -255,48 +255,3 @@ def compute_conv_transpose_output_shape(\n     else:\n         output_shape = [input_shape[0], filters] + output_shape\n     return output_shape\n-\n-\n-def encode_categorical_inputs(\n-    inputs,\n-    output_mode,\n-    depth,\n-    dtype=\"float32\",\n-    sparse=False,\n-    count_weights=None,\n-):\n-    from keras_core import ops\n-\n-    \"\"\"Encodes categoical inputs according to output_mode.\"\"\"\n-    if output_mode == \"int\":\n-        return ops.identity(ops.cast(inputs, dtype))\n-\n-    original_shape = inputs.shape\n-    # In all cases, we should uprank scalar input to a single sample.\n-    if len(ops.shape(inputs)) == 0:\n-        inputs = ops.expand_dims(inputs, -1)\n-    # One hot will unprank only if the final output dimension is not already 1.\n-    if output_mode == \"one_hot\":\n-        if ops.shape(inputs)[-1] != 1:\n-            inputs = ops.expand_dims(inputs, -1)\n-\n-    if len(ops.shape(inputs)) > 2:\n-        raise ValueError(\n-            \"When output_mode is not `'int'`, maximum supported output rank \"\n-            f\"is 2. Received output_mode {output_mode} and input shape \"\n-            f\"{original_shape}, \"\n-            f\"which would result in output rank {inputs.shape.rank}.\"\n-        )\n-\n-    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n-    bincounts = ops.bincount(\n-        inputs,\n-        weights=count_weights,\n-        minlength=depth,\n-    )\n-    if binary_output:\n-        one_hot_input = ops.one_hot(inputs, depth)\n-        bincounts = ops.where(ops.any(one_hot_input, axis=-2), 1, 0)\n-    bincounts = ops.cast(bincounts, dtype)\n-\n-    return bincounts\n\n@@ -79,3 +79,8 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n     return jax.lax.select(\n         mask, inputs / keep_prob, jax.numpy.zeros_like(inputs)\n     )\n+\n+\n+def shuffle(x, axis=0, seed=None):\n+    seed = jax_draw_seed(seed)\n+    return jax.random.shuffle(seed, x, axis)\n\n@@ -86,3 +86,9 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n     mask = rng.uniform(size=noise_shape) < keep_prob\n     mask = np.broadcast_to(mask, inputs.shape)\n     return np.where(mask, inputs / keep_prob, np.zeros_like(inputs))\n+\n+\n+def shuffle(x, axis=0, seed=None):\n+    seed = draw_seed(seed)\n+    rng = np.random.default_rng(seed)\n+    return rng.permuted(x, axis=axis)\n\n@@ -330,7 +330,7 @@ def hstack(xs):\n \n \n def identity(n, dtype=\"float32\"):\n-    return tf.identity(n)\n+    return tfnp.identity(n, dtype=dtype)\n \n \n def imag(x):\n\n@@ -1,4 +1,5 @@\n import tensorflow as tf\n+from tensorflow.experimental import numpy as tfnp\n \n from keras_core.backend.common import standardize_dtype\n from keras_core.backend.config import floatx\n@@ -83,3 +84,13 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n         noise_shape=noise_shape,\n         seed=seed,\n     )\n+\n+\n+def shuffle(x, axis=0, seed=None):\n+    seed = tf_draw_seed(seed)\n+    if axis == 0:\n+        return tf.random.experimental.stateless_shuffle(x, seed=seed)\n+    x = tfnp.swapaxes(x, axis1=0, axis2=axis)\n+    x = tf.random.experimental.stateless_shuffle(x, seed=seed)\n+    x = tfnp.swapaxes(x, axis1=0, axis2=axis)\n+    return x\n\n@@ -160,3 +160,28 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n     return torch.nn.functional.dropout(\n         inputs, p=rate, training=True, inplace=False\n     )\n+\n+\n+def shuffle(x, axis=0, seed=None):\n+    # Ref: https://github.com/pytorch/pytorch/issues/71409\n+    x = convert_to_tensor(x)\n+\n+    # Get permutation indices\n+    # Do not use generator during symbolic execution.\n+    if get_device() == \"meta\":\n+        row_perm = torch.rand(x.shape[: axis + 1], device=get_device()).argsort(\n+            axis\n+        )\n+    else:\n+        generator = torch_seed_generator(seed)\n+        row_perm = torch.rand(\n+            x.shape[: axis + 1], generator=generator, device=get_device()\n+        ).argsort(axis)\n+    for _ in range(x.ndim - axis - 1):\n+        row_perm.unsqueeze_(-1)\n+\n+    # Reformat this for the gather operation\n+    row_perm = row_perm.repeat(\n+        *[1 for _ in range(axis + 1)], *(x.shape[axis + 1 :])\n+    )\n+    return x.gather(axis, row_perm)\n\n@@ -1,21 +1,97 @@\n import numpy as np\n import pytest\n+from absl.testing import parameterized\n \n from keras_core import backend\n from keras_core import layers\n from keras_core import models\n-from keras_core import ops\n from keras_core import testing\n \n \n+def np_dot(a, b, axes):\n+    if isinstance(axes, int):\n+        axes = (axes, axes)\n+    axes = [axis if axis < 0 else axis - 1 for axis in axes]\n+    res = np.stack([np.tensordot(a[i], b[i], axes) for i in range(a.shape[0])])\n+    if len(res.shape) == 1:\n+        res = np.expand_dims(res, axis=1)\n+    return res\n+\n+\n+TEST_PARAMETERS = [\n+    {\n+        \"testcase_name\": \"add\",\n+        \"layer_class\": layers.Add,\n+        \"np_op\": np.add,\n+    },\n+    {\n+        \"testcase_name\": \"substract\",\n+        \"layer_class\": layers.Subtract,\n+        \"np_op\": np.subtract,\n+    },\n+    {\n+        \"testcase_name\": \"minimum\",\n+        \"layer_class\": layers.Minimum,\n+        \"np_op\": np.minimum,\n+    },\n+    {\n+        \"testcase_name\": \"maximum\",\n+        \"layer_class\": layers.Maximum,\n+        \"np_op\": np.maximum,\n+    },\n+    {\n+        \"testcase_name\": \"multiply\",\n+        \"layer_class\": layers.Multiply,\n+        \"np_op\": np.multiply,\n+    },\n+    {\n+        \"testcase_name\": \"average\",\n+        \"layer_class\": layers.Average,\n+        \"np_op\": lambda a, b: np.multiply(np.add(a, b), 0.5),\n+    },\n+    {\n+        \"testcase_name\": \"concat\",\n+        \"layer_class\": layers.Concatenate,\n+        \"np_op\": lambda a, b, **kwargs: np.concatenate((a, b), **kwargs),\n+        \"init_kwargs\": {\"axis\": -1},\n+        \"expected_output_shape\": (2, 4, 10),\n+    },\n+    {\n+        \"testcase_name\": \"dot_2d\",\n+        \"layer_class\": layers.Dot,\n+        \"np_op\": np_dot,\n+        \"init_kwargs\": {\"axes\": -1},\n+        \"input_shape\": (2, 4),\n+        \"expected_output_shape\": (2, 1),\n+        \"skip_mask_test\": True,\n+    },\n+    {\n+        \"testcase_name\": \"dot_3d\",\n+        \"layer_class\": layers.Dot,\n+        \"np_op\": np_dot,\n+        \"init_kwargs\": {\"axes\": -1},\n+        \"expected_output_shape\": (2, 4, 4),\n+        \"skip_mask_test\": True,\n+    },\n+]\n+\n+\n @pytest.mark.requires_trainable_backend\n-class MergingLayersTest(testing.TestCase):\n-    def test_add_basic(self):\n-        self.run_layer_test(\n-            layers.Add,\n+class MergingLayersTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(TEST_PARAMETERS)\n+    def test_basic(\n+        self,\n+        layer_class,\n         init_kwargs={},\n-            input_shape=[(2, 3), (2, 3)],\n-            expected_output_shape=(2, 3),\n+        input_shape=(2, 4, 5),\n+        expected_output_shape=(2, 4, 5),\n+        **kwargs\n+    ):\n+        self.run_layer_test(\n+            layer_class,\n+            init_kwargs=init_kwargs,\n+            input_shape=[input_shape, input_shape],\n+            expected_output_shape=expected_output_shape,\n             expected_num_trainable_weights=0,\n             expected_num_non_trainable_weights=0,\n             expected_num_seed_generators=0,\n@@ -23,118 +99,37 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    def test_add_correctness_dynamic(self):\n-        x1 = np.random.rand(2, 4, 5)\n-        x2 = np.random.rand(2, 4, 5)\n-        x3 = ops.convert_to_tensor(x1 + x2)\n-\n-        input_1 = layers.Input(shape=(4, 5))\n-        input_2 = layers.Input(shape=(4, 5))\n-        add_layer = layers.Add()\n-        out = add_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (2, 4, 5))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            add_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    add_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_add_correctness_static(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-        x2 = np.random.rand(batch_size, *shape)\n-        x3 = ops.convert_to_tensor(x1 + x2)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        add_layer = layers.Add()\n-        out = add_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (batch_size, *shape))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            add_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    add_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_add_errors(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        add_layer = layers.Add()\n-\n-        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n-            add_layer.compute_mask([input_1, input_2], x1)\n-\n-        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n-            add_layer.compute_mask(input_1, [None, None])\n-\n-        with self.assertRaisesRegex(\n-            ValueError, \" should have the same length.\"\n-        ):\n-            add_layer.compute_mask([input_1, input_2], [None])\n-\n-    def test_subtract_basic(self):\n-        self.run_layer_test(\n-            layers.Subtract,\n+    @parameterized.named_parameters(TEST_PARAMETERS)\n+    def test_correctness_static(\n+        self,\n+        layer_class,\n+        np_op,\n         init_kwargs={},\n-            input_shape=[(2, 3), (2, 3)],\n-            expected_output_shape=(2, 3),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=True,\n-        )\n+        input_shape=(2, 4, 5),\n+        expected_output_shape=(2, 4, 5),\n+        skip_mask_test=False,\n+    ):\n+        batch_size = input_shape[0]\n+        shape = input_shape[1:]\n+        x1 = np.random.rand(*input_shape)\n+        x2 = np.random.rand(*input_shape)\n+        x3 = np_op(x1, x2, **init_kwargs)\n \n-    def test_subtract_correctness_dynamic(self):\n-        x1 = np.random.rand(2, 4, 5)\n-        x2 = np.random.rand(2, 4, 5)\n-        x3 = ops.convert_to_tensor(x1 - x2)\n-\n-        input_1 = layers.Input(shape=(4, 5))\n-        input_2 = layers.Input(shape=(4, 5))\n-        subtract_layer = layers.Subtract()\n-        out = subtract_layer([input_1, input_2])\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        layer = layer_class(**init_kwargs)\n+        out = layer([input_1, input_2])\n         model = models.Model([input_1, input_2], out)\n         res = model([x1, x2])\n \n-        self.assertEqual(res.shape, (2, 4, 5))\n+        self.assertEqual(res.shape, expected_output_shape)\n         self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            subtract_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n+        self.assertIsNone(layer.compute_mask([input_1, input_2], [None, None]))\n+        if not skip_mask_test:\n             self.assertTrue(\n                 np.all(\n                     backend.convert_to_numpy(\n-                    subtract_layer.compute_mask(\n+                        layer.compute_mask(\n                             [input_1, input_2],\n                             [backend.Variable(x1), backend.Variable(x2)],\n                         )\n@@ -142,29 +137,36 @@ class MergingLayersTest(testing.TestCase):\n                 )\n             )\n \n-    def test_subtract_correctness_static(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-        x2 = np.random.rand(batch_size, *shape)\n-        x3 = ops.convert_to_tensor(x1 - x2)\n+    @parameterized.named_parameters(TEST_PARAMETERS)\n+    def test_correctness_dynamic(\n+        self,\n+        layer_class,\n+        np_op,\n+        init_kwargs={},\n+        input_shape=(2, 4, 5),\n+        expected_output_shape=(2, 4, 5),\n+        skip_mask_test=False,\n+    ):\n+        shape = input_shape[1:]\n+        x1 = np.random.rand(*input_shape)\n+        x2 = np.random.rand(*input_shape)\n+        x3 = np_op(x1, x2, **init_kwargs)\n \n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        subtract_layer = layers.Subtract()\n-        out = subtract_layer([input_1, input_2])\n+        input_1 = layers.Input(shape=shape)\n+        input_2 = layers.Input(shape=shape)\n+        layer = layer_class(**init_kwargs)\n+        out = layer([input_1, input_2])\n         model = models.Model([input_1, input_2], out)\n         res = model([x1, x2])\n \n-        self.assertEqual(res.shape, (batch_size, *shape))\n+        self.assertEqual(res.shape, expected_output_shape)\n         self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            subtract_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n+        self.assertIsNone(layer.compute_mask([input_1, input_2], [None, None]))\n+        if not skip_mask_test:\n             self.assertTrue(\n                 np.all(\n                     backend.convert_to_numpy(\n-                    subtract_layer.compute_mask(\n+                        layer.compute_mask(\n                             [input_1, input_2],\n                             [backend.Variable(x1), backend.Variable(x2)],\n                         )\n@@ -172,26 +174,44 @@ class MergingLayersTest(testing.TestCase):\n                 )\n             )\n \n-    def test_subtract_errors(self):\n-        batch_size = 2\n-        shape = (4, 5)\n+    @parameterized.named_parameters(TEST_PARAMETERS)\n+    def test_errors(\n+        self,\n+        layer_class,\n+        init_kwargs={},\n+        input_shape=(2, 4, 5),\n+        skip_mask_test=False,\n+        **kwargs\n+    ):\n+        if skip_mask_test:\n+            pytest.skip(\"Masking not supported\")\n+\n+        batch_size = input_shape[0]\n+        shape = input_shape[1:]\n+        x1 = np.random.rand(*input_shape)\n         x1 = np.random.rand(batch_size, *shape)\n \n         input_1 = layers.Input(shape=shape, batch_size=batch_size)\n         input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_3 = layers.Input(shape=shape, batch_size=batch_size)\n-        subtract_layer = layers.Subtract()\n+        layer = layer_class(**init_kwargs)\n \n         with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n-            subtract_layer.compute_mask([input_1, input_2], x1)\n+            layer.compute_mask([input_1, input_2], x1)\n \n         with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n-            subtract_layer.compute_mask(input_1, [None, None])\n+            layer.compute_mask(input_1, [None, None])\n \n         with self.assertRaisesRegex(\n             ValueError, \" should have the same length.\"\n         ):\n-            subtract_layer.compute_mask([input_1, input_2], [None])\n+            layer.compute_mask([input_1, input_2], [None])\n+\n+    def test_subtract_layer_inputs_length_errors(self):\n+        shape = (4, 5)\n+        input_1 = layers.Input(shape=shape)\n+        input_2 = layers.Input(shape=shape)\n+        input_3 = layers.Input(shape=shape)\n+\n         with self.assertRaisesRegex(\n             ValueError, \"layer should be called on exactly 2 inputs\"\n         ):\n@@ -200,515 +220,3 @@ class MergingLayersTest(testing.TestCase):\n             ValueError, \"layer should be called on exactly 2 inputs\"\n         ):\n             layers.Subtract()([input_1])\n-\n-    def test_minimum_basic(self):\n-        self.run_layer_test(\n-            layers.Minimum,\n-            init_kwargs={},\n-            input_shape=[(2, 3), (2, 3)],\n-            expected_output_shape=(2, 3),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=True,\n-        )\n-\n-    def test_minimum_correctness_dynamic(self):\n-        x1 = np.random.rand(2, 4, 5)\n-        x2 = np.random.rand(2, 4, 5)\n-        x3 = ops.minimum(x1, x2)\n-\n-        input_1 = layers.Input(shape=(4, 5))\n-        input_2 = layers.Input(shape=(4, 5))\n-        merge_layer = layers.Minimum()\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (2, 4, 5))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            merge_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    merge_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_minimum_correctness_static(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-        x2 = np.random.rand(batch_size, *shape)\n-        x3 = ops.minimum(x1, x2)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Minimum()\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (batch_size, *shape))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            merge_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    merge_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_minimum_errors(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Minimum()\n-\n-        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n-            merge_layer.compute_mask([input_1, input_2], x1)\n-\n-        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n-            merge_layer.compute_mask(input_1, [None, None])\n-\n-        with self.assertRaisesRegex(\n-            ValueError, \" should have the same length.\"\n-        ):\n-            merge_layer.compute_mask([input_1, input_2], [None])\n-\n-    def test_maximum_basic(self):\n-        self.run_layer_test(\n-            layers.Maximum,\n-            init_kwargs={},\n-            input_shape=[(2, 3), (2, 3)],\n-            expected_output_shape=(2, 3),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=True,\n-        )\n-\n-    def test_maximum_correctness_dynamic(self):\n-        x1 = np.random.rand(2, 4, 5)\n-        x2 = np.random.rand(2, 4, 5)\n-        x3 = ops.maximum(x1, x2)\n-\n-        input_1 = layers.Input(shape=(4, 5))\n-        input_2 = layers.Input(shape=(4, 5))\n-        merge_layer = layers.Maximum()\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (2, 4, 5))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            merge_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    merge_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_maximum_correctness_static(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-        x2 = np.random.rand(batch_size, *shape)\n-        x3 = ops.maximum(x1, x2)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Maximum()\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (batch_size, *shape))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            merge_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    merge_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_maximum_errors(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Maximum()\n-\n-        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n-            merge_layer.compute_mask([input_1, input_2], x1)\n-\n-        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n-            merge_layer.compute_mask(input_1, [None, None])\n-\n-        with self.assertRaisesRegex(\n-            ValueError, \" should have the same length.\"\n-        ):\n-            merge_layer.compute_mask([input_1, input_2], [None])\n-\n-    def test_multiply_basic(self):\n-        self.run_layer_test(\n-            layers.Multiply,\n-            init_kwargs={},\n-            input_shape=[(2, 3), (2, 3)],\n-            expected_output_shape=(2, 3),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=True,\n-        )\n-\n-    def test_multiply_correctness_dynamic(self):\n-        x1 = np.random.rand(2, 4, 5)\n-        x2 = np.random.rand(2, 4, 5)\n-        x3 = ops.convert_to_tensor(x1 * x2)\n-\n-        input_1 = layers.Input(shape=(4, 5))\n-        input_2 = layers.Input(shape=(4, 5))\n-        merge_layer = layers.Multiply()\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (2, 4, 5))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            merge_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    merge_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_multiply_correctness_static(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-        x2 = np.random.rand(batch_size, *shape)\n-        x3 = ops.convert_to_tensor(x1 * x2)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Multiply()\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (batch_size, *shape))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            merge_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    merge_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_multiply_errors(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Multiply()\n-\n-        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n-            merge_layer.compute_mask([input_1, input_2], x1)\n-\n-        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n-            merge_layer.compute_mask(input_1, [None, None])\n-\n-        with self.assertRaisesRegex(\n-            ValueError, \" should have the same length.\"\n-        ):\n-            merge_layer.compute_mask([input_1, input_2], [None])\n-\n-    def test_average_basic(self):\n-        self.run_layer_test(\n-            layers.Average,\n-            init_kwargs={},\n-            input_shape=[(2, 3), (2, 3)],\n-            expected_output_shape=(2, 3),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=True,\n-        )\n-\n-    def test_average_correctness_dynamic(self):\n-        x1 = np.random.rand(2, 4, 5)\n-        x2 = np.random.rand(2, 4, 5)\n-        x3 = ops.average(np.array([x1, x2]), axis=0)\n-\n-        input_1 = layers.Input(shape=(4, 5))\n-        input_2 = layers.Input(shape=(4, 5))\n-        merge_layer = layers.Average()\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (2, 4, 5))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            merge_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    merge_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_average_correctness_static(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-        x2 = np.random.rand(batch_size, *shape)\n-        x3 = ops.average(np.array([x1, x2]), axis=0)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Average()\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (batch_size, *shape))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            merge_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    merge_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_average_errors(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        x1 = np.random.rand(batch_size, *shape)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Average()\n-\n-        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n-            merge_layer.compute_mask([input_1, input_2], x1)\n-\n-        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n-            merge_layer.compute_mask(input_1, [None, None])\n-\n-        with self.assertRaisesRegex(\n-            ValueError, \" should have the same length.\"\n-        ):\n-            merge_layer.compute_mask([input_1, input_2], [None])\n-\n-    def test_concatenate_basic(self):\n-        self.run_layer_test(\n-            layers.Concatenate,\n-            init_kwargs={\"axis\": 1},\n-            input_shape=[(2, 3), (2, 3)],\n-            expected_output_shape=(2, 6),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=True,\n-        )\n-\n-    def test_concatenate_correctness_dynamic(self):\n-        x1 = np.random.rand(2, 4, 5)\n-        x2 = np.random.rand(2, 4, 5)\n-        axis = 1\n-\n-        x3 = ops.concatenate([x1, x2], axis=axis)\n-\n-        input_1 = layers.Input(shape=(4, 5))\n-        input_2 = layers.Input(shape=(4, 5))\n-        merge_layer = layers.Concatenate(axis=axis)\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (2, 8, 5))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            merge_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    merge_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [backend.Variable(x1), backend.Variable(x2)],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_concatenate_correctness_static(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        axis = 1\n-        x1 = np.random.rand(batch_size, *shape)\n-        x2 = np.random.rand(batch_size, *shape)\n-        x3 = ops.concatenate([x1, x2], axis=axis)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Concatenate(axis=axis)\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (batch_size, 8, 5))\n-        self.assertAllClose(res, x3, atol=1e-4)\n-        self.assertIsNone(\n-            merge_layer.compute_mask([input_1, input_2], [None, None])\n-        )\n-        self.assertTrue(\n-            np.all(\n-                backend.convert_to_numpy(\n-                    merge_layer.compute_mask(\n-                        [input_1, input_2],\n-                        [x1, x2],\n-                    )\n-                )\n-            )\n-        )\n-\n-    def test_concatenate_errors(self):\n-        batch_size = 2\n-        shape = (4, 5)\n-        axis = 1\n-        x1 = np.random.rand(batch_size, *shape)\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Concatenate(axis=axis)\n-\n-        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n-            merge_layer.compute_mask([input_1, input_2], x1)\n-\n-        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n-            merge_layer.compute_mask(input_1, [None, None])\n-\n-        with self.assertRaisesRegex(\n-            ValueError, \" should have the same length.\"\n-        ):\n-            merge_layer.compute_mask([input_1, input_2], [None])\n-\n-    def test_dot_basic(self):\n-        self.run_layer_test(\n-            layers.Dot,\n-            init_kwargs={\"axes\": -1},\n-            input_shape=[(4, 3), (4, 3)],\n-            expected_output_shape=(4, 1),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=None,\n-        )\n-\n-    def test_dot_correctness_dynamic(self):\n-        x1 = np.random.rand(2, 4)\n-        x2 = np.random.rand(2, 4)\n-        axes = 1\n-\n-        expected = np.zeros((2, 1))\n-        expected[0, 0] = np.dot(x1[0], x2[0])\n-        expected[1, 0] = np.dot(x1[1], x2[1])\n-\n-        input_1 = layers.Input(shape=(4,))\n-        input_2 = layers.Input(shape=(4,))\n-        merge_layer = layers.Dot(axes=axes)\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (2, 1))\n-        self.assertAllClose(res, expected, atol=1e-4)\n-\n-    def test_dot_correctness_static(self):\n-        batch_size = 2\n-        shape = (4,)\n-        axes = 1\n-\n-        x1 = np.random.rand(2, 4)\n-        x2 = np.random.rand(2, 4)\n-        expected = np.zeros((2, 1))\n-        expected[0, 0] = np.dot(x1[0], x2[0])\n-        expected[1, 0] = np.dot(x1[1], x2[1])\n-\n-        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n-        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n-        merge_layer = layers.Dot(axes=axes)\n-        out = merge_layer([input_1, input_2])\n-        model = models.Model([input_1, input_2], out)\n-        res = model([x1, x2])\n-\n-        self.assertEqual(res.shape, (2, 1))\n-        self.assertAllClose(res, expected, atol=1e-4)\n\n@@ -1,23 +1,30 @@\n import numpy as np\n \n from keras_core import backend\n-from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.backend.common.backend_utils import encode_categorical_inputs\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.layers.layer import Layer\n from keras_core.utils import argument_validation\n from keras_core.utils import backend_utils\n+from keras_core.utils import tf_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.Discretization\")\n-class Discretization(TFDataLayer):\n+class Discretization(Layer):\n     \"\"\"A preprocessing layer which buckets continuous features by ranges.\n \n     This layer will place each element of its input data into one of several\n     contiguous ranges and output an integer index indicating which range each\n     element was placed in.\n \n+    **Note:** This layer uses TensorFlow internally. It cannot\n+    be used as part of the compiled computation graph of a model with\n+    any backend other than TensorFlow.\n+    It can however be used with any backend when running eagerly.\n+    It can also always be used as part of an input preprocessing pipeline\n+    with any backend (outside the model itself), which is how we recommend\n+    to use this layer.\n+\n     **Note:** This layer is safe to use inside a `tf.data` pipeline\n     (independently of which backend you're using).\n \n@@ -71,14 +78,14 @@ class Discretization(TFDataLayer):\n \n     Examples:\n \n-    Discretize float values based on provided buckets.\n+    Bucketize float values based on provided buckets.\n     >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n     >>> layer = Discretization(bin_boundaries=[0., 1., 2.])\n     >>> layer(input)\n     array([[0, 2, 3, 1],\n            [1, 3, 2, 1]])\n \n-    Discretize float values based on a number of buckets to compute.\n+    Bucketize float values based on a number of buckets to compute.\n     >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n     >>> layer = Discretization(num_bins=4, epsilon=0.01)\n     >>> layer.adapt(input)\n@@ -234,18 +241,26 @@ class Discretization(TFDataLayer):\n         if not isinstance(\n             inputs,\n             (\n+                tf.Tensor,\n+                tf.SparseTensor,\n+                tf.RaggedTensor,\n                 np.ndarray,\n                 backend.KerasTensor,\n             ),\n         ):\n-            self._convert_input_args = True\n+            inputs = tf.convert_to_tensor(\n+                backend.convert_to_numpy(inputs), dtype=self.input_dtype\n+            )\n \n-        indices = ops.digitize(inputs, self.bin_boundaries)\n+        from keras_core.backend.tensorflow.numpy import digitize\n \n-        outputs = encode_categorical_inputs(\n+        indices = digitize(inputs, self.bin_boundaries)\n+\n+        outputs = tf_utils.encode_categorical_inputs(\n             indices,\n             output_mode=self.output_mode,\n             depth=len(self.bin_boundaries) + 1,\n+            sparse=self.sparse,\n             dtype=self.compute_dtype,\n         )\n         if (\n@@ -355,3 +370,7 @@ def compress_summary(summary, epsilon):\n     )\n     summary = np.stack((new_bins, new_weights))\n     return summary.astype(\"float32\")\n+\n+\n+def bucketize(inputs, boundaries):\n+    return tf.raw_ops.Bucketize(input=inputs, boundaries=boundaries)\n\n@@ -70,9 +70,7 @@ class DicretizationTest(testing.TestCase):\n \n     def test_tf_data_compatibility(self):\n         # With fixed bins\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.35, 0.5, 1.0], dtype=\"float32\"\n-        )\n+        layer = layers.Discretization(bin_boundaries=[0.0, 0.35, 0.5, 1.0])\n         x = np.array([[-1.0, 0.0, 0.1, 0.2, 0.4, 0.5, 1.0, 1.2, 0.98]])\n         self.assertAllClose(layer(x), np.array([[0, 1, 1, 1, 2, 3, 4, 4, 3]]))\n         ds = tf_data.Dataset.from_tensor_slices(x).batch(1).map(layer)\n\n@@ -188,3 +188,23 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n     return backend.random.dropout(\n         inputs, rate, noise_shape=noise_shape, seed=seed\n     )\n+\n+\n+@keras_core_export(\"keras_core.random.shuffle\")\n+def shuffle(x, axis=0, seed=None):\n+    \"\"\"Shuffle the elements of a tensor uniformly at random along an axis.\n+\n+    Args:\n+        x: The tensor to be shuffled.\n+        axis: An integer specifying the axis along which to shuffle. Defaults to\n+            `0`.\n+        seed: A Python integer or instance of\n+            `keras_core.random.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.random.SeedGenerator`.\n+    \"\"\"\n+    return backend.random.shuffle(x, axis=axis, seed=seed)\n\n@@ -216,3 +216,20 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertGreater(np.abs(y2 - y3), 1e-4)\n \n         seed_generator.global_seed_generator().state.assign(seed)\n+\n+    def test_shuffle(self):\n+        x = np.arange(100).reshape(10, 10)\n+\n+        # Test axis=0\n+        y = random.shuffle(x, seed=0)\n+\n+        self.assertFalse(np.all(x == ops.convert_to_numpy(y)))\n+        self.assertAllClose(np.sum(x, axis=0), ops.sum(y, axis=0))\n+        self.assertNotAllClose(np.sum(x, axis=1), ops.sum(y, axis=1))\n+\n+        # Test axis=1\n+        y = random.shuffle(x, axis=1, seed=0)\n+\n+        self.assertFalse(np.all(x == ops.convert_to_numpy(y)))\n+        self.assertAllClose(np.sum(x, axis=1), ops.sum(y, axis=1))\n+        self.assertNotAllClose(np.sum(x, axis=0), ops.sum(y, axis=0))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
