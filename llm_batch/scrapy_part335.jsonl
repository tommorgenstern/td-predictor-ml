{"custom_id": "scrapy#54bfb9649bdec565f9798cc41643ed1bae25bd67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 274 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -295,6 +295,7 @@ intersphinx_mapping = {\n     'twisted': ('https://twistedmatrix.com/documents/current', None),\n     'twistedapi': ('https://twistedmatrix.com/documents/current/api', None),\n }\n+intersphinx_disabled_reftypes = []\n \n \n # Options for sphinx-hoverxref options\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#387326fad42c4709933851108f2370d3105b8dd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 200 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -104,8 +104,8 @@ class CookiesMiddleware:\n         for key in (\"name\", \"value\", \"path\", \"domain\"):\n             if cookie.get(key) is None:\n                 if key in (\"name\", \"value\"):\n-                    msg = \"Invalid cookie found in request {}: {} ('{}' is missing)\"\n-                    logger.warning(msg.format(request, cookie, key))\n+                    msg = f\"Invalid cookie found in request {request}: {cookie} ('{key}' is missing)\"\n+                    logger.warning(msg)\n                     return\n                 continue\n             if isinstance(cookie[key], (bool, float, int, str)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c4c816624fc4fd5fbc1866c507b661e92704136a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 30 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -9,10 +9,19 @@ import tarfile\n import zipfile\n from io import BytesIO\n from tempfile import mktemp\n+import warnings\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.responsetypes import responsetypes\n \n \n+warnings.warn(\n+    'scrapy.downloadermiddlewares.decompression is deprecated',\n+    ScrapyDeprecationWarning,\n+    stacklevel=2,\n+)\n+\n+\n logger = logging.getLogger(__name__)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe08a119d965b2291e44801901e15b58a1f959ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 34 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -9,13 +9,13 @@ import tarfile\n import zipfile\n from io import BytesIO\n from tempfile import mktemp\n-import warnings\n+from warnings import warn\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.responsetypes import responsetypes\n \n \n-warnings.warn(\n+warn(\n     'scrapy.downloadermiddlewares.decompression is deprecated',\n     ScrapyDeprecationWarning,\n     stacklevel=2,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b33b82a8b802c3906c2f1eaf1b88efee9b2fb09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 280 | Contributors (this commit): 21 | Commits (past 90d): 3 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -291,9 +291,9 @@ intersphinx_mapping = {\n     'pytest': ('https://docs.pytest.org/en/latest', None),\n     'python': ('https://docs.python.org/3', None),\n     'sphinx': ('https://www.sphinx-doc.org/en/master', None),\n-    'tox': ('https://tox.readthedocs.io/en/latest', None),\n-    'twisted': ('https://twistedmatrix.com/documents/current', None),\n-    'twistedapi': ('https://twistedmatrix.com/documents/current/api', None),\n+    'tox': ('https://tox.wiki/en/latest/', None),\n+    'twisted': ('https://docs.twisted.org/en/stable/', None),\n+    'twistedapi': ('https://docs.twisted.org/en/stable/api/', None),\n     'w3lib': ('https://w3lib.readthedocs.io/en/latest', None),\n }\n intersphinx_disabled_reftypes = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26c70318cb14806a07ee09d0283e9d5d306490e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 23 | Churn Cumulative: 3871 | Contributors (this commit): 32 | Commits (past 90d): 5 | Contributors (cumulative): 59 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ see https://docs.scrapy.org/en/latest/contributing.html#running-tests\n \"\"\"\n \n import os\n+import socket\n \n # ignore system-wide proxies for tests\n # which would send requests to a totally unsuspecting server\n@@ -25,6 +26,15 @@ tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n                              'sample_data')\n \n \n+# In some environments accessing a non-existing host doesn't raise an\n+# error. In such cases we're going to skip tests which rely on it.\n+try:\n+    socket.getaddrinfo('non-existing-host', 80)\n+    NON_EXISTING_RESOLVABLE = True\n+except socket.gaierror:\n+    NON_EXISTING_RESOLVABLE = False\n+\n+\n def get_testdata(*paths):\n     \"\"\"Return test data\"\"\"\n     path = os.path.join(tests_datadir, *paths)\n\n@@ -6,7 +6,7 @@ from twisted.internet import defer\n from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n \n-from tests import tests_datadir\n+from tests import tests_datadir, NON_EXISTING_RESOLVABLE\n \n \n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n@@ -109,6 +109,8 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_dns_failures(self):\n+        if NON_EXISTING_RESOLVABLE:\n+            raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n         url = 'www.somedomainthatdoesntexi.st'\n         errcode, out, err = yield self.execute([url, '-c', 'item'], check_code=False)\n         self.assertEqual(errcode, 1, out or err)\n\n@@ -3,6 +3,7 @@ import logging\n from ipaddress import IPv4Address\n from socket import gethostbyname\n from urllib.parse import urlparse\n+import unittest\n \n from pytest import mark\n from testfixtures import LogCapture\n@@ -17,6 +18,7 @@ from scrapy.exceptions import StopDownload\n from scrapy.http import Request\n from scrapy.http.response import Response\n from scrapy.utils.python import to_unicode\n+from tests import NON_EXISTING_RESOLVABLE\n from tests.mockserver import MockServer\n from tests.spiders import (\n     AsyncDefAsyncioGenComplexSpider,\n@@ -137,6 +139,8 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_retry_dns_error(self):\n+        if NON_EXISTING_RESOLVABLE:\n+            raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as log:\n             # try to fetch the homepage of a non-existent domain\n\n@@ -4,7 +4,7 @@ import shutil\n import sys\n import tempfile\n from typing import Optional, Type\n-from unittest import mock\n+from unittest import mock, SkipTest\n \n from testfixtures import LogCapture\n from twisted.cred import checkers, credentials, portal\n@@ -32,6 +32,7 @@ from scrapy.spiders import Spider\n from scrapy.utils.misc import create_instance\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler, skip_if_no_boto\n+from tests import NON_EXISTING_RESOLVABLE\n from tests.mockserver import (\n     Echo,\n     ForeverTakingResource,\n@@ -791,6 +792,8 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n     @defer.inlineCallbacks\n     def test_download_with_proxy_https_timeout(self):\n         \"\"\" Test TunnelingTCP4ClientEndpoint \"\"\"\n+        if NON_EXISTING_RESOLVABLE:\n+            raise SkipTest(\"Non-existing hosts are resolvable\")\n         http_proxy = self.getURL('')\n         domain = 'https://no-such-domain.nosuch'\n         request = Request(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e248360e6e3dbb36fab185caf131707195fa6a26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 91 | Files Changed: 5 | Hunks: 24 | Methods Changed: 6 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 144 | Churn Cumulative: 4314 | Contributors (this commit): 39 | Commits (past 90d): 10 | Contributors (cumulative): 72 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import marshal\n import pickle\n import tempfile\n import unittest\n+import dataclasses\n from io import BytesIO\n from datetime import datetime\n from warnings import catch_warnings, filterwarnings\n@@ -21,31 +22,30 @@ from scrapy.exporters import (\n )\n \n \n+def custom_serializer(value):\n+    return str(int(value) + 2)\n+\n+\n class TestItem(Item):\n     name = Field()\n     age = Field()\n \n \n-def custom_serializer(value):\n-    return str(int(value) + 2)\n-\n-\n class CustomFieldItem(Item):\n     name = Field()\n     age = Field(serializer=custom_serializer)\n \n \n-try:\n-    from dataclasses import make_dataclass, field\n-except ImportError:\n-    TestDataClass = None\n-    CustomFieldDataclass = None\n-else:\n-    TestDataClass = make_dataclass(\"TestDataClass\", [(\"name\", str), (\"age\", int)])\n-    CustomFieldDataclass = make_dataclass(\n-        \"CustomFieldDataclass\",\n-        [(\"name\", str), (\"age\", int, field(metadata={\"serializer\": custom_serializer}))]\n-    )\n+@dataclasses.dataclass\n+class TestDataClass:\n+    name: str\n+    age: int\n+\n+\n+@dataclasses.dataclass\n+class CustomFieldDataclass:\n+    name: str\n+    age: int = dataclasses.field(metadata={\"serializer\": custom_serializer})\n \n \n class BaseItemExporterTest(unittest.TestCase):\n@@ -54,8 +54,6 @@ class BaseItemExporterTest(unittest.TestCase):\n     custom_field_item_class = CustomFieldItem\n \n     def setUp(self):\n-        if self.item_class is None:\n-            raise unittest.SkipTest(\"item class is None\")\n         self.i = self.item_class(name='John\\xa3', age='22')\n         self.output = BytesIO()\n         self.ie = self._get_exporter()\n\n@@ -1,4 +1,5 @@\n import unittest\n+import dataclasses\n \n import attr\n from itemadapter import ItemAdapter\n@@ -10,13 +11,6 @@ from scrapy.loader import ItemLoader\n from scrapy.selector import Selector\n \n \n-try:\n-    from dataclasses import make_dataclass, field as dataclass_field\n-except ImportError:\n-    make_dataclass = None\n-    dataclass_field = None\n-\n-\n # test items\n class NameItem(Item):\n     name = Field()\n@@ -41,6 +35,11 @@ class AttrsNameItem:\n     name = attr.ib(default=\"\")\n \n \n+@dataclasses.dataclass\n+class TestDataClass:\n+    name: list = dataclasses.field(default_factory=list)\n+\n+\n # test item loaders\n class NameItemLoader(ItemLoader):\n     default_item_class = TestItem\n@@ -187,16 +186,8 @@ class InitializationFromAttrsItemTest(InitializationTestMixin, unittest.TestCase\n     item_class = AttrsNameItem\n \n \n-@unittest.skipIf(not make_dataclass, \"dataclasses module is not available\")\n class InitializationFromDataClassTest(InitializationTestMixin, unittest.TestCase):\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        if make_dataclass:\n-            self.item_class = make_dataclass(\n-                \"TestDataClass\",\n-                [(\"name\", list, dataclass_field(default_factory=list))],\n-            )\n+    item_class = TestDataClass\n \n \n class BaseNoInputReprocessingLoader(ItemLoader):\n\n@@ -7,6 +7,7 @@ from shutil import rmtree\n from tempfile import mkdtemp\n from unittest import mock, skipIf\n from urllib.parse import urlparse\n+import dataclasses\n \n import attr\n from itemadapter import ItemAdapter\n@@ -32,13 +33,6 @@ from scrapy.utils.test import (\n )\n \n \n-try:\n-    from dataclasses import make_dataclass, field as dataclass_field\n-except ImportError:\n-    make_dataclass = None\n-    dataclass_field = None\n-\n-\n def _mocked_download_func(request, info):\n     response = request.meta.get('response')\n     return response() if callable(response) else response\n@@ -226,24 +220,19 @@ class FilesPipelineTestCaseFieldsItem(FilesPipelineTestCaseFieldsMixin, unittest\n     item_class = FilesPipelineTestItem\n \n \n-@skipIf(not make_dataclass, \"dataclasses module is not available\")\n-class FilesPipelineTestCaseFieldsDataClass(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        if make_dataclass:\n-            self.item_class = make_dataclass(\n-                \"FilesPipelineTestDataClass\",\n-                [\n-                    (\"name\", str),\n+@dataclasses.dataclass\n+class FilesPipelineTestDataClass:\n+    name: str\n     # default fields\n-                    (\"file_urls\", list, dataclass_field(default_factory=list)),\n-                    (\"files\", list, dataclass_field(default_factory=list)),\n+    file_urls: list = dataclasses.field(default_factory=list)\n+    files: list = dataclasses.field(default_factory=list)\n     # overridden fields\n-                    (\"custom_file_urls\", list, dataclass_field(default_factory=list)),\n-                    (\"custom_files\", list, dataclass_field(default_factory=list)),\n-                ],\n-            )\n+    custom_file_urls: list = dataclasses.field(default_factory=list)\n+    custom_files: list = dataclasses.field(default_factory=list)\n+\n+\n+class FilesPipelineTestCaseFieldsDataClass(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+    item_class = FilesPipelineTestDataClass\n \n \n @attr.s\n\n@@ -4,6 +4,7 @@ import random\n from shutil import rmtree\n from tempfile import mkdtemp\n from unittest import skipIf\n+import dataclasses\n \n import attr\n from itemadapter import ItemAdapter\n@@ -16,13 +17,6 @@ from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n \n \n-try:\n-    from dataclasses import make_dataclass, field as dataclass_field\n-except ImportError:\n-    make_dataclass = None\n-    dataclass_field = None\n-\n-\n try:\n     from PIL import Image\n except ImportError:\n@@ -203,25 +197,19 @@ class ImagesPipelineTestCaseFieldsItem(ImagesPipelineTestCaseFieldsMixin, unitte\n     item_class = ImagesPipelineTestItem\n \n \n-@skipIf(not make_dataclass, \"dataclasses module is not available\")\n-class ImagesPipelineTestCaseFieldsDataClass(ImagesPipelineTestCaseFieldsMixin, unittest.TestCase):\n-    item_class = None\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        if make_dataclass:\n-            self.item_class = make_dataclass(\n-                \"FilesPipelineTestDataClass\",\n-                [\n-                    (\"name\", str),\n+@dataclasses.dataclass\n+class ImagesPipelineTestDataClass:\n+    name: str\n     # default fields\n-                    (\"image_urls\", list, dataclass_field(default_factory=list)),\n-                    (\"images\", list, dataclass_field(default_factory=list)),\n+    image_urls: list = dataclasses.field(default_factory=list)\n+    images: list = dataclasses.field(default_factory=list)\n     # overridden fields\n-                    (\"custom_image_urls\", list, dataclass_field(default_factory=list)),\n-                    (\"custom_images\", list, dataclass_field(default_factory=list)),\n-                ],\n-            )\n+    custom_image_urls: list = dataclasses.field(default_factory=list)\n+    custom_images: list = dataclasses.field(default_factory=list)\n+\n+\n+class ImagesPipelineTestCaseFieldsDataClass(ImagesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+    item_class = ImagesPipelineTestDataClass\n \n \n @attr.s\n\n@@ -1,6 +1,7 @@\n import datetime\n import json\n import unittest\n+import dataclasses\n from decimal import Decimal\n \n import attr\n@@ -10,12 +11,6 @@ from scrapy.http import Request, Response\n from scrapy.utils.serialize import ScrapyJSONEncoder\n \n \n-try:\n-    from dataclasses import make_dataclass\n-except ImportError:\n-    make_dataclass = None\n-\n-\n class JsonEncoderTestCase(unittest.TestCase):\n \n     def setUp(self):\n@@ -56,12 +51,13 @@ class JsonEncoderTestCase(unittest.TestCase):\n         self.assertIn(r.url, rs)\n         self.assertIn(str(r.status), rs)\n \n-    @unittest.skipIf(not make_dataclass, \"No dataclass support\")\n     def test_encode_dataclass_item(self):\n-        TestDataClass = make_dataclass(\n-            \"TestDataClass\",\n-            [(\"name\", str), (\"url\", str), (\"price\", int)],\n-        )\n+        @dataclasses.dataclass\n+        class TestDataClass:\n+            name: str\n+            url: str\n+            price: int\n+\n         item = TestDataClass(name=\"Product\", url=\"http://product.org\", price=1)\n         encoded = self.encoder.encode(item)\n         self.assertEqual(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#105468959363ee50b597038ac30fd32d3ea1b1f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1894 | Contributors (this commit): 27 | Commits (past 90d): 8 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ from datetime import datetime\n from io import BytesIO\n from shutil import rmtree\n from tempfile import mkdtemp\n-from unittest import mock, skipIf\n+from unittest import mock\n from urllib.parse import urlparse\n import dataclasses\n \n\n@@ -3,7 +3,6 @@ import io\n import random\n from shutil import rmtree\n from tempfile import mkdtemp\n-from unittest import skipIf\n import dataclasses\n \n import attr\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b103664bf45b079e5488b13a0737866de1b7dc50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 394 | Lines Deleted: 441 | Files Changed: 21 | Hunks: 260 | Methods Changed: 150 | Complexity Δ (Sum/Max): -30/2 | Churn Δ: 835 | Churn Cumulative: 12360 | Contributors (this commit): 60 | Commits (past 90d): 33 | Contributors (cumulative): 190 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,11 +5,11 @@ from twisted.python import failure\n from twisted.trial import unittest\n \n from scrapy import FormRequest\n-from scrapy.crawler import CrawlerRunner\n from scrapy.spidermiddlewares.httperror import HttpError\n from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.item import Item, Field\n+from scrapy.utils.test import get_crawler\n from scrapy.contracts import ContractsManager, Contract\n from scrapy.contracts.default import (\n     UrlContract,\n@@ -398,7 +398,7 @@ class ContractsManagerTest(unittest.TestCase):\n             TestSameUrlSpider.parse_first.__doc__ = contract_doc\n             TestSameUrlSpider.parse_second.__doc__ = contract_doc\n \n-            crawler = CrawlerRunner().create_crawler(TestSameUrlSpider)\n+            crawler = get_crawler(TestSameUrlSpider)\n             yield crawler.crawl()\n \n         self.assertEqual(crawler.spider.visited, 2)\n\n@@ -18,6 +18,7 @@ from scrapy.exceptions import StopDownload\n from scrapy.http import Request\n from scrapy.http.response import Response\n from scrapy.utils.python import to_unicode\n+from scrapy.utils.test import get_crawler\n from tests import NON_EXISTING_RESOLVABLE\n from tests.mockserver import MockServer\n from tests.spiders import (\n@@ -49,14 +50,13 @@ class CrawlTestCase(TestCase):\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n-        self.runner = CrawlerRunner()\n \n     def tearDown(self):\n         self.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_follow_all(self):\n-        crawler = self.runner.create_crawler(FollowAllSpider)\n+        crawler = get_crawler(FollowAllSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(len(crawler.spider.urls_visited), 11)  # 10 + start_url\n \n@@ -79,7 +79,7 @@ class CrawlTestCase(TestCase):\n \n         settings = {\"DOWNLOAD_DELAY\": delay,\n                     'RANDOMIZE_DOWNLOAD_DELAY': randomize}\n-        crawler = CrawlerRunner(settings).create_crawler(FollowAllSpider)\n+        crawler = get_crawler(FollowAllSpider, settings)\n         yield crawler.crawl(**crawl_kwargs)\n         times = crawler.spider.times\n         total_time = times[-1] - times[0]\n@@ -92,7 +92,7 @@ class CrawlTestCase(TestCase):\n         # of ``total`` and ``delay`` values that are too small for the test\n         # code above to have any meaning.\n         settings[\"DOWNLOAD_DELAY\"] = 0\n-        crawler = CrawlerRunner(settings).create_crawler(FollowAllSpider)\n+        crawler = get_crawler(FollowAllSpider, settings)\n         yield crawler.crawl(**crawl_kwargs)\n         times = crawler.spider.times\n         total_time = times[-1] - times[0]\n@@ -102,7 +102,7 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_timeout_success(self):\n-        crawler = self.runner.create_crawler(DelaySpider)\n+        crawler = get_crawler(DelaySpider)\n         yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n         self.assertTrue(crawler.spider.t1 > 0)\n         self.assertTrue(crawler.spider.t2 > 0)\n@@ -110,7 +110,7 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_timeout_failure(self):\n-        crawler = CrawlerRunner({\"DOWNLOAD_TIMEOUT\": 0.35}).create_crawler(DelaySpider)\n+        crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n         yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n         self.assertTrue(crawler.spider.t1 > 0)\n         self.assertTrue(crawler.spider.t2 == 0)\n@@ -125,14 +125,14 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_retry_503(self):\n-        crawler = self.runner.create_crawler(SimpleSpider)\n+        crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver)\n         self._assert_retried(log)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_failed(self):\n-        crawler = self.runner.create_crawler(SimpleSpider)\n+        crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(\"http://localhost:65432/status?n=503\", mockserver=self.mockserver)\n         self._assert_retried(log)\n@@ -141,7 +141,7 @@ class CrawlTestCase(TestCase):\n     def test_retry_dns_error(self):\n         if NON_EXISTING_RESOLVABLE:\n             raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n-        crawler = self.runner.create_crawler(SimpleSpider)\n+        crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n             # try to fetch the homepage of a non-existent domain\n             yield crawler.crawl(\"http://dns.resolution.invalid./\", mockserver=self.mockserver)\n@@ -150,7 +150,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_start_requests_bug_before_yield(self):\n         with LogCapture('scrapy', level=logging.ERROR) as log:\n-            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n+            crawler = get_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_before_yield=1, mockserver=self.mockserver)\n \n         self.assertEqual(len(log.records), 1)\n@@ -161,7 +161,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_start_requests_bug_yielding(self):\n         with LogCapture('scrapy', level=logging.ERROR) as log:\n-            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n+            crawler = get_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_yielding=1, mockserver=self.mockserver)\n \n         self.assertEqual(len(log.records), 1)\n@@ -172,7 +172,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_start_requests_lazyness(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n-        crawler = CrawlerRunner(settings).create_crawler(BrokenStartRequestsSpider)\n+        crawler = get_crawler(BrokenStartRequestsSpider, settings)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertTrue(\n             crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),\n@@ -181,7 +181,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_start_requests_dupes(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n-        crawler = CrawlerRunner(settings).create_crawler(DuplicateStartRequestsSpider)\n+        crawler = get_crawler(DuplicateStartRequestsSpider, settings)\n         yield crawler.crawl(dont_filter=True, distinct_urls=2, dupe_factor=3, mockserver=self.mockserver)\n         self.assertEqual(crawler.spider.visited, 6)\n \n@@ -210,7 +210,7 @@ Connection: close\n foo body\n with multiples lines\n '''})\n-        crawler = self.runner.create_crawler(SimpleSpider)\n+        crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(f\"/raw?{query}\"), mockserver=self.mockserver)\n         self.assertEqual(str(log).count(\"Got response 200\"), 1)\n@@ -218,7 +218,7 @@ with multiples lines\n     @defer.inlineCallbacks\n     def test_retry_conn_lost(self):\n         # connection lost after receiving data\n-        crawler = self.runner.create_crawler(SimpleSpider)\n+        crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/drop?abort=0\"), mockserver=self.mockserver)\n         self._assert_retried(log)\n@@ -226,7 +226,7 @@ with multiples lines\n     @defer.inlineCallbacks\n     def test_retry_conn_aborted(self):\n         # connection lost before receiving data\n-        crawler = self.runner.create_crawler(SimpleSpider)\n+        crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/drop?abort=1\"), mockserver=self.mockserver)\n         self._assert_retried(log)\n@@ -245,7 +245,7 @@ with multiples lines\n         req0.meta['next'] = req1\n         req1.meta['next'] = req2\n         req2.meta['next'] = req3\n-        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=req0, mockserver=self.mockserver)\n         # basic asserts in case of weird communication errors\n         self.assertIn('responses', crawler.spider.meta)\n@@ -271,7 +271,7 @@ with multiples lines\n         def cb(response):\n             est.append(get_engine_status(crawler.engine))\n \n-        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=self.mockserver.url('/'), callback_func=cb, mockserver=self.mockserver)\n         self.assertEqual(len(est), 1, est)\n         s = dict(est[0])\n@@ -286,7 +286,7 @@ with multiples lines\n         def cb(response):\n             est.append(format_engine_status(crawler.engine))\n \n-        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=self.mockserver.url('/'), callback_func=cb, mockserver=self.mockserver)\n         self.assertEqual(len(est), 1, est)\n         est = est[0].split(\"\\n\")[2:-2]  # remove header & footer\n@@ -317,7 +317,7 @@ with multiples lines\n             def start_requests(self):\n                 raise TestError\n \n-        crawler = self.runner.create_crawler(FaultySpider)\n+        crawler = get_crawler(FaultySpider)\n         yield self.assertFailure(crawler.crawl(mockserver=self.mockserver), TestError)\n         self.assertFalse(crawler.crawling)\n \n@@ -328,26 +328,28 @@ with multiples lines\n                 \"tests.pipelines.ZeroDivisionErrorPipeline\": 300,\n             }\n         }\n-        crawler = CrawlerRunner(settings).create_crawler(SimpleSpider)\n+        crawler = get_crawler(SimpleSpider, settings)\n         yield self.assertFailure(\n-            self.runner.crawl(crawler, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver),\n+            crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver),\n             ZeroDivisionError)\n         self.assertFalse(crawler.crawling)\n \n     @defer.inlineCallbacks\n     def test_crawlerrunner_accepts_crawler(self):\n-        crawler = self.runner.create_crawler(SimpleSpider)\n+        crawler = get_crawler(SimpleSpider)\n+        runner = CrawlerRunner()\n         with LogCapture() as log:\n-            yield self.runner.crawl(crawler, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+            yield runner.crawl(crawler, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n         self.assertIn(\"Got response 200\", str(log))\n \n     @defer.inlineCallbacks\n     def test_crawl_multiple(self):\n-        self.runner.crawl(SimpleSpider, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n-        self.runner.crawl(SimpleSpider, self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver)\n+        runner = CrawlerRunner({'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'})\n+        runner.crawl(SimpleSpider, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        runner.crawl(SimpleSpider, self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver)\n \n         with LogCapture() as log:\n-            yield self.runner.join()\n+            yield runner.join()\n \n         self._assert_retried(log)\n         self.assertIn(\"Got response 200\", str(log))\n@@ -358,7 +360,6 @@ class CrawlSpiderTestCase(TestCase):\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n-        self.runner = CrawlerRunner()\n \n     def tearDown(self):\n         self.mockserver.__exit__(None, None, None)\n@@ -370,7 +371,7 @@ class CrawlSpiderTestCase(TestCase):\n         def _on_item_scraped(item):\n             items.append(item)\n \n-        crawler = self.runner.create_crawler(spider_cls)\n+        crawler = get_crawler(spider_cls)\n         crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n         with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n@@ -378,10 +379,9 @@ class CrawlSpiderTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_crawlspider_with_parse(self):\n-        self.runner.crawl(CrawlSpiderWithParseMethod, mockserver=self.mockserver)\n-\n+        crawler = get_crawler(CrawlSpiderWithParseMethod)\n         with LogCapture() as log:\n-            yield self.runner.join()\n+            yield crawler.crawl(mockserver=self.mockserver)\n \n         self.assertIn(\"[parse] status 200 (foo: None)\", str(log))\n         self.assertIn(\"[parse] status 201 (foo: None)\", str(log))\n@@ -389,10 +389,9 @@ class CrawlSpiderTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_crawlspider_with_errback(self):\n-        self.runner.crawl(CrawlSpiderWithErrback, mockserver=self.mockserver)\n-\n+        crawler = get_crawler(CrawlSpiderWithErrback)\n         with LogCapture() as log:\n-            yield self.runner.join()\n+            yield crawler.crawl(mockserver=self.mockserver)\n \n         self.assertIn(\"[parse] status 200 (foo: None)\", str(log))\n         self.assertIn(\"[parse] status 201 (foo: None)\", str(log))\n@@ -403,18 +402,19 @@ class CrawlSpiderTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_async_def_parse(self):\n-        self.runner.crawl(AsyncDefSpider, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        crawler = get_crawler(AsyncDefSpider)\n         with LogCapture() as log:\n-            yield self.runner.join()\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n         self.assertIn(\"Got response 200\", str(log))\n \n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse(self):\n-        runner = CrawlerRunner({\"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"})\n-        runner.crawl(AsyncDefAsyncioSpider, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        crawler = get_crawler(AsyncDefAsyncioSpider, {\n+            \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+        })\n         with LogCapture() as log:\n-            yield runner.join()\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n         self.assertIn(\"Got response 200\", str(log))\n \n     @mark.only_asyncio()\n@@ -433,7 +433,7 @@ class CrawlSpiderTestCase(TestCase):\n         def _on_item_scraped(item):\n             items.append(item)\n \n-        crawler = self.runner.create_crawler(AsyncDefAsyncioReturnSingleElementSpider)\n+        crawler = get_crawler(AsyncDefAsyncioReturnSingleElementSpider)\n         crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n         with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n@@ -479,14 +479,14 @@ class CrawlSpiderTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_response_ssl_certificate_none(self):\n-        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/echo?body=test\", is_secure=False)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         self.assertIsNone(crawler.spider.meta['responses'][0].certificate)\n \n     @defer.inlineCallbacks\n     def test_response_ssl_certificate(self):\n-        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/echo?body=test\", is_secure=True)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         cert = crawler.spider.meta['responses'][0].certificate\n@@ -497,7 +497,7 @@ class CrawlSpiderTestCase(TestCase):\n     @mark.xfail(reason=\"Responses with no body return early and contain no certificate\")\n     @defer.inlineCallbacks\n     def test_response_ssl_certificate_empty_response(self):\n-        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/status?n=200\", is_secure=True)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         cert = crawler.spider.meta['responses'][0].certificate\n@@ -507,7 +507,7 @@ class CrawlSpiderTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_dns_server_ip_address_none(self):\n-        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url('/status?n=200')\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         ip_address = crawler.spider.meta['responses'][0].ip_address\n@@ -515,7 +515,7 @@ class CrawlSpiderTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_dns_server_ip_address(self):\n-        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url('/echo?body=test')\n         expected_netloc, _ = urlparse(url).netloc.split(':')\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n@@ -525,7 +525,7 @@ class CrawlSpiderTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_bytes_received_stop_download_callback(self):\n-        crawler = self.runner.create_crawler(BytesReceivedCallbackSpider)\n+        crawler = get_crawler(BytesReceivedCallbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n         self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n@@ -534,7 +534,7 @@ class CrawlSpiderTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_bytes_received_stop_download_errback(self):\n-        crawler = self.runner.create_crawler(BytesReceivedErrbackSpider)\n+        crawler = get_crawler(BytesReceivedErrbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIsNone(crawler.spider.meta.get(\"response\"))\n         self.assertIsInstance(crawler.spider.meta[\"failure\"], Failure)\n@@ -549,7 +549,7 @@ class CrawlSpiderTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_headers_received_stop_download_callback(self):\n-        crawler = self.runner.create_crawler(HeadersReceivedCallbackSpider)\n+        crawler = get_crawler(HeadersReceivedCallbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n         self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n@@ -557,7 +557,7 @@ class CrawlSpiderTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_headers_received_stop_download_errback(self):\n-        crawler = self.runner.create_crawler(HeadersReceivedErrbackSpider)\n+        crawler = get_crawler(HeadersReceivedErrbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIsNone(crawler.spider.meta.get(\"response\"))\n         self.assertIsInstance(crawler.spider.meta[\"failure\"], Failure)\n\n@@ -13,11 +13,13 @@ from twisted.trial import unittest\n \n import scrapy\n from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n+from scrapy.utils.test import get_crawler\n from scrapy.extensions.throttle import AutoThrottle\n from scrapy.extensions import telnet\n from scrapy.utils.test import get_testenv\n@@ -34,9 +36,6 @@ class BaseCrawlerTest(unittest.TestCase):\n \n class CrawlerTestCase(BaseCrawlerTest):\n \n-    def setUp(self):\n-        self.crawler = Crawler(DefaultSpider, Settings())\n-\n     def test_populate_spidercls_settings(self):\n         spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n         project_settings = {'TEST1': 'project', 'TEST3': 'project'}\n@@ -46,6 +45,8 @@ class CrawlerTestCase(BaseCrawlerTest):\n \n         settings = Settings()\n         settings.setdict(project_settings, priority='project')\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n             crawler = Crawler(CustomSettingsSpider, settings)\n \n         self.assertEqual(crawler.settings.get('TEST1'), 'spider')\n@@ -56,11 +57,13 @@ class CrawlerTestCase(BaseCrawlerTest):\n         self.assertTrue(crawler.settings.frozen)\n \n     def test_crawler_accepts_dict(self):\n-        crawler = Crawler(DefaultSpider, {'foo': 'bar'})\n+        crawler = get_crawler(DefaultSpider, {'foo': 'bar'})\n         self.assertEqual(crawler.settings['foo'], 'bar')\n         self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')\n \n     def test_crawler_accepts_None(self):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n             crawler = Crawler(DefaultSpider)\n         self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')\n \n@@ -77,7 +80,7 @@ class SpiderSettingsTestCase(unittest.TestCase):\n                 'AUTOTHROTTLE_ENABLED': True\n             }\n \n-        crawler = Crawler(MySpider, {})\n+        crawler = get_crawler(MySpider)\n         enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n         self.assertIn(AutoThrottle, enabled_exts)\n \n@@ -91,7 +94,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         class MySpider(scrapy.Spider):\n             name = 'spider'\n \n-        Crawler(MySpider, {})\n+        get_crawler(MySpider)\n         assert get_scrapy_root_handler() is None\n \n     def test_spider_custom_settings_log_level(self):\n@@ -111,7 +114,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n \n         configure_logging()\n         self.assertEqual(get_scrapy_root_handler().level, logging.DEBUG)\n-        crawler = Crawler(MySpider, {})\n+        crawler = get_crawler(MySpider)\n         self.assertEqual(get_scrapy_root_handler().level, logging.INFO)\n         info_count = crawler.stats.get_value('log_count/INFO')\n         logging.debug('debug message')\n@@ -148,7 +151,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n             }\n \n         configure_logging()\n-        Crawler(MySpider, {})\n+        get_crawler(MySpider)\n         logging.debug('debug message')\n \n         with open(log_file, 'rb') as fo:\n@@ -229,22 +232,25 @@ class NoRequestsSpider(scrapy.Spider):\n @mark.usefixtures('reactor_pytest')\n class CrawlerRunnerHasSpider(unittest.TestCase):\n \n+    def _runner(self):\n+        return CrawlerRunner({'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'})\n+\n     @defer.inlineCallbacks\n     def test_crawler_runner_bootstrap_successful(self):\n-        runner = CrawlerRunner()\n+        runner = self._runner()\n         yield runner.crawl(NoRequestsSpider)\n         self.assertEqual(runner.bootstrap_failed, False)\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_bootstrap_successful_for_several(self):\n-        runner = CrawlerRunner()\n+        runner = self._runner()\n         yield runner.crawl(NoRequestsSpider)\n         yield runner.crawl(NoRequestsSpider)\n         self.assertEqual(runner.bootstrap_failed, False)\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_bootstrap_failed(self):\n-        runner = CrawlerRunner()\n+        runner = self._runner()\n \n         try:\n             yield runner.crawl(ExceptionSpider)\n@@ -257,7 +263,7 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_bootstrap_failed_for_several(self):\n-        runner = CrawlerRunner()\n+        runner = self._runner()\n \n         try:\n             yield runner.crawl(ExceptionSpider)\n@@ -275,12 +281,14 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         if self.reactor_pytest == 'asyncio':\n             CrawlerRunner(settings={\n                 \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"VERSION\",\n             })\n         else:\n             msg = r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\"\n             with self.assertRaisesRegex(Exception, msg):\n                 runner = CrawlerRunner(settings={\n                     \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+                    \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"VERSION\",\n                 })\n                 yield runner.crawl(NoRequestsSpider)\n \n\n@@ -1,7 +1,9 @@\n import unittest\n \n+import pytest\n from w3lib.http import basic_auth_header\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\n from scrapy.spiders import Spider\n@@ -30,6 +32,8 @@ class HttpAuthMiddlewareLegacyTest(unittest.TestCase):\n         self.spider = TestSpiderLegacy('foo')\n \n     def test_auth(self):\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"Using HttpAuthMiddleware without http_auth_domain is deprecated\"):\n             mw = HttpAuthMiddleware()\n             mw.spider_opened(self.spider)\n \n@@ -49,6 +53,8 @@ class HttpAuthMiddlewareLegacyTest(unittest.TestCase):\n         self.assertNotIn('Authorization', req.headers)\n \n     def test_auth_already_set(self):\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"Using HttpAuthMiddleware without http_auth_domain is deprecated\"):\n             mw = HttpAuthMiddleware()\n             mw.spider_opened(self.spider)\n         req = Request('http://example.com/',\n\n@@ -1,13 +1,13 @@\n import os\n-from functools import partial\n+\n+import pytest\n from twisted.trial.unittest import TestCase\n \n from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Request\n from scrapy.spiders import Spider\n-from scrapy.crawler import Crawler\n-from scrapy.settings import Settings\n+from scrapy.utils.test import get_crawler\n \n spider = Spider('foo')\n \n@@ -23,9 +23,9 @@ class TestHttpProxyMiddleware(TestCase):\n         os.environ = self._oldenv\n \n     def test_not_enabled(self):\n-        settings = Settings({'HTTPPROXY_ENABLED': False})\n-        crawler = Crawler(Spider, settings)\n-        self.assertRaises(NotConfigured, partial(HttpProxyMiddleware.from_crawler, crawler))\n+        crawler = get_crawler(Spider, {'HTTPPROXY_ENABLED': False})\n+        with pytest.raises(NotConfigured):\n+            HttpProxyMiddleware.from_crawler(crawler)\n \n     def test_no_environment_proxies(self):\n         os.environ = {'dummy_proxy': 'reset_env_and_do_not_raise'}\n\n@@ -1,7 +1,9 @@\n+import warnings\n from itertools import product\n from unittest import TestCase\n \n from scrapy.downloadermiddlewares.stats import DownloaderStats\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n from scrapy.utils.response import response_httprepr\n@@ -54,7 +56,10 @@ class TestDownloaderStats(TestCase):\n         for test_response in test_responses:\n             self.crawler.stats.set_value('downloader/response_bytes', 0)\n             self.mw.process_response(self.req, test_response, self.spider)\n-            self.assertStatsEqual('downloader/response_bytes', len(response_httprepr(test_response)))\n+            with warnings.catch_warnings():\n+                warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+                resp_size = len(response_httprepr(test_response))\n+            self.assertStatsEqual('downloader/response_bytes', resp_size)\n \n     def test_process_exception(self):\n         self.mw.process_exception(self.req, MyException(), self.spider)\n\n@@ -10,7 +10,6 @@ from scrapy.dupefilters import RFPDupeFilter\n from scrapy.http import Request\n from scrapy.core.scheduler import Scheduler\n from scrapy.utils.python import to_bytes\n-from scrapy.utils.job import job_dir\n from scrapy.utils.test import get_crawler\n from tests.spiders import SimpleSpider\n \n@@ -29,8 +28,7 @@ class FromCrawlerRFPDupeFilter(RFPDupeFilter):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        debug = crawler.settings.getbool('DUPEFILTER_DEBUG')\n-        df = cls(job_dir(crawler.settings), debug)\n+        df = super().from_crawler(crawler)\n         df.method = 'from_crawler'\n         return df\n \n@@ -38,9 +36,8 @@ class FromCrawlerRFPDupeFilter(RFPDupeFilter):\n class FromSettingsRFPDupeFilter(RFPDupeFilter):\n \n     @classmethod\n-    def from_settings(cls, settings):\n-        debug = settings.getbool('DUPEFILTER_DEBUG')\n-        df = cls(job_dir(settings), debug)\n+    def from_settings(cls, settings, *, fingerprinter=None):\n+        df = super().from_settings(settings, fingerprinter=fingerprinter)\n         df.method = 'from_settings'\n         return df\n \n@@ -53,7 +50,8 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n     def test_df_from_crawler_scheduler(self):\n         settings = {'DUPEFILTER_DEBUG': True,\n-                    'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter}\n+                    'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,\n+                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n@@ -61,14 +59,16 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n     def test_df_from_settings_scheduler(self):\n         settings = {'DUPEFILTER_DEBUG': True,\n-                    'DUPEFILTER_CLASS': FromSettingsRFPDupeFilter}\n+                    'DUPEFILTER_CLASS': FromSettingsRFPDupeFilter,\n+                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, 'from_settings')\n \n     def test_df_direct_scheduler(self):\n-        settings = {'DUPEFILTER_CLASS': DirectDupeFilter}\n+        settings = {'DUPEFILTER_CLASS': DirectDupeFilter,\n+                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertEqual(scheduler.df.method, 'n/a')\n@@ -171,7 +171,8 @@ class RFPDupeFilterTest(unittest.TestCase):\n     def test_log(self):\n         with LogCapture() as log:\n             settings = {'DUPEFILTER_DEBUG': False,\n-                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter}\n+                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,\n+                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n             dupefilter = _get_dupefilter(crawler=crawler)\n@@ -197,7 +198,8 @@ class RFPDupeFilterTest(unittest.TestCase):\n     def test_log_debug(self):\n         with LogCapture() as log:\n             settings = {'DUPEFILTER_DEBUG': True,\n-                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter}\n+                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,\n+                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n             dupefilter = _get_dupefilter(crawler=crawler)\n@@ -230,7 +232,8 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n     def test_log_debug_default_dupefilter(self):\n         with LogCapture() as log:\n-            settings = {'DUPEFILTER_DEBUG': True}\n+            settings = {'DUPEFILTER_DEBUG': True,\n+                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n             dupefilter = _get_dupefilter(crawler=crawler)\n\n@@ -13,10 +13,11 @@ module with the ``runserver`` argument::\n import os\n import re\n import sys\n-import warnings\n from collections import defaultdict\n from urllib.parse import urlparse\n+from dataclasses import dataclass\n \n+import pytest\n import attr\n from itemadapter import ItemAdapter\n from pydispatch import dispatcher\n@@ -50,6 +51,13 @@ class AttrsItem:\n     price = attr.ib(default=0)\n \n \n+@dataclass\n+class DataClassItem:\n+    name: str = \"\"\n+    url: str = \"\"\n+    price: int = 0\n+\n+\n class TestSpider(Spider):\n     name = \"scrapytest.org\"\n     allowed_domains = [\"scrapytest.org\", \"localhost\"]\n@@ -92,17 +100,8 @@ class AttrsItemsSpider(TestSpider):\n     item_cls = AttrsItem\n \n \n-try:\n-    from dataclasses import make_dataclass\n-except ImportError:\n-    DataClassItemsSpider = None\n-else:\n-    TestDataClass = make_dataclass(\"TestDataClass\", [(\"name\", str), (\"url\", str), (\"price\", int)])\n-\n-    class DataClassItemsSpider(DictItemsSpider):  # type: ignore[no-redef]\n-        def parse_item(self, response):\n-            item = super().parse_item(response)\n-            return TestDataClass(**item)\n+class DataClassItemsSpider(TestSpider):\n+    item_cls = DataClassItem\n \n \n class ItemZeroDivisionErrorSpider(TestSpider):\n@@ -188,7 +187,7 @@ class CrawlerRun:\n         return self.deferred\n \n     def stop(self):\n-        self.port.stopListening()\n+        self.port.stopListening()  # FIXME: wait for this Deferred\n         for name, signal in vars(signals).items():\n             if not name.startswith('_'):\n                 disconnect_all(signal)\n@@ -239,79 +238,77 @@ class EngineTest(unittest.TestCase):\n     def test_crawler(self):\n \n         for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):\n-            if spider is None:\n-                continue\n-            self.run = CrawlerRun(spider)\n-            yield self.run.run()\n-            self._assert_visited_urls()\n-            self._assert_scheduled_requests(count=9)\n-            self._assert_downloaded_responses(count=9)\n-            self._assert_scraped_items()\n-            self._assert_signals_caught()\n-            self._assert_bytes_received()\n+            run = CrawlerRun(spider)\n+            yield run.run()\n+            self._assert_visited_urls(run)\n+            self._assert_scheduled_requests(run, count=9)\n+            self._assert_downloaded_responses(run, count=9)\n+            self._assert_scraped_items(run)\n+            self._assert_signals_caught(run)\n+            self._assert_bytes_received(run)\n \n     @defer.inlineCallbacks\n     def test_crawler_dupefilter(self):\n-        self.run = CrawlerRun(TestDupeFilterSpider)\n-        yield self.run.run()\n-        self._assert_scheduled_requests(count=8)\n-        self._assert_dropped_requests()\n+        run = CrawlerRun(TestDupeFilterSpider)\n+        yield run.run()\n+        self._assert_scheduled_requests(run, count=8)\n+        self._assert_dropped_requests(run)\n \n     @defer.inlineCallbacks\n     def test_crawler_itemerror(self):\n-        self.run = CrawlerRun(ItemZeroDivisionErrorSpider)\n-        yield self.run.run()\n-        self._assert_items_error()\n+        run = CrawlerRun(ItemZeroDivisionErrorSpider)\n+        yield run.run()\n+        self._assert_items_error(run)\n \n     @defer.inlineCallbacks\n     def test_crawler_change_close_reason_on_idle(self):\n-        self.run = CrawlerRun(ChangeCloseReasonSpider)\n-        yield self.run.run()\n-        self.assertEqual({'spider': self.run.spider, 'reason': 'custom_reason'},\n-                         self.run.signals_caught[signals.spider_closed])\n+        run = CrawlerRun(ChangeCloseReasonSpider)\n+        yield run.run()\n+        self.assertEqual({'spider': run.spider, 'reason': 'custom_reason'},\n+                         run.signals_caught[signals.spider_closed])\n \n-    def _assert_visited_urls(self):\n+    def _assert_visited_urls(self, run: CrawlerRun):\n         must_be_visited = [\"/\", \"/redirect\", \"/redirected\",\n                            \"/item1.html\", \"/item2.html\", \"/item999.html\"]\n-        urls_visited = {rp[0].url for rp in self.run.respplug}\n-        urls_expected = {self.run.geturl(p) for p in must_be_visited}\n+        urls_visited = {rp[0].url for rp in run.respplug}\n+        urls_expected = {run.geturl(p) for p in must_be_visited}\n         assert urls_expected <= urls_visited, f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n \n-    def _assert_scheduled_requests(self, count=None):\n-        self.assertEqual(count, len(self.run.reqplug))\n+    def _assert_scheduled_requests(self, run: CrawlerRun, count=None):\n+        self.assertEqual(count, len(run.reqplug))\n \n         paths_expected = ['/item999.html', '/item2.html', '/item1.html']\n \n-        urls_requested = {rq[0].url for rq in self.run.reqplug}\n-        urls_expected = {self.run.geturl(p) for p in paths_expected}\n+        urls_requested = {rq[0].url for rq in run.reqplug}\n+        urls_expected = {run.geturl(p) for p in paths_expected}\n         assert urls_expected <= urls_requested\n-        scheduled_requests_count = len(self.run.reqplug)\n-        dropped_requests_count = len(self.run.reqdropped)\n-        responses_count = len(self.run.respplug)\n+        scheduled_requests_count = len(run.reqplug)\n+        dropped_requests_count = len(run.reqdropped)\n+        responses_count = len(run.respplug)\n         self.assertEqual(scheduled_requests_count,\n                          dropped_requests_count + responses_count)\n-        self.assertEqual(len(self.run.reqreached),\n+        self.assertEqual(len(run.reqreached),\n                          responses_count)\n \n-    def _assert_dropped_requests(self):\n-        self.assertEqual(len(self.run.reqdropped), 1)\n+    def _assert_dropped_requests(self, run: CrawlerRun):\n+        self.assertEqual(len(run.reqdropped), 1)\n \n-    def _assert_downloaded_responses(self, count):\n+    def _assert_downloaded_responses(self, run: CrawlerRun, count):\n         # response tests\n-        self.assertEqual(count, len(self.run.respplug))\n-        self.assertEqual(count, len(self.run.reqreached))\n+        self.assertEqual(count, len(run.respplug))\n+        self.assertEqual(count, len(run.reqreached))\n \n-        for response, _ in self.run.respplug:\n-            if self.run.getpath(response.url) == '/item999.html':\n+        for response, _ in run.respplug:\n+            if run.getpath(response.url) == '/item999.html':\n                 self.assertEqual(404, response.status)\n-            if self.run.getpath(response.url) == '/redirect':\n+            if run.getpath(response.url) == '/redirect':\n                 self.assertEqual(302, response.status)\n \n-    def _assert_items_error(self):\n-        self.assertEqual(2, len(self.run.itemerror))\n-        for item, response, spider, failure in self.run.itemerror:\n+    def _assert_items_error(self, run: CrawlerRun):\n+        self.assertEqual(2, len(run.itemerror))\n+        for item, response, spider, failure in run.itemerror:\n             self.assertEqual(failure.value.__class__, ZeroDivisionError)\n-            self.assertEqual(spider, self.run.spider)\n+            self.assertEqual(spider, run.spider)\n \n             self.assertEqual(item['url'], response.url)\n             if 'item1.html' in item['url']:\n@@ -321,9 +318,9 @@ class EngineTest(unittest.TestCase):\n                 self.assertEqual('Item 2 name', item['name'])\n                 self.assertEqual('200', item['price'])\n \n-    def _assert_scraped_items(self):\n-        self.assertEqual(2, len(self.run.itemresp))\n-        for item, response in self.run.itemresp:\n+    def _assert_scraped_items(self, run: CrawlerRun):\n+        self.assertEqual(2, len(run.itemresp))\n+        for item, response in run.itemresp:\n             item = ItemAdapter(item)\n             self.assertEqual(item['url'], response.url)\n             if 'item1.html' in item['url']:\n@@ -333,26 +330,26 @@ class EngineTest(unittest.TestCase):\n                 self.assertEqual('Item 2 name', item['name'])\n                 self.assertEqual('200', item['price'])\n \n-    def _assert_headers_received(self):\n-        for headers in self.run.headers.values():\n+    def _assert_headers_received(self, run: CrawlerRun):\n+        for headers in run.headers.values():\n             self.assertIn(b\"Server\", headers)\n             self.assertIn(b\"TwistedWeb\", headers[b\"Server\"])\n             self.assertIn(b\"Date\", headers)\n             self.assertIn(b\"Content-Type\", headers)\n \n-    def _assert_bytes_received(self):\n-        self.assertEqual(9, len(self.run.bytes))\n-        for request, data in self.run.bytes.items():\n+    def _assert_bytes_received(self, run: CrawlerRun):\n+        self.assertEqual(9, len(run.bytes))\n+        for request, data in run.bytes.items():\n             joined_data = b\"\".join(data)\n-            if self.run.getpath(request.url) == \"/\":\n+            if run.getpath(request.url) == \"/\":\n                 self.assertEqual(joined_data, get_testdata(\"test_site\", \"index.html\"))\n-            elif self.run.getpath(request.url) == \"/item1.html\":\n+            elif run.getpath(request.url) == \"/item1.html\":\n                 self.assertEqual(joined_data, get_testdata(\"test_site\", \"item1.html\"))\n-            elif self.run.getpath(request.url) == \"/item2.html\":\n+            elif run.getpath(request.url) == \"/item2.html\":\n                 self.assertEqual(joined_data, get_testdata(\"test_site\", \"item2.html\"))\n-            elif self.run.getpath(request.url) == \"/redirected\":\n+            elif run.getpath(request.url) == \"/redirected\":\n                 self.assertEqual(joined_data, b\"Redirected here\")\n-            elif self.run.getpath(request.url) == '/redirect':\n+            elif run.getpath(request.url) == '/redirect':\n                 self.assertEqual(\n                     joined_data,\n                     b\"\\n<html>\\n\"\n@@ -364,7 +361,7 @@ class EngineTest(unittest.TestCase):\n                     b\"    </body>\\n\"\n                     b\"</html>\\n\"\n                 )\n-            elif self.run.getpath(request.url) == \"/tem999.html\":\n+            elif run.getpath(request.url) == \"/tem999.html\":\n                 self.assertEqual(\n                     joined_data,\n                     b\"\\n<html>\\n\"\n@@ -375,27 +372,27 @@ class EngineTest(unittest.TestCase):\n                     b\"  </body>\\n\"\n                     b\"</html>\\n\"\n                 )\n-            elif self.run.getpath(request.url) == \"/numbers\":\n+            elif run.getpath(request.url) == \"/numbers\":\n                 # signal was fired multiple times\n                 self.assertTrue(len(data) > 1)\n                 # bytes were received in order\n                 numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n                 self.assertEqual(joined_data, b\"\".join(numbers))\n \n-    def _assert_signals_caught(self):\n-        assert signals.engine_started in self.run.signals_caught\n-        assert signals.engine_stopped in self.run.signals_caught\n-        assert signals.spider_opened in self.run.signals_caught\n-        assert signals.spider_idle in self.run.signals_caught\n-        assert signals.spider_closed in self.run.signals_caught\n-        assert signals.headers_received in self.run.signals_caught\n+    def _assert_signals_caught(self, run: CrawlerRun):\n+        assert signals.engine_started in run.signals_caught\n+        assert signals.engine_stopped in run.signals_caught\n+        assert signals.spider_opened in run.signals_caught\n+        assert signals.spider_idle in run.signals_caught\n+        assert signals.spider_closed in run.signals_caught\n+        assert signals.headers_received in run.signals_caught\n \n-        self.assertEqual({'spider': self.run.spider},\n-                         self.run.signals_caught[signals.spider_opened])\n-        self.assertEqual({'spider': self.run.spider},\n-                         self.run.signals_caught[signals.spider_idle])\n-        self.assertEqual({'spider': self.run.spider, 'reason': 'finished'},\n-                         self.run.signals_caught[signals.spider_closed])\n+        self.assertEqual({'spider': run.spider},\n+                         run.signals_caught[signals.spider_opened])\n+        self.assertEqual({'spider': run.spider},\n+                         run.signals_caught[signals.spider_idle])\n+        self.assertEqual({'spider': run.spider, 'reason': 'finished'},\n+                         run.signals_caught[signals.spider_closed])\n \n     @defer.inlineCallbacks\n     def test_close_downloader(self):\n@@ -407,28 +404,29 @@ class EngineTest(unittest.TestCase):\n         e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n         yield e.open_spider(TestSpider(), [])\n         e.start()\n+        try:\n             yield self.assertFailure(e.start(), RuntimeError).addBoth(\n                 lambda exc: self.assertEqual(str(exc), \"Engine already running\")\n             )\n+        finally:\n             yield e.stop()\n \n     @defer.inlineCallbacks\n     def test_close_spiders_downloader(self):\n-        with warnings.catch_warnings(record=True) as warning_list:\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"ExecutionEngine.open_spiders is deprecated, \"\n+                                \"please use ExecutionEngine.spider instead\"):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             yield e.open_spider(TestSpider(), [])\n             self.assertEqual(len(e.open_spiders), 1)\n             yield e.close()\n             self.assertEqual(len(e.open_spiders), 0)\n-            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                str(warning_list[0].message),\n-                \"ExecutionEngine.open_spiders is deprecated, please use ExecutionEngine.spider instead\",\n-            )\n \n     @defer.inlineCallbacks\n     def test_close_engine_spiders_downloader(self):\n-        with warnings.catch_warnings(record=True) as warning_list:\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"ExecutionEngine.open_spiders is deprecated, \"\n+                                \"please use ExecutionEngine.spider instead\"):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             yield e.open_spider(TestSpider(), [])\n             e.start()\n@@ -436,61 +434,47 @@ class EngineTest(unittest.TestCase):\n             yield e.close()\n             self.assertFalse(e.running)\n             self.assertEqual(len(e.open_spiders), 0)\n-            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                str(warning_list[0].message),\n-                \"ExecutionEngine.open_spiders is deprecated, please use ExecutionEngine.spider instead\",\n-            )\n \n     @defer.inlineCallbacks\n     def test_crawl_deprecated_spider_arg(self):\n-        with warnings.catch_warnings(record=True) as warning_list:\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"Passing a 'spider' argument to \"\n+                                \"ExecutionEngine.crawl is deprecated\"):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             spider = TestSpider()\n             yield e.open_spider(spider, [])\n             e.start()\n             e.crawl(Request(\"data:,\"), spider)\n             yield e.close()\n-            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                str(warning_list[0].message),\n-                \"Passing a 'spider' argument to ExecutionEngine.crawl is deprecated\",\n-            )\n \n     @defer.inlineCallbacks\n     def test_download_deprecated_spider_arg(self):\n-        with warnings.catch_warnings(record=True) as warning_list:\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"Passing a 'spider' argument to \"\n+                                \"ExecutionEngine.download is deprecated\"):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             spider = TestSpider()\n             yield e.open_spider(spider, [])\n             e.start()\n             e.download(Request(\"data:,\"), spider)\n             yield e.close()\n-            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                str(warning_list[0].message),\n-                \"Passing a 'spider' argument to ExecutionEngine.download is deprecated\",\n-            )\n \n     @defer.inlineCallbacks\n     def test_deprecated_schedule(self):\n-        with warnings.catch_warnings(record=True) as warning_list:\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"ExecutionEngine.schedule is deprecated, please use \"\n+                                \"ExecutionEngine.crawl or ExecutionEngine.download instead\"):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             spider = TestSpider()\n             yield e.open_spider(spider, [])\n             e.start()\n             e.schedule(Request(\"data:,\"), spider)\n             yield e.close()\n-            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                str(warning_list[0].message),\n-                \"ExecutionEngine.schedule is deprecated, please use \"\n-                \"ExecutionEngine.crawl or ExecutionEngine.download instead\",\n-            )\n \n     @defer.inlineCallbacks\n     def test_deprecated_has_capacity(self):\n-        with warnings.catch_warnings(record=True) as warning_list:\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"ExecutionEngine.has_capacity is deprecated\"):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             self.assertTrue(e.has_capacity())\n             spider = TestSpider()\n@@ -499,8 +483,6 @@ class EngineTest(unittest.TestCase):\n             e.start()\n             yield e.close()\n             self.assertTrue(e.has_capacity())\n-            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n-            self.assertEqual(str(warning_list[0].message), \"ExecutionEngine.has_capacity is deprecated\")\n \n \n if __name__ == \"__main__\":\n\n@@ -23,36 +23,34 @@ class BytesReceivedEngineTest(EngineTest):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n         for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):\n-            if spider is None:\n-                continue\n-            self.run = BytesReceivedCrawlerRun(spider)\n+            run = BytesReceivedCrawlerRun(spider)\n             with LogCapture() as log:\n-                yield self.run.run()\n+                yield run.run()\n                 log.check_present((\"scrapy.core.downloader.handlers.http11\",\n                                    \"DEBUG\",\n-                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/redirected> \"\n+                                   f\"Download stopped for <GET http://localhost:{run.portno}/redirected> \"\n                                    \"from signal handler BytesReceivedCrawlerRun.bytes_received\"))\n                 log.check_present((\"scrapy.core.downloader.handlers.http11\",\n                                    \"DEBUG\",\n-                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/> \"\n+                                   f\"Download stopped for <GET http://localhost:{run.portno}/> \"\n                                    \"from signal handler BytesReceivedCrawlerRun.bytes_received\"))\n                 log.check_present((\"scrapy.core.downloader.handlers.http11\",\n                                    \"DEBUG\",\n-                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/numbers> \"\n+                                   f\"Download stopped for <GET http://localhost:{run.portno}/numbers> \"\n                                    \"from signal handler BytesReceivedCrawlerRun.bytes_received\"))\n-            self._assert_visited_urls()\n-            self._assert_scheduled_requests(count=9)\n-            self._assert_downloaded_responses(count=9)\n-            self._assert_signals_caught()\n-            self._assert_headers_received()\n-            self._assert_bytes_received()\n+            self._assert_visited_urls(run)\n+            self._assert_scheduled_requests(run, count=9)\n+            self._assert_downloaded_responses(run, count=9)\n+            self._assert_signals_caught(run)\n+            self._assert_headers_received(run)\n+            self._assert_bytes_received(run)\n \n-    def _assert_bytes_received(self):\n-        self.assertEqual(9, len(self.run.bytes))\n-        for request, data in self.run.bytes.items():\n+    def _assert_bytes_received(self, run: CrawlerRun):\n+        self.assertEqual(9, len(run.bytes))\n+        for request, data in run.bytes.items():\n             joined_data = b\"\".join(data)\n             self.assertTrue(len(data) == 1)  # signal was fired only once\n-            if self.run.getpath(request.url) == \"/numbers\":\n+            if run.getpath(request.url) == \"/numbers\":\n                 # Received bytes are not the complete response. The exact amount depends\n                 # on the buffer size, which can vary, so we only check that the amount\n                 # of received bytes is strictly less than the full response.\n\n@@ -23,34 +23,32 @@ class HeadersReceivedEngineTest(EngineTest):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n         for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):\n-            if spider is None:\n-                continue\n-            self.run = HeadersReceivedCrawlerRun(spider)\n+            run = HeadersReceivedCrawlerRun(spider)\n             with LogCapture() as log:\n-                yield self.run.run()\n+                yield run.run()\n                 log.check_present((\"scrapy.core.downloader.handlers.http11\",\n                                    \"DEBUG\",\n-                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/redirected> from\"\n+                                   f\"Download stopped for <GET http://localhost:{run.portno}/redirected> from\"\n                                    \" signal handler HeadersReceivedCrawlerRun.headers_received\"))\n                 log.check_present((\"scrapy.core.downloader.handlers.http11\",\n                                    \"DEBUG\",\n-                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/> from signal\"\n+                                   f\"Download stopped for <GET http://localhost:{run.portno}/> from signal\"\n                                    \" handler HeadersReceivedCrawlerRun.headers_received\"))\n                 log.check_present((\"scrapy.core.downloader.handlers.http11\",\n                                    \"DEBUG\",\n-                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/numbers> from\"\n+                                   f\"Download stopped for <GET http://localhost:{run.portno}/numbers> from\"\n                                    \" signal handler HeadersReceivedCrawlerRun.headers_received\"))\n-            self._assert_visited_urls()\n-            self._assert_downloaded_responses(count=6)\n-            self._assert_signals_caught()\n-            self._assert_bytes_received()\n-            self._assert_headers_received()\n+            self._assert_visited_urls(run)\n+            self._assert_downloaded_responses(run, count=6)\n+            self._assert_signals_caught(run)\n+            self._assert_bytes_received(run)\n+            self._assert_headers_received(run)\n \n-    def _assert_bytes_received(self):\n-        self.assertEqual(0, len(self.run.bytes))\n+    def _assert_bytes_received(self, run: CrawlerRun):\n+        self.assertEqual(0, len(run.bytes))\n \n-    def _assert_visited_urls(self):\n+    def _assert_visited_urls(self, run: CrawlerRun):\n         must_be_visited = [\"/\", \"/redirect\", \"/redirected\"]\n-        urls_visited = {rp[0].url for rp in self.run.respplug}\n-        urls_expected = {self.run.geturl(p) for p in must_be_visited}\n+        urls_visited = {rp[0].url for rp in run.respplug}\n+        urls_expected = {run.geturl(p) for p in must_be_visited}\n         assert urls_expected <= urls_visited, f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n\n@@ -22,6 +22,7 @@ from urllib.parse import urljoin, quote\n from urllib.request import pathname2url\n \n import lxml.etree\n+import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial import unittest\n@@ -30,7 +31,6 @@ from zope.interface import implementer\n from zope.interface.verify import verifyObject\n \n import scrapy\n-from scrapy.crawler import CrawlerRunner\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.exporters import CsvItemExporter\n from scrapy.extensions.feedexport import (\n@@ -697,9 +697,9 @@ class FeedExportTest(FeedExportTestBase):\n         content = {}\n         try:\n             with MockServer() as s:\n-                runner = CrawlerRunner(Settings(settings))\n                 spider_cls.start_urls = [s.url('/')]\n-                yield runner.crawl(spider_cls)\n+                crawler = get_crawler(spider_cls, settings)\n+                yield crawler.crawl()\n \n             for file_path, feed_options in FEEDS.items():\n                 if not os.path.exists(str(file_path)):\n@@ -1554,9 +1554,9 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n         content = {}\n         try:\n             with MockServer() as s:\n-                runner = CrawlerRunner(Settings(settings))\n                 spider_cls.start_urls = [s.url('/')]\n-                yield runner.crawl(spider_cls)\n+                crawler = get_crawler(spider_cls, settings)\n+                yield crawler.crawl()\n \n             for file_path, feed_options in FEEDS.items():\n                 if not os.path.exists(str(file_path)):\n@@ -2026,9 +2026,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         content = defaultdict(list)\n         try:\n             with MockServer() as s:\n-                runner = CrawlerRunner(Settings(settings))\n                 spider_cls.start_urls = [s.url('/')]\n-                yield runner.crawl(spider_cls)\n+                crawler = get_crawler(spider_cls, settings)\n+                yield crawler.crawl()\n \n             for path, feed in FEEDS.items():\n                 dir_name = os.path.dirname(path)\n@@ -2048,7 +2048,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'jl', self._file_mark): {'format': 'jl'},\n             },\n         })\n-        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         for batch in data['jl']:\n@@ -2064,7 +2064,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {'format': 'csv'},\n             },\n         })\n-        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         data = yield self.exported_data(items, settings)\n         for batch in data['csv']:\n             got_batch = csv.DictReader(to_unicode(batch).splitlines())\n@@ -2080,7 +2080,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {'format': 'xml'},\n             },\n         })\n-        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         for batch in data['xml']:\n@@ -2098,7 +2098,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'json', self._file_mark): {'format': 'json'},\n             },\n         })\n-        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         # XML\n@@ -2123,7 +2123,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'pickle', self._file_mark): {'format': 'pickle'},\n             },\n         })\n-        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         import pickle\n@@ -2140,7 +2140,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'marshal', self._file_mark): {'format': 'marshal'},\n             },\n         })\n-        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         import marshal\n@@ -2166,7 +2166,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             'FEED_EXPORT_BATCH_ITEM_COUNT': 2\n         }\n         header = self.MyItem.fields.keys()\n-        yield self.assertExported(items, header, rows, settings=Settings(settings))\n+        yield self.assertExported(items, header, rows, settings=settings)\n \n     def test_wrong_path(self):\n         \"\"\" If path is without %(batch_time)s and %(batch_id) an exception must be raised \"\"\"\n@@ -2382,9 +2382,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                     yield item\n \n         with MockServer() as server:\n-            runner = CrawlerRunner(Settings(settings))\n             TestSpider.start_urls = [server.url('/')]\n-            yield runner.crawl(TestSpider)\n+            crawler = get_crawler(TestSpider, settings)\n+            yield crawler.crawl()\n \n         self.assertEqual(len(CustomS3FeedStorage.stubs), len(items) + 1)\n         for stub in CustomS3FeedStorage.stubs[:-1]:\n@@ -2434,25 +2434,16 @@ class StdoutFeedStoragePreFeedOptionsTest(unittest.TestCase):\n                 'file': StdoutFeedStorageWithoutFeedOptions\n             },\n         }\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\"):\n             crawler = get_crawler(settings_dict=settings_dict)\n             feed_exporter = FeedExporter.from_crawler(crawler)\n+\n         spider = scrapy.Spider(\"default\")\n-        with warnings.catch_warnings(record=True) as w:\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"StdoutFeedStorageWithoutFeedOptions does not support \"\n+                                \"the 'feed_options' keyword argument.\"):\n             feed_exporter.open_spider(spider)\n-            messages = tuple(str(item.message) for item in w\n-                             if item.category is ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                messages,\n-                (\n-                    (\n-                        \"StdoutFeedStorageWithoutFeedOptions does not support \"\n-                        \"the 'feed_options' keyword argument. Add a \"\n-                        \"'feed_options' parameter to its signature to remove \"\n-                        \"this warning. This parameter will become mandatory \"\n-                        \"in a future version of Scrapy.\"\n-                    ),\n-                )\n-            )\n \n \n class FileFeedStorageWithoutFeedOptions(FileFeedStorage):\n@@ -2476,25 +2467,16 @@ class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):\n                     'file': FileFeedStorageWithoutFeedOptions\n                 },\n             }\n+            with pytest.warns(ScrapyDeprecationWarning,\n+                              match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\"):\n                 crawler = get_crawler(settings_dict=settings_dict)\n                 feed_exporter = FeedExporter.from_crawler(crawler)\n         spider = scrapy.Spider(\"default\")\n-        with warnings.catch_warnings(record=True) as w:\n+\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"FileFeedStorageWithoutFeedOptions does not support \"\n+                                \"the 'feed_options' keyword argument.\"):\n             feed_exporter.open_spider(spider)\n-            messages = tuple(str(item.message) for item in w\n-                             if item.category is ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                messages,\n-                (\n-                    (\n-                        \"FileFeedStorageWithoutFeedOptions does not support \"\n-                        \"the 'feed_options' keyword argument. Add a \"\n-                        \"'feed_options' parameter to its signature to remove \"\n-                        \"this warning. This parameter will become mandatory \"\n-                        \"in a future version of Scrapy.\"\n-                    ),\n-                )\n-            )\n \n \n class S3FeedStorageWithoutFeedOptions(S3FeedStorage):\n@@ -2524,26 +2506,18 @@ class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n                 'file': S3FeedStorageWithoutFeedOptions\n             },\n         }\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\"):\n             crawler = get_crawler(settings_dict=settings_dict)\n             feed_exporter = FeedExporter.from_crawler(crawler)\n+\n         spider = scrapy.Spider(\"default\")\n         spider.crawler = crawler\n-        with warnings.catch_warnings(record=True) as w:\n+\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"S3FeedStorageWithoutFeedOptions does not support \"\n+                                \"the 'feed_options' keyword argument.\"):\n             feed_exporter.open_spider(spider)\n-            messages = tuple(str(item.message) for item in w\n-                             if item.category is ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                messages,\n-                (\n-                    (\n-                        \"S3FeedStorageWithoutFeedOptions does not support \"\n-                        \"the 'feed_options' keyword argument. Add a \"\n-                        \"'feed_options' parameter to its signature to remove \"\n-                        \"this warning. This parameter will become mandatory \"\n-                        \"in a future version of Scrapy.\"\n-                    ),\n-                )\n-            )\n \n     def test_from_crawler(self):\n         settings_dict = {\n@@ -2552,26 +2526,18 @@ class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n                 'file': S3FeedStorageWithoutFeedOptionsWithFromCrawler\n             },\n         }\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\"):\n             crawler = get_crawler(settings_dict=settings_dict)\n             feed_exporter = FeedExporter.from_crawler(crawler)\n+\n         spider = scrapy.Spider(\"default\")\n         spider.crawler = crawler\n-        with warnings.catch_warnings(record=True) as w:\n+\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"S3FeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler does not support \"\n+                                \"the 'feed_options' keyword argument.\"):\n             feed_exporter.open_spider(spider)\n-            messages = tuple(str(item.message) for item in w\n-                             if item.category is ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                messages,\n-                (\n-                    (\n-                        \"S3FeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler \"\n-                        \"does not support the 'feed_options' keyword argument. Add a \"\n-                        \"'feed_options' parameter to its signature to remove \"\n-                        \"this warning. This parameter will become mandatory \"\n-                        \"in a future version of Scrapy.\"\n-                    ),\n-                )\n-            )\n \n \n class FTPFeedStorageWithoutFeedOptions(FTPFeedStorage):\n@@ -2601,26 +2567,18 @@ class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n                 'file': FTPFeedStorageWithoutFeedOptions\n             },\n         }\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\"):\n             crawler = get_crawler(settings_dict=settings_dict)\n             feed_exporter = FeedExporter.from_crawler(crawler)\n+\n         spider = scrapy.Spider(\"default\")\n         spider.crawler = crawler\n-        with warnings.catch_warnings(record=True) as w:\n+\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"FTPFeedStorageWithoutFeedOptions does not support \"\n+                                \"the 'feed_options' keyword argument.\"):\n             feed_exporter.open_spider(spider)\n-            messages = tuple(str(item.message) for item in w\n-                             if item.category is ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                messages,\n-                (\n-                    (\n-                        \"FTPFeedStorageWithoutFeedOptions does not support \"\n-                        \"the 'feed_options' keyword argument. Add a \"\n-                        \"'feed_options' parameter to its signature to remove \"\n-                        \"this warning. This parameter will become mandatory \"\n-                        \"in a future version of Scrapy.\"\n-                    ),\n-                )\n-            )\n \n     def test_from_crawler(self):\n         settings_dict = {\n@@ -2629,50 +2587,50 @@ class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n                 'file': FTPFeedStorageWithoutFeedOptionsWithFromCrawler\n             },\n         }\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\"):\n             crawler = get_crawler(settings_dict=settings_dict)\n             feed_exporter = FeedExporter.from_crawler(crawler)\n+\n         spider = scrapy.Spider(\"default\")\n         spider.crawler = crawler\n-        with warnings.catch_warnings(record=True) as w:\n+\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"FTPFeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler does not support \"\n+                                \"the 'feed_options' keyword argument.\"):\n             feed_exporter.open_spider(spider)\n-            messages = tuple(str(item.message) for item in w\n-                             if item.category is ScrapyDeprecationWarning)\n-            self.assertEqual(\n-                messages,\n-                (\n-                    (\n-                        \"FTPFeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler \"\n-                        \"does not support the 'feed_options' keyword argument. Add a \"\n-                        \"'feed_options' parameter to its signature to remove \"\n-                        \"this warning. This parameter will become mandatory \"\n-                        \"in a future version of Scrapy.\"\n-                    ),\n-                )\n-            )\n \n \n class URIParamsTest:\n \n     spider_name = \"uri_params_spider\"\n+    deprecated_options = False\n \n     def build_settings(self, uri='file:///tmp/foobar', uri_params=None):\n         raise NotImplementedError\n \n+    def _crawler_feed_exporter(self, settings):\n+        if self.deprecated_options:\n+            with pytest.warns(ScrapyDeprecationWarning,\n+                              match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\"):\n+                crawler = get_crawler(settings_dict=settings)\n+                feed_exporter = FeedExporter.from_crawler(crawler)\n+        else:\n+            crawler = get_crawler(settings_dict=settings)\n+            feed_exporter = FeedExporter.from_crawler(crawler)\n+        return crawler, feed_exporter\n+\n     def test_default(self):\n         settings = self.build_settings(\n             uri='file:///tmp/%(name)s',\n         )\n-        crawler = get_crawler(settings_dict=settings)\n-        feed_exporter = FeedExporter.from_crawler(crawler)\n+        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n         spider = scrapy.Spider(self.spider_name)\n         spider.crawler = crawler\n-        with warnings.catch_warnings(record=True) as w:\n+\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n             feed_exporter.open_spider(spider)\n-            messages = tuple(\n-                str(item.message) for item in w\n-                if item.category is ScrapyDeprecationWarning\n-            )\n-            self.assertEqual(messages, tuple())\n \n         self.assertEqual(\n             feed_exporter.slots[0].uri,\n@@ -2687,28 +2645,13 @@ class URIParamsTest:\n             uri='file:///tmp/%(name)s',\n             uri_params=uri_params,\n         )\n-        crawler = get_crawler(settings_dict=settings)\n-        feed_exporter = FeedExporter.from_crawler(crawler)\n+        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n         spider = scrapy.Spider(self.spider_name)\n         spider.crawler = crawler\n-        with warnings.catch_warnings(record=True) as w:\n+\n+        with pytest.warns(ScrapyDeprecationWarning,\n+                          match=\"Modifying the params dictionary in-place\"):\n             feed_exporter.open_spider(spider)\n-            messages = tuple(\n-                str(item.message) for item in w\n-                if item.category is ScrapyDeprecationWarning\n-            )\n-            self.assertEqual(\n-                messages,\n-                (\n-                    (\n-                        'Modifying the params dictionary in-place in the '\n-                        'function defined in the FEED_URI_PARAMS setting or '\n-                        'in the uri_params key of the FEEDS setting is '\n-                        'deprecated. The function must return a new '\n-                        'dictionary instead.'\n-                    ),\n-                )\n-            )\n \n         self.assertEqual(\n             feed_exporter.slots[0].uri,\n@@ -2723,18 +2666,14 @@ class URIParamsTest:\n             uri='file:///tmp/%(name)s',\n             uri_params=uri_params,\n         )\n-        crawler = get_crawler(settings_dict=settings)\n-        feed_exporter = FeedExporter.from_crawler(crawler)\n+        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n         spider = scrapy.Spider(self.spider_name)\n         spider.crawler = crawler\n-        with warnings.catch_warnings(record=True) as w:\n+\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n             with self.assertRaises(KeyError):\n                 feed_exporter.open_spider(spider)\n-            messages = tuple(\n-                str(item.message) for item in w\n-                if item.category is ScrapyDeprecationWarning\n-            )\n-            self.assertEqual(messages, tuple())\n \n     def test_params_as_is(self):\n         def uri_params(params, spider):\n@@ -2744,17 +2683,12 @@ class URIParamsTest:\n             uri='file:///tmp/%(name)s',\n             uri_params=uri_params,\n         )\n-        crawler = get_crawler(settings_dict=settings)\n-        feed_exporter = FeedExporter.from_crawler(crawler)\n+        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n         spider = scrapy.Spider(self.spider_name)\n         spider.crawler = crawler\n-        with warnings.catch_warnings(record=True) as w:\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n             feed_exporter.open_spider(spider)\n-            messages = tuple(\n-                str(item.message) for item in w\n-                if item.category is ScrapyDeprecationWarning\n-            )\n-            self.assertEqual(messages, tuple())\n \n         self.assertEqual(\n             feed_exporter.slots[0].uri,\n@@ -2769,17 +2703,12 @@ class URIParamsTest:\n             uri='file:///tmp/%(foo)s',\n             uri_params=uri_params,\n         )\n-        crawler = get_crawler(settings_dict=settings)\n-        feed_exporter = FeedExporter.from_crawler(crawler)\n+        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n         spider = scrapy.Spider(self.spider_name)\n         spider.crawler = crawler\n-        with warnings.catch_warnings(record=True) as w:\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n             feed_exporter.open_spider(spider)\n-            messages = tuple(\n-                str(item.message) for item in w\n-                if item.category is ScrapyDeprecationWarning\n-            )\n-            self.assertEqual(messages, tuple())\n \n         self.assertEqual(\n             feed_exporter.slots[0].uri,\n@@ -2788,6 +2717,7 @@ class URIParamsTest:\n \n \n class URIParamsSettingTest(URIParamsTest, unittest.TestCase):\n+    deprecated_options = True\n \n     def build_settings(self, uri='file:///tmp/foobar', uri_params=None):\n         extra_settings = {}\n@@ -2800,6 +2730,7 @@ class URIParamsSettingTest(URIParamsTest, unittest.TestCase):\n \n \n class URIParamsFeedOptionTest(URIParamsTest, unittest.TestCase):\n+    deprecated_options = False\n \n     def build_settings(self, uri='file:///tmp/foobar', uri_params=None):\n         options = {\n\n@@ -5,8 +5,8 @@ from twisted.internet import defer\n from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase as TwistedTestCase\n \n-from scrapy.crawler import CrawlerRunner\n from scrapy.exceptions import DropItem\n+from scrapy.utils.test import get_crawler\n from scrapy.http import Request, Response\n from scrapy.item import Item, Field\n from scrapy.logformatter import LogFormatter\n@@ -202,7 +202,7 @@ class ShowOrSkipMessagesTestCase(TwistedTestCase):\n \n     @defer.inlineCallbacks\n     def test_show_messages(self):\n-        crawler = CrawlerRunner(self.base_settings).create_crawler(ItemSpider)\n+        crawler = get_crawler(ItemSpider, self.base_settings)\n         with LogCapture() as lc:\n             yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIn(\"Scraped from <200 http://127.0.0.1:\", str(lc))\n@@ -213,7 +213,7 @@ class ShowOrSkipMessagesTestCase(TwistedTestCase):\n     def test_skip_messages(self):\n         settings = self.base_settings.copy()\n         settings['LOG_FORMATTER'] = SkipMessagesLogFormatter\n-        crawler = CrawlerRunner(settings).create_crawler(ItemSpider)\n+        crawler = get_crawler(ItemSpider, settings)\n         with LogCapture() as lc:\n             yield crawler.crawl(mockserver=self.mockserver)\n         self.assertNotIn(\"Scraped from <200 http://127.0.0.1:\", str(lc))\n\n@@ -64,6 +64,7 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.tmpmediastore = self.mktemp()\n         os.mkdir(self.tmpmediastore)\n         self.settings = {\n+            'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION',\n             'ITEM_PIPELINES': {self.pipeline_class: 1},\n             self.store_setting_key: self.tmpmediastore,\n         }\n@@ -78,8 +79,10 @@ class FileDownloadCrawlTestCase(TestCase):\n     def _on_item_scraped(self, item):\n         self.items.append(item)\n \n-    def _create_crawler(self, spider_class, **kwargs):\n-        crawler = self.runner.create_crawler(spider_class, **kwargs)\n+    def _create_crawler(self, spider_class, runner=None, **kwargs):\n+        if runner is None:\n+            runner = self.runner\n+        crawler = runner.create_crawler(spider_class, **kwargs)\n         crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n         return crawler\n \n@@ -167,9 +170,8 @@ class FileDownloadCrawlTestCase(TestCase):\n     def test_download_media_redirected_allowed(self):\n         settings = dict(self.settings)\n         settings.update({'MEDIA_ALLOW_REDIRECTS': True})\n-        self.runner = CrawlerRunner(settings)\n-\n-        crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n+        runner = CrawlerRunner(settings)\n+        crawler = self._create_crawler(RedirectedMediaDownloadSpider, runner=runner)\n         with LogCapture() as log:\n             yield crawler.crawl(\n                 self.mockserver.url(\"/files/images/\"),\n\n@@ -2,8 +2,8 @@ from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n from scrapy import Request, signals\n-from scrapy.crawler import CrawlerRunner\n from scrapy.http.response import Response\n+from scrapy.utils.test import get_crawler\n \n from testfixtures import LogCapture\n \n@@ -71,7 +71,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_response_200(self):\n         url = self.mockserver.url(\"/status?n=200\")\n-        crawler = CrawlerRunner().create_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         response = crawler.spider.meta[\"responses\"][0]\n         self.assertEqual(response.request.url, url)\n@@ -80,7 +80,7 @@ class CrawlTestCase(TestCase):\n     def test_response_error(self):\n         for status in (\"404\", \"500\"):\n             url = self.mockserver.url(f\"/status?n={status}\")\n-            crawler = CrawlerRunner().create_crawler(SingleRequestSpider)\n+            crawler = get_crawler(SingleRequestSpider)\n             yield crawler.crawl(seed=url, mockserver=self.mockserver)\n             failure = crawler.spider.meta[\"failure\"]\n             response = failure.value.response\n@@ -90,12 +90,11 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_downloader_middleware_raise_exception(self):\n         url = self.mockserver.url(\"/status?n=200\")\n-        runner = CrawlerRunner(settings={\n+        crawler = get_crawler(SingleRequestSpider, {\n             \"DOWNLOADER_MIDDLEWARES\": {\n                 RaiseExceptionRequestMiddleware: 590,\n             },\n         })\n-        crawler = runner.create_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         failure = crawler.spider.meta[\"failure\"]\n         self.assertEqual(failure.request.url, url)\n@@ -117,12 +116,11 @@ class CrawlTestCase(TestCase):\n             signal_params[\"request\"] = request\n \n         url = self.mockserver.url(\"/status?n=200\")\n-        runner = CrawlerRunner(settings={\n+        crawler = get_crawler(SingleRequestSpider, {\n             \"DOWNLOADER_MIDDLEWARES\": {\n                 ProcessResponseMiddleware: 595,\n             }\n         })\n-        crawler = runner.create_crawler(SingleRequestSpider)\n         crawler.signals.connect(signal_handler, signal=signals.response_received)\n \n         with LogCapture() as log:\n@@ -147,13 +145,12 @@ class CrawlTestCase(TestCase):\n         The spider callback should receive the overridden response.request\n         \"\"\"\n         url = self.mockserver.url(\"/status?n=200\")\n-        runner = CrawlerRunner(settings={\n+        crawler = get_crawler(SingleRequestSpider, {\n             \"DOWNLOADER_MIDDLEWARES\": {\n                 RaiseExceptionRequestMiddleware: 590,\n                 CatchExceptionOverrideRequestMiddleware: 595,\n             },\n         })\n-        crawler = runner.create_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         response = crawler.spider.meta[\"responses\"][0]\n         self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n@@ -168,13 +165,12 @@ class CrawlTestCase(TestCase):\n         The spider callback should receive the original response.request\n         \"\"\"\n         url = self.mockserver.url(\"/status?n=200\")\n-        runner = CrawlerRunner(settings={\n+        crawler = get_crawler(SingleRequestSpider, {\n             \"DOWNLOADER_MIDDLEWARES\": {\n                 RaiseExceptionRequestMiddleware: 590,\n                 CatchExceptionDoNotOverrideRequestMiddleware: 595,\n             },\n         })\n-        crawler = runner.create_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         response = crawler.spider.meta[\"responses\"][0]\n         self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n@@ -186,12 +182,11 @@ class CrawlTestCase(TestCase):\n         Downloader middleware which returns a response with a\n         specific 'request' attribute, with an alternative callback\n         \"\"\"\n-        runner = CrawlerRunner(settings={\n+        crawler = get_crawler(AlternativeCallbacksSpider, {\n             \"DOWNLOADER_MIDDLEWARES\": {\n                 AlternativeCallbacksMiddleware: 595,\n             }\n         })\n-        crawler = runner.create_crawler(AlternativeCallbacksSpider)\n \n         with LogCapture() as log:\n             url = self.mockserver.url(\"/status?n=200\")\n\n@@ -3,7 +3,7 @@ from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n from scrapy.http import Request\n-from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.test import get_crawler\n from tests.spiders import MockServerSpider\n from tests.mockserver import MockServer\n \n@@ -140,14 +140,13 @@ class CallbackKeywordArgumentsTestCase(TestCase):\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n-        self.runner = CrawlerRunner()\n \n     def tearDown(self):\n         self.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_callback_kwargs(self):\n-        crawler = self.runner.create_crawler(KeywordArgumentsSpider)\n+        crawler = get_crawler(KeywordArgumentsSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n         self.assertTrue(all(crawler.spider.checks))\n\n@@ -52,6 +52,7 @@ class MockCrawler(Crawler):\n             SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,\n             JOBDIR=jobdir,\n             DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter',\n+            REQUEST_FINGERPRINTER_IMPLEMENTATION='VERSION',\n         )\n         super().__init__(Spider, settings)\n         self.engine = MockEngine(downloader=MockDownloader())\n@@ -334,7 +335,7 @@ class TestIncompatibility(unittest.TestCase):\n             SCHEDULER_PRIORITY_QUEUE='scrapy.pqueues.DownloaderAwarePriorityQueue',\n             CONCURRENT_REQUESTS_PER_IP=1,\n         )\n-        crawler = Crawler(Spider, settings)\n+        crawler = get_crawler(Spider, settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         spider = Spider(name='spider')\n         scheduler.open(spider)\n\n@@ -7,10 +7,10 @@ from twisted.internet import defer\n from twisted.trial.unittest import TestCase as TwistedTestCase\n \n from scrapy.core.scheduler import BaseScheduler\n-from scrapy.crawler import CrawlerRunner\n from scrapy.http import Request\n from scrapy.spiders import Spider\n-from scrapy.utils.request import request_fingerprint\n+from scrapy.utils.request import fingerprint\n+from scrapy.utils.test import get_crawler\n \n from tests.mockserver import MockServer\n \n@@ -21,13 +21,13 @@ URLS = [urljoin(\"https://example.org\", p) for p in PATHS]\n \n class MinimalScheduler:\n     def __init__(self) -> None:\n-        self.requests: Dict[str, Request] = {}\n+        self.requests: Dict[bytes, Request] = {}\n \n     def has_pending_requests(self) -> bool:\n         return bool(self.requests)\n \n     def enqueue_request(self, request: Request) -> bool:\n-        fp = request_fingerprint(request)\n+        fp = fingerprint(request)\n         if fp not in self.requests:\n             self.requests[fp] = request\n             return True\n@@ -147,9 +147,12 @@ class MinimalSchedulerCrawlTest(TwistedTestCase):\n     @defer.inlineCallbacks\n     def test_crawl(self):\n         with MockServer() as mockserver:\n-            settings = {\"SCHEDULER\": self.scheduler_cls}\n+            settings = {\n+                \"SCHEDULER\": self.scheduler_cls,\n+            }\n             with LogCapture() as log:\n-                yield CrawlerRunner(settings).crawl(TestSpider, mockserver)\n+                crawler = get_crawler(TestSpider, settings)\n+                yield crawler.crawl(mockserver)\n             for path in PATHS:\n                 self.assertIn(f\"{{'path': '{path}'}}\", str(log))\n             self.assertIn(f\"'item_scraped_count': {len(PATHS)}\", str(log))\n\n@@ -96,7 +96,10 @@ class SpiderLoaderTest(unittest.TestCase):\n \n     def test_crawler_runner_loading(self):\n         module = 'tests.test_spiderloader.test_spiders.spider1'\n-        runner = CrawlerRunner({'SPIDER_MODULES': [module]})\n+        runner = CrawlerRunner({\n+            'SPIDER_MODULES': [module],\n+            'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION',\n+        })\n \n         self.assertRaisesRegex(KeyError, 'Spider not found',\n                                runner.create_crawler, 'spider2')\n\n@@ -3,6 +3,7 @@ import os\n import tempfile\n import shutil\n import contextlib\n+import warnings\n \n from pytest import warns\n \n@@ -68,20 +69,21 @@ class GetProjectSettingsTestCase(unittest.TestCase):\n         envvars = {\n             'SCRAPY_SETTINGS_MODULE': value,\n         }\n-        with set_env(**envvars), warns(None) as warnings:\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            with set_env(**envvars):\n                 settings = get_project_settings()\n-        assert not warnings\n+\n         assert settings.get('SETTINGS_MODULE') == value\n \n     def test_invalid_envvar(self):\n         envvars = {\n             'SCRAPY_FOO': 'bar',\n         }\n-        with set_env(**envvars), warns(None) as warnings:\n+        with warns(ScrapyDeprecationWarning, match=': FOO') as record:\n+            with set_env(**envvars):\n                 get_project_settings()\n-        assert len(warnings) == 1\n-        assert warnings[0].category == ScrapyDeprecationWarning\n-        assert str(warnings[0].message).endswith(': FOO')\n+        assert len(record) == 1\n \n     def test_valid_and_invalid_envvars(self):\n         value = 'tests.test_cmdline.settings'\n@@ -89,9 +91,8 @@ class GetProjectSettingsTestCase(unittest.TestCase):\n             'SCRAPY_FOO': 'bar',\n             'SCRAPY_SETTINGS_MODULE': value,\n         }\n-        with set_env(**envvars), warns(None) as warnings:\n+        with warns(ScrapyDeprecationWarning, match=': FOO') as record:\n+            with set_env(**envvars):\n                 settings = get_project_settings()\n-        assert len(warnings) == 1\n-        assert warnings[0].category == ScrapyDeprecationWarning\n-        assert str(warnings[0].message).endswith(': FOO')\n+        assert len(record) == 1\n         assert settings.get('SETTINGS_MODULE') == value\n\n@@ -308,13 +308,22 @@ class RequestFingerprintTest(FingerprintTest):\n         ),\n     )\n \n+    def setUp(self) -> None:\n+        warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+\n+    def tearDown(self) -> None:\n+        warnings.simplefilter(\"default\", ScrapyDeprecationWarning)\n+\n     @pytest.mark.xfail(reason='known bug kept for backward compatibility', strict=True)\n     def test_part_separation(self):\n         super().test_part_separation()\n \n+\n+class RequestFingerprintDeprecationTest(unittest.TestCase):\n+\n     def test_deprecation_default_parameters(self):\n         with pytest.warns(ScrapyDeprecationWarning) as warnings:\n-            self.function(Request(\"http://www.example.com\"))\n+            request_fingerprint(Request(\"http://www.example.com\"))\n         messages = [str(warning.message) for warning in warnings]\n         self.assertTrue(\n             any(\n@@ -326,7 +335,7 @@ class RequestFingerprintTest(FingerprintTest):\n \n     def test_deprecation_non_default_parameters(self):\n         with pytest.warns(ScrapyDeprecationWarning) as warnings:\n-            self.function(Request(\"http://www.example.com\"), keep_fragments=True)\n+            request_fingerprint(Request(\"http://www.example.com\"), keep_fragments=True)\n         messages = [str(warning.message) for warning in warnings]\n         self.assertTrue(\n             any(\n\n@@ -1,7 +1,9 @@\n import os\n import unittest\n+import warnings\n from urllib.parse import urlparse\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response, TextResponse, HtmlResponse\n from scrapy.utils.python import to_bytes\n from scrapy.utils.response import (response_httprepr, open_in_browser,\n@@ -15,14 +17,21 @@ class ResponseUtilsTest(unittest.TestCase):\n     dummy_response = TextResponse(url='http://example.org/', body=b'dummy_response')\n \n     def test_response_httprepr(self):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+\n             r1 = Response(\"http://www.example.com\")\n             self.assertEqual(response_httprepr(r1), b'HTTP/1.1 200 OK\\r\\n\\r\\n')\n \n-        r1 = Response(\"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=b\"Some body\")\n-        self.assertEqual(response_httprepr(r1), b'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+            r1 = Response(\"http://www.example.com\", status=404,\n+                          headers={\"Content-type\": \"text/html\"}, body=b\"Some body\")\n+            self.assertEqual(response_httprepr(r1),\n+                             b'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n \n-        r1 = Response(\"http://www.example.com\", status=6666, headers={\"Content-type\": \"text/html\"}, body=b\"Some body\")\n-        self.assertEqual(response_httprepr(r1), b'HTTP/1.1 6666 \\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+            r1 = Response(\"http://www.example.com\", status=6666,\n+                          headers={\"Content-type\": \"text/html\"}, body=b\"Some body\")\n+            self.assertEqual(response_httprepr(r1),\n+                             b'HTTP/1.1 6666 \\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n \n     def test_open_in_browser(self):\n         url = \"http:///www.example.com/some/page.html\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
