{"custom_id": "keras#f45512db3f22a9b0811d97b9d97fb8c095c43470", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 3917 | Contributors (this commit): 18 | Commits (past 90d): 2 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,7 +34,6 @@ from keras.utils import tf_utils\n from keras.utils import traceback_utils\n \n # isort: off\n-from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n SINGLE_LAYER_OUTPUT_ERROR_MSG = (\n@@ -394,13 +393,6 @@ class Sequential(functional.Functional):\n                 self._build_input_shape = tf.nest.map_structure(\n                     _get_shape_tuple, inputs\n                 )\n-                if tf.__internal__.tf2.enabled():\n-                    logging.warning(\n-                        \"Layers in a Sequential model should only have a \"\n-                        f\"single input tensor. Received: inputs={inputs}. \"\n-                        \"Consider rewriting this model with the Functional \"\n-                        \"API.\"\n-                    )\n             else:\n                 self._build_graph_network_for_inferred_shape(\n                     inputs.shape, inputs.dtype\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f57b45aa5a635a3d75ec957e9ca3f7ce5984421c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 231 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,6 @@ class SpectralNormalization(Wrapper):\n \n     This wrapper controls the Lipschitz constant of the layer by\n     constraining its spectral norm, which can stabilize the training of GANs.\n-    See [Spectral Normalization for GAN](https://arxiv.org/abs/1802.05957).\n \n     Args:\n       layer: a `tf.keras.layers.Layer` instance that\n@@ -37,6 +36,7 @@ class SpectralNormalization(Wrapper):\n       power_iterations: `int`, the number of iterations during normalization.\n \n     Examples:\n+\n     Wrap `tf.keras.layers.Conv2D`:\n     >>> x = np.random.rand(1, 10, 10, 1)\n     >>> conv2d = SpectralNormalization(tf.keras.layers.Conv2D(2, 2))\n@@ -50,6 +50,9 @@ class SpectralNormalization(Wrapper):\n     >>> y = dense(x)\n     >>> y.shape\n     TensorShape([1, 10, 10, 10])\n+\n+    Reference:\n+      - [Spectral Normalization for GAN](https://arxiv.org/abs/1802.05957).\n     \"\"\"\n \n     def __init__(self, layer, power_iterations=1, **kwargs):\n@@ -80,11 +83,11 @@ class SpectralNormalization(Wrapper):\n \n         self.kernel_shape = self.kernel.shape.as_list()\n \n-        self.sn_u = self.add_weight(\n+        self.vector_u = self.add_weight(\n             shape=(1, self.self.kernel_shape[-1]),\n             initializer=TruncatedNormal(stddev=0.02),\n             trainable=False,\n-            name=\"sn_u\",\n+            name=\"vector_u\",\n             dtype=self.kernel.dtype,\n         )\n \n@@ -107,18 +110,22 @@ class SpectralNormalization(Wrapper):\n         spectral normalized value, so that the layer is ready for `call()`.\n         \"\"\"\n \n-        w = tf.reshape(self.kernel, [-1, self.self.kernel_shape[-1]])\n-        u = self.sn_u\n+        weights = tf.reshape(self.kernel, [-1, self.self.kernel_shape[-1]])\n+        vector_u = self.vector_u\n \n         # check for zeroes weights\n-        if not tf.reduce_all(tf.equal(w, 0.0)):\n+        if not tf.reduce_all(tf.equal(weights, 0.0)):\n             for _ in range(self.power_iterations):\n-                v = tf.math.l2_normalize(tf.matmul(u, w, transpose_b=True))\n-                u = tf.math.l2_normalize(tf.matmul(v, w))\n-            u = tf.stop_gradient(u)\n-            v = tf.stop_gradient(v)\n-            sigma = tf.matmul(tf.matmul(v, w), u, transpose_b=True)\n-            self.sn_u.assign(tf.cast(u, self.sn_u.dtype))\n+                vector_v = tf.math.l2_normalize(\n+                    tf.matmul(vector_u, weights, transpose_b=True)\n+                )\n+                vector_u = tf.math.l2_normalize(tf.matmul(vector_v, weights))\n+            vector_u = tf.stop_gradient(vector_u)\n+            vector_v = tf.stop_gradient(vector_v)\n+            sigma = tf.matmul(\n+                tf.matmul(vector_v, weights), vector_u, transpose_b=True\n+            )\n+            self.vector_u.assign(tf.cast(vector_u, self.vector_u.dtype))\n             self.kernel.assign(\n                 tf.cast(\n                     tf.reshape(self.kernel / sigma, self.self.kernel_shape),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b4fa6b0dbba7351aa98fa2b39b74e82b85f5b54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 17422 | Contributors (this commit): 46 | Commits (past 90d): 21 | Contributors (cumulative): 58 | DMM Complexity: None\n\nDIFF:\n@@ -5585,8 +5585,8 @@ def categorical_focal_crossentropy(\n     from_logits=False,\n     axis=-1,\n ):\n-    \"\"\"Computes the alpha balanced focal crossentropy loss between\n-    the labels and predictions.\n+    \"\"\"Computes the alpha balanced focal crossentropy loss.\n+\n     According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n     helps to apply a focal factor to down-weight easy examples and focus more on\n     hard examples. By default, the focal tensor is computed as follows:\n\n@@ -924,8 +924,8 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.CategoricalFocalCrossentropy\")\n class CategoricalFocalCrossentropy(LossFunctionWrapper):\n-    \"\"\"Computes the alpha balanced focal crossentropy loss between\n-    the labels and predictions.\n+    \"\"\"Computes the alpha balanced focal crossentropy loss.\n+\n     According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n     helps to apply a focal factor to down-weight easy examples and focus more on\n     hard examples. By default, the focal tensor is computed as follows:\n@@ -2246,6 +2246,7 @@ def _ragged_tensor_categorical_focal_crossentropy(\n     axis=-1,\n ):\n     \"\"\"Implements support for handling RaggedTensors.\n+\n     Expected shape: (batch, sequence_len, n_classes) with sequence_len\n     being variable per batch.\n     Return shape: (batch, sequence_len).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#49c03a2e1214cd7057dd5381c9602eebeaa4ff18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 6476 | Contributors (this commit): 33 | Commits (past 90d): 13 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -993,7 +993,8 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n             reference. It helps to gradually reduce the importance given to\n             simple (easy) examples in a smooth manner.\n         from_logits: Whether `output` is expected to be a logits tensor. By\n-            default, we consider that `output` encodes a probability distribution.\n+            default, we consider that `output` encodes a probability\n+            distribution.\n         label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n             meaning the confidence on label values are relaxed. For example, if\n             `0.1`, use `0.1 / num_classes` for non-target labels and\n@@ -2179,14 +2180,15 @@ def categorical_focal_crossentropy(\n         y_pred: Tensor of predicted targets.\n         alpha: A weight balancing factor for all classes, default is `0.25` as\n             mentioned in the reference. It can be a list of floats or a scalar.\n-            In the multi-class case, alpha may be set by inverse class frequency by\n-            using `compute_class_weight` from `sklearn.utils`.\n+            In the multi-class case, alpha may be set by inverse class\n+            frequency by using `compute_class_weight` from `sklearn.utils`.\n         gamma: A focusing parameter, default is `2.0` as mentioned in the\n             reference. It helps to gradually reduce the importance given to\n-            simple examples in a smooth manner. When `gamma` = 0, there is no focal\n-            effect on the categorical crossentropy.\n+            simple examples in a smooth manner. When `gamma` = 0, there is\n+            no focal effect on the categorical crossentropy.\n         from_logits: Whether `y_pred` is expected to be a logits tensor. By\n-            default, we assume that `y_pred` encodes a probability distribution.\n+            default, we assume that `y_pred` encodes a probability\n+            distribution.\n         label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n             example, if `0.1`, use `0.1 / num_classes` for non-target labels\n             and `0.9 + 0.1 / num_classes` for target labels.\n@@ -2259,12 +2261,12 @@ def _ragged_tensor_categorical_focal_crossentropy(\n     Args:\n         alpha: A weight balancing factor for all classes, default is `0.25` as\n             mentioned in the reference. It can be a list of floats or a scalar.\n-            In the multi-class case, alpha may be set by inverse class frequency by\n-            using `compute_class_weight` from `sklearn.utils`.\n+            In the multi-class case, alpha may be set by inverse class\n+            frequency by using `compute_class_weight` from `sklearn.utils`.\n         gamma: A focusing parameter, default is `2.0` as mentioned in the\n             reference. It helps to gradually reduce the importance given to\n-            simple examples in a smooth manner. When `gamma` = 0, there is no focal\n-            effect on the categorical crossentropy.\n+            simple examples in a smooth manner. When `gamma` = 0, there is\n+            no focal effect on the categorical crossentropy.\n         from_logits: Whether `y_pred` is expected to be a logits tensor. By\n             default, we assume that `y_pred` encodes a probability distribution.\n         label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c3a1425bd4a7978151359f7fd61799271703d4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 173 | Lines Deleted: 328 | Files Changed: 9 | Hunks: 36 | Methods Changed: 29 | Complexity Δ (Sum/Max): -12/12 | Churn Δ: 501 | Churn Cumulative: 1486 | Contributors (this commit): 9 | Commits (past 90d): 16 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,314 +14,16 @@\n # ==============================================================================\n \"\"\"DTensor specific Keras optimizers.\"\"\"\n \n-\n-import tensorflow.compat.v2 as tf\n-\n-from keras.dtensor import dtensor_api as dtensor\n from keras.optimizers import adadelta\n from keras.optimizers import adagrad\n from keras.optimizers import adam\n from keras.optimizers import adamw\n-from keras.optimizers import optimizer as optimizer_lib\n from keras.optimizers import rmsprop\n from keras.optimizers import sgd\n-from keras.optimizers.schedules import learning_rate_schedule\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n-from tensorflow.tools.docs import doc_controls\n-\n-\n-class Optimizer(optimizer_lib._BaseOptimizer):\n-    \"\"\"DTensor specific optimizers.\n-\n-    The major changes for this class is that all the variable init logic will be\n-    mesh/layout aware.\n-    \"\"\"\n-\n-    # Note that we didn't subclass optimizer_lib.Optimizer since it contains the\n-    # extra logic of handling distribution strategy, which we don't need for\n-    # DTensor\n-\n-    def __init__(self, name, mesh=None):\n-        \"\"\"Create a new Optimizer.\n-\n-        Args:\n-          name: String. The name of the optimizer, which will appear in all the\n-            state variables created by this optimizer.\n-          mesh: dtensor.Mesh. The optional Mesh which will be used to create the\n-            states. Note that usually the state variable will use the layout\n-            from the corresponding model variables. This mesh only used for\n-            global variables like globle steps, learning rate, etc.\n-        \"\"\"\n-        # TODO(scottzhu): Skip the gradients_clip_option and ema_option for now,\n-        # and will cover them in future if really needed.\n-        # TODO(scottzhu): We might want to make mesh to be required in future.\n-        self._mesh = mesh\n-        super().__init__(name=name)\n-\n-    def _create_iteration_variable(self):\n-        init_val = tf.constant(0, dtype=tf.int64)\n-        if self._mesh:\n-            init_val = dtensor.copy_to_mesh(\n-                init_val, dtensor.Layout.replicated(self._mesh, rank=0)\n-            )\n-        with tf.init_scope():\n-            # Lift the variable creation to init scope to avoid environment\n-            # issue.\n-            self._iterations = dtensor.DVariable(init_val, name=\"iteration\")\n-\n-    ################## Override methods from keras.Optimizer ################\n-    def add_variable_from_reference(\n-        self, model_variable, variable_name, initial_value=None\n-    ):\n-        \"\"\"Create an optimizer variable from model variable.\n-\n-        Create an optimizer variable based on the information of model variable.\n-        For example, in SGD optimizer momemtum, for each model variable, a\n-        corresponding momemtum variable is created of the same shape and dtype.\n-\n-        Args:\n-          model_variable: The corresponding model variable to the optimizer\n-            variable to be created.\n-          variable_name: The name prefix of the optimizer variable to be\n-            created.  The create variables name will follow the pattern\n-            `{variable_name}/{model_variable.name}`, e.g., `momemtum/dense_1`.\n-          initial_value: The initial value of the optimizer variable, if None,\n-            the value will be default to 0.\n-\n-        Returns:\n-          An optimizer variable.\n-        \"\"\"\n-        if initial_value is None:\n-            # Use tf.zeros_like which will propagate the layout information from\n-            # the model weights if any.\n-            initial_value = tf.zeros_like(model_variable)\n-        elif isinstance(initial_value, tf.Tensor):\n-            initial_value = dtensor.copy_to_mesh(\n-                initial_value,\n-                dtensor.Layout.replicated(\n-                    self._mesh, rank=initial_value.shape.rank\n-                ),\n-            )\n-        variable = dtensor.DVariable(\n-            initial_value=initial_value,\n-            name=f\"{variable_name}/{model_variable._shared_name}\",\n-            dtype=model_variable.dtype,\n-            trainable=False,\n-        )\n-        self._variables.append(variable)\n-        return variable\n-\n-    @doc_controls.do_not_generate_docs\n-    def aggregate_gradients(self, grads_and_vars):\n-        # Hide the aggregate_gradients from Optimizer.aggregate_gradients\n-        raise NotImplementedError(\n-            \"Dtensor doesn't need to manually aggregate gradients\"\n-        )\n-\n-    def _var_key(self, variable):\n-        \"\"\"Get a unique identifier of the given variable.\"\"\"\n-        return optimizer_lib._BaseOptimizer._var_key(self, variable)\n-\n-    def apply_gradients(self, grads_and_vars):\n-        \"\"\"Apply gradients to variables.\n-\n-        Args:\n-          grads_and_vars: List of (gradient, variable) pairs.\n-\n-        Returns:\n-          None\n-\n-        Raises:\n-          TypeError: If `grads_and_vars` is malformed.\n-        \"\"\"\n-        # Explicitly call the _BaseOptimizer to avoid any chance of using\n-        # Optimizers.apply_gradients which contains distribution strategy logic.\n-        optimizer_lib._BaseOptimizer.apply_gradients(self, grads_and_vars)\n-\n-    def _internal_apply_gradients(self, grads_and_vars):\n-        \"\"\"Helper function of apply gradients.\n-\n-        This is required for separating out distributed training logic.\n-\n-        Args:\n-          grads_and_vars: List of (gradient, variable) pairs.\n-        \"\"\"\n-        # Explicitly call the _BaseOptimizer to avoid any chance of using\n-        # Optimizers.apply_gradients which contains distribution strategy logic.\n-        optimizer_lib._BaseOptimizer._internal_apply_gradients(\n-            self, grads_and_vars\n-        )\n-\n-    def _overwrite_model_variables_with_average_value_helper(self, var_list):\n-        \"\"\"Helper function to _overwrite_model_variables_with_average_value.\"\"\"\n-        (\n-            optimizer_lib._BaseOptimizer._overwrite_model_variables_with_average_value_helper(  # noqa: E501\n-                self, var_list\n-            )\n-        )\n-\n-    def _build_learning_rate(self, learning_rate):\n-        if isinstance(\n-            learning_rate, learning_rate_schedule.LearningRateSchedule\n-        ):\n-            # Create a variable to hold the current learning rate.\n-            # Note that the init value `learning_rate(self.iterations)` should\n-            # have the correct layout information from self.iterations.\n-            self._current_learning_rate = dtensor.DVariable(\n-                learning_rate(self.iterations),\n-                name=\"learning_rate\",\n-                dtype=tf.float32,\n-            )\n-            return learning_rate\n-        init_val = tf.constant(learning_rate, dtype=tf.float32)\n-        if self._mesh:\n-            init_val = dtensor.copy_to_mesh(\n-                init_val, dtensor.Layout.replicated(self._mesh, rank=0)\n-            )\n-        return dtensor.DVariable(init_val, name=\"learning_rate\")\n-\n-\n-@keras_export(\"keras.dtensor.experimental.optimizers.Adadelta\", v1=[])\n-class Adadelta(Optimizer, adadelta.Adadelta):\n-    def __init__(\n-        self,\n-        learning_rate=0.001,\n-        rho=0.95,\n-        epsilon=1e-7,\n-        gradients_clip_option=None,\n-        ema_option=None,\n-        name=\"Adadelta\",\n-        mesh=None,\n-    ):\n-        # Skip the adam.Adadelta.__init__ and only call the Optimizer.__init__\n-        # this is to skip the keras.Optimizer.__init__, which contains the logic\n-        # of distribution strategy. Same for all the optimizers subclasses.\n-        Optimizer.__init__(self, name=name, mesh=mesh)\n-        self._learning_rate = self._build_learning_rate(learning_rate)\n-        self.rho = rho\n-        self.epsilon = epsilon\n-\n-\n-@keras_export(\"keras.dtensor.experimental.optimizers.Adagrad\", v1=[])\n-class Adagrad(Optimizer, adagrad.Adagrad):\n-    def __init__(\n-        self,\n-        learning_rate=0.001,\n-        initial_accumulator_value=0.1,\n-        epsilon=1e-7,\n-        gradients_clip_option=None,\n-        ema_option=None,\n-        name=\"Adagrad\",\n-        mesh=None,\n-    ):\n-        Optimizer.__init__(self, name=name, mesh=mesh)\n-        self._learning_rate = self._build_learning_rate(learning_rate)\n-        self.initial_accumulator_value = initial_accumulator_value\n-        self.epsilon = epsilon\n-\n-\n-@keras_export(\"keras.dtensor.experimental.optimizers.Adam\", v1=[])\n-class Adam(Optimizer, adam.Adam):\n-    def __init__(\n-        self,\n-        learning_rate=0.001,\n-        beta_1=0.9,\n-        beta_2=0.999,\n-        epsilon=1e-7,\n-        amsgrad=False,\n-        gradients_clip_option=None,\n-        ema_option=None,\n-        name=\"Adam\",\n-        mesh=None,\n-    ):\n-        Optimizer.__init__(self, name=name, mesh=mesh)\n-        self._learning_rate = self._build_learning_rate(learning_rate)\n-        self.beta_1 = beta_1\n-        self.beta_2 = beta_2\n-        self.epsilon = epsilon\n-        self.amsgrad = amsgrad\n-\n-\n-@keras_export(\"keras.dtensor.experimental.optimizers.AdamW\", v1=[])\n-class AdamW(Optimizer, adamw.AdamW):\n-    def __init__(\n-        self,\n-        learning_rate=0.001,\n-        weight_decay=0.004,\n-        beta_1=0.9,\n-        beta_2=0.999,\n-        epsilon=1e-7,\n-        amsgrad=False,\n-        name=\"AdamW\",\n-        mesh=None,\n-    ):\n-        Optimizer.__init__(self, name=name, mesh=mesh)\n-        self._learning_rate = self._build_learning_rate(learning_rate)\n-        self.weight_decay = weight_decay\n-        self.beta_1 = beta_1\n-        self.beta_2 = beta_2\n-        self.epsilon = epsilon\n-        self.amsgrad = amsgrad\n-\n-        if self.weight_decay is None:\n-            raise ValueError(\n-                \"Missing value of `weight_decay` which is required and\"\n-                \" must be a float value.\"\n-            )\n-\n-\n-@keras_export(\"keras.dtensor.experimental.optimizers.RMSprop\", v1=[])\n-class RMSprop(Optimizer, rmsprop.RMSprop):\n-    def __init__(\n-        self,\n-        learning_rate=0.001,\n-        rho=0.9,\n-        momentum=0.0,\n-        epsilon=1e-7,\n-        centered=False,\n-        gradients_clip_option=None,\n-        ema_option=None,\n-        jit_compile=False,\n-        name=\"RMSprop\",\n-        mesh=None,\n-    ):\n-        Optimizer.__init__(self, name=name, mesh=mesh)\n-        self._learning_rate = self._build_learning_rate(learning_rate)\n-        self.rho = rho\n-        self.momentum = momentum\n-        self.epsilon = epsilon\n-        self.centered = centered\n-\n-\n-@keras_export(\"keras.dtensor.experimental.optimizers.SGD\", v1=[])\n-class SGD(Optimizer, sgd.SGD):\n-    def __init__(\n-        self,\n-        learning_rate=0.01,\n-        momentum=0.0,\n-        nesterov=False,\n-        amsgrad=False,\n-        gradients_clip_option=None,\n-        ema_option=None,\n-        jit_compile=False,\n-        name=\"SGD\",\n-        mesh=None,\n-    ):\n-        Optimizer.__init__(self, name=name, mesh=mesh)\n-        self._learning_rate = self._build_learning_rate(learning_rate)\n-        self.momentum = momentum\n-        self.nesterov = nesterov\n-        if isinstance(momentum, (int, float)) and (\n-            momentum < 0 or momentum > 1\n-        ):\n-            raise ValueError(\"`momentum` must be between [0, 1].\")\n-\n-\n-Adadelta.__doc__ = Optimizer.__doc__ + adadelta.Adadelta.__doc__\n-Adagrad.__doc__ = Optimizer.__doc__ + adagrad.Adagrad.__doc__\n-Adam.__doc__ = Optimizer.__doc__ + adam.Adam.__doc__\n-AdamW.__doc__ = Optimizer.__doc__ + adamw.AdamW.__doc__\n-RMSprop.__doc__ = Optimizer.__doc__ + rmsprop.RMSprop.__doc__\n-SGD.__doc__ = Optimizer.__doc__ + sgd.SGD.__doc__\n+Adadelta = adadelta.Adadelta\n+Adagrad = adagrad.Adagrad\n+Adam = adam.Adam\n+AdamW = adamw.AdamW\n+RMSprop = rmsprop.RMSprop\n+SGD = sgd.SGD\n\n@@ -26,8 +26,9 @@ from keras import losses\n from keras import models\n from keras.dtensor import dtensor_api as dtensor\n from keras.dtensor import layout_map\n-from keras.dtensor import optimizers\n+from keras.dtensor import optimizers as dtensor_optimizers\n from keras.dtensor import test_util\n+from keras.optimizers import adam\n \n \n class OptimizersTest(test_util.DTensorBaseTest):\n@@ -46,8 +47,9 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         }\n         self.mesh = self.configTestMesh(mesh_dict)\n \n-    def test_add_variable_from_reference(self):\n-        optimizer = optimizers.Adam(mesh=self.mesh)\n+    @parameterized.parameters([adam.Adam, dtensor_optimizers.Adam])\n+    def test_add_variable_from_reference(self, optimizer_cls):\n+        optimizer = optimizer_cls(mesh=self.mesh)\n         variable_init_value = tf.ones([4, 4], dtype=tf.float32)\n         variable_init_value = dtensor.copy_to_mesh(\n             variable_init_value,\n@@ -64,8 +66,9 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         # Make sure the variable contains the correct layout info\n         self.assertEqual(state_variable.layout, model_variable.layout)\n \n-    def test_build_index_dict(self):\n-        optimizer = optimizers.Adam(mesh=self.mesh)\n+    @parameterized.parameters([adam.Adam, dtensor_optimizers.Adam])\n+    def test_build_index_dict(self, optimizer_cls):\n+        optimizer = optimizer_cls(mesh=self.mesh)\n         variable_init_value = tf.ones(shape=(), dtype=tf.float32)\n         variable_init_value = dtensor.copy_to_mesh(\n             variable_init_value,\n@@ -83,37 +86,59 @@ class OptimizersTest(test_util.DTensorBaseTest):\n     @parameterized.named_parameters(\n         (\n             \"Adadelta\",\n-            optimizers.Adadelta,\n+            dtensor_optimizers.Adadelta,\n             {},\n             [\n                 \"Adadelta/accumulated_grad/Variable\",\n                 \"Adadelta/accumulated_delta_var/Variable\",\n+                \"iteration\",\n             ],\n         ),\n         (\n             \"Adam\",\n-            optimizers.Adam,\n+            dtensor_optimizers.Adam,\n             {\"amsgrad\": True},\n-            [\"Adam/m/Variable\", \"Adam/v/Variable\", \"Adam/vhat/Variable\"],\n+            [\n+                \"Adam/m/Variable\",\n+                \"Adam/v/Variable\",\n+                \"Adam/vhat/Variable\",\n+                \"iteration\",\n+            ],\n         ),\n         (\n             \"AdamW\",\n-            optimizers.AdamW,\n+            dtensor_optimizers.AdamW,\n             {\"amsgrad\": True},\n-            [\"AdamW/m/Variable\", \"AdamW/v/Variable\", \"AdamW/vhat/Variable\"],\n+            [\n+                \"AdamW/m/Variable\",\n+                \"AdamW/v/Variable\",\n+                \"AdamW/vhat/Variable\",\n+                \"iteration\",\n+            ],\n+        ),\n+        (\n+            \"Adagrad\",\n+            dtensor_optimizers.Adagrad,\n+            {},\n+            [\"Adagrad/accumulator/Variable\", \"iteration\"],\n         ),\n-        (\"Adagrad\", optimizers.Adagrad, {}, [\"Adagrad/accumulator/Variable\"]),\n         (\n             \"RMSprop\",\n-            optimizers.RMSprop,\n+            dtensor_optimizers.RMSprop,\n             {\"momentum\": 0.1, \"centered\": True},\n             [\n                 \"RMSprop/velocity/Variable\",\n                 \"RMSprop/momentum/Variable\",\n                 \"RMSprop/average_gradient/Variable\",\n+                \"iteration\",\n             ],\n         ),\n-        (\"SGD\", optimizers.SGD, {\"momentum\": 0.1}, [\"SGD/m/Variable\"]),\n+        (\n+            \"SGD\",\n+            dtensor_optimizers.SGD,\n+            {\"momentum\": 0.1},\n+            [\"SGD/m/Variable\", \"iteration\"],\n+        ),\n     )\n     def test_apply_gradients(\n         self, optimizer_cls, init_args, expect_variable_names\n@@ -142,7 +167,8 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         all_names = [var._shared_name for var in optimizer_variables]\n         self.assertCountEqual(all_names, expect_variable_names)\n \n-    def test_embedding_lookup_backward_path(self):\n+    @parameterized.parameters([adam.Adam, dtensor_optimizers.Adam])\n+    def test_embedding_lookup_backward_path(self, optimizer_cls):\n         # See b/265441685 for more context.\n         backend.enable_tf_random_generator()\n         os.environ[\n@@ -186,7 +212,7 @@ class OptimizersTest(test_util.DTensorBaseTest):\n             preds = layers.Dense(output_size, activation=\"softmax\")(x)\n             model = models.Model(inputs, preds)\n \n-        optimizer = optimizers.AdamW(mesh=self.mesh)\n+        optimizer = optimizer_cls(mesh=self.mesh)\n \n         @tf.function\n         def train_func(model, inputs, label, optimizer):\n\n@@ -25,7 +25,10 @@ from tensorflow.python.util.tf_export import keras_export\n \n @register_keras_serializable()\n @keras_export(\n-    \"keras.optimizers.experimental.Adadelta\", \"keras.optimizers.Adadelta\", v1=[]\n+    \"keras.optimizers.experimental.Adadelta\",\n+    \"keras.optimizers.Adadelta\",\n+    \"keras.dtensor.experimental.optimizers.Adadelta\",\n+    v1=[],\n )\n class Adadelta(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Adadelta algorithm.\n\n@@ -26,7 +26,10 @@ from tensorflow.python.util.tf_export import keras_export\n \n @register_keras_serializable()\n @keras_export(\n-    \"keras.optimizers.experimental.Adagrad\", \"keras.optimizers.Adagrad\", v1=[]\n+    \"keras.optimizers.experimental.Adagrad\",\n+    \"keras.optimizers.Adagrad\",\n+    \"keras.dtensor.experimental.optimizers.Adagrad\",\n+    v1=[],\n )\n class Adagrad(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Adagrad algorithm.\n\n@@ -25,7 +25,10 @@ from tensorflow.python.util.tf_export import keras_export\n \n @register_keras_serializable()\n @keras_export(\n-    \"keras.optimizers.Adam\", \"keras.optimizers.experimental.Adam\", v1=[]\n+    \"keras.optimizers.Adam\",\n+    \"keras.optimizers.experimental.Adam\",\n+    \"keras.dtensor.experimental.optimizers.Adam\",\n+    v1=[],\n )\n class Adam(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Adam algorithm.\n\n@@ -26,7 +26,10 @@ from tensorflow.python.util.tf_export import keras_export\n \n @register_keras_serializable()\n @keras_export(\n-    \"keras.optimizers.AdamW\", \"keras.optimizers.experimental.AdamW\", v1=[]\n+    \"keras.optimizers.AdamW\",\n+    \"keras.optimizers.experimental.AdamW\",\n+    \"keras.dtensor.experimental.optimizers.AdamW\",\n+    v1=[],\n )\n class AdamW(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the AdamW algorithm.\n\n@@ -876,6 +876,10 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n       jit_compile: Boolean, defaults to True.\n           If True, the optimizer will use XLA\n           compilation. If no GPU device is found, this flag will be ignored.\n+      mesh: optional `tf.experimental.dtensor.Mesh` instance. When provided,\n+          the optimizer will be run in DTensor mode, e.g. state\n+          tracking variable will be a DVariable, and aggregation/reduction will\n+          happen in the global DTensor context.\n       **kwargs: keyword arguments only used for backward compatibility.\"\"\"\n \n \n@@ -1085,7 +1089,8 @@ class Optimizer(_BaseOptimizer):\n         **kwargs,\n     ):\n         \"\"\"Create a new Optimizer.\"\"\"\n-\n+        mesh = kwargs.pop(\"mesh\", None)\n+        self._mesh = mesh\n         super().__init__(\n             name,\n             weight_decay,\n@@ -1103,12 +1108,50 @@ class Optimizer(_BaseOptimizer):\n     def add_variable_from_reference(\n         self, model_variable, variable_name, shape=None, initial_value=None\n     ):\n+        if self._mesh:\n+            if initial_value is None:\n+                # Use tf.zeros_like which will propagate the layout information\n+                # from the model weights if any.\n+                initial_value = tf.zeros_like(model_variable)\n+            elif isinstance(initial_value, tf.Tensor):\n+                initial_value = tf.experimental.dtensor.copy_to_mesh(\n+                    initial_value,\n+                    tf.experimental.dtensor.Layout.replicated(\n+                        self._mesh, rank=initial_value.shape.rank\n+                    ),\n+                )\n+            variable = tf.experimental.dtensor.DVariable(\n+                initial_value=initial_value,\n+                name=f\"{variable_name}/{model_variable._shared_name}\",\n+                dtype=model_variable.dtype,\n+                trainable=False,\n+            )\n+            self._variables.append(variable)\n+            return variable\n+        else:\n             strategy = tf.distribute.get_strategy()\n             with strategy.extended.colocate_vars_with(model_variable):\n                 return super().add_variable_from_reference(\n                     model_variable, variable_name, shape, initial_value\n                 )\n \n+    def _create_iteration_variable(self):\n+        if self._mesh:\n+            init_val = tf.constant(0, dtype=tf.int64)\n+            init_val = tf.experimental.dtensor.copy_to_mesh(\n+                init_val,\n+                tf.experimental.dtensor.Layout.replicated(self._mesh, rank=0),\n+            )\n+            with tf.init_scope():\n+                # Lift the variable creation to init scope to avoid environment\n+                # issue.\n+                self._iterations = tf.experimental.dtensor.DVariable(\n+                    init_val, name=\"iteration\"\n+                )\n+            self._variables.append(self._iterations)\n+        else:\n+            super()._create_iteration_variable()\n+\n     def _var_key(self, variable):\n         \"\"\"Get a unique identifier of the given variable.\"\"\"\n \n@@ -1129,8 +1172,9 @@ class Optimizer(_BaseOptimizer):\n     def aggregate_gradients(self, grads_and_vars):\n         \"\"\"Aggregate gradients on all devices.\n \n-        By default we will perform reduce_sum of gradients across devices. Users\n-        can implement their own aggregation logic by overriding this method.\n+        By default, we will perform reduce_sum of gradients across devices.\n+        Users can implement their own aggregation logic by overriding this\n+        method.\n \n         Args:\n           grads_and_vars: List of (gradient, variable) pairs.\n@@ -1138,6 +1182,11 @@ class Optimizer(_BaseOptimizer):\n         Returns:\n           List of (gradient, variable) pairs.\n         \"\"\"\n+        if self._mesh:\n+            raise NotImplementedError(\n+                \"Dtensor doesn't need to manually aggregate gradients\"\n+            )\n+        else:\n             return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n \n     def apply_gradients(\n@@ -1165,6 +1214,10 @@ class Optimizer(_BaseOptimizer):\n           TypeError: If `grads_and_vars` is malformed.\n           RuntimeError: If called in a cross-replica context.\n         \"\"\"\n+        if self._mesh:\n+            # Skip any usage of strategy logic for DTensor\n+            return super().apply_gradients(grads_and_vars, name=name)\n+\n         # `experimental_aggregate_gradients` is an arg in `apply_gradients` of\n         # v2 optimizer -- the reverse of `skip_gradients_aggregation`.\n         # We read it from kwargs for backward compatibility.\n@@ -1199,6 +1252,10 @@ class Optimizer(_BaseOptimizer):\n         )\n \n     def _internal_apply_gradients(self, grads_and_vars):\n+        if self._mesh:\n+            # Skip any usage of strategy logic for DTensor\n+            return super()._internal_apply_gradients(grads_and_vars)\n+\n         return tf.__internal__.distribute.interim.maybe_merge_call(\n             self._distributed_apply_gradients_fn,\n             self._distribution_strategy,\n@@ -1212,6 +1269,12 @@ class Optimizer(_BaseOptimizer):\n         Args:\n           var_list: list of model variables.\n         \"\"\"\n+        if self._mesh:\n+            # Skip any usage of strategy logic for DTensor\n+            super()._overwrite_model_variables_with_average_value_helper(\n+                var_list\n+            )\n+\n         strategy = self._distribution_strategy\n         # Override model variable by the stored average value on all devices.\n         for var, average_var in zip(\n@@ -1221,6 +1284,42 @@ class Optimizer(_BaseOptimizer):\n                 var, lambda a, b: a.assign(b), args=(average_var,)\n             )\n \n+    def _build_learning_rate(self, learning_rate):\n+        if not self._mesh:\n+            return super()._build_learning_rate(learning_rate)\n+\n+        # For DTensor\n+        variable_creation = tf.experimental.dtensor.DVariable\n+        init_value_convert_fn = lambda x: tf.experimental.dtensor.copy_to_mesh(\n+            x, tf.experimental.dtensor.Layout.replicated(self._mesh, rank=0)\n+        )\n+        if isinstance(\n+            learning_rate, learning_rate_schedule.LearningRateSchedule\n+        ):\n+            current_learning_rate = tf.convert_to_tensor(\n+                learning_rate(self.iterations)\n+            )\n+            current_learning_rate = init_value_convert_fn(current_learning_rate)\n+            # Create a variable to hold the current learning rate.\n+            # Note that the init value `learning_rate(self.iterations)` should\n+            # have the correct layout information from self.iterations.\n+            self._current_learning_rate = variable_creation(\n+                current_learning_rate,\n+                name=\"learning_rate\",\n+                dtype=tf.float32,\n+            )\n+            return learning_rate\n+\n+        init_val = init_value_convert_fn(\n+            tf.constant(learning_rate, dtype=tf.float32)\n+        )\n+        return variable_creation(\n+            init_val,\n+            name=\"learning_rate\",\n+            dtype=backend.floatx(),\n+            trainable=False,\n+        )\n+\n     def _update_model_variables_moving_average(self, var_list):\n         \"\"\"Update the stored moving average using the latest value.\"\"\"\n         if self.use_ema:\n\n@@ -25,7 +25,10 @@ from tensorflow.python.util.tf_export import keras_export\n \n @register_keras_serializable()\n @keras_export(\n-    \"keras.optimizers.experimental.RMSprop\", \"keras.optimizers.RMSprop\", v1=[]\n+    \"keras.optimizers.experimental.RMSprop\",\n+    \"keras.optimizers.RMSprop\",\n+    \"keras.dtensor.experimental.optimizers.RMSprop\",\n+    v1=[],\n )\n class RMSprop(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the RMSprop algorithm.\n\n@@ -25,7 +25,10 @@ from tensorflow.python.util.tf_export import keras_export\n \n @register_keras_serializable()\n @keras_export(\n-    \"keras.optimizers.experimental.SGD\", \"keras.optimizers.SGD\", v1=[]\n+    \"keras.optimizers.experimental.SGD\",\n+    \"keras.optimizers.SGD\",\n+    \"keras.dtensor.experimental.optimizers.SGD\",\n+    v1=[],\n )\n class SGD(optimizer.Optimizer):\n     r\"\"\"Gradient descent (with momentum) optimizer.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d174fecc0b8c1d013e6f7fd90bcf568afa7b3847", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 3063 | Contributors (this commit): 22 | Commits (past 90d): 5 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -367,7 +367,7 @@ class TensorLikeDataAdapter(DataAdapter):\n         )\n         dataset = dataset.with_options(options)\n \n-        self._dataset = dataset\n+        self._dataset = dataset.prefetch(tf.data.AUTOTUNE)\n \n     def slice_inputs(self, indices_dataset, inputs):\n         \"\"\"Slice inputs into a Dataset of batches.\n@@ -660,7 +660,7 @@ class CompositeTensorDataAdapter(DataAdapter):\n                 num_samples - (self._size - 1) * self._batch_size\n             )\n \n-        self._dataset = dataset\n+        self._dataset = dataset.prefetch(tf.data.AUTOTUNE)\n \n     def get_dataset(self):\n         return self._dataset\n@@ -924,7 +924,7 @@ class GeneratorDataAdapter(DataAdapter):\n         if workers == 1 and not use_multiprocessing:\n             dataset = dataset.prefetch(1)\n \n-        self._dataset = dataset\n+        self._dataset = dataset.prefetch(tf.data.AUTOTUNE)\n \n     def _standardize_batch(self, data):\n         \"\"\"Standardizes a batch output by a generator.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e4204c5b99a6a27f526563f1f600df1fb0f27ad7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 29 | Files Changed: 3 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 59 | Churn Cumulative: 4144 | Contributors (this commit): 13 | Commits (past 90d): 14 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -165,3 +165,22 @@ def call_with_layout(fn, layout, *args, **kwargs):\n             result = fn(*args, **kwargs)\n             return dtensor.relayout(result, layout)\n     return fn(*args, **kwargs)\n+\n+\n+def running_with_dtensor_strategy():\n+    \"\"\"Check whether running with a `Strategy` that is backed by DTensor.\n+\n+    In the DTensor based training, all the tensors are in global context, which\n+    is different from the local context. Some keras components need to\n+    behave differently, e.g. BatchNormalization and SyncBatchNormalization, as\n+    well as optimizers.\n+\n+    This check will help those layer to branch the logic and keep the correct\n+    behavior between different context.\n+    \"\"\"\n+    if not tf.distribute.has_strategy():\n+        return False\n+    strategy = tf.distribute.get_strategy()\n+    # TODO(scottzhu): Finalize the strategy API to check if a strategy is backed\n+    # by DTensor.\n+    return getattr(strategy, \"_mesh\", None) is not None\n\n@@ -697,7 +697,10 @@ class BatchNormalizationBase(Layer):\n                 new_mean = tf.reduce_mean(mean, axis=1, keepdims=True)\n                 new_variance = tf.reduce_mean(variance, axis=1, keepdims=True)\n             else:\n-                if _running_with_dtensor_strategy() and not self.synchronized:\n+                if (\n+                    utils.running_with_dtensor_strategy()\n+                    and not self.synchronized\n+                ):\n                     new_mean = tf.math.reduce_mean(mean, axis=reduction_axes)\n                     new_variance = tf.math.reduce_mean(\n                         variance, axis=reduction_axes\n@@ -1276,7 +1279,7 @@ class BatchNormalizationBase(Layer):\n         )\n \n     def _moments(self, inputs, reduction_axes, keep_dims, mask=None):\n-        if _running_with_dtensor_strategy():\n+        if utils.running_with_dtensor_strategy():\n             mean, variance = self._dtensor_calculate_mean_and_var(\n                 inputs, reduction_axes, keep_dims, mask=mask\n             )\n@@ -1539,24 +1542,6 @@ class SyncBatchNormalization(BatchNormalizationBase):\n         )\n \n \n-def _running_with_dtensor_strategy():\n-    \"\"\"Check whether running with a `Strategy` that is backed by DTensor.\n-\n-    In the DTensor based training, all the tensors are in global context, which\n-    means the existing way of calculating the mean/var will switch from local\n-    context to global context, effectively changing from BN to sync BN.\n-\n-    To keep the status quo, a check of the DTensor context is needed, and\n-    ops behavior need to be switched back.\n-    \"\"\"\n-    if not tf.distribute.has_strategy():\n-        return False\n-    strategy = tf.distribute.get_strategy()\n-    # TODO(scottzhu): Finalize the strategy API to check if a strategy is backed\n-    # by DTensor.\n-    return getattr(strategy, \"_mesh\", None) is not None\n-\n-\n def _expand_tensor_with_local_replica_group(inputs):\n     \"\"\"Reshape the input tensor to have an extra dimension of replica group.\n \n@@ -1593,7 +1578,7 @@ def _raise_for_non_sync_bn_with_renorm_and_dtensor_strategy(\n     synchronized, training, renorm\n ):\n     if (\n-        _running_with_dtensor_strategy()\n+        utils.running_with_dtensor_strategy()\n         and not synchronized\n         and training == True\n         and renorm\n\n@@ -19,6 +19,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n from keras.dtensor import test_util\n+from keras.dtensor import utils\n from keras.layers.normalization import batch_normalization\n from keras.testing_infra import test_utils\n \n@@ -51,20 +52,16 @@ class BatchNormalizationDTensorTest(test_util.DTensorBaseTest):\n         strategy = dtensor_mirrored_strategy.MirroredStrategy(self.mesh)\n \n         with strategy.scope():\n-            self.assertTrue(\n-                batch_normalization._running_with_dtensor_strategy()\n-            )\n+            self.assertTrue(utils.running_with_dtensor_strategy())\n \n-        self.assertFalse(batch_normalization._running_with_dtensor_strategy())\n+        self.assertFalse(utils.running_with_dtensor_strategy())\n \n         normal_mirrored_strategy = tf.distribute.MirroredStrategy(\n             [\"CPU:0\", \"CPU:1\"]\n         )\n-        self.assertFalse(batch_normalization._running_with_dtensor_strategy())\n+        self.assertFalse(utils.running_with_dtensor_strategy())\n         with normal_mirrored_strategy.scope():\n-            self.assertFalse(\n-                batch_normalization._running_with_dtensor_strategy()\n-            )\n+            self.assertFalse(utils.running_with_dtensor_strategy())\n \n     @parameterized.product(\n         training=[True, False],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e29b6292fbaa7cf7f1aa816051bf449c903157ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 15 | Churn Cumulative: 360 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,8 +52,9 @@ class TrainingTest(test_util.DTensorBaseTest):\n     @parameterized.product(\n         run_eagerly=[True, False],\n         jit_compile=[True, False],\n+        optimizer_creator=[lambda: optimizers.Adam(), lambda: \"adam\"],\n     )\n-    def test_model_fit(self, run_eagerly, jit_compile):\n+    def test_model_fit(self, run_eagerly, jit_compile, optimizer_creator):\n         if run_eagerly and jit_compile:\n             self.skipTest(\"run_eagerly can't run with jit_compile\")\n         dtensor_strategy = dtensor_mirrored_strategy.MirroredStrategy(\n@@ -73,7 +74,7 @@ class TrainingTest(test_util.DTensorBaseTest):\n \n         with dtensor_strategy.scope():\n             model = integration_test_utils.get_model()\n-            optimizer = optimizers.Adam(mesh=self.mesh)\n+            optimizer = optimizer_creator()\n \n         model.compile(\n             loss=\"SparseCategoricalCrossentropy\",\n\n@@ -23,6 +23,7 @@ from absl import logging\n \n from keras import backend\n from keras import initializers\n+from keras.dtensor import utils as dtensor_utils\n from keras.optimizers import utils as optimizer_utils\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.utils import tf_utils\n@@ -1104,6 +1105,7 @@ class Optimizer(_BaseOptimizer):\n             **kwargs,\n         )\n         self._distribution_strategy = tf.distribute.get_strategy()\n+        self._run_with_dtensor = dtensor_utils.running_with_dtensor_strategy()\n \n     def add_variable_from_reference(\n         self, model_variable, variable_name, shape=None, initial_value=None\n@@ -1182,7 +1184,7 @@ class Optimizer(_BaseOptimizer):\n         Returns:\n           List of (gradient, variable) pairs.\n         \"\"\"\n-        if self._mesh:\n+        if self._mesh or self._run_with_dtensor:\n             raise NotImplementedError(\n                 \"Dtensor doesn't need to manually aggregate gradients\"\n             )\n@@ -1214,7 +1216,7 @@ class Optimizer(_BaseOptimizer):\n           TypeError: If `grads_and_vars` is malformed.\n           RuntimeError: If called in a cross-replica context.\n         \"\"\"\n-        if self._mesh:\n+        if self._mesh or self._run_with_dtensor:\n             # Skip any usage of strategy logic for DTensor\n             return super().apply_gradients(grads_and_vars, name=name)\n \n@@ -1252,7 +1254,7 @@ class Optimizer(_BaseOptimizer):\n         )\n \n     def _internal_apply_gradients(self, grads_and_vars):\n-        if self._mesh:\n+        if self._mesh or self._run_with_dtensor:\n             # Skip any usage of strategy logic for DTensor\n             return super()._internal_apply_gradients(grads_and_vars)\n \n@@ -1269,7 +1271,7 @@ class Optimizer(_BaseOptimizer):\n         Args:\n           var_list: list of model variables.\n         \"\"\"\n-        if self._mesh:\n+        if self._mesh or self._run_with_dtensor:\n             # Skip any usage of strategy logic for DTensor\n             super()._overwrite_model_variables_with_average_value_helper(\n                 var_list\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
