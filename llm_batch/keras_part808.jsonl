{"custom_id": "keras#7cd5c60f8143c664f1eaa06716d7d087571a8152", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 77 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,6 +39,15 @@ class Resizing(TFDataLayer):\n             largest possible window in the image (of size `(height, width)`)\n             that matches the target aspect ratio. By default\n             (`crop_to_aspect_ratio=False`), aspect ratio may not be preserved.\n+        pad_to_aspect_ratio: If `True`, pad the images without aspect\n+            ratio distortion. When the original aspect ratio differs\n+            from the target aspect ratio, the output image will be\n+            evenly padded on the short side.\n+        fill_mode: When using `pad_to_aspect_ratio=True`, padded areas\n+            are filled according to the given mode. Only `\"constant\"` is\n+            supported at this time\n+            (fill with constant value, equal to `fill_value`).\n+        fill_value: Float. Padding value to use when `pad_to_aspect_ratio=True`.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape `(batch, height, width, channels)`\n@@ -56,6 +65,9 @@ class Resizing(TFDataLayer):\n         width,\n         interpolation=\"bilinear\",\n         crop_to_aspect_ratio=False,\n+        pad_to_aspect_ratio=False,\n+        fill_mode=\"constant\",\n+        fill_value=0.0,\n         data_format=None,\n         **kwargs,\n     ):\n@@ -65,6 +77,9 @@ class Resizing(TFDataLayer):\n         self.interpolation = interpolation\n         self.data_format = backend.standardize_data_format(data_format)\n         self.crop_to_aspect_ratio = crop_to_aspect_ratio\n+        self.pad_to_aspect_ratio = pad_to_aspect_ratio\n+        self.fill_mode = fill_mode\n+        self.fill_value = fill_value\n \n     def call(self, inputs):\n         size = (self.height, self.width)\n@@ -74,6 +89,9 @@ class Resizing(TFDataLayer):\n             interpolation=self.interpolation,\n             data_format=self.data_format,\n             crop_to_aspect_ratio=self.crop_to_aspect_ratio,\n+            pad_to_aspect_ratio=self.pad_to_aspect_ratio,\n+            fill_mode=self.fill_mode,\n+            fill_value=self.fill_value,\n         )\n \n     def compute_output_shape(self, input_shape):\n@@ -101,6 +119,9 @@ class Resizing(TFDataLayer):\n             \"width\": self.width,\n             \"interpolation\": self.interpolation,\n             \"crop_to_aspect_ratio\": self.crop_to_aspect_ratio,\n+            \"pad_to_aspect_ratio\": self.pad_to_aspect_ratio,\n+            \"fill_mode\": self.fill_mode,\n+            \"fill_value\": self.fill_value,\n             \"data_format\": self.data_format,\n         }\n         return {**base_config, **config}\n\n@@ -190,7 +190,7 @@ def resize(\n             largest possible window in the image (of size `(height, width)`)\n             that matches the target aspect ratio. By default\n             (`crop_to_aspect_ratio=False`), aspect ratio may not be preserved.\n-        pad_to_aspect_ratio: If `True`, pad the image without aspect\n+        pad_to_aspect_ratio: If `True`, pad the images without aspect\n             ratio distortion. When the original aspect ratio differs\n             from the target aspect ratio, the output image will be\n             evenly padded on the short side.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b955665444622b88871c50d33a8dc19fc3954f2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 18 | Files Changed: 7 | Hunks: 21 | Methods Changed: 15 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 76 | Churn Cumulative: 246 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n import ml_dtypes\n \n from keras.src import activations\n-from keras.src import backend\n from keras.src import constraints\n from keras.src import dtype_policies\n from keras.src import initializers\n@@ -347,6 +346,7 @@ class Dense(Layer):\n             initializer=kernel_scale_initializer,\n             trainable=False,\n         )\n+        self._is_quantized = True\n \n     def _float8_build(self):\n         if not isinstance(\n@@ -396,6 +396,7 @@ class Dense(Layer):\n         self.kernel_amax_history.overwrite_with_gradient = True\n         self.outputs_grad_scale.overwrite_with_gradient = True\n         self.outputs_grad_amax_history.overwrite_with_gradient = True\n+        self._is_quantized = True\n \n     def quantized_call(self, inputs):\n         if self.dtype_policy.quantization_mode == \"int8\":\n@@ -552,8 +553,6 @@ class Dense(Layer):\n \n         self._tracker.unlock()\n         if mode == \"int8\":\n-            if backend.standardize_dtype(self._kernel.dtype) == \"int8\":\n-                raise ValueError(\"`quantize` can only be done once per layer.\")\n             # Configure `self.inputs_quantizer`\n             self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n@@ -572,8 +571,6 @@ class Dense(Layer):\n                 lambda shape, dtype: kernel_scale,\n             )\n         elif mode == \"float8\":\n-            if hasattr(self, \"inputs_amax_history\"):\n-                raise ValueError(\"`quantize` can only be done once per layer.\")\n             self._float8_build()\n         else:\n             raise NotImplementedError(\n\n@@ -420,10 +420,24 @@ class DenseTest(testing.TestCase, parameterized.TestCase):\n         layer = layers.Dense(units=2)\n         layer.build((None, 2))\n         layer.quantize(mode)\n+        for m in [\"int8\", \"float8\"]:\n             with self.assertRaisesRegex(\n-            ValueError, \"`quantize` can only be done once per layer.\"\n+                ValueError, \"is already quantized with dtype_policy=\"\n             ):\n-            layer.quantize(mode)\n+                layer.quantize(m)\n+\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8_from_float32\"),\n+        (\"float8\", \"float8_from_float32\"),\n+    )\n+    def test_quantize_when_already_quantized_using_dtype_argument(self, mode):\n+        layer = layers.Dense(units=2, dtype=mode)\n+        layer.build((None, 2))\n+        for m in [\"int8\", \"float8\"]:\n+            with self.assertRaisesRegex(\n+                ValueError, \"is already quantized with dtype_policy=\"\n+            ):\n+                layer.quantize(m)\n \n     @parameterized.named_parameters(\n         (\"int8\", \"int8_from_float32\", 3),\n\n@@ -5,7 +5,6 @@ import ml_dtypes\n import numpy as np\n \n from keras.src import activations\n-from keras.src import backend\n from keras.src import constraints\n from keras.src import dtype_policies\n from keras.src import initializers\n@@ -431,6 +430,7 @@ class EinsumDense(Layer):\n             initializer=kernel_scale_initializer,\n             trainable=False,\n         )\n+        self._is_quantized = True\n \n     def _float8_build(self):\n         if not isinstance(\n@@ -480,6 +480,7 @@ class EinsumDense(Layer):\n         self.kernel_amax_history.overwrite_with_gradient = True\n         self.outputs_grad_scale.overwrite_with_gradient = True\n         self.outputs_grad_amax_history.overwrite_with_gradient = True\n+        self._is_quantized = True\n \n     def quantized_call(self, inputs):\n         if self.dtype_policy.quantization_mode == \"int8\":\n@@ -665,8 +666,6 @@ class EinsumDense(Layer):\n \n         self._tracker.unlock()\n         if mode == \"int8\":\n-            if backend.standardize_dtype(self._kernel.dtype) == \"int8\":\n-                raise ValueError(\"`quantize` can only be done once per layer.\")\n             (\n                 self._input_reduced_axes,\n                 self._kernel_reduced_axes,\n@@ -709,8 +708,6 @@ class EinsumDense(Layer):\n                 lambda shape, dtype: kernel_scale,\n             )\n         elif mode == \"float8\":\n-            if hasattr(self, \"inputs_amax_history\"):\n-                raise ValueError(\"`quantize` can only be done once per layer.\")\n             self._float8_build()\n         else:\n             raise NotImplementedError(\n\n@@ -508,15 +508,34 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n     def test_quantize_when_already_quantized(self, mode):\n         layer = layers.EinsumDense(\n             equation=\"ab,bcd->acd\",\n-            output_shape=(8, 32),\n+            output_shape=(8, 16),\n             bias_axes=\"d\",\n         )\n         layer.build((None, 3))\n         layer.quantize(mode)\n+        for m in [\"int8\", \"float8\"]:\n             with self.assertRaisesRegex(\n-            ValueError, \"`quantize` can only be done once per layer.\"\n+                ValueError, \"is already quantized with dtype_policy=\"\n             ):\n-            layer.quantize(mode)\n+                layer.quantize(m)\n+\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8_from_float32\"),\n+        (\"float8\", \"float8_from_float32\"),\n+    )\n+    def test_quantize_when_already_quantized_using_dtype_argument(self, mode):\n+        layer = layers.EinsumDense(\n+            equation=\"ab,bcd->acd\",\n+            output_shape=(8, 16),\n+            bias_axes=\"d\",\n+            dtype=mode,\n+        )\n+        layer.build((None, 3))\n+        for m in [\"int8\", \"float8\"]:\n+            with self.assertRaisesRegex(\n+                ValueError, \"is already quantized with dtype_policy=\"\n+            ):\n+                layer.quantize(m)\n \n     @parameterized.named_parameters(\n         (\"int8\", \"int8_from_float32\", 3),\n\n@@ -325,6 +325,7 @@ class Embedding(Layer):\n             initializer=embeddings_scale_initializer,\n             trainable=False,\n         )\n+        self._is_quantized = True\n \n     def quantized_call(self, inputs):\n         if self.dtype_policy.quantization_mode == \"int8\":\n@@ -374,8 +375,6 @@ class Embedding(Layer):\n \n         self._tracker.unlock()\n         if mode == \"int8\":\n-            if backend.standardize_dtype(self._embeddings.dtype) == \"int8\":\n-                raise ValueError(\"`quantize` can only be done once per layer.\")\n             # Configure `self.inputs_quantizer`\n             self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n             # Quantize `self._embeddings` to int8 and compute corresponding\n\n@@ -314,7 +314,15 @@ class EmbeddingTest(test_case.TestCase, parameterized.TestCase):\n         layer.build()\n         layer.quantize(\"int8\")\n         with self.assertRaisesRegex(\n-            ValueError, \"`quantize` can only be done once per layer.\"\n+            ValueError, \"is already quantized with dtype_policy=\"\n+        ):\n+            layer.quantize(\"int8\")\n+\n+    def test_quantize_when_already_quantized_using_dtype_argument(self):\n+        layer = layers.Embedding(10, 16, dtype=\"int8_from_float32\")\n+        layer.build()\n+        with self.assertRaisesRegex(\n+            ValueError, \"is already quantized with dtype_policy=\"\n         ):\n             layer.quantize(\"int8\")\n \n\n@@ -1143,6 +1143,12 @@ class Layer(BackendLayer, Operation):\n                 f\"Layer '{self.name}' (of type '{self.__class__.__name__}') \"\n                 \"is not built yet.\"\n             )\n+        if getattr(self, \"_is_quantized\", False):\n+            raise ValueError(\n+                f\"Layer '{self.name}' is already quantized with \"\n+                f\"dtype_policy='{self.dtype_policy.name}'. \"\n+                f\"Received: mode={mode}\"\n+            )\n         if mode not in dtype_policies.QUANTIZATION_MODES:\n             raise ValueError(\n                 \"Invalid quantization mode. \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab017ea74896a3069f3386bf9ef5d7eee0f3d3a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 10 | Files Changed: 6 | Hunks: 13 | Methods Changed: 13 | Complexity Δ (Sum/Max): 15/6 | Churn Δ: 99 | Churn Cumulative: 159 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,6 +27,10 @@ def eig(x):\n     return jnp.linalg.eig(x)\n \n \n+def eigh(x):\n+    return jnp.linalg.eigh(x)\n+\n+\n def inv(a):\n     return jnp.linalg.inv(a)\n \n\n@@ -18,6 +18,10 @@ def eig(a):\n     return np.linalg.eig(a)\n \n \n+def eigh(a):\n+    return np.linalg.eigh(a)\n+\n+\n def inv(a):\n     return np.linalg.inv(a)\n \n\n@@ -22,6 +22,10 @@ def eig(a):\n     return tf.linalg.eig(a)\n \n \n+def eigh(a):\n+    return tf.linalg.eigh(a)\n+\n+\n def inv(a):\n     return tf.linalg.inv(a)\n \n\n@@ -19,6 +19,10 @@ def eig(x):\n     return torch.linalg.eig(x)\n \n \n+def eigh(x):\n+    return torch.linalg.eigh(x)\n+\n+\n def inv(x):\n     return torch.linalg.inv(x)\n \n\n@@ -92,8 +92,8 @@ class Eig(Operation):\n         return _eig(x)\n \n     def compute_output_spec(self, x):\n-        _assert_2d(x)\n         _assert_square(x)\n+        _assert_2d(x)\n         return (\n             KerasTensor(x.shape[:-1], x.dtype),\n             KerasTensor(x.shape, x.dtype),\n@@ -110,7 +110,6 @@ def eig(x):\n     Returns:\n         A tuple of two tensors: a tensor of shape `(..., M)` containing\n         eigenvalues and a tensor of shape `(..., M, M)` containing eigenvectors.\n-\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Eig().symbolic_call(x)\n@@ -119,11 +118,52 @@ def eig(x):\n \n def _eig(x):\n     x = backend.convert_to_tensor(x)\n-    _assert_2d(x)\n     _assert_square(x)\n+    _assert_2d(x)\n     return backend.linalg.eig(x)\n \n \n+class Eigh(Operation):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, x):\n+        return _eigh(x)\n+\n+    def compute_output_spec(self, x):\n+        _assert_square(x)\n+        _assert_2d(x)\n+        return (\n+            KerasTensor(x.shape[:-1], x.dtype),\n+            KerasTensor(x.shape, x.dtype),\n+        )\n+\n+\n+@keras_export([\"keras.ops.eigh\", \"keras.ops.linalg.eigh\"])\n+def eigh(x):\n+    \"\"\"Computes the eigenvalues and eigenvectors of a complex Hermitian.\n+\n+    Args:\n+        x: Input tensor of shape `(..., M, M)`.\n+\n+    Returns:\n+        A tuple of two tensors: a tensor of shape `(..., M)` containing\n+        eigenvalues and a tensor of shape `(..., M, M)` containing eigenvectors.\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Eigh().symbolic_call(x)\n+    return _eigh(x)\n+\n+\n+def _eigh(x):\n+    x = backend.convert_to_tensor(x)\n+    _assert_square(x)\n+    _assert_2d(x)\n+    return backend.linalg.eigh(x)\n+\n+\n class Inv(Operation):\n \n     def __init__(self):\n\n@@ -42,13 +42,11 @@ class LinalgOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(w.shape, (None, 20))\n         self.assertEqual(v.shape, (None, 20, 20))\n \n-        x = KerasTensor([None, None, 20])\n-        with self.assertRaises(ValueError):\n-            linalg.eig(x)\n-\n-        x = KerasTensor([None, 20, 15])\n-        with self.assertRaises(ValueError):\n-            linalg.eig(x)\n+    def test_eigh(self):\n+        x = KerasTensor([None, 20, 20])\n+        w, v = linalg.eigh(x)\n+        self.assertEqual(w.shape, (None, 20))\n+        self.assertEqual(v.shape, (None, 20, 20))\n \n     def test_inv(self):\n         x = KerasTensor([None, 20, 20])\n@@ -208,6 +206,16 @@ class LinalgOpsStaticShapeTest(testing.TestCase):\n         with self.assertRaises(ValueError):\n             linalg.eig(x)\n \n+    def test_eigh(self):\n+        x = KerasTensor([4, 3, 3])\n+        w, v = linalg.eigh(x)\n+        self.assertEqual(w.shape, (4, 3))\n+        self.assertEqual(v.shape, (4, 3, 3))\n+\n+        x = KerasTensor([10, 20, 15])\n+        with self.assertRaises(ValueError):\n+            linalg.eigh(x)\n+\n     def test_inv(self):\n         x = KerasTensor([4, 3, 3])\n         out = linalg.inv(x)\n@@ -346,6 +354,21 @@ class LinalgOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         x_reconstructed = (v * w[..., None, :]) @ v.transpose((0, 2, 1))\n         self.assertAllClose(x_reconstructed, x, atol=1e-4)\n \n+    def test_eigh(self):\n+        x = np.random.rand(2, 3, 3)\n+        x = x @ x.transpose((0, 2, 1))\n+        if backend.backend() == \"jax\":\n+            import jax\n+\n+            if jax.default_backend() == \"gpu\":\n+                # eigh not implemented for jax on gpu backend\n+                with self.assertRaises(NotImplementedError):\n+                    linalg.eigh(x)\n+                return\n+        w, v = map(ops.convert_to_numpy, linalg.eigh(x))\n+        x_reconstructed = (v * w[..., None, :]) @ v.transpose((0, 2, 1))\n+        self.assertAllClose(x_reconstructed, x, atol=1e-4)\n+\n     def test_inv(self):\n         x = np.random.rand(4, 3, 3)\n         x_inv = ops.convert_to_numpy(linalg.inv(x))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#65d0e092236b60a6e9bf998cd9f844ffdee8a2d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 27 | Files Changed: 6 | Hunks: 38 | Methods Changed: 27 | Complexity Δ (Sum/Max): 9/7 | Churn Δ: 103 | Churn Cumulative: 231 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -339,12 +339,12 @@ def arctanh(x):\n     return jnp.arctanh(x)\n \n \n-def argmax(x, axis=None):\n-    return jnp.argmax(x, axis=axis)\n+def argmax(x, axis=None, keepdims=False):\n+    return jnp.argmax(x, axis=axis, keepdims=keepdims)\n \n \n-def argmin(x, axis=None):\n-    return jnp.argmin(x, axis=axis)\n+def argmin(x, axis=None, keepdims=False):\n+    return jnp.argmin(x, axis=axis, keepdims=keepdims)\n \n \n def argsort(x, axis=-1):\n\n@@ -227,14 +227,14 @@ def arctanh(x):\n     return np.arctanh(x)\n \n \n-def argmax(x, axis=None):\n+def argmax(x, axis=None, keepdims=False):\n     axis = standardize_axis_for_numpy(axis)\n-    return np.argmax(x, axis=axis).astype(\"int32\")\n+    return np.argmax(x, axis=axis, keepdims=keepdims).astype(\"int32\")\n \n \n-def argmin(x, axis=None):\n+def argmin(x, axis=None, keepdims=False):\n     axis = standardize_axis_for_numpy(axis)\n-    return np.argmin(x, axis=axis).astype(\"int32\")\n+    return np.argmin(x, axis=axis, keepdims=keepdims).astype(\"int32\")\n \n \n def argsort(x, axis=-1):\n\n@@ -745,16 +745,35 @@ def arctanh(x):\n     return tf.math.atanh(x)\n \n \n-def argmax(x, axis=None):\n+def _keepdims(x, y, axis):\n     if axis is None:\n-        x = tf.reshape(x, [-1])\n-    return tf.cast(tf.argmax(x, axis=axis), dtype=\"int32\")\n+        shape = [1 for _ in range(len(x.shape))]\n+    else:\n+        shape = [tf.shape[i] for i in range(len(x.shape))]\n+        for axis in tree.flatten(axis):\n+            shape[axis] = 1\n+    y = tf.reshape(y, shape)\n+    return y\n \n \n-def argmin(x, axis=None):\n+def argmax(x, axis=None, keepdims=False):\n+    _x = x\n     if axis is None:\n         x = tf.reshape(x, [-1])\n-    return tf.cast(tf.argmin(x, axis=axis), dtype=\"int32\")\n+    y = tf.cast(tf.argmax(x, axis=axis), dtype=\"int32\")\n+    if keepdims:\n+        y = _keepdims(_x, y, axis)\n+    return y\n+\n+\n+def argmin(x, axis=None, keepdims=False):\n+    _x = x\n+    if axis is None:\n+        x = tf.reshape(x, [-1])\n+    y = tf.cast(tf.argmin(x, axis=axis), dtype=\"int32\")\n+    if keepdims:\n+        y = _keepdims(_x, y, axis)\n+    return y\n \n \n def argsort(x, axis=-1):\n\n@@ -318,24 +318,24 @@ def arctanh(x):\n     return torch.arctanh(x)\n \n \n-def argmax(x, axis=None):\n+def argmax(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n \n     # TODO: torch.argmax doesn't support bool\n     if standardize_dtype(x.dtype) == \"bool\":\n         x = cast(x, \"uint8\")\n \n-    return cast(torch.argmax(x, dim=axis), dtype=\"int32\")\n+    return cast(torch.argmax(x, dim=axis, keepdim=keepdims), dtype=\"int32\")\n \n \n-def argmin(x, axis=None):\n+def argmin(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n \n     # TODO: torch.argmin doesn't support bool\n     if standardize_dtype(x.dtype) == \"bool\":\n         x = cast(x, \"uint8\")\n \n-    return cast(torch.argmin(x, dim=axis), dtype=\"int32\")\n+    return cast(torch.argmin(x, dim=axis, keepdim=keepdims), dtype=\"int32\")\n \n \n def argsort(x, axis=-1):\n\n@@ -963,14 +963,17 @@ def arctanh(x):\n \n \n class Argmax(Operation):\n-    def __init__(self, axis=None):\n+    def __init__(self, axis=None, keepdims=False):\n         super().__init__()\n         self.axis = axis\n+        self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.numpy.argmax(x, axis=self.axis)\n+        return backend.numpy.argmax(x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n+        if self.keepdims:\n+            return KerasTensor(x.shape, dtype=\"int32\")\n         if self.axis is None:\n             return KerasTensor([], dtype=\"int32\")\n         return KerasTensor(\n@@ -979,13 +982,15 @@ class Argmax(Operation):\n \n \n @keras_export([\"keras.ops.argmax\", \"keras.ops.numpy.argmax\"])\n-def argmax(x, axis=None):\n+def argmax(x, axis=None, keepdims=False):\n     \"\"\"Returns the indices of the maximum values along an axis.\n \n     Args:\n         x: Input tensor.\n         axis: By default, the index is into the flattened tensor, otherwise\n             along the specified axis.\n+        keepdims: If this is set to `True`, the axes which are reduced are left\n+            in the result as dimensions with size one. Defaults to `False`.\n \n     Returns:\n         Tensor of indices. It has the same shape as `x`, with the dimension\n@@ -1004,19 +1009,22 @@ def argmax(x, axis=None):\n     array([2, 2], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return Argmax(axis=axis).symbolic_call(x)\n-    return backend.numpy.argmax(x, axis=axis)\n+        return Argmax(axis=axis, keepdims=keepdims).symbolic_call(x)\n+    return backend.numpy.argmax(x, axis=axis, keepdims=keepdims)\n \n \n class Argmin(Operation):\n-    def __init__(self, axis=None):\n+    def __init__(self, axis=None, keepdims=False):\n         super().__init__()\n         self.axis = axis\n+        self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.numpy.argmin(x, axis=self.axis)\n+        return backend.numpy.argmin(x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n+        if self.keepdims:\n+            return KerasTensor(x.shape, dtype=\"int32\")\n         if self.axis is None:\n             return KerasTensor([], dtype=\"int32\")\n         return KerasTensor(\n@@ -1025,13 +1033,15 @@ class Argmin(Operation):\n \n \n @keras_export([\"keras.ops.argmin\", \"keras.ops.numpy.argmin\"])\n-def argmin(x, axis=None):\n+def argmin(x, axis=None, keepdims=False):\n     \"\"\"Returns the indices of the minium values along an axis.\n \n     Args:\n         x: Input tensor.\n         axis: By default, the index is into the flattened tensor, otherwise\n             along the specified axis.\n+        keepdims: If this is set to `True`, the axes which are reduced are left\n+            in the result as dimensions with size one. Defaults to `False`.\n \n     Returns:\n         Tensor of indices. It has the same shape as `x`, with the dimension\n@@ -1050,8 +1060,8 @@ def argmin(x, axis=None):\n     array([0, 0], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return Argmin(axis=axis).symbolic_call(x)\n-    return backend.numpy.argmin(x, axis=axis)\n+        return Argmin(axis=axis, keepdims=keepdims).symbolic_call(x)\n+    return backend.numpy.argmin(x, axis=axis, keepdims=keepdims)\n \n \n class Argsort(Operation):\n\n@@ -1004,16 +1004,20 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n     def test_argmax(self):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.argmax(x).shape, ())\n+        self.assertEqual(knp.argmax(x, keepdims=True).shape, (None, 3))\n \n         x = KerasTensor((None, 3, 3))\n         self.assertEqual(knp.argmax(x, axis=1).shape, (None, 3))\n+        self.assertEqual(knp.argmax(x, keepdims=True).shape, (None, 3, 3))\n \n     def test_argmin(self):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.argmin(x).shape, ())\n+        self.assertEqual(knp.argmin(x, keepdims=True).shape, (None, 3))\n \n         x = KerasTensor((None, 3, 3))\n         self.assertEqual(knp.argmin(x, axis=1).shape, (None, 3))\n+        self.assertEqual(knp.argmin(x, keepdims=True).shape, (None, 3, 3))\n \n     def test_argsort(self):\n         x = KerasTensor((None, 3))\n@@ -1567,10 +1571,12 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n     def test_argmax(self):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.argmax(x).shape, ())\n+        self.assertEqual(knp.argmax(x, keepdims=True).shape, (2, 3))\n \n     def test_argmin(self):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.argmin(x).shape, ())\n+        self.assertEqual(knp.argmin(x, keepdims=True).shape, (2, 3))\n \n     def test_argsort(self):\n         x = KerasTensor((2, 3))\n@@ -3044,17 +3050,31 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.argmax(x), np.argmax(x))\n         self.assertAllClose(knp.argmax(x, axis=1), np.argmax(x, axis=1))\n+        self.assertAllClose(\n+            knp.argmax(x, keepdims=True), np.argmax(x, keepdims=True)\n+        )\n \n         self.assertAllClose(knp.Argmax()(x), np.argmax(x))\n         self.assertAllClose(knp.Argmax(axis=1)(x), np.argmax(x, axis=1))\n \n+        self.assertAllClose(knp.Argmax()(x), np.argmax(x))\n+        self.assertAllClose(\n+            knp.Argmax(keepdims=True)(x), np.argmax(x, keepdims=True)\n+        )\n+\n     def test_argmin(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.argmin(x), np.argmin(x))\n         self.assertAllClose(knp.argmin(x, axis=1), np.argmin(x, axis=1))\n+        self.assertAllClose(\n+            knp.argmin(x, keepdims=True), np.argmin(x, keepdims=True)\n+        )\n \n         self.assertAllClose(knp.Argmin()(x), np.argmin(x))\n         self.assertAllClose(knp.Argmin(axis=1)(x), np.argmin(x, axis=1))\n+        self.assertAllClose(\n+            knp.Argmin(keepdims=True)(x), np.argmin(x, keepdims=True)\n+        )\n \n     def test_argsort(self):\n         x = np.array([[1, 2, 3], [4, 5, 6]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1135431db4513465f4bf6e65bd5111f68cd6b718", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 48 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -315,7 +315,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n                 at the target location, or instead ask the user\n                 via an interactive prompt.\n         \"\"\"\n-        return saving_api.save_weights(self, filepath, overwrite=True)\n+        return saving_api.save_weights(self, filepath, overwrite=overwrite)\n \n     @traceback_utils.filter_traceback\n     def load_weights(self, filepath, skip_mismatch=False, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c256741e5760bc61367515d743c14a8733b6f15b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 482 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -27,6 +27,7 @@ from keras.src.ops.core import while_loop\n from keras.src.ops.linalg import cholesky\n from keras.src.ops.linalg import det\n from keras.src.ops.linalg import eig\n+from keras.src.ops.linalg import eigh\n from keras.src.ops.linalg import inv\n from keras.src.ops.linalg import lu_factor\n from keras.src.ops.linalg import norm\n\n@@ -7,6 +7,7 @@ since your modifications would be overwritten.\n from keras.src.ops.linalg import cholesky\n from keras.src.ops.linalg import det\n from keras.src.ops.linalg import eig\n+from keras.src.ops.linalg import eigh\n from keras.src.ops.linalg import inv\n from keras.src.ops.linalg import lu_factor\n from keras.src.ops.linalg import norm\n\n@@ -27,6 +27,7 @@ from keras.src.ops.core import while_loop\n from keras.src.ops.linalg import cholesky\n from keras.src.ops.linalg import det\n from keras.src.ops.linalg import eig\n+from keras.src.ops.linalg import eigh\n from keras.src.ops.linalg import inv\n from keras.src.ops.linalg import lu_factor\n from keras.src.ops.linalg import norm\n\n@@ -7,6 +7,7 @@ since your modifications would be overwritten.\n from keras.src.ops.linalg import cholesky\n from keras.src.ops.linalg import det\n from keras.src.ops.linalg import eig\n+from keras.src.ops.linalg import eigh\n from keras.src.ops.linalg import inv\n from keras.src.ops.linalg import lu_factor\n from keras.src.ops.linalg import norm\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f344347fd63a0f0ec1c7c736bdf161382c20f67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 8 | Churn Cumulative: 57 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -357,14 +357,6 @@ class LinalgOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_eigh(self):\n         x = np.random.rand(2, 3, 3)\n         x = x @ x.transpose((0, 2, 1))\n-        if backend.backend() == \"jax\":\n-            import jax\n-\n-            if jax.default_backend() == \"gpu\":\n-                # eigh not implemented for jax on gpu backend\n-                with self.assertRaises(NotImplementedError):\n-                    linalg.eigh(x)\n-                return\n         w, v = map(ops.convert_to_numpy, linalg.eigh(x))\n         x_reconstructed = (v * w[..., None, :]) @ v.transpose((0, 2, 1))\n         self.assertAllClose(x_reconstructed, x, atol=1e-4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#da9af61032d5eaf7b5c64bcc7b215bff42dbe19a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 63 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 114 | Churn Cumulative: 793 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -47,6 +47,8 @@ def create_legacy_directory(package_dir):\n         )\n     with open(os.path.join(api_dir, \"__init__.py\"), \"w\") as f:\n         f.write(init_file)\n+    with open(os.path.join(package_dir, \"__init__.py\"), \"w\") as f:\n+        f.write(init_file)\n     # Remove the import of `_tf_keras` in `keras/_tf_keras/keras/__init__.py`\n     init_file = init_file.replace(\"from keras.api import _tf_keras\\n\", \"\\n\")\n     with open(os.path.join(tf_keras_dirpath, \"__init__.py\"), \"w\") as f:\n@@ -126,40 +128,6 @@ def export_version_string(api_init_fname):\n         f.write(contents)\n \n \n-def update_package_init(init_fname):\n-    contents = \"\"\"\n-# Import everything from /api/ into keras.\n-from keras.api import *  # noqa: F403\n-from keras.api import __version__  # Import * ignores names start with \"_\".\n-\n-import os\n-\n-# Add everything in /api/ to the module search path.\n-__path__.append(os.path.join(os.path.dirname(__file__), \"api\"))  # noqa: F405\n-\n-# Don't pollute namespace.\n-del os\n-\n-# Never autocomplete `.src` or `.api` on an imported keras object.\n-def __dir__():\n-    keys = dict.fromkeys((globals().keys()))\n-    keys.pop(\"src\")\n-    keys.pop(\"api\")\n-    return list(keys)\n-\n-\n-# Don't import `.src` or `.api` during `from keras import *`.\n-__all__ = [\n-    name\n-    for name in globals().keys()\n-    if not (name.startswith(\"_\") or name in (\"src\", \"api\"))\n-]\"\"\"\n-    with open(init_fname) as f:\n-        init_contents = f.read()\n-    with open(init_fname, \"w\") as f:\n-        f.write(init_contents.replace(\"\\nfrom keras import api\", contents))\n-\n-\n def build():\n     # Backup the `keras/__init__.py` and restore it on error in api gen.\n     root_path = os.path.dirname(os.path.abspath(__file__))\n@@ -181,8 +149,6 @@ def build():\n         namex.generate_api_files(\n             \"keras\", code_directory=\"src\", target_directory=\"api\"\n         )\n-        # Creates `keras/__init__.py` importing from `keras/api`\n-        update_package_init(build_init_fname)\n         # Add __version__ to keras package\n         export_version_string(build_api_init_fname)\n         # Creates `_tf_keras` with full keras API\n\n@@ -4,30 +4,52 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n-import os\n-\n-# Import everything from /api/ into keras.\n-from keras.api import *  # noqa: F403\n-from keras.api import __version__  # Import * ignores names start with \"_\".\n-\n-# Add everything in /api/ to the module search path.\n-__path__.append(os.path.join(os.path.dirname(__file__), \"api\"))  # noqa: F405\n-\n-# Don't pollute namespace.\n-del os\n-\n-\n-# Never autocomplete `.src` or `.api` on an imported keras object.\n-def __dir__():\n-    keys = dict.fromkeys((globals().keys()))\n-    keys.pop(\"src\")\n-    keys.pop(\"api\")\n-    return list(keys)\n-\n-\n-# Don't import `.src` or `.api` during `from keras import *`.\n-__all__ = [\n-    name\n-    for name in globals().keys()\n-    if not (name.startswith(\"_\") or name in (\"src\", \"api\"))\n-]\n+from keras.api import _tf_keras\n+from keras.api import activations\n+from keras.api import applications\n+from keras.api import backend\n+from keras.api import callbacks\n+from keras.api import config\n+from keras.api import constraints\n+from keras.api import datasets\n+from keras.api import distribution\n+from keras.api import dtype_policies\n+from keras.api import export\n+from keras.api import initializers\n+from keras.api import layers\n+from keras.api import legacy\n+from keras.api import losses\n+from keras.api import metrics\n+from keras.api import mixed_precision\n+from keras.api import models\n+from keras.api import ops\n+from keras.api import optimizers\n+from keras.api import preprocessing\n+from keras.api import quantizers\n+from keras.api import random\n+from keras.api import regularizers\n+from keras.api import saving\n+from keras.api import tree\n+from keras.api import utils\n+from keras.src.backend.common.keras_tensor import KerasTensor\n+from keras.src.backend.common.stateless_scope import StatelessScope\n+from keras.src.backend.exports import Variable\n+from keras.src.backend.exports import device\n+from keras.src.backend.exports import name_scope\n+from keras.src.dtype_policies.dtype_policy import DTypePolicy\n+from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n+from keras.src.initializers.initializer import Initializer\n+from keras.src.layers.core.input_layer import Input\n+from keras.src.layers.input_spec import InputSpec\n+from keras.src.layers.layer import Layer\n+from keras.src.losses.loss import Loss\n+from keras.src.metrics.metric import Metric\n+from keras.src.models.model import Model\n+from keras.src.models.sequential import Sequential\n+from keras.src.ops.function import Function\n+from keras.src.ops.operation import Operation\n+from keras.src.optimizers.optimizer import Optimizer\n+from keras.src.quantizers.quantizers import Quantizer\n+from keras.src.regularizers.regularizers import Regularizer\n+from keras.src.version import __version__\n+from keras.src.version import version\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04891e89da87c2a433beb12ff7dad59403c71671", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 51 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 114 | Churn Cumulative: 907 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,8 +47,6 @@ def create_legacy_directory(package_dir):\n         )\n     with open(os.path.join(api_dir, \"__init__.py\"), \"w\") as f:\n         f.write(init_file)\n-    with open(os.path.join(package_dir, \"__init__.py\"), \"w\") as f:\n-        f.write(init_file)\n     # Remove the import of `_tf_keras` in `keras/_tf_keras/keras/__init__.py`\n     init_file = init_file.replace(\"from keras.api import _tf_keras\\n\", \"\\n\")\n     with open(os.path.join(tf_keras_dirpath, \"__init__.py\"), \"w\") as f:\n@@ -128,6 +126,40 @@ def export_version_string(api_init_fname):\n         f.write(contents)\n \n \n+def update_package_init(init_fname):\n+    contents = \"\"\"\n+# Import everything from /api/ into keras.\n+from keras.api import *  # noqa: F403\n+from keras.api import __version__  # Import * ignores names start with \"_\".\n+\n+import os\n+\n+# Add everything in /api/ to the module search path.\n+__path__.append(os.path.join(os.path.dirname(__file__), \"api\"))  # noqa: F405\n+\n+# Don't pollute namespace.\n+del os\n+\n+# Never autocomplete `.src` or `.api` on an imported keras object.\n+def __dir__():\n+    keys = dict.fromkeys((globals().keys()))\n+    keys.pop(\"src\")\n+    keys.pop(\"api\")\n+    return list(keys)\n+\n+\n+# Don't import `.src` or `.api` during `from keras import *`.\n+__all__ = [\n+    name\n+    for name in globals().keys()\n+    if not (name.startswith(\"_\") or name in (\"src\", \"api\"))\n+]\"\"\"\n+    with open(init_fname) as f:\n+        init_contents = f.read()\n+    with open(init_fname, \"w\") as f:\n+        f.write(init_contents.replace(\"\\nfrom keras import api\", contents))\n+\n+\n def build():\n     # Backup the `keras/__init__.py` and restore it on error in api gen.\n     root_path = os.path.dirname(os.path.abspath(__file__))\n@@ -149,6 +181,8 @@ def build():\n         namex.generate_api_files(\n             \"keras\", code_directory=\"src\", target_directory=\"api\"\n         )\n+        # Creates `keras/__init__.py` importing from `keras/api`\n+        update_package_init(build_init_fname)\n         # Add __version__ to keras package\n         export_version_string(build_api_init_fname)\n         # Creates `_tf_keras` with full keras API\n\n@@ -4,52 +4,30 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n-from keras.api import _tf_keras\n-from keras.api import activations\n-from keras.api import applications\n-from keras.api import backend\n-from keras.api import callbacks\n-from keras.api import config\n-from keras.api import constraints\n-from keras.api import datasets\n-from keras.api import distribution\n-from keras.api import dtype_policies\n-from keras.api import export\n-from keras.api import initializers\n-from keras.api import layers\n-from keras.api import legacy\n-from keras.api import losses\n-from keras.api import metrics\n-from keras.api import mixed_precision\n-from keras.api import models\n-from keras.api import ops\n-from keras.api import optimizers\n-from keras.api import preprocessing\n-from keras.api import quantizers\n-from keras.api import random\n-from keras.api import regularizers\n-from keras.api import saving\n-from keras.api import tree\n-from keras.api import utils\n-from keras.src.backend.common.keras_tensor import KerasTensor\n-from keras.src.backend.common.stateless_scope import StatelessScope\n-from keras.src.backend.exports import Variable\n-from keras.src.backend.exports import device\n-from keras.src.backend.exports import name_scope\n-from keras.src.dtype_policies.dtype_policy import DTypePolicy\n-from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n-from keras.src.initializers.initializer import Initializer\n-from keras.src.layers.core.input_layer import Input\n-from keras.src.layers.input_spec import InputSpec\n-from keras.src.layers.layer import Layer\n-from keras.src.losses.loss import Loss\n-from keras.src.metrics.metric import Metric\n-from keras.src.models.model import Model\n-from keras.src.models.sequential import Sequential\n-from keras.src.ops.function import Function\n-from keras.src.ops.operation import Operation\n-from keras.src.optimizers.optimizer import Optimizer\n-from keras.src.quantizers.quantizers import Quantizer\n-from keras.src.regularizers.regularizers import Regularizer\n-from keras.src.version import __version__\n-from keras.src.version import version\n+import os\n+\n+# Import everything from /api/ into keras.\n+from keras.api import *  # noqa: F403\n+from keras.api import __version__  # Import * ignores names start with \"_\".\n+\n+# Add everything in /api/ to the module search path.\n+__path__.append(os.path.join(os.path.dirname(__file__), \"api\"))  # noqa: F405\n+\n+# Don't pollute namespace.\n+del os\n+\n+\n+# Never autocomplete `.src` or `.api` on an imported keras object.\n+def __dir__():\n+    keys = dict.fromkeys((globals().keys()))\n+    keys.pop(\"src\")\n+    keys.pop(\"api\")\n+    return list(keys)\n+\n+\n+# Don't import `.src` or `.api` during `from keras import *`.\n+__all__ = [\n+    name\n+    for name in globals().keys()\n+    if not (name.startswith(\"_\") or name in (\"src\", \"api\"))\n+]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#59213cec6a8853f9cdb16a58e88e9ec3e4944699", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 59 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -317,7 +317,7 @@ class Trainer:\n             if loss is not None:\n                 losses.append(loss)\n         for loss in self.losses:\n-            losses.append(ops.cast(loss, dtype=backend.floatx()))\n+            losses.append(ops.sum(ops.cast(loss, dtype=backend.floatx())))\n         if backend.backend() != \"jax\" and len(losses) == 0:\n             raise ValueError(\n                 \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b560406ce7e9dca3434be947ab697229a8087e3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 562 | Lines Deleted: 129 | Files Changed: 8 | Hunks: 86 | Methods Changed: 59 | Complexity Δ (Sum/Max): 66/57 | Churn Δ: 691 | Churn Cumulative: 936 | Contributors (this commit): 3 | Commits (past 90d): 22 | Contributors (cumulative): 22 | DMM Complexity: 0.46537396121883656\n\nDIFF:\n@@ -974,6 +974,17 @@ def tensordot(x1, x2, axes=2):\n @sparse.elementwise_unary(linear=False)\n def round(x, decimals=0):\n     x = convert_to_tensor(x)\n+\n+    # jnp.round doesn't support decimals < 0 for integers\n+    x_dtype = standardize_dtype(x.dtype)\n+    if \"int\" in x_dtype and decimals < 0:\n+        factor = cast(math.pow(10, decimals), config.floatx())\n+        x = cast(x, config.floatx())\n+        x = jnp.multiply(x, factor)\n+        x = jnp.round(x)\n+        x = jnp.divide(x, factor)\n+        return cast(x, x_dtype)\n+    else:\n         return jnp.round(x, decimals=decimals)\n \n \n\n@@ -1,5 +1,4 @@\n import tensorflow as tf\n-from tensorflow.experimental import numpy as tfnp\n \n from keras.src.backend import config\n from keras.src.backend import standardize_dtype\n@@ -36,6 +35,8 @@ def lu_factor(a):\n \n \n def norm(x, ord=None, axis=None, keepdims=False):\n+    from keras.src.backend.tensorflow.numpy import moveaxis\n+\n     x = convert_to_tensor(x)\n     x_shape = x.shape\n     ndim = x_shape.rank\n@@ -129,7 +130,7 @@ def norm(x, ord=None, axis=None, keepdims=False):\n                 keepdims=keepdims,\n             )\n         elif ord in (\"nuc\", 2, -2):\n-            x = tfnp.moveaxis(x, axis, (-2, -1))\n+            x = moveaxis(x, axis, (-2, -1))\n             if ord == -2:\n                 x = tf.math.reduce_min(\n                     tf.linalg.svd(x, compute_uv=False), axis=-1\n\n@@ -1,5 +1,4 @@\n import tensorflow as tf\n-from tensorflow.experimental import numpy as tfnp\n \n from keras.src.backend import config\n from keras.src.backend import standardize_dtype\n@@ -260,6 +259,8 @@ def solve(a, b):\n \n \n def norm(x, ord=None, axis=None, keepdims=False):\n+    from keras.src.backend.tensorflow.numpy import moveaxis\n+\n     x = convert_to_tensor(x)\n     x_shape = x.shape\n     ndim = x_shape.rank\n@@ -328,7 +329,7 @@ def norm(x, ord=None, axis=None, keepdims=False):\n                 keepdims=keepdims,\n             )\n         else:\n-            x = tfnp.moveaxis(x, axis, (-2, -1))\n+            x = moveaxis(x, axis, (-2, -1))\n             if ord == -2:\n                 x = tf.math.reduce_min(\n                     tf.linalg.svd(x, compute_uv=False), axis=-1\n\n@@ -7,7 +7,6 @@ import warnings\n \n import numpy as np\n import tensorflow as tf\n-from tensorflow.experimental import numpy as tfnp\n from tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\n \n from keras.src import tree\n@@ -575,12 +574,18 @@ def mean(x, axis=None, keepdims=False):\n \n \n def max(x, axis=None, keepdims=False, initial=None):\n+    x = convert_to_tensor(x)\n+\n     # The TensorFlow numpy API implementation doesn't support `initial` so we\n     # handle it manually here.\n     if initial is not None:\n-        return tf.math.maximum(\n-            tfnp.max(x, axis=axis, keepdims=keepdims), initial\n-        )\n+        if standardize_dtype(x.dtype) == \"bool\":\n+            x = tf.reduce_any(x, axis=axis, keepdims=keepdims)\n+            x = tf.math.maximum(tf.cast(x, \"int32\"), tf.cast(initial, \"int32\"))\n+            return tf.cast(x, \"bool\")\n+        else:\n+            x = tf.reduce_max(x, axis=axis, keepdims=keepdims)\n+            return tf.math.maximum(x, initial)\n \n     # TensorFlow returns -inf by default for an empty list, but for consistency\n     # with other backends and the numpy API we want to throw in this case.\n@@ -592,7 +597,10 @@ def max(x, axis=None, keepdims=False, initial=None):\n             message=\"Cannot compute the max of an empty tensor.\",\n         )\n \n-    return tfnp.max(x, axis=axis, keepdims=keepdims)\n+    if standardize_dtype(x.dtype) == \"bool\":\n+        return tf.reduce_any(x, axis=axis, keepdims=keepdims)\n+    else:\n+        return tf.reduce_max(x, axis=axis, keepdims=keepdims)\n \n \n def ones(shape, dtype=None):\n@@ -650,8 +658,6 @@ def append(x1, x2, axis=None):\n \n \n def arange(start, stop=None, step=1, dtype=None):\n-    # tfnp.arange has trouble with dynamic Tensors in compiled function.\n-    # tf.range does not.\n     if dtype is None:\n         dtypes_to_resolve = [\n             getattr(start, \"dtype\", type(start)),\n@@ -797,27 +803,39 @@ def array(x, dtype=None):\n \n def average(x, axis=None, weights=None):\n     x = convert_to_tensor(x)\n-    axis = to_tuple_or_list(axis)\n-    dtypes_to_resolve = [x.dtype, float]\n-    if weights is not None:\n+\n+    if weights is None:  # Treat all weights as 1\n+        dtype = dtypes.result_type(x.dtype, float)\n+        x = tf.cast(x, dtype)\n+        avg = tf.reduce_mean(x, axis=axis)\n+    else:\n         weights = convert_to_tensor(weights)\n-        dtypes_to_resolve.append(weights.dtype)\n-    result_dtype = dtypes.result_type(*dtypes_to_resolve)\n-    compute_dtype = result_dtype\n-    # TODO: since tfnp.average incorrectly promote bfloat16 to float64, we\n-    # need to cast to float32 first and then cast back to bfloat16\n-    if compute_dtype == \"bfloat16\":\n-        compute_dtype = \"float32\"\n-    x = tf.cast(x, compute_dtype)\n-    if weights is not None:\n-        weights = tf.cast(weights, compute_dtype)\n+        dtype = dtypes.result_type(x.dtype, weights.dtype, float)\n+        x = tf.cast(x, dtype)\n+        weights = tf.cast(weights, dtype)\n+\n+        def _rank_equal_case():\n+            weights_sum = tf.reduce_sum(weights, axis=axis)\n+            return tf.reduce_sum(x * weights, axis=axis) / weights_sum\n+\n+        def _rank_not_equal_case():\n+            weights_sum = tf.reduce_sum(weights)\n+            axes = tf.convert_to_tensor([[axis], [0]])\n+            return tf.tensordot(x, weights, axes) / weights_sum\n+\n         if axis is None:\n-        x = tfnp.average(x, weights=weights, axis=None)\n-        return tf.cast(x, result_dtype)\n-    for a in axis:\n-        # `tfnp.average` does not handle multiple axes.\n-        x = tfnp.average(x, weights=weights, axis=a)\n-    return tf.cast(x, result_dtype)\n+            avg = _rank_equal_case()\n+        else:\n+            # We condition on rank rather than shape equality, because if we do\n+            # the latter, when the shapes are partially unknown but the ranks\n+            # are known and different, np_utils.cond will run shape checking on\n+            # the true branch, which will raise a shape-checking error.\n+            avg = tf.cond(\n+                tf.equal(tf.rank(x), tf.rank(weights)),\n+                _rank_equal_case,\n+                _rank_not_equal_case,\n+            )\n+    return avg\n \n \n def broadcast_to(x, shape):\n@@ -876,7 +894,8 @@ def conj(x):\n \n @sparse.elementwise_unary\n def copy(x):\n-    return tfnp.copy(x)\n+    x = convert_to_tensor(x)\n+    return tf.identity(x)\n \n \n @sparse.densifying_unary(1)\n@@ -908,18 +927,53 @@ def count_nonzero(x, axis=None):\n def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n+    if axis is not None:\n+        axisa = axis\n+        axisb = axis\n+        axisc = axis\n+    x1 = moveaxis(x1, axisa, -1)\n+    x2 = moveaxis(x2, axisb, -1)\n+\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-    return tfnp.cross(\n-        x1,\n-        x2,\n-        axisa=axisa,\n-        axisb=axisb,\n-        axisc=axisc,\n-        axis=axis,\n+\n+    def maybe_pad_zeros(x, size_of_last_dim):\n+        def pad_zeros(x):\n+            return tf.pad(\n+                x,\n+                tf.concat(\n+                    [\n+                        tf.zeros([tf.rank(x) - 1, 2], \"int32\"),\n+                        tf.constant([[0, 1]], \"int32\"),\n+                    ],\n+                    axis=0,\n+                ),\n             )\n \n+        return tf.cond(\n+            tf.equal(size_of_last_dim, 2), lambda: pad_zeros(x), lambda: x\n+        )\n+\n+    x1_dim = tf.shape(x1)[-1]\n+    x2_dim = tf.shape(x2)[-1]\n+    x1 = maybe_pad_zeros(x1, x1_dim)\n+    x2 = maybe_pad_zeros(x2, x2_dim)\n+\n+    # Broadcast each other\n+    shape = tf.shape(x1)\n+    shape = tf.broadcast_dynamic_shape(shape, tf.shape(x2))\n+    x1 = tf.broadcast_to(x1, shape)\n+    x2 = tf.broadcast_to(x2, shape)\n+\n+    c = tf.linalg.cross(x1, x2)\n+    c = tf.cond(\n+        (x1_dim == 2) & (x2_dim == 2),\n+        lambda: c[..., 2],\n+        lambda: moveaxis(c, -1, axisc),\n+    )\n+    return c\n+\n \n def cumprod(x, axis=None, dtype=None):\n     x = convert_to_tensor(x, dtype=dtype)\n@@ -944,20 +998,70 @@ def cumsum(x, axis=None, dtype=None):\n \n \n def diag(x, k=0):\n-    return tfnp.diag(x, k=k)\n+    x = convert_to_tensor(x)\n+    if len(x.shape) == 1:\n+        return tf.cond(\n+            tf.equal(tf.size(x), 0),\n+            lambda: tf.zeros([builtins.abs(k), builtins.abs(k)], dtype=x.dtype),\n+            lambda: tf.linalg.diag(x, k=k),\n+        )\n+    elif len(x.shape) == 2:\n+        return diagonal(x, offset=k)\n+    else:\n+        raise ValueError(f\"`x` must be 1d or 2d. Received: x.shape={x.shape}\")\n \n \n def diagonal(x, offset=0, axis1=0, axis2=1):\n-    return tfnp.diagonal(\n-        x,\n-        offset=offset,\n-        axis1=axis1,\n-        axis2=axis2,\n+    x = convert_to_tensor(x)\n+    x_rank = x.ndim\n+    if (\n+        offset == 0\n+        and (axis1 == x_rank - 2 or axis1 == -2)\n+        and (axis2 == x_rank - 1 or axis2 == -1)\n+    ):\n+        return tf.linalg.diag_part(x)\n+\n+    x = moveaxis(x, (axis1, axis2), (-2, -1))\n+    x_shape = tf.shape(x)\n+\n+    def _zeros():\n+        return tf.zeros(tf.concat([x_shape[:-1], [0]], 0), dtype=x.dtype)\n+\n+    x, offset = tf.cond(\n+        tf.logical_or(\n+            tf.less_equal(offset, -1 * x_shape[-2]),\n+            tf.greater_equal(offset, x_shape[-1]),\n+        ),\n+        _zeros,\n+        lambda: (x, offset),\n     )\n+    return tf.linalg.diag_part(x, k=offset)\n \n \n def diff(a, n=1, axis=-1):\n-    return tfnp.diff(a, n=n, axis=axis)\n+    a = convert_to_tensor(a)\n+    if n == 0:\n+        return a\n+    elif n < 0:\n+        raise ValueError(f\"Order `n` must be non-negative. Received n={n}\")\n+    elif a.ndim == 0:\n+        raise ValueError(\n+            \"`diff` requires input that is at least one dimensional. \"\n+            f\"Received: a={a}\"\n+        )\n+    axis = canonicalize_axis(axis, a.ndim)\n+    slice1 = [slice(None)] * a.ndim\n+    slice2 = [slice(None)] * a.ndim\n+    slice1[axis] = slice(1, None)\n+    slice2[axis] = slice(None, -1)\n+    slice1_tuple = tuple(slice1)\n+    slice2_tuple = tuple(slice2)\n+    for _ in range(n):\n+        if standardize_dtype(a.dtype) == \"bool\":\n+            a = tf.not_equal(a[slice1_tuple], a[slice2_tuple])\n+        else:\n+            a = tf.subtract(a[slice1_tuple], a[slice2_tuple])\n+    return a\n \n \n def digitize(x, bins):\n@@ -1152,7 +1256,6 @@ def isclose(x1, x2):\n \n @sparse.densifying_unary(True)\n def isfinite(x):\n-    # `tfnp.isfinite` requires `enable_numpy_behavior`, so we reimplement it.\n     x = convert_to_tensor(x)\n     dtype_as_dtype = tf.as_dtype(x.dtype)\n     if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n@@ -1161,7 +1264,6 @@ def isfinite(x):\n \n \n def isinf(x):\n-    # `tfnp.isinf` requires `enable_numpy_behavior`, so we reimplement it.\n     x = convert_to_tensor(x)\n     dtype_as_dtype = tf.as_dtype(x.dtype)\n     if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n@@ -1170,7 +1272,6 @@ def isinf(x):\n \n \n def isnan(x):\n-    # `tfnp.isnan` requires `enable_numpy_behavior`, so we reimplement it.\n     x = convert_to_tensor(x)\n     dtype_as_dtype = tf.as_dtype(x.dtype)\n     if dtype_as_dtype.is_integer or not dtype_as_dtype.is_numeric:\n@@ -1206,15 +1307,37 @@ def linspace(\n             float,\n         ]\n         dtype = dtypes.result_type(*dtypes_to_resolve)\n-    return tfnp.linspace(\n-        start,\n-        stop,\n-        num=num,\n-        endpoint=endpoint,\n-        retstep=retstep,\n-        dtype=dtype,\n-        axis=axis,\n+    else:\n+        dtype = standardize_dtype(dtype)\n+    start = convert_to_tensor(start, dtype=dtype)\n+    stop = convert_to_tensor(stop, dtype=dtype)\n+    if num < 0:\n+        raise ValueError(\n+            f\"`num` must be a non-negative integer. Received: num={num}\"\n         )\n+    step = tf.convert_to_tensor(np.nan)\n+    if endpoint:\n+        result = tf.linspace(start, stop, num, axis=axis)\n+        if num > 1:\n+            step = (stop - start) / (num - 1)\n+    else:\n+        # tf.linspace doesn't support endpoint=False, so we manually handle it\n+        if num > 0:\n+            step = (stop - start) / num\n+        if num > 1:\n+            new_stop = tf.cast(stop, step.dtype) - step\n+            start = tf.cast(start, new_stop.dtype)\n+            result = tf.linspace(start, new_stop, num, axis=axis)\n+        else:\n+            result = tf.linspace(start, stop, num, axis=axis)\n+    if dtype is not None:\n+        if \"int\" in dtype:\n+            result = tf.floor(result)\n+        result = tf.cast(result, dtype)\n+    if retstep:\n+        return (result, step)\n+    else:\n+        return result\n \n \n @sparse.densifying_unary(-np.inf)\n@@ -1271,9 +1394,6 @@ def logaddexp(x1, x2):\n     dtype = dtypes.result_type(x1.dtype, x2.dtype, float)\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n-\n-    # Below is the same implementation as tfnp.logaddexp using all native\n-    # ops to prevent incorrect promotion of bfloat16.\n     delta = x1 - x2\n     return tf.where(\n         tf.math.is_nan(delta),\n@@ -1300,24 +1420,15 @@ def logical_or(x1, x2):\n \n \n def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n-    if dtype is None:\n-        dtypes_to_resolve = [\n-            getattr(start, \"dtype\", type(start)),\n-            getattr(stop, \"dtype\", type(stop)),\n-            float,\n-        ]\n-        dtype = dtypes.result_type(*dtypes_to_resolve)\n-    start = tf.cast(start, dtype)\n-    stop = tf.cast(stop, dtype)\n-    return tfnp.logspace(\n-        start,\n-        stop,\n+    result = linspace(\n+        start=start,\n+        stop=stop,\n         num=num,\n         endpoint=endpoint,\n-        base=base,\n         dtype=dtype,\n         axis=axis,\n     )\n+    return tf.pow(tf.cast(base, result.dtype), result)\n \n \n @sparse.elementwise_binary_union(tf.sparse.maximum, densify_mixed=True)\n@@ -1345,12 +1456,17 @@ def meshgrid(*x, indexing=\"xy\"):\n \n def min(x, axis=None, keepdims=False, initial=None):\n     x = convert_to_tensor(x)\n+\n     # The TensorFlow numpy API implementation doesn't support `initial` so we\n     # handle it manually here.\n     if initial is not None:\n-        return tf.math.minimum(\n-            tfnp.min(x, axis=axis, keepdims=keepdims), initial\n-        )\n+        if standardize_dtype(x.dtype) == \"bool\":\n+            x = tf.reduce_all(x, axis=axis, keepdims=keepdims)\n+            x = tf.math.minimum(tf.cast(x, \"int32\"), tf.cast(initial, \"int32\"))\n+            return tf.cast(x, \"bool\")\n+        else:\n+            x = tf.reduce_min(x, axis=axis, keepdims=keepdims)\n+        return tf.math.minimum(x, initial)\n \n     # TensorFlow returns inf by default for an empty list, but for consistency\n     # with other backends and the numpy API we want to throw in this case.\n@@ -1362,7 +1478,10 @@ def min(x, axis=None, keepdims=False, initial=None):\n             message=\"Cannot compute the min of an empty tensor.\",\n         )\n \n-    return tfnp.min(x, axis=axis, keepdims=keepdims)\n+    if standardize_dtype(x.dtype) == \"bool\":\n+        return tf.reduce_all(x, axis=axis, keepdims=keepdims)\n+    else:\n+        return tf.reduce_min(x, axis=axis, keepdims=keepdims)\n \n \n @sparse.elementwise_binary_union(tf.sparse.minimum, densify_mixed=True)\n@@ -1392,7 +1511,24 @@ def mod(x1, x2):\n \n \n def moveaxis(x, source, destination):\n-    return tfnp.moveaxis(x, source=source, destination=destination)\n+    x = convert_to_tensor(x)\n+\n+    _source = to_tuple_or_list(source)\n+    _destination = to_tuple_or_list(destination)\n+    _source = tuple(canonicalize_axis(i, x.ndim) for i in _source)\n+    _destination = tuple(canonicalize_axis(i, x.ndim) for i in _destination)\n+    if len(_source) != len(_destination):\n+        raise ValueError(\n+            \"Inconsistent number of `source` and `destination`. \"\n+            f\"Received: source={source}, destination={destination}\"\n+        )\n+    # Directly return x if no movement is required\n+    if _source == _destination:\n+        return x\n+    perm = [i for i in range(x.ndim) if i not in _source]\n+    for dest, src in sorted(zip(_destination, _source)):\n+        perm.insert(dest, src)\n+    return tf.transpose(x, perm)\n \n \n def nan_to_num(x):\n@@ -1615,8 +1751,6 @@ def reciprocal(x):\n \n \n def repeat(x, repeats, axis=None):\n-    # tfnp.repeat has trouble with dynamic Tensors in compiled function.\n-    # tf.repeat does not.\n     x = convert_to_tensor(x)\n     # TODO: tf.repeat doesn't support uint16\n     if standardize_dtype(x.dtype) == \"uint16\":\n@@ -1640,7 +1774,14 @@ def reshape(x, newshape):\n \n \n def roll(x, shift, axis=None):\n-    return tfnp.roll(x, shift, axis=axis)\n+    x = convert_to_tensor(x)\n+    if axis is not None:\n+        return tf.roll(x, shift=shift, axis=axis)\n+\n+    # If axis is None, the roll happens as a 1-d tensor.\n+    original_shape = tf.shape(x)\n+    x = tf.roll(tf.reshape(x, [-1]), shift, 0)\n+    return tf.reshape(x, original_shape)\n \n \n @sparse.elementwise_unary\n@@ -1695,14 +1836,12 @@ def split(x, indices_or_sections, axis=0):\n     if not isinstance(indices_or_sections, int):\n         # `tf.split` requires `num_or_size_splits`, so we need to convert\n         # `indices_or_sections` to the appropriate format.\n-        # The following implementation offers better compatibility for the\n-        # tensor argument `indices_or_sections` than original `tfnp.split`.\n         total_size = x.shape[axis]\n         indices_or_sections = convert_to_tensor(indices_or_sections)\n         start_size = indices_or_sections[0:1]\n         end_size = total_size - indices_or_sections[-1:]\n         num_or_size_splits = tf.concat(\n-            [start_size, tfnp.diff(indices_or_sections), end_size], axis=0\n+            [start_size, diff(indices_or_sections), end_size], axis=0\n         )\n     else:\n         num_or_size_splits = indices_or_sections\n@@ -1726,7 +1865,34 @@ def std(x, axis=None, keepdims=False):\n \n \n def swapaxes(x, axis1, axis2):\n-    return tfnp.swapaxes(x, axis1=axis1, axis2=axis2)\n+    x = convert_to_tensor(x)\n+\n+    if (\n+        x.shape.rank is not None\n+        and isinstance(axis1, int)\n+        and isinstance(axis2, int)\n+    ):\n+        # This branch makes sure `perm` is statically known, to avoid a\n+        # not-compile-time-constant XLA error.\n+        axis1 = canonicalize_axis(axis1, x.ndim)\n+        axis2 = canonicalize_axis(axis2, x.ndim)\n+\n+        # Directly return x if no movement is required\n+        if axis1 == axis2:\n+            return x\n+\n+        perm = list(range(x.ndim))\n+        perm[axis1] = axis2\n+        perm[axis2] = axis1\n+    else:\n+        x_rank = tf.rank(x)\n+        axis1 = tf.where(axis1 < 0, tf.add(axis1, x_rank), axis1)\n+        axis2 = tf.where(axis2 < 0, tf.add(axis2, x_rank), axis2)\n+        perm = tf.range(x_rank)\n+        perm = tf.tensor_scatter_nd_update(\n+            perm, [[axis1], [axis2]], [axis2, axis1]\n+        )\n+    return tf.transpose(x, perm)\n \n \n def take(x, indices, axis=None):\n@@ -1737,9 +1903,7 @@ def take(x, indices, axis=None):\n                 f\"`x.dtype={x.dtype}` when `indices` is a sparse tensor; \"\n                 \"densifying `indices`.\"\n             )\n-            return tfnp.take(\n-                x, convert_to_tensor(indices, sparse=False), axis=axis\n-            )\n+            return take(x, convert_to_tensor(indices, sparse=False), axis=axis)\n         if axis is None:\n             x = tf.reshape(x, (-1,))\n         elif axis != 0:\n@@ -1748,9 +1912,7 @@ def take(x, indices, axis=None):\n                 f\"`axis={axis}` when `indices` is a sparse tensor; \"\n                 \"densifying `indices`.\"\n             )\n-            return tfnp.take(\n-                x, convert_to_tensor(indices, sparse=False), axis=axis\n-            )\n+            return take(x, convert_to_tensor(indices, sparse=False), axis=axis)\n         output = tf.nn.safe_embedding_lookup_sparse(\n             embedding_weights=tf.convert_to_tensor(x),\n             sparse_ids=tf.sparse.expand_dims(indices, axis=-1),\n@@ -1758,11 +1920,72 @@ def take(x, indices, axis=None):\n         )\n         output.set_shape(indices.shape + output.shape[len(indices.shape) :])\n         return output\n-    return tfnp.take(x, indices, axis=axis)\n+\n+    x = convert_to_tensor(x)\n+    indices = convert_to_tensor(indices)\n+    if axis is None:\n+        x = tf.reshape(x, [-1])\n+        axis = 0\n+    # Correct the indices using \"fill\" mode which is the same as in jax\n+    indices = tf.where(\n+        indices < 0,\n+        indices + tf.cast(tf.shape(x)[axis], indices.dtype),\n+        indices,\n+    )\n+    return tf.gather(x, indices, axis=axis)\n \n \n def take_along_axis(x, indices, axis=None):\n-    return tfnp.take_along_axis(x, indices, axis=axis)\n+    x = convert_to_tensor(x)\n+    indices = convert_to_tensor(indices, \"int64\")\n+    if axis is None:\n+        if indices.ndim != 1:\n+            raise ValueError(\n+                \"`indices` must be 1D if axis=None. \"\n+                f\"Received: indices.shape={indices.shape}\"\n+            )\n+        return take_along_axis(tf.reshape(x, [-1]), indices, 0)\n+    rank = tf.rank(x)\n+    static_axis = axis\n+    axis = axis + rank if axis < 0 else axis\n+\n+    # Broadcast shapes to match, ensure that the axis of interest is not\n+    # broadcast.\n+    x_shape_original = tf.shape(x, out_type=indices.dtype)\n+    indices_shape_original = tf.shape(indices, out_type=indices.dtype)\n+    x_shape = tf.tensor_scatter_nd_update(x_shape_original, [[axis]], [1])\n+    indices_shape = tf.tensor_scatter_nd_update(\n+        indices_shape_original, [[axis]], [1]\n+    )\n+    broadcasted_shape = tf.broadcast_dynamic_shape(x_shape, indices_shape)\n+    x_shape = tf.tensor_scatter_nd_update(\n+        broadcasted_shape, [[axis]], [x_shape_original[axis]]\n+    )\n+    indices_shape = tf.tensor_scatter_nd_update(\n+        broadcasted_shape, [[axis]], [indices_shape_original[axis]]\n+    )\n+    x = tf.broadcast_to(x, x_shape)\n+    indices = tf.broadcast_to(indices, indices_shape)\n+\n+    # Save indices shape so we can restore it later.\n+    possible_result_shape = indices.shape\n+\n+    # Correct the indices using \"fill\" mode which is the same as in jax\n+    indices = tf.where(indices < 0, indices + x_shape[static_axis], indices)\n+\n+    x = swapaxes(x, static_axis, -1)\n+    indices = swapaxes(indices, static_axis, -1)\n+\n+    x_shape = tf.shape(x)\n+    x = tf.reshape(x, [-1, x_shape[-1]])\n+    indices_shape = tf.shape(indices)\n+    indices = tf.reshape(indices, [-1, indices_shape[-1]])\n+\n+    result = tf.gather(x, indices, batch_dims=1)\n+    result = tf.reshape(result, indices_shape)\n+    result = swapaxes(result, static_axis, -1)\n+    result.set_shape(possible_result_shape)\n+    return result\n \n \n @sparse.elementwise_unary\n@@ -1800,7 +2023,6 @@ def tensordot(x1, x2, axes=2):\n \n @sparse.elementwise_unary\n def round(x, decimals=0):\n-    # `tfnp.round` requires `enable_numpy_behavior`, so we reimplement it.\n     if decimals == 0:\n         return tf.round(x)\n     x_dtype = x.dtype\n@@ -1821,7 +2043,6 @@ def round(x, decimals=0):\n \n \n def tile(x, repeats):\n-    # The TFNP implementation is buggy, we roll our own.\n     x = convert_to_tensor(x)\n     repeats = tf.reshape(convert_to_tensor(repeats, dtype=\"int32\"), [-1])\n     repeats_size = tf.size(repeats)\n@@ -1844,12 +2065,39 @@ def trace(x, offset=0, axis1=0, axis2=1):\n     dtype = standardize_dtype(x.dtype)\n     if dtype not in (\"int64\", \"uint32\", \"uint64\"):\n         dtype = dtypes.result_type(dtype, \"int32\")\n-    return tfnp.trace(x, offset=offset, axis1=axis1, axis2=axis2, dtype=dtype)\n+    x_shape = tf.shape(x)\n+    x = moveaxis(x, (axis1, axis2), (-2, -1))\n+    # Mask out the diagonal and reduce.\n+    x = tf.where(\n+        eye(x_shape[axis1], x_shape[axis2], k=offset, dtype=\"bool\"),\n+        x,\n+        tf.zeros_like(x),\n+    )\n+    # The output dtype is set to \"int32\" if the input dtype is \"bool\"\n+    if standardize_dtype(x.dtype) == \"bool\":\n+        x = tf.cast(x, \"int32\")\n+    return tf.cast(tf.reduce_sum(x, axis=(-2, -1)), dtype)\n \n \n def tri(N, M=None, k=0, dtype=None):\n-    dtype = dtype or config.floatx()\n-    return tfnp.tri(N, M=M, k=k, dtype=dtype)\n+    M = M if M is not None else N\n+    dtype = standardize_dtype(dtype or config.floatx())\n+    if k < 0:\n+        lower = -k - 1\n+        if lower > N:\n+            r = tf.zeros([N, M], dtype=dtype)\n+        else:\n+            o = tf.ones([N, M], dtype=\"bool\")\n+            r = tf.cast(\n+                tf.logical_not(tf.linalg.band_part(o, lower, -1)), dtype=dtype\n+            )\n+    else:\n+        o = tf.ones([N, M], dtype=dtype)\n+        if k > M:\n+            r = o\n+        else:\n+            r = tf.linalg.band_part(o, -1, k)\n+    return r\n \n \n def tril(x, k=0):\n@@ -2069,7 +2317,29 @@ def sum(x, axis=None, keepdims=False):\n \n def eye(N, M=None, k=0, dtype=None):\n     dtype = dtype or config.floatx()\n-    return tfnp.eye(N, M=M, k=k, dtype=dtype)\n+    if not M:\n+        M = N\n+    # Making sure N, M and k are `int`\n+    N, M, k = int(N), int(M), int(k)\n+    if k >= M or -k >= N:\n+        # tf.linalg.diag will raise an error in this case\n+        return zeros([N, M], dtype=dtype)\n+    if k == 0:\n+        return tf.eye(N, M, dtype=dtype)\n+    # We need the precise length, otherwise tf.linalg.diag will raise an error\n+    diag_len = builtins.min(N, M)\n+    if k > 0:\n+        if N >= M:\n+            diag_len -= k\n+        elif N + k > M:\n+            diag_len = M - k\n+    elif k <= 0:\n+        if M >= N:\n+            diag_len += k\n+        elif M - k > N:\n+            diag_len = N + k\n+    diagonal_ = tf.ones([diag_len], dtype=dtype)\n+    return tf.linalg.diag(diagonal=diagonal_, num_rows=N, num_cols=M, k=k)\n \n \n def floor_divide(x1, x2):\n\n@@ -1,5 +1,4 @@\n import tensorflow as tf\n-from tensorflow.experimental import numpy as tfnp\n \n from keras.src.backend.common import standardize_dtype\n from keras.src.backend.config import floatx\n@@ -87,12 +86,14 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n \n \n def shuffle(x, axis=0, seed=None):\n+    from keras.src.backend.tensorflow.numpy import swapaxes\n+\n     seed = tf_draw_seed(seed)\n     if axis == 0:\n         return tf.random.experimental.stateless_shuffle(x, seed=seed)\n-    x = tfnp.swapaxes(x, axis1=0, axis2=axis)\n+    x = swapaxes(x, axis1=0, axis2=axis)\n     x = tf.random.experimental.stateless_shuffle(x, seed=seed)\n-    x = tfnp.swapaxes(x, axis1=0, axis2=axis)\n+    x = swapaxes(x, axis1=0, axis2=axis)\n     return x\n \n \n\n@@ -172,8 +172,11 @@ def max(x, axis=None, keepdims=False, initial=None):\n         result = result.values\n \n     if initial is not None:\n-        initial = convert_to_tensor(initial)\n-        return torch.maximum(result, torch.full(result.shape, initial))\n+        dtype = to_torch_dtype(result.dtype)\n+        initial = convert_to_tensor(initial, dtype=dtype)\n+        return torch.maximum(\n+            result, torch.full(result.shape, initial, dtype=dtype)\n+        )\n     return result\n \n \n@@ -744,7 +747,14 @@ def linspace(\n         dtype = dtypes.result_type(*dtypes_to_resolve)\n     dtype = to_torch_dtype(dtype)\n \n-    if endpoint is False:\n+    step = convert_to_tensor(torch.nan)\n+    if endpoint:\n+        if num > 1:\n+            step = (stop - start) / (num - 1)\n+    else:\n+        if num > 0:\n+            step = (stop - start) / num\n+        if num > 1:\n             stop = stop - ((stop - start) / num)\n     if hasattr(start, \"__len__\") and hasattr(stop, \"__len__\"):\n         start = convert_to_tensor(start, dtype=dtype)\n@@ -766,7 +776,7 @@ def linspace(\n             device=get_device(),\n         )\n     if retstep is True:\n-        return (linspace, num)\n+        return (linspace, step)\n     return linspace\n \n \n@@ -949,7 +959,8 @@ def min(x, axis=None, keepdims=False, initial=None):\n         result = result.values\n \n     if initial is not None:\n-        initial = convert_to_tensor(initial)\n+        dtype = to_torch_dtype(result.dtype)\n+        initial = convert_to_tensor(initial, dtype=dtype)\n         return torch.minimum(result, initial)\n     return result\n \n@@ -1265,6 +1276,13 @@ def swapaxes(x, axis1, axis2):\n def take(x, indices, axis=None):\n     x = convert_to_tensor(x)\n     indices = convert_to_tensor(indices).long()\n+    # Correct the indices using \"fill\" mode which is the same as in jax\n+    x_dim = x.shape[axis] if axis is not None else x.shape[0]\n+    indices = torch.where(\n+        indices < 0,\n+        indices + x_dim,\n+        indices,\n+    )\n     if x.ndim == 2 and axis == 0:\n         # This case is equivalent to embedding lookup.\n         return torch.nn.functional.embedding(indices, x)\n@@ -1285,6 +1303,13 @@ def take(x, indices, axis=None):\n def take_along_axis(x, indices, axis=None):\n     x = convert_to_tensor(x)\n     indices = convert_to_tensor(indices).long()\n+    # Correct the indices using \"fill\" mode which is the same as in jax\n+    x_dim = x.shape[axis] if axis is not None else x.shape[0]\n+    indices = torch.where(\n+        indices < 0,\n+        indices + x_dim,\n+        indices,\n+    )\n     return torch.take_along_dim(x, indices, dim=axis)\n \n \n\n@@ -5266,14 +5266,18 @@ def trace(x, offset=0, axis1=0, axis2=1):\n \n \n class Tri(Operation):\n-    def call(self, N, M=None, k=0, dtype=None):\n-        return backend.numpy.tri(N, M=M, k=k, dtype=dtype)\n+    def __init__(self, k=0, dtype=None):\n+        super().__init__()\n+        self.k = k\n+        self.dtype = dtype or backend.floatx()\n \n-    def compute_output_spec(self, N, M=None, k=0, dtype=None):\n+    def call(self, N, M=None):\n+        return backend.numpy.tri(N=N, M=M, k=self.k, dtype=self.dtype)\n+\n+    def compute_output_spec(self, N, M=None):\n         if M is None:\n             M = N\n-        dtype = dtype or backend.floatx()\n-        return KerasTensor((N, M), dtype=dtype)\n+        return KerasTensor((N, M), dtype=self.dtype)\n \n \n @keras_export([\"keras.ops.tri\", \"keras.ops.numpy.tri\"])\n@@ -6021,14 +6025,18 @@ def ones(shape, dtype=None):\n \n \n class Eye(Operation):\n-    def call(self, N, M=None, k=0, dtype=None):\n-        return backend.numpy.eye(N, M=M, k=k, dtype=dtype)\n+    def __init__(self, k=0, dtype=None):\n+        super().__init__()\n+        self.k = k\n+        self.dtype = dtype or backend.floatx()\n \n-    def compute_output_spec(self, N, M=None, k=0, dtype=None):\n+    def call(self, N, M=None):\n+        return backend.numpy.eye(N, M=M, k=self.k, dtype=self.dtype)\n+\n+    def compute_output_spec(self, N, M=None):\n         if M is None:\n             M = N\n-        dtype = dtype or backend.floatx()\n-        return KerasTensor((N, M), dtype=dtype)\n+        return KerasTensor((N, M), dtype=self.dtype)\n \n \n @keras_export([\"keras.ops.eye\", \"keras.ops.numpy.eye\"])\n\n@@ -2170,6 +2170,14 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             self.assertAllClose(knp.Cross()(x1, y3), np.cross(x1, y3))\n             self.assertAllClose(knp.Cross()(x2, y3), np.cross(x2, y3))\n \n+        # Test axis is not None\n+        self.assertAllClose(\n+            knp.cross(x1, y1, axis=-1), np.cross(x1, y1, axis=-1)\n+        )\n+        self.assertAllClose(\n+            knp.Cross(axis=-1)(x1, y1), np.cross(x1, y1, axis=-1)\n+        )\n+\n     def test_einsum(self):\n         x = np.arange(24).reshape([2, 3, 4]).astype(\"float32\")\n         y = np.arange(24).reshape([2, 4, 3]).astype(\"float32\")\n@@ -2452,6 +2460,10 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             knp.Linspace(num=5, endpoint=False)(0, 10),\n             np.linspace(0, 10, 5, endpoint=False),\n         )\n+        self.assertAllClose(\n+            knp.Linspace(num=0, endpoint=False)(0, 10),\n+            np.linspace(0, 10, 0, endpoint=False),\n+        )\n \n         start = np.zeros([2, 3, 4])\n         stop = np.ones([2, 3, 4])\n@@ -2659,27 +2671,33 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.Take()(x, 0), np.take(x, 0))\n         self.assertAllClose(knp.Take(axis=1)(x, 0), np.take(x, 0, axis=1))\n \n-        # test with multi-dimensional indices\n+        # Test with multi-dimensional indices\n         rng = np.random.default_rng(0)\n         x = rng.standard_normal((2, 3, 4, 5))\n         indices = rng.integers(0, 4, (6, 7))\n         self.assertAllClose(\n-            knp.take(x, indices, axis=2),\n-            np.take(x, indices, axis=2),\n+            knp.take(x, indices, axis=2), np.take(x, indices, axis=2)\n         )\n \n-        # test with negative axis\n+        # Test with negative axis\n         self.assertAllClose(\n-            knp.take(x, indices, axis=-2),\n-            np.take(x, indices, axis=-2),\n+            knp.take(x, indices, axis=-2), np.take(x, indices, axis=-2)\n         )\n-        # test with axis=None & x.ndim=2\n+\n+        # Test with axis=None & x.ndim=2\n         x = np.array(([1, 2], [3, 4]))\n         indices = np.array([2, 3])\n         self.assertAllClose(\n             knp.take(x, indices, axis=None), np.take(x, indices, axis=None)\n         )\n \n+        # Test with negative indices\n+        x = rng.standard_normal((2, 3, 4, 5))\n+        indices = rng.integers(-3, 0, (6, 7))\n+        self.assertAllClose(\n+            knp.take(x, indices, axis=2), np.take(x, indices, axis=2)\n+        )\n+\n     @parameterized.named_parameters(\n         named_product(\n             [\n@@ -2744,6 +2762,30 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             np.take_along_axis(x, indices, axis=2),\n         )\n \n+        # Test with axis=None\n+        x = np.arange(12).reshape([1, 1, 3, 4])\n+        indices = np.array([1, 2, 3], dtype=np.int32)\n+        self.assertAllClose(\n+            knp.take_along_axis(x, indices, axis=None),\n+            np.take_along_axis(x, indices, axis=None),\n+        )\n+        self.assertAllClose(\n+            knp.TakeAlongAxis(axis=None)(x, indices),\n+            np.take_along_axis(x, indices, axis=None),\n+        )\n+\n+        # Test with negative indices\n+        x = np.arange(12).reshape([1, 1, 3, 4])\n+        indices = np.full([1, 4, 1, 1], -1, dtype=np.int32)\n+        self.assertAllClose(\n+            knp.take_along_axis(x, indices, axis=2),\n+            np.take_along_axis(x, indices, axis=2),\n+        )\n+        self.assertAllClose(\n+            knp.TakeAlongAxis(axis=2)(x, indices),\n+            np.take_along_axis(x, indices, axis=2),\n+        )\n+\n     def test_tensordot(self):\n         x = np.arange(24).reshape([1, 2, 3, 4]).astype(\"float32\")\n         y = np.arange(24).reshape([3, 4, 1, 2]).astype(\"float32\")\n@@ -3448,6 +3490,10 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.diff(x, n=2, axis=0), np.diff(x, n=2, axis=0))\n         self.assertAllClose(knp.diff(x, n=2, axis=1), np.diff(x, n=2, axis=1))\n \n+        # Test n=0\n+        x = np.array([1, 2, 4, 7, 0])\n+        self.assertAllClose(knp.diff(x, n=0), np.diff(x, n=0))\n+\n     def test_dot(self):\n         x = np.arange(24).reshape([2, 3, 4]).astype(\"float32\")\n         y = np.arange(12).reshape([4, 3]).astype(\"float32\")\n@@ -3865,6 +3911,20 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.round(x), np.round(x))\n         self.assertAllClose(knp.Round()(x), np.round(x))\n \n+        # Test with decimal=1\n+        self.assertAllClose(knp.round(x, decimals=1), np.round(x, decimals=1))\n+        self.assertAllClose(knp.Round(decimals=1)(x), np.round(x, decimals=1))\n+\n+        # Test with integers\n+        x = np.array([[1, 2, 3], [3, 2, 1]], dtype=\"int32\")\n+        self.assertAllClose(knp.round(x, decimals=1), np.round(x, decimals=1))\n+        self.assertAllClose(knp.Round(decimals=1)(x), np.round(x, decimals=1))\n+\n+        # Test with integers and decimal < 0\n+        x = np.array([[123, 234, 345], [345, 234, 123]], dtype=\"int32\")\n+        self.assertAllClose(knp.round(x, decimals=-1), np.round(x, decimals=-1))\n+        self.assertAllClose(knp.Round(decimals=-1)(x), np.round(x, decimals=-1))\n+\n     def test_sign(self):\n         x = np.array([[1, -2, 3], [-3, 2, -1]])\n         self.assertAllClose(knp.sign(x), np.sign(x))\n@@ -4028,6 +4088,14 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.tile(x, [2, 3]), np.tile(x, [2, 3]))\n         self.assertAllClose(knp.Tile([2, 3])(x), np.tile(x, [2, 3]))\n \n+        # If repeats.ndim > x.ndim\n+        self.assertAllClose(knp.tile(x, [2, 3, 4]), np.tile(x, [2, 3, 4]))\n+        self.assertAllClose(knp.Tile([2, 3, 4])(x), np.tile(x, [2, 3, 4]))\n+\n+        # If repeats.ndim < x.ndim\n+        self.assertAllClose(knp.tile(x, [2]), np.tile(x, [2]))\n+        self.assertAllClose(knp.Tile([2])(x), np.tile(x, [2]))\n+\n     def test_trace(self):\n         x = np.arange(24).reshape([1, 2, 3, 4])\n         self.assertAllClose(knp.trace(x), np.trace(x))\n@@ -4186,7 +4254,22 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n \n         self.assertAllClose(knp.Eye()(3), np.eye(3))\n         self.assertAllClose(knp.Eye()(3, 4), np.eye(3, 4))\n-        self.assertAllClose(knp.Eye()(3, 4, 1), np.eye(3, 4, 1))\n+        self.assertAllClose(knp.Eye(k=1)(3, 4), np.eye(3, 4, k=1))\n+\n+        # Test k >= N\n+        self.assertAllClose(knp.Eye(k=3)(3), np.eye(3, k=3))\n+\n+        # Test k > 0 and N >= M\n+        self.assertAllClose(knp.Eye(k=1)(3), np.eye(3, k=1))\n+\n+        # Test k > 0 and N < M and N + k > M\n+        self.assertAllClose(knp.Eye(k=2)(3, 4), np.eye(3, 4, k=2))\n+\n+        # Test k < 0 and M >= N\n+        self.assertAllClose(knp.Eye(k=-1)(3), np.eye(3, k=-1))\n+\n+        # Test k < 0 and M < N and M - k > N\n+        self.assertAllClose(knp.Eye(k=-2)(4, 3), np.eye(4, 3, k=-2))\n \n     def test_arange(self):\n         self.assertAllClose(knp.arange(3), np.arange(3))\n@@ -4228,7 +4311,16 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n \n         self.assertAllClose(knp.Tri()(3), np.tri(3))\n         self.assertAllClose(knp.Tri()(3, 4), np.tri(3, 4))\n-        self.assertAllClose(knp.Tri()(3, 4, 1), np.tri(3, 4, 1))\n+        self.assertAllClose(knp.Tri(k=1)(3, 4), np.tri(3, 4, 1))\n+\n+        # Test k < 0\n+        self.assertAllClose(knp.Tri(k=-1)(3), np.tri(3, k=-1))\n+\n+        # Test -k-1 > N\n+        self.assertAllClose(knp.Tri(k=-5)(3), np.tri(3, k=-5))\n+\n+        # Test k > M\n+        self.assertAllClose(knp.Tri(k=4)(3), np.tri(3, k=4))\n \n \n def create_sparse_tensor(x, indices_from=None, start=0, delta=2):\n@@ -5074,6 +5166,18 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(standardize_dtype(knp.max(x).dtype), expected_dtype)\n         self.assertEqual(knp.Max().symbolic_call(x).dtype, expected_dtype)\n \n+        # Test with initial\n+        initial = 1\n+        expected_dtype = standardize_dtype(\n+            jnp.max(x_jax, initial=initial).dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.max(x, initial=initial).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            knp.Max(initial=initial).symbolic_call(x).dtype, expected_dtype\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_ones(self, dtype):\n         import jax.numpy as jnp\n@@ -6045,19 +6149,19 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n         self.assertEqual(\n-            standardize_dtype(knp.Eye().symbolic_call(3, dtype=dtype).dtype),\n+            standardize_dtype(knp.Eye(dtype=dtype).symbolic_call(3).dtype),\n             expected_dtype,\n         )\n \n         expected_dtype = standardize_dtype(jnp.eye(3, 4, 1, dtype=dtype).dtype)\n \n         self.assertEqual(\n-            standardize_dtype(knp.eye(3, 4, 1, dtype=dtype).dtype),\n+            standardize_dtype(knp.eye(3, 4, k=1, dtype=dtype).dtype),\n             expected_dtype,\n         )\n         self.assertEqual(\n             standardize_dtype(\n-                knp.Eye().symbolic_call(3, 4, 1, dtype=dtype).dtype\n+                knp.Eye(k=1, dtype=dtype).symbolic_call(3, 4).dtype\n             ),\n             expected_dtype,\n         )\n@@ -6743,6 +6847,18 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(standardize_dtype(knp.min(x).dtype), expected_dtype)\n         self.assertEqual(knp.Min().symbolic_call(x).dtype, expected_dtype)\n \n+        # Test with initial\n+        initial = 0\n+        expected_dtype = standardize_dtype(\n+            jnp.min(x_jax, initial=initial).dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.min(x, initial=initial).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            knp.Min(initial=initial).symbolic_call(x).dtype, expected_dtype\n+        )\n+\n     @parameterized.named_parameters(\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n     )\n@@ -7518,7 +7634,7 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n         self.assertEqual(\n-            standardize_dtype(knp.Tri().symbolic_call(3, dtype=dtype).dtype),\n+            standardize_dtype(knp.Tri(dtype=dtype).symbolic_call(3).dtype),\n             expected_dtype,\n         )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
