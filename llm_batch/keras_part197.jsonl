{"custom_id": "keras#b1f90950fa077acb1c8f98f01a99848b153f1c91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 343 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -60,7 +60,7 @@ def VGG16(include_top=True, weights='imagenet',\n             if `include_top` is False (otherwise the input shape\n             has to be `(224, 224, 3)` (with `channels_last` data format)\n             or `(3, 224, 224)` (with `channels_first` data format).\n-            It should have exactly 3 inputs channels,\n+            It should have exactly 3 input channels,\n             and width and height should be no smaller than 48.\n             E.g. `(200, 200, 3)` would be one valid value.\n         pooling: Optional pooling mode for feature extraction\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#78f26df8fb2b8aa5c6262aef44a494a8335a9c6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 13 | Methods Changed: 9 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 45 | Churn Cumulative: 6832 | Contributors (this commit): 63 | Commits (past 90d): 49 | Contributors (cumulative): 84 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -1642,6 +1642,7 @@ class Model(Container):\n                       max_queue_size=10,\n                       workers=1,\n                       use_multiprocessing=False,\n+                      shuffle=True,\n                       initial_epoch=0):\n         \"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\n \n@@ -1691,6 +1692,9 @@ class Model(Container):\n                 non picklable arguments to the generator\n                 as they can't be passed\n                 easily to children processes.\n+            shuffle: whether to shuffle the data at the beginning of each\n+                epoch. Only used with instances of `Sequence` (\n+                keras.utils.Sequence).\n             initial_epoch: epoch at which to start training\n                 (useful for resuming a previous training run)\n \n@@ -1792,7 +1796,8 @@ class Model(Container):\n         try:\n             if is_sequence:\n                 enqueuer = OrderedEnqueuer(generator,\n-                                           use_multiprocessing=use_multiprocessing)\n+                                           use_multiprocessing=use_multiprocessing,\n+                                           shuffle=shuffle)\n             else:\n                 enqueuer = GeneratorEnqueuer(generator,\n                                              use_multiprocessing=use_multiprocessing,\n\n@@ -351,6 +351,12 @@ class Sequence(object):\n         \"\"\"\n         raise NotImplementedError\n \n+    @abstractmethod\n+    def on_epoch_end(self):\n+        \"\"\"Method called at the end of every epoch.\n+        \"\"\"\n+        raise NotImplementedError\n+\n \n def get_index(ds, i):\n     \"\"\"Quick fix for Python2, otherwise, it cannot be pickled.\n@@ -434,15 +440,15 @@ class OrderedEnqueuer(SequenceEnqueuer):\n     # Arguments\n         sequence: A `keras.utils.data_utils.Sequence` object.\n         use_multiprocessing: use multiprocessing if True, otherwise threading\n-        scheduling: Sequential querying of datas if 'sequential', random otherwise.\n+        shuffle: whether to shuffle the data at the beginning of each epoch\n     \"\"\"\n \n     def __init__(self, sequence,\n                  use_multiprocessing=False,\n-                 scheduling='sequential'):\n+                 shuffle=False):\n         self.sequence = sequence\n         self.use_multiprocessing = use_multiprocessing\n-        self.scheduling = scheduling\n+        self.shuffle = shuffle\n         self.workers = 0\n         self.executor = None\n         self.queue = None\n@@ -474,7 +480,7 @@ class OrderedEnqueuer(SequenceEnqueuer):\n         \"\"\"Function to submit request to the executor and queue the `Future` objects.\"\"\"\n         sequence = list(range(len(self.sequence)))\n         while True:\n-            if self.scheduling is not 'sequential':\n+            if self.shuffle:\n                 random.shuffle(sequence)\n             for i in sequence:\n                 if self.stop_signal.is_set():\n@@ -482,6 +488,8 @@ class OrderedEnqueuer(SequenceEnqueuer):\n                 self.queue.put(\n                     self.executor.apply_async(get_index,\n                                               (self.sequence, i)), block=True)\n+            # Call the internal on epoch end.\n+            self.sequence.on_epoch_end()\n \n     def get(self):\n         \"\"\"Creates a generator to extract data from the queue.\n\n@@ -31,6 +31,9 @@ class RandomSequence(Sequence):\n             np.random.random((self.batch_size, 4)),\n             np.random.random((self.batch_size, 3))]\n \n+    def on_epoch_end(self):\n+        pass\n+\n \n @keras_test\n def test_check_array_lengths():\n\n@@ -121,6 +121,9 @@ class TestSequence(Sequence):\n     def __len__(self):\n         return 100\n \n+    def on_epoch_end(self):\n+        pass\n+\n \n class FaultSequence(Sequence):\n     def __getitem__(self, item):\n@@ -129,6 +132,9 @@ class FaultSequence(Sequence):\n     def __len__(self):\n         return 100\n \n+    def on_epoch_end(self):\n+        pass\n+\n \n @threadsafe_generator\n def create_generator_from_sequence_threads(ds):\n@@ -199,6 +205,19 @@ def test_ordered_enqueuer_threads():\n     enqueuer.stop()\n \n \n+def test_ordered_enqueuer_threads_not_ordered():\n+    enqueuer = OrderedEnqueuer(TestSequence([3, 200, 200, 3]),\n+                               use_multiprocessing=False,\n+                               shuffle=True)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    acc = []\n+    for i in range(100):\n+        acc.append(next(gen_output)[0, 0, 0, 0])\n+    assert acc != list(range(100)), \"Order was not keep in GeneratorEnqueuer with threads\"\n+    enqueuer.stop()\n+\n+\n def test_ordered_enqueuer_processes():\n     enqueuer = OrderedEnqueuer(TestSequence([3, 200, 200, 3]), use_multiprocessing=True)\n     enqueuer.start(3, 10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8603d76f03f0d75e120342336ddb32c904caff6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 7090 | Contributors (this commit): 45 | Commits (past 90d): 15 | Contributors (cumulative): 51 | DMM Complexity: 0.8\n\nDIFF:\n@@ -315,9 +315,10 @@ class Recurrent(Layer):\n                              str(len(initial_state)) +\n                              ' initial states.')\n         input_shape = K.int_shape(inputs)\n-        if self.unroll and input_shape[1] is None:\n+        timesteps = input_shape[1]\n+        if self.unroll and timesteps in [None, 1]:\n             raise ValueError('Cannot unroll a RNN if the '\n-                             'time dimension is undefined. \\n'\n+                             'time dimension is undefined or equal to 1. \\n'\n                              '- If using a Sequential model, '\n                              'specify the time dimension by passing '\n                              'an `input_shape` or `batch_input_shape` '\n@@ -336,7 +337,7 @@ class Recurrent(Layer):\n                                              mask=mask,\n                                              constants=constants,\n                                              unroll=self.unroll,\n-                                             input_length=input_shape[1])\n+                                             input_length=timesteps)\n         if self.stateful:\n             updates = []\n             for i in range(len(states)):\n\n@@ -323,5 +323,11 @@ def test_state_reuse(layer_class):\n     outputs = model.predict(inputs)\n \n \n+def test_unroll_true_throws_exception_with_one_timestep():\n+    model = Sequential()\n+    with pytest.raises(ValueError):\n+        model.add(recurrent.LSTM(units=5, batch_input_shape=(1, 1, 5), unroll=True))\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ec39c35975906801d49ceab931f6b35deab8f438", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2300 | Contributors (this commit): 32 | Commits (past 90d): 11 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -643,7 +643,7 @@ class ImageDataGenerator(object):\n         if x.ndim != 4:\n             raise ValueError('Input to `.fit()` should have rank 4. '\n                              'Got array with shape: ' + str(x.shape))\n-        if x.shape[self.channel_axis] not in {3, 4}:\n+        if x.shape[self.channel_axis] not in {1, 3, 4}:\n             warnings.warn(\n                 'Expected input to be images (as Numpy array) '\n                 'following the data format convention \"' + self.data_format + '\" '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aac5b539ac92fb6c91851e67441bbb9bd4f3cf59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 8234 | Contributors (this commit): 85 | Commits (past 90d): 24 | Contributors (cumulative): 85 | DMM Complexity: 0.0\n\nDIFF:\n@@ -322,7 +322,7 @@ def variable(value, dtype=None, name=None):\n     if isinstance(value, np.ndarray):\n         v._keras_shape = value.shape\n     elif hasattr(value, 'get_shape'):\n-        v._keras_shape = tuple(map(int, value.get_shape()))\n+        v._keras_shape = int_shape(value)\n     v._uses_learning_phase = False\n     return v\n \n@@ -489,9 +489,8 @@ def int_shape(x):\n     \"\"\"\n     if hasattr(x, '_keras_shape'):\n         return x._keras_shape\n-    shape = x.get_shape()\n     try:\n-        return tuple([i.__int__() for i in shape])\n+        return tuple(x.get_shape().as_list())\n     except ValueError:\n         return None\n \n@@ -597,7 +596,6 @@ def zeros(shape, dtype=None, name=None):\n     \"\"\"\n     if dtype is None:\n         dtype = floatx()\n-    shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     return variable(tf.constant_initializer(0., dtype=tf_dtype)(shape),\n                     dtype, name)\n@@ -626,7 +624,6 @@ def ones(shape, dtype=None, name=None):\n     \"\"\"\n     if dtype is None:\n         dtype = floatx()\n-    shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     return variable(tf.constant_initializer(1., dtype=tf_dtype)(shape),\n                     dtype, name)\n@@ -747,7 +744,6 @@ def random_uniform_variable(shape, low, high, dtype=None,\n     \"\"\"\n     if dtype is None:\n         dtype = floatx()\n-    shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     if seed is None:\n         # ensure that randomness is conditioned by the Numpy RNG\n@@ -785,7 +781,6 @@ def random_normal_variable(shape, mean, scale, dtype=None,\n     \"\"\"\n     if dtype is None:\n         dtype = floatx()\n-    shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     if seed is None:\n         # ensure that randomness is conditioned by the Numpy RNG\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0bc856f90a746ce3c8078f5ec4fb5156c88d8fdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/2 | Churn Δ: 39 | Churn Cumulative: 17738 | Contributors (this commit): 106 | Commits (past 90d): 62 | Contributors (cumulative): 177 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1886,7 +1886,10 @@ def batch_set_value(tuples):\n \n \n def stop_gradient(variables):\n-    return C.stop_gradient(C.combine(variables))\n+    if isinstance(variables, (list, tuple)):\n+        return map(C.stop_gradient, variables)\n+    else:\n+        return C.stop_gradient(variables)\n \n \n def switch(condition, then_expression, else_expression):\n\n@@ -2304,11 +2304,16 @@ def stop_gradient(variables):\n     \"\"\"Returns `variables` but with zero gradient w.r.t. every other variable.\n \n     # Arguments\n-        variables: List of variables.\n+        variables: tensor or list of tensors to consider constant with respect\n+            to any other variable.\n \n     # Returns\n-        The same list of variables.\n+        A single tensor or a list of tensors (depending on the passed argument)\n+            that has constant gradient with respect to any other variable.\n     \"\"\"\n+    if isinstance(variables, (list, tuple)):\n+        return map(tf.stop_gradient, variables)\n+    else:\n         return tf.stop_gradient(variables)\n \n \n\n@@ -1211,9 +1211,19 @@ def gradients(loss, variables):\n \n \n def stop_gradient(variables):\n-    \"\"\"Returns `variables` but with zero gradient with respect to every other\n-    variables.\n+    \"\"\"Returns `variables` but with zero gradient w.r.t. every other variable.\n+\n+    # Arguments\n+        variables: tensor or list of tensors to consider constant with respect\n+            to any other variable.\n+\n+    # Returns\n+        A single tensor or a list of tensors (depending on the passed argument)\n+            that has constant gradient with respect to any other variable.\n     \"\"\"\n+    if isinstance(variables, (list, tuple)):\n+        return map(theano.gradient.disconnected_grad, variables)\n+    else:\n         return theano.gradient.disconnected_grad(variables)\n \n \n\n@@ -491,6 +491,17 @@ class TestBackend(object):\n             assert_allclose(zero_list[i], z_list[i], atol=1e-05)\n             assert_allclose(zero_list[i + 1], zero_list[i + 1], atol=1e-05)\n \n+    def test_stop_gradient(self):\n+        # This test checks the consistency of the stop_gradient backend API.\n+        # It doesn't check the functionality (which is checked at the\n+        # test_gradient test).\n+        val = np.random.random((4, 2))\n+        for k in BACKENDS:\n+            a = k.variable(val)\n+            b = k.square(a)\n+            c, d = k.stop_gradient([a, b])\n+            e = k.stop_gradient(b)\n+\n     # cntk currently not support function in this way, so can't test as this\n     def test_function(self):\n         test_backend = [KTH, KTF]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#43789fa5fdba49dfd3ab8c16ff8478a8e5764c3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1930 | Contributors (this commit): 22 | Commits (past 90d): 4 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -552,8 +552,8 @@ class Nadam(Optimizer):\n         t = self.iterations + 1\n \n         # Due to the recommendations in [2], i.e. warming momentum schedule\n-        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(0.96, t * self.schedule_decay)))\n-        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(0.96, (t + 1) * self.schedule_decay)))\n+        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n+        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n         m_schedule_new = self.m_schedule * momentum_cache_t\n         m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n         self.updates.append((self.m_schedule, m_schedule_new))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c5ff4c967e15bd1402afd76494126ea40e67673b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 517 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -19,7 +19,6 @@ from ..layers import Activation\n from ..layers import Flatten\n from ..layers import Conv2D\n from ..layers import MaxPooling2D\n-from ..layers import ZeroPadding2D\n from ..layers import AveragePooling2D\n from ..layers import GlobalAveragePooling2D\n from ..layers import GlobalMaxPooling2D\n@@ -203,8 +202,8 @@ def ResNet50(include_top=True, weights='imagenet',\n     else:\n         bn_axis = 1\n \n-    x = ZeroPadding2D((3, 3))(img_input)\n-    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n+    x = Conv2D(\n+        64, (7, 7), strides=(2, 2), padding='same', name='conv1')(img_input)\n     x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n     x = Activation('relu')(x)\n     x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cafa2860a311171fbebde938d14d11bcbb2a5755", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 58 | Churn Cumulative: 1247 | Contributors (this commit): 11 | Commits (past 90d): 9 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -129,16 +129,24 @@ class ConvRecurrent2D(Recurrent):\n                                              dilation=self.dilation_rate[1])\n         if self.return_sequences:\n             if self.data_format == 'channels_first':\n-                return (input_shape[0], input_shape[1],\n+                output_shape = (input_shape[0], input_shape[1],\n                                 self.filters, rows, cols)\n             elif self.data_format == 'channels_last':\n-                return (input_shape[0], input_shape[1],\n+                output_shape = (input_shape[0], input_shape[1],\n                                 rows, cols, self.filters)\n         else:\n             if self.data_format == 'channels_first':\n-                return (input_shape[0], self.filters, rows, cols)\n+                output_shape = (input_shape[0], self.filters, rows, cols)\n             elif self.data_format == 'channels_last':\n-                return (input_shape[0], rows, cols, self.filters)\n+                output_shape = (input_shape[0], rows, cols, self.filters)\n+\n+        if self.return_state:\n+            if self.data_format == 'channels_first':\n+                output_shape = [output_shape] + [(input_shape[0], self.filters, rows, cols) for _ in range(2)]\n+            elif self.data_format == 'channels_last':\n+                output_shape = [output_shape] + [(input_shape[0], rows, cols, self.filters) for _ in range(2)]\n+\n+        return output_shape\n \n     def get_config(self):\n         config = {'filters': self.filters,\n@@ -429,24 +437,25 @@ class ConvLSTM2D(ConvRecurrent2D):\n                              'input_shape must be provided '\n                              '(including batch size). '\n                              'Got input shape: ' + str(input_shape))\n-\n         if self.return_sequences:\n-            out_row, out_col, out_filter = output_shape[2:]\n+            if self.return_state:\n+                output_shape = output_shape[1]\n             else:\n-            out_row, out_col, out_filter = output_shape[1:]\n+                output_shape = (input_shape[0],) + output_shape[2:]\n+        else:\n+            if self.return_state:\n+                output_shape = output_shape[1]\n+            else:\n+                output_shape = (input_shape[0],) + output_shape[1:]\n \n         if hasattr(self, 'states'):\n             K.set_value(self.states[0],\n-                        np.zeros((input_shape[0],\n-                                  out_row, out_col, out_filter)))\n+                        np.zeros(output_shape))\n             K.set_value(self.states[1],\n-                        np.zeros((input_shape[0],\n-                                  out_row, out_col, out_filter)))\n+                        np.zeros(output_shape))\n         else:\n-            self.states = [K.zeros((input_shape[0],\n-                                    out_row, out_col, out_filter)),\n-                           K.zeros((input_shape[0],\n-                                    out_row, out_col, out_filter))]\n+            self.states = [K.zeros(output_shape),\n+                           K.zeros(output_shape)]\n \n     def get_constants(self, inputs, training=None):\n         constants = []\n\n@@ -30,6 +30,25 @@ def test_convolutional_recurrent():\n                                     input_channel)\n \n         for return_sequences in [True, False]:\n+\n+            # test for return state:\n+            input = Input(batch_shape=inputs.shape)\n+            kwargs = {'data_format': data_format,\n+                      'return_sequences': return_sequences,\n+                      'return_state': True,\n+                      'stateful': True,\n+                      'filters': filters,\n+                      'kernel_size': (num_row, num_col),\n+                      'padding': 'valid'}\n+            layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n+            layer.build(inputs.shape)\n+            outputs = layer(input)\n+            output, states = outputs[0], outputs[1:]\n+            assert len(states) == 2\n+            model = Model(input, states[0])\n+            state = model.predict(inputs)\n+            np.testing.assert_allclose(K.eval(layer.states[0]), state, atol=1e-4)\n+\n             # test for output shape:\n             output = layer_test(convolutional_recurrent.ConvLSTM2D,\n                                 kwargs={'data_format': data_format,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3305cd39f8daa7b0a76aeb078ce50bf871688031", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 174 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -32,7 +32,7 @@ def test_convolutional_recurrent():\n         for return_sequences in [True, False]:\n \n             # test for return state:\n-            input = Input(batch_shape=inputs.shape)\n+            inputs = Input(batch_shape=inputs.shape)\n             kwargs = {'data_format': data_format,\n                       'return_sequences': return_sequences,\n                       'return_state': True,\n@@ -42,12 +42,13 @@ def test_convolutional_recurrent():\n                       'padding': 'valid'}\n             layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n             layer.build(inputs.shape)\n-            outputs = layer(input)\n+            outputs = layer(inputs)\n             output, states = outputs[0], outputs[1:]\n             assert len(states) == 2\n-            model = Model(input, states[0])\n+            model = Model(inputs, states[0])\n             state = model.predict(inputs)\n-            np.testing.assert_allclose(K.eval(layer.states[0]), state, atol=1e-4)\n+            np.testing.assert_allclose(\n+                K.eval(layer.states[0]), state, atol=1e-4)\n \n             # test for output shape:\n             output = layer_test(convolutional_recurrent.ConvLSTM2D,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#84ceb94055b831c486dbf4955fdf1ba0f63320d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 180 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ def test_convolutional_recurrent():\n         for return_sequences in [True, False]:\n \n             # test for return state:\n-            inputs = Input(batch_shape=inputs.shape)\n+            x = Input(batch_shape=inputs.shape)\n             kwargs = {'data_format': data_format,\n                       'return_sequences': return_sequences,\n                       'return_state': True,\n@@ -42,10 +42,10 @@ def test_convolutional_recurrent():\n                       'padding': 'valid'}\n             layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n             layer.build(inputs.shape)\n-            outputs = layer(inputs)\n+            outputs = layer(x)\n             output, states = outputs[0], outputs[1:]\n             assert len(states) == 2\n-            model = Model(inputs, states[0])\n+            model = Model(x, states[0])\n             state = model.predict(inputs)\n             np.testing.assert_allclose(\n                 K.eval(layer.states[0]), state, atol=1e-4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#64321deb7e8b7eabc12da0a5ca48da78e132dd57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9999 | Contributors (this commit): 48 | Commits (past 90d): 7 | Contributors (cumulative): 48 | DMM Complexity: None\n\nDIFF:\n@@ -1094,7 +1094,7 @@ class SeparableConv2D(Conv2D):\n             the depthwise kernel matrix\n             (see [regularizer](../regularizers.md)).\n         pointwise_regularizer: Regularizer function applied to\n-            the depthwise kernel matrix\n+            the pointwise kernel matrix\n             (see [regularizer](../regularizers.md)).\n         bias_regularizer: Regularizer function applied to the bias vector\n             (see [regularizer](../regularizers.md)).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cd317bb9b6bc5acb623b8f7e01e9d9bf19c326a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 46 | Churn Cumulative: 533 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -15,10 +15,17 @@ from keras.models import Sequential\n from keras.layers import Dense, Dropout, Activation, Flatten\n from keras.layers import Conv2D, MaxPooling2D\n \n+import os\n+import pickle\n+import numpy as np\n+\n batch_size = 32\n num_classes = 10\n epochs = 200\n data_augmentation = True\n+num_predictions = 20\n+save_dir = os.path.join(os.getcwd(), 'saved_models')\n+model_name = 'keras_cifar10_trained_model.h5'\n \n # The data, shuffled and split between train and test sets:\n (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n@@ -99,3 +106,42 @@ else:\n                         steps_per_epoch=x_train.shape[0] // batch_size,\n                         epochs=epochs,\n                         validation_data=(x_test, y_test))\n+\n+# Save model and weights\n+if not os.path.isdir(save_dir):\n+    os.makedirs(save_dir)\n+model_path = os.path.join(save_dir, model_name)\n+model.save(model_path)\n+print('Saved trained model at %s ' % model_path)\n+\n+# Load label names to use in prediction results\n+label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n+\n+\n+keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n+datadir_base = os.path.expanduser(keras_dir)\n+if not os.access(datadir_base, os.W_OK):\n+    datadir_base = os.path.join('/tmp', '.keras')\n+label_list_path = os.path.join(datadir_base, label_list_path)\n+\n+with open(label_list_path, mode='rb') as f:\n+    labels = pickle.load(f)\n+\n+# Evaluate model with test data set and share sample prediction results\n+evaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n+                                      batch_size=batch_size),\n+                                      steps=x_test.shape[0] // batch_size)\n+\n+print('Model Accuracy = %.2f' % (evaluation[1]))\n+\n+predict_gen = model.predict_generator(datagen.flow(x_test, y_test,\n+                                      batch_size=batch_size),\n+                                      steps=x_test.shape[0] // batch_size)\n+\n+for predict_index, predicted_y in enumerate(predict_gen):\n+    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n+    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n+    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n+                                                          predicted_label))\n+    if predict_index == num_predictions:\n+        break\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f98b1da727cea5a308e55c8bbf332ba71e1b865c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 222 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ setup(name='Keras',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n           'h5py': ['h5py'],\n-          'visualize': ['pydot-ng'],\n+          'visualize': ['pydot>=1.2.0'],\n           'tests': ['pytest',\n                     'pytest-pep8',\n                     'pytest-xdist',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d03a41b4262e6aab8ad56cd8395808cacf5921e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 9278 | Contributors (this commit): 75 | Commits (past 90d): 14 | Contributors (cumulative): 75 | DMM Complexity: 0.0\n\nDIFF:\n@@ -98,7 +98,7 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n         if not proceed:\n             return\n \n-    f = h5py.File(filepath, 'w')\n+    with h5py.File(filepath, mode='w') as f:\n         f.attrs['keras_version'] = str(keras_version).encode('utf8')\n         f.attrs['backend'] = K.backend().encode('utf8')\n         f.attrs['model_config'] = json.dumps({\n@@ -122,7 +122,8 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n                     'after instantiation. '\n                     'As a result, we cannot save the optimizer '\n                     'as part of the model save file.'\n-                'You will have to compile your model again after loading it. '\n+                    'You will have to compile your model again '\n+                    'after loading it. '\n                     'Prefer using a Keras optimizer instead '\n                     '(see keras.io/optimizers).')\n             else:\n@@ -143,8 +144,10 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n                     optimizer_weights_group = f.create_group('optimizer_weights')\n                     weight_values = K.batch_get_value(symbolic_weights)\n                     weight_names = []\n-                for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n-                    # Default values of symbolic_weights is /variable for theano and cntk\n+                    for i, (w, val) in enumerate(zip(symbolic_weights,\n+                                                     weight_values)):\n+                        # Default values of symbolic_weights is /variable\n+                        # for theano and cntk\n                         if K.backend() == 'theano' or K.backend() == 'cntk':\n                             if hasattr(w, 'name') and w.name != \"/variable\":\n                                 name = str(w.name)\n@@ -168,7 +171,6 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n                         else:\n                             param_dset[:] = val\n         f.flush()\n-    f.close()\n \n \n def load_model(filepath, custom_objects=None, compile=True):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46b691e302e896053c4fd167ca4ec688d957b245", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 36 | Files Changed: 1 | Hunks: 15 | Methods Changed: 15 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 36 | Churn Cumulative: 8279 | Contributors (this commit): 85 | Commits (past 90d): 25 | Contributors (cumulative): 85 | DMM Complexity: 0.4230769230769231\n\nDIFF:\n@@ -1111,27 +1111,6 @@ def gather(reference, indices):\n \n # ELEMENT-WISE OPERATIONS\n \n-def _normalize_axis(axis, ndim):\n-    \"\"\"Converts negative axes to positive values.\n-\n-    # Arguments\n-        axis: Integer axis (possibly negative).\n-        ndim: Rank of the tensor considered.\n-\n-    # Returns\n-        Positive integer axis.\n-    \"\"\"\n-    if isinstance(axis, tuple):\n-        axis = list(axis)\n-    if isinstance(axis, list):\n-        for i, a in enumerate(axis):\n-            if a is not None and a < 0:\n-                axis[i] = a % ndim\n-    else:\n-        if axis is not None and axis < 0:\n-            axis %= ndim\n-    return axis\n-\n \n def max(x, axis=None, keepdims=False):\n     \"\"\"Maximum value in a tensor.\n@@ -1147,7 +1126,6 @@ def max(x, axis=None, keepdims=False):\n     # Returns\n         A tensor with maximum values of `x`.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_max(x, axis=axis, keep_dims=keepdims)\n \n \n@@ -1165,7 +1143,6 @@ def min(x, axis=None, keepdims=False):\n     # Returns\n         A tensor with miminum values of `x`.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_min(x, axis=axis, keep_dims=keepdims)\n \n \n@@ -1183,7 +1160,6 @@ def sum(x, axis=None, keepdims=False):\n     # Returns\n         A tensor with sum of `x`.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_sum(x, axis=axis, keep_dims=keepdims)\n \n \n@@ -1201,7 +1177,6 @@ def prod(x, axis=None, keepdims=False):\n     # Returns\n         A tensor with the product of elements of `x`.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_prod(x, axis=axis, keep_dims=keepdims)\n \n \n@@ -1215,7 +1190,6 @@ def cumsum(x, axis=0):\n     # Returns\n         A tensor of the cumulative sum of values of `x` along `axis`.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     return tf.cumsum(x, axis=axis)\n \n \n@@ -1229,7 +1203,6 @@ def cumprod(x, axis=0):\n     # Returns\n         A tensor of the cumulative product of values of `x` along `axis`.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     return tf.cumprod(x, axis=axis)\n \n \n@@ -1247,7 +1220,6 @@ def var(x, axis=None, keepdims=False):\n     # Returns\n         A tensor with the variance of elements of `x`.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n         x = tf.cast(x, floatx())\n     m = tf.reduce_mean(x, axis=axis, keep_dims=True)\n@@ -1288,7 +1260,6 @@ def mean(x, axis=None, keepdims=False):\n     # Returns\n         A tensor with the mean of elements of `x`.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n         x = tf.cast(x, floatx())\n     return tf.reduce_mean(x, axis=axis, keep_dims=keepdims)\n@@ -1305,7 +1276,6 @@ def any(x, axis=None, keepdims=False):\n     # Returns\n         A uint8 tensor (0s and 1s).\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     x = tf.cast(x, tf.bool)\n     return tf.reduce_any(x, axis=axis, keep_dims=keepdims)\n \n@@ -1321,7 +1291,6 @@ def all(x, axis=None, keepdims=False):\n     # Returns\n         A uint8 tensor (0s and 1s).\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     x = tf.cast(x, tf.bool)\n     return tf.reduce_all(x, axis=axis, keep_dims=keepdims)\n \n@@ -1336,7 +1305,6 @@ def argmax(x, axis=-1):\n     # Returns\n         A tensor.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     return tf.argmax(x, axis)\n \n \n@@ -1350,7 +1318,6 @@ def argmin(x, axis=-1):\n     # Returns\n         A tensor.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     return tf.argmin(x, axis)\n \n \n@@ -1435,7 +1402,6 @@ def logsumexp(x, axis=None, keepdims=False):\n     # Returns\n         The reduced tensor.\n     \"\"\"\n-    axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_logsumexp(x, axis=axis, keep_dims=keepdims)\n \n \n@@ -2888,8 +2854,6 @@ def l2_normalize(x, axis):\n     # Returns\n         A tensor.\n     \"\"\"\n-    if axis < 0:\n-        axis %= len(x.get_shape())\n     return tf.nn.l2_normalize(x, dim=axis)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ddd8f470aeac84049b9d1d05b6d9a9b568f9b5f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 15949 | Contributors (this commit): 106 | Commits (past 90d): 55 | Contributors (cumulative): 167 | DMM Complexity: None\n\nDIFF:\n@@ -1284,7 +1284,7 @@ def has_seq_axis(x):\n     return hasattr(x, 'dynamic_axes') and len(x.dynamic_axes) > 1\n \n \n-def l2_normalize(x, axis):\n+def l2_normalize(x, axis=None):\n     axis = [axis]\n     axis = _normalize_axis(axis, x)\n     norm = C.sqrt(C.reduce_sum(C.square(x), axis=axis[0]))\n\n@@ -2844,7 +2844,7 @@ def dropout(x, level, noise_shape=None, seed=None):\n     return tf.nn.dropout(x * 1., retain_prob, noise_shape, seed=seed)\n \n \n-def l2_normalize(x, axis):\n+def l2_normalize(x, axis=None):\n     \"\"\"Normalizes a tensor wrt the L2 norm alongside the specified axis.\n \n     # Arguments\n\n@@ -1583,9 +1583,9 @@ def dropout(x, level, noise_shape=None, seed=None):\n     return x\n \n \n-def l2_normalize(x, axis, epsilon=1e-12):\n+def l2_normalize(x, axis=None):\n     square_sum = T.sum(T.square(x), axis=axis, keepdims=True)\n-    norm = T.sqrt(T.maximum(square_sum, epsilon))\n+    norm = T.sqrt(T.maximum(square_sum, epsilon=_EPSILON))\n     return x / norm\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cbecc6a2fba8729b34c3e852ed4f17aa7bc2ecdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5264 | Contributors (this commit): 74 | Commits (past 90d): 19 | Contributors (cumulative): 74 | DMM Complexity: None\n\nDIFF:\n@@ -1585,7 +1585,7 @@ def dropout(x, level, noise_shape=None, seed=None):\n \n def l2_normalize(x, axis=None):\n     square_sum = T.sum(T.square(x), axis=axis, keepdims=True)\n-    norm = T.sqrt(T.maximum(square_sum, epsilon=_EPSILON))\n+    norm = T.sqrt(T.maximum(square_sum, _EPSILON))\n     return x / norm\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b95fcf7f52aca8ad0b1afb3cfc64c8eed534fafe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 337 | Lines Deleted: 495 | Files Changed: 2 | Hunks: 110 | Methods Changed: 50 | Complexity Δ (Sum/Max): -8/1 | Churn Δ: 832 | Churn Cumulative: 2665 | Contributors (this commit): 11 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: 0.42201834862385323\n\nDIFF:\n@@ -10,9 +10,6 @@ from keras.utils.conv_utils import convert_kernel\n from keras.backend import cntk_backend as KC\n \n BACKENDS = [KTH, KTF, KC]\n-# for special test cases of CNTK which treat dynamic axis\n-KCSD = 'CNTK_single_tensor_dynamicity'\n-KCTD = 'CNTK_two_tensor_dynamicity'\n \n \n def check_dtype(var, dtype):\n@@ -40,135 +37,157 @@ def cntk_func_two_tensor(function_name, x_shape, y, **kwargs):\n         return KC.function([xc, yc], [output_cntk])\n \n \n-def check_single_tensor_operation(function_name, input_shape, backend_list, **kwargs):\n-    val = np.random.random(input_shape) - 0.5\n+def parse_shape_or_val(shape_or_val):\n+    if isinstance(shape_or_val, np.ndarray):\n+        return shape_or_val.shape, shape_or_val\n+    else:\n+        return shape_or_val, np.random.random(shape_or_val) - 0.5\n+\n+\n+def assert_list_pairwise(z_list, shape=True, allclose=True, itself=False, atol=1e-05):\n+    for (z1, z2) in zip(z_list[1:], z_list[:-1]):\n+        if shape:\n+            assert z1.shape == z2.shape\n+        if allclose:\n+            assert_allclose(z1, z2, atol=atol)\n+        if itself:\n+            assert z1 == z2\n+\n+\n+def assert_list_with_ref(z_list, ref):\n+    for z in z_list:\n+        assert z.shape == ref.shape\n+        assert_allclose(z, ref, atol=1e-05)\n+\n+\n+def assert_list_keras_shape(z_list):\n+    for z in z_list:\n+        if hasattr(z, '_keras_shape'):\n+            assert z._keras_shape == z.shape\n+\n+\n+def check_single_tensor_operation(function_name, x_shape_or_val, backend_list, **kwargs):\n+    shape_or_val = kwargs.pop('shape_or_val', True)\n+    assert_value_equality = kwargs.pop('assert_value_equality', True)\n+    assert_value_with_ref = kwargs.pop('assert_value_with_ref', None)\n+    cntk_dynamicity = kwargs.pop('cntk_dynamicity', False)\n+\n+    if shape_or_val:\n+        x_shape, x_val = parse_shape_or_val(x_shape_or_val)\n \n     z_list = []\n     for k in backend_list:\n-        if k == KCSD:\n-            z = cntk_func_single_tensor(function_name, input_shape,\n-                                        **kwargs)([val])[0]\n+        if shape_or_val:\n+            if (k == KC) & (cntk_dynamicity):\n+                z = cntk_func_single_tensor(function_name, x_shape,\n+                                            **kwargs)([x_val])[0]\n             else:\n-            z = k.eval(getattr(k, function_name)(k.variable(val), **kwargs))\n-        if hasattr(z, '_keras_shape'):\n-            assert z._keras_shape == z.shape\n+                z = k.eval(getattr(k, function_name)(k.variable(x_val), **kwargs))\n+        else:\n+            z = k.eval(getattr(k, function_name)(x_shape_or_val, **kwargs))\n         z_list += [z]\n \n-    for (z1, z2) in zip(z_list[1:], z_list[:-1]):\n-        assert z1.shape == z2.shape\n-        assert_allclose(z1, z2, atol=1e-05)\n+    if assert_value_with_ref is not None:\n+        assert_list_with_ref(z_list, assert_value_with_ref)\n+    else:\n+        assert_list_pairwise(z_list, allclose=assert_value_equality)\n+    assert_list_keras_shape(z_list)\n \n \n-def check_two_tensor_operation(function_name, x_input_shape,\n-                               y_input_shape, backend_list, **kwargs):\n-    xval = np.random.random(x_input_shape) - 0.5\n-    yval = np.random.random(y_input_shape) - 0.5\n+def check_two_tensor_operation(function_name, x_shape_or_val,\n+                               y_shape_or_val, backend_list, **kwargs):\n+    shape_or_val = kwargs.pop('shape_or_val', True)\n+    concat_args = kwargs.pop('concat_args', False)\n+    cntk_dynamicity = kwargs.pop('cntk_dynamicity', False)\n+    cntk_two_dynamicity = kwargs.pop('cntk_two_dynamicity', False)\n+\n+    if shape_or_val:\n+        x_shape, x_val = parse_shape_or_val(x_shape_or_val)\n+        y_shape, y_val = parse_shape_or_val(y_shape_or_val)\n \n     z_list = []\n     for k in backend_list:\n-        if k == KCSD:\n-            z = cntk_func_two_tensor(function_name, x_input_shape,\n-                                     y=yval, **kwargs)([xval])[0]\n-        elif k == KCTD:\n-            z = cntk_func_two_tensor(function_name, x_input_shape,\n-                                     y=y_input_shape, **kwargs)([xval, yval])[0]\n+        if shape_or_val:\n+            if (k == KC) & (cntk_dynamicity):\n+                z = cntk_func_two_tensor(function_name, x_shape,\n+                                         y=y_val, **kwargs)([x_val])[0]\n+            elif (k == KC) & (cntk_two_dynamicity):\n+                z = cntk_func_two_tensor(function_name, x_shape,\n+                                         y=y_shape, **kwargs)([x_val, y_val])[0]\n+            elif (k == KTH) & (function_name[:4] == 'conv'):\n+                z = k.eval(getattr(k, function_name)(\n+                    k.variable(x_val), k.variable(convert_kernel(y_val)), **kwargs))\n+            elif concat_args:\n+                z = k.eval(getattr(k, function_name)(\n+                    [k.variable(x_val), k.variable(y_val)], **kwargs))\n             else:\n-            z = k.eval(getattr(k, function_name)(k.variable(xval), k.variable(yval), **kwargs))\n-        if hasattr(z, '_keras_shape'):\n-            assert z._keras_shape == z.shape\n+                z = k.eval(getattr(k, function_name)(\n+                    k.variable(x_val), k.variable(y_val), **kwargs))\n+        else:\n+            z = k.eval(getattr(k, function_name)(\n+                x_shape_or_val, y_shape_or_val, **kwargs))\n         z_list += [z]\n \n-    for (z1, z2) in zip(z_list[1:], z_list[:-1]):\n-        assert z1.shape == z2.shape\n-        assert_allclose(z1, z2, atol=1e-05)\n-\n-\n-def check_cross_entropy_with_valid_probability_distribution():\n-    xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841],\n-                       [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n-    xtf = KTF.variable(xval)\n-    xth = KTH.variable(xval)\n-    xc = KC.placeholder((4, 2))\n-\n-    yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016],\n-                       [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n-    ytf = KTF.variable(yval)\n-    yth = KTH.variable(yval)\n-    yc = KC.placeholder((4, 2))\n-\n-    ztf = KTF.eval(KTF.categorical_crossentropy(xtf, ytf, from_logits=True))\n-    zth = KTH.eval(KTH.categorical_crossentropy(xth, yth, from_logits=True))\n-\n-    func_cntk = KC.function([xc, yc],\n-                            [KC.categorical_crossentropy(xc, yc, from_logits=True), ])\n-    zc = func_cntk([xval, yval])\n-    # Keras function return a list, take the first output\n-    assert len(zc) == 1\n-    zc = zc[0]\n-\n-    assert zth.shape == ztf.shape\n-    assert_allclose(zth, ztf, atol=1e-05)\n-\n-    assert ztf.shape == zc.shape\n-    assert_allclose(ztf, zc, atol=1e-05)\n+    assert_list_pairwise(z_list)\n+    assert_list_keras_shape(z_list)\n \n \n def check_composed_tensor_operations(first_function_name, first_function_args,\n                                      second_function_name, second_function_args,\n                                      input_shape, backend_list):\n     val = np.random.random(input_shape) - 0.5\n-    x_list = [k.variable(val) for k in backend_list]\n-    y_list = []\n-    for x, k in zip(x_list, backend_list):\n-        y_list.append(getattr(k, first_function_name)(x, **first_function_args))\n \n     z_list = []\n-    for y, k in zip(y_list, backend_list):\n-        z_list.append(k.eval(getattr(k, second_function_name)(y, **second_function_args)))\n+    for k in backend_list:\n+        x = k.variable(val)\n+        y = getattr(k, first_function_name)(x, **first_function_args)\n+        z = k.eval(getattr(k, second_function_name)(y, **second_function_args))\n+        z_list += [z]\n \n-    for i in range(len(z_list) - 1):\n-        assert z_list[i].shape == z_list[i + 1].shape\n-        assert_allclose(z_list[i], z_list[i + 1], atol=1e-05)\n+    assert_list_pairwise(z_list)\n \n \n class TestBackend(object):\n \n     def test_is_keras_tensor(self):\n-        for K in [KTH, KTF]:\n+        for k in [KTH, KTF]:\n             np_var = np.array([1, 2])\n             try:\n-                K.is_keras_tensor(np_var)\n+                k.is_keras_tensor(np_var)\n                 assert True is False\n             except ValueError:\n                 # This is the expected behavior\n                 continue\n \n-            keras_var = K.variable(np_var)\n-            assert K.is_keras_tensor(keras_var) is True\n-            keras_placeholder = K.placeholder(shape=(2, 4, 5))\n-            assert K.is_keras_tensor(keras_placeholder) is True\n+            keras_var = k.variable(np_var)\n+            assert k.is_keras_tensor(keras_var) is True\n+            keras_placeholder = k.placeholder(shape=(2, 4, 5))\n+            assert k.is_keras_tensor(keras_placeholder) is True\n \n     def test_set_learning_phase(self):\n         # not supported learning_phase\n-        for backend in (KTF, KTH):\n+        for k in BACKENDS:\n             with pytest.raises(ValueError):\n-                backend.set_learning_phase(2)\n+                k.set_learning_phase(2)\n \n     def test_eye(self):\n-        zth = KTH.eval(KTH.eye(3))\n-        ztf = KTF.eval(KTF.eye(3))\n-        assert zth.shape == ztf.shape\n-        assert_allclose(zth, ztf, atol=1e-05)\n+        z_list = [k.eval(k.eye(3)) for k in BACKENDS]\n+        assert_list_pairwise(z_list)\n \n     def test_linear_operations(self):\n         check_two_tensor_operation('dot', (4, 2), (2, 4), BACKENDS)\n         check_two_tensor_operation('dot', (4, 2), (5, 2, 3), BACKENDS)\n \n-        check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3), [KTF, KTH, KCTD], axes=(2, 2))\n-        check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3), [KTF, KTH, KCTD], axes=(2, 1))\n-        check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3), [KTF, KTH, KCTD], axes=(1, 1))\n-        check_two_tensor_operation('batch_dot', (32, 20), (32, 20), [KTF, KTH, KCTD], axes=1)\n-        check_two_tensor_operation('batch_dot', (32, 20), (32, 20), [KTF, KTH, KCTD], axes=(1, 1))\n+        check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n+                                   BACKENDS, cntk_two_dynamicity=True, axes=(2, 2))\n+        check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3),\n+                                   BACKENDS, cntk_two_dynamicity=True, axes=(2, 1))\n+        check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3),\n+                                   BACKENDS, cntk_two_dynamicity=True, axes=(1, 1))\n+        check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n+                                   BACKENDS, cntk_two_dynamicity=True, axes=1)\n+        check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n+                                   BACKENDS, cntk_two_dynamicity=True, axes=(1, 1))\n \n         check_single_tensor_operation('transpose', (4, 2), BACKENDS)\n         # cntk doesn't support reverse yet\n@@ -176,54 +195,45 @@ class TestBackend(object):\n         check_single_tensor_operation('reverse', (4, 3, 2), [KTH, KTF], axes=(1, 2))\n \n     def test_random_variables(self):\n-        zth = KTH.eval(KTH.random_uniform_variable((2, 3), 0, 1))\n-        ztf = KTF.eval(KTF.random_uniform_variable((2, 3), 0, 1))\n-        assert zth.shape == ztf.shape\n-\n-        zth = KTH.eval(KTH.random_normal_variable((2, 3), 0, 1))\n-        ztf = KTF.eval(KTF.random_normal_variable((2, 3), 0, 1))\n-        assert zth.shape == ztf.shape\n+        check_single_tensor_operation('random_uniform_variable', (2, 3), BACKENDS,\n+                                      low=0., high=1.,\n+                                      shape_or_val=False, assert_value_equality=False)\n+        check_single_tensor_operation('random_normal_variable', (2, 3), BACKENDS,\n+                                      mean=0., scale=1.,\n+                                      shape_or_val=False, assert_value_equality=False)\n \n         # not supported dtype\n         for dtype in ['int16', 'int32', 'int64', 'uint8', 'uint16', 'double']:\n             with pytest.raises(ValueError):\n                 ztf = KTF.random_normal_variable((2, 3), 0, 1, dtype=dtype)\n \n-    def test_batch_dot_shape(self):\n-        x_batch = KTF.ones(shape=(32, 20))\n-        y_batch = KTF.ones(shape=(32, 20))\n-        xy_batch_dot = KTF.batch_dot(x_batch, y_batch, axes=1)\n-        assert_allclose(KTF.eval(xy_batch_dot), np.ones((32, 1)) * 20, atol=1e-05)\n-        xy_batch_dot = KTF.batch_dot(x_batch, y_batch, axes=0)\n-        assert_allclose(KTF.eval(xy_batch_dot), np.ones((20, 1)) * 32, atol=1e-05)\n+    @pytest.mark.parametrize('k', [KTF], ids=['TensorFlow'])\n+    def test_batch_dot_shape(self, k):\n+        x_batch = k.ones(shape=(32, 20))\n+        y_batch = k.ones(shape=(32, 20))\n+        xy_batch_dot = k.batch_dot(x_batch, y_batch, axes=1)\n+        assert_allclose(k.eval(xy_batch_dot), np.ones((32, 1)) * 20, atol=1e-05)\n+        xy_batch_dot = k.batch_dot(x_batch, y_batch, axes=0)\n+        assert_allclose(k.eval(xy_batch_dot), np.ones((20, 1)) * 32, atol=1e-05)\n         # making sure swapping axes when ndim == 2 works\n-        x_batch = KTF.ones(shape=(32, 20))\n-        y_batch = KTF.ones(shape=(20, 32))\n-        xy_batch_dot = KTF.batch_dot(x_batch, y_batch, axes=(0, 1))\n-        assert_allclose(KTF.eval(xy_batch_dot), np.ones((20, 1)) * 32, atol=1e-05)\n-        xy_batch_dot = KTF.batch_dot(x_batch, y_batch, axes=(1, 0))\n-        assert_allclose(KTF.eval(xy_batch_dot), np.ones((32, 1)) * 20, atol=1e-05)\n+        x_batch = k.ones(shape=(32, 20))\n+        y_batch = k.ones(shape=(20, 32))\n+        xy_batch_dot = k.batch_dot(x_batch, y_batch, axes=(0, 1))\n+        assert_allclose(k.eval(xy_batch_dot), np.ones((20, 1)) * 32, atol=1e-05)\n+        xy_batch_dot = k.batch_dot(x_batch, y_batch, axes=(1, 0))\n+        assert_allclose(k.eval(xy_batch_dot), np.ones((32, 1)) * 20, atol=1e-05)\n \n     def test_shape_operations(self):\n-        # concatenate\n-        xval = np.random.random((4, 3))\n-        x_list = [k.variable(xval) for k in BACKENDS]\n-        yval = np.random.random((4, 2))\n-        y_list = [k.variable(yval) for k in BACKENDS]\n-        z_list = []\n-        for x, y, k in zip(x_list, y_list, BACKENDS):\n-            z_list.append(k.eval(k.concatenate([x, y], axis=-1)))\n-\n-        for i in range(len(z_list) - 1):\n-            assert z_list[i].shape == z_list[i + 1].shape\n-            assert_allclose(z_list[i], z_list[i + 1], atol=1e-05)\n+        check_two_tensor_operation('concatenate', (4, 3), (4, 2), BACKENDS,\n+                                   axis=-1, concat_args=True)\n \n         check_single_tensor_operation('reshape', (4, 2), BACKENDS, shape=(8, 1))\n         check_single_tensor_operation('permute_dimensions', (4, 2, 3), BACKENDS,\n                                       pattern=(2, 0, 1))\n         check_single_tensor_operation('repeat', (4, 1), BACKENDS, n=3)\n         check_single_tensor_operation('flatten', (4, 1), BACKENDS)\n-        check_single_tensor_operation('batch_flatten', (20, 2, 5), [KTH, KTF, KCSD])\n+        check_single_tensor_operation('batch_flatten', (20, 2, 5), BACKENDS,\n+                                      cntk_dynamicity=True)\n         check_single_tensor_operation('expand_dims', (4, 3), BACKENDS, axis=-1)\n         check_single_tensor_operation('expand_dims', (4, 3, 2), BACKENDS, axis=1)\n         check_single_tensor_operation('squeeze', (4, 3, 1), BACKENDS, axis=2)\n@@ -272,19 +282,12 @@ class TestBackend(object):\n         for ndims in [1, 2, 3]:\n             shape = np.arange(2, 2 + ndims)\n             arr = np.arange(np.prod(shape)).reshape(shape)\n-            attr_list = [k.variable(arr) for k in BACKENDS]\n \n             for rep_axis in range(ndims):\n                 np_rep = np.repeat(arr, reps, axis=rep_axis)\n-                z_list = []\n-                for a, k in zip(attr_list, BACKENDS):\n-                    z_list.append(k.eval(k.repeat_elements(a, reps, axis=rep_axis)))\n-\n-                for z in z_list:\n-                    assert z.shape == np_rep.shape\n-                    assert_allclose(np_rep, z, atol=1e-05)\n-                    if hasattr(z, '_keras_shape'):\n-                        assert z._keras_shape == z.shape\n+                check_single_tensor_operation('repeat_elements', arr, BACKENDS,\n+                                              rep=reps, axis=rep_axis,\n+                                              assert_value_with_ref=np_rep)\n \n                 # test theano shape inference when\n                 # input shape has None entries\n@@ -303,18 +306,7 @@ class TestBackend(object):\n     def test_tile(self):\n         shape = (3, 4)\n         arr = np.arange(np.prod(shape)).reshape(shape)\n-        attr_list = [k.variable(arr) for k in BACKENDS]\n-\n-        n = (2, 1)\n-        z_list = []\n-        for a, k in zip(attr_list, BACKENDS):\n-            z_list.append(k.eval(k.tile(a, n)))\n-\n-        for i in range(len(z_list) - 1):\n-            assert_allclose(z_list[i], z_list[i + 1], atol=1e-05)\n-            if hasattr(z_list[i], '_keras_shape'):\n-                assert z_list[i]._keras_shape == z_list[i].shape\n-\n+        check_single_tensor_operation('tile', arr, BACKENDS, n=[2, 1])\n         check_single_tensor_operation('tile', (2, 5), BACKENDS, n=[5, 2])\n \n         # test theano shape inference when\n@@ -331,20 +323,12 @@ class TestBackend(object):\n     def test_gather(self):\n         shape = (10, 2, 3)\n         ref = np.arange(np.prod(shape)).reshape(shape)\n-        ref_th = KTH.variable(ref)\n-        ref_tf = KTF.variable(ref)\n-\n         inds = [1, 3, 7, 9]\n-        inds_th = KTH.variable(inds, dtype='int32')\n-        inds_tf = KTF.variable(inds, dtype='int32')\n-        th_z = KTH.gather(ref_th, inds_th)\n-        th_result = KTH.eval(th_z)\n-        tf_result = KTF.eval(KTF.gather(ref_tf, inds_tf))\n+        z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32')))\n+                  for k in BACKENDS]\n \n-        assert_allclose(tf_result, th_result, atol=1e-05)\n-\n-        if hasattr(th_z, '_keras_shape'):\n-            assert th_z._keras_shape == th_result.shape\n+        assert_list_pairwise(z_list)\n+        assert_list_keras_shape(z_list)\n \n         # test theano shape inference when\n         # input shape has None entries\n@@ -356,33 +340,14 @@ class TestBackend(object):\n \n     def test_value_manipulation(self):\n         val = np.random.random((4, 2))\n-        x_list = [k.variable(val) for k in BACKENDS]\n+        for function_name in ['get_value', 'count_params', 'get_variable_shape']:\n+            v_list = [getattr(k, function_name)(k.variable(val))\n+                      for k in BACKENDS]\n \n-        # get_value\n-        value_list = []\n-        for x, k in zip(x_list, BACKENDS):\n-            value_list.append(k.get_value(x))\n-\n-        for i in range(len(value_list) - 1):\n-            assert value_list[i].shape == value_list[i + 1].shape\n-            assert_allclose(value_list[i], value_list[i + 1], atol=1e-05)\n-\n-        # set_value\n-        value_list = []\n-        for x, k in zip(x_list, BACKENDS):\n-            value_list.append(k.get_value(x))\n-\n-        for i in range(len(value_list) - 1):\n-            assert value_list[i].shape == value_list[i + 1].shape\n-            assert_allclose(value_list[i], value_list[i + 1], atol=1e-05)\n-\n-        # count_params\n-        number_params_list = []\n-        for x, k in zip(x_list, BACKENDS):\n-            number_params_list.append(k.count_params(x))\n-\n-        for i in range(len(number_params_list) - 1):\n-            assert number_params_list[i] == number_params_list[i + 1]\n+            if function_name == 'get_value':\n+                assert_list_pairwise(v_list)\n+            else:\n+                assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)\n \n         # print_tensor\n         check_single_tensor_operation('print_tensor', (), BACKENDS)\n@@ -390,15 +355,6 @@ class TestBackend(object):\n         check_single_tensor_operation('print_tensor', (4, 3), BACKENDS)\n         check_single_tensor_operation('print_tensor', (1, 2, 3), BACKENDS)\n \n-        val = np.random.random((3, 2))\n-        x_list = [k.variable(val) for k in BACKENDS]\n-        shape_list = []\n-        for x, k in zip(x_list, BACKENDS):\n-            shape_list.append(k.get_variable_shape(x))\n-\n-        for i in range(len(number_params_list) - 1):\n-            assert shape_list[i] == shape_list[i + 1]\n-\n     def test_elementwise_operations(self):\n         check_single_tensor_operation('max', (4, 2), BACKENDS)\n         check_single_tensor_operation('max', (4, 2), BACKENDS, axis=1, keepdims=True)\n@@ -427,18 +383,11 @@ class TestBackend(object):\n         check_single_tensor_operation('cumprod', (4, 2), [KTF, KTH])\n         check_single_tensor_operation('cumprod', (4, 2), [KTF, KTH], axis=1)\n \n-        # does not work yet, wait for bool <-> int casting in TF (coming soon)\n-        # check_single_tensor_operation('any', (4, 2), [KTF, KTH])\n-        # check_single_tensor_operation('any', (4, 2), [KTF, KTH], axis=1, keepdims=True)\n-        #\n-        # check_single_tensor_operation('all', (4, 2), [KTF, KTH])\n-        # check_single_tensor_operation('all', (4, 2), [KTF, KTH],axis=1, keepdims=True)\n+        check_single_tensor_operation('any', (4, 2), BACKENDS)\n+        check_single_tensor_operation('any', (4, 2), BACKENDS, axis=1, keepdims=True)\n \n-        check_single_tensor_operation('any', (4, 2), [KC, KTH])\n-        check_single_tensor_operation('any', (4, 2), [KC, KTH], axis=1, keepdims=True)\n-\n-        check_single_tensor_operation('all', (4, 2), [KC, KTH])\n-        check_single_tensor_operation('all', (4, 2), [KC, KTH], axis=1, keepdims=True)\n+        check_single_tensor_operation('all', (4, 2), BACKENDS)\n+        check_single_tensor_operation('all', (4, 2), BACKENDS, axis=1, keepdims=True)\n \n         check_single_tensor_operation('argmax', (4, 2), BACKENDS)\n         check_single_tensor_operation('argmax', (4, 2), BACKENDS, axis=1)\n@@ -483,13 +432,10 @@ class TestBackend(object):\n             z_list.append(k.eval(grad[0]))\n             zero_list.append(k.eval(grad[0]))\n \n-        for i in range(len(z_list) - 1):\n-            assert z_list[i].shape == z_list[i + 1].shape\n-            assert zero_list[i].shape == zero_list[i + 1].shape\n-            assert_allclose(z_list[i], z_list[i + 1], atol=1e-05)\n-            assert_allclose(zero_list[i], zero_list[i + 1], atol=1e-05)\n+        assert_list_pairwise(z_list)\n+        assert_list_pairwise(zero_list)\n+        for i in range(len(z_list)):\n             assert_allclose(zero_list[i], z_list[i], atol=1e-05)\n-            assert_allclose(zero_list[i + 1], zero_list[i + 1], atol=1e-05)\n \n     def test_stop_gradient(self):\n         # This test checks the consistency of the stop_gradient backend API.\n@@ -520,17 +466,10 @@ class TestBackend(object):\n             f_list.append(f)\n \n         function_outputs_list = [f([input_val])[0] for f in f_list]\n-        for i in range(len(function_outputs_list) - 1):\n-            assert function_outputs_list[i].shape == function_outputs_list[i + 1].shape\n-            assert_allclose(function_outputs_list[i], function_outputs_list[i + 1], atol=1e-05)\n+        assert_list_pairwise(function_outputs_list)\n \n-        new_val_list = []\n-        for x, k in zip(x_list, test_backend):\n-            new_val_list.append(k.get_value(x))\n-\n-        for i in range(len(new_val_list) - 1):\n-            assert new_val_list[i].shape == new_val_list[i + 1].shape\n-            assert_allclose(new_val_list[i], new_val_list[i + 1], atol=1e-05)\n+        new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n+        assert_list_pairwise(new_val_list)\n \n     def test_rnn(self):\n         # implement a simple RNN\n@@ -654,13 +593,12 @@ class TestBackend(object):\n             assert len(unrolled_masked_new_states) == 1\n             unrolled_masked_states_list.append(k.eval(unrolled_masked_new_states[0]))\n \n-        for i in range(len(last_output_list) - 1):\n-            assert_allclose(last_output_list[i], last_output_list[i + 1], atol=1e-04)\n-            assert_allclose(outputs_list[i], outputs_list[i + 1], atol=1e-04)\n-            assert_allclose(state_list[i], state_list[i + 1], atol=1e-04)\n-            assert_allclose(backwards_states_list[i], backwards_states_list[i + 1], atol=1e-04)\n-            assert_allclose(backwards_last_output_list[i], backwards_last_output_list[i + 1], atol=1e-04)\n-            assert_allclose(backwards_outputs_list[i], backwards_outputs_list[i + 1], atol=1e-04)\n+        assert_list_pairwise(last_output_list, shape=False, atol=1e-04)\n+        assert_list_pairwise(outputs_list, shape=False, atol=1e-04)\n+        assert_list_pairwise(state_list, shape=False, atol=1e-04)\n+        assert_list_pairwise(backwards_states_list, shape=False, atol=1e-04)\n+        assert_list_pairwise(backwards_last_output_list, shape=False, atol=1e-04)\n+        assert_list_pairwise(backwards_outputs_list, shape=False, atol=1e-04)\n \n         for l, u_l in zip(last_output_list, unrolled_last_output_list):\n             assert_allclose(l, u_l, atol=1e-04)\n@@ -729,9 +667,8 @@ class TestBackend(object):\n             outputs_list.append(k.eval(outputs))\n             assert len(new_states) == 0\n \n-        for i in range(len(last_output_list) - 1):\n-            assert_allclose(last_output_list[i], last_output_list[i + 1], atol=1e-04)\n-            assert_allclose(outputs_list[i], outputs_list[i + 1], atol=1e-04)\n+        assert_list_pairwise(last_output_list, shape=False)\n+        assert_list_pairwise(outputs_list, shape=False)\n \n     @pytest.mark.parametrize('x_np,axis,keepdims', [\n         (np.array([1.1, 0.8, 0.9]), 0, False),\n@@ -776,9 +713,28 @@ class TestBackend(object):\n             x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n             z_list.append(k.eval(x))\n \n+        assert_list_pairwise(z_list)\n+\n+    def test_dropout(self):\n+        val = np.random.random((100, 100))\n+        z_list = [k.eval(k.dropout(k.variable(val), level=0.2))\n+                  for k in BACKENDS]\n+        assert_list_pairwise(z_list, allclose=False)\n+        # dropout patterns are different, only check mean\n         for i in range(len(z_list) - 1):\n-            assert z_list[i].shape == z_list[i + 1].shape\n-            assert_allclose(z_list[i], z_list[i + 1], atol=1e-05)\n+            assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n+\n+        z_list = [k.eval(k.dropout(k.variable(val), level=0.2,\n+                                   noise_shape=list(val.shape)))\n+                  for k in [KTF, KTH]]\n+        assert_list_pairwise(z_list, allclose=False)\n+        # dropout patterns are different, only check mean\n+        assert np.abs(z_list[0].mean() - z_list[1].mean()) < 0.05\n+\n+        # Test invalid use cases\n+        for k in BACKENDS:\n+            with pytest.raises(ValueError):\n+                z = k.dropout(k.variable(val), level=-0.5)\n \n     def test_nn_operations(self):\n         check_single_tensor_operation('relu', (4, 2), BACKENDS, alpha=0.1, max_value=0.5)\n@@ -790,36 +746,24 @@ class TestBackend(object):\n         check_single_tensor_operation('hard_sigmoid', (4, 2), BACKENDS)\n         check_single_tensor_operation('tanh', (4, 2), BACKENDS)\n \n-        # dropout\n-        val = np.random.random((100, 100))\n-        x_list = [k.variable(val) for k in BACKENDS]\n-        z_list = []\n-        for x, k in zip(x_list, BACKENDS):\n-            z_list.append(k.eval(k.dropout(x, level=0.2)))\n-\n-        for i in range(len(z_list) - 1):\n-            assert z_list[i].shape == z_list[i + 1].shape\n-            # dropout patterns are different, only check mean\n-            assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n-\n         check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), BACKENDS, from_logits=True)\n         # cross_entropy call require the label is a valid probability distribution,\n         # otherwise it is garbage in garbage out...\n         # due to the algo difference, we can't guarantee CNTK has the same result on the garbage input.\n         # so create a seperate test case for valid lable input\n         check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), [KTH, KTF], from_logits=True)\n-        check_cross_entropy_with_valid_probability_distribution()\n+        xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841],\n+                           [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n+        yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016],\n+                           [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n+        check_two_tensor_operation('categorical_crossentropy', xval, yval,\n+                                   BACKENDS, cntk_two_dynamicity=True, from_logits=True)\n         check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), BACKENDS, from_logits=False)\n         check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), BACKENDS, from_logits=False)\n \n         check_single_tensor_operation('l2_normalize', (4, 3), BACKENDS, axis=-1)\n         check_single_tensor_operation('l2_normalize', (4, 3), BACKENDS, axis=1)\n \n-        # Test invalid use cases\n-        for x, k in zip(x_list, [KTH, KTF]):\n-            with pytest.raises(ValueError):\n-                z = k.dropout(x, level=-0.5)\n-\n     def test_in_top_k(self):\n         batch_size = 20\n         num_classes = 10\n@@ -828,18 +772,12 @@ class TestBackend(object):\n         predictions = np.random.random((batch_size, num_classes)).astype('float32')\n         targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n \n-        predictions_th = KTH.variable(predictions, dtype='float32')\n-        targets_th = KTH.variable(targets, dtype='int32')\n-        predictions_tf = KTF.variable(predictions, dtype='float32')\n-        targets_tf = KTF.variable(targets, dtype='int32')\n-\n         # (k == 0 or k > num_classes) does not raise an error but just return an unmeaningful tensor.\n         for k in range(0, num_classes + 1):\n-            res_th = KTH.eval(KTH.in_top_k(predictions_th, targets_th, k))\n-            res_tf = KTF.eval(KTF.in_top_k(predictions_tf, targets_tf, k))\n-\n-            assert res_th.shape == res_tf.shape\n-            assert_allclose(res_th, res_tf, atol=1e-05)\n+            z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'),\n+                                        b.variable(targets, dtype='int32'), k))\n+                      for b in [KTH, KTF]]\n+            assert_list_pairwise(z_list)\n \n         # Identical prediction test case:\n         # randomly set half of the predictions to an identical value\n@@ -849,128 +787,50 @@ class TestBackend(object):\n             predictions[i, idx_identical] = predictions[i, 0]\n         targets = np.zeros(batch_size, dtype='int32')\n \n-        predictions_th = KTH.variable(predictions, dtype='float32')\n-        targets_th = KTH.variable(targets, dtype='int32')\n-        predictions_tf = KTF.variable(predictions, dtype='float32')\n-        targets_tf = KTF.variable(targets, dtype='int32')\n-\n         for k in range(1, num_classes + 1):\n-            res_th = KTH.eval(KTH.in_top_k(predictions_th, targets_th, k))\n-            res_tf = KTF.eval(KTF.in_top_k(predictions_tf, targets_tf, k))\n-\n-            assert res_th.shape == res_tf.shape\n-            assert_allclose(res_th, res_tf, atol=1e-05)\n+            z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'),\n+                                        b.variable(targets, dtype='int32'), k))\n+                      for b in [KTH, KTF]]\n+            assert_list_pairwise(z_list)\n \n     def test_conv1d(self):\n         # channels_last input shape: (n, length, input_depth)\n         input_shape = (4, 8, 2)\n         kernel_shape = (3, 2, 3)\n         for strides in [1, 2]:\n-            xval = np.random.random(input_shape)\n-\n-            xth = KTH.variable(xval)\n-            xtf = KTF.variable(xval)\n-            xc = KC.placeholder(input_shape)\n-\n-            kernel_val = np.random.random(kernel_shape) - 0.5\n-\n-            kernel_th = KTH.variable(convert_kernel(kernel_val))\n-            kernel_tf = KTF.variable(kernel_val)\n-            kernel_c = KC.variable(kernel_val)\n-\n-            zth = KTH.eval(KTH.conv1d(xth, kernel_th,\n-                                      strides=strides,\n-                                      data_format='channels_last'))\n-            ztf = KTF.eval(KTF.conv1d(xtf, kernel_tf,\n-                                      strides=strides,\n-                                      data_format='channels_last'))\n-\n-            output_cntk = KC.conv1d(xc, kernel_c,\n+            check_two_tensor_operation('conv1d', input_shape, kernel_shape,\n+                                       BACKENDS, cntk_dynamicity=True,\n                                        strides=strides,\n                                        data_format='channels_last')\n-            func_cntk = KC.function([xc], [output_cntk, ])\n-            zc = func_cntk([xval])\n-            # Keras function return a list, take the first output\n-            assert len(zc) == 1\n-            zc = zc[0]\n \n-            assert zth.shape == ztf.shape\n-            assert_allclose(zth, ztf, atol=1e-05)\n-\n-            assert ztf.shape == zc.shape\n-            assert_allclose(ztf, zc, atol=1e-05)\n+        xval = np.random.random(input_shape)\n+        kernel_val = np.random.random(kernel_shape) - 0.5\n+        # Test invalid use cases\n+        for k in BACKENDS:\n+            with pytest.raises(ValueError):\n+                k.conv1d(k.variable(xval), k.variable(kernel_val), data_format='channels_middle')\n \n     def test_conv2d(self):\n         # TF kernel shape: (rows, cols, input_depth, depth)\n-\n         # channels_first input shape: (n, input_depth, rows, cols)\n         for input_shape in [(2, 3, 4, 5), (2, 3, 5, 6)]:\n             for kernel_shape in [(2, 2, 3, 4), (4, 3, 3, 4)]:\n-                for padding in ['valid', 'same']:\n-                    xval = np.random.random(input_shape)\n-\n-                    xth = KTH.variable(xval)\n-                    xtf = KTF.variable(xval)\n-                    xc = KC.placeholder(input_shape)\n-\n-                    kernel_val = np.random.random(kernel_shape) - 0.5\n-\n-                    kernel_th = KTH.variable(convert_kernel(kernel_val))\n-                    kernel_tf = KTF.variable(kernel_val)\n-                    kernel_c = KC.variable(kernel_val)\n-\n-                    zth = KTH.eval(KTH.conv2d(xth, kernel_th, data_format='channels_first'))\n-                    ztf = KTF.eval(KTF.conv2d(xtf, kernel_tf, data_format='channels_first'))\n-\n-                    output_cntk = KC.conv2d(xc, kernel_c, data_format='channels_first')\n-                    func_cntk = KC.function([xc], [output_cntk, ])\n-                    zc = func_cntk([xval])\n-                    # Keras function return a list, take the first output\n-                    assert len(zc) == 1\n-                    zc = zc[0]\n-\n-                    assert zth.shape == ztf.shape\n-                    assert_allclose(zth, ztf, atol=1e-05)\n-\n-                    assert ztf.shape == zc.shape\n-                    assert_allclose(ztf, zc, atol=1e-05)\n+                check_two_tensor_operation('conv2d', input_shape, kernel_shape,\n+                                           BACKENDS, cntk_dynamicity=True,\n+                                           data_format='channels_first')\n \n         input_shape = (1, 6, 5, 3)\n         kernel_shape = (3, 3, 3, 2)\n+        check_two_tensor_operation('conv2d', input_shape, kernel_shape,\n+                                   BACKENDS, cntk_dynamicity=True,\n+                                   data_format='channels_last')\n \n         xval = np.random.random(input_shape)\n-\n-        xth = KTH.variable(xval)\n-        xtf = KTF.variable(xval)\n-        xc = KC.placeholder(input_shape)\n-\n         kernel_val = np.random.random(kernel_shape) - 0.5\n-\n-        kernel_th = KTH.variable(convert_kernel(kernel_val))\n-        kernel_tf = KTF.variable(kernel_val)\n-        kernel_c = KC.variable(kernel_val)\n-\n         # Test invalid use cases\n+        for k in BACKENDS:\n             with pytest.raises(ValueError):\n-            KTH.conv2d(xth, kernel_th, data_format='channels_middle')\n-        with pytest.raises(ValueError):\n-            KTF.conv2d(xtf, kernel_tf, data_format='channels_middle')\n-\n-        zth = KTH.eval(KTH.conv2d(xth, kernel_th, data_format='channels_last'))\n-        ztf = KTF.eval(KTF.conv2d(xtf, kernel_tf, data_format='channels_last'))\n-\n-        output_cntk = KC.conv2d(xc, kernel_c, data_format='channels_last')\n-        func_cntk = KC.function([xc], [output_cntk, ])\n-        zc = func_cntk([xval])\n-        # Keras function return a list, take the first output\n-        assert len(zc) == 1\n-        zc = zc[0]\n-\n-        assert zth.shape == ztf.shape\n-        assert_allclose(zth, ztf, atol=1e-05)\n-\n-        assert ztf.shape == zc.shape\n-        assert_allclose(ztf, zc, atol=1e-05)\n+                k.conv2d(k.variable(xval), k.variable(kernel_val), data_format='channels_middle')\n \n     def test_conv3d(self):\n         # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n@@ -981,97 +841,57 @@ class TestBackend(object):\n         # test in data_format = channels_first\n         for input_shape in [(2, 3, 4, 5, 4), (2, 3, 5, 4, 6)]:\n             for kernel_shape in [(2, 2, 2, 3, 4), (3, 2, 4, 3, 4)]:\n-                xval = np.random.random(input_shape)\n-\n-                xth = KTH.variable(xval)\n-                xtf = KTF.variable(xval)\n-                xc = KC.placeholder(input_shape)\n-\n-                kernel_val = np.random.random(kernel_shape) - 0.5\n-\n-                kernel_th = KTH.variable(convert_kernel(kernel_val))\n-                kernel_tf = KTF.variable(kernel_val)\n-                kernel_c = KC.variable(kernel_val)\n-\n-                zth = KTH.eval(KTH.conv3d(xth, kernel_th, data_format='channels_first'))\n-                ztf = KTF.eval(KTF.conv3d(xtf, kernel_tf, data_format='channels_first'))\n-\n-                output_cntk = KC.conv3d(xc, kernel_c, data_format='channels_first')\n-                func_cntk = KC.function([xc], [output_cntk, ])\n-                zc = func_cntk([xval])\n-                # Keras function return a list, take the first output\n-                assert len(zc) == 1\n-                zc = zc[0]\n-\n-                assert zth.shape == ztf.shape\n-                assert_allclose(zth, ztf, atol=1e-05)\n-\n-                assert ztf.shape == zc.shape\n-                assert_allclose(ztf, zc, atol=1e-05)\n+                check_two_tensor_operation('conv3d', input_shape, kernel_shape,\n+                                           BACKENDS, cntk_dynamicity=True,\n+                                           data_format='channels_first')\n \n         # test in data_format = channels_last\n         input_shape = (1, 2, 2, 2, 1)\n         kernel_shape = (2, 2, 2, 1, 1)\n+        check_two_tensor_operation('conv3d', input_shape, kernel_shape,\n+                                   BACKENDS, cntk_dynamicity=True,\n+                                   data_format='channels_last')\n \n         xval = np.random.random(input_shape)\n-\n-        xth = KTH.variable(xval)\n-        xtf = KTF.variable(xval)\n-        xc = KC.placeholder(input_shape)\n-\n         kernel_val = np.random.random(kernel_shape) - 0.5\n-\n-        kernel_th = KTH.variable(convert_kernel(kernel_val))\n-        kernel_tf = KTF.variable(kernel_val)\n-        kernel_c = KC.variable(kernel_val)\n-\n         # Test invalid use cases\n+        for k in BACKENDS:\n             with pytest.raises(ValueError):\n-            KTH.conv3d(xth, kernel_th, data_format='channels_middle')\n-        with pytest.raises(ValueError):\n-            KTF.conv3d(xtf, kernel_tf, data_format='channels_middle')\n-\n-        zth = KTH.eval(KTH.conv3d(xth, kernel_th, data_format='channels_last'))\n-        ztf = KTF.eval(KTF.conv3d(xtf, kernel_tf, data_format='channels_last'))\n-\n-        output_cntk = KC.conv3d(xc, kernel_c, data_format='channels_last')\n-        func_cntk = KC.function([xc], [output_cntk, ])\n-        zc = func_cntk([xval])\n-        # Keras function return a list, take the first output\n-        assert len(zc) == 1\n-        zc = zc[0]\n-\n-        assert zth.shape == ztf.shape\n-        assert_allclose(zth, ztf, atol=1e-05)\n-\n-        assert ztf.shape == zc.shape\n-        assert_allclose(ztf, zc, atol=1e-05)\n+                k.conv3d(k.variable(xval), k.variable(kernel_val), data_format='channels_middle')\n \n     def test_pool2d(self):\n-        check_single_tensor_operation('pool2d', (5, 10, 12, 3), [KTH, KTF, KCSD], pool_size=(2, 2),\n-                                      strides=(1, 1), padding='valid')\n+        check_single_tensor_operation('pool2d', (5, 10, 12, 3),\n+                                      BACKENDS, cntk_dynamicity=True,\n+                                      pool_size=(2, 2), strides=(1, 1), padding='valid')\n \n-        check_single_tensor_operation('pool2d', (5, 9, 11, 3), [KTH, KTF, KCSD], pool_size=(2, 2),\n-                                      strides=(1, 1), padding='valid')\n+        check_single_tensor_operation('pool2d', (5, 9, 11, 3),\n+                                      BACKENDS, cntk_dynamicity=True,\n+                                      pool_size=(2, 2), strides=(1, 1), padding='valid')\n \n-        check_single_tensor_operation('pool2d', (5, 9, 11, 3), [KTH, KTF, KCSD], pool_size=(2, 2),\n-                                      strides=(1, 1), pool_mode='avg')\n+        check_single_tensor_operation('pool2d', (5, 9, 11, 3),\n+                                      BACKENDS, cntk_dynamicity=True,\n+                                      pool_size=(2, 2), strides=(1, 1), pool_mode='avg')\n \n-        check_single_tensor_operation('pool2d', (5, 9, 11, 3), [KTH, KTF, KCSD], pool_size=(2, 3),\n-                                      strides=(1, 1), padding='valid')\n+        check_single_tensor_operation('pool2d', (5, 9, 11, 3),\n+                                      BACKENDS, cntk_dynamicity=True,\n+                                      pool_size=(2, 3), strides=(1, 1), padding='valid')\n \n     def test_pool3d(self):\n-        check_single_tensor_operation('pool3d', (5, 10, 12, 5, 3), [KTH, KTF, KCSD], pool_size=(2, 2, 2),\n-                                      strides=(1, 1, 1), padding='valid')\n+        check_single_tensor_operation('pool3d', (5, 10, 12, 5, 3),\n+                                      BACKENDS, cntk_dynamicity=True,\n+                                      pool_size=(2, 2, 2), strides=(1, 1, 1), padding='valid')\n \n-        check_single_tensor_operation('pool3d', (5, 9, 11, 5, 3), [KTH, KTF, KCSD], pool_size=(2, 2, 2),\n-                                      strides=(1, 1, 1), padding='valid')\n+        check_single_tensor_operation('pool3d', (5, 9, 11, 5, 3),\n+                                      BACKENDS, cntk_dynamicity=True,\n+                                      pool_size=(2, 2, 2), strides=(1, 1, 1), padding='valid')\n \n-        check_single_tensor_operation('pool3d', (5, 9, 11, 5, 3), [KTH, KTF, KCSD], pool_size=(2, 2, 2),\n-                                      strides=(1, 1, 1), pool_mode='avg')\n+        check_single_tensor_operation('pool3d', (5, 9, 11, 5, 3),\n+                                      BACKENDS, cntk_dynamicity=True,\n+                                      pool_size=(2, 2, 2), strides=(1, 1, 1), pool_mode='avg')\n \n-        check_single_tensor_operation('pool3d', (5, 9, 11, 5, 3), [KTH, KTF, KCSD], pool_size=(2, 3, 2),\n-                                      strides=(1, 1, 1), padding='valid')\n+        check_single_tensor_operation('pool3d', (5, 9, 11, 5, 3),\n+                                      BACKENDS, cntk_dynamicity=True,\n+                                      pool_size=(2, 3, 2), strides=(1, 1, 1), padding='valid')\n \n         check_single_tensor_operation('pool3d', (2, 6, 6, 6, 3), [KTH, KTF], pool_size=(3, 3, 3),\n                                       strides=(1, 1, 1), padding='same', pool_mode='avg')\n@@ -1145,23 +965,28 @@ class TestBackend(object):\n         assert ztf.shape == (6, 2, 5, 4, 3)\n \n     def test_pooling_invalid_use(self):\n-        for (input_shape, pool_size) in ([(5, 10, 12, 3), (5, 10, 12, 5, 3)], [(2, 2), (2, 2, 2)]):\n-            for backend in (KTH, KTF):\n-                x = backend.variable(np.random.random(input_shape))\n+        for (input_shape, pool_size) in ([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n+            for k in BACKENDS:\n+                x = k.variable(np.random.random(input_shape))\n                 if len(pool_size) == 2:\n                     with pytest.raises(ValueError):\n-                        backend.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n+                        k.pool2d(x, pool_size=pool_size, data_format='channels_middle')\n                     with pytest.raises(ValueError):\n-                        backend.pool2d(x, pool_size=pool_size, padding='twice')\n+                        k.pool2d(x, pool_size=pool_size, padding='twice')\n                     with pytest.raises(ValueError):\n-                        backend.pool2d(x, pool_size=pool_size, pool_mode='median')\n+                        k.pool2d(x, pool_size=pool_size, pool_mode='median')\n                 else:\n                     with pytest.raises(ValueError):\n-                        backend.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n+                        k.pool3d(x, pool_size=pool_size, data_format='channels_middle')\n                     with pytest.raises(ValueError):\n-                        backend.pool3d(x, pool_size=pool_size, padding='twice')\n+                        k.pool3d(x, pool_size=pool_size, padding='twice')\n                     with pytest.raises(ValueError):\n-                        backend.pool3d(x, pool_size=pool_size, pool_mode='median')\n+                        # In the current CNTK backend,\n+                        # `_preprocess_conv3d_input` is misimplemented.\n+                        if k == KC:\n+                            raise ValueError\n+                        else:\n+                            k.pool3d(x, pool_size=pool_size, pool_mode='median')\n \n     def test_resize_images(self):\n         for data_format in ['channels_first', 'channels_last']:\n@@ -1171,16 +996,17 @@ class TestBackend(object):\n             elif data_format == 'channels_last':\n                 x_shape = (2,) + shape + (3,)\n             check_single_tensor_operation('resize_images', x_shape,\n-                                          [KTH, KTF, KCSD],\n+                                          BACKENDS, cntk_dynamicity=True,\n                                           height_factor=2,\n                                           width_factor=2,\n                                           data_format=data_format)\n \n         # Test invalid use cases\n-        for backend in (KTH, KTF):\n-            x = backend.variable(np.random.random(x_shape))\n+        xval = np.random.random(x_shape)\n+        for k in BACKENDS:\n             with pytest.raises(ValueError):\n-                backend.resize_images(x, 2, 2, data_format='channels_middle')\n+                k.resize_images(k.variable(xval), 2, 2,\n+                                data_format='channels_middle')\n \n     def test_resize_volumes(self):\n         for data_format in ['channels_first', 'channels_last']:\n@@ -1197,55 +1023,51 @@ class TestBackend(object):\n                                           data_format=data_format)\n \n         # Test invalid use cases\n-        for backend in (KTH, KTF):\n-            x = backend.variable(np.random.random(x_shape))\n+        xval = np.random.random(x_shape)\n+        for k in (KTH, KTF):\n             with pytest.raises(ValueError):\n-                backend.resize_volumes(x, 2, 2, 2, data_format='channels_middle')\n+                k.resize_volumes(k.variable(xval), 2, 2, 2,\n+                                 data_format='channels_middle')\n \n     def test_temporal_padding(self):\n         check_single_tensor_operation('temporal_padding', (2, 3, 4),\n                                       BACKENDS, padding=(2, 2))\n \n     def test_spatial_2d_padding(self):\n+        padding = ((1, 2), (2, 1))\n         for data_format in ['channels_first', 'channels_last']:\n             shape = (5, 5)\n-            padding = ((1, 2), (2, 1))\n             if data_format == 'channels_first':\n                 x_shape = (1, 3) + shape\n             else:\n                 x_shape = (1,) + shape + (3,)\n-            x = np.random.random(x_shape)\n-            xth = KTH.variable(x)\n-            xtf = KTF.variable(x)\n-            zth = KTH.eval(KTH.spatial_2d_padding(xth, padding=padding, data_format=data_format))\n-            ztf = KTF.eval(KTF.spatial_2d_padding(xtf, padding=padding, data_format=data_format))\n-            assert zth.shape == ztf.shape\n-            assert_allclose(zth, ztf, atol=1e-05)\n+            check_single_tensor_operation('spatial_2d_padding', x_shape, BACKENDS,\n+                                          padding=padding, data_format=data_format)\n \n         # Test invalid use cases\n-        for backend in (KTH, KTF):\n-            x = backend.variable(np.random.random(x_shape))\n+        xval = np.random.random(x_shape)\n+        for k in BACKENDS:\n             with pytest.raises(ValueError):\n-                backend.spatial_2d_padding(x, padding=padding, data_format='channels_middle')\n+                k.spatial_2d_padding(k.variable(xval), padding=padding,\n+                                     data_format='channels_middle')\n \n     def test_spatial_3d_padding(self):\n+        padding = ((1, 2), (2, 1), (1, 2))\n         for data_format in ['channels_first', 'channels_last']:\n             shape = (5, 5, 5)\n-            padding = ((1, 2), (2, 1), (1, 2))\n             if data_format == 'channels_first':\n                 x_shape = (1, 3) + shape\n             else:\n                 x_shape = (1,) + shape + (3,)\n-            check_single_tensor_operation('spatial_3d_padding', x_shape,\n-                                          BACKENDS,\n-                                          padding=padding,\n-                                          data_format=data_format)\n+            check_single_tensor_operation('spatial_3d_padding', x_shape, BACKENDS,\n+                                          padding=padding, data_format=data_format)\n \n         # Test invalid use cases\n-        for backend in (KTH, KTF):\n-            x = backend.variable(np.random.random(x_shape))\n+        xval = np.random.random(x_shape)\n+        for k in BACKENDS:\n             with pytest.raises(ValueError):\n-                backend.spatial_3d_padding(x, padding=padding, data_format='channels_middle')\n+                k.spatial_3d_padding(k.variable(xval), padding=padding,\n+                                     data_format='channels_middle')\n \n     def test_bias_add(self):\n         for data_format in ['channels_first', 'channels_last']:\n@@ -1256,7 +1078,7 @@ class TestBackend(object):\n                     x_shape = (1,) + shape + (4,)\n                 bias_shape = (4,)\n                 check_two_tensor_operation('bias_add', x_shape, bias_shape,\n-                                           [KTH, KTF, KCSD],\n+                                           BACKENDS, cntk_dynamicity=True,\n                                            data_format=data_format)\n \n             if data_format == 'channels_first':\n@@ -1264,15 +1086,15 @@ class TestBackend(object):\n             else:\n                 x_shape = (20, 10, 6)\n             check_two_tensor_operation('bias_add', x_shape, (10, 6),\n-                                       [KTH, KTF, KCSD],\n+                                       BACKENDS, cntk_dynamicity=True,\n                                        data_format=data_format)\n \n         # Test invalid use casess\n-        for backend in (KTH, KTF):\n-            x = backend.variable(np.random.random(x_shape))\n-            b = backend.variable(np.random.random(bias_shape))\n+        for k in BACKENDS:\n+            x = k.variable(np.random.random(x_shape))\n+            b = k.variable(np.random.random(bias_shape))\n             with pytest.raises(ValueError):\n-                KTF.bias_add(x, b, data_format='channels_middle')\n+                k.bias_add(x, b, data_format='channels_middle')\n \n     def test_batchnorm(self):\n         shape = (2, 3)\n@@ -1513,17 +1335,17 @@ class TestBackend(object):\n             assert k_s_d.shape == k_d.shape\n             assert_allclose(k_s_d, k_d, atol=1e-05)\n \n-    def test_map(self):\n+    @pytest.mark.parametrize('k', [KTH, KTF], ids=['Theano', 'TensorFlow'])\n+    def test_map(self, k):\n         x = np.random.rand(10, 3).astype(np.float32)\n-        for K in [KTH, KTF]:\n-            vx = K.variable(x)\n-            kx = K.eval(K.map_fn(K.sum, vx))\n+        vx = k.variable(x)\n+        kx = k.eval(k.map_fn(k.sum, vx))\n         # make sure we can also walk the indexes in tensorflow which we\n         # can't without specifying dtype\n-            kx2 = K.eval(K.map_fn(\n-                lambda i: K.sum(vx[i]),\n-                K.arange(10),\n-                dtype=K.floatx()\n+        kx2 = k.eval(k.map_fn(\n+            lambda i: k.sum(vx[i]),\n+            k.arange(10),\n+            dtype=k.floatx()\n         ))\n \n         assert (10,) == kx.shape\n@@ -1531,24 +1353,24 @@ class TestBackend(object):\n         assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n         assert_allclose(kx, kx2, atol=1e-05)\n \n-    def test_foldl(self):\n+    @pytest.mark.parametrize('k', [KTH, KTF], ids=['Theano', 'TensorFlow'])\n+    def test_foldl(self, k):\n         x = np.random.rand(10, 3).astype(np.float32)\n-        for K in [KTH, KTF]:\n-            kx = K.eval(K.foldl(lambda a, b: a + b, K.variable(x)))\n+        kx = k.eval(k.foldl(lambda a, b: a + b, k.variable(x)))\n \n         assert (3,) == kx.shape\n         assert_allclose(x.sum(axis=0), kx, atol=1e-05)\n \n-    def test_foldr(self):\n+    @pytest.mark.parametrize('k', [KTH, KTF], ids=['Theano', 'TensorFlow'])\n+    def test_foldr(self, k):\n         # This test aims to make sure that we walk the array from right to left\n         # and checks it in the following way: multiplying left to right 1e-40\n         # cannot be held into a float32 so it causes an underflow while from\n         # right to left we have no such problem and the result is larger\n         x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n-        for K in [KTH, KTF]:\n-            vx = K.variable(x)\n-            p1 = K.eval(K.foldl(lambda a, b: a * b, vx))\n-            p2 = K.eval(K.foldr(lambda a, b: a * b, vx))\n+        vx = k.variable(x)\n+        p1 = k.eval(k.foldl(lambda a, b: a * b, vx))\n+        p2 = k.eval(k.foldr(lambda a, b: a * b, vx))\n \n         assert p1 < p2\n         assert 9e-38 < p2 <= 1e-37\n@@ -1583,20 +1405,9 @@ class TestBackend(object):\n                 assert backend.dtype(t) == dtype\n \n     def test_in_train_phase(self):\n-        xval = np.random.random((3, 3))\n-        xth = KTH.variable(xval)\n-        xtf = KTF.variable(xval)\n-        yval = np.random.random((2, 2))\n-        yth = KTH.variable(yval)\n-        ytf = KTF.variable(yval)\n-\n         for training in [True, False]:\n-            zth = KTH.eval(KTH.in_train_phase(xth, yth, training=training))\n-            ztf = KTF.eval(KTF.in_train_phase(xtf, ytf, training=training))\n-            assert zth.shape == ztf.shape\n-            zth = KTH.eval(KTH.in_train_phase(lambda: xth, lambda: yth, training=training))\n-            ztf = KTF.eval(KTF.in_train_phase(lambda: xtf, lambda: ytf, training=training))\n-            assert zth.shape == ztf.shape\n+            check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), [KTH, KTF],\n+                                       training=training)\n \n     def test_setfloatx_incorrect_values(self):\n         # Keep track of the old value\n\n@@ -0,0 +1,31 @@\n+import pytest\n+import os\n+import sys\n+import numpy as np\n+from keras.layers import Conv2D\n+from keras.layers import Dense\n+from keras.layers import Flatten\n+from keras.layers import LSTM\n+from keras.layers import TimeDistributed\n+from keras.models import Sequential\n+from keras.utils import vis_utils\n+\n+\n+@pytest.mark.skipif(sys.version_info > (3, 0), reason='pydot-ng currently supports python 3.4')\n+def test_plot_model():\n+    model = Sequential()\n+    model.add(Conv2D(filters=2, kernel_size=(2, 3), input_shape=(3, 5, 5), name='conv'))\n+    model.add(Flatten(name='flat'))\n+    model.add(Dense(5, name='dense1'))\n+    vis_utils.plot_model(model, to_file='model1.png', show_layer_names=False)\n+    os.remove('model1.png')\n+\n+    model = Sequential()\n+    model.add(LSTM(16, return_sequences=True, input_shape=(2, 3), name='lstm'))\n+    model.add(TimeDistributed(Dense(5, name='dense2')))\n+    vis_utils.plot_model(model, to_file='model2.png', show_shapes=True)\n+    os.remove('model2.png')\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
