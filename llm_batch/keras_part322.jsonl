{"custom_id": "keras#93da10007c43448cd2671bfe529796c601cff9aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 270 | Lines Deleted: 46 | Files Changed: 12 | Hunks: 45 | Methods Changed: 36 | Complexity Δ (Sum/Max): 39/21 | Churn Δ: 316 | Churn Cumulative: 21190 | Contributors (this commit): 94 | Commits (past 90d): 80 | Contributors (cumulative): 118 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1980,6 +1980,11 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n   * Activation histograms\n   * Sampled profiling\n \n+  When used in `Model.evaluate`, in addition to epoch summaries, there will be\n+  a summary that records evaluation metrics vs `Model.optimizer.iterations`\n+  written. The metric names will be prepended with `evaluation`, with\n+  `Model.optimizer.iterations` being the step in the visualized TensorBoard.\n+\n   If you have installed TensorFlow with pip, you should be able\n   to launch TensorBoard from the command line:\n \n@@ -2353,6 +2358,13 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     self._push_writer(self._val_writer, self._val_step)\n \n   def on_test_end(self, logs=None):\n+    if self.model.optimizer and hasattr(self.model.optimizer, 'iterations'):\n+      with tf.summary.record_if(True), self._val_writer.as_default():\n+        for name, value in logs.items():\n+          tf.summary.scalar(\n+              'evaluation_' + name + '_vs_iterations',\n+              value,\n+              step=self.model.optimizer.iterations.read_value())\n     self._pop_writer()\n \n   def _implements_train_batch_hooks(self):\n\n@@ -1962,6 +1962,8 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         summary_file.scalars, {\n             _ObservedSummary(logdir=train_dir, tag='epoch_loss'),\n             _ObservedSummary(logdir=validation_dir, tag='epoch_loss'),\n+            _ObservedSummary(\n+                logdir=validation_dir, tag='evaluation_loss_vs_iterations'),\n         })\n \n   def test_TensorBoard_basic(self):\n@@ -1982,6 +1984,9 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         summary_file.scalars, {\n             _ObservedSummary(logdir=self.train_dir, tag='epoch_loss'),\n             _ObservedSummary(logdir=self.validation_dir, tag='epoch_loss'),\n+            _ObservedSummary(\n+                logdir=self.validation_dir,\n+                tag='evaluation_loss_vs_iterations'),\n         })\n \n   def test_TensorBoard_across_invocations(self):\n@@ -2007,6 +2012,9 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         summary_file.scalars, {\n             _ObservedSummary(logdir=self.train_dir, tag='epoch_loss'),\n             _ObservedSummary(logdir=self.validation_dir, tag='epoch_loss'),\n+            _ObservedSummary(\n+                logdir=self.validation_dir,\n+                tag='evaluation_loss_vs_iterations'),\n         })\n \n   def test_TensorBoard_no_spurious_event_files(self):\n@@ -2047,6 +2055,9 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n             _ObservedSummary(logdir=self.train_dir, tag='batch_loss'),\n             _ObservedSummary(logdir=self.train_dir, tag='epoch_loss'),\n             _ObservedSummary(logdir=self.validation_dir, tag='epoch_loss'),\n+            _ObservedSummary(\n+                logdir=self.validation_dir,\n+                tag='evaluation_loss_vs_iterations'),\n         },\n     )\n \n@@ -2128,6 +2139,9 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         {\n             _ObservedSummary(logdir=self.train_dir, tag='epoch_loss'),\n             _ObservedSummary(logdir=self.validation_dir, tag='epoch_loss'),\n+            _ObservedSummary(\n+                logdir=self.validation_dir,\n+                tag='evaluation_loss_vs_iterations'),\n         },\n     )\n     self.assertEqual(\n@@ -2159,6 +2173,9 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         {\n             _ObservedSummary(logdir=self.train_dir, tag='epoch_loss'),\n             _ObservedSummary(logdir=self.validation_dir, tag='epoch_loss'),\n+            _ObservedSummary(\n+                logdir=self.validation_dir,\n+                tag='evaluation_loss_vs_iterations'),\n         },\n     )\n     self.assertEqual(\n@@ -2258,6 +2275,9 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         {\n             _ObservedSummary(logdir=self.train_dir, tag='epoch_loss'),\n             _ObservedSummary(logdir=self.validation_dir, tag='epoch_loss'),\n+            _ObservedSummary(\n+                logdir=self.validation_dir,\n+                tag='evaluation_loss_vs_iterations'),\n             _ObservedSummary(logdir=self.train_dir, tag='batch_loss'),\n             _ObservedSummary(\n                 logdir=self.train_dir,\n\n@@ -72,8 +72,8 @@ class SidecarEvaluator(object):\n       data=data,\n       checkpoint_dir='/tmp/checkpoint_dir',  # dir for training-saved checkpoint\n       steps=None,  # Eval until dataset is exhausted\n-      log_dir='/tmp/log_dir',\n-      max_evaluations=None  # The evaluation needs to be stopped manually\n+      max_evaluations=None,  # The evaluation needs to be stopped manually\n+      callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/tmp/log_dir')]\n   ).start()\n   ```\n \n@@ -81,7 +81,7 @@ class SidecarEvaluator(object):\n   files which can be visualized by tensorboard (which provides a webpage link):\n \n   ```bash\n-  $ tensorboard --logdir=/tmp/log_dir\n+  $ tensorboard --logdir=/tmp/log_dir/validation\n   ...\n   TensorBoard 2.4.0a0 at http://host:port (Press CTRL+C to quit)\n   ```\n@@ -103,9 +103,9 @@ class SidecarEvaluator(object):\n                model,\n                data,\n                checkpoint_dir,\n-               log_dir=None,\n                steps=None,\n-               max_evaluations=None):\n+               max_evaluations=None,\n+               callbacks=None):\n     \"\"\"Initializes an `SidecarEvaluator` object.\n \n     Args:\n@@ -117,7 +117,6 @@ class SidecarEvaluator(object):\n         types that Keras `model.evaluate` supports as the input data `x`, such\n         as a `tf.data.Dataset`.\n       checkpoint_dir: Directory where checkpoint files are saved.\n-      log_dir: Directory where summary files for TensorBoard are saved.\n       steps: Number of steps to perform evaluation for, when evaluating a single\n         checkpoint file. If `None`, evaluation continues until the dataset is\n         exhausted. For repeated evaluation dataset, user must specify `steps` to\n@@ -136,21 +135,19 @@ class SidecarEvaluator(object):\n         checkpoints may be skipped if evaluation is slower than checkpoint\n         creation. If `None`, `SidecarEvaluator` will evaluate indefinitely, and\n         the user must terminate evaluator program themselves.\n+      callbacks: List of `keras.callbacks.Callback` instances to apply during\n+        evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks).\n     \"\"\"\n     self.model = model\n     self.data = data\n     self.checkpoint_dir = checkpoint_dir\n-    if log_dir:\n-      self._summary_writer = tf.summary.create_file_writer(\n-          logdir=log_dir)\n-    else:\n-      self._summary_writer = None\n     self._iterations = tf.Variable(\n         name='iterations',\n         initial_value=_ITERATIONS_UNINITIALIZED,\n         dtype=tf.int64)\n     self.max_evaluations = max_evaluations\n     self.steps = steps\n+    self.callbacks = callbacks or []\n \n   def start(self):\n     \"\"\"Starts the evaluation loop.\"\"\"\n@@ -179,6 +176,12 @@ class SidecarEvaluator(object):\n             if re.match(r'^layer_with_weights-[\\d+]', attribute) is not None:\n               self.model.load_weights(latest_checkpoint)\n               break\n+        else:\n+          # The model checkpoint might not include optimizer in cases, e.g.\n+          # using a custom training loop. Directly assign the iterations\n+          # property to be used in callbacks.\n+          if self.model.optimizer:\n+            self.model.optimizer.iterations.assign(self._iterations)\n       except (tf.errors.OpError,) as e:\n         # A couple errors can happen here with the coordinator racing to write\n         # checkpoint:\n@@ -202,8 +205,7 @@ class SidecarEvaluator(object):\n           'Evaluation starts: Model weights loaded from latest '\n           'checkpoint file: %s.', latest_checkpoint)\n \n-      # TODO(rchao): Support arbitrary callback for extensibility.\n-      self.model.evaluate(self.data, steps=self.steps)\n+      self.model.evaluate(self.data, steps=self.steps, callbacks=self.callbacks)\n \n       logging.info(\n           'End of evaluation. Metrics: %s', ' '.join([\n@@ -212,14 +214,6 @@ class SidecarEvaluator(object):\n               for metric in self.model.metrics\n           ]))\n \n-      if self._summary_writer:\n-        with tf.summary.record_if(True), self._summary_writer.as_default():\n-          for metric in self.model.metrics:\n-            tf.summary.scalar(\n-                metric.name,\n-                metric.result(),\n-                step=self._iterations.read_value())\n-\n       # TODO(rchao): Make the max evaluation robust in case users save the\n       # checkpoints with epoch format {epoch:03d}.\n       if (self.max_evaluations and\n\n@@ -54,13 +54,17 @@ class SidecarEvaluatorTest(tf.test.TestCase):\n     # Asserts the content of the summary file.\n     event_pb_written = False\n     event_tags = []\n+    for summary_file in summary_files:\n       for event_pb in tf.compat.v1.train.summary_iterator(\n-        os.path.join(log_dir, summary_files[0])):\n+          os.path.join(log_dir, summary_file)):\n         if event_pb.step > 0:\n           self.assertEqual(event_pb.step, 32)\n           event_tags.append(event_pb.summary.value[0].tag)\n           event_pb_written = True\n-    self.assertCountEqual(event_tags, ['categorical_accuracy', 'loss'])\n+    self.assertCountEqual(event_tags, [\n+        'evaluation_categorical_accuracy_vs_iterations',\n+        'evaluation_loss_vs_iterations'\n+    ])\n \n     # Verifying at least one non-zeroth step is written to summary.\n     self.assertTrue(event_pb_written)\n@@ -83,7 +87,7 @@ class SidecarEvaluatorTest(tf.test.TestCase):\n     checkpoint_manager.save()\n \n     sidecar_evaluator = sidecar_evaluator_lib.SidecarEvaluator(\n-        model, data=None, checkpoint_dir=checkpoint_dir, log_dir=None)\n+        model, data=None, checkpoint_dir=checkpoint_dir)\n     with self.assertRaisesRegexp(\n         RuntimeError, '`iterations` cannot be loaded '\n         'from the checkpoint file.'):\n@@ -119,14 +123,14 @@ class SidecarEvaluatorTest(tf.test.TestCase):\n         eval_model,\n         data=dataset,\n         checkpoint_dir=checkpoint_dir,\n-        log_dir=log_dir,\n-        max_evaluations=1).start()\n+        max_evaluations=1,\n+        callbacks=[keras.callbacks.TensorBoard(log_dir=log_dir)]).start()\n     # Eval model has been restored to the same state as the original model, so\n     # their weights should match. If not, restoration of the model didn't\n     # work.\n     self.assertModelsSameVariables(model, eval_model)\n \n-    self.assertSummaryEventsWritten(log_dir)\n+    self.assertSummaryEventsWritten(os.path.join(log_dir, 'validation'))\n \n   def testSidecarEvaluatorOutputsSummarySavedWithCallback(self):\n     checkpoint_dir = os.path.join(self.get_temp_dir(), 'checkpoints')\n@@ -153,8 +157,8 @@ class SidecarEvaluatorTest(tf.test.TestCase):\n         eval_model,\n         data=dataset,\n         checkpoint_dir=checkpoint_dir,\n-        log_dir=log_dir,\n-        max_evaluations=1)\n+        max_evaluations=1,\n+        callbacks=[keras.callbacks.TensorBoard(log_dir=log_dir)])\n     sidecar_evaluator.start()\n \n     # Eval model has been restored to the same state as the original model, so\n@@ -165,7 +169,7 @@ class SidecarEvaluatorTest(tf.test.TestCase):\n     # check the iterations is restored.\n     self.assertEqual(sidecar_evaluator._iterations.numpy(), _BATCH_SIZE)\n \n-    self.assertSummaryEventsWritten(log_dir)\n+    self.assertSummaryEventsWritten(os.path.join(log_dir, 'validation'))\n \n \n if __name__ == '__main__':\n\n@@ -2954,12 +2954,15 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     self.__class__._call_accepts_kwargs.fget.cache.pop(self, None)\n \n     call_fn_args = self._call_fn_args\n+    call_fn_args += self._call_full_argspec.kwonlyargs or []\n     self._expects_training_arg = ('training' in call_fn_args or\n                                   self._call_accepts_kwargs)\n     # The default training arg will be any (non-None) default specified in the\n     # method signature, or None if no value is specified.\n-    self._default_training_arg = self._call_fn_arg_defaults.get(\n-        'training')\n+    call_fn_arg_defaults = self._call_fn_arg_defaults.copy()\n+    call_fn_arg_defaults.update(self._call_full_argspec.kwonlydefaults or {})\n+    self._default_training_arg = call_fn_arg_defaults.get('training')\n+\n     self._expects_mask_arg = ('mask' in call_fn_args or\n                               self._call_accepts_kwargs)\n \n\n@@ -736,6 +736,88 @@ class BaseLayerTest(keras_parameterized.TestCase):\n         else:\n           return self._nested_layer(inputs) * 0.5\n \n+    self._test_custom_layer_training_arg(\n+        CustomLayerNoTrainingArg=CustomLayerNoTrainingArg,\n+        CustomLayerDefaultTrainingMissing=CustomLayerDefaultTrainingMissing,\n+        CustomLayerDefaultTrainingNone=CustomLayerDefaultTrainingNone,\n+        CustomLayerDefaultTrainingFalse=CustomLayerDefaultTrainingFalse,\n+        CustomLayerDefaultTrainingTrue=CustomLayerDefaultTrainingTrue)\n+\n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def test_custom_layer_training_arg_kwargonly(self):\n+    class CustomLayerNoTrainingArg(base_layer.Layer):\n+\n+      def __init__(self, nested_layer=None):\n+        super(CustomLayerNoTrainingArg, self).__init__()\n+        self._nested_layer = nested_layer or tf.identity\n+\n+      def call(self, inputs):\n+        return self._nested_layer(inputs)\n+\n+    class CustomLayerDefaultTrainingMissing(base_layer.Layer):\n+\n+      def __init__(self, nested_layer=None):\n+        super(CustomLayerDefaultTrainingMissing, self).__init__()\n+        self._nested_layer = nested_layer or tf.identity\n+\n+      def call(self, inputs, *, training):\n+        if training:\n+          return self._nested_layer(inputs)\n+        else:\n+          return self._nested_layer(inputs) * 0.5\n+\n+    class CustomLayerDefaultTrainingNone(base_layer.Layer):\n+\n+      def __init__(self, nested_layer=None):\n+        super(CustomLayerDefaultTrainingNone, self).__init__()\n+        self._nested_layer = nested_layer or tf.identity\n+\n+      def call(self, inputs, *, training=None):\n+        if training:\n+          return self._nested_layer(inputs)\n+        else:\n+          return self._nested_layer(inputs) * 0.5\n+\n+    class CustomLayerDefaultTrainingFalse(base_layer.Layer):\n+\n+      def __init__(self, nested_layer=None):\n+        super(CustomLayerDefaultTrainingFalse, self).__init__()\n+        self._nested_layer = nested_layer or tf.identity\n+\n+      def call(self, inputs, *, training=False):\n+        if training:\n+          return self._nested_layer(inputs)\n+        else:\n+          return self._nested_layer(inputs) * 0.5\n+\n+    class CustomLayerDefaultTrainingTrue(base_layer.Layer):\n+\n+      def __init__(self, nested_layer=None):\n+        super(CustomLayerDefaultTrainingTrue, self).__init__()\n+        self._nested_layer = nested_layer or tf.identity\n+\n+      def call(self, inputs, *, training=True):\n+        if training:\n+          return self._nested_layer(inputs)\n+        else:\n+          return self._nested_layer(inputs) * 0.5\n+\n+    self._test_custom_layer_training_arg(\n+        CustomLayerNoTrainingArg=CustomLayerNoTrainingArg,\n+        CustomLayerDefaultTrainingMissing=CustomLayerDefaultTrainingMissing,\n+        CustomLayerDefaultTrainingNone=CustomLayerDefaultTrainingNone,\n+        CustomLayerDefaultTrainingFalse=CustomLayerDefaultTrainingFalse,\n+        CustomLayerDefaultTrainingTrue=CustomLayerDefaultTrainingTrue)\n+\n+  def _test_custom_layer_training_arg(self,\n+                                      # pylint: disable=invalid-name\n+                                      CustomLayerNoTrainingArg,\n+                                      CustomLayerDefaultTrainingMissing,\n+                                      CustomLayerDefaultTrainingNone,\n+                                      CustomLayerDefaultTrainingFalse,\n+                                      CustomLayerDefaultTrainingTrue,\n+                                      # pylint: enable=invalid-name\n+                                      ):\n     x = tf.ones(shape=(1, 1))\n \n     # If the layer signature doesn't specify a default training arg,\n\n@@ -114,8 +114,7 @@ class _BaseFeaturesLayer(Layer):\n \n   def _verify_and_concat_tensors(self, output_tensors):\n     \"\"\"Verifies and concatenates the dense output of several columns.\"\"\"\n-    feature_column_v2._verify_static_batch_size_equality(  # pylint: disable=protected-access\n-        output_tensors, self._feature_columns)\n+    _verify_static_batch_size_equality(output_tensors, self._feature_columns)\n     return tf.concat(output_tensors, -1)\n \n   def get_config(self):\n@@ -148,3 +147,30 @@ def _sanitize_column_name_for_variable_scope(name):\n   \"\"\"Sanitizes user-provided feature names for use as variable scopes.\"\"\"\n   invalid_char = re.compile('[^A-Za-z0-9_.\\\\-]')\n   return invalid_char.sub('_', name)\n+\n+\n+def _verify_static_batch_size_equality(tensors, columns):\n+  \"\"\"Verify equality between static batch sizes.\n+\n+  Args:\n+    tensors: iterable of input tensors.\n+    columns: Corresponding feature columns.\n+\n+  Raises:\n+    ValueError: in case of mismatched batch sizes.\n+  \"\"\"\n+  expected_batch_size = None\n+  for i in range(0, len(tensors)):\n+    # bath_size is a Dimension object.\n+    batch_size = tf.compat.v1.Dimension(tf.compat.dimension_value(\n+        tensors[i].shape[0]))\n+    if batch_size.value is not None:\n+      if expected_batch_size is None:\n+        bath_size_column_index = i\n+        expected_batch_size = batch_size\n+      elif not expected_batch_size.is_compatible_with(batch_size):\n+        raise ValueError(\n+            'Batch size (first dimension) of each feature must be same. '\n+            'Batch size of columns ({}, {}): ({}, {})'.format(\n+                columns[bath_size_column_index].name, columns[i].name,\n+                expected_batch_size, batch_size))\n\n@@ -23,6 +23,7 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.feature_column import feature_column_v2 as fc\n from keras.feature_column import base_feature_layer as kfc\n from keras.feature_column import dense_features\n+from keras.utils import tf_contextlib\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -86,7 +87,7 @@ class DenseFeatures(dense_features.DenseFeatures):\n         trainable=trainable,\n         name=name,\n         **kwargs)\n-    self._state_manager = fc._StateManagerImplV2(self, self.trainable)  # pylint: disable=protected-access\n+    self._state_manager = _StateManagerImplV2(self, self.trainable)\n \n   def build(self, _):\n     for column in self._feature_columns:\n@@ -95,3 +96,65 @@ class DenseFeatures(dense_features.DenseFeatures):\n     # We would like to call Layer.build and not _DenseFeaturesHelper.build.\n     # pylint: disable=protected-access\n     super(kfc._BaseFeaturesLayer, self).build(None)  # pylint: disable=bad-super-call\n+\n+\n+class _StateManagerImplV2(fc._StateManagerImpl):  # pylint: disable=protected-access\n+  \"\"\"Manages the state of DenseFeatures.\"\"\"\n+\n+  def create_variable(self,\n+                      feature_column,\n+                      name,\n+                      shape,\n+                      dtype=None,\n+                      trainable=True,\n+                      use_resource=True,\n+                      initializer=None):\n+    if name in self._cols_to_vars_map[feature_column]:\n+      raise ValueError('Variable already exists.')\n+\n+    # We explicitly track these variables since `name` is not guaranteed to be\n+    # unique and disable manual tracking that the add_weight call does.\n+    with no_manual_dependency_tracking_scope(self._layer):\n+      var = self._layer.add_weight(\n+          name=name,\n+          shape=shape,\n+          dtype=dtype,\n+          initializer=initializer,\n+          trainable=self._trainable and trainable,\n+          use_resource=use_resource)\n+    if isinstance(var, tf.__internal__.tracking.Trackable):\n+      self._layer._track_trackable(var, feature_column.name + '/' + name)  # pylint: disable=protected-access\n+    self._cols_to_vars_map[feature_column][name] = var\n+    return var\n+\n+\n+@tf_contextlib.contextmanager\n+def no_manual_dependency_tracking_scope(obj):\n+  \"\"\"A context that disables manual dependency tracking for the given `obj`.\n+\n+  Sometimes library methods might track objects on their own and we might want\n+  to disable that and do the tracking on our own. One can then use this context\n+  manager to disable the tracking the library method does and do your own\n+  tracking.\n+\n+  For example:\n+\n+  class TestLayer(tf.keras.Layer):\n+    def build():\n+      with no_manual_dependency_tracking_scope(self):\n+        var = self.add_variable(\"name1\")  # Creates a var and doesn't track it\n+      self._track_trackable(\"name2\", var)  # We track variable with name `name2`\n+\n+  Args:\n+    obj: A trackable object.\n+\n+  Yields:\n+    a scope in which the object doesn't track dependencies manually.\n+  \"\"\"\n+  # pylint: disable=protected-access\n+  previous_value = getattr(obj, '_manual_tracking', True)\n+  obj._manual_tracking = False\n+  try:\n+    yield\n+  finally:\n+    obj._manual_tracking = previous_value\n\n@@ -162,8 +162,8 @@ class SequenceFeatures(kfc._BaseFeaturesLayer):\n         sequence_lengths.append(sequence_length)\n \n     # Check and process sequence lengths.\n-    fc._verify_static_batch_size_equality(sequence_lengths,\n-                                          self._feature_columns)\n+    kfc._verify_static_batch_size_equality(    # pylint: disable=protected-access\n+        sequence_lengths, self._feature_columns)\n     sequence_length = _assert_all_equal_and_return(sequence_lengths)\n \n     return self._verify_and_concat_tensors(output_tensors), sequence_length\n\n@@ -374,6 +374,20 @@ class MultiHeadAttention(Layer):\n       # These computations could be wrapped into the keras attention layer once\n       # it support mult-head einsum computations.\n       self._build_attention(output_rank)\n+      self._output_dense = self._make_output_dense(\n+          free_dims, common_kwargs, \"attention_output\")\n+\n+  def _make_output_dense(self, free_dims, common_kwargs, name=None):\n+    \"\"\"Builds the output projection matrix.\n+\n+    Args:\n+      free_dims: Number of free dimensions for einsum equation building.\n+      common_kwargs: Common keyword arguments for einsum layer.\n+      name: the name for the projection layer.\n+\n+    Returns:\n+      Projection layer.\n+    \"\"\"\n     if self._output_shape:\n       if not isinstance(self._output_shape, collections.abc.Sized):\n         output_shape = [self._output_shape]\n@@ -383,11 +397,11 @@ class MultiHeadAttention(Layer):\n       output_shape = [self._query_shape[-1]]\n     einsum_equation, bias_axes, output_rank = _build_proj_equation(\n         free_dims, bound_dims=2, output_dims=len(output_shape))\n-      self._output_dense = einsum_dense.EinsumDense(\n+    return einsum_dense.EinsumDense(\n         einsum_equation,\n         output_shape=_get_output_shape(output_rank - 1, output_shape),\n         bias_axes=bias_axes if self._use_bias else None,\n-          name=\"attention_output\",\n+        name=name,\n         **common_kwargs)\n \n   def _build_attention(self, rank):\n\n@@ -156,11 +156,13 @@ class RNNSavedModelSaver(LayerSavedModelSaver):\n         super(RNNSavedModelSaver, self)._get_serialized_attributes_internal(\n             serialization_cache))\n     states = data_structures.wrap_or_unwrap(self.obj.states)\n-    # Force the tuple into TupleWrapper which is a trackable object. The\n-    # save/load code requires all the objects to be trackable.\n-    # Tuple is not converted to TupleWrapper by data_structures.wrap_or_unwrap()\n-    # if it doesn't contains any trackable objects.\n+    # SaveModel require all the objects to be Trackable when saving.\n+    # If the states is still a tuple after wrap_or_unwrap, it means it doesn't\n+    # contain any trackable item within it, eg empty tuple or (None, None) for\n+    # stateless ConvLSTM2D. We convert them to list so that wrap_or_unwrap can\n+    # make it a Trackable again for saving. When loaded, ConvLSTM2D is\n+    # able to handle the tuple/list conversion.\n     if isinstance(states, tuple):\n-      states = data_structures._TupleWrapper(states)  # pylint: disable=protected-access\n+      states = data_structures.wrap_or_unwrap(list(states))\n     objects['states'] = states\n     return objects, functions\n\n@@ -803,13 +803,15 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     self.assertAllClose(layer.states, loaded_layer.states)\n     self.assertAllClose(model(input_arr), loaded(input_arr))\n \n-  def testSaveStatelessConvLSTM2D(self):\n+  @parameterized.named_parameters([('stateful', True), ('stateless', False)])\n+  def testSaveConvLSTM2D(self, stateful):\n     data_format = 'channels_first'\n     batch, timesteps, channels, rows, cols = 12, 10, 8, 4, 4\n     input_arr = np.ones(\n         (batch, timesteps, channels, rows, cols)).astype('float32')\n     layer = keras.layers.ConvLSTM2D(\n-        filters=16, kernel_size=(1, 1), data_format=data_format)\n+        filters=16, kernel_size=(1, 1), data_format=data_format,\n+        stateful=stateful)\n     x = keras.Input(batch_shape=(batch, timesteps, channels, rows, cols))\n     y = layer(x)\n     model = keras.Model(x, y)\n@@ -820,6 +822,8 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     del model\n \n     loaded = keras_load.load(saved_model_dir)\n+    if stateful:\n+      loaded.reset_states()\n     predict_2 = loaded(input_arr)\n     self.assertAllClose(predict_1, predict_2)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b31e462e47486e4bb61a3166c121118d9c4f85e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 1693 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -546,8 +546,8 @@ class CompositeTensorDataAdapter(DataAdapter):\n       # Dataset/iterator inherits from CompositeTensor but should be handled\n       # by DatasetAdapter and GeneratorAdapter.\n       if (tf_utils.is_extension_type(v) and\n-          not isinstance(v, (tf.data.Dataset,\n-                             tf.data.Iterator))):\n+          not isinstance(v, (tf.data.Dataset, tf.data.Iterator)) and\n+          not _is_distributed_dataset(v)):\n         return True\n       # Support Scipy sparse tensors if scipy is installed\n       if scipy_sparse is not None and scipy_sparse.issparse(v):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#db955d02720a415605a4060c8f41222ae06d9065", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 36 | Files Changed: 7 | Hunks: 25 | Methods Changed: 11 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 64 | Churn Cumulative: 6554 | Contributors (this commit): 25 | Commits (past 90d): 42 | Contributors (cumulative): 37 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,7 +22,6 @@ import tensorflow.compat.v2 as tf\n \n import json\n import os\n-import sys\n \n from absl.testing import parameterized\n from tensorflow.python.distribute import multi_worker_test_base as test_base\n@@ -31,11 +30,6 @@ from keras.distribute import distributed_file_utils\n from keras.distribute import multi_worker_testing_utils\n \n \n-def _is_oss():\n-  \"\"\"Returns whether the test is run under OSS.\"\"\"\n-  return len(sys.argv) >= 1 and 'bazel' in sys.argv[0]\n-\n-\n def checkpoint_exists(filepath):\n   \"\"\"Returns whether the checkpoint `filepath` refers to exists.\"\"\"\n   if filepath.endswith('.h5'):\n@@ -184,8 +178,6 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n \n     def proc_model_checkpoint_works_with_same_file_path(test_obj,\n                                                         saving_filepath):\n-      if _is_oss():\n-        test_obj.skipTest('TODO(b/170838633): Failing in OSS')\n       model, _, train_ds, steps = _model_setup(test_obj, file_format='')\n       num_epoch = 4\n \n\n@@ -543,10 +543,11 @@ class CompositeTensorDataAdapter(DataAdapter):\n       flat_inputs += tf.nest.flatten(y)\n \n     def _is_composite(v):\n-      # Dataset/iterator inherits from CompositeTensor but should be handled\n-      # by DatasetAdapter and GeneratorAdapter.\n+      # Dataset/iterator/DistributedDataset inherits from CompositeTensor but\n+      # should be handled by DatasetAdapter and GeneratorAdapter.\n       if (tf_utils.is_extension_type(v) and\n-          not isinstance(v, (tf.data.Dataset, tf.data.Iterator)) and\n+          not isinstance(v,\n+                         (tf.data.Dataset, tf.data.Iterator)) and\n           not _is_distributed_dataset(v)):\n         return True\n       # Support Scipy sparse tensors if scipy is installed\n\n@@ -403,7 +403,7 @@ class Bidirectional(Wrapper):\n       automatically.\n       Note that the provided `backward_layer` layer should have properties\n       matching those of the `layer` argument, in particular it should have the\n-      same values for `stateful`, `return_states`, `return_sequence`, etc.\n+      same values for `stateful`, `return_states`, `return_sequences`, etc.\n       In addition, `backward_layer` and `layer` should have different\n       `go_backwards` argument values.\n       A `ValueError` will be raised if these requirements are not met.\n\n@@ -26,7 +26,6 @@ from keras.saving.saved_model import constants\n from keras.saving.saved_model import save_impl\n from keras.saving.saved_model import serialized_attributes\n from keras.utils import generic_utils\n-from tensorflow.python.training.tracking import data_structures\n \n \n class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n@@ -155,7 +154,7 @@ class RNNSavedModelSaver(LayerSavedModelSaver):\n     objects, functions = (\n         super(RNNSavedModelSaver, self)._get_serialized_attributes_internal(\n             serialization_cache))\n-    states = data_structures.wrap_or_unwrap(self.obj.states)\n+    states = tf.__internal__.tracking.wrap(self.obj.states)\n     # SaveModel require all the objects to be Trackable when saving.\n     # If the states is still a tuple after wrap_or_unwrap, it means it doesn't\n     # contain any trackable item within it, eg empty tuple or (None, None) for\n@@ -163,6 +162,6 @@ class RNNSavedModelSaver(LayerSavedModelSaver):\n     # make it a Trackable again for saving. When loaded, ConvLSTM2D is\n     # able to handle the tuple/list conversion.\n     if isinstance(states, tuple):\n-      states = data_structures.wrap_or_unwrap(list(states))\n+      states = tf.__internal__.tracking.wrap(list(states))\n     objects['states'] = states\n     return objects, functions\n\n@@ -18,10 +18,11 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import tensorflow.compat.v2 as tf\n+\n from keras.saving.saved_model import constants\n from keras.saving.saved_model import layer_serialization\n from keras.utils import generic_utils\n-from tensorflow.python.training.tracking import data_structures\n \n \n class MetricSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n@@ -42,6 +43,6 @@ class MetricSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n     return metadata\n \n   def _get_serialized_attributes_internal(self, unused_serialization_cache):\n-    return (dict(variables=data_structures.wrap_or_unwrap(self.obj.variables)),\n+    return (dict(variables=tf.__internal__.tracking.wrap(self.obj.variables)),\n             dict())  # TODO(b/135550038): save functions to enable saving\n                      # custom metrics.\n\n@@ -42,7 +42,6 @@ from keras.utils import version_utils\n from keras.utils.generic_utils import LazyLoader\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training.tracking import base as trackable\n-from tensorflow.python.training.tracking import data_structures\n \n # To avoid circular dependencies between keras/engine and keras/saving,\n # code in keras/saving must delay imports.\n@@ -111,19 +110,19 @@ def wrap_layer_objects(layer, serialization_cache):\n   wrapped_layer_losses = [keras_loss_cache[fn]\n                           for fn in layer._callable_losses[:]]  # pylint: disable=protected-access\n \n-  layer_metrics = data_structures.wrap_or_unwrap(\n+  layer_metrics = tf.__internal__.tracking.wrap(\n       {m.name: m for m in layer._metrics})  # pylint: disable=protected-access\n   return dict(\n-      variables=data_structures.wrap_or_unwrap(layer.variables),\n-      trainable_variables=data_structures.wrap_or_unwrap(\n+      variables=tf.__internal__.tracking.wrap(layer.variables),\n+      trainable_variables=tf.__internal__.tracking.wrap(\n           layer.trainable_variables),\n-      non_trainable_variables=data_structures.wrap_or_unwrap(\n+      non_trainable_variables=tf.__internal__.tracking.wrap(\n           layer.non_trainable_variables),\n-      layers=data_structures.wrap_or_unwrap(utils.list_all_layers(layer)),\n-      metrics=data_structures.wrap_or_unwrap(layer.metrics),\n-      regularization_losses=data_structures.wrap_or_unwrap(\n+      layers=tf.__internal__.tracking.wrap(utils.list_all_layers(layer)),\n+      metrics=tf.__internal__.tracking.wrap(layer.metrics),\n+      regularization_losses=tf.__internal__.tracking.wrap(\n           wrapped_loss_functions),\n-      layer_regularization_losses=data_structures.wrap_or_unwrap(\n+      layer_regularization_losses=tf.__internal__.tracking.wrap(\n           wrapped_layer_losses),\n       layer_metrics=layer_metrics)\n   # pylint: disable=protected-access\n\n@@ -38,7 +38,7 @@ class HasList(training.Model):\n \n   def __init__(self):\n     super(HasList, self).__init__()\n-    self.layer_list = data_structures.wrap_or_unwrap([core.Dense(3)])\n+    self.layer_list = tf.__internal__.tracking.wrap([core.Dense(3)])\n     self.layer_list.append(core.Dense(4))\n     self.layer_list.extend(\n         [core.Dense(5),\n@@ -48,12 +48,12 @@ class HasList(training.Model):\n         core.Dense(8)\n     ]\n     self.layer_list += (\n-        data_structures.wrap_or_unwrap([core.Dense(9)]) +\n-        data_structures.wrap_or_unwrap([core.Dense(10)]))\n+        tf.__internal__.tracking.wrap([core.Dense(9)]) +\n+        tf.__internal__.tracking.wrap([core.Dense(10)]))\n     self.layer_list.extend(\n-        data_structures.wrap_or_unwrap(\n+        tf.__internal__.tracking.wrap(\n             list([core.Dense(11)]) + [core.Dense(12)]))\n-    self.layers_with_updates = data_structures.wrap_or_unwrap(\n+    self.layers_with_updates = tf.__internal__.tracking.wrap(\n         [normalization.BatchNormalization()])\n \n   def call(self, x):\n@@ -225,7 +225,7 @@ class ListWrapperTest(tf.test.TestCase):\n \n   def testLayerCollectionWithExternalMutation(self):\n     l = []\n-    l_wrapper = data_structures.wrap_or_unwrap(l)\n+    l_wrapper = tf.__internal__.tracking.wrap(l)\n     layer = core.Dense(1)\n     l.append(layer)\n     self.assertEqual([layer], l_wrapper.layers)\n@@ -235,9 +235,9 @@ class HasMapping(training.Model):\n \n   def __init__(self):\n     super(HasMapping, self).__init__()\n-    self.layer_dict = data_structures.wrap_or_unwrap(dict(output=core.Dense(7)))\n-    self.layer_dict[\"norm\"] = data_structures.wrap_or_unwrap([])\n-    self.layer_dict[\"dense\"] = data_structures.wrap_or_unwrap([])\n+    self.layer_dict = tf.__internal__.tracking.wrap(dict(output=core.Dense(7)))\n+    self.layer_dict[\"norm\"] = tf.__internal__.tracking.wrap([])\n+    self.layer_dict[\"dense\"] = tf.__internal__.tracking.wrap([])\n     self.layer_dict[\"dense\"].extend(\n         [core.Dense(5),\n          core.Dense(6, kernel_regularizer=tf.reduce_sum)])\n@@ -293,7 +293,7 @@ class MappingTests(keras_parameterized.TestCase):\n   def testDictWrapperBadKeys(self):\n     a = tf.Module()\n     a.d = {}\n-    a.d[1] = data_structures.wrap_or_unwrap([])\n+    a.d[1] = tf.__internal__.tracking.wrap([])\n     model = training.Model()\n     model.sub = a\n     save_path = os.path.join(self.get_temp_dir(), \"ckpt\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4584ed2a120c18cca53ea7cf2a3764dd18421821", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 171 | Lines Deleted: 53 | Files Changed: 16 | Hunks: 47 | Methods Changed: 28 | Complexity Δ (Sum/Max): 17/4 | Churn Δ: 224 | Churn Cumulative: 24619 | Contributors (this commit): 10 | Commits (past 90d): 84 | Contributors (cumulative): 39 | DMM Complexity: 0.6777777777777778\n\nDIFF:\n@@ -28,13 +28,14 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import tensorflow as tf\n+\n import argparse\n import os\n import re\n import sys\n \n import six\n-import tensorflow as tf\n \n from google.protobuf import message\n from google.protobuf import text_format\n@@ -248,7 +249,7 @@ class ApiCompatibilityTest(tf.test.TestCase):\n         # Remove files.\n         for key in only_in_expected:\n           filepath = _KeyToFilePath(key, api_version)\n-          file_io.delete_file(filepath)\n+          tf.compat.v1.gfile.Remove(filepath)\n \n         # If the files are only in actual (current library), these are new\n         # modules. Write them to files. Also record all updates in files.\n@@ -286,7 +287,7 @@ class ApiCompatibilityTest(tf.test.TestCase):\n     proto_dict = visitor.GetProtos()\n \n     # Read all golden files.\n-    golden_file_list = file_io.get_matching_files(golden_file_patterns)\n+    golden_file_list = tf.compat.v1.gfile.Glob(golden_file_patterns)\n \n     def _ReadFileToProto(filename):\n       \"\"\"Read a filename, create a protobuf from its contents.\"\"\"\n\n@@ -548,7 +548,7 @@ class TestDistributionStrategyWithNumpyArrays(tf.test.TestCase,\n         return self.v2 + inp\n \n     with self.cached_session(), distribution.scope():\n-      layer = MyLayer(dtype=policy.Policy(policy_name))\n+      layer = MyLayer(dtype=policy_name)\n       def run_fn():\n         x = np.array([1.])\n         with tf.GradientTape() as tape:\n\n@@ -2324,6 +2324,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       self._dtype_policy = dtype\n     elif isinstance(dtype, dict):\n       self._dtype_policy = policy.deserialize(dtype)\n+    elif isinstance(dtype, str) and dtype in ('mixed_float16',\n+                                              'mixed_bfloat16'):\n+      # The isinstance check is required since np.dtype raises an error if\n+      # compared to a non-dtype string.\n+      self._dtype_policy = policy.Policy(dtype)\n     elif dtype:\n       self._dtype_policy = policy.Policy(tf.as_dtype(dtype).name)\n     else:\n\n@@ -1733,6 +1733,11 @@ class Layer(base_layer.Layer):\n       self._dtype_policy = dtype\n     elif isinstance(dtype, dict):\n       self._dtype_policy = policy.deserialize(dtype)\n+    elif isinstance(dtype, str) and dtype in ('mixed_float16',\n+                                              'mixed_bfloat16'):\n+      # The isinstance check is required since np.dtype raises an error if\n+      # compared to a non-dtype string.\n+      self._dtype_policy = policy.Policy(dtype)\n     elif dtype:\n       self._dtype_policy = policy.Policy(tf.as_dtype(dtype).name)\n     else:\n\n@@ -146,6 +146,10 @@ class LossesContainer(Container):\n     self._create_metrics()\n     self._built = True\n \n+  @property\n+  def built(self):\n+    return self._built\n+\n   def _create_metrics(self):\n     \"\"\"Creates per-output loss metrics, but only for multi-output Models.\"\"\"\n     if len(self._output_names) == 1:\n@@ -376,6 +380,10 @@ class MetricsContainer(Container):\n     self._create_ordered_metrics()\n     self._built = True\n \n+  @property\n+  def built(self):\n+    return self._built\n+\n   def _set_metric_names(self):\n     \"\"\"Sets unique metric names.\"\"\"\n     # For multi-output models, prepend the output name to the metric name.\n\n@@ -29,7 +29,6 @@ from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers import normalization\n from keras.layers import normalization_v2\n-from keras.mixed_precision import policy\n \n \n class BatchNormalizationTest(keras_parameterized.TestCase):\n@@ -159,7 +158,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n         axis=-1,\n         input_shape=(4, 4, 3),\n         momentum=0.8,\n-        dtype=policy.Policy('mixed_float16'))\n+        dtype='mixed_float16')\n     x = np.random.normal(size=(10, 4, 4, 3))\n     y = norm(x)\n     self.assertEqual(y.dtype, 'float16')\n@@ -174,7 +173,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n         axis=-1,\n         input_shape=(1, 1, 1),\n         fused=fused,\n-        dtype=policy.Policy('mixed_float16'))\n+        dtype='mixed_float16')\n     x = np.array([-1000., 1000.]).reshape((2, 1, 1, 1))\n     y = norm(x, training=True)\n     expected_y = np.array([-1.0, 1.0]).reshape((2, 1, 1, 1))\n\n@@ -141,6 +141,15 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     self._num_special_tokens = self.num_oov_indices\n     if self.mask_token is not None:\n       self._num_special_tokens += 1\n+    self._vocab_size = 0\n+    # We need to keep track our current vocab size outside of our layer weights\n+    # to support a static output shape when `output_mode != INT`. The bincount\n+    # ops do not set shape on their outputs, which means we have to set it\n+    # ourselves. We persist the current vocab size as a hidden part of the\n+    # config when serializing our model.\n+    if \"vocab_size\" in kwargs:\n+      self._vocab_size = kwargs[\"vocab_size\"]\n+      del kwargs[\"vocab_size\"]\n \n     # If there is only one OOV bucket, we can determine the OOV value (either 0\n     # or 1 depending on whether 0 is reserved) and set that as the default\n@@ -238,10 +247,13 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     tracked_table.shape = tf.TensorShape((0,))\n \n   def compute_output_shape(self, input_shape):\n-    if self.output_mode != INT:\n-      return tf.TensorShape([input_shape[0], self.max_tokens])\n-\n+    if self.output_mode == INT:\n       return input_shape\n+    if self._vocab_size and not self.pad_to_max_tokens:\n+      out_depth = self._vocab_size\n+    else:\n+      out_depth = self.max_tokens\n+    return tf.TensorShape([input_shape[0], out_depth])\n \n   def compute_output_signature(self, input_spec):\n     output_shape = self.compute_output_shape(input_spec.shape.as_list())\n@@ -285,7 +297,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     return tokens\n \n   def vocab_size(self):\n-    return int(self._table_handler.table_size()) + self.num_oov_indices\n+    return self._vocab_size\n \n   def get_config(self):\n     config = {\n@@ -296,6 +308,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         \"mask_token\": self.mask_token,\n         \"output_mode\": self.output_mode,\n         \"pad_to_max_tokens\": self.pad_to_max_tokens,\n+        \"vocab_size\": self._vocab_size\n     }\n     base_config = super(IndexLookup, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n@@ -411,12 +424,12 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n                        \"OOV token for this layer.\".format(\n                            self.oov_token, tokens.index(self.oov_token)))\n \n-    total_vocab_size = len(tokens) + self._num_special_tokens\n-    if self.max_tokens is not None and total_vocab_size > self.max_tokens:\n+    self._vocab_size = len(tokens) + self._num_special_tokens\n+    if self.max_tokens is not None and self._vocab_size > self.max_tokens:\n       raise ValueError(\n           \"Attempted to set a vocabulary larger than the maximum vocab size. \"\n           \"Passed vocab size is {}, max vocab size is {}.\".format(\n-              total_vocab_size, self.max_tokens))\n+              self._vocab_size, self.max_tokens))\n \n     if self.output_mode == TFIDF:\n       if idf_weights is None:\n@@ -470,13 +483,6 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n           constant_values=(front_padding_value, back_padding_value))\n       K.set_value(self.tf_idf_weights, idf_weights)\n \n-    # TODO(mattdangerw): we should not overwrite max_tokens. We currently\n-    # need to persist our vocab size in the config, and not our weights, to\n-    # force a static shape for our output. This could be avoided if the\n-    # underlying bincount ops set the shape of their output tensors.\n-    if not self.pad_to_max_tokens or self.max_tokens is None:\n-      self.max_tokens = total_vocab_size\n-\n   def _set_state_variables(self, updates):\n     if not self.built:\n       raise RuntimeError(\"_set_state_variables() must be called after build().\")\n@@ -484,7 +490,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         updates[_VOCAB_NAME], idf_weights=updates[_IDF_WEIGHTS_NAME])\n \n   def call(self, inputs):\n-    if not self.max_tokens:\n+    if not self.max_tokens and not self._vocab_size:\n       raise ValueError(\"You must set the layer's vocabulary before calling it. \"\n                        \"Either pass a `vocabulary` argument to the layer, or \"\n                        \"call `layer.adapt(dataset)` with some sample data.\")\n@@ -497,6 +503,9 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n       return lookup_result\n \n     binary_output = (self.output_mode == BINARY)\n+    if self._vocab_size and not self.pad_to_max_tokens:\n+      out_depth = self._vocab_size\n+    else:\n       out_depth = self.max_tokens\n     if self.sparse:\n       bincounts = category_encoding.sparse_bincount(lookup_result, out_depth,\n\n@@ -798,6 +798,44 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n+  def test_binary_output_hard_maximum_multiple_adapts(self):\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"earth\"],\n+                            [\"ohio\", \"and\", \"earth\", \"michigan\"]])\n+    adapt_data = [\"earth\", \"earth\", \"earth\", \"earth\", \"wind\", \"wind\", \"wind\"]\n+    first_expected_output = [\n+        [0, 1, 1, 1, 0],\n+        [0, 1, 1, 0, 0],\n+    ]\n+    second_adapt_data = [\n+        \"earth\", \"earth\", \"earth\", \"earth\", \"wind\", \"wind\", \"wind\", \"and\",\n+        \"and\", \"fire\"\n+    ]\n+    second_expected_output = [\n+        [0, 0, 1, 1, 1],\n+        [0, 1, 1, 0, 1],\n+    ]\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = get_layer_class()(\n+        max_tokens=5,\n+        num_oov_indices=1,\n+        mask_token=\"\",\n+        oov_token=\"[OOV]\",\n+        output_mode=index_lookup.BINARY,\n+        pad_to_max_tokens=True,\n+        dtype=tf.string)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+\n+    # Test the first adapt\n+    layer.adapt(adapt_data)\n+    first_output = model.predict(input_array)\n+    # Test the second adapt\n+    layer.adapt(second_adapt_data)\n+    second_output = model.predict(input_array)\n+    self.assertAllEqual(first_expected_output, first_output)\n+    self.assertAllEqual(second_expected_output, second_output)\n+\n   def test_binary_output_soft_maximum(self):\n     \"\"\"Check binary output when pad_to_max_tokens=False.\"\"\"\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n\n@@ -320,7 +320,13 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     self._output_mode = output_mode\n     self._output_sequence_length = output_sequence_length\n     self._pad_to_max = pad_to_max_tokens\n-    self._vocab_size = 0\n+    vocab_size = 0\n+    # IndexLookup needs to keep track the current vocab size outside of its\n+    # layer weights. We persist it as a hidden part of the config during\n+    # serialization.\n+    if \"vocab_size\" in kwargs:\n+      vocab_size = kwargs[\"vocab_size\"]\n+      del kwargs[\"vocab_size\"]\n \n     super(TextVectorization, self).__init__(\n         combiner=None,\n@@ -334,7 +340,8 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n         mask_token=mask_token,\n         vocabulary=vocabulary,\n         pad_to_max_tokens=pad_to_max_tokens,\n-        output_mode=output_mode if output_mode is not None else INT)\n+        output_mode=output_mode if output_mode is not None else INT,\n+        vocab_size=vocab_size)\n \n   def _get_index_lookup_class(self):\n     return string_lookup.StringLookup\n@@ -429,6 +436,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n         \"output_mode\": self._output_mode,\n         \"output_sequence_length\": self._output_sequence_length,\n         \"pad_to_max_tokens\": self._pad_to_max,\n+        \"vocab_size\": self._index_lookup_layer.vocab_size(),\n     }\n     base_config = super(TextVectorization, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -1071,6 +1071,42 @@ class TextVectorizationOutputTest(\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n+  def test_bag_output_hard_maximum_multiple_adapts(self):\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"earth\"],\n+                            [\"ohio\", \"and\", \"earth\", \"michigan\"]])\n+    adapt_data = [\"earth\", \"earth\", \"earth\", \"earth\", \"wind\", \"wind\", \"wind\"]\n+    first_expected_output = [\n+        [1, 1, 1, 0, 0],\n+        [1, 1, 0, 0, 0],\n+    ]\n+    second_adapt_data = [\n+        \"earth\", \"earth\", \"earth\", \"earth\", \"wind\", \"wind\", \"wind\", \"and\",\n+        \"and\", \"fire\"\n+    ]\n+    second_expected_output = [\n+        [0, 1, 1, 1, 0],\n+        [1, 1, 0, 1, 0],\n+    ]\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = get_layer_class()(\n+        max_tokens=5,\n+        standardize=None,\n+        split=None,\n+        output_mode=text_vectorization.BINARY,\n+        pad_to_max_tokens=True)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+\n+    # Test the first adapt\n+    layer.adapt(adapt_data)\n+    first_output = model.predict(input_array)\n+    # Test the second adapt\n+    layer.adapt(second_adapt_data)\n+    second_output = model.predict(input_array)\n+    self.assertAllEqual(first_expected_output, first_output)\n+    self.assertAllEqual(second_expected_output, second_output)\n+\n   def test_bag_output_soft_maximum_set_state_after_build(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"earth\"],\n\n@@ -149,7 +149,7 @@ class KerasLayerTest(keras_parameterized.TestCase):\n         return tf.cast(inputs, 'int32') + self.v\n \n     x = tf.constant([1.])\n-    layer = LayerWithIntVar(dtype=policy.Policy('mixed_float16'))\n+    layer = LayerWithIntVar(dtype='mixed_float16')\n     self.assertEqual(layer(x).dtype, 'int32')\n \n   @parameterized.named_parameters(*TESTCASES)\n@@ -224,14 +224,6 @@ class KerasLayerTest(keras_parameterized.TestCase):\n         self.assertEqual(layer(x).dtype, tf.float64)\n         self.assertEqual(layer.v.dtype, tf.float64)\n \n-  def test_error_passing_policy_string_to_layer(self):\n-    with self.assertRaisesRegex(\n-        TypeError, \"Cannot convert value 'mixed_float16' to a \"\n-        'TensorFlow DType'):\n-      # This is not allowed, as otherwise a \"mixed_float16\" policy could be\n-      # created without an API call that has the name \"experimental\" in it.\n-      mp_test_util.MultiplyLayer(dtype='mixed_float16')\n-\n   @parameterized.named_parameters(*TESTCASES)\n   def test_gradient(self, strategy_fn):\n     x = tf.constant([1.])\n@@ -329,7 +321,7 @@ class KerasLayerTest(keras_parameterized.TestCase):\n         self.assertEqual(layer(x).dtype, dtype)\n         self.assertEqual(layer.v.dtype, dtype)\n \n-      layer = mp_test_util.MultiplyLayer(dtype=policy.Policy('mixed_float16'))\n+      layer = mp_test_util.MultiplyLayer(dtype='mixed_float16')\n       config = layer.get_config()\n       self.assertEqual(config['dtype'],\n                        {'class_name': 'Policy',\n@@ -415,7 +407,7 @@ class KerasLayerTest(keras_parameterized.TestCase):\n       self.assertEqual(config['dtype'], 'float16')\n \n   def test_delete_variable(self):\n-    layer = base_layer.Layer(dtype=policy.Policy('mixed_float16'))\n+    layer = base_layer.Layer(dtype='mixed_float16')\n     layer.x = layer.add_weight('x')\n     self.assertEqual(layer.trainable_weights, [layer.x])\n     del layer.x\n@@ -440,7 +432,7 @@ class KerasLayerTest(keras_parameterized.TestCase):\n         'stop using mixed precision by removing the use of the '\n         '\"mixed_float16\" policy or use a different Strategy, e.g. '\n         'a MirroredStrategy.'):\n-      mp_test_util.MultiplyLayer(dtype=policy.Policy('mixed_float16'))\n+      mp_test_util.MultiplyLayer(dtype='mixed_float16')\n     # Non-mixed policies are fine\n     mp_test_util.MultiplyLayer(dtype=policy.Policy('float64'))\n \n\n@@ -1008,23 +1008,23 @@ class LossScaleOptimizerV1(LossScaleOptimizer):\n \n     if isinstance(loss_scale, (int, float)):\n       tf_logging.warn(\n-          warn_msg_prefix + 'For example\\n'\n-          '  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer('\n+          warn_msg_prefix + 'For example:\\n'\n+          '  opt = tf.keras.mixed_precision.LossScaleOptimizer('\n           'opt, dynamic=False, initial_scale={})'.format(loss_scale))\n       super(LossScaleOptimizerV1, self).__init__(optimizer, dynamic=False,\n                                                  initial_scale=loss_scale)\n     elif isinstance(loss_scale, tf.mixed_precision.experimental.FixedLossScale):\n       ls_val = loss_scale._loss_scale_value  # pylint: disable=protected-access\n       tf_logging.warn(\n-          warn_msg_prefix + 'For example\\n'\n-          '  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer('\n+          warn_msg_prefix + 'For example:\\n'\n+          '  opt = tf.keras.mixed_precision.LossScaleOptimizer('\n           'opt, dynamic=False, initial_scale={})'.format(ls_val))\n       super(LossScaleOptimizerV1, self).__init__(optimizer, dynamic=False,\n                                                  initial_scale=ls_val)\n     elif loss_scale == 'dynamic':\n       tf_logging.warn(\n-          warn_msg_prefix + 'For example\\n'\n-          '  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer('\n+          warn_msg_prefix + 'For example:\\n'\n+          '  opt = tf.keras.mixed_precision.LossScaleOptimizer('\n           'opt)')\n       super(LossScaleOptimizerV1, self).__init__(optimizer)\n     elif isinstance(loss_scale, tf.mixed_precision.experimental.DynamicLossScale):\n@@ -1047,7 +1047,7 @@ class LossScaleOptimizerV1(LossScaleOptimizer):\n           'Note that the non-experimental LossScaleOptimizer does not take a '\n           'DynamicLossScale but instead takes the dynamic configuration '\n           'directly in the constructor. For example:\\n'\n-          '  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer('\n+          '  opt = tf.keras.mixed_precision.LossScaleOptimizer('\n           'opt{})\\n'.format(extra_arguments))\n       super(LossScaleOptimizerV1, self).__init__(optimizer, **kwargs)\n     elif isinstance(loss_scale, tf.mixed_precision.experimental.LossScale):\n\n@@ -70,8 +70,9 @@ class Policy(object):\n \n   In the example above, passing `dtype='float32'` to the layer is equivalent to\n   passing `dtype=tf.keras.mixed_precision.Policy('float32')`. In general,\n-  passing a dtype to a layer is equivalent to passing the corresponding policy,\n-  so it is never necessary to explicitly construct a `Policy` object.\n+  passing a dtype policy name to a layer is equivalent to passing the\n+  corresponding policy, so it is never necessary to explicitly construct a\n+  `Policy` object.\n \n   Note: `Model.compile` will automatically wrap an optimizer with a\n   `tf.keras.mixed_precision.LossScaleOptimizer` if you use the `'mixed_float16'`\n@@ -145,8 +146,7 @@ class Policy(object):\n   ...     # With mixed precision, self.kernel will be casted to float16\n   ...     return tf.linalg.matmul(inputs, self.kernel)\n   ...\n-  >>> dtype_policy = tf.keras.mixed_precision.Policy('mixed_float16')\n-  >>> layer = SimpleDense(dtype=dtype_policy)\n+  >>> layer = SimpleDense(dtype='mixed_float16')\n   >>> y = layer(tf.ones((10, 10)))\n   >>> y.dtype\n   tf.float16\n@@ -178,9 +178,7 @@ class Policy(object):\n   ...     # occur when adding `inputs` to `rand`.\n   ...     rand = tf.random.normal(shape=inputs.shape, dtype=inputs.dtype)\n   ...     return inputs + rand\n-\n-  >>> dtype_policy = tf.keras.mixed_precision.Policy('mixed_float16')\n-  >>> layer = AddRandom(dtype=dtype_policy)\n+  >>> layer = AddRandom(dtype='mixed_float16')\n   >>> y = layer(x)\n   >>> y.dtype\n   tf.float16\n\n@@ -132,6 +132,11 @@ def save_model(model,\n \n   filepath = path_to_string(filepath)\n \n+  # If the user has not already called fit or built the underlying metrics, we\n+  # should do that before saving to ensure the metric names have all\n+  # appropriate name transformations applied.\n+  saving_utils.try_build_compiled_arguments(model)\n+\n   if (save_format == 'h5' or\n       (h5py is not None and isinstance(filepath, h5py.File)) or\n       saving_utils.is_hdf5_filepath(filepath)):\n\n@@ -995,13 +995,22 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     loaded = keras.models.load_model(saved_model_dir)\n     self.assertIs(loaded.layers[1], loaded.layers[2].layer)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_multi_output_metrics_name_stay_same(self):\n+  @combinations.generate(\n+      combinations.combine(mode=['graph', 'eager'], fit=[True, False]))\n+  def test_multi_output_metrics_name_stay_same(self, fit):\n     \"\"\"Tests that metric names don't change with each save/load cycle.\n \n     e.g. \"head_0_accuracy\" should not become \"head_0_head_0_accuracy\" after\n     saving and loading a model.\n+\n+    Arguments:\n+      fit: Whether the model should be fit before saving.\n     \"\"\"\n+    # This doesn't work at all, so we can't check whether metric names are\n+    # correct.\n+    if not tf.executing_eagerly() and not fit:\n+      self.skipTest('b/181767784')\n+\n     with self.cached_session():\n       input_ = keras.Input((4,))\n       model = keras.Model(\n@@ -1013,10 +1022,13 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n                     loss='mse',\n                     metrics={'head_0': [metric, 'accuracy']})\n \n-      # Run one iteration.\n       x = np.random.rand(2, 4)\n       y = {'head_0': np.random.randint(2, size=(2, 3)),\n            'head_1': np.random.randint(2, size=(2, 5))}\n+\n+      # Make sure metrix prefixing works the same regardless of whether the user\n+      # has fit the model before saving.\n+      if fit:\n         model.fit(x, y, verbose=0)\n \n       # Save and reload.\n\n@@ -315,7 +315,9 @@ def try_build_compiled_arguments(model):\n   if (not version_utils.is_v1_layer_or_model(model) and\n       model.outputs is not None):\n     try:\n+      if not model.compiled_loss.built:\n         model.compiled_loss.build(model.outputs)\n+      if not model.compiled_metrics.built:\n         model.compiled_metrics.build(model.outputs, model.outputs)\n     except:  # pylint: disable=bare-except\n       logging.warning(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
