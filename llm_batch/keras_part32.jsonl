{"custom_id": "keras#a9d437198ee6df214c80baa5413246c3033779b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 2 | Churn Cumulative: 698 | Contributors (this commit): 10 | Commits (past 90d): 27 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -149,7 +149,7 @@ class Convolution2D(Layer):\n     def get_output(self, train):\n         X = self.get_input(train)\n         border_mode = self.border_mode\n-        if dnn.dnn_available() and theano.config.device == 'gpu':\n+        if dnn.dnn_available() and theano.config.device[:3] == 'gpu':\n             if border_mode == 'same':\n                 assert(self.subsample == (1, 1))\n                 pad_x = (self.nb_row - self.subsample[0]) // 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7a86ff7f5bcbf8bb56cdab90fd873c2f2b561b3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 23 | Methods Changed: 11 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 99 | Churn Cumulative: 3341 | Contributors (this commit): 21 | Commits (past 90d): 104 | Contributors (cumulative): 24 | DMM Complexity: 0.47058823529411764\n\nDIFF:\n@@ -64,12 +64,7 @@ def slice_X(X, start=None, stop=None):\n \n def weighted_objective(fn):\n     def weighted(y_true, y_pred, weights, mask=None):\n-        '''\n-        y_true dimension:  (sample, timestep, dims)\n-        y_pred dimension:  (sample, timestep, dims)\n-        weights dimension: (sample, timestep, 1)\n-        '''\n-        # it's important that 0 * Inf == 0, not NaN, so I need to filter\n+        # it's important that 0 * Inf == 0, not NaN, so we need to filter\n         # those out first\n         filtered_y_true = y_true[weights.nonzero()[:-1]]\n         filtered_y_pred = y_pred[weights.nonzero()[:-1]]\n@@ -160,7 +155,7 @@ def get_function_name(o):\n \n class Model(object):\n     def _fit(self, f, ins, out_labels=[], batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n-             validation_split=0., val_f=None, val_ins=None, shuffle=True, metrics=[]):\n+             val_f=None, val_ins=None, shuffle=True, metrics=[]):\n         '''\n             Abstract fit function for f(*ins). Assume that f returns a list, labelled by out_labels.\n         '''\n@@ -169,13 +164,6 @@ class Model(object):\n             do_validation = True\n             if verbose:\n                 print(\"Train on %d samples, validate on %d samples\" % (len(ins[0]), len(val_ins[0])))\n-        else:\n-            if 0 < validation_split < 1:\n-                do_validation = True\n-                split_at = int(len(ins[0]) * (1 - validation_split))\n-                (ins, val_ins) = (slice_X(ins, 0, split_at), slice_X(ins, split_at))\n-                if verbose:\n-                    print(\"Train on %d samples, validate on %d samples\" % (len(ins[0]), len(val_ins[0])))\n \n         nb_train_sample = len(ins[0])\n         index_array = np.arange(nb_train_sample)\n@@ -451,7 +439,6 @@ class Sequential(Model, containers.Sequential):\n \n         X = standardize_X(X)\n         y = standardize_y(y)\n-        sample_weight = standardize_weights(y, class_weight=class_weight, sample_weight=sample_weight)\n \n         val_f = None\n         val_ins = None\n@@ -461,14 +448,29 @@ class Sequential(Model, containers.Sequential):\n             else:\n                 val_f = self._test\n         if validation_data:\n-            try:\n+            if len(validation_data) == 2:\n                 X_val, y_val = validation_data\n-            except:\n-                raise Exception(\"Invalid format for validation data; provide a tuple (X_val, y_val). \\\n+                sample_weight_val = np.ones(y_val.shape[:-1] + (1,))\n+            elif len(validation_data) == 3:\n+                X_val, y_val, sample_weight_val = validation_data\n+            else:\n+                raise Exception(\"Invalid format for validation data; provide a tuple (X_val, y_val) or (X_val, y_val, sample_weight). \\\n                     X_val may be a numpy array or a list of numpy arrays depending on your model input.\")\n             X_val = standardize_X(X_val)\n             y_val = standardize_y(y_val)\n-            val_ins = X_val + [y_val, np.ones(y_val.shape[:-1] + (1,))]\n+            sample_weight_val = standardize_weights(y_val, sample_weight=sample_weight_val)\n+            val_ins = X_val + [y_val, sample_weight_val]\n+\n+        elif 0 < validation_split < 1:\n+            split_at = int(len(X[0]) * (1 - validation_split))\n+            X, X_val = (slice_X(X, 0, split_at), slice_X(X, split_at))\n+            y, y_val = (slice_X(y, 0, split_at), slice_X(y, split_at))\n+            if sample_weight is not None:\n+                sample_weight, sample_weight_val = (slice_X(sample_weight, 0, split_at), slice_X(sample_weight, split_at))\n+                sample_weight_val = standardize_weights(y_val, sample_weight=sample_weight_val)\n+            else:\n+                sample_weight_val = np.ones(y_val.shape[:-1] + (1,))\n+            val_ins = X_val + [y_val, sample_weight_val]\n \n         if show_accuracy:\n             f = self._train_with_acc\n@@ -477,11 +479,12 @@ class Sequential(Model, containers.Sequential):\n             f = self._train\n             out_labels = ['loss']\n \n+        sample_weight = standardize_weights(y, class_weight=class_weight, sample_weight=sample_weight)\n         ins = X + [y, sample_weight]\n         metrics = ['loss', 'acc', 'val_loss', 'val_acc']\n         return self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch,\n                          verbose=verbose, callbacks=callbacks,\n-                         validation_split=validation_split, val_f=val_f, val_ins=val_ins,\n+                         val_f=val_f, val_ins=val_ins,\n                          shuffle=shuffle, metrics=metrics)\n \n     def predict(self, X, batch_size=128, verbose=0):\n@@ -624,8 +627,8 @@ class Graph(Model, containers.Graph):\n \n     def test_on_batch(self, data, sample_weight={}):\n         # data is a dictionary mapping input names to arrays\n-        sample_weight = [standardize_weights(data[name]) for name in self.output_order]\n-\n+        sample_weight = [standardize_weights(data[name],\n+                                             sample_weight=sample_weight.get(name)) for name in self.output_order]\n         ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n         return self._test(*ins)\n \n@@ -636,30 +639,46 @@ class Graph(Model, containers.Graph):\n \n     def fit(self, data, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True, class_weight={}, sample_weight={}):\n-        sample_weight = [standardize_weights(data[name],\n-                                             sample_weight=sample_weight.get(name),\n-                                             class_weight=class_weight.get(name)) for name in self.output_order]\n-        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n+        X = [data[name] for name in self.input_order]\n+        y = [standardize_y(data[name]) for name in self.output_order]\n+        sample_weight_list = [standardize_weights(data[name],\n+                                                  sample_weight=sample_weight.get(name)) for name in self.output_order]\n+        class_weight_list = [class_weight.get(name) for name in self.output_order]\n \n         val_f = None\n         val_ins = None\n         if validation_data or validation_split:\n             val_f = self._test\n         if validation_data:\n+            # can't use sample weights with validation data at this point\n             sample_weight = [standardize_weights(validation_data[name]) for name in self.output_order]\n             val_ins = [validation_data[name] for name in self.input_order] + [standardize_y(validation_data[name]) for name in self.output_order] + sample_weight\n \n+        elif 0 < validation_split < 1:\n+            split_at = int(len(X[0]) * (1 - validation_split))\n+            X, X_val = (slice_X(X, 0, split_at), slice_X(X, split_at))\n+            y, y_val = (slice_X(y, 0, split_at), slice_X(y, split_at))\n+            sample_weight_list, sample_weight_list_val = (slice_X(sample_weight_list, 0, split_at), slice_X(sample_weight_list, split_at))\n+            val_ins = X_val + y_val + sample_weight_list_val\n+\n         f = self._train\n         out_labels = ['loss']\n         metrics = ['loss', 'val_loss']\n+\n+        sample_weight_list = [standardize_weights(y[i],\n+                                                  sample_weight=sample_weight_list[i],\n+                                                  class_weight=class_weight_list[i]) for i in range(len(self.output_order))]\n+        ins = X + y + sample_weight_list\n+\n         history = self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch,\n                             verbose=verbose, callbacks=callbacks,\n-                            validation_split=validation_split, val_f=val_f, val_ins=val_ins,\n+                            val_f=val_f, val_ins=val_ins,\n                             shuffle=shuffle, metrics=metrics)\n         return history\n \n     def evaluate(self, data, batch_size=128, verbose=0, sample_weight={}):\n-        sample_weight = [standardize_weights(data[name], sample_weight.get(name)) for name in self.output_order]\n+        sample_weight = [standardize_weights(data[name],\n+                                             sample_weight=sample_weight.get(name)) for name in self.output_order]\n \n         ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n         outs = self._test_loop(self._test, ins, batch_size, verbose)\n\n@@ -144,6 +144,8 @@ class TestGraph(unittest.TestCase):\n \n         weights1 = np.random.uniform(size=y_train.shape[0])\n         weights2 = np.random.uniform(size=y2_train.shape[0])\n+        weights1_test = np.random.uniform(size=y_test.shape[0])\n+        weights2_test = np.random.uniform(size=y2_test.shape[0])\n \n         graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n \n@@ -153,7 +155,7 @@ class TestGraph(unittest.TestCase):\n         assert(type(out == dict))\n         assert(len(out) == 2)\n         loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test},\n-                                   sample_weight={'output1': weights1, 'output2': weights2})\n+                                   sample_weight={'output1': weights1_test, 'output2': weights2_test})\n         loss = graph.train_on_batch({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n                                     sample_weight={'output1': weights1, 'output2': weights2})\n         loss = graph.evaluate({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n\n@@ -12,7 +12,7 @@ import unittest\n \n nb_classes = 10\n batch_size = 128\n-nb_epoch = 5\n+nb_epoch = 8\n weighted_class = 9\n standard_weight = 1\n high_weight = 5\n@@ -59,8 +59,19 @@ def create_graph_model():\n \n \n def _test_weights_sequential(model, class_weight=None, sample_weight=None):\n-    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0,\n+    if sample_weight is not None:\n+        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch // 3, verbose=0,\n                   class_weight=class_weight, sample_weight=sample_weight)\n+        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch // 3, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight, validation_split=0.1)\n+        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch // 3, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight, validation_data=(X_train, Y_train, sample_weight))\n+    else:\n+        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight)\n+        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight, validation_split=0.1)\n+\n     model.train_on_batch(X_train[:32], Y_train[:32],\n                          class_weight=class_weight, sample_weight=sample_weight[:32] if sample_weight is not None else None)\n     model.test_on_batch(X_train[:32], Y_train[:32],\n@@ -70,8 +81,11 @@ def _test_weights_sequential(model, class_weight=None, sample_weight=None):\n \n \n def _test_weights_graph(model, class_weight=None, sample_weight=None):\n-    model.fit({'input': X_train, 'output': Y_train}, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0,\n+    model.fit({'input': X_train, 'output': Y_train}, batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n               class_weight={'output': class_weight}, sample_weight={'output': sample_weight})\n+    model.fit({'input': X_train, 'output': Y_train}, batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n+              class_weight={'output': class_weight}, sample_weight={'output': sample_weight}, validation_split=0.1)\n+\n     model.train_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n                          class_weight={'output': class_weight}, sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n     model.test_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c30d503eada5cb5429b6f6d8ced0e996760e40e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2897 | Contributors (this commit): 21 | Commits (past 90d): 86 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -450,15 +450,17 @@ class Sequential(Model, containers.Sequential):\n         if validation_data:\n             if len(validation_data) == 2:\n                 X_val, y_val = validation_data\n+                X_val = standardize_X(X_val)\n+                y_val = standardize_y(y_val)\n                 sample_weight_val = np.ones(y_val.shape[:-1] + (1,))\n             elif len(validation_data) == 3:\n                 X_val, y_val, sample_weight_val = validation_data\n-            else:\n-                raise Exception(\"Invalid format for validation data; provide a tuple (X_val, y_val) or (X_val, y_val, sample_weight). \\\n-                    X_val may be a numpy array or a list of numpy arrays depending on your model input.\")\n                 X_val = standardize_X(X_val)\n                 y_val = standardize_y(y_val)\n                 sample_weight_val = standardize_weights(y_val, sample_weight=sample_weight_val)\n+            else:\n+                raise Exception(\"Invalid format for validation data; provide a tuple (X_val, y_val) or (X_val, y_val, sample_weight). \\\n+                    X_val may be a numpy array or a list of numpy arrays depending on your model input.\")\n             val_ins = X_val + [y_val, sample_weight_val]\n \n         elif 0 < validation_split < 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab8642e0ff47776d41b51e73523ee1010e525916", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 7 | Churn Cumulative: 1728 | Contributors (this commit): 17 | Commits (past 90d): 62 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -163,10 +163,12 @@ class Masking(MaskedLayer):\n class Merge(Layer):\n     def __init__(self, layers, mode='sum'):\n         ''' Merge the output of a list of layers or containers into a single tensor.\n-            mode: {'sum', 'concat'}\n+            mode: {'sum', 'mul', 'concat'}\n         '''\n         if len(layers) < 2:\n             raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n+        elif len(layers) > 2 and mode == 'mul':\n+            raise Exception(\"Elemwise multiplication mode can only merge two layers\")\n         self.mode = mode\n         self.layers = layers\n         self.params = []\n@@ -195,6 +197,9 @@ class Merge(Layer):\n         elif self.mode == 'concat':\n             inputs = [self.layers[i].get_output(train) for i in range(len(self.layers))]\n             return T.concatenate(inputs, axis=-1)\n+        elif self.mode == 'mul':\n+            s = self.layers[0].get_output(train) * self.layers[1].get_output(train)\n+            return s\n         else:\n             raise Exception('Unknown merge mode')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#da24be79ab5d06e7be16d9eb0a35f47b7b91939b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 24/24 | Churn Δ: 27 | Churn Cumulative: 835 | Contributors (this commit): 7 | Commits (past 90d): 24 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -182,6 +182,7 @@ class ModelCheckpoint(Callback):\n         self.best = np.Inf\n \n     def on_epoch_end(self, epoch, logs={}):\n+        filepath = self.filepath.format(epoch=epoch, **logs)\n         if self.save_best_only:\n             current = logs.get(self.monitor)\n             if current is None:\n@@ -190,16 +191,16 @@ class ModelCheckpoint(Callback):\n                 if current < self.best:\n                     if self.verbose > 0:\n                         print(\"Epoch %05d: %s improved from %0.5f to %0.5f, saving model to %s\"\n-                              % (epoch, self.monitor, self.best, current, self.filepath))\n+                              % (epoch, self.monitor, self.best, current, filepath))\n                     self.best = current\n-                    self.model.save_weights(self.filepath, overwrite=True)\n+                    self.model.save_weights(filepath, overwrite=True)\n                 else:\n                     if self.verbose > 0:\n                         print(\"Epoch %05d: %s did not improve\" % (epoch, self.monitor))\n         else:\n             if self.verbose > 0:\n-                print(\"Epoch %05d: saving model to %s\" % (epoch, self.filepath))\n-            self.model.save_weights(self.filepath, overwrite=True)\n+                print(\"Epoch %05d: saving model to %s\" % (epoch, filepath))\n+            self.model.save_weights(filepath, overwrite=True)\n \n \n class EarlyStopping(Callback):\n\n@@ -215,12 +215,22 @@ print(\"Test model checkpointer without validation data\")\n import warnings\n warnings.filterwarnings('error')\n try:\n+    passed = False\n     # this should issue a warning\n     model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, callbacks =[checkpointer])\n except:\n-    print(\"Tests passed\")\n-    import sys\n-    sys.exit(0)\n-\n+    passed = True\n+if not passed:\n     raise Exception(\"Modelcheckpoint tests did not pass\")\n \n+print(\"Test model checkpointer with pattern\")\n+filename = \"model_weights.{epoch:04d}.hdf5\"\n+f = os.path.join(path, filename)\n+nb_epoch = 3\n+checkpointer = cbks.ModelCheckpoint(f)\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, callbacks=[checkpointer])\n+for i in range(nb_epoch):\n+    if not os.path.isfile(f.format(epoch=i)):\n+        raise Exception(\"Model weights were not saved separately for each epoch\")\n+\n+print(\"Tests passed\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16d0e405608e02d3864a13d4bc4a6b0cb2e3361a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 1732 | Contributors (this commit): 17 | Commits (past 90d): 62 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -198,7 +198,9 @@ class Merge(Layer):\n             inputs = [self.layers[i].get_output(train) for i in range(len(self.layers))]\n             return T.concatenate(inputs, axis=-1)\n         elif self.mode == 'mul':\n-            s = self.layers[0].get_output(train) * self.layers[1].get_output(train)\n+            s = self.layers[0].get_output(train)\n+            for i in range(1, len(self.layers)):\n+                s *= self.layers[i].get_output(train)\n             return s\n         else:\n             raise Exception('Unknown merge mode')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f84fe7ce1739b4df8b821e1898c17d7503c6af09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 442 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -1,15 +1,15 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n import sys\n-import six.moves.cPickle\n+from six.moves import cPickle\n from six.moves import range\n \n def load_batch(fpath, label_key='labels'):\n     f = open(fpath, 'rb')\n     if sys.version_info < (3,):\n-        d = six.moves.cPickle.load(f)\n+        d = cPickle.load(f)\n     else:\n-        d = six.moves.cPickle.load(f, encoding=\"bytes\")\n+        d = cPickle.load(f, encoding=\"bytes\")\n         # decode utf8\n         for k, v in d.items():\n             del(d[k])\n\n@@ -1,5 +1,5 @@\n from __future__ import absolute_import\n-import six.moves.cPickle\n+import cPickle\n import gzip\n from .data_utils import get_file\n import random\n@@ -17,7 +17,7 @@ def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0, maxlen=None, test_spli\n     else:\n         f = open(path, 'rb')\n \n-    X, labels = six.moves.cPickle.load(f)\n+    X, labels = cPickle.load(f)\n     f.close()\n \n     np.random.seed(seed)\n\n@@ -1,7 +1,7 @@\n # -*- coding: utf-8 -*-\n import gzip\n from .data_utils import get_file\n-import six.moves.cPickle\n+from six.moves import cPickle\n import sys\n \n \n@@ -14,9 +14,9 @@ def load_data(path=\"mnist.pkl.gz\"):\n         f = open(path, 'rb')\n \n     if sys.version_info < (3,):\n-        data = six.moves.cPickle.load(f)\n+        data = cPickle.load(f)\n     else:\n-        data = six.moves.cPickle.load(f, encoding=\"bytes\")\n+        data = cPickle.load(f, encoding=\"bytes\")\n \n     f.close()\n \n\n@@ -5,7 +5,7 @@ from .data_utils import get_file\n import string\n import random\n import os\n-import six.moves.cPickle\n+from six.moves import cPickle\n from six.moves import zip\n import numpy as np\n \n@@ -78,8 +78,8 @@ def make_reuters_dataset(path=os.path.join('datasets', 'temp', 'reuters21578'),\n     dataset = (X, labels)\n     print('-')\n     print('Saving...')\n-    six.moves.cPickle.dump(dataset, open(os.path.join('datasets', 'data', 'reuters.pkl'), 'w'))\n-    six.moves.cPickle.dump(tokenizer.word_index, open(os.path.join('datasets', 'data', 'reuters_word_index.pkl'), 'w'))\n+    cPickle.dump(dataset, open(os.path.join('datasets', 'data', 'reuters.pkl'), 'w'))\n+    cPickle.dump(tokenizer.word_index, open(os.path.join('datasets', 'data', 'reuters_word_index.pkl'), 'w'))\n \n \n def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113,\n@@ -88,7 +88,7 @@ def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_s\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters.pkl\")\n     f = open(path, 'rb')\n \n-    X, labels = six.moves.cPickle.load(f)\n+    X, labels = cPickle.load(f)\n     f.close()\n \n     np.random.seed(seed)\n@@ -140,7 +140,7 @@ def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_s\n def get_word_index(path=\"reuters_word_index.pkl\"):\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl\")\n     f = open(path, 'rb')\n-    return six.moves.cPickle.load(f)\n+    return cPickle.load(f)\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#332d43e023073561fec53828ee21e206ac1b34b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 867 | Contributors (this commit): 7 | Commits (past 90d): 27 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -473,6 +473,7 @@ class JZS1(Recurrent):\n             self.W_z, self.b_z,\n             self.W_r, self.U_r, self.b_r,\n             self.U_h, self.b_h,\n+            self.Pmat\n         ]\n \n         if weights is not None:\n@@ -579,6 +580,7 @@ class JZS2(Recurrent):\n             self.W_z, self.U_z, self.b_z,\n             self.U_r, self.b_r,\n             self.W_h, self.U_h, self.b_h,\n+            self.Pmat\n         ]\n \n         if weights is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#94fbbd1c7e9a9a868aa7552a58a744e1580e81ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 21 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n \n import numpy as np\n+import re\n from scipy import ndimage\n from scipy import linalg\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f3bdeb0a303caa3fc1de25a038b3a35f9601386", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 87 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -58,7 +58,7 @@ def he_uniform(shape):\n \n \n def orthogonal(shape, scale=1.1):\n-    ''' From Lasagne\n+    ''' From Lasagne. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\n     '''\n     flat_shape = (shape[0], np.prod(shape[1:]))\n     a = np.random.normal(0.0, 1.0, flat_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93c1a8c6757b0bcc73254eb4c47ddc3adc2e8324", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 90 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -22,11 +22,20 @@ batch_size = 128\n nb_classes = 10\n nb_epoch = 12\n \n+# shape of the image (SHAPE x SHAPE)\n+shapex, shapey = 28, 28\n+# number of convolutional filters to use\n+nb_filters = 32\n+# level of pooling to perform (POOL x POOL)\n+nb_pool = 2\n+# level of convolution to perform (CONV x CONV)\n+nb_conv = 3\n+\n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n-X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n-X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n+X_train = X_train.reshape(X_train.shape[0], 1, shapex, shapey)\n+X_test = X_test.reshape(X_test.shape[0], 1, shapex, shapey)\n X_train = X_train.astype(\"float32\")\n X_test = X_test.astype(\"float32\")\n X_train /= 255\n@@ -41,15 +50,18 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n model = Sequential()\n \n-model.add(Convolution2D(32, 1, 3, 3, border_mode='full'))\n+model.add(Convolution2D(nb_filters, 1, nb_conv, nb_conv, border_mode='full'))\n model.add(Activation('relu'))\n-model.add(Convolution2D(32, 32, 3, 3))\n+model.add(Convolution2D(nb_filters, nb_filters, nb_conv, nb_conv))\n model.add(Activation('relu'))\n-model.add(MaxPooling2D(poolsize=(2, 2)))\n+model.add(MaxPooling2D(poolsize=(nb_pool, nb_pool)))\n model.add(Dropout(0.25))\n \n model.add(Flatten())\n-model.add(Dense(32*196, 128))\n+# the resulting image after conv and pooling is the original shape\n+# divided by the pooling with a number of filters for each \"pixel\"\n+# (the number of filters is determined by the last Conv2D)\n+model.add(Dense(nb_filters * (shapex / nb_pool) * (shapey / nb_pool), 128))\n model.add(Activation('relu'))\n model.add(Dropout(0.5))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#49335d4345c52adee9e8db5057d4e5d78acd4088", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 215 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -28,6 +28,17 @@ nb_classes = 10\n nb_epoch = 200\n data_augmentation = True\n \n+# shape of the image (SHAPE x SHAPE)\n+shapex, shapey = 32, 32\n+# number of convolutional filters to use at each layer\n+nb_filters = [32, 64]\n+# level of pooling to perform at each layer (POOL x POOL)\n+nb_pool = [2, 2]\n+# level of convolution to perform at each layer (CONV x CONV)\n+nb_conv = [3, 3]\n+# the CIFAR10 images are RGB\n+image_dimensions = 3\n+\n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n print('X_train shape:', X_train.shape)\n@@ -40,22 +51,24 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n model = Sequential()\n \n-model.add(Convolution2D(32, 3, 3, 3, border_mode='full'))\n+model.add(Convolution2D(nb_filters[0], image_dimensions, nb_conv[0], nb_conv[0], border_mode='full'))\n model.add(Activation('relu'))\n-model.add(Convolution2D(32, 32, 3, 3))\n+model.add(Convolution2D(nb_filters[0], nb_filters[0], nb_conv[0], nb_conv[0]))\n model.add(Activation('relu'))\n-model.add(MaxPooling2D(poolsize=(2, 2)))\n+model.add(MaxPooling2D(poolsize=(nb_pool[0], nb_pool[0])))\n model.add(Dropout(0.25))\n \n-model.add(Convolution2D(64, 32, 3, 3, border_mode='full'))\n+model.add(Convolution2D(nb_filters[1], nb_filters[0], nb_conv[0], nb_conv[0], border_mode='full'))\n model.add(Activation('relu'))\n-model.add(Convolution2D(64, 64, 3, 3))\n+model.add(Convolution2D(nb_filters[1], nb_filters[1], nb_conv[1], nb_conv[1]))\n model.add(Activation('relu'))\n-model.add(MaxPooling2D(poolsize=(2, 2)))\n+model.add(MaxPooling2D(poolsize=(nb_pool[1], nb_pool[1])))\n model.add(Dropout(0.25))\n \n model.add(Flatten())\n-model.add(Dense(64*8*8, 512))\n+# the image dimensions are the original dimensions divided by any pooling\n+# each pixel has a number of filters, determined by the last Convolution2D layer\n+model.add(Dense(nb_filters[-1] * (shapex / nb_pool[0] / nb_pool[1]) * (shapey / nb_pool[0] / nb_pool[1]), 512))\n model.add(Activation('relu'))\n model.add(Dropout(0.5))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4bb6ac0b04838e8d82d14d7c0f001569f543518b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 819 | Contributors (this commit): 8 | Commits (past 90d): 21 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -271,4 +271,4 @@ class LearningRateScheduler(Callback):\n         self.schedule = schedule\n \n     def on_epoch_begin(self, epoch, logs={}):\n-        model.lr.set_value(self.schedule(epoch))\n+        self.model.optimizer.lr.set_value(self.schedule(epoch))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#034822359d9a4c3b2fc63de0676bad512b426112", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 322 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ from __future__ import print_function\n \n import numpy as np\n import theano\n-import six.moves.cPickle\n+from six.moves import cPickle\n import os, re, json\n \n from keras.preprocessing import sequence, text\n@@ -90,7 +90,7 @@ def text_generator(path=data_path):\n # model management\n if load_tokenizer:\n     print('Load tokenizer...')\n-    tokenizer = six.moves.cPickle.load(open(os.path.join(save_dir, tokenizer_fname), 'rb'))\n+    tokenizer = cPickle.load(open(os.path.join(save_dir, tokenizer_fname), 'rb'))\n else:\n     print(\"Fit tokenizer...\")\n     tokenizer = text.Tokenizer(nb_words=max_features)\n@@ -99,13 +99,13 @@ else:\n         print(\"Save tokenizer...\")\n         if not os.path.exists(save_dir):\n             os.makedirs(save_dir)\n-        six.moves.cPickle.dump(tokenizer, open(os.path.join(save_dir, tokenizer_fname), \"wb\"))\n+        cPickle.dump(tokenizer, open(os.path.join(save_dir, tokenizer_fname), \"wb\"))\n \n # training process\n if train_model:\n     if load_model:\n         print('Load model...')\n-        model = six.moves.cPickle.load(open(os.path.join(save_dir, model_load_fname), 'rb'))\n+        model = cPickle.load(open(os.path.join(save_dir, model_load_fname), 'rb'))\n     else:\n         print('Build model...')\n         model = Sequential()\n@@ -129,7 +129,7 @@ if train_model:\n             if couples:\n                 # one gradient update per sentence (one sentence = a few 1000s of word couples)\n                 X = np.array(couples, dtype=\"int32\")\n-                loss = model.train(X, labels)\n+                loss = model.fit(X, labels)\n                 losses.append(loss)\n                 if len(losses) % 100 == 0:\n                     progbar.update(i, values=[(\"loss\", np.mean(losses))])\n@@ -142,7 +142,7 @@ if train_model:\n         print(\"Saving model...\")\n         if not os.path.exists(save_dir):\n             os.makedirs(save_dir)\n-        six.moves.cPickle.dump(model, open(os.path.join(save_dir, model_save_fname), \"wb\"))\n+        cPickle.dump(model, open(os.path.join(save_dir, model_save_fname), \"wb\"))\n \n \n print(\"It's test time!\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e98b24a7672fda78051304943dcb7be405e0623e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 324 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -129,7 +129,7 @@ if train_model:\n             if couples:\n                 # one gradient update per sentence (one sentence = a few 1000s of word couples)\n                 X = np.array(couples, dtype=\"int32\")\n-                loss = model.fit(X, labels)\n+                loss = model.train_on_batch(X, labels)\n                 losses.append(loss)\n                 if len(losses) % 100 == 0:\n                     progbar.update(i, values=[(\"loss\", np.mean(losses))])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5648119b667bf932d6f502e4c2b7c44bbbe80047", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 2 | Churn Cumulative: 1734 | Contributors (this commit): 17 | Commits (past 90d): 55 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -167,8 +167,6 @@ class Merge(Layer):\n         '''\n         if len(layers) < 2:\n             raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n-        elif len(layers) > 2 and mode == 'mul':\n-            raise Exception(\"Elemwise multiplication mode can only merge two layers\")\n         self.mode = mode\n         self.layers = layers\n         self.params = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0969c569a69e067bef903fdb50fc29a9cd6a8748", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 128 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 27/27 | Churn Δ: 128 | Churn Cumulative: 128 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.45054945054945056\n\nDIFF:\n@@ -0,0 +1,128 @@\n+import unittest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+import theano\n+\n+from keras.layers import convolutional\n+\n+\n+class TestConvolutions(unittest.TestCase):\n+    def test_convolution_1d(self):\n+        nb_samples = 9\n+        nb_steps = 7\n+        input_dim = 10\n+        filter_length = 6\n+        nb_filter = 5\n+\n+        weights_in = [np.ones((nb_filter, input_dim, filter_length, 1)), np.ones(nb_filter)]\n+\n+        self.assertRaises(Exception, convolutional.Convolution1D,\n+                          input_dim, nb_filter, filter_length, border_mode='foo')\n+\n+        input = np.ones((nb_samples, nb_steps, input_dim))\n+        for weight in [None, weights_in]:\n+            for border_mode in ['valid', 'full']:\n+                for subsample_length in [1, 3]:\n+                    for W_regularizer in [None, 'l2']:\n+                        for b_regularizer in [None, 'l2']:\n+                            layer = convolutional.Convolution1D(\n+                                input_dim, nb_filter, filter_length, weights=weight,\n+                                border_mode=border_mode, W_regularizer=W_regularizer,\n+                                b_regularizer=b_regularizer, subsample_length=subsample_length)\n+\n+                            layer.input = theano.shared(value=input)\n+                            for train in [True, False]:\n+                                layer.get_output(train).eval()\n+\n+                            config = layer.get_config()\n+\n+    def test_maxpooling_1d(self):\n+        nb_samples = 9\n+\n+        nb_steps = 7\n+        input_dim = 10\n+\n+        input = np.ones((nb_samples, nb_steps, input_dim))\n+        for ignore_border in [True, False]:\n+            for stride in [None, 2]:\n+                layer = convolutional.MaxPooling1D(stride=stride, ignore_border=ignore_border)\n+                layer.input = theano.shared(value=input)\n+                for train in [True, False]:\n+                    layer.get_output(train).eval()\n+\n+                config = layer.get_config()\n+\n+    def test_convolution_2d(self):\n+        nb_samples = 8\n+\n+        nb_filter = 9\n+        stack_size = 7\n+        nb_row = 10\n+        nb_col = 6\n+\n+        input_nb_row = 11\n+        input_nb_col = 12\n+\n+        weights_in = [np.ones((nb_filter, stack_size, nb_row, nb_col)), np.ones(nb_filter)]\n+\n+        self.assertRaises(Exception, convolutional.Convolution2D,\n+                          nb_filter, stack_size, nb_row, nb_col, border_mode='foo')\n+\n+        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+        for weight in [None, weights_in]:\n+            for border_mode in ['valid', 'full', 'same']:\n+                for subsample in [(1, 1), (2, 3)]:\n+                    for W_regularizer in [None, 'l2']:\n+                        for b_regularizer in [None, 'l2']:\n+                            layer = convolutional.Convolution2D(\n+                                nb_filter, stack_size, nb_row, nb_col, weights=weight,\n+                                border_mode=border_mode, W_regularizer=W_regularizer,\n+                                b_regularizer=b_regularizer, subsample=subsample)\n+\n+                            layer.input = theano.shared(value=input)\n+                            for train in [True, False]:\n+                                out = layer.get_output(train).eval()\n+                                if border_mode == 'same' and subsample == (1, 1):\n+                                    assert out.shape[2:] == input.shape[2:]\n+\n+                            config = layer.get_config()\n+\n+    def test_maxpooling_2d(self):\n+        nb_samples = 9\n+\n+        stack_size = 7\n+        input_nb_row = 11\n+        input_nb_col = 12\n+        poolsize = (3, 3)\n+\n+        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+        for ignore_border in [True, False]:\n+            for stride in [None, (2, 2)]:\n+                layer = convolutional.MaxPooling2D(stride=stride, ignore_border=ignore_border, poolsize=poolsize)\n+                layer.input = theano.shared(value=input)\n+                for train in [True, False]:\n+                    layer.get_output(train).eval()\n+\n+                config = layer.get_config()\n+\n+    def test_zero_padding_2d(self):\n+        nb_samples = 9\n+\n+        stack_size = 7\n+        input_nb_row = 11\n+        input_nb_col = 12\n+\n+        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+        layer = convolutional.ZeroPadding2D(pad=(2,2))\n+        layer.input = theano.shared(value=input)\n+        for train in [True, False]:\n+            out = layer.get_output(train).eval()\n+            for offset in [0,1,-1,-2]:\n+                assert_allclose(out[:, :, offset, :], 0.)\n+                assert_allclose(out[:, :, :, offset], 0.)\n+            assert_allclose(out[:, :, 2:-2, 2:-2], 1.)\n+\n+        config = layer.get_config()\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#84909a49c2c58d7cda893897c183067acc0794ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 44 | Churn Cumulative: 172 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.8709677419354839\n\nDIFF:\n@@ -25,10 +25,12 @@ class TestConvolutions(unittest.TestCase):\n                 for subsample_length in [1, 3]:\n                     for W_regularizer in [None, 'l2']:\n                         for b_regularizer in [None, 'l2']:\n+                            for act_regularizer in [None, 'l2']:\n                                 layer = convolutional.Convolution1D(\n                                     input_dim, nb_filter, filter_length, weights=weight,\n                                     border_mode=border_mode, W_regularizer=W_regularizer,\n-                                b_regularizer=b_regularizer, subsample_length=subsample_length)\n+                                    b_regularizer=b_regularizer, activity_regularizer=act_regularizer,\n+                                    subsample_length=subsample_length)\n \n                             layer.input = theano.shared(value=input)\n                             for train in [True, False]:\n@@ -74,10 +76,12 @@ class TestConvolutions(unittest.TestCase):\n                 for subsample in [(1, 1), (2, 3)]:\n                     for W_regularizer in [None, 'l2']:\n                         for b_regularizer in [None, 'l2']:\n+                            for act_regularizer in [None, 'l2']:\n                                 layer = convolutional.Convolution2D(\n                                     nb_filter, stack_size, nb_row, nb_col, weights=weight,\n                                     border_mode=border_mode, W_regularizer=W_regularizer,\n-                                b_regularizer=b_regularizer, subsample=subsample)\n+                                    b_regularizer=b_regularizer, activity_regularizer=act_regularizer,\n+                                    subsample=subsample)\n \n                                 layer.input = theano.shared(value=input)\n                                 for train in [True, False]:\n@@ -124,5 +128,41 @@ class TestConvolutions(unittest.TestCase):\n \n         config = layer.get_config()\n \n+    def test_upsample_1d(self):\n+        nb_samples = 9\n+\n+        nb_steps = 7\n+        input_dim = 10\n+\n+        input = np.ones((nb_samples, nb_steps, input_dim))\n+        for length in [2,3,9]:\n+            layer = convolutional.UpSample1D(length=length)\n+            layer.input = theano.shared(value=input)\n+            for train in [True, False]:\n+                out = layer.get_output(train).eval()\n+                assert out.shape[1] == length*nb_steps\n+\n+            config = layer.get_config()\n+\n+    def test_upsample_2d(self):\n+        nb_samples = 9\n+\n+        stack_size = 7\n+        input_nb_row = 11\n+        input_nb_col = 12\n+\n+        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+\n+        for length_row in [2,3,9]:\n+            for length_col in [2,3,9]:\n+                layer = convolutional.UpSample2D(size=(length_row,length_col))\n+                layer.input = theano.shared(value=input)\n+                for train in [True, False]:\n+                    out = layer.get_output(train).eval()\n+                    assert out.shape[2] == length_row*input_nb_row\n+                    assert out.shape[3] == length_col*input_nb_col\n+\n+            config = layer.get_config()\n+\n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e67b040e88abad2e9befca3748521ca1a0d5254", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 179 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,7 +21,7 @@ class TestConvolutions(unittest.TestCase):\n \n         input = np.ones((nb_samples, nb_steps, input_dim))\n         for weight in [None, weights_in]:\n-            for border_mode in ['valid', 'full']:\n+            for border_mode in ['valid', 'full', 'same']:\n                 for subsample_length in [1, 3]:\n                     for W_regularizer in [None, 'l2']:\n                         for b_regularizer in [None, 'l2']:\n@@ -34,7 +34,10 @@ class TestConvolutions(unittest.TestCase):\n \n                             layer.input = theano.shared(value=input)\n                             for train in [True, False]:\n-                                layer.get_output(train).eval()\n+                                out = layer.get_output(train).eval()\n+                                assert input.shape[0] == out.shape[0]\n+                                if border_mode == 'same' and subsample_length == 1:\n+                                    assert input.shape[1] == out.shape[1]\n \n                             config = layer.get_config()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
