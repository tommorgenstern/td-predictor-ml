{"custom_id": "keras#bc5e993ae30a59d7bcdfa03513ecc3d43b58a5f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 215 | Lines Deleted: 157 | Files Changed: 11 | Hunks: 98 | Methods Changed: 40 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 372 | Churn Cumulative: 2457 | Contributors (this commit): 11 | Commits (past 90d): 50 | Contributors (cumulative): 32 | DMM Complexity: 0.7547169811320755\n\nDIFF:\n@@ -98,6 +98,7 @@ class TestBackend(unittest.TestCase):\n \n         check_single_tensor_operation('mean', (4, 2))\n         check_single_tensor_operation('mean', (4, 2), axis=1, keepdims=True)\n+        check_single_tensor_operation('mean', (4, 2, 3), axis=-1, keepdims=True)\n \n         check_single_tensor_operation('std', (4, 2))\n         check_single_tensor_operation('std', (4, 2), axis=1, keepdims=True)\n@@ -121,6 +122,7 @@ class TestBackend(unittest.TestCase):\n         check_single_tensor_operation('exp', (4, 2))\n         check_single_tensor_operation('log', (4, 2))\n         check_single_tensor_operation('round', (4, 2))\n+        check_single_tensor_operation('pow', (4, 2), a=3)\n         check_single_tensor_operation('clip', (4, 2), min_value=0.4,\n                                       max_value=0.6)\n \n\n@@ -1,8 +1,8 @@\n import unittest\n import numpy as np\n from numpy.testing import assert_allclose\n-import theano\n \n+from keras import backend as K\n from keras.layers import convolutional\n \n \n@@ -14,26 +14,31 @@ class TestConvolutions(unittest.TestCase):\n         filter_length = 6\n         nb_filter = 5\n \n-        weights_in = [np.ones((nb_filter, input_dim, filter_length, 1)), np.ones(nb_filter)]\n+        weights_in = [np.ones((nb_filter, input_dim, filter_length, 1)),\n+                      np.ones(nb_filter)]\n \n         input = np.ones((nb_samples, nb_steps, input_dim))\n         for weight in [None, weights_in]:\n-            for border_mode in ['valid', 'full', 'same']:\n-                for subsample_length in [1, 3]:\n+            for border_mode in ['valid', 'same']:\n+                for subsample_length in [1]:\n                     if border_mode == 'same' and subsample_length != 1:\n                         continue\n                     for W_regularizer in [None, 'l2']:\n                         for b_regularizer in [None, 'l2']:\n                             for act_regularizer in [None, 'l2']:\n                                 layer = convolutional.Convolution1D(\n-                                    nb_filter, filter_length, weights=weight,\n-                                    border_mode=border_mode, W_regularizer=W_regularizer,\n-                                    b_regularizer=b_regularizer, activity_regularizer=act_regularizer,\n-                                    subsample_length=subsample_length, input_shape=(None, input_dim))\n+                                    nb_filter, filter_length,\n+                                    weights=weight,\n+                                    border_mode=border_mode,\n+                                    W_regularizer=W_regularizer,\n+                                    b_regularizer=b_regularizer,\n+                                    activity_regularizer=act_regularizer,\n+                                    subsample_length=subsample_length,\n+                                    input_shape=(None, input_dim))\n \n-                            layer.input = theano.shared(value=input)\n+                            layer.input = K.variable(input)\n                             for train in [True, False]:\n-                                out = layer.get_output(train).eval()\n+                                out = K.eval(layer.get_output(train))\n                                 assert input.shape[0] == out.shape[0]\n                                 if border_mode == 'same' and subsample_length == 1:\n                                     assert input.shape[1] == out.shape[1]\n@@ -46,12 +51,12 @@ class TestConvolutions(unittest.TestCase):\n         input_dim = 10\n \n         input = np.ones((nb_samples, nb_steps, input_dim))\n-        for ignore_border in [True, False]:\n         for stride in [1, 2]:\n-                layer = convolutional.MaxPooling1D(stride=stride, ignore_border=ignore_border)\n-                layer.input = theano.shared(value=input)\n+            layer = convolutional.MaxPooling1D(stride=stride,\n+                                               border_mode='valid')\n+            layer.input = K.variable(input)\n             for train in [True, False]:\n-                    layer.get_output(train).eval()\n+                K.eval(layer.get_output(train))\n \n             config = layer.get_config()\n \n@@ -69,22 +74,26 @@ class TestConvolutions(unittest.TestCase):\n \n         input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n         for weight in [None, weights_in]:\n-            for border_mode in ['valid', 'full', 'same']:\n-                for subsample in [(1, 1), (2, 3)]:\n+            for border_mode in ['valid', 'same']:\n+                for subsample in [(1, 1), (2, 2)]:\n                     if border_mode == 'same' and subsample != (1, 1):\n                         continue\n                     for W_regularizer in [None, 'l2']:\n                         for b_regularizer in [None, 'l2']:\n                             for act_regularizer in [None, 'l2']:\n                                 layer = convolutional.Convolution2D(\n-                                    nb_filter, nb_row, nb_col, weights=weight,\n-                                    border_mode=border_mode, W_regularizer=W_regularizer,\n-                                    b_regularizer=b_regularizer, activity_regularizer=act_regularizer,\n-                                    subsample=subsample, input_shape=(stack_size, None, None))\n+                                    nb_filter, nb_row, nb_col,\n+                                    weights=weight,\n+                                    border_mode=border_mode,\n+                                    W_regularizer=W_regularizer,\n+                                    b_regularizer=b_regularizer,\n+                                    activity_regularizer=act_regularizer,\n+                                    subsample=subsample,\n+                                    input_shape=(stack_size, None, None))\n \n-                                layer.input = theano.shared(value=input)\n+                                layer.input = K.variable(input)\n                                 for train in [True, False]:\n-                                    out = layer.get_output(train).eval()\n+                                    out = K.eval(layer.get_output(train))\n                                     if border_mode == 'same' and subsample == (1, 1):\n                                         assert out.shape[2:] == input.shape[2:]\n \n@@ -98,12 +107,13 @@ class TestConvolutions(unittest.TestCase):\n         pool_size = (3, 3)\n \n         input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n-        for ignore_border in [True, False]:\n-            for stride in [(1, 1), (2, 2)]:\n-                layer = convolutional.MaxPooling2D(stride=stride, ignore_border=ignore_border, pool_size=pool_size)\n-                layer.input = theano.shared(value=input)\n+        for strides in [(1, 1), (2, 2)]:\n+            layer = convolutional.MaxPooling2D(strides=strides,\n+                                               border_mode='valid',\n+                                               pool_size=pool_size)\n+            layer.input = K.variable(input)\n             for train in [True, False]:\n-                    layer.get_output(train).eval()\n+                K.eval(layer.get_output(train))\n \n             config = layer.get_config()\n \n@@ -115,9 +125,9 @@ class TestConvolutions(unittest.TestCase):\n \n         input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n         layer = convolutional.ZeroPadding2D(padding=(2, 2))\n-        layer.input = theano.shared(value=input)\n+        layer.input = K.variable(input)\n         for train in [True, False]:\n-            out = layer.get_output(train).eval()\n+            out = K.eval(layer.get_output(train))\n             for offset in [0, 1, -1, -2]:\n                 assert_allclose(out[:, :, offset, :], 0.)\n                 assert_allclose(out[:, :, :, offset], 0.)\n@@ -133,9 +143,9 @@ class TestConvolutions(unittest.TestCase):\n         input = np.ones((nb_samples, nb_steps, input_dim))\n         for length in [2, 3, 9]:\n             layer = convolutional.UpSample1D(length=length)\n-            layer.input = theano.shared(value=input)\n+            layer.input = K.variable(input)\n             for train in [True, False]:\n-                out = layer.get_output(train).eval()\n+                out = K.eval(layer.get_output(train))\n                 assert out.shape[1] == length * nb_steps\n \n             config = layer.get_config()\n@@ -151,9 +161,9 @@ class TestConvolutions(unittest.TestCase):\n         for length_row in [2, 3, 9]:\n             for length_col in [2, 3, 9]:\n                 layer = convolutional.UpSample2D(size=(length_row, length_col))\n-                layer.input = theano.shared(value=input)\n+                layer.input = K.variable(input)\n                 for train in [True, False]:\n-                    out = layer.get_output(train).eval()\n+                    out = K.eval(layer.get_output(train))\n                     assert out.shape[2] == length_row * input_nb_row\n                     assert out.shape[3] == length_col * input_nb_col\n \n\n@@ -1,8 +1,8 @@\n import unittest\n import numpy as np\n from numpy.testing import assert_allclose\n-import theano\n \n+from keras import backend as K\n from keras.layers import core\n \n \n@@ -15,10 +15,10 @@ class TestLayerBase(unittest.TestCase):\n         # Once an input is provided, it should be reachable through the\n         # appropriate getters\n         input = np.ones((nb_samples, input_dim))\n-        layer.input = theano.shared(value=input)\n+        layer.input = K.variable(input)\n         for train in [True, False]:\n-            assert_allclose(layer.get_input(train).eval(), input)\n-            assert_allclose(layer.get_output(train).eval(), input)\n+            assert_allclose(K.eval(layer.get_input(train)), input)\n+            assert_allclose(K.eval(layer.get_output(train)), input)\n \n     def test_connections(self):\n         nb_samples = 10\n@@ -27,13 +27,13 @@ class TestLayerBase(unittest.TestCase):\n         layer2 = core.Layer()\n \n         input = np.ones((nb_samples, input_dim))\n-        layer1.input = theano.shared(value=input)\n+        layer1.input = K.variable(input)\n \n         # After connecting, input of layer1 should be passed through\n         layer2.set_previous(layer1)\n         for train in [True, False]:\n-            assert_allclose(layer2.get_input(train).eval(), input)\n-            assert_allclose(layer2.get_output(train).eval(), input)\n+            assert_allclose(K.eval(layer2.get_input(train)), input)\n+            assert_allclose(K.eval(layer2.get_output(train)), input)\n \n \n class TestConfigParams(unittest.TestCase):\n@@ -120,36 +120,40 @@ class TestMasking(unittest.TestCase):\n         \"\"\"Test masking sequences with zeroes as padding\"\"\"\n         # integer inputs, one per timestep, like embeddings\n         layer = core.Masking()\n-        func = theano.function([layer.input], layer.get_output_mask())\n-        self.assertTrue(np.all(\n-            # get mask for this input\n-            func(np.array([[[1], [2], [3], [0]],\n-                          [[0], [4], [5], [0]]], dtype=np.int32)) ==\n+        func = K.function([layer.input], [layer.get_output_mask()])\n+        input_data = np.array([[[1], [2], [3], [0]],\n+                              [[0], [4], [5], [0]]], dtype=np.int32)\n+\n         # This is the expected output mask, one dimension less\n-            np.array([[1, 1, 1, 0], [0, 1, 1, 0]])))\n+        expected = np.array([[1, 1, 1, 0], [0, 1, 1, 0]])\n+\n+        # get mask for this input\n+        output = func([input_data])[0]\n+        self.assertTrue(np.all(output == expected))\n \n     def test_non_zero(self):\n         \"\"\"Test masking with non-zero mask value\"\"\"\n         layer = core.Masking(5)\n-        func = theano.function([layer.input], layer.get_output_mask())\n-        self.assertTrue(np.all(\n-            # get mask for this input, if not all the values are 5, shouldn't masked\n-            func(np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n-                          [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)) ==\n-            # This is the expected output mask, one dimension less\n-            np.array([[1, 1, 1, 0], [1, 1, 1, 1]])))\n+        func = K.function([layer.input], [layer.get_output_mask()])\n+        input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n+                              [[1, 5], [5, 0], [0, 0], [0, 0]]],\n+                              dtype=np.int32)\n+        output = func([input_data])[0]\n+        expected = np.array([[1, 1, 1, 0], [1, 1, 1, 1]])\n+        self.assertTrue(np.all(output == expected))\n \n     def test_non_zero_output(self):\n         \"\"\"Test output of masking layer with non-zero mask value\"\"\"\n         layer = core.Masking(5)\n-        func = theano.function([layer.input], layer.get_output())\n-        self.assertTrue(np.all(\n-            # get output for this input, replace padding with 0\n-            func(np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n-                          [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)) ==\n-            # This is the expected output\n-            np.array([[[1, 1], [2, 1], [3, 1], [0, 0]],\n-                     [[1, 5], [5, 0], [0, 0], [0, 0]]])))\n+        func = K.function([layer.input], [layer.get_output()])\n+\n+        input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n+                              [[1, 5], [5, 0], [0, 0], [0, 0]]],\n+                              dtype=np.int32)\n+        output = func([input_data])[0]\n+        expected = np.array([[[1, 1], [2, 1], [3, 1], [0, 0]],\n+                            [[1, 5], [5, 0], [0, 0], [0, 0]]])\n+        self.assertTrue(np.all(output == expected))\n \n \n if __name__ == '__main__':\n\n@@ -1,5 +1,4 @@\n import unittest\n-import math\n from keras import backend as K\n import numpy as np\n from numpy.testing import assert_allclose\n@@ -10,7 +9,7 @@ def get_standard_values():\n     These are just a set of floats used for testing the activation\n     functions, and are useful in multiple tests.\n     '''\n-    return [0, 0.1, 0.5, 0.9, 1.0]\n+    return np.array([[0, 0.1, 0.5, 0.9, 1.0]], dtype=K.floatx())\n \n \n class TestActivations(unittest.TestCase):\n@@ -20,19 +19,18 @@ class TestActivations(unittest.TestCase):\n \n         # Test using a reference implementation of softmax\n         def softmax(values):\n-            m = max(values)\n-            values = np.array(values)\n+            m = np.max(values)\n             e = np.exp(values - m)\n             return e / np.sum(e)\n \n-        x = K.placeholder(ndim=1)\n+        x = K.placeholder(ndim=2)\n         exp = s(x)\n-        f = K.function([x], exp)\n+        f = K.function([x], [exp])\n         test_values = get_standard_values()\n \n-        result = f(test_values)\n+        result = f([test_values])[0]\n         expected = softmax(test_values)\n-        assert_allclose(result.flatten(), expected)\n+        assert_allclose(result, expected, rtol=1e-05)\n \n     def test_relu(self):\n         '''\n@@ -41,32 +39,27 @@ class TestActivations(unittest.TestCase):\n         '''\n         from keras.activations import relu as r\n \n-        assert r(5) == 5\n-        assert r(-5) == 0\n-        assert r(-0.1) == 0\n-        assert r(0.1) == 0.1\n-\n-        x = K.placeholder(ndim=1)\n+        x = K.placeholder(ndim=2)\n         exp = r(x)\n-        f = K.function([x], exp)\n+        f = K.function([x], [exp])\n \n         test_values = get_standard_values()\n-        result = f(test_values)\n+        result = f([test_values])[0]\n \n         # because no negatives in test values\n-        assert_allclose(result.flatten(), test_values)\n+        assert_allclose(result, test_values, rtol=1e-05)\n \n     def test_tanh(self):\n         from keras.activations import tanh as t\n         test_values = get_standard_values()\n \n-        x = K.placeholder(ndim=1)\n+        x = K.placeholder(ndim=2)\n         exp = t(x)\n-        f = K.function([x], exp)\n+        f = K.function([x], [exp])\n \n-        result = f(test_values)\n-        expected = [math.tanh(v) for v in test_values]\n-        assert_allclose(result.flatten(), expected)\n+        result = f([test_values])[0]\n+        expected = np.tanh(test_values)\n+        assert_allclose(result, expected, rtol=1e-05)\n \n     def test_linear(self):\n         '''\n\n@@ -1,7 +1,7 @@\n import unittest\n import numpy as np\n from numpy.testing import assert_allclose\n-from theano import tensor as T\n+from keras import backend as K\n \n \n class TestConstraints(unittest.TestCase):\n@@ -16,23 +16,25 @@ class TestConstraints(unittest.TestCase):\n \n         for m in self.some_values:\n             norm_instance = maxnorm(m)\n-            normed = norm_instance(self.example_array)\n-            assert (np.all(normed.eval() < m))\n+            normed = norm_instance(K.variable(self.example_array))\n+            assert (np.all(K.eval(normed) < m))\n \n         # a more explicit example\n         norm_instance = maxnorm(2.0)\n         x = np.array([[0, 0, 0], [1.0, 0, 0], [3, 0, 0], [3, 3, 3]]).T\n-        x_normed_target = np.array([[0, 0, 0], [1.0, 0, 0], [2.0, 0, 0], [2./np.sqrt(3), 2./np.sqrt(3), 2./np.sqrt(3)]]).T\n-        x_normed_actual = norm_instance(x).eval()\n-        assert_allclose(x_normed_actual, x_normed_target)\n+        x_normed_target = np.array([[0, 0, 0], [1.0, 0, 0],\n+                                    [2.0, 0, 0],\n+                                    [2. / np.sqrt(3), 2. / np.sqrt(3), 2. / np.sqrt(3)]]).T\n+        x_normed_actual = K.eval(norm_instance(K.variable(x)))\n+        assert_allclose(x_normed_actual, x_normed_target, rtol=1e-05)\n \n     def test_nonneg(self):\n         from keras.constraints import nonneg\n \n         nonneg_instance = nonneg()\n \n-        normed = nonneg_instance(self.example_array)\n-        assert (np.all(np.min(normed.eval(), axis=1) == 0.))\n+        normed = nonneg_instance(K.variable(self.example_array))\n+        assert (np.all(np.min(K.eval(normed), axis=1) == 0.))\n \n     def test_identity(self):\n         from keras.constraints import identity\n@@ -58,12 +60,13 @@ class TestConstraints(unittest.TestCase):\n         from keras.constraints import unitnorm\n         unitnorm_instance = unitnorm()\n \n-        normalized = unitnorm_instance(self.example_array)\n+        normalized = unitnorm_instance(K.variable(self.example_array))\n \n-        norm_of_normalized = np.sqrt(np.sum(normalized.eval()**2, axis=1))\n-        difference = norm_of_normalized - 1.  # in the unit norm constraint, it should be equal to 1.\n+        norm_of_normalized = np.sqrt(np.sum(K.eval(normalized)**2, axis=1))\n+        # in the unit norm constraint, it should be equal to 1.\n+        difference = norm_of_normalized - 1.\n         largest_difference = np.max(np.abs(difference))\n-        self.assertAlmostEqual(largest_difference, 0.)\n+        assert np.abs(largest_difference) < 10e-5\n \n if __name__ == '__main__':\n     unittest.main()\n\n@@ -19,7 +19,8 @@ class TestBatchNormalization(unittest.TestCase):\n         norm_m1 = normalization.BatchNormalization(input_shape=(10, 10), mode=1)\n \n         # mode 3 does not exist\n-        self.assertRaises(Exception, normalization.BatchNormalization(input_shape=(10, 10), mode=3))\n+        self.assertRaises(Exception,\n+                          normalization.BatchNormalization(input_shape=(10, 10), mode=3))\n \n     def test_mode_0(self):\n         model = Sequential()\n@@ -77,8 +78,8 @@ class TestBatchNormalization(unittest.TestCase):\n             else:\n                 self.assertAlmostEqual(K.eval(K.std(out)), 0.0, places=2)\n \n-        assert_allclose(norm_m1.gamma.eval(), np.ones(10))\n-        assert_allclose(norm_m1.beta.eval(), np.ones(10))\n+        assert_allclose(K.eval(norm_m1.gamma), np.ones(10))\n+        assert_allclose(K.eval(norm_m1.beta), np.ones(10))\n \n     def test_config(self):\n         norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n\n@@ -1,10 +1,10 @@\n import unittest\n import numpy as np\n from keras.models import Sequential\n-from keras.layers.core import Merge, Dense, Activation, Flatten\n+from keras.layers.core import Dense, Activation, Flatten\n from keras.layers.embeddings import Embedding\n-from theano import function\n from keras.constraints import unitnorm\n+from keras import backend as K\n \n \n class TestEmbedding(unittest.TestCase):\n@@ -14,13 +14,16 @@ class TestEmbedding(unittest.TestCase):\n \n     def test_unitnorm_constraint(self):\n         lookup = Sequential()\n-        lookup.add(Embedding(3, 2, weights=[self.W1], W_constraint=unitnorm(), input_length=1))\n+        lookup.add(Embedding(3, 2, weights=[self.W1],\n+                             W_constraint=unitnorm(),\n+                             input_length=1))\n         lookup.add(Flatten())\n         lookup.add(Dense(1))\n         lookup.add(Activation('sigmoid'))\n-        lookup.compile(loss='binary_crossentropy', optimizer='sgd', class_mode='binary')\n+        lookup.compile(loss='binary_crossentropy', optimizer='sgd',\n+                       class_mode='binary')\n         lookup.train_on_batch(self.X1, np.array([[1], [0]], dtype='int32'))\n-        norm = np.linalg.norm(lookup.params[0].get_value(), axis=1)\n+        norm = np.linalg.norm(K.get_value(lookup.params[0]), axis=1)\n         self.assertTrue(np.allclose(norm, np.ones_like(norm).astype('float32')))\n \n if __name__ == '__main__':\n\n@@ -13,10 +13,16 @@ X2 = np.random.random((100, 32))\n y = np.random.random((100, 4))\n y2 = np.random.random((100,))\n \n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n-                                                     classification=False, output_shape=(4,))\n-(X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n-                                                         classification=False, output_shape=(1,))\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n+                                                     nb_test=200,\n+                                                     input_shape=(32,),\n+                                                     classification=False,\n+                                                     output_shape=(4,))\n+(X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000,\n+                                                         nb_test=200,\n+                                                         input_shape=(32,),\n+                                                         classification=False,\n+                                                         output_shape=(1,))\n \n \n class TestGraph(unittest.TestCase):\n@@ -29,10 +35,13 @@ class TestGraph(unittest.TestCase):\n         graph.add_node(Dense(4), name='dense2', input='input1')\n         graph.add_node(Dense(4), name='dense3', input='dense1')\n \n-        graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n+        graph.add_output(name='output1',\n+                         inputs=['dense2', 'dense3'],\n+                         merge_mode='sum')\n         graph.compile('rmsprop', {'output1': 'mse'})\n \n-        history = graph.fit({'input1': X_train, 'output1': y_train}, nb_epoch=10)\n+        history = graph.fit({'input1': X_train, 'output1': y_train},\n+                            nb_epoch=10)\n         out = graph.predict({'input1': X_test})\n         assert(type(out == dict))\n         assert(len(out) == 1)\n@@ -60,12 +69,15 @@ class TestGraph(unittest.TestCase):\n         graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n \n         graph.add_node(Dense(16), name='dense3', input='dense2')\n-        graph.add_node(Dense(4), name='dense4', inputs=['dense1', 'dense3'], merge_mode='sum')\n+        graph.add_node(Dense(4), name='dense4', inputs=['dense1', 'dense3'],\n+                       merge_mode='sum')\n \n-        graph.add_output(name='output1', inputs=['dense2', 'dense4'], merge_mode='sum')\n+        graph.add_output(name='output1', inputs=['dense2', 'dense4'],\n+                         merge_mode='sum')\n         graph.compile('rmsprop', {'output1': 'mse'})\n \n-        history = graph.fit({'input1': X_train, 'output1': y_train}, nb_epoch=10)\n+        history = graph.fit({'input1': X_train, 'output1': y_train},\n+                            nb_epoch=10)\n         out = graph.predict({'input1': X_train})\n         assert(type(out == dict))\n         assert(len(out) == 1)\n@@ -86,10 +98,12 @@ class TestGraph(unittest.TestCase):\n         graph.add_node(Dense(4), name='dense2', input='input2')\n         graph.add_node(Dense(4), name='dense3', input='dense1')\n \n-        graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n+        graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+                         merge_mode='sum')\n         graph.compile('rmsprop', {'output1': 'mse'})\n \n-        history = graph.fit({'input1': X_train, 'input2': X2_train, 'output1': y_train}, nb_epoch=10)\n+        history = graph.fit({'input1': X_train, 'input2': X2_train, 'output1': y_train},\n+                            nb_epoch=10)\n         out = graph.predict({'input1': X_test, 'input2': X2_test})\n         assert(type(out == dict))\n         assert(len(out) == 1)\n@@ -113,7 +127,8 @@ class TestGraph(unittest.TestCase):\n         graph.add_output(name='output2', input='dense3')\n         graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n \n-        history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train}, nb_epoch=10)\n+        history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n+                            nb_epoch=10)\n         out = graph.predict({'input1': X_test})\n         assert(type(out == dict))\n         assert(len(out) == 2)\n@@ -157,7 +172,8 @@ class TestGraph(unittest.TestCase):\n \n         graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n \n-        history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train}, nb_epoch=10,\n+        history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n+                            nb_epoch=10,\n                             sample_weight={'output1': weights1, 'output2': weights2})\n         out = graph.predict({'input1': X_test})\n         assert(type(out == dict))\n@@ -178,7 +194,8 @@ class TestGraph(unittest.TestCase):\n         graph.add_node(Dense(16), name='dense1', input='input1')\n         graph.add_node(Dense(4), name='dense2', input='input1')\n         graph.add_node(Dense(4), name='dense3', input='dense1')\n-        graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n+        graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+                         merge_mode='sum')\n \n         seq = Sequential()\n         seq.add(Dense(32, input_shape=(32,)))\n@@ -204,10 +221,12 @@ class TestGraph(unittest.TestCase):\n         graph.add_node(Dense(16), name='dense1', input='input1')\n         graph.add_node(Dense(4), name='dense2', input='input1')\n         graph.add_node(Dense(4), name='dense3', input='dense1')\n-        graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'], merge_mode='sum', create_output=True)\n+        graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'],\n+                       merge_mode='sum', create_output=True)\n         graph.compile('rmsprop', {'output1': 'mse'})\n \n-        history = graph.fit({'input1': X_train, 'output1': y_train}, nb_epoch=10)\n+        history = graph.fit({'input1': X_train, 'output1': y_train},\n+                            nb_epoch=10)\n         out = graph.predict({'input1': X_test})\n         assert(type(out == dict))\n         assert(len(out) == 1)\n\n@@ -18,8 +18,8 @@ class TestLossMasking(unittest.TestCase):\n         model.add(TimeDistributedDense(1, init='one'))\n         model.compile(loss='mse', optimizer='sgd')\n         y = model.predict(X)\n-        loss = model.fit(X, 4*y, nb_epoch=1, batch_size=2, verbose=1).history['loss'][0]\n-        assert loss == 285.\n+        history = model.fit(X, 4 * y, nb_epoch=1, batch_size=2, verbose=1)\n+        assert history.history['loss'][0] == 285.\n \n     def test_loss_masking_time(self):\n         weighted_loss = weighted_objective(objectives.get('mae'))\n@@ -27,17 +27,16 @@ class TestLossMasking(unittest.TestCase):\n         X = np.arange(24).reshape(shape)\n         Y = 2 * X\n \n-        weights = np.ones((3, 4, 1))  # Normally the trailing 1 is added by standardize_weights\n-        weights[0, 0] = 0\n+        # Normally the trailing 1 is added by standardize_weights\n+        weights = np.ones((3,))\n         mask = np.ones((3, 4))\n         mask[1, 0] = 0\n \n-        out = K.eval(weighted_loss(X, Y, weights, mask))\n-        weights[0, 0] = 1e-9  # so that nonzero() doesn't remove this weight\n-        out2 = K.eval(weighted_loss(X, Y, weights, mask))\n+        out = K.eval(weighted_loss(K.variable(X),\n+                                   K.variable(Y),\n+                                   K.variable(weights),\n+                                   K.variable(mask)))\n         print(out)\n-        print(out2)\n-        assert abs(out - out2) < 1e-8\n \n \n if __name__ == '__main__':\n\n@@ -60,20 +60,29 @@ def create_graph_model():\n \n def _test_weights_sequential(model, class_weight=None, sample_weight=None):\n     if sample_weight is not None:\n-        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch // 3, verbose=0,\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch // 3, verbose=0,\n                   class_weight=class_weight, sample_weight=sample_weight)\n-        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch // 3, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight, validation_split=0.1)\n-        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch // 3, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight, validation_data=(X_train, Y_train, sample_weight))\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch // 3, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight,\n+                  validation_split=0.1)\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch // 3, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight,\n+                  validation_data=(X_train, Y_train, sample_weight))\n     else:\n-        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch // 2, verbose=0,\n                   class_weight=class_weight, sample_weight=sample_weight)\n-        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight, validation_split=0.1)\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch // 2, verbose=0,\n+                  class_weight=class_weight, sample_weight=sample_weight,\n+                  validation_split=0.1)\n \n     model.train_on_batch(X_train[:32], Y_train[:32],\n-                         class_weight=class_weight, sample_weight=sample_weight[:32] if sample_weight is not None else None)\n+                         class_weight=class_weight,\n+                         sample_weight=sample_weight[:32] if sample_weight is not None else None)\n     model.test_on_batch(X_train[:32], Y_train[:32],\n                         sample_weight=sample_weight[:32] if sample_weight is not None else None)\n     score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n@@ -81,16 +90,23 @@ def _test_weights_sequential(model, class_weight=None, sample_weight=None):\n \n \n def _test_weights_graph(model, class_weight=None, sample_weight=None):\n-    model.fit({'input': X_train, 'output': Y_train}, batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n-              class_weight={'output': class_weight}, sample_weight={'output': sample_weight})\n-    model.fit({'input': X_train, 'output': Y_train}, batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n-              class_weight={'output': class_weight}, sample_weight={'output': sample_weight}, validation_split=0.1)\n+    model.fit({'input': X_train, 'output': Y_train},\n+              batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n+              class_weight={'output': class_weight},\n+              sample_weight={'output': sample_weight})\n+    model.fit({'input': X_train, 'output': Y_train},\n+              batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n+              class_weight={'output': class_weight},\n+              sample_weight={'output': sample_weight}, validation_split=0.1)\n \n     model.train_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n-                         class_weight={'output': class_weight}, sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n+                         class_weight={'output': class_weight},\n+                         sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n     model.test_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n                         sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n-    score = model.evaluate({'input': X_test[test_ids, :], 'output': Y_test[test_ids, :]}, verbose=0)\n+    score = model.evaluate({'input': X_test[test_ids, :],\n+                            'output': Y_test[test_ids, :]},\n+                           verbose=0)\n     return score\n \n \n\n@@ -1,7 +1,7 @@\n import unittest\n import numpy as np\n-import theano\n-from keras.utils.theano_utils import ndim_tensor\n+\n+from keras import backend as K\n from keras.layers.core import *\n from keras.layers.convolutional import *\n from keras.layers.recurrent import SimpleRNN\n@@ -9,12 +9,12 @@ from keras.layers.recurrent import SimpleRNN\n \n def check_layer_output_shape(layer, input_data):\n     ndim = len(input_data.shape)\n-    layer.input = ndim_tensor(ndim)\n+    layer.input = K.placeholder(ndim=ndim)\n     layer.set_input_shape(input_data.shape[1:])\n     expected_output_shape = layer.output_shape[1:]\n \n-    function = theano.function([layer.input], [layer.get_output()])\n-    output = function(input_data)[0]\n+    function = K.function([layer.input], [layer.get_output()])\n+    output = function([input_data])[0]\n     assert output.shape[1:] == expected_output_shape\n \n \n@@ -56,26 +56,30 @@ class TestShapeInference(unittest.TestCase):\n     # Convolutional #\n     #################\n     def test_Convolution1D(self):\n-        for border_mode in ['same', 'full', 'valid']:\n+        for border_mode in ['same', 'valid']:\n             for filter_length in [2, 3]:\n                 for subsample_length in [1, 2]:\n                     if subsample_length > 1 and border_mode == 'same':\n                         continue\n                     for input_data_shape in [(2, 3, 2), (2, 4, 2)]:\n-                        layer = Convolution1D(nb_filter=1, filter_length=filter_length,\n-                                              border_mode=border_mode, subsample_length=subsample_length)\n+                        layer = Convolution1D(nb_filter=1,\n+                                              filter_length=filter_length,\n+                                              border_mode=border_mode,\n+                                              subsample_length=subsample_length)\n                         input_data = np.random.random(input_data_shape)\n                         check_layer_output_shape(layer, input_data)\n \n     def test_Convolution2D(self):\n-        for border_mode in ['same', 'full', 'valid']:\n+        for border_mode in ['same', 'valid']:\n             for nb_row, nb_col in [(2, 1), (3, 2)]:\n                 for subsample in [(1, 1), (2, 2)]:\n                     if (subsample[0] > 1 or subsample[1] > 1) and border_mode == 'same':\n                         continue\n                     for input_data_shape in [(2, 1, 3, 3), (2, 1, 4, 4)]:\n-                        layer = Convolution2D(nb_filter=1, nb_row=nb_row, nb_col=nb_row,\n-                                              border_mode=border_mode, subsample=subsample)\n+                        layer = Convolution2D(nb_filter=1, nb_row=nb_row,\n+                                              nb_col=nb_row,\n+                                              border_mode=border_mode,\n+                                              subsample=subsample)\n                         input_data = np.random.random(input_data_shape)\n                         check_layer_output_shape(layer, input_data)\n \n@@ -84,16 +88,20 @@ class TestShapeInference(unittest.TestCase):\n             for stride in [1, 2]:\n                 for pool_length in [1, 2]:\n                     for input_data_shape in [(2, 1, 3), (2, 1, 4)]:\n-                        layer = MaxPooling1D(pool_length=pool_length, stride=stride, ignore_border=ignore_border)\n+                        layer = MaxPooling1D(pool_length=pool_length,\n+                                             stride=stride,\n+                                             border_mode='valid')\n                         input_data = np.random.random(input_data_shape)\n                         check_layer_output_shape(layer, input_data)\n \n     def test_MaxPooling2D(self):\n         for ignore_border in [True, False]:\n-            for stride in [(1, 1), (2, 2)]:\n+            for strides in [(1, 1), (2, 2)]:\n                 for pool_size in [(2, 2), (3, 3), (4, 4)]:\n                     for input_data_shape in [(2, 1, 3, 3), (2, 1, 4, 4), (2, 1, 5, 5), (2, 1, 6, 6)]:\n-                        layer = MaxPooling2D(pool_size=pool_size, stride=stride, ignore_border=ignore_border)\n+                        layer = MaxPooling2D(pool_size=pool_size,\n+                                             strides=strides,\n+                                             border_mode='valid')\n                         input_data = np.random.random(input_data_shape)\n                         check_layer_output_shape(layer, input_data)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8b75182a177ce4772082d1fcb8376a9e535a12c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 11 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n from __future__ import absolute_import\n+from .common import epsilon, floatx, set_epsilon, set_floatx\n \n _BACKEND = 'theano'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#89132a6986781ae6519e3ee912336b50caa11a31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 221 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -10,12 +10,16 @@ from keras.models import Sequential\n from keras.layers.core import TimeDistributedDense, Activation\n from keras.layers.recurrent import LSTM\n from keras.optimizers import Adam\n+from keras.utils import generic_utils\n \n from keras.layers.ntm import NeuralTuringMachine as NTM\n \n \"\"\"\n Copy Problem defined in Graves et. al [0]\n-After 3000 gradients updates, the accuracy becomes >92%.\n+After about 3500 updates, the accuracy becomes jumps from around 50% to >90%.\n+\n+Estimated compile time: 12 min\n+Estimated time to train Neural Turing Machine and 3 layer LSTM on an NVidia GTX 680: 2h\n \n \n [0]: http://arxiv.org/pdf/1410.5401v2.pdf\n@@ -24,11 +28,11 @@ After 3000 gradients updates, the accuracy becomes >92%.\n batch_size = 100\n \n h_dim = 128\n-n_slots = 121\n+n_slots = 128\n m_length = 20\n input_dim = 8\n lr = 1e-3\n-clipnorm = 10\n+clipvalue = 10\n \n ##### Neural Turing Machine ######\n \n@@ -39,12 +43,12 @@ model.add(ntm)\n model.add(TimeDistributedDense(input_dim))\n model.add(Activation('sigmoid'))\n \n-sgd = Adam(lr=lr, clipnorm=clipnorm)\n+sgd = Adam(lr=lr, clipvalue=clipvalue)\n model.compile(loss='binary_crossentropy', optimizer=sgd)\n \n # LSTM - Run this for comparison\n \n-sgd = Adam(lr=lr, clipnorm=clipnorm)\n+sgd2 = Adam(lr=lr, clipvalue=clipvalue)\n lstm = Sequential()\n lstm.add(LSTM(input_dim=input_dim, output_dim=h_dim*2, return_sequences=True))\n lstm.add(LSTM(output_dim=h_dim*2, return_sequences=True))\n@@ -120,9 +124,9 @@ for e in range(nb_epoch):\n     if e % 500 == 0:\n         print(\"\")\n         acc1 = test_model(model, 'ntm.png')\n-        acc2 = test_model(model, 'lstm.png')\n-        print(\"NTM  test acc: {}\".format(a))\n-        print(\"LSTM test acc: {}\".format(a))\n+        acc2 = test_model(lstm, 'lstm.png')\n+        print(\"NTM  test acc: {}\".format(acc1))\n+        print(\"LSTM test acc: {}\".format(acc2))\n \n ##### VISUALIZATION #####\n X = model.get_input()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1b54159b8afacdbb1f54e905f21466e4e3d2d75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 55 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,8 +3,7 @@ import pydot\n # that works with python3 such as pydot2 or pydot\n from keras.models import Sequential, Graph\n \n-def plot(model, to_file='model.png'):\n-\n+def to_graph(model):\n     graph = pydot.Dot(graph_type='digraph')\n     if type(model) == Sequential:\n         previous_node = None\n@@ -20,8 +19,6 @@ def plot(model, to_file='model.png'):\n             if previous_node:\n                 graph.add_edge(pydot.Edge(previous_node, current_node))\n             previous_node = current_node\n-        graph.write_png(to_file)\n-\n     elif type(model) == Graph:\n         # don't need to append number for names since all nodes labeled\n         for input_node in model.input_config:\n@@ -37,5 +34,8 @@ def plot(model, to_file='model.png'):\n                         graph.add_edge(pydot.Edge(e, node['name']))\n                 else:\n                     graph.add_edge(pydot.Edge(node['input'], node['name']))\n+    return graph\n \n+def plot(model, to_file='model.png'):\n+    graph = to_graph(model)\n     graph.write_png(to_file)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d3880dc359ef67e30dc6fd751fcfafef7aa281d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 228 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -16,12 +16,15 @@ from keras.layers.ntm import NeuralTuringMachine as NTM\n \n \"\"\"\n Copy Problem defined in Graves et. al [0]\n-After about 3500 updates, the accuracy becomes jumps from around 50% to >90%.\n+\n+Training data is made of sequences with length 1 to 20. \n+Test data are sequences of length 100.\n+The model is tested every 500 weight updates.\n+After about 3500 updates, the accuracy jumps from around 50% to >90%.\n \n Estimated compile time: 12 min\n Estimated time to train Neural Turing Machine and 3 layer LSTM on an NVidia GTX 680: 2h\n \n-\n [0]: http://arxiv.org/pdf/1410.5401v2.pdf\n \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#37ebbc3a1cf1d284d9ef2afefb405884effb6928", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 615 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -534,7 +534,4 @@ more TODO:\n \n tensordot -> soon to be introduced in TF\n batched_tensordot -> reimplement\n-\n-addbroadcast -> remove usage?\n-unbroadcast -> remove usage?\n '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3227855439b69883bfc5e0ed7d94efd704dec29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 405 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -155,7 +155,7 @@ input_encoder_m.add(Embedding(input_dim=vocab_size,\n                               output_dim=64,\n                               input_length=story_maxlen))\n # output: (samples, story_maxlen, embedding_dim)\n-# embed the question into a single vector\n+# embed the question into a sequence of vectors\n question_encoder = Sequential()\n question_encoder.add(Embedding(input_dim=vocab_size,\n                                output_dim=64,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#819f569ca88b849d7335e4b4161c5130245d09ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 407 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -162,7 +162,7 @@ question_encoder.add(Embedding(input_dim=vocab_size,\n                                input_length=query_maxlen))\n # output: (samples, query_maxlen, embedding_dim)\n # compute a 'match' between input sequence elements (which are vectors)\n-# and the question vector\n+# and the question vector sequence\n match = Sequential()\n match.add(Merge([input_encoder_m, question_encoder],\n                 mode='dot',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98c6c8b3b6c81de7ec3583f49d45a52c38b42d2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 137 | Lines Deleted: 31 | Files Changed: 1 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 168 | Churn Cumulative: 223 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 0.6170212765957447\n\nDIFF:\n@@ -1,40 +1,146 @@\n import pydot\n # old pydot will not work with python3, must use one\n # that works with python3 such as pydot2 or pydot\n-from keras.models import Sequential, Graph\n+import itertools\n+from keras.layers.containers import Graph, Sequential\n+from keras.layers.core import Merge\n \n-def to_graph(model):\n-    graph = pydot.Dot(graph_type='digraph')\n-    if type(model) == Sequential:\n-        previous_node = None\n-        written_nodes = []\n-        n = 1\n-        for node in model.get_config()['layers']:\n-            # append number in case layers have same name to differentiate\n-            if (node['name'] + str(n)) in written_nodes:\n-                n += 1\n-            current_node = pydot.Node(node['name'] + str(n))\n-            written_nodes.append(node['name'] + str(n))\n-            graph.add_node(current_node)\n-            if previous_node:\n-                graph.add_edge(pydot.Edge(previous_node, current_node))\n-            previous_node = current_node\n-    elif type(model) == Graph:\n-        # don't need to append number for names since all nodes labeled\n-        for input_node in model.input_config:\n-            graph.add_node(pydot.Node(input_node['name']))\n \n-        # intermediate and output nodes have input defined\n-        for layer_config in [model.node_config, model.output_config]:\n-            for node in layer_config:\n-                graph.add_node(pydot.Node(node['name']))\n-                # possible to have multiple 'inputs' vs 1 'input'\n-                if node['inputs']:\n-                    for e in node['inputs']:\n-                        graph.add_edge(pydot.Edge(e, node['name']))\n+def layer_typename(layer):\n+    return type(layer).__module__ + \".\" + type(layer).__name__\n+\n+\n+def get_layer_to_name(model):\n+    \"\"\"Returns a dict mapping layer to their name in the model\"\"\"\n+    if not isinstance(model, Graph):\n+        return {}\n     else:\n-                    graph.add_edge(pydot.Edge(node['input'], node['name']))\n-    return graph\n+        node_to_name = itertools.chain(\n+            model.nodes.items(), model.inputs.items(), model.outputs.items()\n+        )\n+        return {v: k for k, v in node_to_name}\n+\n+\n+class ModelToDot(object):\n+    \"\"\"\n+    This is a helper class which visits a keras model (Sequential or Graph) and\n+    returns a pydot.Graph representation.\n+\n+    This is implemented as a class because we need to maintain various states.\n+\n+    Use it as ```ModelToDot()(model)```\n+\n+    Keras models can have an arbitrary number of inputs and outputs. A given\n+    layer can have multiple inputs but has a single output. We therefore\n+    explore the model by starting at its output and crawling \"up\" the tree.\n+    \"\"\"\n+    def _pydot_node_for_layer(self, layer, label):\n+        \"\"\"\n+        Returns the pydot.Node corresponding to the given layer.\n+        `label` specify the name of the layer (only used if the layer isn't yet\n+            associated with a pydot.Node)\n+        \"\"\"\n+        # Check if this already exists (will be the case for nodes that\n+        # serve as input to more than one layer)\n+        if layer in self.layer_to_pydotnode:\n+            node = self.layer_to_pydotnode[layer]\n+        else:\n+            layer_id = 'layer%d' % self.idgen\n+            self.idgen += 1\n+\n+            label = label + \" (\" + layer_typename(layer) + \")\"\n+\n+            if self.show_shape:\n+                # Build the label that will actually contain a table with the\n+                # input/output\n+                outputlabels = str(layer.output_shape)\n+                if hasattr(layer, 'input_shape'):\n+                    inputlabels = str(layer.input_shape)\n+                elif hasattr(layer, 'input_shapes'):\n+                    inputlabels = ', '.join(\n+                        [str(ishape) for ishape in layer.input_shapes])\n+                else:\n+                    inputlabels = ''\n+                label = \"%s\\n|{input:|output:}|{{%s}|{%s}}\" % (\n+                        label, inputlabels, outputlabels)\n+\n+            node = pydot.Node(layer_id, label=label)\n+            self.g.add_node(node)\n+            self.layer_to_pydotnode[layer] = node\n+        return node\n+\n+    def _process_layer(self, layer, layer_to_name=None, connect_to=None):\n+        \"\"\"\n+        Process a layer, adding its node to the graph and creating edges to its\n+        outputs.\n+\n+        `connect_to` specify where the output of the current layer will be\n+            connected\n+        `layer_to_name` is a dict mapping layer to their name in the Graph\n+            model. Should be {} when processing a Sequential model\n+        \"\"\"\n+        # The layer can be a container layer, in which case we can recurse\n+        is_graph = isinstance(layer, Graph)\n+        is_seq = isinstance(layer, Sequential)\n+        if self.recursive and (is_graph or is_seq):\n+            # We got a container layer, recursively transform it\n+            if is_graph:\n+                child_layers = layer.outputs.values()\n+            else:\n+                child_layers = [layer.layers[-1]]\n+            for l in child_layers:\n+                self._process_layer(l, layer_to_name=get_layer_to_name(layer),\n+                                    connect_to=connect_to)\n+        else:\n+            # This is a simple layer.\n+            label = layer_to_name.get(layer, '')\n+            layer_node = self._pydot_node_for_layer(layer, label=label)\n+\n+            if connect_to is not None:\n+                self.g.add_edge(pydot.Edge(layer_node, connect_to))\n+\n+            # Proceed upwards to the parent(s). Only Merge layers have more\n+            # than one parent\n+            if isinstance(layer, Merge):  # Merge layer\n+                for l in layer.layers:\n+                    self._process_layer(l, layer_to_name,\n+                                        connect_to=layer_node)\n+            elif hasattr(layer, 'previous') and layer.previous is not None:\n+                self._process_layer(layer.previous, layer_to_name,\n+                                    connect_to=layer_node)\n+\n+    def __call__(self, model, recursive=True, show_shape=False,\n+                 connect_to=None):\n+        self.idgen = 0\n+        # Maps keras layer to the pydot.Node representing them\n+        self.layer_to_pydotnode = {}\n+        self.recursive = recursive\n+        self.show_shape = show_shape\n+\n+        self.g = pydot.Dot()\n+        self.g.set('rankdir', 'TB')\n+        self.g.set('concentrate', True)\n+        self.g.set_node_defaults(shape='record', fontname=\"Fira Mono\")\n+\n+        if hasattr(model, 'outputs'):\n+            # Graph\n+            for name, l in model.outputs.items():\n+                self._process_layer(l, get_layer_to_name(model),\n+                                    connect_to=connect_to)\n+        else:\n+            # Sequential container\n+            self._process_layer(model.layers[-1], {}, connect_to=connect_to)\n+\n+        return self.g\n+\n+\n+def to_graph(model, **kwargs):\n+    \"\"\"\n+    `recursive` controls wether we recursively explore container layers\n+    `show_shape` controls wether the shape is shown in the graph\n+    \"\"\"\n+    return ModelToDot()(model, **kwargs)\n+\n \n def plot(model, to_file='model.png'):\n     graph = to_graph(model)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
