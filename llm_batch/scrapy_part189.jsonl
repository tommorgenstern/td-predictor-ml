{"custom_id": "scrapy#21b9f377d6d09cfc3bc88d1c09decce9bf75c8ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 37 | Churn Cumulative: 718 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,9 +2,46 @@\n This module is kept to provide a helpful warning about its removal.\n \"\"\"\n \n+import logging\n import warnings\n+\n+from twisted.python.failure import Failure\n+\n from scrapy.exceptions import ScrapyDeprecationWarning\n+\n+logger = logging.getLogger('scrapy')\n+\n warnings.warn(\"Module `scrapy.log` has been deprecated, Scrapy now relies on \"\n               \"the builtin Python library for logging. Read the updated \"\n               \"logging entry in the documentation to learn more.\",\n               ScrapyDeprecationWarning, stacklevel=2)\n+\n+\n+# Imports kept for backwards-compatibility\n+\n+DEBUG = logging.DEBUG\n+INFO = logging.INFO\n+WARNING = logging.WARNING\n+ERROR = logging.ERROR\n+CRITICAL = logging.CRITICAL\n+SILENT = CRITICAL + 1\n+\n+\n+def msg(message, _level=logging.INFO, **kw):\n+    warnings.warn('log.msg has been deprecated, create a python logger and '\n+                  'log through it instead',\n+                  ScrapyDeprecationWarning, stacklevel=2)\n+\n+    level = kw.pop('level', _level)\n+    logger.log(level, message, kw)\n+\n+\n+def err(_stuff=None, _why=None, **kw):\n+    warnings.warn('log.err has been deprecated, create a python logger and '\n+                  'use its error method instead',\n+                  ScrapyDeprecationWarning, stacklevel=2)\n+\n+    level = kw.pop('level', logging.ERROR)\n+    failure = kw.pop('failure', _stuff) or Failure()\n+    message = kw.pop('why', _why) or failure.value\n+    logger.log(level, message, kw, extra={'failure': failure})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ccdd8bfbccddc6fcf1bbc1f28cef3266d7c12eca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 404 | Contributors (this commit): 17 | Commits (past 90d): 12 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -167,6 +167,8 @@ ITEM_PIPELINES_BASE = {}\n LOG_ENABLED = True\n LOG_ENCODING = 'utf-8'\n LOG_FORMATTER = 'scrapy.logformatter.LogFormatter'\n+LOG_FORMAT = '%(asctime)s [%(name)s] %(levelname)s: %(message)s'\n+LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S%z'\n LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n LOG_FILE = None\n\n@@ -86,8 +86,8 @@ def configure_logging(settings=None):\n             handler = logging.NullHandler()\n \n         formatter = logging.Formatter(\n-            fmt='%(asctime)s [%(name)s] %(levelname)s: %(message)s',\n-            datefmt='%Y-%m-%d %H:%M:%S%z'\n+            fmt=settings.get('LOG_FORMAT'),\n+            datefmt=settings.get('LOG_DATEFORMAT')\n         )\n         handler.setFormatter(formatter)\n         handler.setLevel(settings.get('LOG_LEVEL'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f54ca3294600893ed5da242d07fa4d285da5adf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 34 | Files Changed: 34 | Hunks: 34 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 68 | Churn Cumulative: 12301 | Contributors (this commit): 51 | Commits (past 90d): 107 | Contributors (cumulative): 214 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ from scrapy.utils.conf import arglist_to_dict\n from scrapy.utils.spider import iterate_spider_output, spidercls_for_request\n from scrapy.exceptions import UsageError\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class Command(ScrapyCommand):\n\n@@ -14,7 +14,7 @@ from pdb import Pdb\n from scrapy.utils.engine import format_engine_status\n from scrapy.utils.trackref import format_live_refs\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class StackTraceDump(object):\n\n@@ -11,7 +11,7 @@ from scrapy.http import HtmlResponse\n from scrapy.utils.response import _noscript_re, _script_re\n \n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class AjaxCrawlMiddleware(object):\n\n@@ -7,7 +7,7 @@ from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class CookiesMiddleware(object):\n\n@@ -18,7 +18,7 @@ except ImportError:\n \n from scrapy.responsetypes import responsetypes\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class DecompressionMiddleware(object):\n\n@@ -5,7 +5,7 @@ from scrapy.http import HtmlResponse\n from scrapy.utils.response import get_meta_refresh\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class BaseRedirectMiddleware(object):\n\n@@ -28,7 +28,7 @@ from scrapy.exceptions import NotConfigured\n from scrapy.utils.response import response_status_message\n from scrapy.xlib.tx import ResponseFailed\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class RetryMiddleware(object):\n\n@@ -12,7 +12,7 @@ from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class RobotsTxtMiddleware(object):\n\n@@ -23,7 +23,7 @@ from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import get_func_args\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class IFeedStorage(Interface):\n\n@@ -5,7 +5,7 @@ from twisted.internet import task\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class LogStats(object):\n\n@@ -16,7 +16,7 @@ from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.utils.engine import get_engine_status\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class MemoryUsage(object):\n\n@@ -26,7 +26,7 @@ from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class FileException(Exception):\n\n@@ -9,7 +9,7 @@ from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class MediaPipeline(object):\n\n@@ -8,7 +8,7 @@ import logging\n \n from scrapy.http import Request\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class DepthMiddleware(object):\n\n@@ -7,7 +7,7 @@ import logging\n \n from scrapy.exceptions import IgnoreRequest\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class HttpError(IgnoreRequest):\n\n@@ -11,7 +11,7 @@ from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n class OffsiteMiddleware(object):\n \n\n@@ -9,7 +9,7 @@ import logging\n from scrapy.http import Request\n from scrapy.exceptions import NotConfigured\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class UrlLengthMiddleware(object):\n\n@@ -6,7 +6,7 @@ from scrapy.http import Request, XmlResponse\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n from scrapy.utils.gz import gunzip, is_gzipped\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class SitemapSpider(Spider):\n\n@@ -3,7 +3,7 @@ import logging\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class AutoThrottle(object):\n\n@@ -21,7 +21,7 @@ from scrapy.core.downloader.webclient import _parse\n from scrapy.utils.misc import load_object\n from scrapy import twisted_version\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class HTTP11DownloadHandler(object):\n\n@@ -17,7 +17,7 @@ from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class Slot(object):\n\n@@ -8,7 +8,7 @@ from scrapy.utils.reqser import request_to_dict, request_from_dict\n from scrapy.utils.misc import load_object\n from scrapy.utils.job import job_dir\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class Scheduler(object):\n\n@@ -16,7 +16,7 @@ from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class Slot(object):\n\n@@ -18,7 +18,7 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.log import LogCounterHandler, configure_logging, log_scrapy_info\n from scrapy import signals\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class Crawler(object):\n\n@@ -33,7 +33,7 @@ class RFPDupeFilter(BaseDupeFilter):\n         self.fingerprints = set()\n         self.logdupes = True\n         self.debug = debug\n-        self.logger = logging.getLogger('scrapy')\n+        self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n             self.fingerprints.update(x.rstrip() for x in self.file)\n\n@@ -9,7 +9,7 @@ from twisted.python.failure import Failure\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n warnings.warn(\"Module `scrapy.log` has been deprecated, Scrapy now relies on \"\n               \"the builtin Python library for logging. Read the updated \"\n\n@@ -22,7 +22,7 @@ else:\n from twisted.internet import defer, reactor, ssl\n from twisted.mail.smtp import ESMTPSenderFactory\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class MailSender(object):\n\n@@ -5,7 +5,7 @@ from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import process_parallel, process_chain, process_chain_both\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class MiddlewareManager(object):\n\n@@ -4,7 +4,7 @@ Scrapy extension for collecting scraping stats\n import pprint\n import logging\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class StatsCollector(object):\n\n@@ -27,7 +27,7 @@ try:\n except ImportError:\n     hpy = None\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n # signal to update telnet variables\n # args: telnet_vars\n\n@@ -13,7 +13,7 @@ from scrapy.http import TextResponse, Response\n from scrapy.selector import Selector\n from scrapy.utils.python import re_rsearch, str_to_unicode\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n def xmliter(obj, nodename):\n\n@@ -11,7 +11,7 @@ from twisted.python import log as twisted_log\n import scrapy\n from scrapy.settings import overridden_settings\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n class FailureFormatter(logging.Filter):\n\n@@ -9,7 +9,7 @@ from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \\\n     getAllReceivers, disconnect\n from scrapy.xlib.pydispatch.robustapply import robustApply\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n\n@@ -6,7 +6,7 @@ import six\n from scrapy.spider import Spider\n from scrapy.utils.misc import  arg_to_iter\n \n-logger = logging.getLogger('scrapy')\n+logger = logging.getLogger(__name__)\n \n \n def iterate_spider_output(result):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d1205063c50563cb9867ae254c152e1e8ad5afa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 8 | Complexity Δ (Sum/Max): 10/5 | Churn Δ: 60 | Churn Cumulative: 299 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 0.9666666666666667\n\nDIFF:\n@@ -32,6 +32,27 @@ class FailureFormatter(logging.Filter):\n         return True\n \n \n+class TopLevelFormatter(logging.Filter):\n+    \"\"\"Keep only top level loggers's name (direct children from root) from\n+    records.\n+\n+    This filter will replace Scrapy loggers' names with 'scrapy'. This mimics\n+    the old Scrapy log behaviour and helps shortening long names.\n+\n+    Since it can't be set for just one logger (it won't propagate for its\n+    children), it's going to be set in the root handler, with a parametrized\n+    `loggers` list where it should act.\n+    \"\"\"\n+\n+    def __init__(self, loggers=None):\n+        self.loggers = loggers or []\n+\n+    def filter(self, record):\n+        if any(record.name.startswith(l + '.') for l in self.loggers):\n+            record.name = record.name.split('.', 1)[0]\n+        return True\n+\n+\n DEFAULT_LOGGING = {\n     'version': 1,\n     'disable_existing_loggers': False,\n@@ -91,6 +112,7 @@ def configure_logging(settings=None):\n         )\n         handler.setFormatter(formatter)\n         handler.setLevel(settings.get('LOG_LEVEL'))\n+        handler.addFilter(TopLevelFormatter(['scrapy']))\n         logging.root.addHandler(handler)\n \n \n\n@@ -8,7 +8,8 @@ import unittest\n from testfixtures import LogCapture\n from twisted.python.failure import Failure\n \n-from scrapy.utils.log import FailureFormatter, LogCounterHandler, StreamLogger\n+from scrapy.utils.log import (FailureFormatter, TopLevelFormatter,\n+                              LogCounterHandler, StreamLogger)\n from scrapy.utils.test import get_crawler\n \n \n@@ -49,6 +50,41 @@ class FailureFormatterTest(unittest.TestCase):\n                                   'test log msg' + os.linesep + '3')\n \n \n+class TopLevelFormatterTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.handler = LogCapture()\n+        self.handler.addFilter(TopLevelFormatter(['test']))\n+\n+    def test_top_level_logger(self):\n+        logger = logging.getLogger('test')\n+        with self.handler as l:\n+            logger.warning('test log msg')\n+\n+        l.check(('test', 'WARNING', 'test log msg'))\n+\n+    def test_children_logger(self):\n+        logger = logging.getLogger('test.test1')\n+        with self.handler as l:\n+            logger.warning('test log msg')\n+\n+        l.check(('test', 'WARNING', 'test log msg'))\n+\n+    def test_overlapping_name_logger(self):\n+        logger = logging.getLogger('test2')\n+        with self.handler as l:\n+            logger.warning('test log msg')\n+\n+        l.check(('test2', 'WARNING', 'test log msg'))\n+\n+    def test_different_name_logger(self):\n+        logger = logging.getLogger('different')\n+        with self.handler as l:\n+            logger.warning('test log msg')\n+\n+        l.check(('different', 'WARNING', 'test log msg'))\n+\n+\n class LogCounterHandlerTest(unittest.TestCase):\n \n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4858af4e946919b80de6e67204f5a2925bc85800", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 730 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,13 +27,17 @@ CRITICAL = logging.CRITICAL\n SILENT = CRITICAL + 1\n \n \n-def msg(message, _level=logging.INFO, **kw):\n+def msg(message=None, _level=logging.INFO, **kw):\n     warnings.warn('log.msg has been deprecated, create a python logger and '\n                   'log through it instead',\n                   ScrapyDeprecationWarning, stacklevel=2)\n \n     level = kw.pop('level', _level)\n-    logger.log(level, message, kw)\n+    message = kw.pop('format', message)\n+    # NOTE: logger.log doesn't handle well passing empty dictionaries with format\n+    # arguments because of some weird use-case:\n+    # https://hg.python.org/cpython/file/648dcafa7e5f/Lib/logging/__init__.py#l269\n+    logger.log(level, message, *[kw] if kw else [])\n \n \n def err(_stuff=None, _why=None, **kw):\n@@ -44,4 +48,4 @@ def err(_stuff=None, _why=None, **kw):\n     level = kw.pop('level', logging.ERROR)\n     failure = kw.pop('failure', _stuff) or Failure()\n     message = kw.pop('why', _why) or failure.value\n-    logger.log(level, message, kw, extra={'failure': failure})\n+    logger.log(level, message, *[kw] if kw else [], extra={'failure': failure})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d8f8221e6234959f0be4673a80b142451d59bc3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 34 | Churn Cumulative: 3281 | Contributors (this commit): 20 | Commits (past 90d): 18 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,6 +16,7 @@ from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\n+from scrapy.utils.log import logformatter_adapter\n \n logger = logging.getLogger(__name__)\n \n@@ -204,7 +205,7 @@ class ExecutionEngine(object):\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n                 logkws = self.logformatter.crawled(request, response, spider)\n-                logger._log(extra={'spider': spider}, **logkws)\n+                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                 self.signals.send_catch_log(signal=signals.response_received, \\\n                     response=response, request=request, spider=spider)\n             return response\n\n@@ -10,6 +10,7 @@ from twisted.internet import defer\n from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\n+from scrapy.utils.log import logformatter_adapter\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n from scrapy.http import Request, Response\n@@ -220,7 +221,7 @@ class Scraper(object):\n             ex = output.value\n             if isinstance(ex, DropItem):\n                 logkws = self.logformatter.dropped(item, ex, response, spider)\n-                logger._log(extra={'spider': spider}, **logkws)\n+                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                 return self.signals.send_catch_log_deferred(\n                     signal=signals.item_dropped, item=item, response=response,\n                     spider=spider, exception=output.value)\n@@ -229,7 +230,7 @@ class Scraper(object):\n                              extra={'spider': spider, 'failure': output})\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n-            logger._log(extra={'spider': spider}, **logkws)\n+            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n             return self.signals.send_catch_log_deferred(\n                 signal=signals.item_scraped, item=output, response=response,\n                 spider=spider)\n\n@@ -3,6 +3,7 @@\n import os\n import sys\n import logging\n+import warnings\n from logging.config import dictConfig\n \n from twisted.python.failure import Failure\n@@ -10,6 +11,7 @@ from twisted.python import log as twisted_log\n \n import scrapy\n from scrapy.settings import overridden_settings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n logger = logging.getLogger(__name__)\n \n@@ -153,3 +155,27 @@ class LogCounterHandler(logging.Handler):\n     def emit(self, record):\n         sname = 'log_count/{}'.format(record.levelname)\n         self.crawler.stats.inc_value(sname)\n+\n+\n+def logformatter_adapter(logkws):\n+    \"\"\"\n+    Helper that takes the dictionary output from the methods in LogFormatter\n+    and adapts it into a tuple of positional arguments for logger.log calls,\n+    handling backward compatibility as well.\n+    \"\"\"\n+    if not {'level', 'msg', 'args'} <= set(logkws):\n+        warnings.warn('Missing keys in LogFormatter method',\n+                      ScrapyDeprecationWarning)\n+\n+    if 'format' in logkws:\n+        warnings.warn('`format` key in LogFormatter methods has been '\n+                      'deprecated, use `msg` instead',\n+                      ScrapyDeprecationWarning)\n+\n+    level = logkws.get('level', logging.INFO)\n+    message = logkws.get('format', logkws.get('msg'))\n+    # NOTE: This also handles 'args' being an empty dict, that case doesn't\n+    # play well in logger.log calls\n+    args = logkws if not logkws.get('args') else logkws['args']\n+\n+    return (level, message, args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc4c31e42673813be0739f3a01dfb5d7fb9417cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 18 | Churn Cumulative: 1372 | Contributors (this commit): 7 | Commits (past 90d): 22 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ import signal\n import warnings\n \n from twisted.internet import reactor, defer\n-from zope.interface.verify import verifyClass\n+from zope.interface.verify import verifyClass, DoesNotImplement\n \n from scrapy.core.engine import ExecutionEngine\n from scrapy.resolver import CachingThreadedResolver\n@@ -195,5 +195,13 @@ def _get_spider_loader(settings):\n     cls_path = settings.get('SPIDER_MANAGER_CLASS',\n                             settings.get('SPIDER_LOADER_CLASS'))\n     loader_cls = load_object(cls_path)\n+    try:\n         verifyClass(ISpiderLoader, loader_cls)\n+    except DoesNotImplement:\n+        warnings.warn(\n+            'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n+            'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n+            'Please add all missing methods to avoid unexpected runtime errors.',\n+            category=ScrapyDeprecationWarning, stacklevel=2\n+        )\n     return loader_cls.from_settings(settings.frozencopy())\n\n@@ -1,8 +1,6 @@\n import warnings\n import unittest\n \n-from zope.interface.verify import DoesNotImplement\n-\n from scrapy.crawler import Crawler, CrawlerRunner\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n@@ -71,8 +69,12 @@ class CrawlerRunnerTestCase(unittest.TestCase):\n         settings = Settings({\n             'SPIDER_LOADER_CLASS': 'tests.test_crawler.SpiderLoaderWithWrongInterface'\n         })\n-        with self.assertRaises(DoesNotImplement):\n+        with warnings.catch_warnings(record=True) as w, \\\n+                self.assertRaises(AttributeError):\n             CrawlerRunner(settings)\n+            self.assertEqual(len(w), 1)\n+            self.assertIn(\"SPIDER_LOADER_CLASS\", str(w[0].message))\n+            self.assertIn(\"scrapy.interfaces.ISpiderLoader\", str(w[0].message))\n \n     def test_crawler_runner_accepts_dict(self):\n         runner = CrawlerRunner({'foo': 'bar'})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d3633f3d2b6829f9bf90242cfc2c61539d006c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 671 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n \"\"\"\n-Scrapy - a web crawling and screen scraping framework written for Python\n+Scrapy - a web crawling and web scraping framework written for Python\n \"\"\"\n \n __all__ = ['__version__', 'version_info', 'optional_features', 'twisted_version',\n\n@@ -10,7 +10,7 @@ setup(\n     name='Scrapy',\n     version=version,\n     url='http://scrapy.org',\n-    description='A high-level Web Crawling and Screen Scraping framework',\n+    description='A high-level Web Crawling and Web Scraping framework',\n     long_description=open('README.rst').read(),\n     author='Scrapy developers',\n     maintainer='Pablo Hoffman',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d7c444fefbd9b69698618d3d18b0c3e4858ec3e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 33 | Files Changed: 33 | Hunks: 19 | Methods Changed: 1 | Complexity Δ (Sum/Max): 16/11 | Churn Δ: 66 | Churn Cumulative: 1033 | Contributors (this commit): 29 | Commits (past 90d): 43 | Contributors (cumulative): 89 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n import warnings\n from scrapy.exceptions import ScrapyDeprecationWarning\n warnings.warn(\"Module `scrapy.contrib_exp.downloadermiddleware.decompression` is deprecated, \"\n-              \"use `scrapy.contrib.downloadermiddleware.decompression` instead\",\n+              \"use `scrapy.downloadermiddlewares.decompression` instead\",\n     ScrapyDeprecationWarning, stacklevel=2)\n \n-from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware\n+from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@@ -80,21 +80,21 @@ DOWNLOADER_MIDDLEWARES = {}\n \n DOWNLOADER_MIDDLEWARES_BASE = {\n     # Engine side\n-    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n-    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n-    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n-    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n-    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n-    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n-    'scrapy.contrib.downloadermiddleware.ajaxcrawl.AjaxCrawlMiddleware': 560,\n-    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n-    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n-    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n-    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n-    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n-    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n-    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n-    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n+    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,\n+    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,\n+    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,\n+    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 400,\n+    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 500,\n+    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 550,\n+    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,\n+    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,\n+    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,\n+    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,\n+    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,\n+    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,\n+    'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware': 830,\n+    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,\n+    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,\n     # Downloader side\n }\n \n\n@@ -32,7 +32,7 @@ def load_object(path):\n     \"\"\"Load an object given its absolute object path, and return it.\n \n     object can be a class, function, variable o instance.\n-    path ie: 'scrapy.contrib.downloadermiddelware.redirect.RedirectMiddleware'\n+    path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'\n     \"\"\"\n \n     try:\n\n@@ -1,11 +1,11 @@\n import unittest\n \n-from scrapy.contrib.downloadermiddleware.ajaxcrawl import AjaxCrawlMiddleware\n+from scrapy.downloadermiddlewares.ajaxcrawl import AjaxCrawlMiddleware\n from scrapy.spider import Spider\n from scrapy.http import Request, HtmlResponse, Response\n from scrapy.utils.test import get_crawler\n \n-__doctests__ = ['scrapy.contrib.downloadermiddleware.ajaxcrawl']\n+__doctests__ = ['scrapy.downloadermiddlewares.ajaxcrawl']\n \n class AjaxCrawlMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n\n@@ -3,7 +3,7 @@ import re\n \n from scrapy.http import Response, Request\n from scrapy.spider import Spider\n-from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware\n+from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n \n \n class CookiesMiddlewareTest(TestCase):\n\n@@ -1,6 +1,6 @@\n from unittest import TestCase, main\n from scrapy.http import Response, XmlResponse\n-from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware\n+from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware\n from scrapy.spider import Spider\n from tests import get_testdata\n from scrapy.utils.test import assert_samelines\n\n@@ -1,7 +1,7 @@\n from unittest import TestCase\n import six\n \n-from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware\n+from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\n from scrapy.http import Request\n from scrapy.spider import Spider\n from scrapy.utils.test import get_crawler\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.contrib.downloadermiddleware.downloadtimeout import DownloadTimeoutMiddleware\n+from scrapy.downloadermiddlewares.downloadtimeout import DownloadTimeoutMiddleware\n from scrapy.spider import Spider\n from scrapy.http import Request\n from scrapy.utils.test import get_crawler\n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.http import Request\n-from scrapy.contrib.downloadermiddleware.httpauth import HttpAuthMiddleware\n+from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\n from scrapy.spider import Spider\n \n class TestSpider(Spider):\n\n@@ -12,7 +12,7 @@ from scrapy.spider import Spider\n from scrapy.settings import Settings\n from scrapy.exceptions import IgnoreRequest\n from scrapy.utils.test import get_crawler\n-from scrapy.contrib.downloadermiddleware.httpcache import HttpCacheMiddleware\n+from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware\n \n \n class _BaseTest(unittest.TestCase):\n\n@@ -5,7 +5,7 @@ from gzip import GzipFile\n \n from scrapy.spider import Spider\n from scrapy.http import Response, Request, HtmlResponse\n-from scrapy.contrib.downloadermiddleware.httpcompression import HttpCompressionMiddleware\n+from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware\n from tests import tests_datadir\n from w3lib.encoding import resolve_encoding\n \n\n@@ -2,7 +2,7 @@ import os\n import sys\n from twisted.trial.unittest import TestCase, SkipTest\n \n-from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware\n+from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n from scrapy.spider import Spider\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware, MetaRefreshMiddleware\n+from scrapy.downloadermiddlewares.redirect import RedirectMiddleware, MetaRefreshMiddleware\n from scrapy.spider import Spider\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response, HtmlResponse\n\n@@ -5,7 +5,7 @@ from twisted.internet.error import TimeoutError, DNSLookupError, \\\n         ConnectionLost, TCPTimedOutError\n \n from scrapy import optional_features\n-from scrapy.contrib.downloadermiddleware.retry import RetryMiddleware\n+from scrapy.downloadermiddlewares.retry import RetryMiddleware\n from scrapy.xlib.tx import ResponseFailed\n from scrapy.spider import Spider\n from scrapy.http import Request, Response\n\n@@ -3,7 +3,7 @@ import re\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n-from scrapy.contrib.downloadermiddleware.robotstxt import RobotsTxtMiddleware\n+from scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n\n@@ -1,6 +1,6 @@\n from unittest import TestCase\n \n-from scrapy.contrib.downloadermiddleware.stats import DownloaderStats\n+from scrapy.downloadermiddlewares.stats import DownloaderStats\n from scrapy.http import Request, Response\n from scrapy.spider import Spider\n from scrapy.utils.test import get_crawler\n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.spider import Spider\n from scrapy.http import Request\n-from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware\n+from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware\n from scrapy.utils.test import get_crawler\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b4c00cc9bfc00e715bd8fe71f1e4b214e7575da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 105 | Lines Deleted: 0 | Files Changed: 16 | Hunks: 15 | Methods Changed: 0 | Complexity Δ (Sum/Max): -176/0 | Churn Δ: 105 | Churn Cumulative: 2642 | Contributors (this commit): 24 | Commits (past 90d): 32 | Contributors (cumulative): 79 | DMM Complexity: None\n\nDIFF:\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.ajaxcrawl` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.ajaxcrawl` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.ajaxcrawl import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.chunked` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.chunked` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.chunked import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.cookies` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.cookies` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.cookies import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.decompression` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.decompression` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.decompression import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.defaultheaders` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.defaultheaders` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.defaultheaders import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.downloadtimeout` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.downloadtimeout` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.downloadtimeout import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.httpauth` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.httpauth` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.httpauth import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.httpcache` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.httpcache` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.httpcache import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.httpcompression` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.httpcompression` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.httpcompression import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.httpproxy` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.httpproxy` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.httpproxy import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.redirect` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.redirect` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.redirect import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.retry` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.retry` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.retry import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.robotstxt` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.robotstxt` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.robotstxt import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.stats` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.stats` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.stats import *\n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.useragent` is deprecated, \"\n+              \"use `scrapy.downloadermiddlewares.useragent` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.downloadermiddlewares.useragent import *\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7804b3d778902d93b844f4adf59f650c6b61ef00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 63/63 | Churn Δ: 16 | Churn Cumulative: 313 | Contributors (this commit): 17 | Commits (past 90d): 10 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n\n@@ -139,13 +139,13 @@ FEED_STORAGES_BASE = {\n }\n FEED_EXPORTERS = {}\n FEED_EXPORTERS_BASE = {\n-    'json': 'scrapy.contrib.exporter.JsonItemExporter',\n-    'jsonlines': 'scrapy.contrib.exporter.JsonLinesItemExporter',\n-    'jl': 'scrapy.contrib.exporter.JsonLinesItemExporter',\n-    'csv': 'scrapy.contrib.exporter.CsvItemExporter',\n-    'xml': 'scrapy.contrib.exporter.XmlItemExporter',\n-    'marshal': 'scrapy.contrib.exporter.MarshalItemExporter',\n-    'pickle': 'scrapy.contrib.exporter.PickleItemExporter',\n+    'json': 'scrapy.exporters.JsonItemExporter',\n+    'jsonlines': 'scrapy.exporters.JsonLinesItemExporter',\n+    'jl': 'scrapy.exporters.JsonLinesItemExporter',\n+    'csv': 'scrapy.exporters.CsvItemExporter',\n+    'xml': 'scrapy.exporters.XmlItemExporter',\n+    'marshal': 'scrapy.exporters.MarshalItemExporter',\n+    'pickle': 'scrapy.exporters.PickleItemExporter',\n }\n \n HTTPCACHE_ENABLED = False\n\n@@ -9,7 +9,7 @@ import lxml.etree\n \n from scrapy.item import Item, Field\n from scrapy.utils.python import str_to_unicode\n-from scrapy.contrib.exporter import (\n+from scrapy.exporters import (\n     BaseItemExporter, PprintItemExporter, PickleItemExporter, CsvItemExporter,\n     XmlItemExporter, JsonLinesItemExporter, JsonItemExporter, PythonItemExporter\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#152594ce990e0f5923d87f968e354c1a17bf463d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -67/0 | Churn Δ: 7 | Churn Cumulative: 503 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib.exporter` is deprecated, \"\n+              \"use `scrapy.exporters` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.exporters import *\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf064b143747f4fd7e48670f478d40fa2101bb01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 12 | Hunks: 11 | Methods Changed: 1 | Complexity Δ (Sum/Max): 78/48 | Churn Δ: 26 | Churn Cumulative: 462 | Contributors (this commit): 12 | Commits (past 90d): 17 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ from six.moves.urllib.parse import urlencode\n \n import scrapy\n from scrapy.command import ScrapyCommand\n-from scrapy.contrib.linkextractors import LinkExtractor\n+from scrapy.linkextractors import LinkExtractor\n \n \n class Command(ScrapyCommand):\n\n@@ -1,7 +1,7 @@\n \"\"\"\n This module defines the Link object used in Link extractors.\n \n-For actual link extractors implementation see scrapy.contrib.linkextractor, or\n+For actual link extractors implementation see scrapy.linkextractors, or\n its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n \n\n@@ -1,6 +1,6 @@\n \"\"\"\n Common code and definitions used by Link extractors (located in\n-scrapy.contrib.linkextractor).\n+scrapy.linkextractors).\n \"\"\"\n import re\n from six.moves.urllib.parse import urlparse\n\n@@ -1,5 +1,5 @@\n \"\"\"\n-scrapy.contrib.linkextractors\n+scrapy.linkextractors\n \n This package contains a collection of Link Extractors.\n \n\n\n\n\n@@ -20,7 +20,7 @@ class BaseSgmlLinkExtractor(SGMLParser):\n     def __init__(self, tag=\"a\", attr=\"href\", unique=False, process_value=None):\n         warnings.warn(\n             \"BaseSgmlLinkExtractor is deprecated and will be removed in future releases. \"\n-            \"Please use scrapy.contrib.linkextractors.LinkExtractor\",\n+            \"Please use scrapy.linkextractors.LinkExtractor\",\n             ScrapyDeprecationWarning\n         )\n         SGMLParser.__init__(self)\n@@ -103,7 +103,7 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n \n         warnings.warn(\n             \"SgmlLinkExtractor is deprecated and will be removed in future releases. \"\n-            \"Please use scrapy.contrib.linkextractors.LinkExtractor\",\n+            \"Please use scrapy.linkextractors.LinkExtractor\",\n             ScrapyDeprecationWarning\n         )\n \n\n@@ -8,7 +8,7 @@ from six.moves.urllib.parse import urlencode\n from scrapy.spider import Spider\n from scrapy.http import Request\n from scrapy.item import Item\n-from scrapy.contrib.linkextractors import LinkExtractor\n+from scrapy.linkextractors import LinkExtractor\n \n \n class MetaSpider(Spider):\n\n@@ -24,7 +24,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from tests import tests_datadir\n from scrapy.spider import Spider\n from scrapy.item import Item, Field\n-from scrapy.contrib.linkextractors import LinkExtractor\n+from scrapy.linkextractors import LinkExtractor\n from scrapy.http import Request\n from scrapy.utils.signal import disconnect_all\n \n\n@@ -1,11 +1,11 @@\n import re\n import unittest\n-from scrapy.contrib.linkextractors.regex import RegexLinkExtractor\n+from scrapy.linkextractors.regex import RegexLinkExtractor\n from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.link import Link\n-from scrapy.contrib.linkextractors.htmlparser import HtmlParserLinkExtractor\n-from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor, BaseSgmlLinkExtractor\n-from scrapy.contrib.linkextractors.lxmlhtml import LxmlLinkExtractor\n+from scrapy.linkextractors.htmlparser import HtmlParserLinkExtractor\n+from scrapy.linkextractors.sgml import SgmlLinkExtractor, BaseSgmlLinkExtractor\n+from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n from tests import get_testdata\n \n \n\n@@ -13,7 +13,7 @@ from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlRespon\n from scrapy.contrib.spiders.init import InitSpider\n from scrapy.contrib.spiders import CrawlSpider, Rule, XMLFeedSpider, \\\n     CSVFeedSpider, SitemapSpider\n-from scrapy.contrib.linkextractors import LinkExtractor\n+from scrapy.linkextractors import LinkExtractor\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.test import get_crawler\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
