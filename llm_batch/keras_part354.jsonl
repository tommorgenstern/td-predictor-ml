{"custom_id": "keras#dbfe90afa2ba938fc199a7948e605d22293f131a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 157 | Lines Deleted: 138 | Files Changed: 2 | Hunks: 37 | Methods Changed: 20 | Complexity Δ (Sum/Max): 1/5 | Churn Δ: 295 | Churn Cumulative: 2852 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,12 +22,12 @@ import tensorflow.compat.v2 as tf\n \n import functools\n from keras.engine import base_layer\n-from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n from keras.utils import layer_utils\n \n from tensorflow.python.ops import variable_scope as vs\n from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n \n \n def as_shape(shape):\n@@ -126,7 +126,7 @@ def validate_synchronization_aggregation_trainable(\n   return synchronization, aggregation, trainable\n \n \n-class _EagerVariableStore(object):\n+class _EagerVariableStore(tf.Module):\n   \"\"\"TF2-compatible VariableStore that avoids collections & tracks regularizers.\n \n   New variable names and new variables can be created; all stored\n@@ -144,8 +144,6 @@ class _EagerVariableStore(object):\n       the corresponding TensorFlow Variables as values.\n   \"\"\"\n \n-  __slots__ = [\"_vars\", \"_regularizers\", \"_store_eager_variables\"]\n-\n   def __init__(self):\n     \"\"\"Create a variable store.\"\"\"\n     self._vars = {}  # A dictionary of the stored TensorFlow variables.\n@@ -549,61 +547,36 @@ class _EagerVariableStore(object):\n     return initializer, initializing_from_value\n \n \n-# pylint: disable=not-context-manager\n-# The linter doesn't pick up that `scope` is a context manager\n-class VariableAndLossTracker(tf.Module):\n-  \"\"\"Module that has a scope to capture vars/losses made by `get_variable`.\"\"\"\n+@keras_export(v1=[\"keras.utils.track_tf1_style_variables\"])\n+def track_tf1_style_variables(method):\n+  \"\"\"Wrap layer & module methods in this decorator to capture tf1-style weights.\n \n-  def __init__(self):\n-    self._var_store = _EagerVariableStore()  # pylint: disable=protected-access\n-    self._variables = {}\n+  Decorating a `tf.keras.Layer`'s  or `tf.Module`'s methods with this\n+  decorator will cause the layer/module to track weights created/used\n+  via `tf.compat.v1.get_variable` (and by extension `tf.compat.v1.layers`)\n+  inside the decorated method.\n \n-    # Because this is a module, self._variables becomes a trackable dict\n-    # object that supports restoration. We must make the var_store share\n-    # this same trackable dict object.\n-    self._var_store._vars = self._variables  # pylint: disable=protected-access\n+  In addition to tracking the weights themselves under the standard\n+  `layer.variable`/`module.variable`/etc. properties, if the method belongs\n+  to a `tf.keras.Layer` then any regularization losses specified via the\n+  `get_variable` or `tf.compat.v1.layers` regularizer arguments will get\n+  tracked by the layer under the standard `layer.losses` property.\n \n-  @tf_contextlib.contextmanager\n-  def scope(self):\n-    with vs.with_variable_store(self._var_store):\n-      yield\n+  This tracking enables using large classes of TF1-style model-forward-pass\n+  code inside of Keras layers or `tf.Modules` in TF2 with TF2 behaviors enabled.\n \n-  def get_regularization_losses(self):\n-    # TODO(kaftan): Consider adding a regex scope like the collection access.\n-    # But, < 40-50 usages of get_regularization_loss(es) with `scope`\n-    # & possible to do manually?\n-    losses = {}\n-    for var_name, regularizer in self._var_store._regularizers.items():  # pylint: disable=protected-access\n-      losses[var_name] = regularizer()\n-    return losses\n-\n-\n-class VariableScopeModule(tf.Module):\n-  \"\"\"Wrapper Module to capture `compat.v1.get_variable` and `compat.v1.layers`.\n-\n-  This shim module allows using large sets of TF1 model-forward-pass code as a\n-  tf.Module that works in TF2 with TF2 behaviors enabled. To use it,\n-  override this class and put your TF1 model's forward pass inside your\n-  implementation for `forward_pass`. You can then call this module directly\n-  to run your TF1 code with working variable reuse.\n-\n-  Caution: Unless `tf.compat.v1.VariableScopeLayer` does not work for your code\n-  because of Keras layers' call signature semantics around the first argument,\n-  and you need a lower-level solution, we recommend using\n-  `tf.compat.v1.VariableScopeLayer` instead. `VariableScopeModule` can be used\n-  if you cannot adopt Keras layers to your use case.\n-\n-  Example of capturing tf.compat.v1.layer-based modeling code as a tf.Module:\n+  Example of capturing tf.compat.v1.layer-based modeling code as a Keras layer:\n \n   ```python\n-  class WrappedDoubleDenseModule(variable_scope_shim.VariableScopeModule):\n+  class WrappedDoubleDenseLayer(tf.keras.layers.Layer):\n \n     def __init__(self, units, *args, **kwargs):\n       super().__init__(*args, **kwargs)\n       self.units = units\n \n-    def forward_pass(self, inputs):\n-      with variable_scope.variable_scope(\"double_dense_layer\"):\n+    @tf.compat.v1.keras.utils.track_tf1_style_variables\n+    def call(self, inputs):\n+      with tf.compat.v1.variable_scope(\"double_dense_layer\"):\n         out = tf.compat.v1.layers.dense(\n             inputs, self.units, name=\"dense_one\",\n             kernel_initializer=tf.compat.v1.random_normal_initializer,\n@@ -614,37 +587,35 @@ class VariableScopeModule(tf.Module):\n             kernel_regularizer=\"l2\")\n       return out\n \n-  # Create a layer that can be used as a callable Module\n-  layer = WrappedDoubleDenseModule(10)\n+  # Create a layer that can be used as a standard keras layer\n+  layer = WrappedDoubleDenseLayer(10)\n \n   # call the layer on inputs\n   layer(...)\n \n-  # Variables created/used within `forward_pass` will be tracked\n-  layer.variables\n+  # Variables created/used within the scope will be tracked by the layer\n+  layer.weights\n   layer.trainable_variables\n \n-  # Regularization losses created by `tf.compat.v1.get_variable` or by\n-  # `tf.compat.v1.layers` will be tracked as well. Note: all other\n-  # regularization losses must be tracked manually if using modules. If you\n-  # would like fully-automated regularization loss tracking, use\n-  # `tf.compat.v1.VariableScopeLayer` instead.\n-  reg_losses = layer.get_compat_v1_regularization_losses()\n+  # Regularization losses will be captured in layer.losses after a call,\n+  # just like any other Keras layer\n+  reg_losses = layer.losses\n   ```\n \n   Example of capturing tf.compat.v1.get_variable-based modeling code as\n-  a tf.Module:\n+  a Keras layer:\n \n   ```python\n-  class WrappedDoubleDenseModule(variable_scope_shim.VariableScopeModule):\n+  class WrappedDoubleDenseLayer(tf.keras.layers.Layer):\n \n     def __init__(self, units, *args, **kwargs):\n       super().__init__(*args, **kwargs)\n       self.units = units\n \n-    def forward_pass(self, inputs):\n+    @tf.compat.v1.keras.utils.track_tf1_style_variables\n+    def call(self, inputs):\n       out = inputs\n-      with variable_scope.variable_scope(\"double_dense_layer\"):\n+      with tf.compat.v1.variable_scope(\"double_dense_layer\"):\n         with tf.compat.v1.variable_scope(\"dense_one\"):\n           # The weights are created with a `regularizer`,\n           # so the layer should track their regularization losses\n@@ -673,77 +644,115 @@ class VariableScopeModule(tf.Module):\n           out = tf.compat.v1.nn.bias_add(out, bias)\n       return out\n \n-  # Create a module that can be used as a callable Module\n-  layer = WrappedDoubleDenseModule(10)\n+  # Create a layer that can be used as a standard keras layer\n+  layer = WrappedDoubleDenseLayer(10)\n \n-  # call the module on inputs\n+  # call the layer on inputs\n   layer(...)\n \n-  # Variables created/used within the scope will be tracked by the module\n-  layer.variables\n+  # Variables created/used within the scope will be tracked by the layer\n+  layer.weights\n   layer.trainable_variables\n \n-  # Regularization losses created by `tf.compat.v1.get_variable` or by\n-  # `tf.compat.v1.layers` will be tracked as well. Note: all other\n-  # regularization losses must be tracked manually if using modules. If you\n-  # would like fully-automated regularization loss tracking, use\n-  # `tf.compat.v1.VariableScopeLayer` instead.\n-  reg_losses = layer.get_compat_v1_regularization_losses()\n+  # Regularization losses will be captured in layer.losses after a call,\n+  # just like any other Keras layer\n+  reg_losses = layer.losses\n   ```\n \n+  Regularization losses:\n+    Any regularizers specified in the `get_variable` calls or `compat.v1.layer`\n+    creations will get captured if they occur in your decorated method\n+    and the method belongs to a `tf.keras.Layer`/`tf.keras.Module`.\n+    Regularization losses\n+    are accessible in `layer.losses` after a call just like in a standard\n+    Keras layer, and will be captured by any model that includes this layer.\n+    Regularization losses attached to Keras layers/models set as attributes\n+    of your layer will also get captured in the standard Keras regularization\n+    loss tracking.\n+\n+    (While Modules have no `losses` property, no-arg callables to compute\n+     the regularization losses may be tracked as dict values in a private\n+     `module._tf1_style_var_store._regularizers` property, but only for\n+     `tf.compat.v1.layers` and `get_variable` weights and not for any other\n+     nested Keras layers/tf.Modules)\n+\n+  Variable scope / variable reuse:\n+    variable-scope based reuse in your decorated method will be respected,\n+    and work like variable-scope based reuse in TF1.\n+\n+  Variable Names/Pre-trained checkpoint loading:\n+    Variable naming from get_variable and `compat.v1.layer` layers will match\n+    the TF1 names, so you should be able to re-use your old name-based\n+    checkpoints. Variable naming for Keras layers/models or for variables\n+    created by `tf.Variable` may change when going to eager execution.\n+\n+  Training Arg if you decorate `layer.call`:\n+    Keras will pass a `training` arg to this layer if `call` contains\n+    a `training` arg or a `**kwargs` varargs in its call signature,\n+    similarly to how keras passes `training` to other layers in TF2 that have\n+    similar signatures in their `call` implementations.\n+    See more details in the docs\n+    on `tf.keras.layers.Layer` to understand what will be passed and when.\n+    Note: tf.compat.v1.layers are usually not called with `training=None`,\n+    so the training arg to `forward_pass` might not feed through to them\n+    unless you pass it to their calls explicitly.\n+\n   Caveats:\n     * TF2 will not prune unused variable updates (or unused outputs). You may\n       need to adjust your forward pass code to avoid computations or variable\n-      updates that you don't intend to use. (E.g. by adding a flag to the\n-      `forward_pass` call signature and branching on it).\n-    * Avoid Nesting variable creation in tf.function inside of `forward_pass`\n-      While the layer may safely be used from inside a `tf.function`, using\n-      a function inside of `forward_pass` will break the variable scoping.\n-    * If you would like to nest modules or other\n-      `VariableScopeModules`s directly in `forward_pass`, you need to\n-      assign them as properties of your VariableScopeModule so that\n-      object-oriented weights will activate in for them.\n+      updates that you don't intend to use.\n+    * Avoid Nesting variable creation in tf.function inside of\n+      methods decorated with `track_tf1_style_variables`\n+      While the method may safely be used from inside a `tf.function`, using\n+      a function inside of a decorated method may break the variable scoping.\n+    * This decorator only adds implicit tracking for legacy tf1-style\n+      get_variable / compat.v1.layers usage.\n+      If you would like to use nested Keras layers/models\n+      inside the decorated method, you need to\n+      assign them as attributes of your layer so that Keras/Module's standard\n+      object-oriented weights (and loss tracking for layers) will kick in.\n       See the intro to modules, layers, and models\n       [guide](https://www.tensorflow.org/guide/intro_to_modules) for more info\n-      on this explicit recursive weight tracking.\n-      Only weights made by `tf.compat.v1.get_variable` calls and\n-      `tf.compat.v1.layers` will get implicitly tracked by your\n-      VariableScopeModule without being assigned as properties. Note: if these\n-      modules have their own regularization losses you must track them\n-      manually.\n-  \"\"\"\n \n-  def __init__(self, name=None):\n-    super().__init__(name=name)\n-    self._var_tracker = VariableAndLossTracker()\n-\n-  def forward_pass(self, *args, **kwargs):\n-    \"\"\"Implement this method. It should include your model forward pass.\"\"\"\n-    raise NotImplementedError\n-\n-  def __call__(self, *args, **kwargs):\n-    with self.name_scope:\n-      with self._var_tracker.scope():\n-        return self.forward_pass(*args, **kwargs)\n-\n-  def get_compat_v1_regularization_losses(self):\n-    \"\"\"Dict w/ regularization losses from `get_variable` and `compat.v1.layers`.\n-\n-    VariableScopeModule is able to track regularization losses from\n-    `tf.compat.v1.get_variable` calls and `compat.v1.layers` that are used\n-    within the `forward_pass`. All other regularization losses (such as\n-    those belonging to Keras layers or nested VariableScopeModules)\n-    must be tracked via your own manual mechanism as is standard for tf.Modules.\n-    If you would like more comprehensive regularization loss tracking, use\n-    `VariableScopeModule` instead.\n+  Args:\n+    method: The method to decorate. This should belong to a custom tf.Module,\n+    tf.keras.layers.Layer, or tf.keras.Model.\n \n   Returns:\n-      A dict of the form { variable_name : regularization_loss } for any\n-      regularization losses specified for variables created and used by\n-      `tf.compat.v1.get_variable` (with the regularizer argument passed)\n-      or `tf.compat.v1.layers` within `forward_pass`.\n+    The decorated method.\n   \"\"\"\n-    return self._var_tracker.get_regularization_losses()\n+\n+  def _method_wrapper(self, *args, **kwargs):\n+    var_store = getattr(self, \"_tf1_style_var_store\", None)\n+    if not var_store:\n+      if not isinstance(self, tf.Module):\n+        # Raise an error if you incorrectly decorate a method\n+        # that is not a method of a Module, Layer, or Model:\n+        raise ValueError(\n+            \"`@tf.compat.v1.keras.utils.track_tf1_layers_and_variables` must \"\n+            \"be applied to a method of a subclassed `tf.Module`, \"\n+            \"`tf.keras.layers.Layer`, or `tf.keras.Model` and which takes \"\n+            \"`self` as the first argument. But, the first argument passed \"\n+            \"to the decorated method was {}, which does not \"\n+            \"extend Module, Layer, or Model.\".format(self))\n+      var_store = _EagerVariableStore()\n+      self._tf1_style_var_store = var_store  # pylint: disable=protected-access\n+\n+    existing_regularized_variables = set(var_store._regularizers.keys())  # pylint: disable=protected-access\n+    with vs.with_variable_store(var_store):\n+      out = method(self, *args, **kwargs)\n+\n+    # If this is a layer method, add the regularization losses\n+    # to the layer for any newly-created regularized variables\n+    if isinstance(self, base_layer.Layer):\n+      for var_name, regularizer in var_store._regularizers.items():  # pylint: disable=protected-access\n+        if var_name not in existing_regularized_variables:\n+          self.add_loss(regularizer)\n+\n+    return out\n+\n+  return tf.__internal__.decorator.make_decorator(\n+      target=method, decorator_func=_method_wrapper)\n \n \n class VariableScopeLayer(base_layer.Layer):\n@@ -903,12 +912,6 @@ class VariableScopeLayer(base_layer.Layer):\n       [guide](https://www.tensorflow.org/guide/intro_to_modules) for more info\n   \"\"\"\n \n-  def __init__(self, **kwargs):\n-    super().__init__(**kwargs)\n-    # Relies on keras layers tracking Modules\n-    self._var_tracker = VariableAndLossTracker()\n-    # May need to inspect func to see if it should pass a `training` arg or not\n-\n   @property\n   @layer_utils.cached_per_instance\n   def _call_full_argspec(self):\n@@ -920,14 +923,6 @@ class VariableScopeLayer(base_layer.Layer):\n     \"\"\"Implement this method. It should include your model forward pass.\"\"\"\n     raise NotImplementedError\n \n+  @track_tf1_style_variables\n   def call(self, *args, **kwargs):\n-    with self._var_tracker.scope():\n-      out = self.forward_pass(*args, **kwargs)\n-    if not self._eager_losses:\n-      # We have to record regularization losses in the call as if they\n-      # are activity losses.\n-      # So, don't double-count regularization losses if the layer is used\n-      # multiple times in a model\n-      for loss in self._var_tracker.get_regularization_losses().values():\n-        self.add_loss(loss)\n-    return out\n+    return self.forward_pass(*args, **kwargs)\n\n@@ -50,8 +50,8 @@ def run_inside_wrap_function_in_eager_mode(graph_function):\n     decorated function\n   \"\"\"\n   def wrap_and_execute(self):\n-    tracker = variable_scope_shim.VariableAndLossTracker()\n-    with tracker.scope():\n+    store = variable_scope_shim._EagerVariableStore()\n+    with variable_scope.with_variable_store(store):\n       # use the original function\n       graph_function(self)\n   return wrap_and_execute\n@@ -820,6 +820,20 @@ class CompatV1TemplateScaleByY(variable_scope_shim.VariableScopeLayer):\n       return self.scale_by_y(inputs)\n \n \n+class VariableScopeModule(tf.Module):\n+  \"\"\"Module that uses the shim.\"\"\"\n+\n+  @variable_scope_shim.track_tf1_style_variables\n+  def __call__(self, *args, **kwargs):\n+    with self.name_scope:\n+      return self.forward_pass(*args, **kwargs)\n+\n+  def get_compat_v1_regularization_losses(self):\n+    \"\"\"Dict w/ regularization losses from `get_variable`&`compat.v1.layers`.\"\"\"\n+    return {name: regularizer() for name, regularizer\n+            in self._tf1_style_var_store._regularizers.items()}  # pylint: disable=protected-access\n+\n+\n @combinations.generate(combinations.combine(mode=[\"eager\"]))\n class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n@@ -957,7 +971,7 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n   def test_module_get_variable(self):\n     # Test the module shim when using `get_variable` (and regularizers) directly\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeModule):\n+    class WrappedDenseLayer(VariableScopeModule):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -1019,7 +1033,7 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n   def test_module_compat_v1_layer(self):\n     # Test the module shim when using `compat.v1` layers\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeModule):\n+    class WrappedDenseLayer(VariableScopeModule):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -1255,7 +1269,7 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n     # Test the module shim when embedding a Keras model inside of it\n     # And assigning the model to an attribute\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeModule):\n+    class WrappedDenseLayer(VariableScopeModule):\n \n       def __init__(self, units, **kwargs):\n         super().__init__(**kwargs)\n@@ -1330,5 +1344,15 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(weights.keys(),\n                      {\"dense_no_training/bias:0\", \"dense_no_training/kernel:0\"})\n \n+  def test_incorrect_decoration(self):\n+    # Raise an error if you incorrectly decorate a method\n+    # that is not a method of a Module, layer, or model:\n+    @variable_scope_shim.track_tf1_style_variables\n+    def foo(x):\n+      return x * 2\n+\n+    with self.assertRaisesRegex(ValueError, \"does not extend\"):\n+      foo(tf.ones(shape=(4, 4)))\n+\n if __name__ == \"__main__\":\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#304c2d06dc7ab25d7e9bb236e2f514f94bbeb4cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 13341 | Contributors (this commit): 91 | Commits (past 90d): 3 | Contributors (cumulative): 94 | DMM Complexity: 1.0\n\nDIFF:\n@@ -183,7 +183,7 @@ def _clone_functional_model(model, input_tensors=None, layer_fn=_clone_layer):\n         newly_created_input_layer = input_tensor._keras_history.layer\n         new_input_layers[original_input_layer] = newly_created_input_layer\n       else:\n-        new_input_layers[original_input_layer] = original_input_layer\n+        new_input_layers[original_input_layer] = input_tensor._keras_history.layer\n \n   if not callable(layer_fn):\n     raise ValueError('Expected `layer_fn` argument to be a callable.')\n\n@@ -121,11 +121,14 @@ class TestModelCloning(keras_parameterized.TestCase):\n       self.assertGreaterEqual(len(new_model.updates), 2)\n \n     # On top of new tensor  -- clone model should always have an InputLayer.\n-    input_a = keras.Input(shape=(4,))\n+    input_a = keras.Input(shape=(4,), name=\"a\")\n     new_model = clone_fn(model, input_tensors=input_a)\n     self.assertIsInstance(\n         list(new_model._flatten_layers(include_self=False, recursive=False))[0],\n         keras.layers.InputLayer)\n+    # The new models inputs should have the properties of the new input tensor\n+    self.assertEqual(new_model.input_names[0], input_a.name)\n+    self.assertEqual(new_model.inputs[0].shape, input_a.shape)\n     self.assertTrue(new_model._is_graph_network)\n \n     # On top of new, non-Keras tensor  -- clone model should always have an\n@@ -183,8 +186,9 @@ class TestModelCloning(keras_parameterized.TestCase):\n     # On top of new tensors\n     input_a = keras.Input(shape=(4,), name='a')\n     input_b = keras.Input(shape=(4,), name='b')\n+    new_input_tensors = [input_a, input_b]\n     new_model = keras.models.clone_model(\n-        model, input_tensors=[input_a, input_b])\n+        model, input_tensors=new_input_tensors)\n     if not tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertLen(new_model.updates, 2)\n     new_model.compile(\n@@ -193,6 +197,9 @@ class TestModelCloning(keras_parameterized.TestCase):\n         run_eagerly=testing_utils.should_run_eagerly())\n     new_model.train_on_batch([val_a, val_b], val_out)\n \n+    # New model should use provided input tensors\n+    self.assertListEqual(new_model.inputs, new_input_tensors)\n+\n     # On top of new, non-Keras tensors\n     if not tf.executing_eagerly():\n       # TODO(b/121277734):Skip Eager contexts, as Input() layers raise an error\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b59e2f691142ef8466a5aeed12586700cc4d455", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 128 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,13 @@ class DatasetCreator(object):\n   with strategy.scope():\n     model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n   model.compile(tf.keras.optimizers.SGD(), loss=\"mse\")\n+\n+  def dataset_fn(input_context):\n     ...\n+\n+  input_options = ...\n+  model.fit(tf.keras.utils.experimental.DatasetCreator(\n+      dataset_fn, input_options=input_options), epochs=10, steps_per_epoch=10)\n   ```\n \n   Note: When using `DatasetCreator`, `steps_per_epoch` argument in `Model.fit`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10b165f06ec9385ed7251b46eef22836bd04ad55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 23 | Files Changed: 22 | Hunks: 23 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 46 | Churn Cumulative: 26844 | Contributors (this commit): 40 | Commits (past 90d): 83 | Contributors (cumulative): 124 | DMM Complexity: None\n\nDIFF:\n@@ -224,7 +224,7 @@ def MobileNetV3(stack_fn,\n         rows = backend.int_shape(input_tensor)[1]\n         cols = backend.int_shape(input_tensor)[2]\n         input_shape = (cols, rows, 3)\n-  # If input_shape is None and input_tensor is None using standart shape\n+  # If input_shape is None and input_tensor is None using standard shape\n   if input_shape is None and input_tensor is None:\n     input_shape = (None, None, 3)\n \n\n@@ -154,7 +154,7 @@ class KerasLayerCallOverheadBenchmarks(  # pylint: disable=undefined-variable\n   # 3) The shape of the input to the layer;\n   # 4) The kwargs used in the benchmark. It can include the number of\n   #    iterations to run the benchmarks, and kwargs used in the layer call.\n-  #    By default, # of iteratons is 10000.\n+  #    By default, # of iteration is 10000.\n   _benchmark_parameters = [\n       (\"advanced_activations_leaky_relu\", tf.keras.layers.LeakyReLU(),\n        (1, 1)),\n\n@@ -46,7 +46,7 @@ def _get_input_data(inputs):\n   elif \"input\" in inputs:\n     return inputs[\"input\"]\n   else:\n-    raise ValueError(\"Please specificy either `input_shape` or `input`\"\n+    raise ValueError(\"Please specify either `input_shape` or `input`\"\n                      \"for the benchmark test\")\n \n \n\n@@ -631,7 +631,7 @@ class TestDistributionStrategyWithNumpyArrays(tf.test.TestCase,\n           batch_size=2,\n           sample_weight=sample_weights,\n           verbose=1)\n-      # The per sample loss is multipled by the corresponding sample weight. The\n+      # The per sample loss is multiplied by the corresponding sample weight. The\n       # average of these weighted losses is the return value of the `evaluate`\n       # call. For example, in the test above the average weighted loss is\n       # calculated in the following manner:\n@@ -1545,7 +1545,7 @@ class TestDistributionStrategyWithDatasets(tf.test.TestCase,\n       ds = tf.data.Dataset.from_tensor_slices(\n           (inputs, targets, sample_weights)).batch(2)\n       result = model.evaluate(ds, verbose=1)\n-      # The per sample loss is multipled by the corresponding sample weight. The\n+      # The per sample loss is multiplied by the corresponding sample weight. The\n       # average of these weighted losses is the return value of the `evaluate`\n       # call. For example, in the test above the average weighted loss is\n       # calculated in the following manner:\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Utilites for `Model.compile`.\"\"\"\n+\"\"\"Utilities for `Model.compile`.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -1620,7 +1620,7 @@ def pack_x_y_sample_weight(x, y=None, sample_weight=None):\n     # For single x-input, we do no tuple wrapping since in this case\n     # there is no ambiguity. This also makes NumPy and Dataset\n     # consistent in that the user does not have to wrap their Dataset\n-    # data in an unecessary tuple\n+    # data in an unnecessary tuple\n     if not tf.nest.is_nested(x):\n       return x\n     else:\n\n@@ -235,7 +235,7 @@ class KerasTensor(object):\n   # operators to run when the left operand is an ndarray, because it\n   # accords the Tensor class higher priority than an ndarray, or a\n   # numpy matrix.\n-  # In the future explore chaning this to using numpy's __numpy_ufunc__\n+  # In the future explore changing this to using numpy's __numpy_ufunc__\n   # mechanism, which allows more control over how Tensors interact\n   # with ndarrays.\n   __array_priority__ = 100\n\n@@ -350,7 +350,7 @@ class Sequential(functional.Functional):\n     if not self._has_explicit_input_shape:\n       if not tf.is_tensor(inputs) and not isinstance(\n           inputs, tf.Tensor):\n-        # This is a Sequential with mutiple inputs. This is technically an\n+        # This is a Sequential with multiple inputs. This is technically an\n         # invalid use case of Sequential, but we tolerate it for backwards\n         # compatibility.\n         self._use_legacy_deferred_behavior = True\n\n@@ -237,7 +237,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     ds = tf.data.Dataset.from_tensor_slices(\n         (inputs, targets, sample_weights)).batch(2)\n     result = model.evaluate(ds, verbose=1)\n-    # The per sample loss is multipled by the corresponding sample weight. The\n+    # The per sample loss is multiplied by the corresponding sample weight. The\n     # average of these weighted losses is the return value of the `evaluate`\n     # call. For example, in the test above the average weighted loss is\n     # calculated in the following manner:\n\n@@ -122,7 +122,7 @@ class RespectCompiledTrainableState(object):\n   at `Model.compile` time will be used when training that model. In order to\n   respect this requirement, it may be necessary to set the trainable value of\n   layers to their compile time values before beginning a training endpoint and\n-  restore the values before returing from said endpoint. This scope checks if\n+  restore the values before returning from said endpoint. This scope checks if\n   any layer's trainable state has changed since Model compile, and performs this\n   set and un-set bookkeeping.\n \n\n@@ -1666,7 +1666,7 @@ def infer_steps_for_dataset(model,\n       (dataset.options().experimental_distribute.auto_shard_policy !=\n        tf.data.experimental.AutoShardPolicy.OFF)):\n     # If the dataset would be auto-sharded, we should not infer a local\n-    # steps_per_epoch due to the possible inbalanced sharding between workers.\n+    # steps_per_epoch due to the possible imbalanced sharding between workers.\n     return None\n \n   size = backend.get_value(tf.data.experimental.cardinality(dataset))\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Common utilities for our Keras preprocessing intergration tests.\"\"\"\n+\"\"\"Common utilities for our Keras preprocessing integration tests.\"\"\"\n \n import os\n \n\n@@ -32,7 +32,7 @@ class _Merge(Layer):\n   \"\"\"\n \n   def __init__(self, **kwargs):\n-    \"\"\"Intializes a Merge layer.\n+    \"\"\"Initializes a Merge layer.\n \n     Args:\n       **kwargs: standard layer keyword arguments.\n\n@@ -207,7 +207,7 @@ class MultiHeadAttention(Layer):\n       where `T` is for target sequence shapes and `E` is the query input last\n       dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n       are project to the shape specified by `output_shape`.\n-    attention_scores: [Optional] multi-head attention coeffients over\n+    attention_scores: [Optional] multi-head attention coefficients over\n       attention axes.\n   \"\"\"\n \n\n@@ -92,7 +92,7 @@ class PreprocessingStage(sequential.Sequential,\n                                              reset_state=reset_state)\n \n \n-# Functional methods shoud take precedence.\n+# Functional methods should take precedence.\n class FunctionalPreprocessingStage(functional.Functional,\n                                    base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"A functional preprocessing stage.\n\n@@ -332,7 +332,7 @@ class StringLookup(index_lookup.IndexLookup):\n     base_config = super(StringLookup, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n-  # Overriden methods from IndexLookup.\n+  # Overridden methods from IndexLookup.\n   def _tensor_vocab_to_numpy(self, vocabulary):\n     vocabulary = vocabulary.numpy()\n     return np.array([tf.compat.as_text(x, self.encoding) for x in vocabulary])\n\n@@ -79,7 +79,7 @@ def _get_end_to_end_test_cases():\n       {\n           \"testcase_name\":\n               \"test_special_tokens_int_mode\",\n-          # Mask tokens in the vocab data should be ingored, and mapped to 0 in\n+          # Mask tokens in the vocab data should be ignored, and mapped to 0 in\n           # from the input data.\n           \"vocab_data\":\n               np.array([[\"fire\"], [\"earth\"], [\"earth\"], [\"earth\"], [\"earth\"],\n\n@@ -706,7 +706,7 @@ class LossScaleOptimizer(_DelegatingTrackableMixin, optimizer_v2.OptimizerV2):\n     if experimental_aggregate_gradients:\n       # We must aggregate the gradients here instead of in\n       # self.optimizer.apply_gradients, so that any NaN or Inf gradients are\n-      # propogated to each replica. If any replica has a NaN or Inf gradient,\n+      # propagated to each replica. If any replica has a NaN or Inf gradient,\n       # they must all have a NaN or Inf gradient so that they all skip the step.\n       # pylint: disable=protected-access\n       grads_and_vars = self._optimizer._transform_unaggregated_gradients(\n\n@@ -607,7 +607,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n       name: Optional name for the returned operation. Default to the name passed\n         to the `Optimizer` constructor.\n       experimental_aggregate_gradients: Whether to sum gradients from different\n-        replicas in the presense of `tf.distribute.Strategy`. If False, it's\n+        replicas in the presence of `tf.distribute.Strategy`. If False, it's\n         user responsibility to aggregate the gradients. Default to True.\n \n     Returns:\n\n@@ -42,7 +42,7 @@ def index_directory(directory,\n         to the alphanumeric order of the image file paths\n         (obtained via `os.walk(directory)` in Python).\n     formats: Allowlist of file extensions to index (e.g. \".jpg\", \".txt\").\n-    class_names: Only valid if \"labels\" is \"inferred\". This is the explict\n+    class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n         list of class names (must match names of subdirectories). Used\n         to control the order of the classes\n         (otherwise alphanumerical order is used).\n\n@@ -89,7 +89,7 @@ def image_dataset_from_directory(directory,\n             are encoded as `float32` scalars with values 0 or 1\n             (e.g. for `binary_crossentropy`).\n         - None (no labels).\n-    class_names: Only valid if \"labels\" is \"inferred\". This is the explict\n+    class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n         list of class names (must match names of subdirectories). Used\n         to control the order of the classes\n         (otherwise alphanumerical order is used).\n\n@@ -78,7 +78,7 @@ def text_dataset_from_directory(directory,\n             are encoded as `float32` scalars with values 0 or 1\n             (e.g. for `binary_crossentropy`).\n         - None (no labels).\n-    class_names: Only valid if \"labels\" is \"inferred\". This is the explict\n+    class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n         list of class names (must match names of subdirectories). Used\n         to control the order of the classes\n         (otherwise alphanumerical order is used).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc0e67ebb13900ef736ceaa020776eecebfeebfc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 67 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,6 +26,8 @@ class ConvertResource(tf.test.TestCase):\n \n   def testConvertResource(self):\n     \"\"\"Test general resource inputs don't crash the converter.\"\"\"\n+    if not tf.test.is_built_with_cuda():\n+      self.skipTest('test is only applicable with CUDA')\n \n     class TokenizeLayer(tf.keras.layers.Layer):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24c07293a1b700da9f715f045b248d821fbca425", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 9724 | Contributors (this commit): 17 | Commits (past 90d): 18 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -295,6 +295,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       else:\n         keras_layers_gauge.get_cell(self._get_cell_name()).set(True)\n         self._instrumented_keras_layer_class = True\n+    else:\n+      # This is a legacy layer that has disabled instrumentation\n+      # as a native keras object. We still instrument this as\n+      # legacy usage.\n+      keras_api_gauge.get_cell('legacy_layer').set(True)\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self,\n\n@@ -68,6 +68,10 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     self.assertTrue(layer._instrumented_keras_layer_class)\n     self.assertFalse(layer._instrumented_keras_model_class)\n     self.assertTrue(base_layer.keras_api_gauge.get_cell('tf.keras.layers.Add'))\n+\n+    # Verify this was not instrumented as a legacy layer\n+    self.assertFalse(\n+        base_layer.keras_api_gauge.get_cell('legacy_layer').value())\n     base_layer.keras_api_gauge.get_cell('tf.keras.layers.Add').set(False)\n \n   @combinations.generate(combinations.keras_model_type_combinations())\n\n@@ -51,6 +51,11 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     # Assert that the layer was not instrumented as a Keras layer\n     self.assertFalse(layer._instrumented_keras_api)\n \n+    # Assert this was instrumented as a legacy layer\n+    self.assertTrue(\n+        keras_base_layer.keras_api_gauge.get_cell('legacy_layer').value())\n+    keras_base_layer.keras_api_gauge.get_cell('legacy_layer').set(False)\n+\n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def testInt64Layer(self):\n     layer = base_layers.Layer(name='my_layer', dtype='int64')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b10611fba6119157137ff098aba542e3590718fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 49 | Churn Cumulative: 6638 | Contributors (this commit): 5 | Commits (past 90d): 31 | Contributors (cumulative): 9 | DMM Complexity: 0.8461538461538461\n\nDIFF:\n@@ -653,10 +653,11 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n \n   def _lookup_dense(self, inputs):\n     \"\"\"Lookup table values for a dense Tensor, handling masking and OOV.\"\"\"\n-    # Avoid calling lookup on the table when tracing keras.Inputs. This is\n-    # critical for SavedModel, which expects an uninitialized StaticHashTable\n-    # and will trace the model before restoring the table.\n-    if backend.is_keras_tensor(inputs):\n+    # When executing eagerly and tracing keras.Inputs, do not call lookup. This\n+    # is critical for restoring SavedModel, which will first trace layer.call\n+    # and then attempt to restore the table. We need the table to be unitialized\n+    # for the restore to work, but calling the table unitialized would error.\n+    if tf.executing_eagerly() and backend.is_keras_tensor(inputs):\n       lookups = tf.zeros_like(inputs, dtype=self._value_dtype)\n     else:\n       lookups = self.lookup_table.lookup(inputs)\n@@ -711,7 +712,10 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n       initializer = tf.lookup.KeyValueTensorInitializer(keys, values,\n                                                         self._key_dtype,\n                                                         self._value_dtype)\n-      return tf.lookup.StaticHashTable(initializer, self._default_value)\n+      table = tf.lookup.StaticHashTable(initializer, self._default_value)\n+      if not tf.compat.v1.executing_eagerly_outside_functions():\n+        backend.get_session().run(initializer.initialize(table))\n+      return table\n \n   def _lookup_table_from_file(self, filename):\n     if self.invert:\n@@ -728,7 +732,10 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n           value_dtype=self._value_dtype,\n           value_index=value_index,\n           value_index_offset=self._token_start_index())\n-      return tf.lookup.StaticHashTable(initializer, self._default_value)\n+      table = tf.lookup.StaticHashTable(initializer, self._default_value)\n+      if not tf.compat.v1.executing_eagerly_outside_functions():\n+        backend.get_session().run(initializer.initialize(table))\n+      return table\n \n   def _standardize_inputs(self, inputs, dtype):\n     if isinstance(inputs, (list, tuple, np.ndarray)):\n@@ -764,6 +771,10 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     if self.output_mode == INT or self.pad_to_max_tokens:\n       return\n     with tf.init_scope():\n+      if not tf.executing_eagerly():\n+        raise RuntimeError(\n+            \"When using `output_mode={}` eager mode execution must be enabled.\"\n+            .format(self.output_mode))\n       new_vocab_size = self.vocabulary_size()\n     if new_vocab_size == self._token_start_index():\n       raise RuntimeError(\n\n@@ -2381,5 +2381,31 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n     self.assertAllEqual(new_output_dataset, expected_output)\n \n \n+@keras_parameterized.run_all_keras_modes(always_skip_eager=True)\n+class EagerExecutionDisabled(keras_parameterized.TestCase,\n+                             preprocessing_test_utils.PreprocessingLayerTest):\n+\n+  def test_lookup(self):\n+    # We need this test for model_to_estimator followed by export_saved_model,\n+    # which will call the layer in a legacy session. This could also happen\n+    # directly if a user calls disable_v2_behavior or disable_eager_execution.\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = np.array([\"earth\", \"wind\", \"and\", \"fire\"])\n+    expected_output = [1, 2, 3, 4]\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = index_lookup.IndexLookup(\n+        max_tokens=None,\n+        num_oov_indices=1,\n+        mask_token=None,\n+        oov_token=\"[OOV]\",\n+        dtype=tf.string,\n+        vocabulary=vocab_data)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_dataset = model(input_array)\n+    self.assertAllEqual(output_dataset, expected_output)\n+\n+\n if __name__ == \"__main__\":\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3688b72924a4235598f0f80038de8c897f44799", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 390 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 9 | Complexity Δ (Sum/Max): 30/25 | Churn Δ: 390 | Churn Cumulative: 1048 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 0.7844311377245509\n\nDIFF:\n@@ -0,0 +1,242 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Utilities for keras functional model.\"\"\"\n+\n+from keras import backend\n+from keras.engine import input_layer as input_layer_module\n+from keras.engine import keras_tensor\n+from keras.engine import node as node_module\n+\n+import tensorflow.compat.v2 as tf\n+\n+_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG = (\n+    'Found unexpected instance while processing input tensors for keras '\n+    'functional model. Expecting KerasTensor which is from tf.keras.Input() '\n+    'or output from keras layer call(). Got: {}')\n+\n+\n+def is_input_keras_tensor(tensor):\n+  \"\"\"Check if tensor is directly generated from `tf.keras.Input`.\n+\n+  This check is useful when constructing the functional model, since we will\n+  need to clone Nodes and KerasTensors if the model is building from non input\n+  tensor.\n+\n+  Args:\n+    tensor: A `KerasTensor` as inputs to the functional model.\n+\n+  Returns:\n+    bool. Whether the tensor is directly generated from `tf.keras.Input`.\n+\n+  Raises:\n+    ValueError: if the tensor is not a KerasTensor instance.\n+  \"\"\"\n+  if not node_module.is_keras_tensor(tensor):\n+    raise ValueError(_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG.format(tensor))\n+  return tensor.node.is_input\n+\n+\n+def find_nodes_by_inputs_and_outputs(inputs, outputs):\n+  \"\"\"Fetch all Nodes in the graph defined by \"inputs\" and \"outputs\".\n+\n+  This method is used to find and then clone Nodes when creating a new\n+  sub-model from an existing functional model.\n+\n+  Args:\n+    inputs: A nested structure of KerasTensor to use as model inputs.\n+    outputs: A nested structure of KerasTensor to use as model outputs.\n+\n+  Returns:\n+    A list of Nodes that are connected to the inputs and outputs.\n+\n+  Raises:\n+    ValueError: when inputs and outputs are disconnected or in case of\n+      unexpected objects in the inputs/outputs.\n+  \"\"\"\n+  # We walk the graph bottom up, starting from output nodes, and keep tracing\n+  # the upstream node, until we find all the inputs nodes. We don't use top\n+  # down search here since we don't know whether a certain node is in the graph\n+  # between inputs and outputs, e.g. a functional graph could have multiple\n+  # outputs, and the user could choose a subset of them to build the model.\n+  # The bottom up approach will ensure all the nodes we visit are actually\n+  # in use. If we reach the top and didn't find the nodes in the `inputs`,\n+  # that's an error, since the user didn't specify the correct inputs.\n+  start_keras_tensors = tf.nest.flatten(outputs)\n+  end_keras_tensors = tf.nest.flatten(inputs)\n+\n+  for t in start_keras_tensors + end_keras_tensors:\n+    if not node_module.is_keras_tensor(t):\n+      raise ValueError(_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG.format(t))\n+  end_ids = set([id(kt) for kt in end_keras_tensors])\n+  # Track all the end tensors we found so far, if we didn't reach all the\n+  # user-specified keras inputs after we finish the search, then that's an\n+  # error since the inputs are disconnected from the outputs.\n+  end_ids_found = set()\n+\n+  nodes_to_visit = []\n+  nodes_in_graph = []\n+  node_id_visited = set()\n+  for t in start_keras_tensors:\n+    nodes_to_visit.append(t.node)\n+\n+  while nodes_to_visit:\n+    node = nodes_to_visit.pop(0)\n+    if id(node) in node_id_visited:\n+      continue\n+    node_id_visited.add(id(node))\n+    nodes_in_graph.append(node)\n+    # Any input keras_tensor that produce the current node.\n+    for kt in node.keras_inputs:\n+      if id(kt) in end_ids:\n+        # We found the inputs of the model, stop tracing upstream nodes\n+        end_ids_found.add(id(kt))\n+        continue\n+\n+      inbound_node = kt.node\n+      # In case this is the tf.keras.Input node, we have reached the end of the\n+      # tracing of upstream nodes. Any further tracing will just be an\n+      # infinite loop. we should raise an error here since we didn't find the\n+      # input in the user-specified inputs.\n+      if inbound_node.is_input:\n+        raise ValueError('Found input tensor cannot be reached given provided '\n+                         'output tensors. Please make sure the tensor {} is '\n+                         'included in the model inputs when building '\n+                         'functional model.'.format(kt))\n+      nodes_to_visit.append(inbound_node)\n+\n+  # Do a final check and make sure we have reached all the user-specified inputs\n+  if end_ids != end_ids_found:\n+    unvisited_inputs = [kt for kt in end_keras_tensors\n+                        if id(kt) not in end_ids_found]\n+    raise ValueError('Found unvisited input tensors that are disconnected from '\n+                     'the outputs: {}'.format(unvisited_inputs))\n+  return nodes_in_graph\n+\n+\n+def clone_graph_nodes(inputs, outputs):\n+  \"\"\"Clone the `Node` between the inputs and output tensors.\n+\n+  This function is used to create a new functional model from any intermediate\n+  keras tensors. The clone of the nodes mimic the behavior of reconstructing the\n+  functional graph network by re-executing all the __call__ methods. The cloned\n+  nodes will be appended to the layers.\n+\n+  Note that a new tf.keras.Inputs will be created for any items in the `inputs`\n+\n+  Args:\n+    inputs: A nested structure of keras_tensors.\n+    outputs: A nested structure of keras_tensors.\n+\n+  Returns:\n+    A pair of inputs and outputs, with cloned keras_tensors. They can be used to\n+    create a new functional model.\n+  \"\"\"\n+  nodes_to_clone = find_nodes_by_inputs_and_outputs(inputs, outputs)\n+  cloned_inputs = []\n+  cloned_outputs = []\n+  # We not only need to create copies of Nodes (mimic the calls), also need to\n+  # clone keras_tensors to avoid the override of _keras_history attached on the\n+  # keras_tensor. The following dict is used to track any keras tensor we cloned\n+  # The key is the string ID of the original keras tensor, and value is the\n+  # cloned keras_tensor instance.\n+  kt_id_mapping = {}\n+\n+  for kt_input in tf.nest.flatten(inputs):\n+    if kt_input.node.is_input:\n+      # For any existing keras_tensor from tf.keras.Input, we leave them as is.\n+      cloned_inputs.append(kt_input)\n+      kt_id_mapping[id(kt_input)] = kt_input\n+    else:\n+      # We need to create a new tf.keras.Input for any intermediate keras_tensor\n+      cpy = _clone_keras_tensor(kt_input)\n+      cloned_input = input_layer_module.Input(tensor=cpy)\n+      cloned_inputs.append(cloned_input)\n+      kt_id_mapping[id(kt_input)] = cloned_input\n+  cloned_inputs = tf.nest.pack_sequence_as(inputs, cloned_inputs)\n+\n+  for kt_output in tf.nest.flatten(outputs):\n+    cpy = keras_tensor.keras_tensor_from_type_spec(kt_output.type_spec)\n+    cloned_outputs.append(cpy)\n+    kt_id_mapping[id(kt_output)] = cpy\n+  cloned_outputs = tf.nest.pack_sequence_as(outputs, cloned_outputs)\n+\n+  for node in nodes_to_clone:\n+    # Clone any keras_tensors to avoid override of _keras_history\n+    # Or reuse an existing keras_tensor if it has already been cloned.\n+    output_copy = clone_keras_tensors(node.output_tensors, kt_id_mapping)\n+    call_args_copy = clone_keras_tensors(node.call_args, kt_id_mapping)\n+    call_kwargs_copy = clone_keras_tensors(node.call_kwargs, kt_id_mapping)\n+    # Creating new nodes based on the existing node information.\n+    # Node wires itself to inbound and outbound layers.\n+    # The Node constructor actually updates this layer's self._inbound_nodes,\n+    # sets _keras_history on the outputs, and adds itself to the\n+    # `_outbound_nodes` of the layers that produced the inputs to this\n+    # layer call.\n+    node_module.Node(node.layer,\n+                     call_args=call_args_copy,\n+                     call_kwargs=call_kwargs_copy,\n+                     outputs=output_copy)\n+  return cloned_inputs, cloned_outputs\n+\n+\n+def clone_keras_tensors(args, keras_tensor_mapping):\n+  \"\"\"Clone the keras tensors from the inputs.\n+\n+  For any KerasTensor instance in the `args`, a new copy of KerasTensor will\n+  be created if it has not been cloned yet (by checking the\n+  `keras_tensor_mapping`). For any other types, the instance will be unchanged.\n+  This function is useful for cloning the Nodes since KerasTensor can't be\n+  reused across the models.\n+\n+  Args:\n+    args: A nested structure of objects, which could contain KerasTensor.\n+    keras_tensor_mapping: A dict contains the ID of original KerasTensor, and\n+      the cloned KerasTensor instance. The dict will be updated with newly\n+      copied KerasTensor instances within this method.\n+  Returns:\n+    Same structure as inputs, with KerasTensor cloned.\n+  \"\"\"\n+  result = []\n+  for obj in tf.nest.flatten(args):\n+    if node_module.is_keras_tensor(obj):\n+      if id(obj) in keras_tensor_mapping:\n+        cpy = keras_tensor_mapping[id(obj)]\n+      else:\n+        # Create copy of keras_tensor if we haven't done it before\n+        cpy = _clone_keras_tensor(obj)\n+        keras_tensor_mapping[id(obj)] = cpy\n+      result.append(cpy)\n+    else:\n+      result.append(obj)\n+  return tf.nest.pack_sequence_as(args, result)\n+\n+\n+def _clone_keras_tensor(kt):\n+  \"\"\"Create an idential keras_tensor based on the input.\n+\n+  We use keras_tensor_to_placeholder and keras_tensor_from_tensor to make sure\n+  inferred shape are not lost during the copy.\n+\n+  Args:\n+    kt: the input KerasTensor.\n+\n+  Returns:\n+    An indential copy of the input KerasTensor.\n+  \"\"\"\n+  # Create a scratch graph since we don't intend to use the placeholders.\n+  with backend._scratch_graph() as scratch_graph:  # pylint: disable=protected-access\n+    with scratch_graph.as_default():\n+      placeholder = keras_tensor.keras_tensor_to_placeholder(kt)\n+      return keras_tensor.keras_tensor_from_tensor(placeholder)\n\n@@ -0,0 +1,135 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#,============================================================================\n+\"\"\"Tests for functional_utils.\"\"\"\n+\n+from keras import keras_parameterized\n+from keras import layers\n+from keras import models\n+from keras.engine import functional_utils\n+from keras.engine import input_layer as input_layer_lib\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+class FunctionalModelSlideTest(keras_parameterized.TestCase):\n+\n+  def testfind_nodes_by_inputs_and_outputs(self):\n+    inputs = input_layer_lib.Input((10,))\n+    unconnected_inputs = input_layer_lib.Input((10,))\n+    x = layers.Dense(8)(inputs)\n+    y = layers.Dense(6)(x)\n+    output = layers.Dense(4)(y)\n+\n+    nodes_in_graph = functional_utils.find_nodes_by_inputs_and_outputs(\n+        x, output)\n+    self.assertLen(nodes_in_graph, 2)\n+    expected_nodes = [output.node, y.node]\n+    self.assertCountEqual(nodes_in_graph, expected_nodes)\n+\n+    # Make sure we raise error if we specify invalid input/output pair\n+    with self.assertRaisesRegex(\n+        ValueError, 'Found input tensor cannot be reached'):\n+      functional_utils.find_nodes_by_inputs_and_outputs(output, x)\n+\n+    with self.assertRaisesRegex(\n+        ValueError, 'Found input tensor cannot be reached'):\n+      functional_utils.find_nodes_by_inputs_and_outputs(unconnected_inputs,\n+                                                        output)\n+\n+    with self.assertRaisesRegex(\n+        ValueError, 'Found unvisited input tensors that are disconnected'):\n+      functional_utils.find_nodes_by_inputs_and_outputs(\n+          [inputs, unconnected_inputs], output)\n+\n+  def testfind_nodes_by_inputs_and_outputs_with_complicated_network(self):\n+    input1 = input_layer_lib.Input((10,))\n+    input2 = input_layer_lib.Input((10,))\n+    input3 = input_layer_lib.Input((10,))\n+    unconnected_input = input_layer_lib.Input((10,))\n+\n+    dense1 = layers.Dense(4, name='dense1')\n+    dense2 = layers.Dense(4, name='dense2')\n+    # dense1 are shared between input1 and input2\n+    a = dense1(input1)\n+    b = dense1(input2)\n+\n+    c = layers.Add()([a, b])\n+    d = dense2(input3)\n+    e = layers.Add()([c, d])\n+    # There are 5 nodes (invoke of __call__) in the graph.\n+\n+    nodes = functional_utils.find_nodes_by_inputs_and_outputs(input1, a)\n+    self.assertCountEqual(nodes, [a.node])\n+\n+    nodes = functional_utils.find_nodes_by_inputs_and_outputs(input2, b)\n+    self.assertCountEqual(nodes, [b.node])\n+\n+    nodes = functional_utils.find_nodes_by_inputs_and_outputs([input2, input1],\n+                                                              c)\n+    # This should contains 2 dense call and 1 add\n+    self.assertCountEqual(nodes, [a.node, b.node, c.node])\n+\n+    # Missing input3\n+    with self.assertRaisesRegex(\n+        ValueError, 'Found input tensor cannot be reached'):\n+      functional_utils.find_nodes_by_inputs_and_outputs([input1, input2], e)\n+\n+    nodes = functional_utils.find_nodes_by_inputs_and_outputs(\n+        [input1, input2, input3], e)\n+    self.assertCountEqual(nodes, [a.node, b.node, c.node, d.node, e.node])\n+\n+    # Make sure we can create from intermediate tensors\n+    nodes = functional_utils.find_nodes_by_inputs_and_outputs([a, b, input3], e)\n+    self.assertCountEqual(nodes, [c.node, d.node, e.node])\n+    # Also make sure we can add intermediate outputs\n+    nodes = functional_utils.find_nodes_by_inputs_and_outputs([a, b, input3],\n+                                                              [d, e])\n+    self.assertCountEqual(nodes, [c.node, d.node, e.node])\n+\n+    # input1 and 2 are not needed for computing d\n+    with self.assertRaisesRegex(\n+        ValueError, 'Found unvisited input tensors that are disconnected'):\n+      functional_utils.find_nodes_by_inputs_and_outputs(\n+          [input1, input2, input3], d)\n+\n+    with self.assertRaisesRegex(\n+        ValueError, 'Found unvisited input tensors that are disconnected'):\n+      functional_utils.find_nodes_by_inputs_and_outputs(\n+          [a, b, input3, unconnected_input], [e, d, c])\n+\n+  def test_build_model_from_intermediate_tensor(self):\n+    batch_size = 4\n+    inputs = input_layer_lib.Input(shape=(8,))\n+    layer1 = layers.Dense(32)\n+    layer2 = layers.Dense(16)\n+    x = layer1(inputs)\n+    y = layer2(x)\n+    cloned_inputs, cloned_outputs = functional_utils.clone_graph_nodes(x, y)\n+    # Make sure the inputs and outputs are cloned.\n+    self.assertIsNot(x, cloned_inputs)\n+    self.assertIsNot(y, cloned_outputs)\n+    # Make sure a new node is attached to layer2, which mimic y = layer2(x)\n+    self.assertLen(layer2.inbound_nodes, 2)\n+\n+    model = models.Model(cloned_inputs, cloned_outputs)\n+    self.assertIsInstance(model, models.Model)\n+\n+    model.compile('rmsprop', 'mse')\n+    model.fit(np.random.randn(batch_size, 32), np.random.randn(batch_size, 16))\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -329,6 +329,19 @@ class KerasTensor(object):\n     \"\"\"\n     return object_identity.Reference(self)\n \n+  @property\n+  def node(self):\n+    \"\"\"Find the corresponding `Node` that produce this keras_tensor.\n+\n+    During functional model construction, Keras will attach `KerasHistory` to\n+    keras tensor to track the connectivity between calls of layers. Return\n+    None if there isn't any KerasHistory attached to this tensor.\n+    \"\"\"\n+    if hasattr(self, '_keras_history'):\n+      layer, node_index, _ = self._keras_history\n+      return layer.inbound_nodes[node_index]\n+    return None\n+\n   def __iter__(self):\n     shape = None\n     if self.shape.ndims is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
