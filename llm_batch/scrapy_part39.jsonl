{"custom_id": "scrapy#bf87fb8455072883747bb935d6d275864a172829", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 460 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -157,4 +157,12 @@ def items_to_csv(file, items, delimiter=';', headers=None):\n \n CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z]')\n def string_camelcase(string):\n+    \"\"\" Convert a word  to it CamelCase version\n+    >>> string_camelcase('lost-pound')\n+    LostPound\n+\n+    >>> string_camelcase('missing_images')\n+    MissingImages\n+\n+    \"\"\"\n     return CAMELCASE_INVALID_CHARS.sub('', string.title())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe2f018b1af456b7a5a5738cd3b0ac9602ad4087", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 18 | Churn Cumulative: 582 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,13 +10,22 @@ import urllib\n from scrapy.http.request import Request\n from scrapy.utils.python import unicode_to_str\n \n+def _unicode_to_str(string, encoding):\n+    if hasattr(string, '__iter__'):\n+        return [unicode_to_str(k, encoding) for k in string]\n+    else:\n+        return unicode_to_str(string, encoding)\n+\n+\n class FormRequest(Request):\n \n     def __init__(self, *args, **kwargs):\n         formdata = kwargs.pop('formdata', None)\n         Request.__init__(self, *args, **kwargs)\n+\n         if formdata:\n             items = formdata.iteritems() if isinstance(formdata, dict) else formdata\n-            query = [(unicode_to_str(k, self.encoding), unicode_to_str(v, self.encoding)) for k, v in items]\n-            self.body = urllib.urlencode(query)\n+            query = [(unicode_to_str(k, self.encoding), _unicode_to_str(v, self.encoding))\n+                    for k, v in items]\n+            self.body = urllib.urlencode(query, doseq=1)\n             self.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n\n@@ -174,5 +174,10 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r3.encoding, 'latin1')\n         self.assertEqual(r3.body, 'price=%A3+100')\n \n+        # using multiples values for a single key\n+        data = {'price': u'\\xa3 100', 'colours': ['red', 'blue', 'green']}\n+        r3 = FormRequest(\"http://www.example.com\", formdata=data)\n+        self.assertEqual(r3.body, 'colours=red&colours=blue&colours=green&price=%C2%A3+100')\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#be04a7f328a2f391a77965848a1006eb2c51127a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 25 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -12,18 +12,21 @@\n # serve to show the default.\n \n import sys, os\n+from os import path\n \n # If your extensions are in another directory, add it here. If the directory\n # is relative to the documentation root, use os.path.abspath to make it\n # absolute, like shown here.\n-sys.path.append(os.path.join(os.path.dirname(__file__), \"_ext\"))\n+sys.path.append(path.join(path.dirname(__file__), \"_ext\"))\n+sys.path.append(path.join(path.dirname(path.dirname(__file__)), \"scrapy\"))\n+\n \n # General configuration\n # ---------------------\n \n # Add any Sphinx extension module names here, as strings. They can be extensions\n # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n-extensions = ['scrapydocs']\n+extensions = ['scrapydocs', 'sphinx.ext.autodoc']\n \n # Add any paths that contain templates here, relative to this directory.\n templates_path = ['../templates/docs']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#485b9c9ed5bace168c3632b283c9be0abf4b549a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 189 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -17,11 +17,11 @@ class XMLFeedSpider(BaseSpider):\n     This class intends to be the base class for spiders that scrape\n     from XML feeds.\n \n-    You can choose whether to parse the file using the 'iternodes' iterator,\n-    an 'xml' selector, or an 'html' selector.\n-    In most cases, it's convenient to use iternodes, since it's a faster and\n-    cleaner.\n+    You can choose whether to parse the file using the 'iternodes' iterator, an\n+    'xml' selector, or an 'html' selector.  In most cases, it's convenient to\n+    use iternodes, since it's a faster and cleaner.\n     \"\"\"\n+\n     iterator = 'iternodes'\n     itertag = 'item'\n \n@@ -30,16 +30,26 @@ class XMLFeedSpider(BaseSpider):\n         returned by the spider, and it's intended to perform any last time\n         processing required before returning the results to the framework core,\n         for example setting the item GUIDs. It receives a list of results and\n-        the response which originated that results. It must return a list\n-        of results (Items or Requests).\"\"\"\n+        the response which originated that results. It must return a list of\n+        results (Items or Requests).\n+        \"\"\"\n         return results\n \n     def adapt_response(self, response):\n         \"\"\"You can override this function in order to make any changes you want\n-        to into the feed before parsing it. This function must return a response.\"\"\"\n+        to into the feed before parsing it. This function must return a\n+        response.\n+        \"\"\"\n         return response\n \n     def parse_nodes(self, response, nodes):\n+        \"\"\"This method is called for the nodes matching the provided tag name\n+        (itertag). Receives the response and an XPathSelector for each node.\n+        Overriding this method is mandatory. Otherwise, you spider won't work.\n+        This method must return either a ScrapedItem, a Request, or a list\n+        containing any of them.\n+        \"\"\"\n+\n         for xSel in nodes:\n             ret = self.parse_item(response, xSel)\n             if isinstance(ret, (ScrapedItem, Request)):\n@@ -66,14 +76,14 @@ class XMLFeedSpider(BaseSpider):\n         return self.parse_nodes(response, nodes)\n \n class CSVFeedSpider(BaseSpider):\n-    \"\"\"\n-    Spider for parsing CSV feeds.\n+    \"\"\"Spider for parsing CSV feeds.\n     It receives a CSV file in a response; iterates through each of its rows,\n     and calls parse_row with a dict containing each field's data.\n \n     You can set some options regarding the CSV file, such as the delimiter\n     and the file's headers.\n     \"\"\"\n+\n     delimiter = None # When this is None, python's csv module's default delimiter is used\n     headers = None\n \n@@ -86,6 +96,12 @@ class CSVFeedSpider(BaseSpider):\n         return response\n \n     def parse_rows(self, response):\n+        \"\"\"Receives a response and a dict (representing each row) with a key for\n+        each provided (or detected) header of the CSV file.  This spider also\n+        gives the opportunity to override adapt_response and\n+        process_results methods for pre and post-processing purposes.\n+        \"\"\"\n+\n         for row in csviter(response, self.delimiter, self.headers):\n             ret = self.parse_row(response, row)\n             if isinstance(ret, (ScrapedItem, Request)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3fb945be7b0a3ed42058b21bdec79daa4a3a13f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 200 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -50,7 +50,6 @@ class ISpider(Interface, IPlugin) :\n class BaseSpider(object):\n     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n     class.\n-\n     \"\"\"\n \n     implements(ISpider)\n@@ -68,14 +67,16 @@ class BaseSpider(object):\n     def start_requests(self, urls=None):\n         \"\"\"Return the requests to crawl when this spider is opened for\n         scraping. urls contain the urls passed from command line (if any),\n-        otherwise None if the enrie domain was requested for scraping.\n+        otherwise None if the entire domain was requested for scraping.\n \n         This function must return a list of Requests to be crawled, based on\n         the given urls. The Requests must include a callback function which\n         must return a list of:\n+\n         * Request's for further crawling\n         * ScrapedItem's for processing\n         * Both\n+\n         Or None (which will be treated the same way as an empty list)\n \n         When a Request object is returned, the Request is scheduled, then\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a3723ba2ecd9c7863ca9c3e3ae9a5d0c22402dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 677 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -3,8 +3,6 @@ import copy\n class ScrapedItem(object):\n     \"\"\"\n     This is the base class for all scraped items.\n-    'guid' attribute is required, and is an attribute\n-    that identifies uniquely the given scraped item.\n     \"\"\"\n \n     def __init__(self, data=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90c1597cef44538b5ee7852973a50e8307f5aa8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 465 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -140,7 +140,7 @@ def items_to_csv(file, items, delimiter=';', headers=None):\n         return\n \n     if isinstance(file, basestring):\n-        file = open(file, 'a+')\n+        file = open(file, 'ab+')\n     csv_file = csv.writer(file, delimiter=delimiter, quoting=csv.QUOTE_ALL)\n     header = headers or sorted([key for key in items[0].__dict__.keys() if not key.startswith('_')])\n     if not file.tell():\n@@ -157,7 +157,8 @@ def items_to_csv(file, items, delimiter=';', headers=None):\n \n CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z]')\n def string_camelcase(string):\n-    \"\"\" Convert a word  to it CamelCase version\n+    \"\"\" Convert a word  to its CamelCase version and remove invalid chars\n+\n     >>> string_camelcase('lost-pound')\n     LostPound\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d376b2099642cb8830a69d8c4ef1200038df6872", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 469 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -160,10 +160,10 @@ def string_camelcase(string):\n     \"\"\" Convert a word  to its CamelCase version and remove invalid chars\n \n     >>> string_camelcase('lost-pound')\n-    LostPound\n+    'LostPound'\n \n     >>> string_camelcase('missing_images')\n-    MissingImages\n+    'MissingImages'\n \n     \"\"\"\n     return CAMELCASE_INVALID_CHARS.sub('', string.title())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4224be4118dd40fc4c22646d87a238648181daf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 102 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 108 | Churn Cumulative: 308 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,11 +4,11 @@ from scrapy import log\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import HttpException\n from scrapy.utils.url import urljoin_rfc as urljoin\n+from scrapy.utils.response import get_meta_refresh\n \n class RedirectLoop(Exception):\n     pass\n \n-META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\\d+);url=([^\"\\']+)', re.IGNORECASE)\n # some sites use meta-refresh for redirecting to a session expired page, so we\n # restrict automatic redirection to a maximum delay (in number of seconds)\n META_REFRESH_MAXSEC = 100\n@@ -53,10 +53,9 @@ class RedirectMiddleware(object):\n \n     def process_response(self, request, response, spider):\n         if isinstance(response, Response):\n-            m = META_REFRESH_RE.search(response.body[0:4096])\n-            if m and int(m.group(1)) < META_REFRESH_MAXSEC:\n-                redirected = request.copy()\n-                redirected.url = urljoin(request.url, m.group(2))\n+            interval, url = get_meta_refresh(response)\n+            if url and int(interval) < META_REFRESH_MAXSEC:\n+                redirected = request.replace(url=urljoin(request.url, url))\n                 log.msg(\"Redirecting (meta refresh) to %s from %s\" % (redirected, request), level=log.DEBUG, domain=spider.domain_name)\n                 return redirected\n         return response\n\n@@ -0,0 +1,68 @@\n+import unittest\n+\n+from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware\n+from scrapy.core.exceptions import HttpException\n+from scrapy.spider import spiders\n+from scrapy.http import Request, Response, Headers\n+\n+class RedirectMiddlewareTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        spiders.spider_modules = ['scrapy.tests.test_spiders']\n+        spiders.reload()\n+        self.spider = spiders.fromdomain('scrapytest.org')\n+\n+    def test_process_exception(self):\n+\n+        mw = RedirectMiddleware()\n+\n+        url = 'http://www.example.com/301'\n+        url2 = 'http://www.example.com/redirected'\n+        req = Request(url)\n+        hdr = Headers({'Location': [url2]})\n+        rsp = Response(url, headers=hdr)\n+        exc = HttpException('301', None, rsp)\n+\n+        req2 = mw.process_exception(req, exc, self.spider)\n+        assert isinstance(req2, Request)\n+        self.assertEqual(req2.url, url2)\n+\n+        url = 'http://www.example.com/302'\n+        url2 = 'http://www.example.com/redirected'\n+        req = Request(url, method='POST')\n+        hdr = Headers({'Location': [url2]})\n+        rsp = Response(url, headers=hdr)\n+        exc = HttpException('302', None, rsp)\n+\n+        req2 = mw.process_exception(req, exc, self.spider)\n+        assert isinstance(req2, Request)\n+        self.assertEqual(req2.url, url2)\n+        self.assertEqual(req2.method, 'GET')\n+        assert not req2.body\n+\n+    def test_process_response(self):\n+\n+        mw = RedirectMiddleware()\n+        \n+        body = \"\"\"<html>\n+            <head><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n+            </html>\"\"\"\n+        req = Request(url='http://example.org')\n+        rsp = Response(url='http://example.org', body=body)\n+        req2 = mw.process_response(req, rsp, self.spider)\n+\n+        assert isinstance(req2, Request)\n+        self.assertEqual(req2.url, 'http://example.org/newpage')\n+\n+        # meta-refresh with high intervals don't trigger redirects\n+        body = \"\"\"<html>\n+            <head><meta http-equiv=\"refresh\" content=\"1000;url=http://example.org/newpage\" /></head>\n+            </html>\"\"\"\n+        req = Request(url='http://example.org')\n+        rsp = Response(url='http://example.org', body=body)\n+        rsp2 = mw.process_response(req, rsp, self.spider)\n+\n+        assert rsp is rsp2\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -1,6 +1,6 @@\n import unittest\n from scrapy.http import Response, TextResponse\n-from scrapy.utils.response import body_or_str, get_base_url\n+from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh\n \n class ResponseUtilsTest(unittest.TestCase):\n     dummy_response = TextResponse(url='http://example.org/', body='dummy_response')\n@@ -32,6 +32,24 @@ class ResponseUtilsTest(unittest.TestCase):\n             </html>\"\"\")\n         self.assertEqual(get_base_url(response), 'http://example.org/something')\n \n+    def test_get_meta_refresh(self):\n+        body=\"\"\"\n+            <html>\n+            <head><title>Dummy</title><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n+            <body>blahablsdfsal&amp;</body>\n+            </html>\"\"\"\n+        response = Response(url='http://example.org', body=body)\n+        self.assertEqual(get_meta_refresh(response), ('5', 'http://example.org/newpage'))\n+\n+        # refresh without url should return (None, None)\n+        body=\"\"\"<meta http-equiv=\"refresh\" content=\"5\" />\"\"\"\n+        response = Response(url='http://example.org', body=body)\n+        self.assertEqual(get_meta_refresh(response), (None, None))\n+\n+        body=\"\"\"<meta http-equiv=\"refresh\" content=\"5;\n+            url=http://example.org/newpage\" /></head>\"\"\"\n+        response = Response(url='http://example.org', body=body)\n+        self.assertEqual(get_meta_refresh(response), ('5', 'http://example.org/newpage'))\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -25,3 +25,14 @@ def get_base_url(response):\n         match = BASEURL_RE.search(response.body[0:4096])\n         response.cache['base_url'] = match.group(1) if match else response.url\n     return response.cache['base_url']\n+\n+META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\\d+);\\s*url=([^\"\\']+)', re.IGNORECASE)\n+def get_meta_refresh(response):\n+    \"\"\" Return a tuple of two strings containing the interval and url included\n+    in the http-equiv parameter of the HTML meta element. If no url is included\n+    (None, None) is returned [instead of (interval, None)]\n+    \"\"\"\n+    if 'meta_refresh_url' not in response.cache:\n+        match = META_REFRESH_RE.search(response.body[0:4096])\n+        response.cache['meta_refresh_url'] = match.groups() if match else (None, None)\n+    return response.cache['meta_refresh_url']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8fdf06ed08c5fc9ccd4c3a49a38599ac927f56e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 191 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -53,7 +53,7 @@ def create_factory(request, spider):\n     agent = request.headers.pop('user-agent', default_agent)\n     factory = HTTPClientFactory(url=url, # never pass unicode urls to twisted\n                                 method=request.method,\n-                                postdata=request.body,\n+                                postdata=request.body or None,\n                                 headers=request.headers,\n                                 agent=agent,\n                                 cookies=request.cookies,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f43c58da1f587197291d938e48ebb9118040bdeb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 193 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -53,7 +53,7 @@ def create_factory(request, spider):\n     agent = request.headers.pop('user-agent', default_agent)\n     factory = HTTPClientFactory(url=url, # never pass unicode urls to twisted\n                                 method=request.method,\n-                                postdata=request.body or None,\n+                                postdata=request.body or None, # see http://dev.scrapy.org/ticket/60\n                                 headers=request.headers,\n                                 agent=agent,\n                                 cookies=request.cookies,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c9f2865c83c6bda427791bd9db6522a383242655", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 85 | Files Changed: 5 | Hunks: 31 | Methods Changed: 17 | Complexity Δ (Sum/Max): -2/3 | Churn Δ: 207 | Churn Cumulative: 692 | Contributors (this commit): 2 | Commits (past 90d): 21 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,6 +76,8 @@ DOWNLOADER_MIDDLEWARES = [\n \n DOWNLOADER_STATS = True\n \n+DUPLICATESFILTER_FILTERCLASS = 'scrapy.contrib.spidermiddleware.duplicatesfilter.SimplePerDomainFilter'\n+\n ENABLED_SPIDERS_FILE = ''\n \n ENGINE_DEBUG = False\n\n@@ -14,7 +14,20 @@ from scrapy import log\n \n \n class DuplicatesFilterMiddleware(object):\n-    \"\"\"Filter out duplicate requests to avoid visiting same page more than once\"\"\"\n+    \"\"\"Filter out duplicate requests to avoid visiting same page more than once.\n+\n+    filter class (defined by DUPLICATESFILTER_FILTERCLASS setting) must\n+    inplement a simple API:\n+\n+      * open(domain) called when a new domain starts\n+\n+      * close(domain) called when a domain is going to be closed.\n+\n+      * add(domain, request) called each time a new request needs to be tested\n+        looking if it is was already seen or is a new one.  This is the most\n+        important method for a filtering class.\n+\n+    \"\"\"\n \n     def __init__(self):\n         clspath = settings.get('DUPLICATESFILTER_FILTERCLASS')\n@@ -57,9 +70,3 @@ class SimplePerDomainFilter(dict):\n             self[domain].add(fp)\n             return True\n         return False\n-\n-    def has(self, domain, request):\n-        \"\"\"Check if a request was already seen for a domain\"\"\"\n-        return request_fingerprint(request) in self[domain]\n-\n-\n\n@@ -4,49 +4,47 @@ The Scrapy Scheduler\n \n from twisted.internet import defer\n \n-from scrapy.core.scheduler.filter import GroupFilter\n from scrapy import log\n-from scrapy.core.exceptions import IgnoreRequest\n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n-from scrapy.utils.request import request_fingerprint\n-from scrapy.utils.defer import defer_fail\n from scrapy.conf import settings\n \n \n class Scheduler(object) :\n-    \"\"\"\n-    The scheduler decides what to scrape next. In other words, it defines the\n-    crawling order.  The scheduler schedules websites and requests to be\n-    scraped.  Individual web pages that are to be scraped are batched up into a\n-    \"run\" for a website.  As the domain is being scraped, pages that are\n-    discovered are added to the scheduler. \n+    \"\"\"The scheduler decides what to scrape next. In other words, it defines the\n+    crawling order.\n+\n+    The scheduler schedules websites and requests to be scraped.  Individual\n+    web pages that are to be scraped are batched up into a \"run\" for a website.\n+\n+    As the domain is being scraped, pages that are discovered are added to the\n+    scheduler.\n \n     Typical usage:\n \n-    * next_available_domain() called to find out when there is something to do\n+        * next_domain() called each time a domain slot is freed, and return\n+        next domain to be scraped.\n+\n         * open_domain() called to commence scraping a website\n-    * enqueue_request() called multiple times when new links found\n-    * next_request() called multiple times when there is capacity to process urls\n-    * close_domain() called when there are no more pages or upon error\n \n-    Note a couple things:\n-    1) The order in which you get back the list of pages to scrape is not\n+        * enqueue_request() called multiple times to enqueue new requests to be downloaded\n+\n+        * next_request() called multiple times when there is capacity to download requests\n+\n+        * close_domain() called when there are no more pages for a website\n+\n+    Notes:\n+\n+        1. The order in which you get back the list of pages to scrape is not\n         necesarily the order you put them in.\n-    2) A canonical URL is calculated for each url for each domain to check that\n-       it is unique, however the actual url passed in is returned when\n-       get_next_page is called.\n-       This is for two main reasons:\n-        * To be nice to the screen scraped site just incase the specific\n-          format of the url is significant.\n-        * To take advantage of any caching that uses the URL/URI as a key\n \n-    all_domains contains the names of all domains that are to be scheduled.\n+    ``pending_domains_count`` contains the names of all domains that are to be scheduled.\n \n     Two crawling orders are available by default, which can be set with the\n     SCHEDULER_ORDER settings:\n \n         * BFO - breath-first order (default). Consumes more memory than DFO but reaches\n                 most relevant pages faster.\n+\n         * DFO - depth-first order. Consumes less memory than BFO but usually takes\n                 longer to reach the most relevant pages.\n     \"\"\"\n@@ -55,23 +53,23 @@ class Scheduler(object) :\n         self.pending_domains_count = {}\n         self.domains_queue = PriorityQueue()\n         self.pending_requests = {}\n-        self.groupfilter = GroupFilter()\n         self.dfo = settings.get('SCHEDULER_ORDER', '').upper() == 'DFO'\n \n     def domain_is_open(self, domain):\n+        \"\"\"Check if scheduler's resources were allocated for a domain\"\"\"\n         return domain in self.pending_requests\n \n     def is_pending(self, domain):\n+        \"\"\"Check if a domain is waiting to be scraped in domain's queue.\"\"\"\n         return domain in self.pending_domains_count\n \n     def domain_has_pending(self, domain):\n+        \"\"\"Check if are there pending requests for a domain\"\"\"\n         if domain in self.pending_requests:\n             return bool(self.pending_requests[domain])\n \n     def next_domain(self) :\n-        \"\"\"\n-        Return next domain available to scrape and remove it from available domains queue\n-        \"\"\"\n+        \"\"\"Return next domain available to scrape and remove it from available domains queue\"\"\"\n         if self.pending_domains_count:\n             domain, priority = self.domains_queue.pop()\n             if self.pending_domains_count[domain] == 1:\n@@ -82,11 +80,13 @@ class Scheduler(object) :\n         return None\n \n     def add_domain(self, domain, priority=1):\n-        \"\"\"\n-        This functions schedules a new domain to be scraped, with the given\n-        priority. It doesn't check if the domain is already scheduled.  A\n-        domain can be scheduled twice, either with the same or with different\n+        \"\"\"This functions schedules a new domain to be scraped, with the given priority.\n+\n+        It doesn't check if the domain is already scheduled.\n+\n+        A domain can be scheduled twice, either with the same or with different\n         priority.\n+\n         \"\"\"\n         self.domains_queue.push(domain, priority)\n         if domain not in self.pending_domains_count:\n@@ -95,53 +95,37 @@ class Scheduler(object) :\n             self.pending_domains_count[domain] += 1\n \n     def open_domain(self, domain):\n-        \"\"\" \n-        Allocates resources for maintaining a schedule for domain.\n-        \"\"\"\n-        if self.dfo:\n-            self.pending_requests[domain] = PriorityStack()\n-        else:\n-            self.pending_requests[domain] = PriorityQueue()\n-\n-        self.groupfilter.open(domain)\n+        \"\"\"Allocates resources for maintaining a schedule for domain.\"\"\"\n+        Priority = PriorityStack if self.dfo else PriorityQueue\n+        self.pending_requests[domain] = Priority()\n \n     def enqueue_request(self, domain, request, priority=1):\n-        \"\"\"\n-        Add a page to be scraped for a domain that is currently being scraped.\n-        \"\"\"\n-        requestid = request_fingerprint(request)\n-        added = self.groupfilter.add(domain, requestid)\n-\n-        if request.dont_filter or added:\n-            deferred = defer.Deferred()\n-            self.pending_requests[domain].push((request, deferred), priority)\n-            return deferred\n-        else:\n-            return defer_fail(IgnoreRequest('Skipped (already visited): %s' % request))\n-\n-    def request_seen(self, domain, request):\n-        \"\"\"\n-        Returns True if the given Request was scheduled before for the given\n-        domain\n-        \"\"\"\n-        return self.groupfilter.has(domain, request_fingerprint(request))\n+        \"\"\"Enqueue a request to be downloaded for a domain that is currently being scraped.\"\"\"\n+        dfd = defer.Deferred()\n+        self.pending_requests[domain].push((request, dfd), priority)\n+        return dfd\n \n     def next_request(self, domain):\n-        \"\"\"\n-        Get the next request to be scraped.\n+        \"\"\"Return the next available request to be downloaded for a domain.\n+\n+        Returns a pair ``(request, deferred)`` where ``deferred`` is the\n+        `Deferred` instance returned to the original requester.\n+\n+        ``(None, None)`` should be returned if there aren't requests pending\n+        for the domain.\n \n-        None should be returned if there are no more request pending for the domain passed.\n         \"\"\"\n-        pending_list = self.pending_requests.get(domain)\n-        if pending_list:\n-            return pending_list.pop()[0]\n-        else:\n+        try:\n+            # The second value is the request scheduled priority, returns the first one.\n+            return self.pending_requests[domain].pop()[0]\n+        except (KeyError, IndexError), ex:\n             return (None, None)\n \n     def close_domain(self, domain) :\n-        \"\"\"\n-        Called once we are finished scraping a domain. The scheduler will\n-        free any resources associated with the domain.\n+        \"\"\"Called once we are finished scraping a domain.\n+\n+        The scheduler will free any resources associated with the domain.\n+\n         \"\"\"\n         try :\n             del self.pending_requests[domain]\n@@ -149,12 +133,6 @@ class Scheduler(object) :\n             msg = \"Could not clear pending pages for domain %s, %s\" % (domain, inst)\n             log.msg(msg, level=log.WARNING)\n \n-        try :\n-            self.groupfilter.close(domain)\n-        except Exception, inst:\n-            msg = \"Could not clear url filter for domain %s, %s\" % (domain, inst)\n-            log.msg(msg, level=log.WARNING)\n-\n     def remove_pending_domain(self, domain):\n         \"\"\"\n         Remove a pending domain not yet started. If the domain was enqueued\n@@ -165,9 +143,11 @@ class Scheduler(object) :\n \n         If domain is open (not pending) it is not removed and returns None. You\n         need to call close_domain for open domains.\n+\n         \"\"\"\n         if not self.domain_is_open(domain):\n             return self.pending_domains_count.pop(domain, 0)\n \n     def is_idle(self):\n+        \"\"\"Checks if the schedulers has any request pendings\"\"\"\n         return not self.pending_requests\n\n@@ -104,5 +104,3 @@ URLLENGTH_LIMIT = 2083\n WS_ENABLED = 0\n \n SPIDERPROFILER_ENABLED = 0\n-\n-#DUPLICATESFILTER_FILTERCLASS = 'scrapy.contrib.spidermiddleware.duplicatesfilter.SimplePerDomainFilter'\n\n@@ -0,0 +1,50 @@\n+import unittest\n+\n+from scrapy.spider import spiders\n+from scrapy.http import Request, Response\n+from scrapy.contrib.spidermiddleware.duplicatesfilter import DuplicatesFilterMiddleware, SimplePerDomainFilter\n+\n+class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        spiders.spider_modules = ['scrapy.tests.test_spiders']\n+        spiders.reload()\n+        self.spider = spiders.fromdomain('scrapytest.org')\n+\n+    def test_process_spider_output(self):\n+        mw = DuplicatesFilterMiddleware()\n+        mw.filter.open('scrapytest.org')\n+\n+        response = Response('')\n+        r1 = Request('http://scrapytest.org/1')\n+        r2 = Request('http://scrapytest.org/2')\n+        r3 = Request('http://scrapytest.org/2')\n+\n+        filtered = list(mw.process_spider_output(response, [r1, r2, r3], self.spider))\n+\n+        assert r1 in filtered\n+        assert r2 in filtered\n+        assert r3 not in filtered\n+\n+        mw.filter.close('scrapytest.org')\n+\n+\n+class SimplePerDomainFilterTest(unittest.TestCase):\n+\n+    def test_filter(self):\n+        domain = 'scrapytest.org'\n+        filter = SimplePerDomainFilter()\n+        filter.open(domain)\n+        assert domain in filter\n+\n+        r1 = Request('http://scrapytest.org/1')\n+        r2 = Request('http://scrapytest.org/2')\n+        r3 = Request('http://scrapytest.org/2')\n+\n+        assert filter.add(domain, r1)\n+        assert filter.add(domain, r2)\n+        assert not filter.add(domain, r3)\n+\n+        filter.close(domain)\n+        assert domain not in filter\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e4bc6141aff1aa7e04844712f720aa0dfb71214", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 47 | Files Changed: 3 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 47 | Churn Cumulative: 96 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,3 +1,2 @@\n from scrapy.core.scheduler.schedulers import Scheduler\n-from scrapy.core.scheduler.filter import GroupFilter\n from scrapy.core.scheduler.store import MemoryStore\n\n@@ -1,19 +0,0 @@\n-class GroupFilter(dict):\n-    \"\"\"Filter groups of keys\"\"\"\n-    def open(self, group):\n-        self[group] = set()\n-\n-    def close(self, group):\n-        del self[group]\n-\n-    def add(self, group, key):\n-        \"\"\"Add a key to the group if an equivalent key has not already been added.\n-        This method will return true if the key was added and false otherwise.\n-        \"\"\"\n-        if key not in self[group]:\n-            self[group].add(key)\n-            return True\n-        return False\n-\n-    def has(self, group, key):\n-        return key in self[group]\n\n@@ -1,27 +0,0 @@\n-import unittest\n-from scrapy.core.scheduler import GroupFilter\n-\n-class GroupFilterTest(unittest.TestCase):\n-\n-    def test_groupfilter(self):\n-        k1 = \"id1\"\n-        k2 = \"id1\"\n-\n-        f = GroupFilter()\n-        f.open(\"mygroup\")\n-        self.assertTrue(f.add(\"mygroup\", k1))\n-        self.assertFalse(f.add(\"mygroup\", k1))\n-        self.assertFalse(f.add(\"mygroup\", k2))\n-\n-        f.open('anothergroup')\n-        self.assertTrue(f.add(\"anothergroup\", k1))\n-        self.assertFalse(f.add(\"anothergroup\", k1))\n-        self.assertFalse(f.add(\"anothergroup\", k2))\n-\n-        f.close('mygroup')\n-        f.open('mygroup')\n-        self.assertTrue(f.add(\"mygroup\", k2))\n-        self.assertFalse(f.add(\"mygroup\", k1))\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d68646422db3b3ffb2de45f15080c40522458bd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 147 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,8 @@\n+try:\n     import MySQLdb\n+except ImportError:\n+    pass\n+\n from twisted.trial import unittest\n from datetime import datetime, timedelta\n from time import sleep\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b14251510cfc802e6493fc477602e9a9266a00f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 32 | Churn Cumulative: 351 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from __future__ import with_statement\n \n+import errno\n import os\n import hashlib\n import datetime\n@@ -111,36 +112,37 @@ class Cache(object):\n                 if not os.path.exists(linkpath):\n                     os.makedirs(linkpath)\n \n-    def is_cached(self, domain, key):\n+    def read_meta(self, domain, key):\n+        \"\"\"Return the metadata dictionary (possibly empty) if the entry is\n+        cached, None otherwise.\n+        \"\"\"\n         requestpath = self.requestpath(domain, key)\n-        if os.path.exists(requestpath):\n+        try:\n             with open(os.path.join(requestpath, 'pickled_meta'), 'r') as f:\n                 metadata = pickle.load(f)\n+        except IOError, e:\n+            if e.errno != errno.ENOENT:\n+                raise\n+            return None\n         expiration_secs = settings.getint('CACHE2_EXPIRATION_SECS')\n         if expiration_secs >= 0:\n-                if datetime.datetime.utcnow() <= metadata['timestamp'] + datetime.timedelta(seconds=expiration_secs):\n-                    return True\n-                else:\n+            expiration_date = metadata['timestamp'] + datetime.timedelta(seconds=expiration_secs)\n+            if datetime.datetime.utcnow() > expiration_date:\n                 log.msg('dropping old cached response from %s' % metadata['timestamp'], level=log.DEBUG)\n-                    return False\n-            else:\n-                # disabled cache expiration\n-                return True\n-        else:\n-            return False\n+                return None\n+        return metadata\n \n     def retrieve_response(self, domain, key):\n         \"\"\"\n         Return response dictionary if request has correspondent cache record;\n         return None if not.\n         \"\"\"\n-        if not self.is_cached(domain, key):\n+        metadata = self.read_meta(domain, key)\n+        if metadata is None:\n             return None # not cached\n \n         requestpath = self.requestpath(domain, key)\n-        metadata = responsebody = responseheaders = None\n-        with open(os.path.join(requestpath, 'pickled_meta'), 'r') as f:\n-            metadata = pickle.load(f)\n+        responsebody = responseheaders = None\n         with open(os.path.join(requestpath, 'response_body')) as f:\n             responsebody = f.read()\n         with open(os.path.join(requestpath, 'response_headers')) as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8557f6bc578f03afc32f75e72c21180980f6cdb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 88 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -45,7 +45,7 @@ class DuplicatesFilterMiddleware(object):\n             if isinstance(req, Request):\n                 added = self.filter.add(domain, req)\n                 if not (added or req.dont_filter):\n-                    log.msg('Skipped (already visited): %s' % req, log.DEBUG, domain=domain)\n+                    log.msg('Skipped (already visited): %s' % req, log.TRACE, domain=domain)\n                     continue\n             yield req\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3876a1827f502c4977272a3819d8234d1bf6a3c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 13 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 142/89 | Churn Δ: 10 | Churn Cumulative: 13 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +0,0 @@\n-from scrapy.contrib.pbcluster.worker.manager import ClusterWorker\n-from scrapy.contrib.pbcluster.master.web import ClusterMasterWeb\n-from scrapy.contrib.pbcluster.crawler.manager import ClusterCrawler\n\\ No newline at end of file\n\n@@ -0,0 +1,3 @@\n+from scrapy.contrib_exp.cluster.worker.manager import ClusterWorker\n+from scrapy.contrib_exp.cluster.master.web import ClusterMasterWeb\n+from scrapy.contrib_exp.cluster.crawler.manager import ClusterCrawler\n\n\n\n\n\n\n@@ -11,7 +11,7 @@ from scrapy.core import signals\n from scrapy import log\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.contrib.pbcluster.worker.manager import ResponseCode\n+from scrapy.contrib_exp.cluster.worker.manager import ResponseCode\n from scrapy.conf import settings\n \n def my_import(name):\n\n@@ -4,7 +4,7 @@ from pydispatch import dispatcher\n \n from scrapy.spider import spiders \n from scrapy.management.web import banner, webconsole_discover_module\n-from scrapy.contrib.pbcluster.master.manager import ClusterMaster\n+from scrapy.contrib_exp.cluster.master.manager import ClusterMaster\n from scrapy.utils.serialization import serialize\n \n class ClusterMasterWeb(ClusterMaster):\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#755235b5686890d410d7dbda1a0ba13c5e275ea9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 34 | Files Changed: 12 | Hunks: 28 | Methods Changed: 13 | Complexity Δ (Sum/Max): 27/27 | Churn Δ: 75 | Churn Cumulative: 1954 | Contributors (this commit): 3 | Commits (past 90d): 47 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ import scrapy\n from scrapy.command import ScrapyCommand\n from scrapy.spider import spiders\n from scrapy.xpath import XmlXPathSelector, HtmlXPathSelector\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n from scrapy.extension import extensions\n from scrapy.conf import settings\n from scrapy.core.manager import scrapymanager\n@@ -85,7 +85,7 @@ class Command(ScrapyCommand):\n             return True\n \n     def generate_vars(self, url, response):\n-        itemcls = load_class(settings['DEFAULT_ITEM_CLASS'])\n+        itemcls = load_object(settings['DEFAULT_ITEM_CLASS'])\n         item = itemcls()\n         self.vars['item'] = item\n         if url:\n\n@@ -3,7 +3,7 @@ from datetime import datetime\n \n from pydispatch import dispatcher\n \n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n from scrapy.core import signals\n from scrapy import log\n from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n@@ -16,7 +16,7 @@ class HistoryMiddleware(object):\n     MIN_PROCESS_UNCHANGED_DAYS = 12\n \n     def __init__(self):\n-        historycls = load_class(settings['MEMORYSTORE'])\n+        historycls = load_object(settings['MEMORYSTORE'])\n         if not historycls:\n             raise NotConfigured\n         self.historydata = historycls()\n\n@@ -8,7 +8,7 @@ from scrapy.core import signals\n from scrapy.http import Request\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.request import request_fingerprint\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n from scrapy import log\n \n@@ -34,7 +34,7 @@ class DuplicatesFilterMiddleware(object):\n         if not clspath:\n             raise NotConfigured\n \n-        self.filter = load_class(clspath)()\n+        self.filter = load_object(clspath)()\n         dispatcher.connect(self.filter.open, signals.domain_open)\n         dispatcher.connect(self.filter.close, signals.domain_closed)\n \n\n@@ -11,7 +11,7 @@ from twisted.spread import pb\n from scrapy import log\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n@@ -95,7 +95,7 @@ class ClusterWorker(pb.Root):\n         self.running = {} # dict of domain->ScrapyProcessControl \n         self.crawlers = {} # dict of pid->scrapy process remote pb connection\n         self.starttime = datetime.datetime.utcnow()\n-        self.prerun_hooks = [load_class(f) for f in settings.getlist('CLUSTER_WORKER_PRERUN_HOOKS', [])]\n+        self.prerun_hooks = [load_object(f) for f in settings.getlist('CLUSTER_WORKER_PRERUN_HOOKS', [])]\n         port = settings.getint('CLUSTER_WORKER_PORT')\n         factory = pb.PBServerFactory(self, unsafeTracebacks=True)\n         scrapyengine.listenTCP(port, factory)\n\n@@ -10,7 +10,7 @@ from scrapy.core import signals\n from scrapy import log\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.core.downloader.handlers import download_any\n from scrapy.conf import settings\n@@ -38,7 +38,7 @@ class DownloaderMiddlewareManager(object):\n         \"\"\"\n         mws = []\n         for mwpath in settings.getlist('DOWNLOADER_MIDDLEWARES') or ():\n-            cls = load_class(mwpath)\n+            cls = load_object(mwpath)\n             if cls:\n                 try:\n                     mw = cls()\n\n@@ -8,7 +8,7 @@ from os.path import abspath, dirname, join\n from mimetypes import MimeTypes\n \n from scrapy.http import Response\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n from scrapy.utils.python import isbinarytext\n from scrapy.conf import settings\n \n@@ -27,7 +27,7 @@ class ResponseTypes(object):\n         mimefile = join(abspath(dirname(__file__)), 'mime.types')\n         self.mimetypes = MimeTypes([mimefile])\n         for mimetype, cls in self.CLASSES.iteritems():\n-            self.classes[mimetype] = load_class(cls)\n+            self.classes[mimetype] = load_object(cls)\n \n     def from_mimetype(self, mimetype):\n         \"\"\"Return the most appropiate Response class for the given mimetype\"\"\"\n\n@@ -5,7 +5,7 @@ from scrapy import log\n from scrapy.http import Request\n from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n from scrapy.utils.url import is_url\n from scrapy.conf import settings\n \n@@ -27,11 +27,11 @@ class ExecutionManager(object):\n         extensions.load()\n         log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()))\n \n-        scheduler = load_class(settings['SCHEDULER'])()\n+        scheduler = load_object(settings['SCHEDULER'])()\n \n         scrapyengine.configure(scheduler=scheduler)\n \n-        self.prioritizer_class = load_class(settings['PRIORITIZER'])\n+        self.prioritizer_class = load_object(settings['PRIORITIZER'])\n \n         requests = self._parse_args(args)\n         self.priorities = self.prioritizer_class(requests.keys())\n\n@@ -4,7 +4,7 @@ keeping track of all enabled extensions. It also contains an instantiated\n ExtensionManager (extensions) to be used as singleton.\n \"\"\"\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n from scrapy import log\n from scrapy.conf import settings\n \n@@ -25,7 +25,7 @@ class ExtensionManager(object):\n         self.disabled.clear()\n         for extension_path in settings.getlist('EXTENSIONS'):\n             try:\n-                cls = load_class(extension_path)\n+                cls = load_object(extension_path)\n                 self.enabled[cls.__name__] = cls()\n             except NotConfigured, e:\n                 self.disabled[cls.__name__] = extension_path\n\n@@ -1,7 +1,7 @@\n from scrapy import log\n from scrapy.core.exceptions import DropItem, NotConfigured\n from scrapy.item import ScrapedItem\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n from scrapy.utils.defer import defer_succeed, mustbe_deferred\n from scrapy.conf import settings\n \n@@ -18,7 +18,7 @@ class ItemPipelineManager(object):\n         Load pipelines stages defined in settings module\n         \"\"\"\n         for stage in settings.getlist('ITEM_PIPELINES') or ():\n-            cls = load_class(stage)\n+            cls = load_object(stage)\n             if cls:\n                 try:\n                     stageinstance = cls()\n\n@@ -12,7 +12,7 @@ from scrapy.spider.models import ISpider\n from scrapy import log\n from scrapy.conf import settings\n from scrapy.utils.url import url_is_from_spider\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n \n class SpiderManager(object):\n     \"\"\"Spider locator and manager\"\"\"\n@@ -54,7 +54,7 @@ class SpiderManager(object):\n         if not spider:                          # create a custom spider\n             spiderclassname = settings.get('DEFAULT_SPIDER')\n             if spiderclassname:\n-                spider = load_class(spiderclassname)(domain)\n+                spider = load_object(spiderclassname)(domain)\n                 self.add_spider(spider)\n             \n         return spider\n\n@@ -8,7 +8,7 @@ docs/topics/spider-middleware.rst\n \n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.conf import settings\n \n@@ -38,7 +38,7 @@ class SpiderMiddlewareManager(object):\n         \"\"\"Load middleware defined in settings module\"\"\"\n         mws = []\n         for mwpath in settings.getlist('SPIDER_MIDDLEWARES') or ():\n-            cls = load_class(mwpath)\n+            cls = load_object(mwpath)\n             if cls:\n                 try:\n                     mw = cls()\n\n@@ -60,24 +60,31 @@ def stats_getpath(dict_, path, default=None):\n             return default\n     return dict_\n \n-def load_class(class_path):\n-    \"\"\"Load a class given its absolute class path, and return it without\n-    instantiating it\"\"\"\n+def load_object(path):\n+    \"\"\"Load an object given its absolute object path, and return it.\n+\n+    object can be a class, function, variable o instance.\n+    path ie: 'scrapy.contrib.downloadermiddelware.redirect.RedirectMiddleware'\n+    \"\"\"\n+\n     try:\n-        dot = class_path.rindex('.')\n+        dot = path.rindex('.')\n     except ValueError:\n-        raise UsageError, '%s isn\\'t a module' % class_path\n-    module, classname = class_path[:dot], class_path[dot+1:]\n+        raise UsageError, '%s isn\\'t a module' % path\n+\n+    module, name = path[:dot], path[dot+1:]\n     try:\n         mod = __import__(module, {}, {}, [''])\n     except ImportError, e:\n         raise UsageError, 'Error importing %s: \"%s\"' % (module, e)\n-    try:\n-        cls = getattr(mod, classname)\n-    except AttributeError:\n-        raise UsageError, 'module \"%s\" does not define a \"%s\" class' % (module, classname)\n \n-    return cls\n+    try:\n+        obj = getattr(mod, name)\n+    except AttributeError:\n+        raise UsageError, 'module \"%s\" does not define any object named \"%s\"' % (module, name)\n+\n+    return obj\n+load_class = load_object # backwards compatibility, but isnt going to be available for too long.\n \n def extract_regex(regex, text, encoding):\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
