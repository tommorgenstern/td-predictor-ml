{"custom_id": "keras#4dcb2c04e396ee812a5fee981b45f723efee68f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 62 | Churn Cumulative: 217 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,19 @@ def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncati\n \n         Supports post-padding and pre-padding (default).\n \n+        Parameters:\n+        -----------\n+        sequences: list of lists where each element is a sequence\n+        maxlen: int, maximum length\n+        dtype: type to cast the resulting sequence.\n+        padding: 'pre' or 'post', pad either before or after each sequence.\n+        truncating: 'pre' or 'post', remove values from sequences larger than\n+            maxlen either in the beginning or in the end of the sequence\n+        value: float, value to pad the sequences to the desired value.\n+\n+        Returns:\n+        x: numpy array with dimensions (number_of_sequences, maxlen)\n+\n     \"\"\"\n     lengths = [len(s) for s in sequences]\n \n@@ -55,12 +68,17 @@ def make_sampling_table(size, sampling_factor=1e-5):\n         a numerical approximation of frequency(rank):\n            frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank))\n         where gamma is the Euler-Mascheroni constant.\n+\n+        Parameters:\n+        -----------\n+        size: int, number of possible words to sample. \n     '''\n     gamma = 0.577\n     rank = np.array(list(range(size)))\n     rank[0] = 1\n     inv_fq = rank * (np.log(rank) + gamma) + 0.5 - 1./(12.*rank)\n     f = sampling_factor * inv_fq\n+\n     return np.minimum(1., f / np.sqrt(f))\n \n \n@@ -73,13 +91,22 @@ def skipgrams(sequence, vocabulary_size,\n         where label = 1 if 'other_word' belongs to the context of 'word',\n         and label=0 if 'other_word' is ramdomly sampled\n \n-        @param vocabulary_size: int. maximum possible word index + 1\n-        @param window_size: int. actually half-window. The window of a word wi will be [i-window_size, i+window_size+1]\n-        @param negative_samples: float >= 0. 0 for no negative (=random) samples. 1 for same number as positive samples. etc.\n-        @param categorical: bool. if False, labels will be integers (eg. [0, 1, 1 .. ]), \n+        Paramaters:\n+        -----------\n+        vocabulary_size: int. maximum possible word index + 1\n+        window_size: int. actually half-window. The window of a word wi will be [i-window_size, i+window_size+1]\n+        negative_samples: float >= 0. 0 for no negative (=random) samples. 1 for same number as positive samples. etc.\n+        categorical: bool. if False, labels will be integers (eg. [0, 1, 1 .. ]),\n             if True labels will be categorical eg. [[1,0],[0,1],[0,1] .. ]\n \n-        Note: by convention, index 0 in the vocabulary is a non-word and will be skipped.\n+        Returns:\n+        --------\n+        couples, lables: where `couples` are int pairs and\n+            `labels` are either 0 or 1.\n+\n+        Notes:\n+        ------\n+        By convention, index 0 in the vocabulary is a non-word and will be skipped.\n     '''\n     couples = []\n     labels = []\n\n@@ -0,0 +1,25 @@\n+import numpy as np\n+from numpy.testing import assert_allclose\n+\n+from keras.preprocessing.sequences import (pad_sequences, make_sampling_table,\n+                                           skipgrams)\n+\n+\n+def test_pad_sequences():\n+    a = [np.arange(i) for i in np.arange(2, 5, 1)]\n+    b = pad_sequences(a, maxlen=3)\n+    assert_allclose(b, [[0, 0, 1], [0, 1, 2], [1, 2, 3]])\n+\n+\n+def test_make_sampling_table():\n+    a = make_sampling_table(3)\n+    assert_allclose(a, np.asarray([0.00315225,  0.00315225,  0.00547597]),\n+                    rtol=.1)\n+\n+\n+def test_skipgrams():\n+    couples, labels = skipgrams(np.arange(3), 3)\n+    for couple in couples:\n+        assert couple[0] in [0, 1, 2] and couple[1] in [0, 1, 2]\n+\n+    assert 0 in labels and 1 in labels\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3dddabebc4759a51a0e6bc32012fc10c8de3a85b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 17 | Methods Changed: 4 | Complexity Δ (Sum/Max): -21/0 | Churn Δ: 24 | Churn Cumulative: 7952 | Contributors (this commit): 33 | Commits (past 90d): 136 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -264,6 +264,7 @@ class Layer(object):\n         '''\n         return sum([K.count_params(p) for p in self.params])\n \n+\n class MaskedLayer(Layer):\n     '''If your layer trivially supports masking\n     (by simply copying the input mask to the output),\n\n@@ -16,10 +16,10 @@ def test_layer_call():\n     W = np.asarray(K.eval(layer.W)).astype(K.floatx())\n     X = K.placeholder(ndim=2)\n     Y = layer(X)\n-    F = K.function([X], [Y])\n+    f = K.function([X], [Y])\n \n     x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n-    y = F([x])[0].astype(K.floatx())\n+    y = f([x])[0].astype(K.floatx())\n     t = np.dot(x, W).astype(K.floatx())\n     assert_allclose(t, y, rtol=.2)\n \n@@ -34,10 +34,10 @@ def test_sequential_call():\n     # test flat model\n     X = K.placeholder(ndim=2)\n     Y = model(X)\n-    F = K.function([X], [Y])\n+    f = K.function([X], [Y])\n \n     x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n-    y1 = F([x])[0].astype(K.floatx())\n+    y1 = f([x])[0].astype(K.floatx())\n     y2 = model.predict(x)\n     # results of __call__ should match model.predict\n     assert_allclose(y1, y2)\n@@ -48,12 +48,13 @@ def test_sequential_call():\n     model2.compile('sgd', 'mse')\n \n     Y2 = model2(X)\n-    F = K.function([X], [Y2])\n+    f = K.function([X], [Y2])\n \n-    y1 = F([x])[0].astype(K.floatx())\n+    y1 = f([x])[0].astype(K.floatx())\n     y2 = model2.predict(x)\n     # results of __call__ should match model.predict\n     assert_allclose(y1, y2)\n \n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -56,7 +56,7 @@ def test_sequential():\n     model.train_on_batch(X_train[:32], y_train[:32])\n \n     loss = model.evaluate(X_train, y_train, verbose=0)\n-    assert(loss < 0.7)\n+    assert(loss < 0.8)\n \n     model.predict(X_test, verbose=0)\n     model.predict_classes(X_test, verbose=0)\n@@ -388,13 +388,12 @@ def test_sequential_count_params():\n     model.add(Dense(nb_units))\n     model.add(Dense(nb_classes))\n     model.add(Activation('softmax'))\n-\n     assert(n == model.count_params())\n \n     model.compile('sgd', 'binary_crossentropy')\n-\n     assert(n == model.count_params())\n \n+\n def test_siamese_1():\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n@@ -448,6 +447,7 @@ def test_siamese_1():\n     nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n     assert(loss == nloss)\n \n+\n def test_siamese_2():\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n@@ -626,6 +626,7 @@ def test_1o_2i():\n \n     graph.get_config(verbose=1)\n \n+\n def test_siamese_3():\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n@@ -651,6 +652,7 @@ def test_siamese_3():\n \n     graph.get_config(verbose=1)\n \n+\n def test_siamese_4():\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n@@ -678,6 +680,7 @@ def test_siamese_4():\n \n     graph.get_config(verbose=1)\n \n+\n def test_siamese_5():\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n@@ -706,6 +709,7 @@ def test_siamese_5():\n \n     graph.get_config(verbose=1)\n \n+\n def test_2o_1i_weights():\n     # test a non-sequential graph with 1 input and 2 outputs\n     graph = Graph()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2e29ef31a7d61a84720a437801d2035b61d264fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 55 | Churn Cumulative: 55 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,55 @@\n+import numpy as np\n+from numpy.testing import assert_allclose\n+\n+import pytest\n+\n+from keras.preprocessing.sequence import (pad_sequences, make_sampling_table,\n+                                          skipgrams)\n+\n+\n+def test_pad_sequences():\n+    a = [[1], [1, 2], [1, 2, 3]]\n+\n+    # test padding\n+    b = pad_sequences(a, maxlen=3, padding='pre')\n+    assert_allclose(b, [[0, 0, 1], [0, 1, 2], [1, 2, 3]])\n+    b = pad_sequences(a, maxlen=3, padding='post')\n+    assert_allclose(b, [[1, 0, 0], [1, 2, 0], [1, 2, 3]])\n+\n+    # test truncating\n+    b = pad_sequences(a, maxlen=2, truncating='pre')\n+    assert_allclose(b, [[0, 1], [1, 2], [2, 3]])\n+    b = pad_sequences(a, maxlen=2, truncating='post')\n+    assert_allclose(b, [[0, 1], [1, 2], [1, 2]])\n+\n+    # test value\n+    b = pad_sequences(a, maxlen=3, value=1)\n+    assert_allclose(b, [[1, 1, 1], [1, 1, 2], [1, 2, 3]])\n+\n+\n+def test_make_sampling_table():\n+    a = make_sampling_table(3)\n+    assert_allclose(a, np.asarray([0.00315225,  0.00315225,  0.00547597]),\n+                    rtol=.1)\n+\n+\n+def test_skipgrams():\n+    # test with no window size and binary labels\n+    couples, labels = skipgrams(np.arange(3), vocabulary_size=3)\n+    for couple in couples:\n+        assert couple[0] in [0, 1, 2] and couple[1] in [0, 1, 2]\n+\n+    # test window size and categorical labels\n+    couples, labels = skipgrams(np.arange(5), vocabulary_size=5, window_size=1,\n+                                categorical=True)\n+    for couple in couples:\n+        assert couple[0] - couple[1] < 3\n+    for l in labels:\n+        assert len(l) == 2\n+\n+    # test categorical laels\n+\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#54d3b9e67355be8eb41aab22ba6f481b7d07c65c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 25 | Churn Cumulative: 50 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,25 +0,0 @@\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras.preprocessing.sequences import (pad_sequences, make_sampling_table,\n-                                           skipgrams)\n-\n-\n-def test_pad_sequences():\n-    a = [np.arange(i) for i in np.arange(2, 5, 1)]\n-    b = pad_sequences(a, maxlen=3)\n-    assert_allclose(b, [[0, 0, 1], [0, 1, 2], [1, 2, 3]])\n-\n-\n-def test_make_sampling_table():\n-    a = make_sampling_table(3)\n-    assert_allclose(a, np.asarray([0.00315225,  0.00315225,  0.00547597]),\n-                    rtol=.1)\n-\n-\n-def test_skipgrams():\n-    couples, labels = skipgrams(np.arange(3), 3)\n-    for couple in couples:\n-        assert couple[0] in [0, 1, 2] and couple[1] in [0, 1, 2]\n-\n-    assert 0 in labels and 1 in labels\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e27587334b2327cfd145c8d0957ac896e54427cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 58 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -47,9 +47,6 @@ def test_skipgrams():\n     for l in labels:\n         assert len(l) == 2\n \n-    # test categorical laels\n-\n-\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#42b3d37a54545882699283b5764cf3c997f8d9cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 60 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ def test_skipgrams():\n     couples, labels = skipgrams(np.arange(5), vocabulary_size=5, window_size=1,\n                                 categorical=True)\n     for couple in couples:\n-        assert couple[0] - couple[1] < 3\n+        assert couple[0] - couple[1] <= 3\n     for l in labels:\n         assert len(l) == 2\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f754c79f10190b7fef105bae39c43e140cd41ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d04cac6526171af608baf38f68f9767af62b0555", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2667 | Contributors (this commit): 13 | Commits (past 90d): 25 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -326,7 +326,7 @@ class GRU(Recurrent):\n         z = self.inner_activation(x_z + K.dot(h_tm1, self.U_z))\n         r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n \n-        hh = self.inner_activation(x_h + K.dot(r * h_tm1, self.U_h))\n+        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n         h = z * h_tm1 + (1 - z) * hh\n         return h, [h]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#69cabdf6bc501c1f3c58fe71ce9a93c92bfd3d13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 128 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 128 | Churn Cumulative: 6996 | Contributors (this commit): 32 | Commits (past 90d): 112 | Contributors (cumulative): 32 | DMM Complexity: 0.2028985507246377\n\nDIFF:\n@@ -1815,131 +1815,3 @@ class Highway(Layer):\n         base_config = super(Highway, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n-\n-class TimeDistributedHighway(MaskedLayer):\n-    '''Apply a same Highway layer for each dimension[1] (time_dimension) input.\n-    Especially useful after a recurrent network with 'return_sequence=True'.\n-\n-    # Input shape\n-        3D tensor with shape `(nb_sample, time_dimension, input_dim)`.\n-\n-    # Output shape\n-        3D tensor with shape `(nb_sample, time_dimension, input_dim)`.\n-\n-    # Arguments\n-        init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)),\n-            or alternatively, Theano function to use for weights\n-            initialization. This parameter is only relevant\n-            if you don't pass a `weights` argument.\n-        transform_bias: value for the bias to take on initially (default -2)\n-        activation: name of activation function to use\n-            (see [activations](../activations.md)),\n-            or alternatively, elementwise Theano function.\n-            If you don't specify anything, no activation is applied\n-            (ie. \"linear\" activation: a(x) = x).\n-        weights: list of numpy arrays to set as initial weights.\n-            The list should have 1 element, of shape `(input_dim, output_dim)`.\n-        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n-            (eg. L1 or L2 regularization), applied to the main weights matrix.\n-        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n-            applied to the bias.\n-        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n-            applied to the network output.\n-        W_constraint: instance of the [constraints](../constraints.md) module\n-            (eg. maxnorm, nonneg), applied to the main weights matrix.\n-        b_constraint: instance of the [constraints](../constraints.md) module,\n-            applied to the bias.\n-        input_dim: dimensionality of the input (integer).\n-            This argument (or alternatively, the keyword argument `input_shape`)\n-            is required when using this layer as the first layer in a model.\n-    '''\n-    input_ndim = 3\n-\n-    def __init__(self, \n-                 init='glorot_uniform', transform_bias=-1, activation='linear', weights=None,\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None,\n-                 input_dim=None, input_length=None, **kwargs):\n-        self.init = initializations.get(init)\n-        self.transform_bias = transform_bias\n-        self.activation = activations.get(activation)\n-        \n-        self.W_regularizer = regularizers.get(W_regularizer)\n-        self.b_regularizer = regularizers.get(b_regularizer)\n-        self.activity_regularizer = regularizers.get(activity_regularizer)\n-\n-        self.W_constraint = constraints.get(W_constraint)\n-        self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n-\n-        self.initial_weights = weights\n-\n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        self.input = K.placeholder(ndim=3)\n-        super(TimeDistributedHighway, self).__init__(**kwargs)\n-\n-    def build(self):\n-        input_dim = self.input_shape[2]\n-\n-        self.W = self.init((input_dim, input_dim))\n-        self.b = K.zeros((input_dim))\n-\n-        self.W_carry = self.init((input_dim, input_dim))\n-        self.b_carry = K.variable(np.ones((input_dim)) * self.transform_bias)\n-\n-        self.params = [self.W, self.b, self.W_carry, self.b_carry]\n-        self.regularizers = []\n-\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-\n-    @property\n-    def output_shape(self):\n-        input_shape = self.input_shape\n-        return (input_shape[0], input_shape[1], input_shape[2])\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-\n-        def step(x, states):\n-            output = self.activation(K.dot(x, self.W) + self.b)\n-            transform_weight = activations.sigmoid(K.dot(x, self.W_carry) + self.b_carry)\n-            output *= transform_weight\n-            output = output + (1 - transform_weight) * x\n-            return output, []\n-\n-        last_output, outputs, states = K.rnn(step, X, [], masking=False)\n-        return outputs\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"init\": self.init.__name__,\n-                  \"transform_bias\": self.transform_bias,\n-                  \"activation\": self.activation.__name__,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n-        base_config = super(TimeDistributedHighway, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ec663aeda11910be5c334a207faa5308d1ea8871", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 468 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -100,11 +100,6 @@ def test_time_dist_merge():\n     _runner(layer)\n \n \n-def test_time_dist_highway():\n-    layer = core.TimeDistributedHighway(input_shape=(None, 10))\n-    _runner(layer)\n-\n-\n def test_highway():\n     layer = core.Highway(input_shape=(10,))\n     _runner(layer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#490ba423c4fd2f5417b4f18e088402587c936784", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 957 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,7 @@ from keras.utils import np_utils\n from keras.utils.test_utils import get_test_data\n \n import os\n-from keras.utils.layer_utils import model_summary\n+\n \n input_dim = 32\n nb_hidden = 16\n@@ -20,6 +20,10 @@ nb_class = 4\n batch_size = 32\n nb_epoch = 1\n \n+\n+def _get_test_data():\n+    np.random.seed(1234)\n+\n     train_samples = 2000\n     test_samples = 500\n \n@@ -30,6 +34,7 @@ test_samples = 500\n                                                          nb_class=4)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n+    return (X_train, y_train), (X_test, y_test)\n \n \n ####################\n@@ -38,6 +43,8 @@ y_train = np_utils.to_categorical(y_train)\n \n \n def test_sequential():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+\n     model = Sequential()\n     model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n@@ -87,6 +94,7 @@ def test_sequential():\n \n \n def test_merge_sum():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -140,6 +148,8 @@ def test_merge_sum():\n @pytest.mark.skipif(K._BACKEND == 'tensorflow',\n                     reason='currently not working with TensorFlow')\n def test_merge_dot():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+\n     left = Sequential()\n     left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n     left.add(Activation('relu'))\n@@ -172,6 +182,8 @@ def test_merge_dot():\n \n \n def test_merge_concat():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -226,6 +238,7 @@ def test_merge_concat():\n \n \n def test_merge_recursivity():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -274,6 +287,7 @@ def test_merge_recursivity():\n \n \n def test_merge_overlap():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -310,6 +324,7 @@ def test_merge_overlap():\n \n \n def test_lambda():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n     def func(X):\n         s = X[0]\n         for i in range(1, len(X)):\n@@ -395,6 +410,7 @@ def test_sequential_count_params():\n \n \n def test_siamese_1():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -449,6 +465,7 @@ def test_siamese_1():\n \n \n def test_siamese_2():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#827a6323b0d68c804f3b64498f2c42bf63f1d60f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 7 | Churn Cumulative: 7003 | Contributors (this commit): 32 | Commits (past 90d): 113 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1726,6 +1726,7 @@ def add_shared_layer(layer, inputs):\n         inputs[i].add(s)\n         inputs[i].add(sh)\n \n+\n class Highway(Layer):\n     '''Densely connected highway network,\n     a natural extension of LSTMs to feedforward networks\n@@ -1768,7 +1769,8 @@ class Highway(Layer):\n     '''\n     input_ndim = 2\n \n-    def __init__(self, init='glorot_uniform', transform_bias=-2, activation='linear', weights=None,\n+    def __init__(self, init='glorot_uniform', transform_bias=-2,\n+                 activation='linear', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None, input_dim=None, **kwargs):\n         self.init = initializations.get(init)\n@@ -1798,7 +1800,7 @@ class Highway(Layer):\n         self.W_carry = self.init((input_dim, input_dim))\n \n         self.b = K.zeros((input_dim,))\n-        # -- initialize with a vector of values `transform_bias`\n+        # initialize with a vector of values `transform_bias`\n         self.b_carry = K.variable(np.ones((input_dim,)) * self.transform_bias)\n \n         self.params = [self.W, self.b, self.W_carry, self.b_carry]\n@@ -1845,4 +1847,3 @@ class Highway(Layer):\n                   \"input_dim\": self.input_dim}\n         base_config = super(Highway, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c914f793b868d5ab1534b61a0358b195686a35d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 260 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -250,4 +250,7 @@ for module, module_name in MODULES:\n         print('...inserting autogenerated content into template:', path)\n     else:\n         print('...creating new page with autogenerated content:', path)\n+    subdir = os.path.dirname(path)\n+    if not os.path.exists(subdir):\n+        os.makedirs(subdir)\n     open(path, 'w').write(module_page)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ed92c141851f85eace4ebc2eb3c69b9a11706a30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 50 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -31,6 +31,11 @@ else:\n         # add new line in order for bash 'cat' display the content correctly\n         f.write(json.dumps(_config) + '\\n')\n \n+if 'KERAS_BACKEND' in os.environ:\n+    _backend = os.environ['KERAS_BACKEND']\n+    assert _backend in {'theano', 'tensorflow'}\n+    _BACKEND = _backend\n+\n if _BACKEND == 'theano':\n     print('Using Theano backend.')\n     from .theano_backend import *\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#554ed5bfc80f126595cfa0fa76b9c04d55f1d161", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 41 | Churn Cumulative: 1931 | Contributors (this commit): 6 | Commits (past 90d): 42 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -235,6 +235,19 @@ def permute_dimensions(x, pattern):\n     '''\n     return tf.transpose(x, perm=pattern)\n \n+def repeat_elements(x, rep, axis):\n+    '''Repeats the elements of a tensor along an axis, like np.repeat\n+\n+    If x has shape (s1, s2, s3) and axis=1, the output\n+    will have shape (s1, s2 * rep, s3)\n+    '''\n+    x_shape = x.get_shape().as_list()\n+    # slices along the repeat axis\n+    splits = tf.split(axis, x_shape[axis], x)\n+    # repeat each slice the given number of reps\n+    x_rep = [s for s in splits for i in xrange(rep)]\n+    return tf.concat(axis, x_rep)\n+\n \n def repeat(x, n):\n     '''Repeat a 2D tensor:\n\n@@ -243,6 +243,14 @@ def permute_dimensions(x, pattern):\n     return x.dimshuffle(pattern)\n \n \n+def repeat_elements(x, rep, axis):\n+    '''Repeats the elements of a tensor along an axis, like np.repeat\n+\n+    If x has shape (s1, s2, s3) and axis=1, the output\n+    will have shape (s1, s2 * rep, s3)\n+    '''\n+    return T.repeat(x, rep, axis=axis)\n+\n def repeat(x, n):\n     '''Repeat a 2D tensor:\n \n\n@@ -64,6 +64,26 @@ class TestBackend(object):\n         check_single_tensor_operation('expand_dims', (4, 3, 2), dim=1)\n         check_single_tensor_operation('squeeze', (4, 3, 1), axis=2)\n \n+    def test_repeat_elements(self):\n+        reps = 3\n+        for ndims in [1, 2, 3]:\n+            shape = np.arange(2, 2+ndims)\n+            arr = np.arange(np.prod(shape)).reshape(shape)\n+            arr_th = KTH.variable(arr)\n+            arr_tf = KTF.variable(arr)\n+\n+            for rep_axis in xrange(ndims):\n+                np_rep = np.repeat(arr, reps, axis=rep_axis)\n+                th_rep = KTH.eval(\n+                    KTH.repeat_elements(arr_th, reps, axis=rep_axis))\n+                tf_rep = KTF.eval(\n+                    KTF.repeat_elements(arr_tf, reps, axis=rep_axis))\n+\n+                assert th_rep.shape == np_rep.shape\n+                assert tf_rep.shape == np_rep.shape\n+                assert_allclose(np_rep, th_rep, atol=1e-05)\n+                assert_allclose(np_rep, tf_rep, atol=1e-05)\n+\n     def test_value_manipulation(self):\n         val = np.random.random((4, 2))\n         xth = KTH.variable(val)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8715c70a7413dd9fc0be1f7581cc53c7c913f694", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 2355 | Contributors (this commit): 14 | Commits (past 90d): 35 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -609,7 +609,7 @@ class UpSampling1D(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        output = K.concatenate([X] * self.length, axis=1)\n+        output = K.repeat_elements(X, self.length, axis=1)\n         return output\n \n     def get_config(self):\n@@ -669,11 +669,11 @@ class UpSampling2D(Layer):\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         if self.dim_ordering == 'th':\n-            output = K.concatenate([X] * self.size[0], axis=2)\n-            output = K.concatenate([output] * self.size[1], axis=3)\n+            output = K.repeat_elements(X, self.size[0], axis=2)\n+            output = K.repeat_elements(output, self.size[1], axis=3)\n         elif self.dim_ordering == 'tf':\n-            output = K.concatenate([X] * self.size[0], axis=1)\n-            output = K.concatenate([output] * self.size[1], axis=2)\n+            output = K.repeat_elements(X, self.size[0], axis=1)\n+            output = K.repeat_elements(output, self.size[1], axis=2)\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n         return output\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50d3fddead17b0fb8ffe315a2b3a585d7bcb3055", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 225 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -120,7 +120,7 @@ class ModelToDot(object):\n         self.g = pydot.Dot()\n         self.g.set('rankdir', 'TB')\n         self.g.set('concentrate', True)\n-        self.g.set_node_defaults(shape='record', fontname=\"Fira Mono\")\n+        self.g.set_node_defaults(shape='record')\n \n         if hasattr(model, 'outputs'):\n             # Graph\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d685f44474286a7a05e46dde333488a520f07aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1123 | Contributors (this commit): 4 | Commits (past 90d): 26 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -245,7 +245,7 @@ def repeat_elements(x, rep, axis):\n     # slices along the repeat axis\n     splits = tf.split(axis, x_shape[axis], x)\n     # repeat each slice the given number of reps\n-    x_rep = [s for s in splits for i in xrange(rep)]\n+    x_rep = [s for s in splits for i in range(rep)]\n     return tf.concat(axis, x_rep)\n \n \n\n@@ -72,7 +72,7 @@ class TestBackend(object):\n             arr_th = KTH.variable(arr)\n             arr_tf = KTF.variable(arr)\n \n-            for rep_axis in xrange(ndims):\n+            for rep_axis in range(ndims):\n                 np_rep = np.repeat(arr, reps, axis=rep_axis)\n                 th_rep = KTH.eval(\n                     KTH.repeat_elements(arr_th, reps, axis=rep_axis))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2a7ccd1cc5b4a3b3a1a0f920c8a29073d32ce42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 718 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -287,9 +287,6 @@ def squeeze(x, axis):\n def temporal_padding(x, padding=1):\n     '''Pad the middle dimension of a 3D tensor\n     with \"padding\" zeros left and right.\n-\n-    Appologies for the inane API, but Theano makes this\n-    really hard.\n     '''\n     pattern = [[0, 0], [padding, padding], [0, 0]]\n     return tf.pad(x, pattern)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58fb2b8af56c76c332cc7dfeada7496687d65163", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 373 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,9 @@ from .. import backend as K\n \n \n class BatchNormalization(Layer):\n-    '''Normalize the activations of the previous layer at each batch.\n+    '''Normalize the activations of the previous layer at each batch,\n+    i.e. applies a transformation that maintains the mean activation\n+    close to 0. and the activation standard deviation close to 1.\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n@@ -18,7 +20,13 @@ class BatchNormalization(Layer):\n         epsilon: small float > 0. Fuzz parameter.\n         mode: integer, 0 or 1.\n             - 0: feature-wise normalization.\n-            - 1: sample-wise normalization.\n+                If the input has multiple feature dimensions,\n+                each will be normalized separately\n+                (e.g. for an image input with shape\n+                `(channels, rows, cols)`,\n+                each combination of a channel, row and column\n+                will be normalized separately).\n+            - 1: sample-wise normalization. This mode assumes a 2D input.\n         momentum: momentum in the computation of the\n             exponential average of the mean and standard deviation\n             of the data, for feature-wise normalization.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5255b5df548860362de29dec9a8c91ae1a7ec4a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 46 | Files Changed: 1 | Hunks: 17 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 97 | Churn Cumulative: 7100 | Contributors (this commit): 32 | Commits (past 90d): 114 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,7 +38,7 @@ class Layer(object):\n                           'batch_input_shape',\n                           'cache_enabled'}\n         for kwarg in kwargs:\n-            assert kwarg in allowed_kwargs, \"Keyword argument not understood: \" + kwarg\n+            assert kwarg in allowed_kwargs, 'Keyword argument not understood: ' + kwarg\n         if 'input_shape' in kwargs:\n             self.set_input_shape((None,) + tuple(kwargs['input_shape']))\n         if 'batch_input_shape' in kwargs:\n@@ -77,12 +77,14 @@ class Layer(object):\n     def set_previous(self, layer, connection_map={}):\n         '''Connect a layer to its parent in the computational graph.\n         '''\n-        assert self.nb_input == layer.nb_output == 1, \"Cannot connect layers: input count and output count should be 1.\"\n+        assert self.nb_input == layer.nb_output == 1, 'Cannot connect layers: input count and output count should be 1.'\n         if hasattr(self, 'input_ndim'):\n-            assert self.input_ndim == len(layer.output_shape), \"Incompatible shapes: layer expected input with ndim=\" +\\\n-                str(self.input_ndim) + \" but previous layer has output_shape \" + str(layer.output_shape)\n+            assert self.input_ndim == len(layer.output_shape), ('Incompatible shapes: layer expected input with ndim=' +\n+                                                                str(self.input_ndim) +\n+                                                                ' but previous layer has output_shape ' +\n+                                                                str(layer.output_shape))\n         if layer.get_output_mask() is not None:\n-            assert self.supports_masked_input(), \"Cannot connect non-masking layer to layer with masked output\"\n+            assert self.supports_masked_input(), 'Cannot connect non-masking layer to layer with masked output'\n         self.previous = layer\n         self.build()\n \n@@ -206,11 +208,12 @@ class Layer(object):\n             of the layer (i.e. it should match the\n             output of `get_weights`).\n         '''\n-        assert len(self.params) == len(weights), 'Provided weight array does not match layer weights (' + \\\n-            str(len(self.params)) + ' layer params vs. ' + str(len(weights)) + ' provided weights)'\n+        assert len(self.params) == len(weights), ('Provided weight array does not match layer weights (' +\n+                                                  str(len(self.params)) + ' layer params vs. ' +\n+                                                  str(len(weights)) + ' provided weights)')\n         for p, w in zip(self.params, weights):\n             if K.get_value(p).shape != w.shape:\n-                raise Exception(\"Layer shape %s not compatible with weight shape %s.\" % (K.get_value(p).shape, w.shape))\n+                raise Exception('Layer shape %s not compatible with weight shape %s.' % (K.get_value(p).shape, w.shape))\n             K.set_value(p, w)\n \n     def get_weights(self):\n@@ -225,7 +228,7 @@ class Layer(object):\n     def get_config(self):\n         '''Return the parameters of the layer, as a dictionary.\n         '''\n-        config = {\"name\": self.__class__.__name__}\n+        config = {'name': self.__class__.__name__}\n         if hasattr(self, '_input_shape'):\n             config['input_shape'] = self._input_shape[1:]\n         if hasattr(self, '_trainable'):\n@@ -304,8 +307,8 @@ class Masking(MaskedLayer):\n         self.input = K.placeholder(ndim=3)\n \n     def get_output_mask(self, train=False):\n-        if K._BACKEND == \"tensorflow\":\n-            raise Exception(\"Masking is Theano-only for the time being.\")\n+        if K._BACKEND == 'tensorflow':\n+            raise Exception('Masking is Theano-only for the time being.')\n         X = self.get_input(train)\n         return K.any(K.ones_like(X) * (1. - K.equal(X, self.mask_value)),\n                      axis=-1)\n@@ -316,8 +319,8 @@ class Masking(MaskedLayer):\n                          axis=-1, keepdims=True)\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"mask_value\": self.mask_value}\n+        config = {'name': self.__class__.__name__,\n+                  'mask_value': self.mask_value}\n         base_config = super(Masking, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -363,8 +366,8 @@ class TimeDistributedMerge(Layer):\n             raise Exception('Unknown merge mode')\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"mode\": self.mode}\n+        config = {'name': self.__class__.__name__,\n+                  'mode': self.mode}\n         base_config = super(TimeDistributedMerge, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -792,8 +795,8 @@ class RepeatVector(Layer):\n         return K.repeat(X, self.n)\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"n\": self.n}\n+        config = {'name': self.__class__.__name__,\n+                  'n': self.n}\n         base_config = super(RepeatVector, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -938,9 +941,9 @@ class ActivityRegularization(Layer):\n         return self.get_input(train)\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"l1\": self.l1,\n-                  \"l2\": self.l2}\n+        config = {'name': self.__class__.__name__,\n+                  'l1': self.l1,\n+                  'l2': self.l2}\n         base_config = super(ActivityRegularization, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1053,17 +1056,17 @@ class TimeDistributedDense(MaskedLayer):\n         return outputs\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n+        config = {'name': self.__class__.__name__,\n+                  'output_dim': self.output_dim,\n+                  'init': self.init.__name__,\n+                  'activation': self.activation.__name__,\n+                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n+                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'input_dim': self.input_dim,\n+                  'input_length': self.input_length}\n         base_config = super(TimeDistributedDense, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1168,10 +1171,10 @@ class AutoEncoder(Layer):\n         return self.decoder.get_output(train)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"encoder_config\": self.encoder.get_config(),\n-                \"decoder_config\": self.decoder.get_config(),\n-                \"output_reconstruction\": self.output_reconstruction}\n+        return {'name': self.__class__.__name__,\n+                'encoder_config': self.encoder.get_config(),\n+                'decoder_config': self.decoder.get_config(),\n+                'output_reconstruction': self.output_reconstruction}\n \n \n class MaxoutDense(Layer):\n@@ -1295,6 +1298,8 @@ class Lambda(Layer):\n         if py3:\n             self.function = marshal.dumps(function.__code__)\n         else:\n+            assert hasattr(function, 'func_code'), ('The Lambda layer \"function\"'\n+                                                    ' argument must be a Python function.')\n             self.function = marshal.dumps(function.func_code)\n         if output_shape is None:\n             self._output_shape = None\n@@ -1318,7 +1323,7 @@ class Lambda(Layer):\n             output_shape_func = types.FunctionType(output_shape_func, globals())\n             shape = output_shape_func(self.previous.output_shape)\n             if type(shape) not in {list, tuple}:\n-                raise Exception(\"output_shape function must return a tuple\")\n+                raise Exception('output_shape function must return a tuple')\n             return tuple(shape)\n \n     def get_output(self, train=False):\n@@ -1835,15 +1840,15 @@ class Highway(Layer):\n         return output\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"init\": self.init.__name__,\n-                  \"transform_bias\": self.transform_bias,\n-                  \"activation\": self.activation.__name__,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n-                  \"input_dim\": self.input_dim}\n+        config = {'name': self.__class__.__name__,\n+                  'init': self.init.__name__,\n+                  'transform_bias': self.transform_bias,\n+                  'activation': self.activation.__name__,\n+                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n+                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'input_dim': self.input_dim}\n         base_config = super(Highway, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
