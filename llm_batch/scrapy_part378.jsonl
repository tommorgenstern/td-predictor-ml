{"custom_id": "scrapy#aa025d7eacb461ccb0c724584532d402bb400bd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 739 | Contributors (this commit): 19 | Commits (past 90d): 3 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class Selector(_ParselSelector, object_ref):\n     ``response`` isn't available. Using ``text`` and ``response`` together is\n     undefined behavior.\n \n-    ``type`` defines the selector type, it can be ``\"html\"``, ``\"xml\"``\n+    ``type`` defines the selector type, it can be ``\"html\"``, ``\"xml\"``, ``\"json\"``\n     or ``None`` (default).\n \n     If ``type`` is ``None``, the selector automatically chooses the best type\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2cba7896d26dda51ff2e598300363531ebd328b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1079 | Contributors (this commit): 17 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -143,7 +143,7 @@ class Downloader:\n \n         return key, self.slots[key]\n \n-    def _get_slot_key(self, request: Request, spider: Any) -> str:\n+    def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n         if self.DOWNLOAD_SLOT in request.meta:\n             return cast(str, request.meta[self.DOWNLOAD_SLOT])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c4d2748ff572adad8150f1bc09b2d52e61d9dfc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 3702 | Contributors (this commit): 42 | Commits (past 90d): 9 | Contributors (cumulative): 68 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,7 @@ from scrapy.core.downloader.tls import (\n     ScrapyClientTLSOptions,\n     openssl_methods,\n )\n+from scrapy.crawler import Crawler\n from scrapy.settings import BaseSettings\n from scrapy.utils.misc import build_from_crawler, load_object\n \n@@ -102,7 +103,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n     # kept for old-style HTTP/1.0 downloader context twisted calls,\n     # e.g. connectSSL()\n     def getContext(self, hostname: Any = None, port: Any = None) -> SSL.Context:\n-        ctx = self.getCertificateOptions().getContext()\n+        ctx: SSL.Context = self.getCertificateOptions().getContext()\n         ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT\n         return ctx\n \n@@ -165,7 +166,9 @@ class AcceptableProtocolsContextFactory:\n         return options\n \n \n-def load_context_factory_from_settings(settings, crawler):\n+def load_context_factory_from_settings(\n+    settings: BaseSettings, crawler: Crawler\n+) -> IPolicyForHTTPS:\n     ssl_method = openssl_methods[settings.get(\"DOWNLOADER_CLIENT_TLS_METHOD\")]\n     context_factory_cls = load_object(settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"])\n     # try method-aware context factory\n\n@@ -16,7 +16,7 @@ if TYPE_CHECKING:\n class HTTP10DownloadHandler:\n     lazy = False\n \n-    def __init__(self, settings, crawler=None):\n+    def __init__(self, settings, crawler):\n         self.HTTPClientFactory = load_object(settings[\"DOWNLOADER_HTTPCLIENTFACTORY\"])\n         self.ClientContextFactory = load_object(\n             settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"]\n\n@@ -46,7 +46,7 @@ logger = logging.getLogger(__name__)\n class HTTP11DownloadHandler:\n     lazy = False\n \n-    def __init__(self, settings, crawler=None):\n+    def __init__(self, settings, crawler):\n         self._crawler = crawler\n \n         from twisted.internet import reactor\n\n@@ -24,7 +24,7 @@ if TYPE_CHECKING:\n \n \n class H2DownloadHandler:\n-    def __init__(self, settings: Settings, crawler: Optional[Crawler] = None):\n+    def __init__(self, settings: Settings, crawler: Crawler):\n         self._crawler = crawler\n \n         from twisted.internet import reactor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec4d40702227f5486c184de9439da4896121a33b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 17 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 3558 | Contributors (this commit): 45 | Commits (past 90d): 9 | Contributors (cumulative): 69 | DMM Complexity: 0.5\n\nDIFF:\n@@ -2,6 +2,8 @@ from pathlib import Path\n \n from w3lib.url import file_uri_to_path\n \n+from scrapy import Request, Spider\n+from scrapy.http import Response\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.decorators import defers\n \n@@ -10,7 +12,7 @@ class FileDownloadHandler:\n     lazy = False\n \n     @defers\n-    def download_request(self, request, spider):\n+    def download_request(self, request: Request, spider: Spider) -> Response:\n         filepath = file_uri_to_path(request.url)\n         body = Path(filepath).read_bytes()\n         respcls = responsetypes.from_args(filename=filepath, body=body)\n\n@@ -27,11 +27,11 @@ from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer\n from zope.interface import implementer\n \n-from scrapy import signals\n+from scrapy import Request, Spider, signals\n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n from scrapy.core.downloader.webclient import _parse\n from scrapy.exceptions import StopDownload\n-from scrapy.http import Headers\n+from scrapy.http import Headers, Response\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.python import to_bytes, to_unicode\n \n@@ -67,7 +67,7 @@ class HTTP11DownloadHandler:\n     def from_crawler(cls, crawler) -> Self:\n         return cls(crawler.settings, crawler)\n \n-    def download_request(self, request, spider):\n+    def download_request(self, request: Request, spider: Spider) -> Response:\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n         agent = ScrapyAgent(\n             contextFactory=self._contextFactory,\n\n@@ -8,6 +8,7 @@ from twisted.internet.base import DelayedCall\n from twisted.internet.defer import Deferred\n from twisted.internet.error import TimeoutError\n from twisted.web.client import URI\n+from twisted.web.iweb import IPolicyForHTTPS\n \n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n from scrapy.core.downloader.webclient import _parse\n@@ -54,7 +55,7 @@ class ScrapyH2Agent:\n \n     def __init__(\n         self,\n-        context_factory,\n+        context_factory: IPolicyForHTTPS,\n         pool: H2ConnectionPool,\n         connect_timeout: int = 10,\n         bind_address: Optional[bytes] = None,\n\n@@ -1,9 +1,13 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, Any, Optional, Type\n \n+from scrapy import Request, Spider\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n+from scrapy.http import Response\n+from scrapy.settings import BaseSettings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import build_from_crawler\n@@ -16,14 +20,14 @@ if TYPE_CHECKING:\n class S3DownloadHandler:\n     def __init__(\n         self,\n-        settings,\n+        settings: BaseSettings,\n         *,\n-        crawler=None,\n-        aws_access_key_id=None,\n-        aws_secret_access_key=None,\n-        aws_session_token=None,\n-        httpdownloadhandler=HTTPDownloadHandler,\n-        **kw,\n+        crawler: Crawler,\n+        aws_access_key_id: Optional[str] = None,\n+        aws_secret_access_key: Optional[str] = None,\n+        aws_session_token: Optional[str] = None,\n+        httpdownloadhandler: Type[HTTPDownloadHandler] = HTTPDownloadHandler,\n+        **kw: Any,\n     ):\n         if not is_botocore_available():\n             raise NotConfigured(\"missing botocore library\")\n@@ -51,6 +55,8 @@ class S3DownloadHandler:\n         if kw:\n             raise TypeError(f\"Unexpected keyword arguments: {kw}\")\n         if not self.anon:\n+            assert aws_access_key_id is not None\n+            assert aws_secret_access_key is not None\n             SignerCls = botocore.auth.AUTH_TYPE_MAPS[\"s3\"]\n             self._signer = SignerCls(\n                 botocore.credentials.Credentials(\n@@ -65,10 +71,10 @@ class S3DownloadHandler:\n         self._download_http = _http_handler.download_request\n \n     @classmethod\n-    def from_crawler(cls, crawler, **kwargs) -> Self:\n+    def from_crawler(cls, crawler: Crawler, **kwargs: Any) -> Self:\n         return cls(crawler.settings, crawler=crawler, **kwargs)\n \n-    def download_request(self, request, spider):\n+    def download_request(self, request: Request, spider: Spider) -> Response:\n         p = urlparse_cached(request)\n         scheme = \"https\" if request.meta.get(\"is_secure\") else \"http\"\n         bucket = p.hostname\n@@ -85,6 +91,7 @@ class S3DownloadHandler:\n                 headers=request.headers.to_unicode_dict(),\n                 data=request.body,\n             )\n+            assert self._signer\n             self._signer.add_auth(awsrequest)\n             request = request.replace(url=url, headers=awsrequest.headers.items())\n         return self._download_http(request, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8e13ebb78dd1d2db425b285335f31dba7c1fd39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 18 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 295 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,15 +32,19 @@ from __future__ import annotations\n \n import re\n from io import BytesIO\n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, Any, BinaryIO, Dict, Optional\n from urllib.parse import unquote\n \n+from twisted.internet.defer import Deferred\n from twisted.internet.protocol import ClientCreator, Protocol\n from twisted.protocols.ftp import CommandFailed, FTPClient\n+from twisted.python.failure import Failure\n \n+from scrapy import Request, Spider\n from scrapy.crawler import Crawler\n from scrapy.http import Response\n from scrapy.responsetypes import responsetypes\n+from scrapy.settings import BaseSettings\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes\n \n@@ -50,20 +54,20 @@ if TYPE_CHECKING:\n \n \n class ReceivedDataProtocol(Protocol):\n-    def __init__(self, filename=None):\n-        self.__filename = filename\n-        self.body = open(filename, \"wb\") if filename else BytesIO()\n-        self.size = 0\n+    def __init__(self, filename: Optional[str] = None):\n+        self.__filename: Optional[str] = filename\n+        self.body: BinaryIO = open(filename, \"wb\") if filename else BytesIO()\n+        self.size: int = 0\n \n-    def dataReceived(self, data):\n+    def dataReceived(self, data: bytes) -> None:\n         self.body.write(data)\n         self.size += len(data)\n \n     @property\n-    def filename(self):\n+    def filename(self) -> Optional[str]:\n         return self.__filename\n \n-    def close(self):\n+    def close(self) -> None:\n         self.body.close() if self.filename else self.body.seek(0)\n \n \n@@ -73,12 +77,12 @@ _CODE_RE = re.compile(r\"\\d+\")\n class FTPDownloadHandler:\n     lazy = False\n \n-    CODE_MAPPING = {\n+    CODE_MAPPING: Dict[str, int] = {\n         \"550\": 404,\n         \"default\": 503,\n     }\n \n-    def __init__(self, settings):\n+    def __init__(self, settings: BaseSettings):\n         self.default_user = settings[\"FTP_USER\"]\n         self.default_password = settings[\"FTP_PASSWORD\"]\n         self.passive_mode = settings[\"FTP_PASSIVE_MODE\"]\n@@ -87,7 +91,7 @@ class FTPDownloadHandler:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings)\n \n-    def download_request(self, request, spider):\n+    def download_request(self, request: Request, spider: Spider) -> Deferred:\n         from twisted.internet import reactor\n \n         parsed_url = urlparse_cached(request)\n@@ -99,10 +103,10 @@ class FTPDownloadHandler:\n         creator = ClientCreator(\n             reactor, FTPClient, user, password, passive=passive_mode\n         )\n-        dfd = creator.connectTCP(parsed_url.hostname, parsed_url.port or 21)\n+        dfd: Deferred = creator.connectTCP(parsed_url.hostname, parsed_url.port or 21)\n         return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))\n \n-    def gotClient(self, client, request, filepath):\n+    def gotClient(self, client: FTPClient, request: Request, filepath: str) -> Deferred:\n         self.client = client\n         protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n         return client.retrieveFile(filepath, protocol).addCallbacks(\n@@ -112,15 +116,18 @@ class FTPDownloadHandler:\n             errbackArgs=(request,),\n         )\n \n-    def _build_response(self, result, request, protocol):\n+    def _build_response(\n+        self, result: Any, request: Request, protocol: ReceivedDataProtocol\n+    ) -> Response:\n         self.result = result\n         protocol.close()\n         headers = {\"local filename\": protocol.filename or \"\", \"size\": protocol.size}\n         body = to_bytes(protocol.filename or protocol.body.read())\n         respcls = responsetypes.from_args(url=request.url, body=body)\n-        return respcls(url=request.url, status=200, body=body, headers=headers)\n+        # hints for Headers-related types may need to be fixed to not use AnyStr\n+        return respcls(url=request.url, status=200, body=body, headers=headers)  # type: ignore[arg-type]\n \n-    def _failed(self, result, request):\n+    def _failed(self, result: Failure, request: Request) -> Response:\n         message = result.getErrorMessage()\n         if result.type == CommandFailed:\n             m = _CODE_RE.search(message)\n@@ -130,4 +137,5 @@ class FTPDownloadHandler:\n                 return Response(\n                     url=request.url, status=httpcode, body=to_bytes(message)\n                 )\n+        assert result.type\n         raise result.type(result.value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af3e38ab1f3dc7095ae27f572cbde0ea652d7664", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 136 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,8 +3,13 @@\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, Type\n \n+from twisted.internet.defer import Deferred\n+\n+from scrapy import Request, Spider\n+from scrapy.crawler import Crawler\n+from scrapy.settings import BaseSettings\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import to_unicode\n \n@@ -12,29 +17,33 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n+    from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n \n class HTTP10DownloadHandler:\n     lazy = False\n \n-    def __init__(self, settings, crawler):\n-        self.HTTPClientFactory = load_object(settings[\"DOWNLOADER_HTTPCLIENTFACTORY\"])\n-        self.ClientContextFactory = load_object(\n+    def __init__(self, settings: BaseSettings, crawler: Crawler):\n+        self.HTTPClientFactory: Type[ScrapyHTTPClientFactory] = load_object(\n+            settings[\"DOWNLOADER_HTTPCLIENTFACTORY\"]\n+        )\n+        self.ClientContextFactory: Type[ScrapyClientContextFactory] = load_object(\n             settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"]\n         )\n-        self._settings = settings\n-        self._crawler = crawler\n+        self._settings: BaseSettings = settings\n+        self._crawler: Crawler = crawler\n \n     @classmethod\n-    def from_crawler(cls, crawler) -> Self:\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings, crawler)\n \n-    def download_request(self, request, spider):\n+    def download_request(self, request: Request, spider: Spider) -> Deferred:\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n         factory = self.HTTPClientFactory(request)\n         self._connect(factory)\n         return factory.deferred\n \n-    def _connect(self, factory):\n+    def _connect(self, factory: ScrapyHTTPClientFactory) -> Deferred:\n         from twisted.internet import reactor\n \n         host, port = to_unicode(factory.host), factory.port\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a828da98c3834ae70a1258278890485803ac7a5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 137 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -20,6 +20,7 @@ if TYPE_CHECKING:\n     from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n     from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n \n+\n class HTTP10DownloadHandler:\n     lazy = False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#180bc9bad7aceb3a9e10c1411212914bc32fb721", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1510 | Contributors (this commit): 74 | Commits (past 90d): 3 | Contributors (cumulative): 84 | DMM Complexity: 0.0\n\nDIFF:\n@@ -239,7 +239,7 @@ MEMUSAGE_NOTIFY_MAIL = []\n MEMUSAGE_WARNING_MB = 0\n \n METAREFRESH_ENABLED = True\n-METAREFRESH_IGNORE_TAGS = []\n+METAREFRESH_IGNORE_TAGS = [\"noscript\"]\n METAREFRESH_MAXDELAY = 100\n \n NEWSPIDER_MODULE = \"\"\n\n@@ -395,9 +395,8 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n             \"\"\"content=\"0;URL='http://example.org/newpage'\"></noscript>\"\"\"\n         )\n         rsp = HtmlResponse(req.url, body=body.encode())\n-        req2 = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(req2, Request)\n-        self.assertEqual(req2.url, \"http://example.org/newpage\")\n+        response = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(response, Response)\n \n     def test_ignore_tags_1_x_list(self):\n         \"\"\"Test that Scrapy 1.x behavior remains possible\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ae7bb849f50af0b91eea4f022d93ad201e545c06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 2884 | Contributors (this commit): 39 | Commits (past 90d): 5 | Contributors (cumulative): 39 | DMM Complexity: 0.0\n\nDIFF:\n@@ -239,15 +239,14 @@ class TunnelingAgent(Agent):\n \n     def __init__(\n         self,\n+        *,\n         reactor: ReactorBase,\n         proxyConf: Tuple[str, int, Optional[bytes]],\n-        contextFactory: Optional[IPolicyForHTTPS] = None,\n+        contextFactory: IPolicyForHTTPS,\n         connectTimeout: Optional[float] = None,\n         bindAddress: Optional[bytes] = None,\n         pool: Optional[HTTPConnectionPool] = None,\n     ):\n-        # TODO make this arg required instead\n-        assert contextFactory is not None\n         super().__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n         self._proxyConf: Tuple[str, int, Optional[bytes]] = proxyConf\n         self._contextFactory: IPolicyForHTTPS = contextFactory\n@@ -335,18 +334,16 @@ class ScrapyAgent:\n \n     def __init__(\n         self,\n-        contextFactory: Optional[IPolicyForHTTPS] = None,\n+        *,\n+        contextFactory: IPolicyForHTTPS,\n         connectTimeout: float = 10,\n         bindAddress: Optional[bytes] = None,\n         pool: Optional[HTTPConnectionPool] = None,\n         maxsize: int = 0,\n         warnsize: int = 0,\n         fail_on_dataloss: bool = True,\n-        crawler: Optional[Crawler] = None,\n+        crawler: Crawler,\n     ):\n-        # TODO make these args required instead\n-        assert contextFactory is not None\n-        assert crawler is not None\n         self._contextFactory: IPolicyForHTTPS = contextFactory\n         self._connectTimeout: float = connectTimeout\n         self._bindAddress: Optional[bytes] = bindAddress\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c9ef5209365bb820ba8f2a3cd9df9fdeca0c9591", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 3276 | Contributors (this commit): 24 | Commits (past 90d): 11 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import random\n+import warnings\n from collections import deque\n from datetime import datetime\n from time import time\n@@ -10,6 +11,7 @@ from twisted.internet.defer import Deferred\n from scrapy import Request, Spider, signals\n from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response\n from scrapy.resolver import dnscache\n from scrapy.settings import BaseSettings\n@@ -125,7 +127,7 @@ class Downloader:\n         return len(self.active) >= self.total_concurrency\n \n     def _get_slot(self, request: Request, spider: Spider) -> Tuple[str, Slot]:\n-        key = self._get_slot_key(request, spider)\n+        key = self.get_slot_key(request)\n         if key not in self.slots:\n             slot_settings = self.per_slot_settings.get(key, {})\n             conc = (\n@@ -143,7 +145,7 @@ class Downloader:\n \n         return key, self.slots[key]\n \n-    def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n+    def get_slot_key(self, request: Request) -> str:\n         if self.DOWNLOAD_SLOT in request.meta:\n             return cast(str, request.meta[self.DOWNLOAD_SLOT])\n \n@@ -153,6 +155,14 @@ class Downloader:\n \n         return key\n \n+    def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n+        warnings.warn(\n+            \"Use of this protected method is deprecated. Consider using its corresponding public method get_slot_key() instead.\",\n+            ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return self.get_slot_key(request)\n+\n     def _enqueue_request(self, request: Request, spider: Spider) -> Deferred:\n         key, slot = self._get_slot(request, spider)\n         request.meta[self.DOWNLOAD_SLOT] = key\n\n@@ -180,7 +180,7 @@ class DownloaderInterface:\n         return [(self._active_downloads(slot), slot) for slot in possible_slots]\n \n     def get_slot_key(self, request: Request) -> str:\n-        return self.downloader._get_slot_key(request, None)\n+        return self.downloader.get_slot_key(request)\n \n     def _active_downloads(self, slot: str) -> int:\n         \"\"\"Return a number of requests in a Downloader for a given slot\"\"\"\n\n@@ -25,7 +25,7 @@ class MockDownloader:\n     def __init__(self):\n         self.slots = {}\n \n-    def _get_slot_key(self, request, spider):\n+    def get_slot_key(self, request):\n         if Downloader.DOWNLOAD_SLOT in request.meta:\n             return request.meta[Downloader.DOWNLOAD_SLOT]\n \n@@ -273,14 +273,14 @@ class DownloaderAwareSchedulerTestMixin:\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n             # pylint: disable=protected-access\n-            slot = downloader._get_slot_key(request, None)\n+            slot = downloader.get_slot_key(request)\n             dequeued_slots.append(slot)\n             downloader.increment(slot)\n             requests.append(request)\n \n         for request in requests:\n             # pylint: disable=protected-access\n-            slot = downloader._get_slot_key(request, None)\n+            slot = downloader.get_slot_key(request)\n             downloader.decrement(slot)\n \n         self.assertTrue(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93f06285309bd46e96fd147bf41e564c94b5bf2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 381 | Contributors (this commit): 19 | Commits (past 90d): 6 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -60,7 +60,7 @@ class _BenchSpider(scrapy.Spider):\n         url = f\"{self.baseurl}?{urlencode(qargs, doseq=True)}\"\n         return [scrapy.Request(url, dont_filter=True)]\n \n-    def parse(self, response: Response) -> Any:  # type: ignore[override]\n+    def parse(self, response: Response) -> Any:\n         assert isinstance(Response, TextResponse)\n         for link in self.link_extractor.extract_links(response):\n             yield scrapy.Request(link.url, callback=self.parse)\n\n@@ -17,12 +17,17 @@ from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable\n+\n+    # typing.Concatenate requires Python 3.10\n     # typing.Self requires Python 3.11\n-    from typing_extensions import Self\n+    from typing_extensions import Concatenate, Self\n \n     from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n+    CallbackT = Callable[Concatenate[Response, ...], Any]\n+\n \n class Spider(object_ref):\n     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n@@ -79,6 +84,10 @@ class Spider(object_ref):\n     def _parse(self, response: Response, **kwargs: Any) -> Any:\n         return self.parse(response, **kwargs)\n \n+    if TYPE_CHECKING:\n+        parse: CallbackT\n+    else:\n+\n         def parse(self, response: Response, **kwargs: Any) -> Any:\n             raise NotImplementedError(\n                 f\"{self.__class__.__name__}.parse callback is not defined\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ed5c5ae91318768efa338680df67337ed0f67fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 204 | Lines Deleted: 101 | Files Changed: 13 | Hunks: 70 | Methods Changed: 60 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 305 | Churn Cumulative: 13351 | Contributors (this commit): 109 | Commits (past 90d): 35 | Contributors (cumulative): 279 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,10 +1,12 @@\n+from __future__ import annotations\n+\n import argparse\n import cProfile\n import inspect\n import os\n import sys\n from importlib.metadata import entry_points\n-from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Type\n+from typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Optional, Tuple, Type\n \n import scrapy\n from scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter\n@@ -15,6 +17,12 @@ from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import get_project_settings, inside_project\n from scrapy.utils.python import garbage_collect\n \n+if TYPE_CHECKING:\n+    # typing.ParamSpec requires Python 3.10\n+    from typing_extensions import ParamSpec\n+\n+    _P = ParamSpec(\"_P\")\n+\n \n class ScrapyArgumentParser(argparse.ArgumentParser):\n     def _parse_optional(\n@@ -121,7 +129,10 @@ def _print_unknown_command(\n \n \n def _run_print_help(\n-    parser: argparse.ArgumentParser, func: Callable, *a: Any, **kw: Any\n+    parser: argparse.ArgumentParser,\n+    func: Callable[_P, None],\n+    *a: _P.args,\n+    **kw: _P.kwargs,\n ) -> None:\n     try:\n         func(*a, **kw)\n\n@@ -109,12 +109,10 @@ class FTPDownloadHandler:\n     def gotClient(self, client: FTPClient, request: Request, filepath: str) -> Deferred:\n         self.client = client\n         protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n-        return client.retrieveFile(filepath, protocol).addCallbacks(\n-            callback=self._build_response,\n-            callbackArgs=(request, protocol),\n-            errback=self._failed,\n-            errbackArgs=(request,),\n-        )\n+        d = client.retrieveFile(filepath, protocol)\n+        d.addCallback(self._build_response, request, protocol)\n+        d.addErrback(self._failed, request)\n+        return d\n \n     def _build_response(\n         self, result: Any, request: Request, protocol: ReceivedDataProtocol\n\n@@ -347,7 +347,7 @@ class ExecutionEngine:\n \n         assert self.spider is not None\n         dwld = self.downloader.fetch(request, self.spider)\n-        dwld.addCallbacks(_on_success)\n+        dwld.addCallback(_on_success)\n         dwld.addBoth(_on_complete)\n         return dwld\n \n\n@@ -8,7 +8,6 @@ from collections import deque\n from typing import (\n     TYPE_CHECKING,\n     Any,\n-    AsyncGenerator,\n     AsyncIterable,\n     Deque,\n     Generator,\n@@ -18,6 +17,7 @@ from typing import (\n     Tuple,\n     Type,\n     Union,\n+    cast,\n )\n \n from itemadapter import is_item\n@@ -184,7 +184,9 @@ class Scraper:\n             result, request, spider\n         )  # returns spider's processed output\n         dfd.addErrback(self.handle_spider_error, request, result, spider)\n-        dfd.addCallback(self.handle_spider_output, request, result, spider)\n+        dfd.addCallback(\n+            self.handle_spider_output, request, cast(Response, result), spider\n+        )\n         return dfd\n \n     def _scrape2(\n@@ -256,12 +258,12 @@ class Scraper:\n         self,\n         result: Union[Iterable, AsyncIterable],\n         request: Request,\n-        response: Union[Response, Failure],\n+        response: Response,\n         spider: Spider,\n     ) -> Deferred:\n         if not result:\n             return defer_succeed(None)\n-        it: Union[Generator, AsyncGenerator]\n+        it: Union[Iterable, AsyncIterable]\n         if isinstance(result, AsyncIterable):\n             it = aiter_errback(\n                 result, self.handle_spider_error, request, response, spider\n\n@@ -303,10 +303,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         dfd = mustbe_deferred(\n             self._process_spider_input, scrape_func, response, request, spider\n         )\n-        dfd.addCallbacks(\n-            callback=deferred_f_from_coro_f(process_callback_output),\n-            errback=process_spider_exception,\n-        )\n+        dfd.addCallback(deferred_f_from_coro_f(process_callback_output))\n+        dfd.addErrback(process_spider_exception)\n         return dfd\n \n     def process_start_requests(\n\n@@ -154,12 +154,8 @@ class MailSender:\n             return None\n \n         dfd = self._sendmail(rcpts, msg.as_string().encode(charset or \"utf-8\"))\n-        dfd.addCallbacks(\n-            callback=self._sent_ok,\n-            errback=self._sent_failed,\n-            callbackArgs=(to, cc, subject, len(attachs)),\n-            errbackArgs=(to, cc, subject, len(attachs)),\n-        )\n+        dfd.addCallback(self._sent_ok, to, cc, subject, len(attachs))\n+        dfd.addErrback(self._sent_failed, to, cc, subject, len(attachs))\n         reactor.addSystemEventTrigger(\"before\", \"shutdown\", lambda: dfd)\n         return dfd\n \n\n@@ -459,7 +459,8 @@ class FilesPipeline(MediaPipeline):\n \n         path = self.file_path(request, info=info, item=item)\n         dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n-        dfd.addCallbacks(_onsuccess, lambda _: None)\n+        dfd.addCallback(_onsuccess)\n+        dfd.addErrback(lambda _: None)\n         dfd.addErrback(\n             lambda f: logger.error(\n                 self.__class__.__name__ + \".store.stat_file\",\n\n@@ -106,10 +106,17 @@ class MediaPipeline:\n \n         # Return cached result if request was already seen\n         if fp in info.downloaded:\n-            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n+            d = defer_result(info.downloaded[fp])\n+            d.addCallback(cb)\n+            if eb:\n+                d.addErrback(eb)\n+            return d\n \n         # Otherwise, wait for result\n-        wad = Deferred().addCallbacks(cb, eb)\n+        wad = Deferred()\n+        wad.addCallback(cb)\n+        if eb:\n+            wad.addErrback(eb)\n         info.waiting[fp].append(wad)\n \n         # Check if request is downloading right now to avoid doing it twice\n@@ -140,23 +147,11 @@ class MediaPipeline:\n         if self.download_func:\n             # this ugly code was left only to support tests. TODO: remove\n             dfd = mustbe_deferred(self.download_func, request, info.spider)\n-            dfd.addCallbacks(\n-                callback=self.media_downloaded,\n-                callbackArgs=(request, info),\n-                callbackKeywords={\"item\": item},\n-                errback=self.media_failed,\n-                errbackArgs=(request, info),\n-            )\n         else:\n             self._modify_media_request(request)\n             dfd = self.crawler.engine.download(request)\n-            dfd.addCallbacks(\n-                callback=self.media_downloaded,\n-                callbackArgs=(request, info),\n-                callbackKeywords={\"item\": item},\n-                errback=self.media_failed,\n-                errbackArgs=(request, info),\n-            )\n+        dfd.addCallback(self.media_downloaded, request, info, item=item)\n+        dfd.addErrback(self.media_failed, request, info)\n         return dfd\n \n     def _cache_result_and_execute_waiters(self, result, fp, info):\n\n@@ -231,7 +231,9 @@ def _request_deferred(request: Request) -> defer.Deferred:\n     d: defer.Deferred = defer.Deferred()\n     d.addBoth(_restore_callbacks)\n     if request.callback:\n-        d.addCallbacks(request.callback, request.errback)\n+        d.addCallback(request.callback)\n+    if request.errback:\n+        d.addErrback(request.errback)\n \n     request.callback, request.errback = d.callback, d.errback\n     return d\n\n@@ -1,21 +1,34 @@\n+from __future__ import annotations\n+\n import warnings\n from functools import wraps\n-from typing import Any, Callable\n+from typing import TYPE_CHECKING, Any, Callable, TypeVar\n \n from twisted.internet import defer, threads\n from twisted.internet.defer import Deferred\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n+if TYPE_CHECKING:\n+    # typing.ParamSpec requires Python 3.10\n+    from typing_extensions import ParamSpec\n \n-def deprecated(use_instead: Any = None) -> Callable:\n+    _P = ParamSpec(\"_P\")\n+\n+\n+_T = TypeVar(\"_T\")\n+\n+\n+def deprecated(\n+    use_instead: Any = None,\n+) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]:\n     \"\"\"This is a decorator which can be used to mark functions\n     as deprecated. It will result in a warning being emitted\n     when the function is used.\"\"\"\n \n-    def deco(func: Callable) -> Callable:\n+    def deco(func: Callable[_P, _T]) -> Callable[_P, _T]:\n         @wraps(func)\n-        def wrapped(*args: Any, **kwargs: Any) -> Any:\n+        def wrapped(*args: _P.args, **kwargs: _P.kwargs) -> Any:\n             message = f\"Call to deprecated function {func.__name__}.\"\n             if use_instead:\n                 message += f\" Use {use_instead} instead.\"\n@@ -30,23 +43,23 @@ def deprecated(use_instead: Any = None) -> Callable:\n     return deco\n \n \n-def defers(func: Callable) -> Callable[..., Deferred]:\n+def defers(func: Callable[_P, _T]) -> Callable[_P, Deferred[_T]]:\n     \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n \n     @wraps(func)\n-    def wrapped(*a: Any, **kw: Any) -> Deferred:\n+    def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n         return defer.maybeDeferred(func, *a, **kw)\n \n     return wrapped\n \n \n-def inthread(func: Callable) -> Callable[..., Deferred]:\n+def inthread(func: Callable[_P, _T]) -> Callable[_P, Deferred[_T]]:\n     \"\"\"Decorator to call a function in a thread and return a deferred with the\n     result\n     \"\"\"\n \n     @wraps(func)\n-    def wrapped(*a: Any, **kw: Any) -> Deferred:\n+    def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n         return threads.deferToThread(func, *a, **kw)\n \n     return wrapped\n\n@@ -2,21 +2,22 @@\n Helper functions for dealing with Twisted deferreds\n \"\"\"\n \n+from __future__ import annotations\n+\n import asyncio\n import inspect\n from asyncio import Future\n from functools import wraps\n from types import CoroutineType\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n-    AsyncGenerator,\n     AsyncIterable,\n     AsyncIterator,\n     Awaitable,\n     Callable,\n     Coroutine,\n     Dict,\n-    Generator,\n     Iterable,\n     Iterator,\n     List,\n@@ -37,6 +38,14 @@ from twisted.python.failure import Failure\n from scrapy.exceptions import IgnoreRequest\n from scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed\n \n+if TYPE_CHECKING:\n+    # typing.Concatenate and typing.ParamSpec require Python 3.10\n+    from typing_extensions import Concatenate, ParamSpec\n+\n+    _P = ParamSpec(\"_P\")\n+\n+_T = TypeVar(\"_T\")\n+\n \n def defer_fail(_failure: Failure) -> Deferred:\n     \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n@@ -74,7 +83,31 @@ def defer_result(result: Any) -> Deferred:\n     return defer_succeed(result)\n \n \n-def mustbe_deferred(f: Callable, *args: Any, **kw: Any) -> Deferred:\n+@overload\n+def mustbe_deferred(\n+    f: Callable[_P, Deferred[_T]], *args: _P.args, **kw: _P.kwargs\n+) -> Deferred[_T]: ...\n+\n+\n+@overload\n+def mustbe_deferred(\n+    f: Callable[_P, Coroutine[Deferred[Any], Any, _T]],\n+    *args: _P.args,\n+    **kw: _P.kwargs,\n+) -> Deferred[_T]: ...\n+\n+\n+@overload\n+def mustbe_deferred(\n+    f: Callable[_P, _T], *args: _P.args, **kw: _P.kwargs\n+) -> Deferred[_T]: ...\n+\n+\n+def mustbe_deferred(\n+    f: Callable[_P, Union[Deferred[_T], Coroutine[Deferred[Any], Any, _T], _T]],\n+    *args: _P.args,\n+    **kw: _P.kwargs,\n+) -> Deferred[_T]:\n     \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n     callback/errback to next reactor loop\n     \"\"\"\n@@ -92,7 +125,11 @@ def mustbe_deferred(f: Callable, *args: Any, **kw: Any) -> Deferred:\n \n \n def parallel(\n-    iterable: Iterable, count: int, callable: Callable, *args: Any, **named: Any\n+    iterable: Iterable[_T],\n+    count: int,\n+    callable: Callable[Concatenate[_T, _P], Any],\n+    *args: _P.args,\n+    **named: _P.kwargs,\n ) -> Deferred:\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n@@ -104,7 +141,7 @@ def parallel(\n     return DeferredList([coop.coiterate(work) for _ in range(count)])\n \n \n-class _AsyncCooperatorAdapter(Iterator):\n+class _AsyncCooperatorAdapter(Iterator[Deferred]):\n     \"\"\"A class that wraps an async iterable into a normal iterator suitable\n     for using in Cooperator.coiterate(). As it's only needed for parallel_async(),\n     it calls the callable directly in the callback, instead of providing a more\n@@ -152,28 +189,30 @@ class _AsyncCooperatorAdapter(Iterator):\n \n     def __init__(\n         self,\n-        aiterable: AsyncIterable,\n-        callable: Callable,\n-        *callable_args: Any,\n-        **callable_kwargs: Any,\n+        aiterable: AsyncIterable[_T],\n+        callable: Callable[Concatenate[_T, _P], Any],\n+        *callable_args: _P.args,\n+        **callable_kwargs: _P.kwargs,\n     ):\n-        self.aiterator: AsyncIterator = aiterable.__aiter__()\n-        self.callable: Callable = callable\n+        self.aiterator: AsyncIterator[_T] = aiterable.__aiter__()\n+        self.callable: Callable[Concatenate[_T, _P], Any] = callable\n         self.callable_args: Tuple[Any, ...] = callable_args\n         self.callable_kwargs: Dict[str, Any] = callable_kwargs\n         self.finished: bool = False\n         self.waiting_deferreds: List[Deferred] = []\n-        self.anext_deferred: Optional[Deferred] = None\n+        self.anext_deferred: Optional[Deferred[_T]] = None\n \n-    def _callback(self, result: Any) -> None:\n+    def _callback(self, result: _T) -> None:\n         # This gets called when the result from aiterator.__anext__() is available.\n         # It calls the callable on it and sends the result to the oldest waiting Deferred\n         # (by chaining if the result is a Deferred too or by firing if not).\n         self.anext_deferred = None\n-        result = self.callable(result, *self.callable_args, **self.callable_kwargs)\n+        callable_result = self.callable(\n+            result, *self.callable_args, **self.callable_kwargs\n+        )\n         d = self.waiting_deferreds.pop(0)\n-        if isinstance(result, Deferred):\n-            result.chainDeferred(d)\n+        if isinstance(callable_result, Deferred):\n+            callable_result.chainDeferred(d)\n         else:\n             d.callback(None)\n         if self.waiting_deferreds:\n@@ -207,11 +246,11 @@ class _AsyncCooperatorAdapter(Iterator):\n \n \n def parallel_async(\n-    async_iterable: AsyncIterable,\n+    async_iterable: AsyncIterable[_T],\n     count: int,\n-    callable: Callable,\n-    *args: Any,\n-    **named: Any,\n+    callable: Callable[Concatenate[_T, _P], Any],\n+    *args: _P.args,\n+    **named: _P.kwargs,\n ) -> Deferred:\n     \"\"\"Like parallel but for async iterators\"\"\"\n     coop = Cooperator()\n@@ -221,7 +260,10 @@ def parallel_async(\n \n \n def process_chain(\n-    callbacks: Iterable[Callable], input: Any, *a: Any, **kw: Any\n+    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n+    input: Any,\n+    *a: _P.args,\n+    **kw: _P.kwargs,\n ) -> Deferred:\n     \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n     d: Deferred = Deferred()\n@@ -232,23 +274,17 @@ def process_chain(\n \n \n def process_chain_both(\n-    callbacks: Iterable[Callable],\n-    errbacks: Iterable[Callable],\n+    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n+    errbacks: Iterable[Callable[Concatenate[Failure, _P], Any]],\n     input: Any,\n-    *a: Any,\n-    **kw: Any,\n+    *a: _P.args,\n+    **kw: _P.kwargs,\n ) -> Deferred:\n     \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n     d: Deferred = Deferred()\n     for cb, eb in zip(callbacks, errbacks):\n-        d.addCallbacks(\n-            callback=cb,\n-            errback=eb,\n-            callbackArgs=a,\n-            callbackKeywords=kw,\n-            errbackArgs=a,\n-            errbackKeywords=kw,\n-        )\n+        d.addCallback(cb, *a, **kw)\n+        d.addErrback(eb, *a, **kw)\n     if isinstance(input, failure.Failure):\n         d.errback(input)\n     else:\n@@ -257,20 +293,27 @@ def process_chain_both(\n \n \n def process_parallel(\n-    callbacks: Iterable[Callable], input: Any, *a: Any, **kw: Any\n+    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n+    input: Any,\n+    *a: _P.args,\n+    **kw: _P.kwargs,\n ) -> Deferred:\n     \"\"\"Return a Deferred with the output of all successful calls to the given\n     callbacks\n     \"\"\"\n     dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n     d: Deferred = DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n-    d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n+    d.addCallback(lambda r: [x[1] for x in r])\n+    d.addErrback(lambda f: f.value.subFailure)\n     return d\n \n \n def iter_errback(\n-    iterable: Iterable, errback: Callable, *a: Any, **kw: Any\n-) -> Generator:\n+    iterable: Iterable[_T],\n+    errback: Callable[Concatenate[Failure, _P], Any],\n+    *a: _P.args,\n+    **kw: _P.kwargs,\n+) -> Iterable[_T]:\n     \"\"\"Wraps an iterable calling an errback if an error is caught while\n     iterating it.\n     \"\"\"\n@@ -285,8 +328,11 @@ def iter_errback(\n \n \n async def aiter_errback(\n-    aiterable: AsyncIterable, errback: Callable, *a: Any, **kw: Any\n-) -> AsyncGenerator:\n+    aiterable: AsyncIterable[_T],\n+    errback: Callable[Concatenate[Failure, _P], Any],\n+    *a: _P.args,\n+    **kw: _P.kwargs,\n+) -> AsyncIterable[_T]:\n     \"\"\"Wraps an async iterable calling an errback if an error is caught while\n     iterating it. Similar to scrapy.utils.defer.iter_errback()\n     \"\"\"\n@@ -301,7 +347,6 @@ async def aiter_errback(\n \n \n _CT = TypeVar(\"_CT\", bound=Union[Awaitable, CoroutineType, Future])\n-_T = TypeVar(\"_T\")\n \n \n @overload\n@@ -327,7 +372,9 @@ def deferred_from_coro(o: _T) -> Union[Deferred, _T]:\n     return o\n \n \n-def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]) -> Callable:\n+def deferred_f_from_coro_f(\n+    coro_f: Callable[_P, Coroutine[Any, Any, _T]]\n+) -> Callable[_P, Deferred[_T]]:\n     \"\"\"Converts a coroutine function into a function that returns a Deferred.\n \n     The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.\n@@ -335,13 +382,15 @@ def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]) -> Callable:\n     \"\"\"\n \n     @wraps(coro_f)\n-    def f(*coro_args: Any, **coro_kwargs: Any) -> Any:\n+    def f(*coro_args: _P.args, **coro_kwargs: _P.kwargs) -> Any:\n         return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\n \n     return f\n \n \n-def maybeDeferred_coro(f: Callable, *args: Any, **kw: Any) -> Deferred:\n+def maybeDeferred_coro(\n+    f: Callable[_P, Any], *args: _P.args, **kw: _P.kwargs\n+) -> Deferred:\n     \"\"\"Copy of defer.maybeDeferred that also converts coroutines to Deferreds.\"\"\"\n     try:\n         result = f(*args, **kw)\n\n@@ -2,6 +2,8 @@\n This module contains essential stuff that should've come with Python itself ;)\n \"\"\"\n \n+from __future__ import annotations\n+\n import collections.abc\n import gc\n import inspect\n@@ -11,6 +13,7 @@ import weakref\n from functools import partial, wraps\n from itertools import chain\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     AsyncGenerator,\n     AsyncIterable,\n@@ -25,12 +28,21 @@ from typing import (\n     Optional,\n     Pattern,\n     Tuple,\n+    TypeVar,\n     Union,\n     overload,\n )\n \n from scrapy.utils.asyncgen import as_async_generator\n \n+if TYPE_CHECKING:\n+    # typing.Concatenate and typing.ParamSpec require Python 3.10\n+    from typing_extensions import Concatenate, ParamSpec\n+\n+    _P = ParamSpec(\"_P\")\n+\n+_T = TypeVar(\"_T\")\n+\n \n def flatten(x: Iterable) -> list:\n     \"\"\"flatten(sequence) -> list\n@@ -169,14 +181,19 @@ def re_rsearch(\n     return None\n \n \n-def memoizemethod_noargs(method: Callable) -> Callable:\n+_SelfT = TypeVar(\"_SelfT\")\n+\n+\n+def memoizemethod_noargs(\n+    method: Callable[Concatenate[_SelfT, _P], _T]\n+) -> Callable[Concatenate[_SelfT, _P], _T]:\n     \"\"\"Decorator to cache the result of a method (without arguments) using a\n     weak reference to its object\n     \"\"\"\n-    cache: weakref.WeakKeyDictionary[Any, Any] = weakref.WeakKeyDictionary()\n+    cache: weakref.WeakKeyDictionary[_SelfT, _T] = weakref.WeakKeyDictionary()\n \n     @wraps(method)\n-    def new_method(self: Any, *args: Any, **kwargs: Any) -> Any:\n+    def new_method(self: _SelfT, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n         if self not in cache:\n             cache[self] = method(self, *args, **kwargs)\n         return cache[self]\n\n@@ -1,8 +1,21 @@\n+from __future__ import annotations\n+\n import asyncio\n import sys\n from asyncio import AbstractEventLoop, AbstractEventLoopPolicy\n from contextlib import suppress\n-from typing import Any, Callable, Dict, List, Optional, Sequence, Type\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    Dict,\n+    Generic,\n+    List,\n+    Optional,\n+    Tuple,\n+    Type,\n+    TypeVar,\n+)\n from warnings import catch_warnings, filterwarnings, warn\n \n from twisted.internet import asyncioreactor, error\n@@ -13,6 +26,14 @@ from twisted.internet.tcp import Port\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import load_object\n \n+if TYPE_CHECKING:\n+    # typing.ParamSpec requires Python 3.10\n+    from typing_extensions import ParamSpec\n+\n+    _P = ParamSpec(\"_P\")\n+\n+_T = TypeVar(\"_T\")\n+\n \n def listen_tcp(portrange: List[int], host: str, factory: ServerFactory) -> Port:  # type: ignore[return]\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n@@ -32,14 +53,14 @@ def listen_tcp(portrange: List[int], host: str, factory: ServerFactory) -> Port:\n                 raise\n \n \n-class CallLaterOnce:\n+class CallLaterOnce(Generic[_T]):\n     \"\"\"Schedule a function to be called in the next reactor loop, but only if\n     it hasn't been already scheduled since the last time it ran.\n     \"\"\"\n \n-    def __init__(self, func: Callable, *a: Any, **kw: Any):\n-        self._func: Callable = func\n-        self._a: Sequence[Any] = a\n+    def __init__(self, func: Callable[_P, _T], *a: _P.args, **kw: _P.kwargs):\n+        self._func: Callable[_P, _T] = func\n+        self._a: Tuple[Any, ...] = a\n         self._kw: Dict[str, Any] = kw\n         self._call: Optional[DelayedCall] = None\n \n@@ -53,7 +74,7 @@ class CallLaterOnce:\n         if self._call:\n             self._call.cancel()\n \n-    def __call__(self) -> Any:\n+    def __call__(self) -> _T:\n         self._call = None\n         return self._func(*self._a, **self._kw)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
