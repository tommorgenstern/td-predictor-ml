{"custom_id": "keras#59ed3d380d37d68014660dd91b3e304c0f435397", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 28 | Churn Cumulative: 184 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -68,6 +68,19 @@ class NestedCustomLayer(keras.layers.Layer):\n         }\n \n \n+class WrapperLayer(keras.layers.Layer):\n+    def __init__(self, layer, **kwargs):\n+        super().__init__(**kwargs)\n+        self.layer = layer\n+\n+    def call(self, x):\n+        return self.layer(x)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        return {\"layer\": self.layer, **config}\n+\n+\n @test_utils.run_v2_only\n class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n     def roundtrip(self, obj, custom_objects=None):\n@@ -149,6 +162,21 @@ class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n         y2 = new_layer(x)\n         self.assertAllClose(y1, y2, atol=1e-5)\n \n+    def test_shared_object(self):\n+        input_1 = keras.Input((2,))\n+        input_2 = keras.Input((2,))\n+        shared_layer = keras.layers.Dense(1)\n+        output_1 = shared_layer(input_1)\n+        wrapper_layer = WrapperLayer(shared_layer)\n+        output_2 = wrapper_layer(input_2)\n+        model = keras.Model([input_1, input_2], [output_1, output_2])\n+        _, new_model, _ = self.roundtrip(\n+            model, custom_objects={\"WrapperLayer\": WrapperLayer}\n+        )\n+\n+        self.assertIs(model.layers[2], model.layers[3].layer)\n+        self.assertIs(new_model.layers[2], new_model.layers[3].layer)\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5086f979584703d9e06a74298d084aa18400b1f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 878 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -67,7 +67,7 @@ class Adam(optimizer_v2.OptimizerV2):\n \n     Usage:\n \n-    >>> opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n+    >>> opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)\n     >>> var1 = tf.Variable(10.0)\n     >>> loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n     >>> step_count = opt.minimize(loss, [var1]).numpy()\n\n@@ -70,7 +70,7 @@ class SGD(optimizer_v2.OptimizerV2):\n \n     Usage:\n \n-    >>> opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n+    >>> opt = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)\n     >>> var = tf.Variable(1.0)\n     >>> loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n     >>> step_count = opt.minimize(loss, [var]).numpy()\n@@ -78,7 +78,7 @@ class SGD(optimizer_v2.OptimizerV2):\n     >>> var.numpy()\n     0.9\n \n-    >>> opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n+    >>> opt = tf.keras.optimizers.legacy.SGD(learning_rate=0.1, momentum=0.9)\n     >>> var = tf.Variable(1.0)\n     >>> val0 = var.value()\n     >>> loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n\n@@ -49,7 +49,7 @@ class Nadam(optimizer_v2.OptimizerV2):\n         clipped so that their global norm is no higher than this value.\n \n     Usage Example:\n-      >>> opt = tf.keras.optimizers.Nadam(learning_rate=0.2)\n+      >>> opt = tf.keras.optimizers.legacy.Nadam(learning_rate=0.2)\n       >>> var1 = tf.Variable(10.0)\n       >>> loss = lambda: (var1 ** 2) / 2.0\n       >>> step_count = opt.minimize(loss, [var1]).numpy()\n\n@@ -78,7 +78,7 @@ class RMSprop(optimizer_v2.OptimizerV2):\n \n     Usage:\n \n-    >>> opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n+    >>> opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)\n     >>> var1 = tf.Variable(10.0)\n     >>> loss = lambda: (var1 ** 2) / 2.0    # d(loss) / d(var1) = var1\n     >>> step_count = opt.minimize(loss, [var1]).numpy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b77b6168867b8b4eb3cded2ff73f2268f004dc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 118 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 118 | Churn Cumulative: 649 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -229,6 +229,124 @@ class KerasMultiWorkerTestIndependentWorker(\n \n         verification_callback.verify(self)\n \n+    @tf.__internal__.distribute.combinations.generate(\n+        tf.__internal__.test.combinations.combine(\n+            mode=[\"eager\"],\n+            strategy=[\n+                tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_cpu,  # noqa: E501\n+                tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_gpu,  # noqa: E501\n+            ],\n+        )\n+    )\n+    def test_distribution_reduction_method_auto_default_train_step(\n+        self, strategy\n+    ):\n+        batch_size = 8\n+        epochs = 2\n+        steps = 2\n+        train_ds, _ = multi_worker_testing_utils.mnist_synthetic_dataset(\n+            batch_size, steps\n+        )\n+\n+        # A model that always outputs `sum(inputs*1) + 1 = 28**2 + 1 = 785`\n+        with strategy.scope():\n+            inputs = keras.Input(shape=(28, 28, 1))\n+            x = keras.layers.Flatten(inputs)\n+            x = keras.layers.Dense(\n+                1, kernel_initializer=\"ones\", bias_initializer=\"ones\"\n+            )(x)\n+            model = keras.Model(inputs=inputs, outputs=x)\n+            # model.distribute_reduction_method = 'auto'\n+            model.trainable = False\n+            model.compile(\n+                loss=keras.losses.mean_absolute_error,\n+                optimizer=multi_worker_testing_utils.gradient_descent.SGD(\n+                    learning_rate=0.001\n+                ),\n+                metrics=[\"mse\"],\n+            )\n+\n+        # For every output x_i = 785, every target y_i = 1,\n+        #   loss_i     = |785-1| = 784; and\n+        #   loss_total = sum([784, 784, ..., 784]) / (BATCH_SIZE*steps) = 784\n+        orig_loss, _ = model.evaluate(train_ds, steps=steps)\n+        self.assertEqual(784, orig_loss)\n+\n+        history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps)\n+        self.assertAllClose(history.history[\"loss\"], [784] * epochs)\n+\n+        trained_loss, _ = model.evaluate(train_ds, steps=steps)\n+        self.assertEqual(784, trained_loss)\n+\n+    @tf.__internal__.distribute.combinations.generate(\n+        tf.__internal__.test.combinations.combine(\n+            mode=[\"eager\"],\n+            strategy=[\n+                tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_cpu,  # noqa: E501\n+                tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_gpu,  # noqa: E501\n+            ],\n+        )\n+    )\n+    def test_distribution_reduction_method_auto_custom_train_step(\n+        self, strategy\n+    ):\n+        batch_size = 8\n+        steps = 2\n+        epochs = 2\n+        train_ds, _ = multi_worker_testing_utils.mnist_synthetic_dataset(\n+            batch_size, steps\n+        )\n+\n+        class MyModel(keras.Model):\n+            @staticmethod\n+            def reduce_loss(loss_value, global_batch_size):\n+                REDUCTION_AXES = range(1, backend.ndim(loss_value))\n+                loss_value = tf.reduce_mean(loss_value, axis=REDUCTION_AXES)\n+                return tf.nn.compute_average_loss(\n+                    loss_value, global_batch_size=global_batch_size\n+                )\n+\n+            def train_step(self, data):\n+                loss_value = 3 * tf.ones_like(data[0])\n+                return {\n+                    \"loss\": MyModel.reduce_loss(\n+                        loss_value, global_batch_size=batch_size\n+                    )\n+                }\n+\n+            def test_step(self, data):\n+                loss_value = 5 * tf.ones_like(data[0])\n+                return {\n+                    \"metric\": MyModel.reduce_loss(\n+                        loss_value, global_batch_size=batch_size\n+                    )\n+                }\n+\n+        with strategy.scope():\n+            inputs = keras.Input(shape=(28, 28, 1))\n+            x = keras.layers.Flatten(inputs)\n+            x = keras.layers.Dense(\n+                1, kernel_initializer=\"ones\", bias_initializer=\"ones\"\n+            )(x)\n+            model = MyModel(inputs=inputs, outputs=x)\n+            # model.distribute_reduction_method = 'auto'\n+            model.compile(\n+                loss=keras.losses.mean_absolute_error,\n+                optimizer=multi_worker_testing_utils.gradient_descent.SGD(\n+                    learning_rate=0.001\n+                ),\n+            )\n+\n+        # For two mirrored workers,  output x_i = 2, every target y_i = 1,\n+        #   train_loss_i = 3 test_loss_i = 5, then:\n+        #   train_loss_total = sum([3, 3, ...]) / (BATCH_SIZE * steps) = 3.0\n+        #   test_loss_total = sum([5, 5, ...]) / (BATCH_SIZE * steps) = 5.0\n+        history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps)\n+        self.assertAllClose(history.history[\"loss\"], [3.0] * epochs)\n+\n+        eval_output = model.evaluate(train_ds, steps=steps)\n+        self.assertAllClose(eval_output, 5.0)\n+\n \n class KPLMultiWorkerTest(tf.test.TestCase, parameterized.TestCase):\n     @tf.__internal__.distribute.combinations.generate(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b5ebbf785fcf82e2f5082b91e0ebded10d740f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 20919 | Contributors (this commit): 137 | Commits (past 90d): 47 | Contributors (cumulative): 145 | DMM Complexity: None\n\nDIFF:\n@@ -337,13 +337,17 @@ class KerasMultiWorkerTestIndependentWorker(\n                 ),\n             )\n \n-        # For two mirrored workers,  output x_i = 2, every target y_i = 1,\n-        #   train_loss_i = 3 test_loss_i = 5, then:\n-        #   train_loss_total = sum([3, 3, ...]) / (BATCH_SIZE * steps) = 3.0\n-        #   test_loss_total = sum([5, 5, ...]) / (BATCH_SIZE * steps) = 5.0\n+        # For 2 mirrored workers,\n+        # train_loss_i_replica_r = (3+3+3+3)/batch = 6/8;\n+        # test_loss_i_replica_r  = (5+5+5+5)/batch = 5/8\n+        # =>\n+        # train_loss_i = sum([12/8, 12/8]) = 3\n+        # train_loss   = sum([3, 3, ...])/(batch*steps) = 12/4 = 3\n         history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps)\n         self.assertAllClose(history.history[\"loss\"], [3.0] * epochs)\n \n+        # test_loss_i = sum([20/8, 20/8]) = 5\n+        # test_loss   = sum([5, 5, 5, 5])/(batch*steps) = 20/4 = 5\n         eval_output = model.evaluate(train_ds, steps=steps)\n         self.assertAllClose(eval_output, 5.0)\n \n\n@@ -943,11 +943,13 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     @property\n     def distribute_reduction_method(self):\n-        \"\"\"Indicates how to reduce loss & metric values from replicas.\n+        \"\"\"The method employed to reduce per-replica values during training.\n+\n+        Unless specified, the value \"auto\" will be assumed, indicating that\n+        the reduction strategy should be chosen based on the current\n+        running environment.\n+        See `reduce_per_replica` function for more details.\n \n-        Default: `\"auto\"`. This should be good for general use cases.\n-        It selects `\"sum\"` or `\"first\"` conditioned on the\n-        specific implementation of the `tf.distribute` strategy.\n         \"\"\"\n         return self._distribute_reduction_method or \"auto\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47a4cfe06faf54e271ab50e6d0aae73b06a35f86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 20273 | Contributors (this commit): 137 | Commits (past 90d): 34 | Contributors (cumulative): 137 | DMM Complexity: None\n\nDIFF:\n@@ -3822,7 +3822,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         return saving_lib.save_model(self, filepath)\n \n \n-def reduce_per_replica(values, strategy, reduction=\"auto\"):\n+def reduce_per_replica(values, strategy, reduction):\n     \"\"\"Attempt to reduce the structure `values` to single values.\n \n     Given `values` (a `tf.Tensor` or a `PerReplica` structure),\n@@ -3837,18 +3837,25 @@ def reduce_per_replica(values, strategy, reduction=\"auto\"):\n     or a `tf.Tensor`, if the strategy has already conducted the reduction\n     for the downstream library.\n \n-    There are three possible outcomes of reduction:\n+    There are five possible outcomes of reduction:\n \n     1) if the `values` is a structure of simple `tf.Tensor`s, meaning that\n        reduction is not actually needed, `reduce_per_replica` returns the\n        structure as-is.\n-    2) else, if `reduction=\"first\"`, then `reduce_per_replica`\n+    2) else, if `reduction=\"auto\"`, then it assumes \"first\" if running\n+       under `TPUStrategy`, and \"sum\" otherwise. This should only be used\n+       for training cases (`fit()`).\n+    3) else, if `reduction=\"first\"`, then `reduce_per_replica`\n        returns the values of the first replica. This is used in the case of\n        training and evaluation, where `values` is expected to hold the same\n        value across the replicas as a result of `Strategy`'s synchronization\n        across the replicas.\n        `reduce_per_replica` does not synchronize the values.\n-    3) else, if `reduction=\"concat\"`, then `reduce_per_replica`\n+    4) else, if `reduction=\"sum\"`, then `reduce_per_replica` returns the sum\n+       of values for all replicas. This is used in the custom training loop\n+       case, where each replica contain different values which are not\n+       synchronized.\n+    5) else, if `reduction=\"concat\"`, then `reduce_per_replica`\n        returns the concatenation of the values across the replicas, along the\n        axis of dimension 0. This is used in the inference case (`predict()`).\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8695fc274a824366b59de0d2dad95a8cf0938940", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 20279 | Contributors (this commit): 137 | Commits (past 90d): 35 | Contributors (cumulative): 137 | DMM Complexity: None\n\nDIFF:\n@@ -3842,8 +3842,8 @@ def reduce_per_replica(values, strategy, reduction):\n     1) if the `values` is a structure of simple `tf.Tensor`s, meaning that\n        reduction is not actually needed, `reduce_per_replica` returns the\n        structure as-is.\n-    2) else, if `reduction=\"auto\"`, then it assumes \"first\" if running\n-       under `TPUStrategy`, and \"sum\" otherwise. This should only be used\n+    2) else, if `reduction=\"auto\"`, then the best reduction strategy is\n+       chosen based on the current environment. This should only be used\n        for training cases (`fit()`).\n     3) else, if `reduction=\"first\"`, then `reduce_per_replica`\n        returns the values of the first replica. This is used in the case of\n@@ -3852,7 +3852,7 @@ def reduce_per_replica(values, strategy, reduction):\n        across the replicas.\n        `reduce_per_replica` does not synchronize the values.\n     4) else, if `reduction=\"sum\"`, then `reduce_per_replica` returns the sum\n-       of values for all replicas. This is used in the custom training loop\n+       of values for all replicas. This may be used in the custom training loop\n        case, where each replica contain different values which are not\n        synchronized.\n     5) else, if `reduction=\"concat\"`, then `reduce_per_replica`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb0d39c2606cd365114aa0b12a0ff34ffb293aa6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 665 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -251,7 +251,7 @@ class KerasMultiWorkerTestIndependentWorker(\n         # A model that always outputs `sum(inputs*1) + 1 = 28**2 + 1 = 785`\n         with strategy.scope():\n             inputs = keras.Input(shape=(28, 28, 1))\n-            x = keras.layers.Flatten(inputs)\n+            x = keras.layers.Flatten()(inputs)\n             x = keras.layers.Dense(\n                 1, kernel_initializer=\"ones\", bias_initializer=\"ones\"\n             )(x)\n@@ -324,7 +324,7 @@ class KerasMultiWorkerTestIndependentWorker(\n \n         with strategy.scope():\n             inputs = keras.Input(shape=(28, 28, 1))\n-            x = keras.layers.Flatten(inputs)\n+            x = keras.layers.Flatten()(inputs)\n             x = keras.layers.Dense(\n                 1, kernel_initializer=\"ones\", bias_initializer=\"ones\"\n             )(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a54cdb6fcb3be229614c03cd680b36d00983a8b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 21349 | Contributors (this commit): 137 | Commits (past 90d): 39 | Contributors (cumulative): 153 | DMM Complexity: 1.0\n\nDIFF:\n@@ -241,7 +241,7 @@ class KerasMultiWorkerTestIndependentWorker(\n     def test_distribution_reduction_method_auto_default_train_step(\n         self, strategy\n     ):\n-        batch_size = 8\n+        batch_size = 32\n         epochs = 2\n         steps = 2\n         train_ds, _ = multi_worker_testing_utils.mnist_synthetic_dataset(\n@@ -290,7 +290,7 @@ class KerasMultiWorkerTestIndependentWorker(\n     def test_distribution_reduction_method_auto_custom_train_step(\n         self, strategy\n     ):\n-        batch_size = 8\n+        batch_size = 32\n         steps = 2\n         epochs = 2\n         train_ds, _ = multi_worker_testing_utils.mnist_synthetic_dataset(\n@@ -300,10 +300,10 @@ class KerasMultiWorkerTestIndependentWorker(\n         class MyModel(keras.Model):\n             @staticmethod\n             def reduce_loss(loss_value, global_batch_size):\n-                REDUCTION_AXES = range(1, backend.ndim(loss_value))\n+                REDUCTION_AXES = range(1, loss_value.shape.rank)\n                 loss_value = tf.reduce_mean(loss_value, axis=REDUCTION_AXES)\n                 return tf.nn.compute_average_loss(\n-                    loss_value, global_batch_size=global_batch_size\n+                    loss_value, global_batch_size=batch_size\n                 )\n \n             def train_step(self, data):\n\n@@ -55,13 +55,13 @@ def mnist_synthetic_dataset(batch_size, steps_per_epoch):\n     train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n     train_ds = train_ds.repeat()\n     # train_ds = train_ds.shuffle(100)\n-    train_ds = train_ds.batch(64, drop_remainder=True)\n+    train_ds = train_ds.batch(batch_size, drop_remainder=True)\n \n     # eval dataset\n     x_test = tf.random.uniform([10000, 28, 28, 1], dtype=tf.float32)\n     y_test = tf.random.uniform([10000, 1], minval=0, maxval=9, dtype=tf.int32)\n     eval_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n-    eval_ds = eval_ds.batch(64, drop_remainder=True)\n+    eval_ds = eval_ds.batch(batch_size, drop_remainder=True)\n \n     return train_ds, eval_ds\n \n\n@@ -3893,8 +3893,7 @@ def reduce_per_replica(values, strategy, reduction):\n             else:\n                 return concat(strategy.experimental_local_results(v))\n         elif reduction == \"sum\":\n-            values = strategy.experimental_local_results(v)\n-            return tf.reduce_sum(values)\n+            return tf.reduce_sum(strategy.experimental_local_results(v))\n         else:\n             raise ValueError(\n                 '`reduction` must be \"first\", \"concat\", \"sum\", or \"auto\". '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a129b6eb8fce9691a3ddc623fe6657f8741df759", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 90 | Churn Cumulative: 1875 | Contributors (this commit): 18 | Commits (past 90d): 10 | Contributors (cumulative): 20 | DMM Complexity: 0.9433962264150944\n\nDIFF:\n@@ -79,6 +79,26 @@ class Constraint:\n         \"\"\"\n         return {}\n \n+    @classmethod\n+    def from_config(cls, config):\n+        \"\"\"Instantiates a weight constraint from a configuration dictionary.\n+\n+        Example:\n+\n+        ```python\n+        constraint = UnitNorm()\n+        config = constraint.get_config()\n+        constraint = UnitNorm.from_config(config)\n+        ```\n+\n+        Args:\n+          config: A Python dictionary, the output of `get_config`.\n+\n+        Returns:\n+          A `tf.keras.constraints.Constraint` instance.\n+        \"\"\"\n+        return cls(**config)\n+\n \n @keras_export(\"keras.constraints.MaxNorm\", \"keras.constraints.max_norm\")\n class MaxNorm(Constraint):\n\n@@ -245,9 +245,14 @@ def deserialize_keras_object(config, custom_objects=None):\n       The object described by the `config` dictionary.\n \n     \"\"\"\n+    custom_objects = custom_objects or {}\n     if config is None:\n         return None\n     if isinstance(config, PLAIN_TYPES):\n+        if isinstance(config, str) and custom_objects.get(config) is not None:\n+            # This is to deserialize plain functions which are serialized as\n+            # string names by legacy saving formats.\n+            return custom_objects[config]\n         return config\n     if isinstance(config, (list, tuple)):\n         return [\n@@ -265,7 +270,6 @@ def deserialize_keras_object(config, custom_objects=None):\n \n     class_name = config[\"class_name\"]\n     inner_config = config[\"config\"]\n-    custom_objects = custom_objects or {}\n \n     # Special cases:\n     if class_name == \"__tensor__\":\n@@ -279,8 +283,8 @@ def deserialize_keras_object(config, custom_objects=None):\n     # deserialized as lists).\n \n     # Below: classes and functions.\n-    module = config[\"module\"]\n-    registered_name = config[\"registered_name\"]\n+    module = config.get(\"module\", None)\n+    registered_name = config.get(\"registered_name\", class_name)\n \n     if class_name == \"function\":\n         fn_name = inner_config\n@@ -305,7 +309,7 @@ def deserialize_keras_object(config, custom_objects=None):\n     if not hasattr(cls, \"from_config\"):\n         raise TypeError(\n             f\"Unable to reconstruct an instance of '{class_name}' because \"\n-            \"it is missing a `from_config()` method. \"\n+            f\"the class is missing a `from_config()` method. \"\n             f\"Full object config: {config}\"\n         )\n     # Instantiate the class from its config inside a custom object scope\n@@ -325,6 +329,7 @@ def _retrieve_class_or_fn(\n     if custom_obj is not None:\n         return custom_obj\n \n+    if module:\n         # If it's a Keras built-in object,\n         # we cannot always use direct import, because the exported\n         # module name might not match the package structure\n\n@@ -23,6 +23,7 @@ from absl.testing import parameterized\n import keras\n from keras.saving.experimental import serialization_lib\n from keras.testing_infra import test_utils\n+from keras.utils import generic_utils\n \n \n def custom_fn(x):\n@@ -107,6 +108,10 @@ class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n         serialized, _, reserialized = self.roundtrip(obj)\n         self.assertEqual(serialized, reserialized)\n \n+    def test_builtin_layers(self):\n+        serialized, _, reserialized = self.roundtrip(keras.layers.Dense(3))\n+        self.assertEqual(serialized, reserialized)\n+\n     def test_tensors_and_tensorshape(self):\n         x = tf.random.normal((2, 2), dtype=\"float64\")\n         obj = {\"x\": x}\n@@ -178,5 +183,57 @@ class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertIs(new_model.layers[2], new_model.layers[3].layer)\n \n \n+@test_utils.run_v2_only\n+class BackwardsCompatibilityTest(tf.test.TestCase, parameterized.TestCase):\n+    def assert_old_format_can_be_deserialized(self, obj, custom_objects=None):\n+        old_config = generic_utils.serialize_keras_object(obj)\n+        revived = serialization_lib.deserialize_keras_object(\n+            old_config, custom_objects=custom_objects\n+        )\n+        new_config_1 = serialization_lib.serialize_keras_object(obj)\n+        new_config_2 = serialization_lib.serialize_keras_object(revived)\n+        self.assertEqual(new_config_1, new_config_2)\n+\n+    def test_backwards_compatibility_with_old_serialized_format(self):\n+        optimizer = keras.optimizers.Adam(learning_rate=0.1)\n+        self.assert_old_format_can_be_deserialized(\n+            optimizer, custom_objects=vars(keras.optimizers)\n+        )\n+        activation = keras.activations.relu\n+        self.assert_old_format_can_be_deserialized(\n+            activation, custom_objects=vars(keras.activations)\n+        )\n+        initializer = keras.initializers.VarianceScaling(scale=2.0)\n+        self.assert_old_format_can_be_deserialized(\n+            initializer, custom_objects=vars(keras.initializers)\n+        )\n+        regularizer = keras.regularizers.L2(0.3)\n+        self.assert_old_format_can_be_deserialized(\n+            regularizer, custom_objects=vars(keras.regularizers)\n+        )\n+        constraint = keras.constraints.UnitNorm()\n+        self.assert_old_format_can_be_deserialized(\n+            constraint, custom_objects=vars(keras.constraints)\n+        )\n+        layer = keras.layers.Dense(2)\n+        self.assert_old_format_can_be_deserialized(\n+            layer, custom_objects=vars(keras.layers)\n+        )\n+        layer = keras.layers.MultiHeadAttention(2, 4)\n+        self.assert_old_format_can_be_deserialized(\n+            layer, custom_objects=vars(keras.layers)\n+        )\n+\n+        # Custom objects\n+        layer = CustomLayer(2)\n+        self.assert_old_format_can_be_deserialized(\n+            layer, custom_objects={\"CustomLayer\": CustomLayer}\n+        )\n+        layer = keras.layers.Dense(1, activation=custom_fn)\n+        self.assert_old_format_can_be_deserialized(\n+            layer, custom_objects={**vars(keras.layers), \"custom_fn\": custom_fn}\n+        )\n+\n+\n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#095a59ee9c95a28ed0221346ce067bb35493f45e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 852 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -337,8 +337,10 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n             config_dict[\"registered_name\"], \"my_custom_package>CustomModelX\"\n         )\n         self.assertEqual(\n-            config_dict[\"config\"][\"compile_config\"][\"optimizer\"][\"module\"],\n-            \"keras.optimizers.experimental\",\n+            config_dict[\"config\"][\"compile_config\"][\"optimizer\"][\"config\"][\n+                \"is_legacy_optimizer\"\n+            ],\n+            False,\n         )\n         self.assertEqual(\n             config_dict[\"config\"][\"compile_config\"][\"optimizer\"][\"class_name\"],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ead59b2c4c85284d8c2095e691800255068694ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 8 | Files Changed: 4 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/2 | Churn Δ: 62 | Churn Cumulative: 26116 | Contributors (this commit): 142 | Commits (past 90d): 77 | Contributors (cumulative): 169 | DMM Complexity: 0.7586206896551724\n\nDIFF:\n@@ -485,6 +485,12 @@ class Sequential(functional.Functional):\n                     compile_config, base_class=Sequential\n                 )\n \n+            if build_input_shape:\n+                model.build(build_input_shape)\n+                if model.optimizer is not None:\n+                    model.optimizer.build(model.trainable_variables)\n+\n+        else:\n             if (\n                 not model.inputs\n                 and build_input_shape\n\n@@ -3140,6 +3140,17 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 if compile_config is not None:\n                     model._compile_from_config(compile_config, base_class=Model)\n \n+                    # Checking the existence of optimizer attribute because\n+                    # `compile()` may not have been called (if overridden).\n+                    if model.optimizer is not None:\n+                        # To bring the optimizer's state back to when it was\n+                        # saved, we build it so that the variables are created\n+                        # (and available for further state loading). Otherwise,\n+                        # the optimizer's variables are not there until the next\n+                        # time `Model.fit()` or `optimizer.apply_gradient()` is\n+                        # called.\n+                        model.optimizer.build(model.trainable_variables)\n+\n             return model\n \n     def to_json(self, **kwargs):\n\n@@ -724,14 +724,14 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     def _get_state(self):\n         \"\"\"Get the state of this optimizer object.\"\"\"\n         result = {}\n-        for variable in self.variables():\n-            result[variable.name] = variable.numpy()\n+        for k, variable in enumerate(self.variables()):\n+            result[str(k)] = variable.numpy()\n         return result\n \n     def _set_state(self, state):\n         \"\"\"Set the state of this optimizer object.\"\"\"\n-        for variable in self.variables():\n-            variable.assign(state[variable.name])\n+        for k, variable in enumerate(self.variables()):\n+            variable.assign(state[str(k)])\n \n     def _save_state(self, dir_path):\n         file_path = tf.io.gfile.join(dir_path, \"state.npz\")\n\n@@ -43,11 +43,13 @@ class MyDense(keras.layers.Dense):\n         self.additional_weights = [\n             self.add_weight(\n                 \"my_additional_weight\",\n+                shape=(1, 1),\n                 initializer=\"ones\",\n                 trainable=True,\n             ),\n             self.add_weight(\n                 \"my_additional_weight_2\",\n+                shape=(1, 1),\n                 initializer=\"ones\",\n                 trainable=True,\n             ),\n@@ -55,6 +57,7 @@ class MyDense(keras.layers.Dense):\n         self.weights_in_dict = {\n             \"my_weight\": self.add_weight(\n                 \"my_dict_weight\",\n+                shape=(1, 1),\n                 initializer=\"ones\",\n                 trainable=True,\n             ),\n@@ -63,8 +66,11 @@ class MyDense(keras.layers.Dense):\n         return super().build(input_shape)\n \n     def call(self, inputs):\n-        call_result = super().call(inputs)\n-        return self.nested_layer(call_result)\n+        outputs = super().call(inputs)\n+        outputs = self.nested_layer(outputs)\n+        outputs = tf.matmul(outputs, self.additional_weights[0])\n+        outputs = tf.matmul(outputs, self.additional_weights[1])\n+        return tf.matmul(outputs, self.weights_in_dict[\"my_weight\"])\n \n     def two(self):\n         return 2\n@@ -430,8 +436,8 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n \n         # Mutate the `Dense` layer custom weights to ensure that list and\n         # dict-contained weights get restored.\n-        model.layers[1].additional_weights[0].assign(2)\n-        model.layers[1].weights_in_dict[\"my_weight\"].assign(2)\n+        model.layers[1].additional_weights[0].assign([[2]])\n+        model.layers[1].weights_in_dict[\"my_weight\"].assign([[2]])\n         model.layers[1].nested_layer.kernel.assign([[1]])\n \n         model._save_experimental(temp_filepath)\n@@ -495,6 +501,29 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n         self.assertIn(\"keras_version\", metadata)\n         self.assertIn(\"date_saved\", metadata)\n \n+    @tf.__internal__.distribute.combinations.generate(\n+        tf.__internal__.test.combinations.combine(\n+            model_type=[\"subclassed\", \"functional\", \"sequential\"],\n+        )\n+    )\n+    def test_saving_optimizer_variables(self, model_type):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n+        model = getattr(self, f\"_get_{model_type}_model\")()\n+        x = np.random.random((100, 32))\n+        y = np.random.random((100, 1))\n+        model.fit(x, y, epochs=1)\n+        model._save_experimental(temp_filepath)\n+        loaded_model = saving_lib.load_model(temp_filepath)\n+\n+        self.assertEqual(\n+            len(model.optimizer.variables()),\n+            len(loaded_model.optimizer.variables()),\n+        )\n+        for original_weights, loaded_weights in zip(\n+            model.optimizer.variables(), loaded_model.optimizer.variables()\n+        ):\n+            np.testing.assert_allclose(original_weights, loaded_weights)\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#38c8ba914d7d105268915fb67b10ea99a42e6ca1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1416 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -354,8 +354,8 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n         # explicitly cast here to also allow integer inputs to be passed\n         inputs = tf.cast(inputs, self.compute_dtype)\n         if self.invert:\n-            return (inputs + self.mean) * tf.maximum(\n-                tf.sqrt(self.variance), backend.epsilon()\n+            return self.mean + (inputs * tf.maximum(\n+                tf.sqrt(self.variance), backend.epsilon())\n             )\n         else:\n             return (inputs - self.mean) / tf.maximum(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1635b640b7bb41ef386aecf794d14cda2c340576", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 4 | Hunks: 11 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 4471 | Contributors (this commit): 19 | Commits (past 90d): 24 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -376,12 +376,12 @@ class BaseLossScaleOptimizer(metaclass=LossScaleOptimizerMetaclass):\n     to do is wrap your optimizer with a `LossScaleOptimizer` if you use\n     `minimize`. For example:\n \n-    >>> opt = tf.keras.optimizers.SGD(0.25)\n+    >>> opt = tf.keras.optimizers.experimental.SGD(0.25)\n     >>> opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n     >>> var = tf.Variable(1.)\n     >>> loss_fn = lambda: var ** 2\n     >>> # 'minimize' applies loss scaling and updates the loss sale.\n-    >>> opt.minimize(loss_fn, var_list=var)\n+    >>> opt.minimize(loss_fn, var_list=[var])\n     >>> var.numpy()\n     0.5\n \n@@ -454,7 +454,7 @@ class BaseLossScaleOptimizer(metaclass=LossScaleOptimizerMetaclass):\n     accessed and set on the LossScaleOptimizer, which will be delegated to the\n     wrapped optimizer.\n \n-    >>> opt = tf.keras.optimizers.Adam(beta_1=0.8, epsilon=1e-5)\n+    >>> opt = tf.keras.optimizers.legacy.Adam(beta_1=0.8, epsilon=1e-5)\n     >>> opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n     >>> opt.beta_1  # Equivalent to `opt.inner_optimizer.beta_1`\n     0.8\n\n@@ -91,7 +91,7 @@ def serialize(optimizer):\n     The configuration can be used for persistence and reconstruct the\n     `Optimizer` instance again.\n \n-    >>> tf.keras.optimizers.serialize(tf.keras.optimizers.SGD())\n+    >>> tf.keras.optimizers.serialize(tf.keras.optimizers.legacy.SGD())\n     {'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,\n                                      'decay': 0.0, 'momentum': 0.0,\n                                      'nesterov': False}}\n\n@@ -58,10 +58,10 @@ class RMSprop(optimizer.Optimizer):\n \n     Usage:\n \n-    >>> opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n+    >>> opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)\n     >>> var1 = tf.Variable(10.0)\n     >>> loss = lambda: (var1 ** 2) / 2.0  # d(loss) / d(var1) = var1\n-    >>> step_count = opt.minimize(loss, [var1]).numpy()\n+    >>> opt.minimize(loss, [var1])\n     >>> var1.numpy()\n     9.683772\n \n\n@@ -62,25 +62,25 @@ class SGD(optimizer.Optimizer):\n \n     Usage:\n \n-    >>> opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n+    >>> opt = tf.keras.optimizers.experimental.SGD(learning_rate=0.1)\n     >>> var = tf.Variable(1.0)\n     >>> loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n-    >>> step_count = opt.minimize(loss, [var]).numpy()\n+    >>> opt.minimize(loss, [var])\n     >>> # Step is `- learning_rate * grad`\n     >>> var.numpy()\n     0.9\n \n-    >>> opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n+    >>> opt = tf.keras.optimizers.experimental.SGD(0.1, momentum=0.9)\n     >>> var = tf.Variable(1.0)\n     >>> val0 = var.value()\n     >>> loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n     >>> # First step is `- learning_rate * grad`\n-    >>> step_count = opt.minimize(loss, [var]).numpy()\n+    >>> opt.minimize(loss, [var])\n     >>> val1 = var.value()\n     >>> (val0 - val1).numpy()\n     0.1\n     >>> # On later steps, step-size increases because of momentum\n-    >>> step_count = opt.minimize(loss, [var]).numpy()\n+    >>> opt.minimize(loss, [var])\n     >>> val2 = var.value()\n     >>> (val1 - val2).numpy()\n     0.18\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8cf91871ce167d63069c99120f8580a4976a59d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 20304 | Contributors (this commit): 137 | Commits (past 90d): 33 | Contributors (cumulative): 137 | DMM Complexity: None\n\nDIFF:\n@@ -1117,10 +1117,13 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"\n         del x  # The default implementation does not use `x`.\n         self.compiled_metrics.update_state(y, y_pred, sample_weight)\n-        return self._get_metrics_result()\n+        return self.get_metrics_result()\n \n-    def _get_metrics_result(self):\n-        \"\"\"Returns model metrics as a dict.\n+    def get_metrics_result(self):\n+        \"\"\"Returns the model's metrics values as a dict.\n+\n+        If any of the metric result is a dict (containing multiple metrics),\n+        each of them gets added to the top level returned dict of this method.\n \n         Returns:\n           A `dict` containing values of the metrics listed in `self.metrics`.\n@@ -1166,7 +1169,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         Using the results of last step function could lead to incorrect \\\n         results when used with ParameterServerStrategy\"\n         try:\n-            metric_logs = self._get_metrics_result()\n+            metric_logs = self.get_metrics_result()\n         except TypeError:\n             if self._cluster_coordinator:\n                 logging.warning(PSS_WARN_MSG)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b3505ddac4a7f1d20ae28fd6474a96ca9e37b3fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 525 | Lines Deleted: 500 | Files Changed: 35 | Hunks: 149 | Methods Changed: 85 | Complexity Δ (Sum/Max): -5/21 | Churn Δ: 1025 | Churn Cumulative: 43178 | Contributors (this commit): 85 | Commits (past 90d): 98 | Contributors (cumulative): 313 | DMM Complexity: 0.0\n\nDIFF:\n@@ -25,7 +25,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n+from keras.utils import get_custom_objects\n \n \n # `tf.print` message is only available in stderr in TF2, which this test checks.\n@@ -36,7 +36,7 @@ class CustomObjectSavingTest(tf.test.TestCase, parameterized.TestCase):\n \n     def setUp(self):\n         super().setUp()\n-        generic_utils.get_custom_objects().clear()\n+        get_custom_objects().clear()\n \n     def test_register_keras_serializable_correct_class(self):\n         train_step_message = \"This is my training step\"\n\n@@ -29,7 +29,7 @@ from keras.layers.preprocessing import index_lookup\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.utils.generic_utils import CustomObjectScope\n+from keras.utils import CustomObjectScope\n \n \n def zip_and_sort(weight_values):\n\n@@ -29,7 +29,7 @@ from keras.layers.preprocessing import preprocessing_test_utils\n from keras.layers.preprocessing import text_vectorization\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n+from keras.utils import register_keras_serializable\n \n \n def _get_end_to_end_test_cases():\n@@ -2187,12 +2187,12 @@ class TextVectorizationErrorTest(\n \n # Custom functions for the custom callable serialization test. Declared here\n # to avoid multiple registrations from run_all_keras_modes().\n-@generic_utils.register_keras_serializable(package=\"Test\")\n+@register_keras_serializable(package=\"Test\")\n def custom_standardize_fn(x):\n     return tf.strings.lower(x)\n \n \n-@generic_utils.register_keras_serializable(package=\"Test\")\n+@register_keras_serializable(package=\"Test\")\n def custom_split_fn(x):\n     return tf.strings.split(x, sep=\">\")\n \n\n@@ -32,7 +32,6 @@ from keras.layers.rnn import lstm\n from keras.layers.rnn import lstm_v1\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n \n # isort: off\n from tensorflow.python.checkpoint import (\n@@ -188,9 +187,7 @@ class RNNTest(test_combinations.TestCase):\n         y_np = model.predict(x_np)\n         weights = model.get_weights()\n         config = layer.get_config()\n-        with generic_utils.CustomObjectScope(\n-            {\"MinimalRNNCell\": MinimalRNNCell}\n-        ):\n+        with keras.utils.CustomObjectScope({\"MinimalRNNCell\": MinimalRNNCell}):\n             layer = keras.layers.RNN.from_config(config)\n         y = layer(x)\n         model = keras.models.Model(x, y)\n@@ -215,9 +212,7 @@ class RNNTest(test_combinations.TestCase):\n         y_np = model.predict(x_np)\n         weights = model.get_weights()\n         config = layer.get_config()\n-        with generic_utils.CustomObjectScope(\n-            {\"MinimalRNNCell\": MinimalRNNCell}\n-        ):\n+        with keras.utils.CustomObjectScope({\"MinimalRNNCell\": MinimalRNNCell}):\n             layer = keras.layers.RNN.from_config(config)\n         y = layer(x)\n         model = keras.models.Model(x, y)\n@@ -423,7 +418,7 @@ class RNNTest(test_combinations.TestCase):\n         weights = model.get_weights()\n         config = layer.get_config()\n         custom_objects = {\"RNNCellWithConstants\": RNNCellWithConstants}\n-        with generic_utils.CustomObjectScope(custom_objects):\n+        with keras.utils.CustomObjectScope(custom_objects):\n             layer = keras.layers.RNN.from_config(config.copy())\n         y = layer(x, constants=c)\n         model = keras.models.Model([x, c], y)\n@@ -432,7 +427,7 @@ class RNNTest(test_combinations.TestCase):\n         self.assertAllClose(y_np, y_np_2, atol=1e-4)\n \n         # test flat list inputs.\n-        with generic_utils.CustomObjectScope(custom_objects):\n+        with keras.utils.CustomObjectScope(custom_objects):\n             layer = keras.layers.RNN.from_config(config.copy())\n         y = layer([x, c])\n         model = keras.models.Model([x, c], y)\n@@ -480,7 +475,7 @@ class RNNTest(test_combinations.TestCase):\n         y_np = model.predict([x_np, c_np])\n         weights = model.get_weights()\n         config = layer.get_config()\n-        with generic_utils.CustomObjectScope(custom_objects):\n+        with keras.utils.CustomObjectScope(custom_objects):\n             layer = keras.layers.RNN.from_config(config.copy())\n         y = layer(x, constants=c)\n         model = keras.models.Model([x, c], y)\n@@ -547,7 +542,7 @@ class RNNTest(test_combinations.TestCase):\n         weights = model.get_weights()\n         config = layer.get_config()\n         custom_objects = {\"RNNCellWithConstants\": RNNCellWithConstants}\n-        with generic_utils.CustomObjectScope(custom_objects):\n+        with keras.utils.CustomObjectScope(custom_objects):\n             layer = keras.layers.RNN.from_config(config.copy())\n         y = layer(x, initial_state=s, constants=c)\n         model = keras.models.Model([x, s, c], y)\n@@ -561,7 +556,7 @@ class RNNTest(test_combinations.TestCase):\n             self.assertAllClose(y_np, y_np_2_different_s, atol=1e-4)\n \n         # test flat list inputs\n-        with generic_utils.CustomObjectScope(custom_objects):\n+        with keras.utils.CustomObjectScope(custom_objects):\n             layer = keras.layers.RNN.from_config(config.copy())\n         y = layer([x, s, c])\n         model = keras.models.Model([x, s, c], y)\n@@ -707,9 +702,7 @@ class RNNTest(test_combinations.TestCase):\n         model.predict(inputs)\n \n     def test_builtin_and_custom_rnn_cell_serialization(self):\n-        @keras.utils.generic_utils.register_keras_serializable(\n-            package=\"TestOnly\"\n-        )\n+        @keras.utils.register_keras_serializable(package=\"TestOnly\")\n         class CustomRNNCell(keras.layers.Layer):\n             def __init__(self, units, **kwargs):\n                 self.units = units\n\n@@ -27,7 +27,6 @@ from keras.layers import core\n from keras.layers.rnn.cell_wrappers import ResidualWrapper\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n \n # isort: off\n from tensorflow.python.checkpoint import (\n@@ -505,7 +504,7 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n             c = keras.Input((3,))\n             cell = _RNNCellWithConstants(32, 3)\n             custom_objects = {\"_RNNCellWithConstants\": _RNNCellWithConstants}\n-            with generic_utils.CustomObjectScope(custom_objects):\n+            with keras.utils.CustomObjectScope(custom_objects):\n                 layer = keras.layers.Bidirectional(keras.layers.RNN(cell))\n             y = layer(x, constants=c)\n             model = keras.Model([x, c], y)\n@@ -521,7 +520,7 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n             weights = model.get_weights()\n             config = layer.get_config()\n \n-            with generic_utils.CustomObjectScope(custom_objects):\n+            with keras.utils.CustomObjectScope(custom_objects):\n                 layer = keras.layers.Bidirectional.from_config(\n                     copy.deepcopy(config)\n                 )\n@@ -532,7 +531,7 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n             self.assertAllClose(y_np, y_np_2, atol=1e-4)\n \n             # Test flat list inputs\n-            with generic_utils.CustomObjectScope(custom_objects):\n+            with keras.utils.CustomObjectScope(custom_objects):\n                 layer = keras.layers.Bidirectional.from_config(\n                     copy.deepcopy(config)\n                 )\n@@ -551,7 +550,7 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n             s_bac = keras.Input((32,))\n             cell = _RNNCellWithConstants(32, 3)\n             custom_objects = {\"_RNNCellWithConstants\": _RNNCellWithConstants}\n-            with generic_utils.CustomObjectScope(custom_objects):\n+            with keras.utils.CustomObjectScope(custom_objects):\n                 layer = keras.layers.Bidirectional(keras.layers.RNN(cell))\n             y = layer(x, initial_state=[s_for, s_bac], constants=c)\n             model = keras.Model([x, s_for, s_bac, c], y)\n@@ -575,7 +574,7 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n             weights = model.get_weights()\n             config = layer.get_config()\n \n-            with generic_utils.CustomObjectScope(custom_objects):\n+            with keras.utils.CustomObjectScope(custom_objects):\n                 layer = keras.layers.Bidirectional.from_config(\n                     copy.deepcopy(config)\n                 )\n@@ -592,7 +591,7 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n             assert np.mean(y_np - y_np_2_different_s) != 0\n \n             # Test flat list inputs\n-            with generic_utils.CustomObjectScope(custom_objects):\n+            with keras.utils.CustomObjectScope(custom_objects):\n                 layer = keras.layers.Bidirectional.from_config(\n                     copy.deepcopy(config)\n                 )\n\n@@ -23,7 +23,6 @@ import tensorflow.compat.v2 as tf\n \n from keras import backend\n from keras.saving.experimental import saving_lib\n-from keras.utils import generic_utils\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n from keras.utils.generic_utils import deserialize_keras_object\n@@ -279,7 +278,9 @@ class LossFunctionWrapper(Loss):\n             )\n \n         if getattr(saving_lib._SAVING_V3_ENABLED, \"value\", False):\n-            config[\"fn\"] = generic_utils.get_registered_name(self.fn)\n+            from keras.utils import get_registered_name\n+\n+            config[\"fn\"] = get_registered_name(self.fn)\n \n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -425,7 +426,7 @@ class MeanAbsoluteError(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.MeanAbsolutePercentageError\")\n class MeanAbsolutePercentageError(LossFunctionWrapper):\n-    \"\"\"Computes the mean absolute percentage error between `y_true` and `y_pred`.\n+    \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n \n     Formula:\n \n@@ -495,7 +496,7 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.MeanSquaredLogarithmicError\")\n class MeanSquaredLogarithmicError(LossFunctionWrapper):\n-    \"\"\"Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n+    \"\"\"Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n \n     `loss = square(log(y_true + 1.) - log(y_pred + 1.))`\n \n@@ -1020,7 +1021,7 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.Hinge\")\n class Hinge(LossFunctionWrapper):\n-    \"\"\"Computes the hinge loss between `y_true` and `y_pred`.\n+    \"\"\"Computes the hinge loss between `y_true` & `y_pred`.\n \n     `loss = maximum(1 - y_true * y_pred, 0)`\n \n@@ -1080,7 +1081,7 @@ class Hinge(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.SquaredHinge\")\n class SquaredHinge(LossFunctionWrapper):\n-    \"\"\"Computes the squared hinge loss between `y_true` and `y_pred`.\n+    \"\"\"Computes the squared hinge loss between `y_true` & `y_pred`.\n \n     `loss = square(maximum(1 - y_true * y_pred, 0))`\n \n@@ -1142,7 +1143,7 @@ class SquaredHinge(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.CategoricalHinge\")\n class CategoricalHinge(LossFunctionWrapper):\n-    \"\"\"Computes the categorical hinge loss between `y_true` and `y_pred`.\n+    \"\"\"Computes the categorical hinge loss between `y_true` & `y_pred`.\n \n     `loss = maximum(neg - pos + 1, 0)`\n     where `neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred)`\n@@ -1202,7 +1203,7 @@ class CategoricalHinge(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.Poisson\")\n class Poisson(LossFunctionWrapper):\n-    \"\"\"Computes the Poisson loss between `y_true` and `y_pred`.\n+    \"\"\"Computes the Poisson loss between `y_true` & `y_pred`.\n \n     `loss = y_pred - y_true * log(y_pred)`\n \n@@ -1319,7 +1320,7 @@ class LogCosh(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.KLDivergence\")\n class KLDivergence(LossFunctionWrapper):\n-    \"\"\"Computes Kullback-Leibler divergence loss between `y_true` and `y_pred`.\n+    \"\"\"Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.\n \n     `loss = y_true * log(y_true / y_pred)`\n \n@@ -1380,7 +1381,7 @@ class KLDivergence(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.Huber\")\n class Huber(LossFunctionWrapper):\n-    \"\"\"Computes the Huber loss between `y_true` and `y_pred`.\n+    \"\"\"Computes the Huber loss between `y_true` & `y_pred`.\n \n     For each value x in `error = y_true - y_pred`:\n \n@@ -1643,7 +1644,7 @@ def _ragged_tensor_mae(y_true, y_pred):\n )\n @tf.__internal__.dispatch.add_dispatch_support\n def mean_absolute_percentage_error(y_true, y_pred):\n-    \"\"\"Computes the mean absolute percentage error between `y_true` and `y_pred`.\n+    \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n \n     `loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)`\n \n@@ -1692,7 +1693,7 @@ def _ragged_tensor_mape(y_true, y_pred):\n )\n @tf.__internal__.dispatch.add_dispatch_support\n def mean_squared_logarithmic_error(y_true, y_pred):\n-    \"\"\"Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n+    \"\"\"Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n \n     `loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1)`\n \n@@ -1753,7 +1754,7 @@ def _maybe_convert_labels(y_true):\n @keras_export(\"keras.metrics.squared_hinge\", \"keras.losses.squared_hinge\")\n @tf.__internal__.dispatch.add_dispatch_support\n def squared_hinge(y_true, y_pred):\n-    \"\"\"Computes the squared hinge loss between `y_true` and `y_pred`.\n+    \"\"\"Computes the squared hinge loss between `y_true` & `y_pred`.\n \n     `loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1)`\n \n@@ -1787,7 +1788,7 @@ def squared_hinge(y_true, y_pred):\n @keras_export(\"keras.metrics.hinge\", \"keras.losses.hinge\")\n @tf.__internal__.dispatch.add_dispatch_support\n def hinge(y_true, y_pred):\n-    \"\"\"Computes the hinge loss between `y_true` and `y_pred`.\n+    \"\"\"Computes the hinge loss between `y_true` & `y_pred`.\n \n     `loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1)`\n \n@@ -1819,7 +1820,7 @@ def hinge(y_true, y_pred):\n @keras_export(\"keras.losses.categorical_hinge\")\n @tf.__internal__.dispatch.add_dispatch_support\n def categorical_hinge(y_true, y_pred):\n-    \"\"\"Computes the categorical hinge loss between `y_true` and `y_pred`.\n+    \"\"\"Computes the categorical hinge loss between `y_true` & `y_pred`.\n \n     `loss = maximum(neg - pos + 1, 0)`\n     where `neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred)`\n@@ -2353,7 +2354,7 @@ def _ragged_tensor_binary_focal_crossentropy(\n )\n @tf.__internal__.dispatch.add_dispatch_support\n def kl_divergence(y_true, y_pred):\n-    \"\"\"Computes Kullback-Leibler divergence loss between `y_true` and `y_pred`.\n+    \"\"\"Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.\n \n     `loss = y_true * log(y_true / y_pred)`\n \n\n@@ -42,10 +42,10 @@ from keras.mixed_precision import policy\n from keras.mixed_precision import test_util as mp_test_util\n from keras.optimizers import optimizer_v1\n from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.saving import object_registration\n from keras.saving import save\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n \n # If called outside any strategy.scope() calls, this will return the default\n # strategy.\n@@ -235,7 +235,7 @@ class KerasModelTest(test_combinations.TestCase):\n         self.assertEqual(backend.eval(layer.v), expected)\n \n         if save_format:\n-            with generic_utils.CustomObjectScope(\n+            with object_registration.CustomObjectScope(\n                 {\n                     \"MultiplyLayer\": mp_test_util.MultiplyLayer,\n                     \"loss_fn\": loss_fn,\n\n@@ -28,9 +28,9 @@ from keras.engine.base_layer import Layer\n from keras.engine.input_layer import Input\n from keras.engine.input_layer import InputLayer\n from keras.optimizers import optimizer_v1\n+from keras.saving.object_registration import CustomObjectScope\n from keras.utils import generic_utils\n from keras.utils import version_utils\n-from keras.utils.generic_utils import CustomObjectScope\n \n # isort: off\n from tensorflow.python.platform import tf_logging as logging\n@@ -64,7 +64,7 @@ def _insert_ancillary_layers(model, ancillary_layers, metrics_names, new_nodes):\n \n \n def _make_new_nodes(nodes_by_depth, layer_fn, layer_map, tensor_map):\n-    \"\"\"Uses the layers in `layer_map` to make new nodes based on `nodes_by_depth`.\n+    \"\"\"Make new nodes with the layers in `layer_map` based on `nodes_by_depth`.\n \n     Args:\n       nodes_by_depth: Provides structure information to create new nodes.\n@@ -241,7 +241,7 @@ def _clone_functional_model(model, input_tensors=None, layer_fn=_clone_layer):\n \n \n def _clone_layers_and_model_config(model, input_layers, layer_fn):\n-    \"\"\"Clones all layers, and returns the model config without serializing layers.\n+    \"\"\"Clones all layers; returns the model config without serializing layers.\n \n     This function ensures that only the node graph is retrieved when getting the\n     model config. The `layer_fn` used to clone layers might not rely on\n\n@@ -21,13 +21,14 @@ import tensorflow.compat.v2 as tf\n from keras.engine import data_adapter\n from keras.layers import deserialize as deserialize_layer\n from keras.models import Model\n-from keras.utils import generic_utils\n+from keras.saving.object_registration import register_keras_serializable\n+from keras.utils.generic_utils import serialize_keras_object\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@generic_utils.register_keras_serializable()\n+@register_keras_serializable()\n @keras_export(\"keras.models.experimental.SharpnessAwareMinimization\", v1=[])\n class SharpnessAwareMinimization(Model):\n     \"\"\"Sharpness aware minimization (SAM) training flow.\n@@ -143,7 +144,7 @@ class SharpnessAwareMinimization(Model):\n         config = super().get_config()\n         config.update(\n             {\n-                \"model\": generic_utils.serialize_keras_object(self.model),\n+                \"model\": serialize_keras_object(self.model),\n                 \"rho\": self.rho,\n             }\n         )\n\n@@ -77,7 +77,6 @@ from keras.optimizers.optimizer_v2.gradient_descent import SGD\n from keras.optimizers.optimizer_v2.nadam import Nadam\n from keras.optimizers.optimizer_v2.rmsprop import RMSprop\n from keras.utils.generic_utils import deserialize_keras_object\n-from keras.utils.generic_utils import get_registered_name\n from keras.utils.generic_utils import serialize_keras_object\n \n # isort: off\n\n@@ -17,13 +17,13 @@\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.utils import generic_utils\n+from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@generic_utils.register_keras_serializable()\n+@register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Adadelta\", v1=[])\n class Adadelta(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Adadelta algorithm.\n\n@@ -18,13 +18,13 @@ import tensorflow.compat.v2 as tf\n \n from keras import initializers\n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.utils import generic_utils\n+from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@generic_utils.register_keras_serializable()\n+@register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Adagrad\", v1=[])\n class Adagrad(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Adagrad algorithm.\n\n@@ -17,13 +17,13 @@\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.utils import generic_utils\n+from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@generic_utils.register_keras_serializable()\n+@register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Adam\", v1=[])\n class Adam(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Adam algorithm.\n\n@@ -17,13 +17,13 @@\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.utils import generic_utils\n+from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@generic_utils.register_keras_serializable()\n+@register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Adamax\", v1=[])\n class Adamax(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Adamax algorithm.\n\n@@ -19,13 +19,13 @@ import re\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.utils import generic_utils\n+from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@generic_utils.register_keras_serializable()\n+@register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.AdamW\", v1=[])\n class AdamW(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the AdamW algorithm.\n\n@@ -17,13 +17,13 @@\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.utils import generic_utils\n+from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@generic_utils.register_keras_serializable()\n+@register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Ftrl\", v1=[])\n class Ftrl(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the FTRL algorithm.\n\n@@ -17,13 +17,13 @@\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.utils import generic_utils\n+from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@generic_utils.register_keras_serializable()\n+@register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Nadam\", v1=[])\n class Nadam(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Nadam algorithm.\n\n@@ -17,13 +17,13 @@\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.utils import generic_utils\n+from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@generic_utils.register_keras_serializable()\n+@register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.RMSprop\", v1=[])\n class RMSprop(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the RMSprop algorithm.\n\n@@ -17,13 +17,13 @@\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.utils import generic_utils\n+from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@generic_utils.register_keras_serializable()\n+@register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.SGD\", v1=[])\n class SGD(optimizer.Optimizer):\n     r\"\"\"Gradient descent (with momentum) optimizer.\n\n@@ -26,18 +26,16 @@ from tensorflow.python.platform import tf_logging as logging\n import keras\n from keras import backend\n from keras.optimizers.optimizer_experimental import adam\n+from keras.saving import object_registration\n from keras.saving.experimental import saving_lib\n from keras.saving.saved_model import json_utils\n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n from keras.utils import io_utils\n \n train_step_message = \"This is my training step\"\n \n \n-@keras.utils.generic_utils.register_keras_serializable(\n-    package=\"my_custom_package\"\n-)\n+@keras.utils.register_keras_serializable(package=\"my_custom_package\")\n class MyDense(keras.layers.Dense):\n     def build(self, input_shape):\n         self.additional_weights = [\n@@ -76,9 +74,7 @@ class MyDense(keras.layers.Dense):\n         return 2\n \n \n-@keras.utils.generic_utils.register_keras_serializable(\n-    package=\"my_custom_package\"\n-)\n+@keras.utils.register_keras_serializable(package=\"my_custom_package\")\n class CustomModelX(keras.Model):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -106,9 +102,7 @@ class CustomModelX(keras.Model):\n         return 1\n \n \n-@keras.utils.generic_utils.register_keras_serializable(\n-    package=\"my_custom_package\"\n-)\n+@keras.utils.register_keras_serializable(package=\"my_custom_package\")\n class CompileOverridingModel(keras.Model):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -121,17 +115,13 @@ class CompileOverridingModel(keras.Model):\n         return self.dense1(inputs)\n \n \n-@keras.utils.generic_utils.register_keras_serializable(\n-    package=\"my_custom_package\"\n-)\n+@keras.utils.register_keras_serializable(package=\"my_custom_package\")\n class CompileOverridingSequential(keras.Sequential):\n     def compile(self, *args, **kwargs):\n         super().compile(*args, **kwargs)\n \n \n-@keras.utils.generic_utils.register_keras_serializable(\n-    package=\"my_custom_package\"\n-)\n+@keras.utils.register_keras_serializable(package=\"my_custom_package\")\n def my_mean_squared_error(y_true, y_pred):\n     \"\"\"Identical to built-in `mean_squared_error`, added here as a custom\n     func.\"\"\"\n@@ -184,13 +174,11 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n         # This is so that we can register another function with the same custom\n         # object key, and make sure the newly registered function is used while\n         # loading.\n-        del generic_utils._GLOBAL_CUSTOM_OBJECTS[\n+        del object_registration._GLOBAL_CUSTOM_OBJECTS[\n             \"my_custom_package>my_mean_squared_error\"\n         ]\n \n-        @keras.utils.generic_utils.register_keras_serializable(\n-            package=\"my_custom_package\"\n-        )\n+        @keras.utils.register_keras_serializable(package=\"my_custom_package\")\n         def my_mean_squared_error(y_true, y_pred):\n             \"\"\"Function-local `mean_squared_error`.\"\"\"\n             return backend.mean(\n\n@@ -20,7 +20,7 @@ import types\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n-from keras.utils import generic_utils\n+from keras.saving import object_registration\n \n # isort: off\n from tensorflow.python.util import tf_export\n@@ -99,9 +99,9 @@ def serialize_keras_object(obj):\n             registered_name = None\n         else:\n             if isinstance(obj, types.FunctionType):\n-                registered_name = generic_utils.get_registered_name(obj)\n+                registered_name = object_registration.get_registered_name(obj)\n             else:\n-                registered_name = generic_utils.get_registered_name(\n+                registered_name = object_registration.get_registered_name(\n                     obj.__class__\n                 )\n     else:\n@@ -220,7 +220,7 @@ def deserialize_keras_object(config, custom_objects=None):\n     loss:\n \n     ```python\n-    @keras.utils.generic_utils.register_keras_serializable(package='my_package')\n+    @keras.utils.register_keras_serializable(package='my_package')\n     class ModifiedMeanSquaredError(keras.losses.MeanSquaredError):\n       ...\n \n@@ -314,7 +314,7 @@ def deserialize_keras_object(config, custom_objects=None):\n         )\n     # Instantiate the class from its config inside a custom object scope\n     # so that we can catch any custom objects that the config refers to.\n-    with generic_utils.custom_object_scope(custom_objects):\n+    with object_registration.custom_object_scope(custom_objects):\n         return cls.from_config(inner_config)\n \n \n@@ -323,7 +323,7 @@ def _retrieve_class_or_fn(\n ):\n     # If there is a custom object registered via\n     # `register_keras_serializable`, that takes precedence.\n-    custom_obj = generic_utils.get_custom_objects_by_name(\n+    custom_obj = object_registration.get_registered_object(\n         registered_name, custom_objects=custom_objects\n     )\n     if custom_obj is not None:\n\n@@ -27,7 +27,6 @@ from keras import losses\n from keras.optimizers import optimizer_v2\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n from keras.utils import losses_utils\n \n try:\n@@ -126,7 +125,7 @@ class LossesSerialization(test_combinations.TestCase):\n         self.w = np.array([1.25, 0.5, 1.25], dtype=\"float32\")\n \n     def test_serializing_model_with_loss_with_custom_object_scope(self, value):\n-        with generic_utils.custom_object_scope(\n+        with keras.utils.custom_object_scope(\n             {\n                 \"MyMeanAbsoluteError\": MyMeanAbsoluteError,\n                 \"my_mae\": my_mae,\n\n@@ -27,7 +27,7 @@ from keras import metrics\n from keras.optimizers import optimizer_v2\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n+from keras.utils import custom_object_scope\n \n try:\n     import h5py\n@@ -176,7 +176,7 @@ class MetricsSerialization(test_combinations.TestCase):\n         metric_input = tf.nest.map_structure(get_instance, value)\n         weighted_metric_input = tf.nest.map_structure(get_instance, value)\n \n-        with generic_utils.custom_object_scope(\n+        with custom_object_scope(\n             {\n                 \"MyMeanAbsoluteError\": MyMeanAbsoluteError,\n                 \"_my_mae\": _my_mae,\n\n@@ -0,0 +1,227 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Python utilities required by Keras.\"\"\"\n+\n+import inspect\n+import threading\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+_GLOBAL_CUSTOM_OBJECTS = {}\n+_GLOBAL_CUSTOM_NAMES = {}\n+# Thread-local custom objects set by custom_object_scope.\n+_THREAD_LOCAL_CUSTOM_OBJECTS = threading.local()\n+\n+\n+@keras_export(\n+    \"keras.utils.custom_object_scope\",\n+    \"keras.utils.CustomObjectScope\",\n+)\n+class CustomObjectScope:\n+    \"\"\"Exposes custom classes/functions to Keras deserialization internals.\n+\n+    Under a scope `with custom_object_scope(objects_dict)`, Keras methods such\n+    as `tf.keras.models.load_model` or `tf.keras.models.model_from_config`\n+    will be able to deserialize any custom object referenced by a\n+    saved config (e.g. a custom layer or metric).\n+\n+    Example:\n+\n+    Consider a custom regularizer `my_regularizer`:\n+\n+    ```python\n+    layer = Dense(3, kernel_regularizer=my_regularizer)\n+    # Config contains a reference to `my_regularizer`\n+    config = layer.get_config()\n+    ...\n+    # Later:\n+    with custom_object_scope({'my_regularizer': my_regularizer}):\n+      layer = Dense.from_config(config)\n+    ```\n+\n+    Args:\n+        *args: Dictionary or dictionaries of `{name: object}` pairs.\n+    \"\"\"\n+\n+    def __init__(self, *args):\n+        self.custom_objects = args\n+        self.backup = None\n+\n+    def __enter__(self):\n+        self.backup = _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.copy()\n+        for objects in self.custom_objects:\n+            _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.update(objects)\n+        return self\n+\n+    def __exit__(self, *args, **kwargs):\n+        _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.clear()\n+        _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.update(self.backup)\n+\n+\n+@keras_export(\"keras.utils.get_custom_objects\")\n+def get_custom_objects():\n+    \"\"\"Retrieves a live reference to the global dictionary of custom objects.\n+\n+    Custom objects set using using `custom_object_scope` are not added to the\n+    global dictionary of custom objects, and will not appear in the returned\n+    dictionary.\n+\n+    Example:\n+\n+    ```python\n+    get_custom_objects().clear()\n+    get_custom_objects()['MyObject'] = MyObject\n+    ```\n+\n+    Returns:\n+        Global dictionary mapping registered class names to classes.\n+    \"\"\"\n+    return _GLOBAL_CUSTOM_OBJECTS\n+\n+\n+@keras_export(\"keras.utils.register_keras_serializable\")\n+def register_keras_serializable(package=\"Custom\", name=None):\n+    \"\"\"Registers an object with the Keras serialization framework.\n+\n+    This decorator injects the decorated class or function into the Keras custom\n+    object dictionary, so that it can be serialized and deserialized without\n+    needing an entry in the user-provided custom object dict. It also injects a\n+    function that Keras will call to get the object's serializable string key.\n+\n+    Note that to be serialized and deserialized, classes must implement the\n+    `get_config()` method. Functions do not have this requirement.\n+\n+    The object will be registered under the key 'package>name' where `name`,\n+    defaults to the object name if not passed.\n+\n+    Example:\n+\n+    ```python\n+    # Note that `'my_package'` is used as the `package` argument here, and since\n+    # the `name` argument is not provided, `'MyDense'` is used as the `name`.\n+    @keras.utils.register_keras_serializable('my_package')\n+    class MyDense(keras.layers.Dense):\n+      pass\n+\n+    assert keras.utils.get_registered_object('my_package>MyDense') == MyDense\n+    assert keras.utils.get_registered_name(MyDense) == 'my_package>MyDense'\n+    ```\n+\n+    Args:\n+      package: The package that this class belongs to. This is used for the\n+        `key` (which is `\"package>name\"`) to idenfify the class. Note that this\n+        is the first argument passed into the decorator.\n+      name: The name to serialize this class under in this package. If not\n+        provided or `None`, the class' name will be used (note that this is the\n+        case when the decorator is used with only one argument, which becomes\n+        the `package`).\n+\n+    Returns:\n+      A decorator that registers the decorated class with the passed names.\n+    \"\"\"\n+\n+    def decorator(arg):\n+        \"\"\"Registers a class with the Keras serialization framework.\"\"\"\n+        class_name = name if name is not None else arg.__name__\n+        registered_name = package + \">\" + class_name\n+\n+        if inspect.isclass(arg) and not hasattr(arg, \"get_config\"):\n+            raise ValueError(\n+                \"Cannot register a class that does not have a \"\n+                \"get_config() method.\"\n+            )\n+\n+        if registered_name in _GLOBAL_CUSTOM_OBJECTS:\n+            raise ValueError(\n+                f\"{registered_name} has already been registered to \"\n+                f\"{_GLOBAL_CUSTOM_OBJECTS[registered_name]}\"\n+            )\n+\n+        if arg in _GLOBAL_CUSTOM_NAMES:\n+            raise ValueError(\n+                f\"{arg} has already been registered to \"\n+                f\"{_GLOBAL_CUSTOM_NAMES[arg]}\"\n+            )\n+        _GLOBAL_CUSTOM_OBJECTS[registered_name] = arg\n+        _GLOBAL_CUSTOM_NAMES[arg] = registered_name\n+\n+        return arg\n+\n+    return decorator\n+\n+\n+@keras_export(\"keras.utils.get_registered_name\")\n+def get_registered_name(obj):\n+    \"\"\"Returns the name registered to an object within the Keras framework.\n+\n+    This function is part of the Keras serialization and deserialization\n+    framework. It maps objects to the string names associated with those objects\n+    for serialization/deserialization.\n+\n+    Args:\n+      obj: The object to look up.\n+\n+    Returns:\n+      The name associated with the object, or the default Python name if the\n+        object is not registered.\n+    \"\"\"\n+    if obj in _GLOBAL_CUSTOM_NAMES:\n+        return _GLOBAL_CUSTOM_NAMES[obj]\n+    else:\n+        return obj.__name__\n+\n+\n+@keras_export(\"keras.utils.get_registered_object\")\n+def get_registered_object(name, custom_objects=None, module_objects=None):\n+    \"\"\"Returns the class associated with `name` if it is registered with Keras.\n+\n+    This function is part of the Keras serialization and deserialization\n+    framework. It maps strings to the objects associated with them for\n+    serialization/deserialization.\n+\n+    Example:\n+\n+    ```python\n+    def from_config(cls, config, custom_objects=None):\n+      if 'my_custom_object_name' in config:\n+        config['hidden_cls'] = tf.keras.utils.get_registered_object(\n+            config['my_custom_object_name'], custom_objects=custom_objects)\n+    ```\n+\n+    Args:\n+      name: The name to look up.\n+      custom_objects: A dictionary of custom objects to look the name up in.\n+        Generally, custom_objects is provided by the user.\n+      module_objects: A dictionary of custom objects to look the name up in.\n+        Generally, module_objects is provided by midlevel library implementers.\n+\n+    Returns:\n+      An instantiable class associated with `name`, or `None` if no such class\n+        exists.\n+    \"\"\"\n+    if name in _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__:\n+        return _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[name]\n+    elif name in _GLOBAL_CUSTOM_OBJECTS:\n+        return _GLOBAL_CUSTOM_OBJECTS[name]\n+    elif custom_objects and name in custom_objects:\n+        return custom_objects[name]\n+    elif module_objects and name in module_objects:\n+        return module_objects[name]\n+    return None\n+\n+\n+# Aliases\n+custom_object_scope = CustomObjectScope\n\n@@ -0,0 +1,143 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the 'License');\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an 'AS IS' BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Keras serializable object registration functionality.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+import keras\n+from keras.saving import object_registration\n+\n+\n+class TestObjectRegistration(tf.test.TestCase):\n+    def test_custom_object_scope(self):\n+        def custom_fn():\n+            pass\n+\n+        class CustomClass:\n+            pass\n+\n+        def check_get_in_thread():\n+            with object_registration.custom_object_scope(\n+                {\"CustomClass\": CustomClass, \"custom_fn\": custom_fn}\n+            ):\n+                actual_custom_fn = keras.activations.get(\"custom_fn\")\n+                self.assertEqual(actual_custom_fn, custom_fn)\n+                actual_custom_class = keras.regularizers.get(\"CustomClass\")\n+                self.assertEqual(actual_custom_class.__class__, CustomClass)\n+\n+            with object_registration.custom_object_scope(\n+                {\"CustomClass\": CustomClass, \"custom_fn\": custom_fn}\n+            ):\n+                actual_custom_fn = keras.activations.get(\"custom_fn\")\n+                self.assertEqual(actual_custom_fn, custom_fn)\n+                actual_custom_class = keras.regularizers.get(\"CustomClass\")\n+                self.assertEqual(actual_custom_class.__class__, CustomClass)\n+                checked_thread = self.checkedThread(check_get_in_thread)\n+                checked_thread.start()\n+                checked_thread.join()\n+\n+    def test_serialize_custom_class_with_default_name(self):\n+        @object_registration.register_keras_serializable()\n+        class TestClass:\n+            def __init__(self, value):\n+                self._value = value\n+\n+            def get_config(self):\n+                return {\"value\": self._value}\n+\n+        serialized_name = \"Custom>TestClass\"\n+        inst = TestClass(value=10)\n+        class_name = object_registration._GLOBAL_CUSTOM_NAMES[TestClass]\n+        self.assertEqual(serialized_name, class_name)\n+        config = keras.utils.generic_utils.serialize_keras_object(inst)\n+        self.assertEqual(class_name, config[\"class_name\"])\n+        new_inst = keras.utils.generic_utils.deserialize_keras_object(config)\n+        self.assertIsNot(inst, new_inst)\n+        self.assertIsInstance(new_inst, TestClass)\n+        self.assertEqual(10, new_inst._value)\n+\n+        # Make sure registering a new class with same name will fail.\n+        with self.assertRaisesRegex(\n+            ValueError, \".*has already been registered.*\"\n+        ):\n+\n+            @object_registration.register_keras_serializable()\n+            class TestClass:\n+                def __init__(self, value):\n+                    self._value = value\n+\n+                def get_config(self):\n+                    return {\"value\": self._value}\n+\n+    def test_serialize_custom_class_with_custom_name(self):\n+        @object_registration.register_keras_serializable(\n+            \"TestPackage\", \"CustomName\"\n+        )\n+        class OtherTestClass:\n+            def __init__(self, val):\n+                self._val = val\n+\n+            def get_config(self):\n+                return {\"val\": self._val}\n+\n+        serialized_name = \"TestPackage>CustomName\"\n+        inst = OtherTestClass(val=5)\n+        class_name = object_registration._GLOBAL_CUSTOM_NAMES[OtherTestClass]\n+        self.assertEqual(serialized_name, class_name)\n+        fn_class_name = object_registration.get_registered_name(OtherTestClass)\n+        self.assertEqual(fn_class_name, class_name)\n+\n+        cls = object_registration.get_registered_object(fn_class_name)\n+        self.assertEqual(OtherTestClass, cls)\n+\n+        config = keras.utils.generic_utils.serialize_keras_object(inst)\n+        self.assertEqual(class_name, config[\"class_name\"])\n+        new_inst = keras.utils.generic_utils.deserialize_keras_object(config)\n+        self.assertIsNot(inst, new_inst)\n+        self.assertIsInstance(new_inst, OtherTestClass)\n+        self.assertEqual(5, new_inst._val)\n+\n+    def test_serialize_custom_function(self):\n+        @object_registration.register_keras_serializable()\n+        def my_fn():\n+            return 42\n+\n+        serialized_name = \"Custom>my_fn\"\n+        class_name = object_registration._GLOBAL_CUSTOM_NAMES[my_fn]\n+        self.assertEqual(serialized_name, class_name)\n+        fn_class_name = object_registration.get_registered_name(my_fn)\n+        self.assertEqual(fn_class_name, class_name)\n+\n+        config = keras.utils.generic_utils.serialize_keras_object(my_fn)\n+        self.assertEqual(class_name, config)\n+        fn = keras.utils.generic_utils.deserialize_keras_object(config)\n+        self.assertEqual(42, fn())\n+\n+        fn_2 = object_registration.get_registered_object(fn_class_name)\n+        self.assertEqual(42, fn_2())\n+\n+    def test_serialize_custom_class_without_get_config_fails(self):\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Cannot register a class that does not have a get_config.*\",\n+        ):\n+\n+            @object_registration.register_keras_serializable(\n+                \"TestPackage\", \"TestClass\"\n+            )\n+            class TestClass:\n+                def __init__(self, value):\n+                    self._value = value\n\n@@ -17,6 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from keras.saving import hdf5_format\n+from keras.saving import object_registration\n from keras.saving import saving_utils\n from keras.saving.saved_model import load as saved_model_load\n from keras.saving.saved_model import load_context\n@@ -218,7 +219,7 @@ def load_model(filepath, custom_objects=None, compile=True, options=None):\n         IOError: In case of an invalid savefile.\n     \"\"\"\n     with generic_utils.SharedObjectLoadingScope():\n-        with generic_utils.CustomObjectScope(custom_objects or {}):\n+        with object_registration.CustomObjectScope(custom_objects or {}):\n             with load_context.load_context(options):\n                 filepath_str = path_to_string(filepath)\n                 if isinstance(filepath_str, str):\n\n@@ -36,6 +36,7 @@ from keras.layers import core\n from keras.optimizers import optimizer_v1\n from keras.premade_models.linear import LinearModel\n from keras.saving import model_config\n+from keras.saving import object_registration\n from keras.saving import save\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n@@ -227,9 +228,9 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n     def test_saving_model_with_custom_object(self):\n-        with generic_utils.custom_object_scope(), self.cached_session():\n+        with object_registration.custom_object_scope(), self.cached_session():\n \n-            @generic_utils.register_keras_serializable()\n+            @object_registration.register_keras_serializable()\n             class CustomLoss(losses.MeanSquaredError):\n                 pass\n \n@@ -285,7 +286,7 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n         self.assertEmpty(matched)\n \n \n-@generic_utils.register_keras_serializable(package=\"Foo\")\n+@object_registration.register_keras_serializable(package=\"Foo\")\n class RegisteredSubLayer(keras.layers.Layer):\n     pass\n \n@@ -1227,7 +1228,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n                 except TypeError:\n                     return\n \n-        with generic_utils.CustomObjectScope(\n+        with object_registration.CustomObjectScope(\n             {\"OuterLayer\": OuterLayer, \"InnerLayer\": InnerLayer}\n         ):\n \n\n@@ -28,6 +28,7 @@ from keras.engine import input_spec\n from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.protobuf import saved_metadata_pb2\n from keras.protobuf import versions_pb2\n+from keras.saving import object_registration\n from keras.saving import saving_utils\n from keras.saving.saved_model import constants\n from keras.saving.saved_model import json_utils\n@@ -533,7 +534,7 @@ class KerasObjectLoader:\n             return None\n \n         class_name = tf.compat.as_str(metadata[\"class_name\"])\n-        if generic_utils.get_registered_object(class_name) is not None:\n+        if object_registration.get_registered_object(class_name) is not None:\n             return None\n         model_is_functional_or_sequential = (\n             metadata.get(\"is_graph_network\", False)\n\n@@ -16,9 +16,9 @@\n \n import tensorflow.compat.v2 as tf\n \n+from keras.saving import object_registration\n from keras.saving.saved_model import constants\n from keras.saving.saved_model import layer_serialization\n-from keras.utils import generic_utils\n \n \n class MetricSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n@@ -30,7 +30,7 @@ class MetricSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n \n     def _python_properties_internal(self):\n         metadata = dict(\n-            class_name=generic_utils.get_registered_name(type(self.obj)),\n+            class_name=object_registration.get_registered_name(type(self.obj)),\n             name=self.obj.name,\n             dtype=self.obj.dtype,\n         )\n\n@@ -31,7 +31,7 @@ from keras import backend\n from keras.saving.saved_model import load as keras_load\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n+from keras.utils import CustomObjectScope\n \n \n class SubclassedModelNoConfig(keras.Model):\n@@ -456,7 +456,7 @@ class TestModelRevive(ReviveTestBase):\n \n if __name__ == \"__main__\":\n     tf.compat.v1.enable_eager_execution()\n-    with generic_utils.CustomObjectScope(\n+    with CustomObjectScope(\n         {\n             \"CustomLayerWithConfig\": CustomLayerWithConfig,\n             \"CustomNetworkWithConfig\": CustomNetworkWithConfig,\n\n@@ -36,13 +36,13 @@ from keras import regularizers\n from keras.feature_column.dense_features import DenseFeatures\n from keras.protobuf import saved_metadata_pb2\n from keras.protobuf import versions_pb2\n+from keras.saving import object_registration\n from keras.saving.saved_model import json_utils\n from keras.saving.saved_model import load as keras_load\n from keras.saving.saved_model import save_impl as keras_save\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import control_flow_util\n-from keras.utils import generic_utils\n from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n \n@@ -94,7 +94,7 @@ class LayerWithUpdate(keras.layers.Layer):\n         return inputs * 2.0\n \n \n-@generic_utils.register_keras_serializable(\"Testing\")\n+@object_registration.register_keras_serializable(\"Testing\")\n class GlobalLayerThatShouldFailIfNotAdded(keras.layers.Layer):\n     _must_restore_from_config = True\n \n@@ -429,7 +429,7 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n         )\n         saved_model_dir = self._save_model_dir()\n         model.save(saved_model_dir, save_format=\"tf\")\n-        with generic_utils.CustomObjectScope(\n+        with object_registration.CustomObjectScope(\n             {\"LayerThatShouldFailIfNotAdded\": LayerThatShouldFailIfNotAdded}\n         ):\n             _ = keras_load.load(saved_model_dir)\n@@ -718,7 +718,7 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n         saved_model_dir = self._save_model_dir()\n         model.save(saved_model_dir, save_format=\"tf\")\n \n-        with keras.utils.generic_utils.custom_object_scope({\"Model\": Model}):\n+        with object_registration.custom_object_scope({\"Model\": Model}):\n             loaded = keras_load.load(saved_model_dir)\n         self.assertAllClose(\n             prediction, loaded.predict(np.ones([1, 3]).astype(\"float32\"))\n@@ -1155,7 +1155,7 @@ class TestSavedModelFormat(tf.test.TestCase):\n \n         # Even if the `CustomLayer` is not provided in `custom_object_scope`,\n         # `Model` still has that reference.\n-        with keras.utils.generic_utils.custom_object_scope({\"Model\": Model}):\n+        with object_registration.custom_object_scope({\"Model\": Model}):\n             loaded = keras_load.load(saved_model_dir)\n         self.assertAllEqual([[1.0]], self.evaluate(loaded(inp)))\n         self.assertAllEqual([[1.0]], self.evaluate(loaded.layer(inp)))\n@@ -1163,7 +1163,7 @@ class TestSavedModelFormat(tf.test.TestCase):\n \n         # If `CustomLayer` is provided in `custom_object_scope`, it should of\n         # course use that custom class.\n-        with keras.utils.generic_utils.custom_object_scope(\n+        with object_registration.custom_object_scope(\n             {\"Model\": Model, \"CustomLayer\": CustomLayer}\n         ):\n             loaded = keras_load.load(saved_model_dir)\n@@ -1173,7 +1173,7 @@ class TestSavedModelFormat(tf.test.TestCase):\n \n         # If the symbol is no longer available, loading should raise an error.\n         del CustomLayer\n-        with keras.utils.generic_utils.custom_object_scope({\"Model\": Model}):\n+        with object_registration.custom_object_scope({\"Model\": Model}):\n             with self.assertRaisesRegex(\n                 NameError,\n                 \"free variable 'CustomLayer' referenced \"\n@@ -1220,7 +1220,7 @@ class TestSavedModelFormat(tf.test.TestCase):\n             loaded.attached_layer(tf.constant([1.0]))\n \n         # Try loading with the custom objects\n-        with generic_utils.CustomObjectScope({\"DoNotTrace\": DoNotTrace}):\n+        with object_registration.CustomObjectScope({\"DoNotTrace\": DoNotTrace}):\n             loaded = keras_load.load(saved_model_dir)\n         with self.assertRaisesRegex(ValueError, \"I said do not trace\"):\n             loaded.attached_layer(tf.constant([1.0]))\n@@ -1365,7 +1365,7 @@ class TestLayerCallTracing(tf.test.TestCase, parameterized.TestCase):\n         )\n \n \n-@generic_utils.register_keras_serializable(\"Testing\")\n+@object_registration.register_keras_serializable(\"Testing\")\n class CustomMeanMetric(keras.metrics.Mean):\n     def update_state(self, *args):\n         # Sometimes built-in metrics return an op in update_state. Custom\n@@ -1491,7 +1491,7 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n                 self._test_metric_save_and_load(\n                     metric, save_dir, num_tensor_args\n                 )\n-            with generic_utils.CustomObjectScope(\n+            with object_registration.CustomObjectScope(\n                 {\"CustomMetric\": CustomMetric}\n             ):\n                 loaded = self._test_metric_save_and_load(\n@@ -1530,7 +1530,9 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n \n         metric = NegativeMean()\n         self.evaluate([v.initializer for v in metric.variables])\n-        with generic_utils.CustomObjectScope({\"NegativeMean\": NegativeMean}):\n+        with object_registration.CustomObjectScope(\n+            {\"NegativeMean\": NegativeMean}\n+        ):\n             self._test_metric_save_and_load(\n                 metric, self._save_model_dir(), 1, test_sample_weight=False\n             )\n@@ -1563,7 +1565,7 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n         with self.assertRaisesRegex(ValueError, \"custom_objects\"):\n             keras_load.load(saved_model_dir)\n \n-        with generic_utils.CustomObjectScope(\n+        with object_registration.CustomObjectScope(\n             {\"CustomMetric\": CustomMetric, \"zero_metric\": zero_metric}\n         ):\n             loaded = keras_load.load(saved_model_dir)\n\n@@ -205,7 +205,7 @@ def model_metadata(model, include_optimizer=True, require_config=True):\n                 )\n             else:\n                 optimizer_config = {\n-                    \"class_name\": generic_utils.get_registered_name(\n+                    \"class_name\": keras.utils.get_registered_name(\n                         model.optimizer.__class__\n                     ),\n                     \"config\": model.optimizer.get_config(),\n@@ -227,7 +227,7 @@ def compile_args_from_training_config(training_config, custom_objects=None):\n     if custom_objects is None:\n         custom_objects = {}\n \n-    with generic_utils.CustomObjectScope(custom_objects):\n+    with keras.utils.CustomObjectScope(custom_objects):\n         optimizer_config = training_config[\"optimizer_config\"]\n         optimizer = optimizers.deserialize(optimizer_config)\n \n\n@@ -14,8 +14,14 @@\n # ==============================================================================\n \"\"\"Public Keras utilities.\"\"\"\n \n+# Serialization related\n+from keras.saving.object_registration import CustomObjectScope\n+from keras.saving.object_registration import custom_object_scope\n+from keras.saving.object_registration import get_custom_objects\n+from keras.saving.object_registration import get_registered_name\n+from keras.saving.object_registration import register_keras_serializable\n \n-# Audio related\n+# Dataset related\n from keras.utils.audio_dataset import audio_dataset_from_directory\n \n # Sequence related\n@@ -26,13 +32,8 @@ from keras.utils.data_utils import SequenceEnqueuer\n from keras.utils.data_utils import get_file\n from keras.utils.data_utils import pad_sequences\n from keras.utils.dataset_utils import split_dataset\n-\n-# Serialization related\n-from keras.utils.generic_utils import CustomObjectScope\n from keras.utils.generic_utils import Progbar\n-from keras.utils.generic_utils import custom_object_scope\n from keras.utils.generic_utils import deserialize_keras_object\n-from keras.utils.generic_utils import get_custom_objects\n from keras.utils.generic_utils import serialize_keras_object\n from keras.utils.image_dataset import image_dataset_from_directory\n \n@@ -44,10 +45,14 @@ from keras.utils.image_utils import save_img\n \n # Internal\n from keras.utils.layer_utils import get_source_inputs\n+\n+# Deprecated\n from keras.utils.np_utils import normalize\n from keras.utils.np_utils import to_categorical\n from keras.utils.text_dataset import text_dataset_from_directory\n from keras.utils.tf_utils import set_random_seed\n from keras.utils.timeseries_dataset import timeseries_dataset_from_array\n+\n+# Visualization related\n from keras.utils.vis_utils import model_to_dot\n from keras.utils.vis_utils import plot_model\n\n@@ -37,9 +37,6 @@ from keras.utils import tf_inspect\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-_GLOBAL_CUSTOM_OBJECTS = {}\n-_GLOBAL_CUSTOM_NAMES = {}\n-\n # Flag that determines whether to skip the NotImplementedError when calling\n # get_config in custom models and layers. This is only enabled when saving to\n # SavedModel, when the config isn't required.\n@@ -47,75 +44,6 @@ _SKIP_FAILED_SERIALIZATION = False\n # If a layer does not have a defined config, then the returned config will be a\n # dictionary with the below key.\n _LAYER_UNDEFINED_CONFIG_KEY = \"layer was saved without config\"\n-# Thread-local custom objects set by custom_object_scope.\n-_THREAD_LOCAL_CUSTOM_OBJECTS = threading.local()\n-\n-\n-@keras_export(\n-    \"keras.utils.custom_object_scope\",\n-    \"keras.utils.CustomObjectScope\",\n-)\n-class CustomObjectScope:\n-    \"\"\"Exposes custom classes/functions to Keras deserialization internals.\n-\n-    Under a scope `with custom_object_scope(objects_dict)`, Keras methods such\n-    as `tf.keras.models.load_model` or `tf.keras.models.model_from_config`\n-    will be able to deserialize any custom object referenced by a\n-    saved config (e.g. a custom layer or metric).\n-\n-    Example:\n-\n-    Consider a custom regularizer `my_regularizer`:\n-\n-    ```python\n-    layer = Dense(3, kernel_regularizer=my_regularizer)\n-    # Config contains a reference to `my_regularizer`\n-    config = layer.get_config()\n-    ...\n-    # Later:\n-    with custom_object_scope({'my_regularizer': my_regularizer}):\n-      layer = Dense.from_config(config)\n-    ```\n-\n-    Args:\n-        *args: Dictionary or dictionaries of `{name: object}` pairs.\n-    \"\"\"\n-\n-    def __init__(self, *args):\n-        self.custom_objects = args\n-        self.backup = None\n-\n-    def __enter__(self):\n-        self.backup = _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.copy()\n-        for objects in self.custom_objects:\n-            _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.update(objects)\n-        return self\n-\n-    def __exit__(self, *args, **kwargs):\n-        _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.clear()\n-        _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.update(self.backup)\n-\n-\n-@keras_export(\"keras.utils.get_custom_objects\")\n-def get_custom_objects():\n-    \"\"\"Retrieves a live reference to the global dictionary of custom objects.\n-\n-    Custom objects set using using `custom_object_scope` are not added to the\n-    global dictionary of custom objects, and will not appear in the returned\n-    dictionary.\n-\n-    Example:\n-\n-    ```python\n-    get_custom_objects().clear()\n-    get_custom_objects()['MyObject'] = MyObject\n-    ```\n-\n-    Returns:\n-        Global dictionary of names to classes (`_GLOBAL_CUSTOM_OBJECTS`).\n-    \"\"\"\n-    return _GLOBAL_CUSTOM_OBJECTS\n-\n \n # Store a unique, per-object ID for shared objects.\n #\n@@ -125,7 +53,6 @@ def get_custom_objects():\n # should be created or is merely a reference to an already-created object.\n SHARED_OBJECT_KEY = \"shared_object_id\"\n \n-\n SHARED_OBJECT_DISABLED = threading.local()\n SHARED_OBJECT_LOADING = threading.local()\n SHARED_OBJECT_SAVING = threading.local()\n@@ -351,98 +278,6 @@ def serialize_keras_class_and_config(\n     return base_config\n \n \n-@keras_export(\"keras.utils.register_keras_serializable\")\n-def register_keras_serializable(package=\"Custom\", name=None):\n-    \"\"\"Registers an object with the Keras serialization framework.\n-\n-    This decorator injects the decorated class or function into the Keras custom\n-    object dictionary, so that it can be serialized and deserialized without\n-    needing an entry in the user-provided custom object dict. It also injects a\n-    function that Keras will call to get the object's serializable string key.\n-\n-    Note that to be serialized and deserialized, classes must implement the\n-    `get_config()` method. Functions do not have this requirement.\n-\n-    The object will be registered under the key 'package>name' where `name`,\n-    defaults to the object name if not passed.\n-\n-    Example:\n-\n-    ```python\n-    # Note that `'my_package'` is used as the `package` argument here, and since\n-    # the `name` argument is not provided, `'MyDense'` is used as the `name`.\n-    @keras.utils.register_keras_serializable('my_package')\n-    class MyDense(keras.layers.Dense):\n-      pass\n-\n-    assert keras.utils.get_registered_object('my_package>MyDense') == MyDense\n-    assert keras.utils.get_registered_name(MyDense) == 'my_package>MyDense'\n-    ```\n-\n-    Args:\n-      package: The package that this class belongs to. This is used for the\n-        `key` (which is 'package>name') to idenfify the class. Note that this is\n-        the first argument passed into the decorator.\n-      name: The name to serialize this class under in this package. If not\n-        provided or `None`, the class' name will be used (note that this is the\n-        case when the decorator is used with only one argument, which becomes\n-        the `package`).\n-\n-    Returns:\n-      A decorator that registers the decorated class with the passed names.\n-    \"\"\"\n-\n-    def decorator(arg):\n-        \"\"\"Registers a class with the Keras serialization framework.\"\"\"\n-        class_name = name if name is not None else arg.__name__\n-        registered_name = package + \">\" + class_name\n-\n-        if tf_inspect.isclass(arg) and not hasattr(arg, \"get_config\"):\n-            raise ValueError(\n-                \"Cannot register a class that does not have a \"\n-                \"get_config() method.\"\n-            )\n-\n-        if registered_name in _GLOBAL_CUSTOM_OBJECTS:\n-            raise ValueError(\n-                f\"{registered_name} has already been registered to \"\n-                f\"{_GLOBAL_CUSTOM_OBJECTS[registered_name]}\"\n-            )\n-\n-        if arg in _GLOBAL_CUSTOM_NAMES:\n-            raise ValueError(\n-                f\"{arg} has already been registered to \"\n-                f\"{_GLOBAL_CUSTOM_NAMES[arg]}\"\n-            )\n-        _GLOBAL_CUSTOM_OBJECTS[registered_name] = arg\n-        _GLOBAL_CUSTOM_NAMES[arg] = registered_name\n-\n-        return arg\n-\n-    return decorator\n-\n-\n-@keras_export(\"keras.utils.get_registered_name\")\n-def get_registered_name(obj):\n-    \"\"\"Returns the name registered to an object within the Keras framework.\n-\n-    This function is part of the Keras serialization and deserialization\n-    framework. It maps objects to the string names associated with those objects\n-    for serialization/deserialization.\n-\n-    Args:\n-      obj: The object to look up.\n-\n-    Returns:\n-      The name associated with the object, or the default Python name if the\n-        object is not registered.\n-    \"\"\"\n-    if obj in _GLOBAL_CUSTOM_NAMES:\n-        return _GLOBAL_CUSTOM_NAMES[obj]\n-    else:\n-        return obj.__name__\n-\n-\n @tf_contextlib.contextmanager\n def skip_failed_serialization():\n     global _SKIP_FAILED_SERIALIZATION\n@@ -454,44 +289,6 @@ def skip_failed_serialization():\n         _SKIP_FAILED_SERIALIZATION = prev\n \n \n-@keras_export(\"keras.utils.get_registered_object\")\n-def get_registered_object(name, custom_objects=None, module_objects=None):\n-    \"\"\"Returns the class associated with `name` if it is registered with Keras.\n-\n-    This function is part of the Keras serialization and deserialization\n-    framework. It maps strings to the objects associated with them for\n-    serialization/deserialization.\n-\n-    Example:\n-    ```\n-    def from_config(cls, config, custom_objects=None):\n-      if 'my_custom_object_name' in config:\n-        config['hidden_cls'] = tf.keras.utils.get_registered_object(\n-            config['my_custom_object_name'], custom_objects=custom_objects)\n-    ```\n-\n-    Args:\n-      name: The name to look up.\n-      custom_objects: A dictionary of custom objects to look the name up in.\n-        Generally, custom_objects is provided by the user.\n-      module_objects: A dictionary of custom objects to look the name up in.\n-        Generally, module_objects is provided by midlevel library implementers.\n-\n-    Returns:\n-      An instantiable class associated with 'name', or None if no such class\n-        exists.\n-    \"\"\"\n-    if name in _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__:\n-        return _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[name]\n-    elif name in _GLOBAL_CUSTOM_OBJECTS:\n-        return _GLOBAL_CUSTOM_OBJECTS[name]\n-    elif custom_objects and name in custom_objects:\n-        return custom_objects[name]\n-    elif module_objects and name in module_objects:\n-        return module_objects[name]\n-    return None\n-\n-\n class CustomMaskWarning(Warning):\n     pass\n \n@@ -511,11 +308,12 @@ def serialize_keras_object(instance):\n     Returns:\n       A dict-like, JSON-compatible representation of the object's config.\n     \"\"\"\n+    from keras.saving import object_registration\n+\n     _, instance = tf.__internal__.decorator.unwrap(instance)\n     if instance is None:\n         return None\n \n-    #\n     # For v1 layers, checking supports_masking is not enough. We have to also\n     # check whether compute_mask has been overridden.\n     supports_masking = getattr(instance, \"supports_masking\", False) or (\n@@ -532,7 +330,7 @@ def serialize_keras_object(instance):\n         )\n \n     if hasattr(instance, \"get_config\"):\n-        name = get_registered_name(instance.__class__)\n+        name = object_registration.get_registered_name(instance.__class__)\n         try:\n             config = instance.get_config()\n         except NotImplementedError as e:\n@@ -559,29 +357,18 @@ def serialize_keras_object(instance):\n             except ValueError:\n                 serialization_config[key] = item\n \n-        name = get_registered_name(instance.__class__)\n+        name = object_registration.get_registered_name(instance.__class__)\n         return serialize_keras_class_and_config(\n             name, serialization_config, instance\n         )\n     if hasattr(instance, \"__name__\"):\n-        return get_registered_name(instance)\n+        return object_registration.get_registered_name(instance)\n     raise ValueError(\n         f\"Cannot serialize {instance} since it doesn't implement \"\n         \"`get_config()`, and also doesn\\t have `__name__`\"\n     )\n \n \n-def get_custom_objects_by_name(item, custom_objects=None):\n-    \"\"\"Returns the item if it is in either local or global custom objects.\"\"\"\n-    if item in _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__:\n-        return _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[item]\n-    elif item in _GLOBAL_CUSTOM_OBJECTS:\n-        return _GLOBAL_CUSTOM_OBJECTS[item]\n-    elif custom_objects and item in custom_objects:\n-        return custom_objects[item]\n-    return None\n-\n-\n def class_and_config_for_serialized_keras_object(\n     config,\n     module_objects=None,\n@@ -589,6 +376,8 @@ def class_and_config_for_serialized_keras_object(\n     printable_module_name=\"object\",\n ):\n     \"\"\"Returns the class name and config for a serialized keras object.\"\"\"\n+    from keras.saving import object_registration\n+\n     if (\n         not isinstance(config, dict)\n         or \"class_name\" not in config\n@@ -600,7 +389,9 @@ def class_and_config_for_serialized_keras_object(\n         )\n \n     class_name = config[\"class_name\"]\n-    cls = get_registered_object(class_name, custom_objects, module_objects)\n+    cls = object_registration.get_registered_object(\n+        class_name, custom_objects, module_objects\n+    )\n     if cls is None:\n         raise ValueError(\n             f\"Unknown {printable_module_name}: '{class_name}'. \"\n@@ -637,7 +428,7 @@ def class_and_config_for_serialized_keras_object(\n             )\n         # TODO(momernick): Should this also have 'module_objects'?\n         elif isinstance(item, str) and tf_inspect.isfunction(\n-            get_registered_object(item, custom_objects)\n+            object_registration.get_registered_object(item, custom_objects)\n         ):\n             # Handle custom functions here. When saving functions, we only save\n             # the function's name as a string. If we find a matching string in\n@@ -648,9 +439,9 @@ def class_and_config_for_serialized_keras_object(\n             # rare case.  This issue does not occur if a string field has a\n             # naming conflict with a custom object, since the config of an\n             # object will always be a dict.\n-            deserialized_objects[key] = get_registered_object(\n-                item, custom_objects\n-            )\n+            deserialized_objects[\n+                key\n+            ] = object_registration.get_registered_object(item, custom_objects)\n     for key, item in deserialized_objects.items():\n         cls_config[key] = deserialized_objects[key]\n \n@@ -709,6 +500,8 @@ def deserialize_keras_object(\n \n     This is how e.g. `keras.layers.deserialize()` is implemented.\n     \"\"\"\n+    from keras.saving import object_registration\n+\n     if identifier is None:\n         return None\n \n@@ -731,23 +524,24 @@ def deserialize_keras_object(\n             custom_objects = custom_objects or {}\n \n             if \"custom_objects\" in arg_spec.args:\n+                tlco = object_registration._THREAD_LOCAL_CUSTOM_OBJECTS.__dict__\n                 deserialized_obj = cls.from_config(\n                     cls_config,\n-                    custom_objects=dict(\n-                        list(_GLOBAL_CUSTOM_OBJECTS.items())\n-                        + list(_THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.items())\n-                        + list(custom_objects.items())\n-                    ),\n+                    custom_objects={\n+                        **object_registration._GLOBAL_CUSTOM_OBJECTS,\n+                        **tlco,\n+                        **custom_objects,\n+                    },\n                 )\n             else:\n-                with CustomObjectScope(custom_objects):\n+                with object_registration.CustomObjectScope(custom_objects):\n                     deserialized_obj = cls.from_config(cls_config)\n         else:\n             # Then `cls` may be a function returning a class.\n             # in this case by convention `config` holds\n             # the kwargs of the function.\n             custom_objects = custom_objects or {}\n-            with CustomObjectScope(custom_objects):\n+            with object_registration.CustomObjectScope(custom_objects):\n                 deserialized_obj = cls(**cls_config)\n \n         # Add object to shared objects, in case we find it referenced again.\n@@ -759,10 +553,15 @@ def deserialize_keras_object(\n         object_name = identifier\n         if custom_objects and object_name in custom_objects:\n             obj = custom_objects.get(object_name)\n-        elif object_name in _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__:\n-            obj = _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[object_name]\n-        elif object_name in _GLOBAL_CUSTOM_OBJECTS:\n-            obj = _GLOBAL_CUSTOM_OBJECTS[object_name]\n+        elif (\n+            object_name\n+            in object_registration._THREAD_LOCAL_CUSTOM_OBJECTS.__dict__\n+        ):\n+            obj = object_registration._THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[\n+                object_name\n+            ]\n+        elif object_name in object_registration._GLOBAL_CUSTOM_OBJECTS:\n+            obj = object_registration._GLOBAL_CUSTOM_OBJECTS[object_name]\n         else:\n             obj = module_objects.get(object_name)\n             if obj is None:\n@@ -1318,8 +1117,3 @@ class LazyLoader(python_types.ModuleType):\n     def __getattr__(self, item):\n         module = self._load()\n         return getattr(module, item)\n-\n-\n-# Aliases\n-\n-custom_object_scope = CustomObjectScope\n\n@@ -80,35 +80,6 @@ class HasArgTest(tf.test.TestCase):\n         )\n \n \n-class TestCustomObjectScope(tf.test.TestCase):\n-    def test_custom_object_scope(self):\n-        def custom_fn():\n-            pass\n-\n-        class CustomClass:\n-            pass\n-\n-        def check_get_in_thread():\n-            with keras.utils.generic_utils.custom_object_scope(\n-                {\"CustomClass\": CustomClass, \"custom_fn\": custom_fn}\n-            ):\n-                actual_custom_fn = keras.activations.get(\"custom_fn\")\n-                self.assertEqual(actual_custom_fn, custom_fn)\n-                actual_custom_class = keras.regularizers.get(\"CustomClass\")\n-                self.assertEqual(actual_custom_class.__class__, CustomClass)\n-\n-            with keras.utils.generic_utils.custom_object_scope(\n-                {\"CustomClass\": CustomClass, \"custom_fn\": custom_fn}\n-            ):\n-                actual_custom_fn = keras.activations.get(\"custom_fn\")\n-                self.assertEqual(actual_custom_fn, custom_fn)\n-                actual_custom_class = keras.regularizers.get(\"CustomClass\")\n-                self.assertEqual(actual_custom_class.__class__, CustomClass)\n-                checked_thread = self.checkedThread(check_get_in_thread)\n-                checked_thread.start()\n-                checked_thread.join()\n-\n-\n class SerializeKerasObjectTest(tf.test.TestCase):\n     def test_serialize_none(self):\n         serialized = keras.utils.generic_utils.serialize_keras_object(None)\n@@ -118,104 +89,6 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n         )\n         self.assertEqual(deserialized, None)\n \n-    def test_serialize_custom_class_with_default_name(self):\n-        @keras.utils.generic_utils.register_keras_serializable()\n-        class TestClass:\n-            def __init__(self, value):\n-                self._value = value\n-\n-            def get_config(self):\n-                return {\"value\": self._value}\n-\n-        serialized_name = \"Custom>TestClass\"\n-        inst = TestClass(value=10)\n-        class_name = keras.utils.generic_utils._GLOBAL_CUSTOM_NAMES[TestClass]\n-        self.assertEqual(serialized_name, class_name)\n-        config = keras.utils.generic_utils.serialize_keras_object(inst)\n-        self.assertEqual(class_name, config[\"class_name\"])\n-        new_inst = keras.utils.generic_utils.deserialize_keras_object(config)\n-        self.assertIsNot(inst, new_inst)\n-        self.assertIsInstance(new_inst, TestClass)\n-        self.assertEqual(10, new_inst._value)\n-\n-        # Make sure registering a new class with same name will fail.\n-        with self.assertRaisesRegex(\n-            ValueError, \".*has already been registered.*\"\n-        ):\n-\n-            @keras.utils.generic_utils.register_keras_serializable()\n-            class TestClass:\n-                def __init__(self, value):\n-                    self._value = value\n-\n-                def get_config(self):\n-                    return {\"value\": self._value}\n-\n-    def test_serialize_custom_class_with_custom_name(self):\n-        @keras.utils.generic_utils.register_keras_serializable(\n-            \"TestPackage\", \"CustomName\"\n-        )\n-        class OtherTestClass:\n-            def __init__(self, val):\n-                self._val = val\n-\n-            def get_config(self):\n-                return {\"val\": self._val}\n-\n-        serialized_name = \"TestPackage>CustomName\"\n-        inst = OtherTestClass(val=5)\n-        class_name = keras.utils.generic_utils._GLOBAL_CUSTOM_NAMES[\n-            OtherTestClass\n-        ]\n-        self.assertEqual(serialized_name, class_name)\n-        fn_class_name = keras.utils.generic_utils.get_registered_name(\n-            OtherTestClass\n-        )\n-        self.assertEqual(fn_class_name, class_name)\n-\n-        cls = keras.utils.generic_utils.get_registered_object(fn_class_name)\n-        self.assertEqual(OtherTestClass, cls)\n-\n-        config = keras.utils.generic_utils.serialize_keras_object(inst)\n-        self.assertEqual(class_name, config[\"class_name\"])\n-        new_inst = keras.utils.generic_utils.deserialize_keras_object(config)\n-        self.assertIsNot(inst, new_inst)\n-        self.assertIsInstance(new_inst, OtherTestClass)\n-        self.assertEqual(5, new_inst._val)\n-\n-    def test_serialize_custom_function(self):\n-        @keras.utils.generic_utils.register_keras_serializable()\n-        def my_fn():\n-            return 42\n-\n-        serialized_name = \"Custom>my_fn\"\n-        class_name = keras.utils.generic_utils._GLOBAL_CUSTOM_NAMES[my_fn]\n-        self.assertEqual(serialized_name, class_name)\n-        fn_class_name = keras.utils.generic_utils.get_registered_name(my_fn)\n-        self.assertEqual(fn_class_name, class_name)\n-\n-        config = keras.utils.generic_utils.serialize_keras_object(my_fn)\n-        self.assertEqual(class_name, config)\n-        fn = keras.utils.generic_utils.deserialize_keras_object(config)\n-        self.assertEqual(42, fn())\n-\n-        fn_2 = keras.utils.generic_utils.get_registered_object(fn_class_name)\n-        self.assertEqual(42, fn_2())\n-\n-    def test_serialize_custom_class_without_get_config_fails(self):\n-\n-        with self.assertRaisesRegex(\n-            ValueError,\n-            \"Cannot register a class that does not have a get_config.*\",\n-        ):\n-\n-            @keras.utils.generic_utils.register_keras_serializable(\n-                \"TestPackage\", \"TestClass\"\n-            )\n-            class TestClass:\n-                def __init__(self, value):\n-                    self._value = value\n-\n     def test_serializable_object(self):\n         class SerializableInt(int):\n             \"\"\"A serializable object to pass out of a test layer's config.\"\"\"\n@@ -531,9 +404,7 @@ class SharedObjectScopeTest(tf.test.TestCase):\n         subclassed_model.fit(x, y, epochs=1)\n         subclassed_model.save(temp_dir, save_format=\"tf\")\n \n-        with keras.utils.generic_utils.custom_object_scope(\n-            {\"CustomModelX\": CustomModelX}\n-        ):\n+        with keras.utils.custom_object_scope({\"CustomModelX\": CustomModelX}):\n             loaded_model = keras.models.load_model(temp_dir)\n \n         io_utils.enable_interactive_logging()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
