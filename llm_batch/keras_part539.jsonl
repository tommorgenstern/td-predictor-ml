{"custom_id": "keras#13a1e5fe0668568f7d0db5f98244b51002cde935", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 49 | Churn Cumulative: 49 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,49 @@\n+import os\n+\n+\n+def count_loc(directory, exclude=(\"_test\",), extensions=(\".py\",), verbose=0):\n+    loc = 0\n+    for root, _, fnames in os.walk(directory):\n+        skip = False\n+        for ex in exclude:\n+            if root.endswith(ex):\n+                skip = True\n+        if skip:\n+            continue\n+\n+        for fname in fnames:\n+            skip = False\n+            for ext in extensions:\n+                if not fname.endswith(ext):\n+                    skip = True\n+                    break\n+\n+                for ex in exclude:\n+                    if fname.endswith(ex + ext):\n+                        skip = True\n+                        break\n+            if skip:\n+                continue\n+\n+            fname = os.path.join(root, fname)\n+            if verbose:\n+                print(f\"Count LoCs in {fname}\")\n+\n+            with open(fname) as f:\n+                lines = f.read().split(\"\\n\")\n+\n+            string_open = False\n+            for line in lines:\n+                line = line.strip()\n+                if not line or line.startswith(\"#\"):\n+                    continue\n+                if not string_open:\n+                    if not line.startswith('\"\"\"'):\n+                        loc += 1\n+                    else:\n+                        if not line.endswith('\"\"\"'):\n+                            string_open = True\n+                else:\n+                    if line.startswith('\"\"\"'):\n+                        string_open = False\n+    return loc\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8c9abc14f94de9a1b9ec88a5478577532808258", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 9 | Methods Changed: 14 | Complexity Δ (Sum/Max): 15/10 | Churn Δ: 104 | Churn Cumulative: 404 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 0.9354838709677419\n\nDIFF:\n@@ -42,9 +42,8 @@ class ArrayDataAdapter(DataAdapter):\n         batch_size=None,\n         steps=None,\n         shuffle=False,\n-        **kwargs,\n     ):\n-        super().__init__(x, y, **kwargs)\n+        super().__init__(x, y)\n         x, y, sample_weights = convert_to_arrays((x, y, sample_weights))\n \n         inputs = data_adapters_utils.pack_x_y_sample_weight(\n@@ -81,15 +80,19 @@ class ArrayDataAdapter(DataAdapter):\n         ds = ds.prefetch(tf.data.AUTOTUNE)\n         return ds\n \n-    def get_size(self):\n+    @property\n+    def num_batches(self):\n         return self._size\n \n+    @property\n     def batch_size(self):\n         return self._batch_size\n \n+    @property\n     def has_partial_batch(self):\n         return self._partial_batch_size > 0\n \n+    @property\n     def partial_batch_size(self):\n         return self._partial_batch_size or None\n \n\n@@ -25,6 +25,11 @@ class TestArrayDataAdapter(testing.TestCase):\n             steps=None,\n             shuffle=False,\n         )\n+        self.assertEqual(adapter.num_batches, 3)\n+        self.assertEqual(adapter.batch_size, 16)\n+        self.assertEqual(adapter.has_partial_batch, True)\n+        self.assertEqual(adapter.partial_batch_size, 2)\n+\n         gen = adapter.get_numpy_iterator()\n         for i, batch in enumerate(gen):\n             self.assertEqual(len(batch), 2)\n\n@@ -0,0 +1,42 @@\n+import tensorflow as tf\n+from keras_core.trainers.data_adapters.data_adapter import DataAdapter\n+\n+class TFDatasetAdapter(DataAdapter):\n+    \"\"\"Adapter that handles `tf.data.Dataset`.\"\"\"\n+\n+    @staticmethod\n+    def can_handle(x, y=None):\n+        return isinstance(x, tf.data.Dataset)\n+\n+    def __init__(self, x):\n+        super().__init__(x, None)\n+        # Note that the dataset instance is immutable, it's fine to reuse the\n+        # user provided dataset.\n+        self._dataset = x\n+\n+    def get_numpy_iterator(self):\n+        for batch in self._dataset:\n+            yield tf.nest.map_structure(lambda x: x.numpy(), batch)\n+\n+    def get_tf_dataset(self):\n+        return self._dataset\n+\n+    @property\n+    def num_batches(self):\n+        cardinality = int(self._dataset.cardinality())\n+        if cardinality == -1:\n+            return None\n+        return cardinality\n+\n+    @property\n+    def batch_size(self):\n+        first_element_spec = tf.nest.flatten(self._dataset.element_spec)[0]\n+        return first_element_spec.shape[0]\n+\n+    @property\n+    def has_partial_batch(self):\n+        return None\n+\n+    @property\n+    def partial_batch_size(self):\n+        return None\n\n@@ -0,0 +1,48 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.trainers.data_adapters import tf_dataset_adapter\n+\n+\n+class TestTFDatasetAdapter(testing.TestCase):\n+    def test_basic_flow(self):\n+        x = tf.random.normal((34, 4))\n+        y = tf.random.normal((34, 2))\n+        base_ds = tf.data.Dataset.from_tensor_slices((x, y)).batch(16)\n+        adapter = tf_dataset_adapter.TFDatasetAdapter(base_ds)\n+\n+        self.assertEqual(adapter.num_batches, 3)\n+        self.assertEqual(adapter.batch_size, None)\n+        self.assertEqual(adapter.has_partial_batch, None)\n+        self.assertEqual(adapter.partial_batch_size, None)\n+\n+        gen = adapter.get_numpy_iterator()\n+        for i, batch in enumerate(gen):\n+            self.assertEqual(len(batch), 2)\n+            bx, by = batch\n+            self.assertTrue(isinstance(bx, np.ndarray))\n+            self.assertTrue(isinstance(by, np.ndarray))\n+            self.assertEqual(bx.dtype, by.dtype)\n+            self.assertEqual(bx.dtype, \"float32\")\n+            if i < 2:\n+                self.assertEqual(bx.shape, (16, 4))\n+                self.assertEqual(by.shape, (16, 2))\n+            else:\n+                self.assertEqual(bx.shape, (2, 4))\n+                self.assertEqual(by.shape, (2, 2))\n+        ds = adapter.get_tf_dataset()\n+        for i, batch in enumerate(ds):\n+            self.assertEqual(len(batch), 2)\n+            bx, by = batch\n+            self.assertTrue(isinstance(bx, tf.Tensor))\n+            self.assertTrue(isinstance(by, tf.Tensor))\n+            self.assertEqual(bx.dtype, by.dtype)\n+            self.assertEqual(bx.dtype, \"float32\")\n+            if i < 2:\n+                self.assertEqual(tuple(bx.shape), (16, 4))\n+                self.assertEqual(tuple(by.shape), (16, 2))\n+            else:\n+                self.assertEqual(tuple(bx.shape), (2, 4))\n+                self.assertEqual(tuple(by.shape), (2, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#68d026ecd5c7f0f045aa72f2a9c38d8f08234f78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 151 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,32 +1,5 @@\n-from keras_core import backend\n-from keras_core import initializers\n from keras_core import operations as ops\n from keras_core.metrics import reduction_metrics\n-from keras_core.metrics.metric import Metric\n-\n-# class MeanSquareError(Metric):\n-#     def __init__(self, name=\"mean_square_error\", dtype=None):\n-#         super().__init__(name=name, dtype=dtype)\n-#         self.sum = self.add_variable(\n-#             name=\"sum\", shape=(), initializer=initializers.Zeros()\n-#         )\n-#         self.total = self.add_variable(\n-#             name=\"total\", shape=(), initializer=initializers.Zeros()\n-#         )\n-\n-#     def update_state(self, y_true, y_pred, sample_weight=None):\n-#         # TODO: add support for sample_weight\n-#         batch_sum = ops.cast(ops.sum((y_true - y_pred) ** 2), dtype=self.dtype)\n-#         self.sum.assign(self.sum + batch_sum)\n-#         batch_size = backend.shape(y_true)[0]\n-#         self.total.assign(self.total + ops.cast(batch_size, dtype=self.total.dtype))\n-\n-#     def result(self):\n-#         return self.sum / (self.total + backend.epsilon())\n-\n-#     def reset_states(self):\n-#         self.sum.assign(0.0)\n-#         self.total.assign(0.0)\n \n \n def mean_square_error(y_true, y_pred):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#72b5fc85436f3badd04d75d92443aadde916dca8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 42/42 | Churn Δ: 15 | Churn Cumulative: 1023 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -39,7 +39,6 @@ import tensorflow as tf\n from google.protobuf import message\n from google.protobuf import text_format\n from tensorflow.python.lib.io import file_io\n-from tensorflow.python.platform import tf_logging as logging\n from tensorflow.tools.api.lib import api_objects_pb2\n from tensorflow.tools.api.lib import (\n     python_object_to_proto_visitor,\n@@ -262,14 +261,14 @@ class ApiCompatibilityTest(tf.test.TestCase):\n         # If diffs are found, handle them based on flags.\n         if diffs:\n             diff_count = len(diffs)\n-            logging.error(self._test_readme_message)\n-            logging.error(\n+            tf.compat.v1.logging.error(self._test_readme_message)\n+            tf.compat.v1.logging.error(\n                 \"%d differences found between API and golden.\", diff_count\n             )\n \n             if update_goldens:\n                 # Write files if requested.\n-                logging.warning(self._update_golden_warning)\n+                tf.compat.v1.logging.warning(self._update_golden_warning)\n \n                 # If the keys are only in expected, some objects are deleted.\n                 # Remove files.\n@@ -288,15 +287,17 @@ class ApiCompatibilityTest(tf.test.TestCase):\n             else:\n                 # Include the actual differences to help debugging.\n                 for d, verbose_d in zip(diffs, verbose_diffs):\n-                    logging.error(\"    %s\", d)\n-                    logging.error(\"    %s\", verbose_d)\n+                    tf.compat.v1.logging.error(\"    %s\", d)\n+                    tf.compat.v1.logging.error(\"    %s\", verbose_d)\n                 # Fail if we cannot fix the test by updating goldens.\n                 self.fail(\n                     \"%d differences found between API and golden.\" % diff_count\n                 )\n \n         else:\n-            logging.info(\"No differences found between API and golden.\")\n+            tf.compat.v1.logging.info(\n+                \"No differences found between API and golden.\"\n+            )\n \n     def _checkBackwardsCompatibility(\n         self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#afe86bcebd3009ba1dd3aecf66428e47eaa577fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 520/520 | Churn Δ: 12 | Churn Cumulative: 12875 | Contributors (this commit): 118 | Commits (past 90d): 7 | Contributors (cumulative): 118 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1889,9 +1889,21 @@ class BackupAndRestore(Callback):\n         self._training_state.restore()\n \n     def on_train_batch_begin(self, batch, logs=None):\n+        # Skip batch update for PSS Strategy\n+        if isinstance(\n+            self.model.distribute_strategy,\n+            tf.distribute.ParameterServerStrategy,\n+        ):\n+            return\n         self._training_state._ckpt_saved_batch.assign(batch)\n \n     def on_train_batch_end(self, batch, logs=None):\n+        # Skip batch update for PSS Strategy\n+        if isinstance(\n+            self.model.distribute_strategy,\n+            tf.distribute.ParameterServerStrategy,\n+        ):\n+            return\n         self._training_state.backup_if_preempted()\n         if self.save_freq and self.save_freq != \"epoch\":\n             self._batches_count += 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e4c590ac434a898d33619ca533470979f92a84a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 11 | Files Changed: 6 | Hunks: 13 | Methods Changed: 11 | Complexity Δ (Sum/Max): 4/5 | Churn Δ: 86 | Churn Cumulative: 1534 | Contributors (this commit): 3 | Commits (past 90d): 41 | Contributors (cumulative): 12 | DMM Complexity: 0.9230769230769231\n\nDIFF:\n@@ -16,7 +16,6 @@ class Variable(KerasVariable):\n         self.name = name or auto_name(self.__class__.__name__)\n         dtype = standardize_dtype(dtype)\n         self.trainable = trainable\n-        with tf.init_scope():\n         self._value = tf.Variable(value, dtype=dtype)\n \n     def assign(self, value):\n\n@@ -125,7 +125,7 @@ class Layer(Operation):\n         shape,\n         initializer,\n         dtype=None,\n-        trainable=None,\n+        trainable=True,\n         regularizer=None,\n         constraint=None,\n         name=None,\n\n@@ -1,2 +1,49 @@\n+from keras_core.api_export import keras_core_export\n from keras_core.losses.loss import Loss\n from keras_core.losses.losses import LossFunctionWrapper\n+from keras_core.losses.losses import MeanSquaredError\n+\n+\n+@keras_core_export(\"keras_core.losses.get\")\n+def get(identifier):\n+    \"\"\"Retrieves a Keras loss as a `function`/`Loss` class instance.\n+\n+    The `identifier` may be the string name of a loss function or `Loss` class.\n+\n+    >>> loss = losses.get(\"categorical_crossentropy\")\n+    >>> type(loss)\n+    <class 'function'>\n+    >>> loss = losses.get(\"CategoricalCrossentropy\")\n+    >>> type(loss)\n+    <class '...CategoricalCrossentropy'>\n+\n+    You can also specify `config` of the loss to this function by passing dict\n+    containing `class_name` and `config` as an identifier. Also note that the\n+    `class_name` must map to a `Loss` class\n+\n+    >>> identifier = {\"class_name\": \"CategoricalCrossentropy\",\n+    ...               \"config\": {\"from_logits\": True}}\n+    >>> loss = losses.get(identifier)\n+    >>> type(loss)\n+    <class '...CategoricalCrossentropy'>\n+\n+    Args:\n+        identifier: A loss identifier. One of None or string name of a loss\n+            function/class or loss configuration dictionary or a loss function or a\n+            loss class instance.\n+\n+    Returns:\n+        A Keras loss as a `function`/ `Loss` class instance.\n+    \"\"\"\n+    if identifier is None:\n+        return None\n+    if isinstance(identifier, str):\n+        identifier = str(identifier)\n+        return deserialize(identifier)\n+    if isinstance(identifier, dict):\n+        return deserialize(identifier)\n+    if callable(identifier):\n+        return identifier\n+    raise ValueError(\n+        f\"Could not interpret loss function identifier: {identifier}\"\n+    )\n\n@@ -27,7 +27,11 @@ def mean_squared_error(y_true, y_pred):\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n     y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n-    return (y_true - y_pred) ** 2\n+    output = (y_true - y_pred) ** 2\n+    ndim = len(y_true.shape)\n+    if ndim > 1:\n+        output = ops.sum(output, axis=list(range(1, ndim)))\n+    return output\n \n \n class MeanSquaredError(LossFunctionWrapper):\n\n@@ -169,6 +169,7 @@ class CompileMetrics(metrics_module.Metric):\n         self._user_metrics = metrics\n         self._user_weighted_metrics = weighted_metrics\n         self.built = False\n+        self.name = \"compile_metrics\"\n \n     def build(self, y_true, y_pred):\n         if isinstance(y_pred, dict):\n@@ -383,12 +384,7 @@ class CompileMetrics(metrics_module.Metric):\n \n \n class CompileLoss(losses_module.Loss):\n-    def __init__(self, loss, loss_weights):\n-        if loss and not isinstance(loss, (list, tuple, dict)):\n-            raise ValueError(\n-                \"Expected `loss` argument to be a list, tuple, or dict. \"\n-                f\"Received instead: loss={loss} of type {type(loss)}\"\n-            )\n+    def __init__(self, loss, loss_weights=None, reduction=\"sum_over_batch_size\"):\n         if loss_weights and not isinstance(loss_weights, (list, tuple, dict)):\n             raise ValueError(\n                 \"Expected `loss_weights` argument to be a list, tuple, or dict. \"\n@@ -398,6 +394,7 @@ class CompileLoss(losses_module.Loss):\n         self._user_loss = loss\n         self._user_loss_weights = loss_weights\n         self.built = False\n+        super().__init__(name=\"compile_loss\", reduction=reduction)\n \n     def build(self, y_true, y_pred):\n         if isinstance(y_pred, dict):\n@@ -539,6 +536,8 @@ class CompileLoss(losses_module.Loss):\n                         flat_loss_weights.append(loss_weights[name])\n                     else:\n                         flat_loss_weights.append(1.0)\n+        self.flat_losses = flat_losses\n+        self.flat_loss_weights = flat_loss_weights\n \n     def call(self, y_true, y_pred):\n         if not self.built:\n@@ -548,7 +547,7 @@ class CompileLoss(losses_module.Loss):\n         y_pred = nest.flatten(y_pred)\n         loss_values = []\n         for loss, y_t, y_p, w in zip(\n-            self.losses, y_true, y_pred, self.loss_weights\n+            self.flat_losses, y_true, y_pred, self.flat_loss_weights\n         ):\n             if loss:\n                 value = w * ops.cast(loss(y_t, y_p), dtype=backend.floatx())\n\n@@ -2,8 +2,10 @@ import numpy as np\n \n from keras_core import backend\n from keras_core import metrics as metrics_module\n+from keras_core import metrics as losses_module\n from keras_core import testing\n from keras_core.trainers.compile_utils import CompileMetrics\n+from keras_core.trainers.compile_utils import CompileLoss\n \n \n class TestCompileMetrics(testing.TestCase):\n@@ -179,4 +181,17 @@ class TestCompileMetrics(testing.TestCase):\n \n class TestCompileLoss(testing.TestCase):\n     def test_single_output_case(self):\n-        pass\n+        compile_loss = CompileLoss(\n+            loss=losses_module.MeanSquareError(),\n+        )\n+        # Test symbolic build\n+        y_true, y_pred = backend.KerasTensor((3, 4)), backend.KerasTensor(\n+            (3, 4)\n+        )\n+        compile_loss.build(y_true, y_pred)\n+        # Test eager build\n+        y_true = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n+        y_pred = np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]])\n+        compile_loss.build(y_true, y_pred)\n+        value = compile_loss(y_true, y_pred)\n+        self.assertAllClose(value, 0.5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d562cd935363fe5707baa7c7271c6f706b3228a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 569 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 59 | Complexity Δ (Sum/Max): 104/50 | Churn Δ: 569 | Churn Cumulative: 569 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 0.9292929292929293\n\nDIFF:\n@@ -0,0 +1,4 @@\n+from keras_core.callbacks.callback import Callback\n+from keras_core.callbacks.callback_list import CallbackList\n+from keras_core.callbacks.progbar_logger import ProgbarLogger\n+from keras_core.callbacks.history import History\n\n@@ -0,0 +1,266 @@\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export(\"keras_core.callbacks.Callback\")\n+class Callback:\n+    \"\"\"Base class used to build new callbacks.\n+\n+    Callbacks can be passed to keras methods such as `fit()`, `evaluate()`, and\n+    `predict()` in order to hook into the various stages of the model training,\n+    evaluation, and inference lifecycle.\n+\n+    To create a custom callback, subclass `keras.callbacks.Callback` and\n+    override the method associated with the stage of interest. See\n+    https://www.tensorflow.org/guide/keras/custom_callback for more information.\n+\n+    Example:\n+\n+    >>> training_finished = False\n+    >>> class MyCallback(Callback):\n+    ...   def on_train_end(self, logs=None):\n+    ...     global training_finished\n+    ...     training_finished = True\n+    >>> model = Sequential([\n+    ...     layers.Dense(1, input_shape=(1,))])\n+    >>> model.compile(loss='mean_squared_error')\n+    >>> model.fit(np.array([[1.0]]), np.array([[1.0]]),\n+    ...           callbacks=[MyCallback()])\n+    >>> assert training_finished == True\n+\n+    If you want to use `Callback` objects in a custom training loop:\n+\n+    1. You should pack all your callbacks into a single `callbacks.CallbackList`\n+       so they can all be called together.\n+    2. You will need to manually call all the `on_*` methods at the appropriate\n+       locations in your loop. Like this:\n+\n+    Example:\n+\n+    ```python\n+    callbacks =  keras_core.callbacks.CallbackList([...])\n+    callbacks.append(...)\n+    callbacks.on_train_begin(...)\n+    for epoch in range(EPOCHS):\n+        callbacks.on_epoch_begin(epoch)\n+        for i, data in dataset.enumerate():\n+        callbacks.on_train_batch_begin(i)\n+        batch_logs = model.train_step(data)\n+        callbacks.on_train_batch_end(i, batch_logs)\n+        epoch_logs = ...\n+        callbacks.on_epoch_end(epoch, epoch_logs)\n+    final_logs=...\n+    callbacks.on_train_end(final_logs)\n+    ```\n+\n+    Attributes:\n+        params: Dict. Training parameters\n+            (eg. verbosity, batch size, number of epochs...).\n+        model: Instance of `Model`.\n+            Reference of the model being trained.\n+\n+    The `logs` dictionary that callback methods\n+    take as argument will contain keys for quantities relevant to\n+    the current batch or epoch (see method-specific docstrings).\n+    \"\"\"\n+\n+    def __init__(self):\n+        self.validation_data = None\n+        self.model = None\n+\n+    def set_params(self, params):\n+        self.params = params\n+\n+    def set_model(self, model):\n+        self.model = model\n+\n+    def on_batch_begin(self, batch, logs=None):\n+        \"\"\"A backwards compatibility alias for `on_train_batch_begin`.\"\"\"\n+\n+    def on_batch_end(self, batch, logs=None):\n+        \"\"\"A backwards compatibility alias for `on_train_batch_end`.\"\"\"\n+\n+    def on_epoch_begin(self, epoch, logs=None):\n+        \"\"\"Called at the start of an epoch.\n+\n+        Subclasses should override for any actions to run. This function should\n+        only be called during TRAIN mode.\n+\n+        Args:\n+            epoch: Integer, index of epoch.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n+        \"\"\"\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        \"\"\"Called at the end of an epoch.\n+\n+        Subclasses should override for any actions to run. This function should\n+        only be called during TRAIN mode.\n+\n+        Args:\n+            epoch: Integer, index of epoch.\n+            logs: Dict, metric results for this training epoch, and for the\n+              validation epoch if validation is performed. Validation result\n+              keys are prefixed with `val_`. For training epoch, the values of\n+              the `Model`'s metrics are returned. Example:\n+              `{'loss': 0.2, 'accuracy': 0.7}`.\n+        \"\"\"\n+\n+    def on_train_batch_begin(self, batch, logs=None):\n+        \"\"\"Called at the beginning of a training batch in `fit` methods.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Note that if the `steps_per_execution` argument to `compile` in\n+        `Model` is set to `N`, this method will only be called every\n+        `N` batches.\n+\n+        Args:\n+            batch: Integer, index of batch within the current epoch.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n+        \"\"\"\n+        # For backwards compatibility.\n+        self.on_batch_begin(batch, logs=logs)\n+\n+    def on_train_batch_end(self, batch, logs=None):\n+        \"\"\"Called at the end of a training batch in `fit` methods.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Note that if the `steps_per_execution` argument to `compile` in\n+        `Model` is set to `N`, this method will only be called every\n+        `N` batches.\n+\n+        Args:\n+            batch: Integer, index of batch within the current epoch.\n+            logs: Dict. Aggregated metric results up until this batch.\n+        \"\"\"\n+        # For backwards compatibility.\n+        self.on_batch_end(batch, logs=logs)\n+\n+    def on_test_batch_begin(self, batch, logs=None):\n+        \"\"\"Called at the beginning of a batch in `evaluate` methods.\n+\n+        Also called at the beginning of a validation batch in the `fit`\n+        methods, if validation data is provided.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Note that if the `steps_per_execution` argument to `compile` in\n+        `Model` is set to `N`, this method will only be called every\n+        `N` batches.\n+\n+        Args:\n+            batch: Integer, index of batch within the current epoch.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n+        \"\"\"\n+\n+    def on_test_batch_end(self, batch, logs=None):\n+        \"\"\"Called at the end of a batch in `evaluate` methods.\n+\n+        Also called at the end of a validation batch in the `fit`\n+        methods, if validation data is provided.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Note that if the `steps_per_execution` argument to `compile` in\n+        `Model` is set to `N`, this method will only be called every\n+        `N` batches.\n+\n+        Args:\n+            batch: Integer, index of batch within the current epoch.\n+            logs: Dict. Aggregated metric results up until this batch.\n+        \"\"\"\n+\n+    def on_predict_batch_begin(self, batch, logs=None):\n+        \"\"\"Called at the beginning of a batch in `predict` methods.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Note that if the `steps_per_execution` argument to `compile` in\n+        `Model` is set to `N`, this method will only be called every\n+        `N` batches.\n+\n+        Args:\n+            batch: Integer, index of batch within the current epoch.\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n+        \"\"\"\n+\n+    def on_predict_batch_end(self, batch, logs=None):\n+        \"\"\"Called at the end of a batch in `predict` methods.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Note that if the `steps_per_execution` argument to `compile` in\n+        `Model` is set to `N`, this method will only be called every\n+        `N` batches.\n+\n+        Args:\n+            batch: Integer, index of batch within the current epoch.\n+            logs: Dict. Aggregated metric results up until this batch.\n+        \"\"\"\n+\n+    def on_train_begin(self, logs=None):\n+        \"\"\"Called at the beginning of training.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Args:\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n+        \"\"\"\n+\n+    def on_train_end(self, logs=None):\n+        \"\"\"Called at the end of training.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Args:\n+            logs: Dict. Currently the output of the last call to\n+              `on_epoch_end()` is passed to this argument for this method but\n+              that may change in the future.\n+        \"\"\"\n+\n+    def on_test_begin(self, logs=None):\n+        \"\"\"Called at the beginning of evaluation or validation.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Args:\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n+        \"\"\"\n+\n+    def on_test_end(self, logs=None):\n+        \"\"\"Called at the end of evaluation or validation.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Args:\n+            logs: Dict. Currently the output of the last call to\n+              `on_test_batch_end()` is passed to this argument for this method\n+              but that may change in the future.\n+        \"\"\"\n+\n+    def on_predict_begin(self, logs=None):\n+        \"\"\"Called at the beginning of prediction.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Args:\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n+        \"\"\"\n+\n+    def on_predict_end(self, logs=None):\n+        \"\"\"Called at the end of prediction.\n+\n+        Subclasses should override for any actions to run.\n+\n+        Args:\n+            logs: Dict. Currently no data is passed to this argument for this\n+              method but that may change in the future.\n+        \"\"\"\n\n@@ -0,0 +1,140 @@\n+from tensorflow import nest\n+from keras_core.api_export import keras_core_export\n+from keras_core.callbacks.callback import Callback\n+from keras_core.callbacks.progbar_logger import ProgbarLogger\n+from keras_core.callbacks.history import History\n+\n+\n+@keras_core_export(\"keras_core.callbacks.CallbackList\")\n+class CallbackList(Callback):\n+    \"\"\"Container abstracting a list of callbacks.\"\"\"\n+\n+    def __init__(\n+        self,\n+        callbacks=None,\n+        add_history=False,\n+        add_progbar=False,\n+        model=None,\n+        **params,\n+    ):\n+        \"\"\"Container for `Callback` instances.\n+\n+        This object wraps a list of `Callback` instances, making it possible\n+        to call them all at once via a single endpoint\n+        (e.g. `callback_list.on_epoch_end(...)`).\n+\n+        Args:\n+            callbacks: List of `Callback` instances.\n+            add_history: Whether a `History` callback should be added, if one does\n+                not already exist in the `callbacks` list.\n+            add_progbar: Whether a `ProgbarLogger` callback should be added, if\n+                one does not already exist in the `callbacks` list.\n+            model: The `Model` these callbacks are used with.\n+            **params: If provided, parameters will be passed to each `Callback`\n+                via `Callback.set_params`.\n+        \"\"\"\n+        self.callbacks = nest.flatten(callbacks) if callbacks else []\n+        self._add_default_callbacks(add_history, add_progbar)\n+\n+        if model:\n+            self.set_model(model)\n+        if params:\n+            self.set_params(params)\n+\n+    def _add_default_callbacks(self, add_history, add_progbar):\n+        \"\"\"Adds `Callback`s that are always present.\"\"\"\n+        self._progbar = None\n+        self._history = None\n+\n+        for cb in self.callbacks:\n+            if isinstance(cb, ProgbarLogger):\n+                self._progbar = cb\n+            elif isinstance(cb, History):\n+                self._history = cb\n+\n+        if self._history is None and add_history:\n+            self._history = History()\n+            self.callbacks.append(self._history)\n+\n+        if self._progbar is None and add_progbar:\n+            self._progbar = ProgbarLogger()\n+            self.callbacks.append(self._progbar)\n+\n+    def append(self, callback):\n+        self.callbacks.append(callback)\n+\n+    def set_params(self, params):\n+        self.params = params\n+        for callback in self.callbacks:\n+            callback.set_params(params)\n+\n+    def set_model(self, model):\n+        self.model = model\n+        if self._history:\n+            model.history = self._history\n+        for callback in self.callbacks:\n+            callback.set_model(model)\n+\n+    def on_batch_begin(self, batch, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_batch_begin(batch, logs=logs)\n+\n+    def on_batch_end(self, batch, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_batch_end(batch, logs=logs)\n+\n+    def on_epoch_begin(self, epoch, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_epoch_begin(epoch, logs)\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_epoch_end(epoch, logs)\n+\n+    def on_train_batch_begin(self, batch, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_train_batch_begin(batch, logs=logs)\n+\n+    def on_train_batch_end(self, batch, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_train_batch_end(batch, logs=logs)\n+\n+    def on_test_batch_begin(self, batch, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_test_batch_begin(batch, logs=logs)\n+\n+    def on_test_batch_end(self, batch, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_test_batch_end(batch, logs=logs)\n+\n+    def on_predict_batch_begin(self, batch, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_predict_batch_begin(batch, logs=logs)\n+\n+    def on_predict_batch_end(self, batch, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_predict_batch_end(batch, logs=logs)\n+\n+    def on_train_begin(self, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_train_begin(logs)\n+\n+    def on_train_end(self, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_train_end(logs)\n+\n+    def on_test_begin(self, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_test_begin(logs)\n+\n+    def on_test_end(self, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_test_end(logs)\n+\n+    def on_predict_begin(self, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_predict_begin(logs)\n+\n+    def on_predict_end(self, logs=None):\n+        for callback in self.callbacks:\n+            callback.on_predict_end(logs)\n\n@@ -0,0 +1,42 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.callbacks.callback import Callback\n+\n+\n+@keras_core_export(\"keras_core.callbacks.History\")\n+class History(Callback):\n+    \"\"\"Callback that records events into a `History` object.\n+\n+    This callback is automatically applied to\n+    every Keras model. The `History` object\n+    gets returned by the `fit()` method of models.\n+\n+    Example:\n+\n+    >>> model = Sequential([layers.Dense(10)])\n+    >>> model.compile(SGD(), loss='mse')\n+    >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n+    ...                     epochs=10, verbose=1)\n+    >>> print(history.params)\n+    {'verbose': 1, 'epochs': 10, 'steps': 1}\n+    >>> # check the keys of history object\n+    >>> print(history.history.keys())\n+    dict_keys(['loss'])\n+\n+    \"\"\"\n+\n+    def __init__(self):\n+        super().__init__()\n+        self.history = {}\n+\n+    def on_train_begin(self, logs=None):\n+        self.epoch = []\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n+        self.epoch.append(epoch)\n+        for k, v in logs.items():\n+            self.history.setdefault(k, []).append(v)\n+\n+        # Set the history attribute on the model after the epoch ends. This will\n+        # make sure that the state which is set is the latest one.\n+        self.model.history = self\n\n@@ -0,0 +1,117 @@\n+import copy\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.callbacks.callback import Callback\n+from keras_core.utils import io_utils\n+from keras_core.utils.progbar import Progbar\n+\n+\n+@keras_core_export(\"keras_core.callbacks.ProgbarLogger\")\n+class ProgbarLogger(Callback):\n+    \"\"\"Callback that prints metrics to stdout.\n+\n+    Args:\n+        count_mode: One of `\"steps\"` or `\"samples\"`.\n+            Whether the progress bar should\n+            count samples seen or steps (batches) seen.\n+\n+    Raises:\n+        ValueError: In case of invalid `count_mode`.\n+    \"\"\"\n+\n+    def __init__(self):\n+        super().__init__()\n+        self.seen = 0\n+        self.progbar = None\n+        self.target = None\n+        self.verbose = 1\n+        self.epochs = 1\n+\n+        self._train_step, self._test_step, self._predict_step = None, None, None\n+        self._call_batch_hooks = True\n+\n+        self._called_in_fit = False\n+\n+    def set_params(self, params):\n+        self.verbose = params[\"verbose\"]\n+        self.epochs = params[\"epochs\"]\n+        self.target = params[\"steps\"]\n+\n+        self._call_batch_hooks = self.verbose == 1\n+        if self.target is None:\n+            try:\n+                self._train_step = self.model._train_counter\n+                self._test_step = self.model._test_counter\n+                self._predict_step = self.model._predict_counter\n+            except AttributeError:\n+                self._call_batch_hooks = True\n+\n+    def on_train_begin(self, logs=None):\n+        # When this logger is called inside `fit`, validation is silent.\n+        self._called_in_fit = True\n+\n+    def on_test_begin(self, logs=None):\n+        if not self._called_in_fit:\n+            self._reset_progbar()\n+            self._maybe_init_progbar()\n+\n+    def on_predict_begin(self, logs=None):\n+        self._reset_progbar()\n+        self._maybe_init_progbar()\n+\n+    def on_epoch_begin(self, epoch, logs=None):\n+        self._reset_progbar()\n+        self._maybe_init_progbar()\n+        if self.verbose and self.epochs > 1:\n+            io_utils.print_msg(f\"Epoch {epoch + 1}/{self.epochs}\")\n+\n+    def on_train_batch_end(self, batch, logs=None):\n+        self._batch_update_progbar(batch, logs)\n+\n+    def on_test_batch_end(self, batch, logs=None):\n+        if not self._called_in_fit:\n+            self._batch_update_progbar(batch, logs)\n+\n+    def on_predict_batch_end(self, batch, logs=None):\n+        # Don't pass prediction results.\n+        self._batch_update_progbar(batch, None)\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        self._finalize_progbar(logs, self._train_step)\n+\n+    def on_test_end(self, logs=None):\n+        if not self._called_in_fit:\n+            self._finalize_progbar(logs, self._test_step)\n+\n+    def on_predict_end(self, logs=None):\n+        self._finalize_progbar(logs, self._predict_step)\n+\n+    def _reset_progbar(self):\n+        self.seen = 0\n+        self.progbar = None\n+\n+    def _maybe_init_progbar(self):\n+        if self.progbar is None:\n+            self.progbar = Progbar(\n+                target=self.target,\n+                verbose=self.verbose,\n+                unit_name=\"step\"\n+            )\n+\n+    def _batch_update_progbar(self, batch, logs=None):\n+        \"\"\"Updates the progbar.\"\"\"\n+        logs = logs or {}\n+        self._maybe_init_progbar()\n+        self.seen = batch + 1  # One-indexed.\n+\n+        if self.verbose == 1:\n+            self.progbar.update(self.seen, list(logs.items()), finalize=False)\n+\n+    def _finalize_progbar(self, logs, counter):\n+        logs = logs or {}\n+        if self.target is None:\n+            if counter is not None:\n+                counter = counter.numpy()\n+            self.target = counter or self.seen\n+            self.progbar.target = self.target\n+        self.progbar.update(self.target, list(logs.items()), finalize=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#425601c2eac2424014743739da771f3367db72ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 124 | Lines Deleted: 96 | Files Changed: 3 | Hunks: 36 | Methods Changed: 21 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 220 | Churn Cumulative: 1197 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,26 +5,33 @@ import tensorflow as tf\n \n from keras_core import callbacks as callbacks_module\n from keras_core import optimizers as optimizers_module\n-from keras_core.trainers import data_adapter\n from keras_core.trainers import trainer\n+from keras_core.trainers.data_adapters import data_adapters_utils\n+from keras_core.trainers.epoch_iterator import EpochIterator\n \n \n class TensorFlowTrainer(trainer.Trainer):\n     def train_step(self, data):\n-        # TODO: should be\n-        # x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n-        x, y = data\n-        sample_weight = None\n+        x, y, sample_weight = data_adapters_utils.unpack_x_y_sample_weight(data)\n \n+        # Forward pass\n         with tf.GradientTape() as tape:\n+            if self._call_has_training_arg():\n                 y_pred = self(x, training=True)\n+            else:\n+                y_pred = self(x)\n             loss = self.compute_loss(\n                 x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n             )\n \n         # Compute gradients\n-        trainable_weights = self.trainable_weights\n+        trainable_weights = [v.value for v in self.trainable_weights]\n         gradients = tape.gradient(loss, trainable_weights)\n+\n+        print(\"loss\", loss)\n+        print(\"trainable_weights\", trainable_weights)\n+        print(\"gradients\", gradients)\n+\n         # Update weights\n         self.optimizer.apply_gradients(zip(gradients, trainable_weights))\n         return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n@@ -35,13 +42,34 @@ class TensorFlowTrainer(trainer.Trainer):\n     def predict_step(self, data):\n         raise NotImplementedError\n \n-    def make_train_function(self):\n+    def make_train_function(self, force=False):\n+        # TODO: support tf.distribute and steps_per_execution.\n+        if self.train_function is not None and not force:\n+            return self.train_function\n+\n+        def step_function(data):\n+            \"\"\"Runs a single training step.\"\"\"\n+            return self.train_step(data)\n+\n+        if self.jit_compile:\n+            train_function = tf.function(\n+                step_function, jit_compile=True, reduce_retracing=True\n+            )\n+        elif not self.run_eagerly:\n+            train_function = tf.function(step_function, reduce_retracing=True)\n+        else:\n+            train_function = step_function\n+\n+        print(\"run eagerly:\", self.run_eagerly)\n+        print(\"train_function:\", train_function)\n+        print(\"self._compile_loss\", self._compile_loss)\n+\n+        self.train_function = train_function\n+\n+    def make_test_function(self, force=False):\n         raise NotImplementedError\n \n-    def make_test_function(self):\n-        raise NotImplementedError\n-\n-    def make_predict_function(self):\n+    def make_predict_function(self, force=False):\n         raise NotImplementedError\n \n     def fit(\n@@ -63,6 +91,10 @@ class TensorFlowTrainer(trainer.Trainer):\n         validation_batch_size=None,\n         validation_freq=1,\n     ):\n+        if not self.compiled:\n+            raise ValueError(\n+                \"You must call `compile()` before calling `fit()`.\"\n+            )\n         # TODO: respect compiled trainable state\n         if validation_split and validation_data is None:\n             # Create the validation data using the training data. Only supported\n@@ -71,7 +103,7 @@ class TensorFlowTrainer(trainer.Trainer):\n                 x,\n                 y,\n                 sample_weight,\n-            ), validation_data = data_adapter.train_validation_split(\n+            ), validation_data = data_adapters_utils.train_validation_split(\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n@@ -80,21 +112,17 @@ class TensorFlowTrainer(trainer.Trainer):\n                 val_x,\n                 val_y,\n                 val_sample_weight,\n-            ) = data_adapter.unpack_x_y_sample_weight(validation_data)\n+            ) = data_adapters_utils.unpack_x_y_sample_weight(validation_data)\n \n         # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n-        data_handler = data_adapter.get_data_handler(\n+        epoch_iterator = EpochIterator(\n             x=x,\n             y=y,\n             sample_weight=sample_weight,\n             batch_size=batch_size,\n             steps_per_epoch=steps_per_epoch,\n-            initial_epoch=initial_epoch,\n-            epochs=epochs,\n             shuffle=shuffle,\n             class_weight=class_weight,\n-            model=self,\n-            steps_per_execution=self._steps_per_execution,\n         )\n \n         # Container that configures and calls `tf.keras.Callback`s.\n@@ -103,57 +131,39 @@ class TensorFlowTrainer(trainer.Trainer):\n                 callbacks,\n                 add_history=True,\n                 add_progbar=verbose != 0,\n-                model=self,\n                 verbose=verbose,\n                 epochs=epochs,\n-                steps=data_handler.inferred_steps,\n+                steps=epoch_iterator.num_batches,\n+                model=self,\n             )\n \n         self.stop_training = False\n-        self.train_function = self.make_train_function()\n+        self.make_train_function()\n         callbacks.on_train_begin()\n         training_logs = None\n         logs = None\n-        for epoch, iterator in data_handler.enumerate_epochs():\n+        for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n-            with data_handler.catch_stop_iteration():\n-                for step in data_handler.steps():\n+            for step, batch in epoch_iterator.enumerate_epoch():\n                 callbacks.on_train_batch_begin(step)\n-                    logs = self.train_function(iterator)\n-                    end_step = step + data_handler.step_increment\n-                    callbacks.on_train_batch_end(end_step, logs)\n+                logs = self.train_function(batch)\n+                callbacks.on_train_batch_end(step, logs)\n                 if self.stop_training:\n                     break\n \n-            logs = sync_to_numpy_or_python_type(logs)\n-            if logs is None:\n-                raise ValueError(\n-                    \"Unexpected result of `train_function` \"\n-                    \"(Empty logs). This could be due to issues in input \"\n-                    \"pipeline that resulted in an empty dataset. \"\n-                    \"Otherwise, please use \"\n-                    \"`Model.compile(..., run_eagerly=True)`, or \"\n-                    \"`tf.config.run_functions_eagerly(True)` for more \"\n-                    \"information of where went wrong.\"\n-                )\n             # Override with model metrics instead of last step logs\n-            logs = self._validate_and_get_metrics_result(logs)\n-            epoch_logs = copy.copy(logs)\n+            epoch_logs = self.get_metrics_result()\n \n             # Run validation.\n             if validation_data and self._should_eval(epoch, validation_freq):\n-                if self._pss_evaluation_shards:\n-                    self._disallow_exact_eval_with_add_metrics()\n-                # Create data_handler for evaluation and cache it.\n-                if getattr(self, \"_eval_data_handler\", None) is None:\n-                    self._eval_data_handler = data_adapter.get_data_handler(\n+                # Create EpochIterator for evaluation and cache it.\n+                if getattr(self, \"_eval_epoch_iterator\", None) is None:\n+                    self._eval_epoch_iterator = EpochIterator(\n                         x=val_x,\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n-                        steps_per_epoch=validation_steps,\n-                        initial_epoch=0,\n                         epochs=1,\n                     )\n                 val_logs = self.evaluate(\n@@ -180,11 +190,11 @@ class TensorFlowTrainer(trainer.Trainer):\n             isinstance(self.optimizer, optimizers_module.Optimizer)\n             and epochs > 0\n         ):\n-            self.optimizer.finalize_variable_values(self.trainable_variables)\n+            self.optimizer.finalize_variable_values(self.trainable_weights)\n \n-        # If eval data_handler exists, delete it after all epochs are done.\n-        if getattr(self, \"_eval_data_handler\", None) is not None:\n-            del self._eval_data_handler\n+        # If _eval_epoch_iterator exists, delete it after all epochs are done.\n+        if getattr(self, \"_eval_epoch_iterator\", None) is not None:\n+            del self._eval_epoch_iterator\n         callbacks.on_train_end(logs=training_logs)\n         return self.history\n \n@@ -206,45 +216,3 @@ class TensorFlowTrainer(trainer.Trainer):\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n         raise NotImplementedError\n-\n-\n-def sync_to_numpy_or_python_type(tensors):\n-    \"\"\"Syncs and converts a structure of `Tensor`s to `NumPy` arrays or Python\n-    scalar types.\n-\n-    For each tensor, it calls `tensor.numpy()`. If the result is a scalar value,\n-    it converts it to a Python type, such as a float or int, by calling\n-    `result.item()`.\n-\n-    Numpy scalars are converted, as Python types are often more convenient to\n-    deal with. This is especially useful for bfloat16 Numpy scalars, which don't\n-    support as many operations as other Numpy values.\n-\n-    Async strategies (such as `TPUStrategy` and `ParameterServerStrategy`) are\n-    forced to sync during this process.\n-\n-    Args:\n-        tensors: A structure of tensors.\n-\n-    Returns:\n-        `tensors`, but scalar tensors are converted to Python types and non-scalar\n-        tensors are converted to Numpy arrays.\n-    \"\"\"\n-    if isinstance(tensors, tf.distribute.experimental.coordinator.RemoteValue):\n-        tensors = tensors.fetch()\n-    if isinstance(tensors, list) and isinstance(\n-        tensors[0], tf.distribute.experimental.coordinator.RemoteValue\n-    ):\n-        tensors = tf.nest.map_structure(lambda t: t.fetch(), tensors)\n-\n-    def _to_single_numpy_or_python_type(t):\n-        # Don't turn ragged or sparse tensors to NumPy.\n-        if isinstance(t, tf.Tensor):\n-            t = t.numpy()\n-        # Strings, ragged and sparse tensors don't have .item(). Return them\n-        # as-is.\n-        if not isinstance(t, (np.ndarray, np.generic)):\n-            return t\n-        return t.item() if np.ndim(t) == 0 else t\n-\n-    return tf.nest.map_structure(_to_single_numpy_or_python_type, tensors)\n\n@@ -0,0 +1,35 @@\n+import numpy as np\n+\n+from keras_core import testing\n+from keras_core import layers\n+from keras_core import optimizers\n+from keras_core import losses\n+from keras_core.backend.tensorflow.trainer import TensorFlowTrainer\n+\n+\n+# A model is just a layer mixed in with a Trainer.\n+class TFModel(layers.Dense, TensorFlowTrainer):\n+    def __init__(self, units):\n+        layers.Dense.__init__(self, units=units)\n+        TensorFlowTrainer.__init__(self)\n+\n+\n+class TestTFTrainer(testing.TestCase):\n+    def _test_basic_flow(self, run_eagerly, jit_compile):\n+        model = TFModel(units=3)\n+        x = np.random.random((100, 4))\n+        y = np.random.random((100, 3))\n+        batch_size = 16\n+        epochs = 3\n+\n+        model.compile(optimizer=optimizers.SGD(), loss=losses.MeanSquaredError(), run_eagerly=run_eagerly, jit_compile=jit_compile)\n+        model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+\n+    def test_basic_flow_eager(self):\n+        self._test_basic_flow(run_eagerly=True, jit_compile=False)\n+\n+    def test_basic_flow_graph_fn(self):\n+        self._test_basic_flow(run_eagerly=False, jit_compile=False)\n+\n+    def test_basic_flow_jit(self):\n+        self._test_basic_flow(run_eagerly=False, jit_compile=True)\n\n@@ -5,6 +5,14 @@ from keras_core.trainers.compile_utils import CompileMetrics\n \n \n class Trainer:\n+    def __init__(self):\n+        self._run_eagerly = False\n+        self._jit_compile = True\n+        self.compiled = False\n+        self.train_function = None\n+        self.test_function = None\n+        self.predict_function = None\n+ \n     def compile(\n         self,\n         optimizer,\n@@ -13,7 +21,7 @@ class Trainer:\n         metrics=None,\n         weighted_metrics=None,\n         run_eagerly=False,\n-        jit_compile=False,\n+        jit_compile=True,\n     ):\n         # TODO: get from module\n         self.optimizer = optimizer\n@@ -25,8 +33,12 @@ class Trainer:\n             self._compile_metrics = CompileMetrics(metrics, weighted_metrics)\n         else:\n             self._compile_metrics = None\n+        if jit_compile and run_eagerly:\n+            raise ValueError(\"If `jit_compile` is True, then `run_eagerly` cannot also be True.\")\n         self.jit_compile = jit_compile\n         self.run_eagerly = run_eagerly\n+        self.stop_training = False\n+        self.compiled = True\n \n     @property\n     def jit_compile(self):\n@@ -106,8 +118,8 @@ class Trainer:\n         \"\"\"\n         del x  # The default implementation does not use `x`.\n         losses = []\n-        if self._compile_metrics is not None:\n-            loss = self.compile_loss(y, y_pred, sample_weight)\n+        if self._compile_loss is not None:\n+            loss = self._compile_loss(y, y_pred, sample_weight)\n             if loss is not None:\n                 losses.append(loss)\n         for l in self.losses:\n@@ -236,6 +248,19 @@ class Trainer:\n     ):\n         raise NotImplementedError\n     \n+    def _should_eval(self, epoch, validation_freq):\n+        epoch = epoch + 1  # one-index the user-facing epoch.\n+        if isinstance(validation_freq, int):\n+            return epoch % validation_freq == 0\n+        elif isinstance(validation_freq, list):\n+            return epoch in validation_freq\n+        else:\n+            raise ValueError(\n+                \"Expected `validation_freq` to be a list or int. \"\n+                f\"Received: validation_freq={validation_freq} of the \"\n+                f\"type {type(validation_freq)}.\"\n+            )\n+\n     def get_compile_config(self):\n         # TODO\n         raise NotImplementedError\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
