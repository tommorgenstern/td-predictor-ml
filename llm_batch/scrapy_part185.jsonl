{"custom_id": "scrapy#5846d6154ca3adf87dfee7ba7294ef35949177ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 660 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n \"\"\"\n-Scrapy - a screen scraping framework written in Python\n+Scrapy - a web crawling and screen scraping framework written for Python\n \"\"\"\n \n __all__ = ['__version__', 'version_info', 'optional_features', 'twisted_version',\n\n@@ -10,7 +10,7 @@ setup(\n     name='Scrapy',\n     version=version,\n     url='http://scrapy.org',\n-    description='A high-level Python Screen Scraping framework',\n+    description='A high-level Web Crawling and Screen Scraping framework',\n     long_description=open('README.rst').read(),\n     author='Scrapy developers',\n     maintainer='Pablo Hoffman',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6c7bd54fc37d2f21bbbd0927eab2da827f070852", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 319 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -178,6 +178,10 @@ class SelectorList(list):\n     def extract(self):\n         return [x.extract() for x in self]\n \n+    def extract_first(self):\n+        for x in self.extract():\n+            return x\n+\n     @deprecated(use_instead='.extract()')\n     def extract_unquoted(self):\n         return [x.extract_unquoted() for x in self]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd126be3569ddd77c458c79ef9e066cacf3a3af1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 323 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -179,8 +179,8 @@ class SelectorList(list):\n         return [x.extract() for x in self]\n \n     def extract_first(self):\n-        for x in self.extract():\n-            return x\n+        for x in self:\n+            return x.extract()\n \n     @deprecated(use_instead='.extract()')\n     def extract_unquoted(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2742b4d8c26f946c20767fd7fd3f227d00002597", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 52/52 | Churn Δ: 17 | Churn Cumulative: 17 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,6 +55,23 @@ class SelectorTestCase(unittest.TestCase):\n             [\"<Selector xpath=u'//input[@value=\\\"\\\\xa9\\\"]/@value' data=u'\\\\xa9'>\"]\n         )\n \n+    def test_extract_first(self):\n+        \"\"\"Test if extract_first() returns first element\"\"\"\n+        body = '<ul><li id=\"1\">1</li><li id=\"2\">2</li></ul>'\n+        response = TextResponse(url=\"http://example.com\", body=body)\n+        sel = self.sscls(response)\n+\n+        self.assertEqual(sel.xpath('//ul/li/text()').extract_first(),\n+                         sel.xpath('//ul/li/text()').extract()[0])\n+\n+        self.assertEqual(sel.xpath('//ul/li[@id=\"1\"]/text()').extract_first(),\n+                         sel.xpath('//ul/li[@id=\"1\"]/text()').extract()[0])\n+\n+        self.assertEqual(sel.xpath('//ul/li[2]/text()').extract_first(),\n+                         sel.xpath('//ul/li/text()').extract()[1])\n+\n+        self.assertEqual(sel.xpath('/ul/li[@id=\"doesnt-exist\"]/text()').extract_first(), None)\n+\n     def test_select_unicode_query(self):\n         body = u\"<p><input name='\\xa9' value='1'/></p>\"\n         response = TextResponse(url=\"http://example.com\", body=body, encoding='utf8')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f92bc09bf433c43ab5669f7bc14108ac6fd49e5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 39 | Churn Cumulative: 742 | Contributors (this commit): 12 | Commits (past 90d): 23 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,7 @@ from lxml import etree\n \n from scrapy.utils.misc import extract_regex\n from scrapy.utils.trackref import object_ref\n-from scrapy.utils.python import unicode_to_str, flatten\n+from scrapy.utils.python import unicode_to_str, flatten, iflatten\n from scrapy.utils.decorator import deprecated\n from scrapy.http import HtmlResponse, XmlResponse\n from .lxmldocument import LxmlDocument\n@@ -175,6 +175,10 @@ class SelectorList(list):\n     def re(self, regex):\n         return flatten([x.re(regex) for x in self])\n \n+    def re_first(self, regex):\n+        for el in iflatten((x.re(regex) for x in self)):\n+            return el\n+\n     def extract(self):\n         return [x.extract() for x in self]\n \n\n@@ -27,13 +27,20 @@ def flatten(x):\n     >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n     [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\"\"\"\n \n-    result = []\n+    return list(iflatten(x))\n+\n+\n+def iflatten(x):\n+    \"\"\"iflatten(sequence) -> iterator\n+\n+    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n+\n     for el in x:\n         if hasattr(el, \"__iter__\"):\n-            result.extend(flatten(el))\n+            for el_ in flatten(el):\n+                yield el_\n         else:\n-            result.append(el)\n-    return result\n+            yield el\n \n \n def unique(list_, key=lambda x: x):\n\n@@ -72,6 +72,24 @@ class SelectorTestCase(unittest.TestCase):\n \n         self.assertEqual(sel.xpath('/ul/li[@id=\"doesnt-exist\"]/text()').extract_first(), None)\n \n+    def test_re_first(self):\n+        \"\"\"Test if re_first() returns first matched element\"\"\"\n+        body = '<ul><li id=\"1\">1</li><li id=\"2\">2</li></ul>'\n+        response = TextResponse(url=\"http://example.com\", body=body)\n+        sel = self.sscls(response)\n+\n+        self.assertEqual(sel.xpath('//ul/li/text()').re_first('\\d'),\n+                         sel.xpath('//ul/li/text()').re('\\d')[0])\n+\n+        self.assertEqual(sel.xpath('//ul/li[@id=\"1\"]/text()').re_first('\\d'),\n+                         sel.xpath('//ul/li[@id=\"1\"]/text()').re('\\d')[0])\n+\n+        self.assertEqual(sel.xpath('//ul/li[2]/text()').re_first('\\d'),\n+                         sel.xpath('//ul/li/text()').re('\\d')[1])\n+\n+        self.assertEqual(sel.xpath('/ul/li/text()').re_first('\\w+'), None)\n+        self.assertEqual(sel.xpath('/ul/li[@id=\"doesnt-exist\"]/text()').re_first('\\d'), None)\n+\n     def test_select_unicode_query(self):\n         body = u\"<p><input name='\\xa9' value='1'/></p>\"\n         response = TextResponse(url=\"http://example.com\", body=body, encoding='utf8')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0dade7315bc59ce6b2cfacdd17895985882f5ae1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 331 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -176,7 +176,7 @@ class SelectorList(list):\n         return flatten([x.re(regex) for x in self])\n \n     def re_first(self, regex):\n-        for el in iflatten((x.re(regex) for x in self)):\n+        for el in iflatten(x.re(regex) for x in self):\n             return el\n \n     def extract(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cda39225073ac06ef5563b7a0fd8f8a09194ad69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 185 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,8 @@ See documentation in docs/topics/request-response.rst\n \n import copy\n \n+from six.moves.urllib.parse import urljoin\n+\n from scrapy.http.headers import Headers\n from scrapy.utils.trackref import object_ref\n from scrapy.http.common import obsolete_setter\n@@ -75,3 +77,8 @@ class Response(object_ref):\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n+\n+    def urljoin(self, url):\n+        \"\"\"Join this Response's url with a possible relative url to form an\n+        absolute interpretation of the latter.\"\"\"\n+        return urljoin(self.url, url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b6d5a011a5485f3f2c01b32fa425b512d90d7ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 27/27 | Churn Δ: 12 | Churn Cumulative: 12 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -132,6 +132,18 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             \"\\r\\n\"\n             \"name=value\")\n \n+        # test a POST method with no body provided\n+        factory = client.ScrapyHTTPClientFactory(Request(\n+            method='POST',\n+            url='http://foo/bar'\n+        ))\n+\n+        self._test(factory,\n+                   \"POST /bar HTTP/1.0\\r\\n\"\n+                   \"Host: foo\\r\\n\"\n+                   \"Content-Length: 0\\r\\n\"\n+                   \"\\r\\n\")\n+\n         # test with single and multivalued headers\n         factory = client.ScrapyHTTPClientFactory(Request(\n             url='http://foo/bar',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#deb5bb530cfe14993f92d3031820153ae53e0edb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 247 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -112,12 +112,14 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n \n         # set Host header based on url\n         self.headers.setdefault('Host', self.netloc)\n-\n         # set Content-Length based len of body\n         if self.body is not None:\n             self.headers['Content-Length'] = len(self.body)\n             # just in case a broken http/1.1 decides to keep connection alive\n             self.headers.setdefault(\"Connection\", \"close\")\n+        # Content-Length must be specified in POST method even with no body\n+        elif self.method == 'POST':\n+            self.headers['Content-Length'] = 0\n \n     def _build_response(self, body, request):\n         request.meta['download_latency'] = self.headers_time-self.start_time\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#549882590bfdb3d66b623ea7414fac5097560a96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 250 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -118,6 +118,9 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n             self.headers['Content-Length'] = len(self.body)\n             # just in case a broken http/1.1 decides to keep connection alive\n             self.headers.setdefault(\"Connection\", \"close\")\n+        # Content-Length must be specified in POST method even with no body\n+        elif self.method == 'POST':\n+            self.headers['Content-Length'] = 0\n \n     def _build_response(self, body, request):\n         request.meta['download_latency'] = self.headers_time-self.start_time\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aaeb837db409442579d260f703e2f2ca705020ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 28 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,17 @@\n from OpenSSL import SSL\n from twisted.internet.ssl import ClientContextFactory\n+try:\n+    # available since twisted 14.0\n+    from twisted.internet._sslverify import ClientTLSOptions\n+except ImportError:\n+    ClientTLSOptions = None\n \n \n class ScrapyClientContextFactory(ClientContextFactory):\n     \"A SSL context factory which is more permissive against SSL bugs.\"\n     # see https://github.com/scrapy/scrapy/issues/82\n     # and https://github.com/scrapy/scrapy/issues/26\n+    # and https://github.com/scrapy/scrapy/issues/981\n \n     def __init__(self):\n         # see this issue on why we use TLSv1_METHOD by default\n@@ -17,4 +23,6 @@ class ScrapyClientContextFactory(ClientContextFactory):\n         # Enable all workarounds to SSL bugs as documented by\n         # http://www.openssl.org/docs/ssl/SSL_CTX_set_options.html\n         ctx.set_options(SSL.OP_ALL)\n+        if hostname and ClientTLSOptions is not None: # workaround for TLS SNI\n+            ClientTLSOptions(hostname, ctx)\n         return ctx\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1134a9cab055969b3232a16037d16e5ab93759f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 115 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,7 +63,10 @@ def get_config(use_closest=True):\n \n \n def get_sources(use_closest=True):\n+    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or \\\n+        os.path.expanduser('~/.config')\n     sources = ['/etc/scrapy.cfg', r'c:\\scrapy\\scrapy.cfg',\n+               xdg_config_home + '/scrapy.cfg',\n                os.path.expanduser('~/.scrapy.cfg')]\n     if use_closest:\n         sources.append(closest_scrapy_cfg())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#84edc2ebc7afcdfd6b4789a6e1725b83eb9076ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 31/30 | Churn Δ: 35 | Churn Cumulative: 780 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,7 +55,7 @@ class Response(object_ref):\n         elif body is None:\n             self._body = ''\n         else:\n-            raise TypeError(\"Response body must either str or unicode. Got: '%s'\" \\\n+            raise TypeError(\"Response body must either be str or unicode. Got: '%s'\" \\\n                 % type(body).__name__)\n \n     body = property(_get_body, obsolete_setter(_set_body, 'body'))\n\n@@ -5,9 +5,12 @@ discovering (through HTTP headers) to base Response class.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n+from six.moves.urllib.parse import urljoin\n+\n from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n     html_body_declared_encoding, http_content_type_encoding\n from scrapy.http.response import Response\n+from scrapy.utils.response import get_base_url\n from scrapy.utils.python import memoizemethod_noargs\n \n \n@@ -63,6 +66,11 @@ class TextResponse(Response):\n             self._cached_ubody = html_to_unicode(charset, self.body)[1]\n         return self._cached_ubody\n \n+    def urljoin(self, url):\n+        \"\"\"Join this Response's url with a possible relative url to form an\n+        absolute interpretation of the latter.\"\"\"\n+        return urljoin(get_base_url(self), url)\n+\n     @memoizemethod_noargs\n     def _headers_encoding(self):\n         content_type = self.headers.get('Content-Type')\n\n@@ -13,7 +13,6 @@ from twisted.web import http\n from twisted.web.http import RESPONSES\n from w3lib import html\n \n-from scrapy.http import HtmlResponse, TextResponse\n from scrapy.utils.decorator import deprecated\n \n \n@@ -73,6 +72,7 @@ def open_in_browser(response, _openfunc=webbrowser.open):\n     \"\"\"Open the given response in a local web browser, populating the <base>\n     tag for external links to work\n     \"\"\"\n+    from scrapy.http import HtmlResponse, TextResponse\n     # XXX: this implementation is a bit dirty and could be improved\n     body = response.body\n     if isinstance(response, HtmlResponse):\n\n@@ -113,6 +113,12 @@ class BaseResponseTest(unittest.TestCase):\n         self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')\n         self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')\n \n+    def test_urljoin(self):\n+        \"\"\"Test urljoin shortcut (only for existence, since behavior equals urljoin)\"\"\"\n+        joined = self.response_class('http://www.example.com').urljoin('/test')\n+        absolute = 'http://www.example.com/test'\n+        self.assertEqual(joined, absolute)\n+\n \n class ResponseText(BaseResponseTest):\n \n@@ -295,6 +301,23 @@ class TextResponseTest(BaseResponseTest):\n             response.selector.css(\"title::text\").extract(),\n         )\n \n+    def test_urljoin_with_base_url(self):\n+        \"\"\"Test urljoin shortcut which also evaluates base-url through get_base_url().\"\"\"\n+        body = '<html><body><base href=\"https://example.net\"></body></html>'\n+        joined = self.response_class('http://www.example.com', body=body).urljoin('/test')\n+        absolute = 'https://example.net/test'\n+        self.assertEqual(joined, absolute)\n+\n+        body = '<html><body><base href=\"/elsewhere\"></body></html>'\n+        joined = self.response_class('http://www.example.com', body=body).urljoin('test')\n+        absolute = 'http://www.example.com/test'\n+        self.assertEqual(joined, absolute)\n+\n+        body = '<html><body><base href=\"/elsewhere/\"></body></html>'\n+        joined = self.response_class('http://www.example.com', body=body).urljoin('test')\n+        absolute = 'http://www.example.com/elsewhere/test'\n+        self.assertEqual(joined, absolute)\n+\n \n class HtmlResponseTest(TextResponseTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c1cd019f56b703509bff5cf97da7e2c70fb466b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1343 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,7 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n from scrapy import log, signals\n+from pdb import set_trace\n \n \n class Crawler(object):\n@@ -152,6 +153,8 @@ class CrawlerProcess(CrawlerRunner):\n         if self.settings.getbool('DNSCACHE_ENABLED'):\n             reactor.installResolver(CachingThreadedResolver(reactor))\n \n+        tp = reactor.getThreadPool()\n+        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n\n@@ -194,6 +194,8 @@ NEWSPIDER_MODULE = ''\n \n RANDOMIZE_DOWNLOAD_DELAY = True\n \n+REACTOR_THREADPOOL_MAXSIZE = 10\n+\n REDIRECT_ENABLED = True\n REDIRECT_MAX_TIMES = 20  # uses Firefox default setting\n REDIRECT_PRIORITY_ADJUST = +2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5916df64cd20ccc5b7fe02410b7ea485892a39da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1084 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,6 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n from scrapy import log, signals\n-from pdb import set_trace\n \n \n class Crawler(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dc88be75e26cf970f65cd5c3c7f5908cb4b68834", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 22 | Files Changed: 5 | Hunks: 24 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 1601 | Contributors (this commit): 24 | Commits (past 90d): 6 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -5,10 +5,11 @@ import glob\n import tempfile\n import shutil\n import time\n-import urllib2\n import netrc\n import json\n+from six.moves.urllib import request\n from six.moves.urllib.parse import urlparse, urljoin\n+from six.moves.urllib.error import HTTPError, URLError\n from subprocess import Popen, PIPE, check_call\n \n from w3lib.form import encode_multipart\n@@ -69,7 +70,7 @@ class Command(ScrapyCommand):\n         except ImportError:\n             raise UsageError(\"setuptools not installed\")\n \n-        urllib2.install_opener(urllib2.build_opener(HTTPRedirectHandler))\n+        request.install_opener(request.build_opener(HTTPRedirectHandler))\n \n         if opts.list_targets:\n             for name, target in _get_targets().items():\n@@ -78,9 +79,9 @@ class Command(ScrapyCommand):\n \n         if opts.list_projects:\n             target = _get_target(opts.list_projects)\n-            req = urllib2.Request(_url(target, 'listprojects.json'))\n+            req = request.Request(_url(target, 'listprojects.json'))\n             _add_auth_header(req, target)\n-            f = urllib2.urlopen(req)\n+            f = request.urlopen(req)\n             projects = json.loads(f.read())['projects']\n             print(os.linesep.join(projects))\n             return\n@@ -188,7 +189,7 @@ def _upload_egg(target, eggpath, project, version):\n         'Content-Type': 'multipart/form-data; boundary=%s' % boundary,\n         'Content-Length': str(len(body)),\n     }\n-    req = urllib2.Request(url, body, headers)\n+    req = request.Request(url, body, headers)\n     _add_auth_header(req, target)\n     _log('Deploying to project \"%s\" in %s' % (project, url))\n     return _http_post(req)\n@@ -207,14 +208,14 @@ def _add_auth_header(request, target):\n \n def _http_post(request):\n     try:\n-        f = urllib2.urlopen(request)\n+        f = request.urlopen(request)\n         _log(\"Server response (%s):\" % f.code)\n         print(f.read())\n         return True\n-    except urllib2.HTTPError as e:\n+    except HTTPError as e:\n         _log(\"Deploy failed (%s):\" % e.code)\n         print(e.read())\n-    except urllib2.URLError as e:\n+    except URLError as e:\n         _log(\"Deploy failed: %s\" % e)\n \n def _build_egg():\n@@ -237,12 +238,12 @@ def _create_default_setup_py(**kwargs):\n         f.write(_SETUP_PY_TEMPLATE % kwargs)\n \n \n-class HTTPRedirectHandler(urllib2.HTTPRedirectHandler):\n+class HTTPRedirectHandler(request.HTTPRedirectHandler):\n \n     def redirect_request(self, req, fp, code, msg, headers, newurl):\n         newurl = newurl.replace(' ', '%20')\n         if code in (301, 307):\n-            return urllib2.Request(newurl,\n+            return request.Request(newurl,\n                                    data=req.get_data(),\n                                    headers=req.headers,\n                                    origin_req_host=req.get_origin_req_host(),\n@@ -250,9 +251,9 @@ class HTTPRedirectHandler(urllib2.HTTPRedirectHandler):\n         elif code in (302, 303):\n             newheaders = dict((k, v) for k, v in req.headers.items()\n                               if k.lower() not in (\"content-length\", \"content-type\"))\n-            return urllib2.Request(newurl,\n+            return request.Request(newurl,\n                                    headers=newheaders,\n                                    origin_req_host=req.get_origin_req_host(),\n                                    unverifiable=True)\n         else:\n-            raise urllib2.HTTPError(req.get_full_url(), code, msg, headers, fp)\n+            raise HTTPError(req.get_full_url(), code, msg, headers, fp)\n\n@@ -1,6 +1,10 @@\n import base64\n-from urllib import getproxies, unquote, proxy_bypass\n+from six.moves.urllib.request import getproxies, proxy_bypass\n+from six.moves.urllib.parse import unquote\n+try:\n     from urllib2 import _parse_proxy\n+except ImportError:\n+    from urllib.request import _parse_proxy\n from six.moves.urllib.parse import urlunparse\n \n from scrapy.utils.httpobj import urlparse_cached\n\n@@ -5,8 +5,7 @@ This module implements the FormRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n-import urllib\n-from six.moves.urllib.parse import urljoin\n+from six.moves.urllib.parse import urljoin, urlencode\n import lxml.html\n import six\n from scrapy.http.request import Request\n@@ -52,7 +51,7 @@ def _urlencode(seq, enc):\n     values = [(unicode_to_str(k, enc), unicode_to_str(v, enc))\n               for k, vs in seq\n               for v in (vs if hasattr(vs, '__iter__') else [vs])]\n-    return urllib.urlencode(values, doseq=1)\n+    return urlencode(values, doseq=1)\n \n \n def _get_form(response, formname, formnumber, formxpath):\n\n@@ -7,8 +7,8 @@ to the w3lib.url module. Always import those from there instead.\n \"\"\"\n import posixpath\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n-                                    urlparse, parse_qsl)\n-import urllib\n+                                    urlparse, parse_qsl, urlencode,\n+                                    unquote)\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this move doesn't break old code\n from w3lib.url import *\n@@ -56,7 +56,7 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n     scheme, netloc, path, params, query, fragment = parse_url(url)\n     keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n-    query = urllib.urlencode(keyvals)\n+    query = urlencode(keyvals)\n     path = safe_url_string(_unquotepath(path)) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n@@ -65,7 +65,7 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n-    return urllib.unquote(path)\n+    return unquote(path)\n \n \n def parse_url(url, encoding=None):\n\n@@ -1,6 +1,7 @@\n from __future__ import print_function\n-import sys, time, random, urllib, os, json\n+import sys, time, random, os, json\n import six\n+from six.moves.urllib.parse import urlencode\n from subprocess import Popen, PIPE\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n@@ -73,7 +74,7 @@ class Follow(LeafResource):\n         args = request.args.copy()\n         for nl in nlist:\n             args[\"n\"] = [str(nl)]\n-            argstr = urllib.urlencode(args, doseq=True)\n+            argstr = urlencode(args, doseq=True)\n             s += \"<a href='/follow?%s'>follow %d</a><br>\" % (argstr, nl)\n         s += \"\"\"</body>\"\"\"\n         request.write(s)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f9265b44be6f52a2af94458a86fb4ba503fa74d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 33 | Churn Cumulative: 1176 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,13 +18,17 @@ class Crawler(object):\n \n     def __init__(self, spidercls, settings):\n         self.spidercls = spidercls\n-        self.settings = settings\n+        self.settings = settings.copy()\n+\n         self.signals = SignalManager(self)\n         self.stats = load_object(self.settings['STATS_CLASS'])(self)\n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n         self.extensions = ExtensionManager.from_crawler(self)\n \n+        self.spidercls.update_settings(self.settings)\n+        self.settings.freeze()\n+\n         self.crawling = False\n         self.spider = None\n         self.engine = None\n@@ -95,11 +99,7 @@ class CrawlerRunner(object):\n     def _create_crawler(self, spidercls):\n         if isinstance(spidercls, six.string_types):\n             spidercls = self.spiders.load(spidercls)\n-\n-        crawler_settings = self.settings.copy()\n-        spidercls.update_settings(crawler_settings)\n-        crawler_settings.freeze()\n-        return Crawler(spidercls, crawler_settings)\n+        return Crawler(spidercls, self.settings)\n \n     def _setup_crawler_logging(self, crawler):\n         log_observer = log.start_from_crawler(crawler)\n\n@@ -25,16 +25,6 @@ class CrawlerTestCase(unittest.TestCase):\n             self.crawler.spiders\n             self.assertEqual(len(w), 1, \"Warn deprecated access only once\")\n \n-\n-class CrawlerRunnerTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        self.crawler_runner = CrawlerRunner(Settings())\n-\n-    def tearDown(self):\n-        return self.crawler_runner.stop()\n-\n-    @defer.inlineCallbacks\n     def test_populate_spidercls_settings(self):\n         spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n         project_settings = {'TEST1': 'project', 'TEST3': 'project'}\n@@ -42,12 +32,13 @@ class CrawlerRunnerTest(unittest.TestCase):\n         class CustomSettingsSpider(DefaultSpider):\n             custom_settings = spider_settings\n \n-        self.crawler_runner.settings.setdict(project_settings,\n-                                             priority='project')\n+        settings = Settings()\n+        settings.setdict(project_settings, priority='project')\n+        crawler = Crawler(CustomSettingsSpider, settings)\n \n-        d = self.crawler_runner.crawl(CustomSettingsSpider)\n-        crawler = list(self.crawler_runner.crawlers)[0]\n-        yield d\n         self.assertEqual(crawler.settings.get('TEST1'), 'spider')\n         self.assertEqual(crawler.settings.get('TEST2'), 'spider')\n         self.assertEqual(crawler.settings.get('TEST3'), 'project')\n+\n+        self.assertFalse(settings.frozen)\n+        self.assertTrue(crawler.settings.frozen)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85aa3c7596c6e9c66daaa5503faadd03a16e1d59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 1413 | Contributors (this commit): 19 | Commits (past 90d): 8 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -149,9 +149,9 @@ class CrawlerProcess(CrawlerRunner):\n                 return\n             d.addBoth(lambda _: self._stop_reactor())\n \n-        if self.settings.getbool('DNSCACHE_ENABLED'):\n-            reactor.installResolver(CachingThreadedResolver(reactor))\n-\n+        cache_size = self.settings.getint('DNSCACHE_SIZE') if self.settings.getbool('DNSCACHE_ENABLED') else 0\n+        reactor.installResolver(CachingThreadedResolver(reactor, cache_size,\n+                                                            self.settings.getfloat('DNS_TIMEOUT')))\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n\n@@ -4,15 +4,20 @@ from twisted.internet.base import ThreadedResolver\n from scrapy.utils.datatypes import LocalCache\n \n # TODO: cache misses\n-# TODO: make cache size a setting\n \n dnscache = LocalCache(10000)\n \n class CachingThreadedResolver(ThreadedResolver):\n+    def __init__(self, reactor, cache_size, timeout):\n+        super(CachingThreadedResolver, self).__init__(reactor)\n+        dnscache.limit = cache_size\n+        self.timeout = timeout\n \n-    def getHostByName(self, name, timeout = (1, 3, 11, 45)):\n+    def getHostByName(self, name, timeout=None):\n         if name in dnscache:\n             return defer.succeed(dnscache[name])\n+        if not timeout:\n+            timeout = self.timeout\n         d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n         d.addCallback(self._cache_result, name)\n         return d\n\n@@ -52,6 +52,8 @@ DEPTH_STATS = True\n DEPTH_PRIORITY = 0\n \n DNSCACHE_ENABLED = True\n+DNSCACHE_SIZE = 10000\n+DNS_TIMEOUT = 60\n \n DOWNLOAD_DELAY = 0\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42f7674619a779b029a3a4a9db8242d90f140453", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 178 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,8 +49,14 @@ class RobotsTxtMiddleware(object):\n             )\n             dfd = self.crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n+            dfd.addErrback(self._logerror, robotsreq, spider)\n         return self._parsers[netloc]\n \n+    def _logerror(self, failure, request, spider):\n+        if failure.type is not IgnoreRequest:\n+            log.msg(format=\"Error downloading %%(request)s: %s\" % failure.value,\n+                    level=log.ERROR, request=request, spider=spider)\n+\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)\n         rp.parse(response.body.splitlines())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5a96a16914ad6e4e54a7bd313802891f82a8dc08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 670 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,10 +31,10 @@ class FormRequest(Request):\n                 self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)\n \n     @classmethod\n-    def from_response(cls, response, formname=None, formnumber=0, formdata=None,\n+    def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n                       clickdata=None, dont_click=False, formxpath=None, **kwargs):\n         kwargs.setdefault('encoding', response.encoding)\n-        form = _get_form(response, formname, formnumber, formxpath)\n+        form = _get_form(response, formname, formid, formnumber, formxpath)\n         formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n         url = _get_form_url(form, kwargs.pop('url', None))\n         method = kwargs.pop('method', form.method)\n@@ -67,6 +67,11 @@ def _get_form(response, formname, formnumber, formxpath):\n         if f:\n             return f[0]\n \n+    if formid is not None:\n+        f = root.xpath('//form[@id=\"%s\"]' % formid)\n+        if f:\n+            return f[0]\n+            \n     # Get form element from xpath, if not found, go up\n     if formxpath is not None:\n         nodes = root.xpath(formxpath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d48c19acd2eb9d5e9e7e8808d349b14c290bda4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 139 | Lines Deleted: 144 | Files Changed: 8 | Hunks: 10 | Methods Changed: 22 | Complexity Δ (Sum/Max): 0/13 | Churn Δ: 283 | Churn Cumulative: 860 | Contributors (this commit): 12 | Commits (past 90d): 8 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,86 @@\n+\"\"\" This module implements the DecompressionMiddleware which tries to recognise\n+and extract the potentially compressed responses that may arrive. \n+\"\"\"\n+\n+import bz2\n+import gzip\n+import zipfile\n+import tarfile\n+from tempfile import mktemp\n+\n+import six\n+\n+try:\n+    from cStringIO import StringIO as BytesIO\n+except ImportError:\n+    from io import BytesIO\n+\n+from scrapy import log\n+from scrapy.responsetypes import responsetypes\n+\n+\n+class DecompressionMiddleware(object):\n+    \"\"\" This middleware tries to recognise and extract the possibly compressed\n+    responses that may arrive. \"\"\"\n+\n+    def __init__(self):\n+        self._formats = {\n+            'tar': self._is_tar,\n+            'zip': self._is_zip,\n+            'gz': self._is_gzip,\n+            'bz2': self._is_bzip2\n+        }\n+\n+    def _is_tar(self, response):\n+        archive = BytesIO(response.body)\n+        try:\n+            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n+        except tarfile.ReadError:\n+            return\n+\n+        body = tar_file.extractfile(tar_file.members[0]).read()\n+        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n+        return response.replace(body=body, cls=respcls)\n+\n+    def _is_zip(self, response):\n+        archive = BytesIO(response.body)\n+        try:\n+            zip_file = zipfile.ZipFile(archive)\n+        except zipfile.BadZipfile:\n+            return\n+\n+        namelist = zip_file.namelist()\n+        body = zip_file.read(namelist[0])\n+        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n+        return response.replace(body=body, cls=respcls)\n+\n+    def _is_gzip(self, response):\n+        archive = BytesIO(response.body)\n+        try:\n+            body = gzip.GzipFile(fileobj=archive).read()\n+        except IOError:\n+            return\n+\n+        respcls = responsetypes.from_args(body=body)\n+        return response.replace(body=body, cls=respcls)\n+\n+    def _is_bzip2(self, response):\n+        try:\n+            body = bz2.decompress(response.body)\n+        except IOError:\n+            return\n+\n+        respcls = responsetypes.from_args(body=body)\n+        return response.replace(body=body, cls=respcls)\n+\n+    def process_response(self, request, response, spider):\n+        if not response.body:\n+            return response\n+\n+        for fmt, func in six.iteritems(self._formats):\n+            new_response = func(response)\n+            if new_response:\n+                log.msg(format='Decompressed response with format: %(responsefmt)s',\n+                        level=log.DEBUG, spider=spider, responsefmt=fmt)\n+                return new_response\n+        return response\n\n@@ -1,9 +0,0 @@\n-\"\"\"\n-This module contains experimental code that may go into scrapy.contrib in the\n-future, but it's not yet stable enough to go there (either API stable or\n-functionality stable).\n-\n-Subscribe to Scrapy developers mailing list or join the IRC channel if you want\n-to discuss about this code.\n-\n-\"\"\"\n\n@@ -1,6 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib_exp.djangoitem` is deprecated, use `scrapy.contrib.djangoitem` instead\",\n-    ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.contrib.djangoitem import DjangoItem\n\n@@ -1,86 +1,7 @@\n-\"\"\" This module implements the DecompressionMiddleware which tries to recognise\n-and extract the potentially compressed responses that may arrive. \n-\"\"\"\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib_exp.downloadermiddleware.decompression` is deprecated, \"\n+              \"use `scrapy.contrib.downloadermiddleware.decompression` instead\",\n+    ScrapyDeprecationWarning, stacklevel=2)\n \n-import bz2\n-import gzip\n-import zipfile\n-import tarfile\n-from tempfile import mktemp\n-\n-import six\n-\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n-    from io import BytesIO\n-\n-from scrapy import log\n-from scrapy.responsetypes import responsetypes\n-\n-\n-class DecompressionMiddleware(object):\n-    \"\"\" This middleware tries to recognise and extract the possibly compressed\n-    responses that may arrive. \"\"\"\n-\n-    def __init__(self):\n-        self._formats = {\n-            'tar': self._is_tar,\n-            'zip': self._is_zip,\n-            'gz': self._is_gzip,\n-            'bz2': self._is_bzip2\n-        }\n-\n-    def _is_tar(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n-        except tarfile.ReadError:\n-            return\n-\n-        body = tar_file.extractfile(tar_file.members[0]).read()\n-        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_zip(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            zip_file = zipfile.ZipFile(archive)\n-        except zipfile.BadZipfile:\n-            return\n-\n-        namelist = zip_file.namelist()\n-        body = zip_file.read(namelist[0])\n-        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_gzip(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            body = gzip.GzipFile(fileobj=archive).read()\n-        except IOError:\n-            return\n-\n-        respcls = responsetypes.from_args(body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_bzip2(self, response):\n-        try:\n-            body = bz2.decompress(response.body)\n-        except IOError:\n-            return\n-\n-        respcls = responsetypes.from_args(body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def process_response(self, request, response, spider):\n-        if not response.body:\n-            return response\n-\n-        for fmt, func in six.iteritems(self._formats):\n-            new_response = func(response)\n-            if new_response:\n-                log.msg(format='Decompressed response with format: %(responsefmt)s',\n-                        level=log.DEBUG, spider=spider, responsefmt=fmt)\n-                return new_response\n-        return response\n+from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware\n\n@@ -1,42 +1,6 @@\n-from scrapy.http import Response\n-from scrapy.selector import Selector\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib_exp.iterators` is deprecated, use `scrapy.utils.iterators` instead\",\n+    ScrapyDeprecationWarning, stacklevel=2)\n \n-\n-def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n-    from lxml import etree\n-    reader = _StreamReader(obj)\n-    tag = '{%s}%s' % (namespace, nodename) if namespace else nodename\n-    iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n-    selxpath = '//' + ('%s:%s' % (prefix, nodename) if namespace else nodename)\n-    for _, node in iterable:\n-        nodetext = etree.tostring(node)\n-        node.clear()\n-        xs = Selector(text=nodetext, type='xml')\n-        if namespace:\n-            xs.register_namespace(prefix, namespace)\n-        yield xs.xpath(selxpath)[0]\n-\n-\n-class _StreamReader(object):\n-\n-    def __init__(self, obj):\n-        self._ptr = 0\n-        if isinstance(obj, Response):\n-            self._text, self.encoding = obj.body, obj.encoding\n-        else:\n-            self._text, self.encoding = obj, 'utf-8'\n-        self._is_unicode = isinstance(self._text, unicode)\n-\n-    def read(self, n=65535):\n-        self.read = self._read_unicode if self._is_unicode else self._read_string\n-        return self.read(n).lstrip()\n-\n-    def _read_string(self, n=65535):\n-        s, e = self._ptr, self._ptr + n\n-        self._ptr = e\n-        return self._text[s:e]\n-\n-    def _read_unicode(self, n=65535):\n-        s, e = self._ptr, self._ptr + n\n-        self._ptr = e\n-        return self._text[s:e].encode('utf-8')\n+from scrapy.utils.iterators import xmliter_lxml\n\n@@ -35,6 +35,46 @@ def xmliter(obj, nodename):\n         yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]\n \n \n+def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n+    from lxml import etree\n+    reader = _StreamReader(obj)\n+    tag = '{%s}%s' % (namespace, nodename) if namespace else nodename\n+    iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n+    selxpath = '//' + ('%s:%s' % (prefix, nodename) if namespace else nodename)\n+    for _, node in iterable:\n+        nodetext = etree.tostring(node)\n+        node.clear()\n+        xs = Selector(text=nodetext, type='xml')\n+        if namespace:\n+            xs.register_namespace(prefix, namespace)\n+        yield xs.xpath(selxpath)[0]\n+\n+\n+class _StreamReader(object):\n+\n+    def __init__(self, obj):\n+        self._ptr = 0\n+        if isinstance(obj, Response):\n+            self._text, self.encoding = obj.body, obj.encoding\n+        else:\n+            self._text, self.encoding = obj, 'utf-8'\n+        self._is_unicode = isinstance(self._text, unicode)\n+\n+    def read(self, n=65535):\n+        self.read = self._read_unicode if self._is_unicode else self._read_string\n+        return self.read(n).lstrip()\n+\n+    def _read_string(self, n=65535):\n+        s, e = self._ptr, self._ptr + n\n+        self._ptr = e\n+        return self._text[s:e]\n+\n+    def _read_unicode(self, n=65535):\n+        s, e = self._ptr, self._ptr + n\n+        self._ptr = e\n+        return self._text[s:e].encode('utf-8')\n+\n+\n def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n     \"\"\" Returns an iterator of dictionaries from the given csv object\n \n\n@@ -1,6 +1,6 @@\n from unittest import TestCase, main\n from scrapy.http import Response, XmlResponse\n-from scrapy.contrib_exp.downloadermiddleware.decompression import DecompressionMiddleware\n+from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware\n from scrapy.spider import Spider\n from tests import get_testdata\n from scrapy.utils.test import assert_samelines\n\n@@ -1,8 +1,7 @@\n import os\n from twisted.trial import unittest\n \n-from scrapy.utils.iterators import csviter, xmliter, _body_or_str\n-from scrapy.contrib_exp.iterators import xmliter_lxml\n+from scrapy.utils.iterators import csviter, xmliter, _body_or_str, xmliter_lxml\n from scrapy.http import XmlResponse, TextResponse, Response\n from tests import get_testdata\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
