{"custom_id": "keras#a095c1b16f8368a996ec345c4f3fd0a77cdf54f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 8219 | Contributors (this commit): 47 | Commits (past 90d): 21 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -1098,7 +1098,7 @@ class Layer(object):\n         if hasattr(self, '_losses'):\n             self._losses += losses\n         # Update self._per_input_updates\n-        if isinstance(input, list) and inputs == []:\n+        if isinstance(inputs, list) and inputs == []:\n             inputs = None\n         if inputs is not None:\n             inputs_hash = _object_list_uid(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#630a398ea8665f7062d40e3ba3c9b11205d902db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 45 | Churn Cumulative: 136 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,8 @@ from numpy.testing import assert_allclose\n from keras import backend as K\n from keras import activations\n \n+from keras.layers.core import Dense\n+\n \n def get_standard_values():\n     \"\"\"A set of floats used for testing the activations.\n@@ -25,7 +27,31 @@ def test_serialization():\n         assert fn == ref_fn\n \n \n-def test_softmax():\n+def test_get_fn():\n+    \"\"\"Activations has a convenience \"get\" function. All paths of this\n+    function are tested here, although the behaviour in some instances\n+    seems potentially surprising (e.g. situation 3)\n+    \"\"\"\n+\n+    # 1. Default returns linear\n+    a = activations.get(None)\n+    assert a == activations.linear\n+\n+    # 2. Passing in a layer raises a warning\n+    layer = Dense(32)\n+    with pytest.warns(UserWarning):\n+        a = activations.get(layer)\n+\n+    # 3. Callables return themselves for some reason\n+    a = activations.get(lambda x: 5)\n+    assert a(None) == 5\n+\n+    # 4. Anything else is not a valid argument\n+    with pytest.raises(ValueError):\n+        a = activations.get(6)\n+\n+\n+def test_softmax_valid():\n     \"\"\"Test using a reference implementation of softmax.\n     \"\"\"\n     def softmax(values):\n@@ -42,6 +68,17 @@ def test_softmax():\n     assert_allclose(result, expected, rtol=1e-05)\n \n \n+def test_softmax_invalid():\n+    \"\"\"Test for the expected exception behaviour on invalid input\n+    \"\"\"\n+\n+    x = K.placeholder(ndim=1)\n+\n+    # One dimensional arrays are supposed to raise a value error\n+    with pytest.raises(ValueError):\n+        f = K.function([x], [activations.softmax(x)])\n+\n+\n def test_time_distributed_softmax():\n     x = K.placeholder(shape=(1, 1, 5))\n     f = K.function([x], [activations.softmax(x)])\n@@ -136,7 +173,8 @@ def test_elu():\n     assert_allclose(result, test_values, rtol=1e-05)\n \n     negative_values = np.array([[-1, -2]], dtype=K.floatx())\n-    # cntk can't rebind the input shape, so create the model again to test different batch size\n+    # cntk can't rebind the input shape, so create the model again to\n+    # test different batch size\n     if (K.backend() == 'cntk'):\n         x2 = K.placeholder(ndim=2)\n         f = K.function([x2], [activations.elu(x2, 0.5)])\n@@ -158,7 +196,8 @@ def test_selu():\n \n     negative_values = np.array([[-1, -2]], dtype=K.floatx())\n \n-    # cntk can't rebind the input shape, so create the model again to test different batch size\n+    # cntk can't rebind the input shape, so create the model again to\n+    # test different batch size\n     if (K.backend() == 'cntk'):\n         x2 = K.placeholder(ndim=2)\n         f = K.function([x2], [activations.selu(x2)])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bdff7ac9b6e230dce2a0bcfa788c63bb948e8192", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 38 | Churn Cumulative: 120 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,6 +42,29 @@ def test_sparse_metrics():\n         assert K.eval(metric(y_a, y_b)).shape == (6,)\n \n \n+def test_serialize():\n+    '''This is a mock 'round trip' of serialize and deserialize.\n+    '''\n+\n+    class MockMetric:\n+        def __init__(self):\n+            self.__name__ = \"mock_metric\"\n+\n+    mock = MockMetric()\n+    found = metrics.serialize(mock)\n+    assert found == \"mock_metric\"\n+\n+    found = metrics.deserialize('mock_metric',\n+                                custom_objects={'mock_metric': True})\n+    assert found is True\n+\n+\n+def test_invalid_get():\n+\n+    with pytest.raises(ValueError):\n+        metrics.get(5)\n+\n+\n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason=\"keras cntk backend does not support top_k yet\")\n def test_top_k_categorical_accuracy():\n@@ -63,14 +86,17 @@ def test_top_k_categorical_accuracy():\n def test_sparse_top_k_categorical_accuracy():\n     y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n     y_true = K.variable(np.array([[1], [0]]))\n-    success_result = K.eval(metrics.sparse_top_k_categorical_accuracy(y_true, y_pred,\n-                                                                      k=3))\n+    success_result = K.eval(\n+        metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=3))\n+\n     assert success_result == 1\n-    partial_result = K.eval(metrics.sparse_top_k_categorical_accuracy(y_true, y_pred,\n-                                                                      k=2))\n+    partial_result = K.eval(\n+        metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=2))\n+\n     assert partial_result == 0.5\n-    failure_result = K.eval(metrics.sparse_top_k_categorical_accuracy(y_true, y_pred,\n-                                                                      k=1))\n+    failure_result = K.eval(\n+        metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=1))\n+\n     assert failure_result == 0\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1918e6a799a8a4721dfdc917d5dbc7e45491ef89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 2762 | Contributors (this commit): 57 | Commits (past 90d): 11 | Contributors (cumulative): 57 | DMM Complexity: None\n\nDIFF:\n@@ -462,7 +462,7 @@ class EarlyStopping(Callback):\n \n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '\n-                          'fallback to auto mode.' % (self.mode),\n+                          'fallback to auto mode.' % mode,\n                           RuntimeWarning)\n             mode = 'auto'\n \n@@ -471,7 +471,7 @@ class EarlyStopping(Callback):\n         elif mode == 'max':\n             self.monitor_op = np.greater\n         else:\n-            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n+            if 'acc' in self.monitor:\n                 self.monitor_op = np.greater\n             else:\n                 self.monitor_op = np.less\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aed00c3c0258451294e4c6f26c59d066dc1cae53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2764 | Contributors (this commit): 57 | Commits (past 90d): 12 | Contributors (cumulative): 57 | DMM Complexity: 1.0\n\nDIFF:\n@@ -495,7 +495,7 @@ class EarlyStopping(Callback):\n                 'which is not available. Available metrics are: %s' %\n                 (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n             )\n-\n+            return\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5ec6285e906788bd5669ac418239d36d1881e8ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/2 | Churn Δ: 50 | Churn Cumulative: 22521 | Contributors (this commit): 150 | Commits (past 90d): 117 | Contributors (cumulative): 245 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -264,9 +264,22 @@ def placeholder(\n         name=name)\n     x._keras_shape = shape\n     x._uses_learning_phase = False\n+    x._cntk_placeholder = True\n     return x\n \n \n+def is_placeholder(x):\n+    \"\"\"Returns whether `x` is a placeholder.\n+\n+    # Arguments\n+        x: A candidate placeholder.\n+\n+    # Returns\n+        Boolean.\n+    \"\"\"\n+    return hasattr(x, '_cntk_placeholder') and x._cntk_placeholder\n+\n+\n def is_keras_tensor(x):\n     if not isinstance(x, (C.variables.Constant,\n                           C.variables.Variable,\n\n@@ -442,6 +442,21 @@ def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n     return x\n \n \n+def is_placeholder(x):\n+    \"\"\"Returns whether `x` is a placeholder.\n+\n+    # Arguments\n+        x: A candidate placeholder.\n+\n+    # Returns\n+        Boolean.\n+    \"\"\"\n+    try:\n+        return x.op.type == 'Placeholder'\n+    except AttributeError:\n+        return False\n+\n+\n def shape(x):\n     \"\"\"Returns the symbolic shape of a tensor or variable.\n \n\n@@ -234,9 +234,22 @@ def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n         x = T.TensorType(dtype, broadcast)(name)\n     x._keras_shape = shape\n     x._uses_learning_phase = False\n+    x._theano_placeholder = True\n     return x\n \n \n+def is_placeholder(x):\n+    \"\"\"Returns whether `x` is a placeholder.\n+\n+    # Arguments\n+        x: A candidate placeholder.\n+\n+    # Returns\n+        Boolean.\n+    \"\"\"\n+    return hasattr(x, '_theano_placeholder') and x._theano_placeholder\n+\n+\n def shape(x):\n     \"\"\"Returns the shape of a tensor.\n \n\n@@ -737,6 +737,7 @@ class Model(Container):\n                     target = target_tensors[i]\n                 else:\n                     target = None\n+                if target is None or K.is_placeholder(target):\n                     if target is None:\n                         target = K.placeholder(ndim=len(shape),\n                                                name=name + '_target',\n\n@@ -807,6 +807,14 @@ def test_target_tensors():\n     model.train_on_batch(input_val, None,\n                          sample_weight={'dense_a': np.random.random((10,))})\n \n+    if K.backend() == 'tensorflow':\n+        import tensorflow as tf\n+        # test with custom TF placeholder as target\n+        pl_target_a = tf.placeholder('float32', shape=(None, 4))\n+        model.compile(optimizer='rmsprop', loss='mse',\n+                      target_tensors=[pl_target_a, target_b])\n+        model.train_on_batch(input_val, target_val_a)\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9772b1179c39433cf6babf96b53b7987e5cb5027", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 271 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -72,7 +72,7 @@ class HDF5Matrix(object):\n                 idx = slice(start + self.start, stop + self.start)\n             else:\n                 raise IndexError\n-        elif isinstance(key, int):\n+        elif isinstance(key, (int, np.integer)):\n             if key + self.start < self.end:\n                 idx = key + self.start\n             else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7af02c324004e6a743120c244101f44ce661dcdf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 10009 | Contributors (this commit): 49 | Commits (past 90d): 6 | Contributors (cumulative): 49 | DMM Complexity: None\n\nDIFF:\n@@ -1903,14 +1903,14 @@ class Cropping3D(Layer):\n     \"\"\"Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n \n     # Arguments\n-        cropping: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n+        cropping: int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n             - If int: the same symmetric cropping\n-                is applied to width and height.\n-            - If tuple of 2 ints:\n+                is applied to depth, height, and width.\n+            - If tuple of 3 ints:\n                 interpreted as two different\n-                symmetric cropping values for height and width:\n+                symmetric cropping values for depth, height, and width:\n                 `(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop)`.\n-            - If tuple of 2 tuples of 2 ints:\n+            - If tuple of 3 tuples of 2 ints:\n                 interpreted as\n                 `((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop))`\n         data_format: A string,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9a24cc6bf97951b0cd2334a6897ee21e2308e1d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 45 | Churn Cumulative: 1059 | Contributors (this commit): 13 | Commits (past 90d): 15 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -816,5 +816,50 @@ def test_target_tensors():\n         model.train_on_batch(input_val, target_val_a)\n \n \n+@keras_test\n+def test_model_custom_target_tensors():\n+    a = Input(shape=(3,), name='input_a')\n+    b = Input(shape=(3,), name='input_b')\n+\n+    a_2 = Dense(4, name='dense_1')(a)\n+    dp = Dropout(0.5, name='dropout')\n+    b_2 = dp(b)\n+\n+    y = K.placeholder([10, 4], name='y')\n+    y1 = K.placeholder([10, 3], name='y1')\n+    y2 = K.placeholder([7, 5], name='y2')\n+    model = Model([a, b], [a_2, b_2])\n+\n+    optimizer = 'rmsprop'\n+    loss = 'mse'\n+    loss_weights = [1., 0.5]\n+\n+    # test list of target tensors\n+    with pytest.raises(ValueError):\n+        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n+                      sample_weight_mode=None, target_tensors=[y, y1, y2])\n+    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n+                  sample_weight_mode=None, target_tensors=[y, y1])\n+    input_a_np = np.random.random((10, 3))\n+    input_b_np = np.random.random((10, 3))\n+\n+    output_a_np = np.random.random((10, 4))\n+    output_b_np = np.random.random((10, 3))\n+\n+    out = model.train_on_batch([input_a_np, input_b_np],\n+                               [output_a_np, output_b_np], {y: np.random.random((10, 4)), y1: np.random.random((10, 3))})\n+\n+    # test dictionary of target_tensors\n+    with pytest.raises(ValueError):\n+        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n+                      sample_weight_mode=None, target_tensors={'does_not_exist': y2})\n+    # test dictionary of target_tensors\n+    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n+                  sample_weight_mode=None, target_tensors={'dense_1': y, 'dropout': y1})\n+\n+    out = model.train_on_batch([input_a_np, input_b_np],\n+                               [output_a_np, output_b_np], {y: np.random.random((10, 4)), y1: np.random.random((10, 3))})\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b889a4d88b8cd0189e1d77fb503f485b1e861b99", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 1100 | Contributors (this commit): 13 | Commits (past 90d): 16 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -807,14 +807,6 @@ def test_target_tensors():\n     model.train_on_batch(input_val, None,\n                          sample_weight={'dense_a': np.random.random((10,))})\n \n-    if K.backend() == 'tensorflow':\n-        import tensorflow as tf\n-        # test with custom TF placeholder as target\n-        pl_target_a = tf.placeholder('float32', shape=(None, 4))\n-        model.compile(optimizer='rmsprop', loss='mse',\n-                      target_tensors=[pl_target_a, target_b])\n-        model.train_on_batch(input_val, target_val_a)\n-\n \n @keras_test\n def test_model_custom_target_tensors():\n@@ -847,18 +839,35 @@ def test_model_custom_target_tensors():\n     output_b_np = np.random.random((10, 3))\n \n     out = model.train_on_batch([input_a_np, input_b_np],\n-                               [output_a_np, output_b_np], {y: np.random.random((10, 4)), y1: np.random.random((10, 3))})\n-\n+                               [output_a_np, output_b_np],\n+                               {y: np.random.random((10, 4)),\n+                                y1: np.random.random((10, 3))})\n     # test dictionary of target_tensors\n     with pytest.raises(ValueError):\n-        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n-                      sample_weight_mode=None, target_tensors={'does_not_exist': y2})\n+        model.compile(optimizer, loss,\n+                      metrics=[],\n+                      loss_weights=loss_weights,\n+                      sample_weight_mode=None,\n+                      target_tensors={'does_not_exist': y2})\n     # test dictionary of target_tensors\n-    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n-                  sample_weight_mode=None, target_tensors={'dense_1': y, 'dropout': y1})\n-\n+    model.compile(optimizer, loss,\n+                  metrics=[],\n+                  loss_weights=loss_weights,\n+                  sample_weight_mode=None,\n+                  target_tensors={'dense_1': y, 'dropout': y1})\n     out = model.train_on_batch([input_a_np, input_b_np],\n-                               [output_a_np, output_b_np], {y: np.random.random((10, 4)), y1: np.random.random((10, 3))})\n+                               [output_a_np, output_b_np],\n+                               {y: np.random.random((10, 4)),\n+                                y1: np.random.random((10, 3))})\n+\n+    if K.backend() == 'tensorflow':\n+        import tensorflow as tf\n+        # test with custom TF placeholder as target\n+        pl_target_a = tf.placeholder('float32', shape=(None, 4))\n+        model.compile(optimizer='rmsprop', loss='mse',\n+                      target_tensors={'dense_1': pl_target_a})\n+        model.train_on_batch([input_a_np, input_b_np],\n+                             [output_a_np, output_b_np])\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3add406d68baeba609ff3b287bc9079c5fd97a0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 31 | Churn Cumulative: 381 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,19 +23,25 @@ def preprocess_input(x, data_format=None):\n     assert data_format in {'channels_last', 'channels_first'}\n \n     if data_format == 'channels_first':\n+        if x.ndim == 3:\n             # 'RGB'->'BGR'\n-        x = x[:, ::-1, :, :]\n+            x = x[::-1, ...]\n             # Zero-center by mean pixel\n+            x[0, :, :] -= 103.939\n+            x[1, :, :] -= 116.779\n+            x[2, :, :] -= 123.68\n+        else:\n+            x = x[:, ::-1, ...]\n             x[:, 0, :, :] -= 103.939\n             x[:, 1, :, :] -= 116.779\n             x[:, 2, :, :] -= 123.68\n     else:\n         # 'RGB'->'BGR'\n-        x = x[:, :, :, ::-1]\n+        x = x[..., ::-1]\n         # Zero-center by mean pixel\n-        x[:, :, :, 0] -= 103.939\n-        x[:, :, :, 1] -= 116.779\n-        x[:, :, :, 2] -= 123.68\n+        x[..., 0] -= 103.939\n+        x[..., 1] -= 116.779\n+        x[..., 2] -= 123.68\n     return x\n \n \n\n@@ -6,13 +6,24 @@ from keras.applications import imagenet_utils as utils\n \n \n def test_preprocess_input():\n-    x = np.random.uniform(0, 255, (2, 3, 2, 3))\n+    # Test image batch\n+    x = np.random.uniform(0, 255, (2, 10, 10, 3))\n     assert utils.preprocess_input(x).shape == x.shape\n \n     out1 = utils.preprocess_input(x, 'channels_last')\n-    out2 = utils.preprocess_input(np.transpose(x, (0, 3, 1, 2)), 'channels_first')\n+    out2 = utils.preprocess_input(np.transpose(x, (0, 3, 1, 2)),\n+                                  'channels_first')\n     assert_allclose(out1, out2.transpose(0, 2, 3, 1))\n \n+    # Test single image\n+    x = np.random.uniform(0, 255, (10, 10, 3))\n+    assert utils.preprocess_input(x).shape == x.shape\n+\n+    out1 = utils.preprocess_input(x, 'channels_last')\n+    out2 = utils.preprocess_input(np.transpose(x, (2, 0, 1)),\n+                                  'channels_first')\n+    assert_allclose(out1, out2.transpose(1, 2, 0))\n+\n \n def test_decode_predictions():\n     x = np.zeros((2, 1000))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e61e79ec59a098636b4fe87b1e67790b5c0b683", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 32 | Churn Cumulative: 9802 | Contributors (this commit): 49 | Commits (past 90d): 31 | Contributors (cumulative): 58 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2440,11 +2440,11 @@ class Container(Layer):\n                     raise ValueError('Improperly formatted model config.')\n                 if inbound_layer_name not in created_layers:\n                     add_unprocessed_node(layer, node_data)\n-                    continue\n+                    return\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer.inbound_nodes) <= inbound_node_index:\n                     add_unprocessed_node(layer, node_data)\n-                    continue\n+                    return\n                 inbound_node = inbound_layer.inbound_nodes[inbound_node_index]\n                 input_tensors.append(inbound_node.output_tensors[inbound_tensor_index])\n             # Call layer on its inputs, thus creating the node\n\n@@ -710,6 +710,34 @@ def test_layer_sharing_at_heterogenous_depth():\n     np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)\n \n \n+@keras_test\n+def test_layer_sharing_at_heterogenous_depth_with_concat():\n+    input_shape = (16, 9, 3)\n+    input_layer = Input(shape=input_shape)\n+\n+    A = Dense(3, name='dense_A')\n+    B = Dense(3, name='dense_B')\n+    C = Dense(3, name='dense_C')\n+\n+    x1 = B(A(input_layer))\n+    x2 = A(C(input_layer))\n+    output = layers.concatenate([x1, x2])\n+\n+    M = Model(inputs=input_layer, outputs=output)\n+\n+    x_val = np.random.random((10, 16, 9, 3))\n+    output_val = M.predict(x_val)\n+\n+    config = M.get_config()\n+    weights = M.get_weights()\n+\n+    M2 = Model.from_config(config)\n+    M2.set_weights(weights)\n+\n+    output_val_2 = M2.predict(x_val)\n+    np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)\n+\n+\n @keras_test\n def test_multi_output_mask():\n     \"\"\"Fixes #7589\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#abd23fa4cf50f02784f9b7336f7710c696a5f286", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 172 | Lines Deleted: 109 | Files Changed: 2 | Hunks: 69 | Methods Changed: 16 | Complexity Δ (Sum/Max): 23/23 | Churn Δ: 281 | Churn Cumulative: 6461 | Contributors (this commit): 62 | Commits (past 90d): 43 | Contributors (cumulative): 73 | DMM Complexity: 0.0\n\nDIFF:\n@@ -573,15 +573,15 @@ class Model(Container):\n         \"\"\"Configures the model for training.\n \n         # Arguments\n-            optimizer: str (name of optimizer) or optimizer object.\n+            optimizer: String (name of optimizer) or optimizer object.\n                 See [optimizers](/optimizers).\n-            loss: str (name of objective function) or objective function.\n+            loss: String (name of objective function) or objective function.\n                 See [losses](/losses).\n                 If the model has multiple outputs, you can use a different loss\n                 on each output by passing a dictionary or a list of losses.\n                 The loss value that will be minimized by the model\n                 will then be the sum of all individual losses.\n-            metrics: list of metrics to be evaluated by the model\n+            metrics: List of metrics to be evaluated by the model\n                 during training and testing.\n                 Typically you will use `metrics=['accuracy']`.\n                 To specify different metrics for different outputs of a\n@@ -596,13 +596,13 @@ class Model(Container):\n                 If a list, it is expected to have a 1:1 mapping\n                 to the model's outputs. If a tensor, it is expected to map\n                 output names (strings) to scalar coefficients.\n-            sample_weight_mode: if you need to do timestep-wise\n+            sample_weight_mode: If you need to do timestep-wise\n                 sample weighting (2D weights), set this to `\"temporal\"`.\n                 `None` defaults to sample-wise weights (1D).\n                 If the model has multiple outputs, you can use a different\n                 `sample_weight_mode` on each output by passing a\n                 dictionary or a list of modes.\n-            target_tensors: by default, Keras will create placeholders for the\n+            target_tensors: By default, Keras will create placeholders for the\n                 model's target, which will be fed with the target data during\n                 training. If instead you would like to use your own\n                 target tensors (in turn, Keras will not expect external\n@@ -610,9 +610,9 @@ class Model(Container):\n                 can specify them via the `target_tensors` argument. It can be\n                 a single tensor (for a single-output model), a list of tensors,\n                 or a dict mapping output names to target tensors.\n-            weighted_metrics: list of metrics to be evaluated and weighted\n+            weighted_metrics: List of metrics to be evaluated and weighted\n                 by sample_weight or class_weight during training and testing\n-            **kwargs: when using the Theano/CNTK backends, these arguments\n+            **kwargs: When using the Theano/CNTK backends, these arguments\n                 are passed into K.function. When using the TensorFlow backend,\n                 these arguments are passed into `tf.Session.run`.\n \n@@ -1005,8 +1005,8 @@ class Model(Container):\n         in which case the number of samples is set to `None`.\n \n         # Arguments\n-            ins: list of tensors to be fed to the Keras function.\n-            batch_size: integer batch size or `None` if not defined.\n+            ins: List of tensors to be fed to the Keras function.\n+            batch_size: Integer batch size or `None` if not defined.\n             steps: Total number of steps (batches of samples)\n                 before declaring `_predict_loop` finished.\n                 Ignored with the default value of `None`.\n@@ -1028,12 +1028,13 @@ class Model(Container):\n             num_samples = None\n             if batch_size is not None:\n                 raise ValueError('If ' + steps_name +\n-                                 ' is set the batch_size must be None.')\n+                                 ' is set, the `batch_size` must be None.')\n         elif ins and hasattr(ins[0], 'shape'):\n             num_samples = ins[0].shape[0]\n         else:\n-            raise ValueError('The input data should have shape, or '\n-                             'please specify ' + steps_name + '.')\n+            raise ValueError('Either the input data should have '\n+                             'a defined shape, or ' + steps_name +\n+                             ' should be specified.')\n         return num_samples\n \n     def _fit_loop(self, f, ins, out_labels=None, batch_size=None,\n@@ -1047,21 +1048,21 @@ class Model(Container):\n \n         # Arguments\n             f: Keras function returning a list of tensors\n-            ins: list of tensors to be fed to `f`\n-            out_labels: list of strings, display names of\n+            ins: List of tensors to be fed to `f`\n+            out_labels: List of strings, display names of\n                 the outputs of `f`\n-            batch_size: integer batch size or None if unknown.\n-            epochs: number of times to iterate over the data\n-            verbose: verbosity mode, 0, 1 or 2\n-            callbacks: list of callbacks to be called during training\n+            batch_size: Integer batch size or None if unknown.\n+            epochs: Number of times to iterate over the data\n+            verbose: Verbosity mode, 0, 1 or 2\n+            callbacks: List of callbacks to be called during training\n             val_f: Keras function to call for validation\n-            val_ins: list of tensors to be fed to `val_f`\n-            shuffle: whether to shuffle the data at the beginning of each epoch\n-            callback_metrics: list of strings, the display names of the metrics\n+            val_ins: List of tensors to be fed to `val_f`\n+            shuffle: Whether to shuffle the data at the beginning of each epoch\n+            callback_metrics: List of strings, the display names of the metrics\n                 passed to the callbacks. They should be the\n                 concatenation of list the display names of the outputs of\n                  `f` and the list of display names of the outputs of `f_val`.\n-            initial_epoch: epoch at which to start training\n+            initial_epoch: Epoch at which to start training\n                 (useful for resuming a previous training run)\n             steps_per_epoch: Total number of steps (batches of samples)\n                 before declaring one epoch finished and starting the\n@@ -1129,11 +1130,11 @@ class Model(Container):\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:\n-                for step_num in range(steps_per_epoch):\n+                for step_index in range(steps_per_epoch):\n                     batch_logs = {}\n-                    batch_logs['batch'] = step_num\n+                    batch_logs['batch'] = step_index\n                     batch_logs['size'] = 1\n-                    callbacks.on_batch_begin(step_num, batch_logs)\n+                    callbacks.on_batch_begin(step_index, batch_logs)\n                     outs = f(ins)\n \n                     if not isinstance(outs, list):\n@@ -1141,7 +1142,7 @@ class Model(Container):\n                     for l, o in zip(out_labels, outs):\n                         batch_logs[l] = o\n \n-                    callbacks.on_batch_end(step_num, batch_logs)\n+                    callbacks.on_batch_end(step_index, batch_logs)\n                     if callback_model.stop_training:\n                         break\n \n@@ -1204,7 +1205,7 @@ class Model(Container):\n         callbacks.on_train_end()\n         return self.history\n \n-    def _predict_loop(self, f, ins, batch_size=32, verbose=0):\n+    def _predict_loop(self, f, ins, batch_size=32, verbose=0, steps=None):\n         \"\"\"Abstract method to loop over some data in batches.\n \n         # Arguments\n@@ -1221,21 +1222,42 @@ class Model(Container):\n             or list of arrays of predictions\n             (if the model has multiple outputs).\n         \"\"\"\n-        if ins and hasattr(ins[0], 'shape'):\n-            samples = ins[0].shape[0]\n-        else:\n-            # May happen if we are running `predict` without Numpy input data,\n-            # i.e. if all inputs to the models are data tensors\n-            # instead of placeholders.\n-            # In that case we will run `predict` over a single batch.\n-            samples = batch_size\n-            verbose = 2\n-\n-        outs = []\n+        num_samples = self._check_num_samples(ins, batch_size,\n+                                              steps,\n+                                              'steps')\n         if verbose == 1:\n-            progbar = Progbar(target=samples)\n-        batches = _make_batches(samples, batch_size)\n-        index_array = np.arange(samples)\n+            if steps is not None:\n+                progbar = Progbar(target=steps)\n+            else:\n+                progbar = Progbar(target=num_samples)\n+        if steps is not None:\n+            # Step-based predictions.\n+            # Since we do not know how many samples\n+            # we will see, we cannot pre-allocate\n+            # the returned Numpy arrays.\n+            # Instead, we store one array per batch seen\n+            # and concatenate them upon returning.\n+            unconcatenated_outs = []\n+            for step in range(steps):\n+                batch_outs = f(ins)\n+                if not isinstance(batch_outs, list):\n+                    batch_outs = [batch_outs]\n+                if step == 0:\n+                    for batch_out in batch_outs:\n+                        unconcatenated_outs.append([])\n+                for i, batch_out in enumerate(batch_outs):\n+                    unconcatenated_outs[i].append(batch_out)\n+                if verbose == 1:\n+                    progbar.update(step)\n+            if len(unconcatenated_outs) == 1:\n+                return np.concatenate(unconcatenated_outs[0], axis=0)\n+            return [np.concatenate(unconcatenated_outs[i], axis=0)\n+                    for i in range(len(unconcatenated_outs))]\n+        else:\n+            # Sample-based predictions.\n+            outs = []\n+            batches = _make_batches(num_samples, batch_size)\n+            index_array = np.arange(num_samples)\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 batch_ids = index_array[batch_start:batch_end]\n                 if ins and isinstance(ins[-1], float):\n@@ -1243,15 +1265,14 @@ class Model(Container):\n                     ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n                 else:\n                     ins_batch = _slice_arrays(ins, batch_ids)\n-\n                 batch_outs = f(ins_batch)\n                 if not isinstance(batch_outs, list):\n                     batch_outs = [batch_outs]\n                 if batch_index == 0:\n+                    # Pre-allocate the results arrays.\n                     for batch_out in batch_outs:\n-                    shape = (samples,) + batch_out.shape[1:]\n+                        shape = (num_samples,) + batch_out.shape[1:]\n                         outs.append(np.zeros(shape, dtype=batch_out.dtype))\n-\n                 for i, batch_out in enumerate(batch_outs):\n                     outs[i][batch_start:batch_end] = batch_out\n                 if verbose == 1:\n@@ -1278,30 +1299,34 @@ class Model(Container):\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n         \"\"\"\n+        num_samples = self._check_num_samples(ins, batch_size,\n+                                              steps,\n+                                              'steps')\n         outs = []\n-        if batch_size is None and steps is not None:\n+        if steps is not None:\n             if verbose == 1:\n                 progbar = Progbar(target=steps)\n-            for step_num in range(steps):\n-                f(ins)\n-                if verbose == 1:\n-                    progbar.update(step_num)\n-\n+            for step in range(steps):\n+                batch_outs = f(ins)\n+                if isinstance(batch_outs, list):\n+                    if step == 0:\n+                        for _ in enumerate(batch_outs):\n+                            outs.append(0.)\n+                    for i, batch_out in enumerate(batch_outs):\n+                        outs[i] += batch_out\n                 else:\n-            if ins and hasattr(ins[0], 'shape'):\n-                samples = ins[0].shape[0]\n-            else:\n-                # May happen if we are running `predict` without Numpy input data,\n-                # i.e. if all inputs to the models are data tensors\n-                # instead of placeholders.\n-                # In that case we will run `predict` over a single batch.\n-                samples = batch_size\n-                verbose = 2\n-\n+                    if step == 0:\n+                        outs.append(0.)\n+                    outs[0] += batch_outs\n                 if verbose == 1:\n-                progbar = Progbar(target=samples)\n-            batches = _make_batches(samples, batch_size)\n-            index_array = np.arange(samples)\n+                    progbar.update(step)\n+            for i in range(len(outs)):\n+                outs[i] /= steps\n+        else:\n+            if verbose == 1:\n+                progbar = Progbar(target=num_samples)\n+            batches = _make_batches(num_samples, batch_size)\n+            index_array = np.arange(num_samples)\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 batch_ids = index_array[batch_start:batch_end]\n                 if isinstance(ins[-1], float):\n@@ -1325,7 +1350,7 @@ class Model(Container):\n                 if verbose == 1:\n                     progbar.update(batch_end)\n             for i in range(len(outs)):\n-            outs[i] /= samples\n+                outs[i] /= num_samples\n         if len(outs) == 1:\n             return outs[0]\n         return outs\n@@ -1416,44 +1441,44 @@ class Model(Container):\n                 If all outputs in the model are named,\n                 you can also pass a dictionary\n                 mapping output names to Numpy arrays.\n-            batch_size: integer or `None`. Number of samples per gradient update.\n-                Defaults to 32 if training numpy arrays and no batch\n-                size is specified. Defaults to `None` when `steps_per_epoch` is set.\n-            epochs: integer, the number of times to iterate\n+            batch_size: Integer or `None`.\n+                Number of samples per gradient update.\n+                If unspecified, it will default to 32.\n+            epochs: Integer, the number of times to iterate\n                 over the training data arrays.\n             verbose: 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = verbose, 2 = one log line per epoch.\n-            callbacks: list of callbacks to be called during training.\n+            callbacks: List of callbacks to be called during training.\n                 See [callbacks](/callbacks).\n-            validation_split: float between 0 and 1:\n+            validation_split: Float between 0 and 1:\n                 fraction of the training data to be used as validation data.\n                 The model will set apart this fraction of the training data,\n                 will not train on it, and will evaluate\n                 the loss and any model metrics\n                 on this data at the end of each epoch.\n-            validation_data: data on which to evaluate\n+            validation_data: Data on which to evaluate\n                 the loss and any model metrics\n                 at the end of each epoch. The model will not\n                 be trained on this data.\n                 This could be a tuple (x_val, y_val)\n                 or a tuple (x_val, y_val, val_sample_weights).\n-            shuffle: boolean, whether to shuffle the training data\n+            shuffle: Boolean, whether to shuffle the training data\n                 before each epoch. Has no effect when `steps_per_epoch`\n                 is not `None`.\n-            class_weight: optional dictionary mapping\n+            class_weight: Optional dictionary mapping\n                 class indices (integers) to\n                 a weight (float) to apply to the model's loss for the samples\n                 from this class during training.\n                 This can be useful to tell the model to \"pay more attention\" to\n                 samples from an under-represented class.\n-            sample_weight: optional array of the same length as x, containing\n+            sample_weight: Optional array of the same length as x, containing\n                 weights to apply to the model's loss for each sample.\n                 In the case of temporal data, you can pass a 2D array\n                 with shape (samples, sequence_length),\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify\n                 sample_weight_mode=\"temporal\" in compile().\n-            initial_epoch: epoch at which to start training\n+            initial_epoch: Epoch at which to start training\n                 (useful for resuming a previous training run)\n             steps_per_epoch: Total number of steps (batches of samples)\n                 before declaring one epoch finished and starting the\n@@ -1473,7 +1498,7 @@ class Model(Container):\n             ValueError: In case of mismatch between the provided input data\n                 and what the model expects.\n         \"\"\"\n-        # backwards compatibility\n+        # Backwards compatibility\n         if batch_size is None and steps_per_epoch is None:\n             batch_size = 32\n         # Legacy support\n@@ -1483,7 +1508,10 @@ class Model(Container):\n             epochs = kwargs.pop('nb_epoch')\n         if kwargs:\n             raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n-\n+        if x is None and y is None and steps_per_epoch is None:\n+            raise ValueError('If fitting from data tensors, '\n+                             'you should specify the `steps_per_epoch` '\n+                             'argument.')\n         # Validate user data.\n         x, y, sample_weights = self._standardize_user_data(\n             x, y,\n@@ -1566,7 +1594,11 @@ class Model(Container):\n                               steps_per_epoch=steps_per_epoch,\n                               validation_steps=validation_steps)\n \n-    def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\n+    def evaluate(self, x, y,\n+                 batch_size=None,\n+                 verbose=1,\n+                 sample_weight=None,\n+                 steps=None):\n         \"\"\"Returns the loss value & metrics values for the model in test mode.\n \n         Computation is done in batches.\n@@ -1582,10 +1614,13 @@ class Model(Container):\n                 If all outputs in the model are named,\n                 you can also pass a dictionary\n                 mapping output names to Numpy arrays.\n-            batch_size: integer. Number of samples per gradient update.\n-            verbose: verbosity mode, 0 or 1.\n+            batch_size: Integer. If unspecified, it will default to 32.\n+            verbose: Verbosity mode, 0 or 1.\n             sample_weight: Array of weights to weight the contribution\n                 of different samples to the loss and metrics.\n+            steps: Total number of steps (batches of samples)\n+                before declaring the evaluation round finished.\n+                Ignored with the default value of `None`.\n \n         # Returns\n             Scalar test loss (if the model has a single output and no metrics)\n@@ -1593,6 +1628,13 @@ class Model(Container):\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n         \"\"\"\n+        # Backwards compatibility.\n+        if batch_size is None and steps is None:\n+            batch_size = 32\n+        if x is None and y is None and steps is None:\n+            raise ValueError('If evaluating from data tensors, '\n+                             'you should specify the `steps` '\n+                             'argument.')\n         # Validate user data.\n         x, y, sample_weights = self._standardize_user_data(\n             x, y,\n@@ -1608,18 +1650,25 @@ class Model(Container):\n         f = self.test_function\n         return self._test_loop(f, ins,\n                                batch_size=batch_size,\n-                               verbose=verbose)\n+                               verbose=verbose,\n+                               steps=steps)\n \n-    def predict(self, x, batch_size=32, verbose=0):\n+    def predict(self, x,\n+                batch_size=None,\n+                verbose=0,\n+                steps=None):\n         \"\"\"Generates output predictions for the input samples.\n \n         Computation is done in batches.\n \n         # Arguments\n-            x: the input data, as a Numpy array\n+            x: The input data, as a Numpy array\n                 (or list of Numpy arrays if the model has multiple outputs).\n-            batch_size: integer.\n-            verbose: verbosity mode, 0 or 1.\n+            batch_size: Integer. If unspecified, it will default to 32.\n+            verbose: Verbosity mode, 0 or 1.\n+            steps: Total number of steps (batches of samples)\n+                before declaring the prediction round finished.\n+                Ignored with the default value of `None`.\n \n         # Returns\n             Numpy array(s) of predictions.\n@@ -1630,6 +1679,13 @@ class Model(Container):\n                 or in case a stateful model receives a number of samples\n                 that is not a multiple of the batch size.\n         \"\"\"\n+        # Backwards compatibility.\n+        if batch_size is None and steps is None:\n+            batch_size = 32\n+        if x is None and steps is None:\n+            raise ValueError('If predicting from data tensors, '\n+                             'you should specify the `steps` '\n+                             'argument.')\n         # Validate user data.\n         x = _standardize_input_data(x, self._feed_input_names,\n                                     self._feed_input_shapes,\n@@ -1651,10 +1707,11 @@ class Model(Container):\n         self._make_predict_function()\n         f = self.predict_function\n         return self._predict_loop(f, ins, batch_size=batch_size,\n-                                  verbose=verbose)\n+                                  verbose=verbose, steps=steps)\n \n     def train_on_batch(self, x, y,\n-                       sample_weight=None, class_weight=None):\n+                       sample_weight=None,\n+                       class_weight=None):\n         \"\"\"Runs a single gradient update on a single batch of data.\n \n         # Arguments\n@@ -1668,14 +1725,14 @@ class Model(Container):\n                 If all outputs in the model are named,\n                 you can also pass a dictionary\n                 mapping output names to Numpy arrays.\n-            sample_weight: optional array of the same length as x, containing\n+            sample_weight: Optional array of the same length as x, containing\n                 weights to apply to the model's loss for each sample.\n                 In the case of temporal data, you can pass a 2D array\n                 with shape (samples, sequence_length),\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify\n                 sample_weight_mode=\"temporal\" in compile().\n-            class_weight: optional dictionary mapping\n+            class_weight: Optional dictionary mapping\n                 class indices (integers) to\n                 a weight (float) to apply to the model's loss for the samples\n                 from this class during training.\n@@ -1718,7 +1775,7 @@ class Model(Container):\n                 If all outputs in the model are named,\n                 you can also pass a dictionary\n                 mapping output names to Numpy arrays.\n-            sample_weight: optional array of the same length as x, containing\n+            sample_weight: Optional array of the same length as x, containing\n                 weights to apply to the model's loss for each sample.\n                 In the case of temporal data, you can pass a 2D array\n                 with shape (samples, sequence_length),\n@@ -1792,7 +1849,7 @@ class Model(Container):\n         using `use_multiprocessing=True`.\n \n         # Arguments\n-            generator: a generator or an instance of Sequence (keras.utils.Sequence)\n+            generator: A generator or an instance of Sequence (keras.utils.Sequence)\n                     object in order to avoid duplicate data\n                     when using multiprocessing.\n                 The output of the generator must be either\n@@ -1807,32 +1864,32 @@ class Model(Container):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of unique samples if your dataset\n                 divided by the batch size.\n-            epochs: integer, total number of iterations on the data.\n-            verbose: verbosity mode, 0, 1, or 2.\n-            callbacks: list of callbacks to be called during training.\n-            validation_data: this can be either\n+            epochs: Integer, total number of iterations on the data.\n+            verbose: Verbosity mode, 0, 1, or 2.\n+            callbacks: List of callbacks to be called during training.\n+            validation_data: This can be either\n                 - a generator for the validation data\n                 - a tuple (inputs, targets)\n                 - a tuple (inputs, targets, sample_weights).\n             validation_steps: Only relevant if `validation_data`\n                 is a generator. Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n-            class_weight: dictionary mapping class indices to a weight\n+            class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n-            max_queue_size: maximum size for the generator queue\n-            workers: maximum number of processes to spin up\n+            max_queue_size: Maximum size for the generator queue\n+            workers: Maximum number of processes to spin up\n                 when using process based threading\n-            use_multiprocessing: if True, use process based threading.\n+            use_multiprocessing: If True, use process based threading.\n                 Note that because\n                 this implementation relies on multiprocessing,\n                 you should not pass\n                 non picklable arguments to the generator\n                 as they can't be passed\n                 easily to children processes.\n-            shuffle: whether to shuffle the data at the beginning of each\n+            shuffle: Whether to shuffle the data at the beginning of each\n                 epoch. Only used with instances of `Sequence` (\n                 keras.utils.Sequence).\n-            initial_epoch: epoch at which to start training\n+            initial_epoch: Epoch at which to start training\n                 (useful for resuming a previous training run)\n \n         # Returns\n\n@@ -583,9 +583,9 @@ def test_model_with_input_feed_tensor():\n                          output_a_np, batch_size=10)\n \n     # test predict\n-    out = model.predict(None, batch_size=10)\n-    out = model.predict(None, batch_size=10)\n-    assert out.shape == (10, 4)\n+    out = model.predict(None, steps=3)\n+    out = model.predict(None, steps=3)\n+    assert out.shape == (10 * 3, 4)\n \n     # Same, without learning phase\n     # i.e. we don't pass any data to fit the model.\n@@ -624,9 +624,9 @@ def test_model_with_input_feed_tensor():\n                          output_a_np, batch_size=10)\n \n     # test predict\n-    out = model.predict(None, batch_size=10)\n-    out = model.predict(None, batch_size=10)\n-    assert out.shape == (10, 4)\n+    out = model.predict(None, steps=3)\n+    out = model.predict(None, steps=3)\n+    assert out.shape == (10 * 3, 4)\n \n \n @keras_test\n@@ -739,14 +739,20 @@ def test_model_with_external_loss():\n         out = model.predict_on_batch(None)\n \n         # test fit\n-        out = model.fit(None, None, epochs=1, batch_size=None, steps_per_epoch=1)\n+        with pytest.raises(ValueError):\n+            out = model.fit(None, None, epochs=1, batch_size=10)\n+        out = model.fit(None, None, epochs=1, steps_per_epoch=1)\n \n         # test evaluate\n+        with pytest.raises(ValueError):\n             out = model.evaluate(None, None, batch_size=10)\n+        out = model.evaluate(None, None, steps=3)\n \n         # test predict\n+        with pytest.raises(ValueError):\n             out = model.predict(None, batch_size=10)\n-        assert out.shape == (10, 4)\n+        out = model.predict(None, steps=3)\n+        assert out.shape == (10 * 3, 4)\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
