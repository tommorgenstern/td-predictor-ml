{"custom_id": "keras#8c85f92c570476ee4086a54adad37b9f5e8e0a19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 15564 | Contributors (this commit): 105 | Commits (past 90d): 23 | Contributors (cumulative): 105 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2490,6 +2490,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   def layers(self):\n     return list(self._flatten_layers(include_self=False, recursive=False))\n \n+  @layers.setter\n+  def layers(self, _):\n+    raise AttributeError(\n+        '`Model.layers` attribute is reserved and should not be used. '\n+        'Please use another name.')\n+\n   def get_layer(self, name=None, index=None):\n     \"\"\"Retrieves a layer based on either its name (unique) or index.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3010db921533f9b7c01d87b67eea882249f61062", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 180 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -73,7 +73,9 @@ def filter_empty_gradients(grads_and_vars):\n                      ([v.name for _, v in grads_and_vars],))\n   if vars_with_empty_grads:\n     logging.warning(\n-        (\"Gradients do not exist for variables %s when minimizing the loss.\"),\n+        (\"Gradients do not exist for variables %s when minimizing the loss. \"\n+         \"If you're using `model.compile()`, did you forget to provide a `loss`\"\n+         \"argument?\"),\n         ([v.name for v in vars_with_empty_grads]))\n   return filtered\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d5476a644f69d044197e7c6e09086519442be43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 52 | Churn Cumulative: 6954 | Contributors (this commit): 38 | Commits (past 90d): 5 | Contributors (cumulative): 41 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -1080,6 +1080,19 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     cb_list.on_predict_batch_end(logs)\n     cb_list.on_predict_end(logs)\n \n+  def test_verbose_2_logging(self):\n+    data = np.random.random((100, 1))\n+    labels = np.where(data > 0.5, 1, 0)\n+    model = keras.models.Sequential((keras.layers.Dense(\n+        1, input_dim=1, activation='relu'), keras.layers.Dense(\n+            1, activation='sigmoid'),))\n+    model.compile(\n+        optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n+    expected_log = r'(.*- loss:.*- accuracy:.*epoch)+'\n+    with self.captureWritesToStream(sys.stdout) as printed:\n+      model.fit(data, labels, verbose=2, epochs=20)\n+      self.assertRegex(printed.contents(), expected_log)\n+\n   def test_ProgbarLogger_verbose_2_nonblocking(self):\n     # Should only cause a sync block on epoch end methods.\n     callback = keras.callbacks.ProgbarLogger(count_mode='steps')\n\n@@ -855,6 +855,8 @@ class Progbar(object):\n     self._values = {}\n     self._values_order = []\n     self._start = time.time()\n+    self._time_at_epoch_start = self._start\n+    self._time_at_epoch_end = None\n     self._last_update = 0\n \n     self._time_after_first_step = None\n@@ -899,6 +901,8 @@ class Progbar(object):\n     self._seen_so_far = current\n \n     now = time.time()\n+    if current == self.target:\n+      self._time_at_epoch_end = now\n     info = ' - %.0fs' % (now - self._start)\n     if self.verbose == 1:\n       if now - self._last_update < self.interval and not finalize:\n@@ -933,12 +937,7 @@ class Progbar(object):\n       time_per_unit = self._estimate_step_duration(current, now)\n \n       if self.target is None or finalize:\n-        if time_per_unit >= 1 or time_per_unit == 0:\n-          info += ' %.0fs/%s' % (time_per_unit, self.unit_name)\n-        elif time_per_unit >= 1e-3:\n-          info += ' %.0fms/%s' % (time_per_unit * 1e3, self.unit_name)\n-        else:\n-          info += ' %.0fus/%s' % (time_per_unit * 1e6, self.unit_name)\n+        info += self._format_time(time_per_unit, self.unit_name)\n       else:\n         eta = time_per_unit * (self.target - current)\n         if eta > 3600:\n@@ -984,8 +983,14 @@ class Progbar(object):\n             info += ' %.4f' % avg\n           else:\n             info += ' %.4e' % avg\n+        if self._time_at_epoch_end:\n+          time_per_epoch = self._time_at_epoch_end - self._time_at_epoch_start\n+          avg_time_per_step = time_per_epoch / self.target\n+          self._time_at_epoch_start = now\n+          self._time_at_epoch_end = None\n+          info += ' -' + self._format_time(time_per_epoch, 'epoch')\n+          info += ' -' + self._format_time(avg_time_per_step, self.unit_name)\n         info += '\\n'\n-\n         sys.stdout.write(info)\n         sys.stdout.flush()\n \n@@ -994,6 +999,26 @@ class Progbar(object):\n   def add(self, n, values=None):\n     self.update(self._seen_so_far + n, values)\n \n+  def _format_time(self, time_per_unit, unit_name):\n+    \"\"\"format a given duration to display to the user.\n+\n+    Given the duration, this function formats it in either milliseconds\n+    or seconds and displays the unit (i.e. ms/step or s/epoch)\n+    Args:\n+      time_per_unit: the duration to display\n+      unit_name: the name of the unit to display\n+    Returns:\n+      a string with the correctly formatted duration and units\n+    \"\"\"\n+    formatted = ''\n+    if time_per_unit >= 1 or time_per_unit == 0:\n+      formatted += ' %.0fs/%s' % (time_per_unit, unit_name)\n+    elif time_per_unit >= 1e-3:\n+      formatted += ' %.0fms/%s' % (time_per_unit * 1e3, unit_name)\n+    else:\n+      formatted += ' %.0fus/%s' % (time_per_unit * 1e6, unit_name)\n+    return formatted\n+\n   def _estimate_step_duration(self, current, now):\n     \"\"\"Estimate the duration of a single step.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7af871aae4ccdbe761ac59feb04ec7f300bb727d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 45 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 52 | Churn Cumulative: 7006 | Contributors (this commit): 39 | Commits (past 90d): 7 | Contributors (cumulative): 43 | DMM Complexity: 0.2222222222222222\n\nDIFF:\n@@ -1080,19 +1080,6 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     cb_list.on_predict_batch_end(logs)\n     cb_list.on_predict_end(logs)\n \n-  def test_verbose_2_logging(self):\n-    data = np.random.random((100, 1))\n-    labels = np.where(data > 0.5, 1, 0)\n-    model = keras.models.Sequential((keras.layers.Dense(\n-        1, input_dim=1, activation='relu'), keras.layers.Dense(\n-            1, activation='sigmoid'),))\n-    model.compile(\n-        optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n-    expected_log = r'(.*- loss:.*- accuracy:.*epoch)+'\n-    with self.captureWritesToStream(sys.stdout) as printed:\n-      model.fit(data, labels, verbose=2, epochs=20)\n-      self.assertRegex(printed.contents(), expected_log)\n-\n   def test_ProgbarLogger_verbose_2_nonblocking(self):\n     # Should only cause a sync block on epoch end methods.\n     callback = keras.callbacks.ProgbarLogger(count_mode='steps')\n\n@@ -855,8 +855,6 @@ class Progbar(object):\n     self._values = {}\n     self._values_order = []\n     self._start = time.time()\n-    self._time_at_epoch_start = self._start\n-    self._time_at_epoch_end = None\n     self._last_update = 0\n \n     self._time_after_first_step = None\n@@ -901,8 +899,6 @@ class Progbar(object):\n     self._seen_so_far = current\n \n     now = time.time()\n-    if current == self.target:\n-      self._time_at_epoch_end = now\n     info = ' - %.0fs' % (now - self._start)\n     if self.verbose == 1:\n       if now - self._last_update < self.interval and not finalize:\n@@ -937,7 +933,12 @@ class Progbar(object):\n       time_per_unit = self._estimate_step_duration(current, now)\n \n       if self.target is None or finalize:\n-        info += self._format_time(time_per_unit, self.unit_name)\n+        if time_per_unit >= 1 or time_per_unit == 0:\n+          info += ' %.0fs/%s' % (time_per_unit, self.unit_name)\n+        elif time_per_unit >= 1e-3:\n+          info += ' %.0fms/%s' % (time_per_unit * 1e3, self.unit_name)\n+        else:\n+          info += ' %.0fus/%s' % (time_per_unit * 1e6, self.unit_name)\n       else:\n         eta = time_per_unit * (self.target - current)\n         if eta > 3600:\n@@ -983,14 +984,8 @@ class Progbar(object):\n             info += ' %.4f' % avg\n           else:\n             info += ' %.4e' % avg\n-        if self._time_at_epoch_end:\n-          time_per_epoch = self._time_at_epoch_end - self._time_at_epoch_start\n-          avg_time_per_step = time_per_epoch / self.target\n-          self._time_at_epoch_start = now\n-          self._time_at_epoch_end = None\n-          info += ' -' + self._format_time(time_per_epoch, 'epoch')\n-          info += ' -' + self._format_time(avg_time_per_step, self.unit_name)\n         info += '\\n'\n+\n         sys.stdout.write(info)\n         sys.stdout.flush()\n \n@@ -999,26 +994,6 @@ class Progbar(object):\n   def add(self, n, values=None):\n     self.update(self._seen_so_far + n, values)\n \n-  def _format_time(self, time_per_unit, unit_name):\n-    \"\"\"format a given duration to display to the user.\n-\n-    Given the duration, this function formats it in either milliseconds\n-    or seconds and displays the unit (i.e. ms/step or s/epoch)\n-    Args:\n-      time_per_unit: the duration to display\n-      unit_name: the name of the unit to display\n-    Returns:\n-      a string with the correctly formatted duration and units\n-    \"\"\"\n-    formatted = ''\n-    if time_per_unit >= 1 or time_per_unit == 0:\n-      formatted += ' %.0fs/%s' % (time_per_unit, unit_name)\n-    elif time_per_unit >= 1e-3:\n-      formatted += ' %.0fms/%s' % (time_per_unit * 1e3, unit_name)\n-    else:\n-      formatted += ' %.0fus/%s' % (time_per_unit * 1e6, unit_name)\n-    return formatted\n-\n   def _estimate_step_duration(self, current, now):\n     \"\"\"Estimate the duration of a single step.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f4254fb496508778edb7edb92342b6fe1babefc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 4444 | Contributors (this commit): 39 | Commits (past 90d): 6 | Contributors (cumulative): 42 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1108,7 +1108,7 @@ def to_list(x):\n \n \n def to_snake_case(name):\n-  intermediate = re.sub('(.)([A-Z][a-z0-9]+)', r'\\1_\\2', name)\n+  intermediate = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n   insecure = re.sub('([a-z])([A-Z])', r'\\1_\\2', intermediate).lower()\n   # If the class is private the name starts with \"_\" which is not secure\n   # for creating scopes. We prefix the name with \"private\" in this case.\n\n@@ -24,6 +24,14 @@ import keras\n from keras.utils import generic_utils\n \n \n+class SnakeCaseTest(tf.test.TestCase):\n+\n+  def test_snake_case(self):\n+    self.assertEqual(generic_utils.to_snake_case('SomeClass'), 'some_class')\n+    self.assertEqual(generic_utils.to_snake_case('Conv2D'), 'conv2d')\n+    self.assertEqual(generic_utils.to_snake_case('ConvLSTM2D'), 'conv_lstm2d')\n+\n+\n class HasArgTest(tf.test.TestCase):\n \n   def test_has_arg(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a33c7dbd91a4e8c7e1a5cdbc65ad5d9c8ca29e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 128 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -71,10 +71,8 @@ def serialize_model_as_bytecode(model):\n         t = tarfile.TarInfo(dest_path)\n         t.type = tarfile.DIRTYPE\n         archive.addfile(t)\n-        print(\"Adding \", dest_path)\n       for filename in filenames:\n         dest_path = os.path.join(root, filename)\n-        print(\"Adding \", dest_path)\n         with tf.io.gfile.GFile(dest_path, \"rb\") as f:\n           info = tarfile.TarInfo(name=os.path.relpath(dest_path, temp_dir))\n           info.size = f.size()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dca259a79ba5f8e6cfb4dbb7c01d0ff7946f8335", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 44 | Files Changed: 3 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): -11/1 | Churn Δ: 106 | Churn Cumulative: 22836 | Contributors (this commit): 109 | Commits (past 90d): 42 | Contributors (cumulative): 121 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1228,16 +1228,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n   @trainable.setter\n   def trainable(self, value):\n-    \"\"\"Sets trainable attribute for the layer and its sublayers.\n-\n-    When this value is changed during training (e.g. with a\n-    `tf.keras.callbacks.Callback`) you need to call the parent\n-    `tf.keras.Model.make_train_function` with `force=True` in order to recompile\n-    the training graph.\n-\n-    Args:\n-      value: Boolean with the desired state for the layer's trainable attribute.\n-    \"\"\"\n     for layer in self._flatten_layers():\n       layer._trainable = value\n \n\n@@ -323,11 +323,27 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     super(Model, self).__setattr__(name, value)\n \n   def __reduce__(self):\n+    if self.built:\n       return (deserialize_model_from_bytecode, serialize_model_as_bytecode(self))\n+    else:\n+      # SavedModel (and hence serialize_model_as_bytecode) only support built models,\n+      # but if the model is not built, it can be serialized as a plain Python object\n+      # Thus we call up the MRO to get an implementation of __reduce__,\n+      # possibly landing at object.__reduce__\n+      return super(Model, self).__reduce__()\n \n   def __deepcopy__(self, memo):\n+    if self.built:\n       new = deserialize_model_from_bytecode(*serialize_model_as_bytecode(self))\n       memo[id(self)] = new\n+    else:\n+      # See comment in __reduce__ for explanation \n+      deserializer, serialized, *rest = super(Model, self).__reduce__()\n+      new = deserializer(*serialized)\n+      memo[id(self)] = new\n+      if rest:\n+        state = copy.deepcopy(rest[0], memo=memo)\n+        new.__setstate__(state)\n     return new\n \n   def __copy__(self):\n\n@@ -25,50 +25,62 @@ from keras import keras_parameterized\n from keras import testing_utils\n \n \n-SERIALIZERS = (\n-  (\"copy\", copy.copy),\n-  (\"deepcopy\", copy.deepcopy),\n-  *(\n-    (f\"pickle-{protocol}\", lambda model: pickle.loads(pickle.dumps(model, protocol=protocol)))\n-    for protocol in range(pickle.HIGHEST_PROTOCOL+1)\n-  )\n-)\n-\n class TestPickleProtocol(keras_parameterized.TestCase):\n   \"\"\"Tests pickle protoocol support.\n   \"\"\"\n \n   @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.parameterized.named_parameters(*SERIALIZERS)\n-  def test_pickle_model_fitted(self, serializer):\n-    \"\"\"Fitted models should be copyable/picklable.\"\"\"\n+  @keras_parameterized.parameterized.named_parameters(\n+    (\"copy\", copy.copy),\n+    (\"deepcopy\", copy.deepcopy),\n+    *(\n+      (f\"pickle_protocol_level_{protocol}\", lambda model: pickle.loads(pickle.dumps(model, protocol=protocol)))\n+      for protocol in range(pickle.HIGHEST_PROTOCOL+1)\n+    )\n+  )\n+  def test_built_models(self, serializer):\n+    \"\"\"Built models should be copyable and picklable for all model types\"\"\"\n+\n+    model = testing_utils.get_small_mlp(\n+      num_hidden=1, num_classes=2, input_dim=3\n+    )\n+    model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')\n+  \n+    # train\n+    x = np.random.random(size=(1000, 3))\n+    y = np.random.randint(low=0, high=2, size=(1000,))\n+    model.fit(x, y)  # builds model\n+    y1 = model.predict(x)\n+    # roundtrip with training\n+    model = serializer(model)\n+    y2 = model.predict(x)\n+    # check that the predictions are the same\n+    self.assertAllClose(y1, y2)\n+    # and that we can continue training\n+    model.fit(x, y)\n+    y3 = model.predict(x)\n+    # check that the predictions are the same\n+    self.assertNotAllClose(y2, y3)\n+\n+\n+  @keras_parameterized.run_with_all_model_types\n+  @keras_parameterized.parameterized.named_parameters(\n+    (\"copy\", copy.copy),\n+    (\"deepcopy\", copy.deepcopy),\n+  )\n+  def test_unbuilt_models(self, serializer):\n+    \"\"\"Unbuilt models should be copyable and deepcopyable for all model types\"\"\"\n \n-    # create model\n     original_model = testing_utils.get_small_mlp(\n       num_hidden=1, num_classes=2, input_dim=3\n     )\n-    original_model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')\n+    # roundtrip without compiling or training\n+    model = serializer(original_model)\n+    # compile\n+    model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')\n+    # roundtrip compiled but not trained\n+    model = serializer(model)\n \n-    # train\n-    x = np.random.random((1000, 3))\n-    y = np.random.randint(low=0, high=2, size=(1000, ))\n-    original_model.fit(x, y)\n-    y1 = original_model.predict(x)\n-    original_weights = original_model.get_weights()\n-\n-    # roundtrip and check that the predictions are the same\n-    new_model = serializer(original_model)\n-\n-    y2 = new_model.predict(x)\n-    self.assertAllClose(y1, y2)\n-\n-    # make sure we can keep training\n-    new_model.fit(x, y)\n-    new_weights = new_model.get_weights()\n-    # the weights on the new model should have changed\n-    self.assertNotAllClose(new_weights, original_weights)\n-    # but the weights on the original model have not been touched\n-    self.assertAllClose(original_weights, original_model.get_weights())\n \n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b6bffe08faa3e1313885b14f82ac955e42db1ad7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 7030 | Contributors (this commit): 15 | Commits (past 90d): 12 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -1228,6 +1228,16 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n   @trainable.setter\n   def trainable(self, value):\n+    \"\"\"Sets trainable attribute for the layer and its sublayers.\n+\n+    When this value is changed during training (e.g. with a\n+    `tf.keras.callbacks.Callback`) you need to call the parent\n+    `tf.keras.Model.make_train_function` with `force=True` in order to recompile\n+    the training graph.\n+\n+    Args:\n+      value: Boolean with the desired state for the layer's trainable attribute.\n+    \"\"\"\n     for layer in self._flatten_layers():\n       layer._trainable = value\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f50628fb0938e47b37722651e6dce1db8a8d856", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 15589 | Contributors (this commit): 105 | Commits (past 90d): 25 | Contributors (cumulative): 105 | DMM Complexity: None\n\nDIFF:\n@@ -327,9 +327,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       return (deserialize_model_from_bytecode, serialize_model_as_bytecode(self))\n     else:\n       # SavedModel (and hence serialize_model_as_bytecode) only support built models,\n-      # but if the model is not built, it can be serialized as a plain Python object\n-      # Thus we call up the MRO to get an implementation of __reduce__,\n-      # possibly landing at object.__reduce__\n+      # but if the model is not built,\n+      # it _may_ be possible to serialize as a plain Python object,\n+      # as long as the constituent parts (layers, optimizers, losses, etc.)\n+      # can be serialized as plain Python objects.\n+      # Thus we call up the MRO to get an implementation of __reduce__\n+      # to try to pickle this Model as a plain Python object.\n       return super(Model, self).__reduce__()\n \n   def __deepcopy__(self, memo):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a61ffc52b231325eda91d2f01c7df2a99d91222", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 16 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 11148 | Contributors (this commit): 27 | Commits (past 90d): 16 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -272,7 +272,7 @@ def swish(x):\n \n @keras_export('keras.activations.relu')\n @tf.__internal__.dispatch.add_dispatch_support\n-def relu(x, alpha=0., max_value=None, threshold=0):\n+def relu(x, alpha=0., max_value=None, threshold=.0):\n   \"\"\"Applies the rectified linear unit activation function.\n \n   With default values, this returns the standard ReLU activation:\n@@ -289,9 +289,9 @@ def relu(x, alpha=0., max_value=None, threshold=0):\n   array([ 0.,  0.,  0.,  5., 10.], dtype=float32)\n   >>> tf.keras.activations.relu(foo, alpha=0.5).numpy()\n   array([-5. , -2.5,  0. ,  5. , 10. ], dtype=float32)\n-  >>> tf.keras.activations.relu(foo, max_value=5).numpy()\n+  >>> tf.keras.activations.relu(foo, max_value=5.).numpy()\n   array([0., 0., 0., 5., 5.], dtype=float32)\n-  >>> tf.keras.activations.relu(foo, threshold=5).numpy()\n+  >>> tf.keras.activations.relu(foo, threshold=5.).numpy()\n   array([-0., -0.,  0.,  0., 10.], dtype=float32)\n \n   Args:\n\n@@ -4667,7 +4667,7 @@ def in_test_phase(x, alt, training=None):\n @keras_export('keras.backend.relu')\n @tf.__internal__.dispatch.add_dispatch_support\n @doc_controls.do_not_generate_docs\n-def relu(x, alpha=0., max_value=None, threshold=0):\n+def relu(x, alpha=0., max_value=None, threshold=.0):\n   \"\"\"Rectified linear unit.\n \n   With default values, it returns element-wise `max(x, 0)`.\n\n@@ -312,17 +312,17 @@ class BackendVariableTest(tf.test.TestCase):\n     self.assertAllClose(val, np.ones((3, 4)))\n \n   def test_random_uniform_variable(self):\n-    x = backend.random_uniform_variable((30, 20), low=1, high=2, seed=0)\n+    x = backend.random_uniform_variable((30, 20), low=1., high=2., seed=0)\n     val = backend.eval(x)\n     self.assertAllClose(val.mean(), 1.5, atol=1e-1)\n     self.assertAllClose(val.max(), 2., atol=1e-1)\n     self.assertAllClose(val.min(), 1., atol=1e-1)\n \n   def test_random_normal_variable(self):\n-    x = backend.random_normal_variable((30, 20), 1., 0.5, seed=0)\n+    x = backend.random_normal_variable((30, 20), 1., .5, seed=0)\n     val = backend.eval(x)\n     self.assertAllClose(val.mean(), 1., atol=1e-1)\n-    self.assertAllClose(val.std(), 0.5, atol=1e-1)\n+    self.assertAllClose(val.std(), .5, atol=1e-1)\n \n   def test_count_params(self):\n     x = backend.zeros((4, 5))\n@@ -505,17 +505,17 @@ class BackendLinearAlgebraTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(backend.eval(relu_op), [[-2, 0], [2, 7]])\n \n     # max_value < some elements\n-    relu_op = backend.relu(x, max_value=5)\n+    relu_op = backend.relu(x, max_value=5.)\n     self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 5]])\n \n     # nn.relu6 used\n-    relu_op = backend.relu(x, max_value=6)\n+    relu_op = backend.relu(x, max_value=6.)\n     if not tf.executing_eagerly():\n       self.assertTrue('Relu6' in relu_op.name)  # uses tf.nn.relu6\n     self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 6]])\n \n     # max value > 6\n-    relu_op = backend.relu(x, max_value=10)\n+    relu_op = backend.relu(x, max_value=10.)\n     self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 7]])\n \n     # max value is float\n@@ -523,11 +523,11 @@ class BackendLinearAlgebraTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 4.3]])\n \n     # max value == 0\n-    relu_op = backend.relu(x, max_value=0)\n+    relu_op = backend.relu(x, max_value=.0)\n     self.assertAllClose(backend.eval(relu_op), [[0, 0], [0, 0]])\n \n     # alpha and max_value\n-    relu_op = backend.relu(x, alpha=0.25, max_value=3)\n+    relu_op = backend.relu(x, alpha=0.25, max_value=3.)\n     self.assertAllClose(backend.eval(relu_op), [[-1, 0], [2, 3]])\n \n     # threshold\n@@ -543,20 +543,20 @@ class BackendLinearAlgebraTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(backend.eval(relu_op), [[-4, 0], [2, 7]])\n \n     # threshold and max_value\n-    relu_op = backend.relu(x, threshold=3, max_value=5)\n+    relu_op = backend.relu(x, threshold=3, max_value=5.)\n     self.assertAllClose(backend.eval(relu_op), [[0, 0], [0, 5]])\n \n     # threshold and alpha\n-    relu_op = backend.relu(x, alpha=0.25, threshold=4)\n+    relu_op = backend.relu(x, alpha=0.25, threshold=4.)\n     self.assertAllClose(backend.eval(relu_op), [[-2, -1], [-0.5, 7]])\n \n     # threshold, alpha, and max_value\n-    relu_op = backend.relu(x, alpha=0.25, threshold=4, max_value=5)\n+    relu_op = backend.relu(x, alpha=0.25, threshold=4., max_value=5.)\n     self.assertAllClose(backend.eval(relu_op), [[-2, -1], [-0.5, 5]])\n \n     # Test case for GitHub issue 35430, with integer dtype\n     x = input_layer.Input(shape=(), name='x', dtype='int64')\n-    _ = advanced_activations.ReLU(max_value=100, dtype='int64')(x)\n+    _ = advanced_activations.ReLU(max_value=100., dtype='int64')(x)\n \n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6325142d655bac2724187fa426cb8728563c92f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 32 | Files Changed: 6 | Hunks: 22 | Methods Changed: 26 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 64 | Churn Cumulative: 12662 | Contributors (this commit): 15 | Commits (past 90d): 19 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -1945,7 +1945,7 @@ def dot(x, y):\n \n   If `x` is an N-D array and `y` is an M-D array (where M>=2), it is a sum\n   product over the last axis of `x` and the second-to-last axis of `y`.\n-  >>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=0, high=1)\n+  >>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=.0, high=1.)\n   >>> y = tf.keras.backend.ones((4, 3, 5))\n   >>> xy = tf.keras.backend.dot(x, y)\n   >>> tf.keras.backend.int_shape(xy)\n\n@@ -46,7 +46,7 @@ def strategy_combinations_eager_data_fn():\n \n \n def get_numpy():\n-  inputs = np.random.uniform(low=-5, high=5, size=(64, 2)).astype(np.float32)\n+  inputs = np.random.uniform(low=-5., high=5., size=(64, 2)).astype(np.float32)\n   output = .3 * inputs[:, 0] + .2 * inputs[:, 1]\n   return inputs, output\n \n\n@@ -556,7 +556,7 @@ class BinaryCrossentropy(LossFunctionWrapper):\n \n   def __init__(self,\n                from_logits=False,\n-               label_smoothing=0,\n+               label_smoothing=.0,\n                axis=-1,\n                reduction=losses_utils.ReductionV2.AUTO,\n                name='binary_crossentropy'):\n@@ -640,7 +640,7 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n \n   def __init__(self,\n                from_logits=False,\n-               label_smoothing=0,\n+               label_smoothing=.0,\n                axis=-1,\n                reduction=losses_utils.ReductionV2.AUTO,\n                name='categorical_crossentropy'):\n@@ -1623,7 +1623,7 @@ def log_cosh(y_true, y_pred):\n def categorical_crossentropy(y_true,\n                              y_pred,\n                              from_logits=False,\n-                             label_smoothing=0,\n+                             label_smoothing=.0,\n                              axis=-1):\n   \"\"\"Computes the categorical crossentropy loss.\n \n@@ -1671,7 +1671,7 @@ def categorical_crossentropy(y_true,\n def _ragged_tensor_categorical_crossentropy(y_true,\n                                             y_pred,\n                                             from_logits=False,\n-                                            label_smoothing=0,\n+                                            label_smoothing=.0,\n                                             axis=-1):\n   \"\"\"Implements support for handling RaggedTensors.\n \n@@ -1768,7 +1768,7 @@ def _ragged_tensor_sparse_categorical_crossentropy(y_true,\n def binary_crossentropy(y_true,\n                         y_pred,\n                         from_logits=False,\n-                        label_smoothing=0,\n+                        label_smoothing=.0,\n                         axis=-1):\n   \"\"\"Computes the binary crossentropy loss.\n \n@@ -1814,7 +1814,7 @@ def binary_crossentropy(y_true,\n def _ragged_tensor_binary_crossentropy(y_true,\n                                        y_pred,\n                                        from_logits=False,\n-                                       label_smoothing=0,\n+                                       label_smoothing=.0,\n                                        axis=-1):\n   \"\"\"Implements support for handling RaggedTensors.\n \n\n@@ -60,7 +60,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n     for policy in ('float32', 'int8', 'mixed_bfloat16', '_infer'):\n       self.assertEqual(repr(mp_policy.PolicyV1(policy)),\n                        '<PolicyV1 \"%s\", loss_scale=None>' % policy)\n-    self.assertEqual(repr(mp_policy.PolicyV1('float16', loss_scale=2)),\n+    self.assertEqual(repr(mp_policy.PolicyV1('float16', loss_scale=2.)),\n                      '<PolicyV1 \"float16\", loss_scale=FixedLossScale(2.0)>')\n     self.assertStartsWith(\n         repr(mp_policy.PolicyV1('mixed_float16')),\n\n@@ -34,7 +34,7 @@ class LinearModelTest(keras_parameterized.TestCase):\n \n   def test_linear_model_with_single_input(self):\n     model = linear.LinearModel()\n-    inp = np.random.uniform(low=-5, high=5, size=(64, 2))\n+    inp = np.random.uniform(low=-5., high=5., size=(64, 2))\n     output = .3 * inp[:, 0] + .2 * inp[:, 1]\n     model.compile('sgd', 'mse', [])\n     model.fit(inp, output, epochs=5)\n@@ -42,16 +42,16 @@ class LinearModelTest(keras_parameterized.TestCase):\n \n   def test_linear_model_with_list_input(self):\n     model = linear.LinearModel()\n-    input_a = np.random.uniform(low=-5, high=5, size=(64, 1))\n-    input_b = np.random.uniform(low=-5, high=5, size=(64, 1))\n+    input_a = np.random.uniform(low=-5., high=5., size=(64, 1))\n+    input_b = np.random.uniform(low=-5., high=5., size=(64, 1))\n     output = .3 * input_a + .2 * input_b\n     model.compile('sgd', 'mse', [])\n     model.fit([input_a, input_b], output, epochs=5)\n \n   def test_linear_model_with_mismatched_dict_inputs(self):\n     model = linear.LinearModel()\n-    input_a = np.random.uniform(low=-5, high=5, size=(64, 1))\n-    input_b = np.random.uniform(low=-5, high=5, size=(64, 1))\n+    input_a = np.random.uniform(low=-5., high=5., size=(64, 1))\n+    input_b = np.random.uniform(low=-5., high=5., size=(64, 1))\n     output = .3 * input_a + .2 * input_b\n     model.compile('sgd', 'mse', [])\n     model.build({'a': tf.TensorShape([None, 1]),\n@@ -61,8 +61,8 @@ class LinearModelTest(keras_parameterized.TestCase):\n \n   def test_linear_model_with_dict_input(self):\n     model = linear.LinearModel()\n-    input_a = np.random.uniform(low=-5, high=5, size=(64, 1))\n-    input_b = np.random.uniform(low=-5, high=5, size=(64, 1))\n+    input_a = np.random.uniform(low=-5., high=5., size=(64, 1))\n+    input_b = np.random.uniform(low=-5., high=5., size=(64, 1))\n     output = .3 * input_a + .2 * input_b\n     model.compile('sgd', 'mse', [])\n     model.fit({'a': input_a, 'b': input_b}, output, epochs=5)\n@@ -74,8 +74,8 @@ class LinearModelTest(keras_parameterized.TestCase):\n     output_b = core.Dense(units=1)(input_b)\n     output = output_a + output_b\n     model = training.Model(inputs=[input_a, input_b], outputs=[output])\n-    input_a_np = np.random.uniform(low=-5, high=5, size=(64, 1))\n-    input_b_np = np.random.uniform(low=-5, high=5, size=(64, 1))\n+    input_a_np = np.random.uniform(low=-5., high=5., size=(64, 1))\n+    input_b_np = np.random.uniform(low=-5., high=5., size=(64, 1))\n     output_np = .3 * input_a_np + .2 * input_b_np\n     model.compile('sgd', 'mse', [])\n     model.fit([input_a_np, input_b_np], output_np, epochs=5)\n@@ -105,19 +105,19 @@ class LinearModelTest(keras_parameterized.TestCase):\n       rand_int = np.random.randint(3)\n       if rand_int == 0:\n         indices.append((i, 0))\n-        val = np.random.uniform(low=-5, high=5)\n+        val = np.random.uniform(low=-5., high=5.)\n         values.append(val)\n         target[i] = 0.3 * val\n       elif rand_int == 1:\n         indices.append((i, 1))\n-        val = np.random.uniform(low=-5, high=5)\n+        val = np.random.uniform(low=-5., high=5.)\n         values.append(val)\n         target[i] = 0.2 * val\n       else:\n         indices.append((i, 0))\n         indices.append((i, 1))\n-        val_1 = np.random.uniform(low=-5, high=5)\n-        val_2 = np.random.uniform(low=-5, high=5)\n+        val_1 = np.random.uniform(low=-5., high=5.)\n+        val_2 = np.random.uniform(low=-5., high=5.)\n         values.append(val_1)\n         values.append(val_2)\n         target[i] = 0.3 * val_1 + 0.2 * val_2\n\n@@ -36,8 +36,8 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n     linear_model = linear.LinearModel(units=1)\n     dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])\n     wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)\n-    linear_inp = np.random.uniform(low=-5, high=5, size=(64, 2))\n-    dnn_inp = np.random.uniform(low=-5, high=5, size=(64, 3))\n+    linear_inp = np.random.uniform(low=-5., high=5., size=(64, 2))\n+    dnn_inp = np.random.uniform(low=-5., high=5., size=(64, 3))\n     inputs = [linear_inp, dnn_inp]\n     output = .3 * linear_inp[:, 0] + .2 * dnn_inp[:, 1]\n     wide_deep_model.compile(\n@@ -78,7 +78,7 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n     linear_model = linear.LinearModel(units=1)\n     dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])\n     wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)\n-    inputs = np.random.uniform(low=-5, high=5, size=(64, 3))\n+    inputs = np.random.uniform(low=-5., high=5., size=(64, 3))\n     output = .3 * inputs[:, 0]\n     wide_deep_model.compile(\n         optimizer=['sgd', 'adam'],\n@@ -115,8 +115,8 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n     linear_model = linear.LinearModel(units=1)\n     dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])\n     wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)\n-    linear_inp = np.random.uniform(low=-5, high=5, size=(64, 2))\n-    dnn_inp = np.random.uniform(low=-5, high=5, size=(64, 3))\n+    linear_inp = np.random.uniform(low=-5., high=5., size=(64, 2))\n+    dnn_inp = np.random.uniform(low=-5., high=5., size=(64, 3))\n     inputs = [linear_inp, dnn_inp]\n     output = .3 * linear_inp[:, 0] + .2 * dnn_inp[:, 1]\n     wide_deep_model.compile(\n@@ -139,9 +139,9 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n     model = training.Model(\n         inputs=[linear_input, dnn_input, input_b],\n         outputs=[wide_deep_output + output_b])\n-    linear_input_np = np.random.uniform(low=-5, high=5, size=(64, 3))\n-    dnn_input_np = np.random.uniform(low=-5, high=5, size=(64, 5))\n-    input_b_np = np.random.uniform(low=-5, high=5, size=(64,))\n+    linear_input_np = np.random.uniform(low=-5., high=5., size=(64, 3))\n+    dnn_input_np = np.random.uniform(low=-5., high=5., size=(64, 5))\n+    input_b_np = np.random.uniform(low=-5., high=5., size=(64,))\n     output_np = linear_input_np[:, 0] + .2 * dnn_input_np[:, 1] + input_b_np\n     model.compile(\n         optimizer='sgd',\n@@ -156,8 +156,8 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n     wide_deep_model = wide_deep.WideDeepModel(\n         linear.LinearModel(units=1),\n         sequential.Sequential([core.Dense(units=1, input_dim=3)]))\n-    linear_inp = np.random.uniform(low=-5, high=5, size=(64, 2))\n-    dnn_inp = np.random.uniform(low=-5, high=5, size=(64, 3))\n+    linear_inp = np.random.uniform(low=-5., high=5., size=(64, 2))\n+    dnn_inp = np.random.uniform(low=-5., high=5., size=(64, 3))\n     inputs = [linear_inp, dnn_inp]\n     output = .3 * linear_inp[:, 0] + .2 * dnn_inp[:, 1]\n     linear_model.compile(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1cc01da4392729bf379904ffdf6bb1706076867", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 5711 | Contributors (this commit): 29 | Commits (past 90d): 4 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -2091,7 +2091,7 @@ class Model(training_lib.Model):\n                                           sample_weight=None,\n                                           class_weight=None,\n                                           batch_size=None,\n-                                          validation_split=0,\n+                                          validation_split=.0,\n                                           shuffle=False,\n                                           epochs=1,\n                                           allow_partial_batch=False):\n@@ -2205,7 +2205,7 @@ class Model(training_lib.Model):\n                              check_steps=False,\n                              steps_name='steps',\n                              steps=None,\n-                             validation_split=0,\n+                             validation_split=.0,\n                              shuffle=False,\n                              extract_tensors_from_dataset=False):\n     \"\"\"Runs validation checks on input and target data passed by the user.\n\n@@ -406,7 +406,7 @@ class ReLU(Layer):\n       to 0.\n   \"\"\"\n \n-  def __init__(self, max_value=None, negative_slope=0, threshold=0, **kwargs):\n+  def __init__(self, max_value=None, negative_slope=.0, threshold=.0, **kwargs):\n     super(ReLU, self).__init__(**kwargs)\n     if max_value is not None and max_value < 0.:\n       raise ValueError('max_value of a ReLU layer cannot be a negative '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#122d5d10f9cab0a74e6aa124ba1355a16fce8752", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 391 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -46,7 +46,7 @@ def model_to_estimator(\n \n   For usage example, please see:\n   [Creating estimators from Keras Models](\n-    https://www.tensorflow.org/guide/estimators#creating_estimators_from_keras_models).\n+    https://www.tensorflow.org/guide/estimator#create_an_estimator_from_a_keras_model).\n \n   Sample Weights:\n   Estimators returned by `model_to_estimator` are configured so that they can\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24cc55d4245b93ecc9685ee389243ca7002bd196", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 15 | Files Changed: 6 | Hunks: 15 | Methods Changed: 25 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 21258 | Contributors (this commit): 53 | Commits (past 90d): 32 | Contributors (cumulative): 82 | DMM Complexity: None\n\nDIFF:\n@@ -272,7 +272,7 @@ def swish(x):\n \n @keras_export('keras.activations.relu')\n @tf.__internal__.dispatch.add_dispatch_support\n-def relu(x, alpha=0., max_value=None, threshold=.0):\n+def relu(x, alpha=0., max_value=None, threshold=0.):\n   \"\"\"Applies the rectified linear unit activation function.\n \n   With default values, this returns the standard ReLU activation:\n\n@@ -1945,7 +1945,7 @@ def dot(x, y):\n \n   If `x` is an N-D array and `y` is an M-D array (where M>=2), it is a sum\n   product over the last axis of `x` and the second-to-last axis of `y`.\n-  >>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=.0, high=1.)\n+  >>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=0., high=1.)\n   >>> y = tf.keras.backend.ones((4, 3, 5))\n   >>> xy = tf.keras.backend.dot(x, y)\n   >>> tf.keras.backend.int_shape(xy)\n@@ -4667,7 +4667,7 @@ def in_test_phase(x, alt, training=None):\n @keras_export('keras.backend.relu')\n @tf.__internal__.dispatch.add_dispatch_support\n @doc_controls.do_not_generate_docs\n-def relu(x, alpha=0., max_value=None, threshold=.0):\n+def relu(x, alpha=0., max_value=None, threshold=0.):\n   \"\"\"Rectified linear unit.\n \n   With default values, it returns element-wise `max(x, 0)`.\n\n@@ -319,10 +319,10 @@ class BackendVariableTest(tf.test.TestCase):\n     self.assertAllClose(val.min(), 1., atol=1e-1)\n \n   def test_random_normal_variable(self):\n-    x = backend.random_normal_variable((30, 20), 1., .5, seed=0)\n+    x = backend.random_normal_variable((30, 20), 1., 0.5, seed=0)\n     val = backend.eval(x)\n     self.assertAllClose(val.mean(), 1., atol=1e-1)\n-    self.assertAllClose(val.std(), .5, atol=1e-1)\n+    self.assertAllClose(val.std(), 0.5, atol=1e-1)\n \n   def test_count_params(self):\n     x = backend.zeros((4, 5))\n@@ -523,7 +523,7 @@ class BackendLinearAlgebraTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 4.3]])\n \n     # max value == 0\n-    relu_op = backend.relu(x, max_value=.0)\n+    relu_op = backend.relu(x, max_value=0.)\n     self.assertAllClose(backend.eval(relu_op), [[0, 0], [0, 0]])\n \n     # alpha and max_value\n\n@@ -2091,7 +2091,7 @@ class Model(training_lib.Model):\n                                           sample_weight=None,\n                                           class_weight=None,\n                                           batch_size=None,\n-                                          validation_split=.0,\n+                                          validation_split=0.,\n                                           shuffle=False,\n                                           epochs=1,\n                                           allow_partial_batch=False):\n@@ -2205,7 +2205,7 @@ class Model(training_lib.Model):\n                              check_steps=False,\n                              steps_name='steps',\n                              steps=None,\n-                             validation_split=.0,\n+                             validation_split=0.,\n                              shuffle=False,\n                              extract_tensors_from_dataset=False):\n     \"\"\"Runs validation checks on input and target data passed by the user.\n\n@@ -406,7 +406,7 @@ class ReLU(Layer):\n       to 0.\n   \"\"\"\n \n-  def __init__(self, max_value=None, negative_slope=.0, threshold=.0, **kwargs):\n+  def __init__(self, max_value=None, negative_slope=0., threshold=0., **kwargs):\n     super(ReLU, self).__init__(**kwargs)\n     if max_value is not None and max_value < 0.:\n       raise ValueError('max_value of a ReLU layer cannot be a negative '\n\n@@ -556,7 +556,7 @@ class BinaryCrossentropy(LossFunctionWrapper):\n \n   def __init__(self,\n                from_logits=False,\n-               label_smoothing=.0,\n+               label_smoothing=0.,\n                axis=-1,\n                reduction=losses_utils.ReductionV2.AUTO,\n                name='binary_crossentropy'):\n@@ -640,7 +640,7 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n \n   def __init__(self,\n                from_logits=False,\n-               label_smoothing=.0,\n+               label_smoothing=0.,\n                axis=-1,\n                reduction=losses_utils.ReductionV2.AUTO,\n                name='categorical_crossentropy'):\n@@ -1623,7 +1623,7 @@ def log_cosh(y_true, y_pred):\n def categorical_crossentropy(y_true,\n                              y_pred,\n                              from_logits=False,\n-                             label_smoothing=.0,\n+                             label_smoothing=0.,\n                              axis=-1):\n   \"\"\"Computes the categorical crossentropy loss.\n \n@@ -1671,7 +1671,7 @@ def categorical_crossentropy(y_true,\n def _ragged_tensor_categorical_crossentropy(y_true,\n                                             y_pred,\n                                             from_logits=False,\n-                                            label_smoothing=.0,\n+                                            label_smoothing=0.,\n                                             axis=-1):\n   \"\"\"Implements support for handling RaggedTensors.\n \n@@ -1768,7 +1768,7 @@ def _ragged_tensor_sparse_categorical_crossentropy(y_true,\n def binary_crossentropy(y_true,\n                         y_pred,\n                         from_logits=False,\n-                        label_smoothing=.0,\n+                        label_smoothing=0.,\n                         axis=-1):\n   \"\"\"Computes the binary crossentropy loss.\n \n@@ -1814,7 +1814,7 @@ def binary_crossentropy(y_true,\n def _ragged_tensor_binary_crossentropy(y_true,\n                                        y_pred,\n                                        from_logits=False,\n-                                       label_smoothing=.0,\n+                                       label_smoothing=0.,\n                                        axis=-1):\n   \"\"\"Implements support for handling RaggedTensors.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a96eedc7f78aa3e70dc2ebd0d1eba301bf2663a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 15740 | Contributors (this commit): 105 | Commits (past 90d): 34 | Contributors (cumulative): 107 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,7 +37,7 @@ from keras.mixed_precision import policy\n from keras.saving import hdf5_format\n from keras.saving import save\n from keras.saving import saving_utils\n-from keras.saving.pickle_utils import deserialize_model_from_bytecode, serialize_model_as_bytecode\n+from keras.saving import pickle_utils\n from keras.saving.saved_model import json_utils\n from keras.saving.saved_model import model_serialization\n from keras.utils import generic_utils\n@@ -324,7 +324,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n   def __reduce__(self):\n     if self.built:\n-      return (deserialize_model_from_bytecode, serialize_model_as_bytecode(self))\n+      return (\n+        pickle_utils.deserialize_model_from_bytecode,\n+        pickle_utils.serialize_model_as_bytecode(self)\n+      )\n     else:\n       # SavedModel (and hence serialize_model_as_bytecode) only support built models,\n       # but if the model is not built,\n@@ -337,7 +340,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n   def __deepcopy__(self, memo):\n     if self.built:\n-      new = deserialize_model_from_bytecode(*serialize_model_as_bytecode(self))\n+      new = pickle_utils.deserialize_model_from_bytecode(\n+        *pickle_utils.serialize_model_as_bytecode(self)\n+      )\n       memo[id(self)] = new\n     else:\n       # See comment in __reduce__ for explanation \n\n@@ -22,16 +22,16 @@ import tarfile\n from io import BytesIO\n from uuid import uuid4\n \n-from numpy import asarray\n+import numpy\n \n-from keras.saving.save import load_model\n+from keras.saving import save as save_module\n \n \n def deserialize_model_from_bytecode(serialized_model):\n   \"\"\"Reconstruct a Model from the output of `serialize_model_as_bytecode`.\n \n   Args:\n-      serialized_model (np.array): return value from `serialize_model_as_bytecode`.\n+      serialized_model: (np.array) return value from `serialize_model_as_bytecode`.\n \n   Returns:\n       keras.Model: Keras Model instance.\n@@ -46,7 +46,7 @@ def deserialize_model_from_bytecode(serialized_model):\n       if member.isfile():\n         with tf.io.gfile.GFile(dest_path, \"wb\") as f:\n           f.write(archive.extractfile(name).read())\n-  model = load_model(temp_dir)\n+  model = save_module.load_model(temp_dir)\n   tf.io.gfile.rmtree(temp_dir)\n   return model\n \n@@ -55,7 +55,7 @@ def serialize_model_as_bytecode(model):\n   \"\"\"Convert a Keras Model into a bytecode representation for pickling.\n \n   Args:\n-      model (tf.keras.Model): Keras Model instance.\n+      model: (tf.keras.Model) Keras Model instance.\n \n   Returns:\n       tuple: tuple of arguments that can be sent to\n@@ -79,4 +79,4 @@ def serialize_model_as_bytecode(model):\n           archive.addfile(tarinfo=info, fileobj=f)\n   tf.io.gfile.rmtree(temp_dir)\n   b.seek(0)\n-  return (asarray(memoryview(b.read())), )\n+  return (numpy.asarray(memoryview(b.read())), )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
