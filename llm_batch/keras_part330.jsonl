{"custom_id": "keras#07bb88c02a9f816bd33d012deb877eb5660a5911", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 290 | Lines Deleted: 70 | Files Changed: 21 | Hunks: 60 | Methods Changed: 33 | Complexity Δ (Sum/Max): 31/25 | Churn Δ: 360 | Churn Cumulative: 57198 | Contributors (this commit): 164 | Commits (past 90d): 166 | Contributors (cumulative): 234 | DMM Complexity: 0.46540880503144655\n\nDIFF:\n@@ -6476,11 +6476,19 @@ def convert_inputs_if_ragged(inputs):\n   return inputs, nested_row_lengths\n \n \n-def maybe_convert_to_ragged(is_ragged_input, output, nested_row_lengths):\n+def maybe_convert_to_ragged(is_ragged_input, output, nested_row_lengths,\n+                            go_backwards=False):\n   \"\"\"Converts any ragged input back to its initial structure.\"\"\"\n   if not is_ragged_input:\n     return output\n \n+  if go_backwards:\n+    # Reverse based on the timestep dim, so that nested_row_lengths will mask\n+    # from the correct direction. Return the reverse ragged tensor.\n+    output = reverse(output, [1])\n+    ragged = tf.RaggedTensor.from_tensor(output, nested_row_lengths)\n+    return reverse(ragged, [1])\n+  else:\n     return tf.RaggedTensor.from_tensor(output, nested_row_lengths)\n \n \n\n@@ -1625,7 +1625,8 @@ class BackupAndRestore(Callback):\n     self._supported_strategies = (\n         tf.distribute.MirroredStrategy,\n         tf.distribute.MultiWorkerMirroredStrategy,\n-        tf.distribute.experimental.TPUStrategy, tf.distribute.TPUStrategy)\n+        tf.distribute.experimental.TPUStrategy, tf.distribute.TPUStrategy,\n+        tf.distribute.experimental.ParameterServerStrategy)\n \n     if not tf.executing_eagerly():\n       if tf.inside_function():\n@@ -2413,7 +2414,8 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     if self.write_steps_per_second:\n       batch_run_time = time.time() - self._batch_start_time\n       self._train_accumulated_time += batch_run_time\n-      tf.summary.scalar('batch_steps_per_second', 1. / batch_run_time)\n+      tf.summary.scalar(\n+          'batch_steps_per_second', 1. / batch_run_time, step=self._train_step)\n     if not self._should_trace:\n       return\n \n\n@@ -22,12 +22,12 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from tensorflow.python.distribute import multi_worker_test_base\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n from keras import backend\n from keras import testing_utils\n from keras.distribute import distributed_training_utils\n from keras.distribute import distributed_training_utils_v1\n+from keras.distribute import multi_worker_testing_utils\n from keras.distribute import optimizer_combinations\n from keras.distribute.strategy_combinations import all_strategies\n from keras.distribute.strategy_combinations import multi_worker_mirrored_strategies\n@@ -2398,7 +2398,7 @@ class TestDistributionStrategyWithKerasModels(tf.test.TestCase,\n \n   @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=['graph', 'eager']))\n   def test_unimplemented_parameter_server_strategy(self):\n-    cluster_spec = multi_worker_test_base.create_in_process_cluster(\n+    cluster_spec = multi_worker_testing_utils.create_in_process_cluster(\n         num_workers=3, num_ps=2)\n     cluster_resolver = SimpleClusterResolver(\n         cluster_spec=tf.train.ClusterSpec(cluster_spec),\n\n@@ -16,13 +16,26 @@\n \n import tensorflow.compat.v2 as tf\n \n+import threading\n+import unittest\n import keras\n-from tensorflow.python.distribute import multi_worker_test_base\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n from keras.optimizer_v2 import gradient_descent\n+from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training.server_lib import ClusterSpec\n \n \n+_portpicker_import_error = None\n+try:\n+  import portpicker  # pylint: disable=g-import-not-at-top\n+except (ImportError, ModuleNotFoundError) as _error:  # pylint: disable=invalid-name\n+  _portpicker_import_error = _error\n+  portpicker = None\n+\n+ASSIGNED_PORTS = set()\n+lock = threading.Lock()\n+\n+\n def mnist_synthetic_dataset(batch_size, steps_per_epoch):\n   \"\"\"Generate synthetic MNIST dataset for testing.\"\"\"\n   # train dataset\n@@ -74,6 +87,151 @@ def get_mnist_model(input_shape):\n \n \n def make_parameter_server_cluster(num_workers, num_ps):\n-  cluster_def = multi_worker_test_base.create_in_process_cluster(\n+  cluster_def = create_in_process_cluster(\n       num_workers=num_workers, num_ps=num_ps, rpc_layer=\"grpc\")\n   return SimpleClusterResolver(ClusterSpec(cluster_def), rpc_layer=\"grpc\")\n+\n+\n+def pick_unused_port():\n+  \"\"\"Returns an unused and unassigned local port.\"\"\"\n+  if _portpicker_import_error:\n+    raise _portpicker_import_error  # pylint: disable=raising-bad-type\n+\n+  global ASSIGNED_PORTS\n+  with lock:\n+    while True:\n+      try:\n+        port = portpicker.pick_unused_port()\n+      except portpicker.NoFreePortFoundError:\n+        raise unittest.SkipTest(\"Flakes in portpicker library do not represent \"\n+                                \"TensorFlow errors.\")\n+      if port > 10000 and port not in ASSIGNED_PORTS:\n+        ASSIGNED_PORTS.add(port)\n+        logging.info(\"Using local port %r\", port)\n+        return port\n+\n+\n+def _create_cluster(num_workers,\n+                    num_ps,\n+                    has_chief=False,\n+                    has_eval=False,\n+                    protocol=\"grpc\",\n+                    worker_config=None,\n+                    ps_config=None,\n+                    eval_config=None,\n+                    worker_name=\"worker\",\n+                    ps_name=\"ps\",\n+                    chief_name=\"chief\"):\n+  \"\"\"Creates and starts local servers and returns the cluster_spec dict.\"\"\"\n+  if _portpicker_import_error:\n+    raise _portpicker_import_error  # pylint: disable=raising-bad-type\n+  worker_ports = [pick_unused_port() for _ in range(num_workers)]\n+  ps_ports = [pick_unused_port() for _ in range(num_ps)]\n+\n+  cluster_dict = {}\n+  if num_workers > 0:\n+    cluster_dict[worker_name] = [\"localhost:%s\" % port for port in worker_ports]\n+  if num_ps > 0:\n+    cluster_dict[ps_name] = [\"localhost:%s\" % port for port in ps_ports]\n+  if has_eval:\n+    cluster_dict[\"evaluator\"] = [\"localhost:%s\" % pick_unused_port()]\n+  if has_chief:\n+    cluster_dict[chief_name] = [\"localhost:%s\" % pick_unused_port()]\n+\n+  cs = tf.train.ClusterSpec(cluster_dict)\n+\n+  for i in range(num_workers):\n+    tf.distribute.Server(\n+        cs,\n+        job_name=worker_name,\n+        protocol=protocol,\n+        task_index=i,\n+        config=worker_config,\n+        start=True)\n+\n+  for i in range(num_ps):\n+    tf.distribute.Server(\n+        cs,\n+        job_name=ps_name,\n+        protocol=protocol,\n+        task_index=i,\n+        config=ps_config,\n+        start=True)\n+\n+  if has_chief:\n+    tf.distribute.Server(\n+        cs,\n+        job_name=chief_name,\n+        protocol=protocol,\n+        task_index=0,\n+        config=worker_config,\n+        start=True)\n+\n+  if has_eval:\n+    tf.distribute.Server(\n+        cs,\n+        job_name=\"evaluator\",\n+        protocol=protocol,\n+        task_index=0,\n+        config=eval_config,\n+        start=True)\n+\n+  return cluster_dict\n+\n+\n+def create_in_process_cluster(num_workers,\n+                              num_ps,\n+                              has_chief=False,\n+                              has_eval=False,\n+                              rpc_layer=\"grpc\"):\n+  \"\"\"Create an in-process cluster that consists of only standard server.\"\"\"\n+  # Leave some memory for cuda runtime.\n+  gpu_mem_frac = 0.7 / (num_workers + int(has_chief) + int(has_eval))\n+  worker_config = tf.compat.v1.ConfigProto()\n+  worker_config.gpu_options.per_process_gpu_memory_fraction = gpu_mem_frac\n+\n+  # The cluster may hang if workers don't have enough inter_op threads. See\n+  # b/172296720 for more details.\n+  if worker_config.inter_op_parallelism_threads < num_workers + 1:\n+    worker_config.inter_op_parallelism_threads = num_workers + 1\n+\n+  # Enable collective ops which has no impact on non-collective ops.\n+  if has_chief:\n+    worker_config.experimental.collective_group_leader = (\n+        \"/job:chief/replica:0/task:0\")\n+  else:\n+    worker_config.experimental.collective_group_leader = (\n+        \"/job:worker/replica:0/task:0\")\n+\n+  ps_config = tf.compat.v1.ConfigProto()\n+  ps_config.device_count[\"GPU\"] = 0\n+\n+  eval_config = tf.compat.v1.ConfigProto()\n+  eval_config.experimental.collective_group_leader = \"\"\n+\n+  # Create in-process servers. Once an in-process tensorflow server is created,\n+  # there is no way to terminate it. So we create one cluster per test process.\n+  # We could've started the server in another process, we could then kill that\n+  # process to terminate the server. The reasons why we don\"t want multiple\n+  # processes are\n+  # 1) it is more difficult to manage these processes;\n+  # 2) there is something global in CUDA such that if we initialize CUDA in the\n+  # parent process, the child process cannot initialize it again and thus cannot\n+  # use GPUs (https://stackoverflow.com/questions/22950047).\n+  cluster = None\n+  try:\n+    cluster = _create_cluster(\n+        num_workers,\n+        num_ps=num_ps,\n+        has_chief=has_chief,\n+        has_eval=has_eval,\n+        worker_config=worker_config,\n+        ps_config=ps_config,\n+        eval_config=eval_config,\n+        protocol=rpc_layer)\n+  except tf.errors.UnknownError as e:\n+    if \"Could not start gRPC server\" in e.message:\n+      raise unittest.SkipTest(\"Cannot start std servers.\")\n+    else:\n+      raise\n+  return cluster\n\n@@ -63,10 +63,10 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n   \"\"\"Text vectorization layer.\n \n   This layer has basic options for managing text in a Keras model. It\n-  transforms a batch of strings (one sample = one string) into either a list of\n-  token indices (one sample = 1D tensor of integer token indices) or a dense\n-  representation (one sample = 1D tensor of float values representing data about\n-  the sample's tokens).\n+  transforms a batch of strings (one example = one string) into either a list of\n+  token indices (one example = 1D tensor of integer token indices) or a dense\n+  representation (one example = 1D tensor of float values representing data \n+  about the example's tokens).\n \n   If desired, the user can call this layer's adapt() method on a dataset.\n   When this layer is adapted, it will analyze the dataset, determine the\n@@ -76,13 +76,13 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n   input than the maximum vocabulary size, the most frequent terms will be used\n   to create the vocabulary.\n \n-  The processing of each sample contains the following steps:\n+  The processing of each example contains the following steps:\n \n-    1. standardize each sample (usually lowercasing + punctuation stripping)\n-    2. split each sample into substrings (usually words)\n+    1. standardize each example (usually lowercasing + punctuation stripping)\n+    2. split each example into substrings (usually words)\n     3. recombine substrings into tokens (usually ngrams)\n     4. index tokens (associate a unique int value with each token)\n-    5. transform each sample using this index, either into a vector of ints or\n+    5. transform each example using this index, either into a vector of ints or\n        a dense float vector.\n \n   Some notes on passing Callables to customize splitting and normalization for\n\n@@ -811,7 +811,7 @@ class RNN(Layer):\n \n     if self.return_sequences:\n       output = backend.maybe_convert_to_ragged(\n-          is_ragged_input, outputs, row_lengths)\n+          is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)\n     else:\n       output = last_output\n \n\n@@ -1666,6 +1666,40 @@ class RNNTest(keras_parameterized.TestCase):\n         output_dense, lengths=row_lengths)\n     self.assertAllClose(output_ragged, output_dense)\n \n+    # Check if return sequences and go_backwards outputs are correct\n+    np.random.seed(100)\n+    returning_rnn_layer = layer(4, go_backwards=True, return_sequences=True)\n+\n+    x_ragged = keras.Input(shape=(None, 5), ragged=True)\n+    y_ragged = returning_rnn_layer(x_ragged)\n+    model = keras.models.Model(x_ragged, y_ragged)\n+    output_ragged = model.predict(ragged_data, steps=1)\n+    self.assertAllClose(output_ragged.ragged_rank, ragged_data.ragged_rank)\n+    self.assertAllClose(output_ragged.row_splits, ragged_data.row_splits)\n+\n+    x_dense = keras.Input(shape=(3, 5))\n+    masking = keras.layers.Masking()(x_dense)\n+    y_dense = returning_rnn_layer(masking)\n+    model_2 = keras.models.Model(x_dense, y_dense)\n+    dense_data = ragged_data.to_tensor()\n+    output_dense = model_2.predict(dense_data, steps=1)\n+\n+    # Note that the raw output for dense and ragged input when go_backward=True\n+    # will be different. Consider following input\n+    # [[a, b, 0], [c, 0, 0], [d, e, f]] where 0s are masked value.\n+    # The dense output will be [[0, b, a], [0, 0, c], [f, e, d]] since it will\n+    # process the whole sequence from the end.\n+    # While ragged output will be [[b, a], [c], [f, e, d]] since it just ignore\n+    # the 0s. And if we densify the ragged output, it will by default inserting\n+    # 0s to the end (rather than from the beginning), which make the output to\n+    # be [[b, a, 0], [c, 0, 0], [f, e, d]]. With this, we need to verify that\n+    # reverse(ragged_output.to_tensor()) == reverse(dense_output)\n+    output_dense = keras.backend.reverse(output_dense, [1])\n+    output_dense = tf.RaggedTensor.from_tensor(\n+        output_dense, lengths=row_lengths)\n+\n+    self.assertAllClose(keras.backend.reverse(output_ragged, [1]), output_dense)\n+\n   def test_stateless_rnn_cell(self):\n \n     class StatelessCell(keras.layers.Layer):\n\n@@ -449,7 +449,7 @@ class GRU(recurrent.DropoutRNNCellMixin, recurrent.GRU):\n \n     if self.return_sequences:\n       output = backend.maybe_convert_to_ragged(\n-          is_ragged_input, outputs, row_lengths)\n+          is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)\n     else:\n       output = last_output\n \n@@ -1262,7 +1262,7 @@ class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):\n \n     if self.return_sequences:\n       output = backend.maybe_convert_to_ragged(\n-          is_ragged_input, outputs, row_lengths)\n+          is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)\n     else:\n       output = last_output\n \n\n@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=unused-import\n # pylint: disable=g-classes-have-attributes\n # pylint: disable=g-doc-return-or-yield\n \"\"\"Built-in metrics.\"\"\"\n@@ -46,7 +45,6 @@ from keras.saving.saved_model import metric_serialization\n from keras.utils import generic_utils\n from keras.utils import losses_utils\n from keras.utils import metrics_utils\n-from keras.utils import tf_inspect\n from keras.utils.generic_utils import deserialize_keras_object\n from keras.utils.generic_utils import serialize_keras_object\n from keras.utils.generic_utils import to_list\n@@ -1553,13 +1551,11 @@ class SensitivitySpecificityBase(Metric, metaclass=abc.ABCMeta):\n \n     Returns maximal dependent value, if no value satiesfies the constraint 0.0.\n     \"\"\"\n-    feasible = tf.compat.v1.where(predicate(constrained, self.value))\n+    feasible = tf.where(predicate(constrained, self.value))\n     feasible_exists = tf.greater(tf.compat.v1.size(feasible), 0)\n+    max_dependent = tf.reduce_max(tf.compat.v1.gather(dependent, feasible))\n \n-    def get_max():\n-      return tf.reduce_max(tf.compat.v1.gather(dependent, feasible))\n-\n-    return tf.compat.v1.cond(feasible_exists, get_max, lambda: 0.0)\n+    return tf.where(feasible_exists, max_dependent, 0.0)\n \n \n @keras_export('keras.metrics.SensitivityAtSpecificity')\n\n@@ -21,7 +21,6 @@ import threading\n \n from absl.testing import parameterized\n import numpy as np\n-from tensorflow.python.distribute import test_util\n from keras.mixed_precision import autocast_variable\n from keras.optimizer_v2 import adadelta\n from keras.optimizer_v2 import adagrad\n@@ -42,11 +41,30 @@ def get_var(val, dtype, name=None):\n   return tf.compat.v1.Variable(val, use_resource=True, dtype=dtype, name=name)\n \n \n+def set_cpu_logical_devices_to_at_least(num):\n+  \"\"\"Create cpu logical devices of at least a given number.\"\"\"\n+  physical_devices = tf.config.list_physical_devices('CPU')\n+  if not physical_devices:\n+    raise RuntimeError('No CPU found')\n+  if len(physical_devices) >= num:\n+    return\n+  # By default each physical device corresponds to one logical device. We create\n+  # multiple logical devices for the last physical device so that we have `num`\n+  # logical devices.\n+  num = num - len(physical_devices) + 1\n+  logical_devices = []\n+  for _ in range(num):\n+    logical_devices.append(tf.config.LogicalDeviceConfiguration())\n+  # Create logical devices from the last device since sometimes the first GPU\n+  # is the primary graphic card and may have less memory available.\n+  tf.config.set_logical_device_configuration(physical_devices[-1], logical_devices)\n+\n+\n @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=['graph', 'eager']))\n class AutoCastVariableTest(tf.test.TestCase, parameterized.TestCase):\n \n   def setUp(self):\n-    test_util.set_logical_devices_to_at_least('CPU', 3)\n+    set_cpu_logical_devices_to_at_least(3)\n     super(AutoCastVariableTest, self).setUp()\n \n   @tf.__internal__.distribute.combinations.generate(maybe_distribute)\n\n@@ -25,14 +25,6 @@ from keras.mixed_precision import policy\n from keras.optimizer_v2 import gradient_descent as gradient_descent_v2\n \n \n-if tf.__internal__.tf2.enabled():\n-  enable_mixed_precision_graph_rewrite = (\n-      tf.train.experimental.enable_mixed_precision_graph_rewrite)\n-else:\n-  enable_mixed_precision_graph_rewrite = (\n-      tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite)\n-\n-\n class MixedPrecisionTest(keras_parameterized.TestCase):\n \n   IGNORE_PERF_VAR = 'TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_IGNORE_PERFORMANCE'\n@@ -51,13 +43,13 @@ class MixedPrecisionTest(keras_parameterized.TestCase):\n     else:\n       del os.environ[self.IGNORE_PERF_VAR]\n \n-    tf.train.experimental.disable_mixed_precision_graph_rewrite()\n+    tf.compat.v1.mixed_precision.disable_mixed_precision_graph_rewrite()\n     super(MixedPrecisionTest, self).tearDown()\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def test_wrap_optimizer(self):\n     opt = gradient_descent_v2.SGD(1.0)\n-    opt = enable_mixed_precision_graph_rewrite(opt, 123.)\n+    opt = tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(opt, 123.)\n     self.assertIsInstance(\n         opt, loss_scale_optimizer_v2.LossScaleOptimizerV1)\n     self.assertEqual(self.evaluate(opt.loss_scale), 123.)\n@@ -69,7 +61,7 @@ class MixedPrecisionTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(\n         ValueError, '\"opt\" must not already be an instance of a '\n         'LossScaleOptimizer.'):\n-      enable_mixed_precision_graph_rewrite(opt)\n+      tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(opt)\n     self.assertFalse(tf.config.optimizer.get_experimental_options()\n                      .get('auto_mixed_precision', False))\n \n@@ -78,12 +70,15 @@ class MixedPrecisionTest(keras_parameterized.TestCase):\n     with policy.policy_scope('mixed_float16'):\n       with self.assertRaisesRegex(ValueError,\n                                   'the global Keras dtype Policy has been set'):\n-        enable_mixed_precision_graph_rewrite(gradient_descent_v2.SGD(1.0))\n+        tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n+            gradient_descent_v2.SGD(1.0))\n     # Test no error is thrown when the policy is currently the default.\n-    enable_mixed_precision_graph_rewrite(gradient_descent_v2.SGD(1.0))\n+    tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n+        gradient_descent_v2.SGD(1.0))\n     # Test no error is thrown when the policy is a non-mixed policy.\n     with policy.policy_scope('float64'):\n-      enable_mixed_precision_graph_rewrite(gradient_descent_v2.SGD(1.0))\n+      tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n+          gradient_descent_v2.SGD(1.0))\n \n \n if __name__ == '__main__':\n\n@@ -448,7 +448,7 @@ def _check_if_mixed_precision_graph_rewrite_is_enabled(policy):\n         'The global dtype policy cannot be set to \"{policy.name}\", because the '\n         'mixed precision graph rewrite has already been enabled.\\n'\n         'At most, one of the following can be called:\\n\\n'\n-        '  1. tf.train.experimental.enable_mixed_precision_graph_rewrite() '\n+        '  1. tf.compat.v1.train.enable_mixed_precision_graph_rewrite() '\n         '(You called this first)\\n'\n         '  2. tf.keras.mixed_precision.experimental.set_policy() with a mixed '\n         'precision policy (You called this second)\\n\\n'\n\n@@ -295,7 +295,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n   @testing_utils.enable_v2_dtype_behavior\n   def test_error_if_graph_rewrite_enabled(self):\n     try:\n-      tf.train.experimental.enable_mixed_precision_graph_rewrite(\n+      tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n           gradient_descent.SGD(1.))\n       with self.assertRaisesRegex(\n           ValueError, 'cannot be set to \"mixed_float16\", .* the mixed '\n@@ -304,7 +304,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n       with mp_policy.policy_scope('float64'):\n         pass  # Non-mixed policies are allowed\n     finally:\n-      tf.train.experimental.disable_mixed_precision_graph_rewrite()\n+      tf.compat.v1.mixed_precision.disable_mixed_precision_graph_rewrite()\n \n   @testing_utils.disable_v2_dtype_behavior\n   def test_v1_dtype_behavior(self):\n\n@@ -411,9 +411,9 @@ class PolynomialDecay(LearningRateSchedule):\n       if self.cycle:\n         # Find the first multiple of decay_steps that is bigger than\n         # global_step. If global_step is zero set the multiplier to 1\n-        multiplier = tf.compat.v1.cond(\n-            tf.equal(global_step_recomp, 0), lambda: 1.0,\n-            lambda: tf.math.ceil(global_step_recomp / self.decay_steps))\n+        multiplier = tf.where(\n+            tf.equal(global_step_recomp, 0), 1.0,\n+            tf.math.ceil(global_step_recomp / self.decay_steps))\n         decay_steps_recomp = tf.multiply(decay_steps_recomp, multiplier)\n       else:\n         # Make sure that the global_step used is not bigger than decay_steps.\n\n@@ -30,8 +30,6 @@ from keras.utils import generic_utils\n from keras.utils import layer_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n-from tensorflow.python.saved_model import revived_types\n-from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -657,8 +655,16 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         grads_and_vars = self._aggregate_gradients(grads_and_vars)\n       grads_and_vars = self._transform_gradients(grads_and_vars)\n \n+      if optimizer_utils.strategy_supports_no_merge_call():\n         return self._distributed_apply(strategy, grads_and_vars, name,\n                                        apply_state)\n+      else:\n+        return tf.distribute.get_replica_context().merge_call(\n+            functools.partial(self._distributed_apply, apply_state=apply_state),\n+            args=(grads_and_vars,),\n+            kwargs={\n+                \"name\": name,\n+            })\n \n   def _distributed_apply(self, distribution, grads_and_vars, name, apply_state):\n     \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n@@ -697,8 +703,16 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n           with name_scope_only_in_function_or_graph(\n               \"update\" if eagerly_outside_functions else \"update_\" +\n               var.op.name):\n-            update_ops.append(tf.distribute.get_replica_context()._update(  # pylint: disable=protected-access\n-                var, apply_grad_to_update_var, args=(grad,), group=False))\n+            update_op = distribution.extended.update(\n+                var, apply_grad_to_update_var, args=(grad,), group=False)\n+            if tf.distribute.in_cross_replica_context():\n+              # In cross-replica context, extended.update returns a list of\n+              # update ops from all replicas (group=False).\n+              update_ops.extend(update_op)\n+            else:\n+              # In replica context, extended.update return the single update op\n+              # of current replica.\n+              update_ops.append(update_op)\n \n       any_symbolic = any(isinstance(i, tf.Operation) or\n                          tf_utils.is_symbolic_tensor(i) for i in update_ops)\n@@ -863,7 +877,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         initializer = initializers.get(initializer)\n         if isinstance(\n             initializer,\n-            trackable.CheckpointInitialValueCallable) or (shape is not None):\n+            tf.__internal__.tracking.CheckpointInitialValueCallable) or (shape is not None):\n           slot_shape = shape\n         else:\n           slot_shape = var.shape\n@@ -1355,7 +1369,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         # variables.\n         and (not tf.compat.v1.get_default_graph()._variable_creator_stack or  # pylint: disable=protected-access\n              self._distribution_strategy)):\n-      initializer = trackable.CheckpointInitialValueCallable(\n+      initializer = tf.__internal__.tracking.CheckpointInitialValueCallable(\n           checkpoint_position=slot_variable_position)\n       slot_variable = self.add_slot(\n           var=variable,\n@@ -1444,10 +1458,10 @@ class RestoredOptimizer(OptimizerV2):\n         \"supported. Please file a feature request if this limitation bothers \"\n         \"you.\")\n \n-revived_types.register_revived_type(\n+tf.__internal__.saved_model.load.register_revived_type(\n     \"oss_optimizer\",  # TODO(scottzhu): Change this back after repo split.\n     lambda obj: isinstance(obj, OptimizerV2),\n-    versions=[revived_types.VersionedTypeRegistration(\n+    versions=[tf.__internal__.saved_model.load.VersionedTypeRegistration(\n         object_factory=lambda proto: RestoredOptimizer(),\n         version=1,\n         min_producer_version=1,\n\n@@ -62,7 +62,7 @@ class TimeseriesGenerator(sequence.TimeseriesGenerator, data_utils.Sequence):\n       batch_size: Number of timeseries samples in each batch\n           (except maybe the last one).\n   # Returns\n-      A [Sequence](/utils/#sequence) instance.\n+      A [Sequence](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence) instance.\n   # Examples\n   ```python\n   from keras.preprocessing.sequence import TimeseriesGenerator\n\n@@ -41,7 +41,6 @@ from keras.layers import core\n from keras.saving import model_config\n from keras.saving import save\n from keras.utils import generic_utils\n-from tensorflow.python.saved_model import loader_impl\n \n \n if sys.version_info >= (3, 6):\n@@ -66,7 +65,7 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n                       .format(path))\n \n   def assert_saved_model(self, path):\n-    loader_impl.parse_saved_model(path)\n+    tf.__internal__.saved_model.parse_saved_model(path)\n \n   @testing_utils.run_v2_only\n   def test_save_format_defaults(self):\n\n@@ -35,8 +35,6 @@ from keras.utils import generic_utils\n from keras.utils import metrics_utils\n from keras.utils.generic_utils import LazyLoader\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.saved_model import loader_impl\n-from tensorflow.python.saved_model import revived_types\n \n # To avoid circular dependencies between keras/engine and keras/saving,\n # code in keras/saving must delay imports.\n@@ -106,7 +104,7 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n \n   # Look for metadata file or parse the SavedModel\n   metadata = saved_metadata_pb2.SavedMetadata()\n-  meta_graph_def = loader_impl.parse_saved_model(path).meta_graphs[0]\n+  meta_graph_def = tf.__internal__.saved_model.parse_saved_model(path).meta_graphs[0]\n   object_graph_def = meta_graph_def.object_graph_def\n   path_to_metadata_pb = os.path.join(path, constants.SAVED_METADATA_PATH)\n   if tf.compat.v1.gfile.Exists(path_to_metadata_pb):\n@@ -320,8 +318,8 @@ class KerasObjectLoader(object):\n       if not isinstance(obj_child, tf.__internal__.tracking.Trackable):\n         continue\n       if (child_proto.user_object.identifier in\n-          revived_types.registered_identifiers()):\n-        setter = revived_types.get_setter(child_proto.user_object)\n+          tf.__internal__.saved_model.load.registered_identifiers()):\n+        setter = tf.__internal__.saved_model.load.get_setter(child_proto.user_object)\n       elif obj_child._object_identifier in constants.KERAS_OBJECT_IDENTIFIERS:\n         setter = _revive_setter\n       else:\n\n@@ -27,7 +27,6 @@ from keras.saving import utils_v1 as model_utils\n from keras.utils import mode_keys\n from keras.utils.generic_utils import LazyLoader\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.saved_model import builder as saved_model_builder\n from tensorflow.python.util.tf_export import keras_export\n \n # To avoid circular dependencies between keras/engine and keras/saving,\n@@ -172,7 +171,7 @@ def _save_v1_format(model, path, custom_objects, as_text, input_signature):\n           'Subclassed models can only be exported for serving. Please set '\n           'argument serving_only=True.')\n \n-  builder = saved_model_builder._SavedModelBuilder(path)  # pylint: disable=protected-access\n+  builder = tf.__internal__.saved_model.SavedModelBuilder(path)  # pylint: disable=protected-access\n \n   # Manually save variables to export them in an object-based checkpoint. This\n   # skips the `builder.add_meta_graph_and_variables()` step, which saves a\n\n@@ -17,9 +17,9 @@\n import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n-from tensorflow.python.distribute import multi_worker_test_base\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n from keras import combinations\n+from keras.distribute import multi_worker_testing_utils\n from keras.engine import data_adapter\n from keras.engine import sequential\n from keras.layers import core as core_layers\n@@ -75,7 +75,7 @@ class DatasetCreatorTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertLen(history.history[\"loss\"], 10)\n \n   def _get_parameter_server_strategy(self):\n-    cluster_def = multi_worker_test_base.create_in_process_cluster(\n+    cluster_def = multi_worker_testing_utils.create_in_process_cluster(\n         num_workers=2, num_ps=1, rpc_layer=\"grpc\")\n     return tf.distribute.experimental.ParameterServerStrategy(\n         SimpleClusterResolver(ClusterSpec(cluster_def), rpc_layer=\"grpc\"))\n\n@@ -365,9 +365,8 @@ def update_confusion_matrix_variables(variables_to_update,\n     num_labels = 1\n   else:\n     num_labels = tf.raw_ops.Prod(input=pred_shape[1:], axis=0)\n-  thresh_label_tile = tf.compat.v1.cond(\n-      one_thresh, lambda: num_labels,\n-      lambda: tf.cast(1, dtype=tf.int32))\n+  thresh_label_tile = tf.where(one_thresh, num_labels,\n+                                         tf.ones([], dtype=tf.int32))\n \n   # Reshape predictions and labels, adding a dim for thresholding.\n   if multi_label:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dad71ab422adaccc93d4b239cd15feaa87ec4073", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 139 | Lines Deleted: 166 | Files Changed: 5 | Hunks: 29 | Methods Changed: 29 | Complexity Δ (Sum/Max): -8/2 | Churn Δ: 305 | Churn Cumulative: 16010 | Contributors (this commit): 90 | Commits (past 90d): 52 | Contributors (cumulative): 99 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3802,7 +3802,7 @@ set_value.__doc__ = set_value.__doc__.format(snippet=_VALUE_SET_CODE_STRING)\n @keras_export('keras.backend.print_tensor')\n @tf.__internal__.dispatch.add_dispatch_support\n @doc_controls.do_not_generate_docs\n-def print_tensor(x, message=''):\n+def print_tensor(x, message='', summarize=3):\n   \"\"\"Prints `message` and the tensor value when evaluated.\n \n   Note that `print_tensor` returns a new tensor identical to `x`\n@@ -3820,17 +3820,23 @@ def print_tensor(x, message=''):\n   Args:\n       x: Tensor to print.\n       message: Message to print jointly with the tensor.\n+      summarize: The first and last `summarize` elements within each dimension\n+          are recursively printed per Tensor. If None, then the first 3 and last\n+          3 elements of each dimension are printed for each tensor. If set to\n+          -1, it will print all elements of every tensor.\n \n   Returns:\n       The same tensor `x`, unchanged.\n   \"\"\"\n   if isinstance(x, tf.Tensor) and hasattr(x, 'graph'):\n     with get_graph().as_default():\n-      op = tf.print(message, x, output_stream=sys.stdout)\n+      op = tf.print(\n+          message, x, output_stream=sys.stdout, summarize=summarize)\n       with tf.control_dependencies([op]):\n         return tf.identity(x)\n   else:\n-    tf.print(message, x, output_stream=sys.stdout)\n+    tf.print(\n+        message, x, output_stream=sys.stdout, summarize=summarize)\n     return x\n \n # GRAPH MANIPULATION\n\n@@ -21,7 +21,6 @@ import tensorflow.compat.v2 as tf\n import collections\n import copy\n import csv\n-import io\n import json\n import os\n import re\n@@ -2718,21 +2717,17 @@ class CSVLogger(Callback):\n     self.writer = None\n     self.keys = None\n     self.append_header = True\n-    self.file_flags = ''\n-    self._open_args = {'newline': '\\n'}\n     super(CSVLogger, self).__init__()\n \n   def on_train_begin(self, logs=None):\n     if self.append:\n       if tf.io.gfile.exists(self.filename):\n-        with open(self.filename, 'r' + self.file_flags) as f:\n+        with tf.io.gfile.GFile(self.filename, 'r') as f:\n           self.append_header = not bool(len(f.readline()))\n       mode = 'a'\n     else:\n       mode = 'w'\n-    self.csv_file = io.open(self.filename,\n-                            mode + self.file_flags,\n-                            **self._open_args)\n+    self.csv_file = tf.io.gfile.GFile(self.filename, mode)\n \n   def on_epoch_end(self, epoch, logs=None):\n     logs = logs or {}\n\n@@ -17,20 +17,14 @@\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-classes-have-attributes\n \n-import collections\n-import json\n-\n import numpy as np\n from keras.engine import base_preprocessing_layer\n-from keras.engine.base_preprocessing_layer import Combiner\n from keras.utils import tf_utils\n+from tensorflow.python.ops.parallel_for.control_flow_ops import vectorized_map\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n-_BINS_NAME = \"bins\"\n-\n-\n def summarize(values, epsilon):\n   \"\"\"Reduce a 1D sequence of values to a summary.\n \n@@ -51,18 +45,17 @@ def summarize(values, epsilon):\n       interpolated partition values, the second is the weights (counts).\n   \"\"\"\n \n-  num_bins = 1.0 / epsilon\n-  value_shape = values.shape\n-  n = np.prod([[(1 if dim is None else dim) for dim in value_shape]])\n-  if num_bins >= n:\n-    return np.hstack((np.expand_dims(np.sort(values), 1), np.ones((n, 1))))\n-  step_size = int(n / num_bins)\n-  partition_indices = np.arange(step_size, n, step_size, np.int64)\n-\n-  part = np.partition(values, partition_indices)[partition_indices]\n-\n-  return np.hstack((np.expand_dims(part, 1),\n-                    step_size * np.ones((np.prod(part.shape), 1))))\n+  values = tf.reshape(values, [-1])\n+  values = tf.sort(values)\n+  elements = tf.cast(tf.compat.v1.size(values), tf.float32)\n+  num_buckets = 1. / epsilon\n+  increment = tf.cast(elements / num_buckets, tf.int32)\n+  start = increment\n+  step = tf.maximum(increment, 1)\n+  boundaries = values[start::step]\n+  weights = tf.compat.v1.ones_like(boundaries)\n+  weights = weights * tf.cast(step, tf.float32)\n+  return tf.stack([boundaries, weights])\n \n \n def compress(summary, epsilon):\n@@ -82,18 +75,24 @@ def compress(summary, epsilon):\n       A 2-D `np.ndarray` that is a compressed summary. First column is the\n       interpolated partition values, the second is the weights (counts).\n   \"\"\"\n-  if np.prod(summary[:, 0].shape) * epsilon < 1:\n+  # TODO(b/184863356): remove the numpy escape hatch here.\n+  return tf.numpy_function(\n+      lambda s: _compress_summary_numpy(s, epsilon), [summary], tf.float32)\n+\n+\n+def _compress_summary_numpy(summary, epsilon):\n+  \"\"\"Compress a summary with numpy.\"\"\"\n+  if summary.shape[1] * epsilon < 1:\n     return summary\n \n   percents = epsilon + np.arange(0.0, 1.0, epsilon)\n-  cum_weights = summary[:, 1].cumsum()\n+  cum_weights = summary[1].cumsum()\n   cum_weight_percents = cum_weights / cum_weights[-1]\n-  new_bins = np.interp(percents, cum_weight_percents, summary[:, 0])\n+  new_bins = np.interp(percents, cum_weight_percents, summary[0])\n   cum_weights = np.interp(percents, cum_weight_percents, cum_weights)\n   new_weights = cum_weights - np.concatenate((np.array([0]), cum_weights[:-1]))\n-\n-  return np.hstack((np.expand_dims(new_bins, 1),\n-                    np.expand_dims(new_weights, 1)))\n+  summary = np.stack((new_bins, new_weights))\n+  return summary.astype(np.float32)\n \n \n def merge_summaries(prev_summary, next_summary, epsilon):\n@@ -105,25 +104,23 @@ def merge_summaries(prev_summary, next_summary, epsilon):\n   Args:\n       prev_summary: 2-D `np.ndarray` summary to be merged with `next_summary`.\n       next_summary: 2-D `np.ndarray` summary to be merged with `prev_summary`.\n-      epsilon: A `'float32'` that determines the approxmiate desired precision.\n+      epsilon: A float that determines the approxmiate desired precision.\n \n   Returns:\n       A 2-D `np.ndarray` that is a merged summary. First column is the\n       interpolated partition values, the second is the weights (counts).\n   \"\"\"\n-  merged = np.concatenate((prev_summary, next_summary))\n-  merged = merged[merged[:, 0].argsort()]\n-  if np.prod(merged.shape) * epsilon < 1:\n-    return merged\n+  merged = tf.concat((prev_summary, next_summary), axis=1)\n+  merged = tf.gather(merged, tf.argsort(merged[0]), axis=1)\n   return compress(merged, epsilon)\n \n \n-def get_bucket_boundaries(summary, num_bins):\n-  return compress(summary, 1.0 / num_bins)[:-1, 0]\n+def get_bin_boundaries(summary, num_bins):\n+  return compress(summary, 1.0 / num_bins)[0, :-1]\n \n \n @keras_export(\"keras.layers.experimental.preprocessing.Discretization\")\n-class Discretization(base_preprocessing_layer.CombinerPreprocessingLayer):\n+class Discretization(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"Buckets data into discrete ranges.\n \n   This layer will place each element of its input data into one of several\n@@ -185,12 +182,9 @@ class Discretization(base_preprocessing_layer.CombinerPreprocessingLayer):\n       elif bin_boundaries is None:\n         bin_boundaries = kwargs[\"bins\"]\n       del kwargs[\"bins\"]\n-    super(Discretization, self).__init__(\n-        combiner=Discretization.DiscretizingCombiner(\n-            epsilon, num_bins if num_bins is not None else 1),\n-        **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        \"Discretization\").set(True)\n+    super().__init__(**kwargs)\n+    base_preprocessing_layer.keras_kpl_gauge.get_cell(\"Discretization\").set(\n+        True)\n     if num_bins is not None and num_bins < 0:\n       raise ValueError(\"`num_bins` must be must be greater than or equal to 0. \"\n                        \"You passed `num_bins={}`\".format(num_bins))\n@@ -201,18 +195,71 @@ class Discretization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     self.bin_boundaries = bin_boundaries\n     self.num_bins = num_bins\n     self.epsilon = epsilon\n+    # Bins only need to be `adapt`'d when `num_bins` is passed.\n+    self.stateful = self.num_bins is not None\n \n   def build(self, input_shape):\n     if self.bin_boundaries is not None:\n       initial_bins = np.append(self.bin_boundaries, [np.Inf])\n     else:\n       initial_bins = np.zeros(self.num_bins)\n-    self.bins = self._add_state_variable(\n-        name=_BINS_NAME,\n+    self.bins = self.add_weight(\n+        name=\"bins\",\n         shape=(initial_bins.size,),\n         dtype=tf.float32,\n-        initializer=tf.compat.v1.constant_initializer(initial_bins))\n-    super(Discretization, self).build(input_shape)\n+        initializer=tf.compat.v1.constant_initializer(initial_bins),\n+        trainable=False)\n+\n+    if self.stateful:\n+      # Summary contains two equal length vectors of bins at index 0 and weights\n+      # at index 1.\n+      self.summary = self.add_weight(\n+          name=\"summary\",\n+          shape=(2, None),\n+          dtype=tf.float32,\n+          initializer=lambda shape, dtype: [[], []],  # pylint: disable=unused-arguments\n+          trainable=False)\n+\n+    self.built = True\n+\n+  def update_state(self, data):\n+    if not self.stateful:\n+      return\n+\n+    if not self.built:\n+      raise RuntimeError(\"`build` must be called before `update_state`.\")\n+\n+    data = tf.convert_to_tensor(data)\n+    if data.dtype != tf.float32:\n+      data = tf.cast(data, tf.float32)\n+    summary = summarize(data, self.epsilon)\n+    self.summary.assign(merge_summaries(summary, self.summary, self.epsilon))\n+\n+  def merge_state(self, layers):\n+    for l in layers + [self]:\n+      if not l.stateful:\n+        raise ValueError(\n+            \"Cannot merge non-stateful Discretization layer {}. All layers to \"\n+            \"be adapted and merged should be initialized with `num_bins`.\"\n+            .format(l.name))\n+      if not l.built:\n+        raise ValueError(\n+            \"Cannot merge unbuilt Discretization layer {}. You need to call \"\n+            \"`adapt` on this layer before merging.\".format(l.name))\n+\n+    summary = self.summary\n+    for l in layers:\n+      summary = merge_summaries(summary, l.summary, self.epsilon)\n+    self.summary.assign(summary)\n+    self.finalize_state()\n+\n+  def finalize_state(self):\n+    if not self.stateful:\n+      return\n+\n+    boundaries = get_bin_boundaries(self.summary, self.num_bins)\n+    boundaries = tf.concat([boundaries, [np.inf]], axis=0)\n+    self.bins.assign(boundaries)\n \n   def get_config(self):\n     config = {\n@@ -266,84 +313,4 @@ class Discretization(base_preprocessing_layer.CombinerPreprocessingLayer):\n       # inputs after batch size and vectorized_map over each sample.\n       reshaped = tf.reshape(inputs, [dynamic_shape[0], -1])\n       return tf.reshape(\n-          tf.vectorized_map(_bucketize_fn, reshaped),\n-          dynamic_shape)\n-\n-  class DiscretizingCombiner(Combiner):\n-    \"\"\"Combiner for the Discretization preprocessing layer.\n-\n-    This class encapsulates the computations for finding the quantile boundaries\n-    of a set of data in a stable and numerically correct way. Its associated\n-    accumulator is a namedtuple('summaries'), representing summarizations of\n-    the data used to generate boundaries.\n-\n-    Attributes:\n-      epsilon: Error tolerance.\n-      num_bins: The desired number of buckets.\n-    \"\"\"\n-\n-    def __init__(self, epsilon, num_bins,):\n-      self.epsilon = epsilon\n-      self.num_bins = num_bins\n-\n-      # TODO(mwunder): Implement elementwise per-column discretization.\n-\n-    def compute(self, values, accumulator=None):\n-      \"\"\"Compute a step in this computation, returning a new accumulator.\"\"\"\n-\n-      if isinstance(values, tf.SparseTensor):\n-        values = values.values\n-      if tf_utils.is_ragged(values):\n-        values = values.flat_values\n-      flattened_input = np.reshape(values, newshape=(-1, 1))\n-\n-      summaries = [summarize(v, self.epsilon) for v in flattened_input.T]\n-\n-      if accumulator is None:\n-        return self._create_accumulator(summaries)\n-      else:\n-        return self._create_accumulator(\n-            [merge_summaries(prev_summ, summ, self.epsilon)\n-             for prev_summ, summ in zip(accumulator.summaries, summaries)])\n-\n-    def merge(self, accumulators):\n-      \"\"\"Merge several accumulators to a single accumulator.\"\"\"\n-      # Combine accumulators and return the result.\n-\n-      merged = accumulators[0].summaries\n-      for accumulator in accumulators[1:]:\n-        merged = [merge_summaries(prev, summary, self.epsilon)\n-                  for prev, summary in zip(merged, accumulator.summaries)]\n-\n-      return self._create_accumulator(merged)\n-\n-    def extract(self, accumulator):\n-      \"\"\"Convert an accumulator into a dict of output values.\"\"\"\n-\n-      boundaries = [np.append(get_bucket_boundaries(summary, self.num_bins),\n-                              [np.Inf])\n-                    for summary in accumulator.summaries]\n-      return {\n-          _BINS_NAME: np.squeeze(np.vstack(boundaries))\n-      }\n-\n-    def restore(self, output):\n-      \"\"\"Create an accumulator based on 'output'.\"\"\"\n-      raise NotImplementedError(\n-          \"Discretization does not restore or support streaming updates.\")\n-\n-    def serialize(self, accumulator):\n-      \"\"\"Serialize an accumulator for a remote call.\"\"\"\n-      output_dict = {\n-          _BINS_NAME: [summary.tolist() for summary in accumulator.summaries]\n-      }\n-      return tf.compat.as_bytes(json.dumps(output_dict))\n-\n-    def deserialize(self, encoded_accumulator):\n-      \"\"\"Deserialize an accumulator received from 'serialize()'.\"\"\"\n-      value_dict = json.loads(tf.compat.as_text(encoded_accumulator))\n-      return self._create_accumulator(np.array(value_dict[_BINS_NAME]))\n-\n-    def _create_accumulator(self, summaries):\n-      \"\"\"Represent the accumulator as one or more summaries of the dataset.\"\"\"\n-      return collections.namedtuple(\"Accumulator\", [\"summaries\"])(summaries)\n+          vectorized_map(_bucketize_fn, reshaped), dynamic_shape)\n\n@@ -136,35 +136,6 @@ class DiscretizationTest(keras_parameterized.TestCase,\n         r\"`num_bins` and `bin_boundaries` should not be set.*5.*\\[1, 2\\]\"):\n       _ = discretization.Discretization(num_bins=5, bins=[1, 2])\n \n-  @parameterized.named_parameters(\n-      {\n-          \"num_bins\": 5,\n-          \"data\": np.array([[1.], [2.], [3.], [4.], [5.]]),\n-          \"expected\": {\n-              \"bins\": np.array([1., 2., 3., 4., np.Inf])\n-          },\n-          \"testcase_name\": \"2d_single_element_all_bins\"\n-      }, {\n-          \"num_bins\": 5,\n-          \"data\": np.array([[1., 6.], [2., 7.], [3., 8.], [4., 9.], [5., 10.]]),\n-          \"expected\": {\n-              \"bins\": np.array([2., 4., 6., 8., np.Inf])\n-          },\n-          \"testcase_name\": \"2d_multi_element_all_bins\",\n-      }, {\n-          \"num_bins\": 3,\n-          \"data\": np.array([[0.], [1.], [2.], [3.], [4.], [5.]]),\n-          \"expected\": {\n-              \"bins\": np.array([1., 3., np.Inf])\n-          },\n-          \"testcase_name\": \"2d_single_element_3_bins\"\n-      })\n-  def test_combiner_computation(self, num_bins, data, expected):\n-    epsilon = 0.01\n-    combiner = discretization.Discretization.DiscretizingCombiner(\n-        epsilon, num_bins)\n-    self.validate_accumulator_extract(combiner, data, expected)\n-\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class DiscretizationAdaptTest(keras_parameterized.TestCase,\n@@ -245,6 +216,43 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n     output_data = model.predict(test_data)\n     self.assertAllClose(expected, output_data)\n \n+  def test_merge_state(self):\n+    data = np.arange(300)\n+    partial_ds_1 = tf.data.Dataset.from_tensor_slices(data[:100])\n+    partial_ds_2 = tf.data.Dataset.from_tensor_slices(data[100:200])\n+    partial_ds_3 = tf.data.Dataset.from_tensor_slices(data[200:])\n+    full_ds = partial_ds_1.concatenate(partial_ds_2).concatenate(partial_ds_3)\n+\n+    # Use a higher epsilon to avoid any discrepencies from the quantile\n+    # approximation.\n+    full_layer = discretization.Discretization(num_bins=3, epsilon=0.001)\n+    full_layer.adapt(full_ds.batch(2))\n+\n+    partial_layer_1 = discretization.Discretization(num_bins=3, epsilon=0.001)\n+    partial_layer_1.adapt(partial_ds_1.batch(2))\n+    partial_layer_2 = discretization.Discretization(num_bins=3, epsilon=0.001)\n+    partial_layer_2.adapt(partial_ds_2.batch(2))\n+    partial_layer_3 = discretization.Discretization(num_bins=3, epsilon=0.001)\n+    partial_layer_3.adapt(partial_ds_3.batch(2))\n+    partial_layer_1.merge_state([partial_layer_2, partial_layer_3])\n+    merged_layer = partial_layer_1\n+\n+    data = np.arange(300)\n+    self.assertAllClose(full_layer(data), merged_layer(data))\n+\n+  def test_merge_with_stateless_layers_fails(self):\n+    layer1 = discretization.Discretization(num_bins=2, name=\"layer1\")\n+    layer1.adapt([1, 2, 3])\n+    layer2 = discretization.Discretization(bin_boundaries=[0, 1], name=\"layer2\")\n+    with self.assertRaisesRegex(ValueError, \"Cannot merge.*layer2\"):\n+      layer1.merge_state([layer2])\n+\n+  def test_merge_with_unadapted_layers_fails(self):\n+    layer1 = discretization.Discretization(num_bins=2, name=\"layer1\")\n+    layer1.adapt([1, 2, 3])\n+    layer2 = discretization.Discretization(num_bins=2, name=\"layer2\")\n+    with self.assertRaisesRegex(ValueError, \"Cannot merge.*layer2\"):\n+      layer1.merge_state([layer2])\n \n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -260,9 +260,6 @@ class NormalizationAdaptTest(keras_parameterized.TestCase,\n     model.summary()\n \n   def test_merge_state(self):\n-    if not tf.executing_eagerly():\n-      self.skipTest(\"`merge_state` only supported in TF2\")\n-\n     data = np.random.rand(30, 10, 2)\n     ds = tf.data.Dataset.from_tensor_slices(data).batch(2)\n     norm = normalization.Normalization(axis=(1, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
