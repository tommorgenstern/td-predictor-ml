{"custom_id": "keras#7924aff56665c09bbd66330ec79de0d29154cafa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 85 | Lines Deleted: 20 | Files Changed: 9 | Hunks: 29 | Methods Changed: 18 | Complexity Δ (Sum/Max): 11/4 | Churn Δ: 105 | Churn Cumulative: 3223 | Contributors (this commit): 41 | Commits (past 90d): 46 | Contributors (cumulative): 71 | DMM Complexity: 0.08695652173913043\n\nDIFF:\n@@ -1,7 +1,9 @@\n import types\n \n+import h5py\n import jax\n import jax.numpy as jnp\n+import ml_dtypes\n import numpy as np\n import tree\n from jax.tree_util import Partial\n@@ -61,10 +63,20 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n         if dtype and dtype != x.dtype:\n             return x.value.astype(dtype)\n         return x.value\n+    if isinstance(x, h5py.Dataset):\n+        # h5py will handle bfloat16 as an opaque dtype.\n+        # We assume any two byte void dtypes are in fact bfloat16 type.\n+        if x.dtype == np.dtype((np.void, 2)):\n+            x = np.array(x, dtype=ml_dtypes.bfloat16)\n+        # h5py Datasets do not support converting on the fly for many dtypes.\n+        # Instead we convert \"as is\" and cast.\n+        return jnp.asarray(x).astype(dtype)\n     return jnp.asarray(x, dtype=dtype)\n \n \n def convert_to_numpy(x):\n+    if is_tensor(x) and x.dtype == \"bfloat16\":\n+        return np.asarray(x, ml_dtypes.bfloat16)\n     return np.asarray(x)\n \n \n\n@@ -1,3 +1,5 @@\n+import h5py\n+import ml_dtypes\n import numpy as np\n import tree\n \n@@ -39,6 +41,14 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n         dtype = result_type(\n             *[getattr(item, \"dtype\", type(item)) for item in tree.flatten(x)]\n         )\n+    if isinstance(x, h5py.Dataset):\n+        # h5py will handle bfloat16 as an opaque dtype.\n+        # We assume any two byte void dtypes are in fact bfloat16 type.\n+        if x.dtype == np.dtype((np.void, 2)):\n+            x = np.array(x, dtype=ml_dtypes.bfloat16)\n+        # h5py Datasets do not support converting on the fly for many dtypes.\n+        # Instead we convert \"as is\" and cast.\n+        return np.array(x).astype(dtype)\n     return np.array(x, dtype=dtype)\n \n \n\n@@ -1,5 +1,7 @@\n import types\n \n+import h5py\n+import ml_dtypes\n import numpy as np\n import tensorflow as tf\n from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice\n@@ -107,6 +109,10 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n     if dtype is not None:\n         dtype = standardize_dtype(dtype)\n     if not tf.is_tensor(x):\n+        # h5py will handle bfloat16 as an opaque dtype.\n+        # We assume any two byte void dtypes are in fact bfloat16 type.\n+        if isinstance(x, h5py.Dataset) and x.dtype == np.dtype((np.void, 2)):\n+            x = np.array(x, dtype=ml_dtypes.bfloat16)\n         if dtype == \"bool\":\n             # TensorFlow boolean conversion is stricter than other backends.\n             # It does not allow ints. We convert without dtype and cast instead.\n\n@@ -1,6 +1,8 @@\n import contextlib\n import os\n \n+import h5py\n+import ml_dtypes\n import numpy as np\n import torch\n import tree\n@@ -179,6 +181,10 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n                 x, dtype=to_torch_dtype(floatx()), device=get_device()\n             )\n \n+    # h5py will handle bfloat16 as an opaque dtype.\n+    # We assume any two byte void dtypes are in fact bfloat16 type.\n+    if isinstance(x, h5py.Dataset) and x.dtype == np.dtype((np.void, 2)):\n+        x = np.array(x, dtype=ml_dtypes.bfloat16)\n     # Convert to np in case of any array-like that is not list or tuple.\n     if not isinstance(x, (list, tuple)):\n         x = np.array(x)\n@@ -210,6 +216,12 @@ def convert_to_numpy(x):\n             # Tensor has to be moved to CPU before converting to numpy.\n             if x.is_cuda or x.is_mps:\n                 x = x.cpu()\n+            if x.dtype == torch.bfloat16:\n+                # Attempting to call .numpy() on a bfloat16 torch tensor leads\n+                # to an immediate error. Instead we upcast to float32 and then\n+                # convert to the numpy friendly bfloat16 type.\n+                # https://github.com/pytorch/pytorch/issues/90574\n+                return np.array(x.to(torch.float32)).astype(ml_dtypes.bfloat16)\n         return np.array(x)\n \n     if isinstance(x, (list, tuple)):\n\n@@ -188,7 +188,7 @@ class Dense(Layer):\n         kernel_value = ops.convert_to_numpy(self.kernel)\n         store[\"0\"] = kernel_value\n         if self.use_bias:\n-            store[\"1\"] = self.bias.numpy()\n+            store[\"1\"] = ops.convert_to_numpy(self.bias)\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n\n@@ -265,7 +265,7 @@ class EinsumDense(Layer):\n         kernel_value = ops.convert_to_numpy(self.kernel)\n         store[\"0\"] = kernel_value\n         if self.bias is not None:\n-            store[\"1\"] = self.bias.numpy()\n+            store[\"1\"] = ops.convert_to_numpy(self.bias)\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n\n@@ -26,6 +26,7 @@ from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import mixed_precision\n+from keras import ops\n from keras import regularizers\n from keras import utils\n from keras.api_export import keras_export\n@@ -715,7 +716,7 @@ class Layer(BackendLayer, Operation):\n                     x.dtype = self.input_dtype\n                 return x\n             elif hasattr(x, \"__array__\"):\n-                return backend.convert_to_tensor(x, dtype=self.input_dtype)\n+                return ops.convert_to_tensor(x, dtype=self.input_dtype)\n             return x\n \n         # Used to avoid expensive `tree` operations in the most common case.\n@@ -1119,7 +1120,7 @@ class Layer(BackendLayer, Operation):\n         \"\"\"\n         all_vars = self._trainable_variables + self._non_trainable_variables\n         for i, v in enumerate(all_vars):\n-            store[f\"{i}\"] = v.numpy()\n+            store[f\"{i}\"] = ops.convert_to_numpy(v)\n \n     def load_own_variables(self, store):\n         \"\"\"Loads the state of the layer.\n\n@@ -3,11 +3,13 @@ import unittest.mock as mock\n \n import numpy as np\n from absl import logging\n+from absl.testing import parameterized\n \n from keras import layers\n from keras.models import Sequential\n from keras.saving import saving_api\n from keras.testing import test_case\n+from keras.testing.test_utils import named_product\n \n \n class SaveModelTests(test_case.TestCase):\n@@ -63,23 +65,32 @@ class SaveModelTests(test_case.TestCase):\n             saving_api.save_model(model, \"model.png\")\n \n \n-class LoadModelTests(test_case.TestCase):\n-    def get_model(self):\n+class LoadModelTests(test_case.TestCase, parameterized.TestCase):\n+    def get_model(self, dtype=None):\n         return Sequential(\n             [\n-                layers.Dense(5, input_shape=(3,)),\n+                layers.Dense(5, input_shape=(3,), dtype=dtype),\n                 layers.Softmax(),\n             ]\n         )\n \n-    def test_basic_load(self):\n+    @parameterized.named_parameters(\n+        [\n+            {\"testcase_name\": \"bfloat16\", \"dtype\": \"bfloat16\"},\n+            {\"testcase_name\": \"float16\", \"dtype\": \"float16\"},\n+            {\"testcase_name\": \"float32\", \"dtype\": \"float32\"},\n+            {\"testcase_name\": \"float64\", \"dtype\": \"float64\"},\n+        ]\n+    )\n+    def test_basic_load(self, dtype):\n         \"\"\"Test basic model loading.\"\"\"\n-        model = self.get_model()\n+        model = self.get_model(dtype)\n         filepath = os.path.join(self.get_temp_dir(), \"test_model.keras\")\n         saving_api.save_model(model, filepath)\n \n         loaded_model = saving_api.load_model(filepath)\n         x = np.random.uniform(size=(10, 3))\n+        self.assertEqual(loaded_model.weights[0].dtype, dtype)\n         self.assertTrue(np.allclose(model.predict(x), loaded_model.predict(x)))\n \n     def test_load_unsupported_format(self):\n@@ -119,25 +130,37 @@ class LoadModelTests(test_case.TestCase):\n         os.remove(filepath)\n \n \n-class LoadWeightsTests(test_case.TestCase):\n-    def get_model(self):\n+class LoadWeightsTests(test_case.TestCase, parameterized.TestCase):\n+    def get_model(self, dtype=None):\n         return Sequential(\n             [\n-                layers.Dense(5, input_shape=(3,)),\n+                layers.Dense(5, input_shape=(3,), dtype=dtype),\n                 layers.Softmax(),\n             ]\n         )\n \n-    def test_load_keras_weights(self):\n+    @parameterized.named_parameters(\n+        named_product(\n+            source_dtype=[\"float64\", \"float32\", \"float16\", \"bfloat16\"],\n+            dest_dtype=[\"float64\", \"float32\", \"float16\", \"bfloat16\"],\n+        )\n+    )\n+    def test_load_keras_weights(self, source_dtype, dest_dtype):\n         \"\"\"Test loading keras weights.\"\"\"\n-        model = self.get_model()\n+        src_model = self.get_model(dtype=source_dtype)\n         filepath = os.path.join(self.get_temp_dir(), \"test_weights.weights.h5\")\n-        model.save_weights(filepath)\n-        original_weights = model.get_weights()\n-        model.load_weights(filepath)\n-        loaded_weights = model.get_weights()\n-        for orig, loaded in zip(original_weights, loaded_weights):\n-            self.assertTrue(np.array_equal(orig, loaded))\n+        src_model.save_weights(filepath)\n+        src_weights = src_model.get_weights()\n+        dest_model = self.get_model(dtype=dest_dtype)\n+        dest_model.load_weights(filepath)\n+        dest_weights = dest_model.get_weights()\n+        for orig, loaded in zip(src_weights, dest_weights):\n+            self.assertAllClose(\n+                orig.astype(\"float32\"),\n+                loaded.astype(\"float32\"),\n+                atol=0.001,\n+                rtol=0.01,\n+            )\n \n     def test_load_h5_weights_by_name(self):\n         \"\"\"Test loading h5 weights by name.\"\"\"\n\n@@ -45,6 +45,7 @@ setup(\n         \"namex\",\n         \"h5py\",\n         \"dm-tree\",\n+        \"ml-dtypes\",\n     ],\n     # Supported Python versions\n     python_requires=\">=3.9\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a762b418bfc162d16b92670d938d53c28ad2766c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 49 | Files Changed: 9 | Hunks: 20 | Methods Changed: 17 | Complexity Δ (Sum/Max): 9/13 | Churn Δ: 132 | Churn Cumulative: 4352 | Contributors (this commit): 23 | Commits (past 90d): 59 | Contributors (cumulative): 57 | DMM Complexity: 0.6470588235294118\n\nDIFF:\n@@ -1,6 +1,5 @@\n import types\n \n-import h5py\n import jax\n import jax.numpy as jnp\n import ml_dtypes\n@@ -63,13 +62,9 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n         if dtype and dtype != x.dtype:\n             return x.value.astype(dtype)\n         return x.value\n-    if isinstance(x, h5py.Dataset):\n-        # h5py will handle bfloat16 as an opaque dtype.\n-        # We assume any two byte void dtypes are in fact bfloat16 type.\n-        if x.dtype == np.dtype((np.void, 2)):\n-            x = np.array(x, dtype=ml_dtypes.bfloat16)\n-        # h5py Datasets do not support converting on the fly for many dtypes.\n-        # Instead we convert \"as is\" and cast.\n+    if not is_tensor(x) and standardize_dtype(dtype) == \"bfloat16\":\n+        # Can't create bfloat16 arrays on the fly (e.g. from a h5 Dataset).\n+        # Instead we convert \"as is\" (to stored dtype) and cast.\n         return jnp.asarray(x).astype(dtype)\n     return jnp.asarray(x, dtype=dtype)\n \n\n@@ -1,5 +1,3 @@\n-import h5py\n-import ml_dtypes\n import numpy as np\n import tree\n \n@@ -37,18 +35,14 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n         if dtype and dtype != x.dtype:\n             return x.value.astype(dtype)\n         return x.value\n+    if not is_tensor(x) and standardize_dtype(dtype) == \"bfloat16\":\n+        # Can't create bfloat16 arrays on the fly (e.g. from a h5 Dataset).\n+        # Instead we convert \"as is\" (to stored dtype) and cast.\n+        return np.asarray(x).astype(dtype)\n     if dtype is None:\n         dtype = result_type(\n             *[getattr(item, \"dtype\", type(item)) for item in tree.flatten(x)]\n         )\n-    if isinstance(x, h5py.Dataset):\n-        # h5py will handle bfloat16 as an opaque dtype.\n-        # We assume any two byte void dtypes are in fact bfloat16 type.\n-        if x.dtype == np.dtype((np.void, 2)):\n-            x = np.array(x, dtype=ml_dtypes.bfloat16)\n-        # h5py Datasets do not support converting on the fly for many dtypes.\n-        # Instead we convert \"as is\" and cast.\n-        return np.array(x).astype(dtype)\n     return np.array(x, dtype=dtype)\n \n \n\n@@ -1,7 +1,5 @@\n import types\n \n-import h5py\n-import ml_dtypes\n import numpy as np\n import tensorflow as tf\n from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice\n@@ -109,10 +107,6 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n     if dtype is not None:\n         dtype = standardize_dtype(dtype)\n     if not tf.is_tensor(x):\n-        # h5py will handle bfloat16 as an opaque dtype.\n-        # We assume any two byte void dtypes are in fact bfloat16 type.\n-        if isinstance(x, h5py.Dataset) and x.dtype == np.dtype((np.void, 2)):\n-            x = np.array(x, dtype=ml_dtypes.bfloat16)\n         if dtype == \"bool\":\n             # TensorFlow boolean conversion is stricter than other backends.\n             # It does not allow ints. We convert without dtype and cast instead.\n\n@@ -1,7 +1,6 @@\n import contextlib\n import os\n \n-import h5py\n import ml_dtypes\n import numpy as np\n import torch\n@@ -181,10 +180,6 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n                 x, dtype=to_torch_dtype(floatx()), device=get_device()\n             )\n \n-    # h5py will handle bfloat16 as an opaque dtype.\n-    # We assume any two byte void dtypes are in fact bfloat16 type.\n-    if isinstance(x, h5py.Dataset) and x.dtype == np.dtype((np.void, 2)):\n-        x = np.array(x, dtype=ml_dtypes.bfloat16)\n     # Convert to np in case of any array-like that is not list or tuple.\n     if not isinstance(x, (list, tuple)):\n         x = np.array(x)\n\n@@ -185,10 +185,10 @@ class Dense(Layer):\n         if not self.lora_enabled:\n             return super().save_own_variables(store)\n \n-        kernel_value = ops.convert_to_numpy(self.kernel)\n+        kernel_value = self.kernel\n         store[\"0\"] = kernel_value\n         if self.use_bias:\n-            store[\"1\"] = ops.convert_to_numpy(self.bias)\n+            store[\"1\"] = self.bias\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n\n@@ -262,10 +262,10 @@ class EinsumDense(Layer):\n         if not self.lora_enabled:\n             return super().save_own_variables(store)\n \n-        kernel_value = ops.convert_to_numpy(self.kernel)\n+        kernel_value = self.kernel\n         store[\"0\"] = kernel_value\n         if self.bias is not None:\n-            store[\"1\"] = ops.convert_to_numpy(self.bias)\n+            store[\"1\"] = self.bias\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n\n@@ -164,7 +164,7 @@ class Embedding(Layer):\n         if not self.lora_enabled:\n             return super().save_own_variables(store)\n \n-        embeddings_value = ops.convert_to_numpy(self.embeddings)\n+        embeddings_value = self.embeddings\n         store[\"0\"] = embeddings_value\n \n     def load_own_variables(self, store):\n\n@@ -1120,7 +1120,7 @@ class Layer(BackendLayer, Operation):\n         \"\"\"\n         all_vars = self._trainable_variables + self._non_trainable_variables\n         for i, v in enumerate(all_vars):\n-            store[f\"{i}\"] = ops.convert_to_numpy(v)\n+            store[f\"{i}\"] = v\n \n     def load_own_variables(self, store):\n         \"\"\"Loads the state of the layer.\n\n@@ -7,8 +7,10 @@ import tempfile\n import warnings\n import zipfile\n \n+import ml_dtypes\n import numpy as np\n \n+from keras import backend\n from keras.backend.common import global_state\n from keras.layers.layer import Layer\n from keras.losses.loss import Loss\n@@ -602,22 +604,10 @@ class H5IOStore:\n             self.h5_file = h5py.File(root_path, mode=self.mode)\n \n     def make(self, path):\n-        if not path:\n-            return self.h5_file.create_group(\"vars\")\n-        return self.h5_file.create_group(path).create_group(\"vars\")\n+        return H5Entry(self.h5_file, path, mode=\"w\")\n \n     def get(self, path):\n-        if not path:\n-            return self.h5_file[\"vars\"]\n-        if path in self.h5_file and \"vars\" in self.h5_file[path]:\n-            return self.h5_file[path][\"vars\"]\n-        # No hit.\n-        # Fix for 2.13 compatibility\n-        if \"_layer_checkpoint_dependencies\" in self.h5_file:\n-            path = path.replace(\"layers\", \"_layer_checkpoint_dependencies\")\n-            if path in self.h5_file and \"vars\" in self.h5_file[path]:\n-                return self.h5_file[path][\"vars\"]\n-        return {}\n+        return H5Entry(self.h5_file, path, mode=\"r\")\n \n     def close(self):\n         self.h5_file.close()\n@@ -627,6 +617,72 @@ class H5IOStore:\n             self.io_file.close()\n \n \n+class H5Entry:\n+    \"\"\"Leaf entry in a H5IOStore.\"\"\"\n+\n+    def __init__(self, h5_file, path, mode):\n+        self.h5_file = h5_file\n+        self.path = path\n+        self.mode = mode\n+\n+        if mode == \"w\":\n+            if not path:\n+                self.group = self.h5_file.create_group(\"vars\")\n+            else:\n+                self.group = self.h5_file.create_group(self.path).create_group(\n+                    \"vars\"\n+                )\n+        else:\n+            found = False\n+            if not path:\n+                self.group = self.h5_file[\"vars\"]\n+                found = True\n+            elif path in self.h5_file and \"vars\" in self.h5_file[path]:\n+                self.group = self.h5_file[path][\"vars\"]\n+                found = True\n+            else:\n+                # No hit.\n+                # Fix for 2.13 compatibility\n+                if \"_layer_checkpoint_dependencies\" in self.h5_file:\n+                    path = path.replace(\n+                        \"layers\", \"_layer_checkpoint_dependencies\"\n+                    )\n+                    self.path = path\n+                    if path in self.h5_file and \"vars\" in self.h5_file[path]:\n+                        self.group = self.h5_file[path][\"vars\"]\n+                        found = True\n+            if not found:\n+                self.group = {}\n+\n+    def __len__(self):\n+        return self.group.__len__()\n+\n+    def keys(self):\n+        return self.group.keys()\n+\n+    def items(self):\n+        return self.group.items()\n+\n+    def values(self):\n+        return self.group.values()\n+\n+    def __setitem__(self, key, value):\n+        if self.mode != \"w\":\n+            raise ValueError(\"Setting a value is only allowed in write mode.\")\n+        value = backend.convert_to_numpy(value)\n+        if backend.standardize_dtype(value.dtype) == \"bfloat16\":\n+            ds = self.group.create_dataset(key, data=value)\n+            ds.attrs[\"dtype\"] = \"bfloat16\"\n+        else:\n+            self.group[key] = value\n+\n+    def __getitem__(self, name):\n+        value = self.group[name]\n+        if \"dtype\" in value.attrs and value.attrs[\"dtype\"] == \"bfloat16\":\n+            value = np.array(value, dtype=ml_dtypes.bfloat16)\n+        return value\n+\n+\n class NpzIOStore:\n     def __init__(self, root_path, archive=None, mode=\"r\"):\n         \"\"\"Numerical variable store backed by NumPy.savez/load.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d84e14bf1403c68752748bc9f0722b6f2c93f5fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 119 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 4 | Methods Changed: 10 | Complexity Δ (Sum/Max): 8/9 | Churn Δ: 126 | Churn Cumulative: 2023 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -184,12 +184,7 @@ class Attention(Layer):\n         return ops.matmul(weights, value), weights\n \n     def _calculate_score_mask(self, scores, v_mask, use_causal_mask):\n-        if v_mask is not None:\n-            # Mask of shape [batch_size, 1, Tv].\n-            v_mask = ops.expand_dims(v_mask, axis=-2)\n-        if not use_causal_mask:\n-            return v_mask\n-\n+        if use_causal_mask:\n             # Creates a lower triangular mask, so position i cannot attend to\n             # positions j > i. This prevents the flow of information from the\n             # future into the past.\n@@ -202,8 +197,14 @@ class Attention(Layer):\n             causal_mask = ops.greater_equal(row_index, col_index)\n \n             if v_mask is not None:\n-            return causal_mask\n+                # Mask of shape [batch_size, 1, Tv].\n+                v_mask = ops.expand_dims(v_mask, axis=-2)\n                 return ops.logical_and(v_mask, causal_mask)\n+            return causal_mask\n+        else:\n+            # If not using causal mask, return the value mask as is,\n+            # or None if the value mask is not provided.\n+            return v_mask\n \n     def call(\n         self,\n\n@@ -114,3 +114,114 @@ class AttentionTest(testing.TestCase):\n         )\n         self.assertNotAllClose(output1, output2)\n         self.assertNotAllClose(scores1, scores2)\n+\n+    def test_attention_invalid_score_mode(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument score_mode. \"\n+            \"Expected one of {'dot', 'concat'}\",\n+        ):\n+            layers.Attention(score_mode=\"invalid_mode\")\n+\n+    def test_attention_calculate_scores_with_scale(self):\n+        query = np.random.random((2, 3, 4))\n+        key = np.random.random((2, 4, 4))\n+        layer = layers.Attention(use_scale=True, score_mode=\"dot\")\n+        layer.build(input_shape=[(2, 3, 4), (2, 4, 4)])\n+        expected_scores = np.matmul(query, key.transpose((0, 2, 1)))\n+        expected_scores *= layer.scale.numpy()\n+        actual_scores = layer._calculate_scores(query, key)\n+        self.assertAllClose(actual_scores, expected_scores)\n+\n+    def test_attention_calculate_score_mask_no_causal_no_vmask(self):\n+        scores = np.random.random((2, 3, 4))\n+        layer = layers.Attention()\n+        mask = layer._calculate_score_mask(\n+            scores, v_mask=None, use_causal_mask=False\n+        )\n+        self.assertIsNone(\n+            mask,\n+            \"Mask should be None when no causal mask and no value mask \"\n+            \"are used\",\n+        )\n+\n+    def test_attention_calculate_score_mask_with_causal_no_vmask(self):\n+        scores = np.random.random((2, 3, 4))\n+        layer = layers.Attention()\n+\n+        causal_mask = layer._calculate_score_mask(\n+            scores, v_mask=None, use_causal_mask=True\n+        )\n+        expected_causal_mask = np.tril(\n+            np.ones((1, scores.shape[1], scores.shape[2])), k=0\n+        )\n+        self.assertAllClose(causal_mask, expected_causal_mask, atol=1e-6)\n+\n+    def test_attention_calculate_score_mask_with_causal_and_vmask(self):\n+        scores = np.random.random((2, 3, 4))\n+        layer = layers.Attention()\n+        v_mask = np.array([[True, False, True, False]])\n+\n+        combined_mask = layer._calculate_score_mask(\n+            scores, v_mask=v_mask, use_causal_mask=True\n+        )\n+        expected_causal_mask = np.tril(\n+            np.ones((1, scores.shape[1], scores.shape[2])), k=0\n+        )\n+        expected_combined_mask = np.logical_and(\n+            expected_causal_mask, v_mask[:, np.newaxis, :]\n+        )\n+        self.assertAllClose(combined_mask, expected_combined_mask, atol=1e-6)\n+\n+    def test_attention_compute_mask_with_no_mask(self):\n+        layer = layers.Attention()\n+        dummy_inputs = [\n+            np.random.random((2, 3, 4)),\n+            np.random.random((2, 4, 4)),\n+        ]\n+        self.assertIsNone(\n+            layer.compute_mask(inputs=dummy_inputs, mask=None),\n+            \"compute_mask should return None when mask is None\",\n+        )\n+\n+    def test_attention_compute_mask_with_first_element_none(self):\n+        layer = layers.Attention()\n+        dummy_inputs = [\n+            np.random.random((2, 3, 4)),\n+            np.random.random((2, 4, 4)),\n+        ]\n+        mask = [None, np.array([True, False, True])]\n+        self.assertIsNone(\n+            layer.compute_mask(inputs=dummy_inputs, mask=mask),\n+            \"compute_mask should return None when the first element is None\",\n+        )\n+\n+    def test_attention_compute_mask_does_not_return_none_with_valid_mask(self):\n+        layer = layers.Attention()\n+        dummy_inputs = [\n+            np.random.random((2, 3, 4)),\n+            np.random.random((2, 4, 4)),\n+        ]\n+        valid_mask = np.array([True, False, True])\n+        mask = [valid_mask, np.array([False, True, False])]\n+        computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        self.assertIsNotNone(\n+            computed_mask,\n+            \"compute_mask should not return None with a valid mask\",\n+        )\n+\n+    def test_attention_compute_mask_returns_correct_tensor_with_valid_mask(\n+        self,\n+    ):\n+        layer = layers.Attention()\n+        dummy_inputs = [\n+            np.random.random((2, 3, 4)),\n+            np.random.random((2, 4, 4)),\n+        ]\n+        valid_mask = np.array([True, False, True])\n+        mask = [valid_mask, np.array([False, True, False])]\n+        computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        self.assertTrue(\n+            np.array_equal(computed_mask, valid_mask),\n+            \"compute_mask did not return the correct mask tensor\",\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#adacf2c49504acf228e556a9131d42e680cec3de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 30 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 47 | Churn Cumulative: 414 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,10 +36,12 @@ def build_keras_model(keras_module, num_classes):\n             keras_module.layers.Conv2D(\n                 32, kernel_size=(3, 3), activation=\"relu\"\n             ),\n+            keras_module.layers.BatchNormalization(),\n             keras_module.layers.MaxPooling2D(pool_size=(2, 2)),\n             keras_module.layers.Conv2D(\n                 64, kernel_size=(3, 3), activation=\"relu\"\n             ),\n+            keras_module.layers.BatchNormalization(scale=False, center=True),\n             keras_module.layers.MaxPooling2D(pool_size=(2, 2)),\n             keras_module.layers.Flatten(),\n             keras_module.layers.Dense(num_classes, activation=\"softmax\"),\n\n@@ -684,7 +684,7 @@ def moments(x, axes, keepdims=False, synchronized=False):\n     # gradient is zero.\n     variance = torch.mean(\n         torch.square(x), dim=axes, keepdim=True\n-    ) - torch.square(mean.detach())\n+    ) - torch.square(mean)\n \n     if not keepdims:\n         mean = torch.squeeze(mean, axes)\n@@ -710,46 +710,31 @@ def batch_normalization(\n     x, mean, variance, axis, offset=None, scale=None, epsilon=1e-3\n ):\n     x = convert_to_tensor(x)\n-    mean = convert_to_tensor(mean).detach()\n-    variance = convert_to_tensor(variance).detach()\n+    mean = convert_to_tensor(mean)\n+    variance = convert_to_tensor(variance)\n+\n+    shape = [1] * len(x.shape)\n+    shape[axis] = mean.shape[0]\n+    mean = torch.reshape(mean, shape)\n+    variance = torch.reshape(variance, shape)\n+\n     if offset is not None:\n         offset = convert_to_tensor(offset)\n+        offset = torch.reshape(offset, shape)\n     else:\n         offset = torch.zeros_like(mean)\n     if scale is not None:\n         scale = convert_to_tensor(scale)\n+        scale = torch.reshape(scale, shape)\n     else:\n         scale = torch.ones_like(variance)\n \n-    def _batch_norm():\n-        return tnn.batch_norm(\n-            input=x,\n-            running_mean=mean,\n-            running_var=variance,\n-            weight=scale,\n-            bias=offset,\n-            training=False,\n-            eps=epsilon,\n+    return (\n+        x.subtract(mean)\n+        .mul_(variance.add(epsilon).rsqrt_().mul(scale))\n+        .add_(offset)\n     )\n \n-    if axis == 1:\n-        return _batch_norm()\n-\n-    if axis < 0:\n-        axis = len(x.shape) + axis\n-\n-    order = list(range(len(x.shape)))\n-    order.pop(axis)\n-    order.insert(1, axis)\n-    x = x.permute(order)\n-\n-    x = _batch_norm()\n-\n-    order = list(range(len(x.shape)))\n-    order.pop(1)\n-    order.insert(axis, 1)\n-    return x.permute(order)\n-\n \n def ctc_loss(\n     target,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06cde6056cf14268b87a1fdfe3747d9fa3ff9104", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 317 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 10 | Methods Changed: 11 | Complexity Δ (Sum/Max): 66/38 | Churn Δ: 317 | Churn Cumulative: 1114 | Contributors (this commit): 6 | Commits (past 90d): 21 | Contributors (cumulative): 27 | DMM Complexity: 0.3302752293577982\n\nDIFF:\n@@ -258,3 +258,7 @@ def solve(a, b):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n     return jnp.linalg.solve(a, b)\n+\n+\n+def norm(x, ord=None, axis=None, keepdims=False):\n+    return jnp.linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n\n@@ -312,3 +312,7 @@ def solve(a, b):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n     return np.linalg.solve(a, b)\n+\n+\n+def norm(x, ord=None, axis=None, keepdims=False):\n+    return np.linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n\n@@ -1,4 +1,5 @@\n import tensorflow as tf\n+from tensorflow.experimental import numpy as tfnp\n \n from keras.backend import standardize_dtype\n from keras.backend.tensorflow.core import convert_to_tensor\n@@ -249,3 +250,126 @@ def solve(a, b):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n     return tf.linalg.solve(a, b)\n+\n+\n+def norm(x, ord=None, axis=None, keepdims=False):\n+    x = convert_to_tensor(x)\n+    x_shape = x.shape\n+    ndim = x_shape.rank\n+\n+    if ord is None:\n+        ord = \"euclidean\"\n+\n+    if axis is None:\n+        axis = tuple(range(ndim))\n+    elif isinstance(axis, int):\n+        axis = (axis,)\n+\n+    axis = axis[0] if len(axis) == 1 else axis\n+    num_axes = 1 if isinstance(axis, int) else len(axis)\n+\n+    # Fast path to utilze `tf.linalg.norm`\n+    if (num_axes == 1 and ord in (\"euclidean\", 2)) or (\n+        num_axes == 2 and ord in (\"euclidean\", \"fro\")\n+    ):\n+        return tf.linalg.norm(x, axis=axis, keepdims=keepdims)\n+\n+    # Ref: jax.numpy.linalg.norm\n+    if num_axes == 1 and ord not in (\"euclidean\", 2, \"fro\", \"nuc\"):\n+        if ord == float(\"-inf\"):\n+            return tf.math.reduce_min(\n+                tf.math.abs(x), axis=axis, keepdims=keepdims\n+            )\n+        elif ord == 0:\n+            return tf.math.reduce_sum(\n+                tf.cast(tf.not_equal(x, 0), dtype=x.dtype),\n+                axis=axis,\n+                keepdims=keepdims,\n+            )\n+        elif ord == 1:\n+            return tf.math.reduce_sum(\n+                tf.math.abs(x), axis=axis, keepdims=keepdims\n+            )\n+        elif ord == float(\"inf\"):\n+            return tf.math.reduce_max(\n+                tf.math.abs(x), axis=axis, keepdims=keepdims\n+            )\n+        else:\n+            ord = convert_to_tensor(ord, dtype=x.dtype)\n+            out = tf.math.reduce_sum(\n+                tf.pow(tf.math.abs(x), ord), axis=axis, keepdims=keepdims\n+            )\n+            return tf.pow(out, 1.0 / ord)\n+    elif num_axes == 2 and ord in (\n+        \"nuc\",\n+        float(\"-inf\"),\n+        -2,\n+        -1,\n+        1,\n+        2,\n+        float(\"inf\"),\n+    ):\n+        row_axis, col_axis = axis[0], axis[1]\n+        row_axis = row_axis + ndim if row_axis < 0 else row_axis\n+        col_axis = col_axis + ndim if col_axis < 0 else col_axis\n+        if ord == float(\"-inf\"):\n+            if not keepdims and row_axis > col_axis:\n+                row_axis -= 1\n+            x = tf.math.reduce_min(\n+                tf.reduce_sum(tf.math.abs(x), axis=col_axis, keepdims=keepdims),\n+                axis=row_axis,\n+                keepdims=keepdims,\n+            )\n+        elif ord == -1:\n+            if not keepdims and col_axis > row_axis:\n+                col_axis -= 1\n+            x = tf.math.reduce_min(\n+                tf.reduce_sum(tf.math.abs(x), axis=row_axis, keepdims=keepdims),\n+                axis=col_axis,\n+                keepdims=keepdims,\n+            )\n+        elif ord == 1:\n+            if not keepdims and col_axis > row_axis:\n+                col_axis -= 1\n+            x = tf.math.reduce_max(\n+                tf.reduce_sum(tf.math.abs(x), axis=row_axis, keepdims=keepdims),\n+                axis=col_axis,\n+                keepdims=keepdims,\n+            )\n+        elif ord == float(\"inf\"):\n+            if not keepdims and row_axis > col_axis:\n+                row_axis -= 1\n+            x = tf.math.reduce_max(\n+                tf.reduce_sum(tf.math.abs(x), axis=col_axis, keepdims=keepdims),\n+                axis=row_axis,\n+                keepdims=keepdims,\n+            )\n+        elif ord in (\"nuc\", 2, -2):\n+            x = tfnp.moveaxis(x, axis, (-2, -1))\n+            if ord == 2:\n+                x = tf.math.reduce_max(\n+                    tf.linalg.svd(x, compute_uv=False), axis=-1\n+                )\n+            elif ord == -2:\n+                x = tf.math.reduce_min(\n+                    tf.linalg.svd(x, compute_uv=False), axis=-1\n+                )\n+            else:\n+                x = tf.math.reduce_sum(\n+                    tf.linalg.svd(x, compute_uv=False), axis=-1\n+                )\n+            if keepdims:\n+                x = tf.expand_dims(x, axis[0])\n+                x = tf.expand_dims(x, axis[1])\n+        return x\n+\n+    if num_axes == 1:\n+        raise ValueError(\n+            f\"Invalid `ord` argument for vector norm. Received: ord={ord}\"\n+        )\n+    elif num_axes == 2:\n+        raise ValueError(\n+            f\"Invalid `ord` argument for matrix norm. Received: ord={ord}\"\n+        )\n+    else:\n+        raise ValueError(f\"Invalid axis values. Received: axis={axis}\")\n\n@@ -419,3 +419,8 @@ def solve(a, b):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n     return torch.linalg.solve(a, b)\n+\n+\n+def norm(x, ord=None, axis=None, keepdims=False):\n+    x = convert_to_tensor(x)\n+    return torch.linalg.norm(x, ord=ord, dim=axis, keepdim=keepdims)\n\n@@ -1001,3 +1001,122 @@ def solve(a, b):\n     a = backend.convert_to_tensor(a)\n     b = backend.convert_to_tensor(b)\n     return backend.math.solve(a, b)\n+\n+\n+class Norm(Operation):\n+    def __init__(self, ord=None, axis=None, keepdims=False):\n+        super().__init__()\n+        if isinstance(ord, str):\n+            if ord not in (\"fro\", \"nuc\"):\n+                raise ValueError(\n+                    \"Invalid `ord` argument. \"\n+                    \"Expected one of {'fro', 'nuc'} when using string. \"\n+                    f\"Received: ord={ord}\"\n+                )\n+        if isinstance(axis, int):\n+            axis = [axis]\n+        self.ord = ord\n+        self.axis = axis\n+        self.keepdims = keepdims\n+\n+    def compute_output_spec(self, x):\n+        output_dtype = backend.standardize_dtype(x.dtype)\n+        if \"int\" in output_dtype or output_dtype == \"bool\":\n+            output_dtype = backend.floatx()\n+        if self.axis is None:\n+            axis = tuple(range(len(x.shape)))\n+        else:\n+            axis = self.axis\n+        num_axes = len(axis)\n+        if num_axes == 1 and isinstance(self.ord, str):\n+            raise ValueError(\n+                \"Invalid `ord` argument for vector norm. \"\n+                f\"Received: ord={self.ord}\"\n+            )\n+        elif num_axes == 2 and self.ord not in (\n+            None,\n+            \"fro\",\n+            \"nuc\",\n+            float(\"inf\"),\n+            float(\"-inf\"),\n+            1,\n+            -1,\n+            2,\n+            -2,\n+        ):\n+            raise ValueError(\n+                \"Invalid `ord` argument for matrix norm. \"\n+                f\"Received: ord={self.ord}\"\n+            )\n+        return KerasTensor(\n+            reduce_shape(x.shape, axis=self.axis, keepdims=self.keepdims),\n+            dtype=output_dtype,\n+        )\n+\n+    def call(self, x):\n+        x = backend.convert_to_tensor(x)\n+        return backend.math.norm(\n+            x, ord=self.ord, axis=self.axis, keepdims=self.keepdims\n+        )\n+\n+\n+@keras_export(\"keras.ops.norm\")\n+def norm(x, ord=None, axis=None, keepdims=False):\n+    \"\"\"Matrix or vector norm.\n+\n+    This function is able to return one of eight different matrix norms, or one\n+    of an infinite number of vector norms (described below), depending on the\n+    value of the `ord` parameter.\n+\n+    Args:\n+        x: Input tensor.\n+        ord: Order of the norm (see table under Notes). The default is `None`.\n+        axis: If `axis` is an integer, it specifies the axis of `x` along which\n+            to compute the vector norms. If `axis` is a 2-tuple, it specifies\n+            the axes that hold 2-D matrices, and the matrix norms of these\n+            matrices are computed.\n+        keepdims: If this is set to `True`, the axes which are reduced are left\n+            in the result as dimensions with size one.\n+\n+    Note:\n+        For values of `ord < 1`, the result is, strictly speaking, not a\n+        mathematical 'norm', but it may still be useful for various numerical\n+        purposes. The following norms can be calculated:\n+        - For matrices:\n+            - `ord=None`: Frobenius norm\n+            - `ord=\"fro\"`: Frobenius norm\n+            - `ord=nuc`: nuclear norm\n+            - `ord=np.inf`: `max(sum(abs(x), axis=1))`\n+            - `ord=-np.inf`: `min(sum(abs(x), axis=1))`\n+            - `ord=0`: not supported\n+            - `ord=1`: `max(sum(abs(x), axis=0))`\n+            - `ord=-1`: `min(sum(abs(x), axis=0))`\n+            - `ord=2`: 2-norm (largest sing. value)\n+            - `ord=-2`: smallest singular value\n+            - other: not supported\n+        - For vectors:\n+            - `ord=None`: 2-norm\n+            - `ord=\"fro\"`: not supported\n+            - `ord=nuc`: not supported\n+            - `ord=np.inf`: `max(abs(x))`\n+            - `ord=-np.inf`: `min(abs(x))`\n+            - `ord=0`: `sum(x != 0)`\n+            - `ord=1`: as below\n+            - `ord=-1`: as below\n+            - `ord=2`: as below\n+            - `ord=-2`: as below\n+            - other: `sum(abs(x)**ord)**(1./ord)`\n+\n+    Returns:\n+        Norm of the matrix or vector(s).\n+\n+    Example:\n+\n+    >>> x = keras.ops.reshape(keras.ops.arange(9, dtype=\"float32\") - 4, (3, 3))\n+    >>> keras.ops.norm(x)\n+    7.7459664\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Norm(ord=ord, axis=axis, keepdims=keepdims).symbolic_call(x)\n+    x = backend.convert_to_tensor(x)\n+    return backend.math.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n\n@@ -9,6 +9,7 @@ from keras import backend\n from keras import testing\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.ops import math as kmath\n+from keras.testing.test_utils import named_product\n \n \n def _stft(\n@@ -281,6 +282,16 @@ class MathOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(kmath.rsqrt(x).shape, (None, 3))\n \n+    def test_norm(self):\n+        x = KerasTensor((None, 3))\n+        self.assertEqual(kmath.norm(x).shape, ())\n+\n+        x = KerasTensor((None, 3, 3))\n+        self.assertEqual(kmath.norm(x, axis=1).shape, (None, 3))\n+        self.assertEqual(\n+            kmath.norm(x, axis=1, keepdims=True).shape, (None, 1, 3)\n+        )\n+\n \n class MathOpsStaticShapeTest(testing.TestCase):\n     @pytest.mark.skipif(\n@@ -425,6 +436,10 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         outputs = kmath.solve(x1, x2)\n         self.assertEqual(outputs.shape, (2, 2))\n \n+    def test_norm(self):\n+        x = KerasTensor((2, 3))\n+        self.assertEqual(kmath.norm(x).shape, ())\n+\n \n class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     @pytest.mark.skipif(\n@@ -879,6 +894,52 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         expected_result = np.array([[2, 0], [0, 2]], dtype=\"float32\")\n         self.assertAllClose(output, expected_result)\n \n+    @parameterized.named_parameters(\n+        named_product(\n+            ord=[None, \"fro\", \"nuc\", -np.inf, -2, -1, 0, 1, 2, np.inf, 123],\n+            axis=[None, (0, 1), (0, 2)],\n+            keepdims=[False, True],\n+        )\n+    )\n+    def test_norm_matrices(self, ord, axis, keepdims):\n+        if axis is None:\n+            x = np.random.random((6, 7))\n+        else:\n+            x = np.random.random((5, 6, 7))\n+        if ord in (0, 123):\n+            error = RuntimeError if backend.backend() == \"torch\" else ValueError\n+            with self.assertRaises(error):\n+                kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n+            return\n+        output = kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n+        expected_result = np.linalg.norm(\n+            x, ord=ord, axis=axis, keepdims=keepdims\n+        )\n+        self.assertAllClose(output, expected_result)\n+\n+    @parameterized.named_parameters(\n+        named_product(\n+            ord=[None, \"fro\", \"nuc\", -np.inf, -2, -1, 0, 1, 2, np.inf, 123],\n+            axis=[None, 1, -1],\n+            keepdims=[False, True],\n+        )\n+    )\n+    def test_norm_vectors(self, ord, axis, keepdims):\n+        if axis is None:\n+            x = np.random.random((5,))\n+        else:\n+            x = np.random.random((5, 6))\n+        if ord in (\"fro\", \"nuc\"):\n+            error = RuntimeError if backend.backend() == \"torch\" else ValueError\n+            with self.assertRaises(error):\n+                kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n+            return\n+        output = kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n+        expected_result = np.linalg.norm(\n+            x, ord=ord, axis=axis, keepdims=keepdims\n+        )\n+        self.assertAllClose(output, expected_result)\n+\n \n class QrOpTest(testing.TestCase):\n     def test_qr_init_mode_reduced(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d55a293702fd5f1aba1d4a546aabe48af3abf529", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 7331 | Contributors (this commit): 9 | Commits (past 90d): 48 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -409,7 +409,13 @@ def arange(start, stop=None, step=1, dtype=None):\n             dtypes_to_resolve.append(getattr(stop, \"dtype\", type(stop)))\n         dtype = dtypes.result_type(*dtypes_to_resolve)\n     dtype = standardize_dtype(dtype)\n-    return tf.range(start, stop, delta=step, dtype=dtype)\n+    try:\n+        out = tf.range(start, stop, delta=step, dtype=dtype)\n+    except tf.errors.NotFoundError:\n+        # Some dtypes may not work in eager mode on CPU or GPU.\n+        out = tf.range(start, stop, delta=step, dtype=\"float32\")\n+        out = tf.cast(out, dtype)\n+    return out\n \n \n @sparse.densifying_unary(0.5 * np.pi)\n\n@@ -4929,6 +4929,8 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n         (0.0, 10, 1, None),\n         (10, None, 1, \"float32\"),\n         (10, None, 1, \"int32\"),\n+        (10, None, 1, \"int16\"),\n+        (10, None, 1, \"float16\"),\n     )\n     def test_arange(self, start, stop, step, dtype):\n         import jax.numpy as jnp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#34f58489919d215e639d3d3e0e96b32cbb684ba2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 2966 | Contributors (this commit): 19 | Commits (past 90d): 1 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -295,7 +295,9 @@ def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n             Can be a single integer to specify the same value for all\n             spatial dimensions. Specifying any stride value != 1 is\n             incompatible with specifying any `dilation_rate`\n-            value != 1. # Input shape\n+            value != 1.\n+\n+    Input shape:\n         4D tensor with shape: `(samples, channels, rows, cols)` if\n             data_format='channels_first'\n         or 4D tensor with shape: `(samples, rows, cols, channels)` if\n@@ -358,7 +360,8 @@ def _depthwise_conv_block(\n             all spatial dimensions. Specifying any stride value != 1 is\n             incompatible with specifying any `dilation_rate` value != 1.\n         block_id: Integer, a unique identification designating the block number.\n-            # Input shape\n+\n+    Input shape:\n         4D tensor with shape: `(batch, channels, rows, cols)` if\n             data_format='channels_first'\n         or 4D tensor with shape: `(batch, rows, cols, channels)` if\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f872091bc7d76516f6cfb8a0f65a92c63835a8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 296 | Contributors (this commit): 7 | Commits (past 90d): 18 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -265,9 +265,10 @@ def scatter(indices, values, shape):\n \n \n def scatter_update(inputs, indices, updates):\n+    inputs = jnp.array(inputs)\n     indices = jnp.array(indices)\n     indices = jnp.transpose(indices)\n-    inputs[tuple(indices)] = updates\n+    inputs = inputs.at[tuple(indices)].set(updates)\n     return inputs\n \n \n\n@@ -150,6 +150,7 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         indices = [[1, 1], [2, 2]]\n         updates = np.array([[0, 1, 2, 3], [3, 2, 1, 0]], dtype=np.float64)\n         outputs = core.scatter_update(inputs, indices, updates)\n+        self.assertTrue(ops.is_tensor(outputs))\n         self.assertAllClose(outputs[1, 1, :], [0, 1, 2, 3])\n         self.assertAllClose(outputs[2, 2, :], [3, 2, 1, 0])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e149772cc42828e6eaea1ab156bb0aae8effb849", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 95 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -265,7 +265,7 @@ def scatter(indices, values, shape):\n \n \n def scatter_update(inputs, indices, updates):\n-    inputs = jnp.array(inputs)\n+    inputs = convert_to_tensor(inputs)\n     indices = jnp.array(indices)\n     indices = jnp.transpose(indices)\n     inputs = inputs.at[tuple(indices)].set(updates)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
