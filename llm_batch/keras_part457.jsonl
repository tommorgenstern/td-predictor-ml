{"custom_id": "keras#27983375096358bb2ff1de0e07f11d1496eec64e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 353 | Lines Deleted: 285 | Files Changed: 8 | Hunks: 122 | Methods Changed: 43 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 638 | Churn Cumulative: 9220 | Contributors (this commit): 10 | Commits (past 90d): 39 | Contributors (cumulative): 39 | DMM Complexity: 0.75\n\nDIFF:\n@@ -168,8 +168,8 @@ def get(identifier):\n \n     Args:\n       identifier: A metric identifier. One of None or string name of a metric\n-        function/class or metric configuration dictionary or a metric function or\n-        a metric class instance\n+        function/class or metric configuration dictionary or a metric function\n+        or a metric class instance\n \n     Returns:\n       A Keras metric as a `function`/ `Metric` class instance.\n\n@@ -127,9 +127,9 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n         obj = super(Metric, cls).__new__(cls)\n \n         # If `update_state` is not in eager/tf.function and it is not from a\n-        # built-in metric, wrap it in `tf.function`. This is so that users writing\n-        # custom metrics in v1 need not worry about control dependencies and\n-        # return ops.\n+        # built-in metric, wrap it in `tf.function`. This is so that users\n+        # writing custom metrics in v1 need not worry about control dependencies\n+        # and return ops.\n         if base_layer_utils.is_in_eager_or_tf_function() or is_built_in(cls):\n             obj_update_state = obj.update_state\n \n@@ -194,10 +194,11 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n             with tf.control_dependencies(update_ops):\n                 result_t = self.result()  # pylint: disable=not-callable\n \n-                # We are adding the metric object as metadata on the result tensor.\n-                # This is required when we want to use a metric with `add_metric` API on\n-                # a Model/Layer in graph mode. This metric instance will later be used\n-                # to reset variable state after each epoch of training.\n+                # We are adding the metric object as metadata on the result\n+                # tensor.  This is required when we want to use a metric with\n+                # `add_metric` API on a Model/Layer in graph mode. This metric\n+                # instance will later be used to reset variable state after each\n+                # epoch of training.\n                 # Example:\n                 #   model = Model()\n                 #   mean = Mean()\n@@ -224,13 +225,13 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n         for k, v in self.__dict__.items():\n             if k in [\"update_state\", \"result\"]:\n                 # `update_state` keeps a closure of `update_state_fn`, and deep\n-                # copying it would result in copying that old reference. Avoid that.\n-                # Likewise for `result`.\n+                # copying it would result in copying that old reference. Avoid\n+                # that.  Likewise for `result`.\n                 continue\n             if k in [\"_obj_reference_counts_dict\"]:\n                 # `Layer.__setattr__` attempts to flatten the\n-                # `ObjectIdentityDictionary`, which can't be done since it stores\n-                # heterogeneous instances.\n+                # `ObjectIdentityDictionary`, which can't be done since it\n+                # stores heterogeneous instances.\n                 tf.Module.__setattr__(result, k, copy.deepcopy(v, memo))\n             elif k in [\"_thread_local\", \"_metrics_lock\"]:\n                 # Can't pickle _thread.lock objects.\n@@ -276,7 +277,8 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n              This should make it easier to do things like add the updated\n              value of a variable to another, for example.\n           b) You don't need to worry about collecting the update ops to execute.\n-             All update ops added to the graph by this function will be executed.\n+             All update ops added to the graph by this function will be\n+             executed.\n           As a result, code should generally work the same way with graph or\n           eager execution.\n \n@@ -289,12 +291,13 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n     def merge_state(self, metrics):\n         \"\"\"Merges the state from one or more metrics.\n \n-        This method can be used by distributed systems to merge the state computed\n-        by different metric instances. Typically the state will be stored in the\n-        form of the metric's weights. For example, a tf.keras.metrics.Mean metric\n-        contains a list of two weight values: a total and a count. If there were two\n-        instances of a tf.keras.metrics.Accuracy that each independently aggregated\n-        partial state for an overall accuracy calculation, these two metric's states\n+        This method can be used by distributed systems to merge the state\n+        computed by different metric instances. Typically the state will be\n+        stored in the form of the metric's weights. For example, a\n+        tf.keras.metrics.Mean metric contains a list of two weight values: a\n+        total and a count. If there were two instances of a\n+        tf.keras.metrics.Accuracy that each independently aggregated partial\n+        state for an overall accuracy calculation, these two metric's states\n         could be combined as follows:\n \n         >>> m1 = tf.keras.metrics.Accuracy()\n@@ -308,11 +311,12 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n         0.75\n \n         Args:\n-          metrics: an iterable of metrics. The metrics must have compatible state.\n+          metrics: an iterable of metrics. The metrics must have compatible\n+            state.\n \n         Raises:\n-          ValueError: If the provided iterable does not contain metrics matching the\n-            metric's required specifications.\n+          ValueError: If the provided iterable does not contain metrics matching\n+            the metric's required specifications.\n         \"\"\"\n         assign_add_ops = []\n         for metric in metrics:\n@@ -326,7 +330,8 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n \n     @abc.abstractmethod\n     def result(self):\n-        \"\"\"Computes and returns the scalar metric value tensor or a dict of scalars.\n+        \"\"\"Computes and returns the scalar metric value tensor or a dict of\n+        scalars.\n \n         Result computation is an idempotent operation that simply calculates the\n         metric value using the state variables.\n@@ -542,11 +547,11 @@ class Sum(Reduce):\n     For example, if values is [1, 3, 5, 7] then the sum is 16.\n     If the weights were specified as [1, 1, 0, 0] then the sum would be 4.\n \n-    This metric creates one variable, `total`, that is used to compute the sum of\n-    `values`. This is ultimately returned as `sum`.\n+    This metric creates one variable, `total`, that is used to compute the sum\n+    of `values`. This is ultimately returned as `sum`.\n \n-    If `sample_weight` is `None`, weights default to 1.  Use `sample_weight` of 0\n-    to mask values.\n+    If `sample_weight` is `None`, weights default to 1.  Use `sample_weight` of\n+    0 to mask values.\n \n     Args:\n       name: (Optional) string name of the metric instance.\n@@ -582,8 +587,9 @@ class Mean(Reduce):\n     If the weights were specified as [1, 1, 0, 0] then the mean would be 2.\n \n     This metric creates two variables, `total` and `count` that are used to\n-    compute the average of `values`. This average is ultimately returned as `mean`\n-    which is an idempotent operation that simply divides `total` by `count`.\n+    compute the average of `values`. This average is ultimately returned as\n+    `mean` which is an idempotent operation that simply divides `total` by\n+    `count`.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n@@ -663,14 +669,15 @@ class MeanMetricWrapper(Mean):\n           y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n           y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n           sample_weight: Optional `sample_weight` acts as a\n-            coefficient for the metric. If a scalar is provided, then the metric is\n-            simply scaled by the given value. If `sample_weight` is a tensor of size\n-            `[batch_size]`, then the metric for each sample of the batch is rescaled\n-            by the corresponding element in the `sample_weight` vector. If the shape\n-            of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted\n-            to this shape), then each metric element of `y_pred` is scaled by the\n-            corresponding value of `sample_weight`. (Note on `dN-1`: all metric\n-            functions reduce by 1 dimension, usually the last axis (-1)).\n+            coefficient for the metric. If a scalar is provided, then the metric\n+            is simply scaled by the given value. If `sample_weight` is a tensor\n+            of size `[batch_size]`, then the metric for each sample of the batch\n+            is rescaled by the corresponding element in the `sample_weight`\n+            vector. If the shape of `sample_weight` is `[batch_size, d0, ..\n+            dN-1]` (or can be broadcasted to this shape), then each metric\n+            element of `y_pred` is scaled by the corresponding value of\n+            `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1\n+            dimension, usually the last axis (-1)).\n \n         Returns:\n           Update op.\n@@ -699,8 +706,8 @@ class MeanMetricWrapper(Mean):\n         if (\n             type(self) is MeanMetricWrapper\n         ):  # pylint: disable=unidiomatic-typecheck\n-            # Only include function argument when the object is a MeanMetricWrapper\n-            # and not a subclass.\n+            # Only include function argument when the object is a\n+            # MeanMetricWrapper and not a subclass.\n             config[\"fn\"] = self._fn\n \n         for k, v in self._fn_kwargs.items():\n@@ -733,8 +740,8 @@ class MeanTensor(Metric):\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n       shape: (Optional) A list of integers, a tuple of integers, or a 1-D Tensor\n-        of type int32. If not specified, the shape is inferred from the values at\n-        the first call of update_state.\n+        of type int32. If not specified, the shape is inferred from the values\n+        at the first call of update_state.\n \n     Standalone usage:\n \n@@ -808,7 +815,8 @@ class MeanTensor(Metric):\n         elif values.shape != self._shape:\n             raise ValueError(\n                 \"MeanTensor input values must always have the same \"\n-                f\"shape. Expected shape (set during the first call): {self._shape}. \"\n+                f\"shape. Expected shape (set during the first call): \"\n+                f\"{self._shape}. \"\n                 f\"Got: {values.shape}.\"\n             )\n \n@@ -847,8 +855,9 @@ class MeanTensor(Metric):\n     def result(self):\n         if not self._built:\n             raise ValueError(\n-                \"MeanTensor does not have any value yet. Please call the MeanTensor \"\n-                \"instance or use `.update_state(value)` before retrieving the result.\"\n+                \"MeanTensor does not have any value yet. Please call the \"\n+                \"MeanTensor instance or use `.update_state(value)` \"\n+                \"before retrieving the result.\"\n             )\n         return tf.math.divide_no_nan(self.total, self.count)\n \n@@ -870,8 +879,8 @@ class SumOverBatchSize(Reduce):\n     over batch size which is an idempotent operation that simply divides `total`\n     by `count`.\n \n-    If `sample_weight` is `None`, weights default to 1.  Use `sample_weight` of 0\n-    to mask values.\n+    If `sample_weight` is `None`, weights default to 1.  Use `sample_weight` of\n+    0 to mask values.\n     \"\"\"\n \n     def __init__(self, name=\"sum_over_batch_size\", dtype=None):\n\n@@ -49,7 +49,8 @@ class KerasSumTest(tf.test.TestCase, parameterized.TestCase):\n             self.assertEqual(self.evaluate(m(100)), 100)\n             self.assertEqual(self.evaluate(m.total), 100)\n \n-            # check update_state() and result() + state accumulation + tensor input\n+            # check update_state() and result() + state accumulation + tensor\n+            # input\n             update_op = m.update_state(tf.convert_to_tensor([1, 5]))\n             self.evaluate(update_op)\n             self.assertAlmostEqual(self.evaluate(m.result()), 106)\n@@ -411,7 +412,8 @@ class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n             self.assertAllClose(self.evaluate(m.total), [100, 40])\n             self.assertAllClose(self.evaluate(m.count), [1, 1])\n \n-            # check update_state() and result() + state accumulation + tensor input\n+            # check update_state() and result() + state accumulation + tensor\n+            # input\n             update_op = m.update_state(\n                 [tf.convert_to_tensor(1), tf.convert_to_tensor(5)]\n             )\n\n@@ -1195,8 +1195,10 @@ class RecallAtPrecisionTest(tf.test.TestCase, parameterized.TestCase):\n             0.95,\n         ]\n         label_values = [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n-        # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2, 1].\n-        # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6, 1/6].\n+        # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2,\n+        # 1].\n+        # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6,\n+        # 1/6].\n         y_pred = tf.constant(pred_values, dtype=tf.float32)\n         y_true = tf.constant(label_values)\n         self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))\n@@ -1221,8 +1223,10 @@ class RecallAtPrecisionTest(tf.test.TestCase, parameterized.TestCase):\n             0.95,\n         ]\n         label_values = [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n-        # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2, 1].\n-        # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6, 1/6].\n+        # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2,\n+        # 1].\n+        # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6,\n+        # 1/6].\n         y_pred = tf.constant(pred_values, dtype=tf.float32)\n         y_true = tf.constant(label_values)\n         self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))\n@@ -1247,8 +1251,10 @@ class RecallAtPrecisionTest(tf.test.TestCase, parameterized.TestCase):\n             0.95,\n         ]\n         label_values = [0, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2]\n-        # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2, 1].\n-        # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6, 1/6].\n+        # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2,\n+        # 1].\n+        # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6,\n+        # 1/6].\n         y_pred = tf.transpose([pred_values] * 3)\n         y_true = tf.one_hot(label_values, depth=3)\n         self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))\n@@ -1451,7 +1457,8 @@ class AUCTest(tf.test.TestCase, parameterized.TestCase):\n \n     def test_manual_thresholds(self):\n         self.setup()\n-        # Verify that when specified, thresholds are used instead of num_thresholds.\n+        # Verify that when specified, thresholds are used instead of\n+        # num_thresholds.\n         auc_obj = metrics.AUC(num_thresholds=2, thresholds=[0.5])\n         self.assertEqual(auc_obj.num_thresholds, 3)\n         self.assertAllClose(auc_obj.thresholds, [0.0, 0.5, 1.0])\n@@ -1957,8 +1964,8 @@ class ThresholdsTest(tf.test.TestCase, parameterized.TestCase):\n         ]\n     )\n     def test_with_default_thresholds(self, metric_obj):\n-        # By default, the thresholds will be evenly distributed if there are more\n-        # than 1. In case there is only 1 thresholds, then we expect\n+        # By default, the thresholds will be evenly distributed if there are\n+        # more than 1. In case there is only 1 thresholds, then we expect\n         # _thresholds_distributed_evenly to be false.\n         expected = len(metric_obj.thresholds) > 1\n         self.assertEqual(metric_obj._thresholds_distributed_evenly, expected)\n@@ -1983,8 +1990,8 @@ class ThresholdsTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertFalse(metric_obj._thresholds_distributed_evenly)\n \n     def test_manual_thresholds_auc(self):\n-        # The AUC metric handles manual thresholds input differently (it will add\n-        # 0.0 and 1.0 for user).\n+        # The AUC metric handles manual thresholds input differently (it will\n+        # add 0.0 and 1.0 for user).\n         even_thresholds = [0.25, 0.5, 0.75]\n         auc = metrics.AUC(thresholds=even_thresholds)\n         self.assertTrue(auc._thresholds_distributed_evenly)\n\n@@ -50,10 +50,10 @@ from tensorflow.python.util.tf_export import keras_export\n class MeanRelativeError(base_metric.Mean):\n     \"\"\"Computes the mean relative error by normalizing with the given values.\n \n-    This metric creates two local variables, `total` and `count` that are used to\n-    compute the mean relative error. This is weighted by `sample_weight`, and\n-    it is ultimately returned as `mean_relative_error`:\n-    an idempotent operation that simply divides `total` by `count`.\n+    This metric creates two local variables, `total` and `count` that are used\n+    to compute the mean relative error. This is weighted by `sample_weight`, and\n+    it is ultimately returned as `mean_relative_error`: an idempotent operation\n+    that simply divides `total` by `count`.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n@@ -96,9 +96,9 @@ class MeanRelativeError(base_metric.Mean):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -140,10 +140,10 @@ class MeanRelativeError(base_metric.Mean):\n class Accuracy(base_metric.MeanMetricWrapper):\n     \"\"\"Calculates how often predictions equal labels.\n \n-    This metric creates two local variables, `total` and `count` that are used to\n-    compute the frequency with which `y_pred` matches `y_true`. This frequency is\n-    ultimately returned as `binary accuracy`: an idempotent operation that simply\n-    divides `total` by `count`.\n+    This metric creates two local variables, `total` and `count` that are used\n+    to compute the frequency with which `y_pred` matches `y_true`. This\n+    frequency is ultimately returned as `binary accuracy`: an idempotent\n+    operation that simply divides `total` by `count`.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n@@ -183,10 +183,10 @@ class Accuracy(base_metric.MeanMetricWrapper):\n class BinaryAccuracy(base_metric.MeanMetricWrapper):\n     \"\"\"Calculates how often predictions match binary labels.\n \n-    This metric creates two local variables, `total` and `count` that are used to\n-    compute the frequency with which `y_pred` matches `y_true`. This frequency is\n-    ultimately returned as `binary accuracy`: an idempotent operation that simply\n-    divides `total` by `count`.\n+    This metric creates two local variables, `total` and `count` that are used\n+    to compute the frequency with which `y_pred` matches `y_true`. This\n+    frequency is ultimately returned as `binary accuracy`: an idempotent\n+    operation that simply divides `total` by `count`.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n@@ -233,13 +233,14 @@ class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n     You can provide logits of classes as `y_pred`, since argmax of\n     logits and probabilities are same.\n \n-    This metric creates two local variables, `total` and `count` that are used to\n-    compute the frequency with which `y_pred` matches `y_true`. This frequency is\n-    ultimately returned as `categorical accuracy`: an idempotent operation that\n-    simply divides `total` by `count`.\n+    This metric creates two local variables, `total` and `count` that are used\n+    to compute the frequency with which `y_pred` matches `y_true`. This\n+    frequency is ultimately returned as `categorical accuracy`: an idempotent\n+    operation that simply divides `total` by `count`.\n \n-    `y_pred` and `y_true` should be passed in as vectors of probabilities, rather\n-    than as labels. If necessary, use `tf.one_hot` to expand `y_true` as a vector.\n+    `y_pred` and `y_true` should be passed in as vectors of probabilities,\n+    rather than as labels. If necessary, use `tf.one_hot` to expand `y_true` as\n+    a vector.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n@@ -276,7 +277,7 @@ class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n     @dtensor_utils.inject_mesh\n     def __init__(self, name=\"categorical_accuracy\", dtype=None):\n         super().__init__(\n-            lambda y_true, y_pred: metrics_utils.sparse_categorical_matches(  # pylint: disable=g-long-lambda\n+            lambda y_true, y_pred: metrics_utils.sparse_categorical_matches(\n                 tf.math.argmax(y_true, axis=-1), y_pred\n             ),\n             name,\n@@ -295,10 +296,10 @@ class SparseCategoricalAccuracy(base_metric.MeanMetricWrapper):\n     You can provide logits of classes as `y_pred`, since argmax of\n     logits and probabilities are same.\n \n-    This metric creates two local variables, `total` and `count` that are used to\n-    compute the frequency with which `y_pred` matches `y_true`. This frequency is\n-    ultimately returned as `sparse categorical accuracy`: an idempotent operation\n-    that simply divides `total` by `count`.\n+    This metric creates two local variables, `total` and `count` that are used\n+    to compute the frequency with which `y_pred` matches `y_true`. This\n+    frequency is ultimately returned as `sparse categorical accuracy`: an\n+    idempotent operation that simply divides `total` by `count`.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n@@ -402,7 +403,7 @@ class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n     @dtensor_utils.inject_mesh\n     def __init__(self, k=5, name=\"top_k_categorical_accuracy\", dtype=None):\n         super().__init__(\n-            lambda yt, yp, k: metrics_utils.sparse_top_k_categorical_matches(  # pylint: disable=g-long-lambda\n+            lambda yt, yp, k: metrics_utils.sparse_top_k_categorical_matches(\n                 tf.math.argmax(yt, axis=-1), yp, k\n             ),\n             name,\n@@ -466,11 +467,11 @@ class _ConfusionMatrixConditionCount(base_metric.Metric):\n \n     Args:\n       confusion_matrix_cond: One of `metrics_utils.ConfusionMatrix` conditions.\n-      thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple\n-        of float threshold values in [0, 1]. A threshold is compared with\n-        prediction values to determine the truth value of predictions (i.e., above\n-        the threshold is `true`, below is `false`). One metric value is generated\n-        for each threshold value.\n+      thresholds: (Optional) Defaults to 0.5. A float value or a python\n+        list/tuple of float threshold values in [0, 1]. A threshold is compared\n+        with prediction values to determine the truth value of predictions\n+        (i.e., above the threshold is `true`, below is `false`). One metric\n+        value is generated for each threshold value.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n     \"\"\"\n@@ -497,9 +498,9 @@ class _ConfusionMatrixConditionCount(base_metric.Metric):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -779,29 +780,30 @@ class TruePositives(_ConfusionMatrixConditionCount):\n class Precision(base_metric.Metric):\n     \"\"\"Computes the precision of the predictions with respect to the labels.\n \n-    The metric creates two local variables, `true_positives` and `false_positives`\n-    that are used to compute the precision. This value is ultimately returned as\n-    `precision`, an idempotent operation that simply divides `true_positives`\n-    by the sum of `true_positives` and `false_positives`.\n+    The metric creates two local variables, `true_positives` and\n+    `false_positives` that are used to compute the precision. This value is\n+    ultimately returned as `precision`, an idempotent operation that simply\n+    divides `true_positives` by the sum of `true_positives` and\n+    `false_positives`.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n \n     If `top_k` is set, we'll calculate precision as how often on average a class\n-    among the top-k classes with the highest predicted values of a batch entry is\n-    correct and can be found in the label for that entry.\n+    among the top-k classes with the highest predicted values of a batch entry\n+    is correct and can be found in the label for that entry.\n \n     If `class_id` is specified, we calculate precision by considering only the\n-    entries in the batch for which `class_id` is above the threshold and/or in the\n-    top-k highest predictions, and computing the fraction of them for which\n+    entries in the batch for which `class_id` is above the threshold and/or in\n+    the top-k highest predictions, and computing the fraction of them for which\n     `class_id` is indeed a correct label.\n \n     Args:\n       thresholds: (Optional) A float value, or a Python list/tuple of float\n         threshold values in [0, 1]. A threshold is compared with prediction\n         values to determine the truth value of predictions (i.e., above the\n-        threshold is `true`, below is `false`). If used with a loss function that\n-        sets `from_logits=True` (i.e. no sigmoid applied to predictions),\n+        threshold is `true`, below is `false`). If used with a loss function\n+        that sets `from_logits=True` (i.e. no sigmoid applied to predictions),\n         `thresholds` should be set to 0. One metric value is generated for each\n         threshold value. If neither thresholds nor top_k are set, the default is\n         to calculate precision with `thresholds=0.5`.\n@@ -825,13 +827,15 @@ class Precision(base_metric.Metric):\n     >>> m.result().numpy()\n     1.0\n \n-    >>> # With top_k=2, it will calculate precision over y_true[:2] and y_pred[:2]\n+    >>> # With top_k=2, it will calculate precision over y_true[:2]\n+    >>> # and y_pred[:2]\n     >>> m = tf.keras.metrics.Precision(top_k=2)\n     >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n     >>> m.result().numpy()\n     0.0\n \n-    >>> # With top_k=4, it will calculate precision over y_true[:4] and y_pred[:4]\n+    >>> # With top_k=4, it will calculate precision over y_true[:4]\n+    >>> # and y_pred[:4]\n     >>> m = tf.keras.metrics.Precision(top_k=4)\n     >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n     >>> m.result().numpy()\n@@ -885,10 +889,11 @@ class Precision(base_metric.Metric):\n         Args:\n           y_true: The ground truth values, with the same dimensions as `y_pred`.\n             Will be cast to `bool`.\n-          y_pred: The predicted values. Each element must be in the range `[0, 1]`.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          y_pred: The predicted values. Each element must be in the range\n+            `[0, 1]`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -957,8 +962,8 @@ class Recall(base_metric.Metric):\n       thresholds: (Optional) A float value, or a Python list/tuple of float\n         threshold values in [0, 1]. A threshold is compared with prediction\n         values to determine the truth value of predictions (i.e., above the\n-        threshold is `true`, below is `false`). If used with a loss function that\n-        sets `from_logits=True` (i.e. no sigmoid applied to predictions),\n+        threshold is `true`, below is `false`). If used with a loss function\n+        that sets `from_logits=True` (i.e. no sigmoid applied to predictions),\n         `thresholds` should be set to 0. One metric value is generated for each\n         threshold value. If neither thresholds nor top_k are set, the default is\n         to calculate recall with `thresholds=0.5`.\n@@ -1030,10 +1035,11 @@ class Recall(base_metric.Metric):\n         Args:\n           y_true: The ground truth values, with the same dimensions as `y_pred`.\n             Will be cast to `bool`.\n-          y_pred: The predicted values. Each element must be in the range `[0, 1]`.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          y_pred: The predicted values. Each element must be in the range\n+            `[0, 1]`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -1127,9 +1133,9 @@ class SensitivitySpecificityBase(base_metric.Metric, metaclass=abc.ABCMeta):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -1170,7 +1176,8 @@ class SensitivitySpecificityBase(base_metric.Metric, metaclass=abc.ABCMeta):\n         return dict(list(base_config.items()) + list(config.items()))\n \n     def _find_max_under_constraint(self, constrained, dependent, predicate):\n-        \"\"\"Returns the maximum of dependent_statistic that satisfies the constraint.\n+        \"\"\"Returns the maximum of dependent_statistic that satisfies the\n+        constraint.\n \n         Args:\n           constrained: Over these values the constraint\n@@ -1182,7 +1189,8 @@ class SensitivitySpecificityBase(base_metric.Metric, metaclass=abc.ABCMeta):\n           predicate: A binary boolean functor to be applied to arguments\n           `constrained` and `self.value`, e.g. `tf.greater`.\n \n-        Returns maximal dependent value, if no value satiesfies the constraint 0.0.\n+        Returns:\n+          maximal dependent value, if no value satiesfies the constraint 0.0.\n         \"\"\"\n         feasible = tf.where(predicate(constrained, self.value))\n         feasible_exists = tf.greater(tf.size(feasible), 0)\n@@ -1202,18 +1210,19 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n     `Specificity` measures the proportion of actual negatives that are correctly\n     identified as such (tn / (tn + fp)).\n \n-    This metric creates four local variables, `true_positives`, `true_negatives`,\n-    `false_positives` and `false_negatives` that are used to compute the\n-    sensitivity at the given specificity. The threshold for the given specificity\n-    value is computed and used to evaluate the corresponding sensitivity.\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the sensitivity at the given specificity. The threshold for the\n+    given specificity value is computed and used to evaluate the corresponding\n+    sensitivity.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n \n     If `class_id` is specified, we calculate precision by considering only the\n-    entries in the batch for which `class_id` is above the threshold predictions,\n-    and computing the fraction of them for which `class_id` is indeed a correct\n-    label.\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n \n     For additional information about specificity and sensitivity, see\n     [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n@@ -1306,18 +1315,19 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n     `Specificity` measures the proportion of actual negatives that are correctly\n     identified as such (tn / (tn + fp)).\n \n-    This metric creates four local variables, `true_positives`, `true_negatives`,\n-    `false_positives` and `false_negatives` that are used to compute the\n-    specificity at the given sensitivity. The threshold for the given sensitivity\n-    value is computed and used to evaluate the corresponding specificity.\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the specificity at the given sensitivity. The threshold for the\n+    given sensitivity value is computed and used to evaluate the corresponding\n+    specificity.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n \n     If `class_id` is specified, we calculate precision by considering only the\n-    entries in the batch for which `class_id` is above the threshold predictions,\n-    and computing the fraction of them for which `class_id` is indeed a correct\n-    label.\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n \n     For additional information about specificity and sensitivity, see\n     [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n@@ -1405,18 +1415,18 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n class PrecisionAtRecall(SensitivitySpecificityBase):\n     \"\"\"Computes best precision where recall is >= specified value.\n \n-    This metric creates four local variables, `true_positives`, `true_negatives`,\n-    `false_positives` and `false_negatives` that are used to compute the\n-    precision at the given recall. The threshold for the given recall\n-    value is computed and used to evaluate the corresponding precision.\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the precision at the given recall. The threshold for the given\n+    recall value is computed and used to evaluate the corresponding precision.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n \n     If `class_id` is specified, we calculate precision by considering only the\n-    entries in the batch for which `class_id` is above the threshold predictions,\n-    and computing the fraction of them for which `class_id` is indeed a correct\n-    label.\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n \n     Args:\n       recall: A scalar value in range `[0, 1]`.\n@@ -1496,18 +1506,18 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n     For a given score-label-distribution the required precision might not\n     be achievable, in this case 0.0 is returned as recall.\n \n-    This metric creates four local variables, `true_positives`, `true_negatives`,\n-    `false_positives` and `false_negatives` that are used to compute the\n-    recall at the given precision. The threshold for the given precision\n-    value is computed and used to evaluate the corresponding recall.\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the recall at the given precision. The threshold for the given\n+    precision value is computed and used to evaluate the corresponding recall.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n \n     If `class_id` is specified, we calculate precision by considering only the\n-    entries in the batch for which `class_id` is above the threshold predictions,\n-    and computing the fraction of them for which `class_id` is indeed a correct\n-    label.\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n \n     Args:\n       precision: A scalar value in range `[0, 1]`.\n@@ -1593,36 +1603,38 @@ class AUC(base_metric.Metric):\n     \"\"\"Approximates the AUC (Area under the curve) of the ROC or PR curves.\n \n     The AUC (Area under the curve) of the ROC (Receiver operating\n-    characteristic; default) or PR (Precision Recall) curves are quality measures\n-    of binary classifiers. Unlike the accuracy, and like cross-entropy\n+    characteristic; default) or PR (Precision Recall) curves are quality\n+    measures of binary classifiers. Unlike the accuracy, and like cross-entropy\n     losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.\n \n     This class approximates AUCs using a Riemann sum. During the metric\n     accumulation phrase, predictions are accumulated within predefined buckets\n-    by value. The AUC is then computed by interpolating per-bucket averages. These\n-    buckets define the evaluated operational points.\n+    by value. The AUC is then computed by interpolating per-bucket averages.\n+    These buckets define the evaluated operational points.\n \n-    This metric creates four local variables, `true_positives`, `true_negatives`,\n-    `false_positives` and `false_negatives` that are used to compute the AUC.\n-    To discretize the AUC curve, a linearly spaced set of thresholds is used to\n-    compute pairs of recall and precision values. The area under the ROC-curve is\n-    therefore computed using the height of the recall values by the false positive\n-    rate, while the area under the PR-curve is the computed using the height of\n-    the precision values by the recall.\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the AUC.  To discretize the AUC curve, a linearly spaced set of\n+    thresholds is used to compute pairs of recall and precision values. The area\n+    under the ROC-curve is therefore computed using the height of the recall\n+    values by the false positive rate, while the area under the PR-curve is the\n+    computed using the height of the precision values by the recall.\n \n     This value is ultimately returned as `auc`, an idempotent operation that\n-    computes the area under a discretized curve of precision versus recall values\n-    (computed using the aforementioned variables). The `num_thresholds` variable\n-    controls the degree of discretization with larger numbers of thresholds more\n-    closely approximating the true AUC. The quality of the approximation may vary\n-    dramatically depending on `num_thresholds`. The `thresholds` parameter can be\n-    used to manually specify thresholds which split the predictions more evenly.\n+    computes the area under a discretized curve of precision versus recall\n+    values (computed using the aforementioned variables). The `num_thresholds`\n+    variable controls the degree of discretization with larger numbers of\n+    thresholds more closely approximating the true AUC. The quality of the\n+    approximation may vary dramatically depending on `num_thresholds`. The\n+    `thresholds` parameter can be used to manually specify thresholds which\n+    split the predictions more evenly.\n \n-    For a best approximation of the real AUC, `predictions` should be distributed\n-    approximately uniformly in the range [0, 1] (if `from_logits=False`). The\n-    quality of the AUC approximation may be poor if this is not the case. Setting\n-    `summation_method` to 'minoring' or 'majoring' can help quantify the error in\n-    the approximation by providing lower or upper bound estimate of the AUC.\n+    For a best approximation of the real AUC, `predictions` should be\n+    distributed approximately uniformly in the range [0, 1] (if\n+    `from_logits=False`). The quality of the AUC approximation may be poor if\n+    this is not the case. Setting `summation_method` to 'minoring' or 'majoring'\n+    can help quantify the error in the approximation by providing lower or upper\n+    bound estimate of the AUC.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n@@ -1634,12 +1646,11 @@ class AUC(base_metric.Metric):\n         [default] or 'PR' for the Precision-Recall-curve.\n       summation_method: (Optional) Specifies the [Riemann summation method](\n           https://en.wikipedia.org/wiki/Riemann_sum) used.\n-          'interpolation' (default) applies mid-point summation scheme for `ROC`.\n-          For PR-AUC, interpolates (true/false) positives but not the ratio that\n-          is precision (see Davis & Goadrich 2006 for details);\n-          'minoring' applies left summation\n-          for increasing intervals and right summation for decreasing intervals;\n-          'majoring' does the opposite.\n+          'interpolation' (default) applies mid-point summation scheme for\n+          `ROC`.  For PR-AUC, interpolates (true/false) positives but not the\n+          ratio that is precision (see Davis & Goadrich 2006 for details);\n+          'minoring' applies left summation for increasing intervals and right\n+          summation for decreasing intervals; 'majoring' does the opposite.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n       thresholds: (Optional) A list of floating point values to use as the\n@@ -1731,7 +1742,8 @@ class AUC(base_metric.Metric):\n             summation_method, metrics_utils.AUCSummationMethod\n         ) and summation_method not in list(metrics_utils.AUCSummationMethod):\n             raise ValueError(\n-                f'Invalid `summation_method` argument value \"{summation_method}\". '\n+                f\"Invalid `summation_method` \"\n+                f'argument value \"{summation_method}\". '\n                 f\"Expected one of: {list(metrics_utils.AUCSummationMethod)}\"\n             )\n \n@@ -1813,7 +1825,8 @@ class AUC(base_metric.Metric):\n         return list(self._thresholds)\n \n     def _build(self, shape):\n-        \"\"\"Initialize TP, FP, TN, and FN tensors, given the shape of the data.\"\"\"\n+        \"\"\"Initialize TP, FP, TN, and FN tensors, given the shape of the\n+        data.\"\"\"\n         if self.multi_label:\n             if shape.ndims != 2:\n                 raise ValueError(\n@@ -1845,9 +1858,9 @@ class AUC(base_metric.Metric):\n \n         if self.multi_label:\n             with tf.init_scope():\n-                # This should only be necessary for handling v1 behavior. In v2, AUC\n-                # should be initialized outside of any tf.functions, and therefore in\n-                # eager mode.\n+                # This should only be necessary for handling v1 behavior. In v2,\n+                # AUC should be initialized outside of any tf.functions, and\n+                # therefore in eager mode.\n                 if not tf.executing_eagerly():\n                     backend._initialize_variables(\n                         backend._get_session()\n@@ -1861,9 +1874,9 @@ class AUC(base_metric.Metric):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -1886,15 +1899,16 @@ class AUC(base_metric.Metric):\n                     ]\n                 )\n             if self.label_weights is not None:\n-                # label_weights should be of length equal to the number of labels.\n+                # label_weights should be of length equal to the number of\n+                # labels.\n                 shapes.append((self.label_weights, (\"L\",)))\n                 tf.debugging.assert_shapes(\n                     shapes, message=\"Number of labels is not consistent.\"\n                 )\n \n         # Only forward label_weights to update_confusion_matrix_variables when\n-        # multi_label is False. Otherwise the averaging of individual label AUCs is\n-        # handled in AUC.result\n+        # multi_label is False. Otherwise the averaging of individual label AUCs\n+        # is handled in AUC.result\n         label_weights = None if self.multi_label else self.label_weights\n \n         if self._from_logits:\n@@ -1927,8 +1941,8 @@ class AUC(base_metric.Metric):\n           Precision = TP / (TP + FP) = TP / P\n \n         Modeling all of TP (true positive), FP (false positive) and their sum\n-        P = TP + FP (predicted positive) as varying linearly within each interval\n-        [A, B] between successive thresholds, we get\n+        P = TP + FP (predicted positive) as varying linearly within each\n+        interval [A, B] between successive thresholds, we get\n \n           Precision slope = dTP / dP\n                           = (TP_B - TP_A) / (P_B - P_A)\n@@ -1944,7 +1958,8 @@ class AUC(base_metric.Metric):\n \n           int_A^B{Precision.dP} = TP_B - TP_A + intercept * log(P_B / P_A)\n \n-        Bringing back the factor (slope / total_pos_weight) we'd put aside, we get\n+        Bringing back the factor (slope / total_pos_weight) we'd put aside, we\n+        get\n \n           slope * [dTP + intercept *  log(P_B / P_A)] / total_pos_weight\n \n@@ -2044,7 +2059,8 @@ class AUC(base_metric.Metric):\n             heights = (y[: self.num_thresholds - 1] + y[1:]) / 2.0\n         elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING:\n             heights = tf.minimum(y[: self.num_thresholds - 1], y[1:])\n-        else:  # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:\n+        # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:\n+        else:\n             heights = tf.maximum(y[: self.num_thresholds - 1], y[1:])\n \n         # Sum up the areas of all the rectangles.\n@@ -2109,11 +2125,12 @@ class AUC(base_metric.Metric):\n             \"multi_label\": self.multi_label,\n             \"label_weights\": label_weights,\n         }\n-        # optimization to avoid serializing a large number of generated thresholds\n+        # optimization to avoid serializing a large number of generated\n+        # thresholds\n         if self._init_from_thresholds:\n-            # We remove the endpoint thresholds as an inverse of how the thresholds\n-            # were initialized. This ensures that a metric initialized from this\n-            # config has the same thresholds.\n+            # We remove the endpoint thresholds as an inverse of how the\n+            # thresholds were initialized. This ensures that a metric\n+            # initialized from this config has the same thresholds.\n             config[\"thresholds\"] = self.thresholds[1:-1]\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -2207,7 +2224,8 @@ class MeanAbsoluteError(base_metric.MeanMetricWrapper):\n \n @keras_export(\"keras.metrics.MeanAbsolutePercentageError\")\n class MeanAbsolutePercentageError(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the mean absolute percentage error between `y_true` and `y_pred`.\n+    \"\"\"Computes the mean absolute percentage error between `y_true` and\n+    `y_pred`.\n \n     Args:\n       name: (Optional) string name of the metric instance.\n@@ -2279,7 +2297,8 @@ class MeanSquaredError(base_metric.MeanMetricWrapper):\n \n @keras_export(\"keras.metrics.MeanSquaredLogarithmicError\")\n class MeanSquaredLogarithmicError(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n+    \"\"\"Computes the mean squared logarithmic error between `y_true` and\n+    `y_pred`.\n \n     Args:\n       name: (Optional) string name of the metric instance.\n@@ -2340,7 +2359,8 @@ class Hinge(base_metric.MeanMetricWrapper):\n     Usage with `compile()` API:\n \n     ```python\n-    model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Hinge()])\n+    model.compile(\n+        optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Hinge()])\n     ```\n     \"\"\"\n \n@@ -2461,9 +2481,9 @@ class RootMeanSquaredError(base_metric.Mean):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -2484,7 +2504,8 @@ class RootMeanSquaredError(base_metric.Mean):\n class LogCoshError(base_metric.MeanMetricWrapper):\n     \"\"\"Computes the logarithm of the hyperbolic cosine of the prediction error.\n \n-    `logcosh = log((exp(x) + exp(-x))/2)`, where x is the error (y_pred - y_true)\n+    `logcosh = log((exp(x) + exp(-x))/2)`, where x is the error (y_pred -\n+    y_true)\n \n     Args:\n       name: (Optional) string name of the metric instance.\n@@ -2556,7 +2577,8 @@ class Poisson(base_metric.MeanMetricWrapper):\n \n @keras_export(\"keras.metrics.KLDivergence\")\n class KLDivergence(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes Kullback-Leibler divergence metric between `y_true` and `y_pred`.\n+    \"\"\"Computes Kullback-Leibler divergence metric between `y_true` and\n+    `y_pred`.\n \n     `metric = y_true * log(y_true / y_pred)`\n \n@@ -2637,9 +2659,9 @@ class _IoUBase(base_metric.Metric):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -2697,10 +2719,10 @@ class IoU(_IoUBase):\n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n \n-    Note, this class first computes IoUs for all individual classes, then returns\n-    the mean of IoUs for the classes that are specified by `target_class_ids`. If\n-    `target_class_ids` has only one id value, the IoU of that specific class is\n-    returned.\n+    Note, this class first computes IoUs for all individual classes, then\n+    returns the mean of IoUs for the classes that are specified by\n+    `target_class_ids`. If `target_class_ids` has only one id value, the IoU of\n+    that specific class is returned.\n \n     Args:\n       num_classes: The possible number of labels the prediction task can have.\n@@ -2729,7 +2751,8 @@ class IoU(_IoUBase):\n     ...                sample_weight=[0.3, 0.3, 0.3, 0.1])\n     >>> # cm = [[0.3, 0.3],\n     >>> #        [0.3, 0.1]]\n-    >>> # sum_row = [0.6, 0.4], sum_col = [0.6, 0.4], true_positives = [0.3, 0.1]\n+    >>> # sum_row = [0.6, 0.4], sum_col = [0.6, 0.4],\n+    >>> # true_positives = [0.3, 0.1]\n     >>> # iou = [0.33, 0.14]\n     >>> m.result().numpy()\n     0.33333334\n@@ -2759,7 +2782,8 @@ class IoU(_IoUBase):\n         )\n         if max(target_class_ids) >= num_classes:\n             raise ValueError(\n-                f\"Target class id {max(target_class_ids)} is out of range, which is \"\n+                f\"Target class id {max(target_class_ids)} \"\n+                f\"is out of range, which is \"\n                 f\"[{0}, {num_classes}).\"\n             )\n         self.target_class_ids = list(target_class_ids)\n@@ -2825,10 +2849,11 @@ class BinaryIoU(IoU):\n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n \n-    This class can be used to compute IoUs for a binary classification task where\n-    the predictions are provided as logits. First a `threshold` is applied to the\n-    predicted values such that those that are below the `threshold` are converted\n-    to class 0 and those that are above the `threshold` are converted to class 1.\n+    This class can be used to compute IoUs for a binary classification task\n+    where the predictions are provided as logits. First a `threshold` is applied\n+    to the predicted values such that those that are below the `threshold` are\n+    converted to class 0 and those that are above the `threshold` are converted\n+    to class 1.\n \n     IoUs for classes 0 and 1 are then computed, the mean of IoUs for the classes\n     that are specified by `target_class_ids` is returned.\n@@ -2837,12 +2862,13 @@ class BinaryIoU(IoU):\n \n     Args:\n       target_class_ids: A tuple or list of target class ids for which the metric\n-        is returned. Options are `[0]`, `[1]`, or `[0, 1]`. With `[0]` (or `[1]`),\n-        the IoU metric for class 0 (or class 1, respectively) is returned. With\n-        `[0, 1]`, the mean of IoUs for the two classes is returned.\n-      threshold: A threshold that applies to the prediction logits to convert them\n-        to either predicted class 0 if the logit is below `threshold` or predicted\n-        class 1 if the logit is above `threshold`.\n+        is returned. Options are `[0]`, `[1]`, or `[0, 1]`. With `[0]` (or\n+        `[1]`), the IoU metric for class 0 (or class 1, respectively) is\n+        returned. With `[0, 1]`, the mean of IoUs for the two classes is\n+        returned.\n+      threshold: A threshold that applies to the prediction logits to convert\n+        them to either predicted class 0 if the logit is below `threshold` or\n+        predicted class 1 if the logit is above `threshold`.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n \n@@ -2858,7 +2884,8 @@ class BinaryIoU(IoU):\n     ...                sample_weight=[0.2, 0.3, 0.4, 0.1])\n     >>> # cm = [[0.2, 0.4],\n     >>> #        [0.3, 0.1]]\n-    >>> # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n+    >>> # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5],\n+    >>> # true_positives = [0.2, 0.1]\n     >>> # iou = [0.222, 0.125]\n     >>> m.result().numpy()\n     0.17361112\n@@ -2893,17 +2920,17 @@ class BinaryIoU(IoU):\n     def update_state(self, y_true, y_pred, sample_weight=None):\n         \"\"\"Accumulates the confusion matrix statistics.\n \n-        Before the confusion matrix is updated, the predicted values are thresholded\n-        to be:\n+        Before the confusion matrix is updated, the predicted values are\n+        thresholded to be:\n           0 for values that are smaller than the `threshold`\n           1 for values that are larger or equal to the `threshold`\n \n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -3020,11 +3047,11 @@ class OneHotIoU(IoU):\n     Use `sample_weight` of 0 to mask values.\n \n     This class can be used to compute IoU for multi-class classification tasks\n-    where the labels are one-hot encoded (the last axis should have one dimension\n-    per class). Note that the predictions should also have the same shape. To\n-    compute the IoU, first the labels and predictions are converted back into\n-    integer format by taking the argmax over the class axis. Then the same\n-    computation steps as for the base `IoU` class apply.\n+    where the labels are one-hot encoded (the last axis should have one\n+    dimension per class). Note that the predictions should also have the same\n+    shape. To compute the IoU, first the labels and predictions are converted\n+    back into integer format by taking the argmax over the class axis. Then the\n+    same computation steps as for the base `IoU` class apply.\n \n     Note, if there is only one channel in the labels and predictions, this class\n     is the same as class `IoU`. In this case, use `IoU` instead.\n@@ -3050,7 +3077,8 @@ class OneHotIoU(IoU):\n     ...                       [0.1, 0.4, 0.5]])\n     >>> sample_weight = [0.1, 0.2, 0.3, 0.4]\n     >>> m = tf.keras.metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n-    >>> m.update_state(y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n+    >>> m.update_state(\n+    ...     y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n     >>> # cm = [[0, 0, 0.2+0.4],\n     >>> #       [0.3, 0, 0],\n     >>> #       [0, 0, 0.1]]\n@@ -3092,9 +3120,9 @@ class OneHotIoU(IoU):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -3127,12 +3155,13 @@ class OneHotMeanIoU(MeanIoU):\n     If `sample_weight` is `None`, weights default to 1.\n     Use `sample_weight` of 0 to mask values.\n \n-    This class can be used to compute the mean IoU for multi-class classification\n-    tasks where the labels are one-hot encoded (the last axis should have one\n-    dimension per class). Note that the predictions should also have the same\n-    shape. To compute the mean IoU, first the labels and predictions are converted\n-    back into integer format by taking the argmax over the class axis. Then the\n-    same computation steps as for the base `MeanIoU` class apply.\n+    This class can be used to compute the mean IoU for multi-class\n+    classification tasks where the labels are one-hot encoded (the last axis\n+    should have one dimension per class). Note that the predictions should also\n+    have the same shape. To compute the mean IoU, first the labels and\n+    predictions are converted back into integer format by taking the argmax over\n+    the class axis. Then the same computation steps as for the base `MeanIoU`\n+    class apply.\n \n     Note, if there is only one channel in the labels and predictions, this class\n     is the same as class `MeanIoU`. In this case, use `MeanIoU` instead.\n@@ -3155,7 +3184,8 @@ class OneHotMeanIoU(MeanIoU):\n     ...                       [0.1, 0.4, 0.5]])\n     >>> sample_weight = [0.1, 0.2, 0.3, 0.4]\n     >>> m = tf.keras.metrics.OneHotMeanIoU(num_classes=3)\n-    >>> m.update_state(y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n+    >>> m.update_state(\n+    ...     y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n     >>> # cm = [[0, 0, 0.2+0.4],\n     >>> #       [0.3, 0, 0],\n     >>> #       [0, 0, 0.1]]\n@@ -3195,9 +3225,9 @@ class OneHotMeanIoU(MeanIoU):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n-            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n-            be broadcastable to `y_true`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n \n         Returns:\n           Update op.\n@@ -3271,8 +3301,8 @@ class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n     \"\"\"Computes the crossentropy metric between the labels and predictions.\n \n     This is the crossentropy metric class to be used when there are multiple\n-    label classes (2 or more). Here we assume that labels are given as a `one_hot`\n-    representation. eg., When labels values are [2, 0, 1],\n+    label classes (2 or more). Here we assume that labels are given as a\n+    `one_hot` representation. eg., When labels values are [2, 0, 1],\n      `y_true` = [[0, 0, 1], [1, 0, 0], [0, 1, 0]].\n \n     Args:\n@@ -3443,15 +3473,16 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n     Args:\n       y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n       y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n-      threshold: (Optional) Float representing the threshold for deciding whether\n-        prediction values are 1 or 0.\n+      threshold: (Optional) Float representing the threshold for deciding\n+        whether prediction values are 1 or 0.\n \n     Returns:\n       Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`\n     \"\"\"\n-    # Note: calls metrics_utils.binary_matches with mean reduction. This maintains\n-    # public facing binary_accuracy behavior and seperates it from the vital\n-    # behavior of the binary_matches method needed in backend dependencies.\n+    # Note: calls metrics_utils.binary_matches with mean reduction. This\n+    # maintains public facing binary_accuracy behavior and seperates it from the\n+    # vital behavior of the binary_matches method needed in backend\n+    # dependencies.\n \n     return tf.reduce_mean(\n         metrics_utils.binary_matches(y_true, y_pred, threshold), axis=-1\n@@ -3481,8 +3512,8 @@ def categorical_accuracy(y_true, y_pred):\n     Returns:\n       Categorical accuracy values.\n     \"\"\"\n-    # Note: wraps metrics_utils.categorical_matches. This seperates public facing\n-    # categorical_accuracy behavior from the vital behavior of the\n+    # Note: wraps metrics_utils.categorical_matches. This seperates public\n+    # facing categorical_accuracy behavior from the vital behavior of the\n     # categorical_matches method needed in backend dependencies.\n \n     return metrics_utils.sparse_categorical_matches(\n\n@@ -118,7 +118,8 @@ class TestMetricsCorrectnessMultiIO(test_combinations.TestCase):\n         #   Result = 56\n \n         # Loss `output_1` without weights/Metric `output_1`:\n-        #   Total = ((3 - 2)^2 + (6 - 4)^2) + ((9 - 6)^2 + (12 - 8)^2) + (15 - 10)^2\n+        #   Total = ((3 - 2)^2 + (6 - 4)^2) + ((9 - 6)^2 + \\\n+        #           (12 - 8)^2) + (15 - 10)^2\n         #         = 55\n         #   Count = 2 + 2 + 1\n         #   Result = 11\n@@ -132,7 +133,8 @@ class TestMetricsCorrectnessMultiIO(test_combinations.TestCase):\n         #   Result = 88\n \n         # Loss `output_2` without weights/Metric `output_2`:\n-        #   Total = ((3 - 1)^2 + (6 - 2)^2) + ((9 - 3)^2 + (12 - 4)^2) + (15 - 5)^2\n+        #   Total = ((3 - 1)^2 + (6 - 2)^2) + ((9 - 3)^2 + \\\n+        #           (12 - 4)^2) + (15 - 5)^2\n         #         = 220\n         #   Count = 2 + 2 + 1\n         #   Result = 44\n@@ -266,7 +268,8 @@ class TestMetricsCorrectnessMultiIO(test_combinations.TestCase):\n             eval_result, self.expected_batch_result_with_weights_output_2, 1e-3\n         )\n \n-        # Verify that metric value is same with arbitrary weights and batch size.\n+        # Verify that metric value is same with arbitrary weights and batch\n+        # size.\n         x = np.random.random((50, 1))\n         y = np.random.random((50, 1))\n         w = np.random.random((50,))\n@@ -545,7 +548,8 @@ class TestMetricsCorrectnessSingleIO(test_combinations.TestCase):\n             eval_result, self.expected_batch_result_with_weights, 1e-3\n         )\n \n-        # Verify that metric value is same with arbitrary weights and batch size.\n+        # Verify that metric value is same with arbitrary weights and batch\n+        # size.\n         x = np.random.random((50, 1))\n         y = np.random.random((50, 1))\n         w = np.random.random((50,))\n@@ -686,9 +690,9 @@ class TestOutputLossMetrics(test_combinations.TestCase):\n         #   Result (reduction=SUM) = ((14 + 40)*2 + (54 + 32)*2 + 300) / 5 = 116\n         #   Result (reduction=SUM_OVER_BATCH_SIZE/AUTO/NONE) = 440 / 5 = 88\n \n-        # When reduction is 'NONE' loss value that is passed to the optimizer will\n-        # be vector loss but what is reported is a scalar, which is an average of\n-        # all the values in all the batch vectors.\n+        # When reduction is 'NONE' loss value that is passed to the optimizer\n+        # will be vector loss but what is reported is a scalar, which is an\n+        # average of all the values in all the batch vectors.\n \n         # Total loss = Output_loss_1 + Output_loss_2\n \n\n@@ -61,8 +61,8 @@ class KerasFunctionalMetricsTest(tf.test.TestCase, parameterized.TestCase):\n                 backend.eval(metric(y_true, y_pred)), [0.0, 1.0, 1.0, 1.0]\n             )\n \n-            # Test correctness if the shape of y_true is (batch_size, seq_length) and\n-            # y_pred is (batch_size, seq_length, num_classes)\n+            # Test correctness if the shape of y_true is (batch_size,\n+            # seq_length) and y_pred is (batch_size, seq_length, num_classes)\n             y_pred = backend.variable(\n                 np.array(\n                     [\n@@ -85,7 +85,8 @@ class KerasFunctionalMetricsTest(tf.test.TestCase, parameterized.TestCase):\n \n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def test_sparse_categorical_accuracy_eager(self):\n-        \"\"\"Tests that ints passed in via Eager return results. See b/113504761.\"\"\"\n+        \"\"\"Tests that ints passed in via Eager return results. See\n+        b/113504761.\"\"\"\n         metric = metrics.sparse_categorical_accuracy\n         y_true = np.arange(6).reshape([6, 1])\n         y_pred = np.arange(36).reshape([6, 6])\n@@ -95,7 +96,8 @@ class KerasFunctionalMetricsTest(tf.test.TestCase, parameterized.TestCase):\n \n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def test_sparse_categorical_accuracy_float_eager(self):\n-        \"\"\"Tests that floats passed in via Eager return results. See b/113504761.\"\"\"\n+        \"\"\"Tests that floats passed in via Eager return results. See\n+        b/113504761.\"\"\"\n         metric = metrics.sparse_categorical_accuracy\n         y_true = np.arange(6, dtype=np.float32).reshape([6, 1])\n         y_pred = np.arange(36).reshape([6, 6])\n@@ -141,8 +143,8 @@ class KerasFunctionalMetricsTest(tf.test.TestCase, parameterized.TestCase):\n             )\n             self.assertEqual(np.mean(result), 0.0)\n \n-            # Test correctness if the shape of y_true is (batch_size, seq_length) and\n-            # y_pred is (batch_size, seq_length, num_classes)\n+            # Test correctness if the shape of y_true is (batch_size,\n+            # seq_length) and y_pred is (batch_size, seq_length, num_classes)\n             y_pred = backend.variable(\n                 np.array(\n                     [\n\n@@ -598,10 +598,12 @@ class SquaredHingeTest(tf.test.TestCase):\n         # y_true = [[-1, 1, -1, 1], [-1, -1, 1, 1]]\n         # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n         # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n-        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5, 0.4]]\n+        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5,\n+        # 0.4]]\n         # squared(max(0, 1 - y_true * y_pred)) = [[0.49, 0.64, 0.81, 0],\n         #                                         [0.5625, 0, 0.25, 0.16]]\n-        # metric = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) / 4]\n+        # metric = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) /\n+        # 4]\n         #        = [0.485, 0.2431]\n         # reduced metric = (0.485 + 0.2431) / 2\n \n@@ -623,10 +625,12 @@ class SquaredHingeTest(tf.test.TestCase):\n \n         # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n         # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n-        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5, 0.4]]\n+        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5,\n+        # 0.4]]\n         # squared(max(0, 1 - y_true * y_pred)) = [[0.49, 0.64, 0.81, 0],\n         #                                         [0.5625, 0, 0.25, 0.16]]\n-        # metric = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) / 4]\n+        # metric = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) /\n+        # 4]\n         #        = [0.485, 0.2431]\n         # weighted metric = [0.485 * 1.5, 0.2431 * 2]\n         # reduced metric = (0.485 * 1.5 + 0.2431 * 2) / (1.5 + 2)\n@@ -816,8 +820,9 @@ class SparseTopKCategoricalAccuracyTest(tf.test.TestCase):\n     def test_sparse_top_k_categorical_accuracy_mismatched_dims_dynamic(self):\n \n         if not tf.compat.v1.executing_eagerly():\n-            # Test will fail in v1 graph mode since the metric is not a normal layer.\n-            # It will aggregate the output by batch dim, which failed on v1 code.\n+            # Test will fail in v1 graph mode since the metric is not a normal\n+            # layer.  It will aggregate the output by batch dim, which failed on\n+            # v1 code.\n             self.skipTest(\"v2 eager mode only\")\n \n         class AccLayer(layers.Layer):\n@@ -1085,7 +1090,8 @@ class IoUTest(tf.test.TestCase):\n \n         # cm = [[0.2, 0.3],\n         #       [0.4, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n+        # 0.1]\n         # iou = true_positives / (sum_row + sum_col - true_positives))\n         expected_result = (\n             0.1 / (0.4 + 0.5 - 0.1) + 0.2 / (0.6 + 0.5 - 0.2)\n@@ -1104,7 +1110,8 @@ class IoUTest(tf.test.TestCase):\n \n         # cm = [[0.2, 0.3],\n         #       [0.4, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n+        # 0.1]\n         # iou = true_positives / (sum_row + sum_col - true_positives))\n         expected_result = (\n             0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n@@ -1155,7 +1162,8 @@ class BinaryIoUTest(tf.test.TestCase):\n         # with threshold = 0.3, y_pred will be converted to [0, 0, 1, 1]\n         # cm = [[0.2, 0.4],\n         #       [0.3, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n+        # 0.1]\n         # iou = true_positives / (sum_row + sum_col - true_positives))\n         expected_result = (\n             0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n@@ -1169,7 +1177,8 @@ class BinaryIoUTest(tf.test.TestCase):\n         # with threshold = 0.5, y_pred will be converted to [0, 0, 0, 1]\n         # cm = [[0.1+0.4, 0],\n         #       [0.2, 0.3]]\n-        # sum_row = [0.5, 0.5], sum_col = [0.7, 0.3], true_positives = [0.5, 0.3]\n+        # sum_row = [0.5, 0.5], sum_col = [0.7, 0.3], true_positives = [0.5,\n+        # 0.3]\n         # iou = true_positives / (sum_row + sum_col - true_positives))\n         expected_result = (\n             0.5 / (0.5 + 0.7 - 0.5) + 0.3 / (0.5 + 0.3 - 0.3)\n@@ -1212,7 +1221,8 @@ class BinaryIoUTest(tf.test.TestCase):\n         sample_weight = tf.constant([[0.2, 0.3], [0.4, 0.1]])\n         # cm = [[0.2, 0.4],\n         #       [0.1, 0.3]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.3, 0.7], true_positives = [0.2, 0.3]\n+        # sum_row = [0.6, 0.4], sum_col = [0.3, 0.7], true_positives = [0.2,\n+        # 0.3]\n         # iou = true_positives / (sum_row + sum_col - true_positives))\n         expected_result = (\n             0.2 / (0.6 + 0.3 - 0.2) + 0.3 / (0.4 + 0.7 - 0.3)\n@@ -1283,7 +1293,8 @@ class MeanIoUTest(tf.test.TestCase):\n \n         # cm = [[0.2, 0.3],\n         #       [0.4, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n+        # 0.1]\n         # iou = true_positives / (sum_row + sum_col - true_positives))\n         expected_result = (\n             0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n@@ -1302,7 +1313,8 @@ class MeanIoUTest(tf.test.TestCase):\n \n         # cm = [[0.2, 0.3],\n         #       [0.4, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n+        # 0.1]\n         # iou = true_positives / (sum_row + sum_col - true_positives))\n         expected_result = (\n             0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n@@ -1946,7 +1958,8 @@ class ResetStatesTest(test_combinations.TestCase):\n \n     def test_precision_update_state_with_logits(self):\n         p_obj = metrics.Precision()\n-        # Update state with logits (not in range (0, 1)) should not an raise error.\n+        # Update state with logits (not in range (0, 1)) should not an raise\n+        # error.\n         p_obj.update_state([-0.5, 0.5], [-2.0, 2.0])\n \n     def test_reset_state_recall(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
