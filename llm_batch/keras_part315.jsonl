{"custom_id": "keras#a1cb584d43280fed11d5c15b937b843339b5b47b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 145 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 158 | Churn Cumulative: 243 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.32608695652173914\n\nDIFF:\n@@ -0,0 +1,138 @@\n+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Utils to help build and verify pip package for Keras.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import argparse\n+import fnmatch\n+import os\n+\n+PIP_EXCLUDED_FILES = frozenset([\n+    'keras/combinations.py',\n+    'keras/api/create_python_api_wrapper.py',\n+    'keras/applications/efficientnet_weight_update_util.py',\n+    'keras/distribute/keras_correctness_test_base.py',\n+    'keras/distribute/model_collection_base.py',\n+    'keras/distribute/model_combinations.py',\n+    'keras/distribute/multi_worker_testing_utils.py',\n+    'keras/distribute/optimizer_combinations.py',\n+    'keras/distribute/saved_model_test_base.py',\n+    'keras/distribute/simple_models.py',\n+    'keras/distribute/strategy_combinations.py',\n+    'keras/distribute/tpu_strategy_test_utils.py',\n+    'keras/distribute/test_example.py',\n+    'keras/mixed_precision/test_util.py',\n+    'keras/tools/pip_package/setup.py',\n+    'keras/tools/pip_package/create_pip_helper.py',\n+])\n+\n+PIP_EXCLUDED_DIRS = frozenset([\n+    'keras/benchmarks',\n+    'keras/integration_tests',\n+    'keras/tests',\n+])\n+\n+# Directories that should not have __init__.py files generated within them.\n+EXCLUDED_INIT_FILE_DIRECTORIES = frozenset([\n+    'keras/benchmarks',\n+    'keras/tools',\n+])\n+\n+\n+class PipPackagingError(Exception):\n+  pass\n+\n+\n+def create_init_files(pip_root):\n+  \"\"\"Create __init__.py in pip directory tree.\n+\n+  These files are auto-generated by Bazel when doing typical build/test, but\n+  do not get auto-generated by the pip build process. Currently, the entire\n+  directory tree is just python files, so its fine to just create all of the\n+  init files.\n+\n+  Args:\n+    pip_root: Root directory of code being packaged into pip.\n+  \"\"\"\n+  for path, subdirs, _ in os.walk(pip_root):\n+    for subdir in subdirs:\n+      init_file_path = os.path.join(path, subdir, '__init__.py')\n+      if any(excluded_path in init_file_path\n+             for excluded_path in EXCLUDED_INIT_FILE_DIRECTORIES):\n+        continue\n+      if not os.path.exists(init_file_path):\n+        # Create empty file\n+        open(init_file_path, 'w').close()\n+\n+\n+def verify_python_files_in_pip(pip_root, bazel_root):\n+  \"\"\"Verifies all expected files are packaged into Pip.\n+\n+  Args:\n+    pip_root: Root directory of code being packaged into pip.\n+    bazel_root: Root directory of Keras Bazel workspace.\n+\n+  Raises:\n+    PipPackagingError: Missing file in pip.\n+  \"\"\"\n+  for path, _, files in os.walk(bazel_root):\n+    if any(d for d in PIP_EXCLUDED_DIRS if d in path):\n+      # Skip any directories that are exclude from PIP, eg tests.\n+      continue\n+\n+    python_files = set(fnmatch.filter(files, '*.py'))\n+    python_test_files = set(fnmatch.filter(files, '*test.py'))\n+    python_benchmark_files = set(fnmatch.filter(files, '*benchmark.py'))\n+    # We only care about python files in the pip package, see create_init_files.\n+    files = python_files - python_test_files - python_benchmark_files\n+    for f in files:\n+      pip_path = os.path.join(pip_root, os.path.relpath(path, bazel_root), f)\n+      file_name = os.path.join(path, f)\n+      path_exists = os.path.exists(pip_path)\n+      file_excluded = file_name.lstrip('./') in PIP_EXCLUDED_FILES\n+      if not path_exists and not file_excluded:\n+        raise PipPackagingError(\n+            ('Pip package missing the file %s. If this is expected, add it '\n+             'to PIP_EXCLUDED_FILES in create_pip_helper.py. Otherwise, '\n+             'make sure it is a build dependency of the pip package') %\n+            file_name)\n+      if path_exists and file_excluded:\n+        raise PipPackagingError(\n+            ('File in PIP_EXCLUDED_FILES included in pip. %s' % file_name))\n+\n+\n+def main():\n+  parser = argparse.ArgumentParser()\n+  parser.add_argument(\n+      '--bazel-root',\n+      type=str,\n+      required=True,\n+      help='Root directory of Keras Bazel workspace.')\n+  parser.add_argument(\n+      '--pip-root',\n+      type=str,\n+      required=True,\n+      help='Root directory of code being packaged into pip.')\n+\n+  args = parser.parse_args()\n+  create_init_files(args.pip_root)\n+  verify_python_files_in_pip(args.pip_root, args.bazel_root)\n+\n+\n+if __name__ == '__main__':\n+  main()\n\n@@ -30,32 +30,28 @@ DOCLINES = __doc__.split('\\n')\n # This version string is semver compatible, but incompatible with pip.\n # For pip, we will remove all '-' characters from this string, and use the\n # result for pip.\n-_VERSION = '1.12.0'\n+_VERSION = '2.5.0'\n \n REQUIRED_PACKAGES = [\n-    'absl-py >= 0.1.6',\n-    'numpy >= 1.16.0',\n-    # Depends on TensorFlow. Not declared here to avoid circular dependency.\n+    # We depend on TensorFlow's declared pip dependencies.\n+    # Add a new dep there if one is needed.\n ]\n \n-project_name = 'tensorflow_keras'\n+project_name = 'keras'\n if '--project_name' in sys.argv:\n   project_name_idx = sys.argv.index('--project_name')\n   project_name = sys.argv[project_name_idx + 1]\n   sys.argv.remove('--project_name')\n   sys.argv.pop(project_name_idx)\n \n-if sys.version_info.major == 2:\n-  # mock comes with unittest.mock for python3, need to install for python2\n-  REQUIRED_PACKAGES.append('mock >= 2.0.0')\n \n setuptools.setup(\n     name=project_name,\n     version=_VERSION.replace('-', ''),\n     description=DOCLINES[0],\n     long_description='\\n'.join(DOCLINES[2:]),\n-    url='https://www.tensorflow.org/',\n-    download_url='https://github.com/tensorflow/keras/tags',\n+    url='https://keras.io/',\n+    download_url='https://github.com/keras-team/keras/tags',\n     author='Google Inc.',\n     author_email='packages@tensorflow.org',\n     packages=setuptools.find_packages(),\n@@ -67,12 +63,10 @@ setuptools.setup(\n         'Intended Audience :: Education',\n         'Intended Audience :: Science/Research',\n         'License :: OSI Approved :: Apache Software License',\n-        'Programming Language :: Python :: 2',\n-        'Programming Language :: Python :: 2.7',\n         'Programming Language :: Python :: 3',\n-        'Programming Language :: Python :: 3.4',\n         'Programming Language :: Python :: 3.5',\n         'Programming Language :: Python :: 3.6',\n+        'Programming Language :: Python :: 3.7',\n         'Topic :: Scientific/Engineering',\n         'Topic :: Scientific/Engineering :: Mathematics',\n         'Topic :: Scientific/Engineering :: Artificial Intelligence',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a0351962dddbd374d7a996afbe5f3b23603d70f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 32 | Churn Cumulative: 2027 | Contributors (this commit): 28 | Commits (past 90d): 4 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -299,38 +299,6 @@ def gather_trainable_weights(trainable, sub_layers, extra_variables):\n   return weights + trainable_extra_variables\n \n \n-def gather_non_trainable_weights(trainable, sub_layers, extra_variables):\n-  \"\"\"Lists the non-trainable weights for an object with sub-layers.\n-\n-  Args:\n-    trainable: Whether the object collecting the variables is trainable.\n-    sub_layers: A flat list of Layer objects owned by this object, to collect\n-      variables from.\n-    extra_variables: Any extra variables to include. Their `.trainable` property\n-      is used to categorize them.\n-\n-  Returns:\n-    A list of collected non-trainable weights/variables.\n-  \"\"\"\n-  trainable_extra_variables = []\n-  non_trainable_extra_variables = []\n-  for v in extra_variables:\n-    if v.trainable:\n-      trainable_extra_variables.append(v)\n-    else:\n-      non_trainable_extra_variables.append(v)\n-  weights = []\n-  for layer in sub_layers:\n-    weights += layer.non_trainable_weights\n-  if not trainable:\n-    trainable_weights = []\n-    for layer in sub_layers:\n-      trainable_weights += layer.trainable_weights\n-    return (trainable_weights + trainable_extra_variables\n-            + weights + non_trainable_extra_variables)\n-  return weights + non_trainable_extra_variables\n-\n-\n def convert_dense_weights_data_format(dense,\n                                       previous_feature_map_shape,\n                                       target_data_format='channels_first'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#77309a482b6d32b79230bbce35e4d0083d5f8bb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 18 | Churn Cumulative: 953 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -84,24 +84,6 @@ class InterfaceTests(tf.test.TestCase):\n       checkpoint = tf.train.Checkpoint(model=model)\n       checkpoint.save(os.path.join(self.get_temp_dir(), \"ckpt\"))\n \n-  def testObjectMetadata(self):\n-    if not tf.executing_eagerly():\n-      self.skipTest(\"Run in eager mode only.\")\n-\n-    checkpoint_directory = self.get_temp_dir()\n-    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n-    dense = core.Dense(1)\n-    checkpoint = tf.train.Checkpoint(dense=dense)\n-    dense(tf.constant([[1.]]))\n-    save_path = checkpoint.save(checkpoint_prefix)\n-\n-    objects = trackable_utils.object_metadata(save_path)\n-    all_variable_names = []\n-    for obj in objects.nodes:\n-      for attribute in obj.attributes:\n-        all_variable_names.append(attribute.full_name)\n-    self.assertIn(\"dense/kernel\", all_variable_names)\n-\n \n class CheckpointingTests(keras_parameterized.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#461c7b1dc1797d2b05110837f5a4cd12886cc390", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 23 | Churn Cumulative: 2050 | Contributors (this commit): 28 | Commits (past 90d): 5 | Contributors (cumulative): 28 | DMM Complexity: 0.0\n\nDIFF:\n@@ -276,29 +276,6 @@ def print_summary(model, line_length=None, positions=None, print_fn=None):\n   print_fn('_' * line_length)\n \n \n-def gather_trainable_weights(trainable, sub_layers, extra_variables):\n-  \"\"\"Lists the trainable weights for an object with sub-layers.\n-\n-  Args:\n-    trainable: Whether the object collecting the variables is trainable.\n-    sub_layers: A flat list of Layer objects owned by this object, to collect\n-      variables from.\n-    extra_variables: Any extra variables to include. Their `.trainable` property\n-      is used to categorize them.\n-\n-  Returns:\n-    A list of collected trainable weights/variables.\n-  \"\"\"\n-  if not trainable:\n-    return []\n-  weights = []\n-  for layer in sub_layers:\n-    weights += layer.trainable_weights\n-  trainable_extra_variables = [\n-      v for v in extra_variables if v.trainable]\n-  return weights + trainable_extra_variables\n-\n-\n def convert_dense_weights_data_format(dense,\n                                       previous_feature_map_shape,\n                                       target_data_format='channels_first'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1d2a776dfa29bf08a8a13745be82e7d6788e365", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 709 | Lines Deleted: 259 | Files Changed: 31 | Hunks: 134 | Methods Changed: 85 | Complexity Δ (Sum/Max): 62/30 | Churn Δ: 968 | Churn Cumulative: 67473 | Contributors (this commit): 202 | Commits (past 90d): 101 | Contributors (cumulative): 324 | DMM Complexity: 0.8301158301158301\n\nDIFF:\n@@ -28,6 +28,7 @@ import keras\n from tensorflow.python.distribute import multi_worker_test_base\n from tensorflow.python.distribute import values as ds_values_lib\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n+from keras import backend\n from keras import testing_utils\n from keras.distribute import distributed_training_utils\n from keras.distribute import distributed_training_utils_v1\n@@ -137,7 +138,7 @@ def batch_wrapper(dataset, batch_size, distribution, repeat=None):\n     dataset = dataset.repeat(repeat)\n   # TPUs currently require fully defined input shapes, drop_remainder ensures\n   # the input will have fully defined shapes.\n-  if _is_tpu_strategy(distribution):\n+  if backend.is_tpu_strategy(distribution):\n     return dataset.batch(batch_size, drop_remainder=True)\n   else:\n     return dataset.batch(batch_size)\n@@ -478,7 +479,7 @@ class TestDistributionStrategyWithNumpyArrays(tf.test.TestCase,\n                    tf.distribute.experimental.CentralStorageStrategy,\n                    tf.compat.v1.distribute.experimental.CentralStorageStrategy)):\n       self.skipTest('b/152097775')\n-    if _is_tpu_strategy(distribution):\n+    if backend.is_tpu_strategy(distribution):\n       policy_name = 'mixed_bfloat16'\n     else:\n       policy_name = 'mixed_float16'\n@@ -531,7 +532,7 @@ class TestDistributionStrategyWithNumpyArrays(tf.test.TestCase,\n                    tf.compat.v1.distribute.experimental.CentralStorageStrategy)):\n       self.skipTest('b/152097775')\n \n-    if _is_tpu_strategy(distribution):\n+    if backend.is_tpu_strategy(distribution):\n       policy_name = 'mixed_bfloat16'\n     else:\n       policy_name = 'mixed_float16'\n@@ -975,7 +976,7 @@ class TestDistributionStrategyWithDatasets(tf.test.TestCase,\n   def test_fit_with_dictionary_in_the_dataset_b135161171(\n       self, distribution):\n \n-    if _is_tpu_strategy(distribution):\n+    if backend.is_tpu_strategy(distribution):\n       self.skipTest('b/142805125')\n \n     def custom_loss(predict, label, weight):\n@@ -1058,7 +1059,7 @@ class TestDistributionStrategyWithDatasets(tf.test.TestCase,\n   def test_predict_on_dataset_with_unknown_cardinality_without_steps(\n       self, distribution, mode):\n \n-    if mode == 'graph' and _is_tpu_strategy(distribution):\n+    if mode == 'graph' and backend.is_tpu_strategy(distribution):\n       self.skipTest('partial batch not supported with TPU in graph mode.')\n \n     with self.cached_session():\n@@ -1091,10 +1092,10 @@ class TestDistributionStrategyWithDatasets(tf.test.TestCase,\n       self, distribution, mode):\n     # TODO(b/155867206): Investigate why this test occasionally segfaults on TPU\n     # in eager mode.\n-    if mode == 'eager' and _is_tpu_strategy(distribution):\n+    if mode == 'eager' and backend.is_tpu_strategy(distribution):\n       self.skipTest('caused segfault with TPU in eager mode.')\n \n-    if mode == 'graph' and _is_tpu_strategy(distribution):\n+    if mode == 'graph' and backend.is_tpu_strategy(distribution):\n       self.skipTest('partial batch not supported with TPU in graph mode.')\n \n     with self.cached_session():\n@@ -1568,13 +1569,6 @@ class TestDistributionStrategyWithDatasets(tf.test.TestCase,\n       self.assertAllClose(result, 13.5)\n \n \n-def _is_tpu_strategy(strategy):\n-  if isinstance(strategy,\n-                (tf.distribute.experimental.TPUStrategy, tf.compat.v1.distribute.experimental.TPUStrategy)):\n-    return True\n-  return False\n-\n-\n class TestDistributionStrategyWithDatasetsFile(tf.test.TestCase,\n                                                parameterized.TestCase):\n \n\n@@ -19,13 +19,7 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n-\n-\n-# TODO(sourabhbajaj): Remove this once we use the same API for all strategies.\n-def is_tpu_strategy(strategy):\n-  \"\"\"We're executing TPU Strategy.\"\"\"\n-  return (strategy is not None and\n-          strategy.__class__.__name__.startswith('TPUStrategy'))\n+from keras import backend\n \n \n # TODO(b/118776054): Currently we support global batch size for TPUStrategy and\n@@ -59,7 +53,7 @@ def call_replica_local_fn(fn, *args, **kwargs):\n       strategy = tf.distribute.get_strategy()\n \n   # TODO(b/120571621): TPUStrategy does not implement replica-local variables.\n-  is_tpu = is_tpu_strategy(strategy)\n+  is_tpu = backend.is_tpu_strategy(strategy)\n   if ((not is_tpu) and strategy and tf.distribute.in_cross_replica_context()):\n     with strategy.scope():\n       return strategy.extended.call_for_each_replica(fn, args, kwargs)\n\n@@ -138,7 +138,7 @@ def unwrap_output_dict(strategy, grouped_outputs, mode):\n                                        grouped_outputs['metrics'])\n   batch_size = strategy.reduce(tf.distribute.ReduceOp.SUM,\n                                grouped_outputs['batch_size'], axis=None)\n-  if (dist_utils.is_tpu_strategy(strategy) and\n+  if (K.is_tpu_strategy(strategy) and\n       tf.compat.v1.executing_eagerly_outside_functions()):\n     # Choose 1 value per replica in the TPU case since all replicas produce the\n     # same output.\n@@ -186,7 +186,7 @@ def unwrap_outputs(distribution_strategy, grouped_outputs,\n                                       grouped_outputs[0], axis=None)\n   all_outputs = flatten_per_replica_values(distribution_strategy,\n                                            grouped_outputs[1:])\n-  if (dist_utils.is_tpu_strategy(distribution_strategy) and\n+  if (K.is_tpu_strategy(distribution_strategy) and\n       tf.compat.v1.executing_eagerly_outside_functions()):\n     # Choose 1 value per replica in the TPU case since all replicas produce the\n     # same output.\n@@ -481,12 +481,12 @@ def get_input_params(distribution_strategy,\n   if tf.executing_eagerly():\n     allow_partial_batch = (\n         mode != ModeKeys.TRAIN or\n-        not dist_utils.is_tpu_strategy(distribution_strategy))\n+        not K.is_tpu_strategy(distribution_strategy))\n   else:\n     allow_partial_batch = (\n         mode == ModeKeys.TRAIN or\n         ((mode == ModeKeys.PREDICT or mode == ModeKeys.TEST) and\n-         dist_utils.is_tpu_strategy(distribution_strategy)))\n+         K.is_tpu_strategy(distribution_strategy)))\n \n   if steps is None:\n     if batch_size is None:\n@@ -608,7 +608,7 @@ def _prepare_feed_values(model, inputs, targets, sample_weights, mode):\n   \"\"\"\n   strategy = model._distribution_strategy\n   inputs, targets, sample_weights = _get_input_from_iterator(inputs, model)\n-  if dist_utils.is_tpu_strategy(strategy):\n+  if K.is_tpu_strategy(strategy):\n     if sample_weights is not None:\n       raise ValueError('TPUStrategy does not support sample weights.')\n \n@@ -654,7 +654,7 @@ def is_distributing_by_cloning(model):\n     True if the `model` is going to be distributed using cloning and False\n     otherwise.\n   \"\"\"\n-  if (dist_utils.is_tpu_strategy(model._distribution_strategy) and\n+  if (K.is_tpu_strategy(model._distribution_strategy) and\n       tf.executing_eagerly):  # b/137580852\n     return False\n   elif tf.compat.v1.executing_eagerly_outside_functions():\n\n@@ -24,6 +24,7 @@ import os\n \n from absl.testing import parameterized\n import numpy as np\n+from keras import backend\n from keras.distribute import model_combinations\n \n _RANDOM_SEED = 1337\n@@ -57,13 +58,8 @@ strategies = [\n ]\n \n \n-def is_tpu_strategy(distribution):\n-  return (distribution is not None and\n-          distribution.__class__.__name__.startswith('TPUStrategy'))\n-\n-\n def get_tolerance(save_distribution, restore_distribution):\n-  if is_tpu_strategy(save_distribution) or is_tpu_strategy(\n+  if backend.is_tpu_strategy(save_distribution) or backend.is_tpu_strategy(\n       restore_distribution):\n     return _TPU_TOLERANCE\n   return _TOLERANCE\n\n@@ -666,6 +666,7 @@ class Functional(training_lib.Model):\n     Raises:\n         ValueError: In case of improperly formatted config dict.\n     \"\"\"\n+    with generic_utils.SharedObjectLoadingScope():\n       input_tensors, output_tensors, created_layers = reconstruct_from_config(\n           config, custom_objects)\n       model = cls(inputs=input_tensors, outputs=output_tensors,\n@@ -1341,6 +1342,8 @@ def get_network_config(network, serialize_layer_fn=None):\n         node_conversion_map[node_key] = kept_nodes\n         kept_nodes += 1\n   layer_configs = []\n+\n+  with generic_utils.SharedObjectSavingScope():\n     for layer in network.layers:  # From the earliest layers on.\n       filtered_inbound_nodes = []\n       for original_node_index, node in enumerate(layer._inbound_nodes):\n\n@@ -33,7 +33,6 @@ from keras import backend\n from keras import callbacks as callbacks_module\n from keras import optimizer_v1\n from keras import optimizers\n-from keras.distribute import distributed_training_utils as dist_utils\n from keras.engine import base_layer\n from keras.engine import base_layer_utils\n from keras.engine import compile_utils\n@@ -2199,7 +2198,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n           `False`.\n     \"\"\"\n-    if dist_utils.is_tpu_strategy(self._distribution_strategy):\n+    if backend.is_tpu_strategy(self._distribution_strategy):\n       if (self._distribution_strategy.extended.steps_per_run > 1 and\n           (not saving_utils.is_hdf5_filepath(filepath))):\n         raise ValueError('Load weights is not yet supported with TPUStrategy '\n@@ -2721,7 +2720,7 @@ def concat(tensors, axis=0):\n \n \n def _is_tpu_multi_host(strategy):\n-  return (dist_utils.is_tpu_strategy(strategy) and\n+  return (backend.is_tpu_strategy(strategy) and\n           strategy.extended.num_hosts > 1)\n \n \n\n@@ -27,7 +27,6 @@ from tensorflow.python.distribute import distribute_coordinator as dc\n from tensorflow.python.distribute import input_lib\n from keras import backend as K\n from keras import callbacks as cbks\n-from keras.distribute import distributed_training_utils as dist_utils_v2\n from keras.distribute import distributed_training_utils_v1 as dist_utils\n from keras.engine import partial_batch_padding_handler as padding_util\n from keras.engine import training_arrays_v1\n@@ -645,7 +644,7 @@ class DistributionSingleWorkerTrainingLoop(training_utils_v1.TrainingLoop):\n       raise ValueError('validation_split argument is not supported with '\n                        'distribution strategies.')\n \n-    if dist_utils_v2.is_tpu_strategy(model._distribution_strategy):\n+    if K.is_tpu_strategy(model._distribution_strategy):\n       steps_per_epoch = training_utils_v1.infer_steps_for_dataset(\n           model, dataset, steps_per_epoch, epochs, steps_name='steps_per_epoch')\n       if steps_per_epoch is None:\n@@ -702,7 +701,7 @@ class DistributionSingleWorkerTrainingLoop(training_utils_v1.TrainingLoop):\n         batch_size=batch_size,\n         allow_partial_batch=True)\n \n-    if dist_utils_v2.is_tpu_strategy(model._distribution_strategy):\n+    if K.is_tpu_strategy(model._distribution_strategy):\n       steps = training_utils_v1.infer_steps_for_dataset(\n           model, dataset, steps, steps_name='steps')\n       if steps is None:\n@@ -739,7 +738,7 @@ class DistributionSingleWorkerTrainingLoop(training_utils_v1.TrainingLoop):\n         x,\n         batch_size=batch_size,\n         allow_partial_batch=True)\n-    if dist_utils_v2.is_tpu_strategy(model._distribution_strategy):\n+    if K.is_tpu_strategy(model._distribution_strategy):\n       steps = training_utils_v1.infer_steps_for_dataset(\n           model, dataset, steps, steps_name='steps')\n       if steps is None:\n\n@@ -211,7 +211,7 @@ class Model(training_lib.Model):\n         ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n           `False`.\n     \"\"\"\n-    if distributed_training_utils.is_tpu_strategy(self._distribution_strategy):\n+    if K.is_tpu_strategy(self._distribution_strategy):\n       if (self._distribution_strategy.extended.steps_per_run > 1 and\n           (not saving_utils.is_hdf5_filepath(filepath))):  # pylint: disable=protected-access\n         raise ValueError('Load weights is not yet supported with TPUStrategy '\n@@ -2120,8 +2120,7 @@ class Model(training_lib.Model):\n                                 'when using tf.distribute.Strategy.')\n \n     if (sample_weight is not None and sample_weight.all() and\n-        distributed_training_utils.is_tpu_strategy(\n-            self._distribution_strategy)):\n+        K.is_tpu_strategy(self._distribution_strategy)):\n       raise NotImplementedError('`sample_weight` is currently not supported '\n                                 'when using TPUStrategy.')\n \n@@ -2175,8 +2174,7 @@ class Model(training_lib.Model):\n         # TODO(b/131720208): We still drop remainder here if number of examples\n         # is divisible by batch size, as sometimes dynamic padder will time out\n         # with keras.metrics.CategoricalAccuracy() metric.\n-        if distributed_training_utils.is_tpu_strategy(\n-            strategy) and not drop_remainder:\n+        if K.is_tpu_strategy(strategy) and not drop_remainder:\n           dataset_size = first_x_value.shape[0]\n           if dataset_size % batch_size == 0:\n             drop_remainder = True\n\n@@ -27,7 +27,6 @@ import tensorflow as tf\n from tensorflow.python.feature_column import feature_column_v2\n from keras.engine.base_layer import Layer\n from keras.utils import generic_utils\n-from tensorflow.python.ops import variable_scope\n \n \n class _BaseFeaturesLayer(Layer):\n@@ -73,10 +72,9 @@ class _BaseFeaturesLayer(Layer):\n \n   def build(self, _):\n     for column in self._feature_columns:\n-      with variable_scope._pure_variable_scope(  # pylint: disable=protected-access\n-          self.name,\n-          partitioner=self._partitioner):\n-        with variable_scope._pure_variable_scope(  # pylint: disable=protected-access\n+      with tf.compat.v1.variable_scope(\n+          self.name, partitioner=self._partitioner):\n+        with tf.compat.v1.variable_scope(\n             feature_column_v2._sanitize_column_name_for_variable_scope(  # pylint: disable=protected-access\n                 column.name)):\n           column.create_state(self._state_manager)\n\n@@ -23,7 +23,6 @@ import tensorflow as tf\n from absl.testing import parameterized\n import numpy as np\n from tensorflow.python.eager import backprop\n-from tensorflow.python.feature_column import feature_column_v2 as fc\n from tensorflow.python.framework import test_util\n from keras import combinations\n from keras import keras_parameterized\n@@ -1048,7 +1047,9 @@ class DenseFeaturesSerializationTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(new_layer._feature_columns[0].name, 'a')\n     self.assertEqual(new_layer._feature_columns[1].initializer.mean, 0.0)\n     self.assertEqual(new_layer._feature_columns[1].categorical_column.name, 'b')\n-    self.assertIsInstance(new_layer._feature_columns[2], fc.IndicatorColumn)\n+    self.assertIsInstance(new_layer._feature_columns[0], cols[0].__class__)\n+    self.assertIsInstance(new_layer._feature_columns[1], cols[1].__class__)\n+    self.assertIsInstance(new_layer._feature_columns[2], cols[2].__class__)\n \n   def test_crossed_column(self):\n     a = tf.feature_column.categorical_column_with_vocabulary_list(\n\n@@ -72,11 +72,8 @@ class SequenceFeatures(kfc._BaseFeaturesLayer):\n     sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)\n     sequence_input, sequence_length = sequence_input_layer(\n        features, training=training)\n-\n     sequence_length_mask = tf.sequence_mask(sequence_length)\n-\n     hidden_size = 32\n-\n     rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)\n     rnn_layer = tf.keras.layers.RNN(rnn_cell)\n     outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)\n\n@@ -25,10 +25,11 @@ from google.protobuf import text_format\n \n from tensorflow.core.example import example_pb2\n from tensorflow.core.example import feature_pb2\n-from tensorflow.python.feature_column import sequence_feature_column as sfc\n from tensorflow.python.framework import test_util\n from keras.feature_column import dense_features\n from keras.feature_column import sequence_feature_column as ksfc\n+from keras.layers import core\n+from keras.layers import merge\n from keras.layers import recurrent\n \n \n@@ -88,13 +89,14 @@ class SequenceFeatureColumnIntegrationTest(tf.test.TestCase):\n \n     # Tile the context features across the sequence features\n     sequence_input_layer = ksfc.SequenceFeatures(seq_cols)\n-    seq_layer, _ = sequence_input_layer(features)\n-    input_layer = dense_features.DenseFeatures(ctx_cols)\n-    ctx_layer = input_layer(features)\n-    input_layer = sfc.concatenate_context_input(ctx_layer, seq_layer)\n+    seq_input, _ = sequence_input_layer(features)\n+    dense_input_layer = dense_features.DenseFeatures(ctx_cols)\n+    ctx_input = dense_input_layer(features)\n+    ctx_input = core.RepeatVector(tf.compat.v1.shape(seq_input)[1])(ctx_input)\n+    concatenated_input = merge.concatenate([seq_input, ctx_input])\n \n     rnn_layer = recurrent.RNN(recurrent.SimpleRNNCell(10))\n-    output = rnn_layer(input_layer)\n+    output = rnn_layer(concatenated_input)\n \n     with self.cached_session() as sess:\n       sess.run(tf.compat.v1.global_variables_initializer())\n\n@@ -120,7 +120,11 @@ class BaseDenseAttention(Layer):\n     if scores_mask is not None:\n       padding_mask = tf.logical_not(scores_mask)\n       # Bias so padding positions do not contribute to attention distribution.\n-      scores -= 1.e9 * tf.cast(padding_mask, dtype=K.floatx())\n+      # Note 65504. is the max float16 value.\n+      if scores.dtype is tf.float16:\n+        scores -= 65504. * tf.cast(padding_mask, dtype=scores.dtype)\n+      else:\n+        scores -= 1.e9 * tf.cast(padding_mask, dtype=scores.dtype)\n     if training is None:\n       training = K.learning_phase()\n     weights = tf.compat.v1.math.softmax(scores)\n\n@@ -25,8 +25,10 @@ import numpy as np\n \n import keras\n from keras import combinations\n+from keras import testing_utils\n from keras.layers import core\n from keras.layers import dense_attention\n+from keras.mixed_precision import policy\n \n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n@@ -756,6 +758,17 @@ class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n     new_layer = dense_attention.AdditiveAttention.from_config(config)\n     self.assertEqual(new_layer.use_scale, True)\n \n+  @testing_utils.enable_v2_dtype_behavior\n+  def test_mixed_float16_policy(self):\n+    # Test case for GitHub issue:\n+    # https://github.com/tensorflow/tensorflow/issues/46064\n+    with policy.policy_scope('mixed_float16'):\n+      q = tf.cast(tf.random.uniform((2, 3, 4), seed=1), 'float16')\n+      v = tf.cast(tf.random.uniform((2, 3, 4), seed=2), 'float16')\n+      k = tf.cast(tf.random.uniform((2, 3, 4), seed=3), 'float16')\n+      layer = dense_attention.AdditiveAttention(causal=True)\n+      _ = layer([q, v, k])\n+\n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class LowerTriangularMaskTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -756,7 +756,7 @@ def add(inputs, **kwargs):\n   >>> print(y.shape)\n   (2, 3, 4)\n \n-  Used in a functiona model:\n+  Used in a functional model:\n \n   >>> input1 = tf.keras.layers.Input(shape=(16,))\n   >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n\n@@ -106,13 +106,11 @@ class CategoryCrossing(base_preprocessing_layer.PreprocessingLayer):\n     `[[b'1_X_2_X_3'], [b'4_X_5_X_6']]`\n   \"\"\"\n \n-  def __init__(self, depth=None, name=None, separator=None, **kwargs):\n+  def __init__(self, depth=None, name=None, separator='_X_', **kwargs):\n     super(CategoryCrossing, self).__init__(name=name, **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\n         'CategoryCrossing').set(True)\n     self.depth = depth\n-    if separator is None:\n-      separator = '_X_'\n     self.separator = separator\n     if isinstance(depth, (tuple, list)):\n       self._depth_tuple = depth\n\n@@ -396,11 +396,11 @@ class RandomFlip(PreprocessingLayer):\n     def random_flipped_inputs():\n       flipped_outputs = inputs\n       if self.horizontal:\n-        flipped_outputs = tf.image.random_flip_left_right(flipped_outputs,\n-                                                           self.seed)\n-      if self.vertical:\n-        flipped_outputs = tf.image.random_flip_up_down(\n+        flipped_outputs = tf.image.random_flip_left_right(\n             flipped_outputs, self.seed)\n+      if self.vertical:\n+        flipped_outputs = tf.image.random_flip_up_down(flipped_outputs,\n+                                                        self.seed)\n       return flipped_outputs\n \n     output = control_flow_util.smart_cond(training, random_flipped_inputs,\n@@ -426,52 +426,46 @@ class RandomTranslation(PreprocessingLayer):\n   \"\"\"Randomly translate each image during training.\n \n   Args:\n-    height_factor: a float represented as fraction of value, or a tuple\n-      of size 2 representing lower and upper bound for shifting vertically.\n-      A negative value means shifting image up, while a positive value\n-      means shifting image down. When represented as a single positive float,\n-      this value is used for both the upper and lower bound. For instance,\n-      `height_factor=(-0.2, 0.3)` results in an output shifted by a random\n-      amount in the range [-20%, +30%].\n-      `height_factor=0.2` results in an output height shifted by a random\n-      amount in the range [-20%, +20%].\n-    width_factor: a float represented as fraction of value, or a tuple\n-      of size 2 representing lower and upper bound for shifting horizontally.\n-      A negative value means shifting image left, while a positive value\n-      means shifting image right. When represented as a single positive float,\n-      this value is used for both the upper and lower bound. For instance,\n+    height_factor: a float represented as fraction of value, or a tuple of size\n+      2 representing lower and upper bound for shifting vertically. A negative\n+      value means shifting image up, while a positive value means shifting image\n+      down. When represented as a single positive float, this value is used for\n+      both the upper and lower bound. For instance, `height_factor=(-0.2, 0.3)`\n+      results in an output shifted by a random amount in the range [-20%, +30%].\n+      `height_factor=0.2` results in an output height shifted by a random amount\n+      in the range [-20%, +20%].\n+    width_factor: a float represented as fraction of value, or a tuple of size 2\n+      representing lower and upper bound for shifting horizontally. A negative\n+      value means shifting image left, while a positive value means shifting\n+      image right. When represented as a single positive float, this value is\n+      used for both the upper and lower bound. For instance,\n       `width_factor=(-0.2, 0.3)` results in an output shifted left by 20%, and\n-      shifted right by 30%.\n-      `width_factor=0.2` results in an output height shifted left or right\n-      by 20%.\n+      shifted right by 30%. `width_factor=0.2` results in an output height\n+      shifted left or right by 20%.\n     fill_mode: Points outside the boundaries of the input are filled according\n       to the given mode (one of `{'constant', 'reflect', 'wrap', 'nearest'}`).\n-      - *reflect*: `(d c b a | a b c d | d c b a)`\n-        The input is extended by reflecting about the edge of the last pixel.\n-      - *constant*: `(k k k k | a b c d | k k k k)`\n-        The input is extended by filling all values beyond the edge with the\n-        same constant value k = 0.\n-      - *wrap*: `(a b c d | a b c d | a b c d)`\n-        The input is extended by wrapping around to the opposite edge.\n-      - *nearest*: `(a a a a | a b c d | d d d d)`\n-        The input is extended by the nearest pixel.\n+      - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n+        reflecting about the edge of the last pixel.\n+      - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n+        filling all values beyond the edge with the same constant value k = 0.\n+      - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n+        wrapping around to the opposite edge.\n+      - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n+        nearest pixel.\n     interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n     seed: Integer. Used to create a random seed.\n     name: A string, the name of the layer.\n-    fill_value: a float represents the value to be filled outside the\n-      boundaries when `fill_mode` is \"constant\".\n-\n+    fill_value: a float represents the value to be filled outside the boundaries\n+      when `fill_mode` is \"constant\".\n   Input shape:\n     4D tensor with shape: `(samples, height, width, channels)`,\n       data_format='channels_last'.\n-\n   Output shape:\n     4D tensor with shape: `(samples, height, width, channels)`,\n       data_format='channels_last'.\n-\n   Raise:\n-    ValueError: if either bound is not between [0, 1], or upper bound is\n-      less than lower bound.\n+    ValueError: if either bound is not between [0, 1], or upper bound is less\n+      than lower bound.\n   \"\"\"\n \n   def __init__(self,\n@@ -520,8 +514,8 @@ class RandomTranslation(PreprocessingLayer):\n     self._rng = make_generator(self.seed)\n     self.input_spec = InputSpec(ndim=4)\n     super(RandomTranslation, self).__init__(name=name, **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        'RandomTranslation').set(True)\n+    base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomTranslation').set(\n+        True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -633,29 +627,19 @@ def transform(images,\n       not backpropagated into transformation parameters.\n     fill_mode: Points outside the boundaries of the input are filled according\n       to the given mode (one of `{'constant', 'reflect', 'wrap', 'nearest'}`).\n-    fill_value: a float represents the value to be filled outside the\n-      boundaries when `fill_mode` is \"constant\".\n+    fill_value: a float represents the value to be filled outside the boundaries\n+      when `fill_mode` is \"constant\".\n     interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n     output_shape: Output dimesion after the transform, [height, width]. If None,\n       output is the same size as input image.\n-    name: The name of the op.\n-\n-  ## Fill mode.\n-  Behavior for each valid value is as follows:\n-\n-  reflect (d c b a | a b c d | d c b a)\n-  The input is extended by reflecting about the edge of the last pixel.\n-\n-  constant (k k k k | a b c d | k k k k)\n-  The input is extended by filling all values beyond the edge with the same\n-  constant value k = 0.\n-\n-  wrap (a b c d | a b c d | a b c d)\n-  The input is extended by wrapping around to the opposite edge.\n-\n-  nearest (a a a a | a b c d | d d d d)\n-  The input is extended by the nearest pixel.\n-\n+    name: The name of the op.  ## Fill mode.\n+  Behavior for each valid value is as follows:  reflect (d c b a | a b c d | d c\n+    b a) The input is extended by reflecting about the edge of the last pixel.\n+    constant (k k k k | a b c d | k k k k) The input is extended by filling all\n+    values beyond the edge with the same constant value k = 0.  wrap (a b c d |\n+    a b c d | a b c d) The input is extended by wrapping around to the opposite\n+    edge.  nearest (a a a a | a b c d | d d d d) The input is extended by the\n+    nearest pixel.\n   Input shape:\n     4D tensor with shape: `(samples, height, width, channels)`,\n       data_format='channels_last'.\n@@ -765,42 +749,32 @@ class RandomRotation(PreprocessingLayer):\n     `(samples, height, width, channels)`, data_format='channels_last'.\n \n   Attributes:\n-    factor: a float represented as fraction of 2pi, or a tuple of size\n-      2 representing lower and upper bound for rotating clockwise and\n+    factor: a float represented as fraction of 2pi, or a tuple of size 2\n+      representing lower and upper bound for rotating clockwise and\n       counter-clockwise. A positive values means rotating counter clock-wise,\n       while a negative value means clock-wise. When represented as a single\n       float, this value is used for both the upper and lower bound. For\n-      instance, `factor=(-0.2, 0.3)` results in an output\n-      rotation by a random amount in the range `[-20% * 2pi, 30% * 2pi]`.\n-      `factor=0.2` results in an output rotating by a random amount in the range\n-      `[-20% * 2pi, 20% * 2pi]`.\n+      instance, `factor=(-0.2, 0.3)` results in an output rotation by a random\n+      amount in the range `[-20% * 2pi, 30% * 2pi]`. `factor=0.2` results in an\n+      output rotating by a random amount in the range `[-20% * 2pi, 20% * 2pi]`.\n     fill_mode: Points outside the boundaries of the input are filled according\n       to the given mode (one of `{'constant', 'reflect', 'wrap', 'nearest'}`).\n-      - *reflect*: `(d c b a | a b c d | d c b a)`\n-        The input is extended by reflecting about the edge of the last pixel.\n-      - *constant*: `(k k k k | a b c d | k k k k)`\n-        The input is extended by filling all values beyond the edge with the\n-        same constant value k = 0.\n-      - *wrap*: `(a b c d | a b c d | a b c d)`\n-        The input is extended by wrapping around to the opposite edge.\n-      - *nearest*: `(a a a a | a b c d | d d d d)`\n-        The input is extended by the nearest pixel.\n+      - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n+        reflecting about the edge of the last pixel.\n+      - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n+        filling all values beyond the edge with the same constant value k = 0.\n+      - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n+        wrapping around to the opposite edge.\n+      - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n+        nearest pixel.\n     interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n     seed: Integer. Used to create a random seed.\n     name: A string, the name of the layer.\n-    fill_value: a float represents the value to be filled outside the\n-      boundaries when `fill_mode` is \"constant\".\n-\n-  Input shape:\n-    4D tensor with shape: `(samples, height, width, channels)`,\n-      data_format='channels_last'.\n-  Output shape:\n-    4D tensor with shape: `(samples, height, width, channels)`,\n-      data_format='channels_last'.\n-\n+    fill_value: a float represents the value to be filled outside the boundaries\n+      when `fill_mode` is \"constant\".\n   Raise:\n-    ValueError: if either bound is not between [0, 1], or upper bound is\n-      less than lower bound.\n+    ValueError: if either bound is not between [0, 1], or upper bound is less\n+      than lower bound.\n   \"\"\"\n \n   def __init__(self,\n@@ -829,8 +803,8 @@ class RandomRotation(PreprocessingLayer):\n     self._rng = make_generator(self.seed)\n     self.input_spec = InputSpec(ndim=4)\n     super(RandomRotation, self).__init__(name=name, **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        'RandomRotation').set(True)\n+    base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomRotation').set(\n+        True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -878,57 +852,45 @@ class RandomZoom(PreprocessingLayer):\n   \"\"\"Randomly zoom each image during training.\n \n   Args:\n-    height_factor: a float represented as fraction of value, or a tuple\n-      of size 2 representing lower and upper bound for zooming vertically.\n-      When represented as a single float, this value is used for both the\n-      upper and lower bound. A positive value means zooming out, while a\n-      negative value means zooming in.\n-      For instance, `height_factor=(0.2, 0.3)` result in an output zoomed out\n-      by a random amount in the range [+20%, +30%].\n+    height_factor: a float represented as fraction of value, or a tuple of size\n+      2 representing lower and upper bound for zooming vertically. When\n+      represented as a single float, this value is used for both the upper and\n+      lower bound. A positive value means zooming out, while a negative value\n+      means zooming in. For instance, `height_factor=(0.2, 0.3)` result in an\n+      output zoomed out by a random amount in the range [+20%, +30%].\n       `height_factor=(-0.3, -0.2)` result in an output zoomed in by a random\n       amount in the range [+20%, +30%].\n-    width_factor: a float represented as fraction of value, or a tuple\n-      of size 2 representing lower and upper bound for zooming horizontally.\n-      When represented as a single float, this value is used for both the\n-      upper and lower bound.\n-      For instance, `width_factor=(0.2, 0.3)` result in an output zooming out\n-      between 20% to 30%.\n-      `width_factor=(-0.3, -0.2)` result in an output zooming in between 20%\n-      to 30%. Defaults to `None`, i.e., zooming vertical and horizontal\n-      directions by preserving the aspect ratio.\n+    width_factor: a float represented as fraction of value, or a tuple of size 2\n+      representing lower and upper bound for zooming horizontally. When\n+      represented as a single float, this value is used for both the upper and\n+      lower bound. For instance, `width_factor=(0.2, 0.3)` result in an output\n+      zooming out between 20% to 30%. `width_factor=(-0.3, -0.2)` result in an\n+      output zooming in between 20% to 30%. Defaults to `None`, i.e., zooming\n+      vertical and horizontal directions by preserving the aspect ratio.\n     fill_mode: Points outside the boundaries of the input are filled according\n       to the given mode (one of `{'constant', 'reflect', 'wrap', 'nearest'}`).\n-      - *reflect*: `(d c b a | a b c d | d c b a)`\n-        The input is extended by reflecting about the edge of the last pixel.\n-      - *constant*: `(k k k k | a b c d | k k k k)`\n-        The input is extended by filling all values beyond the edge with the\n-        same constant value k = 0.\n-      - *wrap*: `(a b c d | a b c d | a b c d)`\n-        The input is extended by wrapping around to the opposite edge.\n-      - *nearest*: `(a a a a | a b c d | d d d d)`\n-        The input is extended by the nearest pixel.\n+      - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n+        reflecting about the edge of the last pixel.\n+      - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n+        filling all values beyond the edge with the same constant value k = 0.\n+      - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n+        wrapping around to the opposite edge.\n+      - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n+        nearest pixel.\n     interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n     seed: Integer. Used to create a random seed.\n     name: A string, the name of the layer.\n-    fill_value: a float represents the value to be filled outside the\n-      boundaries when `fill_mode` is \"constant\".\n-\n-  Example:\n-\n-  >>> input_img = np.random.random((32, 224, 224, 3))\n-  >>> layer = tf.keras.layers.experimental.preprocessing.RandomZoom(.5, .2)\n-  >>> out_img = layer(input_img)\n-  >>> out_img.shape\n-  TensorShape([32, 224, 224, 3])\n-\n+    fill_value: a float represents the value to be filled outside the boundaries\n+      when `fill_mode` is \"constant\".\n+  Example:  >>> input_img = np.random.random((32, 224, 224, 3)) >>> layer =\n+    tf.keras.layers.experimental.preprocessing.RandomZoom(.5, .2) >>> out_img =\n+    layer(input_img) >>> out_img.shape TensorShape([32, 224, 224, 3])\n   Input shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)`, data_format='channels_last'.\n-\n+    4D tensor with shape: `(samples, height, width, channels)`,\n+      data_format='channels_last'.\n   Output shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)`, data_format='channels_last'.\n-\n+    4D tensor with shape: `(samples, height, width, channels)`,\n+      data_format='channels_last'.\n   Raise:\n     ValueError: if lower bound is not between [0, 1], or upper bound is\n       negative.\n@@ -1035,8 +997,8 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n   \"\"\"Returns projective transform(s) for the given zoom(s).\n \n   Args:\n-    zooms: A matrix of 2-element lists representing [zx, zy] to zoom\n-      for each image (for a batch of images).\n+    zooms: A matrix of 2-element lists representing [zx, zy] to zoom for each\n+      image (for a batch of images).\n     image_height: Height of the image(s) to be transformed.\n     image_width: Width of the image(s) to be transformed.\n     name: The name of the op.\n@@ -1098,7 +1060,6 @@ class RandomContrast(PreprocessingLayer):\n       [1.0 - lower, 1.0 + upper].\n     seed: Integer. Used to create a random seed.\n     name: A string, the name of the layer.\n-\n   Raise:\n     ValueError: if lower bound is not between [0, 1], or upper bound is\n       negative.\n@@ -1117,8 +1078,8 @@ class RandomContrast(PreprocessingLayer):\n     self.seed = seed\n     self.input_spec = InputSpec(ndim=4)\n     super(RandomContrast, self).__init__(name=name, **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        'RandomContrast').set(True)\n+    base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomContrast').set(\n+        True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -1168,7 +1129,6 @@ class RandomHeight(PreprocessingLayer):\n       `gaussian`, `mitchellcubic`\n     seed: Integer. Used to create a random seed.\n     name: A string, the name of the layer.\n-\n   Input shape:\n     4D tensor with shape: `(samples, height, width, channels)`\n       (data_format='channels_last').\n@@ -1258,23 +1218,20 @@ class RandomWidth(PreprocessingLayer):\n       representing lower and upper bound for resizing vertically. When\n       represented as a single float, this value is used for both the upper and\n       lower bound. For instance, `factor=(0.2, 0.3)` results in an output with\n-      width changed by a random amount in the range `[20%, 30%]`.\n-      `factor=(-0.2, 0.3)` results in an output with width changed by a random\n-      amount in the range `[-20%, +30%]. `factor=0.2` results in an output with\n-      width changed by a random amount in the range `[-20%, +20%]`.\n+      width changed by a random amount in the range `[20%, 30%]`. `factor=(-0.2,\n+      0.3)` results in an output with width changed by a random amount in the\n+      range `[-20%, +30%]. `factor=0.2` results in an output with width changed\n+      by a random amount in the range `[-20%, +20%]`.\n     interpolation: String, the interpolation method. Defaults to `bilinear`.\n       Supports `bilinear`, `nearest`, `bicubic`, `area`, `lanczos3`, `lanczos5`,\n       `gaussian`, `mitchellcubic`\n     seed: Integer. Used to create a random seed.\n     name: A string, the name of the layer.\n-\n   Input shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)` (data_format='channels_last').\n-\n+    4D tensor with shape: `(samples, height, width, channels)`\n+      (data_format='channels_last').\n   Output shape:\n-    4D tensor with shape:\n-    `(samples, height, random_width, channels)`.\n+    4D tensor with shape: `(samples, height, random_width, channels)`.\n   \"\"\"\n \n   def __init__(self,\n\n@@ -614,6 +614,10 @@ def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n             go_backwards, sequence_lengths):\n   \"\"\"GRU with CuDNN implementation which is only available for GPU.\"\"\"\n+  if not time_major and mask is None:\n+    inputs = tf.compat.v1.transpose(inputs, perm=(1, 0, 2))\n+    seq_axis, batch_axis = (0, 1)\n+  else:\n     seq_axis, batch_axis = (0, 1) if time_major else (1, 0)\n   # For init_h, cuDNN expects one more dim of num_layers before or after batch\n   # dim for time major or batch major inputs respectively\n@@ -645,12 +649,8 @@ def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n \n   if mask is not None:\n     sequence_lengths = calculate_sequence_by_mask(mask, time_major)\n-  else:\n-    input_shape = tf.cast(tf.compat.v1.shape(inputs), tf.int32)\n-    timesteps = input_shape[seq_axis]\n-    batches = input_shape[batch_axis]\n-    sequence_lengths = tf.fill([batches], timesteps)\n \n+  if sequence_lengths is not None:\n     if go_backwards:\n       # Three reversals are required. E.g.,\n       # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked\n@@ -672,8 +672,17 @@ def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n       outputs = tf.reverse_sequence(\n           outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)\n       outputs = tf.reverse(outputs, axis=[seq_axis])\n+  else:\n+    if go_backwards:\n+      # Reverse axis 0 since the input is already convert to time major.\n+      inputs = tf.reverse(inputs, axis=[0])\n+    outputs, h, _, _ = tf.raw_ops.CudnnRNN(\n+        input=inputs, input_h=init_h, input_c=0, params=params,\n+        is_training=True, rnn_mode='gru')\n \n   last_output = outputs[-1]\n+  if not time_major and mask is None:\n+    outputs = tf.compat.v1.transpose(outputs, perm=[1, 0, 2])\n   h = tf.compat.v1.squeeze(h, axis=seq_axis)\n \n   # In the case of variable length input, the cudnn kernel will fill zeros for\n@@ -682,6 +691,7 @@ def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n   # get the final effect output instead just 0s at the last timestep.\n   # In order to mimic the default keras behavior, we copy the final h state as\n   # the last_output, since it is numerically same as the output.\n+  if mask is not None:\n     last_output = h\n \n   return last_output, outputs, h, _runtime(_RUNTIME_GPU)\n@@ -1419,6 +1429,10 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n     runtime: Constant string tensor which indicate real runtime hardware. This\n       value is for testing purpose and should not be used by user.\n   \"\"\"\n+  if not time_major and mask is None:\n+    inputs = tf.compat.v1.transpose(inputs, perm=(1, 0, 2))\n+    seq_axis, batch_axis = (0, 1)\n+  else:\n     seq_axis, batch_axis = (0, 1) if time_major else (1, 0)\n   # For init_h and init_c, cuDNN expects one more dim of num_layers before or\n   # after batch dim for time major or batch major inputs respectively\n@@ -1452,12 +1466,8 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n \n   if mask is not None:\n     sequence_lengths = calculate_sequence_by_mask(mask, time_major)\n-  else:\n-    input_shape = tf.cast(tf.compat.v1.shape(inputs), tf.int32)\n-    timesteps = input_shape[seq_axis]\n-    batches = input_shape[batch_axis]\n-    sequence_lengths = tf.fill([batches], timesteps)\n \n+  if sequence_lengths is not None:\n     if go_backwards:\n       # Three reversals are required. E.g.,\n       # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked\n@@ -1479,8 +1489,20 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n       outputs = tf.reverse_sequence(\n           outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)\n       outputs = tf.reverse(outputs, axis=[seq_axis])\n+  else:\n+    # # Fill the array with shape [batch] with value of max timesteps.\n+    # sequence_length = array_ops.fill([array_ops.shape(inputs)[1]],\n+    #                                  array_ops.shape(inputs)[0])\n+    if go_backwards:\n+      # Reverse axis 0 since the input is already convert to time major.\n+      inputs = tf.reverse(inputs, axis=[0])\n+    outputs, h, c, _ = tf.raw_ops.CudnnRNN(\n+        input=inputs, input_h=init_h, input_c=init_c, params=params,\n+        is_training=True, rnn_mode='lstm')\n \n   last_output = outputs[-1]\n+  if not time_major and mask is None:\n+    outputs = tf.compat.v1.transpose(outputs, perm=[1, 0, 2])\n   h = tf.compat.v1.squeeze(h, axis=seq_axis)\n   c = tf.compat.v1.squeeze(c, axis=seq_axis)\n \n@@ -1490,6 +1512,7 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n   # get the final effect output instead just 0s at the last timestep.\n   # In order to mimic the default keras behavior, we copy the final h state as\n   # the last_output, since it is numerically same as the output.\n+  if mask is not None:\n     last_output = h\n   return last_output, outputs, h, c, _runtime(_RUNTIME_GPU)\n \n\n@@ -1408,7 +1408,8 @@ def categorical_hinge(y_true, y_pred):\n   >>> assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))\n   \n   Args:\n-    y_true: The ground truth values. `y_true` values are expected to be 0 or 1.\n+    y_true: The ground truth values. `y_true` values are expected to be\n+    either `{-1, +1}` or `{0, 1}` (i.e. a one-hot-encoded tensor).\n     y_pred: The predicted values.\n \n   Returns:\n\n@@ -276,15 +276,13 @@ class Metric(base_layer.Layer):\n                  initializer=None,\n                  dtype=None):\n     \"\"\"Adds state variable. Only for use by subclasses.\"\"\"\n-    from keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top\n-\n     if tf.distribute.has_strategy():\n       strategy = tf.distribute.get_strategy()\n     else:\n       strategy = None\n \n     # TODO(b/120571621): Make `ON_READ` work with Keras metrics on TPU.\n-    if distributed_training_utils.is_tpu_strategy(strategy):\n+    if K.is_tpu_strategy(strategy):\n       synchronization = tf.VariableSynchronization.ON_WRITE\n \n     with tf.init_scope():\n\n@@ -96,12 +96,13 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n       ('Activation', lambda: core.Activation('sigmoid'), (2, 2)),\n       ('Reshape', lambda: core.Reshape((1, 4, 1)), (2, 2, 2)),\n       ('Permute', lambda: core.Permute((2, 1)), (2, 2, 2)),\n-      ('Attention', dense_attention.Attention,\n-       [(2, 2, 3), (2, 3, 3), (2, 3, 3)]),\n-      ('AdditiveAttention', dense_attention.AdditiveAttention,\n-       [(2, 2, 3), (2, 3, 3), (2, 3, 3)]),\n-      ('Embedding', lambda: embeddings.Embedding(4, 4), (2, 4), 2e-3, 2e-3,\n-       np.random.randint(4, size=(2, 4))),\n+      ('Attention', dense_attention.Attention, [(2, 2, 3), (2, 3, 3),\n+                                                (2, 3, 3)]),\n+      ('AdditiveAttention', dense_attention.AdditiveAttention, [(2, 2, 3),\n+                                                                (2, 3, 3),\n+                                                                (2, 3, 3)]),\n+      ('Embedding', lambda: embeddings.Embedding(4, 4),\n+       (2, 4), 2e-3, 2e-3, np.random.randint(4, size=(2, 4))),\n       ('LocallyConnected1D', lambda: local.LocallyConnected1D(2, 2), (2, 2, 1)),\n       ('LocallyConnected2D', lambda: local.LocallyConnected2D(2, 2),\n        (2, 2, 2, 1)),\n@@ -116,8 +117,8 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n       ('GaussianNoise', lambda: noise.GaussianNoise(0.5), (2, 2)),\n       ('GaussianDropout', lambda: noise.GaussianDropout(0.5), (2, 2)),\n       ('AlphaDropout', lambda: noise.AlphaDropout(0.5), (2, 2)),\n-      ('BatchNormalization', normalization_v2.BatchNormalization, (2, 2),\n-       1e-2, 1e-2),\n+      ('BatchNormalization', normalization_v2.BatchNormalization,\n+       (2, 2), 1e-2, 1e-2),\n       ('LayerNormalization', normalization.LayerNormalization, (2, 2)),\n       ('LayerNormalizationUnfused',\n        lambda: normalization.LayerNormalization(axis=1), (2, 2, 2)),\n@@ -125,8 +126,8 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n       ('AveragePooling2D', pooling.AveragePooling2D, (2, 2, 2, 1)),\n       ('GlobalMaxPooling2D', pooling.GlobalMaxPooling2D, (2, 2, 2, 1)),\n       ('GlobalAveragePooling2D', pooling.GlobalAveragePooling2D, (2, 2, 2, 1)),\n-      ('SimpleRNN', lambda: recurrent.SimpleRNN(units=4), (4, 4, 4),\n-       1e-2, 1e-2),\n+      ('SimpleRNN', lambda: recurrent.SimpleRNN(units=4),\n+       (4, 4, 4), 1e-2, 1e-2),\n       ('GRU', lambda: recurrent.GRU(units=4), (4, 4, 4)),\n       ('LSTM', lambda: recurrent.LSTM(units=4), (4, 4, 4)),\n       ('GRUV2', lambda: recurrent_v2.GRU(units=4), (4, 4, 4)),\n@@ -135,6 +136,13 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n        (2, 2, 2)),\n       ('Bidirectional',\n        lambda: wrappers.Bidirectional(recurrent.SimpleRNN(units=4)), (2, 2, 2)),\n+      ('AttentionLayerCausal', lambda: dense_attention.Attention(causal=True), [\n+          (2, 2, 3), (2, 3, 3), (2, 3, 3)\n+      ]),\n+      ('AdditiveAttentionLayerCausal',\n+       lambda: dense_attention.AdditiveAttention(causal=True), [(2, 3, 4),\n+                                                                (2, 3, 4),\n+                                                                (2, 3, 4)]),\n   )\n   def test_layer(self, f32_layer_fn, input_shape, rtol=2e-3, atol=2e-3,\n                  input_data=None):\n\n@@ -392,6 +392,10 @@ def clone_model(model, input_tensors=None, clone_function=None):\n   except that it creates new layers (and thus new weights) instead\n   of sharing the weights of the existing layers.\n \n+  `clone_model` will not preserve the uniqueness of shared objects within the\n+  model (e.g. a single variable attached to two distinct layers will be\n+  restored as two separate variables).\n+\n   Args:\n       model: Instance of `Model`\n           (could be a functional model or a Sequential model).\n@@ -419,6 +423,7 @@ def clone_model(model, input_tensors=None, clone_function=None):\n   Raises:\n       ValueError: in case of invalid `model` argument value.\n   \"\"\"\n+  with generic_utils.DisableSharedObjectScope():\n     if clone_function is None:\n       clone_function = _clone_layer\n \n\n@@ -239,6 +239,28 @@ class TestModelCloning(keras_parameterized.TestCase):\n     loss = model.train_on_batch(x, y)\n     self.assertEqual(float(loss), 0.)\n \n+  def test_clone_rnn(self):\n+    # Test cloning a model with multiple cells in an RNN.  This exercises a\n+    # few \"fancier\" features such as the `Bidrectional` wrapper and\n+    # `StackedRNNCells` under the hood.\n+    inputs = keras.Input(shape=(3, 3))\n+    cells = [\n+        keras.layers.LSTMCell(\n+            units=32,\n+            enable_caching_device=True,\n+            implementation=2,\n+            activation='relu')]\n+    rnn = keras.layers.RNN(cells, return_sequences=True)\n+    outputs = keras.layers.Bidirectional(rnn)(inputs)\n+    outputs = keras.layers.Dense(\n+        12, activation='softmax', name='scores')(outputs)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    model.compile(\n+        loss=keras.losses.CategoricalCrossentropy(),\n+        optimizer=keras.optimizer_v2.rmsprop.RMSprop(lr=0.01),\n+        metrics=['accuracy'])\n+    keras.models.clone_model(model)\n+\n   def test_model_cloning_invalid_use_cases(self):\n     seq_model = keras.models.Sequential()\n     seq_model.add(keras.layers.Dense(4, input_shape=(4,)))\n\n@@ -148,6 +148,7 @@ def save_model(model,\n     hdf5_format.save_model_to_hdf5(\n         model, filepath, overwrite, include_optimizer)\n   else:\n+    with generic_utils.SharedObjectSavingScope():\n       saved_model_save.save(model, filepath, overwrite, include_optimizer,\n                             signatures, options, save_traces)\n \n@@ -194,6 +195,7 @@ def load_model(filepath, custom_objects=None, compile=True, options=None):  # py\n       ImportError: if loading from an hdf5 file and h5py is not available.\n       IOError: In case of an invalid savefile.\n   \"\"\"\n+  with generic_utils.SharedObjectLoadingScope():\n     with generic_utils.CustomObjectScope(custom_objects or {}):\n       with load_context.load_context(options):\n         if (h5py is not None and\n\n@@ -20,6 +20,7 @@ from __future__ import print_function\n \n import tensorflow as tf\n \n+import collections\n import os\n import shutil\n import sys\n@@ -27,6 +28,7 @@ import tempfile\n \n from absl.testing import parameterized\n import numpy as np\n+from six import string_types\n \n import keras\n from keras import combinations\n@@ -850,6 +852,125 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     self.assertAllEqual(loaded_model.predict(args, batch_size=batch_size),\n                         expected)\n \n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def test_shared_objects(self):\n+    class OuterLayer(keras.layers.Layer):\n+\n+      def __init__(self, inner_layer):\n+        super(OuterLayer, self).__init__()\n+        self.inner_layer = inner_layer\n+\n+      def call(self, inputs):\n+        return self.inner_layer(inputs)\n+\n+      def get_config(self):\n+        return {\n+            'inner_layer': generic_utils.serialize_keras_object(\n+                self.inner_layer)\n+        }\n+\n+      @classmethod\n+      def from_config(cls, config):\n+        return cls(generic_utils.deserialize_keras_object(\n+            config['inner_layer']))\n+\n+    class InnerLayer(keras.layers.Layer):\n+\n+      def __init__(self):\n+        super(InnerLayer, self).__init__()\n+        self.v = self.add_weight(name='v', shape=[], dtype=tf.float32)\n+\n+      def call(self, inputs):\n+        return self.v + inputs\n+\n+      @classmethod\n+      def from_config(cls, config):\n+        return cls()\n+\n+    # Create a model with 2 output layers that share the same inner layer.\n+    inner_layer = InnerLayer()\n+    outer_layer_1 = OuterLayer(inner_layer)\n+    outer_layer_2 = OuterLayer(inner_layer)\n+    input_ = keras.Input(shape=(1,))\n+    model = keras.Model(\n+        inputs=input_, outputs=[outer_layer_1(input_), outer_layer_2(input_)])\n+\n+    # Changes to the shared layer should affect both outputs.\n+    model.layers[1].inner_layer.v.assign(5)\n+    self.assertAllEqual(model(1), [6.0, 6.0])\n+    model.layers[1].inner_layer.v.assign(3)\n+    self.assertAllEqual(model(1), [4.0, 4.0])\n+\n+    # After loading, changes to the shared layer should still affect both\n+    # outputs.\n+    def _do_assertions(loaded):\n+      loaded.layers[1].inner_layer.v.assign(5)\n+      self.assertAllEqual(loaded(1), [6.0, 6.0])\n+      loaded.layers[1].inner_layer.v.assign(3)\n+      self.assertAllEqual(loaded(1), [4.0, 4.0])\n+      loaded.layers[2].inner_layer.v.assign(5)\n+      self.assertAllEqual(loaded(1), [6.0, 6.0])\n+      loaded.layers[2].inner_layer.v.assign(3)\n+      self.assertAllEqual(loaded(1), [4.0, 4.0])\n+\n+    # We'd like to make sure we only attach shared object IDs when strictly\n+    # necessary, so we'll recursively traverse the generated config to count\n+    # whether we have the exact number we expect.\n+    def _get_all_keys_recursive(dict_or_iterable):\n+      if isinstance(dict_or_iterable, dict):\n+        for key in dict_or_iterable.keys():\n+          yield key\n+        for key in _get_all_keys_recursive(dict_or_iterable.values()):\n+          yield key\n+      elif isinstance(dict_or_iterable, string_types):\n+        return\n+      else:\n+        try:\n+          for item in dict_or_iterable:\n+            for key in _get_all_keys_recursive(item):\n+              yield key\n+        # Not an iterable or dictionary\n+        except TypeError:\n+          return\n+\n+    with generic_utils.CustomObjectScope({\n+        'OuterLayer': OuterLayer, 'InnerLayer': InnerLayer}):\n+\n+      # Test saving and loading to disk\n+      save_format = testing_utils.get_save_format()\n+      saved_model_dir = self._save_model_dir()\n+      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+      loaded = keras.models.load_model(saved_model_dir)\n+      _do_assertions(loaded)\n+\n+      # Test recreating directly from config\n+      config = model.get_config()\n+      key_count = collections.Counter(_get_all_keys_recursive(config))\n+      self.assertEqual(key_count[generic_utils.SHARED_OBJECT_KEY], 2)\n+      loaded = keras.Model.from_config(config)\n+      _do_assertions(loaded)\n+\n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def test_shared_objects_wrapper(self):\n+    \"\"\"Tests that shared layers wrapped with `Wrapper` restore correctly.\"\"\"\n+    input_ = keras.Input(shape=(1,))\n+    unwrapped = keras.layers.Layer(name='unwrapped')\n+    wrapped = keras.layers.Wrapper(unwrapped, name='wrapped')\n+    model = keras.Model(inputs=input_,\n+                        outputs=[unwrapped(input_), wrapped(input_)])\n+\n+    # Test recreating directly from config\n+    config = model.get_config()\n+    loaded = keras.Model.from_config(config)\n+    self.assertIs(loaded.layers[1], loaded.layers[2].layer)\n+\n+    # Test saving and loading to disk\n+    save_format = testing_utils.get_save_format()\n+    saved_model_dir = self._save_model_dir()\n+    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+    loaded = keras.models.load_model(saved_model_dir)\n+    self.assertIs(loaded.layers[1], loaded.layers[2].layer)\n+\n \n # Factory functions to create models that will be serialized inside a Network.\n def _make_graph_network(input_size, output_size):\n\n@@ -47,7 +47,6 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n     # TODO(kathywu): Synchronize with the keras spec (go/keras-json-spec) once\n     # the python config serialization has caught up.\n     metadata = dict(\n-        class_name=generic_utils.get_registered_name(type(self.obj)),\n         name=self.obj.name,\n         trainable=self.obj.trainable,\n         expects_training_arg=self.obj._expects_training_arg,  # pylint: disable=protected-access\n@@ -57,7 +56,7 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n         must_restore_from_config=self.obj._must_restore_from_config,  # pylint: disable=protected-access\n     )\n \n-    metadata.update(get_config(self.obj))\n+    metadata.update(get_serialized(self.obj))\n     if self.obj.input_spec is not None:\n       # Layer's input_spec has already been type-checked in the property setter.\n       metadata['input_spec'] = tf.nest.map_structure(\n@@ -111,16 +110,12 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n \n # TODO(kathywu): Move serialization utils (and related utils from\n # generic_utils.py) to a separate file.\n-def get_config(obj):\n+def get_serialized(obj):\n   with generic_utils.skip_failed_serialization():\n     # Store the config dictionary, which may be used when reviving the object.\n     # When loading, the program will attempt to revive the object from config,\n     # and if that fails, the object will be revived from the SavedModel.\n-    config = generic_utils.serialize_keras_object(obj)['config']\n-\n-  if config is not None:\n-    return {'config': config}\n-  return {}\n+    return generic_utils.serialize_keras_object(obj)\n \n \n class InputLayerSavedModelSaver(base_serialization.SavedModelSaver):\n\n@@ -485,13 +485,15 @@ class KerasObjectLoader(object):\n     #       found.\n     class_name = metadata.get('class_name')\n     config = metadata.get('config')\n+    shared_object_id = metadata.get('shared_object_id')\n     must_restore_from_config = metadata.get('must_restore_from_config')\n     if not generic_utils.validate_config(config):\n       return None\n \n     try:\n       obj = layers_module.deserialize(\n-          generic_utils.serialize_keras_class_and_config(class_name, config))\n+          generic_utils.serialize_keras_class_and_config(\n+              class_name, config, shared_object_id=shared_object_id))\n     except ValueError:\n       if must_restore_from_config:\n         raise RuntimeError(\n\n@@ -36,7 +36,7 @@ class MetricSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n         class_name=generic_utils.get_registered_name(type(self.obj)),\n         name=self.obj.name,\n         dtype=self.obj.dtype)\n-    metadata.update(layer_serialization.get_config(self.obj))\n+    metadata.update(layer_serialization.get_serialized(self.obj))\n     if self.obj._build_input_shape is not None:  # pylint: disable=protected-access\n       metadata['build_input_shape'] = self.obj._build_input_shape  # pylint: disable=protected-access\n     return metadata\n\n@@ -26,8 +26,10 @@ import marshal\n import os\n import re\n import sys\n+import threading\n import time\n import types as python_types\n+import weakref\n \n import numpy as np\n import six\n@@ -110,9 +112,223 @@ def get_custom_objects():\n   return _GLOBAL_CUSTOM_OBJECTS\n \n \n-def serialize_keras_class_and_config(cls_name, cls_config):\n+# Store a unique, per-object ID for shared objects.\n+#\n+# We store a unique ID for each object so that we may, at loading time,\n+# re-create the network properly.  Without this ID, we would have no way of\n+# determining whether a config is a description of a new object that\n+# should be created or is merely a reference to an already-created object.\n+SHARED_OBJECT_KEY = 'shared_object_id'\n+\n+\n+SHARED_OBJECT_DISABLED = threading.local()\n+SHARED_OBJECT_LOADING = threading.local()\n+SHARED_OBJECT_SAVING = threading.local()\n+\n+\n+# Attributes on the threadlocal variable must be set per-thread, thus we\n+# cannot initialize these globally. Instead, we have accessor functions with\n+# default values.\n+def _shared_object_disabled():\n+  \"\"\"Get whether shared object handling is disabled in a threadsafe manner.\"\"\"\n+  return getattr(SHARED_OBJECT_DISABLED, 'disabled', False)\n+\n+\n+def _shared_object_loading_scope():\n+  \"\"\"Get the current shared object saving scope in a threadsafe manner.\"\"\"\n+  return getattr(SHARED_OBJECT_LOADING, 'scope', NoopLoadingScope())\n+\n+\n+def _shared_object_saving_scope():\n+  \"\"\"Get the current shared object saving scope in a threadsafe manner.\"\"\"\n+  return getattr(SHARED_OBJECT_SAVING, 'scope', None)\n+\n+\n+class DisableSharedObjectScope(object):\n+  \"\"\"A context manager for disabling handling of shared objects.\n+\n+  Disables shared object handling for both saving and loading.\n+\n+  Created primarily for use with `clone_model`, which does extra surgery that\n+  is incompatible with shared objects.\n+  \"\"\"\n+\n+  def __enter__(self):\n+    SHARED_OBJECT_DISABLED.disabled = True\n+    self._orig_loading_scope = _shared_object_loading_scope()\n+    self._orig_saving_scope = _shared_object_saving_scope()\n+\n+  def __exit__(self, *args, **kwargs):\n+    SHARED_OBJECT_DISABLED.disabled = False\n+    SHARED_OBJECT_LOADING.scope = self._orig_loading_scope\n+    SHARED_OBJECT_SAVING.scope = self._orig_saving_scope\n+\n+\n+class NoopLoadingScope(object):\n+  \"\"\"The default shared object loading scope. It does nothing.\n+\n+  Created to simplify serialization code that doesn't care about shared objects\n+  (e.g. when serializing a single object).\n+  \"\"\"\n+\n+  def get(self, unused_object_id):\n+    return None\n+\n+  def set(self, object_id, obj):\n+    pass\n+\n+\n+class SharedObjectLoadingScope(object):\n+  \"\"\"A context manager for keeping track of loaded objects.\n+\n+  During the deserialization process, we may come across objects that are\n+  shared across multiple layers. In order to accurately restore the network\n+  structure to its original state, `SharedObjectLoadingScope` allows us to\n+  re-use shared objects rather than cloning them.\n+  \"\"\"\n+\n+  def __enter__(self):\n+    if _shared_object_disabled():\n+      return NoopLoadingScope()\n+\n+    global SHARED_OBJECT_LOADING\n+    SHARED_OBJECT_LOADING.scope = self\n+    self._obj_ids_to_obj = {}\n+    return self\n+\n+  def get(self, object_id):\n+    \"\"\"Given a shared object ID, returns a previously instantiated object.\n+\n+    Args:\n+      object_id: shared object ID to use when attempting to find already-loaded\n+        object.\n+\n+    Returns:\n+      The object, if we've seen this ID before. Else, `None`.\n+    \"\"\"\n+    # Explicitly check for `None` internally to make external calling code a\n+    # bit cleaner.\n+    if object_id is None:\n+      return\n+    return self._obj_ids_to_obj.get(object_id)\n+\n+  def set(self, object_id, obj):\n+    \"\"\"Stores an instantiated object for future lookup and sharing.\"\"\"\n+    if object_id is None:\n+      return\n+    self._obj_ids_to_obj[object_id] = obj\n+\n+  def __exit__(self, *args, **kwargs):\n+    global SHARED_OBJECT_LOADING\n+    SHARED_OBJECT_LOADING.scope = NoopLoadingScope()\n+\n+\n+class SharedObjectConfig(dict):\n+  \"\"\"A configuration container that keeps track of references.\n+\n+  `SharedObjectConfig` will automatically attach a shared object ID to any\n+  configs which are referenced more than once, allowing for proper shared\n+  object reconstruction at load time.\n+\n+  In most cases, it would be more proper to subclass something like\n+  `collections.UserDict` or `collections.Mapping` rather than `dict` directly.\n+  Unfortunately, python's json encoder does not support `Mapping`s. This is\n+  important functionality to retain, since we are dealing with serialization.\n+\n+  We should be safe to subclass `dict` here, since we aren't actually\n+  overriding any core methods, only augmenting with a new one for reference\n+  counting.\n+  \"\"\"\n+\n+  def __init__(self, base_config, object_id, **kwargs):\n+    self.ref_count = 1\n+    self.object_id = object_id\n+    super(SharedObjectConfig, self).__init__(base_config, **kwargs)\n+\n+  def increment_ref_count(self):\n+    # As soon as we've seen the object more than once, we want to attach the\n+    # shared object ID. This allows us to only attach the shared object ID when\n+    # it's strictly necessary, making backwards compatibility breakage less\n+    # likely.\n+    if self.ref_count == 1:\n+      self[SHARED_OBJECT_KEY] = self.object_id\n+    self.ref_count += 1\n+\n+\n+class SharedObjectSavingScope(object):\n+  \"\"\"Keeps track of shared object configs when serializing.\"\"\"\n+\n+  def __enter__(self):\n+    if _shared_object_disabled():\n+      return None\n+\n+    global SHARED_OBJECT_SAVING\n+\n+    # Serialization can happen at a number of layers for a number of reasons.\n+    # We may end up with a case where we're opening a saving scope within\n+    # another saving scope. In that case, we'd like to use the outermost scope\n+    # available and ignore inner scopes, since there is not (yet) a reasonable\n+    # use case for having these nested and distinct.\n+    if _shared_object_saving_scope() is not None:\n+      self._passthrough = True\n+      return _shared_object_saving_scope()\n+    else:\n+      self._passthrough = False\n+\n+    SHARED_OBJECT_SAVING.scope = self\n+    self._shared_objects_config = weakref.WeakKeyDictionary()\n+    self._next_id = 0\n+    return self\n+\n+  def get_config(self, obj):\n+    \"\"\"Gets a `SharedObjectConfig` if one has already been seen for `obj`.\n+\n+    Args:\n+      obj: The object for which to retrieve the `SharedObjectConfig`.\n+\n+    Returns:\n+      The SharedObjectConfig for a given object, if already seen. Else,\n+        `None`.\n+    \"\"\"\n+    if obj in self._shared_objects_config:\n+      shared_object_config = self._shared_objects_config[obj]\n+      shared_object_config.increment_ref_count()\n+      return shared_object_config\n+\n+  def create_config(self, base_config, obj):\n+    shared_object_config = SharedObjectConfig(base_config, self._next_id)\n+    self._next_id += 1\n+    self._shared_objects_config[obj] = shared_object_config\n+    return shared_object_config\n+\n+  def __exit__(self, *args, **kwargs):\n+    if not getattr(self, '_passthrough', False):\n+      global SHARED_OBJECT_SAVING\n+      SHARED_OBJECT_SAVING.scope = None\n+\n+\n+def serialize_keras_class_and_config(\n+    cls_name, cls_config, obj=None, shared_object_id=None):\n   \"\"\"Returns the serialization of the class with the given config.\"\"\"\n-  return {'class_name': cls_name, 'config': cls_config}\n+  base_config = {'class_name': cls_name, 'config': cls_config}\n+\n+  # We call `serialize_keras_class_and_config` for some branches of the load\n+  # path. In that case, we may already have a shared object ID we'd like to\n+  # retain.\n+  if shared_object_id is not None:\n+    base_config[SHARED_OBJECT_KEY] = shared_object_id\n+\n+  # If we have an active `SharedObjectSavingScope`, check whether we've already\n+  # serialized this config. If so, just use that config. This will store an\n+  # extra ID field in the config, allowing us to re-create the shared object\n+  # relationship at load time.\n+  if _shared_object_saving_scope() is not None and obj is not None:\n+    shared_object_config = _shared_object_saving_scope().get_config(obj)\n+    if shared_object_config is None:\n+      return _shared_object_saving_scope().create_config(base_config, obj)\n+    return shared_object_config\n+\n+  return base_config\n \n \n @keras_export('keras.utils.register_keras_serializable')\n@@ -234,7 +450,19 @@ def get_registered_object(name, custom_objects=None, module_objects=None):\n \n @keras_export('keras.utils.serialize_keras_object')\n def serialize_keras_object(instance):\n-  \"\"\"Serialize a Keras object into a JSON-compatible representation.\"\"\"\n+  \"\"\"Serialize a Keras object into a JSON-compatible representation.\n+\n+  Calls to `serialize_keras_object` while underneath the\n+  `SharedObjectSavingScope` context manager will cause any objects re-used\n+  across multiple layers to be saved with a special shared object ID. This\n+  allows the network to be re-created properly during deserialization.\n+\n+  Args:\n+    instance: The object to serialize.\n+\n+  Returns:\n+    A dict-like, JSON-compatible representation of the object's config.\n+  \"\"\"\n   _, instance = tf.__internal__.decorator.unwrap(instance)\n   if instance is None:\n     return None\n@@ -265,7 +493,8 @@ def serialize_keras_object(instance):\n         serialization_config[key] = item\n \n     name = get_registered_name(instance.__class__)\n-    return serialize_keras_class_and_config(name, serialization_config)\n+    return serialize_keras_class_and_config(\n+        name, serialization_config, instance)\n   if hasattr(instance, '__name__'):\n     return get_registered_name(instance)\n   raise ValueError('Cannot serialize', instance)\n@@ -286,8 +515,9 @@ def class_and_config_for_serialized_keras_object(\n     custom_objects=None,\n     printable_module_name='object'):\n   \"\"\"Returns the class name and config for a serialized keras object.\"\"\"\n-  if (not isinstance(config, dict) or 'class_name' not in config or\n-      'config' not in config):\n+  if (not isinstance(config, dict)\n+      or 'class_name' not in config\n+      or 'config' not in config):\n     raise ValueError('Improper config format: ' + str(config))\n \n   class_name = config['class_name']\n@@ -341,7 +571,24 @@ def deserialize_keras_object(identifier,\n                              module_objects=None,\n                              custom_objects=None,\n                              printable_module_name='object'):\n-  \"\"\"Turns the serialized form of a Keras object back into an actual object.\"\"\"\n+  \"\"\"Turns the serialized form of a Keras object back into an actual object.\n+\n+  Calls to `deserialize_keras_object` while underneath the\n+  `SharedObjectLoadingScope` context manager will cause any already-seen shared\n+  objects to be returned as-is rather than creating a new object.\n+\n+  Args:\n+    identifier: the serialized form of the object.\n+    module_objects: A dictionary of custom objects to look the name up in.\n+      Generally, module_objects is provided by midlevel library implementers.\n+    custom_objects: A dictionary of custom objects to look the name up in.\n+      Generally, custom_objects is provided by the user.\n+    printable_module_name: A human-readable string representing the type of the\n+      object. Printed in case of exception.\n+\n+  Returns:\n+    The deserialized object.\n+  \"\"\"\n   if identifier is None:\n     return None\n \n@@ -351,25 +598,39 @@ def deserialize_keras_object(identifier,\n     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n         config, module_objects, custom_objects, printable_module_name)\n \n+    # If this object has already been loaded (i.e. it's shared between multiple\n+    # objects), return the already-loaded object.\n+    shared_object_id = config.get(SHARED_OBJECT_KEY)\n+    shared_object = _shared_object_loading_scope().get(shared_object_id)  # pylint: disable=assignment-from-none\n+    if shared_object is not None:\n+      return shared_object\n+\n     if hasattr(cls, 'from_config'):\n       arg_spec = tf_inspect.getfullargspec(cls.from_config)\n       custom_objects = custom_objects or {}\n \n       if 'custom_objects' in arg_spec.args:\n-        return cls.from_config(\n+        deserialized_obj = cls.from_config(\n             cls_config,\n             custom_objects=dict(\n                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n                 list(custom_objects.items())))\n+      else:\n         with CustomObjectScope(custom_objects):\n-        return cls.from_config(cls_config)\n+          deserialized_obj = cls.from_config(cls_config)\n     else:\n       # Then `cls` may be a function returning a class.\n       # in this case by convention `config` holds\n       # the kwargs of the function.\n       custom_objects = custom_objects or {}\n       with CustomObjectScope(custom_objects):\n-        return cls(**cls_config)\n+        deserialized_obj = cls(**cls_config)\n+\n+    # Add object to shared objects, in case we find it referenced again.\n+    _shared_object_loading_scope().set(shared_object_id, deserialized_obj)\n+\n+    return deserialized_obj\n+\n   elif isinstance(identifier, six.string_types):\n     object_name = identifier\n     if custom_objects and object_name in custom_objects:\n\n@@ -25,6 +25,7 @@ from functools import partial\n import numpy as np\n \n import keras\n+from keras.utils import generic_utils\n \n \n class HasArgTest(tf.test.TestCase):\n@@ -385,5 +386,63 @@ class SliceArraysTest(tf.test.TestCase):\n         [None, None, None])\n \n \n+# object() alone isn't compatible with WeakKeyDictionary, which we use to\n+# track shared configs.\n+class MaybeSharedObject(object):\n+  pass\n+\n+\n+class SharedObjectScopeTest(tf.test.TestCase):\n+\n+  def test_shared_object_saving_scope_single_object_doesnt_export_id(self):\n+    with generic_utils.SharedObjectSavingScope() as scope:\n+      single_object = MaybeSharedObject()\n+      self.assertIsNone(scope.get_config(single_object))\n+      single_object_config = scope.create_config({}, single_object)\n+      self.assertIsNotNone(single_object_config)\n+      self.assertNotIn(generic_utils.SHARED_OBJECT_KEY,\n+                       single_object_config)\n+\n+  def test_shared_object_saving_scope_shared_object_exports_id(self):\n+    with generic_utils.SharedObjectSavingScope() as scope:\n+      shared_object = MaybeSharedObject()\n+      self.assertIsNone(scope.get_config(shared_object))\n+      scope.create_config({}, shared_object)\n+      first_object_config = scope.get_config(shared_object)\n+      second_object_config = scope.get_config(shared_object)\n+      self.assertIn(generic_utils.SHARED_OBJECT_KEY,\n+                    first_object_config)\n+      self.assertIn(generic_utils.SHARED_OBJECT_KEY,\n+                    second_object_config)\n+      self.assertIs(first_object_config, second_object_config)\n+\n+  def test_shared_object_loading_scope_noop(self):\n+    # Test that, without a context manager scope, adding configs will do\n+    # nothing.\n+    obj_id = 1\n+    obj = MaybeSharedObject()\n+    generic_utils._shared_object_loading_scope().set(obj_id, obj)\n+    self.assertIsNone(generic_utils._shared_object_loading_scope().get(obj_id))\n+\n+  def test_shared_object_loading_scope_returns_shared_obj(self):\n+    obj_id = 1\n+    obj = MaybeSharedObject()\n+    with generic_utils.SharedObjectLoadingScope() as scope:\n+      scope.set(obj_id, obj)\n+      self.assertIs(scope.get(obj_id), obj)\n+\n+  def test_nested_shared_object_saving_scopes(self):\n+    my_obj = MaybeSharedObject()\n+    with generic_utils.SharedObjectSavingScope() as scope_1:\n+      scope_1.create_config({}, my_obj)\n+      with generic_utils.SharedObjectSavingScope() as scope_2:\n+        # Nesting saving scopes should return the original scope and should\n+        # not clear any objects we're tracking.\n+        self.assertIs(scope_1, scope_2)\n+        self.assertIsNotNone(scope_2.get_config(my_obj))\n+      self.assertIsNotNone(scope_1.get_config(my_obj))\n+    self.assertIsNone(generic_utils._shared_object_saving_scope())\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -25,11 +25,11 @@ import functools\n import weakref\n \n from enum import Enum\n+from keras import backend\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n from keras.utils.generic_utils import to_list\n from tensorflow.python.ops import control_flow_ops\n-from tensorflow.python.tpu import tpu\n \n NEG_INF = -1e10\n \n@@ -67,7 +67,7 @@ def update_state_wrapper(update_state_fn):\n     # replica.\n \n     for weight in metric_obj.weights:\n-      if (tpu.is_tpu_strategy(strategy) and\n+      if (backend.is_tpu_strategy(strategy) and\n           not strategy.extended.variable_created_in_scope(weight)\n           and not tf.distribute.in_cross_replica_context()):\n         raise ValueError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
