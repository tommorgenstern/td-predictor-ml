{"custom_id": "keras#dfe8af72989844a141c68a534a4444b35f4fbae1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 27 | Files Changed: 3 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 50 | Churn Cumulative: 663 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -160,9 +160,6 @@ from keras.src.layers.preprocessing.image_preprocessing.random_flip import (\n from keras.src.layers.preprocessing.image_preprocessing.random_rotation import (\n     RandomRotation,\n )\n-from keras.src.layers.preprocessing.image_preprocessing.random_shear import (\n-    RandomShear,\n-)\n from keras.src.layers.preprocessing.image_preprocessing.random_translation import (\n     RandomTranslation,\n )\n\n@@ -160,9 +160,6 @@ from keras.src.layers.preprocessing.image_preprocessing.random_flip import (\n from keras.src.layers.preprocessing.image_preprocessing.random_rotation import (\n     RandomRotation,\n )\n-from keras.src.layers.preprocessing.image_preprocessing.random_shear import (\n-    RandomShear,\n-)\n from keras.src.layers.preprocessing.image_preprocessing.random_translation import (\n     RandomTranslation,\n )\n\n@@ -66,12 +66,16 @@ class RandomRotation(BaseImagePreprocessingLayer):\n         seed: Integer. Used to create a random seed.\n         fill_value: a float represents the value to be filled outside\n             the boundaries when `fill_mode=\"constant\"`.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, height, width)`. It defaults to the\n+            `image_data_format` value found in your Keras config file at\n+            `~/.keras/keras.json`. If you never set it, then it will be\n+            `\"channels_last\"`.\n     \"\"\"\n \n-    _VALUE_RANGE_VALIDATION_ERROR = (\n-        \"The `value_range` argument should be a list of two numbers. \"\n-    )\n-\n     _SUPPORTED_FILL_MODE = (\"reflect\", \"wrap\", \"constant\", \"nearest\")\n     _SUPPORTED_INTERPOLATION = (\"nearest\", \"bilinear\")\n \n@@ -82,14 +86,12 @@ class RandomRotation(BaseImagePreprocessingLayer):\n         interpolation=\"bilinear\",\n         seed=None,\n         fill_value=0.0,\n-        value_range=(0, 255),\n         data_format=None,\n         **kwargs,\n     ):\n         super().__init__(factor=factor, data_format=data_format, **kwargs)\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n-        self._set_value_range(value_range)\n         self.fill_mode = fill_mode\n         self.interpolation = interpolation\n         self.fill_value = fill_value\n@@ -106,19 +108,6 @@ class RandomRotation(BaseImagePreprocessingLayer):\n                 f\"{self._SUPPORTED_INTERPOLATION}.\"\n             )\n \n-    def _set_value_range(self, value_range):\n-        if not isinstance(value_range, (tuple, list)):\n-            raise ValueError(\n-                self.value_range_VALIDATION_ERROR\n-                + f\"Received: value_range={value_range}\"\n-            )\n-        if len(value_range) != 2:\n-            raise ValueError(\n-                self.value_range_VALIDATION_ERROR\n-                + f\"Received: value_range={value_range}\"\n-            )\n-        self.value_range = sorted(value_range)\n-\n     def transform_images(self, images, transformation, training=True):\n         images = self.backend.cast(images, self.compute_dtype)\n         if training:\n@@ -138,7 +127,21 @@ class RandomRotation(BaseImagePreprocessingLayer):\n     def transform_bounding_boxes(\n         self, bounding_boxes, transformation, training=True\n     ):\n-        raise NotImplementedError\n+        boxes = bounding_boxes[\"boxes\"]\n+        shape = self.backend.shape(boxes)\n+        ones = self.backend.ones((shape[0], shape[1], 1, 1))\n+        homogeneous_boxes = self.backend.concatenate([boxes, ones], axis=2)\n+        transformed_boxes = self.backend.matmul(\n+            transformation[\"rotation_matrix\"], homogeneous_boxes\n+        )\n+        # Convert back to xyxy format\n+        transformed_boxes = (\n+            transformed_boxes[:, :, :2, :] / transformed_boxes[:, :, 2:3, :]\n+        )\n+        transformed_boxes = self.backend.reshape(\n+            transformed_boxes, (shape[0], shape[1], 4)\n+        )\n+        return {\"boxes\": transformed_boxes, \"labels\": bounding_boxes[\"labels\"]}\n \n     def transform_segmentation_masks(\n         self, segmentation_masks, transformation, training=True\n@@ -233,7 +236,6 @@ class RandomRotation(BaseImagePreprocessingLayer):\n     def get_config(self):\n         config = {\n             \"factor\": self.factor,\n-            \"value_range\": self.value_range,\n             \"data_format\": self.data_format,\n             \"fill_mode\": self.fill_mode,\n             \"fill_value\": self.fill_value,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#18e64f841957b2a624f4bcf5d82fd63d5db7f61a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 928 | Contributors (this commit): 11 | Commits (past 90d): 11 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -223,6 +223,7 @@ class DataParallelDistributionTest(testing.TestCase):\n         self.assertIs(data_layout.device_mesh, self.device_mesh)\n         self.assertEqual(data_layout.axes, (\"data\", None, None))\n \n+    @pytest.mark.skipif(testing.jax_uses_gpu(), reason=\"CI segfault\")\n     def test_get_variable_layout(self):\n         distribution = distribution_lib.DataParallel(\n             device_mesh=self.device_mesh\n\n@@ -8334,6 +8334,10 @@ class NumpyDtypeTest(testing.TestCase):\n         )\n \n \n+@pytest.mask.skipif(\n+    testing.torch_uses_gpu(),\n+    reason=\"histogram op not implemented for torch on gpu\",\n+)\n class HistogramTest(testing.TestCase):\n     def test_histogram_default_args(self):\n         hist_op = knp.histogram\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#26dea1b431a92432d4e92dbf43f62548c8697450", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 910 | Contributors (this commit): 11 | Commits (past 90d): 11 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -8334,7 +8334,7 @@ class NumpyDtypeTest(testing.TestCase):\n         )\n \n \n-@pytest.mask.skipif(\n+@pytest.mark.skipif(\n     testing.torch_uses_gpu(),\n     reason=\"histogram op not implemented for torch on gpu\",\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca8861328bf96864ff98d8f9b439976071371a08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 21 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -268,6 +268,7 @@ class ModelParallelDistributionTest(testing.TestCase):\n             shape, axis_names, self.devices\n         )\n \n+    @pytest.mark.skipif(testing.jax_uses_gpu(), reason=\"CI segfault\")\n     def test_distribute_weights(self):\n         layout_map = distribution_lib.LayoutMap(self.device_mesh)\n         layout_map[\".*kernel\"] = distribution_lib.TensorLayout([None, \"model\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#726a38fc52cf2277f6a82815e2212ab410528dd2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 1382 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,8 @@ from keras.src.saving import serialization_lib\n class Pipeline(Layer):\n     \"\"\"Applies a series of layers to an input.\n \n-    This class is useful to build a preprocessing pipeline.\n+    This class is useful to build a preprocessing pipeline,\n+    in particular an image data augmentation pipeline.\n     Compared to a `Sequential` model, `Pipeline` features\n     a few important differences:\n \n\n@@ -111,7 +111,7 @@ class KerasFileEditor:\n         \"\"\"Prints the weight structure of the opened file.\"\"\"\n         self._weights_summary_cli()\n \n-    def compare_to(self, reference_model):\n+    def compare(self, reference_model):\n         \"\"\"Compares the opened file to a reference model.\n \n         This method will list all mismatches between the\n@@ -411,7 +411,7 @@ class KerasFileEditor:\n \n         self._edit_object(add_weight_fn, object_name)\n \n-    def resave_weights(self, filepath):\n+    def save(self, filepath):\n         \"\"\"Save the edited weights file.\n \n         Args:\n@@ -448,6 +448,9 @@ class KerasFileEditor:\n         _save(self.weights_dict, weights_store, inner_path=\"\")\n         weights_store.close()\n \n+    def resave_weights(self, filepath):\n+        self.save(filepath)\n+\n     def _extract_weights_from_store(self, data, metadata=None, inner_path=\"\"):\n         metadata = metadata or {}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ec0f46a789e17b8d9244080cb3fc8131c924331", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 11 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 115 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -37,55 +37,55 @@ class SavingTest(testing.TestCase):\n \n         target_model = get_target_model()\n \n-        out = editor.compare_to(model)  # Succeeds\n+        out = editor.compare(model)  # Succeeds\n         self.assertEqual(out[\"status\"], \"success\")\n-        out = editor.compare_to(target_model)  # Fails\n+        out = editor.compare(target_model)  # Fails\n \n         editor.add_object(\n             \"layers/dense_3\", weights={\"0\": np.random.random((3, 3))}\n         )\n-        out = editor.compare_to(target_model)  # Fails\n+        out = editor.compare(target_model)  # Fails\n         self.assertEqual(out[\"status\"], \"error\")\n         self.assertEqual(out[\"error_count\"], 2)\n \n         editor.rename_object(\"dense_3\", \"dense_4\")\n         editor.rename_object(\"layers/dense_4\", \"dense_2\")\n         editor.add_weights(\"dense_2\", weights={\"1\": np.random.random((3,))})\n-        out = editor.compare_to(target_model)  # Succeeds\n+        out = editor.compare(target_model)  # Succeeds\n         self.assertEqual(out[\"status\"], \"success\")\n \n         editor.add_object(\n             \"layers/dense_3\", weights={\"0\": np.random.random((3, 3))}\n         )\n-        out = editor.compare_to(target_model)  # Fails\n+        out = editor.compare(target_model)  # Fails\n         self.assertEqual(out[\"status\"], \"error\")\n         self.assertEqual(out[\"error_count\"], 1)\n \n         editor.delete_object(\"layers/dense_3\")\n-        out = editor.compare_to(target_model)  # Succeeds\n+        out = editor.compare(target_model)  # Succeeds\n         self.assertEqual(out[\"status\"], \"success\")\n         editor.summary()\n \n         temp_filepath = os.path.join(self.get_temp_dir(), \"resaved.weights.h5\")\n-        editor.resave_weights(temp_filepath)\n+        editor.save(temp_filepath)\n         target_model.load_weights(temp_filepath)\n \n         editor = KerasFileEditor(temp_filepath)\n         editor.summary()\n-        out = editor.compare_to(target_model)  # Succeeds\n+        out = editor.compare(target_model)  # Succeeds\n         self.assertEqual(out[\"status\"], \"success\")\n \n         editor.delete_weight(\"dense_2\", \"1\")\n-        out = editor.compare_to(target_model)  # Fails\n+        out = editor.compare(target_model)  # Fails\n         self.assertEqual(out[\"status\"], \"error\")\n         self.assertEqual(out[\"error_count\"], 1)\n \n         editor.add_weights(\"dense_2\", {\"1\": np.zeros((7,))})\n-        out = editor.compare_to(target_model)  # Fails\n+        out = editor.compare(target_model)  # Fails\n         self.assertEqual(out[\"status\"], \"error\")\n         self.assertEqual(out[\"error_count\"], 1)\n \n         editor.delete_weight(\"dense_2\", \"1\")\n         editor.add_weights(\"dense_2\", {\"1\": np.zeros((3,))})\n-        out = editor.compare_to(target_model)  # Succeeds\n+        out = editor.compare(target_model)  # Succeeds\n         self.assertEqual(out[\"status\"], \"success\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7004f526cfd0933f407f1b4f530b865079aa9bef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 86 | Churn Cumulative: 284 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,3 @@\n-import jax\n-from jax import numpy as jnp\n-\n-from keras.src.optimizers import base_optimizer\n-\n-\n-class JaxOptimizer(base_optimizer.BaseOptimizer):\n \"\"\"A class for JAX specific optimizer logic.\n \n Its purpose is to route around statelessness\n@@ -13,6 +6,14 @@ class JaxOptimizer(base_optimizer.BaseOptimizer):\n by skipping conditionals entirely.\n \"\"\"\n \n+import jax\n+from jax import numpy as jnp\n+\n+from keras.src.optimizers import base_optimizer\n+\n+\n+class JaxOptimizer(base_optimizer.BaseOptimizer):\n+\n     def _backend_apply_gradients(self, grads, trainable_variables):\n         if self.gradient_accumulation_steps:\n             is_update_step = (\n\n@@ -1,3 +1,12 @@\n+\"\"\"A class for Tensorflow specific optimizer logic.\n+\n+The major behavior change for this class is for tf.distribute.\n+\n+It will override methods from base Keras core Optimizer,\n+which provide distribute specific functionality, e.g. variable\n+creation, loss reduction, etc.\n+\"\"\"\n+\n import warnings\n \n import tensorflow as tf\n@@ -9,14 +18,6 @@ from keras.src.optimizers import base_optimizer\n \n \n class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n-    \"\"\"A class for Tensorflow specific optimizer logic.\n-\n-    The major behavior change for this class is for tf.distribute.\n-\n-    It will override methods from base Keras core Optimizer,\n-    which provide distribute specific functionality, e.g. variable\n-    creation, loss reduction, etc.\n-    \"\"\"\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n\n@@ -12,6 +12,59 @@ from keras.src.utils.naming import auto_name\n \n \n class BaseOptimizer(KerasSaveable):\n+    \"\"\"Abstract optimizer base class.\n+\n+    If you intend to create your own optimization algorithm, please inherit from\n+    this class and override the following methods:\n+\n+    - `build`: Create your optimizer-related variables, such as momentum\n+        variables in the SGD optimizer.\n+    - `update_step`: Implement your optimizer's variable updating logic.\n+    - `get_config`: serialization of the optimizer.\n+\n+    Example:\n+\n+    ```python\n+    class SGD(Optimizer):\n+        def __init__(self, **kwargs):\n+            super().__init__(**kwargs)\n+            self.momentum = 0.9\n+\n+        def build(self, variables):\n+            super().build(variables)\n+            self.momentums = []\n+            for variable in variables:\n+                self.momentums.append(\n+                    self.add_variable_from_reference(\n+                        reference_variable=variable, name=\"momentum\"\n+                    )\n+                )\n+\n+        def update_step(self, gradient, variable, learning_rate):\n+            learning_rate = ops.cast(learning_rate, variable.dtype)\n+            gradient = ops.cast(gradient, variable.dtype)\n+            m = self.momentums[self._get_variable_index(variable)]\n+            self.assign(\n+                m,\n+                ops.subtract(\n+                    ops.multiply(m, ops.cast(self.momentum, variable.dtype)),\n+                    ops.multiply(gradient, learning_rate),\n+                ),\n+            )\n+            self.assign_add(variable, m)\n+\n+        def get_config(self):\n+            config = super().get_config()\n+            config.update(\n+                {\n+                    \"momentum\": self.momentum,\n+                    \"nesterov\": self.nesterov,\n+                }\n+            )\n+            return config\n+    ```\n+    \"\"\"\n+\n     def __init__(\n         self,\n         learning_rate,\n\n@@ -23,4 +23,5 @@ class Optimizer(BackendOptimizer, base_optimizer.BaseOptimizer):\n     pass\n \n \n+Optimizer.__doc__ = base_optimizer.BaseOptimizer.__doc__\n base_optimizer_keyword_args = base_optimizer.base_optimizer_keyword_args\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5aa5f88dc200bbf2cd765d5a213c23c58da48e80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 30 | Churn Cumulative: 755 | Contributors (this commit): 8 | Commits (past 90d): 17 | Contributors (cumulative): 13 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -1122,5 +1122,14 @@ def model_supports_jit(model):\n                 return False\n     # XLA not supported by some layers\n     if all(x.supports_jit for x in model._flatten_layers()):\n+        if backend.backend() == \"tensorflow\":\n+            from tensorflow.python.framework.config import (\n+                is_op_determinism_enabled,\n+            )\n+\n+            if is_op_determinism_enabled():\n+                # disable XLA with determinism enabled since not all ops are\n+                # supported by XLA with determinism enabled.\n+                return False\n         return True\n     return False\n\n@@ -1718,6 +1718,27 @@ class TestTrainer(testing.TestCase):\n         for v in model._compile_loss.variables:\n             self.assertAllClose(v, 0.0)\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\",\n+        reason=\"This test is only applicable to TensorFlow.\",\n+    )\n+    @pytest.mark.requires_trainable_backend\n+    def test_jit_compile_with_tf_determinism(self):\n+        from tensorflow.python.framework.config import disable_op_determinism\n+        from tensorflow.python.framework.config import enable_op_determinism\n+\n+        enable_op_determinism()\n+\n+        model = ExampleModel(units=3)\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+        )\n+\n+        self.assertFalse(model.jit_compile)\n+        disable_op_determinism()\n+\n \n class TrainerDistributeTest(testing.TestCase):\n     @pytest.mark.skipif(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b083f2e95d4d6b09f6ec22717fd1becccbdbcec1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 11 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,9 +36,8 @@ class AlphaDropout(Layer):\n             else:\n                 noise_shape = self.noise_shape\n             kept_idx = tf.greater_equal(\n-                backend.random.uniform(noise_shape),\n+                backend.random.uniform(noise_shape, seed=self.seed_generator),\n                 self.rate,\n-                seed=self.seed_generator,\n             )\n             kept_idx = tf.cast(kept_idx, inputs.dtype)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e67e0e779fff4ec378b49710cbc3ad75fb4c096", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 165 | Lines Deleted: 9 | Files Changed: 6 | Hunks: 22 | Methods Changed: 17 | Complexity Δ (Sum/Max): 16/9 | Churn Δ: 174 | Churn Cumulative: 3108 | Contributors (this commit): 15 | Commits (past 90d): 30 | Contributors (cumulative): 50 | DMM Complexity: 0.0\n\nDIFF:\n@@ -986,7 +986,14 @@ def _dot_product_attention_core(\n \n \n def dot_product_attention(\n-    query, key, value, bias=None, mask=None, scale=None, is_causal=False\n+    query,\n+    key,\n+    value,\n+    bias=None,\n+    mask=None,\n+    scale=None,\n+    is_causal=False,\n+    flash_attention=False,\n ):\n     query = convert_to_tensor(query)\n     key = convert_to_tensor(key)\n@@ -1000,6 +1007,7 @@ def dot_product_attention(\n \n     # `dot_product_attention` is only available in jax>=0.4.31\n     if hasattr(jax.nn, \"dot_product_attention\"):\n+        implementation = \"cudnn\" if flash_attention else \"xla\"\n         return jax.nn.dot_product_attention(\n             query,\n             key,\n@@ -1008,6 +1016,14 @@ def dot_product_attention(\n             mask=mask,\n             scale=scale,\n             is_causal=is_causal,\n+            implementation=implementation,\n+        )\n+\n+    if flash_attention:\n+        raise ValueError(\n+            \"Flash attention is not supported in your \"\n+            \"current JAX version. Please update it \"\n+            \"using `pip install -U jax jaxlib`.\"\n         )\n \n     # Ref: jax.nn.dot_product_attention\n\n@@ -1033,8 +1033,17 @@ def _dot_product_attention_xla(query, key, value, bias, mask, is_causal, scale):\n \n \n def dot_product_attention(\n-    query, key, value, bias=None, mask=None, scale=None, is_causal=False\n+    query,\n+    key,\n+    value,\n+    bias=None,\n+    mask=None,\n+    scale=None,\n+    is_causal=False,\n+    flash_attention=False,\n ):\n+    if flash_attention:\n+        raise ValueError(\"Flash attention is not implemented in NumPy.\")\n     # Ref: jax.nn.dot_product_attention\n     # https://github.com/jax-ml/jax/blob/jax-v0.4.32/jax/_src/nn/functions.py#L828\n     # Not support `query_seq_lengths` and `key_value_seq_lengths` args\n\n@@ -964,8 +964,20 @@ def _dot_product_attention_xla(query, key, value, bias, mask, is_causal, scale):\n \n \n def dot_product_attention(\n-    query, key, value, bias=None, mask=None, scale=None, is_causal=False\n+    query,\n+    key,\n+    value,\n+    bias=None,\n+    mask=None,\n+    scale=None,\n+    is_causal=False,\n+    flash_attention=False,\n ):\n+    if flash_attention:\n+        raise ValueError(\n+            \"Flash attention is not supported yet in TensorFlow backend.\"\n+        )\n+\n     # Ref: jax.nn.dot_product_attention\n     # https://github.com/jax-ml/jax/blob/jax-v0.4.32/jax/_src/nn/functions.py#L828\n     # Not support `query_seq_lengths` and `key_value_seq_lengths` args\n\n@@ -864,8 +864,28 @@ def _get_large_negative(dtype):\n     return convert_to_tensor(val * -0.7, dtype=dtype)\n \n \n+def is_flash_attention_enabled(query, key, value, mask=None, is_causal=False):\n+    params = torch.backends.cuda.SDPAParams(\n+        query,\n+        key,\n+        value,\n+        mask,\n+        0.0,\n+        is_causal,\n+    )\n+    is_enabled = torch.backends.cuda.can_use_flash_attention(params, False)\n+    return is_enabled\n+\n+\n def dot_product_attention(\n-    query, key, value, bias=None, mask=None, scale=None, is_causal=False\n+    query,\n+    key,\n+    value,\n+    bias=None,\n+    mask=None,\n+    scale=None,\n+    is_causal=False,\n+    flash_attention=False,\n ):\n     if bias is not None:\n         raise ValueError(\n@@ -891,6 +911,34 @@ def dot_product_attention(\n     query = torch.transpose(query, axis0, axis1)\n     key = torch.transpose(key, axis0, axis1)\n     value = torch.transpose(value, axis0, axis1)\n+\n+    if flash_attention:\n+        is_enabled = is_flash_attention_enabled(\n+            query=query,\n+            key=key,\n+            value=value,\n+            mask=mask,\n+            is_causal=is_causal,\n+        )\n+        if not is_enabled:\n+            raise ValueError(\n+                \"Flash attention is not enabled in `torch` backend. \"\n+                \"The dtype of the inputs should be float16/bfloat16 \"\n+                \"and your GPU should support flash attention implementation.\"\n+            )\n+\n+        with torch.nn.attention.sdpa_kernel(\n+            backends=[torch.nn.attention.SDPBackend.FLASH_ATTENTION],\n+        ):\n+            attention_output = torch.nn.functional.scaled_dot_product_attention(\n+                query,\n+                key,\n+                value,\n+                attn_mask=mask,\n+                is_causal=is_causal,\n+                scale=scale,\n+            )\n+    else:\n         attention_output = torch.nn.functional.scaled_dot_product_attention(\n             query, key, value, attn_mask=mask, is_causal=is_causal, scale=scale\n         )\n\n@@ -2131,7 +2131,16 @@ class DotProductAttention(Operation):\n         super().__init__()\n         self.is_causal = is_causal\n \n-    def call(self, query, key, value, bias=None, mask=None, scale=None):\n+    def call(\n+        self,\n+        query,\n+        key,\n+        value,\n+        bias=None,\n+        mask=None,\n+        scale=None,\n+        flash_attention=False,\n+    ):\n         return backend.nn.dot_product_attention(\n             query,\n             key,\n@@ -2140,10 +2149,18 @@ class DotProductAttention(Operation):\n             mask=mask,\n             scale=scale,\n             is_causal=self.is_causal,\n+            flash_attention=flash_attention,\n         )\n \n     def compute_output_spec(\n-        self, query, key, value, bias=None, mask=None, scale=None\n+        self,\n+        query,\n+        key,\n+        value,\n+        bias=None,\n+        mask=None,\n+        scale=None,\n+        flash_attention=False,\n     ):\n         return KerasTensor(query.shape, dtype=query.dtype)\n \n@@ -2152,7 +2169,14 @@ class DotProductAttention(Operation):\n     [\"keras.ops.dot_product_attention\", \"keras.ops.nn.dot_product_attention\"]\n )\n def dot_product_attention(\n-    query, key, value, bias=None, mask=None, scale=None, is_causal=False\n+    query,\n+    key,\n+    value,\n+    bias=None,\n+    mask=None,\n+    scale=None,\n+    is_causal=False,\n+    flash_attention=False,\n ):\n     \"\"\"Scaled dot product attention function.\n \n@@ -2207,6 +2231,7 @@ def dot_product_attention(\n             bias=bias,\n             mask=mask,\n             scale=scale,\n+            flash_attention=flash_attention,\n         )\n     return backend.nn.dot_product_attention(\n         query,\n@@ -2216,4 +2241,5 @@ def dot_product_attention(\n         mask=mask,\n         scale=scale,\n         is_causal=is_causal,\n+        flash_attention=flash_attention,\n     )\n\n@@ -2208,9 +2208,12 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             bias=(None, True),\n             scale=(None, 1.0),\n             mask_and_is_causal=((None, False), (True, False), (None, True)),\n+            flash_attention=(True, False),\n         )\n     )\n-    def test_dot_product_attention(self, bias, scale, mask_and_is_causal):\n+    def test_dot_product_attention(\n+        self, bias, scale, mask_and_is_causal, flash_attention\n+    ):\n         mask, is_causal = mask_and_is_causal\n         query_shape = (2, 3, 4, 5)\n         key_shape = (2, 6, 4, 5)\n@@ -2232,7 +2235,19 @@ class NNOpsCorrectnessTest(testing.TestCase):\n                 mask_shape\n             )\n \n-        expected = _dot_product_attention(\n+        if flash_attention and backend.backend() in [\n+            \"torch\",\n+            \"tensorflow\",\n+            \"numpy\",\n+        ]:\n+            self.skipTest(\n+                \"Not supported in TF and NumPy and supported for \"\n+                \"PyTorch with specific requirements.\"\n+            )\n+\n+        if flash_attention and backend.backend() == \"jax\":\n+            try:\n+                outputs = knn.dot_product_attention(\n                     query,\n                     key,\n                     value,\n@@ -2240,7 +2255,26 @@ class NNOpsCorrectnessTest(testing.TestCase):\n                     mask=mask,\n                     scale=scale,\n                     is_causal=is_causal,\n+                    flash_attention=flash_attention,\n                 )\n+            except ValueError as e:\n+                if e.args[0].startswith(\n+                    \"Flash attention is not supported in your \"\n+                    \"current JAX version\"\n+                ):\n+                    self.skipTest(\n+                        \"JAX version does not have \"\n+                        \"`dot_product_attention` function.\"\n+                    )\n+            except RuntimeError as e:\n+                if e.args[0] == \"cuDNN is not detected.\":\n+                    self.skipTest(\"No CuDNN to run flash attention for JAX.\")\n+                elif e.args[0] == \"Require at least Ampere arch to run\":\n+                    self.skipTest(\n+                        \"Requires at least Ampere arch to run flash attention \"\n+                        \"for JAX.\"\n+                    )\n+        else:\n             outputs = knn.dot_product_attention(\n                 query,\n                 key,\n@@ -2249,6 +2283,17 @@ class NNOpsCorrectnessTest(testing.TestCase):\n                 mask=mask,\n                 scale=scale,\n                 is_causal=is_causal,\n+                flash_attention=flash_attention,\n+            )\n+\n+        expected = _dot_product_attention(\n+            query,\n+            key,\n+            value,\n+            bias=bias,\n+            mask=mask,\n+            scale=scale,\n+            is_causal=is_causal,\n         )\n         self.assertAllClose(outputs, expected)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f5592bcb61ff48c96560c8923e482db1076b54a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 6 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -14,8 +14,8 @@ class Initializer:\n         # containing values drawn from a distribution of your choice.\n     ```\n \n-    Optionally, you an also implement the method `get_config()` and the class\n-    method `from_config` in order to support serialization -- just like with\n+    Optionally, you can also implement the method `get_config()` and the class\n+    method `from_config` in order to support serialization, just like with\n     any Keras object.\n \n     Here's a simple example: a random normal initializer.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c06ea7360ba0a76151171862547d53f03ce44ead", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 90 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -106,16 +106,11 @@ class RNN(Layer):\n \n     - Specify `stateful=True` in the layer constructor.\n     - Specify a fixed batch size for your model, by passing\n-    If sequential model:\n-        `batch_input_shape=(...)` to the first layer in your model.\n-    Else for functional model with 1 or more Input layers:\n-        `batch_shape=(...)` to all the first layers in your model.\n-    This is the expected shape of your inputs\n-    *including the batch size*.\n-    It should be a tuple of integers, e.g. `(32, 10, 100)`.\n-    - Specify `shuffle=False` when calling `fit()`.\n+        `batch_size=...` to the `Input` layer(s) of your model.\n+    - Specify `shuffle=False` when calling `fit()`, since your\n+        batches are expected to be temporally ordered.\n \n-    To reset the states of your model, call `.reset_states()` on either\n+    To reset the states of your model, call `.reset_state()` on either\n     a specific layer, or on your entire model.\n \n     Note on specifying the initial state of RNNs:\n@@ -126,18 +121,18 @@ class RNN(Layer):\n     the initial state of the RNN layer.\n \n     You can specify the initial state of RNN layers numerically by\n-    calling `reset_states` with the keyword argument `states`. The value of\n+    calling `reset_state()` with the keyword argument `states`. The value of\n     `states` should be a numpy array or list of numpy arrays representing\n     the initial state of the RNN layer.\n \n     Examples:\n \n     ```python\n-    from keras.src.layers import RNN\n-    from keras.src import ops\n+    from keras.layers import RNN\n+    from keras import ops\n \n     # First, let's define a RNN Cell, as a layer subclass.\n-    class MinimalRNNCell(keras.layers.Layer):\n+    class MinimalRNNCell(keras.Layer):\n \n         def __init__(self, units, **kwargs):\n             super().__init__(**kwargs)\n\n@@ -11,6 +11,9 @@ from keras.src.utils.naming import auto_name\n class Loss(KerasSaveable):\n     \"\"\"Loss base class.\n \n+    This is the class to subclass in order to create new\n+    custom losses.\n+\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ffff0488e618214cc7a0c159c9ea3f5b6a1db9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 49 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -107,7 +107,8 @@ class RNN(Layer):\n     - Specify `stateful=True` in the layer constructor.\n     - Specify a fixed batch size for your model, by passing\n     If sequential model:\n-        `batch_input_shape=(...)` to the first layer in your model.\n+        Pass `batch_size=1` to `model.fit()` or use a dataset based on a\n+        generator or `tf.data.Dataset`.\n     Else for functional model with 1 or more Input layers:\n         `batch_shape=(...)` to all the first layers in your model.\n     This is the expected shape of your inputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba6552bf06a0b53a6e4bff99cdd1658225d5774f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 54 | Churn Cumulative: 80 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.5483870967741935\n\nDIFF:\n@@ -0,0 +1,45 @@\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src import testing\n+from keras.src.backend.common.masking import get_keras_mask\n+from keras.src.backend.common.masking import set_keras_mask\n+\n+\n+class MaskingTest(testing.TestCase):\n+\n+    def test_mask_on_eager_tensor(self):\n+        x = ops.zeros((2, 3))\n+        self.assertIsNone(get_keras_mask(x))\n+\n+        set_keras_mask(x, None)\n+        self.assertIsNone(get_keras_mask(x))\n+\n+        mask = ops.ones((2, 3))\n+        set_keras_mask(x, mask)\n+        self.assertIs(get_keras_mask(x), mask)\n+\n+        set_keras_mask(x, None)\n+        self.assertIsNone(get_keras_mask(x))\n+\n+        set_keras_mask(x, None)\n+        self.assertIsNone(get_keras_mask(x))\n+\n+    def test_mask_on_tracer_tensor(self):\n+\n+        def fn(x):\n+            self.assertIsNone(get_keras_mask(x))\n+\n+            set_keras_mask(x, None)\n+            self.assertIsNone(get_keras_mask(x))\n+\n+            mask = ops.ones((2, 3))\n+            set_keras_mask(x, mask)\n+            self.assertIs(get_keras_mask(x), mask)\n+\n+            set_keras_mask(x, None)\n+            self.assertIsNone(get_keras_mask(x))\n+\n+            set_keras_mask(x, None)  # key is now deleted, should be a no-op\n+            self.assertIsNone(get_keras_mask(x))\n+\n+        backend.compute_output_spec(fn, backend.KerasTensor((2, 3)))\n\n@@ -7,13 +7,16 @@ def set_tensor_attr(tensor, attr, value):\n     try:\n         setattr(tensor, attr, value)\n     except AttributeError:\n-        if value is None:\n-            return\n         attr_dict = global_state.get_global_attribute(f\"{attr}_dict\")\n         if attr_dict is None:\n+            if value is None:\n+                return\n             attr_dict = weakref.WeakValueDictionary()\n             global_state.set_global_attribute(f\"{attr}_dict\", attr_dict)\n+        if value is not None:\n             attr_dict[id(tensor)] = value\n+        elif id(tensor) in attr_dict:\n+            del attr_dict[id(tensor)]\n \n \n def get_tensor_attr(tensor, attr):\n@@ -21,4 +24,6 @@ def get_tensor_attr(tensor, attr):\n         attr_dict = global_state.get_global_attribute(f\"{attr}_dict\")\n         if attr_dict is not None:\n             return attr_dict.get(id(tensor), None)\n+        else:\n+            return None\n     return getattr(tensor, attr, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7bd02bc6b1f3293b77bac72bffc3854e29142041", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 66 | Files Changed: 14 | Hunks: 33 | Methods Changed: 13 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 121 | Churn Cumulative: 1142 | Contributors (this commit): 10 | Commits (past 90d): 31 | Contributors (cumulative): 58 | DMM Complexity: None\n\nDIFF:\n@@ -31,12 +31,12 @@ from keras.api import regularizers\n from keras.api import saving\n from keras.api import tree\n from keras.api import utils\n+from keras.src.backend import Variable\n+from keras.src.backend import device\n+from keras.src.backend import name_scope\n from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.backend.common.symbolic_scope import SymbolicScope\n-from keras.src.backend.exports import Variable\n-from keras.src.backend.exports import device\n-from keras.src.backend.exports import name_scope\n from keras.src.dtype_policies.dtype_policy import DTypePolicy\n from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n from keras.src.initializers.initializer import Initializer\n\n@@ -29,12 +29,12 @@ from keras.api._tf_keras.keras import layers\n from keras.api._tf_keras.keras import losses\n from keras.api._tf_keras.keras import metrics\n from keras.api._tf_keras.keras import preprocessing\n+from keras.src.backend import Variable\n+from keras.src.backend import device\n+from keras.src.backend import name_scope\n from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.backend.common.symbolic_scope import SymbolicScope\n-from keras.src.backend.exports import Variable\n-from keras.src.backend.exports import device\n-from keras.src.backend.exports import name_scope\n from keras.src.dtype_policies.dtype_policy import DTypePolicy\n from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n from keras.src.initializers.initializer import Initializer\n\n@@ -6,13 +6,13 @@ if backend() == \"torch\":\n     # upon import.\n     import torch\n \n+from keras.src.api_export import keras_export\n from keras.src.backend.common.dtypes import result_type\n from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.keras_tensor import any_symbolic_tensors\n from keras.src.backend.common.keras_tensor import is_keras_tensor\n from keras.src.backend.common.masking import get_keras_mask\n from keras.src.backend.common.masking import set_keras_mask\n-from keras.src.backend.common.name_scope import name_scope\n from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.backend.common.stateless_scope import get_stateless_scope\n from keras.src.backend.common.stateless_scope import in_stateless_scope\n@@ -47,3 +47,24 @@ elif backend() == \"numpy\":\n     distribution_lib = None\n else:\n     raise ValueError(f\"Unable to import backend : {backend()}\")\n+\n+\n+BackendVariable = Variable  # noqa: F405\n+\n+\n+@keras_export(\"keras.Variable\")\n+class Variable(BackendVariable):\n+    pass\n+\n+\n+backend_name_scope = name_scope  # noqa: F405\n+\n+\n+@keras_export(\"keras.name_scope\")\n+class name_scope(backend_name_scope):\n+    pass\n+\n+\n+@keras_export(\"keras.device\")\n+def device(device_name):\n+    return device_scope(device_name)  # noqa: F405\n\n@@ -1,35 +0,0 @@\n-from keras.src import backend\n-from keras.src.api_export import keras_export\n-from keras.src.backend.common import KerasVariable\n-\n-if backend.backend() == \"tensorflow\":\n-    BackendVariable = backend.tensorflow.core.Variable\n-    backend_name_scope = backend.tensorflow.core.name_scope\n-elif backend.backend() == \"jax\":\n-    BackendVariable = backend.jax.core.Variable\n-    backend_name_scope = backend.common.name_scope.name_scope\n-elif backend.backend() == \"torch\":\n-    BackendVariable = backend.torch.core.Variable\n-    backend_name_scope = backend.common.name_scope.name_scope\n-elif backend.backend() == \"numpy\":\n-    from keras.src.backend.numpy.core import Variable as NumpyVariable\n-\n-    BackendVariable = NumpyVariable\n-    backend_name_scope = backend.common.name_scope.name_scope\n-else:\n-    raise RuntimeError(f\"Invalid backend: {backend.backend()}\")\n-\n-\n-@keras_export(\"keras.Variable\")\n-class Variable(BackendVariable, KerasVariable):\n-    pass\n-\n-\n-@keras_export(\"keras.name_scope\")\n-class name_scope(backend_name_scope):\n-    pass\n-\n-\n-@keras_export(\"keras.device\")\n-def device(device_name):\n-    return backend.device_scope(device_name)\n\n@@ -1,3 +1,4 @@\n+from keras.src.backend.common.name_scope import name_scope\n from keras.src.backend.jax import core\n from keras.src.backend.jax import distribution_lib\n from keras.src.backend.jax import image\n\n@@ -341,7 +341,7 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n \n def stop_gradient(variable):\n-    if isinstance(variable, KerasVariable):\n+    if isinstance(variable, Variable):\n         variable = variable.value\n     return jax.lax.stop_gradient(variable)\n \n\n@@ -1,3 +1,4 @@\n+from keras.src.backend.common.name_scope import name_scope\n from keras.src.backend.numpy import core\n from keras.src.backend.numpy import image\n from keras.src.backend.numpy import linalg\n@@ -12,6 +13,7 @@ from keras.src.backend.numpy.core import compute_output_spec\n from keras.src.backend.numpy.core import cond\n from keras.src.backend.numpy.core import convert_to_numpy\n from keras.src.backend.numpy.core import convert_to_tensor\n+from keras.src.backend.numpy.core import device_scope\n from keras.src.backend.numpy.core import is_tensor\n from keras.src.backend.numpy.core import random_seed_dtype\n from keras.src.backend.numpy.core import shape\n\n@@ -1,4 +1,5 @@\n import builtins\n+import contextlib\n import functools\n import warnings\n \n@@ -433,3 +434,8 @@ class custom_gradient:\n     def __call__(self, *args, **kwargs):\n         outputs, _ = self.fun(*args, **kwargs)\n         return outputs\n+\n+\n+@contextlib.contextmanager\n+def device_scope(device_name):\n+    yield\n\n@@ -12,10 +12,10 @@ class DeviceTest(testing.TestCase):\n         if not tf.config.list_physical_devices(\"GPU\"):\n             self.skipTest(\"Need at least one GPU for testing\")\n \n-        with backend.device_scope(\"cpu:0\"):\n+        with backend.device(\"cpu:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertIn(\"CPU:0\", t.device)\n-        with backend.device_scope(\"CPU:0\"):\n+        with backend.device(\"CPU:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertIn(\"CPU:0\", t.device)\n \n@@ -24,7 +24,7 @@ class DeviceTest(testing.TestCase):\n         self.assertIn(\"GPU:0\", t.device)\n \n         # Also verify the explicit gpu device\n-        with backend.device_scope(\"gpu:0\"):\n+        with backend.device(\"gpu:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertIn(\"GPU:0\", t.device)\n \n@@ -42,10 +42,10 @@ class DeviceTest(testing.TestCase):\n         if platform != \"gpu\":\n             self.skipTest(\"Need at least one GPU for testing\")\n \n-        with backend.device_scope(\"cpu:0\"):\n+        with backend.device(\"cpu:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertEqual(get_device(t), jax.devices(\"cpu\")[0])\n-        with backend.device_scope(\"CPU:0\"):\n+        with backend.device(\"CPU:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertEqual(get_device(t), jax.devices(\"cpu\")[0])\n \n@@ -54,14 +54,14 @@ class DeviceTest(testing.TestCase):\n         self.assertEqual(get_device(t), jax.devices(\"gpu\")[0])\n \n         # Also verify the explicit gpu device\n-        with backend.device_scope(\"gpu:0\"):\n+        with backend.device(\"gpu:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertEqual(get_device(t), jax.devices(\"gpu\")[0])\n \n     @pytest.mark.skipif(backend.backend() != \"jax\", reason=\"jax only\")\n     def test_invalid_jax_device(self):\n         with self.assertRaisesRegex(ValueError, \"Received: device_name='123'\"):\n-            backend.device_scope(123).__enter__()\n+            backend.device(123).__enter__()\n \n     @pytest.mark.skipif(backend.backend() != \"torch\", reason=\"torch only\")\n     def test_torch_device_scope(self):\n@@ -70,10 +70,10 @@ class DeviceTest(testing.TestCase):\n         if not torch.cuda.device_count():\n             self.skipTest(\"Need at least one GPU for testing\")\n \n-        with backend.device_scope(\"cpu:0\"):\n+        with backend.device(\"cpu:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertEqual(t.device, torch.device(\"cpu\"))\n-        with backend.device_scope(\"CPU:0\"):\n+        with backend.device(\"CPU:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertEqual(t.device, torch.device(\"cpu\"))\n \n@@ -82,11 +82,11 @@ class DeviceTest(testing.TestCase):\n         self.assertEqual(t.device, torch.device(\"cuda\", 0))\n \n         # Also verify the explicit gpu -> cuda conversion\n-        with backend.device_scope(\"gpu:0\"):\n+        with backend.device(\"gpu:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertEqual(t.device, torch.device(\"cuda\", 0))\n \n     @pytest.mark.skipif(backend.backend() != \"torch\", reason=\"torch only\")\n     def test_invalid_torch_device(self):\n         with self.assertRaisesRegex(ValueError, \"Received: device_name='123'\"):\n-            backend.device_scope(123).__enter__()\n+            backend.device(123).__enter__()\n\n@@ -14,6 +14,7 @@ we are doing the following to automate device placement if a GPU is available:\n - `convert_to_numpy` will bring the tensor to CPU before converting it to NumPy.\n \"\"\"\n \n+from keras.src.backend.common.name_scope import name_scope\n from keras.src.backend.torch import core\n from keras.src.backend.torch import image\n from keras.src.backend.torch import linalg\n\n@@ -187,11 +187,7 @@ class Variable(KerasVariable):\n def convert_to_tensor(x, dtype=None, sparse=None):\n     if sparse:\n         raise ValueError(\"`sparse=True` is not supported with torch backend\")\n-    if type(x) is Variable:\n-        # We cannot use `isinstance(x, Variable)` due to the failure of\n-        # TorchDynamo.\n-        # torch._dynamo.exc.InternalTorchDynamoError:\n-        # GetAttrVariable(SuperVariable(), value) has no type.\n+    if isinstance(x, Variable):\n         # TorchDynamo has bugs supporting nn.Parameter type check.\n         # Return it directly instead of pass it to the rest of the logic in the\n         # function.\n\n@@ -96,12 +96,10 @@ class NormalizationTest(testing.TestCase):\n         reason=\"Test symbolic call for torch meta device.\",\n     )\n     def test_call_on_meta_device_after_built(self):\n-        from keras.src.backend.torch import core\n-\n         layer = layers.Normalization()\n         data = np.random.random((32, 4))\n         layer.adapt(data)\n-        with core.device_scope(\"meta\"):\n+        with backend.device(\"meta\"):\n             layer(data)\n \n     def test_normalization_with_mean_only_raises_error(self):\n\n@@ -71,7 +71,7 @@ class SeedGenerator:\n             dtype = kwargs.get(\"dtype\", None)\n             return self.backend.convert_to_tensor([seed, 0], dtype=dtype)\n \n-        with backend.name_scope(self.name, caller=self):\n+        with self.backend.name_scope(self.name, caller=self):\n             self.state = self.backend.Variable(\n                 seed_initializer,\n                 shape=(2,),\n\n@@ -2,6 +2,7 @@ import io\n \n from packaging.version import parse\n \n+from keras.src import backend\n from keras.src.api_export import keras_export\n from keras.src.layers import Layer\n from keras.src.ops import convert_to_numpy\n@@ -101,12 +102,10 @@ class TorchModuleWrapper(Layer):\n         return self.module.parameters(recurse=recurse)\n \n     def _track_module_parameters(self):\n-        from keras.src.backend.torch import Variable\n-\n         for param in self.module.parameters():\n             # The Variable will reuse the raw `param`\n             # and simply wrap it.\n-            variable = Variable(\n+            variable = backend.Variable(\n                 initializer=param, trainable=param.requires_grad\n             )\n             self._track_variable(variable)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
