{"custom_id": "keras#0ec57f28bc16466de1335a8a9797af49d52887b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 4897 | Contributors (this commit): 39 | Commits (past 90d): 41 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -1015,8 +1015,7 @@ class Sequential(Model, containers.Sequential):\n                 at least `nb_val_samples` examples have been obtained,\n                 with these examples used for validation.\n             nb_val_samples: number of samples to use from validation\n-                generator at the end of every epoch. If `None` defaults\n-                to `samples_per_epoch/5`\n+                generator at the end of every epoch.\n             class_weight: dictionary mapping class indices to a weight\n                 for the class.\n             nb_worker: integer, number of workers to use for running\n@@ -1528,8 +1527,7 @@ class Graph(Model, containers.Graph):\n                 end of every epoch. These examples will then be used\n                 as the validation data.\n             nb_val_samples: number of samples to use from validation\n-                generator at the end of every epoch. If `None` defaults\n-                to `samples_per_epoch/5`\n+                generator at the end of every epoch.\n             class_weight: dictionary mapping class indices to a weight\n                 for the class.\n             nb_worker: integer, number of workers to use for running\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35c5fa911dcc304f39f0bc6c335d4f67ce082b79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 121 | Files Changed: 3 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): -29/0 | Churn Δ: 136 | Churn Cumulative: 9602 | Contributors (this commit): 55 | Commits (past 90d): 88 | Contributors (cumulative): 72 | DMM Complexity: 0.5714285714285714\n\nDIFF:\n@@ -493,75 +493,3 @@ class TensorBoard(Callback):\n             summary_value.tag = name\n             self.writer.add_summary(summary, epoch)\n         self.writer.flush()\n-\n-\n-class ModelTest(Callback):\n-    ''' Test model at the end of every X epochs.\n-\n-    The model is tested using both MC dropout and the dropout \n-    approximation. Output metrics for various losses are supported.\n-\n-    # Arguments\n-        Xt: model inputs to test.\n-        Yt: model outputs to get accuracy / error (ground truth).\n-        T: number of samples to use in MC dropout.\n-        test_every_X_epochs: test every test_every_X_epochs epochs.\n-        batch_size: number of data points to put in each batch \n-            (often larger than training batch size).\n-        verbose: verbosity mode, 0 or 1.\n-        loss: a string from ['binary', 'categorical', 'euclidean'] \n-            used to calculate the testing metric.\n-        mean_y_train: mean of outputs in regression cases to add back \n-            to model output ('euclidean' loss).\n-        std_y_train: std of outputs in regression cases to add back \n-            to model output ('euclidean' loss).\n-\n-    # References\n-        - [Dropout: A simple way to prevent neural networks from overfitting](http://jmlr.org/papers/v15/srivastava14a.html)\n-        - [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](http://arxiv.org/abs/1506.02142)\n-    '''\n-    def __init__(self, Xt, Yt, T=10, test_every_X_epochs=1, batch_size=500, verbose=1, \n-                 loss=None, mean_y_train=None, std_y_train=None):\n-        super(ModelTest, self).__init__()\n-        self.Xt = Xt\n-        self.Yt = np.array(Yt)\n-        self.T = T\n-        self.test_every_X_epochs = test_every_X_epochs\n-        self.batch_size = batch_size\n-        self.verbose = verbose\n-        self.loss = loss\n-        self.mean_y_train = mean_y_train\n-        self.std_y_train = std_y_train\n-\n-    def on_epoch_begin(self, epoch, logs={}):\n-        if epoch % self.test_every_X_epochs != 0:\n-            return\n-        model_output = self.model.predict(self.Xt, batch_size=self.batch_size, \n-                                          verbose=self.verbose)\n-        MC_model_output = []\n-        for _ in xrange(self.T):\n-            MC_model_output += [self.model.predict_stochastic(self.Xt, \n-                                                              batch_size=self.batch_size, \n-                                                              verbose=self.verbose)]\n-        MC_model_output = np.array(MC_model_output)\n-        MC_model_output_mean = np.mean(MC_model_output, 0)\n-\n-        if self.loss == 'binary':\n-            standard_acc = np.mean(self.Yt == np.round(model_output.flatten()))\n-            MC_acc = np.mean(self.Yt == np.round(MC_model_output_mean.flatten()))\n-            print(\"Standard accuracy at epoch %05d: %0.5f\" % (epoch, float(standard_acc)))\n-            print(\"MC accuracy at epoch %05d: %0.5f\" % (epoch, float(MC_acc)))\n-        elif self.loss == 'categorical':\n-            standard_acc = np.mean(np.argmax(self.Yt, axis=-1) == np.argmax(model_output, axis=-1))\n-            MC_acc = np.mean(np.argmax(self.Yt, axis=-1) == np.argmax(MC_model_output_mean, axis=-1))\n-            print(\"Standard accuracy at epoch %05d: %0.5f\" % (epoch, float(standard_acc)))\n-            print(\"MC accuracy at epoch %05d: %0.5f\" % (epoch, float(MC_acc)))\n-        elif self.loss == 'euclidean':\n-            model_output = model_output * self.std_y_train + self.mean_y_train\n-            standard_err = np.mean((self.Yt - model_output)**2.0, 0)**0.5\n-            MC_model_output_mean = MC_model_output_mean * self.std_y_train + self.mean_y_train\n-            MC_err = np.mean((self.Yt - MC_model_output_mean)**2.0, 0)**0.5\n-            print(\"Standard error at epoch %05d: %0.5f\" % (epoch, float(standard_err)))\n-            print(\"MC error at epoch %05d: %0.5f\" % (epoch, float(MC_err)))\n-        else:\n-            raise Exception('No loss: ' + loss)\n\n@@ -5,6 +5,7 @@ import numpy as np\n from .. import backend as K\n from .. import activations, initializations, regularizers\n from ..layers.core import MaskedLayer\n+from ..backend.common import _FLOATX\n \n \n class Recurrent(MaskedLayer):\n@@ -267,10 +268,6 @@ class SimpleRNN(Recurrent):\n         # states only contains the previous output.\n         assert len(states) == 3 # 1 state and 2 constants\n         prev_output = states[0]\n-        if self.dropout_W == 0 and self.dropout_U == 0: \n-            # this uses less memory when dropout is disabled\n-            B_W, B_U = 1, 1\n-        else:\n         B_W = states[1]\n         B_U = states[2]\n         h = K.dot(x * B_W, self.W) + self.b\n@@ -290,8 +287,8 @@ class SimpleRNN(Recurrent):\n             B_W = K.random_binomial((nb_samples, self.input_dim), p=retain_p_W)\n             B_U = K.random_binomial((nb_samples, self.output_dim), p=retain_p_U)\n         else:\n-            B_W = retain_p_W\n-            B_U = retain_p_U\n+            B_W = np.ones(1, dtype=_FLOATX) * retain_p_W\n+            B_U = np.ones(1, dtype=_FLOATX) * retain_p_U\n         return [B_W, B_U]\n \n     def get_config(self):\n@@ -411,12 +408,8 @@ class GRU(Recurrent):\n     def step(self, x, states):\n         assert len(states) == 3 # 1 state and 2 constants\n         h_tm1 = states[0]\n-        if self.dropout_W == 0 and self.dropout_U == 0: \n-            # this uses less memory when dropout is disabled\n-            B_W, B_U = [1] * 3, [1] * 3\n-        else:\n-            B_W = [K.gather(states[1], i) for i in range(3)]\n-            B_U = [K.gather(states[2], i) for i in range(3)]\n+        B_W = states[1]\n+        B_U = states[2]\n \n         x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n         x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n@@ -439,11 +432,11 @@ class GRU(Recurrent):\n                     raise Exception('For RNN dropout in tensorflow, a complete ' +\n                                     'input_shape must be provided (including batch size).')\n                 nb_samples = self.input_shape[0]\n-            B_W = K.random_binomial((3, nb_samples, self.input_dim), p=retain_p_W)\n-            B_U = K.random_binomial((3, nb_samples, self.output_dim), p=retain_p_U)\n+            B_W = [K.random_binomial((nb_samples, self.input_dim), p=retain_p_W) for _ in range(3)]\n+            B_U = [K.random_binomial((nb_samples, self.output_dim), p=retain_p_U) for _ in range(3)]\n         else:\n-            B_W = K.ones(3) * retain_p_W\n-            B_U = K.ones(3) * retain_p_U\n+            B_W = np.ones(3, dtype=_FLOATX) * retain_p_W\n+            B_U = np.ones(3, dtype=_FLOATX) * retain_p_U\n         return [B_W, B_U]\n \n     def get_config(self):\n@@ -583,12 +576,8 @@ class LSTM(Recurrent):\n         assert len(states) == 4 # 2 states and 2 constants\n         h_tm1 = states[0]\n         c_tm1 = states[1]\n-        if self.dropout_W == 0 and self.dropout_U == 0: \n-            # this uses less memory when dropout is disabled\n-            B_W, B_U = [1] * 4, [1] * 4\n-        else:\n-            B_W = [K.gather(states[2], i) for i in range(4)]\n-            B_U = [K.gather(states[3], i) for i in range(4)]\n+        B_W = states[2]\n+        B_U = states[3]\n \n         x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n         x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n@@ -612,11 +601,11 @@ class LSTM(Recurrent):\n                     raise Exception('For RNN dropout in tensorflow, a complete ' +\n                                     'input_shape must be provided (including batch size).')\n                 nb_samples = self.input_shape[0]\n-            B_W = K.random_binomial((4, nb_samples, self.input_dim), p=retain_p_W)\n-            B_U = K.random_binomial((4, nb_samples, self.output_dim), p=retain_p_U)\n+            B_W = [K.random_binomial((nb_samples, self.input_dim), p=retain_p_W) for _ in range(4)]\n+            B_U = [K.random_binomial((nb_samples, self.output_dim), p=retain_p_U) for _ in range(4)]\n         else:\n-            B_W = K.ones(4) * retain_p_W\n-            B_U = K.ones(4) * retain_p_U\n+            B_W = np.ones(4, dtype=_FLOATX) * retain_p_W\n+            B_U = np.ones(4, dtype=_FLOATX) * retain_p_U\n         return [B_W, B_U]\n \n     def get_config(self):\n\n@@ -515,7 +515,6 @@ class Sequential(Model, containers.Sequential):\n         self._train = K.function(train_ins, [train_loss], updates=updates)\n         self._train_with_acc = K.function(train_ins, [train_loss, train_accuracy], updates=updates)\n         self._predict = K.function(predict_ins, [self.y_test], updates=self.state_updates)\n-        self._predict_stochastic = K.function(predict_ins, [self.y_train], updates=self.state_updates)\n         self._test = K.function(test_ins, [test_loss], updates=self.state_updates)\n         self._test_with_acc = K.function(test_ins, [test_loss, test_accuracy], updates=self.state_updates)\n \n@@ -661,28 +660,6 @@ class Sequential(Model, containers.Sequential):\n         X = standardize_X(X)\n         return self._predict_loop(self._predict, X, batch_size, verbose)[0]\n \n-    def predict_stochastic(self, X, batch_size=128, verbose=0):\n-        '''Generate output predictions for the input samples\n-        batch by batch, using stochastic forward passes. If \n-        dropout is used at training, during prediction network \n-        units will be dropped at random as well. This procedure\n-        can be used for MC dropout (see [ModelTest callback](callbacks.md)).\n-\n-        # Arguments\n-            X: the input data, as a numpy array.\n-            batch_size: integer.\n-            verbose: verbosity mode, 0 or 1.\n-\n-        # Returns\n-            A numpy array of predictions.\n-    \n-        # References\n-            - [Dropout: A simple way to prevent neural networks from overfitting](http://jmlr.org/papers/v15/srivastava14a.html)\n-            - [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](http://arxiv.org/abs/1506.02142)\n-        '''\n-        X = standardize_X(X)\n-        return self._predict_loop(self._predict_stochastic, X, batch_size, verbose)[0]\n-\n     def predict_proba(self, X, batch_size=128, verbose=1):\n         '''Generate class probability predictions for the input samples\n         batch by batch.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f10c430731904a11558632a0a4936eb3f154647e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 4158 | Contributors (this commit): 25 | Commits (past 90d): 63 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,8 @@\n from __future__ import absolute_import\n-import numpy as np\n \n from .. import backend as K\n-from .. import activations, initializations, regularizers, constraints\n-from ..layers.core import Layer, MaskedLayer\n-\n-from ..constraints import unitnorm\n+from .. import initializations, regularizers, constraints\n+from ..layers.core import Layer\n \n \n class Embedding(Layer):\n@@ -108,7 +105,8 @@ class Embedding(Layer):\n             B = K.random_binomial((self.input_dim,), p=retain_p)\n         else:\n             B = K.ones((self.input_dim)) * retain_p\n-        out = K.gather(self.W * K.expand_dims(B), X) # we zero-out rows of W at random\n+        # we zero-out rows of W at random\n+        out = K.gather(self.W * K.expand_dims(B), X)\n         return out\n \n     def get_config(self):\n\n@@ -5,7 +5,6 @@ import numpy as np\n from .. import backend as K\n from .. import activations, initializations, regularizers\n from ..layers.core import MaskedLayer\n-from ..backend.common import _FLOATX\n \n \n class Recurrent(MaskedLayer):\n@@ -287,8 +286,8 @@ class SimpleRNN(Recurrent):\n             B_W = K.random_binomial((nb_samples, self.input_dim), p=retain_p_W)\n             B_U = K.random_binomial((nb_samples, self.output_dim), p=retain_p_U)\n         else:\n-            B_W = np.ones(1, dtype=_FLOATX) * retain_p_W\n-            B_U = np.ones(1, dtype=_FLOATX) * retain_p_U\n+            B_W = np.ones(1, dtype=K.floatx()) * retain_p_W\n+            B_U = np.ones(1, dtype=K.floatx()) * retain_p_U\n         return [B_W, B_U]\n \n     def get_config(self):\n@@ -435,8 +434,8 @@ class GRU(Recurrent):\n             B_W = [K.random_binomial((nb_samples, self.input_dim), p=retain_p_W) for _ in range(3)]\n             B_U = [K.random_binomial((nb_samples, self.output_dim), p=retain_p_U) for _ in range(3)]\n         else:\n-            B_W = np.ones(3, dtype=_FLOATX) * retain_p_W\n-            B_U = np.ones(3, dtype=_FLOATX) * retain_p_U\n+            B_W = np.ones(3, dtype=K.floatx()) * retain_p_W\n+            B_U = np.ones(3, dtype=K.floatx()) * retain_p_U\n         return [B_W, B_U]\n \n     def get_config(self):\n@@ -604,8 +603,8 @@ class LSTM(Recurrent):\n             B_W = [K.random_binomial((nb_samples, self.input_dim), p=retain_p_W) for _ in range(4)]\n             B_U = [K.random_binomial((nb_samples, self.output_dim), p=retain_p_U) for _ in range(4)]\n         else:\n-            B_W = np.ones(4, dtype=_FLOATX) * retain_p_W\n-            B_U = np.ones(4, dtype=_FLOATX) * retain_p_U\n+            B_W = np.ones(4, dtype=K.floatx()) * retain_p_W\n+            B_U = np.ones(4, dtype=K.floatx()) * retain_p_U\n         return [B_W, B_U]\n \n     def get_config(self):\n\n@@ -273,7 +273,7 @@ class TestBackend(object):\n         check_single_tensor_operation('tanh', (4, 2))\n \n         # dropout\n-        val = np.random.random((20, 20))\n+        val = np.random.random((100, 100))\n         xth = KTH.variable(val)\n         xtf = KTF.variable(val)\n         zth = KTH.eval(KTH.dropout(xth, level=0.2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7f7ffe7c42e165a7121f87328af206cd1613a0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3214 | Contributors (this commit): 20 | Commits (past 90d): 33 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -207,7 +207,7 @@ class SimpleRNN(Recurrent):\n     '''\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n-                 activation='sigmoid',\n+                 activation='tanh',\n                  W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                  dropout_W=0., dropout_U=0., **kwargs):\n         self.output_dim = output_dim\n@@ -333,7 +333,7 @@ class GRU(Recurrent):\n     '''\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n-                 activation='sigmoid', inner_activation='hard_sigmoid',\n+                 activation='tanh', inner_activation='hard_sigmoid',\n                  W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                  dropout_W=0., dropout_U=0., **kwargs):\n         self.output_dim = output_dim\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#654404c2ed8db47a5361a3bff9126a16507c9c4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 3223 | Contributors (this commit): 20 | Commits (past 90d): 34 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -264,7 +264,8 @@ class SimpleRNN(Recurrent):\n             self.states = [K.zeros((input_shape[0], self.output_dim))]\n \n     def step(self, x, states):\n-        # states only contains the previous output.\n+        # states contains the previous output,\n+        # and the two dropout matrices from self.get_constants()\n         assert len(states) == 3  # 1 state and 2 constants\n         prev_output = states[0]\n         B_W = states[1]\n@@ -406,9 +407,9 @@ class GRU(Recurrent):\n \n     def step(self, x, states):\n         assert len(states) == 3  # 1 state and 2 constants\n-        h_tm1 = states[0]\n-        B_W = states[1]\n-        B_U = states[2]\n+        h_tm1 = states[0]  # previous memory\n+        B_W = states[1]  # dropout matrix for input units\n+        B_U = states[2]  # dropout matrix for recurrent units\n \n         x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n         x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58ca064f9321ad4e6db09d9e8b22db4a3d8fc7dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 86 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,7 @@ model.add(LSTM(50,\n                return_sequences=False,\n                stateful=True))\n model.add(Dense(1))\n-model.compile(loss='rmse', optimizer='rmsprop')\n+model.compile(loss='mse', optimizer='rmsprop')\n \n print('Training')\n for i in range(epochs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cbd11315b7a399874569ef95373b078f3786e615", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 205 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -46,8 +46,8 @@ print('X_test shape:', X_test.shape)\n \n print('Build model...')\n model = Sequential()\n-model.add(Embedding(max_features, 128, input_length=maxlen))\n-model.add(LSTM(128))  # try using a GRU instead, for fun\n+model.add(Embedding(max_features, 128, input_length=maxlen, dropout=0.5))\n+model.add(LSTM(128, dropout_W=0.5, dropout_U=0.5))  # try using a GRU instead, for fun\n model.add(Dropout(0.5))\n model.add(Dense(1))\n model.add(Activation('sigmoid'))\n@@ -58,7 +58,7 @@ model.compile(loss='binary_crossentropy',\n               class_mode=\"binary\")\n \n print(\"Train...\")\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3,\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n           validation_data=(X_test, y_test), show_accuracy=True)\n score, acc = model.evaluate(X_test, y_test,\n                             batch_size=batch_size,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44d558ad7f13251650f40475eef6652df59e4b09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 259 | Lines Deleted: 50 | Files Changed: 3 | Hunks: 18 | Methods Changed: 20 | Complexity Δ (Sum/Max): 22/13 | Churn Δ: 309 | Churn Cumulative: 9265 | Contributors (this commit): 44 | Commits (past 90d): 117 | Contributors (cumulative): 63 | DMM Complexity: 0.8391608391608392\n\nDIFF:\n@@ -26,29 +26,6 @@ class Sequential(Layer):\n             self.add(layer)\n         self._cache_enabled = True\n \n-    def __call__(self, X, mask=None, train=False):\n-        # turn off layer cache temporarily\n-        tmp_cache_enabled = self.cache_enabled\n-        self.cache_enabled = False\n-        # recursively search for a layer which is not a Sequential model\n-        layer = self\n-        while issubclass(layer.__class__, Sequential):\n-            layer = layer.layers[0]\n-        # set temporary input to first layer\n-        tmp_input = layer.get_input\n-        tmp_mask = None\n-        layer.get_input = lambda _: X\n-        if hasattr(layer, 'get_input_mask'):\n-            tmp_mask = layer.get_input_mask\n-            layer.get_input_mask = lambda _: mask\n-        Y = self.get_output(train=train)\n-        # return input from first layer to what it was\n-        layer.get_input = tmp_input\n-        if hasattr(layer, 'get_input_mask'):\n-            layer.get_input_mask = tmp_mask\n-        self.cache_enabled = tmp_cache_enabled\n-        return Y\n-\n     @property\n     def cache_enabled(self):\n         return self._cache_enabled\n@@ -59,8 +36,11 @@ class Sequential(Layer):\n         for l in self.layers:\n             l.cache_enabled = value\n \n-    def set_previous(self, layer):\n-        self.layers[0].previous = layer\n+    def set_previous(self, layer, reset_weights=True):\n+        self.layers[0].set_previous(layer, reset_weights)\n+\n+    def clear_previous(self, reset_weights=True):\n+        self.layers[0].clear_previous(reset_weights)\n \n     def add(self, layer):\n         layer.layer_cache = self.layer_cache\n@@ -191,6 +171,47 @@ class Graph(Layer):\n         self.node_config = []  # dicts\n         self.layer_cache = {}\n         self.shape_cache = {}\n+        self._cache_enabled = True\n+\n+    def __call__(self, X, mask=None, train=False):\n+        if type(X) != dict:\n+            return super(Graph, self).__call__(X, mask, train)\n+        else:\n+            # turn off layer cache temporarily\n+            tmp_cache_enabled = self.cache_enabled\n+            self.cache_enabled = False\n+            # create a temporary layer for each input\n+            tmp_previous = {}\n+            for name, input in self.inputs.items():\n+                layer = Layer(batch_input_shape=input.input_shape)\n+                layer.input = X[name]\n+                if hasattr(self, 'get_input_mask'):\n+                    layer.get_input_mask = lambda _: mask[name]\n+                # set temporary previous\n+                if hasattr(input, 'previous'):\n+                    tmp_previous[name] = input.previous\n+                input.set_previous(layer, False)\n+            Y = self.get_output(train=train)\n+            # return previous to what it was\n+            for name, input in self.inputs.items():\n+                if name in tmp_previous:\n+                    input.set_previous(tmp_previous[name], False)\n+                else:\n+                    input.clear_previous(False)\n+            self.cache_enabled = tmp_cache_enabled\n+            return Y\n+\n+    @property\n+    def cache_enabled(self):\n+        return self._cache_enabled\n+\n+    @cache_enabled.setter\n+    def cache_enabled(self, value):\n+        self._cache_enabled = value\n+        for l in self.nodes.values():\n+            l.cache_enabled = value\n+        for l in self.inputs.values():\n+            l.cache_enabled = value\n \n     @property\n     def nb_input(self):\n@@ -251,22 +272,35 @@ class Graph(Layer):\n             if hasattr(l, 'reset_states') and getattr(l, 'stateful', False):\n                 l.reset_states()\n \n-    def set_previous(self, layer, connection_map={}):\n+    def set_previous(self, layer, connection_map={}, reset_weights=True):\n         if self.nb_input != layer.nb_output:\n             raise Exception('Cannot connect layers: '\n                             'input count does not match output count.')\n         if self.nb_input == 1:\n-            self.inputs[self.input_order[0]].set_previous(layer)\n+            self.inputs[self.input_order[0]].set_previous(layer, reset_weights)\n         else:\n             if not connection_map:\n                 raise Exception('Cannot attach multi-input layer: '\n                                 'no connection_map provided.')\n             for k, v in connection_map.items():\n                 if k in self.inputs and v in layer.outputs:\n-                    self.inputs[k].set_previous(layer.outputs[v])\n+                    self.inputs[k].set_previous(layer.outputs[v], reset_weights)\n                 else:\n                     raise Exception('Invalid connection map.')\n \n+    def clear_previous(self, reset_weights=True):\n+        for k in self.inputs.values():\n+            k.clear_previous(reset_weights)\n+\n+    @property\n+    def input_shape(self):\n+        if self.nb_input == 1:\n+            # return tuple\n+            return self.inputs[self.input_order[0]].input_shape\n+        else:\n+            # return dictionary mapping input names to shape tuples\n+            return dict([(k, v.input_shape) for k, v in self.inputs.items()])\n+\n     def get_input(self, train=False):\n         if len(self.inputs) == len(self.outputs) == 1:\n             return self.inputs[self.input_order[0]].get_input(train)\n\n@@ -76,21 +76,30 @@ class Layer(object):\n         self._cache_enabled = value\n \n     def __call__(self, X, mask=None, train=False):\n-        # set temporary input\n-        tmp_input = self.get_input\n-        tmp_mask = None\n+        # turn off layer cache temporarily\n+        tmp_cache_enabled = self.cache_enabled\n+        self.cache_enabled = False\n+        # create a temporary layer\n+        layer = Layer(batch_input_shape=self.input_shape)\n+        layer.name = \"dummy\"\n+        layer.input = X\n         if hasattr(self, 'get_input_mask'):\n-            tmp_mask = self.get_input_mask\n-            self.get_input_mask = lambda _: mask\n-        self.get_input = lambda _: X\n+            layer.get_input_mask = lambda _: mask \n+        # set temporary previous\n+        tmp_previous = None\n+        if hasattr(self, 'previous'):\n+            tmp_previous = self.previous\n+        self.set_previous(layer, False)\n         Y = self.get_output(train=train)\n-        # return input to what it was\n-        if hasattr(self, 'get_input_mask'):\n-            self.get_input_mask = tmp_mask\n-        self.get_input = tmp_input\n+        # return previous to what it was\n+        if tmp_previous is not None:\n+            self.set_previous(tmp_previous, False)\n+        else:\n+            self.clear_previous(False)\n+        self.cache_enabled = tmp_cache_enabled\n         return Y\n \n-    def set_previous(self, layer):\n+    def set_previous(self, layer, reset_weights=True):\n         '''Connect a layer to its parent in the computational graph.\n         '''\n         assert self.nb_input == layer.nb_output == 1, 'Cannot connect layers: input count and output count should be 1.'\n@@ -101,7 +110,26 @@ class Layer(object):\n                                                                 str(layer.output_shape))\n         if layer.get_output_mask() is not None:\n             assert self.supports_masked_input(), 'Cannot connect non-masking layer to layer with masked output.'\n+        if not reset_weights:\n+            assert layer.output_shape == self.input_shape, ('Cannot connect layers without resetting weights: ' + \n+                                                            'expected input with shape ' +\n+                                                            str(self.input_shape) +\n+                                                            ' but previous layer has output_shape ' +\n+                                                            str(layer.output_shape))\n         self.previous = layer\n+        if reset_weights:\n+            self.build()\n+\n+    def clear_previous(self, reset_weights=True):\n+        '''Unlink a layer from its parent in the computational graph.\n+\n+        This is only allowed if the layer has an `input` attribute.\n+        '''\n+        if not hasattr(self, 'input'):\n+            raise Exception('Cannot clear previous for non-input layers')\n+        if hasattr(self, 'previous'):\n+            del self.previous\n+            if reset_weights:\n                 self.build()\n \n     def build(self):\n@@ -1250,8 +1278,10 @@ class AutoEncoder(Layer):\n                     self.trainable_weights.append(p)\n                     self.constraints.append(c)\n \n-    def set_previous(self, node):\n-        self.encoder.set_previous(node)\n+    def set_previous(self, node, reset_weights=True):\n+        self.encoder.set_previous(node, reset_weights)\n+        if reset_weights:\n+            self.build()\n \n     def get_weights(self):\n         weights = []\n@@ -1660,11 +1690,7 @@ class Siamese(Layer):\n         return self.trainable_weights, self.regularizers, self.constraints, self.updates\n \n     def set_layer_input(self, head):\n-        layer = self.layer\n-        from ..layers.containers import Sequential\n-        while issubclass(layer.__class__, Sequential):\n-            layer = layer.layers[0]\n-        layer.previous = self.inputs[head]\n+        self.layer.set_previous(self.inputs[head], reset_weights=False)\n \n     def get_output_at(self, head, train=False):\n         X = self.inputs[head].get_output(train)\n@@ -1833,9 +1859,6 @@ class SiameseHead(Layer):\n         base_config = super(SiameseHead, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n-    def set_previous(self, layer):\n-        self.previous = layer\n-\n \n def add_shared_layer(layer, inputs):\n     '''Use this function to add a shared layer across\n\n@@ -6,7 +6,7 @@ from numpy.testing import assert_allclose\n \n from keras import backend as K\n from keras.layers.core import Dense\n-from keras.models import Sequential\n+from keras.models import Sequential, Graph\n \n \n def test_layer_call():\n@@ -56,5 +56,157 @@ def test_sequential_call():\n     assert_allclose(y1, y2)\n \n \n+def test_graph_call():\n+    \"\"\"Test keras.models.Graph.__call__\"\"\"\n+    nb_samples, input_dim, output_dim = 3, 10, 5\n+    model = Graph()\n+    model.add_input('input', input_shape=(input_dim, ))\n+    model.add_node(Dense(output_dim=output_dim, input_dim=input_dim),\n+                   input='input', name='output', create_output=True)\n+\n+    model.compile('sgd', {'output': 'mse'})\n+\n+    # test flat model\n+    X = K.placeholder(ndim=2)\n+    Y = model(X)\n+    f = K.function([X], [Y])\n+\n+    x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n+    y1 = f([x])[0].astype(K.floatx())\n+    y2 = model.predict({'input': x})['output']\n+    # results of __call__ should match model.predict\n+    assert_allclose(y1, y2)\n+\n+    # test nested Graph models\n+    model2 = Graph()\n+    model2.add_input('input', input_shape=(input_dim, ))\n+    model2.add_node(model, input='input', name='output', create_output=True)\n+    # need to turn off cache because we're reusing model\n+    model2.cache_enabled = False\n+    model2.compile('sgd', {'output': 'mse'})\n+\n+    Y2 = model2(X)\n+    f = K.function([X], [Y2])\n+\n+    y1 = f([x])[0].astype(K.floatx())\n+    y2 = model2.predict({'input': x})['output']\n+    # results of __call__ should match model.predict\n+    assert_allclose(y1, y2)\n+\n+\n+def test_graph_multiple_in_out_call():\n+    \"\"\"Test keras.models.Graph.__call__ with multiple inputs\"\"\"\n+    nb_samples, input_dim, output_dim = 3, 10, 5\n+    model = Graph()\n+    model.add_input('input1', input_shape=(input_dim, ))\n+    model.add_input('input2', input_shape=(input_dim, ))\n+    model.add_node(Dense(output_dim=output_dim, input_dim=input_dim),\n+                   inputs=['input1', 'input2'], merge_mode='sum', name='output', create_output=True)\n+\n+    model.compile('sgd', {'output': 'mse'})\n+\n+    # test flat model\n+    X1 = K.placeholder(ndim=2)\n+    X2 = K.placeholder(ndim=2)\n+    Y = model({'input1': X1, 'input2': X2})['output']\n+    f = K.function([X1, X2], [Y])\n+\n+    x1 = np.ones((nb_samples, input_dim)).astype(K.floatx())\n+    x2 = np.ones((nb_samples, input_dim)).astype(K.floatx()) * -2\n+    y1 = f([x1, x2])[0].astype(K.floatx())\n+    y2 = model.predict({'input1': x1, 'input2': x2})['output']\n+    # results of __call__ should match model.predict\n+    assert_allclose(y1, y2)\n+\n+    # test with single input, multiple outputs\n+    model2 = Graph()\n+    model2.add_input('input', input_shape=(input_dim, ))\n+    model2.add_node(Dense(output_dim=output_dim, input_dim=input_dim),\n+                    input='input', name='output1', create_output=True)\n+    model2.add_node(Dense(output_dim=output_dim, input_dim=input_dim),\n+                    input='input', name='output2', create_output=True)\n+\n+    model2.compile('sgd', {'output1': 'mse', 'output2': 'mse'})\n+\n+    # test flat model\n+    X = K.placeholder(ndim=2)\n+    Y = model2(X)\n+    f = K.function([X], [Y['output1'], Y['output2']])\n+\n+    x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n+    out = f([x])\n+    y1a = out[0].astype(K.floatx())\n+    y1b = out[1].astype(K.floatx())\n+    y2 = model2.predict({'input': x})\n+    # results of __call__ should match model.predict\n+    assert_allclose(y1a, y2['output1'])\n+    assert_allclose(y1b, y2['output2'])\n+\n+    # test with multiple inputs, multiple outputs\n+    model3 = Graph()\n+    model3.add_input('input1', input_shape=(input_dim, ))\n+    model3.add_input('input2', input_shape=(input_dim, ))\n+    model3.add_shared_node(Dense(output_dim=output_dim, input_dim=input_dim),\n+                           inputs=['input1', 'input2'], name='output',\n+                           outputs=['output1', 'output2'], create_output=True)\n+    model3.compile('sgd', {'output1': 'mse', 'output2': 'mse'})\n+\n+    # test flat model\n+    Y = model3({'input1': X1, 'input2': X2})\n+    f = K.function([X1, X2], [Y['output1'], Y['output2']])\n+\n+    x1 = np.ones((nb_samples, input_dim)).astype(K.floatx())\n+    x2 = np.ones((nb_samples, input_dim)).astype(K.floatx()) * -2\n+    out = f([x1, x2])\n+    y1a = out[0].astype(K.floatx())\n+    y1b = out[1].astype(K.floatx())\n+    y2 = model3.predict({'input1': x1, 'input2': x2})\n+    # results of __call__ should match model.predict\n+    assert_allclose(y1a, y2['output1'])\n+    assert_allclose(y1b, y2['output2'])\n+\n+\n+def test_nested_call():\n+    \"\"\"Test nested Sequential and Graph models\"\"\"\n+    nb_samples, input_dim, output_dim = 3, 10, 5\n+    X = K.placeholder(ndim=2)\n+    x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n+\n+    # test Graph model nested inside Sequential model\n+    model = Graph()\n+    model.add_input('input', input_shape=(input_dim, ))\n+    model.add_node(Dense(output_dim=output_dim, input_dim=input_dim),\n+                   input='input', name='output', create_output=True)\n+\n+    model2 = Sequential()\n+    model2.add(model)\n+    model2.compile('sgd', 'mse')\n+\n+    Y2 = model2(X)\n+    f = K.function([X], [Y2])\n+\n+    y1 = f([x])[0].astype(K.floatx())\n+    y2 = model2.predict(x)\n+    # results of __call__ should match model.predict\n+    assert_allclose(y1, y2)\n+\n+    # test Sequential model inside Graph model\n+    model3 = Sequential()\n+    model3.add(Dense(output_dim=output_dim, input_dim=input_dim))\n+\n+    model4 = Graph()\n+    model4.add_input('input', input_shape=(input_dim, ))\n+    model4.add_node(model3, input='input', name='output', create_output=True)\n+    model4.compile('sgd', {'output': 'mse'})\n+\n+    Y2 = model4(X)\n+    f = K.function([X], [Y2])\n+\n+    y1 = f([x])[0].astype(K.floatx())\n+    y2 = model4.predict({'input': x})['output']\n+    # results of __call__ should match model.predict\n+    assert_allclose(y1, y2)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#523e9845d7c11259feda86076c5ecadb7ed4075a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 99 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 12 | Methods Changed: 12 | Complexity Δ (Sum/Max): 24/14 | Churn Δ: 107 | Churn Cumulative: 9072 | Contributors (this commit): 44 | Commits (past 90d): 108 | Contributors (cumulative): 59 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,6 +36,26 @@ class Sequential(Layer):\n         for l in self.layers:\n             l.cache_enabled = value\n \n+    @property\n+    def layer_cache(self):\n+        return super(Sequential, self).layer_cache\n+\n+    @layer_cache.setter\n+    def layer_cache(self, value):\n+        self._layer_cache = value\n+        for layer in self.layers:\n+            layer.layer_cache = self._layer_cache\n+\n+    @property\n+    def shape_cache(self):\n+        return super(Sequential, self).shape_cache\n+\n+    @shape_cache.setter\n+    def shape_cache(self, value):\n+        self._shape_cache = value\n+        for layer in self.layers:\n+            layer.shape_cache = self._shape_cache\n+\n     def set_previous(self, layer, reset_weights=True):\n         self.layers[0].set_previous(layer, reset_weights)\n \n@@ -213,6 +233,30 @@ class Graph(Layer):\n         for l in self.inputs.values():\n             l.cache_enabled = value\n \n+    @property\n+    def layer_cache(self):\n+        return super(Graph, self).layer_cache\n+\n+    @layer_cache.setter\n+    def layer_cache(self, value):\n+        self._layer_cache = value\n+        for layer in self.nodes.values():\n+            layer.layer_cache = self._layer_cache\n+        for layer in self.inputs.values():\n+            layer.layer_cache = self._layer_cache\n+\n+    @property\n+    def shape_cache(self):\n+        return super(Graph, self).shape_cache\n+\n+    @shape_cache.setter\n+    def shape_cache(self, value):\n+        self._shape_cache = value\n+        for layer in self.nodes.values():\n+            layer.shape_cache = self._shape_cache\n+        for layer in self.inputs.values():\n+            layer.shape_cache = self._shape_cache\n+\n     @property\n     def nb_input(self):\n         return len(self.inputs)\n\n@@ -75,10 +75,34 @@ class Layer(object):\n     def cache_enabled(self, value):\n         self._cache_enabled = value\n \n+    @property\n+    def layer_cache(self):\n+        if hasattr(self, '_layer_cache'):\n+            return self._layer_cache\n+        else:\n+            return None\n+\n+    @layer_cache.setter\n+    def layer_cache(self, value):\n+        self._layer_cache = value\n+\n+    @property\n+    def shape_cache(self):\n+        if hasattr(self, '_shape_cache'):\n+            return self._shape_cache\n+        else:\n+            return None\n+\n+    @shape_cache.setter\n+    def shape_cache(self, value):\n+        self._shape_cache = value\n+\n     def __call__(self, X, mask=None, train=False):\n-        # turn off layer cache temporarily\n-        tmp_cache_enabled = self.cache_enabled\n-        self.cache_enabled = False\n+        # reset layer cache temporarily\n+        tmp_layer_cache = self.layer_cache\n+        tmp_shape_cache = self.shape_cache\n+        self.layer_cache = {}\n+        self.shape_cache = {}\n         # create a temporary layer\n         layer = Layer(batch_input_shape=self.input_shape)\n         layer.name = \"dummy\"\n@@ -96,7 +120,8 @@ class Layer(object):\n             self.set_previous(tmp_previous, False)\n         else:\n             self.clear_previous(False)\n-        self.cache_enabled = tmp_cache_enabled\n+        self.layer_cache = tmp_layer_cache\n+        self.shape_cache = tmp_shape_cache\n         return Y\n \n     def set_previous(self, layer, reset_weights=True):\n@@ -165,12 +190,12 @@ class Layer(object):\n         # if layer is not connected (e.g. input layer),\n         # input shape can be set manually via _input_shape attribute.\n         if hasattr(self, 'previous'):\n-            if hasattr(self, 'shape_cache') and self.cache_enabled:\n+            if self.shape_cache is not None and self.cache_enabled:\n                 previous_layer_id = id(self.previous)\n                 if previous_layer_id in self.shape_cache:\n                     return self.shape_cache[previous_layer_id]\n             previous_size = self.previous.output_shape\n-            if hasattr(self, 'shape_cache') and self.cache_enabled:\n+            if self.shape_cache is not None and self.cache_enabled:\n                 previous_layer_id = id(self.previous)\n                 self.shape_cache[previous_layer_id] = previous_size\n             return previous_size\n@@ -204,12 +229,12 @@ class Layer(object):\n         if hasattr(self, 'previous'):\n             # to avoid redundant computations,\n             # layer outputs are cached when possible.\n-            if hasattr(self, 'layer_cache') and self.cache_enabled:\n+            if self.layer_cache is not None and self.cache_enabled:\n                 previous_layer_id = '%s_%s' % (id(self.previous), train)\n                 if previous_layer_id in self.layer_cache:\n                     return self.layer_cache[previous_layer_id]\n             previous_output = self.previous.get_output(train=train)\n-            if hasattr(self, 'layer_cache') and self.cache_enabled:\n+            if self.layer_cache is not None and self.cache_enabled:\n                 previous_layer_id = '%s_%s' % (id(self.previous), train)\n                 self.layer_cache[previous_layer_id] = previous_output\n             return previous_output\n@@ -1240,7 +1265,9 @@ class AutoEncoder(Layer):\n \n         self._output_reconstruction = output_reconstruction\n         self.encoder = encoder\n+        self.encoder.layer_cache = self.layer_cache\n         self.decoder = decoder\n+        self.decoder.layer_cache = self.layer_cache\n \n         if output_reconstruction:\n             self.decoder.set_previous(self.encoder)\n@@ -1278,6 +1305,26 @@ class AutoEncoder(Layer):\n                     self.trainable_weights.append(p)\n                     self.constraints.append(c)\n \n+    @property\n+    def layer_cache(self):\n+        return super(AutoEncoder, self).layer_cache\n+\n+    @layer_cache.setter\n+    def layer_cache(self, value):\n+        self._layer_cache = value\n+        self.encoder.layer_cache = self._layer_cache\n+        self.decoder.layer_cache = self._layer_cache\n+\n+    @property\n+    def shape_cache(self):\n+        return super(AutoEncoder, self).shape_cache\n+\n+    @shape_cache.setter\n+    def shape_cache(self, value):\n+        self._shape_cache = value\n+        self.encoder.shape_cache = self._shape_cache\n+        self.decoder.shape_cache = self._shape_cache\n+\n     def set_previous(self, node, reset_weights=True):\n         self.encoder.set_previous(node, reset_weights)\n         if reset_weights:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bbaa66c5304fad4a45ecbb33445d5c619c1a76e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 1 | Complexity Δ (Sum/Max): -34/0 | Churn Δ: 27 | Churn Cumulative: 1776 | Contributors (this commit): 14 | Commits (past 90d): 51 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -650,10 +650,10 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n def pool2d(x, pool_size, strides=(1, 1), border_mode='valid',\n            dim_ordering='th', pool_mode='max'):\n     if border_mode == 'same':\n-        # TODO: add implementation for border_mode=\"same\"\n-        raise Exception('border_mode=\"same\" not supported with Theano.')\n+        w_pad = pool_size[0] - 2 if pool_size[0] % 2 == 1 else pool_size[0] - 1\n+        h_pad = pool_size[1] - 2 if pool_size[1] % 2 == 1 else pool_size[1] - 1\n+        padding = (w_pad, h_pad)\n     elif border_mode == 'valid':\n-        ignore_border = True\n         padding = (0, 0)\n     else:\n         raise Exception('Invalid border mode: ' + str(border_mode))\n@@ -666,17 +666,25 @@ def pool2d(x, pool_size, strides=(1, 1), border_mode='valid',\n \n     if pool_mode == 'max':\n         pool_out = downsample.max_pool_2d(x, ds=pool_size, st=strides,\n-                                          ignore_border=ignore_border,\n+                                          ignore_border=True,\n                                           padding=padding,\n                                           mode='max')\n     elif pool_mode == 'avg':\n         pool_out = downsample.max_pool_2d(x, ds=pool_size, st=strides,\n-                                          ignore_border=ignore_border,\n+                                          ignore_border=True,\n                                           padding=padding,\n                                           mode='average_exc_pad')\n     else:\n         raise Exception('Invalid pooling mode: ' + str(pool_mode))\n \n+    if border_mode == 'same':\n+        expected_width = (x.shape[2] + strides[0] - 1) // strides[0]\n+        expected_height = (x.shape[3] + strides[1] - 1) // strides[1]\n+\n+        pool_out = pool_out[:, :,\n+                            : expected_width,\n+                            : expected_height]\n+\n     if dim_ordering == 'tf':\n         pool_out = pool_out.dimshuffle((0, 2, 3, 1))\n     return pool_out\n\n@@ -191,16 +191,19 @@ def test_averagepooling_2d():\n     stack_size = 7\n     input_nb_row = 11\n     input_nb_col = 12\n-    pool_size = (3, 3)\n \n     input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+    for border_mode in ['valid', 'same']:\n+        for pool_size in [(2, 2), (3, 3), (4, 4), (5, 5)]:\n             for strides in [(1, 1), (2, 2)]:\n                 layer = convolutional.AveragePooling2D(strides=strides,\n-                                               border_mode='valid',\n+                                                       border_mode=border_mode,\n                                                        pool_size=pool_size)\n                 layer.input = K.variable(input)\n                 for train in [True, False]:\n-            K.eval(layer.get_output(train))\n+                    out = K.eval(layer.get_output(train))\n+                    if border_mode == 'same' and strides == (1, 1):\n+                        assert input.shape == out.shape\n                 layer.get_config()\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3ffba424663fdef32657082e5a60f00d5922c59e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 4 | Churn Cumulative: 7776 | Contributors (this commit): 41 | Commits (past 90d): 68 | Contributors (cumulative): 41 | DMM Complexity: None\n\nDIFF:\n@@ -1804,12 +1804,12 @@ class Siamese(Layer):\n         return weights\n \n     def set_weights(self, weights):\n-        nb_param = len(self.layer.trainable_weights)\n+        nb_param = len(self.layer.get_weights())\n         self.layer.set_weights(weights[:nb_param])\n         weights = weights[nb_param:]\n         if self.merge_mode and not self.is_graph:\n             for i in range(len(self.inputs)):\n-                nb_param = len(self.inputs[i].trainable_weights)\n+                nb_param = len(self.inputs[i].get_weights())\n                 self.inputs[i].set_weights(weights[:nb_param])\n                 weights = weights[nb_param:]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
