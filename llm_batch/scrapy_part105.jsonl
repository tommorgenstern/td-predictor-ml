{"custom_id": "scrapy#7394aa926d4f5ee1ad297fcd8452e69412d4aef2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 53 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -33,7 +33,7 @@ class Command(ScrapyCommand):\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n-            sys.stderr.write(\"Invalid --arg value, use --arg NAME=VALUE\\n\")\n+            sys.stderr.write(\"Invalid -a value, use -a NAME=VALUE\\n\")\n             sys.exit(2)\n         if opts.nofollow:\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59f09c50e41a8518e7742bd61acb257f557fa61f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 103 | Files Changed: 14 | Hunks: 66 | Methods Changed: 35 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 212 | Churn Cumulative: 1313 | Contributors (this commit): 4 | Commits (past 90d): 65 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,11 +6,10 @@ import optparse\n import cProfile\n \n import scrapy\n-from scrapy import log\n from scrapy.crawler import CrawlerProcess\n from scrapy.xlib import lsprofcalltree\n from scrapy.conf import settings\n-from scrapy.command import ScrapyCommand\n+from scrapy.exceptions import UsageError\n \n def _find_commands(dir):\n     try:\n@@ -19,47 +18,61 @@ def _find_commands(dir):\n     except OSError:\n         return []\n \n-def _get_commands_from_module(module):\n+def _get_commands_from_module(module, inproject):\n     d = {}\n     mod = __import__(module, {}, {}, [''])\n     for cmdname in _find_commands(mod.__path__[0]):\n         modname = '%s.%s' % (module, cmdname)\n         command = getattr(__import__(modname, {}, {}, [cmdname]), 'Command', None)\n         if callable(command):\n+            if inproject or not command.requires_project:\n                 d[cmdname] = command()\n         else:\n-            print 'WARNING: Module %r does not define a Command class' % modname\n+            raise RuntimeError(\"Module %r does not define a Command class\" % modname)\n     return d\n \n-def _get_commands_dict():\n-    cmds = _get_commands_from_module('scrapy.commands')\n+def _get_commands_dict(inproject):\n+    cmds = _get_commands_from_module('scrapy.commands', inproject)\n     cmds_module = settings['COMMANDS_MODULE']\n     if cmds_module:\n-        cmds.update(_get_commands_from_module(cmds_module))\n+        cmds.update(_get_commands_from_module(cmds_module, inproject))\n     return cmds\n \n-def _get_command_name(argv):\n+def _pop_command_name(argv):\n+    i = 0\n     for arg in argv[1:]:\n         if not arg.startswith('-'):\n+            del argv[i]\n             return arg\n+        i += 1\n \n-def _print_usage(inside_project):\n-    if inside_project:\n+def _print_header(inproject):\n+    if inproject:\n         print \"Scrapy %s - project: %s\\n\" % (scrapy.__version__, \\\n             settings['BOT_NAME'])\n     else:\n         print \"Scrapy %s - no active project\\n\" % scrapy.__version__\n+\n+def _print_commands(inproject):\n+    _print_header(inproject)\n     print \"Usage:\"\n     print \"  scrapy <command> [options] [args]\\n\"\n     print \"Available commands:\"\n-    cmds = _get_commands_dict()\n+    cmds = _get_commands_dict(inproject)\n     for cmdname, cmdclass in sorted(cmds.iteritems()):\n-        if inside_project or not cmdclass.requires_project:\n         print \"  %-13s %s\" % (cmdname, cmdclass.short_desc())\n     print\n-    print 'Use \"scrapy <command> -h\" for more info about a command'\n+    print 'Use \"scrapy <command> -h\" to see more info about a command'\n \n-def check_deprecated_scrapy_ctl(argv):\n+def _print_unknown_command(cmdname, inproject):\n+    _print_header(inproject)\n+    print \"Unknown command: %s\\n\" % cmdname\n+    print 'Use \"scrapy\" to see available commands' \n+    if not inproject:\n+        print\n+        print \"More commands are available in project mode\"\n+\n+def _check_deprecated_scrapy_ctl(argv, inproject):\n     \"\"\"Check if Scrapy was called using the deprecated scrapy-ctl command and\n     warn in that case, also creating a scrapy.cfg if it doesn't exist.\n     \"\"\"\n@@ -68,7 +81,7 @@ def check_deprecated_scrapy_ctl(argv):\n     import warnings\n     warnings.warn(\"`scrapy-ctl.py` command-line tool is deprecated and will be removed in Scrapy 0.11, use `scrapy` instead\",\n         DeprecationWarning, stacklevel=3)\n-    if settings.settings_module:\n+    if inproject:\n         projpath = os.path.abspath(os.path.dirname(os.path.dirname(settings.settings_module.__file__)))\n         cfg_path = os.path.join(projpath, 'scrapy.cfg')\n         if not os.path.exists(cfg_path):\n@@ -77,72 +90,64 @@ def check_deprecated_scrapy_ctl(argv):\n                 f.write(\"[default]\" + os.linesep)\n                 f.write(\"settings = %s\" % settings.settings_module_path + os.linesep)\n \n+def _run_print_help(parser, func, *a, **kw):\n+    try:\n+        func(*a, **kw)\n+    except UsageError, e:\n+        if str(e):\n+            parser.error(str(e))\n+        if e.print_help:\n+            parser.print_help()\n+        sys.exit(2)\n+\n def execute(argv=None):\n     if argv is None:\n         argv = sys.argv\n     crawler = CrawlerProcess(settings)\n     crawler.install()\n-    check_deprecated_scrapy_ctl(argv) # TODO: remove for Scrapy 0.11\n-    cmds = _get_commands_dict()\n-    cmdname = _get_command_name(argv)\n+    inproject = bool(settings.settings_module)\n+    _check_deprecated_scrapy_ctl(argv, inproject) # TODO: remove for Scrapy 0.11\n+    cmds = _get_commands_dict(inproject)\n+    cmdname = _pop_command_name(argv)\n     parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \\\n-        conflict_handler='resolve', add_help_option=False)\n-    if cmdname in cmds:\n-        cmd = cmds[cmdname]\n-        cmd.add_options(parser)\n-        opts, args = parser.parse_args(args=argv[1:])\n-        cmd.process_options(args, opts)\n-        parser.usage = \"%%prog %s %s\" % (cmdname, cmd.syntax())\n-        parser.description = cmd.long_desc()\n-        if cmd.requires_project and not settings.settings_module:\n-            print \"Error running: scrapy %s\\n\" % cmdname\n-            print \"Cannot find project settings module in python path: %s\" % \\\n-                settings.settings_module_path\n-            sys.exit(1)\n-        if opts.help:\n-            parser.print_help()\n-            sys.exit()\n-    elif not cmdname:\n-        cmd = ScrapyCommand()\n-        cmd.add_options(parser)\n-        opts, args = parser.parse_args(args=argv)\n-        cmd.process_options(args, opts)\n-        _print_usage(settings.settings_module)\n-        sys.exit(2)\n-    else:\n-        print \"Unknown command: %s\\n\" % cmdname\n-        print 'Use \"scrapy -h\" for help' \n+        conflict_handler='resolve')\n+    if not cmdname:\n+        _print_commands(inproject)\n+        sys.exit(0)\n+    elif cmdname not in cmds:\n+        _print_unknown_command(cmdname, inproject)\n         sys.exit(2)\n \n+    cmd = cmds[cmdname]\n+    parser.usage = \"scrapy %s %s\" % (cmdname, cmd.syntax())\n+    parser.description = cmd.long_desc()\n     settings.defaults.update(cmd.default_settings)\n-    del args[0]  # remove command name from args\n-    log.start()\n     cmd.set_crawler(crawler)\n-    ret = _run_command(cmd, args, opts)\n-    if ret is False:\n-        parser.print_help()\n+    cmd.add_options(parser)\n+    opts, args = parser.parse_args(args=argv[1:])\n+    _run_print_help(parser, cmd.process_options, args, opts)\n+    _run_print_help(parser, _run_command, cmd, args, opts)\n \n def _run_command(cmd, args, opts):\n     if opts.profile or opts.lsprof:\n-        return _run_command_profiled(cmd, args, opts)\n+        _run_command_profiled(cmd, args, opts)\n     else:\n-        return cmd.run(args, opts)\n+        cmd.run(args, opts)\n \n def _run_command_profiled(cmd, args, opts):\n     if opts.profile:\n-        log.msg(\"writing cProfile stats to %r\" % opts.profile)\n+        sys.stderr.write(\"scrapy: writing cProfile stats to %r\\n\" % opts.profile)\n     if opts.lsprof:\n-        log.msg(\"writing lsprof stats to %r\" % opts.lsprof)\n+        sys.stderr.write(\"scrapy: writing lsprof stats to %r\\n\" % opts.lsprof)\n     loc = locals()\n     p = cProfile.Profile()\n-    p.runctx('ret = cmd.run(args, opts)', globals(), loc)\n+    p.runctx('cmd.run(args, opts)', globals(), loc)\n     if opts.profile:\n         p.dump_stats(opts.profile)\n     k = lsprofcalltree.KCacheGrind(p)\n     if opts.lsprof:\n         with open(opts.lsprof, 'w') as f:\n             k.output(f)\n-    return loc['ret']\n \n if __name__ == '__main__':\n     execute()\n\n@@ -9,8 +9,10 @@ import sys\n from optparse import OptionGroup\n \n import scrapy\n+from scrapy import log\n from scrapy.conf import settings\n from scrapy.utils.conf import arglist_to_dict\n+from scrapy.exceptions import UsageError\n \n class ScrapyCommand(object):\n \n@@ -24,6 +26,8 @@ class ScrapyCommand(object):\n \n     @property\n     def crawler(self):\n+        if not log.started:\n+            log.start()\n         self._crawler.configure()\n         return self._crawler\n \n@@ -58,10 +62,6 @@ class ScrapyCommand(object):\n         Populate option parse with options available for this command\n         \"\"\"\n         group = OptionGroup(parser, \"Global Options\")\n-        group.add_option(\"-h\", \"--help\", action=\"store_true\", dest=\"help\", \\\n-            help=\"print command help and options\")\n-        group.add_option(\"--version\", action=\"store_true\", dest=\"version\", \\\n-            help=\"print Scrapy version and exit\")\n         group.add_option(\"--logfile\", dest=\"logfile\", metavar=\"FILE\", \\\n             help=\"log file. if omitted stderr will be used\")\n         group.add_option(\"-L\", \"--loglevel\", dest=\"loglevel\", metavar=\"LEVEL\", \\\n@@ -77,23 +77,13 @@ class ScrapyCommand(object):\n             help=\"write process ID to FILE\")\n         group.add_option(\"--set\", dest=\"set\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n             help=\"set/override setting (may be repeated)\")\n-        group.add_option(\"--settings\", dest=\"settings\", metavar=\"MODULE\",\n-            help=\"python path to the Scrapy project settings\")\n         parser.add_option_group(group)\n         \n     def process_options(self, args, opts):\n-        if opts.settings:\n-            settings.set_settings_module(opts.settings)\n-\n         try:\n             settings.overrides.update(arglist_to_dict(opts.set))\n         except ValueError:\n-            sys.stderr.write(\"Invalid --set value, use --set NAME=VALUE\\n\")\n-            sys.exit(2)\n-\n-        if opts.version:\n-            print \"Scrapy %s\" % scrapy.__version__\n-            sys.exit()\n+            raise UsageError(\"Invalid --set value, use --set NAME=VALUE\", print_help=False)\n \n         if opts.logfile:\n             settings.overrides['LOG_ENABLED'] = True\n@@ -115,4 +105,3 @@ class ScrapyCommand(object):\n         Entry point for running commands\n         \"\"\"\n         raise NotImplementedError\n-\n\n@@ -6,6 +6,7 @@ from scrapy.conf import settings\n from scrapy.http import Request\n from scrapy.utils.url import is_url\n from scrapy.utils.conf import arglist_to_dict\n+from scrapy.exceptions import UsageError\n \n from collections import defaultdict\n \n@@ -33,8 +34,7 @@ class Command(ScrapyCommand):\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n-            sys.stderr.write(\"Invalid -a value, use -a NAME=VALUE\\n\")\n-            sys.exit(2)\n+            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n         if opts.nofollow:\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n\n@@ -5,6 +5,7 @@ from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n from scrapy.utils.url import is_url\n+from scrapy.exceptions import UsageError\n \n class Command(ScrapyCommand):\n \n@@ -35,7 +36,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         if len(args) != 1 or not is_url(args[0]):\n-            return False\n+            raise UsageError()\n         cb = lambda x: self._print_response(x, opts)\n         request = Request(args[0], callback=cb, dont_filter=True)\n \n\n@@ -7,6 +7,7 @@ import scrapy\n from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n from scrapy.utils.template import render_templatefile, string_camelcase\n+from scrapy.exceptions import UsageError\n \n def sanitize_module_name(module_name):\n     \"\"\"Sanitize the given module name, by replacing dashes and points\n@@ -54,7 +55,7 @@ class Command(ScrapyCommand):\n                 print open(template_file, 'r').read()\n             return\n         if len(args) != 2:\n-            return False\n+            raise UsageError()\n \n         name, domain = args[0:2]\n         module = sanitize_module_name(name)\n\n@@ -4,6 +4,7 @@ from scrapy.item import BaseItem\n from scrapy.utils import display\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.url import is_url\n+from scrapy.exceptions import UsageError\n from scrapy import log\n \n class Command(ScrapyCommand):\n@@ -93,7 +94,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         if not len(args) == 1 or not is_url(args[0]):\n-            return False\n+            raise UsageError()\n         response, spider = self.get_response_and_spider(args[0], opts)\n         if not response:\n             return\n\n@@ -1,6 +1,6 @@\n from scrapy.command import ScrapyCommand\n from scrapy.commands import runserver\n-from scrapy.utils.misc import load_object\n+from scrapy.exceptions import UsageError\n from scrapy.conf import settings\n \n class Command(runserver.Command):\n@@ -23,7 +23,7 @@ class Command(runserver.Command):\n \n     def run(self, args, opts):\n         if len(args) < 1:\n-            return False\n+            raise UsageError()\n         cmd = args[0]\n \n         botname = settings['BOT_NAME']\n@@ -31,7 +31,7 @@ class Command(runserver.Command):\n \n         if cmd == 'add':\n             if len(args) < 2:\n-                return False\n+                raise UsageError()\n             msg = dict(x for x in [x.split('=', 1) for x in opts.spargs])\n             for x in args[1:]:\n                 msg.update(name=x)\n@@ -44,4 +44,4 @@ class Command(runserver.Command):\n             queue.clear()\n             print \"Cleared %s queue\" % botname\n         else:\n-            return False\n+            raise UsageError()\n\n@@ -1,9 +1,9 @@\n import sys\n import os\n \n-from scrapy import log\n from scrapy.utils.spider import iter_spider_classes\n from scrapy.command import ScrapyCommand\n+from scrapy.exceptions import UsageError\n \n def _import_file(filepath):\n     abspath = os.path.abspath(filepath)\n@@ -33,22 +33,22 @@ class Command(ScrapyCommand):\n     def long_desc(self):\n         return \"Run the spider defined in the given file\"\n \n+    def process_options(self, args, opts):\n+        ScrapyCommand.process_options(self, args, opts)\n+\n     def run(self, args, opts):\n         if len(args) != 1:\n-            return False\n+            raise UsageError()\n         filename = args[0]\n         if not os.path.exists(filename):\n-            log.msg(\"File not found: %s\\n\" % filename, log.ERROR)\n-            return\n+            raise UsageError(\"File not found: %s\\n\" % filename)\n         try:\n             module = _import_file(filename)\n         except (ImportError, ValueError), e:\n-            log.msg(\"Unable to load %r: %s\\n\" % (filename, e), log.ERROR)\n-            return\n+            raise UsageError(\"Unable to load %r: %s\\n\" % (filename, e))\n         spclasses = list(iter_spider_classes(module))\n         if not spclasses:\n-            log.msg(\"No spider found in file: %s\\n\" % filename, log.ERROR)\n-            return\n+            raise UsageError(\"No spider found in file: %s\\n\" % filename)\n         spider = spclasses.pop()()\n         # schedule spider and start engine\n         self.crawler.queue.append_spider(spider)\n\n@@ -8,6 +8,7 @@ import scrapy\n from scrapy.command import ScrapyCommand\n from scrapy.utils.template import render_templatefile, string_camelcase\n from scrapy.utils.py26 import ignore_patterns, copytree\n+from scrapy.exceptions import UsageError\n \n TEMPLATES_PATH = join(scrapy.__path__[0], 'templates', 'project')\n \n@@ -33,7 +34,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         if len(args) != 1:\n-            return False\n+            raise UsageError()\n         project_name = args[0]\n         if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n             print 'Error: Project names must begin with a letter and contain only\\n' \\\n\n@@ -0,0 +1,10 @@\n+import scrapy\n+from scrapy.command import ScrapyCommand\n+\n+class Command(ScrapyCommand):\n+\n+    def short_desc(self):\n+        return \"Print Scrapy version\"\n+\n+    def run(self, args, opts):\n+        print \"Scrapy %s\" % scrapy.__version__\n\n@@ -38,3 +38,11 @@ class DropItem(Exception):\n class NotSupported(Exception):\n     \"\"\"Indicates a feature or method is not supported\"\"\"\n     pass\n+\n+# Commands\n+\n+class UsageError(Exception):\n+    \"\"\"To indicate a command-line usage error\"\"\"\n+    def __init__(self, *a, **kw):\n+        self.print_help = kw.pop('print_help', True)\n+        super(UsageError, self).__init__(*a, **kw)\n\n@@ -23,11 +23,6 @@ class CmdlineTest(unittest.TestCase):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n             'default + loaded + started')\n \n-    def test_override_settings_using_settings_arg(self):\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1', \\\n-            '--settings', 'scrapy.tests.test_cmdline.settings2'), \\\n-            'override + loaded + started')\n-\n     def test_override_settings_using_set_arg(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1', '--set', 'TEST1=override'), \\\n             'override + loaded + started')\n\n@@ -1,5 +0,0 @@\n-EXTENSIONS = [\n-    'scrapy.tests.test_cmdline.extensions.TestExtension'\n-]\n-\n-TEST1 = 'override'\n\n@@ -69,7 +69,7 @@ class GenspiderCommandTest(CommandTest):\n \n     def test_arguments(self):\n         # only pass one argument. spider script shouldn't be created\n-        self.assertEqual(0, self.call('genspider', 'test_name'))\n+        self.assertEqual(2, self.call('genspider', 'test_name'))\n         assert not exists(join(self.proj_mod_path, 'spiders', 'test_name.py'))\n         # pass two arguments <name> <domain>. spider script should be created\n         self.assertEqual(0, self.call('genspider', 'test_name', 'test.com'))\n@@ -147,12 +147,12 @@ from scrapy.spider import BaseSpider\n \"\"\")\n         p = self.proc('runspider', fname)\n         log = p.stderr.read()\n-        self.assert_(\"ERROR: No spider found in file\" in log)\n+        self.assert_(\"No spider found in file\" in log)\n \n     def test_runspider_file_not_found(self):\n         p = self.proc('runspider', 'some_non_existent_file')\n         log = p.stderr.read()\n-        self.assert_(\"ERROR: File not found: some_non_existent_file\" in log)\n+        self.assert_(\"File not found: some_non_existent_file\" in log)\n \n     def test_runspider_unable_to_load(self):\n         tmpdir = self.mktemp()\n@@ -162,5 +162,5 @@ from scrapy.spider import BaseSpider\n             f.write(\"\")\n         p = self.proc('runspider', fname)\n         log = p.stderr.read()\n-        self.assert_(\"ERROR: Unable to load\" in log)\n+        self.assert_(\"Unable to load\" in log)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#758d21b2f95989c283460d72b1d4fbff4be6a2bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 8 | Churn Cumulative: 574 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy import log\n from scrapy.stats import stats\n from scrapy.utils.misc import md5sum\n+from scrapy.http import Request\n from scrapy import signals\n from scrapy.exceptions import DropItem, NotConfigured, IgnoreRequest\n from scrapy.contrib.pipeline.media import MediaPipeline\n@@ -283,3 +284,10 @@ class ImagesPipeline(MediaPipeline):\n     def thumb_key(self, url, thumb_id):\n         image_guid = hashlib.sha1(url).hexdigest()\n         return 'thumbs/%s/%s.jpg' % (thumb_id, image_guid)\n+\n+    def get_media_requests(self, item, info):\n+        return [Request(x) for x in item.get('image_urls', [])]\n+\n+    def item_completed(self, results, item, info):\n+        item['images'] = [x for ok, x in results if ok]\n+        return item\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb69655a9fbaae6d120044438cb390c28c6261ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 22 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import operator\n-from itertools import groupby\n from collections import defaultdict\n from scrapy.xlib.pydispatch import dispatcher\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#37776618a30cd50a80c71a27fbb9da05d0d8eca2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 44 | Churn Cumulative: 630 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -87,8 +87,8 @@ def _check_deprecated_scrapy_ctl(argv, inproject):\n         if not os.path.exists(cfg_path):\n             with open(cfg_path, 'w') as f:\n                 f.write(\"# generated automatically - feel free to edit\" + os.linesep)\n-                f.write(\"[default]\" + os.linesep)\n-                f.write(\"settings = %s\" % settings.settings_module_path + os.linesep)\n+                f.write(\"[settings]\" + os.linesep)\n+                f.write(\"default = %s\" % settings.settings_module_path + os.linesep)\n \n def _run_print_help(parser, func, *a, **kw):\n     try:\n\n@@ -8,7 +8,7 @@ import os\n import cPickle as pickle\n \n from scrapy.conf import default_settings\n-from scrapy.utils.conf import set_scrapy_settings_envvar\n+from scrapy.utils.conf import init_env\n \n import_ = lambda x: __import__(x, {}, {}, [''])\n \n@@ -60,7 +60,7 @@ class EnvironmentSettings(Settings):\n         self.disabled = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n         if self.ENVVAR not in os.environ:\n             project = os.environ.get('SCRAPY_PROJECT', 'default')\n-            set_scrapy_settings_envvar(project)\n+            init_env(project)\n         settings_module_path = os.environ.get(self.ENVVAR, 'scrapy_settings')\n         self.set_settings_module(settings_module_path)\n \n\n@@ -1,6 +1,6 @@\n import sys\n import os\n-from ConfigParser import RawConfigParser\n+from ConfigParser import SafeConfigParser\n from operator import itemgetter\n \n def build_component_list(base, custom):\n@@ -17,10 +17,14 @@ def build_component_list(base, custom):\n \n def arglist_to_dict(arglist):\n     \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n-    dict\"\"\"\n+    dict\n+    \"\"\"\n     return dict(x.split('=', 1) for x in arglist)\n \n def closest_scrapy_cfg(path='.', prevpath=None):\n+    \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n+    directory and its parents\n+    \"\"\"\n     if path == prevpath:\n         return ''\n     path = os.path.abspath(path)\n@@ -29,13 +33,25 @@ def closest_scrapy_cfg(path='.', prevpath=None):\n         return cfgfile\n     return closest_scrapy_cfg(os.path.dirname(path), path)\n \n-def set_scrapy_settings_envvar(project='default', set_syspath=True):\n-    scrapy_cfg = closest_scrapy_cfg()\n-    cfg_sources = [scrapy_cfg, os.path.expanduser('~/.scrapy.cfg'), '/etc/scrapy.cfg']\n-    cfg = RawConfigParser()\n-    cfg.read(cfg_sources)\n-    if cfg.has_option(project, 'settings'):\n-        os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get(project, 'settings')\n-        projdir = os.path.dirname(scrapy_cfg)\n+def init_env(project='default', set_syspath=True):\n+    \"\"\"Initialize environment to use command-line tool from inside a project\n+    dir. This sets the Scrapy settings module and modifies the Python path to\n+    be able to locate the project module.\n+    \"\"\"\n+    cfg = get_config()\n+    if cfg.has_option('settings', project):\n+        os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('settings', project)\n+    closest = closest_scrapy_cfg()\n+    if closest:\n+        projdir = os.path.dirname(closest)\n         if set_syspath and projdir not in sys.path:\n             sys.path.append(projdir)\n+\n+def get_config(use_closest=True):\n+    \"\"\"Get Scrapy config file as a SafeConfigParser\"\"\"\n+    sources = [os.path.expanduser('~/.scrapy.cfg'), '/etc/scrapy.cfg']\n+    if use_closest:\n+        sources.insert(0, closest_scrapy_cfg())\n+    cfg = SafeConfigParser()\n+    cfg.read(sources)\n+    return cfg\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7cfc37923014fba120a78521b1acc8d529aa5a59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 251 | Lines Deleted: 119 | Files Changed: 16 | Hunks: 43 | Methods Changed: 54 | Complexity Δ (Sum/Max): 43/19 | Churn Δ: 370 | Churn Cumulative: 1561 | Contributors (this commit): 5 | Commits (past 90d): 71 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,47 +1,64 @@\n+from twisted.internet import reactor, threads\n+\n from scrapy.command import ScrapyCommand\n from scrapy.commands import runserver\n from scrapy.exceptions import UsageError\n-from scrapy.conf import settings\n+from scrapy.utils.conf import arglist_to_dict\n \n class Command(runserver.Command):\n \n     requires_project = True\n-    default_settings = {'LOG_ENABLED': False}\n+    default_settings = {'LOG_LEVEL': 'WARNING'}\n \n     def syntax(self):\n-        return \"[options] <list|clear|add spider1 ..>\"\n+        return \"[options] <list|clear|count|add spider1 ..>\"\n \n     def short_desc(self):\n         return \"Control execution queue\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--priority\", dest=\"priority\", type=\"float\", default=0.0, \\\n-            help=\"priority to use for adding spiders\")\n         parser.add_option(\"-a\", \"--arg\", dest=\"spargs\", action=\"append\", default=[], \\\n-            help=\"spider arguments to use for adding spiders\")\n+            help=\"set spider argument (may be repeated)\")\n+\n+    def process_options(self, args, opts):\n+        ScrapyCommand.process_options(self, args, opts)\n+        try:\n+            opts.spargs = arglist_to_dict(opts.spargs)\n+        except ValueError:\n+            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n \n     def run(self, args, opts):\n         if len(args) < 1:\n             raise UsageError()\n         cmd = args[0]\n \n-        botname = settings['BOT_NAME']\n-        queue = self.crawler.queue.queue\n+        q = self.crawler.queue._queue\n \n-        if cmd == 'add':\n-            if len(args) < 2:\n+        if cmd == 'add' and len(args) < 2:\n             raise UsageError()\n-            msg = dict(x for x in [x.split('=', 1) for x in opts.spargs])\n+\n+        d = threads.deferToThread(self._run_in_thread, args, opts, q, cmd)\n+        d.addBoth(lambda _: reactor.stop())\n+        from scrapy import log\n+        log.start()\n+        reactor.run()\n+\n+    def _run_in_thread(self, args, opts, q, cmd):\n+        if cmd == 'add':\n             for x in args[1:]:\n-                msg.update(name=x)\n-                queue.put(msg)\n-                print \"Added (priority=%s): %s\" % (opts.priority, msg)\n+                self._call(q.add, x, **opts.spargs)\n+                print \"Added: name=%s args=%s\" % (x, opts.spargs)\n         elif cmd == 'list':\n-            for x, y in queue:\n-                print \"(priority=%s) %s\" % (y, x)\n+            x = self._call(q.list)\n+            print \"\\n\".join(map(str, x))\n+        elif cmd == 'count':\n+            print self._call(q.count)\n         elif cmd == 'clear':\n-            queue.clear()\n-            print \"Cleared %s queue\" % botname\n+            self._call(q.clear)\n         else:\n             raise UsageError()\n+\n+    def _call(self, f, *a, **kw):\n+        return threads.blockingCallFromThread(reactor, f, *a, **kw)\n+\n\n@@ -4,13 +4,10 @@ from scrapy.conf import settings\n class Command(ScrapyCommand):\n \n     requires_project = True\n+    default_settings = {'KEEP_ALIVE': True}\n \n     def short_desc(self):\n         return \"Start Scrapy in server mode\"\n \n-    def process_options(self, args, opts):\n-        super(Command, self).process_options(args, opts)\n-        settings.overrides['QUEUE_CLASS'] = settings['SERVER_QUEUE_CLASS']\n-\n     def run(self, args, opts):\n         self.crawler.start()\n\n@@ -10,7 +10,7 @@ from scrapy.shell import Shell\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'QUEUE_CLASS': 'scrapy.queue.KeepAliveExecutionQueue'}\n+    default_settings = {'KEEP_ALIVE': True}\n \n     def syntax(self):\n         return \"[url|file]\"\n\n@@ -24,7 +24,6 @@ IGNORE = ignore_patterns('*.pyc', '.svn')\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'LOG_ENABLED': False}\n \n     def syntax(self):\n         return \"<project_name>\"\n\n@@ -150,6 +150,8 @@ ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n # Item pipelines are typically set in specific commands settings\n ITEM_PIPELINES = []\n \n+KEEP_ALIVE = False\n+\n LOG_ENABLED = True\n LOG_ENCODING = 'utf-8'\n LOG_FORMATTER = 'scrapy.logformatter.LogFormatter'\n@@ -175,8 +177,6 @@ MEMUSAGE_WARNING_MB = 0\n \n NEWSPIDER_MODULE = ''\n \n-QUEUE_CLASS = 'scrapy.queue.ExecutionQueue'\n-\n RANDOMIZE_DOWNLOAD_DELAY = True\n \n REDIRECT_MAX_METAREFRESH_DELAY = 100\n@@ -209,8 +209,6 @@ SCHEDULER_MIDDLEWARES_BASE = {\n \n SCHEDULER_ORDER = 'DFO'\n \n-SERVER_QUEUE_CLASS = 'scrapy.contrib.queue.SqliteExecutionQueue'\n-\n SPIDER_MANAGER_CLASS = 'scrapy.spidermanager.SpiderManager'\n \n SPIDER_MIDDLEWARES = {}\n@@ -228,6 +226,8 @@ SPIDER_MIDDLEWARES_BASE = {\n \n SPIDER_MODULES = []\n \n+SPIDER_QUEUE_CLASS = 'scrapy.spiderqueue.SqliteSpiderQueue'\n+\n SQLITE_DB = 'scrapy.db'\n \n SQS_QUEUE = 'scrapy'\n\n@@ -1,20 +0,0 @@\n-from scrapy.queue import ExecutionQueue\n-from scrapy.utils.sqlite import JsonSqlitePriorityQueue\n-from scrapy.conf import settings\n-\n-class SqliteExecutionQueue(ExecutionQueue):\n-\n-    queue_class = JsonSqlitePriorityQueue\n-\n-    def __init__(self, *a, **kw):\n-        super(SqliteExecutionQueue, self).__init__(*a, **kw)\n-        self.queue = JsonSqlitePriorityQueue(settings['SQLITE_DB'])\n-\n-    def _append_next(self):\n-        msg = self.queue.pop()\n-        if msg:\n-            name = msg.pop('name')\n-            self.append_spider_name(name, **msg)\n-\n-    def is_finished(self):\n-        return False\n\n@@ -1,49 +0,0 @@\n-import threading\n-\n-from twisted.internet import threads\n-from boto.sqs.connection import SQSConnection\n-from boto.sqs import regions\n-\n-from scrapy.queue import ExecutionQueue\n-from scrapy.utils.py26 import json\n-from scrapy.conf import settings\n-\n-class SQSExecutionQueue(ExecutionQueue):\n-\n-    polling_delay = settings.getint('SQS_POLLING_DELAY')\n-    queue_name = settings['SQS_QUEUE']\n-    region_name = settings['SQS_REGION']\n-    visibility_timeout = settings.getint('SQS_VISIBILITY_TIMEOUT')\n-    aws_access_key_id = settings['AWS_ACCESS_KEY_ID']\n-    aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n-\n-    def __init__(self, *a, **kw):\n-        super(SQSExecutionQueue, self).__init__(*a, **kw)\n-        self.region = self._get_region()\n-        self._tls = threading.local()\n-\n-    def _append_next(self):\n-        return threads.deferToThread(self._append_next_from_sqs)\n-\n-    def _append_next_from_sqs(self):\n-        q = self._get_sqs_queue()\n-        msgs = q.get_messages(1, visibility_timeout=self.visibility_timeout)\n-        if msgs:\n-            msg = msgs[0]\n-            msg.delete()\n-            spargs = json.loads(msg.get_body())\n-            spname = spargs.pop('name')\n-            self.append_spider_name(spname, **spargs)\n-\n-    def _get_sqs_queue(self):\n-        if not hasattr(self._tls, 'queue'):\n-            c = SQSConnection(self.aws_access_key_id, self.aws_secret_access_key, \\\n-                region=self.region)\n-            self._tls.queue = c.create_queue(self.queue_name)\n-        return self._tls.queue\n-\n-    def _get_region(self, name=region_name):\n-        return [r for r in regions() if r.name == name][0]\n-\n-    def is_finished(self):\n-        return False\n\n@@ -0,0 +1,83 @@\n+from zope.interface import implements\n+\n+from twisted.internet import threads\n+from boto.sqs.connection import SQSConnection\n+from boto.sqs.message import Message\n+from boto.sqs import regions\n+\n+from scrapy.interfaces import ISpiderQueue\n+from scrapy.utils.py26 import json\n+\n+class SQSSpiderQueue(object):\n+\n+    implements(ISpiderQueue)\n+\n+    def __init__(self, *a, **kw):\n+        self.polling_delay = kw.pop('polling_delay', 30)\n+        self.queue_name = kw.pop('queue_name', 'scrapy')\n+        self.region_name = kw.pop('region_name', 'us-east-1')\n+        self.visibility_timeout = kw.pop('visibility_timeout', 7200)\n+        self.aws_access_key_id = kw.pop('aws_access_key_id', None)\n+        self.aws_secret_access_key = kw.pop('aws_secret_access_key', None)\n+        self.region = self._get_region(self.region_name)\n+        self.conn.create_queue(self.queue_name)\n+        super(SQSSpiderQueue, self).__init__(*a, **kw)\n+\n+    @classmethod\n+    def from_settings(cls, settings):\n+        return cls(\n+            polling_delay=settings.getint('SQS_POLLING_DELAY'),\n+            queue_name=settings['SQS_QUEUE'],\n+            region_name=settings['SQS_REGION'],\n+            visibility_timeout=settings.getint('SQS_VISIBILITY_TIMEOUT'),\n+            aws_access_key_id=settings['AWS_ACCESS_KEY_ID'],\n+            aws_secret_access_key=settings['AWS_SECRET_ACCESS_KEY']\n+        )\n+        \n+    def _get_region(self, name):\n+        return [r for r in regions() if r.name == name][0]\n+\n+    @property\n+    def conn(self):\n+        return SQSConnection(self.aws_access_key_id, self.aws_secret_access_key, \\\n+            region=self.region)\n+\n+    @property\n+    def queue(self):\n+        return self.conn.get_queue(self.queue_name)\n+\n+    def _queue_method(self, method, *a, **kw):\n+        return getattr(self.queue, method)(*a, **kw)\n+\n+    def pop(self):\n+        return threads.deferToThread(self._pop)\n+\n+    def _pop(self):\n+        msgs = self.queue.get_messages(1, visibility_timeout=self.visibility_timeout)\n+        if msgs:\n+            msg = msgs[0]\n+            msg.delete()\n+            return json.loads(msg.get_body())\n+\n+    def add(self, name, **spider_args):\n+        d = spider_args.copy()\n+        d['name'] = name\n+        msg = Message(body=json.dumps(d))\n+        return threads.deferToThread(self._queue_method, 'write', msg)\n+\n+    def count(self):\n+        return threads.deferToThread(self._queue_method, 'count')\n+\n+    def list(self):\n+        return threads.deferToThread(self._list)\n+\n+    def _list(self):\n+        msgs = []\n+        m = self.queue.read(visibility_timeout=100)\n+        while m:\n+            msgs.append(json.loads(m.get_body()))\n+            m = self.queue.read(visibility_timeout=100)\n+        return msgs\n+\n+    def clear(self):\n+        return threads.deferToThread(self._queue_method, 'clear')\n\n@@ -3,6 +3,7 @@ import signal\n from twisted.internet import reactor, defer\n \n from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.queue import ExecutionQueue\n from scrapy.core.engine import ExecutionEngine\n from scrapy.extension import ExtensionManager\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n@@ -33,8 +34,10 @@ class Crawler(object):\n         self.extensions = ExtensionManager.from_settings(self.settings)\n         spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n         self.spiders = spman_cls.from_settings(self.settings)\n-        queue_cls = load_object(self.settings['QUEUE_CLASS'])\n-        self.queue = queue_cls(self.spiders)\n+        spq_cls = load_object(self.settings['SPIDER_QUEUE_CLASS'])\n+        spq = spq_cls.from_settings(self.settings)\n+        keepalive = self.settings.getbool('KEEP_ALIVE')\n+        self.queue = ExecutionQueue(self.spiders, spq, keepalive)\n         self.engine = ExecutionEngine(self.settings, self._spider_closed)\n \n     @defer.inlineCallbacks\n\n@@ -0,0 +1,35 @@\n+from zope.interface import Interface\n+\n+class ISpiderQueue(Interface):\n+\n+    def from_settings(settings):\n+        \"\"\"Class method to instantiate from settings\"\"\"\n+\n+    def add(name, **spider_args):\n+        \"\"\"Add a spider to the queue given its name a some spider arguments.\n+        \n+        This method can return a deferred. \"\"\"\n+\n+    def pop():\n+        \"\"\"Pop the next mesasge from the queue. The messages is a dict\n+        conaining a key 'name' with the spider name and other keys as spider\n+        attributes.\n+\n+        This method can return a deferred. \"\"\"\n+\n+    def list():\n+        \"\"\"Return a list with the messages in the queue. Each message is a dict\n+        which must have a 'name' key (with the spider name), and other optional\n+        keys that will be used as spider arguments, to create the spider.\n+        \n+        This method can return a deferred. \"\"\"\n+\n+    def count():\n+        \"\"\"Return the number of spiders in the queue.\n+        \n+        This method can return a deferred. \"\"\"\n+\n+    def clear():\n+        \"\"\"Clear the queue.\n+\n+        This method can return a deferred. \"\"\"\n\n@@ -9,16 +9,21 @@ class ExecutionQueue(object):\n \n     polling_delay = 5\n \n-    def __init__(self, _spiders):\n+    def __init__(self, spiders, queue, keepalive=False):\n         self.spider_requests = []\n-        self._spiders = _spiders\n+        self._spiders = spiders\n+        self._queue = queue\n+        self._keepalive = keepalive\n \n     def _append_next(self):\n         \"\"\"Called when there are no more items left in self.spider_requests.\n         This method is meant to be overriden in subclasses to add new (spider,\n         requests) tuples to self.spider_requests. It can return a Deferred.\n         \"\"\"\n-        pass\n+        msg = self._queue.pop()\n+        if msg:\n+            name = msg.pop('name')\n+            self.append_spider_name(name, **msg)\n \n     def get_next(self):\n         \"\"\"Return a tuple (spider, requests) containing a list of Requests and\n@@ -44,7 +49,7 @@ class ExecutionQueue(object):\n         spiders to crawl (this is for one-shot runs). If it returns ``False``\n         Scrapy will keep polling this queue for new requests to scrape\n         \"\"\"\n-        return not bool(self.spider_requests)\n+        return not self._keepalive and not bool(self.spider_requests)\n \n     def append_spider(self, spider):\n         \"\"\"Append a Spider to crawl\"\"\"\n@@ -82,9 +87,3 @@ class ExecutionQueue(object):\n             log.msg('Unable to find spider: %s' % name, log.ERROR)\n         else:\n             self.append_spider(spider)\n-\n-\n-class KeepAliveExecutionQueue(ExecutionQueue):\n-\n-    def is_finished(self):\n-        return False\n\n@@ -0,0 +1,35 @@\n+from zope.interface import implements\n+from zope.interface.verify import verifyObject\n+\n+from scrapy.interfaces import ISpiderQueue\n+from scrapy.utils.sqlite import JsonSqlitePriorityQueue\n+\n+\n+class SqliteSpiderQueue(object):\n+\n+    implements(ISpiderQueue)\n+\n+    def __init__(self, database=None, table='spider_queue'):\n+        self.q = JsonSqlitePriorityQueue(database, table)\n+\n+    @classmethod\n+    def from_settings(cls, settings):\n+        return cls(settings['SQLITE_DB'])\n+\n+    def add(self, name, **spider_args):\n+        d = spider_args.copy()\n+        d['name'] = name\n+        priority = float(d.pop('priority', 0))\n+        self.q.put(d, priority)\n+\n+    def pop(self):\n+        return self.q.pop()\n+\n+    def count(self):\n+        return len(self.q)\n+\n+    def list(self):\n+        return [x[0] for x in self.q]\n+\n+    def clear(self):\n+        self.q.clear()\n\n@@ -0,0 +1,21 @@\n+import os\n+\n+from twisted.trial import unittest\n+from zope.interface.verify import verifyObject\n+\n+from scrapy.interfaces import ISpiderQueue\n+\n+class SQSSpiderQueueTest(unittest.TestCase):\n+\n+    try:\n+        import boto\n+    except ImportError, e:\n+        skip = str(e)\n+    \n+    if 'AWS_ACCESS_KEY_ID' not in os.environ:\n+        skip = \"AWS keys not found\"\n+\n+    def test_interface(self):\n+        from scrapy.contrib.spiderqueue import SQSSpiderQueue\n+        verifyObject(ISpiderQueue, SQSSpiderQueue())\n+\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.queue import ExecutionQueue, KeepAliveExecutionQueue\n+from scrapy.queue import ExecutionQueue\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n \n@@ -27,10 +27,10 @@ class TestSpiderManager(object):\n \n class ExecutionQueueTest(unittest.TestCase):\n \n-    queue_class = ExecutionQueue\n+    keep_alive = False\n \n     def setUp(self):\n-        self.queue = self.queue_class(_spiders=TestSpiderManager())\n+        self.queue = ExecutionQueue(TestSpiderManager(), None, self.keep_alive)\n         self.spider = TestSpider()\n         self.request = Request('about:none')\n \n@@ -106,7 +106,7 @@ class ExecutionQueueTest(unittest.TestCase):\n \n class KeepAliveExecutionQueueTest(ExecutionQueueTest):\n \n-    queue_class = KeepAliveExecutionQueue\n+    keep_alive = True\n \n     def test_is_finished(self):\n         self.assert_(not self.queue.is_finished())\n\n@@ -0,0 +1,12 @@\n+import unittest\n+\n+from zope.interface.verify import verifyObject\n+\n+from scrapy.interfaces import ISpiderQueue\n+from scrapy.spiderqueue import SqliteSpiderQueue\n+\n+class SpiderQueueTest(unittest.TestCase):\n+\n+    def test_interface(self):\n+        verifyObject(ISpiderQueue, SqliteSpiderQueue())\n+\n\n@@ -8,10 +8,10 @@ from scrapy.utils.py26 import json\n class SqliteDict(DictMixin):\n     \"\"\"SQLite-backed dictionary\"\"\"\n \n-    def __init__(self, database=':memory:', table=\"dict\"):\n-        self.database = database\n+    def __init__(self, database=None, table=\"dict\"):\n+        self.database = database or ':memory:'\n         self.table = table\n-        self.conn = sqlite3.connect(database)\n+        self.conn = sqlite3.connect(self.database)\n         q = \"create table if not exists %s (key text primary key, value blob)\" \\\n             % table\n         self.conn.execute(q)\n@@ -86,10 +86,10 @@ class SqlitePriorityQueue(object):\n     providing atomic inter-process operations.\n     \"\"\"\n \n-    def __init__(self, database=':memory:', table=\"queue\"):\n-        self.database = database\n+    def __init__(self, database=None, table=\"queue\"):\n+        self.database = database or ':memory:'\n         self.table = table\n-        self.conn = sqlite3.connect(database)\n+        self.conn = sqlite3.connect(self.database)\n         q = \"create table if not exists %s (id integer primary key, \" \\\n             \"priority real key, message blob)\" % table\n         self.conn.execute(q)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b766877f10c36f2821d00007a991416035e190c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 36 | Files Changed: 9 | Hunks: 23 | Methods Changed: 16 | Complexity Δ (Sum/Max): 5/6 | Churn Δ: 98 | Churn Cumulative: 999 | Contributors (this commit): 4 | Commits (past 90d): 38 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@ from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n from scrapy.item import BaseItem\n from scrapy.utils import display\n-from scrapy.utils.spider import iterate_spider_output\n+from scrapy.utils.spider import iterate_spider_output, create_spider_for_request\n from scrapy.utils.url import is_url\n from scrapy.exceptions import UsageError\n from scrapy import log\n@@ -73,7 +73,7 @@ class Command(ScrapyCommand):\n             except KeyError:\n                 log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n         else:\n-            spider = self.crawler.spiders.create_for_request(request)\n+            spider = create_spider_for_request(self.crawler.spiders, request)\n             if spider:\n                 return spider\n             log.msg('Unable to find spider for: %s' % request, log.ERROR)\n\n@@ -33,3 +33,20 @@ class ISpiderQueue(Interface):\n         \"\"\"Clear the queue.\n \n         This method can return a deferred. \"\"\"\n+\n+class ISpiderManager(Interface):\n+\n+    def from_settings(settings):\n+        \"\"\"Class method to instantiate from settings\"\"\"\n+\n+    def create(spider_name, **spider_args):\n+        \"\"\"Returns a new Spider instance for the given spider name, and using\n+        the given spider arguments. If the spider name is not found, it must\n+        raise a KeyError.\"\"\"\n+\n+    def list():\n+        \"\"\"Return a list with the names of all spiders available in the\n+        project\"\"\"\n+\n+    def find_by_request(request):\n+        \"\"\"Returns the list of spiders names that can handle the given request\"\"\"\n\n@@ -2,6 +2,7 @@ from twisted.internet import defer\n \n from scrapy.http import Request\n from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.spider import create_spider_for_request\n from scrapy import log\n \n \n@@ -58,7 +59,7 @@ class ExecutionQueue(object):\n \n     def append_request(self, request, spider=None, **kwargs):\n         if spider is None:\n-            spider = self._spiders.create_for_request(request, **kwargs)\n+            spider = create_spider_for_request(self._spiders, request, **kwargs)\n         if spider:\n             self.spider_requests.append((spider, [request]))\n \n@@ -69,7 +70,8 @@ class ExecutionQueue(object):\n         if url is None:\n             raise ValueError(\"A url is required\")\n         if spider is None:\n-            spider = self._spiders.create_for_request(Request(url), **kwargs)\n+            spider = create_spider_for_request(self._spiders, Request(url), \\\n+                **kwargs)\n         if spider:\n             requests = arg_to_iter(spider.make_requests_from_url(url))\n             self.spider_requests.append((spider, requests))\n\n@@ -13,6 +13,7 @@ from scrapy import log\n from scrapy.item import BaseItem\n from scrapy.spider import BaseSpider\n from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n+from scrapy.utils.spider import create_spider_for_request\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n from scrapy.utils.url import any_to_uri\n@@ -58,7 +59,7 @@ class Shell(object):\n             url = any_to_uri(request_or_url)\n             request = Request(url, dont_filter=True)\n         if spider is None:\n-            spider = self.crawler.spiders.create_for_request(request, \\\n+            spider = create_spider_for_request(self.crawler.spiders, request, \\\n                 BaseSpider('default'), log_multiple=True)\n         self.crawler.engine.open_spider(spider)\n         response = None\n\n@@ -3,7 +3,10 @@ SpiderManager is the class which locates and manages all website-specific\n spiders\n \"\"\"\n \n+from zope.interface import implements\n+\n from scrapy import log, signals\n+from scrapy.interfaces import ISpiderManager\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.spider import iter_spider_classes\n from scrapy.xlib.pydispatch import dispatcher\n@@ -11,6 +14,8 @@ from scrapy.xlib.pydispatch import dispatcher\n \n class SpiderManager(object):\n \n+    implements(ISpiderManager)\n+\n     def __init__(self, spider_modules):\n         self.spider_modules = spider_modules\n         self._spiders = {}\n@@ -28,41 +33,13 @@ class SpiderManager(object):\n         return cls(settings.getlist('SPIDER_MODULES'))\n \n     def create(self, spider_name, **spider_kwargs):\n-        \"\"\"Returns a Spider instance for the given spider name, using the given\n-        spider arguments. If the spider name is not found, it raises a\n-        KeyError.\n-        \"\"\"\n         return self._spiders[spider_name](**spider_kwargs)\n \n     def find_by_request(self, request):\n-        \"\"\"Returns list of spiders names that match the given Request\"\"\"\n         return [name for name, cls in self._spiders.iteritems()\n             if cls.handles_request(request)]\n \n-    def create_for_request(self, request, default_spider=None, \\\n-            log_none=False, log_multiple=False, **spider_kwargs):\n-        \"\"\"Create a spider to handle the given Request.\n-\n-        This will look for the spiders that can handle the given request (using\n-        find_by_request) and return a (new) Spider if (and only if) there is\n-        only one Spider able to handle the Request.\n-\n-        If multiple spiders (or no spider) are found, it will return the\n-        default_spider passed. It can optionally log if multiple or no spiders\n-        are found.\n-        \"\"\"\n-        snames = self.find_by_request(request)\n-        if len(snames) == 1:\n-            return self.create(snames[0], **spider_kwargs)\n-        if len(snames) > 1 and log_multiple:\n-            log.msg('More than one spider can handle: %s - %s' % \\\n-                (request, \", \".join(snames)), log.ERROR)\n-        if len(snames) == 0 and log_none:\n-            log.msg('Unable to find spider that handles: %s' % request, log.ERROR)\n-        return default_spider\n-\n     def list(self):\n-        \"\"\"Returns list of spiders available.\"\"\"\n         return self._spiders.keys()\n \n     def close_spider(self, spider, reason):\n\n@@ -1,5 +1,4 @@\n from zope.interface import implements\n-from zope.interface.verify import verifyObject\n \n from scrapy.interfaces import ISpiderQueue\n from scrapy.utils.sqlite import JsonSqlitePriorityQueue\n\n@@ -18,8 +18,8 @@ class TestSpider(BaseSpider):\n \n class TestSpiderManager(object):\n \n-    def create_for_request(self, request, **kwargs):\n-        return TestSpider('create_for_request', **kwargs)\n+    def find_by_request(self, request):\n+        return ['create_for_request']\n \n     def create(self, spider_name, **spider_kwargs):\n         return TestSpider(spider_name, **spider_kwargs)\n\n@@ -3,11 +3,14 @@ import os\n import weakref\n import shutil\n \n+from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n \n+\n # ugly hack to avoid cyclic imports of scrapy.spider when running this test\n # alone\n import scrapy.spider\n+from scrapy.interfaces import ISpiderManager\n from scrapy.spidermanager import SpiderManager\n from scrapy.http import Request\n \n@@ -28,6 +31,9 @@ class SpiderManagerTest(unittest.TestCase):\n         del self.spiderman\n         sys.path.remove(self.tmpdir)\n \n+    def test_interface(self):\n+        verifyObject(ISpiderManager, self.spiderman)\n+\n     def test_list(self):\n         self.assertEqual(set(self.spiderman.list()),\n             set(['spider1', 'spider2', 'spider3']))\n\n@@ -1,5 +1,6 @@\n import inspect\n \n+from scrapy import log\n from scrapy.item import BaseItem\n from scrapy.utils.misc import  arg_to_iter\n \n@@ -21,3 +22,26 @@ def iter_spider_classes(module):\n            obj.__module__ == module.__name__ and \\\n            getattr(obj, 'name', None):\n             yield obj\n+\n+def create_spider_for_request(spidermanager, request, default_spider=None, \\\n+        log_none=False, log_multiple=False, **spider_kwargs):\n+    \"\"\"Create a spider to handle the given Request.\n+\n+    This will look for the spiders that can handle the given request (using\n+    the spider manager) and return a (new) Spider if (and only if) there is\n+    only one Spider able to handle the Request.\n+\n+    If multiple spiders (or no spider) are found, it will return the\n+    default_spider passed. It can optionally log if multiple or no spiders\n+    are found.\n+    \"\"\"\n+    snames = spidermanager.find_by_request(request)\n+    if len(snames) == 1:\n+        return spidermanager.create(snames[0], **spider_kwargs)\n+    if len(snames) > 1 and log_multiple:\n+        log.msg('More than one spider can handle: %s - %s' % \\\n+            (request, \", \".join(snames)), log.ERROR)\n+    if len(snames) == 0 and log_none:\n+        log.msg('Unable to find spider that handles: %s' % request, log.ERROR)\n+    return default_spider\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
