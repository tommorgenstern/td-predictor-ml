{"custom_id": "keras#1eb0c2387caf9fbec8f5685c6937117422d681f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 459 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -276,9 +276,10 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         # The tensor shapes are different and output_sequence_length is None\n         # should work fine and pad each sequence to the length of the longest sequence\n         # in it's batch\n+        max_sequence_length = 30\n         dataset = audio_dataset.audio_dataset_from_directory(directory, batch_size=2)\n-        sequence_lengths = set([batch[0].shape[1] for batch in dataset])\n-        assert len(sequence_lengths) > 1\n+        sequence_lengths = list(set([batch[0].shape[1] for batch in dataset]))\n+        self.assertAllClose(sequence_lengths, [i for i in range(10, max_sequence_length+1)])\n \n     def test_audio_dataset_from_directory_no_output_length_and_same_lengths(self):\n         directory = self._prepare_directory(\n@@ -288,8 +289,8 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         # should work fine and pad each sequence to the length of the longest sequence\n         # in it's batch\n         dataset = audio_dataset.audio_dataset_from_directory(directory, batch_size=2)\n-        sequence_lengths = set([batch[0].shape[1] for batch in dataset])\n-        assert len(sequence_lengths) == 1\n+        sequence_lengths = list(set([batch[0].shape[1] for batch in dataset]))\n+        self.assertEqual(len(sequence_lengths), 1)\n \n     def test_audio_dataset_from_directory_errors(self):\n         directory = self._prepare_directory(num_classes=3, count=5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a749593bbebbb3432fb194faea98e90a5240dd0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 473 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -77,7 +77,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n             path = paths[i % len(paths)]\n             ext = \".wav\"\n             filename = os.path.join(path, \"audio_%s.%s\" % (i, ext))\n-            with open(os.path.join(temp_dir, filename), 'wb') as f:\n+            with open(os.path.join(temp_dir, filename), \"wb\") as f:\n                 f.write(audio.numpy())\n             i += 1\n         return temp_dir\n@@ -89,7 +89,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         directory = self._prepare_directory(count=7, num_classes=2)\n         for i, audio in enumerate(self._get_audio_samples(3)):\n             filename = \"audio_%s.wav\" % (i,)\n-            with open(os.path.join(directory, filename), 'wb') as f:\n+            with open(os.path.join(directory, filename), \"wb\") as f:\n                 f.write(audio.numpy())\n \n         dataset = audio_dataset.audio_dataset_from_directory(\n@@ -269,7 +269,9 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         batch = next(iter(dataset))\n         self.assertEqual(batch[0].shape, (32, None, None))\n \n-    def test_audio_dataset_from_directory_no_output_length_and_different_lengths(self):\n+    def test_audio_dataset_from_directory_no_output_length_no_ragged(self):\n+        # This test case tests `audio_dataset_from_directory` when `ragged` and `output_sequence_length`\n+        # are not passed while the input sequence lengths are different.\n         directory = self._prepare_directory(\n             num_classes=2, count=16, different_sequence_lengths=True\n         )\n@@ -279,9 +281,13 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         max_sequence_length = 30\n         dataset = audio_dataset.audio_dataset_from_directory(directory, batch_size=2)\n         sequence_lengths = list(set([batch[0].shape[1] for batch in dataset]))\n-        self.assertAllClose(sequence_lengths, [i for i in range(10, max_sequence_length+1)])\n+        self.assertAllClose(\n+            sequence_lengths, [i for i in range(10, max_sequence_length + 1)]\n+        )\n \n     def test_audio_dataset_from_directory_no_output_length_and_same_lengths(self):\n+        # This test case tests `audio_dataset_from_directory` when `ragged` and `output_sequence_length`\n+        # are not passed while the input sequence lengths are the same\n         directory = self._prepare_directory(\n             num_classes=2, count=16, different_sequence_lengths=False\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef0e6c5b3af44d0da4a3bc347b2e0cf76809dd07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 9 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 25 | Churn Cumulative: 498 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -75,7 +75,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n             count=count, different_sequence_lengths=different_sequence_lengths\n         ):\n             path = paths[i % len(paths)]\n-            ext = \".wav\"\n+            ext = \"wav\"\n             filename = os.path.join(path, \"audio_%s.%s\" % (i, ext))\n             with open(os.path.join(temp_dir, filename), \"wb\") as f:\n                 f.write(audio.numpy())\n@@ -150,7 +150,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         @tf.function\n         def symbolic_fn(ds):\n             for x, _ in ds.take(1):\n-                test_case.assertListEqual(x.shape.as_list(), [None, 30, 1])\n+                test_case.assertListEqual(x.shape.as_list(), [None, 30, None])\n \n         symbolic_fn(dataset)\n \n@@ -256,7 +256,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n \n     def test_audio_dataset_from_directory_no_audio(self):\n         directory = self._prepare_directory(num_classes=2, count=0)\n-        with self.assertRaisesRegex(ValueError, \"No audio found.\"):\n+        with self.assertRaisesRegex(ValueError, \"No audio files found in directory\"):\n             _ = audio_dataset.audio_dataset_from_directory(directory)\n \n     def test_audio_dataset_from_directory_ragged(self):\n@@ -267,9 +267,10 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n             directory, ragged=True, batch_size=8\n         )\n         batch = next(iter(dataset))\n-        self.assertEqual(batch[0].shape, (32, None, None))\n \n-    def test_audio_dataset_from_directory_no_output_length_no_ragged(self):\n+        self.assertEqual(batch[0].shape.as_list(), [8, None, None])\n+\n+    def test_audio_dataset_from_directory_no_output_sequence_length_no_ragged(self):\n         # This test case tests `audio_dataset_from_directory` when `ragged` and `output_sequence_length`\n         # are not passed while the input sequence lengths are different.\n         directory = self._prepare_directory(\n@@ -278,14 +279,14 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         # The tensor shapes are different and output_sequence_length is None\n         # should work fine and pad each sequence to the length of the longest sequence\n         # in it's batch\n-        max_sequence_length = 30\n+        min_sequence_length, max_sequence_length = 10, 30\n+        possible_sequence_lengths = [i for i in range(min_sequence_length, max_sequence_length+1)]\n         dataset = audio_dataset.audio_dataset_from_directory(directory, batch_size=2)\n-        sequence_lengths = list(set([batch[0].shape[1] for batch in dataset]))\n-        self.assertAllClose(\n-            sequence_lengths, [i for i in range(10, max_sequence_length + 1)]\n-        )\n+        sequence_lengths = list(set([b.shape[1] for b, _ in dataset]))\n+        for seq_len in sequence_lengths:\n+            self.assertIn(seq_len, possible_sequence_lengths)\n \n-    def test_audio_dataset_from_directory_no_output_length_and_same_lengths(self):\n+    def test_audio_dataset_from_directory_no_output_sequence_length_same_lengths(self):\n         # This test case tests `audio_dataset_from_directory` when `ragged` and `output_sequence_length`\n         # are not passed while the input sequence lengths are the same\n         directory = self._prepare_directory(\n@@ -390,7 +391,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n             shuffle=False,\n         )\n         sample = next(iter(dataset))\n-        self.assertEqual(len(sample.shape), 3)\n+        self.assertEqual(len(sample.shape), 2)\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e83105b6cc2ef25959c669b1a220e45bfab75166", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 502 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -280,7 +280,9 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         # should work fine and pad each sequence to the length of the longest sequence\n         # in it's batch\n         min_sequence_length, max_sequence_length = 10, 30\n-        possible_sequence_lengths = [i for i in range(min_sequence_length, max_sequence_length+1)]\n+        possible_sequence_lengths = [\n+            i for i in range(min_sequence_length, max_sequence_length + 1)\n+        ]\n         dataset = audio_dataset.audio_dataset_from_directory(directory, batch_size=2)\n         sequence_lengths = list(set([b.shape[1] for b, _ in dataset]))\n         for seq_len in sequence_lengths:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aadc0b970e096113468412529233bddcbcfc64cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 134 | Contributors (this commit): 13 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -46,6 +46,9 @@ from keras.utils.generic_utils import deserialize_keras_object\n from keras.utils.generic_utils import get_custom_objects\n from keras.utils.generic_utils import serialize_keras_object\n \n+# Audio related\n+from keras.utils.audio_dataset import audio_dataset_from_directory\n+\n # Internal\n from keras.utils.layer_utils import get_source_inputs\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1bd00f7e1ab375e5c382b91024997800b7856d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 135 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -51,4 +51,3 @@ from keras.utils.audio_dataset import audio_dataset_from_directory\n \n # Internal\n from keras.utils.layer_utils import get_source_inputs\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c15c21ebffbde4737292c51e621dc566098c6e42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 16 | Churn Cumulative: 1451 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -668,11 +668,13 @@ class TruePositives(_ConfusionMatrixConditionCount):\n   Use `sample_weight` of 0 to mask values.\n \n   Args:\n-    thresholds: (Optional) Defaults to 0.5. A float value or a python\n+    thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n       list/tuple of float threshold values in [0, 1]. A threshold is compared\n       with prediction values to determine the truth value of predictions\n-      (i.e., above the threshold is `true`, below is `false`). One metric\n-      value is generated for each threshold value.\n+      (i.e., above the threshold is `true`, below is `false`). If used with a\n+      loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n+      predictions), `thresholds` should be set to 0. One metric value is\n+      generated for each threshold value.\n     name: (Optional) string name of the metric instance.\n     dtype: (Optional) data type of the metric result.\n \n@@ -695,6 +697,14 @@ class TruePositives(_ConfusionMatrixConditionCount):\n                 loss='mse',\n                 metrics=[tf.keras.metrics.TruePositives()])\n   ```\n+\n+  Usage with a loss with `from_logits=True`:\n+\n+  ```python\n+  model.compile(optimizer='adam',\n+                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                metrics=[tf.keras.metrics.TruePositives(thresholds=0)])\n+  ```\n   \"\"\"\n \n   @dtensor_utils.inject_mesh\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b990ae7e762b83a275b588db69939f3d63768b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2344 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -184,7 +184,7 @@ class StochasticDepth(layers.Layer):\n     return config\n \n \n-class Block(Model):\n+class ConvNeXtBlock(Model):\n   \"\"\"ConvNeXt block.\n   \n   References:\n@@ -428,7 +428,7 @@ def ConvNeXt(depths,\n     stage = tf.keras.Sequential(\n       [\n         *[\n-            Block(\n+            ConvNeXtBlock(\n               projection_dim=projection_dims[i],\n               drop_path=dp_rates[cur + j],\n               layer_scale_init_value=layer_scale_init_value,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fad35e3a83d80fd79389cb2743590f6645997c00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 15 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 84 | Churn Cumulative: 1535 | Contributors (this commit): 5 | Commits (past 90d): 21 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -518,11 +518,13 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n   Use `sample_weight` of 0 to mask values.\n \n   Args:\n-    thresholds: (Optional) Defaults to 0.5. A float value or a python\n+    thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n       list/tuple of float threshold values in [0, 1]. A threshold is compared\n       with prediction values to determine the truth value of predictions\n-      (i.e., above the threshold is `true`, below is `false`). One metric\n-      value is generated for each threshold value.\n+      (i.e., above the threshold is `true`, below is `false`). If used with a\n+      loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n+      predictions), `thresholds` should be set to 0. One metric value is\n+      generated for each threshold value.\n     name: (Optional) string name of the metric instance.\n     dtype: (Optional) data type of the metric result.\n \n@@ -545,6 +547,14 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n                 loss='mse',\n                 metrics=[tf.keras.metrics.FalsePositives()])\n   ```\n+\n+  Usage with a loss with `from_logits=True`:\n+\n+  ```python\n+  model.compile(optimizer='adam',\n+                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                metrics=[tf.keras.metrics.FalsePositives(thresholds=0)])\n+  ```\n   \"\"\"\n \n   @dtensor_utils.inject_mesh\n@@ -568,11 +578,13 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n   Use `sample_weight` of 0 to mask values.\n \n   Args:\n-    thresholds: (Optional) Defaults to 0.5. A float value or a python\n+    thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n       list/tuple of float threshold values in [0, 1]. A threshold is compared\n       with prediction values to determine the truth value of predictions\n-      (i.e., above the threshold is `true`, below is `false`). One metric\n-      value is generated for each threshold value.\n+      (i.e., above the threshold is `true`, below is `false`). If used with a\n+      loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n+      predictions), `thresholds` should be set to 0. One metric value is\n+      generated for each threshold value.\n     name: (Optional) string name of the metric instance.\n     dtype: (Optional) data type of the metric result.\n \n@@ -595,6 +607,14 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n                 loss='mse',\n                 metrics=[tf.keras.metrics.FalseNegatives()])\n   ```\n+\n+  Usage with a loss with `from_logits=True`:\n+\n+  ```python\n+  model.compile(optimizer='adam',\n+                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                metrics=[tf.keras.metrics.FalseNegatives(thresholds=0)])\n+  ```\n   \"\"\"\n \n   @dtensor_utils.inject_mesh\n@@ -618,11 +638,13 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n   Use `sample_weight` of 0 to mask values.\n \n   Args:\n-    thresholds: (Optional) Defaults to 0.5. A float value or a python\n+    thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n       list/tuple of float threshold values in [0, 1]. A threshold is compared\n       with prediction values to determine the truth value of predictions\n-      (i.e., above the threshold is `true`, below is `false`). One metric\n-      value is generated for each threshold value.\n+      (i.e., above the threshold is `true`, below is `false`). If used with a\n+      loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n+      predictions), `thresholds` should be set to 0. One metric value is\n+      generated for each threshold value.\n     name: (Optional) string name of the metric instance.\n     dtype: (Optional) data type of the metric result.\n \n@@ -645,6 +667,14 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n                 loss='mse',\n                 metrics=[tf.keras.metrics.TrueNegatives()])\n   ```\n+\n+  Usage with a loss with `from_logits=True`:\n+\n+  ```python\n+  model.compile(optimizer='adam',\n+                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                metrics=[tf.keras.metrics.TrueNegatives(thresholds=0)])\n+  ```\n   \"\"\"\n \n   @dtensor_utils.inject_mesh\n@@ -738,12 +768,14 @@ class Precision(base_metric.Metric):\n   `class_id` is indeed a correct label.\n \n   Args:\n-    thresholds: (Optional) A float value or a python list/tuple of float\n+    thresholds: (Optional) A float value, or a Python list/tuple of float\n       threshold values in [0, 1]. A threshold is compared with prediction\n       values to determine the truth value of predictions (i.e., above the\n-      threshold is `true`, below is `false`). One metric value is generated\n-      for each threshold value. If neither thresholds nor top_k are set, the\n-      default is to calculate precision with `thresholds=0.5`.\n+      threshold is `true`, below is `false`). If used with a loss function that\n+      sets `from_logits=True` (i.e. no sigmoid applied to predictions),\n+      `thresholds` should be set to 0. One metric value is generated for each\n+      threshold value. If neither thresholds nor top_k are set, the default is\n+      to calculate precision with `thresholds=0.5`.\n     top_k: (Optional) Unset by default. An int value specifying the top-k\n       predictions to consider when calculating precision.\n     class_id: (Optional) Integer class ID for which we want binary metrics.\n@@ -783,6 +815,14 @@ class Precision(base_metric.Metric):\n                 loss='mse',\n                 metrics=[tf.keras.metrics.Precision()])\n   ```\n+\n+  Usage with a loss with `from_logits=True`:\n+\n+  ```python\n+  model.compile(optimizer='adam',\n+                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                metrics=[tf.keras.metrics.Precision(thresholds=0)])\n+  ```\n   \"\"\"\n \n   @dtensor_utils.inject_mesh\n@@ -881,12 +921,14 @@ class Recall(base_metric.Metric):\n   top-k predictions.\n \n   Args:\n-    thresholds: (Optional) A float value or a python list/tuple of float\n+    thresholds: (Optional) A float value, or a Python list/tuple of float\n       threshold values in [0, 1]. A threshold is compared with prediction\n       values to determine the truth value of predictions (i.e., above the\n-      threshold is `true`, below is `false`). One metric value is generated\n-      for each threshold value. If neither thresholds nor top_k are set, the\n-      default is to calculate recall with `thresholds=0.5`.\n+      threshold is `true`, below is `false`). If used with a loss function that\n+      sets `from_logits=True` (i.e. no sigmoid applied to predictions),\n+      `thresholds` should be set to 0. One metric value is generated for each\n+      threshold value. If neither thresholds nor top_k are set, the default is\n+      to calculate recall with `thresholds=0.5`.\n     top_k: (Optional) Unset by default. An int value specifying the top-k\n       predictions to consider when calculating recall.\n     class_id: (Optional) Integer class ID for which we want binary metrics.\n@@ -914,6 +956,14 @@ class Recall(base_metric.Metric):\n                 loss='mse',\n                 metrics=[tf.keras.metrics.Recall()])\n   ```\n+\n+  Usage with a loss with `from_logits=True`:\n+\n+  ```python\n+  model.compile(optimizer='adam',\n+                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                metrics=[tf.keras.metrics.Recall(thresholds=0)])\n+  ```\n   \"\"\"\n \n   @dtensor_utils.inject_mesh\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c1b1e46d5c688272a63322feb3166469d7d40bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2346 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -102,7 +102,7 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   pre-trained parameters of the models were assembled from the\n   [official repository](https://github.com/facebookresearch/ConvNeXt). To get a\n   sense of how these parameters were converted to Keras compatible parameters,\n-  please refer to [this repository](https://github.com/sayakpaul/ConvNeXt-TF).\n+  please refer to [this repository](https://github.com/sayakpaul/keras-convnext-conversion).\n \n   Note: Each Keras Application expects a specific kind of input preprocessing.\n   For ConvNeXt, preprocessing is included in the model using a `Normalization` layer.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56b75d030cd9c3e3fde0ddf1f908436e6a5be3d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 142 | Files Changed: 2 | Hunks: 50 | Methods Changed: 16 | Complexity Δ (Sum/Max): 76/77 | Churn Δ: 223 | Churn Cumulative: 1524 | Contributors (this commit): 3 | Commits (past 90d): 21 | Contributors (cumulative): 4 | DMM Complexity: 0.08\n\nDIFF:\n@@ -89,12 +89,10 @@ def split_dataset(dataset,\n \n   try:\n     left_split = tf.data.Dataset.from_tensor_slices(left_split)\n-  except Exception as e:\n-    raise ValueError(f' with `left_size`={left_size} and '\n-                     f' `right_size`={right_size}'\n-                     f' unable to create the dataset from '\n-                     f' left_split of shape {np.array(left_split).shape}. '\n-                     f' Received error: {e}')\n+  except ValueError as e:\n+    raise ValueError(f'With left_size={left_size} and right_size={right_size}.'\n+                     f' unable to create the dataset from left_split of shape '\n+                     f'{np.array(left_split).shape}. \\nReceived error: {e}')\n \n   try:\n     right_split = tf.data.Dataset.from_tensor_slices(right_split)\n@@ -115,7 +113,6 @@ def _convert_dataset_to_list(dataset,\n                              ensure_shape_similarity = True):\n   \"\"\"Helper function to convert a tf.data.Dataset  object or a list/tuple of numpy.ndarrays to a list\n   \"\"\"\n-  # TODO (prakashsellathurai): fix issue with tuple of numpy arrays dataset=(np.ones(shape=(3, 2)), np.zeros(shape=(3,)))\n   # TODO (prakashsellathurai): add support for Batched  and unbatched dict tf datasets\n   dataset_type_spec = _get_type_spec(dataset)\n   dataset_iterator = _get_data_iterator_from_dataset(dataset)\n@@ -127,10 +124,10 @@ def _convert_dataset_to_list(dataset,\n                                  data_size_warning_flag,\n                                  start_time):\n \n-    if dataset_type_spec is None:\n+    if dataset_type_spec is tuple:\n+      dataset_as_list.append(np.array(sample) )\n+    else:\n       dataset_as_list.append(sample)\n-    elif dataset_type_spec is tuple:\n-      dataset_as_list.append(map(tuple,map(np.array,sample)))\n \n \n   return dataset_as_list\n\n@@ -1,42 +1,44 @@\n-\"\"\"Tests for dataset_utils.\"\"\"\n+\"\"\"Tests for Dataset Utils\"\"\"\n \n import tensorflow.compat.v2 as tf\n+# pylint: disable=g-classes-have-attributes\n \n import numpy as np\n-\n from keras.utils import dataset_utils\n-from keras.datasets import mnist\n-\n \n class SplitDatasetTest(tf.test.TestCase):\n-  \n   def test_numpy_array(self):\n     dataset=np.ones(shape=(200, 32))\n     res = dataset_utils.split_dataset(dataset, left_size=0.8,right_size=0.2)\n+\n     self.assertLen(res, 2)\n     left_split,right_split = res\n+\n     self.assertIsInstance(left_split, tf.data.Dataset)\n     self.assertIsInstance(right_split, tf.data.Dataset)\n \n     self.assertLen(left_split, 160)\n     self.assertLen(right_split, 40)\n+\n     self.assertAllEqual(dataset[:160] ,list(left_split))\n     self.assertAllEqual(dataset[-40:] ,list(right_split))\n \n-    \n   def test_list_of_numpy_arrays(self):\n     dataset=[np.ones(shape=(200, 32)), np.zeros(shape=(200, 32))]\n     res = dataset_utils.split_dataset(dataset, left_size=4)\n+\n     self.assertLen(res, 2)\n     left_split,right_split = res\n+\n     self.assertIsInstance(left_split, tf.data.Dataset)\n     self.assertIsInstance(right_split, tf.data.Dataset)\n+\n     self.assertLen(left_split, 4)\n     self.assertLen(right_split, 196)\n+\n     self.assertAllEqual(list(zip(*dataset))[:4] ,list(left_split))\n     self.assertAllEqual(list(zip(*dataset))[4:] ,list(right_split))\n-    self.assertAllEqual(list(left_split)+list(right_split), \n-                        list(zip(*dataset)))\n+    self.assertAllEqual(list(left_split)+list(right_split),list(zip(*dataset)))\n \n     dataset=[np.ones(shape=(200, 32))]\n     left_split,right_split = dataset_utils.split_dataset(dataset,\n@@ -44,7 +46,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertAllEqual(list(zip(*dataset)),\n                         list(left_split)+list(right_split))\n \n-  def test_dataset_with_irregular_shape(self):\n+  def test_dataset_with_invalid_shape(self):\n     with self.assertRaises(ValueError):\n       dataset=[np.ones(shape=(200, 32)), np.zeros(shape=(100, 32))]\n       dataset_utils.split_dataset(dataset, left_size=4)\n@@ -63,43 +65,53 @@ class SplitDatasetTest(tf.test.TestCase):\n         (np.ones(shape=(200, 32,32,1)), np.zeros(shape=(32))))\n       dataset_utils.split_dataset(dataset, left_size=4)\n \n-      \n-    \n   def test_tuple_of_numpy_arrays(self):\n-    dataset=(np.ones(shape=(200, 32)), np.zeros(shape=(200, 32)))\n+    dataset=(np.random.rand(4, 3), np.random.rand(4, 3))\n+    left_split,right_split = dataset_utils.split_dataset(dataset, left_size=2)\n \n-    left_split,right_split = dataset_utils.split_dataset(dataset, \n-                                                             left_size=80)\n+    self.assertIsInstance(left_split, tf.data.Dataset)\n+    self.assertIsInstance(right_split, tf.data.Dataset)\n \n-    self.assertEqual(len(left_split), 80)\n-    self.assertAllEqual(list(zip(*dataset))[:80] ,list(left_split))\n-    self.assertAllEqual(list(zip(*dataset))[80:] ,list(right_split))\n-\n-    \n-    \n-    dataset=(np.random.rand(4, 3), np.random.rand(4, 1))\n-    \n-    left_split,right_split = dataset_utils.split_dataset(dataset, \n-                                                             left_size=2)\n     self.assertEqual(len(left_split), 2)\n     self.assertEqual(len(right_split), 2)\n \n-    self.assertEqual(np.array(list(left_split)[0])[0].shape,(3,) )\n-    self.assertEqual(np.array(list(left_split)[0])[1].shape,(1,) )\n-    self.assertEqual(np.array(list(right_split)[0])[0].shape,(3,) )\n-    self.assertEqual(np.array(list(right_split)[0])[1].shape,(1,) )\n+    self.assertEqual(np.array(list(left_split)[0]).shape, (2, 3))\n+    self.assertEqual(np.array(list(left_split)[1]).shape, (2, 3))\n \n-    dataset=(np.random.rand(200,32,32), np.random.rand(200,10))\n+    # test with fractional size\n+    dataset = (np.random.rand(5, 32,32), np.random.rand(5, 32,32))\n     left_split,right_split = dataset_utils.split_dataset(dataset,\n-                                                          right_size=180)\n-    self.assertEqual(len(right_split), 180)\n-    self.assertEqual(len(left_split), 20)\n+                                                         right_size=0.4)\n+    self.assertIsInstance(left_split, tf.data.Dataset)\n+    self.assertIsInstance(right_split, tf.data.Dataset)\n \n-    self.assertEqual(np.array(list(left_split)[0])[0].shape,(32,32) )\n-    self.assertEqual(np.array(list(left_split)[0])[1].shape,(10,) )\n-    self.assertEqual(np.array(list(right_split)[0])[0].shape,(32,32) )\n-    self.assertEqual(np.array(list(right_split)[0])[1].shape,(10,) )\n+    self.assertEqual(np.array(list(left_split)).shape,(3,2,32,32))\n+    self.assertEqual(np.array(list(right_split)).shape,(2,2,32,32))\n \n+    self.assertEqual(np.array(list(left_split))[0].shape,(2,32,32))\n+    self.assertEqual(np.array(list(left_split))[1].shape,(2,32,32))\n+\n+    self.assertEqual(np.array(list(right_split))[0].shape,(2,32,32))\n+    self.assertEqual(np.array(list(right_split))[1].shape,(2,32,32))\n+\n+    # test with tuple of np arrays with different shapes\n+    dataset = (np.random.rand(5, 32,32), np.random.rand(5, ))\n+    left_split,right_split = dataset_utils.split_dataset(dataset,\n+                                                         left_size=2,\n+                                                         right_size=3)\n+    self.assertIsInstance(left_split, tf.data.Dataset)\n+    self.assertIsInstance(right_split, tf.data.Dataset)\n+\n+    self.assertEqual(np.array(list(left_split)).shape,(2,2))\n+    self.assertEqual(np.array(list(right_split)).shape,(3,2))\n+\n+    self.assertEqual(np.array(list(left_split)[0]).shape,(2,))\n+    self.assertEqual(np.array(list(left_split)[0][0]).shape,(32,32))\n+    self.assertEqual(np.array(list(left_split)[0][1]).shape,())\n+\n+    self.assertEqual(np.array(list(right_split)[0]).shape,(2,))\n+    self.assertEqual(np.array(list(right_split)[0][0]).shape,(32,32))\n+    self.assertEqual(np.array(list(right_split)[0][1]).shape,())\n \n   def test_batched_tf_dataset_of_vectors(self):\n     dataset = tf.data.Dataset.from_tensor_slices(np.ones(shape=(100,32, 32,1)))\n@@ -108,8 +120,9 @@ class SplitDatasetTest(tf.test.TestCase):\n \n     self.assertAllEqual(np.array(list(left_split)).shape,(2,32,32,1))\n     self.assertAllEqual(np.array(list(right_split)).shape,(98,32,32,1))\n-    self.assertAllEqual(list(dataset.unbatch()),\n-                        list(left_split)+list(right_split)) \n+\n+    dataset = dataset.unbatch()\n+    self.assertAllEqual(list(dataset),list(left_split)+list(right_split))\n \n   def test_batched_tf_dataset_of_tuple_of_vectors(self):\n     dataset = tf.data.Dataset.from_tensor_slices((np.random.rand(10,32,32),\n@@ -120,10 +133,8 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertAllEqual(np.array(list(left_split)).shape,(4,2,32,32))\n     self.assertAllEqual(np.array(list(right_split)).shape,(6,2,32,32))\n \n-    self.assertAllEqual(list(dataset.unbatch()),\n-                        list(left_split)+list(right_split))\n-    \n-    \n+    dataset = dataset.unbatch()\n+    self.assertAllEqual(list(dataset),list(left_split)+list(right_split))\n \n   def test_unbatched_tf_dataset_of_vectors(self):\n     dataset = tf.data.Dataset.from_tensor_slices(np.ones(shape=(100,16, 16,3)))\n@@ -133,53 +144,19 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertAllEqual(np.array(list(left_split)).shape,(25,16, 16,3))\n     self.assertAllEqual(np.array(list(right_split)).shape,(75,16, 16,3))\n \n-    self.assertAllEqual(list(dataset),\n-                        list(left_split)+list(right_split)) \n-    \n-\n+    self.assertAllEqual(list(dataset),list(left_split)+list(right_split))\n \n   def test_unbatched_tf_dataset_of_tuple_of_vectors(self):\n-    dataset = tf.data.Dataset.from_tensor_slices((np.random.rand(10,32,32,1), \n-                                                  np.random.rand(10,32,32,1)))\n+    X,Y = (np.random.rand(10,32,32,1),np.random.rand(10,32,32,1))\n+    dataset = tf.data.Dataset.from_tensor_slices((X,Y))\n \n     left_split,right_split=dataset_utils.split_dataset(dataset,left_size=5)\n \n     self.assertAllEqual(np.array(list(left_split)).shape,(5,2,32,32,1))\n     self.assertAllEqual(np.array(list(right_split)).shape,(5,2,32,32,1))\n \n-  \n     self.assertAllEqual(list(dataset),list(left_split)+list(right_split))\n \n-  # def test_mnist_dataset(self):\n-  #   (x_train,y_train),(_,_) = mnist.load_data()\n-\n-  #   self.assertEqual(x_train.shape,(60000,28,28))\n-  #   self.assertEqual(y_train.shape,(60000,))\n-    \n-  #   dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n-    \n-  #   with self.assertWarns(ResourceWarning):\n-  #     left_split,right_split = dataset_utils.split_dataset(dataset,\n-  #                                                          left_size=0.80)\n-    \n-    \n-    \n-    \n-    \n-    \n-  # def test_unbatched_tf_dataset_of_dicts_of_vectors(self):\n-  #   dataset = tf.data.Dataset.from_tensor_slices(\n-  #     {'images': np.random.rand(32,32,3), \n-  #     'labels': np.random.rand(32,1)\n-  #     })\n-  #   dataset = dataset.batch(2)\n-  #   left_split,right_split=dataset_utils.split_dataset(dataset,left_size=5)\n-    # self.assertAllEqual(len(list(dataset.unbatch())),len(list(left_split))) \n-\n-    \n-    \n-    \n-    \n   def test_list_dataset(self):\n     dataset = [np.ones(shape=(10,10,10)) for _ in range(10)]\n     left_split,right_split = dataset_utils.split_dataset(dataset,\n@@ -196,23 +173,6 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertEqual(len(left_split), 6)\n     self.assertEqual(len(right_split), 4)\n \n-    \n-  def test_tuple_dataset(self):\n-    dataset = (np.ones(shape=(10,10,10)),np.zeros(shape=(10,10,10)))\n-    left_split,right_split = dataset_utils.split_dataset(dataset,\n-                                                             left_size=0.75,\n-                                                             right_size=0.25)\n-    self.assertLen(left_split, 8)\n-    self.assertLen(right_split, 2)\n-    \n-    left_split,right_split = dataset_utils.split_dataset(dataset,\n-                                                             left_size=0.35,\n-                                                             right_size=0.65)\n-    self.assertLen(left_split, 4)\n-    self.assertLen(right_split, 6)\n-    self.assertIsInstance(left_split, tf.data.Dataset)\n-    self.assertIsInstance(right_split, tf.data.Dataset)\n-    \n   def test_invalid_dataset(self):\n     with self.assertRaises(TypeError):\n       dataset_utils.split_dataset(dataset=None, left_size=5)\n@@ -226,7 +186,6 @@ class SplitDatasetTest(tf.test.TestCase):\n       dataset_utils.split_dataset(dataset=float('INF'), left_size=5)\n \n   def test_valid_left_and_right_sizes(self):\n-    \n     dataset = np.array([1,2,3])\n     splitted_dataset = dataset_utils.split_dataset(dataset,\n                                                    left_size=1,\n@@ -238,7 +197,6 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertEqual(list(left_split), [1])\n     self.assertEqual(list(right_split), [2,3])\n \n-    \n     dataset=np.ones(shape=(200, 32))\n     res = dataset_utils.split_dataset(dataset, left_size=150,right_size=50)\n     self.assertLen(res, 2)\n@@ -257,7 +215,6 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertLen(res[0], 120)\n     self.assertLen(res[1], 80)\n \n-    \n     dataset=np.ones(shape=(10000, 16))\n     res = dataset_utils.split_dataset(dataset, right_size=20)\n     self.assertLen(res, 2)\n@@ -267,7 +224,6 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertLen(res[0], 9980)\n     self.assertLen(res[1], 20)\n \n-    \n     dataset = np.array([1,2,3,4,5,6,7,8,9,10])\n     splitted_dataset = dataset_utils.split_dataset(dataset,\n                                                    left_size=0.1,\n@@ -291,9 +247,8 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertEqual(list(right_split), [6,7,8,9,10])\n \n   def test_float_left_and_right_sizes(self):\n-    dataset = tf.data.Dataset.from_tensor_slices(np.array([[0.1,0.2,0.3],\n-                                                           [0.4,0.5,0.6],\n-                                                           [0.7,0.8,0.9]]))\n+    X = np.array([[0.1,0.2,0.3],[0.4,0.5,0.6],[0.7,0.8,0.9]])\n+    dataset = tf.data.Dataset.from_tensor_slices(X)\n     left_split,right_split = dataset_utils.split_dataset(dataset,\n                                                          left_size=0.8,\n                                                          right_size=0.2)\n@@ -308,50 +263,37 @@ class SplitDatasetTest(tf.test.TestCase):\n       dataset = [1]\n       dataset_utils.split_dataset(dataset, left_size=0.8,right_size=0.2)\n \n-    \n-      \n   def test_None_and_zero_left_and_right_size(self):\n     with self.assertRaises(ValueError):\n       dataset_utils.split_dataset(dataset=np.array([1,2,3]), left_size=None)\n+\n     with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), \n-                                  left_size=None,\n-                                  right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=None,right_size=None)\n+\n     with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), \n-                                  left_size=3,\n-                                  right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=3,right_size=None)\n+\n     with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), \n-                                  left_size=3,\n-                                  right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=3,right_size=None)\n+\n     with self.assertRaises(ValueError):\n       dataset_utils.split_dataset(np.array([1,2,3]), left_size=0,right_size=0)\n \n   def test_invalid_left_and_right_size_types(self):\n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), \n-                                  left_size='1',\n-                                  right_size='1')\n-    with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), \n-                                  left_size=0,\n-                                  right_size='1')\n-    with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), \n-                                  left_size='100',\n-                                  right_size=None)\n-    with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]),\n-                                  right_size='1')\n-    with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), \n-                                  left_size=0.5,\n-                                  right_size='1')\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size='1',right_size='1')\n \n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=0,right_size='1')\n       \n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size='100',right_size=None)\n \n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset(np.array([1,2,3]),right_size='1')\n       \n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=0.5,right_size='1')\n \n if __name__ == \"__main__\":\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#65d68252df9ce08ad5a4b67fbb697c9a7232402a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 11 | Files Changed: 11 | Hunks: 11 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 8032 | Contributors (this commit): 47 | Commits (past 90d): 38 | Contributors (cumulative): 89 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,3 @@\n-# Lint as: python3\n # Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n@@ -1,4 +1,3 @@\n-# Lint as: python3\n # Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n@@ -1,4 +1,3 @@\n-# Lint as: python3\n # Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n@@ -1,4 +1,3 @@\n-# Lint as: python3\n # Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n@@ -1,4 +1,3 @@\n-# Lint as: python3\n # Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n@@ -1,4 +1,3 @@\n-# Lint as: python3\n # Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n@@ -1,4 +1,3 @@\n-# Lint as: python3\n # Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n@@ -1,4 +1,3 @@\n-# Lint as: python3\n # Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n@@ -1,4 +1,3 @@\n-# Lint as: python3\n # Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n@@ -1,4 +1,3 @@\n-# Lint as: python3\n # Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n@@ -1,4 +1,3 @@\n-# Lint as python3\n # Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b09c5126a90acb165147fbf1dc7313b9f1f7d9db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 65 | Churn Cumulative: 11722 | Contributors (this commit): 28 | Commits (past 90d): 24 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2562,7 +2562,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       output_list = tf.nest.flatten(outputs)\n       with backend.name_scope('ActivityRegularizer'):\n         for output in output_list:\n-          activity_loss = self._activity_regularizer(output)\n+          activity_loss = tf.convert_to_tensor(\n+              self._activity_regularizer(output))\n           batch_size = tf.cast(\n               tf.shape(output)[0], activity_loss.dtype)\n           # Make activity regularization strength batch-agnostic.\n\n@@ -1870,7 +1870,8 @@ class Layer(base_layer.Layer):\n       output_list = tf.nest.flatten(outputs)\n       with backend.name_scope('ActivityRegularizer'):\n         for output in output_list:\n-          activity_loss = self._activity_regularizer(output)\n+          activity_loss = tf.convert_to_tensor(\n+              self._activity_regularizer(output))\n           batch_size = tf.cast(\n               tf.compat.v1.shape(output)[0], activity_loss.dtype)\n           # Make activity regularization strength batch-agnostic.\n\n@@ -33,14 +33,32 @@ NUM_CLASSES = 2\n class KerasRegularizersTest(test_combinations.TestCase,\n                             parameterized.TestCase):\n \n-  def create_model(self, kernel_regularizer=None, activity_regularizer=None):\n+  def create_model(self,\n+                   kernel_regularizer=None,\n+                   bias_regularizer=None,\n+                   activity_regularizer=None):\n     model = keras.models.Sequential()\n     model.add(keras.layers.Dense(NUM_CLASSES,\n                                  kernel_regularizer=kernel_regularizer,\n+                                 bias_regularizer=bias_regularizer,\n                                  activity_regularizer=activity_regularizer,\n                                  input_shape=(DATA_DIM,)))\n     return model\n \n+  def regularizer_fn_tensor(x):\n+    return tf.constant(0.)\n+\n+  def regularizer_fn_scalar(x):\n+    return 0.\n+\n+  class RegularizerTensor(regularizers.Regularizer):\n+    def __call__(self, x):\n+      return tf.constant(0.)\n+\n+  class RegularizerScalar(regularizers.Regularizer):\n+    def __call__(self, x):\n+      return 0.\n+\n   def get_data(self):\n     (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n         train_samples=10,\n@@ -67,6 +85,14 @@ class KerasRegularizersTest(test_combinations.TestCase,\n       ('l1', regularizers.l1()),\n       ('l2', regularizers.l2()),\n       ('l1_l2', regularizers.l1_l2()),\n+      ('l2_zero', keras.regularizers.l2(0.)),\n+      ('function_tensor', regularizer_fn_tensor),\n+      ('function_scalar', regularizer_fn_scalar),\n+      ('lambda_tensor', lambda x: tf.constant(0.)),\n+      ('lambda_scalar', lambda x: 0.),\n+      ('regularizer_base_class', regularizers.Regularizer()),\n+      ('regularizer_custom_class_tensor', RegularizerTensor()),\n+      ('regularizer_custom_class_scalar', RegularizerScalar()),\n   ])\n   def test_kernel_regularization(self, regularizer):\n     (x_train, y_train), _ = self.get_data()\n@@ -84,6 +110,37 @@ class KerasRegularizersTest(test_combinations.TestCase,\n       ('l2', regularizers.l2()),\n       ('l1_l2', regularizers.l1_l2()),\n       ('l2_zero', keras.regularizers.l2(0.)),\n+      ('function_tensor', regularizer_fn_tensor),\n+      ('function_scalar', regularizer_fn_scalar),\n+      ('lambda_tensor', lambda x: tf.constant(0.)),\n+      ('lambda_scalar', lambda x: 0.),\n+      ('regularizer_base_class', regularizers.Regularizer()),\n+      ('regularizer_custom_class_tensor', RegularizerTensor()),\n+      ('regularizer_custom_class_scalar', RegularizerScalar()),\n+  ])\n+  def test_bias_regularization(self, regularizer):\n+    (x_train, y_train), _ = self.get_data()\n+    model = self.create_model(bias_regularizer=regularizer)\n+    model.compile(\n+        loss='categorical_crossentropy',\n+        optimizer='sgd',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    self.assertEqual(len(model.losses), 1)\n+    model.fit(x_train, y_train, batch_size=10, epochs=1, verbose=0)\n+\n+  @test_combinations.run_all_keras_modes\n+  @parameterized.named_parameters([\n+      ('l1', regularizers.l1()),\n+      ('l2', regularizers.l2()),\n+      ('l1_l2', regularizers.l1_l2()),\n+      ('l2_zero', keras.regularizers.l2(0.)),\n+      ('function_tensor', regularizer_fn_tensor),\n+      ('function_scalar', regularizer_fn_scalar),\n+      ('lambda_tensor', lambda x: tf.constant(0.)),\n+      ('lambda_scalar', lambda x: 0.),\n+      ('regularizer_base_class', regularizers.Regularizer()),\n+      ('regularizer_custom_class_tensor', RegularizerTensor()),\n+      ('regularizer_custom_class_scalar', RegularizerScalar()),\n   ])\n   def test_activity_regularization(self, regularizer):\n     (x_train, y_train), _ = self.get_data()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
