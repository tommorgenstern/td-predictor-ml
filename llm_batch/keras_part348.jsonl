{"custom_id": "keras#c01b8ea5000261263de579bbf8bbfcdcb07576c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 15558 | Contributors (this commit): 105 | Commits (past 90d): 22 | Contributors (cumulative): 105 | DMM Complexity: None\n\nDIFF:\n@@ -146,6 +146,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       itertools.chain(('_train_counter', '_test_counter', '_predict_counter',\n                        '_steps_per_execution'),\n                       base_layer.Layer._TF_MODULE_IGNORED_PROPERTIES))  # pylint: disable=protected-access\n+  _SCALAR_UPRANKING_ON = True\n \n   def __new__(cls, *args, **kwargs):\n     # Signature detection\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5997cecbb5a45314397eee0e980d1513ce44d637", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 34 | Churn Cumulative: 1759 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -624,7 +624,7 @@ class CosineDecay(LearningRateSchedule):\n       global_step_recomp = tf.minimum(global_step_recomp, decay_steps)\n       completed_fraction = global_step_recomp / decay_steps\n       cosine_decayed = 0.5 * (1.0 + tf.cos(\n-          tf.constant(math.pi) * completed_fraction))\n+          tf.constant(math.pi, dtype=dtype) * completed_fraction))\n \n       decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n       return tf.multiply(initial_learning_rate, decayed)\n@@ -749,7 +749,7 @@ class CosineDecayRestarts(LearningRateSchedule):\n \n       m_fac = m_mul**i_restart\n       cosine_decayed = 0.5 * m_fac * (1.0 + tf.cos(\n-          tf.constant(math.pi) * completed_fraction))\n+          tf.constant(math.pi, dtype=dtype) * completed_fraction))\n       decayed = (1 - alpha) * cosine_decayed + alpha\n \n       return tf.multiply(initial_learning_rate, decayed, name=name)\n@@ -866,7 +866,7 @@ class LinearCosineDecay(LearningRateSchedule):\n       completed_fraction = global_step_recomp / decay_steps\n       fraction = 2.0 * num_periods * completed_fraction\n       cosine_decayed = 0.5 * (\n-          1.0 + tf.cos(tf.constant(math.pi) * fraction))\n+          1.0 + tf.cos(tf.constant(math.pi, dtype=dtype) * fraction))\n \n       linear_cosine_decayed = (alpha + linear_decayed) * cosine_decayed + beta\n       return tf.multiply(initial_learning_rate, linear_cosine_decayed,\n@@ -1001,7 +1001,7 @@ class NoisyLinearCosineDecay(LearningRateSchedule):\n       completed_fraction = global_step_recomp / decay_steps\n       fraction = 2.0 * num_periods * completed_fraction\n       cosine_decayed = 0.5 * (\n-          1.0 + tf.cos(tf.constant(math.pi) * fraction))\n+          1.0 + tf.cos(tf.constant(math.pi, dtype=dtype) * fraction))\n       noisy_linear_cosine_decayed = (\n           (alpha + noisy_linear_decayed) * cosine_decayed + beta)\n \n\n@@ -14,14 +14,16 @@\n # ==============================================================================\n \"\"\"Functional test for learning rate decay.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import math\n \n from absl.testing import parameterized\n+\n from keras import combinations\n from keras.optimizer_v2 import gradient_descent\n from keras.optimizer_v2 import learning_rate_schedule\n+import numpy as np\n+\n+import tensorflow.compat.v2 as tf\n \n \n def _maybe_serialized(lr_decay, serialize_and_deserialize):\n@@ -357,6 +359,16 @@ class CosineDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n       expected = self.np_cosine_decay(step, num_training_steps, alpha)\n       self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)\n \n+  def testFloat64InitLearningRate(self, serialize):\n+    num_training_steps = 1000\n+    initial_lr = np.float64(1.0)\n+    for step in range(0, 1500, 250):\n+      decayed_lr = learning_rate_schedule.CosineDecay(initial_lr,\n+                                                      num_training_steps)\n+      decayed_lr = _maybe_serialized(decayed_lr, serialize)\n+      expected = self.np_cosine_decay(step, num_training_steps)\n+      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)\n+\n \n @combinations.generate(combinations.combine(serialize=[False, True],\n                                             mode=[\"graph\", \"eager\"]))\n@@ -385,6 +397,16 @@ class CosineDecayRestartsTestV2(tf.test.TestCase,\n       expected = self.np_cosine_decay_restarts(step, num_training_steps)\n       self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)\n \n+  def testFloat64InitLearningRate(self, serialize):\n+    num_training_steps = 1000\n+    initial_lr = np.float64(1.0)\n+    for step in range(0, 1500, 250):\n+      decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+          initial_lr, num_training_steps)\n+      decayed_lr = _maybe_serialized(decayed_lr, serialize)\n+      expected = self.np_cosine_decay_restarts(step, num_training_steps)\n+      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)\n+\n   def testAlpha(self, serialize):\n     num_training_steps = 1000\n     initial_lr = 1.0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#77237ea09a4c9c735f9f23dcd99493d3c1cc706d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2123 | Lines Deleted: 1880 | Files Changed: 19 | Hunks: 31 | Methods Changed: 128 | Complexity Δ (Sum/Max): 67/67 | Churn Δ: 4003 | Churn Cumulative: 21391 | Contributors (this commit): 100 | Commits (past 90d): 31 | Contributors (cumulative): 134 | DMM Complexity: 0.2\n\nDIFF:\n@@ -16,11 +16,10 @@\n \n import tensorflow.compat.v2 as tf\n \n+# pylint: disable=g-bad-import-order,g-direct-tensorflow-import,disable=g-import-not-at-top\n from tensorflow.python import tf2\n \n # Generic layers.\n-# pylint: disable=g-bad-import-order\n-# pylint: disable=g-import-not-at-top\n from keras.engine.input_layer import Input\n from keras.engine.input_layer import InputLayer\n from keras.engine.input_spec import InputSpec\n@@ -91,19 +90,24 @@ from keras.layers.convolutional import Cropping2D\n from keras.layers.convolutional import Cropping3D\n \n # Core layers.\n-from keras.layers.core import Masking\n-from keras.layers.core import Dropout\n-from keras.layers.core import SpatialDropout1D\n-from keras.layers.core import SpatialDropout2D\n-from keras.layers.core import SpatialDropout3D\n-from keras.layers.core import Activation\n-from keras.layers.core import Reshape\n-from keras.layers.core import Permute\n-from keras.layers.core import Flatten\n-from keras.layers.core import RepeatVector\n-from keras.layers.core import Lambda\n-from keras.layers.core import Dense\n-from keras.layers.core import ActivityRegularization\n+from keras.layers.core.activation import Activation\n+from keras.layers.core.activity_regularization import ActivityRegularization\n+from keras.layers.core.dense import Dense\n+from keras.layers.core.dropout import Dropout\n+from keras.layers.core.flatten import Flatten\n+from keras.layers.core.lambda_layer import Lambda\n+from keras.layers.core.masking import Masking\n+from keras.layers.core.permute import Permute\n+from keras.layers.core.repeat_vector import RepeatVector\n+from keras.layers.core.reshape import Reshape\n+from keras.layers.core.spatial_dropout import SpatialDropout1D\n+from keras.layers.core.spatial_dropout import SpatialDropout2D\n+from keras.layers.core.spatial_dropout import SpatialDropout3D\n+from keras.layers.core.tf_op_layer import ClassMethod\n+from keras.layers.core.tf_op_layer import InstanceMethod\n+from keras.layers.core.tf_op_layer import InstanceProperty\n+from keras.layers.core.tf_op_layer import SlicingOpLambda\n+from keras.layers.core.tf_op_layer import TFOpLambda\n \n # Dense Attention layers.\n from keras.layers.dense_attention import AdditiveAttention\n\n@@ -1,1834 +0,0 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Core Keras layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import copy\n-import functools\n-import operator\n-import sys\n-import textwrap\n-import types as python_types\n-import warnings\n-\n-import numpy as np\n-from keras import activations\n-from keras import backend as K\n-from keras import constraints\n-from keras import initializers\n-from keras import regularizers\n-from keras.engine import keras_tensor\n-from keras.engine.base_layer import Layer\n-from keras.engine.input_spec import InputSpec\n-from keras.utils import control_flow_util\n-from keras.utils import conv_utils\n-from keras.utils import generic_utils\n-from keras.utils import tf_inspect\n-from keras.utils import tf_utils\n-from tensorflow.python.platform import tf_logging\n-from tensorflow.python.util.tf_export import get_canonical_name_for_symbol\n-from tensorflow.python.util.tf_export import get_symbol_from_name\n-from tensorflow.python.util.tf_export import keras_export\n-\n-# TODO(b/168039935): track dropout rate to decide whether/how to make a\n-# dropout rate fastpath.\n-keras_temporary_dropout_rate = tf.__internal__.monitoring.BoolGauge(\n-    '/tensorflow/api/keras/dropout/temp_rate_is_zero',\n-    'Temporarily record if Keras dropout layer was created w/'\n-    'constant rate = 0')\n-\n-\n-# pylint: disable=g-classes-have-attributes\n-@keras_export('keras.layers.Masking')\n-class Masking(Layer):\n-  \"\"\"Masks a sequence by using a mask value to skip timesteps.\n-\n-  For each timestep in the input tensor (dimension #1 in the tensor),\n-  if all values in the input tensor at that timestep\n-  are equal to `mask_value`, then the timestep will be masked (skipped)\n-  in all downstream layers (as long as they support masking).\n-\n-  If any downstream layer does not support masking yet receives such\n-  an input mask, an exception will be raised.\n-\n-  Example:\n-\n-  Consider a Numpy data array `x` of shape `(samples, timesteps, features)`,\n-  to be fed to an LSTM layer. You want to mask timestep #3 and #5 because you\n-  lack data for these timesteps. You can:\n-\n-  - Set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.`\n-  - Insert a `Masking` layer with `mask_value=0.` before the LSTM layer:\n-\n-  ```python\n-  samples, timesteps, features = 32, 10, 8\n-  inputs = np.random.random([samples, timesteps, features]).astype(np.float32)\n-  inputs[:, 3, :] = 0.\n-  inputs[:, 5, :] = 0.\n-\n-  model = tf.keras.models.Sequential()\n-  model.add(tf.keras.layers.Masking(mask_value=0.,\n-                                    input_shape=(timesteps, features)))\n-  model.add(tf.keras.layers.LSTM(32))\n-\n-  output = model(inputs)\n-  # The time step 3 and 5 will be skipped from LSTM calculation.\n-  ```\n-\n-  See [the masking and padding guide](\n-    https://www.tensorflow.org/guide/keras/masking_and_padding)\n-  for more details.\n-  \"\"\"\n-\n-  def __init__(self, mask_value=0., **kwargs):\n-    super(Masking, self).__init__(**kwargs)\n-    self.supports_masking = True\n-    self.mask_value = mask_value\n-    self._compute_output_and_mask_jointly = True\n-\n-  def compute_mask(self, inputs, mask=None):\n-    return K.any(tf.not_equal(inputs, self.mask_value), axis=-1)\n-\n-  def call(self, inputs):\n-    boolean_mask = K.any(\n-        tf.not_equal(inputs, self.mask_value), axis=-1, keepdims=True)\n-    outputs = inputs * tf.cast(boolean_mask, inputs.dtype)\n-    # Compute the mask and outputs simultaneously.\n-    outputs._keras_mask = tf.squeeze(boolean_mask, axis=-1)  # pylint: disable=protected-access\n-    return outputs\n-\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-  def get_config(self):\n-    config = {'mask_value': self.mask_value}\n-    base_config = super(Masking, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.Dropout')\n-class Dropout(Layer):\n-  \"\"\"Applies Dropout to the input.\n-\n-  The Dropout layer randomly sets input units to 0 with a frequency of `rate`\n-  at each step during training time, which helps prevent overfitting.\n-  Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over\n-  all inputs is unchanged.\n-\n-  Note that the Dropout layer only applies when `training` is set to True\n-  such that no values are dropped during inference. When using `model.fit`,\n-  `training` will be appropriately set to True automatically, and in other\n-  contexts, you can set the kwarg explicitly to True when calling the layer.\n-\n-  (This is in contrast to setting `trainable=False` for a Dropout layer.\n-  `trainable` does not affect the layer's behavior, as Dropout does\n-  not have any variables/weights that can be frozen during training.)\n-\n-  >>> tf.random.set_seed(0)\n-  >>> layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n-  >>> data = np.arange(10).reshape(5, 2).astype(np.float32)\n-  >>> print(data)\n-  [[0. 1.]\n-   [2. 3.]\n-   [4. 5.]\n-   [6. 7.]\n-   [8. 9.]]\n-  >>> outputs = layer(data, training=True)\n-  >>> print(outputs)\n-  tf.Tensor(\n-  [[ 0.    1.25]\n-   [ 2.5   3.75]\n-   [ 5.    6.25]\n-   [ 7.5   8.75]\n-   [10.    0.  ]], shape=(5, 2), dtype=float32)\n-\n-  Args:\n-    rate: Float between 0 and 1. Fraction of the input units to drop.\n-    noise_shape: 1D integer tensor representing the shape of the\n-      binary dropout mask that will be multiplied with the input.\n-      For instance, if your inputs have shape\n-      `(batch_size, timesteps, features)` and\n-      you want the dropout mask to be the same for all timesteps,\n-      you can use `noise_shape=(batch_size, 1, features)`.\n-    seed: A Python integer to use as random seed.\n-\n-  Call arguments:\n-    inputs: Input tensor (of any rank).\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode (adding dropout) or in inference mode (doing nothing).\n-  \"\"\"\n-\n-  def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n-    super(Dropout, self).__init__(**kwargs)\n-    if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n-      raise ValueError(f'Invalid value {rate} received for '\n-                       f'`rate`, expected a value between 0 and 1.')\n-    self.rate = rate\n-    if isinstance(rate, (int, float)) and not rate:\n-      keras_temporary_dropout_rate.get_cell().set(True)\n-    else:\n-      keras_temporary_dropout_rate.get_cell().set(False)\n-    self.noise_shape = noise_shape\n-    self.seed = seed\n-    self.supports_masking = True\n-\n-  def _get_noise_shape(self, inputs):\n-    # Subclasses of `Dropout` may implement `_get_noise_shape(self, inputs)`,\n-    # which will override `self.noise_shape`, and allows for custom noise\n-    # shapes with dynamically sized inputs.\n-    if self.noise_shape is None:\n-      return None\n-\n-    concrete_inputs_shape = tf.shape(inputs)\n-    noise_shape = []\n-    for i, value in enumerate(self.noise_shape):\n-      noise_shape.append(concrete_inputs_shape[i] if value is None else value)\n-    return tf.convert_to_tensor(noise_shape)\n-\n-  def call(self, inputs, training=None):\n-    if training is None:\n-      training = K.learning_phase()\n-\n-    def dropped_inputs():\n-      return tf.nn.dropout(\n-          inputs,\n-          noise_shape=self._get_noise_shape(inputs),\n-          seed=self.seed,\n-          rate=self.rate)\n-\n-    output = control_flow_util.smart_cond(training, dropped_inputs,\n-                                          lambda: tf.identity(inputs))\n-    return output\n-\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-  def get_config(self):\n-    config = {\n-        'rate': self.rate,\n-        'noise_shape': self.noise_shape,\n-        'seed': self.seed\n-    }\n-    base_config = super(Dropout, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.SpatialDropout1D')\n-class SpatialDropout1D(Dropout):\n-  \"\"\"Spatial 1D version of Dropout.\n-\n-  This version performs the same function as Dropout, however, it drops\n-  entire 1D feature maps instead of individual elements. If adjacent frames\n-  within feature maps are strongly correlated (as is normally the case in\n-  early convolution layers) then regular dropout will not regularize the\n-  activations and will otherwise just result in an effective learning rate\n-  decrease. In this case, SpatialDropout1D will help promote independence\n-  between feature maps and should be used instead.\n-\n-  Args:\n-    rate: Float between 0 and 1. Fraction of the input units to drop.\n-\n-  Call arguments:\n-    inputs: A 3D tensor.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode (adding dropout) or in inference mode (doing nothing).\n-\n-  Input shape:\n-    3D tensor with shape:\n-    `(samples, timesteps, channels)`\n-\n-  Output shape:\n-    Same as input.\n-\n-  References:\n-    - [Efficient Object Localization Using Convolutional\n-      Networks](https://arxiv.org/abs/1411.4280)\n-  \"\"\"\n-\n-  def __init__(self, rate, **kwargs):\n-    super(SpatialDropout1D, self).__init__(rate, **kwargs)\n-    self.input_spec = InputSpec(ndim=3)\n-\n-  def _get_noise_shape(self, inputs):\n-    input_shape = tf.shape(inputs)\n-    noise_shape = (input_shape[0], 1, input_shape[2])\n-    return noise_shape\n-\n-\n-@keras_export('keras.layers.SpatialDropout2D')\n-class SpatialDropout2D(Dropout):\n-  \"\"\"Spatial 2D version of Dropout.\n-\n-  This version performs the same function as Dropout, however, it drops\n-  entire 2D feature maps instead of individual elements. If adjacent pixels\n-  within feature maps are strongly correlated (as is normally the case in\n-  early convolution layers) then regular dropout will not regularize the\n-  activations and will otherwise just result in an effective learning rate\n-  decrease. In this case, SpatialDropout2D will help promote independence\n-  between feature maps and should be used instead.\n-\n-  Args:\n-    rate: Float between 0 and 1. Fraction of the input units to drop.\n-    data_format: 'channels_first' or 'channels_last'.\n-      In 'channels_first' mode, the channels dimension\n-      (the depth) is at index 1,\n-      in 'channels_last' mode is it at index 3.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-\n-  Call arguments:\n-    inputs: A 4D tensor.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode (adding dropout) or in inference mode (doing nothing).\n-\n-  Input shape:\n-    4D tensor with shape:\n-    `(samples, channels, rows, cols)` if data_format='channels_first'\n-    or 4D tensor with shape:\n-    `(samples, rows, cols, channels)` if data_format='channels_last'.\n-\n-  Output shape:\n-    Same as input.\n-\n-  References:\n-    - [Efficient Object Localization Using Convolutional\n-      Networks](https://arxiv.org/abs/1411.4280)\n-  \"\"\"\n-\n-  def __init__(self, rate, data_format=None, **kwargs):\n-    super(SpatialDropout2D, self).__init__(rate, **kwargs)\n-    if data_format is None:\n-      data_format = K.image_data_format()\n-    if data_format not in {'channels_last', 'channels_first'}:\n-      raise ValueError('data_format must be in '\n-                       '{\"channels_last\", \"channels_first\"}')\n-    self.data_format = data_format\n-    self.input_spec = InputSpec(ndim=4)\n-\n-  def _get_noise_shape(self, inputs):\n-    input_shape = tf.shape(inputs)\n-    if self.data_format == 'channels_first':\n-      return (input_shape[0], input_shape[1], 1, 1)\n-    elif self.data_format == 'channels_last':\n-      return (input_shape[0], 1, 1, input_shape[3])\n-\n-\n-@keras_export('keras.layers.SpatialDropout3D')\n-class SpatialDropout3D(Dropout):\n-  \"\"\"Spatial 3D version of Dropout.\n-\n-  This version performs the same function as Dropout, however, it drops\n-  entire 3D feature maps instead of individual elements. If adjacent voxels\n-  within feature maps are strongly correlated (as is normally the case in\n-  early convolution layers) then regular dropout will not regularize the\n-  activations and will otherwise just result in an effective learning rate\n-  decrease. In this case, SpatialDropout3D will help promote independence\n-  between feature maps and should be used instead.\n-\n-  Args:\n-    rate: Float between 0 and 1. Fraction of the input units to drop.\n-    data_format: 'channels_first' or 'channels_last'.\n-        In 'channels_first' mode, the channels dimension (the depth)\n-        is at index 1, in 'channels_last' mode is it at index 4.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n-\n-  Call arguments:\n-    inputs: A 5D tensor.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode (adding dropout) or in inference mode (doing nothing).\n-\n-  Input shape:\n-    5D tensor with shape:\n-    `(samples, channels, dim1, dim2, dim3)` if data_format='channels_first'\n-    or 5D tensor with shape:\n-    `(samples, dim1, dim2, dim3, channels)` if data_format='channels_last'.\n-\n-  Output shape:\n-    Same as input.\n-\n-  References:\n-    - [Efficient Object Localization Using Convolutional\n-      Networks](https://arxiv.org/abs/1411.4280)\n-  \"\"\"\n-\n-  def __init__(self, rate, data_format=None, **kwargs):\n-    super(SpatialDropout3D, self).__init__(rate, **kwargs)\n-    if data_format is None:\n-      data_format = K.image_data_format()\n-    if data_format not in {'channels_last', 'channels_first'}:\n-      raise ValueError('data_format must be in '\n-                       '{\"channels_last\", \"channels_first\"}')\n-    self.data_format = data_format\n-    self.input_spec = InputSpec(ndim=5)\n-\n-  def _get_noise_shape(self, inputs):\n-    input_shape = tf.shape(inputs)\n-    if self.data_format == 'channels_first':\n-      return (input_shape[0], input_shape[1], 1, 1, 1)\n-    elif self.data_format == 'channels_last':\n-      return (input_shape[0], 1, 1, 1, input_shape[4])\n-\n-\n-@keras_export('keras.layers.Activation')\n-class Activation(Layer):\n-  \"\"\"Applies an activation function to an output.\n-\n-  Args:\n-    activation: Activation function, such as `tf.nn.relu`, or string name of\n-      built-in activation function, such as \"relu\".\n-\n-  Usage:\n-\n-  >>> layer = tf.keras.layers.Activation('relu')\n-  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n-  >>> list(output.numpy())\n-  [0.0, 0.0, 0.0, 2.0]\n-  >>> layer = tf.keras.layers.Activation(tf.nn.relu)\n-  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n-  >>> list(output.numpy())\n-  [0.0, 0.0, 0.0, 2.0]\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the batch axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as input.\n-  \"\"\"\n-\n-  def __init__(self, activation, **kwargs):\n-    super(Activation, self).__init__(**kwargs)\n-    self.supports_masking = True\n-    self.activation = activations.get(activation)\n-\n-  def call(self, inputs):\n-    return self.activation(inputs)\n-\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-  def get_config(self):\n-    config = {'activation': activations.serialize(self.activation)}\n-    base_config = super(Activation, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.Reshape')\n-class Reshape(Layer):\n-  \"\"\"Layer that reshapes inputs into the given shape.\n-\n-  Input shape:\n-    Arbitrary, although all dimensions in the input shape must be known/fixed.\n-    Use the keyword argument `input_shape` (tuple of integers, does not include\n-    the samples/batch size axis) when using this layer as the first layer\n-    in a model.\n-\n-  Output shape:\n-    `(batch_size,) + target_shape`\n-\n-  Example:\n-\n-  >>> # as first layer in a Sequential model\n-  >>> model = tf.keras.Sequential()\n-  >>> model.add(tf.keras.layers.Reshape((3, 4), input_shape=(12,)))\n-  >>> # model.output_shape == (None, 3, 4), `None` is the batch size.\n-  >>> model.output_shape\n-  (None, 3, 4)\n-\n-  >>> # as intermediate layer in a Sequential model\n-  >>> model.add(tf.keras.layers.Reshape((6, 2)))\n-  >>> model.output_shape\n-  (None, 6, 2)\n-\n-  >>> # also supports shape inference using `-1` as dimension\n-  >>> model.add(tf.keras.layers.Reshape((-1, 2, 2)))\n-  >>> model.output_shape\n-  (None, 3, 2, 2)\n-  \"\"\"\n-\n-  def __init__(self, target_shape, **kwargs):\n-    \"\"\"Creates a `tf.keras.layers.Reshape`  layer instance.\n-\n-    Args:\n-      target_shape: Target shape. Tuple of integers, does not include the\n-        samples dimension (batch size).\n-      **kwargs: Any additional layer keyword arguments.\n-    \"\"\"\n-    super(Reshape, self).__init__(**kwargs)\n-    self.target_shape = tuple(target_shape)\n-\n-  def _fix_unknown_dimension(self, input_shape, output_shape):\n-    \"\"\"Find and replace a missing dimension in an output shape.\n-\n-    This is a near direct port of the internal Numpy function\n-    `_fix_unknown_dimension` in `numpy/core/src/multiarray/shape.c`\n-\n-    Args:\n-      input_shape: Shape of array being reshaped\n-      output_shape: Desired shape of the array with at most\n-        a single -1 which indicates a dimension that should be\n-        derived from the input shape.\n-\n-    Returns:\n-      The new output shape with a -1 replaced with its computed value.\n-\n-    Raises:\n-      ValueError: If the total array size of the output_shape is\n-      different than the input_shape, or more than one unknown dimension\n-      is specified.\n-    \"\"\"\n-    output_shape = list(output_shape)\n-    msg = ('total size of new array must be unchanged, '\n-           'input_shape = {}, output_shape = {}'\n-           .format(input_shape, output_shape))\n-\n-    known, unknown = 1, None\n-    for index, dim in enumerate(output_shape):\n-      if dim < 0:\n-        if unknown is None:\n-          unknown = index\n-        else:\n-          raise ValueError('Can only specify one unknown dimension.')\n-      else:\n-        known *= dim\n-\n-    original = np.prod(input_shape, dtype=int)\n-    if unknown is not None:\n-      if known == 0 or original % known != 0:\n-        raise ValueError(msg)\n-      output_shape[unknown] = original // known\n-    elif original != known:\n-      raise ValueError(msg)\n-    return output_shape\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    if None in input_shape[1:]:\n-      output_shape = [input_shape[0]]\n-      # input shape (partially) unknown? replace -1's with None's\n-      output_shape += tuple(s if s != -1 else None for s in self.target_shape)\n-    else:\n-      output_shape = [input_shape[0]]\n-      output_shape += self._fix_unknown_dimension(input_shape[1:],\n-                                                  self.target_shape)\n-    return tf.TensorShape(output_shape)\n-\n-  def call(self, inputs):\n-    result = tf.reshape(\n-        inputs, (tf.shape(inputs)[0],) + self.target_shape)\n-    if not tf.executing_eagerly():\n-      # Set the static shape for the result since it might lost during array_ops\n-      # reshape, eg, some `None` dim in the result could be inferred.\n-      result.set_shape(self.compute_output_shape(inputs.shape))\n-    return result\n-\n-  def get_config(self):\n-    config = {'target_shape': self.target_shape}\n-    base_config = super(Reshape, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.Permute')\n-class Permute(Layer):\n-  \"\"\"Permutes the dimensions of the input according to a given pattern.\n-\n-  Useful e.g. connecting RNNs and convnets.\n-\n-  Example:\n-\n-  ```python\n-  model = Sequential()\n-  model.add(Permute((2, 1), input_shape=(10, 64)))\n-  # now: model.output_shape == (None, 64, 10)\n-  # note: `None` is the batch dimension\n-  ```\n-\n-  Args:\n-    dims: Tuple of integers. Permutation pattern does not include the\n-      samples dimension. Indexing starts at 1.\n-      For instance, `(2, 1)` permutes the first and second dimensions\n-      of the input.\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the samples axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same as the input shape, but with the dimensions re-ordered according\n-    to the specified pattern.\n-  \"\"\"\n-\n-  def __init__(self, dims, **kwargs):\n-    super(Permute, self).__init__(**kwargs)\n-    self.dims = tuple(dims)\n-    if sorted(dims) != list(range(1, len(dims) + 1)):\n-      raise ValueError(\n-          'Invalid permutation `dims` for Permute Layer: %s. '\n-          'The set of indices in `dims` must be consecutive and start from 1.' %\n-          (dims,))\n-    self.input_spec = InputSpec(ndim=len(self.dims) + 1)\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    output_shape = copy.copy(input_shape)\n-    for i, dim in enumerate(self.dims):\n-      target_dim = input_shape[dim]\n-      output_shape[i + 1] = target_dim\n-    return tf.TensorShape(output_shape)\n-\n-  def call(self, inputs):\n-    return tf.transpose(inputs, perm=(0,) + self.dims)\n-\n-  def get_config(self):\n-    config = {'dims': self.dims}\n-    base_config = super(Permute, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.Flatten')\n-class Flatten(Layer):\n-  \"\"\"Flattens the input. Does not affect the batch size.\n-\n-  Note: If inputs are shaped `(batch,)` without a feature axis, then\n-  flattening adds an extra channel dimension and output shape is `(batch, 1)`.\n-\n-  Args:\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, ..., channels)` while `channels_first` corresponds to\n-      inputs with shape `(batch, channels, ...)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-\n-  Example:\n-\n-  >>> model = tf.keras.Sequential()\n-  >>> model.add(tf.keras.layers.Conv2D(64, 3, 3, input_shape=(3, 32, 32)))\n-  >>> model.output_shape\n-  (None, 1, 10, 64)\n-\n-  >>> model.add(Flatten())\n-  >>> model.output_shape\n-  (None, 640)\n-\n-  \"\"\"\n-\n-  def __init__(self, data_format=None, **kwargs):\n-    super(Flatten, self).__init__(**kwargs)\n-    self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.input_spec = InputSpec(min_ndim=1)\n-    self._channels_first = self.data_format == 'channels_first'\n-\n-  def call(self, inputs):\n-    if self._channels_first:\n-      rank = inputs.shape.rank\n-      if rank and rank > 1:\n-        # Switch to channels-last format.\n-        permutation = [0]\n-        permutation.extend(range(2, rank))\n-        permutation.append(1)\n-        inputs = tf.transpose(inputs, perm=permutation)\n-\n-    if tf.executing_eagerly():\n-      # Full static shape is guaranteed to be available.\n-      # Performance: Using `constant_op` is much faster than passing a list.\n-      flattened_shape = tf.constant([inputs.shape[0], -1])\n-      return tf.reshape(inputs, flattened_shape)\n-    else:\n-      input_shape = inputs.shape\n-      rank = input_shape.rank\n-      if rank == 1:\n-        return tf.expand_dims(inputs, axis=1)\n-      else:\n-        batch_dim = tf.compat.dimension_value(input_shape[0])\n-        non_batch_dims = input_shape[1:]\n-        # Reshape in a way that preserves as much shape info as possible.\n-        if non_batch_dims.is_fully_defined():\n-          last_dim = int(functools.reduce(operator.mul, non_batch_dims))\n-          flattened_shape = tf.constant([-1, last_dim])\n-        elif batch_dim is not None:\n-          flattened_shape = tf.constant([int(batch_dim), -1])\n-        else:\n-          flattened_shape = [tf.shape(inputs)[0], -1]\n-        return tf.reshape(inputs, flattened_shape)\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    if not input_shape:\n-      output_shape = tf.TensorShape([1])\n-    else:\n-      output_shape = [input_shape[0]]\n-    if np.all(input_shape[1:]):\n-      output_shape += [np.prod(input_shape[1:], dtype=int)]\n-    else:\n-      output_shape += [None]\n-    return tf.TensorShape(output_shape)\n-\n-  def get_config(self):\n-    config = super(Flatten, self).get_config()\n-    config.update({'data_format': self.data_format})\n-    return config\n-\n-\n-@keras_export('keras.layers.RepeatVector')\n-class RepeatVector(Layer):\n-  \"\"\"Repeats the input n times.\n-\n-  Example:\n-\n-  ```python\n-  model = Sequential()\n-  model.add(Dense(32, input_dim=32))\n-  # now: model.output_shape == (None, 32)\n-  # note: `None` is the batch dimension\n-\n-  model.add(RepeatVector(3))\n-  # now: model.output_shape == (None, 3, 32)\n-  ```\n-\n-  Args:\n-    n: Integer, repetition factor.\n-\n-  Input shape:\n-    2D tensor of shape `(num_samples, features)`.\n-\n-  Output shape:\n-    3D tensor of shape `(num_samples, n, features)`.\n-  \"\"\"\n-\n-  def __init__(self, n, **kwargs):\n-    super(RepeatVector, self).__init__(**kwargs)\n-    self.n = n\n-    if not isinstance(n, int):\n-      raise TypeError(f'Expected an integer value for `n`, got {type(n)}.')\n-    self.input_spec = InputSpec(ndim=2)\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    return tf.TensorShape([input_shape[0], self.n, input_shape[1]])\n-\n-  def call(self, inputs):\n-    return K.repeat(inputs, self.n)\n-\n-  def get_config(self):\n-    config = {'n': self.n}\n-    base_config = super(RepeatVector, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.Lambda')\n-class Lambda(Layer):\n-  \"\"\"Wraps arbitrary expressions as a `Layer` object.\n-\n-  The `Lambda` layer exists so that arbitrary expressions can be used\n-  as a `Layer` when constructing `Sequential`\n-  and Functional API models. `Lambda` layers are best suited for simple\n-  operations or quick experimentation. For more advanced use cases, follow\n-  [this guide](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n-  for subclassing `tf.keras.layers.Layer`.\n-\n-  WARNING: `tf.keras.layers.Lambda` layers have (de)serialization limitations!\n-\n-  The main reason to subclass `tf.keras.layers.Layer` instead of using a\n-  `Lambda` layer is saving and inspecting a Model. `Lambda` layers\n-  are saved by serializing the Python bytecode, which is fundamentally\n-  non-portable. They should only be loaded in the same environment where\n-  they were saved. Subclassed layers can be saved in a more portable way\n-  by overriding their `get_config` method. Models that rely on\n-  subclassed Layers are also often easier to visualize and reason about.\n-\n-  Examples:\n-\n-  ```python\n-  # add a x -> x^2 layer\n-  model.add(Lambda(lambda x: x ** 2))\n-  ```\n-  ```python\n-  # add a layer that returns the concatenation\n-  # of the positive part of the input and\n-  # the opposite of the negative part\n-\n-  def antirectifier(x):\n-      x -= K.mean(x, axis=1, keepdims=True)\n-      x = K.l2_normalize(x, axis=1)\n-      pos = K.relu(x)\n-      neg = K.relu(-x)\n-      return K.concatenate([pos, neg], axis=1)\n-\n-  model.add(Lambda(antirectifier))\n-  ```\n-\n-  Variables:\n-    While it is possible to use Variables with Lambda layers, this practice is\n-    discouraged as it can easily lead to bugs. For instance, consider the\n-    following layer:\n-\n-    ```python\n-      scale = tf.Variable(1.)\n-      scale_layer = tf.keras.layers.Lambda(lambda x: x * scale)\n-    ```\n-\n-    Because scale_layer does not directly track the `scale` variable, it will\n-    not appear in `scale_layer.trainable_weights` and will therefore not be\n-    trained if `scale_layer` is used in a Model.\n-\n-    A better pattern is to write a subclassed Layer:\n-\n-    ```python\n-      class ScaleLayer(tf.keras.layers.Layer):\n-        def __init__(self):\n-          super(ScaleLayer, self).__init__()\n-          self.scale = tf.Variable(1.)\n-\n-        def call(self, inputs):\n-          return inputs * self.scale\n-    ```\n-\n-    In general, Lambda layers can be convenient for simple stateless\n-    computation, but anything more complex should use a subclass Layer instead.\n-\n-  Args:\n-    function: The function to be evaluated. Takes input tensor as first\n-      argument.\n-    output_shape: Expected output shape from function. This argument can be\n-      inferred if not explicitly provided. Can be a tuple or function. If a\n-      tuple, it only specifies the first dimension onward;\n-      sample dimension is assumed either the same as the input: `output_shape =\n-        (input_shape[0], ) + output_shape` or, the input is `None` and\n-      the sample dimension is also `None`: `output_shape = (None, ) +\n-        output_shape` If a function, it specifies the entire shape as a function\n-        of the\n-      input shape: `output_shape = f(input_shape)`\n-    mask: Either None (indicating no masking) or a callable with the same\n-      signature as the `compute_mask` layer method, or a tensor that will be\n-      returned as output mask regardless of what the input is.\n-    arguments: Optional dictionary of keyword arguments to be passed to the\n-      function.\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument input_shape (tuple of\n-    integers, does not include the samples axis) when using this layer as the\n-    first layer in a model.\n-\n-  Output shape:\n-    Specified by `output_shape` argument\n-  \"\"\"\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def __init__(self, function, output_shape=None, mask=None, arguments=None,\n-               **kwargs):\n-    super(Lambda, self).__init__(**kwargs)\n-\n-    self.arguments = arguments or {}\n-    self.function = function\n-\n-    if mask is not None:\n-      self.supports_masking = True\n-    self.mask = mask\n-    self._output_shape = output_shape\n-\n-    # Warning on every invocation will be quite irksome in Eager mode.\n-    self._already_warned = False\n-\n-    function_args = tf_inspect.getfullargspec(function).args\n-    self._fn_expects_training_arg = 'training' in function_args\n-    self._fn_expects_mask_arg = 'mask' in function_args\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    if self._output_shape is None:\n-      # Make use of existing autocomputation but provide Lambda-specific\n-      # error message. This is always safe to run even when the outer context\n-      # is Graph mode because Lambda layers don't have side effects such as\n-      # `add_loss`.\n-      with tf.__internal__.eager_context.eager_mode():\n-        try:\n-          return super(Lambda, self).compute_output_shape(input_shape)\n-        except NotImplementedError:\n-          raise NotImplementedError(\n-              'We could not automatically infer the shape of the Lambda\\'s '\n-              'output. Please specify `output_shape` for this Lambda.')\n-\n-    if callable(self._output_shape):\n-      output_shapes = self._output_shape(input_shape)\n-      return tf_utils.convert_shapes(output_shapes, to_tuples=False)\n-\n-    # Output shapes are passed directly and don't include batch dimension.\n-    input_tensor_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n-    batch_size = tf.nest.flatten(input_tensor_shape)[0][0] if input_shape else None\n-\n-    def _add_batch(shape):\n-      return tf.TensorShape([batch_size] + shape.as_list())\n-\n-    output_shapes = tf_utils.convert_shapes(self._output_shape, to_tuples=False)\n-    return tf.nest.map_structure(_add_batch, output_shapes)\n-\n-  def call(self, inputs, mask=None, training=None):\n-    # We must copy for thread safety, but it only needs to be a shallow copy.\n-    kwargs = {k: v for k, v in self.arguments.items()}\n-    if self._fn_expects_mask_arg:\n-      kwargs['mask'] = mask\n-    if self._fn_expects_training_arg:\n-      kwargs['training'] = training\n-\n-    created_variables = []\n-    def _variable_creator(next_creator, **kwargs):\n-      var = next_creator(**kwargs)\n-      created_variables.append(var)\n-      return var\n-\n-    with tf.GradientTape(watch_accessed_variables=True) as tape,\\\n-        tf.variable_creator_scope(_variable_creator):\n-      result = self.function(inputs, **kwargs)\n-    self._check_variables(created_variables, tape.watched_variables())\n-    return result\n-\n-  def _check_variables(self, created_variables, accessed_variables):\n-    if not created_variables and not accessed_variables:\n-      # In the common case that a Lambda layer does not touch a Variable, we\n-      # don't want to incur the runtime cost of assembling any state used for\n-      # checking only to immediately discard it.\n-      return\n-\n-    tracked_weights = set(v.ref() for v in self.weights)\n-    untracked_new_vars = [\n-        v for v in created_variables if v.ref() not in tracked_weights\n-    ]\n-    if untracked_new_vars:\n-      variable_str = '\\n'.join('  {}'.format(i) for i in untracked_new_vars)\n-      error_str = textwrap.dedent(\n-          '''\n-          The following Variables were created within a Lambda layer ({name})\n-          but are not tracked by said layer:\n-          {variable_str}\n-          The layer cannot safely ensure proper Variable reuse across multiple\n-          calls, and consquently this behavior is disallowed for safety. Lambda\n-          layers are not well suited to stateful computation; instead, writing a\n-          subclassed Layer is the recommend way to define layers with\n-          Variables.'''\n-      ).format(name=self.name, variable_str=variable_str)\n-      raise ValueError(error_str)\n-\n-    untracked_used_vars = [\n-        v for v in accessed_variables if v.ref() not in tracked_weights\n-    ]\n-    if untracked_used_vars and not self._already_warned:\n-      variable_str = '\\n'.join('  {}'.format(i) for i in untracked_used_vars)\n-      self._warn(textwrap.dedent(\n-          '''\n-          The following Variables were used a Lambda layer's call ({name}), but\n-          are not present in its tracked objects:\n-          {variable_str}\n-          It is possible that this is intended behavior, but it is more likely\n-          an omission. This is a strong indication that this layer should be\n-          formulated as a subclassed Layer rather than a Lambda layer.'''\n-      ).format(name=self.name, variable_str=variable_str))\n-      self._already_warned = True\n-\n-  def _warn(self, msg):\n-    # This method will be overridden in a unit test to raise an error, because\n-    # self.assertWarns is not universally implemented.\n-    return tf_logging.warning(msg)\n-\n-  def compute_mask(self, inputs, mask=None):\n-    if callable(self.mask):\n-      return self.mask(inputs, mask)\n-    return self.mask\n-\n-  def get_config(self):\n-    function_config = self._serialize_function_to_config(self.function)\n-    output_shape_config = self._serialize_function_to_config(self._output_shape,\n-                                                             allow_raw=True)\n-    config = {\n-        'function': function_config[0],\n-        'function_type': function_config[1],\n-        'module': function_config[2],\n-        'output_shape': output_shape_config[0],\n-        'output_shape_type': output_shape_config[1],\n-        'output_shape_module': output_shape_config[2],\n-    }\n-    if self.mask is not None:\n-      mask_config = self._serialize_function_to_config(self.mask)\n-      config.update({\n-          'mask': mask_config[0],\n-          'mask_type': mask_config[1],\n-          'mask_module': mask_config[2]\n-      })\n-    config['arguments'] = self.arguments\n-\n-    base_config = super(Lambda, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  def _serialize_function_to_config(self, inputs, allow_raw=False):\n-    if isinstance(inputs, python_types.LambdaType):\n-      output = generic_utils.func_dump(inputs)\n-      output_type = 'lambda'\n-      module = inputs.__module__\n-    elif callable(inputs):\n-      output = inputs.__name__\n-      output_type = 'function'\n-      module = inputs.__module__\n-    elif allow_raw:\n-      output = inputs\n-      output_type = 'raw'\n-      module = None\n-    else:\n-      raise ValueError(\n-          'Invalid input for serialization, type: %s ' % type(inputs))\n-\n-    return output, output_type, module\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    config = config.copy()\n-    function = cls._parse_function_from_config(\n-        config, custom_objects, 'function', 'module', 'function_type')\n-\n-    output_shape = cls._parse_function_from_config(\n-        config, custom_objects, 'output_shape', 'output_shape_module',\n-        'output_shape_type')\n-    if 'mask' in config:\n-      mask = cls._parse_function_from_config(\n-          config, custom_objects, 'mask', 'mask_module', 'mask_type')\n-    else:\n-      mask = None\n-\n-    config['function'] = function\n-    config['output_shape'] = output_shape\n-    config['mask'] = mask\n-\n-    # If arguments were numpy array, they have been saved as\n-    # list. We need to recover the ndarray\n-    if 'arguments' in config:\n-      for key in config['arguments']:\n-        if isinstance(config['arguments'][key], dict):\n-          arg_dict = config['arguments'][key]\n-          if 'type' in arg_dict and arg_dict['type'] == 'ndarray':\n-            # Overwrite the argument with its numpy translation\n-            config['arguments'][key] = np.array(arg_dict['value'])\n-\n-    return cls(**config)\n-\n-  @classmethod\n-  def _parse_function_from_config(\n-      cls, config, custom_objects, func_attr_name, module_attr_name,\n-      func_type_attr_name):\n-    globs = globals().copy()\n-    module = config.pop(module_attr_name, None)\n-    if module in sys.modules:\n-      globs.update(sys.modules[module].__dict__)\n-    elif module is not None:\n-      # Note: we don't know the name of the function if it's a lambda.\n-      warnings.warn('{} is not loaded, but a Lambda layer uses it. '\n-                    'It may cause errors.'.format(module)\n-                    , UserWarning)\n-    if custom_objects:\n-      globs.update(custom_objects)\n-    function_type = config.pop(func_type_attr_name)\n-    if function_type == 'function':\n-      # Simple lookup in custom objects\n-      function = generic_utils.deserialize_keras_object(\n-          config[func_attr_name],\n-          custom_objects=custom_objects,\n-          printable_module_name='function in Lambda layer')\n-    elif function_type == 'lambda':\n-      # Unsafe deserialization from bytecode\n-      function = generic_utils.func_load(\n-          config[func_attr_name], globs=globs)\n-    elif function_type == 'raw':\n-      function = config[func_attr_name]\n-    else:\n-      raise TypeError('Unknown function type:', function_type)\n-    return function\n-\n-\n-@keras_export('keras.layers.Dense')\n-class Dense(Layer):\n-  \"\"\"Just your regular densely-connected NN layer.\n-\n-  `Dense` implements the operation:\n-  `output = activation(dot(input, kernel) + bias)`\n-  where `activation` is the element-wise activation function\n-  passed as the `activation` argument, `kernel` is a weights matrix\n-  created by the layer, and `bias` is a bias vector created by the layer\n-  (only applicable if `use_bias` is `True`). These are all attributes of\n-  `Dense`.\n-\n-  Note: If the input to the layer has a rank greater than 2, then `Dense`\n-  computes the dot product between the `inputs` and the `kernel` along the\n-  last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n-  For example, if input has dimensions `(batch_size, d0, d1)`,\n-  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n-  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n-  (there are `batch_size * d0` such sub-tensors).\n-  The output in this case will have shape `(batch_size, d0, units)`.\n-\n-  Besides, layer attributes cannot be modified after the layer has been called\n-  once (except the `trainable` attribute).\n-  When a popular kwarg `input_shape` is passed, then keras will create\n-  an input layer to insert before the current layer. This can be treated\n-  equivalent to explicitly defining an `InputLayer`.\n-\n-  Example:\n-\n-  >>> # Create a `Sequential` model and add a Dense layer as the first layer.\n-  >>> model = tf.keras.models.Sequential()\n-  >>> model.add(tf.keras.Input(shape=(16,)))\n-  >>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n-  >>> # Now the model will take as input arrays of shape (None, 16)\n-  >>> # and output arrays of shape (None, 32).\n-  >>> # Note that after the first layer, you don't need to specify\n-  >>> # the size of the input anymore:\n-  >>> model.add(tf.keras.layers.Dense(32))\n-  >>> model.output_shape\n-  (None, 32)\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use.\n-      If you don't specify anything, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix.\n-    bias_initializer: Initializer for the bias vector.\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    activity_regularizer: Regularizer function applied to\n-      the output of the layer (its \"activation\").\n-    kernel_constraint: Constraint function applied to\n-      the `kernel` weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-\n-  Input shape:\n-    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n-    The most common situation would be\n-    a 2D input with shape `(batch_size, input_dim)`.\n-\n-  Output shape:\n-    N-D tensor with shape: `(batch_size, ..., units)`.\n-    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n-    the output would have shape `(batch_size, units)`.\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation=None,\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(Dense, self).__init__(\n-        activity_regularizer=activity_regularizer, **kwargs)\n-\n-    self.units = int(units) if not isinstance(units, int) else units\n-    if self.units < 0:\n-      raise ValueError(f'Received an invalid value for `units`, expected '\n-                       f'a positive integer, got {units}.')\n-    self.activation = activations.get(activation)\n-    self.use_bias = use_bias\n-    self.kernel_initializer = initializers.get(kernel_initializer)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-    self.bias_regularizer = regularizers.get(bias_regularizer)\n-    self.kernel_constraint = constraints.get(kernel_constraint)\n-    self.bias_constraint = constraints.get(bias_constraint)\n-\n-    self.input_spec = InputSpec(min_ndim=2)\n-    self.supports_masking = True\n-\n-  def build(self, input_shape):\n-    dtype = tf.as_dtype(self.dtype or K.floatx())\n-    if not (dtype.is_floating or dtype.is_complex):\n-      raise TypeError('Unable to build `Dense` layer with non-floating point '\n-                      'dtype %s' % (dtype,))\n-\n-    input_shape = tf.TensorShape(input_shape)\n-    last_dim = tf.compat.dimension_value(input_shape[-1])\n-    if last_dim is None:\n-      raise ValueError('The last dimension of the inputs to `Dense` '\n-                       'should be defined. Found `None`.')\n-    self.input_spec = InputSpec(min_ndim=2, axes={-1: last_dim})\n-    self.kernel = self.add_weight(\n-        'kernel',\n-        shape=[last_dim, self.units],\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint,\n-        dtype=self.dtype,\n-        trainable=True)\n-    if self.use_bias:\n-      self.bias = self.add_weight(\n-          'bias',\n-          shape=[self.units,],\n-          initializer=self.bias_initializer,\n-          regularizer=self.bias_regularizer,\n-          constraint=self.bias_constraint,\n-          dtype=self.dtype,\n-          trainable=True)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def call(self, inputs):\n-    if inputs.dtype.base_dtype != self._compute_dtype_object.base_dtype:\n-      inputs = tf.cast(inputs, dtype=self._compute_dtype_object)\n-\n-    if isinstance(inputs, tf.RaggedTensor) and inputs.shape[-1] is not None:\n-      # In case we encounter a RaggedTensor with a fixed last dimension (last\n-      # dimension not ragged), we can map the call method to the flat values.\n-      return tf.ragged.map_flat_values(self.call, inputs)\n-\n-    rank = inputs.shape.rank\n-    if rank == 2 or rank is None:\n-      # We use embedding_lookup_sparse as a more efficient matmul operation for\n-      # large sparse input tensors. The op will result in a sparse gradient, as\n-      # opposed to sparse_ops.sparse_tensor_dense_matmul which results in dense\n-      # gradients. This can lead to sigfinicant speedups, see b/171762937.\n-      if isinstance(inputs, tf.SparseTensor):\n-        # We need to fill empty rows, as the op assumes at least one id per row.\n-        inputs, _ = tf.sparse.fill_empty_rows(inputs, 0)\n-        # We need to do some munging of our input to use the embedding lookup as\n-        # a matrix multiply. We split our input matrix into separate ids and\n-        # weights tensors. The values of the ids tensor should be the column\n-        # indices of our input matrix and the values of the weights tensor\n-        # can continue to the actual matrix weights.\n-        # The column arrangement of ids and weights\n-        # will be summed over and does not matter. See the documentation for\n-        # sparse_ops.sparse_tensor_dense_matmul a more detailed explanation\n-        # of the inputs to both ops.\n-        ids = tf.SparseTensor(\n-            indices=inputs.indices,\n-            values=inputs.indices[:, 1],\n-            dense_shape=inputs.dense_shape)\n-        weights = inputs\n-        outputs = tf.nn.embedding_lookup_sparse(\n-            self.kernel, ids, weights, combiner='sum')\n-      else:\n-        outputs = tf.raw_ops.MatMul(a=inputs, b=self.kernel)\n-    # Broadcast kernel to inputs.\n-    else:\n-      outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])\n-      # Reshape the output back to the original ndim of the input.\n-      if not tf.executing_eagerly():\n-        shape = inputs.shape.as_list()\n-        output_shape = shape[:-1] + [self.kernel.shape[-1]]\n-        outputs.set_shape(output_shape)\n-\n-    if self.use_bias:\n-      outputs = tf.nn.bias_add(outputs, self.bias)\n-\n-    if self.activation is not None:\n-      outputs = self.activation(outputs)\n-    return outputs\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape)\n-    input_shape = input_shape.with_rank_at_least(2)\n-    if tf.compat.dimension_value(input_shape[-1]) is None:\n-      raise ValueError(\n-          'The innermost dimension of input_shape must be defined, but saw: %s'\n-          % (input_shape,))\n-    return input_shape[:-1].concatenate(self.units)\n-\n-  def get_config(self):\n-    config = super(Dense, self).get_config()\n-    config.update({\n-        'units': self.units,\n-        'activation': activations.serialize(self.activation),\n-        'use_bias': self.use_bias,\n-        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-        'bias_initializer': initializers.serialize(self.bias_initializer),\n-        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-        'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-        'activity_regularizer':\n-            regularizers.serialize(self.activity_regularizer),\n-        'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-        'bias_constraint': constraints.serialize(self.bias_constraint)\n-    })\n-    return config\n-\n-\n-@keras_export('keras.layers.ActivityRegularization')\n-class ActivityRegularization(Layer):\n-  \"\"\"Layer that applies an update to the cost function based input activity.\n-\n-  Args:\n-    l1: L1 regularization factor (positive float).\n-    l2: L2 regularization factor (positive float).\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the samples axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as input.\n-  \"\"\"\n-\n-  def __init__(self, l1=0., l2=0., **kwargs):\n-    super(ActivityRegularization, self).__init__(\n-        activity_regularizer=regularizers.L1L2(l1=l1, l2=l2), **kwargs)\n-    self.supports_masking = True\n-    self.l1 = l1\n-    self.l2 = l2\n-\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-  def get_config(self):\n-    config = {'l1': self.l1, 'l2': self.l2}\n-    base_config = super(ActivityRegularization, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class TFOpLambda(Layer):\n-  \"\"\"Wraps TF API symbols in a `Layer` object.\n-\n-  It is inserted by the Functional API construction whenever users call\n-  a supported TF symbol on KerasTensors.\n-\n-  Like Lambda layers, this layer tries to raise warnings when it detects users\n-  explicitly use variables in the call. (To let them know\n-  that the layer will not capture the variables).\n-\n-  This is useful in the case where users do something like:\n-  x = keras.Input(...)\n-  y = tf.Variable(...)\n-  out = x * tf_variable\n-  \"\"\"\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def __init__(self, function, **kwargs):\n-    self.function = function\n-    self.symbol = (\n-        get_canonical_name_for_symbol(self.function,\n-                                      add_prefix_to_v1_names=True) or\n-        get_canonical_name_for_symbol(self.function,\n-                                      api_name='keras',\n-                                      add_prefix_to_v1_names=True))\n-    if 'name' not in kwargs:\n-      # Generate a name.\n-      # TFOpLambda layers avoid already-observed names,\n-      # because users cannot easily control the generated names.\n-      # Without this avoidance, users would be more likely to run\n-      # into unavoidable duplicate layer name collisions.\n-      # (For standard layers users could just set `name` when creating the\n-      # layer to work around a collision, but they can't do that for\n-      # auto-generated layers)\n-      if self.symbol:\n-        name = 'tf.' + self.symbol\n-      else:\n-        name = self.function.__name__\n-      kwargs['name'] = K.unique_object_name(\n-          name, zero_based=True, avoid_observed_names=True)\n-    kwargs['autocast'] = False\n-\n-    # Decorate the function to produce this layer's call method\n-    def _call_wrapper(*args, **kwargs):\n-      return self._call_wrapper(*args, **kwargs)\n-    self.call = tf.__internal__.decorator.make_decorator(function, _call_wrapper)\n-\n-    # Do not individually trace op layers in the SavedModel.\n-    self._must_restore_from_config = True\n-\n-    super(TFOpLambda, self).__init__(**kwargs)\n-\n-    # Preserve all argument data structures when saving/loading a config\n-    # (e.g., don't unnest lists that contain one element)\n-    self._preserve_input_structure_in_config = True\n-\n-    # Warning on every invocation will be quite irksome in Eager mode.\n-    self._already_warned = False\n-\n-    self._expects_training_arg = False\n-    self._expects_mask_arg = False\n-\n-  def _call_wrapper(self, *args, **kwargs):\n-    created_variables = []\n-    def _variable_creator(next_creator, **creator_kwargs):\n-      var = next_creator(**creator_kwargs)\n-      created_variables.append(var)\n-      return var\n-\n-    with tf.GradientTape(watch_accessed_variables=True) as tape, \\\n-        tf.variable_creator_scope(_variable_creator):\n-      # We explicitly drop `name` arguments here,\n-      # to guard against the case where an op explicitly has a\n-      # `name` passed (which is susceptible to producing\n-      # multiple ops w/ the same name when the layer is reused)\n-      kwargs.pop('name', None)\n-      result = self.function(*args, **kwargs)\n-    self._check_variables(created_variables, tape.watched_variables())\n-    return result\n-\n-  def _check_variables(self, created_variables, accessed_variables):\n-    if not created_variables and not accessed_variables:\n-      # In the common case that a Lambda layer does not touch a Variable, we\n-      # don't want to incur the runtime cost of assembling any state used for\n-      # checking only to immediately discard it.\n-      return\n-\n-    tracked_weights = set(v.ref() for v in self.weights)\n-    untracked_new_vars = [\n-        v for v in created_variables if v.ref() not in tracked_weights\n-    ]\n-    if untracked_new_vars:\n-      variable_str = '\\n'.join('  {}'.format(i) for i in untracked_new_vars)\n-      error_str = textwrap.dedent(\n-          '''\n-          The following Variables were created within a Lambda layer ({name})\n-          but are not tracked by said layer:\n-          {variable_str}\n-          The layer cannot safely ensure proper Variable reuse across multiple\n-          calls, and consquently this behavior is disallowed for safety. Lambda\n-          layers are not well suited to stateful computation; instead, writing a\n-          subclassed Layer is the recommend way to define layers with\n-          Variables.'''\n-      ).format(name=self.name, variable_str=variable_str)\n-      raise ValueError(error_str)\n-\n-    untracked_used_vars = [\n-        v for v in accessed_variables if v.ref() not in tracked_weights\n-    ]\n-    if untracked_used_vars and not self._already_warned:\n-      variable_str = '\\n'.join('  {}'.format(i) for i in untracked_used_vars)\n-      self._warn(textwrap.dedent(\n-          '''\n-          The following Variables were used a Lambda layer's call ({name}), but\n-          are not present in its tracked objects:\n-          {variable_str}\n-          It is possible that this is intended behavior, but it is more likely\n-          an omission. This is a strong indication that this layer should be\n-          formulated as a subclassed Layer rather than a Lambda layer.'''\n-      ).format(name=self.name, variable_str=variable_str))\n-      self._already_warned = True\n-\n-  def _warn(self, msg):\n-    # This method will be overridden in a unit test to raise an error, because\n-    # self.assertWarns is not universally implemented.\n-    return tf_logging.warning(msg)\n-\n-  def get_config(self):\n-    if not self.symbol:\n-      raise ValueError('This Keras op layer was generated from %s, a method '\n-                       'that is not an exposed in the TensorFlow API. This '\n-                       'may have happened if the method was explicitly '\n-                       'decorated to add dispatching support, and it was used '\n-                       'during Functional model construction. '\n-                       'To ensure cross-version compatibility of Keras models '\n-                       'that use op layers, only op layers produced from '\n-                       'exported TF API symbols can be serialized.'\n-                       % self.function)\n-    config = {\n-        'function': self.symbol\n-    }\n-\n-    base_config = super(TFOpLambda, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    config = config.copy()\n-    symbol_name = config['function']\n-    function = get_symbol_from_name(symbol_name)\n-    if not function:\n-      raise ValueError(\n-          'TF symbol `tf.%s` could not be found.' % symbol_name)\n-\n-    config['function'] = function\n-\n-    return cls(**config)\n-\n-\n-class KerasOpDispatcher(tf.__internal__.dispatch.GlobalOpDispatcher):\n-  \"\"\"A global dispatcher that allows building a functional model with TF Ops.\"\"\"\n-\n-  def handle(self, op, args, kwargs):\n-    \"\"\"Handle the specified operation with the specified arguments.\"\"\"\n-    if any(\n-        isinstance(x, keras_tensor.KerasTensor)\n-        for x in tf.nest.flatten([args, kwargs])):\n-      return TFOpLambda(op)(*args, **kwargs)\n-    else:\n-      return self.NOT_SUPPORTED\n-\n-KerasOpDispatcher().register()\n-\n-\n-def _slice_to_dict(x):\n-  if isinstance(x, slice):\n-    return {'start': x.start, 'stop': x.stop, 'step': x.step}\n-  return x\n-\n-\n-def _dict_to_slice(x):\n-  if isinstance(x, dict):\n-    return slice(x['start'], x['stop'], x['step'])\n-  return x\n-\n-\n-class SlicingOpLambda(TFOpLambda):\n-  \"\"\"Wraps TF API symbols in a `Layer` object.\n-\n-  It is inserted by the Functional API construction whenever users call\n-  a supported TF symbol on KerasTensors.\n-\n-  Like Lambda layers, this layer tries to raise warnings when it detects users\n-  explicitly use variables in the call. (To let them know\n-  that the layer will not capture the variables).\n-\n-  This is useful in the case where users do something like:\n-  x = keras.Input(...)\n-  y = tf.Variable(...)\n-  out = x * tf_variable\n-  \"\"\"\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def __init__(self, function, **kwargs):\n-    super(SlicingOpLambda, self).__init__(function, **kwargs)\n-\n-    original_call = self.call\n-    # Decorate the function to produce this layer's call method\n-    def _call_wrapper(*args, **kwargs):\n-      # Turn any slice dicts in the args back into `slice` objects.\n-      # This conversion cannot use nest.flatten/map_structure,\n-      # because dicts are flattened by nest while slices aren't.\n-      # So, map_structure would only see the individual elements in the\n-      # dict.\n-      # This can't use map_structure_up_to either because the 'shallowness' of\n-      # the shallow tree would have to vary depending on if only one dim or\n-      # multiple are being sliced.\n-      new_args = []\n-      for arg in args:\n-        arg = _dict_to_slice(arg)\n-        if isinstance(arg, (list, tuple)):\n-          new_arg = []\n-          for sub_arg in arg:\n-            new_arg.append(_dict_to_slice(sub_arg))\n-          arg = new_arg\n-        new_args.append(arg)\n-\n-      # Handle the kwargs too.\n-      new_kwargs = {}\n-      for key, value in kwargs.items():\n-        value = _dict_to_slice(value)\n-        if isinstance(value, (list, tuple)):\n-          new_value = []\n-          for v in value:\n-            new_value.append(_dict_to_slice(v))\n-          value = new_value\n-        new_kwargs[key] = value\n-\n-      return original_call(*new_args, **new_kwargs)\n-    self.call = tf.__internal__.decorator.make_decorator(original_call, _call_wrapper)\n-\n-\n-class TFSlicingOpDispatcher(tf.__internal__.dispatch.OpDispatcher):\n-  \"\"\"A global dispatcher that allows building a functional model with TF Ops.\"\"\"\n-\n-  def __init__(self, op):\n-    self.op = op\n-\n-  def handle(self, args, kwargs):\n-    \"\"\"Handle the specified operation with the specified arguments.\"\"\"\n-    args = tf.nest.map_structure(_slice_to_dict, args)\n-    kwargs = tf.nest.map_structure(_slice_to_dict, kwargs)\n-    if any(\n-        isinstance(x, keras_tensor.KerasTensor)\n-        for x in tf.nest.flatten([args, kwargs])):\n-      return SlicingOpLambda(self.op)(*args, **kwargs)\n-    else:\n-      return self.NOT_SUPPORTED\n-\n-for slicing_op in [\n-    tf.__operators__.getitem,  # pylint: disable=protected-access\n-    tf.compat.v1.boolean_mask,\n-    tf.boolean_mask,\n-    tf.__operators__.ragged_getitem\n-]:\n-  TFSlicingOpDispatcher(slicing_op).register(slicing_op)\n-\n-\n-class InstanceProperty(Layer):\n-  \"\"\"Wraps an instance property access (e.g. `x.foo`) in a Keras Layer.\n-\n-  This layer takes an attribute name `attr_name` in the constructor and,\n-  when called on input tensor `obj` returns `obj.attr_name`.\n-\n-  KerasTensors specialized for specific extension types use it to\n-  represent instance property accesses on the represented object in the\n-  case where the property needs to be dynamically accessed as opposed to\n-  being statically computed from the typespec, e.g.\n-\n-  x = keras.Input(..., ragged=True)\n-  out = x.flat_values\n-  \"\"\"\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def __init__(self, attr_name, **kwargs):\n-    self.attr_name = attr_name\n-\n-    if 'name' not in kwargs:\n-      kwargs['name'] = K.unique_object_name(\n-          'input.' + self.attr_name, zero_based=True, avoid_observed_names=True)\n-    kwargs['autocast'] = False\n-\n-    # Do not individually trace op layers in the SavedModel.\n-    self._must_restore_from_config = True\n-\n-    super(InstanceProperty, self).__init__(**kwargs)\n-\n-    # Preserve all argument data structures when saving/loading a config\n-    # (e.g., don't unnest lists that contain one element)\n-    self._preserve_input_structure_in_config = True\n-\n-  def call(self, obj):\n-    return getattr(obj, self.attr_name)\n-\n-  def get_config(self):\n-    config = {\n-        'attr_name': self.attr_name\n-    }\n-    base_config = super(InstanceProperty, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    return cls(**config)\n-\n-\n-class InstanceMethod(InstanceProperty):\n-  \"\"\"Wraps an instance method access (e.g. `x.foo(arg)` in a Keras Layer.\n-\n-  This layer takes an attribute name `attr_name` in the constructor and,\n-  when called on input tensor `obj` with additional arguments `args` and\n-  `kwargs` returns `obj.attr_name(*args, **kwargs)`.\n-\n-  KerasTensors specialized for specific extension types use it to\n-  represent dynamic instance method calls on the represented object, e.g.\n-\n-  x = keras.Input(..., ragged=True)\n-  new_values = keras.Input(...)\n-  out = x.with_values(new_values)\n-  \"\"\"\n-\n-  def call(self, obj, args, kwargs):\n-    method = getattr(obj, self.attr_name)\n-    return method(*args, **kwargs)\n-\n-\n-def _delegate_property(keras_tensor_cls, property_name):  # pylint: disable=invalid-name\n-  \"\"\"Register property on a KerasTensor class.\n-\n-  Calling this multiple times with the same arguments should be a no-op.\n-\n-  This method exposes a property on the KerasTensor class that will use an\n-  `InstanceProperty` layer to access the property on the represented\n-  intermediate values in the model.\n-\n-  Args:\n-    keras_tensor_cls: The KerasTensor subclass that should expose the property.\n-    property_name: The name of the property to expose and delegate to the\n-      represented (Composite)Tensor.\n-  \"\"\"\n-  # We use a lambda because we can't create a Keras layer at import time\n-  # due to dynamic layer class versioning.\n-  property_access = property(lambda self: InstanceProperty(property_name)(self))  # pylint: disable=unnecessary-lambda\n-  setattr(keras_tensor_cls, property_name, property_access)\n-\n-\n-def _delegate_method(keras_tensor_cls, method_name):  # pylint: disable=invalid-name\n-  \"\"\"Register method on a KerasTensor class.\n-\n-  Calling this function times with the same arguments should be a no-op.\n-\n-  This method exposes an instance method on the KerasTensor class that will use\n-  an `InstanceMethod` layer to run the desired method on the represented\n-  intermediate values in the model.\n-\n-  Args:\n-    keras_tensor_cls: The KerasTensor subclass that should expose the property.\n-    method_name: The name of the method to expose and delegate to the\n-      represented (Composite)Tensor.\n-  \"\"\"\n-  def delegate(self, *args, **kwargs):\n-    return InstanceMethod(method_name)(self, args, kwargs)\n-  setattr(keras_tensor_cls, method_name, delegate)\n-\n-# We do not support the `uniform_row_length` property because it\n-# returns either `None` or an int tensor, and code that relies on it tends\n-# to check `is None` directly. Delegating it here would always return a\n-# `KerasTensor`, regardless of what can be statically inferred. This would\n-# never equal `None`, breaking code that expects it to be partially-static\n-# in unpredictable ways.\n-for ragged_property in [\n-    'values',\n-    'flat_values',\n-    'row_splits',\n-    'nested_row_splits'\n-]:\n-  _delegate_property(keras_tensor.RaggedKerasTensor, ragged_property)\n-\n-for ragged_method_name in [\n-    'value_rowids',\n-    'nested_value_rowids',\n-    'nrows',\n-    'row_starts',\n-    'row_limits',\n-    'row_lengths',\n-    'nested_row_lengths',\n-    'bounding_shape',\n-    'with_values',\n-    'with_flat_values',\n-    'with_row_splits_dtype',\n-    'merge_dims',\n-    'to_tensor',\n-    'to_sparse',\n-]:\n-  _delegate_method(keras_tensor.RaggedKerasTensor, ragged_method_name)\n-\n-for sparse_property in [\n-    'indices',\n-    'values',\n-]:\n-  _delegate_property(keras_tensor.SparseKerasTensor, sparse_property)\n-\n-for sparse_method in [\n-    'with_values',\n-]:\n-  _delegate_method(keras_tensor.SparseKerasTensor, sparse_method)\n-\n-\n-class ClassMethod(Layer):\n-  \"\"\"Wraps a TF API Class's class method  in a `Layer` object.\n-\n-  It is inserted by the Functional API construction whenever users call\n-  a supported TF Class's class method on KerasTensors.\n-\n-  This is useful in the case where users do something like:\n-  x = keras.Input(...)\n-  y = keras.Input(...)\n-  out = tf.RaggedTensor.from_row_splits(x, y)\n-  \"\"\"\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def __init__(self, cls_ref, method_name, **kwargs):\n-    self.cls_ref = cls_ref\n-    self.method_name = method_name\n-    self.cls_symbol = (\n-        get_canonical_name_for_symbol(self.cls_ref,\n-                                      add_prefix_to_v1_names=True) or\n-        get_canonical_name_for_symbol(self.cls_ref,\n-                                      api_name='keras',\n-                                      add_prefix_to_v1_names=True))\n-    if 'name' not in kwargs:\n-      kwargs['name'] = K.unique_object_name(\n-          'tf.' + self.cls_symbol + '.' + self.method_name, zero_based=True,\n-          avoid_observed_names=True)\n-    kwargs['autocast'] = False\n-\n-    # Do not individually trace op layers in the SavedModel.\n-    self._must_restore_from_config = True\n-\n-    super(ClassMethod, self).__init__(**kwargs)\n-\n-    # Preserve all argument data structures when saving/loading a config\n-    # (e.g., don't unnest lists that contain one element)\n-    self._preserve_input_structure_in_config = True\n-\n-    self._expects_training_arg = False\n-    self._expects_mask_arg = False\n-\n-  def call(self, args, kwargs):\n-    return getattr(self.cls_ref, self.method_name)(*args, **kwargs)\n-\n-  def get_config(self):\n-    if not self.cls_symbol:\n-      raise ValueError('This Keras class method conversion tried to convert '\n-                       'a method belonging to class %s, a class '\n-                       'that is not an exposed in the TensorFlow API. '\n-                       'To ensure cross-version compatibility of Keras models '\n-                       'that use op layers, only op layers produced from '\n-                       'exported TF API symbols can be serialized.'\n-                       % self.cls_symbol)\n-    config = {\n-        'cls_symbol': self.cls_symbol,\n-        'method_name': self.method_name\n-    }\n-\n-    base_config = super(ClassMethod, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    config = config.copy()\n-    symbol_name = config.pop('cls_symbol')\n-    cls_ref = get_symbol_from_name(symbol_name)\n-    if not cls_ref:\n-      raise ValueError(\n-          'TF symbol `tf.%s` could not be found.' % symbol_name)\n-\n-    config['cls_ref'] = cls_ref\n-\n-    return cls(**config)\n-\n-\n-class TFClassMethodDispatcher(tf.__internal__.dispatch.OpDispatcher):\n-  \"\"\"A class method dispatcher that allows building a functional model with TF class methods.\"\"\"\n-\n-  def __init__(self, cls, method_name):\n-    self.cls = cls\n-    self.method_name = method_name\n-\n-  def handle(self, args, kwargs):\n-    \"\"\"Handle the specified operation with the specified arguments.\"\"\"\n-    if any(\n-        isinstance(x, keras_tensor.KerasTensor)\n-        for x in tf.nest.flatten([args, kwargs])):\n-      return ClassMethod(self.cls, self.method_name)(args[1:], kwargs)\n-    else:\n-      return self.NOT_SUPPORTED\n-\n-for ragged_class_method in [\n-    'from_value_rowids',\n-    'from_row_splits',\n-    'from_row_lengths',\n-    'from_row_starts',\n-    'from_row_limits',\n-    'from_uniform_row_length',\n-    'from_nested_value_rowids',\n-    'from_nested_row_splits',\n-    'from_nested_row_lengths',\n-    'from_tensor',\n-    'from_sparse',\n-]:\n-  TFClassMethodDispatcher(\n-      tf.RaggedTensor, ragged_class_method).register(\n-          getattr(tf.RaggedTensor, ragged_class_method))\n\n@@ -0,0 +1,34 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Core Keras layers.\"\"\"\n+\n+from keras.layers.core.activation import Activation\n+from keras.layers.core.activity_regularization import ActivityRegularization\n+from keras.layers.core.dense import Dense\n+from keras.layers.core.dropout import Dropout\n+from keras.layers.core.flatten import Flatten\n+from keras.layers.core.lambda_layer import Lambda\n+from keras.layers.core.masking import Masking\n+from keras.layers.core.permute import Permute\n+from keras.layers.core.repeat_vector import RepeatVector\n+from keras.layers.core.reshape import Reshape\n+from keras.layers.core.spatial_dropout import SpatialDropout1D\n+from keras.layers.core.spatial_dropout import SpatialDropout2D\n+from keras.layers.core.spatial_dropout import SpatialDropout3D\n+from keras.layers.core.tf_op_layer import ClassMethod\n+from keras.layers.core.tf_op_layer import InstanceMethod\n+from keras.layers.core.tf_op_layer import InstanceProperty\n+from keras.layers.core.tf_op_layer import SlicingOpLambda\n+from keras.layers.core.tf_op_layer import TFOpLambda\n\n@@ -0,0 +1,66 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the Activation layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras.engine.base_layer import Layer\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Activation')\n+class Activation(Layer):\n+  \"\"\"Applies an activation function to an output.\n+\n+  Args:\n+    activation: Activation function, such as `tf.nn.relu`, or string name of\n+      built-in activation function, such as \"relu\".\n+\n+  Usage:\n+\n+  >>> layer = tf.keras.layers.Activation('relu')\n+  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n+  >>> list(output.numpy())\n+  [0.0, 0.0, 0.0, 2.0]\n+  >>> layer = tf.keras.layers.Activation(tf.nn.relu)\n+  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n+  >>> list(output.numpy())\n+  [0.0, 0.0, 0.0, 2.0]\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the batch axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as input.\n+  \"\"\"\n+\n+  def __init__(self, activation, **kwargs):\n+    super(Activation, self).__init__(**kwargs)\n+    self.supports_masking = True\n+    self.activation = activations.get(activation)\n+\n+  def call(self, inputs):\n+    return self.activation(inputs)\n+\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n+\n+  def get_config(self):\n+    config = {'activation': activations.serialize(self.activation)}\n+    base_config = super(Activation, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n\n@@ -0,0 +1,53 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the ActivityRegularization layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import regularizers\n+from keras.engine.base_layer import Layer\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.ActivityRegularization')\n+class ActivityRegularization(Layer):\n+  \"\"\"Layer that applies an update to the cost function based input activity.\n+\n+  Args:\n+    l1: L1 regularization factor (positive float).\n+    l2: L2 regularization factor (positive float).\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the samples axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as input.\n+  \"\"\"\n+\n+  def __init__(self, l1=0., l2=0., **kwargs):\n+    super(ActivityRegularization, self).__init__(\n+        activity_regularizer=regularizers.L1L2(l1=l1, l2=l2), **kwargs)\n+    self.supports_masking = True\n+    self.l1 = l1\n+    self.l2 = l2\n+\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n+\n+  def get_config(self):\n+    config = {'l1': self.l1, 'l2': self.l2}\n+    base_config = super(ActivityRegularization, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -13,18 +13,17 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Tests for Keras core layers.\"\"\"\n-\n+# pylint: disable=g-bad-import-order\n import tensorflow.compat.v2 as tf\n \n import textwrap\n \n-import numpy as np\n-\n import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers import core\n from keras.mixed_precision import policy\n+import numpy as np\n \n \n @keras_parameterized.run_all_keras_modes\n@@ -379,7 +378,7 @@ class CoreLayersTest(keras_parameterized.TestCase):\n     x = np.ones((10, 10))\n     y = keras.layers.Masking(1.)(x)\n     self.assertTrue(hasattr(y, '_keras_mask'))\n-    self.assertTrue(y._keras_mask is not None)\n+    self.assertIsNotNone(y._keras_mask)\n     self.assertAllClose(self.evaluate(y._keras_mask), np.zeros((10,)))\n \n   def test_compute_mask_with_positional_mask_arg(self):\n\n@@ -0,0 +1,239 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the Dense layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import backend as K\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Dense')\n+class Dense(Layer):\n+  \"\"\"Just your regular densely-connected NN layer.\n+\n+  `Dense` implements the operation:\n+  `output = activation(dot(input, kernel) + bias)`\n+  where `activation` is the element-wise activation function\n+  passed as the `activation` argument, `kernel` is a weights matrix\n+  created by the layer, and `bias` is a bias vector created by the layer\n+  (only applicable if `use_bias` is `True`). These are all attributes of\n+  `Dense`.\n+\n+  Note: If the input to the layer has a rank greater than 2, then `Dense`\n+  computes the dot product between the `inputs` and the `kernel` along the\n+  last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n+  For example, if input has dimensions `(batch_size, d0, d1)`,\n+  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n+  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n+  (there are `batch_size * d0` such sub-tensors).\n+  The output in this case will have shape `(batch_size, d0, units)`.\n+\n+  Besides, layer attributes cannot be modified after the layer has been called\n+  once (except the `trainable` attribute).\n+  When a popular kwarg `input_shape` is passed, then keras will create\n+  an input layer to insert before the current layer. This can be treated\n+  equivalent to explicitly defining an `InputLayer`.\n+\n+  Example:\n+\n+  >>> # Create a `Sequential` model and add a Dense layer as the first layer.\n+  >>> model = tf.keras.models.Sequential()\n+  >>> model.add(tf.keras.Input(shape=(16,)))\n+  >>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n+  >>> # Now the model will take as input arrays of shape (None, 16)\n+  >>> # and output arrays of shape (None, 32).\n+  >>> # Note that after the first layer, you don't need to specify\n+  >>> # the size of the input anymore:\n+  >>> model.add(tf.keras.layers.Dense(32))\n+  >>> model.output_shape\n+  (None, 32)\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use.\n+      If you don't specify anything, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix.\n+    bias_initializer: Initializer for the bias vector.\n+    kernel_regularizer: Regularizer function applied to\n+      the `kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    activity_regularizer: Regularizer function applied to\n+      the output of the layer (its \"activation\").\n+    kernel_constraint: Constraint function applied to\n+      the `kernel` weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+\n+  Input shape:\n+    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n+    The most common situation would be\n+    a 2D input with shape `(batch_size, input_dim)`.\n+\n+  Output shape:\n+    N-D tensor with shape: `(batch_size, ..., units)`.\n+    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n+    the output would have shape `(batch_size, units)`.\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation=None,\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(Dense, self).__init__(\n+        activity_regularizer=activity_regularizer, **kwargs)\n+\n+    self.units = int(units) if not isinstance(units, int) else units\n+    if self.units < 0:\n+      raise ValueError(f'Received an invalid value for `units`, expected '\n+                       f'a positive integer, got {units}.')\n+    self.activation = activations.get(activation)\n+    self.use_bias = use_bias\n+    self.kernel_initializer = initializers.get(kernel_initializer)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+    self.bias_regularizer = regularizers.get(bias_regularizer)\n+    self.kernel_constraint = constraints.get(kernel_constraint)\n+    self.bias_constraint = constraints.get(bias_constraint)\n+\n+    self.input_spec = InputSpec(min_ndim=2)\n+    self.supports_masking = True\n+\n+  def build(self, input_shape):\n+    dtype = tf.as_dtype(self.dtype or K.floatx())\n+    if not (dtype.is_floating or dtype.is_complex):\n+      raise TypeError('Unable to build `Dense` layer with non-floating point '\n+                      'dtype %s' % (dtype,))\n+\n+    input_shape = tf.TensorShape(input_shape)\n+    last_dim = tf.compat.dimension_value(input_shape[-1])\n+    if last_dim is None:\n+      raise ValueError('The last dimension of the inputs to `Dense` '\n+                       'should be defined. Found `None`.')\n+    self.input_spec = InputSpec(min_ndim=2, axes={-1: last_dim})\n+    self.kernel = self.add_weight(\n+        'kernel',\n+        shape=[last_dim, self.units],\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint,\n+        dtype=self.dtype,\n+        trainable=True)\n+    if self.use_bias:\n+      self.bias = self.add_weight(\n+          'bias',\n+          shape=[self.units,],\n+          initializer=self.bias_initializer,\n+          regularizer=self.bias_regularizer,\n+          constraint=self.bias_constraint,\n+          dtype=self.dtype,\n+          trainable=True)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def call(self, inputs):\n+    if inputs.dtype.base_dtype != self._compute_dtype_object.base_dtype:\n+      inputs = tf.cast(inputs, dtype=self._compute_dtype_object)\n+\n+    if isinstance(inputs, tf.RaggedTensor) and inputs.shape[-1] is not None:\n+      # In case we encounter a RaggedTensor with a fixed last dimension (last\n+      # dimension not ragged), we can map the call method to the flat values.\n+      return tf.ragged.map_flat_values(self.call, inputs)\n+\n+    rank = inputs.shape.rank\n+    if rank == 2 or rank is None:\n+      # We use embedding_lookup_sparse as a more efficient matmul operation for\n+      # large sparse input tensors. The op will result in a sparse gradient, as\n+      # opposed to sparse_ops.sparse_tensor_dense_matmul which results in dense\n+      # gradients. This can lead to sigfinicant speedups, see b/171762937.\n+      if isinstance(inputs, tf.SparseTensor):\n+        # We need to fill empty rows, as the op assumes at least one id per row.\n+        inputs, _ = tf.sparse.fill_empty_rows(inputs, 0)\n+        # We need to do some munging of our input to use the embedding lookup as\n+        # a matrix multiply. We split our input matrix into separate ids and\n+        # weights tensors. The values of the ids tensor should be the column\n+        # indices of our input matrix and the values of the weights tensor\n+        # can continue to the actual matrix weights.\n+        # The column arrangement of ids and weights\n+        # will be summed over and does not matter. See the documentation for\n+        # sparse_ops.sparse_tensor_dense_matmul a more detailed explanation\n+        # of the inputs to both ops.\n+        ids = tf.SparseTensor(\n+            indices=inputs.indices,\n+            values=inputs.indices[:, 1],\n+            dense_shape=inputs.dense_shape)\n+        weights = inputs\n+        outputs = tf.nn.embedding_lookup_sparse(\n+            self.kernel, ids, weights, combiner='sum')\n+      else:\n+        outputs = tf.raw_ops.MatMul(a=inputs, b=self.kernel)\n+    # Broadcast kernel to inputs.\n+    else:\n+      outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])\n+      # Reshape the output back to the original ndim of the input.\n+      if not tf.executing_eagerly():\n+        shape = inputs.shape.as_list()\n+        output_shape = shape[:-1] + [self.kernel.shape[-1]]\n+        outputs.set_shape(output_shape)\n+\n+    if self.use_bias:\n+      outputs = tf.nn.bias_add(outputs, self.bias)\n+\n+    if self.activation is not None:\n+      outputs = self.activation(outputs)\n+    return outputs\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape)\n+    input_shape = input_shape.with_rank_at_least(2)\n+    if tf.compat.dimension_value(input_shape[-1]) is None:\n+      raise ValueError(\n+          'The innermost dimension of input_shape must be defined, but saw: %s'\n+          % (input_shape,))\n+    return input_shape[:-1].concatenate(self.units)\n+\n+  def get_config(self):\n+    config = super(Dense, self).get_config()\n+    config.update({\n+        'units': self.units,\n+        'activation': activations.serialize(self.activation),\n+        'use_bias': self.use_bias,\n+        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+        'bias_initializer': initializers.serialize(self.bias_initializer),\n+        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+        'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+        'activity_regularizer':\n+            regularizers.serialize(self.activity_regularizer),\n+        'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+        'bias_constraint': constraints.serialize(self.bias_constraint)\n+    })\n+    return config\n\n@@ -0,0 +1,137 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the dropout layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend as K\n+from keras.engine.base_layer import Layer\n+from keras.utils import control_flow_util\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.util.tf_export import keras_export\n+\n+# TODO(b/168039935): track dropout rate to decide whether/how to make a\n+# dropout rate fastpath.\n+keras_temporary_dropout_rate = tf.__internal__.monitoring.BoolGauge(\n+    '/tensorflow/api/keras/dropout/temp_rate_is_zero',\n+    'Temporarily record if Keras dropout layer was created w/'\n+    'constant rate = 0')\n+\n+\n+@keras_export('keras.layers.Dropout')\n+class Dropout(Layer):\n+  \"\"\"Applies Dropout to the input.\n+\n+  The Dropout layer randomly sets input units to 0 with a frequency of `rate`\n+  at each step during training time, which helps prevent overfitting.\n+  Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over\n+  all inputs is unchanged.\n+\n+  Note that the Dropout layer only applies when `training` is set to True\n+  such that no values are dropped during inference. When using `model.fit`,\n+  `training` will be appropriately set to True automatically, and in other\n+  contexts, you can set the kwarg explicitly to True when calling the layer.\n+\n+  (This is in contrast to setting `trainable=False` for a Dropout layer.\n+  `trainable` does not affect the layer's behavior, as Dropout does\n+  not have any variables/weights that can be frozen during training.)\n+\n+  >>> tf.random.set_seed(0)\n+  >>> layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n+  >>> data = np.arange(10).reshape(5, 2).astype(np.float32)\n+  >>> print(data)\n+  [[0. 1.]\n+   [2. 3.]\n+   [4. 5.]\n+   [6. 7.]\n+   [8. 9.]]\n+  >>> outputs = layer(data, training=True)\n+  >>> print(outputs)\n+  tf.Tensor(\n+  [[ 0.    1.25]\n+   [ 2.5   3.75]\n+   [ 5.    6.25]\n+   [ 7.5   8.75]\n+   [10.    0.  ]], shape=(5, 2), dtype=float32)\n+\n+  Args:\n+    rate: Float between 0 and 1. Fraction of the input units to drop.\n+    noise_shape: 1D integer tensor representing the shape of the\n+      binary dropout mask that will be multiplied with the input.\n+      For instance, if your inputs have shape\n+      `(batch_size, timesteps, features)` and\n+      you want the dropout mask to be the same for all timesteps,\n+      you can use `noise_shape=(batch_size, 1, features)`.\n+    seed: A Python integer to use as random seed.\n+\n+  Call arguments:\n+    inputs: Input tensor (of any rank).\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode (adding dropout) or in inference mode (doing nothing).\n+  \"\"\"\n+\n+  def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n+    super(Dropout, self).__init__(**kwargs)\n+    if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n+      raise ValueError(f'Invalid value {rate} received for '\n+                       f'`rate`, expected a value between 0 and 1.')\n+    self.rate = rate\n+    if isinstance(rate, (int, float)) and not rate:\n+      keras_temporary_dropout_rate.get_cell().set(True)\n+    else:\n+      keras_temporary_dropout_rate.get_cell().set(False)\n+    self.noise_shape = noise_shape\n+    self.seed = seed\n+    self.supports_masking = True\n+\n+  def _get_noise_shape(self, inputs):\n+    # Subclasses of `Dropout` may implement `_get_noise_shape(self, inputs)`,\n+    # which will override `self.noise_shape`, and allows for custom noise\n+    # shapes with dynamically sized inputs.\n+    if self.noise_shape is None:\n+      return None\n+\n+    concrete_inputs_shape = tf.shape(inputs)\n+    noise_shape = []\n+    for i, value in enumerate(self.noise_shape):\n+      noise_shape.append(concrete_inputs_shape[i] if value is None else value)\n+    return tf.convert_to_tensor(noise_shape)\n+\n+  def call(self, inputs, training=None):\n+    if training is None:\n+      training = K.learning_phase()\n+\n+    def dropped_inputs():\n+      return tf.nn.dropout(\n+          inputs,\n+          noise_shape=self._get_noise_shape(inputs),\n+          seed=self.seed,\n+          rate=self.rate)\n+\n+    output = control_flow_util.smart_cond(training, dropped_inputs,\n+                                          lambda: tf.identity(inputs))\n+    return output\n+\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n+\n+  def get_config(self):\n+    config = {\n+        'rate': self.rate,\n+        'noise_shape': self.noise_shape,\n+        'seed': self.seed\n+    }\n+    base_config = super(Dropout, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n\n@@ -0,0 +1,113 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the flatten layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import functools\n+import operator\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Flatten')\n+class Flatten(Layer):\n+  \"\"\"Flattens the input. Does not affect the batch size.\n+\n+  Note: If inputs are shaped `(batch,)` without a feature axis, then\n+  flattening adds an extra channel dimension and output shape is `(batch, 1)`.\n+\n+  Args:\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, ..., channels)` while `channels_first` corresponds to\n+      inputs with shape `(batch, channels, ...)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+\n+  Example:\n+\n+  >>> model = tf.keras.Sequential()\n+  >>> model.add(tf.keras.layers.Conv2D(64, 3, 3, input_shape=(3, 32, 32)))\n+  >>> model.output_shape\n+  (None, 1, 10, 64)\n+\n+  >>> model.add(Flatten())\n+  >>> model.output_shape\n+  (None, 640)\n+\n+  \"\"\"\n+\n+  def __init__(self, data_format=None, **kwargs):\n+    super(Flatten, self).__init__(**kwargs)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.input_spec = InputSpec(min_ndim=1)\n+    self._channels_first = self.data_format == 'channels_first'\n+\n+  def call(self, inputs):\n+    if self._channels_first:\n+      rank = inputs.shape.rank\n+      if rank and rank > 1:\n+        # Switch to channels-last format.\n+        permutation = [0]\n+        permutation.extend(range(2, rank))\n+        permutation.append(1)\n+        inputs = tf.transpose(inputs, perm=permutation)\n+\n+    if tf.executing_eagerly():\n+      # Full static shape is guaranteed to be available.\n+      # Performance: Using `constant_op` is much faster than passing a list.\n+      flattened_shape = tf.constant([inputs.shape[0], -1])\n+      return tf.reshape(inputs, flattened_shape)\n+    else:\n+      input_shape = inputs.shape\n+      rank = input_shape.rank\n+      if rank == 1:\n+        return tf.expand_dims(inputs, axis=1)\n+      else:\n+        batch_dim = tf.compat.dimension_value(input_shape[0])\n+        non_batch_dims = input_shape[1:]\n+        # Reshape in a way that preserves as much shape info as possible.\n+        if non_batch_dims.is_fully_defined():\n+          last_dim = int(functools.reduce(operator.mul, non_batch_dims))\n+          flattened_shape = tf.constant([-1, last_dim])\n+        elif batch_dim is not None:\n+          flattened_shape = tf.constant([int(batch_dim), -1])\n+        else:\n+          flattened_shape = [tf.shape(inputs)[0], -1]\n+        return tf.reshape(inputs, flattened_shape)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if not input_shape:\n+      output_shape = tf.TensorShape([1])\n+    else:\n+      output_shape = [input_shape[0]]\n+    if np.all(input_shape[1:]):\n+      output_shape += [np.prod(input_shape[1:], dtype=int)]\n+    else:\n+      output_shape += [None]\n+    return tf.TensorShape(output_shape)\n+\n+  def get_config(self):\n+    config = super(Flatten, self).get_config()\n+    config.update({'data_format': self.data_format})\n+    return config\n\n@@ -0,0 +1,355 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the Lambda layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+import sys\n+import textwrap\n+import types as python_types\n+import warnings\n+from keras.engine.base_layer import Layer\n+from keras.utils import generic_utils\n+from keras.utils import tf_inspect\n+from keras.utils import tf_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.platform import tf_logging\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Lambda')\n+class Lambda(Layer):\n+  \"\"\"Wraps arbitrary expressions as a `Layer` object.\n+\n+  The `Lambda` layer exists so that arbitrary expressions can be used\n+  as a `Layer` when constructing `Sequential`\n+  and Functional API models. `Lambda` layers are best suited for simple\n+  operations or quick experimentation. For more advanced use cases, follow\n+  [this guide](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n+  for subclassing `tf.keras.layers.Layer`.\n+\n+  WARNING: `tf.keras.layers.Lambda` layers have (de)serialization limitations!\n+\n+  The main reason to subclass `tf.keras.layers.Layer` instead of using a\n+  `Lambda` layer is saving and inspecting a Model. `Lambda` layers\n+  are saved by serializing the Python bytecode, which is fundamentally\n+  non-portable. They should only be loaded in the same environment where\n+  they were saved. Subclassed layers can be saved in a more portable way\n+  by overriding their `get_config` method. Models that rely on\n+  subclassed Layers are also often easier to visualize and reason about.\n+\n+  Examples:\n+\n+  ```python\n+  # add a x -> x^2 layer\n+  model.add(Lambda(lambda x: x ** 2))\n+  ```\n+  ```python\n+  # add a layer that returns the concatenation\n+  # of the positive part of the input and\n+  # the opposite of the negative part\n+\n+  def antirectifier(x):\n+      x -= K.mean(x, axis=1, keepdims=True)\n+      x = K.l2_normalize(x, axis=1)\n+      pos = K.relu(x)\n+      neg = K.relu(-x)\n+      return K.concatenate([pos, neg], axis=1)\n+\n+  model.add(Lambda(antirectifier))\n+  ```\n+\n+  Variables:\n+    While it is possible to use Variables with Lambda layers, this practice is\n+    discouraged as it can easily lead to bugs. For instance, consider the\n+    following layer:\n+\n+    ```python\n+      scale = tf.Variable(1.)\n+      scale_layer = tf.keras.layers.Lambda(lambda x: x * scale)\n+    ```\n+\n+    Because scale_layer does not directly track the `scale` variable, it will\n+    not appear in `scale_layer.trainable_weights` and will therefore not be\n+    trained if `scale_layer` is used in a Model.\n+\n+    A better pattern is to write a subclassed Layer:\n+\n+    ```python\n+      class ScaleLayer(tf.keras.layers.Layer):\n+        def __init__(self):\n+          super(ScaleLayer, self).__init__()\n+          self.scale = tf.Variable(1.)\n+\n+        def call(self, inputs):\n+          return inputs * self.scale\n+    ```\n+\n+    In general, Lambda layers can be convenient for simple stateless\n+    computation, but anything more complex should use a subclass Layer instead.\n+\n+  Args:\n+    function: The function to be evaluated. Takes input tensor as first\n+      argument.\n+    output_shape: Expected output shape from function. This argument can be\n+      inferred if not explicitly provided. Can be a tuple or function. If a\n+      tuple, it only specifies the first dimension onward;\n+      sample dimension is assumed either the same as the input: `output_shape =\n+        (input_shape[0], ) + output_shape` or, the input is `None` and\n+      the sample dimension is also `None`: `output_shape = (None, ) +\n+        output_shape` If a function, it specifies the entire shape as a function\n+        of the\n+      input shape: `output_shape = f(input_shape)`\n+    mask: Either None (indicating no masking) or a callable with the same\n+      signature as the `compute_mask` layer method, or a tensor that will be\n+      returned as output mask regardless of what the input is.\n+    arguments: Optional dictionary of keyword arguments to be passed to the\n+      function.\n+  Input shape: Arbitrary. Use the keyword argument input_shape (tuple of\n+    integers, does not include the samples axis) when using this layer as the\n+    first layer in a model.\n+  Output shape: Specified by `output_shape` argument\n+  \"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self,\n+               function,\n+               output_shape=None,\n+               mask=None,\n+               arguments=None,\n+               **kwargs):\n+    super(Lambda, self).__init__(**kwargs)\n+\n+    self.arguments = arguments or {}\n+    self.function = function\n+\n+    if mask is not None:\n+      self.supports_masking = True\n+    self.mask = mask\n+    self._output_shape = output_shape\n+\n+    # Warning on every invocation will be quite irksome in Eager mode.\n+    self._already_warned = False\n+\n+    function_args = tf_inspect.getfullargspec(function).args\n+    self._fn_expects_training_arg = 'training' in function_args\n+    self._fn_expects_mask_arg = 'mask' in function_args\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    if self._output_shape is None:\n+      # Make use of existing autocomputation but provide Lambda-specific\n+      # error message. This is always safe to run even when the outer context\n+      # is Graph mode because Lambda layers don't have side effects such as\n+      # `add_loss`.\n+      with tf.__internal__.eager_context.eager_mode():\n+        try:\n+          return super(Lambda, self).compute_output_shape(input_shape)\n+        except NotImplementedError:\n+          raise NotImplementedError(\n+              'We could not automatically infer the shape of the Lambda\\'s '\n+              'output. Please specify `output_shape` for this Lambda.')\n+\n+    if callable(self._output_shape):\n+      output_shapes = self._output_shape(input_shape)\n+      return tf_utils.convert_shapes(output_shapes, to_tuples=False)\n+\n+    # Output shapes are passed directly and don't include batch dimension.\n+    input_tensor_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n+    batch_size = tf.nest.flatten(\n+        input_tensor_shape)[0][0] if input_shape else None\n+\n+    def _add_batch(shape):\n+      return tf.TensorShape([batch_size] + shape.as_list())\n+\n+    output_shapes = tf_utils.convert_shapes(self._output_shape, to_tuples=False)\n+    return tf.nest.map_structure(_add_batch, output_shapes)\n+\n+  def call(self, inputs, mask=None, training=None):\n+    # We must copy for thread safety, but it only needs to be a shallow copy.\n+    kwargs = {k: v for k, v in self.arguments.items()}\n+    if self._fn_expects_mask_arg:\n+      kwargs['mask'] = mask\n+    if self._fn_expects_training_arg:\n+      kwargs['training'] = training\n+\n+    created_variables = []\n+\n+    def _variable_creator(next_creator, **kwargs):\n+      var = next_creator(**kwargs)\n+      created_variables.append(var)\n+      return var\n+\n+    with tf.GradientTape(watch_accessed_variables=True) as tape,\\\n+        tf.variable_creator_scope(_variable_creator):\n+      result = self.function(inputs, **kwargs)\n+    self._check_variables(created_variables, tape.watched_variables())\n+    return result\n+\n+  def _check_variables(self, created_variables, accessed_variables):\n+    if not created_variables and not accessed_variables:\n+      # In the common case that a Lambda layer does not touch a Variable, we\n+      # don't want to incur the runtime cost of assembling any state used for\n+      # checking only to immediately discard it.\n+      return\n+\n+    tracked_weights = set(v.ref() for v in self.weights)\n+    untracked_new_vars = [\n+        v for v in created_variables if v.ref() not in tracked_weights\n+    ]\n+    if untracked_new_vars:\n+      variable_str = '\\n'.join('  {}'.format(i) for i in untracked_new_vars)\n+      error_str = textwrap.dedent(\"\"\"\n+          The following Variables were created within a Lambda layer ({name})\n+          but are not tracked by said layer:\n+          {variable_str}\n+          The layer cannot safely ensure proper Variable reuse across multiple\n+          calls, and consquently this behavior is disallowed for safety. Lambda\n+          layers are not well suited to stateful computation; instead, writing a\n+          subclassed Layer is the recommend way to define layers with\n+          Variables.\"\"\").format(\n+              name=self.name, variable_str=variable_str)\n+      raise ValueError(error_str)\n+\n+    untracked_used_vars = [\n+        v for v in accessed_variables if v.ref() not in tracked_weights\n+    ]\n+    if untracked_used_vars and not self._already_warned:\n+      variable_str = '\\n'.join('  {}'.format(i) for i in untracked_used_vars)\n+      self._warn(\n+          textwrap.dedent(\"\"\"\n+          The following Variables were used a Lambda layer's call ({name}), but\n+          are not present in its tracked objects:\n+          {variable_str}\n+          It is possible that this is intended behavior, but it is more likely\n+          an omission. This is a strong indication that this layer should be\n+          formulated as a subclassed Layer rather than a Lambda layer.\"\"\")\n+          .format(name=self.name, variable_str=variable_str))\n+      self._already_warned = True\n+\n+  def _warn(self, msg):\n+    # This method will be overridden in a unit test to raise an error, because\n+    # self.assertWarns is not universally implemented.\n+    return tf_logging.warning(msg)\n+\n+  def compute_mask(self, inputs, mask=None):\n+    if callable(self.mask):\n+      return self.mask(inputs, mask)\n+    return self.mask\n+\n+  def get_config(self):\n+    function_config = self._serialize_function_to_config(self.function)\n+    output_shape_config = self._serialize_function_to_config(\n+        self._output_shape, allow_raw=True)\n+    config = {\n+        'function': function_config[0],\n+        'function_type': function_config[1],\n+        'module': function_config[2],\n+        'output_shape': output_shape_config[0],\n+        'output_shape_type': output_shape_config[1],\n+        'output_shape_module': output_shape_config[2],\n+    }\n+    if self.mask is not None:\n+      mask_config = self._serialize_function_to_config(self.mask)\n+      config.update({\n+          'mask': mask_config[0],\n+          'mask_type': mask_config[1],\n+          'mask_module': mask_config[2]\n+      })\n+    config['arguments'] = self.arguments\n+\n+    base_config = super(Lambda, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  def _serialize_function_to_config(self, inputs, allow_raw=False):\n+    if isinstance(inputs, python_types.LambdaType):\n+      output = generic_utils.func_dump(inputs)\n+      output_type = 'lambda'\n+      module = inputs.__module__\n+    elif callable(inputs):\n+      output = inputs.__name__\n+      output_type = 'function'\n+      module = inputs.__module__\n+    elif allow_raw:\n+      output = inputs\n+      output_type = 'raw'\n+      module = None\n+    else:\n+      raise ValueError('Invalid input for serialization, type: %s ' %\n+                       type(inputs))\n+\n+    return output, output_type, module\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    config = config.copy()\n+    function = cls._parse_function_from_config(config, custom_objects,\n+                                               'function', 'module',\n+                                               'function_type')\n+\n+    output_shape = cls._parse_function_from_config(config, custom_objects,\n+                                                   'output_shape',\n+                                                   'output_shape_module',\n+                                                   'output_shape_type')\n+    if 'mask' in config:\n+      mask = cls._parse_function_from_config(config, custom_objects, 'mask',\n+                                             'mask_module', 'mask_type')\n+    else:\n+      mask = None\n+\n+    config['function'] = function\n+    config['output_shape'] = output_shape\n+    config['mask'] = mask\n+\n+    # If arguments were numpy array, they have been saved as\n+    # list. We need to recover the ndarray\n+    if 'arguments' in config:\n+      for key in config['arguments']:\n+        if isinstance(config['arguments'][key], dict):\n+          arg_dict = config['arguments'][key]\n+          if 'type' in arg_dict and arg_dict['type'] == 'ndarray':\n+            # Overwrite the argument with its numpy translation\n+            config['arguments'][key] = np.array(arg_dict['value'])\n+\n+    return cls(**config)\n+\n+  @classmethod\n+  def _parse_function_from_config(cls, config, custom_objects, func_attr_name,\n+                                  module_attr_name, func_type_attr_name):\n+    globs = globals().copy()\n+    module = config.pop(module_attr_name, None)\n+    if module in sys.modules:\n+      globs.update(sys.modules[module].__dict__)\n+    elif module is not None:\n+      # Note: we don't know the name of the function if it's a lambda.\n+      warnings.warn(\n+          '{} is not loaded, but a Lambda layer uses it. '\n+          'It may cause errors.'.format(module), UserWarning)\n+    if custom_objects:\n+      globs.update(custom_objects)\n+    function_type = config.pop(func_type_attr_name)\n+    if function_type == 'function':\n+      # Simple lookup in custom objects\n+      function = generic_utils.deserialize_keras_object(\n+          config[func_attr_name],\n+          custom_objects=custom_objects,\n+          printable_module_name='function in Lambda layer')\n+    elif function_type == 'lambda':\n+      # Unsafe deserialization from bytecode\n+      function = generic_utils.func_load(config[func_attr_name], globs=globs)\n+    elif function_type == 'raw':\n+      function = config[func_attr_name]\n+    else:\n+      raise TypeError('Unknown function type:', function_type)\n+    return function\n\n@@ -0,0 +1,88 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the Masking layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend as K\n+from keras.engine.base_layer import Layer\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Masking')\n+class Masking(Layer):\n+  \"\"\"Masks a sequence by using a mask value to skip timesteps.\n+\n+  For each timestep in the input tensor (dimension #1 in the tensor),\n+  if all values in the input tensor at that timestep\n+  are equal to `mask_value`, then the timestep will be masked (skipped)\n+  in all downstream layers (as long as they support masking).\n+\n+  If any downstream layer does not support masking yet receives such\n+  an input mask, an exception will be raised.\n+\n+  Example:\n+\n+  Consider a Numpy data array `x` of shape `(samples, timesteps, features)`,\n+  to be fed to an LSTM layer. You want to mask timestep #3 and #5 because you\n+  lack data for these timesteps. You can:\n+\n+  - Set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.`\n+  - Insert a `Masking` layer with `mask_value=0.` before the LSTM layer:\n+\n+  ```python\n+  samples, timesteps, features = 32, 10, 8\n+  inputs = np.random.random([samples, timesteps, features]).astype(np.float32)\n+  inputs[:, 3, :] = 0.\n+  inputs[:, 5, :] = 0.\n+\n+  model = tf.keras.models.Sequential()\n+  model.add(tf.keras.layers.Masking(mask_value=0.,\n+                                    input_shape=(timesteps, features)))\n+  model.add(tf.keras.layers.LSTM(32))\n+\n+  output = model(inputs)\n+  # The time step 3 and 5 will be skipped from LSTM calculation.\n+  ```\n+\n+  See [the masking and padding guide](\n+    https://www.tensorflow.org/guide/keras/masking_and_padding)\n+  for more details.\n+  \"\"\"\n+\n+  def __init__(self, mask_value=0., **kwargs):\n+    super(Masking, self).__init__(**kwargs)\n+    self.supports_masking = True\n+    self.mask_value = mask_value\n+    self._compute_output_and_mask_jointly = True\n+\n+  def compute_mask(self, inputs, mask=None):\n+    return K.any(tf.not_equal(inputs, self.mask_value), axis=-1)\n+\n+  def call(self, inputs):\n+    boolean_mask = K.any(\n+        tf.not_equal(inputs, self.mask_value), axis=-1, keepdims=True)\n+    outputs = inputs * tf.cast(boolean_mask, inputs.dtype)\n+    # Compute the mask and outputs simultaneously.\n+    outputs._keras_mask = tf.squeeze(boolean_mask, axis=-1)  # pylint: disable=protected-access\n+    return outputs\n+\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n+\n+  def get_config(self):\n+    config = {'mask_value': self.mask_value}\n+    base_config = super(Masking, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,80 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the Permute layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import copy\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Permute')\n+class Permute(Layer):\n+  \"\"\"Permutes the dimensions of the input according to a given pattern.\n+\n+  Useful e.g. connecting RNNs and convnets.\n+\n+  Example:\n+\n+  ```python\n+  model = Sequential()\n+  model.add(Permute((2, 1), input_shape=(10, 64)))\n+  # now: model.output_shape == (None, 64, 10)\n+  # note: `None` is the batch dimension\n+  ```\n+\n+  Args:\n+    dims: Tuple of integers. Permutation pattern does not include the\n+      samples dimension. Indexing starts at 1.\n+      For instance, `(2, 1)` permutes the first and second dimensions\n+      of the input.\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the samples axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same as the input shape, but with the dimensions re-ordered according\n+    to the specified pattern.\n+  \"\"\"\n+\n+  def __init__(self, dims, **kwargs):\n+    super(Permute, self).__init__(**kwargs)\n+    self.dims = tuple(dims)\n+    if sorted(dims) != list(range(1, len(dims) + 1)):\n+      raise ValueError(\n+          'Invalid permutation `dims` for Permute Layer: %s. '\n+          'The set of indices in `dims` must be consecutive and start from 1.' %\n+          (dims,))\n+    self.input_spec = InputSpec(ndim=len(self.dims) + 1)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    output_shape = copy.copy(input_shape)\n+    for i, dim in enumerate(self.dims):\n+      target_dim = input_shape[dim]\n+      output_shape[i + 1] = target_dim\n+    return tf.TensorShape(output_shape)\n+\n+  def call(self, inputs):\n+    return tf.transpose(inputs, perm=(0,) + self.dims)\n+\n+  def get_config(self):\n+    config = {'dims': self.dims}\n+    base_config = super(Permute, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,65 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the RepeatVector layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend as K\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.RepeatVector')\n+class RepeatVector(Layer):\n+  \"\"\"Repeats the input n times.\n+\n+  Example:\n+\n+  ```python\n+  model = Sequential()\n+  model.add(Dense(32, input_dim=32))\n+  # now: model.output_shape == (None, 32)\n+  # note: `None` is the batch dimension\n+\n+  model.add(RepeatVector(3))\n+  # now: model.output_shape == (None, 3, 32)\n+  ```\n+\n+  Args:\n+    n: Integer, repetition factor.\n+  Input shape: 2D tensor of shape `(num_samples, features)`.\n+  Output shape: 3D tensor of shape `(num_samples, n, features)`.\n+  \"\"\"\n+\n+  def __init__(self, n, **kwargs):\n+    super(RepeatVector, self).__init__(**kwargs)\n+    self.n = n\n+    if not isinstance(n, int):\n+      raise TypeError(f'Expected an integer value for `n`, got {type(n)}.')\n+    self.input_spec = InputSpec(ndim=2)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    return tf.TensorShape([input_shape[0], self.n, input_shape[1]])\n+\n+  def call(self, inputs):\n+    return K.repeat(inputs, self.n)\n+\n+  def get_config(self):\n+    config = {'n': self.n}\n+    base_config = super(RepeatVector, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,134 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the Reshape layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.engine.base_layer import Layer\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Reshape')\n+class Reshape(Layer):\n+  \"\"\"Layer that reshapes inputs into the given shape.\n+\n+  Input shape:\n+    Arbitrary, although all dimensions in the input shape must be known/fixed.\n+    Use the keyword argument `input_shape` (tuple of integers, does not include\n+    the samples/batch size axis) when using this layer as the first layer\n+    in a model.\n+\n+  Output shape:\n+    `(batch_size,) + target_shape`\n+\n+  Example:\n+\n+  >>> # as first layer in a Sequential model\n+  >>> model = tf.keras.Sequential()\n+  >>> model.add(tf.keras.layers.Reshape((3, 4), input_shape=(12,)))\n+  >>> # model.output_shape == (None, 3, 4), `None` is the batch size.\n+  >>> model.output_shape\n+  (None, 3, 4)\n+\n+  >>> # as intermediate layer in a Sequential model\n+  >>> model.add(tf.keras.layers.Reshape((6, 2)))\n+  >>> model.output_shape\n+  (None, 6, 2)\n+\n+  >>> # also supports shape inference using `-1` as dimension\n+  >>> model.add(tf.keras.layers.Reshape((-1, 2, 2)))\n+  >>> model.output_shape\n+  (None, 3, 2, 2)\n+  \"\"\"\n+\n+  def __init__(self, target_shape, **kwargs):\n+    \"\"\"Creates a `tf.keras.layers.Reshape`  layer instance.\n+\n+    Args:\n+      target_shape: Target shape. Tuple of integers, does not include the\n+        samples dimension (batch size).\n+      **kwargs: Any additional layer keyword arguments.\n+    \"\"\"\n+    super(Reshape, self).__init__(**kwargs)\n+    self.target_shape = tuple(target_shape)\n+\n+  def _fix_unknown_dimension(self, input_shape, output_shape):\n+    \"\"\"Find and replace a missing dimension in an output shape.\n+\n+    This is a near direct port of the internal Numpy function\n+    `_fix_unknown_dimension` in `numpy/core/src/multiarray/shape.c`\n+\n+    Args:\n+      input_shape: Shape of array being reshaped\n+      output_shape: Desired shape of the array with at most a single -1 which\n+        indicates a dimension that should be derived from the input shape.\n+\n+    Returns:\n+      The new output shape with a -1 replaced with its computed value.\n+\n+    Raises:\n+      ValueError: If the total array size of the output_shape is\n+      different than the input_shape, or more than one unknown dimension\n+      is specified.\n+    \"\"\"\n+    output_shape = list(output_shape)\n+    msg = ('total size of new array must be unchanged, '\n+           'input_shape = {}, output_shape = {}'.format(input_shape,\n+                                                        output_shape))\n+\n+    known, unknown = 1, None\n+    for index, dim in enumerate(output_shape):\n+      if dim < 0:\n+        if unknown is None:\n+          unknown = index\n+        else:\n+          raise ValueError('Can only specify one unknown dimension.')\n+      else:\n+        known *= dim\n+\n+    original = np.prod(input_shape, dtype=int)\n+    if unknown is not None:\n+      if known == 0 or original % known != 0:\n+        raise ValueError(msg)\n+      output_shape[unknown] = original // known\n+    elif original != known:\n+      raise ValueError(msg)\n+    return output_shape\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if None in input_shape[1:]:\n+      output_shape = [input_shape[0]]\n+      # input shape (partially) unknown? replace -1's with None's\n+      output_shape += tuple(s if s != -1 else None for s in self.target_shape)\n+    else:\n+      output_shape = [input_shape[0]]\n+      output_shape += self._fix_unknown_dimension(input_shape[1:],\n+                                                  self.target_shape)\n+    return tf.TensorShape(output_shape)\n+\n+  def call(self, inputs):\n+    result = tf.reshape(inputs, (tf.shape(inputs)[0],) + self.target_shape)\n+    if not tf.executing_eagerly():\n+      # Set the static shape for the result since it might lost during array_ops\n+      # reshape, eg, some `None` dim in the result could be inferred.\n+      result.set_shape(self.compute_output_shape(inputs.shape))\n+    return result\n+\n+  def get_config(self):\n+    config = {'target_shape': self.target_shape}\n+    base_config = super(Reshape, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,162 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the spatial dropout layers.\"\"\"\n+\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import,g-bad-import-order\n+import tensorflow.compat.v2 as tf\n+# pylint: enable=g-bad-import-order\n+\n+import keras.backend as K\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.core.dropout import Dropout\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.SpatialDropout1D')\n+class SpatialDropout1D(Dropout):\n+  \"\"\"Spatial 1D version of Dropout.\n+\n+  This version performs the same function as Dropout, however, it drops\n+  entire 1D feature maps instead of individual elements. If adjacent frames\n+  within feature maps are strongly correlated (as is normally the case in\n+  early convolution layers) then regular dropout will not regularize the\n+  activations and will otherwise just result in an effective learning rate\n+  decrease. In this case, SpatialDropout1D will help promote independence\n+  between feature maps and should be used instead.\n+\n+  Args:\n+    rate: Float between 0 and 1. Fraction of the input units to drop.\n+  Call arguments:\n+    inputs: A 3D tensor.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode (adding dropout) or in inference mode (doing nothing).\n+  Input shape:\n+    3D tensor with shape: `(samples, timesteps, channels)`\n+  Output shape: Same as input.\n+  References: - [Efficient Object Localization Using Convolutional\n+      Networks](https://arxiv.org/abs/1411.4280)\n+  \"\"\"\n+\n+  def __init__(self, rate, **kwargs):\n+    super(SpatialDropout1D, self).__init__(rate, **kwargs)\n+    self.input_spec = InputSpec(ndim=3)\n+\n+  def _get_noise_shape(self, inputs):\n+    input_shape = tf.shape(inputs)\n+    noise_shape = (input_shape[0], 1, input_shape[2])\n+    return noise_shape\n+\n+\n+@keras_export('keras.layers.SpatialDropout2D')\n+class SpatialDropout2D(Dropout):\n+  \"\"\"Spatial 2D version of Dropout.\n+\n+  This version performs the same function as Dropout, however, it drops\n+  entire 2D feature maps instead of individual elements. If adjacent pixels\n+  within feature maps are strongly correlated (as is normally the case in\n+  early convolution layers) then regular dropout will not regularize the\n+  activations and will otherwise just result in an effective learning rate\n+  decrease. In this case, SpatialDropout2D will help promote independence\n+  between feature maps and should be used instead.\n+\n+  Args:\n+    rate: Float between 0 and 1. Fraction of the input units to drop.\n+    data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,\n+      the channels dimension (the depth) is at index 1, in 'channels_last' mode\n+      is it at index 3. It defaults to the `image_data_format` value found in\n+      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n+      it will be \"channels_last\".\n+  Call arguments:\n+    inputs: A 4D tensor.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode (adding dropout) or in inference mode (doing nothing).\n+  Input shape:\n+    4D tensor with shape: `(samples, channels, rows, cols)` if\n+      data_format='channels_first'\n+    or 4D tensor with shape: `(samples, rows, cols, channels)` if\n+      data_format='channels_last'.\n+  Output shape: Same as input.\n+  References: - [Efficient Object Localization Using Convolutional\n+      Networks](https://arxiv.org/abs/1411.4280)\n+  \"\"\"\n+\n+  def __init__(self, rate, data_format=None, **kwargs):\n+    super(SpatialDropout2D, self).__init__(rate, **kwargs)\n+    if data_format is None:\n+      data_format = K.image_data_format()\n+    if data_format not in {'channels_last', 'channels_first'}:\n+      raise ValueError('data_format must be in '\n+                       '{\"channels_last\", \"channels_first\"}')\n+    self.data_format = data_format\n+    self.input_spec = InputSpec(ndim=4)\n+\n+  def _get_noise_shape(self, inputs):\n+    input_shape = tf.shape(inputs)\n+    if self.data_format == 'channels_first':\n+      return (input_shape[0], input_shape[1], 1, 1)\n+    elif self.data_format == 'channels_last':\n+      return (input_shape[0], 1, 1, input_shape[3])\n+\n+\n+@keras_export('keras.layers.SpatialDropout3D')\n+class SpatialDropout3D(Dropout):\n+  \"\"\"Spatial 3D version of Dropout.\n+\n+  This version performs the same function as Dropout, however, it drops\n+  entire 3D feature maps instead of individual elements. If adjacent voxels\n+  within feature maps are strongly correlated (as is normally the case in\n+  early convolution layers) then regular dropout will not regularize the\n+  activations and will otherwise just result in an effective learning rate\n+  decrease. In this case, SpatialDropout3D will help promote independence\n+  between feature maps and should be used instead.\n+\n+  Args:\n+    rate: Float between 0 and 1. Fraction of the input units to drop.\n+    data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,\n+      the channels dimension (the depth) is at index 1, in 'channels_last' mode\n+      is it at index 4. It defaults to the `image_data_format` value found in\n+      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n+      it will be \"channels_last\".\n+  Call arguments:\n+    inputs: A 5D tensor.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode (adding dropout) or in inference mode (doing nothing).\n+  Input shape:\n+    5D tensor with shape: `(samples, channels, dim1, dim2, dim3)` if\n+      data_format='channels_first'\n+    or 5D tensor with shape: `(samples, dim1, dim2, dim3, channels)` if\n+      data_format='channels_last'.\n+  Output shape: Same as input.\n+  References: - [Efficient Object Localization Using Convolutional\n+      Networks](https://arxiv.org/abs/1411.4280)\n+  \"\"\"\n+\n+  def __init__(self, rate, data_format=None, **kwargs):\n+    super(SpatialDropout3D, self).__init__(rate, **kwargs)\n+    if data_format is None:\n+      data_format = K.image_data_format()\n+    if data_format not in {'channels_last', 'channels_first'}:\n+      raise ValueError('data_format must be in '\n+                       '{\"channels_last\", \"channels_first\"}')\n+    self.data_format = data_format\n+    self.input_spec = InputSpec(ndim=5)\n+\n+  def _get_noise_shape(self, inputs):\n+    input_shape = tf.shape(inputs)\n+    if self.data_format == 'channels_first':\n+      return (input_shape[0], input_shape[1], 1, 1, 1)\n+    elif self.data_format == 'channels_last':\n+      return (input_shape[0], 1, 1, 1, input_shape[4])\n\n@@ -0,0 +1,552 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the TFOpLambda layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import,g-bad-import-order\n+import tensorflow.compat.v2 as tf\n+# pylint: enable=g-bad-import-order\n+\n+import textwrap\n+from keras import backend as K\n+from keras.engine import keras_tensor\n+from keras.engine.base_layer import Layer\n+\n+from tensorflow.python.platform import tf_logging\n+from tensorflow.python.util.tf_export import get_canonical_name_for_symbol\n+from tensorflow.python.util.tf_export import get_symbol_from_name\n+\n+\n+class ClassMethod(Layer):\n+  \"\"\"Wraps a TF API Class's class method  in a `Layer` object.\n+\n+  It is inserted by the Functional API construction whenever users call\n+  a supported TF Class's class method on KerasTensors.\n+\n+  This is useful in the case where users do something like:\n+  x = keras.Input(...)\n+  y = keras.Input(...)\n+  out = tf.RaggedTensor.from_row_splits(x, y)\n+  \"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self, cls_ref, method_name, **kwargs):\n+    self.cls_ref = cls_ref\n+    self.method_name = method_name\n+    self.cls_symbol = (\n+        get_canonical_name_for_symbol(\n+            self.cls_ref, add_prefix_to_v1_names=True) or\n+        get_canonical_name_for_symbol(\n+            self.cls_ref, api_name='keras', add_prefix_to_v1_names=True))\n+    if 'name' not in kwargs:\n+      kwargs['name'] = K.unique_object_name(\n+          'tf.' + self.cls_symbol + '.' + self.method_name,\n+          zero_based=True,\n+          avoid_observed_names=True)\n+    kwargs['autocast'] = False\n+\n+    # Do not individually trace op layers in the SavedModel.\n+    self._must_restore_from_config = True\n+\n+    super(ClassMethod, self).__init__(**kwargs)\n+\n+    # Preserve all argument data structures when saving/loading a config\n+    # (e.g., don't unnest lists that contain one element)\n+    self._preserve_input_structure_in_config = True\n+\n+    self._expects_training_arg = False\n+    self._expects_mask_arg = False\n+\n+  def call(self, args, kwargs):\n+    return getattr(self.cls_ref, self.method_name)(*args, **kwargs)\n+\n+  def get_config(self):\n+    if not self.cls_symbol:\n+      raise ValueError('This Keras class method conversion tried to convert '\n+                       'a method belonging to class %s, a class '\n+                       'that is not an exposed in the TensorFlow API. '\n+                       'To ensure cross-version compatibility of Keras models '\n+                       'that use op layers, only op layers produced from '\n+                       'exported TF API symbols can be serialized.' %\n+                       self.cls_symbol)\n+    config = {'cls_symbol': self.cls_symbol, 'method_name': self.method_name}\n+\n+    base_config = super(ClassMethod, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    config = config.copy()\n+    symbol_name = config.pop('cls_symbol')\n+    cls_ref = get_symbol_from_name(symbol_name)\n+    if not cls_ref:\n+      raise ValueError('TF symbol `tf.%s` could not be found.' % symbol_name)\n+\n+    config['cls_ref'] = cls_ref\n+\n+    return cls(**config)\n+\n+\n+class KerasOpDispatcher(tf.__internal__.dispatch.GlobalOpDispatcher):\n+  \"\"\"A global dispatcher that allows building a functional model with TF Ops.\"\"\"\n+\n+  def handle(self, op, args, kwargs):\n+    \"\"\"Handle the specified operation with the specified arguments.\"\"\"\n+    if any(\n+        isinstance(x, keras_tensor.KerasTensor)\n+        for x in tf.nest.flatten([args, kwargs])):\n+      return TFOpLambda(op)(*args, **kwargs)\n+    else:\n+      return self.NOT_SUPPORTED\n+\n+\n+KerasOpDispatcher().register()\n+\n+\n+class InstanceProperty(Layer):\n+  \"\"\"Wraps an instance property access (e.g.\n+\n+  `x.foo`) in a Keras Layer.\n+\n+  This layer takes an attribute name `attr_name` in the constructor and,\n+  when called on input tensor `obj` returns `obj.attr_name`.\n+\n+  KerasTensors specialized for specific extension types use it to\n+  represent instance property accesses on the represented object in the\n+  case where the property needs to be dynamically accessed as opposed to\n+  being statically computed from the typespec, e.g.\n+\n+  x = keras.Input(..., ragged=True)\n+  out = x.flat_values\n+  \"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self, attr_name, **kwargs):\n+    self.attr_name = attr_name\n+\n+    if 'name' not in kwargs:\n+      kwargs['name'] = K.unique_object_name(\n+          'input.' + self.attr_name, zero_based=True, avoid_observed_names=True)\n+    kwargs['autocast'] = False\n+\n+    # Do not individually trace op layers in the SavedModel.\n+    self._must_restore_from_config = True\n+\n+    super(InstanceProperty, self).__init__(**kwargs)\n+\n+    # Preserve all argument data structures when saving/loading a config\n+    # (e.g., don't unnest lists that contain one element)\n+    self._preserve_input_structure_in_config = True\n+\n+  def call(self, obj):\n+    return getattr(obj, self.attr_name)\n+\n+  def get_config(self):\n+    config = {'attr_name': self.attr_name}\n+    base_config = super(InstanceProperty, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    return cls(**config)\n+\n+\n+class InstanceMethod(InstanceProperty):\n+  \"\"\"Wraps an instance method access (e.g. `x.foo(arg)` in a Keras Layer.\n+\n+  This layer takes an attribute name `attr_name` in the constructor and,\n+  when called on input tensor `obj` with additional arguments `args` and\n+  `kwargs` returns `obj.attr_name(*args, **kwargs)`.\n+\n+  KerasTensors specialized for specific extension types use it to\n+  represent dynamic instance method calls on the represented object, e.g.\n+\n+  x = keras.Input(..., ragged=True)\n+  new_values = keras.Input(...)\n+  out = x.with_values(new_values)\n+  \"\"\"\n+\n+  def call(self, obj, args, kwargs):\n+    method = getattr(obj, self.attr_name)\n+    return method(*args, **kwargs)\n+\n+\n+class TFOpLambda(Layer):\n+  \"\"\"Wraps TF API symbols in a `Layer` object.\n+\n+  It is inserted by the Functional API construction whenever users call\n+  a supported TF symbol on KerasTensors.\n+\n+  Like Lambda layers, this layer tries to raise warnings when it detects users\n+  explicitly use variables in the call. (To let them know\n+  that the layer will not capture the variables).\n+\n+  This is useful in the case where users do something like:\n+  x = keras.Input(...)\n+  y = tf.Variable(...)\n+  out = x * tf_variable\n+  \"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self, function, **kwargs):\n+    self.function = function\n+    self.symbol = (\n+        get_canonical_name_for_symbol(\n+            self.function, add_prefix_to_v1_names=True) or\n+        get_canonical_name_for_symbol(\n+            self.function, api_name='keras', add_prefix_to_v1_names=True))\n+    if 'name' not in kwargs:\n+      # Generate a name.\n+      # TFOpLambda layers avoid already-observed names,\n+      # because users cannot easily control the generated names.\n+      # Without this avoidance, users would be more likely to run\n+      # into unavoidable duplicate layer name collisions.\n+      # (For standard layers users could just set `name` when creating the\n+      # layer to work around a collision, but they can't do that for\n+      # auto-generated layers)\n+      if self.symbol:\n+        name = 'tf.' + self.symbol\n+      else:\n+        name = self.function.__name__\n+      kwargs['name'] = K.unique_object_name(\n+          name, zero_based=True, avoid_observed_names=True)\n+    kwargs['autocast'] = False\n+\n+    # Decorate the function to produce this layer's call method\n+    def _call_wrapper(*args, **kwargs):\n+      return self._call_wrapper(*args, **kwargs)\n+\n+    self.call = tf.__internal__.decorator.make_decorator(\n+        function, _call_wrapper)\n+\n+    # Do not individually trace op layers in the SavedModel.\n+    self._must_restore_from_config = True\n+\n+    super(TFOpLambda, self).__init__(**kwargs)\n+\n+    # Preserve all argument data structures when saving/loading a config\n+    # (e.g., don't unnest lists that contain one element)\n+    self._preserve_input_structure_in_config = True\n+\n+    # Warning on every invocation will be quite irksome in Eager mode.\n+    self._already_warned = False\n+\n+    self._expects_training_arg = False\n+    self._expects_mask_arg = False\n+\n+  def _call_wrapper(self, *args, **kwargs):\n+    created_variables = []\n+\n+    def _variable_creator(next_creator, **creator_kwargs):\n+      var = next_creator(**creator_kwargs)\n+      created_variables.append(var)\n+      return var\n+\n+    with tf.GradientTape(watch_accessed_variables=True) as tape, \\\n+        tf.variable_creator_scope(_variable_creator):\n+      # We explicitly drop `name` arguments here,\n+      # to guard against the case where an op explicitly has a\n+      # `name` passed (which is susceptible to producing\n+      # multiple ops w/ the same name when the layer is reused)\n+      kwargs.pop('name', None)\n+      result = self.function(*args, **kwargs)\n+    self._check_variables(created_variables, tape.watched_variables())\n+    return result\n+\n+  def _check_variables(self, created_variables, accessed_variables):\n+    if not created_variables and not accessed_variables:\n+      # In the common case that a Lambda layer does not touch a Variable, we\n+      # don't want to incur the runtime cost of assembling any state used for\n+      # checking only to immediately discard it.\n+      return\n+\n+    tracked_weights = set(v.ref() for v in self.weights)\n+    untracked_new_vars = [\n+        v for v in created_variables if v.ref() not in tracked_weights\n+    ]\n+    if untracked_new_vars:\n+      variable_str = '\\n'.join('  {}'.format(i) for i in untracked_new_vars)\n+      error_str = textwrap.dedent(\"\"\"\n+          The following Variables were created within a Lambda layer ({name})\n+          but are not tracked by said layer:\n+          {variable_str}\n+          The layer cannot safely ensure proper Variable reuse across multiple\n+          calls, and consquently this behavior is disallowed for safety. Lambda\n+          layers are not well suited to stateful computation; instead, writing a\n+          subclassed Layer is the recommend way to define layers with\n+          Variables.\"\"\").format(\n+              name=self.name, variable_str=variable_str)\n+      raise ValueError(error_str)\n+\n+    untracked_used_vars = [\n+        v for v in accessed_variables if v.ref() not in tracked_weights\n+    ]\n+    if untracked_used_vars and not self._already_warned:\n+      variable_str = '\\n'.join('  {}'.format(i) for i in untracked_used_vars)\n+      self._warn(\n+          textwrap.dedent(\"\"\"\n+          The following Variables were used a Lambda layer's call ({name}), but\n+          are not present in its tracked objects:\n+          {variable_str}\n+          It is possible that this is intended behavior, but it is more likely\n+          an omission. This is a strong indication that this layer should be\n+          formulated as a subclassed Layer rather than a Lambda layer.\"\"\")\n+          .format(name=self.name, variable_str=variable_str))\n+      self._already_warned = True\n+\n+  def _warn(self, msg):\n+    # This method will be overridden in a unit test to raise an error, because\n+    # self.assertWarns is not universally implemented.\n+    return tf_logging.warning(msg)\n+\n+  def get_config(self):\n+    if not self.symbol:\n+      raise ValueError('This Keras op layer was generated from %s, a method '\n+                       'that is not an exposed in the TensorFlow API. This '\n+                       'may have happened if the method was explicitly '\n+                       'decorated to add dispatching support, and it was used '\n+                       'during Functional model construction. '\n+                       'To ensure cross-version compatibility of Keras models '\n+                       'that use op layers, only op layers produced from '\n+                       'exported TF API symbols can be serialized.' %\n+                       self.function)\n+    config = {'function': self.symbol}\n+\n+    base_config = super(TFOpLambda, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    config = config.copy()\n+    symbol_name = config['function']\n+    function = get_symbol_from_name(symbol_name)\n+    if not function:\n+      raise ValueError('TF symbol `tf.%s` could not be found.' % symbol_name)\n+\n+    config['function'] = function\n+\n+    return cls(**config)\n+\n+\n+def _delegate_property(keras_tensor_cls, property_name):  # pylint: disable=invalid-name\n+  \"\"\"Register property on a KerasTensor class.\n+\n+  Calling this multiple times with the same arguments should be a no-op.\n+\n+  This method exposes a property on the KerasTensor class that will use an\n+  `InstanceProperty` layer to access the property on the represented\n+  intermediate values in the model.\n+\n+  Args:\n+    keras_tensor_cls: The KerasTensor subclass that should expose the property.\n+    property_name: The name of the property to expose and delegate to the\n+      represented (Composite)Tensor.\n+  \"\"\"\n+  # We use a lambda because we can't create a Keras layer at import time\n+  # due to dynamic layer class versioning.\n+  property_access = property(lambda self: InstanceProperty(property_name)(self))  # pylint: disable=unnecessary-lambda\n+  setattr(keras_tensor_cls, property_name, property_access)\n+\n+\n+def _delegate_method(keras_tensor_cls, method_name):  # pylint: disable=invalid-name\n+  \"\"\"Register method on a KerasTensor class.\n+\n+  Calling this function times with the same arguments should be a no-op.\n+\n+  This method exposes an instance method on the KerasTensor class that will use\n+  an `InstanceMethod` layer to run the desired method on the represented\n+  intermediate values in the model.\n+\n+  Args:\n+    keras_tensor_cls: The KerasTensor subclass that should expose the property.\n+    method_name: The name of the method to expose and delegate to the\n+      represented (Composite)Tensor.\n+  \"\"\"\n+\n+  def delegate(self, *args, **kwargs):\n+    return InstanceMethod(method_name)(self, args, kwargs)\n+\n+  setattr(keras_tensor_cls, method_name, delegate)\n+\n+\n+# We do not support the `uniform_row_length` property because it\n+# returns either `None` or an int tensor, and code that relies on it tends\n+# to check `is None` directly. Delegating it here would always return a\n+# `KerasTensor`, regardless of what can be statically inferred. This would\n+# never equal `None`, breaking code that expects it to be partially-static\n+# in unpredictable ways.\n+for ragged_property in [\n+    'values', 'flat_values', 'row_splits', 'nested_row_splits'\n+]:\n+  _delegate_property(keras_tensor.RaggedKerasTensor, ragged_property)\n+\n+for ragged_method_name in [\n+    'value_rowids',\n+    'nested_value_rowids',\n+    'nrows',\n+    'row_starts',\n+    'row_limits',\n+    'row_lengths',\n+    'nested_row_lengths',\n+    'bounding_shape',\n+    'with_values',\n+    'with_flat_values',\n+    'with_row_splits_dtype',\n+    'merge_dims',\n+    'to_tensor',\n+    'to_sparse',\n+]:\n+  _delegate_method(keras_tensor.RaggedKerasTensor, ragged_method_name)\n+\n+for sparse_property in [\n+    'indices',\n+    'values',\n+]:\n+  _delegate_property(keras_tensor.SparseKerasTensor, sparse_property)\n+\n+for sparse_method in [\n+    'with_values',\n+]:\n+  _delegate_method(keras_tensor.SparseKerasTensor, sparse_method)\n+\n+\n+class TFClassMethodDispatcher(tf.__internal__.dispatch.OpDispatcher):\n+  \"\"\"A class method dispatcher that allows building a functional model with TF class methods.\"\"\"\n+\n+  def __init__(self, cls, method_name):\n+    self.cls = cls\n+    self.method_name = method_name\n+\n+  def handle(self, args, kwargs):\n+    \"\"\"Handle the specified operation with the specified arguments.\"\"\"\n+    if any(\n+        isinstance(x, keras_tensor.KerasTensor)\n+        for x in tf.nest.flatten([args, kwargs])):\n+      return ClassMethod(self.cls, self.method_name)(args[1:], kwargs)\n+    else:\n+      return self.NOT_SUPPORTED\n+\n+\n+for ragged_class_method in [\n+    'from_value_rowids',\n+    'from_row_splits',\n+    'from_row_lengths',\n+    'from_row_starts',\n+    'from_row_limits',\n+    'from_uniform_row_length',\n+    'from_nested_value_rowids',\n+    'from_nested_row_splits',\n+    'from_nested_row_lengths',\n+    'from_tensor',\n+    'from_sparse',\n+]:\n+  TFClassMethodDispatcher(tf.RaggedTensor, ragged_class_method).register(\n+      getattr(tf.RaggedTensor, ragged_class_method))\n+\n+\n+class SlicingOpLambda(TFOpLambda):\n+  \"\"\"Wraps TF API symbols in a `Layer` object.\n+\n+  It is inserted by the Functional API construction whenever users call\n+  a supported TF symbol on KerasTensors.\n+\n+  Like Lambda layers, this layer tries to raise warnings when it detects users\n+  explicitly use variables in the call. (To let them know\n+  that the layer will not capture the variables).\n+\n+  This is useful in the case where users do something like:\n+  x = keras.Input(...)\n+  y = tf.Variable(...)\n+  out = x * tf_variable\n+  \"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self, function, **kwargs):\n+    super(SlicingOpLambda, self).__init__(function, **kwargs)\n+\n+    original_call = self.call\n+\n+    # Decorate the function to produce this layer's call method\n+    def _call_wrapper(*args, **kwargs):\n+      # Turn any slice dicts in the args back into `slice` objects.\n+      # This conversion cannot use nest.flatten/map_structure,\n+      # because dicts are flattened by nest while slices aren't.\n+      # So, map_structure would only see the individual elements in the\n+      # dict.\n+      # This can't use map_structure_up_to either because the 'shallowness' of\n+      # the shallow tree would have to vary depending on if only one dim or\n+      # multiple are being sliced.\n+      new_args = []\n+      for arg in args:\n+        arg = _dict_to_slice(arg)\n+        if isinstance(arg, (list, tuple)):\n+          new_arg = []\n+          for sub_arg in arg:\n+            new_arg.append(_dict_to_slice(sub_arg))\n+          arg = new_arg\n+        new_args.append(arg)\n+\n+      # Handle the kwargs too.\n+      new_kwargs = {}\n+      for key, value in kwargs.items():\n+        value = _dict_to_slice(value)\n+        if isinstance(value, (list, tuple)):\n+          new_value = []\n+          for v in value:\n+            new_value.append(_dict_to_slice(v))\n+          value = new_value\n+        new_kwargs[key] = value\n+\n+      return original_call(*new_args, **new_kwargs)\n+\n+    self.call = tf.__internal__.decorator.make_decorator(\n+        original_call, _call_wrapper)\n+\n+\n+def _slice_to_dict(x):\n+  if isinstance(x, slice):\n+    return {'start': x.start, 'stop': x.stop, 'step': x.step}\n+  return x\n+\n+\n+def _dict_to_slice(x):\n+  if isinstance(x, dict):\n+    return slice(x['start'], x['stop'], x['step'])\n+  return x\n+\n+\n+class TFSlicingOpDispatcher(tf.__internal__.dispatch.OpDispatcher):\n+  \"\"\"A global dispatcher that allows building a functional model with TF Ops.\"\"\"\n+\n+  def __init__(self, op):\n+    self.op = op\n+\n+  def handle(self, args, kwargs):\n+    \"\"\"Handle the specified operation with the specified arguments.\"\"\"\n+    args = tf.nest.map_structure(_slice_to_dict, args)\n+    kwargs = tf.nest.map_structure(_slice_to_dict, kwargs)\n+    if any(\n+        isinstance(x, keras_tensor.KerasTensor)\n+        for x in tf.nest.flatten([args, kwargs])):\n+      return SlicingOpLambda(self.op)(*args, **kwargs)\n+    else:\n+      return self.NOT_SUPPORTED\n+\n+\n+for slicing_op in [\n+    tf.__operators__.getitem,  # pylint: disable=protected-access\n+    tf.compat.v1.boolean_mask,\n+    tf.boolean_mask,\n+    tf.__operators__.ragged_getitem\n+]:\n+  TFSlicingOpDispatcher(slicing_op).register(slicing_op)\n\n@@ -13,16 +13,15 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Tests for embedding layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n import keras\n from keras import combinations\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.mixed_precision import policy\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n class EmbeddingTest(keras_parameterized.TestCase):\n\n@@ -12,8 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Layer serialization/deserialization functions.\n-\"\"\"\n+\"\"\"Layer serialization/deserialization functions.\"\"\"\n \n import tensorflow.compat.v2 as tf\n # pylint: disable=wildcard-import\n@@ -59,8 +58,8 @@ from tensorflow.python.util.tf_export import keras_export\n ALL_MODULES = (base_layer, input_layer, advanced_activations, convolutional,\n                convolutional_recurrent, core, cudnn_recurrent, dense_attention,\n                embeddings, einsum_dense, local, merge, noise,\n-               batch_normalization_v1, layer_normalization,\n-               pooling, image_preprocessing, recurrent, wrappers, hashing,\n+               batch_normalization_v1, layer_normalization, pooling,\n+               image_preprocessing, recurrent, wrappers, hashing,\n                category_crossing, category_encoding, discretization,\n                multi_head_attention, integer_lookup,\n                preprocessing_normalization, string_lookup, text_vectorization)\n@@ -72,14 +71,14 @@ LOCAL = threading.local()\n \n \n def populate_deserializable_objects():\n-  \"\"\"Populates dict ALL_OBJECTS with every built-in layer.\n-  \"\"\"\n+  \"\"\"Populates dict ALL_OBJECTS with every built-in layer.\"\"\"\n   global LOCAL\n   if not hasattr(LOCAL, 'ALL_OBJECTS'):\n     LOCAL.ALL_OBJECTS = {}\n     LOCAL.GENERATED_WITH_V2 = None\n \n-  if LOCAL.ALL_OBJECTS and LOCAL.GENERATED_WITH_V2 == tf.__internal__.tf2.enabled():\n+  if LOCAL.ALL_OBJECTS and LOCAL.GENERATED_WITH_V2 == tf.__internal__.tf2.enabled(\n+  ):\n     # Objects dict is already generated for the proper TF version:\n     # do nothing.\n     return\n@@ -172,8 +171,8 @@ def deserialize(config, custom_objects=None):\n \n   Args:\n       config: dict of the form {'class_name': str, 'config': dict}\n-      custom_objects: dict mapping class names (or function names)\n-          of custom (non-Keras) objects to class/functions\n+      custom_objects: dict mapping class names (or function names) of custom\n+        (non-Keras) objects to class/functions\n \n   Returns:\n       Layer instance (may be Model, Sequential, Network, Layer...)\n\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n from keras import backend\n from keras.engine.training import Model\n-from keras.layers.core import Lambda\n+from keras.layers.core.lambda_layer import Lambda\n from keras.layers.merge import concatenate\n \n \n@@ -52,17 +52,15 @@ def multi_gpu_model(model, gpus, cpu_merge=True, cpu_relocation=False):\n   for the time being.\n \n   Args:\n-      model: A Keras model instance. To avoid OOM errors,\n-          this model could have been built on CPU, for instance\n-          (see usage example below).\n-      gpus: Integer >= 2, number of on GPUs on which to create\n-          model replicas.\n-      cpu_merge: A boolean value to identify whether to force\n-          merging model weights under the scope of the CPU or not.\n-      cpu_relocation: A boolean value to identify whether to\n-          create the model's weights under the scope of the CPU.\n-          If the model is not defined under any preceding device\n-          scope, you can still rescue it by activating this option.\n+      model: A Keras model instance. To avoid OOM errors, this model could have\n+        been built on CPU, for instance (see usage example below).\n+      gpus: Integer >= 2, number of on GPUs on which to create model replicas.\n+      cpu_merge: A boolean value to identify whether to force merging model\n+        weights under the scope of the CPU or not.\n+      cpu_relocation: A boolean value to identify whether to create the model's\n+        weights under the scope of the CPU. If the model is not defined under\n+        any preceding device scope, you can still rescue it by activating this\n+        option.\n \n   Returns:\n       A Keras `Model` instance which can be used just like the initial\n@@ -170,8 +168,8 @@ def multi_gpu_model(model, gpus, cpu_merge=True, cpu_relocation=False):\n       raise ValueError('To call `multi_gpu_model` with `gpus=%s`, '\n                        'we expect the following devices to be available: %s. '\n                        'However this machine only has: %s. '\n-                       'Try reducing `gpus`.' % (gpus, target_devices,\n-                                                 available_devices))\n+                       'Try reducing `gpus`.' %\n+                       (gpus, target_devices, available_devices))\n \n   def get_slice(data, i, parts):\n     \"\"\"Slice an array into `parts` slices and return slice `i`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
