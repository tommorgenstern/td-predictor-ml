{"custom_id": "keras#eef3ad0626c6cbedc92c998828671f0d6b766058", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 56 | Files Changed: 5 | Hunks: 27 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 111 | Churn Cumulative: 10620 | Contributors (this commit): 14 | Commits (past 90d): 27 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -783,7 +783,7 @@ class TrackableWeightHandler:\n \n   def __init__(self, trackable):\n     if not isinstance(trackable, tf.__internal__.tracking.Trackable):\n-      raise ValueError('%s is not a Trackable object.' % (trackable,))\n+      raise ValueError(f'{trackable} is not a Trackable object.')\n     self._trackable = trackable\n     self._distribute_strategy = tf.distribute.get_strategy()\n \n@@ -826,9 +826,9 @@ class TrackableWeightHandler:\n         self._setter = self._set_weights_v1\n         self._getter = lambda: [spec.tensor for spec in self._saveable.specs]\n     else:\n-      raise ValueError('Only Trackables with one Saveable are supported. '\n-                       'The Trackable %s has %d Saveables.' %\n-                       (trackable, len(saveables)))\n+      raise ValueError(\n+          'Only Trackables with one Saveable are supported. The Trackable '\n+          f'{trackable} has {len(saveables)} Saveables.')\n \n   @property\n   def num_tensors(self):\n@@ -837,9 +837,9 @@ class TrackableWeightHandler:\n   def set_weights(self, weights):\n     if len(weights) != self._num_tensors:\n       raise ValueError(\n-          ('Weight handler for trackable %s received the wrong number of ' +\n-           'weights: expected %s, got %s.') %\n-          (self._trackable, self._num_tensors, len(weights)))\n+          f'Weight handler for trackable {self._trackable} received '\n+          'an incorrect number of weights: '\n+          f'expected {self._num_tensors} weights, got {len(weights)} weights.')\n     self._setter(weights)\n \n   def get_tensors(self):\n@@ -855,9 +855,10 @@ class TrackableWeightHandler:\n def no_ragged_support(inputs, layer_name):\n   input_list = tf.nest.flatten(inputs)\n   if any(isinstance(x, tf.RaggedTensor) for x in input_list):\n-    raise ValueError('Layer %s does not support RaggedTensors as input. '\n-                     'Inputs received: %s. You can try converting your '\n-                     'input to an uniform tensor.' % (layer_name, inputs))\n+    raise ValueError(\n+        f'Layer {layer_name} does not support RaggedTensors as input. '\n+        f'Inputs received: {inputs}. You can try converting your '\n+        'input to a dense (uniform) tensor.')\n \n \n def is_split_variable(v):\n\n@@ -418,10 +418,11 @@ class Functional(training_lib.Model):\n     # Convert any shapes in tuple format to TensorShapes.\n     input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n \n-    if len(tf.nest.flatten(input_shape)) != len(tf.nest.flatten(self._input_layers)):\n-      raise ValueError('Invalid input_shape argument ' + str(input_shape) +\n-                       ': model has ' + str(len(self._input_layers)) +\n-                       ' tensor inputs.')\n+    if (len(tf.nest.flatten(input_shape)) !=\n+        len(tf.nest.flatten(self._input_layers))):\n+      raise ValueError(f'Invalid `input_shape` argument {input_shape}: '\n+                       f'the model expects {len(self._input_layers)} '\n+                       'input tensors.')\n \n     # Use the tuple of TensorShape as the cache key, since tuple is hashable\n     # and can be used as hash key.\n@@ -483,7 +484,8 @@ class Functional(training_lib.Model):\n         layer, node_index, tensor_index = self._output_coordinates[i]\n         shape_key = layer.name + '_%s_%s' % (node_index, tensor_index)\n         output_shapes.append(layers_to_output_shapes[shape_key])\n-      output_shapes = tf.nest.pack_sequence_as(self._nested_outputs, output_shapes)\n+      output_shapes = tf.nest.pack_sequence_as(self._nested_outputs,\n+                                               output_shapes)\n       # Store in cache.\n       self._output_shape_cache[cache_key] = output_shapes\n \n@@ -671,34 +673,34 @@ class Functional(training_lib.Model):\n     # Check for redundancy in inputs.\n     if len({id(i) for i in self.inputs}) != len(self.inputs):\n       raise ValueError('The list of inputs passed to the model '\n-                       'is redundant. '\n+                       'contains the same input multiple times. '\n                        'All inputs should only appear once.'\n-                       ' Found: ' + str(self.inputs))\n+                       f'Received inputs={self.inputs}')\n \n     for x in self.inputs:\n       # Check that x has appropriate `_keras_history` metadata.\n       if not hasattr(x, '_keras_history'):\n         cls_name = self.__class__.__name__\n-        raise ValueError('Input tensors to a ' + cls_name + ' ' +\n+        raise ValueError(\n+            f'Input tensors to a {cls_name} model '\n             'must come from `tf.keras.Input`. '\n-                         'Received: ' + str(x) +\n-                         ' (missing previous layer metadata).')\n+            f'Received inputs={x} (missing previous layer metadata).')\n       # Check that x is an input tensor.\n       # pylint: disable=protected-access\n       layer = x._keras_history.layer\n       if len(layer._inbound_nodes) > 1 or (\n           layer._inbound_nodes and not layer._inbound_nodes[0].is_input):\n         cls_name = self.__class__.__name__\n-        logging.warning(cls_name + ' model inputs must come from '\n-                        '`tf.keras.Input` (thus holding past layer metadata), '\n-                        'they cannot be the output of '\n+        logging.warning(f'{cls_name} model inputs must come from '\n+                        '`tf.keras.Input` (thus holding past layer metadata). '\n+                        'They cannot be the output of '\n                         'a previous non-Input layer. '\n                         'Here, a tensor specified as '\n-                        'input to \"' + self.name + '\" was not an Input tensor, '\n-                        'it was generated by layer ' + layer.name + '.\\n'\n+                        f'input to \"{self.name}\" was not an Input tensor, '\n+                        f'it was generated by layer \"{layer.name}\".\\n'\n                         'Note that input tensors are '\n                         'instantiated via `tensor = tf.keras.Input(shape)`.\\n'\n-                        'The tensor that caused the issue was: ' + str(x.name))\n+                        f'The tensor that caused the issue was: {x}')\n \n     # Check compatibility of batch sizes of Input Layers.\n     input_batch_sizes = set([\n@@ -706,16 +708,15 @@ class Functional(training_lib.Model):\n         for x in self.inputs])\n     input_batch_sizes.discard(None)\n     if len(input_batch_sizes) > 1:\n-      logging.warning('Found incompatiable static batch sizes among all the '\n-                      'inputs. Batch sizes: {}'.format(\n-                          sorted(input_batch_sizes)))\n+      logging.warning('Found incompatiable static batch sizes among the '\n+                      f'inputs. Batch sizes: {sorted(input_batch_sizes)}')\n \n     for x in self.outputs:\n       if not hasattr(x, '_keras_history'):\n         cls_name = self.__class__.__name__\n-        raise ValueError('Output tensors of a ' + cls_name + ' model must be '\n+        raise ValueError(f'Output tensors of a {cls_name} model must be '\n                          'the output of a TensorFlow `Layer` '\n-                         '(thus holding past layer metadata). Found: ' + str(x))\n+                         f'(thus holding past layer metadata). Found: {x}')\n \n   def _insert_layers(self, layers, relevant_nodes=None):\n     \"\"\"Inserts Layers into the Network after Network creation.\n@@ -724,7 +725,6 @@ class Functional(training_lib.Model):\n     will be included in the `call` computation and `get_config` of this Network.\n     They will not be added to the Network's outputs.\n \n-\n     Args:\n       layers: Arbitrary nested structure of Layers. Layers must be reachable\n         from one or more of the `keras.Input` Tensors that correspond to this\n@@ -744,7 +744,8 @@ class Functional(training_lib.Model):\n     # The nodes of these Layers that are relevant to this Network. If not\n     # provided, assume all Nodes are relevant\n     if not relevant_nodes:\n-      relevant_nodes = tf.nest.flatten([layer._inbound_nodes for layer in layers])\n+      relevant_nodes = tf.nest.flatten(\n+          [layer._inbound_nodes for layer in layers])\n     network_nodes = set(relevant_nodes + list(node_to_depth.keys()))\n \n     def _get_min_depth(node):\n@@ -971,12 +972,10 @@ def _map_graph_network(inputs, outputs):\n       if layer and not node.is_input:\n         for x in tf.nest.flatten(node.keras_inputs):\n           if id(x) not in computable_tensors:\n-            raise ValueError('Graph disconnected: '\n-                             'cannot obtain value for tensor ' + str(x) +\n-                             ' at layer \"' + layer.name + '\". '\n-                             'The following previous layers '\n-                             'were accessed without issue: ' +\n-                             str(layers_with_complete_input))\n+            raise ValueError(\n+                f'Graph disconnected: cannot obtain value for tensor {x} '\n+                f'at layer \"{layer.name}\". The following previous layers '\n+                f'were accessed without issue: {layers_with_complete_input}')\n         for x in tf.nest.flatten(node.outputs):\n           computable_tensors.add(id(x))\n         layers_with_complete_input.append(layer.name)\n@@ -986,9 +985,9 @@ def _map_graph_network(inputs, outputs):\n   all_names = [layer.name for layer in layers]\n   for name in all_names:\n     if all_names.count(name) != 1:\n-      raise ValueError('The name \"' + name + '\" is used ' +\n-                       str(all_names.count(name)) + ' times in the model. '\n-                       'All layer names should be unique.')\n+      raise ValueError(\n+          f'The name \"{name}\" is used {all_names.count(name)} '\n+          'times in the model. All layer names should be unique.')\n   return network_nodes, nodes_by_depth, layers, layers_by_depth\n \n \n@@ -1035,8 +1034,8 @@ def _build_map_helper(tensor, finished_nodes, nodes_in_progress,\n \n   # Prevent cycles.\n   if node in nodes_in_progress:\n-    raise ValueError('The tensor ' + str(tensor) + ' at layer \"' + layer.name +\n-                     '\" is part of a cycle.')\n+    raise ValueError(f'Tensor {tensor} from layer \"{layer.name}\" '\n+                     'is part of a cycle.')\n \n   # Store the traversal order for layer sorting.\n   if layer not in layer_indices:\n\n@@ -228,8 +228,8 @@ class KerasTensor:\n                     'in the Functional Model.')\n \n   def __hash__(self):\n-    raise TypeError('Tensors are unhashable. (%s)'\n-                    'Instead, use tensor.ref() as the key.' % self)\n+    raise TypeError(f'Tensors are unhashable (this tensor: {self}). '\n+                    'Instead, use tensor.ref() as the key.')\n \n   # Note: This enables the KerasTensor's overloaded \"right\" binary\n   # operators to run when the left operand is an ndarray, because it\n@@ -267,9 +267,8 @@ class KerasTensor:\n       shape = tf.TensorShape(dim_list)\n     if not self.shape.is_compatible_with(shape):\n       raise ValueError(\n-          \"Keras symbolic input/output's shape %s is not\"\n-          \"compatible with supplied shape %s\" %\n-          (self.shape, shape))\n+          f\"Keras symbolic input/output's shape {self.shape} is not \"\n+          f\"compatible with supplied shape {shape}.\")\n     else:\n       self._type_spec._shape = shape  # pylint: disable=protected-access\n \n\n@@ -175,16 +175,16 @@ class Sequential(functional.Functional):\n       if not isinstance(layer, base_layer.Layer):\n         layer = functional.ModuleWrapper(layer)\n     else:\n-      raise TypeError('The added layer must be '\n-                      'an instance of class Layer. '\n-                      'Found: ' + str(layer))\n+      raise TypeError('The added layer must be an instance of class Layer. '\n+                      f'Received: layer={layer} of type {type(layer)}.')\n \n     tf_utils.assert_no_legacy_layers([layer])\n     if not self._is_layer_name_unique(layer):\n-      raise ValueError('All layers added to a Sequential model '\n-                       'should have unique names. Name \"%s\" is already the name'\n+      raise ValueError(\n+          'All layers added to a Sequential model '\n+          f'should have unique names. Name \"{layer.name}\" is already the name '\n           'of a layer in this model. Update the `name` argument '\n-                       'to pass a unique name.' % (layer.name,))\n+          'to pass a unique name.')\n \n     self.built = False\n     set_inputs = False\n\n@@ -1099,7 +1099,7 @@ class TrainingTest(keras_parameterized.TestCase):\n       training_module.Model([input1, input2], outputs)\n       self.assertEqual(\n           mock_warn.call_args_list[0][0][0],\n-          'Found incompatiable static batch sizes among all the inputs. '\n+          'Found incompatiable static batch sizes among the inputs. '\n           'Batch sizes: [2, 3]')\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b693bb84200d70aa736f2491ff83509fd1b1b6fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 38 | Files Changed: 6 | Hunks: 30 | Methods Changed: 18 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 105 | Churn Cumulative: 16297 | Contributors (this commit): 52 | Commits (past 90d): 29 | Contributors (cumulative): 74 | DMM Complexity: 0.8928571428571429\n\nDIFF:\n@@ -85,7 +85,7 @@ def softmax(x, axis=-1):\n       output = e / s\n   else:\n     raise ValueError('Cannot apply softmax to a tensor that is 1D. '\n-                     'Received input: %s' % (x,))\n+                     f'Received input: {x}')\n \n   # Cache the logits to use for crossentropy loss.\n   output._keras_logits = x  # pylint: disable=protected-access\n@@ -600,5 +600,4 @@ def get(identifier):\n     return identifier\n   else:\n     raise TypeError(\n-        'Could not interpret activation function identifier: {}'.format(\n-            identifier))\n+        f'Could not interpret activation function identifier: {identifier}')\n\n@@ -103,8 +103,10 @@ def set_floatx(value):\n       ValueError: In case of invalid value.\n   \"\"\"\n   global _FLOATX\n-  if value not in {'float16', 'float32', 'float64'}:\n-    raise ValueError('Unknown floatx type: ' + str(value))\n+  accepted_dtypes = {'float16', 'float32', 'float64'}\n+  if value not in accepted_dtypes:\n+    raise ValueError(\n+        f'Unknown `floatx` value: {value}. Expected one of {accepted_dtypes}')\n   _FLOATX = str(value)\n \n \n@@ -142,6 +144,9 @@ def set_image_data_format(data_format):\n       ValueError: In case of invalid `data_format` value.\n   \"\"\"\n   global _IMAGE_DATA_FORMAT\n-  if data_format not in {'channels_last', 'channels_first'}:\n-    raise ValueError('Unknown data_format: ' + str(data_format))\n+  accepted_formats = {'channels_last', 'channels_first'}\n+  if data_format not in accepted_formats:\n+    raise ValueError(\n+        f'Unknown `data_format`: {data_format}. '\n+        f'Expected one of {accepted_formats}')\n   _IMAGE_DATA_FORMAT = str(data_format)\n\n@@ -257,7 +257,8 @@ class RadialConstraint(Constraint):\n     w_shape = w.shape\n     if w_shape.rank is None or w_shape.rank != 4:\n       raise ValueError(\n-          'The weight tensor must be of rank 4, but is of shape: %s' % w_shape)\n+          'The weight tensor must have rank 4. '\n+          f'Received weight tensor with shape: {w_shape}')\n \n     height, width, channels, kernels = w_shape\n     w = backend.reshape(w, (height, width, channels * kernels))\n@@ -332,6 +333,7 @@ def deserialize(config, custom_objects=None):\n \n @keras_export('keras.constraints.get')\n def get(identifier):\n+  \"\"\"Retrieves a Keras constraint function.\"\"\"\n   if identifier is None:\n     return None\n   if isinstance(identifier, dict):\n@@ -342,5 +344,5 @@ def get(identifier):\n   elif callable(identifier):\n     return identifier\n   else:\n-    raise ValueError('Could not interpret constraint identifier: ' +\n-                     str(identifier))\n+    raise ValueError(\n+        f'Could not interpret constraint function identifier: {identifier}')\n\n@@ -394,9 +394,9 @@ class Reduce(Metric):\n       values = tf.cast(values, self._dtype)\n     except (ValueError, TypeError):\n       msg = ('The output of a metric function can only be a single Tensor. '\n-             'Got: %s' % (values,))\n+             f'Received: {values}. ')\n       if isinstance(values, dict):\n-        msg += ('. To return a dict of values, implement a custom Metric '\n+        msg += ('To return a dict of values, implement a custom Metric '\n                 'subclass.')\n       raise RuntimeError(msg)\n     if sample_weight is not None:\n@@ -438,7 +438,8 @@ class Reduce(Metric):\n         num_values = tf.reduce_sum(sample_weight)\n     else:\n       raise NotImplementedError(\n-          'reduction [%s] not implemented' % self.reduction)\n+          f'Reduction \"{self.reduction}\" not implemented. Expected '\n+          '\"sum\", \"weighted_mean\", or \"sum_over_batch_size\".')\n \n     with tf.control_dependencies([update_total_op]):\n       return self.count.assign_add(num_values)\n@@ -453,7 +454,8 @@ class Reduce(Metric):\n       return tf.math.divide_no_nan(self.total, self.count)\n     else:\n       raise NotImplementedError(\n-          'reduction [%s] not implemented' % self.reduction)\n+          f'Reduction \"{self.reduction}\" not implemented. Expected '\n+          '\"sum\", \"weighted_mean\", or \"sum_over_batch_size\".')\n \n \n @keras_export('keras.metrics.Sum')\n@@ -1530,7 +1532,9 @@ class SensitivitySpecificityBase(Metric, metaclass=abc.ABCMeta):\n                dtype=None):\n     super(SensitivitySpecificityBase, self).__init__(name=name, dtype=dtype)\n     if num_thresholds <= 0:\n-      raise ValueError('`num_thresholds` must be > 0.')\n+      raise ValueError(\n+          'Argument `num_thresholds` must be an integer > 0. '\n+          f'Received: num_thresholds={num_thresholds}')\n     self.value = value\n     self.class_id = class_id\n     self.true_positives = self.add_weight(\n@@ -1689,7 +1693,9 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n                name=None,\n                dtype=None):\n     if specificity < 0 or specificity > 1:\n-      raise ValueError('`specificity` must be in the range [0, 1].')\n+      raise ValueError(\n+          'Argument `specificity` must be in the range [0, 1]. '\n+          f'Received: specificity={specificity}')\n     self.specificity = specificity\n     self.num_thresholds = num_thresholds\n     super(SensitivityAtSpecificity, self).__init__(\n@@ -1781,7 +1787,9 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n                name=None,\n                dtype=None):\n     if sensitivity < 0 or sensitivity > 1:\n-      raise ValueError('`sensitivity` must be in the range [0, 1].')\n+      raise ValueError(\n+          'Argument `sensitivity` must be in the range [0, 1]. '\n+          f'Received: sensitivity={sensitivity}')\n     self.sensitivity = sensitivity\n     self.num_thresholds = num_thresholds\n     super(SpecificityAtSensitivity, self).__init__(\n@@ -1865,7 +1873,9 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n                name=None,\n                dtype=None):\n     if recall < 0 or recall > 1:\n-      raise ValueError('`recall` must be in the range [0, 1].')\n+      raise ValueError(\n+          'Argument `recall` must be in the range [0, 1]. '\n+          f'Received: recall={recall}')\n     self.recall = recall\n     self.num_thresholds = num_thresholds\n     super(PrecisionAtRecall, self).__init__(\n@@ -1949,7 +1959,9 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n                name=None,\n                dtype=None):\n     if precision < 0 or precision > 1:\n-      raise ValueError('`precision` must be in the range [0, 1].')\n+      raise ValueError(\n+          'Argument `precision` must be in the range [0, 1]. '\n+          f'Received: precision={precision}')\n     self.precision = precision\n     self.num_thresholds = num_thresholds\n     super(RecallAtPrecision, self).__init__(\n@@ -2105,15 +2117,16 @@ class AUC(Metric):\n     # Validate configurations.\n     if isinstance(curve, metrics_utils.AUCCurve) and curve not in list(\n         metrics_utils.AUCCurve):\n-      raise ValueError('Invalid curve: \"{}\". Valid options are: \"{}\"'.format(\n-          curve, list(metrics_utils.AUCCurve)))\n+      raise ValueError(\n+          f'Invalid `curve` argument value \"{curve}\". '\n+          f'Expected one of: {list(metrics_utils.AUCCurve)}')\n     if isinstance(\n         summation_method,\n         metrics_utils.AUCSummationMethod) and summation_method not in list(\n             metrics_utils.AUCSummationMethod):\n       raise ValueError(\n-          'Invalid summation method: \"{}\". Valid options are: \"{}\"'.format(\n-              summation_method, list(metrics_utils.AUCSummationMethod)))\n+          f'Invalid `summation_method` argument value \"{summation_method}\". '\n+          f'Expected one of: {list(metrics_utils.AUCSummationMethod)}')\n \n     # Update properties.\n     self._init_from_thresholds = thresholds is not None\n@@ -2126,7 +2139,8 @@ class AUC(Metric):\n               np.array([0.0] + thresholds + [1.0])))\n     else:\n       if num_thresholds <= 1:\n-        raise ValueError('`num_thresholds` must be > 1.')\n+        raise ValueError('Argument `num_thresholds` must be an integer > 1. '\n+                         f'Received: num_thresholds={num_thresholds}')\n \n       # Otherwise, linearly interpolate (num_thresholds - 2) thresholds in\n       # (0, 1).\n@@ -2188,8 +2202,10 @@ class AUC(Metric):\n     \"\"\"Initialize TP, FP, TN, and FN tensors, given the shape of the data.\"\"\"\n     if self.multi_label:\n       if shape.ndims != 2:\n-        raise ValueError('`y_true` must have rank=2 when `multi_label` is '\n-                         'True. Found rank %s.' % shape.ndims)\n+        raise ValueError(\n+            '`y_true` must have rank 2 when `multi_label=True`. '\n+            f'Found rank {shape.ndims}. '\n+            f'Full shape received for `y_true`: {shape}')\n       self._num_labels = shape[1]\n       variable_shape = tf.TensorShape(\n           [tf.compat.v1.Dimension(self.num_thresholds), self._num_labels])\n@@ -3136,9 +3152,10 @@ class MeanTensor(Metric):\n     if not self._built:\n       self._build(values.shape)\n     elif values.shape != self._shape:\n-      raise ValueError('MeanTensor input values must always have the same '\n-                       'shape. Expected shape (set during the first call): {}. '\n-                       'Got: {}'.format(self._shape, values.shape))\n+      raise ValueError(\n+          'MeanTensor input values must always have the same '\n+          f'shape. Expected shape (set during the first call): {self._shape}. '\n+          f'Got: {values.shape}.')\n \n     num_values = tf.ones_like(values)\n     if sample_weight is not None:\n@@ -3168,7 +3185,7 @@ class MeanTensor(Metric):\n   def result(self):\n     if not self._built:\n       raise ValueError(\n-          'MeanTensor does not have any result yet. Please call the MeanTensor '\n+          'MeanTensor does not have any value yet. Please call the MeanTensor '\n           'instance or use `.update_state(value)` before retrieving the result.'\n           )\n     return tf.math.divide_no_nan(self.total, self.count)\n@@ -3715,7 +3732,7 @@ def get(identifier):\n     return identifier\n   else:\n     raise ValueError(\n-        'Could not interpret metric function identifier: {}'.format(identifier))\n+        f'Could not interpret metric identifier: {identifier}')\n \n \n def is_built_in(cls):\n\n@@ -829,7 +829,8 @@ class SensitivityAtSpecificityTest(tf.test.TestCase, parameterized.TestCase):\n       metrics.SensitivityAtSpecificity(-1)\n \n   def test_invalid_num_thresholds(self):\n-    with self.assertRaisesRegex(ValueError, '`num_thresholds` must be > 0.'):\n+    with self.assertRaisesRegex(\n+        ValueError, 'Argument `num_thresholds` must be an integer > 0'):\n       metrics.SensitivityAtSpecificity(0.4, num_thresholds=-1)\n \n \n@@ -941,7 +942,8 @@ class SpecificityAtSensitivityTest(tf.test.TestCase, parameterized.TestCase):\n       metrics.SpecificityAtSensitivity(-1)\n \n   def test_invalid_num_thresholds(self):\n-    with self.assertRaisesRegex(ValueError, '`num_thresholds` must be > 0.'):\n+    with self.assertRaisesRegex(\n+        ValueError, 'Argument `num_thresholds` must be an integer > 0'):\n       metrics.SpecificityAtSensitivity(0.4, num_thresholds=-1)\n \n \n@@ -1054,7 +1056,8 @@ class PrecisionAtRecallTest(tf.test.TestCase, parameterized.TestCase):\n       metrics.PrecisionAtRecall(-1)\n \n   def test_invalid_num_thresholds(self):\n-    with self.assertRaisesRegex(ValueError, '`num_thresholds` must be > 0.'):\n+    with self.assertRaisesRegex(\n+        ValueError, 'Argument `num_thresholds` must be an integer > 0'):\n       metrics.PrecisionAtRecall(0.4, num_thresholds=-1)\n \n \n@@ -1186,7 +1189,8 @@ class RecallAtPrecisionTest(tf.test.TestCase, parameterized.TestCase):\n       metrics.RecallAtPrecision(-1)\n \n   def test_invalid_num_thresholds(self):\n-    with self.assertRaisesRegex(ValueError, '`num_thresholds` must be > 0.'):\n+    with self.assertRaisesRegex(\n+        ValueError, 'Argument `num_thresholds` must be an integer > 0'):\n       metrics.RecallAtPrecision(0.4, num_thresholds=-1)\n \n \n@@ -1453,10 +1457,12 @@ class AUCTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, 1e-3)\n \n   def test_invalid_num_thresholds(self):\n-    with self.assertRaisesRegex(ValueError, '`num_thresholds` must be > 1.'):\n+    with self.assertRaisesRegex(\n+        ValueError, 'Argument `num_thresholds` must be an integer > 1'):\n       metrics.AUC(num_thresholds=-1)\n \n-    with self.assertRaisesRegex(ValueError, '`num_thresholds` must be > 1.'):\n+    with self.assertRaisesRegex(\n+        ValueError, 'Argument `num_thresholds` must be an integer > 1.'):\n       metrics.AUC(num_thresholds=1)\n \n   def test_invalid_curve(self):\n\n@@ -1379,7 +1379,7 @@ class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(m.dtype, tf.float32)\n       self.assertEmpty(m.variables)\n \n-      with self.assertRaisesRegex(ValueError, 'does not have any result yet'):\n+      with self.assertRaisesRegex(ValueError, 'does not have any value yet'):\n         m.result()\n \n       self.evaluate(m([[3], [5], [3]]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d79a2e37734ca141d1b024d37845ca3b02d0bfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 1943 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -158,6 +158,27 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     result = f([t_val, p_val])\n     self.assertArrayNear(result, [.002, 0, .17], 1e-3)\n \n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def test_sparse_categorical_crossentropy_with_float16(self):\n+    # See https://github.com/keras-team/keras/issues/15012 for more details.\n+    # we don't cast y_true to have same dtype as y_pred, since y_pred could be\n+    # float16 which has a small upbound, and the casting could cause an\n+    # underflow. The y_true will be used as int64 anyway.\n+\n+    # create 2 observations with 2049 labels, since 2048 is the largest number\n+    # for float16\n+    y_true = [0, 2049]\n+    # should result in a loss close to 0 since predicting y_true perfectly\n+    y_pred = np.zeros((2, 2050))\n+    y_pred[0][0] = 1\n+    y_pred[1][2049] = 1\n+    y_pred_16 = tf.convert_to_tensor(y_pred, dtype=tf.float16)\n+\n+    # If we did a cast for y_true to float16 in SparseCategoricalCrossentropy,\n+    # then the loss will not be zero.\n+    scce = losses.SparseCategoricalCrossentropy()\n+    self.assertAllClose(scce(y_true, y_pred_16).numpy(), 0.0, atol=1e-3)\n+\n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def test_binary_crossentropy_loss(self):\n     target = backend.variable(np.random.randint(0, 1, (5, 1)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7f83d04ba782c7244615fb0d75e2916f60f63f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 38 | Churn Cumulative: 12744 | Contributors (this commit): 91 | Commits (past 90d): 2 | Contributors (cumulative): 91 | DMM Complexity: 0.0\n\nDIFF:\n@@ -157,15 +157,15 @@ def _clone_functional_model(model, input_tensors=None, layer_fn=_clone_layer):\n   \"\"\"\n   if not isinstance(model, Model):\n     raise ValueError('Expected `model` argument '\n-                     'to be a `Model` instance, got ', model)\n+                     f'to be a `Model` instance. Received: model={model}')\n   if isinstance(model, Sequential):\n     raise ValueError('Expected `model` argument '\n                      'to be a functional `Model` instance, '\n-                     'got a `Sequential` instance instead:', model)\n+                     f'got a `Sequential` instance instead: {model}')\n   if not model._is_graph_network:\n     raise ValueError('Expected `model` argument '\n                      'to be a functional `Model` instance, '\n-                     'but got a subclass model instead.')\n+                     f'but got a subclassed model instead: {model}')\n \n   new_input_layers = {}  # Cache for created layers.\n   if input_tensors is not None:\n@@ -186,7 +186,8 @@ def _clone_functional_model(model, input_tensors=None, layer_fn=_clone_layer):\n         new_input_layers[original_input_layer] = original_input_layer\n \n   if not callable(layer_fn):\n-    raise ValueError('Expected `layer_fn` argument to be a callable.')\n+    raise ValueError('Expected `layer_fn` argument to be a callable. '\n+                     f'Received: layer_fn={layer_fn}')\n \n   model_configs, created_layers = _clone_layers_and_model_config(\n       model, new_input_layers, layer_fn)\n@@ -305,11 +306,13 @@ def _clone_sequential_model(model, input_tensors=None, layer_fn=_clone_layer):\n   \"\"\"\n   if not isinstance(model, Sequential):\n     raise ValueError('Expected `model` argument '\n-                     'to be a `Sequential` model instance, '\n-                     'but got:', model)\n+                     'to be a `Sequential` model instance. '\n+                     f'Received: model={model}')\n \n   if not callable(layer_fn):\n-    raise ValueError('Expected `layer_fn` argument to be a callable.')\n+    raise ValueError(\n+        'Expected `layer_fn` argument to be a callable. '\n+        f'Received: layer_fn={layer_fn}')\n \n   layers = []  # Layers needed to compute the model's outputs.\n   layer_map = {}\n@@ -331,9 +334,9 @@ def _clone_sequential_model(model, input_tensors=None, layer_fn=_clone_layer):\n   if input_tensors is None:\n     cloned_model = Sequential(layers=layers, name=model.name)\n   elif len(generic_utils.to_list(input_tensors)) != 1:\n-    raise ValueError('To clone a `Sequential` model, we expect '\n-                     ' at most one tensor '\n-                     'as part of `input_tensors`.')\n+    raise ValueError(\n+        'To clone a `Sequential` model, we expect at most one tensor as part '\n+        f'of `input_tensors`. Received: input_tensors={input_tensors}')\n   else:\n     # Overwrite the original model's input layer.\n     if isinstance(input_tensors, tuple):\n@@ -348,7 +351,8 @@ def _clone_sequential_model(model, input_tensors=None, layer_fn=_clone_layer):\n         raise ValueError('Cannot clone a `Sequential` model on top '\n                          'of a tensor that comes from a Keras layer '\n                          'other than an `InputLayer`. '\n-                         'Use the functional API instead.')\n+                         'Use the Functional API instead. '\n+                         f'Received: input_tensors={input_tensors}')\n     else:\n       input_tensor = Input(tensor=x, name='input_wrapper_for_' + str(x.name))\n       input_layer = input_tensor._keras_history.layer\n@@ -495,7 +499,7 @@ def _in_place_subclassed_model_reset(model):\n       if hasattr(value, 'layers') and value.layers:\n         raise ValueError('We do not support the use of nested layers '\n                          'in `model_to_estimator` at this time. Found nested '\n-                         'layer: %s' % value)\n+                         f'layer: {value}')\n     elif isinstance(\n         value, (list, tuple)) and name not in ('layers', '_layers', 'metrics',\n                                                '_compile_metric_functions',\n@@ -505,7 +509,7 @@ def _in_place_subclassed_model_reset(model):\n         raise ValueError('We do not support the use of list-of-layers '\n                          'attributes in subclassed models used with '\n                          '`model_to_estimator` at this time. Found list '\n-                         'model: %s' % name)\n+                         f'model: {name}')\n \n   # Replace layers on the model with fresh layers\n   layers_to_names = {value: key for key, value in attributes_cache.items()}\n@@ -522,7 +526,7 @@ def _in_place_subclassed_model_reset(model):\n     if isinstance(layer, training.Model) and not layer._is_graph_network:\n       raise ValueError('We do not support the use of nested subclassed models '\n                        'in `model_to_estimator` at this time. Found nested '\n-                       'model: %s' % layer)\n+                       f'model: {layer}')\n     fresh_layer = layer.__class__.from_config(config)\n     name = layers_to_names[layer]\n     setattr(model, name, fresh_layer)\n@@ -662,8 +666,8 @@ def clone_and_build_model(\n   orig_optimizer = model.optimizer\n   if compile_clone and not orig_optimizer:\n     raise ValueError(\n-        'Error when cloning model: compile_clone was set to True, but the '\n-        'original model has not been compiled.')\n+        'Error when cloning model: `compile_clone` was set to True, but the '\n+        f'original model has not been compiled. Received: model={model}')\n \n   if compile_clone:\n     compile_args = model._get_compile_args()  # pylint: disable=protected-access\n@@ -693,7 +697,7 @@ def clone_and_build_model(\n                         'cloning the model.')\n         if not in_place_reset:\n           raise ValueError(\n-              'This model is a subclassed model. '\n+              f'This model ({model}) is a subclassed model. '\n               'Such a model cannot be cloned, but there is a workaround where '\n               'the model is reset in-place. To use this, please set the '\n               'argument `in_place_reset` to `True`. This will reset the '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fdf91ec03325eb2900d87ccaf572815968779404", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 187 | Lines Deleted: 170 | Files Changed: 8 | Hunks: 72 | Methods Changed: 42 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 357 | Churn Cumulative: 25749 | Contributors (this commit): 85 | Commits (past 90d): 18 | Contributors (cumulative): 123 | DMM Complexity: 0.0\n\nDIFF:\n@@ -94,8 +94,7 @@ class DistributionStrategyStatefulLstmModelCorrectnessTest(\n   def test_incorrectly_use_multiple_cores_for_stateful_lstm_model(\n       self, distribution, use_numpy, use_validation_data):\n     with self.assertRaisesRegex(\n-        ValueError, 'RNNs with stateful=True not yet supported with '\n-        'tf.distribute.Strategy.'):\n+        ValueError, 'not yet supported with tf.distribute.Strategy'):\n       self.run_correctness_test(\n           distribution,\n           use_numpy,\n\n@@ -137,11 +137,15 @@ class ConvRNN(RNN):\n                unroll=False,\n                **kwargs):\n     if unroll:\n-      raise TypeError('Unrolling isn\\'t possible with convolutional RNNs.')\n+      raise TypeError(\n+          'Unrolling is not possible with convolutional RNNs. '\n+          f'Received: unroll={unroll}')\n     if isinstance(cell, (list, tuple)):\n       # The StackedConvRNN3DCells isn't implemented yet.\n       raise TypeError('It is not possible at the moment to'\n-                      'stack convolutional cells.')\n+                      'stack convolutional cells. Only pass a single cell '\n+                      'instance as the `cell` argument. Received: '\n+                      f'cell={cell}')\n     super(ConvRNN, self).__init__(cell, return_sequences, return_state,\n                                   go_backwards, stateful, unroll, **kwargs)\n     self.rank = rank\n@@ -226,11 +230,10 @@ class ConvRNN(RNN):\n         ch_dim = self.rank + 1\n       if [spec.shape[ch_dim] for spec in self.state_spec] != state_size:\n         raise ValueError(\n-            'An initial_state was passed that is not compatible with '\n-            '`cell.state_size`. Received `state_spec`={}; '\n-            'However `cell.state_size` is '\n-            '{}'.format([spec.shape for spec in self.state_spec],\n-                        self.cell.state_size))\n+            'An `initial_state` was passed that is not compatible with '\n+            '`cell.state_size`. Received state shapes '\n+            f'{[spec.shape for spec in self.state_spec]}. '\n+            f'However `cell.state_size` is {self.cell.state_size}')\n     else:\n       img_dims = tuple((None for _ in range(self.rank)))\n       if self.cell.data_format == 'channels_first':\n@@ -283,7 +286,9 @@ class ConvRNN(RNN):\n \n     if constants:\n       if not generic_utils.has_arg(self.cell.call, 'constants'):\n-        raise ValueError('RNN cell does not support constants')\n+        raise ValueError(\n+            f'RNN cell {self.cell} does not support constants. '\n+            f'Received: constants={constants}')\n \n       def step(inputs, states):\n         constants = states[-self._num_constants:]  # pylint: disable=invalid-unary-operand-type\n@@ -351,7 +356,10 @@ class ConvRNN(RNN):\n       elif self.cell.data_format == 'channels_last':\n         result[self.rank + 1] = nb_channels\n       else:\n-        raise KeyError\n+        raise KeyError(\n+            'Cell data format must be one of '\n+            '{\"channels_first\", \"channels_last\"}. Received: '\n+            f'cell.data_format={self.cell.data_format}')\n       return tuple(result)\n \n     # initialize state if None\n@@ -372,22 +380,20 @@ class ConvRNN(RNN):\n       if not isinstance(states, (list, tuple)):\n         states = [states]\n       if len(states) != len(self.states):\n-        raise ValueError('Layer ' + self.name + ' expects ' +\n-                         str(len(self.states)) + ' states, ' +\n-                         'but it received ' + str(len(states)) +\n-                         ' state values. Input received: ' + str(states))\n+        raise ValueError(\n+            f'Layer {self.name} expects {len(self.states)} states, '\n+            f'but it received {len(states)} state values. '\n+            f'States received: {states}')\n       for index, (value, state) in enumerate(zip(states, self.states)):\n         if hasattr(self.cell.state_size, '__len__'):\n           dim = self.cell.state_size[index]\n         else:\n           dim = self.cell.state_size\n         if value.shape != get_tuple_shape(dim):\n-          raise ValueError('State ' + str(index) +\n-                           ' is incompatible with layer ' +\n-                           self.name + ': expected shape=' +\n-                           str(get_tuple_shape(dim)) +\n-                           ', found shape=' + str(value.shape))\n-        # TODO(anjalisridhar): consider batch calls to `set_value`.\n+          raise ValueError(\n+              f'State {index} is incompatible with layer {self.name}: '\n+              f'expected shape={get_tuple_shape(dim)}, '\n+              f'found shape={value.shape}')\n         backend.set_value(state, value)\n \n \n@@ -478,8 +484,8 @@ class ConvLSTMCell(DropoutRNNCellMixin, Layer):\n     super(ConvLSTMCell, self).__init__(**kwargs)\n     self.rank = rank\n     if self.rank > 3:\n-      raise ValueError('Rank ' + str(self.rank) +\n-                       ' convolutions not currently implemented.')\n+      raise ValueError(f'Rank {rank} convolutions are not currently '\n+                       f'implemented. Received: rank={rank}')\n     self.filters = filters\n     self.kernel_size = conv_utils.normalize_tuple(kernel_size, self.rank,\n                                                   'kernel_size')\n@@ -516,8 +522,9 @@ class ConvLSTMCell(DropoutRNNCellMixin, Layer):\n     else:\n       channel_axis = -1\n     if input_shape[channel_axis] is None:\n-      raise ValueError('The channel dimension of the inputs '\n-                       'should be defined. Found `None`.')\n+      raise ValueError(\n+          'The channel dimension of the inputs (last axis) should be defined. '\n+          f'Found None. Full input shape received: input_shape={input_shape}')\n     input_dim = input_shape[channel_axis]\n     self.kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n     recurrent_kernel_shape = self.kernel_size + (self.filters, self.filters * 4)\n\n@@ -167,28 +167,18 @@ class EinsumDense(Layer):\n \n   def get_config(self):\n     config = {\n-        \"output_shape\":\n-            self.partial_output_shape,\n-        \"equation\":\n-            self.equation,\n-        \"activation\":\n-            activations.serialize(self.activation),\n-        \"bias_axes\":\n-            self.bias_axes,\n-        \"kernel_initializer\":\n-            initializers.serialize(self.kernel_initializer),\n-        \"bias_initializer\":\n-            initializers.serialize(self.bias_initializer),\n-        \"kernel_regularizer\":\n-            regularizers.serialize(self.kernel_regularizer),\n-        \"bias_regularizer\":\n-            regularizers.serialize(self.bias_regularizer),\n+        \"output_shape\": self.partial_output_shape,\n+        \"equation\": self.equation,\n+        \"activation\": activations.serialize(self.activation),\n+        \"bias_axes\": self.bias_axes,\n+        \"kernel_initializer\": initializers.serialize(self.kernel_initializer),\n+        \"bias_initializer\": initializers.serialize(self.bias_initializer),\n+        \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\n+        \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n         \"activity_regularizer\":\n             regularizers.serialize(self.activity_regularizer),\n-        \"kernel_constraint\":\n-            constraints.serialize(self.kernel_constraint),\n-        \"bias_constraint\":\n-            constraints.serialize(self.bias_constraint),\n+        \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n+        \"bias_constraint\": constraints.serialize(self.bias_constraint),\n     }\n     base_config = super(EinsumDense, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n@@ -229,8 +219,8 @@ def _analyze_einsum_string(equation, bias_axes, input_shape, output_shape):\n                                  output_shape)\n \n   raise ValueError(\n-      \"Invalid einsum equation '%s'. Equations must be in the form \"\n-      \"[X],[Y]->[Z], ...[X],[Y]->...[Z], or [X]...,[Y]->[Z]....\" % equation)\n+      f\"Invalid einsum equation '{equation}'. Equations must be in the form \"\n+      \"[X],[Y]->[Z], ...[X],[Y]->...[Z], or [X]...,[Y]->[Z]....\")\n \n \n def _analyze_split_string(split_string,\n@@ -281,16 +271,16 @@ def _analyze_split_string(split_string,\n           output_shape_at_dim != input_shape_at_dim):\n         raise ValueError(\n             \"Input shape and output shape do not match at shared \"\n-            \"dimension '%s'. Input shape is %s, and output shape \"\n-            \"is %s.\" %\n-            (dim, input_shape_at_dim, output_shape[output_dim_map[dim]]))\n+            f\"dimension '{dim}'. Input shape is {input_shape_at_dim}, \"\n+            \"and output shape \"\n+            f\"is {output_shape[output_dim_map[dim]]}.\")\n \n   for dim in output_spec:\n     if dim not in input_spec and dim not in weight_spec:\n-      raise ValueError(\"Dimension '%s' was specified in the output '%s' but \"\n-                       \"has no corresponding dim in the input spec '%s' or \"\n-                       \"weight spec '%s.'\" % (dim, output_spec, input_spec,\n-                                              output_spec))\n+      raise ValueError(\n+          f\"Dimension '{dim}' was specified in the output '{output_spec}' but \"\n+          f\"has no corresponding dim in the input spec '{input_spec}' or \"\n+          f\"weight spec '{output_spec}'\")\n \n   weight_shape = []\n   for dim in weight_spec:\n@@ -299,10 +289,10 @@ def _analyze_split_string(split_string,\n     elif dim in output_dim_map:\n       weight_shape.append(output_shape[output_dim_map[dim]])\n     else:\n-      raise ValueError(\"Weight dimension '%s' did not have a match in either \"\n-                       \"the input spec '%s' or the output spec '%s'. For this \"\n-                       \"layer, the weight must be fully specified.\" %\n-                       (dim, input_spec, output_spec))\n+      raise ValueError(\n+          f\"Weight dimension '{dim}' did not have a match in either \"\n+          f\"the input spec '{input_spec}' or the output spec '{output_spec}'. \"\n+          \"For this layer, the weight must be fully specified.\")\n \n   if bias_axes is not None:\n     num_left_elided = elided if left_elided else 0\n@@ -313,9 +303,9 @@ def _analyze_split_string(split_string,\n \n     for char in bias_axes:\n       if char not in output_spec:\n-        raise ValueError(\"Bias dimension '%s' was requested, but is not a part \"\n-                         \"of the output specification '%s'\" %\n-                         (char, output_spec))\n+        raise ValueError(\n+            f\"Bias dimension '{char}' was requested, but is not part \"\n+            f\"of the output spec '{output_spec}'\")\n \n     first_bias_location = min([output_spec.find(char) for char in bias_axes])\n     bias_output_spec = output_spec[first_bias_location:]\n\n@@ -280,7 +280,7 @@ class TestEinsumLayerAPI(keras_parameterized.TestCase):\n     layer = einsum_dense.EinsumDense(\n         equation=\"ab,bc->ac\", output_shape=64, bias_axes=\"y\")\n     with self.assertRaisesRegex(\n-        ValueError, \".*is not a part of the output specification.*\"):\n+        ValueError, \".*is not part of the output spec.*\"):\n       _ = layer(input_tensor)\n \n   def test_incompatible_input_output_shape_fails(self):\n\n@@ -146,16 +146,15 @@ class RandomFourierFeatures(base_layer.Layer):\n                **kwargs):\n     if output_dim <= 0:\n       raise ValueError(\n-          '`output_dim` should be a positive integer. Given: {}.'.format(\n-              output_dim))\n+          f'`output_dim` should be a positive integer. Received: {output_dim}')\n     if isinstance(kernel_initializer, str):\n       if kernel_initializer.lower() not in _SUPPORTED_RBF_KERNEL_TYPES:\n         raise ValueError(\n-            'Unsupported kernel type: \\'{}\\'. Supported kernel types: {}.'\n-            .format(kernel_initializer, _SUPPORTED_RBF_KERNEL_TYPES))\n+            f'Unsupported `kernel_initializer`: {kernel_initializer} '\n+            f'Expected one of: {_SUPPORTED_RBF_KERNEL_TYPES}')\n     if scale is not None and scale <= 0.0:\n       raise ValueError('When provided, `scale` should be a positive float. '\n-                       'Given: {}.'.format(scale))\n+                       f'Received: {scale}')\n     super(RandomFourierFeatures, self).__init__(\n         trainable=trainable, name=name, **kwargs)\n     self.output_dim = output_dim\n@@ -168,12 +167,13 @@ class RandomFourierFeatures(base_layer.Layer):\n     # to have shape [batch_size, dimension].\n     if input_shape.rank != 2:\n       raise ValueError(\n-          'The rank of the input tensor should be 2. Got {} instead.'.format(\n-              input_shape.ndims))\n+          'The rank of the input tensor should be 2. '\n+          f'Received input with rank {input_shape.ndims} instead. '\n+          f'Full input shape received: {input_shape}')\n     if input_shape.dims[1].value is None:\n       raise ValueError(\n-          'The last dimension of the inputs to `RandomFourierFeatures` '\n-          'should be defined. Found `None`.')\n+          'The last dimension of the input tensor should be defined. '\n+          f'Found `None`. Full input shape received: {input_shape}')\n     self.input_spec = input_spec.InputSpec(\n         ndim=2, axes={1: input_shape.dims[1].value})\n     input_dim = input_shape.dims[1].value\n@@ -220,8 +220,8 @@ class RandomFourierFeatures(base_layer.Layer):\n     input_shape = input_shape.with_rank(2)\n     if input_shape.dims[-1].value is None:\n       raise ValueError(\n-          'The innermost dimension of input shape must be defined. Given: %s' %\n-          input_shape)\n+          'The last dimension of the input tensor should be defined. '\n+          f'Found `None`. Full input shape received: {input_shape}')\n     return input_shape[:-1].concatenate(self.output_dim)\n \n   def get_config(self):\n@@ -255,8 +255,8 @@ def _get_random_features_initializer(initializer, shape):\n \n     else:\n       raise ValueError(\n-          'Unsupported kernel type: \\'{}\\'. Supported kernel types: {}.'.format(\n-              random_features_initializer, _SUPPORTED_RBF_KERNEL_TYPES))\n+          f'Unsupported `kernel_initializer`: \"{initializer}\" '\n+          f'Expected one of: {_SUPPORTED_RBF_KERNEL_TYPES}')\n   return random_features_initializer\n \n \n\n@@ -76,19 +76,19 @@ class RandomFourierFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_invalid_output_dim(self):\n     with self.assertRaisesRegex(\n-        ValueError, r'`output_dim` should be a positive integer. Given: -3.'):\n+        ValueError, '`output_dim` should be a positive integer'):\n       _ = kernel_layers.RandomFourierFeatures(output_dim=-3, scale=2.0)\n \n   def test_unsupported_kernel_type(self):\n     with self.assertRaisesRegex(\n-        ValueError, r'Unsupported kernel type: \\'unsupported_kernel\\'.'):\n+        ValueError, 'Unsupported `kernel_initializer`'):\n       _ = kernel_layers.RandomFourierFeatures(\n           3, 'unsupported_kernel', stddev=2.0)\n \n   def test_invalid_scale(self):\n     with self.assertRaisesRegex(\n         ValueError,\n-        r'When provided, `scale` should be a positive float. Given: 0.0.'):\n+        'When provided, `scale` should be a positive float'):\n       _ = kernel_layers.RandomFourierFeatures(output_dim=10, scale=0.0)\n \n   def test_invalid_input_shape(self):\n@@ -96,7 +96,7 @@ class RandomFourierFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n     rff_layer = kernel_layers.RandomFourierFeatures(output_dim=10, scale=3.0)\n     with self.assertRaisesRegex(\n         ValueError,\n-        r'The rank of the input tensor should be 2. Got 3 instead.'):\n+        'The rank of the input tensor should be 2'):\n       _ = rff_layer(inputs)\n \n   @parameterized.named_parameters(\n@@ -155,8 +155,8 @@ class RandomFourierFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n           kernel_initializer=initializer,\n           name='random_fourier_features')\n       with self.assertRaisesRegex(\n-          ValueError, r'The last dimension of the inputs to '\n-          '`RandomFourierFeatures` should be defined. Found `None`.'):\n+          ValueError,\n+          'The last dimension of the input tensor should be defined'):\n         rff_layer(inputs)\n \n       inputs = tf.compat.v1.placeholder(dtype=tf.float32, shape=[2, None])\n@@ -165,8 +165,8 @@ class RandomFourierFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n           kernel_initializer=initializer,\n           name='random_fourier_features')\n       with self.assertRaisesRegex(\n-          ValueError, r'The last dimension of the inputs to '\n-          '`RandomFourierFeatures` should be defined. Found `None`.'):\n+          ValueError,\n+          'The last dimension of the input tensor should be defined'):\n         rff_layer(inputs)\n \n       inputs = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 3])\n@@ -190,7 +190,7 @@ class RandomFourierFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n       rff_layer.compute_output_shape(tf.TensorShape([3, 2, 3]))\n \n     with self.assertRaisesRegex(\n-        ValueError, r'The innermost dimension of input shape must be defined.'):\n+        ValueError, 'The last dimension of the input tensor should be defined'):\n       rff_layer.compute_output_shape(tf.TensorShape([3, None]))\n \n     self.assertEqual([None, output_dim],\n\n@@ -76,8 +76,8 @@ class _Merge(Layer):\n       else:\n         if i != j:\n           raise ValueError(\n-              'Operands could not be broadcast '\n-              'together with shapes ' + str(shape1) + ' ' + str(shape2))\n+              'Inputs have incompatible shapes. '\n+              f'Received shapes {shape1} and {shape2}')\n         output_shape.append(i)\n     return tuple(output_shape)\n \n@@ -85,16 +85,19 @@ class _Merge(Layer):\n   def build(self, input_shape):\n     # Used purely for shape validation.\n     if not isinstance(input_shape[0], tuple):\n-      raise ValueError('A merge layer should be called on a list of inputs.')\n+      raise ValueError(\n+          'A merge layer should be called on a list of inputs. '\n+          f'Received: input_shape={input_shape} (not a list of shapes)')\n     if len(input_shape) < 2:\n       raise ValueError('A merge layer should be called '\n                        'on a list of at least 2 inputs. '\n-                       'Got ' + str(len(input_shape)) + ' inputs.')\n+                       f'Got {len(input_shape)} inputs. '\n+                       f'Full input_shape received: {input_shape}')\n     batch_sizes = {s[0] for s in input_shape if s} - {None}\n     if len(batch_sizes) > 1:\n       raise ValueError(\n-          'Can not merge tensors with different '\n-          'batch sizes. Got tensors with shapes : ' + str(input_shape))\n+          'Cannot merge tensors with different batch sizes. '\n+          f'Got tensors with shapes {input_shape}')\n     if input_shape[0] is None:\n       output_shape = None\n     else:\n@@ -114,7 +117,9 @@ class _Merge(Layer):\n \n   def call(self, inputs):\n     if not isinstance(inputs, (list, tuple)):\n-      raise ValueError('A merge layer should be called on a list of inputs.')\n+      raise ValueError(\n+          'A merge layer should be called on a list of inputs. '\n+          f'Received: inputs={inputs} (not a list of tensors)')\n     if self._reshape_required:\n       reshaped_inputs = []\n       input_ndims = list(map(backend.ndim, inputs))\n@@ -200,12 +205,14 @@ class _Merge(Layer):\n     if mask is None:\n       return None\n     if not isinstance(mask, (tuple, list)):\n-      raise ValueError('`mask` should be a list.')\n+      raise ValueError(f'`mask` should be a list. Received: mask={mask}')\n     if not isinstance(inputs, (tuple, list)):\n-      raise ValueError('`inputs` should be a list.')\n+      raise ValueError(f'`inputs` should be a list. Received: inputs={inputs}')\n     if len(mask) != len(inputs):\n-      raise ValueError('The lists `inputs` and `mask` '\n-                       'should have the same length.')\n+      raise ValueError(\n+          'The lists `inputs` and `mask` should have the same length. '\n+          f'Received: inputs={inputs} of length {len(inputs)}, and '\n+          f'mask={mask} of length {len(mask)}')\n     if all(m is None for m in mask):\n       return None\n     masks = [tf.expand_dims(m, axis=0) for m in mask if m is not None]\n@@ -279,13 +286,15 @@ class Subtract(_Merge):\n   def build(self, input_shape):\n     super(Subtract, self).build(input_shape)\n     if len(input_shape) != 2:\n-      raise ValueError('A `Subtract` layer should be called '\n-                       'on exactly 2 inputs')\n+      raise ValueError(\n+          'A `Subtract` layer should be called on exactly 2 inputs. '\n+          f'Received: input_shape={input_shape}')\n \n   def _merge_function(self, inputs):\n     if len(inputs) != 2:\n-      raise ValueError('A `Subtract` layer should be called '\n-                       'on exactly 2 inputs')\n+      raise ValueError(\n+          'A `Subtract` layer should be called on exactly 2 inputs. '\n+          f'Received: inputs={inputs}')\n     return inputs[0] - inputs[1]\n \n \n@@ -486,8 +495,9 @@ class Concatenate(_Merge):\n   def build(self, input_shape):\n     # Used purely for shape validation.\n     if not isinstance(input_shape[0], tuple) or len(input_shape) < 1:\n-      raise ValueError('A `Concatenate` layer should be called '\n-                       'on a list of at least 1 input.')\n+      raise ValueError(\n+          'A `Concatenate` layer should be called on a list of '\n+          f'at least 1 input. Received: input_shape={input_shape}')\n     if all(shape is None for shape in input_shape):\n       return\n     reduced_inputs_shapes = [list(shape) for shape in input_shape]\n@@ -498,8 +508,8 @@ class Concatenate(_Merge):\n \n     if len(shape_set) != 1:\n       err_msg = ('A `Concatenate` layer requires inputs with matching shapes '\n-                 'except for the concat axis. Got inputs shapes: %s' %\n-                 input_shape)\n+                 'except for the concatenation axis. '\n+                 f'Received: input_shape={input_shape}')\n       # Make sure all the shapes have same ranks.\n       ranks = set(len(shape) for shape in shape_set)\n       if len(ranks) != 1:\n@@ -524,8 +534,9 @@ class Concatenate(_Merge):\n       # The tf_utils.shape_type_conversion decorator turns tensorshapes\n       # into tuples, so we need to verify that `input_shape` is a list/tuple,\n       # *and* that the individual elements are themselves shape tuples.\n-      raise ValueError('A `Concatenate` layer should be called '\n-                       'on a list of inputs.')\n+      raise ValueError(\n+          'A `Concatenate` layer should be called on a list of inputs. '\n+          f'Received: input_shape={input_shape}')\n     input_shapes = input_shape\n     output_shape = list(input_shapes[0])\n     for shape in input_shapes[1:]:\n@@ -539,12 +550,14 @@ class Concatenate(_Merge):\n     if mask is None:\n       return None\n     if not isinstance(mask, (tuple, list)):\n-      raise ValueError('`mask` should be a list.')\n+      raise ValueError(f'`mask` should be a list. Received mask={mask}')\n     if not isinstance(inputs, (tuple, list)):\n-      raise ValueError('`inputs` should be a list.')\n+      raise ValueError(f'`inputs` should be a list. Received: inputs={inputs}')\n     if len(mask) != len(inputs):\n-      raise ValueError('The lists `inputs` and `mask` '\n-                       'should have the same length.')\n+      raise ValueError(\n+          'The lists `inputs` and `mask` should have the same length. '\n+          f'Received: inputs={inputs} of length {len(inputs)}, and '\n+          f'mask={mask} of length {len(mask)}')\n     if all(m is None for m in mask):\n       return None\n     # Make a list of masks while making sure\n@@ -639,14 +652,17 @@ class Dot(_Merge):\n     super(Dot, self).__init__(**kwargs)\n     if not isinstance(axes, int):\n       if not isinstance(axes, (list, tuple)):\n-        raise TypeError('Invalid type for `axes` - '\n-                        'should be a list or an int.')\n+        raise TypeError(\n+            'Invalid type for argument `axes`: it should be '\n+            f'a list or an int. Received: axes={axes}')\n       if len(axes) != 2:\n-        raise ValueError('Invalid format for `axes` - '\n-                         'should contain two elements.')\n+        raise ValueError(\n+            'Invalid format for argument `axes`: it should contain two '\n+            f'elements. Received: axes={axes}')\n       if not isinstance(axes[0], int) or not isinstance(axes[1], int):\n-        raise ValueError('Invalid format for `axes` - '\n-                         'list elements should be \"int\".')\n+        raise ValueError(\n+            'Invalid format for argument `axes`: list elements should be '\n+            f'integers. Received: axes={axes}')\n     self.axes = axes\n     self.normalize = normalize\n     self.supports_masking = True\n@@ -656,8 +672,9 @@ class Dot(_Merge):\n   def build(self, input_shape):\n     # Used purely for shape validation.\n     if not isinstance(input_shape[0], tuple) or len(input_shape) != 2:\n-      raise ValueError('A `Dot` layer should be called '\n-                       'on a list of 2 inputs.')\n+      raise ValueError(\n+          'A `Dot` layer should be called on a list of 2 inputs. '\n+          f'Received: input_shape={input_shape}')\n     shape1 = input_shape[0]\n     shape2 = input_shape[1]\n     if shape1 is None or shape2 is None:\n@@ -670,15 +687,18 @@ class Dot(_Merge):\n     else:\n       axes = self.axes\n     if shape1[axes[0]] != shape2[axes[1]]:\n-      raise ValueError('Dimension incompatibility '\n-                       '%s != %s. ' % (shape1[axes[0]], shape2[axes[1]]) +\n-                       'Layer shapes: %s, %s. ' % (shape1, shape2) +\n-                       'Chosen axes: %s, %s' % (axes[0], axes[1]))\n+      raise ValueError(\n+          'Incompatible input shapes: '\n+          f'axis values {shape1[axes[0]]} (at axis {axes[0]}) != '\n+          f'{shape2[axes[1]]} (at axis {axes[1]}). '\n+          f'Full input shapes: {shape1}, {shape2}')\n \n   def _merge_function(self, inputs):\n     base_layer_utils.no_ragged_support(inputs, self.name)\n     if len(inputs) != 2:\n-      raise ValueError('A `Dot` layer should be called on exactly 2 inputs')\n+      raise ValueError(\n+          'A `Dot` layer should be called on exactly 2 inputs. '\n+          f'Received: inputs={inputs}')\n     x1 = inputs[0]\n     x2 = inputs[1]\n     if isinstance(self.axes, int):\n@@ -702,8 +722,9 @@ class Dot(_Merge):\n   @tf_utils.shape_type_conversion\n   def compute_output_shape(self, input_shape):\n     if not isinstance(input_shape, (tuple, list)) or len(input_shape) != 2:\n-      raise ValueError('A `Dot` layer should be called '\n-                       'on a list of 2 inputs.')\n+      raise ValueError(\n+          'A `Dot` layer should be called on a list of 2 inputs. '\n+          f'Received: input_shape={input_shape}')\n     shape1 = list(input_shape[0])\n     shape2 = list(input_shape[1])\n     if isinstance(self.axes, int):\n\n@@ -72,13 +72,12 @@ class StackedRNNCells(Layer):\n \n   def __init__(self, cells, **kwargs):\n     for cell in cells:\n-      if not 'call' in dir(cell):\n+      if 'call' not in dir(cell):\n         raise ValueError('All cells must have a `call` method. '\n-                         'received cells:', cells)\n-      if not 'state_size' in dir(cell):\n-        raise ValueError('All cells must have a '\n-                         '`state_size` attribute. '\n-                         'received cells:', cells)\n+                         f'Received cell without a `call` method: {cell}')\n+      if 'state_size' not in dir(cell):\n+        raise ValueError('All cells must have a `state_size` attribute. '\n+                         f'Received cell without a `state_size`: {cell}')\n     self.cells = cells\n     # reverse_state_order determines whether the state size will be in a reverse\n     # order of the cells' state. User might want to set this to True to keep the\n@@ -400,14 +399,13 @@ class RNN(Layer):\n                **kwargs):\n     if isinstance(cell, (list, tuple)):\n       cell = StackedRNNCells(cell)\n-    if not 'call' in dir(cell):\n-      raise ValueError('`cell` should have a `call` method. '\n-                       'The RNN was passed:', cell)\n-    if not 'state_size' in dir(cell):\n-      raise ValueError('The RNN cell should have '\n-                       'an attribute `state_size` '\n-                       '(tuple of integers, '\n-                       'one integer per RNN state).')\n+    if 'call' not in dir(cell):\n+      raise ValueError('Argument `cell` should have a `call` method. '\n+                       f'The RNN was passed: cell={cell}')\n+    if 'state_size' not in dir(cell):\n+      raise ValueError('The RNN cell should have a `state_size` attribute '\n+                       '(tuple of integers, one integer per RNN state). '\n+                       f'Received: cell={cell}')\n     # If True, the output for masked timestep will be zeros, whereas in the\n     # False case, output from previous timestep is returned for masked timestep.\n     self.zero_output_for_mask = kwargs.pop('zero_output_for_mask', False)\n@@ -439,8 +437,8 @@ class RNN(Layer):\n \n     if stateful:\n       if tf.distribute.has_strategy():\n-        raise ValueError('RNNs with stateful=True not yet supported with '\n-                         'tf.distribute.Strategy.')\n+        raise ValueError('Stateful RNNs (created with `stateful=True`) '\n+                         'are not yet supported with tf.distribute.Strategy.')\n \n   @property\n   def _use_input_spec_as_call_signature(self):\n@@ -705,11 +703,12 @@ class RNN(Layer):\n         flat_additional_inputs[0]) if flat_additional_inputs else True\n     for tensor in flat_additional_inputs:\n       if backend.is_keras_tensor(tensor) != is_keras_tensor:\n-        raise ValueError('The initial state or constants of an RNN'\n-                         ' layer cannot be specified with a mix of'\n-                         ' Keras tensors and non-Keras tensors'\n-                         ' (a \"Keras tensor\" is a tensor that was'\n-                         ' returned by a Keras layer, or by `Input`)')\n+        raise ValueError(\n+            'The initial state or constants of an RNN layer cannot be '\n+            'specified via a mix of Keras tensors and non-Keras tensors '\n+            '(a \"Keras tensor\" is a tensor that was returned by a Keras layer '\n+            ' or by `Input` during Functional model construction). '\n+            f'Received: initial_state={initial_state}, constants={constants}')\n \n     if is_keras_tensor:\n       # Compute the full input spec, including state and constants\n@@ -792,7 +791,9 @@ class RNN(Layer):\n     cell_call_fn = self.cell.__call__ if callable(self.cell) else self.cell.call\n     if constants:\n       if not generic_utils.has_arg(self.cell.call, 'constants'):\n-        raise ValueError('RNN cell does not support constants')\n+        raise ValueError(\n+            f'RNN cell {self.cell} does not support constants. '\n+            f'Received: constants={constants}')\n \n       def step(inputs, states):\n         constants = states[-self._num_constants:]  # pylint: disable=invalid-unary-operand-type\n@@ -881,9 +882,9 @@ class RNN(Layer):\n       initial_state = self.get_initial_state(inputs)\n \n     if len(initial_state) != len(self.states):\n-      raise ValueError('Layer has ' + str(len(self.states)) +\n-                       ' states but was passed ' + str(len(initial_state)) +\n-                       ' initial states.')\n+      raise ValueError(f'Layer has {len(self.states)} '\n+                       f'states but was passed {len(initial_state)} initial '\n+                       f'states. Received: initial_state={initial_state}')\n     return inputs, initial_state, constants\n \n   def _validate_args_if_ragged(self, is_ragged_input, mask):\n@@ -891,9 +892,9 @@ class RNN(Layer):\n       return\n \n     if mask is not None:\n-      raise ValueError('The mask that was passed in was ' + str(mask) +\n-                       ' and cannot be applied to RaggedTensor inputs. Please '\n-                       'make sure that there is no mask passed in by upstream '\n+      raise ValueError(f'The mask that was passed in was {mask}, which '\n+                       'cannot be applied to RaggedTensor inputs. Please '\n+                       'make sure that there is no mask injected by upstream '\n                        'layers.')\n     if self.unroll:\n       raise ValueError('The input received contains RaggedTensors and does '\n@@ -968,18 +969,17 @@ class RNN(Layer):\n       flat_states = tf.nest.flatten(self.states)\n       flat_input_states = tf.nest.flatten(states)\n       if len(flat_input_states) != len(flat_states):\n-        raise ValueError('Layer ' + self.name + ' expects ' +\n-                         str(len(flat_states)) + ' states, '\n-                         'but it received ' + str(len(flat_input_states)) +\n-                         ' state values. Input received: ' + str(states))\n+        raise ValueError(f'Layer {self.name} expects {len(flat_states)} '\n+                         f'states, but it received {len(flat_input_states)} '\n+                         f'state values. States received: {states}')\n       set_value_tuples = []\n       for i, (value, state) in enumerate(zip(flat_input_states,\n                                              flat_states)):\n         if value.shape != state.shape:\n           raise ValueError(\n-              'State ' + str(i) + ' is incompatible with layer ' +\n-              self.name + ': expected shape=' + str(\n-                  (batch_size, state)) + ', found shape=' + str(value.shape))\n+              f'State {i} is incompatible with layer {self.name}: '\n+              f'expected shape={(batch_size, state)} '\n+              f'but found shape={value.shape}')\n         set_value_tuples.append((state, value))\n       backend.batch_set_value(set_value_tuples)\n \n@@ -1086,7 +1086,7 @@ class AbstractRNNCell(Layer):\n         1. output tensor for the current timestep, with size `output_size`.\n         2. state tensor for next step, which has the shape of `state_size`.\n     \"\"\"\n-    raise NotImplementedError('Abstract method')\n+    raise NotImplementedError\n \n   @property\n   def state_size(self):\n@@ -1095,12 +1095,12 @@ class AbstractRNNCell(Layer):\n     It can be represented by an Integer, a TensorShape or a tuple of Integers\n     or TensorShapes.\n     \"\"\"\n-    raise NotImplementedError('Abstract method')\n+    raise NotImplementedError\n \n   @property\n   def output_size(self):\n     \"\"\"Integer or TensorShape: size of outputs produced by this cell.\"\"\"\n-    raise NotImplementedError('Abstract method')\n+    raise NotImplementedError\n \n   def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n     return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n@@ -1328,8 +1328,8 @@ class SimpleRNNCell(DropoutRNNCellMixin, Layer):\n                recurrent_dropout=0.,\n                **kwargs):\n     if units < 0:\n-      raise ValueError(f'Received an invalid value for units, expected '\n-                       f'a positive integer, got {units}.')\n+      raise ValueError(f'Received an invalid value for argument `units`, '\n+                       f'expected a positive integer, got {units}.')\n     # By default use cached variable under v2 mode, see b/143699808.\n     if tf.compat.v1.executing_eagerly_outside_functions():\n       self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n@@ -1769,8 +1769,8 @@ class GRUCell(DropoutRNNCellMixin, Layer):\n                reset_after=False,\n                **kwargs):\n     if units < 0:\n-      raise ValueError(f'Received an invalid value for units, expected '\n-                       f'a positive integer, got {units}.')\n+      raise ValueError(f'Received an invalid value for argument `units`, '\n+                       f'expected a positive integer, got {units}.')\n     # By default use cached variable under v2 mode, see b/143699808.\n     if tf.compat.v1.executing_eagerly_outside_functions():\n       self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n@@ -2332,8 +2332,8 @@ class LSTMCell(DropoutRNNCellMixin, Layer):\n                recurrent_dropout=0.,\n                **kwargs):\n     if units < 0:\n-      raise ValueError(f'Received an invalid value for units, expected '\n-                       f'a positive integer, got {units}.')\n+      raise ValueError(f'Received an invalid value for argument `units`, '\n+                       f'expected a positive integer, got {units}.')\n     # By default use cached variable under v2 mode, see b/143699808.\n     if tf.compat.v1.executing_eagerly_outside_functions():\n       self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n@@ -3024,8 +3024,8 @@ def _generate_zero_filled_state(batch_size_tensor, state_size, dtype):\n   \"\"\"Generate a zero filled tensor with shape [batch_size, state_size].\"\"\"\n   if batch_size_tensor is None or dtype is None:\n     raise ValueError(\n-        'batch_size and dtype cannot be None while constructing initial state: '\n-        'batch_size={}, dtype={}'.format(batch_size_tensor, dtype))\n+        'batch_size and dtype cannot be None while constructing initial state. '\n+        f'Received: batch_size={batch_size_tensor}, dtype={dtype}')\n \n   def create_zeros(unnested_state_size):\n     flat_dims = tf.TensorShape(unnested_state_size).as_list()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
