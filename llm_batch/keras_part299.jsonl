{"custom_id": "keras#caceebcbe9afd0d4ff51cdfb88c16fd7f34ff104", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 7 | Files Changed: 5 | Hunks: 16 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 13064 | Contributors (this commit): 123 | Commits (past 90d): 24 | Contributors (cumulative): 148 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -680,7 +680,8 @@ class Model(Network):\n                         y_true, y_pred, sample_weight=sample_weight)\n \n                 if len(self.outputs) > 1:\n-                    self._output_loss_metrics[i](output_loss)\n+                    update_ops = self._output_loss_metrics[i].update_state(output_loss)\n+                    self._metric_updates += update_ops\n \n                 if total_loss is None:\n                     total_loss = loss_weight * output_loss\n@@ -698,7 +699,7 @@ class Model(Network):\n             for loss_tensor in self.losses:\n                 total_loss += loss_tensor\n \n-        return total_loss\n+        return K.mean(total_loss)\n \n     def _get_training_eval_metrics(self):\n         \"\"\"Returns all the metrics that are to be reported.\n\n@@ -1030,10 +1030,12 @@ def call_metric_function(metric_fn,\n             weights *= mask\n \n     if y_pred is not None:\n-        update_op = metric_fn.update_state(y_true, y_pred, sample_weight=weights)\n+        update_ops = metric_fn.update_state(y_true, y_pred, sample_weight=weights)\n+        with K.control_dependencies(update_ops):\n             result = metric_fn.result()\n     else:\n         # `Mean` metric only takes a single value.\n-        update_op = metric_fn.update_state(y_true, sample_weight=weights)\n+        update_ops = metric_fn.update_state(y_true, sample_weight=weights)\n+        with K.control_dependencies(update_ops):\n             result = metric_fn.result()\n-    return result, update_op\n+    return result, update_ops\n\n@@ -24,13 +24,13 @@ and is distributed under the MIT license.\n '''\n \n setup(name='Keras',\n-      version='2.2.4',\n+      version='2.3.0',\n       description='Deep Learning for humans',\n       long_description=long_description,\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/keras-team/keras',\n-      download_url='https://github.com/keras-team/keras/tarball/2.2.4',\n+      download_url='https://github.com/keras-team/keras/tarball/2.3.0',\n       license='MIT',\n       install_requires=['numpy>=1.9.1',\n                         'scipy>=0.14',\n\n@@ -159,6 +159,11 @@ class TestLossFunctions(object):\n         np.allclose(K.eval(loss), 16, atol=1e-2)\n \n \n+skipif_not_tf = pytest.mark.skipif(\n+    K.backend() != 'tensorflow',\n+    reason='Need TensorFlow to __call__ a loss')\n+\n+\n class TestLossClasses(object):\n \n     @pytest.mark.parametrize('cls', all_classes)\n@@ -180,6 +185,7 @@ class TestLossClasses(object):\n         assert K.eval(objective_output).shape == ()\n \n \n+@skipif_not_tf\n class TestMeanSquaredError:\n \n     def test_config(self):\n@@ -256,6 +262,7 @@ class TestMeanSquaredError:\n         assert np.isclose(K.eval(loss), 227.69998, rtol=1e-3)\n \n \n+@skipif_not_tf\n class TestMeanAbsoluteError(object):\n \n     def test_config(self):\n@@ -332,6 +339,7 @@ class TestMeanAbsoluteError(object):\n         assert np.isclose(K.eval(loss), 25.29999, rtol=1e-3)\n \n \n+@skipif_not_tf\n class TestMeanAbsolutePercentageError(object):\n \n     def test_config(self):\n@@ -392,6 +400,7 @@ class TestMeanAbsolutePercentageError(object):\n         assert np.allclose(K.eval(loss), [621.8518, 352.6666], rtol=1e-3)\n \n \n+@skipif_not_tf\n class TestMeanSquaredLogarithmicError(object):\n \n     def test_config(self):\n@@ -438,6 +447,7 @@ class TestMeanSquaredLogarithmicError(object):\n         assert np.allclose(K.eval(loss), 0.0, rtol=1e-3)\n \n \n+@skipif_not_tf\n class TestBinaryCrossentropy(object):\n \n     def test_config(self):\n@@ -598,6 +608,7 @@ class TestBinaryCrossentropy(object):\n         assert np.isclose(K.eval(loss), expected_value, atol=1e-3)\n \n \n+@skipif_not_tf\n class TestCategoricalCrossentropy(object):\n \n     def test_config(self):\n@@ -693,6 +704,7 @@ class TestCategoricalCrossentropy(object):\n         assert np.isclose(K.eval(loss), expected_value, atol=1e-3)\n \n \n+@skipif_not_tf\n class TestSparseCategoricalCrossentropy(object):\n \n     def test_config(self):\n\n@@ -6,6 +6,11 @@ from keras import metrics\n from keras import backend as K\n \n \n+if K.backend() != 'tensorflow':\n+    # Need TensorFlow to use metric.__call__\n+    pytestmark = pytest.mark.skip\n+\n+\n class TestSum(object):\n \n     def test_sum(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ccb69dfb46688feed5a06f19fef526f2d8bc884", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1196 | Contributors (this commit): 17 | Commits (past 90d): 12 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -79,7 +79,7 @@ class Metric(Layer):\n             raise RuntimeError(\n                 'Metric calling only supported with TensorFlow backend.')\n         update_op = self.update_state(*args, **kwargs)\n-        with K.control_dependencies([update_op]):  # For TF\n+        with K.control_dependencies(update_op):  # For TF\n             return self.result()\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ee3997f2e6dafa5e1fa93b71dcf428a29f8066f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 228 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 12 | Methods Changed: 17 | Complexity Δ (Sum/Max): 31/14 | Churn Δ: 230 | Churn Cumulative: 6201 | Contributors (this commit): 51 | Commits (past 90d): 9 | Contributors (cumulative): 80 | DMM Complexity: 0.9243243243243243\n\nDIFF:\n@@ -50,6 +50,11 @@ def fit_generator(model,\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the `keras.utils.Sequence'\n                         ' class.'))\n+\n+    # if generator is instance of Sequence and steps_per_epoch are not provided -\n+    # recompute steps_per_epoch after each epoch\n+    recompute_steps_per_epoch = use_sequence_api and steps_per_epoch is None\n+\n     if steps_per_epoch is None:\n         if use_sequence_api:\n             steps_per_epoch = len(generator)\n@@ -257,6 +262,25 @@ def fit_generator(model,\n             if callbacks.model.stop_training:\n                 break\n \n+            if use_sequence_api and workers == 0:\n+                generator.on_epoch_end()\n+\n+            if recompute_steps_per_epoch:\n+                if workers > 0:\n+                    enqueuer.join_end_of_epoch()\n+\n+                # recomute steps per epochs in case if Sequence changes it's length\n+                steps_per_epoch = len(generator)\n+\n+                # update callbacks to make sure params are valid each epoch\n+                callbacks.set_params({\n+                    'epochs': epochs,\n+                    'steps': steps_per_epoch,\n+                    'verbose': verbose,\n+                    'do_validation': do_validation,\n+                    'metrics': callback_metrics,\n+                })\n+\n     finally:\n         try:\n             if enqueuer is not None:\n\n@@ -541,6 +541,7 @@ class OrderedEnqueuer(SequenceEnqueuer):\n     def __init__(self, sequence, use_multiprocessing=False, shuffle=False):\n         super(OrderedEnqueuer, self).__init__(sequence, use_multiprocessing)\n         self.shuffle = shuffle\n+        self.end_of_epoch_signal = threading.Event()\n \n     def _get_executor_init(self, workers):\n         \"\"\"Get the Pool initializer for multiprocessing.\n@@ -561,9 +562,10 @@ class OrderedEnqueuer(SequenceEnqueuer):\n \n     def _run(self):\n         \"\"\"Submits request to the executor and queue the `Future` objects.\"\"\"\n+        while True:\n             sequence = list(range(len(self.sequence)))\n             self._send_sequence()  # Share the initial sequence\n-        while True:\n+\n             if self.shuffle:\n                 random.shuffle(sequence)\n \n@@ -584,7 +586,12 @@ class OrderedEnqueuer(SequenceEnqueuer):\n \n             # Call the internal on epoch end.\n             self.sequence.on_epoch_end()\n-            self._send_sequence()  # Update the pool\n+            # communicate on_epoch_end to the main thread\n+            self.end_of_epoch_signal.set()\n+\n+    def join_end_of_epoch(self):\n+        self.end_of_epoch_signal.wait(timeout=30)\n+        self.end_of_epoch_signal.clear()\n \n     def get(self):\n         \"\"\"Creates a generator to extract data from the queue.\n\n@@ -42,6 +42,28 @@ class RandomSequence(Sequence):\n         pass\n \n \n+class IncreaseBatchSizeRandomSequence(Sequence):\n+    def __init__(self, initial_batch_size, initial_sequence_length=12,\n+                 batch_size_func=lambda x: x + 2):\n+        self.batch_size = initial_batch_size\n+        self.initial_sequence_length = initial_sequence_length\n+        self.batch_size_func = batch_size_func\n+        self.logs = []\n+\n+    def __len__(self):\n+        return int(np.ceil(self.initial_sequence_length / float(self.batch_size)))\n+\n+    def __getitem__(self, idx):\n+        self.logs.append(idx)\n+        return ([np.random.random((self.batch_size, 3)),\n+                 np.random.random((self.batch_size, 3))],\n+                [np.random.random((self.batch_size, 4)),\n+                 np.random.random((self.batch_size, 3))])\n+\n+    def on_epoch_end(self):\n+        self.batch_size = self.batch_size_func(self.batch_size)\n+\n+\n class threadsafe_iter:\n     \"\"\"Takes an iterator/generator and makes it thread-safe by\n     serializing call to the `next` method of given iterator/generator.\n@@ -146,8 +168,13 @@ class TrackerCallback(Callback):\n         # test starting from non-zero initial epoch\n         self.trained_epochs = []\n         self.trained_batches = []\n+        self.steps_per_epoch_log = []\n         super(TrackerCallback, self).__init__()\n \n+    def set_params(self, params):\n+        super(TrackerCallback, self).set_params(params)\n+        self.steps_per_epoch_log.append(params['steps'])\n+\n     # define tracer callback\n     def on_epoch_begin(self, epoch, logs):\n         self.trained_epochs.append(epoch)\n@@ -603,6 +630,134 @@ def test_fit_generator():\n     assert 3 <= gen_counters[0] <= 12\n \n \n+def test_fit_generator_dynamic_size_sequence_with_workers():\n+    model = get_model(num_outputs=2)\n+    optimizer = 'rmsprop'\n+    loss = 'mse'\n+    loss_weights = [1., 0.5]\n+\n+    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n+                  sample_weight_mode=None)\n+    tracker_cb = TrackerCallback()\n+    val_seq = RandomSequence(4)\n+    train_seq = IncreaseBatchSizeRandomSequence(3, 20)\n+    out = model.fit_generator(generator=train_seq,\n+                              epochs=5,\n+                              initial_epoch=0,\n+                              validation_data=val_seq,\n+                              validation_steps=3,\n+                              max_queue_size=1,\n+                              callbacks=[tracker_cb])\n+    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n+    assert tracker_cb.trained_batches == [\n+        0, 1, 2, 3, 4, 5, 6,  # 1st epoch -> ceil(20 / 3) = 7 batches\n+        0, 1, 2, 3,           # 2nd epoch -> ceil(20 / 5) = 4 batches\n+        0, 1, 2,              # 3d  epoch -> ceil(20 / 7) = 3 batches\n+        0, 1, 2,              # 4th epoch -> ceil(20 / 9) = 3 batches\n+        0, 1,                 # 5th epoch -> ceil(20 /11) = 2 batches\n+    ]\n+    assert tracker_cb.steps_per_epoch_log[0:5] == [7, 4, 3, 3, 2]\n+\n+    tracker_cb = TrackerCallback()\n+    val_seq = RandomSequence(4)\n+    train_seq = IncreaseBatchSizeRandomSequence(3, 30)\n+    out = model.fit_generator(generator=train_seq,\n+                              epochs=5,\n+                              initial_epoch=0,\n+                              validation_data=val_seq,\n+                              validation_steps=3,\n+                              max_queue_size=1,\n+                              callbacks=[tracker_cb])\n+    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n+    assert tracker_cb.trained_batches == [\n+        0, 1, 2, 3, 4, 5, 6, 7, 8, 9,  # 1st epoch -> ceil(30 / 3) = 10 batches\n+        0, 1, 2, 3, 4, 5,              # 2nd epoch -> ceil(30 / 5) =  6 batches\n+        0, 1, 2, 3, 4,                 # 3d  epoch -> ceil(30 / 7) =  5 batches\n+        0, 1, 2, 3,                    # 4th epoch -> ceil(30 / 9) =  4 batches\n+        0, 1, 2,                       # 5th epoch -> ceil(30 /11) =  3 batches\n+    ]\n+    assert tracker_cb.steps_per_epoch_log[0:5] == [10, 6, 5, 4, 3]\n+\n+    tracker_cb = TrackerCallback()\n+    val_seq = RandomSequence(4)\n+    train_seq = IncreaseBatchSizeRandomSequence(2, 404, lambda x: x * 2)\n+    out = model.fit_generator(generator=train_seq,\n+                              epochs=5,\n+                              initial_epoch=0,\n+                              validation_data=val_seq,\n+                              validation_steps=3,\n+                              max_queue_size=1,\n+                              callbacks=[tracker_cb])\n+    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n+    # number of trained batches should match sum of steps per each epoch\n+    assert len(tracker_cb.trained_batches) == 202 + 101 + 51 + 26 + 13\n+    assert tracker_cb.steps_per_epoch_log[0:5] == [202, 101, 51, 26, 13]\n+\n+\n+def test_fit_generator_dynamic_size_sequence_main_thread():\n+    model = get_model(num_outputs=2)\n+    optimizer = 'rmsprop'\n+    loss = 'mse'\n+    loss_weights = [1., 0.5]\n+\n+    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n+                  sample_weight_mode=None)\n+    tracker_cb = TrackerCallback()\n+    val_seq = RandomSequence(4)\n+    train_seq = IncreaseBatchSizeRandomSequence(3, 20)\n+    out = model.fit_generator(generator=train_seq,\n+                              epochs=5,\n+                              initial_epoch=0,\n+                              validation_data=val_seq,\n+                              validation_steps=3,\n+                              workers=0,\n+                              callbacks=[tracker_cb])\n+    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n+    assert tracker_cb.trained_batches == [\n+        0, 1, 2, 3, 4, 5, 6,  # 1st epoch -> ceil(20 / 3) = 7 batches\n+        0, 1, 2, 3,           # 2nd epoch -> ceil(20 / 5) = 4 batches\n+        0, 1, 2,              # 3d  epoch -> ceil(20 / 7) = 3 batches\n+        0, 1, 2,              # 4th epoch -> ceil(20 / 9) = 3 batches\n+        0, 1,                 # 5th epoch -> ceil(20 /11) = 2 batches\n+    ]\n+    assert tracker_cb.steps_per_epoch_log[0:5] == [7, 4, 3, 3, 2]\n+\n+    tracker_cb = TrackerCallback()\n+    val_seq = RandomSequence(4)\n+    train_seq = IncreaseBatchSizeRandomSequence(3, 30)\n+    out = model.fit_generator(generator=train_seq,\n+                              epochs=5,\n+                              initial_epoch=0,\n+                              validation_data=val_seq,\n+                              validation_steps=3,\n+                              workers=0,\n+                              callbacks=[tracker_cb])\n+    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n+    assert tracker_cb.trained_batches == [\n+        0, 1, 2, 3, 4, 5, 6, 7, 8, 9,  # 1st epoch -> ceil(30 / 3) = 10 batches\n+        0, 1, 2, 3, 4, 5,              # 2nd epoch -> ceil(30 / 5) =  6 batches\n+        0, 1, 2, 3, 4,                 # 3d  epoch -> ceil(30 / 7) =  5 batches\n+        0, 1, 2, 3,                    # 4th epoch -> ceil(30 / 9) =  4 batches\n+        0, 1, 2,                       # 5th epoch -> ceil(30 /11) =  3 batches\n+    ]\n+    assert tracker_cb.steps_per_epoch_log[0:5] == [10, 6, 5, 4, 3]\n+\n+    tracker_cb = TrackerCallback()\n+    val_seq = RandomSequence(4)\n+    train_seq = IncreaseBatchSizeRandomSequence(2, 404, lambda x: x * 2)\n+    out = model.fit_generator(generator=train_seq,\n+                              epochs=5,\n+                              initial_epoch=0,\n+                              validation_data=val_seq,\n+                              validation_steps=3,\n+                              workers=0,\n+                              callbacks=[tracker_cb])\n+    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n+    # number of trained batches should match sum of steps per each epoch\n+    assert len(tracker_cb.trained_batches) == 202 + 101 + 51 + 26 + 13\n+    assert tracker_cb.steps_per_epoch_log[0:5] == [202, 101, 51, 26, 13]\n+\n+\n def test_fit_generator_shape():\n     # predict_generator output shape behavior should be consistent\n     def expected_shape(batch_size, n_batches):\n\n@@ -182,6 +182,24 @@ class DummySequence(Sequence):\n         self.inner *= 5.0\n \n \n+class LengthChangingSequence(Sequence):\n+    def __init__(self, shape, size=100, value=1.0):\n+        self.shape = shape\n+        self.inner = value\n+        self.size = size\n+\n+    def __getitem__(self, item):\n+        time.sleep(0.05)\n+        return np.ones(self.shape, dtype=np.uint32) * item * self.inner\n+\n+    def __len__(self):\n+        return self.size\n+\n+    def on_epoch_end(self):\n+        self.size = int(np.ceil(self.size / 2))\n+        self.inner *= 5.0\n+\n+\n class FaultSequence(Sequence):\n     def __getitem__(self, item):\n         raise IndexError(item, 'is not present')\n@@ -380,6 +398,28 @@ def test_on_epoch_end_threads():\n     enqueuer.stop()\n \n \n+def test_on_epoch_end_threads_sequence_change_length():\n+    seq = LengthChangingSequence([3, 10, 10, 3])\n+    enqueuer = OrderedEnqueuer(seq,\n+                               use_multiprocessing=False)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    acc = []\n+    for i in range(100):\n+        acc.append(next(gen_output)[0, 0, 0, 0])\n+    assert acc == list(range(100)), ('Order was not keep in GeneratorEnqueuer '\n+                                     'with threads')\n+\n+    enqueuer.join_end_of_epoch()\n+    assert len(seq) == 50\n+    acc = []\n+    for i in range(50):\n+        acc.append(next(gen_output)[0, 0, 0, 0])\n+    assert acc == list([k * 5 for k in range(50)]), (\n+        'Order was not keep in GeneratorEnqueuer with processes')\n+    enqueuer.stop()\n+\n+\n @use_spawn\n def test_ordered_enqueuer_fail_processes():\n     enqueuer = OrderedEnqueuer(FaultSequence(), use_multiprocessing=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#387aea3408a9f2dd6181852ac6e0ceb206ee45d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 7 | Files Changed: 5 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 10982 | Contributors (this commit): 105 | Commits (past 90d): 17 | Contributors (cumulative): 122 | DMM Complexity: 0.0\n\nDIFF:\n@@ -683,6 +683,7 @@ class Model(Network):\n                     # TODO\n                     # update_ops = self._output_loss_metrics[i].update_state(output_loss)\n                     # self._metric_updates += update_ops\n+                    if K.backend() == 'tensorflow':\n                         self._output_loss_metrics[i](output_loss)\n \n                 if total_loss is None:\n\n@@ -418,11 +418,11 @@ def test_loop(model, f, ins,\n                     if i == 0:  # Index 0 == `Loss`\n                         outs[i] = float(batch_out)\n                     else:\n-                        outs[i] += batch_out\n+                        outs[i] += float(batch_out)\n             else:\n                 if step == 0:\n                     outs.append(0.)\n-                outs[0] += batch_outs\n+                outs[0] += float(batch_outs)\n \n             for l, o in zip(model.metrics_names, batch_outs):\n                 batch_logs[l] = o\n@@ -452,16 +452,16 @@ def test_loop(model, f, ins,\n                     outs.extend([0.] * len(batch_outs))\n                 for i, batch_out in enumerate(batch_outs):\n                     if i == 0:  # Index 0 == `Loss`\n-                        outs[i] += batch_out * len(batch_ids)\n+                        outs[i] += float(batch_out) * len(batch_ids)\n                     else:\n-                        outs[i] = batch_out\n+                        outs[i] = float(batch_out)\n             else:\n                 if batch_index == 0:\n                     outs.append(0.)\n-                outs[0] += batch_outs * len(batch_ids)\n+                outs[0] += float(batch_outs) * len(batch_ids)\n \n             for l, o in zip(model.metrics_names, batch_outs):\n-                batch_logs[l] = o\n+                batch_logs[l] = float(o)\n             callbacks._call_batch_hook('test', 'end', batch_index, batch_logs)\n \n             if verbose == 1:\n\n@@ -2,6 +2,12 @@ import os\n from markdown import markdown\n from docs import autogen\n import pytest\n+from keras import backend as K\n+\n+\n+if K.backend() != 'tensorflow':\n+    pytestmark = pytest.mark.skip\n+\n \n test_doc1 = {\n     'doc': \"\"\"Base class for recurrent layers.\n\n@@ -3,9 +3,13 @@ import inspect\n import re\n import sys\n from itertools import compress\n+from keras import backend as K\n \n import pytest\n \n+if K.backend() != 'tensorflow':\n+    pytestmark = pytest.mark.skip\n+\n modules = ['keras.layers', 'keras.models', 'keras',\n            'keras.backend', 'keras.engine',\n            'keras.wrappers', 'keras.utils',\n\n@@ -89,7 +89,6 @@ def test_io_utils(in_tmpdir):\n     out_eval = model.evaluate(X_test, y_test, batch_size=32, verbose=False)\n \n     assert out_pred.shape == (50, 1), 'Prediction shape does not match'\n-    assert out_eval.shape == (), 'Shape of evaluation does not match'\n     assert out_eval > 0, (\n         'Evaluation value does not meet criteria: {}'.format(out_eval))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#544625545afe2e9c2b358e356c11bb8be53ceb51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 66 | Files Changed: 1 | Hunks: 35 | Methods Changed: 15 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 140 | Churn Cumulative: 3233 | Contributors (this commit): 38 | Commits (past 90d): 5 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ from __future__ import print_function\n \n import six\n import copy\n+import numpy as np\n from six.moves import zip\n \n from . import backend as K\n@@ -170,20 +171,19 @@ class SGD(Optimizer):\n         learning_rate: float >= 0. Learning rate.\n         momentum: float >= 0. Parameter that accelerates SGD\n             in the relevant direction and dampens oscillations.\n-        decay: float >= 0. Learning rate decay over each update.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.01, momentum=0., decay=0.,\n+    def __init__(self, learning_rate=0.01, momentum=0.,\n                  nesterov=False, **kwargs):\n-        learning_rate = kwargs.pop('lr', None) or learning_rate\n+        learning_rate = kwargs.pop('lr', learning_rate)\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n         super(SGD, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n             self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.momentum = K.variable(momentum, name='momentum')\n-            self.decay = K.variable(decay, name='decay')\n-        self.initial_decay = decay\n+            self.decay = K.variable(self.initial_decay, name='decay')\n         self.nesterov = nesterov\n \n     @interfaces.legacy_get_updates_support\n@@ -236,27 +236,22 @@ class RMSprop(Optimizer):\n     # Arguments\n         learning_rate: float >= 0. Learning rate.\n         rho: float >= 0.\n-        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [rmsprop: Divide the gradient by a running average of its recent magnitude\n            ](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.001, rho=0.9, epsilon=None, decay=0.,\n-                 **kwargs):\n-        learning_rate = kwargs.pop('lr', None) or learning_rate\n+    def __init__(self, learning_rate=0.001, rho=0.9, **kwargs):\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        learning_rate = kwargs.pop('lr', learning_rate)\n         super(RMSprop, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.rho = K.variable(rho, name='rho')\n-            self.decay = K.variable(decay, name='decay')\n+            self.decay = K.variable(self.initial_decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n-        self.epsilon = epsilon\n-        self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     @K.symbolic\n@@ -266,7 +261,7 @@ class RMSprop(Optimizer):\n                         dtype=K.dtype(p),\n                         name='accumulator_' + str(i))\n                         for (i, p) in enumerate(params)]\n-        self.weights = accumulators\n+        self.weights = [self.iterations] + accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.learning_rate\n@@ -287,6 +282,15 @@ class RMSprop(Optimizer):\n             self.updates.append(K.update(p, new_p))\n         return self.updates\n \n+    def set_weights(self, weights):\n+        params = self.weights\n+        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n+        # since it does not include iteration at head of the weight list. Set\n+        # iteration to 0.\n+        if len(params) == len(weights) + 1:\n+            weights = [np.array(0)] + weights\n+        super(RMSprop, self).set_weights(weights)\n+\n     def get_config(self):\n         config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'rho': float(K.get_value(self.rho)),\n@@ -309,25 +313,21 @@ class Adagrad(Optimizer):\n \n     # Arguments\n         learning_rate: float >= 0. Initial learning rate.\n-        epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.\n-        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adaptive Subgradient Methods for Online Learning and Stochastic\n            Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.01, epsilon=None, decay=0., **kwargs):\n-        learning_rate = kwargs.pop('lr', None) or learning_rate\n+    def __init__(self, learning_rate=0.01, **kwargs):\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        learning_rate = kwargs.pop('lr', learning_rate)\n         super(Adagrad, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.learning_rate = K.variable(learning_rate, name='learning_rate')\n-            self.decay = K.variable(decay, name='decay')\n+            self.decay = K.variable(self.initial_decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n-        self.epsilon = epsilon\n-        self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     @K.symbolic\n@@ -336,7 +336,7 @@ class Adagrad(Optimizer):\n         shapes = [K.int_shape(p) for p in params]\n         accumulators = [K.zeros(shape, name='accumulator_' + str(i))\n                         for (i, shape) in enumerate(shapes)]\n-        self.weights = accumulators\n+        self.weights = [self.iterations] + accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.learning_rate\n@@ -356,6 +356,15 @@ class Adagrad(Optimizer):\n             self.updates.append(K.update(p, new_p))\n         return self.updates\n \n+    def set_weights(self, weights):\n+        params = self.weights\n+        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n+        # since it does not include iteration at head of the weight list. Set\n+        # iteration to 0.\n+        if len(params) == len(weights) + 1:\n+            weights = [np.array(0)] + weights\n+        super(Adagrad, self).set_weights(weights)\n+\n     def get_config(self):\n         config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'decay': float(K.get_value(self.decay)),\n@@ -383,27 +392,22 @@ class Adadelta(Optimizer):\n             It is recommended to leave it at the default value.\n         rho: float >= 0. Adadelta decay factor, corresponding to fraction of\n             gradient to keep at each time step.\n-        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        decay: float >= 0. Initial learning rate decay.\n \n     # References\n         - [Adadelta - an adaptive learning rate method](\n            https://arxiv.org/abs/1212.5701)\n     \"\"\"\n \n-    def __init__(self, learning_rate=1.0, rho=0.95, epsilon=None, decay=0.,\n-                 **kwargs):\n-        learning_rate = kwargs.pop('lr', None) or learning_rate\n+    def __init__(self, learning_rate=1.0, rho=0.95, **kwargs):\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        learning_rate = kwargs.pop('lr', learning_rate)\n         super(Adadelta, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.learning_rate = K.variable(learning_rate, name='learning_rate')\n-            self.decay = K.variable(decay, name='decay')\n+            self.decay = K.variable(self.initial_decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n         self.rho = rho\n-        self.epsilon = epsilon\n-        self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     @K.symbolic\n@@ -414,7 +418,7 @@ class Adadelta(Optimizer):\n                         for (i, shape) in enumerate(shapes)]\n         delta_accumulators = [K.zeros(shape, name='delta_accumulator_' + str(i))\n                               for (i, shape) in enumerate(shapes)]\n-        self.weights = accumulators + delta_accumulators\n+        self.weights = [self.iterations] + accumulators + delta_accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.learning_rate\n@@ -442,6 +446,15 @@ class Adadelta(Optimizer):\n             self.updates.append(K.update(d_a, new_d_a))\n         return self.updates\n \n+    def set_weights(self, weights):\n+        params = self.weights\n+        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n+        # since it does not include iteration at head of the weight list. Set\n+        # iteration to 0.\n+        if len(params) == len(weights) + 1:\n+            weights = [np.array(0)] + weights\n+        super(Adadelta, self).set_weights(weights)\n+\n     def get_config(self):\n         config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'rho': self.rho,\n@@ -460,8 +473,6 @@ class Adam(Optimizer):\n         learning_rate: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n-        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        decay: float >= 0. Learning rate decay over each update.\n         amsgrad: boolean. Whether to apply the AMSGrad variant of this\n             algorithm from the paper \"On the Convergence of Adam and\n             Beyond\".\n@@ -474,19 +485,17 @@ class Adam(Optimizer):\n     \"\"\"\n \n     def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n-                 epsilon=None, decay=0., amsgrad=False, **kwargs):\n-        learning_rate = kwargs.pop('lr', None) or learning_rate\n+                 amsgrad=False, **kwargs):\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        learning_rate = kwargs.pop('lr', learning_rate)\n         super(Adam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n             self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-            self.decay = K.variable(decay, name='decay')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n-        self.epsilon = epsilon\n-        self.initial_decay = decay\n+            self.decay = K.variable(self.initial_decay, name='decay')\n         self.amsgrad = amsgrad\n \n     @interfaces.legacy_get_updates_support\n@@ -565,28 +574,23 @@ class Adamax(Optimizer):\n         learning_rate: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n-        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adam - A Method for Stochastic Optimization](\n            https://arxiv.org/abs/1412.6980v8)\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.002, beta_1=0.9, beta_2=0.999,\n-                 epsilon=None, decay=0., **kwargs):\n-        learning_rate = kwargs.pop('lr', None) or learning_rate\n+    def __init__(self, learning_rate=0.002, beta_1=0.9, beta_2=0.999, **kwargs):\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        learning_rate = kwargs.pop('lr', learning_rate)\n         super(Adamax, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n             self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-            self.decay = K.variable(decay, name='decay')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n-        self.epsilon = epsilon\n-        self.initial_decay = decay\n+            self.decay = K.variable(self.initial_decay, name='decay')\n \n     @interfaces.legacy_get_updates_support\n     @K.symbolic\n@@ -652,8 +656,6 @@ class Nadam(Optimizer):\n         learning_rate: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n-        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        schedule_decay: float, 0 < schedule_decay < 1.\n \n     # References\n         - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n@@ -661,9 +663,10 @@ class Nadam(Optimizer):\n            http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.002, beta_1=0.9, beta_2=0.999,\n-                 epsilon=None, schedule_decay=0.004, **kwargs):\n-        learning_rate = kwargs.pop('lr', None) or learning_rate\n+    def __init__(self, learning_rate=0.002, beta_1=0.9, beta_2=0.999, **kwargs):\n+        self.schedule_decay = kwargs.pop('schedule_decay', 0.004)\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        learning_rate = kwargs.pop('lr', learning_rate)\n         super(Nadam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n@@ -671,10 +674,6 @@ class Nadam(Optimizer):\n             self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n-        self.epsilon = epsilon\n-        self.schedule_decay = schedule_decay\n \n     @interfaces.legacy_get_updates_support\n     @K.symbolic\n@@ -699,7 +698,7 @@ class Nadam(Optimizer):\n         vs = [K.zeros(shape, name='v_' + str(i))\n               for (i, shape) in enumerate(shapes)]\n \n-        self.weights = [self.iterations] + ms + vs\n+        self.weights = [self.iterations, self.m_schedule] + ms + vs\n \n         for p, g, m, v in zip(params, grads, ms, vs):\n             # the following equations given in [1]\n@@ -725,6 +724,15 @@ class Nadam(Optimizer):\n             self.updates.append(K.update(p, new_p))\n         return self.updates\n \n+    def set_weights(self, weights):\n+        params = self.weights\n+        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n+        # since it does not include m_schedule at head of the weight list. Set\n+        # m_schedule to 1.\n+        if len(params) == len(weights) + 1:\n+            weights = [weights[0]] + [np.array(1.)] + weights[1:]\n+        super(Nadam, self).set_weights(weights)\n+\n     def get_config(self):\n         config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6bdacde4c1378062ef784792d9e24268c28b291", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 9812 | Contributors (this commit): 96 | Commits (past 90d): 10 | Contributors (cumulative): 96 | DMM Complexity: 0.0\n\nDIFF:\n@@ -680,11 +680,10 @@ class Model(Network):\n                         y_true, y_pred, sample_weight=sample_weight)\n \n                 if len(self.outputs) > 1:\n-                    # TODO\n-                    # update_ops = self._output_loss_metrics[i].update_state(output_loss)\n-                    # self._metric_updates += update_ops\n-                    if K.backend() == 'tensorflow':\n-                        self._output_loss_metrics[i](output_loss)\n+                    update_ops = self._output_loss_metrics[i].update_state(output_loss)\n+                    self._metric_updates += update_ops\n+                    result = self._output_loss_metrics[i].result()\n+                    self._output_loss_metrics[i]._call_result = result\n \n                 if total_loss is None:\n                     total_loss = loss_weight * output_loss\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2bb96b6fa782abdc57c4ebe07b28cdfb98c49e50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 14740 | Contributors (this commit): 132 | Commits (past 90d): 7 | Contributors (cumulative): 132 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n+from tensorflow.python.framework import device as tfdev\n from tensorflow.python.framework import ops as tf_ops\n from tensorflow.python.training import moving_averages\n from tensorflow.python.ops import tensor_array_ops\n@@ -244,12 +245,18 @@ class _TfDeviceCaptureOp(object):\n     \"\"\"Class for capturing the TF device scope.\"\"\"\n \n     def __init__(self):\n+        # NOTE(robieta): This differs from tf.keras in that self.device is a\n+        # DeviceSpec rather than a string. This is done for compatibility\n+        # with a range of TensorFlow versions.\n         self.device = None\n \n     def _set_device(self, device):\n         \"\"\"This method captures TF's explicit device scope setting.\"\"\"\n         self.device = device\n \n+    def _set_device_from_string(self, device_str):\n+        self.device = tfdev.DeviceSpec.from_string(device_str)\n+\n \n def _get_current_tf_device():\n     \"\"\"Return explicit device of current context, otherwise returns `None`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#61052bc1f1c141c5dba9f83a4af14322ec4e6d7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 7079 | Contributors (this commit): 60 | Commits (past 90d): 2 | Contributors (cumulative): 60 | DMM Complexity: None\n\nDIFF:\n@@ -23,6 +23,25 @@ load_img = image.load_img\n \n \n def array_to_img(x, data_format=None, scale=True, dtype=None):\n+    \"\"\"Converts a 3D Numpy array to a PIL Image instance.\n+\n+    # Arguments\n+        x: Input Numpy array.\n+        data_format: Image data format.\n+            either \"channels_first\" or \"channels_last\".\n+            If omitted (`None`), then `backend.image_data_format()` is used.\n+        scale: Whether to rescale image values\n+            to be within `[0, 255]`.\n+        dtype: Dtype to use.\n+            If omitted (`None`), then `backend.floatx()` or `float32` are used.\n+\n+    # Returns\n+        A PIL Image instance.\n+\n+    # Raises\n+        ImportError: if PIL is not available.\n+        ValueError: if invalid `x` or `data_format` is passed.\n+    \"\"\"\n     if data_format is None:\n         data_format = backend.image_data_format()\n     if 'dtype' in generic_utils.getargspec(image.array_to_img).args:\n@@ -38,6 +57,21 @@ def array_to_img(x, data_format=None, scale=True, dtype=None):\n \n \n def img_to_array(img, data_format=None, dtype=None):\n+    \"\"\"Converts a PIL Image instance to a Numpy array.\n+\n+    # Arguments\n+        img: PIL Image instance.\n+        data_format: Image data format, either \"channels_first\" or \"channels_last\".\n+            If omitted (`None`), then `backend.image_data_format()` is used.\n+        dtype: Dtype to use for the returned array.\n+            If omitted (`None`), then `backend.floatx()` or `float32` are used.\n+\n+    # Returns\n+        A 3D Numpy array.\n+\n+    # Raises\n+        ValueError: if invalid `img` or `data_format` is passed.\n+    \"\"\"\n     if data_format is None:\n         data_format = backend.image_data_format()\n     if 'dtype' in generic_utils.getargspec(image.img_to_array).args:\n@@ -52,6 +86,21 @@ def save_img(path,\n              data_format=None,\n              file_format=None,\n              scale=True, **kwargs):\n+    \"\"\"Saves an image stored as a Numpy array to a path or file object.\n+\n+    # Arguments\n+        path: Path or file object.\n+        x: Numpy array.\n+        data_format: Image data format,\n+            either \"channels_first\" or \"channels_last\".\n+            If omitted (`None`), then `backend.image_data_format()` is used.\n+        file_format: Optional file format override. If omitted, the\n+            format to use is determined from the filename extension.\n+            If a file object was used instead of a filename, this\n+            parameter should always be used.\n+        scale: Whether to rescale image values to be within `[0, 255]`.\n+        **kwargs: Additional keyword arguments passed to `PIL.Image.save()`.\n+    \"\"\"\n     if data_format is None:\n         data_format = backend.image_data_format()\n     return image.save_img(path,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a47f5e231c51fe3f1e6212ca05a20286096bf2e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 361 | Lines Deleted: 18 | Files Changed: 7 | Hunks: 25 | Methods Changed: 40 | Complexity Δ (Sum/Max): -17/21 | Churn Δ: 379 | Churn Cumulative: 33993 | Contributors (this commit): 217 | Commits (past 90d): 45 | Contributors (cumulative): 309 | DMM Complexity: 0.9063829787234042\n\nDIFF:\n@@ -851,6 +851,9 @@ def size(x, name=None):\n     ```\n \n     \"\"\"\n+    if is_symbolic(x):\n+        with get_graph().as_default():\n+            return tf.size(x)\n     return tf.size(x, name=name)\n \n \n\n@@ -105,6 +105,10 @@ class Layer(object):\n         self._per_input_updates = {}\n         self._built = False\n \n+        # A list of metric instances corresponding to the metric tensors added using\n+        # the `add_metric` API.\n+        self._metrics = []\n+\n         # These lists will be filled via successive calls\n         # to self._add_inbound_node().\n         self._inbound_nodes = []\n@@ -926,6 +930,29 @@ class Layer(object):\n                                  'Use `get_output_shape_at(node_index)` '\n                                  'instead.')\n \n+    @property\n+    def metrics(self):\n+        return self._metrics\n+\n+    def add_metric(self, value, name=None):\n+        \"\"\"Adds metric tensor to the layer.\n+\n+        # Arguments\n+            value: Metric tensor.\n+            name: String metric name.\n+        \"\"\"\n+        match = self._get_existing_metric(name)\n+        if match:\n+            return\n+        if hasattr(value, '_metric_obj'):\n+            # We track the instance using the metadata on the result tensor.\n+            # Use case: model.add_metric(metrics.Mean(name='metric_2')(y))\n+            self._metrics.append(value._metric_obj)\n+        else:\n+            # Use cases: model.add_metric(K.sum(y), name='metric_1')\n+            metric_obj = _create_mean_metric(value, name)\n+            self._metrics.append(metric_obj)\n+\n     def add_loss(self, losses, inputs=None):\n         \"\"\"Adds losses to the layer.\n \n@@ -1123,6 +1150,29 @@ class Layer(object):\n                                    self.name + '.build(batch_input_shape)`.')\n         return count_params(self.weights)\n \n+    def _get_existing_metric(self, name=None):\n+        match = [m for m in self._metrics if m.name == name]\n+        if not match:\n+            return\n+        if len(match) > 1:\n+            raise ValueError(\n+                'Please provide different names for the metrics you have added. '\n+                'We found {} metrics with the name: \"{}\"'.format(len(match), name))\n+        return match[0]\n+\n+\n+def _create_mean_metric(value, name=None):\n+    from .. import metrics\n+    metric_obj = metrics.Mean(name=name)\n+    _call_metric(metric_obj, value)\n+    return metric_obj\n+\n+@K.symbolic\n+def _call_metric(metric_obj, *args, **kwargs):\n+    update_op = metric_obj.update_state(*args, **kwargs)\n+    with K.control_dependencies(update_op):  # For TF\n+        result_t = metric_obj.result()\n+\n \n class InputSpec(object):\n     \"\"\"Specifies the ndim, dtype and shape of every input to a layer.\n\n@@ -134,6 +134,10 @@ class Network(Layer):\n         self._per_input_losses = {}\n         self._per_input_updates = {}\n \n+        # A list of metric instances corresponding to the metric tensors added using\n+        # the `add_metric` API.\n+        self._metrics = []\n+\n         # All layers in order of horizontal graph traversal.\n         # Entries are unique. Includes input and output layers.\n         self._layers = []\n@@ -321,10 +325,24 @@ class Network(Layer):\n                     self._layers.append(value)\n         super(Network, self).__setattr__(name, value)\n \n+        # Keep track of metric instance created in subclassed model/layer.\n+        # We do this so that we can maintain the correct order of metrics by adding\n+        # the instance to the `metrics` list as soon as it is created.\n+        from .. import metrics as metrics_module\n+        if isinstance(value, metrics_module.Metric):\n+            self._metrics.append(value)\n+\n     @property\n     def layers(self):\n         return self._layers\n \n+    @property\n+    def metrics(self):\n+        metrics = self._metrics\n+        for l in self.layers:\n+            metrics += l.metrics\n+        return metrics\n+\n     def get_layer(self, name=None, index=None):\n         \"\"\"Retrieves a layer based on either its name (unique) or index.\n \n\n@@ -122,7 +122,6 @@ class Model(Network):\n         self._feed_output_names = []\n         self._feed_output_shapes = []\n         self._feed_loss_fns = []\n-        self._metric_updates = []\n \n         # if loss function is None, then this output will be skipped during total\n         # loss calculation and feed targets preparation.\n@@ -248,6 +247,8 @@ class Model(Network):\n         metrics = []\n         if self._is_compiled:\n             metrics += self._compile_metric_functions\n+        metrics.extend(self._metrics)\n+        metrics.extend(_get_metrics_from_layers(self._layers))\n         return metrics\n \n     @property\n@@ -265,6 +266,11 @@ class Model(Network):\n \n             # Add compile metrics/weighted metrics' names to the metric names list.\n             metrics_names.extend([m.name for m in self._compile_metric_functions])\n+\n+        # Add metric names from layers.\n+        for layer in self.layers:\n+            metrics_names += [m.name for m in layer._metrics]\n+        metrics_names += [m.name for m in self._metrics]\n         return metrics_names\n \n     def reset_metrics(self):\n@@ -308,20 +314,21 @@ class Model(Network):\n                     training_updates = self.optimizer.get_updates(\n                         params=self._collected_trainable_weights,\n                         loss=self.total_loss)\n-                updates = (self.updates +\n-                           training_updates +\n-                           self._metric_updates)\n+                updates = self.updates + training_updates\n \n                 metrics = self._get_training_eval_metrics()\n                 metrics_tensors = [\n                     m._call_result for m in metrics if hasattr(m, '_call_result')\n                 ]\n+                metrics_updates = []\n+                for m in metrics:\n+                    metrics_updates.extend(m.updates)\n \n                 # Gets loss and metrics. Updates weights at each call.\n                 self.train_function = K.function(\n                     inputs,\n                     [self.total_loss] + metrics_tensors,\n-                    updates=updates,\n+                    updates=updates + metrics_updates,\n                     name='train_function',\n                     **self._function_kwargs)\n \n@@ -339,12 +346,17 @@ class Model(Network):\n             metrics_tensors = [\n                 m._call_result for m in metrics if hasattr(m, '_call_result')\n             ]\n+\n+            metrics_updates = []\n+            for m in metrics:\n+                metrics_updates.extend(m.updates)\n+\n             # Return loss and metrics, no gradient updates.\n             # Does update the network states.\n             self.test_function = K.function(\n                 inputs,\n                 [self.total_loss] + metrics_tensors,\n-                updates=self.state_updates + self._metric_updates,\n+                updates=self.state_updates + metrics_updates,\n                 name='test_function',\n                 **self._function_kwargs)\n \n@@ -681,10 +693,8 @@ class Model(Network):\n \n                 if len(self.outputs) > 1:\n                     update_ops = self._output_loss_metrics[i].update_state(output_loss)\n-                    self._metric_updates += update_ops\n-                    result = self._output_loss_metrics[i].result()\n-                    self._output_loss_metrics[i]._call_result = result\n-\n+                    with K.control_dependencies(update_ops):  # For TF\n+                        self._output_loss_metrics[i].result()\n                 if total_loss is None:\n                     total_loss = loss_weight * output_loss\n                 else:\n@@ -827,9 +837,8 @@ class Model(Network):\n \n         for metric_name, metric_fn in metrics_dict.items():\n             with K.name_scope(metric_name):\n-                metric_result, update_ops = training_utils.call_metric_function(\n+                training_utils.call_metric_function(\n                     metric_fn, y_true, y_pred, weights=weights, mask=mask)\n-                self._metric_updates += update_ops\n \n     def _handle_metrics(self,\n                         outputs,\n@@ -1834,3 +1843,25 @@ class Model(Network):\n             workers=workers,\n             use_multiprocessing=use_multiprocessing,\n             verbose=verbose)\n+\n+\n+def _get_metrics_from_layers(layers):\n+    \"\"\"Returns list of metrics from the given layers.\n+    This will not include the `compile` metrics of a model layer.\n+\n+    # Arguments\n+        layers: List of layers.\n+\n+    # Returns\n+        List of metrics.\n+    \"\"\"\n+    metrics = []\n+    for layer in layers:\n+        if isinstance(layer, Model):\n+            # We cannot call 'metrics' on the model because we do not want to\n+            # include the metrics that were added in compile API of a nested model.\n+            metrics.extend(layer._metrics)\n+            metrics.extend(_get_metrics_from_layers(layer.layers))\n+        else:\n+            metrics.extend(layer.metrics)\n+    return metrics\n\n@@ -1031,11 +1031,10 @@ def call_metric_function(metric_fn,\n \n     if y_pred is not None:\n         update_ops = metric_fn.update_state(y_true, y_pred, sample_weight=weights)\n-        with K.control_dependencies(update_ops):\n-            result = metric_fn.result()\n+        with K.control_dependencies(update_ops):  # For TF\n+            metric_fn.result()\n     else:\n         # `Mean` metric only takes a single value.\n         update_ops = metric_fn.update_state(y_true, sample_weight=weights)\n-        with K.control_dependencies(update_ops):\n-            result = metric_fn.result()\n-    return result, update_ops\n+        with K.control_dependencies(update_ops):  # For TF\n+            metric_fn.result()\n\n@@ -73,6 +73,7 @@ class Metric(Layer):\n             metrics_utils.result_wrapper(obj.result), obj)\n         return obj\n \n+    @K.symbolic\n     def __call__(self, *args, **kwargs):\n         \"\"\"Accumulates statistics and then computes metric result value.\"\"\"\n         if K.backend() != 'tensorflow':\n@@ -80,7 +81,18 @@ class Metric(Layer):\n                 'Metric calling only supported with TensorFlow backend.')\n         update_op = self.update_state(*args, **kwargs)\n         with K.control_dependencies(update_op):  # For TF\n-            return self.result()\n+            result_t = self.result()\n+\n+            # We are adding the metric object as metadata on the result tensor.\n+            # This is required when we want to use a metric with `add_metric` API on\n+            # a Model/Layer in graph mode. This metric instance will later be used\n+            # to reset variable state after each epoch of training.\n+            # Example:\n+            #   model = Model()\n+            #   mean = Mean()\n+            #   model.add_metric(mean(values), name='mean')\n+            result_t._metric_obj = self\n+            return result_t\n \n     def get_config(self):\n         \"\"\"Returns the serializable config of the metric.\"\"\"\n\n@@ -10,6 +10,7 @@ from flaky import flaky\n \n import keras\n from keras import losses\n+from keras import metrics\n from keras.layers import Layer, Activation, Dense, Dropout, Conv2D, Concatenate\n from keras.engine import Input\n from keras.engine.training import Model\n@@ -1747,5 +1748,234 @@ def test_loss_correctness():\n     np.allclose(history.history['loss'], [1., 0.9, 0.8, 0.7, 0.6])\n \n \n+def test_model_metrics_list():\n+\n+    class LayerWithAddMetric(Layer):\n+\n+        def __init__(self):\n+            super(LayerWithAddMetric, self).__init__()\n+            self.dense = keras.layers.Dense(1, kernel_initializer='ones')\n+\n+        def __call__(self, inputs):\n+            outputs = self.dense(inputs)\n+            return outputs\n+\n+    class LayerWithNestedAddMetricLayer(Layer):\n+\n+        def __init__(self):\n+            super(LayerWithNestedAddMetricLayer, self).__init__()\n+            self.layer = LayerWithAddMetric()\n+\n+        def call(self, inputs):\n+            outputs = self.layer(inputs)\n+            self.add_metric(K.sum(outputs), name='metric_4')\n+            return outputs\n+\n+    x = Input(shape=(1,))\n+    y = LayerWithNestedAddMetricLayer()(x)\n+\n+    model = keras.models.Model(x, y)\n+    model.add_metric(K.sum(y), name='metric_2')\n+    model.add_metric(metrics.Mean(name='metric_3')(y))\n+\n+    model.compile(\n+        'sgd',\n+        loss='mse',\n+        metrics=[metrics.MeanSquaredError('metric_1')])\n+\n+    # Verify that the metrics added using `compile` and `add_metric` API are\n+    # included\n+    for m1, m2 in zip([m.name for m in model._compile_metrics], ['metric_1']):\n+        assert m1 == m2\n+\n+    for m1, m2 in zip(\n+            [m.name for m in model.metrics],\n+            ['metric_1', 'metric_2', 'metric_3', 'metric_4']):\n+        assert m1 == m2\n+\n+\n+def test_model_metrics_list_in_call():\n+\n+    class TestModel(Model):\n+\n+        def __init__(self):\n+            super(TestModel, self).__init__(name='test_model')\n+            self.dense1 = keras.layers.Dense(2)\n+\n+        def call(self, x):\n+            self.add_metric(K.sum(x), name='metric_2')\n+            return self.dense1(x)\n+\n+    model = TestModel()\n+    model.compile(\n+        loss='mse',\n+        optimizer='adam',\n+        metrics=[metrics.MeanSquaredError('metric_1')])\n+    x = np.ones(shape=(10, 1))\n+    y = np.ones(shape=(10, 2))\n+    model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n+\n+    # Verify that the metrics added using `compile` and `add_metric` API are\n+    # included\n+    for m1, m2 in zip([m.name for m in model._compile_metrics], ['metric_1']):\n+        assert m1 == m2\n+\n+    for m1, m2 in zip(\n+            [m.name for m in model.metrics],\n+            ['metric_1', 'metric_2']):\n+        assert m1 == m2\n+\n+\n+def test_duplicate_metric_name_in_add_metric():\n+\n+    class TestModel(Model):\n+\n+        def __init__(self):\n+            super(TestModel, self).__init__(name='test_model')\n+            self.dense1 = keras.layers.Dense(2, kernel_initializer='ones')\n+            self.mean = metrics.Mean(name='metric_1')\n+            self.mean2 = metrics.Mean(name='metric_1')\n+\n+        def call(self, x):\n+            self.add_metric(self.mean(x), name='metric_1')\n+            return self.dense1(x)\n+\n+    model = TestModel()\n+    model.compile(loss='mse', optimizer='adam')\n+\n+    x = np.ones(shape=(10, 1))\n+    y = np.ones(shape=(10, 2))\n+    with pytest.raises(ValueError):\n+        model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n+\n+\n+def test_add_metric_on_model():\n+    x = Input(shape=(1,))\n+    y = Dense(1, kernel_initializer='ones', trainable=False)(x)\n+    model = Model(x, y)\n+    model.add_metric(K.sum(y), name='metric_1')\n+    model.add_metric(metrics.Mean(name='metric_2')(y))\n+    model.compile('sgd', loss='mse', metrics=['mse'])\n+\n+    inputs = np.ones(shape=(10, 1))\n+    targets = np.zeros(shape=(10, 1))\n+    history = model.fit(\n+        inputs,\n+        targets,\n+        epochs=2,\n+        batch_size=5,\n+        validation_data=(inputs, targets))\n+    assert history.history['metric_1'][-1] == 5\n+    assert history.history['val_metric_1'][-1] == 5\n+\n+    assert history.history['metric_2'][-1] == 1\n+    assert history.history['val_metric_2'][-1] == 1\n+\n+    eval_results = model.evaluate(inputs, targets, batch_size=5)\n+    assert eval_results[-2] == 5\n+    assert eval_results[-1] == 1\n+\n+    model.predict(inputs, batch_size=5)\n+    model.train_on_batch(inputs, targets)\n+    model.test_on_batch(inputs, targets)\n+\n+\n+def test_add_metric_in_model_call():\n+\n+    class TestModel(Model):\n+\n+        def __init__(self):\n+            super(TestModel, self).__init__(name='test_model')\n+            self.dense1 = keras.layers.Dense(2, kernel_initializer='ones')\n+            self.mean = metrics.Mean(name='metric_1')\n+\n+        def call(self, x):\n+            self.add_metric(K.sum(x), name='metric_2')\n+            # Provide same name as in the instance created in __init__\n+            # for eager mode\n+            self.add_metric(self.mean(x), name='metric_1')\n+            return self.dense1(x)\n+\n+    model = TestModel()\n+    model.compile(loss='mse', optimizer='sgd')\n+\n+    x = np.ones(shape=(10, 1))\n+    y = np.ones(shape=(10, 2))\n+    history = model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n+    assert np.isclose(history.history['metric_1'][-1], 1, 0)\n+    assert np.isclose(history.history['val_metric_1'][-1], 1, 0)\n+    assert np.isclose(history.history['metric_2'][-1], 5, 0)\n+    assert np.isclose(history.history['val_metric_2'][-1], 5, 0)\n+\n+    eval_results = model.evaluate(x, y, batch_size=5)\n+    assert np.isclose(eval_results[1], 1, 0)\n+    assert np.isclose(eval_results[2], 5, 0)\n+\n+    model.predict(x, batch_size=5)\n+    model.train_on_batch(x, y)\n+    model.test_on_batch(x, y)\n+\n+\n+def test_multiple_add_metric_calls():\n+\n+    class TestModel(Model):\n+\n+        def __init__(self):\n+            super(TestModel, self).__init__(name='test_model')\n+            self.dense1 = keras.layers.Dense(2, kernel_initializer='ones')\n+            self.mean1 = metrics.Mean(name='metric_1')\n+            self.mean2 = metrics.Mean(name='metric_2')\n+\n+        def call(self, x):\n+            self.add_metric(self.mean2(x), name='metric_2')\n+            self.add_metric(self.mean1(x), name='metric_1')\n+            self.add_metric(K.sum(x), name='metric_3')\n+            return self.dense1(x)\n+\n+    model = TestModel()\n+    model.compile(loss='mse', optimizer='sgd')\n+\n+    x = np.ones(shape=(10, 1))\n+    y = np.ones(shape=(10, 2))\n+    history = model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n+    assert np.isclose(history.history['metric_1'][-1], 1, 0)\n+    assert np.isclose(history.history['metric_2'][-1], 1, 0)\n+    assert np.isclose(history.history['metric_3'][-1], 5, 0)\n+\n+    eval_results = model.evaluate(x, y, batch_size=5)\n+    assert np.allclose(eval_results[1:4], [1, 1, 5], 0.1)\n+\n+    model.predict(x, batch_size=5)\n+    model.train_on_batch(x, y)\n+    model.test_on_batch(x, y)\n+\n+\n+def test_add_metric_in_layer_call():\n+\n+    class TestLayer(Layer):\n+\n+        def build(self, input_shape):\n+            self.a = self.add_weight(\n+                'a', (1, 1), initializer='ones', trainable=False)\n+            self.built = True\n+\n+        def call(self, inputs):\n+            self.add_metric(K.sum(inputs), name='metric_1')\n+            return inputs + 1\n+\n+    inp = Input(shape=(1,))\n+    x = TestLayer(input_shape=(1,))(inp)\n+    x = keras.layers.Dense(2, kernel_initializer='ones')(x)\n+\n+    model = Model(inp, x)\n+    model.compile('adam', loss='mse')\n+\n+    x = np.ones(shape=(10, 1))\n+    y = np.ones(shape=(10, 2))\n+    history = model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n+    assert np.isclose(history.history['metric_1'][-1], 5, 0)\n+    assert np.isclose(history.history['val_metric_1'][-1], 5, 0)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
