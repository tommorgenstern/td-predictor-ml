{"custom_id": "keras#b62836ed3ddcd599d078b0b8d998467e19ca3043", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 5 | Files Changed: 6 | Hunks: 14 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 39 | Churn Cumulative: 2237 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 12 | DMM Complexity: 0.5\n\nDIFF:\n@@ -15,9 +15,11 @@\n \"\"\"Adadelta optimizer implementation.\"\"\"\n \n from keras.optimizer_experimental import optimizer\n+from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n \n \n+@generic_utils.register_keras_serializable()\n class Adadelta(optimizer.Optimizer):\n   r\"\"\"Optimizer that implements the Adadelta algorithm.\n \n\n@@ -16,9 +16,11 @@\n \n from keras import initializers\n from keras.optimizer_experimental import optimizer\n+from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n \n \n+@generic_utils.register_keras_serializable()\n class Adagrad(optimizer.Optimizer):\n   r\"\"\"Optimizer that implements the Adagrad algorithm.\n \n\n@@ -15,9 +15,11 @@\n \"\"\"Adam optimizer implementation.\"\"\"\n \n from keras.optimizer_experimental import optimizer\n-import tensorflow as tf\n+from keras.utils import generic_utils\n+import tensorflow.compat.v2 as tf\n \n \n+@generic_utils.register_keras_serializable()\n class Adam(optimizer.Optimizer):\n   r\"\"\"Optimizer that implements the Adam algorithm.\n \n\n@@ -165,13 +165,25 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n     model.compile(loss=\"mse\", optimizer=optimizer)\n     model.fit(x, y)\n+\n+    # Save in h5 format.\n+    path = os.path.join(self.get_temp_dir(), \"model.h5\")\n+    model.save(path)\n+    loaded_model = keras.models.load_model(path)\n+    loaded_model.load_weights(path)\n+    self.assertEqual(type(optimizer), type(loaded_model.optimizer))\n+    self.assertEqual(loaded_model.optimizer.learning_rate, 0.002)\n+    self.assertEqual(loaded_model.optimizer._clipnorm, 0.1)\n+\n+    # Save in Keras SavedModel format.\n+    model.fit(x, y)\n     path = os.path.join(self.get_temp_dir(), \"model\")\n     model.save(path)\n     loaded_model = keras.models.load_model(path)\n     loaded_model.load_weights(path)\n+    self.assertEqual(type(optimizer), type(loaded_model.optimizer))\n     self.assertEqual(loaded_model.optimizer.learning_rate, 0.002)\n     self.assertEqual(loaded_model.optimizer._clipnorm, 0.1)\n-    model.fit(x, y)\n \n \n class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -15,9 +15,11 @@\n \"\"\"SGD optimizer implementation.\"\"\"\n \n from keras.optimizer_experimental import optimizer\n-import tensorflow as tf\n+from keras.utils import generic_utils\n+import tensorflow.compat.v2 as tf\n \n \n+@generic_utils.register_keras_serializable()\n class SGD(optimizer.Optimizer):\n   r\"\"\"Gradient descent (with momentum) optimizer.\n \n\n@@ -24,6 +24,7 @@ import numpy as np\n \n from keras import backend\n from keras import optimizer_v1\n+from keras.optimizer_experimental import optimizer as optimizer_experimental\n from keras.saving import model_config as model_config_lib\n from keras.saving import saving_utils\n from keras.saving.saved_model import json_utils\n@@ -121,7 +122,11 @@ def save_model_to_hdf5(model, filepath, overwrite=True, include_optimizer=True):\n \n     # TODO(b/128683857): Add integration tests between tf.keras and external\n     # Keras, to avoid breaking TF.js users.\n-    if (include_optimizer and model.optimizer and\n+    if isinstance(model.optimizer, optimizer_experimental.Optimizer):\n+      logging.warning('HDF5 format does not save weights of'\n+                      ' `optimizer_experimental.Optimizer`, your optimizer will'\n+                      ' be recompiled at loading time.')\n+    elif (include_optimizer and model.optimizer and\n           not isinstance(model.optimizer, optimizer_v1.TFOptimizer)):\n       save_optimizer_weights_to_hdf5_group(f, model.optimizer)\n \n@@ -201,7 +206,11 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):  # pylint\n       saving_utils.try_build_compiled_arguments(model)\n \n       # Set optimizer weights.\n-      if 'optimizer_weights' in f:\n+      if isinstance(model.optimizer, optimizer_experimental.Optimizer):\n+        logging.warning('Loading model from HDF5 will not restore the '\n+                        'optimizer\\'s weights, since the optimizer is an '\n+                        'instance of `optimizer_experimental.Optimizer`')\n+      elif 'optimizer_weights' in f:\n         try:\n           model.optimizer._create_all_weights(model.trainable_variables)\n         except (NotImplementedError, AttributeError):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#593c06fc7f5c4fa9ec1df505e8541020589bd1a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 502 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 0.125\n\nDIFF:\n@@ -220,8 +220,10 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n     self._compare_numerical(\n         adagrad_old.Adagrad(), adagrad_new.Adagrad())\n \n-  def testSgd(self):\n-    self._compare_numerical(sgd_old.SGD(), sgd_new.SGD())\n+  @parameterized.product(nesterov=[True, False])\n+  def testSgd(self, nesterov):\n+    self._compare_numerical(\n+        sgd_old.SGD(nesterov=True), sgd_new.SGD(nesterov=True))\n \n \n class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -155,6 +155,10 @@ class SGD(optimizer.Optimizer):\n       if m is not None:\n         m.assign(m * momentum)\n         m.scatter_add(add_value)\n+        if self.nesterov:\n+          variable.scatter_add(add_value)\n+          variable.assign_add(m * momentum)\n+        else:\n           variable.assign_add(m)\n       else:\n         variable.scatter_add(add_value)\n@@ -162,6 +166,9 @@ class SGD(optimizer.Optimizer):\n       # Dense gradients\n       if m is not None:\n         m.assign(-gradient * lr + m * momentum)\n+        if self.nesterov:\n+          variable.assign_add(-gradient * lr + m * momentum)\n+        else:\n           variable.assign_add(m)\n       else:\n         variable.assign_add(-gradient * lr)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46fce0b28645f366e2ab66f3f91ff1027d0e257f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 7328 | Contributors (this commit): 99 | Commits (past 90d): 14 | Contributors (cumulative): 99 | DMM Complexity: None\n\nDIFF:\n@@ -1205,7 +1205,7 @@ class ModelCheckpoint(Callback):\n \n   Args:\n       filepath: string or `PathLike`, path to save the model file. e.g.\n-        filepath = tf.io.gfile.join(working_dir, 'ckpt', file_name). `filepath`\n+        filepath = os.path.join(working_dir, 'ckpt', file_name). `filepath`\n         can contain named formatting options, which will be filled the value of\n         `epoch` and keys in `logs` (passed in `on_epoch_end`). For example: if\n         `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the model\n@@ -1515,9 +1515,9 @@ class ModelCheckpoint(Callback):\n     ```python\n     file_pattern = 'f.batch{batch:02d}epoch{epoch:02d}.h5'\n     test_dir = self.get_temp_dir()\n-    path_pattern = tf.io.gfile.join(test_dir, file_pattern)\n+    path_pattern = os.path.join(test_dir, file_pattern)\n     file_paths = [\n-        tf.io.gfile.join(test_dir, file_name) for file_name in\n+        os.path.join(test_dir, file_name) for file_name in\n         ['f.batch03epoch02.h5', 'f.batch02epoch02.h5', 'f.batch01epoch01.h5']\n     ]\n     for file_path in file_paths:\n@@ -1557,7 +1557,7 @@ class ModelCheckpoint(Callback):\n       for file_name in os.listdir(dir_name):\n         # Only consider if `file_name` matches the pattern.\n         if re.match(base_name_regex, file_name):\n-          file_path = tf.io.gfile.join(dir_name, file_name)\n+          file_path = os.path.join(dir_name, file_name)\n           mod_time = os.path.getmtime(file_path)\n           if (file_path_with_largest_file_name is None or\n               file_path > file_path_with_largest_file_name):\n@@ -1642,7 +1642,7 @@ class BackupAndRestore(Callback):\n \n   Args:\n       backup_dir: String, path to store the checkpoint.\n-        e.g. backup_dir = tf.io.gfile.join(working_dir, 'backup')\n+        e.g. backup_dir = os.path.join(working_dir, 'backup')\n         This is the directory in which the system stores temporary files to\n         recover the model from jobs terminated unexpectedly. The directory\n         cannot be reused elsewhere to store other files, e.g. by\n@@ -2061,7 +2061,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n \n   Args:\n       log_dir: the path of the directory where to save the log files to be\n-        parsed by TensorBoard. e.g. log_dir = tf.io.gfile.join(working_dir, 'logs')\n+        parsed by TensorBoard. e.g. log_dir = os.path.join(working_dir, 'logs')\n         This directory should not be reused by any other callbacks.\n       histogram_freq: frequency (in epochs) at which to compute activation and\n         weight histograms for the layers of the model. If set to 0, histograms\n@@ -2229,10 +2229,10 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     self.model = model\n     self._log_write_dir = self._get_log_write_dir()\n \n-    self._train_dir = tf.io.gfile.join(self._log_write_dir, 'train')\n+    self._train_dir = os.path.join(self._log_write_dir, 'train')\n     self._train_step = self.model._train_counter  # pylint: disable=protected-access\n \n-    self._val_dir = tf.io.gfile.join(self._log_write_dir, 'validation')\n+    self._val_dir = os.path.join(self._log_write_dir, 'validation')\n     self._val_step = self.model._test_counter  # pylint: disable=protected-access\n \n     self._writers = {}  # Resets writers.\n@@ -2316,7 +2316,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n                        f'argument: {self.embeddings_metadata.keys()}')\n \n     config_pbtxt = text_format.MessageToString(config)\n-    path = tf.io.gfile.join(self._log_write_dir, 'projector_config.pbtxt')\n+    path = os.path.join(self._log_write_dir, 'projector_config.pbtxt')\n     with tf.io.gfile.GFile(path, 'w') as f:\n       f.write(config_pbtxt)\n \n@@ -2563,7 +2563,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n       tf.summary.image(weight_name, w_img, step=epoch)\n \n   def _log_embeddings(self, epoch):\n-    embeddings_ckpt = tf.io.gfile.join(self._log_write_dir, 'train',\n+    embeddings_ckpt = os.path.join(self._log_write_dir, 'train',\n                                    'keras_embedding.ckpt-{}'.format(epoch))\n     self.model.save_weights(embeddings_ckpt)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c2f41fce27f85632c494d96a976754ddf3f3a64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 18 | Churn Cumulative: 880 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -143,7 +143,10 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n                        \" optimizer creation time.\")\n     lr = self._learning_rate\n     if isinstance(lr, learning_rate_schedule.LearningRateSchedule):\n-      return lr(self.iterations)  # pylint: disable=not-callable\n+      # If the optimizer takes in LearningRateSchedule, then each call to\n+      # learning_rate would return `self._current_learning_rate`, which is\n+      # updated at each call to `apply_gradients`.\n+      return self._current_learning_rate\n     return lr\n \n   @learning_rate.setter\n@@ -159,6 +162,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n \n   def _build_learning_rate(self, learning_rate):\n     if isinstance(learning_rate, learning_rate_schedule.LearningRateSchedule):\n+      self._current_learning_rate = learning_rate(self.iterations)\n       return learning_rate\n     return tf.Variable(learning_rate, dtype=backend.floatx())\n \n@@ -286,6 +290,10 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     Raises:\n       TypeError: If `grads_and_vars` is malformed.\n     \"\"\"\n+    if isinstance(self._learning_rate,\n+                  learning_rate_schedule.LearningRateSchedule):\n+      # Compute the current learning rate at the beginning of variable update.\n+      self._current_learning_rate = self._learning_rate(self.iterations)\n     grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n     grads, trainable_variables = zip(*grads_and_vars)\n     scope_name = self._name or \"optimizer\"\n\n@@ -102,6 +102,14 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     optimizer = adam_new.Adam(learning_rate=lr_schedule)\n     self.assertIsInstance(optimizer._learning_rate,\n                           learning_rate_schedule.ExponentialDecay)\n+    self.assertEqual(optimizer.learning_rate, 0.01)\n+\n+    x = tf.Variable([1.0, 2.0], dtype=tf.float32)\n+    grads = tf.convert_to_tensor([1.0, 2.0])\n+    for _ in range(2):\n+      optimizer.apply_gradients(zip([grads], [x]))\n+    self.assertTrue(optimizer.learning_rate < 0.01 and\n+                    optimizer.learning_rate > 0.00999)\n     with self.assertRaisesRegex(TypeError, \"This optimizer was created with*\"):\n       optimizer.learning_rate = 2.0\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d945b0df583e94b3d5f416785c6bdc6fa02a3e4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 327 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 12 | Methods Changed: 11 | Complexity Δ (Sum/Max): 15/8 | Churn Δ: 338 | Churn Cumulative: 3491 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,103 @@\n+\"\"\"The DetermisticRandomTestTool.\n+\n+(from www.tensorflow.org/guide/migrate/validate_correctness) is a tool used to\n+make random number generation semantics match between TF1.x graphs/sessions and\n+eager execution.\n+\"\"\"\n+\n+import sys\n+\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n+\n+\n+@keras_export(v1=[\"keras.utils.DeterministicRandomTestTool\"])\n+class DeterministicRandomTestTool(object):\n+  \"\"\"DeterministicRandomTestTool is a testing tool.\n+\n+  This tool is used to validate random number generation semantics match between\n+  TF1.x graphs/sessions and eager execution.\n+\n+  This is useful when you are migrating from TF 1.x to TF2 and need to make sure\n+  your computation is still happening correctly along the way. See the\n+  validating correctness migration guide for more info :\n+  https://www.tensorflow.org/guide/migrate/validate_correctness\n+\n+  The following DeterministicRandomTestTool object provides a context manager\n+  scope() that can make stateful random operations use the same seed across both\n+  TF1 graphs/sessions and eager execution,The tool provides two testing modes:\n+  - constant which uses the same seed for every single operation no matter how\n+  many times it has been called and,\n+  - num_random_ops which uses the number of previously-observed stateful random\n+  operations as the operation seed.\n+  The num_random_ops mode serves as a more sensitive validation check than the\n+  constant mode. It ensures that the random numbers initialization does not get\n+  accidentaly reused.(for example if several weights take on the same\n+  initializations), you can use the num_random_ops mode to avoid this. In the\n+  num_random_ops mode, the generated random numbers will depend on the ordering\n+  of random ops in the program.\n+\n+  This applies both to the stateful random operations used for creating and\n+  initializing variables, and to the stateful random operations used in\n+  computation (such as for dropout layers).\n+  \"\"\"\n+\n+  def __init__(self, seed: int = 42, mode=\"constant\"):\n+    \"\"\"Set mode to 'constant' or 'num_random_ops'. Defaults to 'constant'.\"\"\"\n+    if mode not in {\"constant\", \"num_random_ops\"}:\n+      raise ValueError(\"Mode arg must be 'constant' or 'num_random_ops'. \" +\n+                       \"Got: {}\".format(mode))\n+    self.seed_implementation = sys.modules[tf.compat.v1.get_seed.__module__]\n+    self._mode = mode\n+    self._seed = seed\n+    self.operation_seed = 0\n+    self._observed_seeds = set()\n+\n+  @property\n+  def operation_seed(self):\n+    return self._operation_seed\n+\n+  @operation_seed.setter\n+  def operation_seed(self, value):\n+    self._operation_seed = value\n+\n+  def scope(self):\n+    \"\"\"set random seed.\"\"\"\n+\n+    tf.random.set_seed(self._seed)\n+    def _get_seed(_):\n+      \"\"\"Wraps TF get_seed to make deterministic random generation easier.\n+\n+      This makes a variable's initialization (and calls that involve random\n+      number generation) depend only on how many random number generations\n+      were used in the scope so far, rather than on how many unrelated\n+      operations the graph contains.\n+\n+      Returns:\n+        Random seed tuple.\n+      \"\"\"\n+      op_seed = self._operation_seed\n+      if self._mode == \"constant\":\n+        tf.random.set_seed(op_seed)\n+      else:\n+        if op_seed in self._observed_seeds:\n+          raise ValueError(\n+              \"This `DeterministicTestTool` object is trying to re-use the \" +\n+              \"already-used operation seed {}. \".format(op_seed) +\n+              \"It cannot guarantee random numbers will match between eager \" +\n+              \"and sessions when an operation seed is reused. \" +\n+              \"You most likely set \" +\n+              \"`operation_seed` explicitly but used a value that caused the \" +\n+              \"naturally-incrementing operation seed sequences to overlap \" +\n+              \"with an already-used seed.\")\n+\n+        self._observed_seeds.add(op_seed)\n+        self._operation_seed += 1\n+\n+      return (self._seed, op_seed)\n+    # mock.patch internal symbols to modify the behavior of TF APIs relying on\n+    # them\n+\n+    return tf.compat.v1.test.mock.patch.object(\n+        self.seed_implementation, \"get_seed\", wraps=_get_seed)\n\n@@ -0,0 +1,215 @@\n+\"\"\"Tests for migration_utils.\"\"\"\n+\n+from keras.initializers import GlorotUniform as V2GlorotUniform\n+from keras.legacy_tf_layers import migration_utils\n+import tensorflow as tf\n+\n+\n+class DeterministicRandomTestToolTest(tf.test.TestCase):\n+\n+  def test_constant_mode_no_seed(self):\n+    \"\"\"Test random tensor generation consistancy in constant mode.\n+\n+    Verify that the random tensor generated without using the seed is\n+    consistant between graph and eager mode\n+    \"\"\"\n+\n+    # Generate three random tensors to show how the stateful random number\n+    # generation and glorot_uniform_initializer match between sessions and\n+    # eager execution.\n+    random_tool = migration_utils.DeterministicRandomTestTool()\n+    with random_tool.scope():\n+      graph = tf.Graph()\n+      with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:\n+        a = tf.compat.v1.random.uniform(shape=(3, 1))\n+        # adding additional computation/ops to the graph and ensuring consistant\n+        # random number generation\n+        a = a * 3\n+        b = tf.compat.v1.random.uniform(shape=(3, 3))\n+        b = b * 3\n+        c = tf.compat.v1.random.uniform(shape=(3, 3))\n+        c = c * 3\n+        d = tf.compat.v1.glorot_uniform_initializer()(\n+            shape=(6, 6), dtype=tf.float32)\n+        graph_a, graph_b, graph_c, graph_d = sess.run([a, b, c, d])\n+\n+      a = tf.compat.v2.random.uniform(shape=(3, 1))\n+      a = a * 3\n+      b = tf.compat.v2.random.uniform(shape=(3, 3))\n+      b = b * 3\n+      c = tf.compat.v2.random.uniform(shape=(3, 3))\n+      c = c * 3\n+      d = V2GlorotUniform()(shape=(6, 6), dtype=tf.float32)\n+    # validate that the generated random tensors match\n+    self.assertAllClose(graph_a, a)\n+    self.assertAllClose(graph_b, b)\n+    self.assertAllClose(graph_c, c)\n+    self.assertAllClose(graph_d, d)\n+    # In constant mode, because b and c were generated with the same seed within\n+    # the same scope and have the same shape, they will have exactly the same\n+    # values.\n+    # validate that b and c are the same, also graph_b and graph_c\n+    self.assertAllClose(b, c)\n+    self.assertAllClose(graph_b, graph_c)\n+\n+  def test_constant_mode_seed_argument(self):\n+    \"\"\"Test random tensor generation consistancy in constant mode.\n+\n+    Verify that the random tensor generated by setting the global seeed\n+    in the args is consistant between graph and eager mode.\n+    \"\"\"\n+    random_tool = migration_utils.DeterministicRandomTestTool()\n+    with random_tool.scope():\n+      graph = tf.Graph()\n+      with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:\n+        # adding additional computation/ops to the graph and ensuring consistant\n+        # random number generation\n+        a = tf.compat.v1.random.uniform(shape=(3, 1), seed=1234)\n+        a = a * 3\n+        b = tf.compat.v1.random.uniform(shape=(3, 3), seed=1234)\n+        b = b * 3\n+        c = tf.compat.v1.glorot_uniform_initializer(seed=1234)(\n+            shape=(6, 6), dtype=tf.float32)\n+        graph_a, graph_b, graph_c = sess.run([a, b, c])\n+      a = tf.compat.v2.random.uniform(shape=(3, 1), seed=1234)\n+      a = a * 3\n+      b = tf.compat.v2.random.uniform(shape=(3, 3), seed=1234)\n+      b = b * 3\n+      c = V2GlorotUniform(seed=1234)(shape=(6, 6), dtype=tf.float32)\n+\n+    # validate that the generated random tensors match\n+    self.assertAllClose(graph_a, a)\n+    self.assertAllClose(graph_b, b)\n+    self.assertAllClose(graph_c, c)\n+\n+  def test_num_rand_ops(self):\n+    \"\"\"Test random tensor generation consistancy in num_random_ops mode.\n+\n+    Verify that the random tensor generated without using the seed is\n+    consistant between graph and eager mode.\n+    Random tensor generated should be different based on random ops ordering\n+    \"\"\"\n+    random_tool = migration_utils.DeterministicRandomTestTool(\n+        mode=\"num_random_ops\")\n+    with random_tool.scope():\n+      graph = tf.Graph()\n+      with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:\n+        # adding additional computation/ops to the graph and ensuring consistant\n+        # random number generation\n+        a = tf.compat.v1.random.uniform(shape=(3, 1))\n+        a = a * 3\n+        b = tf.compat.v1.random.uniform(shape=(3, 3))\n+        b = b * 3\n+        c = tf.compat.v1.random.uniform(shape=(3, 3))\n+        c = c * 3\n+        d = tf.compat.v1.glorot_uniform_initializer()(\n+            shape=(6, 6), dtype=tf.float32)\n+        graph_a, graph_b, graph_c, graph_d = sess.run([a, b, c, d])\n+\n+    random_tool = migration_utils.DeterministicRandomTestTool(\n+        mode=\"num_random_ops\")\n+    with random_tool.scope():\n+      a = tf.compat.v2.random.uniform(shape=(3, 1))\n+      a = a * 3\n+      b = tf.compat.v2.random.uniform(shape=(3, 3))\n+      b = b * 3\n+      c = tf.compat.v2.random.uniform(shape=(3, 3))\n+      c = c * 3\n+      d = V2GlorotUniform()(shape=(6, 6), dtype=tf.float32)\n+    # validate that the generated random tensors match\n+    self.assertAllClose(graph_a, a)\n+    self.assertAllClose(graph_b, b)\n+    self.assertAllClose(graph_c, c)\n+    self.assertAllClose(graph_d, d)\n+    # validate that the tensors differ based on ops ordering\n+    self.assertNotAllClose(b, c)\n+    self.assertNotAllClose(graph_b, graph_c)\n+\n+  def test_num_rand_ops_program_order(self):\n+    \"\"\"Test random tensor generation consistancy in num_random_ops mode.\n+\n+    validate that in this mode random number generation is sensitive to program\n+    order, so the generated random tesnors should not match.\n+    \"\"\"\n+    random_tool = migration_utils.DeterministicRandomTestTool(\n+        mode=\"num_random_ops\")\n+    with random_tool.scope():\n+      a = tf.random.uniform(shape=(3, 1))\n+      # adding additional computation/ops to the graph and ensuring consistant\n+      # random number generation\n+      a = a * 3\n+      b = tf.random.uniform(shape=(3, 3))\n+      b = b * 3\n+\n+    random_tool = migration_utils.DeterministicRandomTestTool(\n+        mode=\"num_random_ops\")\n+    with random_tool.scope():\n+      b_prime = tf.random.uniform(shape=(3, 3))\n+      # adding additional computation/ops to the graph and ensuring consistant\n+      # random number generation\n+      b_prime = b_prime * 3\n+      a_prime = tf.random.uniform(shape=(3, 1))\n+      a_prime = a_prime * 3\n+    # validate that the tensors are different\n+    self.assertNotAllClose(a, a_prime)\n+    self.assertNotAllClose(b, b_prime)\n+\n+  def test_num_rand_ops_operation_seed(self):\n+    \"\"\"Test random tensor generation consistancy in num_random_ops mode.\n+\n+    validate if  random number generation match across two different program\n+    orders.\n+    \"\"\"\n+    random_tool = migration_utils.DeterministicRandomTestTool(\n+        mode=\"num_random_ops\")\n+    with random_tool.scope():\n+      # operation seed = 0\n+      a = tf.random.uniform(shape=(3, 1))\n+      a = a * 3\n+      # operation seed = 1\n+      b = tf.random.uniform(shape=(3, 3))\n+      b = b * 3\n+\n+    random_tool = migration_utils.DeterministicRandomTestTool(\n+        mode=\"num_random_ops\")\n+    with random_tool.scope():\n+      random_tool.operation_seed = 1\n+      b_prime = tf.random.uniform(shape=(3, 3))\n+      b_prime = b_prime * 3\n+      random_tool.operation_seed = 0\n+      a_prime = tf.random.uniform(shape=(3, 1))\n+      a_prime = a_prime * 3\n+\n+    self.assertAllClose(a, a_prime)\n+    self.assertAllClose(b, b_prime)\n+\n+  def test_num_rand_ops_disallow_repeated_ops_seed(self):\n+    \"\"\"Test random tensor generation consistancy in num_random_ops mode.\n+\n+    validate if  DeterministicRandomTestTool disallows reusing already-used\n+    operation seeds.\n+    \"\"\"\n+    random_tool = migration_utils.DeterministicRandomTestTool(\n+        mode=\"num_random_ops\")\n+    with random_tool.scope():\n+      random_tool.operation_seed = 1\n+      b_prime = tf.random.uniform(shape=(3, 3))\n+      b_prime = b_prime * 3\n+      random_tool.operation_seed = 0\n+      a_prime = tf.random.uniform(shape=(3, 1))\n+      a_prime = a_prime * 3\n+      error_string = \"An exception should have been raised before this\"\n+      error_raised = \"An exception should have been raised before this\"\n+      try:\n+        c = tf.random.uniform(shape=(3, 1))\n+        raise RuntimeError(error_string)\n+\n+      except ValueError as err:\n+        err_raised = err\n+\n+      self.assertNotEqual(err_raised, error_string)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n+\n\n@@ -20,10 +20,10 @@ from __future__ import print_function\n \n import contextlib\n import functools\n+\n from keras.engine import base_layer\n from keras.utils import layer_utils\n from keras.utils import tf_inspect\n-\n import tensorflow.compat.v2 as tf\n \n from tensorflow.python.ops import variable_scope as vs  # pylint: disable=g-direct-tensorflow-import\n\n@@ -18,14 +18,10 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n-\n import gc\n import threading\n \n from absl.testing import parameterized\n-import numpy\n-from tensorflow.python.framework import test_util\n from keras import combinations\n from keras import regularizers\n from keras.engine import input_layer as input_layer_module\n@@ -33,7 +29,11 @@ from keras.engine import training as training_module\n from keras.layers import core\n from keras.legacy_tf_layers import core as core_layers\n from keras.legacy_tf_layers import variable_scope_shim\n-from tensorflow.python.ops import variable_scope\n+import numpy\n+import tensorflow as tf\n+\n+from tensorflow.python.framework import test_util   # pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.ops import variable_scope   # pylint: disable=g-direct-tensorflow-import\n \n \n def run_inside_wrap_function_in_eager_mode(graph_function):\n@@ -1425,5 +1425,6 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, \"does not extend\"):\n       foo(tf.ones(shape=(4, 4)))\n \n+\n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -14,18 +14,16 @@\n # ==============================================================================\n \"\"\"Tests for Keras utilities to split v1 and v2 classes.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import abc\n \n-import numpy as np\n-\n import keras\n from keras import keras_parameterized\n from keras.engine import base_layer\n from keras.engine import base_layer_v1\n from keras.engine import training\n from keras.engine import training_v1\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @keras_parameterized.run_all_keras_modes\n@@ -179,6 +177,5 @@ class SplitUtilsTest(keras_parameterized.TestCase):\n           ValueError, 'instance was constructed with eager mode enabled'):\n         model.fit(x, y, batch_size=2)\n \n-\n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd8b558b146084db6d2091c224933a6732bb5747", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 321 | Files Changed: 5 | Hunks: 7 | Methods Changed: 11 | Complexity Δ (Sum/Max): -15/0 | Churn Δ: 324 | Churn Cumulative: 3815 | Contributors (this commit): 5 | Commits (past 90d): 14 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,103 +0,0 @@\n-\"\"\"The DetermisticRandomTestTool.\n-\n-(from www.tensorflow.org/guide/migrate/validate_correctness) is a tool used to\n-make random number generation semantics match between TF1.x graphs/sessions and\n-eager execution.\n-\"\"\"\n-\n-import sys\n-\n-import tensorflow.compat.v2 as tf\n-\n-from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n-\n-\n-@keras_export(v1=[\"keras.utils.DeterministicRandomTestTool\"])\n-class DeterministicRandomTestTool(object):\n-  \"\"\"DeterministicRandomTestTool is a testing tool.\n-\n-  This tool is used to validate random number generation semantics match between\n-  TF1.x graphs/sessions and eager execution.\n-\n-  This is useful when you are migrating from TF 1.x to TF2 and need to make sure\n-  your computation is still happening correctly along the way. See the\n-  validating correctness migration guide for more info :\n-  https://www.tensorflow.org/guide/migrate/validate_correctness\n-\n-  The following DeterministicRandomTestTool object provides a context manager\n-  scope() that can make stateful random operations use the same seed across both\n-  TF1 graphs/sessions and eager execution,The tool provides two testing modes:\n-  - constant which uses the same seed for every single operation no matter how\n-  many times it has been called and,\n-  - num_random_ops which uses the number of previously-observed stateful random\n-  operations as the operation seed.\n-  The num_random_ops mode serves as a more sensitive validation check than the\n-  constant mode. It ensures that the random numbers initialization does not get\n-  accidentaly reused.(for example if several weights take on the same\n-  initializations), you can use the num_random_ops mode to avoid this. In the\n-  num_random_ops mode, the generated random numbers will depend on the ordering\n-  of random ops in the program.\n-\n-  This applies both to the stateful random operations used for creating and\n-  initializing variables, and to the stateful random operations used in\n-  computation (such as for dropout layers).\n-  \"\"\"\n-\n-  def __init__(self, seed: int = 42, mode=\"constant\"):\n-    \"\"\"Set mode to 'constant' or 'num_random_ops'. Defaults to 'constant'.\"\"\"\n-    if mode not in {\"constant\", \"num_random_ops\"}:\n-      raise ValueError(\"Mode arg must be 'constant' or 'num_random_ops'. \" +\n-                       \"Got: {}\".format(mode))\n-    self.seed_implementation = sys.modules[tf.compat.v1.get_seed.__module__]\n-    self._mode = mode\n-    self._seed = seed\n-    self.operation_seed = 0\n-    self._observed_seeds = set()\n-\n-  @property\n-  def operation_seed(self):\n-    return self._operation_seed\n-\n-  @operation_seed.setter\n-  def operation_seed(self, value):\n-    self._operation_seed = value\n-\n-  def scope(self):\n-    \"\"\"set random seed.\"\"\"\n-\n-    tf.random.set_seed(self._seed)\n-    def _get_seed(_):\n-      \"\"\"Wraps TF get_seed to make deterministic random generation easier.\n-\n-      This makes a variable's initialization (and calls that involve random\n-      number generation) depend only on how many random number generations\n-      were used in the scope so far, rather than on how many unrelated\n-      operations the graph contains.\n-\n-      Returns:\n-        Random seed tuple.\n-      \"\"\"\n-      op_seed = self._operation_seed\n-      if self._mode == \"constant\":\n-        tf.random.set_seed(op_seed)\n-      else:\n-        if op_seed in self._observed_seeds:\n-          raise ValueError(\n-              \"This `DeterministicTestTool` object is trying to re-use the \" +\n-              \"already-used operation seed {}. \".format(op_seed) +\n-              \"It cannot guarantee random numbers will match between eager \" +\n-              \"and sessions when an operation seed is reused. \" +\n-              \"You most likely set \" +\n-              \"`operation_seed` explicitly but used a value that caused the \" +\n-              \"naturally-incrementing operation seed sequences to overlap \" +\n-              \"with an already-used seed.\")\n-\n-        self._observed_seeds.add(op_seed)\n-        self._operation_seed += 1\n-\n-      return (self._seed, op_seed)\n-    # mock.patch internal symbols to modify the behavior of TF APIs relying on\n-    # them\n-\n-    return tf.compat.v1.test.mock.patch.object(\n-        self.seed_implementation, \"get_seed\", wraps=_get_seed)\n\n@@ -1,215 +0,0 @@\n-\"\"\"Tests for migration_utils.\"\"\"\n-\n-from keras.initializers import GlorotUniform as V2GlorotUniform\n-from keras.legacy_tf_layers import migration_utils\n-import tensorflow as tf\n-\n-\n-class DeterministicRandomTestToolTest(tf.test.TestCase):\n-\n-  def test_constant_mode_no_seed(self):\n-    \"\"\"Test random tensor generation consistancy in constant mode.\n-\n-    Verify that the random tensor generated without using the seed is\n-    consistant between graph and eager mode\n-    \"\"\"\n-\n-    # Generate three random tensors to show how the stateful random number\n-    # generation and glorot_uniform_initializer match between sessions and\n-    # eager execution.\n-    random_tool = migration_utils.DeterministicRandomTestTool()\n-    with random_tool.scope():\n-      graph = tf.Graph()\n-      with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:\n-        a = tf.compat.v1.random.uniform(shape=(3, 1))\n-        # adding additional computation/ops to the graph and ensuring consistant\n-        # random number generation\n-        a = a * 3\n-        b = tf.compat.v1.random.uniform(shape=(3, 3))\n-        b = b * 3\n-        c = tf.compat.v1.random.uniform(shape=(3, 3))\n-        c = c * 3\n-        d = tf.compat.v1.glorot_uniform_initializer()(\n-            shape=(6, 6), dtype=tf.float32)\n-        graph_a, graph_b, graph_c, graph_d = sess.run([a, b, c, d])\n-\n-      a = tf.compat.v2.random.uniform(shape=(3, 1))\n-      a = a * 3\n-      b = tf.compat.v2.random.uniform(shape=(3, 3))\n-      b = b * 3\n-      c = tf.compat.v2.random.uniform(shape=(3, 3))\n-      c = c * 3\n-      d = V2GlorotUniform()(shape=(6, 6), dtype=tf.float32)\n-    # validate that the generated random tensors match\n-    self.assertAllClose(graph_a, a)\n-    self.assertAllClose(graph_b, b)\n-    self.assertAllClose(graph_c, c)\n-    self.assertAllClose(graph_d, d)\n-    # In constant mode, because b and c were generated with the same seed within\n-    # the same scope and have the same shape, they will have exactly the same\n-    # values.\n-    # validate that b and c are the same, also graph_b and graph_c\n-    self.assertAllClose(b, c)\n-    self.assertAllClose(graph_b, graph_c)\n-\n-  def test_constant_mode_seed_argument(self):\n-    \"\"\"Test random tensor generation consistancy in constant mode.\n-\n-    Verify that the random tensor generated by setting the global seeed\n-    in the args is consistant between graph and eager mode.\n-    \"\"\"\n-    random_tool = migration_utils.DeterministicRandomTestTool()\n-    with random_tool.scope():\n-      graph = tf.Graph()\n-      with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:\n-        # adding additional computation/ops to the graph and ensuring consistant\n-        # random number generation\n-        a = tf.compat.v1.random.uniform(shape=(3, 1), seed=1234)\n-        a = a * 3\n-        b = tf.compat.v1.random.uniform(shape=(3, 3), seed=1234)\n-        b = b * 3\n-        c = tf.compat.v1.glorot_uniform_initializer(seed=1234)(\n-            shape=(6, 6), dtype=tf.float32)\n-        graph_a, graph_b, graph_c = sess.run([a, b, c])\n-      a = tf.compat.v2.random.uniform(shape=(3, 1), seed=1234)\n-      a = a * 3\n-      b = tf.compat.v2.random.uniform(shape=(3, 3), seed=1234)\n-      b = b * 3\n-      c = V2GlorotUniform(seed=1234)(shape=(6, 6), dtype=tf.float32)\n-\n-    # validate that the generated random tensors match\n-    self.assertAllClose(graph_a, a)\n-    self.assertAllClose(graph_b, b)\n-    self.assertAllClose(graph_c, c)\n-\n-  def test_num_rand_ops(self):\n-    \"\"\"Test random tensor generation consistancy in num_random_ops mode.\n-\n-    Verify that the random tensor generated without using the seed is\n-    consistant between graph and eager mode.\n-    Random tensor generated should be different based on random ops ordering\n-    \"\"\"\n-    random_tool = migration_utils.DeterministicRandomTestTool(\n-        mode=\"num_random_ops\")\n-    with random_tool.scope():\n-      graph = tf.Graph()\n-      with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:\n-        # adding additional computation/ops to the graph and ensuring consistant\n-        # random number generation\n-        a = tf.compat.v1.random.uniform(shape=(3, 1))\n-        a = a * 3\n-        b = tf.compat.v1.random.uniform(shape=(3, 3))\n-        b = b * 3\n-        c = tf.compat.v1.random.uniform(shape=(3, 3))\n-        c = c * 3\n-        d = tf.compat.v1.glorot_uniform_initializer()(\n-            shape=(6, 6), dtype=tf.float32)\n-        graph_a, graph_b, graph_c, graph_d = sess.run([a, b, c, d])\n-\n-    random_tool = migration_utils.DeterministicRandomTestTool(\n-        mode=\"num_random_ops\")\n-    with random_tool.scope():\n-      a = tf.compat.v2.random.uniform(shape=(3, 1))\n-      a = a * 3\n-      b = tf.compat.v2.random.uniform(shape=(3, 3))\n-      b = b * 3\n-      c = tf.compat.v2.random.uniform(shape=(3, 3))\n-      c = c * 3\n-      d = V2GlorotUniform()(shape=(6, 6), dtype=tf.float32)\n-    # validate that the generated random tensors match\n-    self.assertAllClose(graph_a, a)\n-    self.assertAllClose(graph_b, b)\n-    self.assertAllClose(graph_c, c)\n-    self.assertAllClose(graph_d, d)\n-    # validate that the tensors differ based on ops ordering\n-    self.assertNotAllClose(b, c)\n-    self.assertNotAllClose(graph_b, graph_c)\n-\n-  def test_num_rand_ops_program_order(self):\n-    \"\"\"Test random tensor generation consistancy in num_random_ops mode.\n-\n-    validate that in this mode random number generation is sensitive to program\n-    order, so the generated random tesnors should not match.\n-    \"\"\"\n-    random_tool = migration_utils.DeterministicRandomTestTool(\n-        mode=\"num_random_ops\")\n-    with random_tool.scope():\n-      a = tf.random.uniform(shape=(3, 1))\n-      # adding additional computation/ops to the graph and ensuring consistant\n-      # random number generation\n-      a = a * 3\n-      b = tf.random.uniform(shape=(3, 3))\n-      b = b * 3\n-\n-    random_tool = migration_utils.DeterministicRandomTestTool(\n-        mode=\"num_random_ops\")\n-    with random_tool.scope():\n-      b_prime = tf.random.uniform(shape=(3, 3))\n-      # adding additional computation/ops to the graph and ensuring consistant\n-      # random number generation\n-      b_prime = b_prime * 3\n-      a_prime = tf.random.uniform(shape=(3, 1))\n-      a_prime = a_prime * 3\n-    # validate that the tensors are different\n-    self.assertNotAllClose(a, a_prime)\n-    self.assertNotAllClose(b, b_prime)\n-\n-  def test_num_rand_ops_operation_seed(self):\n-    \"\"\"Test random tensor generation consistancy in num_random_ops mode.\n-\n-    validate if  random number generation match across two different program\n-    orders.\n-    \"\"\"\n-    random_tool = migration_utils.DeterministicRandomTestTool(\n-        mode=\"num_random_ops\")\n-    with random_tool.scope():\n-      # operation seed = 0\n-      a = tf.random.uniform(shape=(3, 1))\n-      a = a * 3\n-      # operation seed = 1\n-      b = tf.random.uniform(shape=(3, 3))\n-      b = b * 3\n-\n-    random_tool = migration_utils.DeterministicRandomTestTool(\n-        mode=\"num_random_ops\")\n-    with random_tool.scope():\n-      random_tool.operation_seed = 1\n-      b_prime = tf.random.uniform(shape=(3, 3))\n-      b_prime = b_prime * 3\n-      random_tool.operation_seed = 0\n-      a_prime = tf.random.uniform(shape=(3, 1))\n-      a_prime = a_prime * 3\n-\n-    self.assertAllClose(a, a_prime)\n-    self.assertAllClose(b, b_prime)\n-\n-  def test_num_rand_ops_disallow_repeated_ops_seed(self):\n-    \"\"\"Test random tensor generation consistancy in num_random_ops mode.\n-\n-    validate if  DeterministicRandomTestTool disallows reusing already-used\n-    operation seeds.\n-    \"\"\"\n-    random_tool = migration_utils.DeterministicRandomTestTool(\n-        mode=\"num_random_ops\")\n-    with random_tool.scope():\n-      random_tool.operation_seed = 1\n-      b_prime = tf.random.uniform(shape=(3, 3))\n-      b_prime = b_prime * 3\n-      random_tool.operation_seed = 0\n-      a_prime = tf.random.uniform(shape=(3, 1))\n-      a_prime = a_prime * 3\n-      error_string = \"An exception should have been raised before this\"\n-      error_raised = \"An exception should have been raised before this\"\n-      try:\n-        c = tf.random.uniform(shape=(3, 1))\n-        raise RuntimeError(error_string)\n-\n-      except ValueError as err:\n-        err_raised = err\n-\n-      self.assertNotEqual(err_raised, error_string)\n-\n-\n-if __name__ == \"__main__\":\n-  tf.test.main()\n-\n\n@@ -20,10 +20,10 @@ from __future__ import print_function\n \n import contextlib\n import functools\n-\n from keras.engine import base_layer\n from keras.utils import layer_utils\n from keras.utils import tf_inspect\n+\n import tensorflow.compat.v2 as tf\n \n from tensorflow.python.ops import variable_scope as vs  # pylint: disable=g-direct-tensorflow-import\n\n@@ -30,7 +30,7 @@ from keras.layers import core\n from keras.legacy_tf_layers import core as core_layers\n from keras.legacy_tf_layers import variable_scope_shim\n import numpy\n-import tensorflow as tf\n+import tensorflow.compat.v2 as tf\n \n from tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.ops import variable_scope  # pylint: disable=g-direct-tensorflow-import\n@@ -1425,6 +1425,5 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, \"does not extend\"):\n       foo(tf.ones(shape=(4, 4)))\n \n-\n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -177,5 +177,6 @@ class SplitUtilsTest(keras_parameterized.TestCase):\n           ValueError, 'instance was constructed with eager mode enabled'):\n         model.fit(x, y, batch_size=2)\n \n+\n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2265d6359422275fc874b8f19b2c55638e24a0f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 41 | Files Changed: 2 | Hunks: 18 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 76 | Churn Cumulative: 12354 | Contributors (this commit): 27 | Commits (past 90d): 8 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3083,7 +3083,9 @@ class _IoUBase(Metric):\n \n @keras_export('keras.metrics.IoU')\n class IoU(_IoUBase):\n-  \"\"\"Computes the Intersection-Over-Union metric for a specific target classes.\n+  \"\"\"Computes the Intersection-Over-Union metric for specific target classes.\n+\n+  General definition and computation:\n \n     Intersection-Over-Union is a common evaluation metric for semantic image\n     segmentation.\n@@ -3101,14 +3103,17 @@ class IoU(_IoUBase):\n     Use `sample_weight` of 0 to mask values.\n \n   Note, this class first computes IoUs for all individual classes, then returns\n-  a 1D tensor of IoUs for the classes that are specified by `target_class_ids`.\n+  the mean of IoUs for the classes that are specified by `target_class_ids`. If\n+  `target_class_ids` has only one id value, the IoU of that specific class is\n+  returned.\n \n   Args:\n     num_classes: The possible number of labels the prediction task can have.\n       A confusion matrix of dimension = [num_classes, num_classes] will be\n       allocated to accumulate predictions from which the metric is calculated.\n     target_class_ids: A tuple or list of target class ids for which the metric\n-      is returned.\n+      is returned. To compute IoU for a specific class, a list (or tuple) of a\n+      single id value should be provided.\n     name: (Optional) string name of the metric instance.\n     dtype: (Optional) data type of the metric result.\n \n@@ -3119,7 +3124,7 @@ class IoU(_IoUBase):\n   >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n   >>> # iou = true_positives / (sum_row + sum_col - true_positives))\n   >>> # iou = [0.33, 0.33]\n-  >>> m = tf.keras.metrics.IoU(num_classes=2, target_class_id=0)\n+  >>> m = tf.keras.metrics.IoU(num_classes=2, target_class_id=[0])\n   >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n   >>> m.result().numpy()\n   0.33333334\n@@ -3140,7 +3145,7 @@ class IoU(_IoUBase):\n   model.compile(\n     optimizer='sgd',\n     loss='mse',\n-    metrics=[tf.keras.metrics.IoU(num_classes=2, target_class_id=0)])\n+    metrics=[tf.keras.metrics.IoU(num_classes=2, target_class_id=[0])])\n   ```\n   \"\"\"\n \n@@ -3175,9 +3180,18 @@ class IoU(_IoUBase):\n     #     2 * true_positives + false_positives + false_negatives.\n     denominator = sum_over_row + sum_over_col - true_positives\n \n+    # Only keep the target classes\n+    true_positives = tf.gather(true_positives, self.target_class_ids)\n+    denominator = tf.gather(denominator, self.target_class_ids)\n+\n+    # If the denominator is 0, we need to ignore the class.\n+    num_valid_entries = tf.reduce_sum(\n+        tf.cast(tf.not_equal(denominator, 0), dtype=self._dtype))\n+\n     iou = tf.math.divide_no_nan(true_positives, denominator)\n \n-    return tf.gather(iou, self.target_class_ids, name='iou')\n+    return tf.math.divide_no_nan(\n+        tf.reduce_sum(iou, name='mean_iou'), num_valid_entries)\n \n   def get_config(self):\n     config = {\n@@ -3189,9 +3203,11 @@ class IoU(_IoUBase):\n \n \n @keras_export('keras.metrics.MeanIoU')\n-class MeanIoU(_IoUBase):\n+class MeanIoU(IoU):\n   \"\"\"Computes the mean Intersection-Over-Union metric.\n \n+  General definition and computation:\n+\n     Intersection-Over-Union is a common evaluation metric for semantic image\n     segmentation.\n \n@@ -3246,40 +3262,20 @@ class MeanIoU(_IoUBase):\n   \"\"\"\n \n   def __init__(self, num_classes, name=None, dtype=None):\n+    target_class_ids = list(range(num_classes))\n     super(MeanIoU, self).__init__(\n         name=name,\n         num_classes=num_classes,\n+        target_class_ids=target_class_ids,\n         dtype=dtype,\n     )\n \n-  def result(self):\n-    \"\"\"Compute the mean intersection-over-union via the confusion matrix.\"\"\"\n-    sum_over_row = tf.cast(\n-        tf.reduce_sum(self.total_cm, axis=0), dtype=self._dtype)\n-    sum_over_col = tf.cast(\n-        tf.reduce_sum(self.total_cm, axis=1), dtype=self._dtype)\n-    true_positives = tf.cast(\n-        tf.linalg.tensor_diag_part(self.total_cm), dtype=self._dtype)\n-\n-    # sum_over_row + sum_over_col =\n-    #     2 * true_positives + false_positives + false_negatives.\n-    denominator = sum_over_row + sum_over_col - true_positives\n-\n-    # The mean is only computed over classes that appear in the\n-    # label or prediction tensor. If the denominator is 0, we need to\n-    # ignore the class.\n-    num_valid_entries = tf.reduce_sum(\n-        tf.cast(tf.not_equal(denominator, 0), dtype=self._dtype))\n-\n-    iou = tf.math.divide_no_nan(true_positives, denominator)\n-\n-    return tf.math.divide_no_nan(\n-        tf.reduce_sum(iou, name='mean_iou'), num_valid_entries)\n-\n   def get_config(self):\n-    config = {'num_classes': self.num_classes}\n-    base_config = super(MeanIoU, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n+    return {\n+        'num_classes': self.num_classes,\n+        'name': self.name,\n+        'dtype': self._dtype,\n+    }\n \n \n @keras_export('keras.metrics.MeanTensor')\n\n@@ -1311,7 +1311,7 @@ class IoUTest(tf.test.TestCase):\n     #       [1, 1]]\n     # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n     # iou = true_positives / (sum_row + sum_col - true_positives))\n-    expected_result = tf.constant([1 / (2 + 2 - 1), 1 / (2 + 2 - 1)])\n+    expected_result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n   def test_weighted(self):\n@@ -1328,8 +1328,7 @@ class IoUTest(tf.test.TestCase):\n     #       [0.4, 0.1]]\n     # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n     # iou = true_positives / (sum_row + sum_col - true_positives))\n-    expected_result = tf.constant(\n-        [0.1 / (0.4 + 0.5 - 0.1), 0.2 / (0.6 + 0.5 - 0.2)])\n+    expected_result = (0.1 / (0.4 + 0.5 - 0.1) + 0.2 / (0.6 + 0.5 - 0.2)) / 2\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n   def test_multi_dim_input(self):\n@@ -1346,15 +1345,14 @@ class IoUTest(tf.test.TestCase):\n     #       [0.4, 0.1]]\n     # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n     # iou = true_positives / (sum_row + sum_col - true_positives))\n-    expected_result = tf.constant(\n-        [0.2 / (0.6 + 0.5 - 0.2), 0.1 / (0.4 + 0.5 - 0.1)])\n+    expected_result = (0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)) / 2\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n   def test_zero_valid_entries(self):\n     obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n     self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n     self.assertAllClose(\n-        self.evaluate(obj.result()), tf.constant([0, 0]), atol=1e-3)\n+        self.evaluate(obj.result()), 0, atol=1e-3)\n \n   def test_zero_and_non_zero_entries(self):\n     y_pred = tf.constant([1], dtype=tf.float32)\n@@ -1368,7 +1366,7 @@ class IoUTest(tf.test.TestCase):\n     #       [0, 1]]\n     # sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]\n     # iou = true_positives / (sum_row + sum_col - true_positives))\n-    expected_result = tf.constant([0, 1 / (1 + 1 - 1)])\n+    expected_result = (1 / (1 + 1 - 1)) / 1\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
