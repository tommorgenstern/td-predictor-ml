{"custom_id": "keras#f2d397b5f628908a5a21656f61d6c4342b9717a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 29 | Files Changed: 6 | Hunks: 20 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 53 | Churn Cumulative: 1943 | Contributors (this commit): 9 | Commits (past 90d): 44 | Contributors (cumulative): 26 | DMM Complexity: 0.2222222222222222\n\nDIFF:\n@@ -123,18 +123,17 @@ class RandomBrightness(TFDataLayer):\n             return inputs\n \n     def _randomly_adjust_brightness(self, images):\n-        images_shape = self.backend.shape(images)\n-        rank = len(images_shape)\n+        rank = len(images.shape)\n         if rank == 3:\n             rgb_delta_shape = (1, 1, 1)\n         elif rank == 4:\n             # Keep only the batch dim. This will ensure to have same adjustment\n             # with in one image, but different across the images.\n-            rgb_delta_shape = [images_shape[0], 1, 1, 1]\n+            rgb_delta_shape = [self.backend.shape(images)[0], 1, 1, 1]\n         else:\n             raise ValueError(\n                 \"Expected the input image to be rank 3 or 4. Received \"\n-                f\"inputs.shape={images_shape}\"\n+                f\"inputs.shape={images.shape}\"\n             )\n \n         seed_generator = self._get_seed_generator(self.backend._backend)\n\n@@ -48,13 +48,10 @@ class RandomFlip(TFDataLayer):\n         self._allow_non_tensor_positional_args = True\n \n     def _randomly_flip_inputs(self, inputs):\n-        inputs_shape = self.backend.shape(inputs)\n-        unbatched = len(inputs_shape) == 3\n+        unbatched = len(inputs.shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n-            inputs_shape = self.backend.shape(inputs)\n-\n-        batch_size = inputs_shape[0]\n+        batch_size = self.backend.shape(inputs)[0]\n         flipped_outputs = inputs\n         seed_generator = self._get_seed_generator(self.backend._backend)\n         if self.mode == HORIZONTAL or self.mode == HORIZONTAL_AND_VERTICAL:\n\n@@ -178,8 +178,6 @@ class RandomRotation(TFDataLayer):\n             else:\n                 image_height = shape[1]\n                 image_width = shape[2]\n-        image_height = float(image_height)\n-        image_width = float(image_width)\n \n         lower = self._factor[0] * 2.0 * self.backend.convert_to_tensor(np.pi)\n         upper = self._factor[1] * 2.0 * self.backend.convert_to_tensor(np.pi)\n\n@@ -164,19 +164,17 @@ class RandomTranslation(TFDataLayer):\n             return inputs\n \n     def _randomly_translate_inputs(self, inputs):\n-        inputs_shape = self.backend.shape(inputs)\n-        unbatched = len(inputs_shape) == 3\n+        unbatched = len(inputs.shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n-            inputs_shape = self.backend.shape(inputs)\n \n-        batch_size = inputs_shape[0]\n+        batch_size = self.backend.shape(inputs)[0]\n         if self.data_format == \"channels_first\":\n-            height = inputs_shape[-2]\n-            width = inputs_shape[-1]\n+            height = inputs.shape[-2]\n+            width = inputs.shape[-1]\n         else:\n-            height = inputs_shape[-3]\n-            width = inputs_shape[-2]\n+            height = inputs.shape[-3]\n+            width = inputs.shape[-2]\n \n         seed_generator = self._get_seed_generator(self.backend._backend)\n         height_translate = self.backend.random.uniform(\n\n@@ -171,19 +171,17 @@ class RandomZoom(TFDataLayer):\n             return inputs\n \n     def _randomly_zoom_inputs(self, inputs):\n-        inputs_shape = self.backend.shape(inputs)\n-        unbatched = len(inputs_shape) == 3\n+        unbatched = len(inputs.shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n-            inputs_shape = self.backend.shape(inputs)\n \n-        batch_size = inputs_shape[0]\n+        batch_size = self.backend.shape(inputs)[0]\n         if self.data_format == \"channels_first\":\n-            height = inputs_shape[-2]\n-            width = inputs_shape[-1]\n+            height = inputs.shape[-2]\n+            width = inputs.shape[-1]\n         else:\n-            height = inputs_shape[-3]\n-            width = inputs_shape[-2]\n+            height = inputs.shape[-3]\n+            width = inputs.shape[-2]\n \n         seed_generator = self._get_seed_generator(self.backend._backend)\n         height_zoom = self.backend.random.uniform(\n@@ -227,8 +225,8 @@ class RandomZoom(TFDataLayer):\n         #      [0 0 1]]\n         # where the last entry is implicit.\n         # zoom matrices are always float32.\n-        x_offset = ((float(image_width) - 1.0) / 2.0) * (1.0 - zooms[:, 0:1])\n-        y_offset = ((float(image_height) - 1.0) / 2.0) * (1.0 - zooms[:, 1:])\n+        x_offset = ((image_width - 1.0) / 2.0) * (1.0 - zooms[:, 0:1])\n+        y_offset = ((image_height - 1.0) / 2.0) * (1.0 - zooms[:, 1:])\n         return self.backend.numpy.concatenate(\n             [\n                 zooms[:, 0:1],\n\n@@ -1,6 +1,8 @@\n import numpy as np\n+import pytest\n import torch\n \n+from keras_core import backend\n from keras_core import models\n from keras_core import testing\n from keras_core.utils.torch_utils import TorchModuleWrapper\n@@ -15,6 +17,9 @@ class Classifier(models.Model):\n         return self.fc(x)\n \n \n+@pytest.mark.skipif(\n+    backend.backend() != \"torch\", reason=\"Requires torch backend\"\n+)\n class TorchUtilsTest(testing.TestCase):\n     def test_basic_usage(self):\n         model = Classifier()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e898f029a6ae86f64e001a2d10dbace52c29fc46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 19 | Files Changed: 5 | Hunks: 17 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 1961 | Contributors (this commit): 9 | Commits (past 90d): 47 | Contributors (cumulative): 25 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -123,17 +123,18 @@ class RandomBrightness(TFDataLayer):\n             return inputs\n \n     def _randomly_adjust_brightness(self, images):\n-        rank = len(images.shape)\n+        images_shape = self.backend.shape(images)\n+        rank = len(images_shape)\n         if rank == 3:\n             rgb_delta_shape = (1, 1, 1)\n         elif rank == 4:\n             # Keep only the batch dim. This will ensure to have same adjustment\n             # with in one image, but different across the images.\n-            rgb_delta_shape = [self.backend.shape(images)[0], 1, 1, 1]\n+            rgb_delta_shape = [images_shape[0], 1, 1, 1]\n         else:\n             raise ValueError(\n                 \"Expected the input image to be rank 3 or 4. Received \"\n-                f\"inputs.shape={images.shape}\"\n+                f\"inputs.shape={images_shape}\"\n             )\n \n         seed_generator = self._get_seed_generator(self.backend._backend)\n\n@@ -48,10 +48,13 @@ class RandomFlip(TFDataLayer):\n         self._allow_non_tensor_positional_args = True\n \n     def _randomly_flip_inputs(self, inputs):\n-        unbatched = len(inputs.shape) == 3\n+        inputs_shape = self.backend.shape(inputs)\n+        unbatched = len(inputs_shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n-        batch_size = self.backend.shape(inputs)[0]\n+            inputs_shape = self.backend.shape(inputs)\n+\n+        batch_size = inputs_shape[0]\n         flipped_outputs = inputs\n         seed_generator = self._get_seed_generator(self.backend._backend)\n         if self.mode == HORIZONTAL or self.mode == HORIZONTAL_AND_VERTICAL:\n\n@@ -178,6 +178,8 @@ class RandomRotation(TFDataLayer):\n             else:\n                 image_height = shape[1]\n                 image_width = shape[2]\n+        image_height = float(image_height)\n+        image_width = float(image_width)\n \n         lower = self._factor[0] * 2.0 * self.backend.convert_to_tensor(np.pi)\n         upper = self._factor[1] * 2.0 * self.backend.convert_to_tensor(np.pi)\n\n@@ -164,17 +164,19 @@ class RandomTranslation(TFDataLayer):\n             return inputs\n \n     def _randomly_translate_inputs(self, inputs):\n-        unbatched = len(inputs.shape) == 3\n+        inputs_shape = self.backend.shape(inputs)\n+        unbatched = len(inputs_shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n+            inputs_shape = self.backend.shape(inputs)\n \n-        batch_size = self.backend.shape(inputs)[0]\n+        batch_size = inputs_shape[0]\n         if self.data_format == \"channels_first\":\n-            height = inputs.shape[-2]\n-            width = inputs.shape[-1]\n+            height = inputs_shape[-2]\n+            width = inputs_shape[-1]\n         else:\n-            height = inputs.shape[-3]\n-            width = inputs.shape[-2]\n+            height = inputs_shape[-3]\n+            width = inputs_shape[-2]\n \n         seed_generator = self._get_seed_generator(self.backend._backend)\n         height_translate = self.backend.random.uniform(\n\n@@ -171,17 +171,19 @@ class RandomZoom(TFDataLayer):\n             return inputs\n \n     def _randomly_zoom_inputs(self, inputs):\n-        unbatched = len(inputs.shape) == 3\n+        inputs_shape = self.backend.shape(inputs)\n+        unbatched = len(inputs_shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n+            inputs_shape = self.backend.shape(inputs)\n \n-        batch_size = self.backend.shape(inputs)[0]\n+        batch_size = inputs_shape[0]\n         if self.data_format == \"channels_first\":\n-            height = inputs.shape[-2]\n-            width = inputs.shape[-1]\n+            height = inputs_shape[-2]\n+            width = inputs_shape[-1]\n         else:\n-            height = inputs.shape[-3]\n-            width = inputs.shape[-2]\n+            height = inputs_shape[-3]\n+            width = inputs_shape[-2]\n \n         seed_generator = self._get_seed_generator(self.backend._backend)\n         height_zoom = self.backend.random.uniform(\n@@ -225,8 +227,8 @@ class RandomZoom(TFDataLayer):\n         #      [0 0 1]]\n         # where the last entry is implicit.\n         # zoom matrices are always float32.\n-        x_offset = ((image_width - 1.0) / 2.0) * (1.0 - zooms[:, 0:1])\n-        y_offset = ((image_height - 1.0) / 2.0) * (1.0 - zooms[:, 1:])\n+        x_offset = ((float(image_width) - 1.0) / 2.0) * (1.0 - zooms[:, 0:1])\n+        y_offset = ((float(image_height) - 1.0) / 2.0) * (1.0 - zooms[:, 1:])\n         return self.backend.numpy.concatenate(\n             [\n                 zooms[:, 0:1],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00e280a69da05762d71ba0d2c31a8034310ea05e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 16 | Churn Cumulative: 2598 | Contributors (this commit): 14 | Commits (past 90d): 46 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -161,6 +161,12 @@ class Functional(Function, Model):\n         output_layers = [x._keras_history[0] for x in self.outputs]\n         self.output_names = [x.name for x in output_layers]\n \n+    def _lock_state(self):\n+        # Unlike other layers, we allow Functional state to be mutable after\n+        # build. E.g. to attach a layer to a model that is not part of the\n+        # functional DAG.\n+        pass\n+\n     @property\n     def layers(self):\n         layers = []\n\n@@ -51,6 +51,16 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(in_val)\n         self.assertAllClose(out_val, np.ones((2, 3)))\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_mutable_state(self):\n+        inputs = Input(shape=(3,), batch_size=2, name=\"input\")\n+        x = layers.Dense(5)(inputs)\n+        outputs = layers.Dense(5)(x)\n+        model = Functional(inputs, outputs)\n+        # Allow attaching state to a model that isn't directly part of the DAG.\n+        # Most useful for functional subclasses.\n+        model.extra_layer = layers.Dense(5)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_basic_flow_multi_output(self):\n         inputs = Input(shape=(3,), batch_size=2, name=\"input\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91354a434f94582dd3a2a512a54560bc62db2a86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 359 | Lines Deleted: 184 | Files Changed: 5 | Hunks: 29 | Methods Changed: 16 | Complexity Δ (Sum/Max): 19/13 | Churn Δ: 543 | Churn Cumulative: 5505 | Contributors (this commit): 15 | Commits (past 90d): 40 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,35 +1,209 @@\n-def compute_conv_transpose_output_length(\n-    input_length,\n-    kernel_size,\n-    padding,\n-    output_padding=None,\n-    stride=1,\n-    dilation=1,\n+import warnings\n+\n+\n+def _convert_conv_tranpose_padding_args_from_keras_to_jax(\n+    kernel_size, stride, dilation_rate, padding, output_padding\n ):\n-    \"\"\"Computes output size of a transposed convolution given input size.\"\"\"\n-    assert padding in {\"same\", \"valid\"}\n-    if input_length is None:\n-        return None\n+    \"\"\"Convert the padding arguments from Keras to the ones used by JAX.\n+    JAX starts with an shape of size `(input-1) * stride - kernel_size + 2`,\n+    then adds `left_pad` on the left, and `right_pad` on the right.\n+    In Keras, the `padding` argument determines a base shape, to which\n+    `output_padding` is added on the right. If `output_padding` is None, it will\n+    be given a default value.\n+    \"\"\"\n \n-    # Get the dilated kernel size\n-    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n+    assert padding.lower() in {\"valid\", \"same\"}\n+    kernel_size = (kernel_size - 1) * dilation_rate + 1\n \n-    # Infer length if output padding is None, else compute the exact length\n-    if output_padding is None:\n-        if padding == \"valid\":\n-            length = input_length * stride + max(kernel_size - stride, 0)\n-        else:\n-            length = input_length * stride\n-    else:\n-        if padding == \"same\":\n-            pad = kernel_size // 2\n-        else:\n-            pad = 0\n-\n-        length = (\n-            (input_length - 1) * stride + kernel_size - 2 * pad + output_padding\n+    if padding.lower() == \"valid\":\n+        # If output_padding is None, we fill it so that the shape of the ouput\n+        # is `(input-1)*s + max(kernel_size, stride)`\n+        output_padding = (\n+            max(kernel_size, stride) - kernel_size\n+            if output_padding is None\n+            else output_padding\n         )\n-    return length\n+        left_pad = kernel_size - 1\n+        right_pad = kernel_size - 1 + output_padding\n+\n+    else:\n+        if output_padding is None:\n+            # When output_padding is None, we want the shape of the ouput to\n+            # be `input * s`, therefore a total padding of\n+            # `stride + kernel_size - 2`\n+            pad_len = stride + kernel_size - 2\n+        else:\n+            # When output_padding is filled, we want the shape of the ouput to\n+            # be `(input-1)*stride + kernel_size%2 + output_padding`\n+            pad_len = kernel_size + kernel_size % 2 - 2 + output_padding\n+        left_pad = min(pad_len // 2 + pad_len % 2, kernel_size - 1)\n+        right_pad = pad_len - left_pad\n+\n+    return left_pad, right_pad\n+\n+\n+def _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+    kernel_size, stride, dilation_rate, padding, output_padding\n+):\n+    \"\"\"Convert the padding arguments from Keras to the ones used by Torch.\n+    Torch starts with an output shape of `(input-1) * stride + kernel_size`,\n+    then removes `torch_padding` from both sides, and adds\n+    `torch_output_padding` on the right.\n+    Because in Torch the output_padding can only be added to the right,\n+    consistency with Tensorflow is not always possible. In particular this is\n+    the case when both the Torch padding and output_padding values are stricly\n+    positive.\n+    \"\"\"\n+    assert padding.lower() in {\"valid\", \"same\"}\n+    original_kernel_size = kernel_size\n+    kernel_size = (kernel_size - 1) * dilation_rate + 1\n+\n+    if padding.lower() == \"valid\":\n+        # If output_padding is None, we fill it so that the shape of the ouput\n+        # is `(i-1)*s + max(k, s)`\n+        output_padding = (\n+            max(kernel_size, stride) - kernel_size\n+            if output_padding is None\n+            else output_padding\n+        )\n+        torch_padding = 0\n+        torch_output_padding = output_padding\n+\n+    else:\n+        # When output_padding is None, we want the shape of the ouput to be\n+        # `input * s`, otherwise we use the value provided.\n+        output_padding = (\n+            stride - kernel_size % 2\n+            if output_padding is None\n+            else output_padding\n+        )\n+        torch_padding = max(\n+            -((kernel_size % 2 - kernel_size + output_padding) // 2), 0\n+        )\n+        torch_output_padding = (\n+            2 * torch_padding + kernel_size % 2 - kernel_size + output_padding\n+        )\n+\n+    if torch_padding > 0 and torch_output_padding > 0:\n+        warnings.warn(\n+            f\"You might experience inconsistencies accross backends when \"\n+            f\"calling conv transpose with kernel_size={original_kernel_size}, \"\n+            f\"stride={stride}, dilation_rate={dilation_rate}, \"\n+            f\"padding={padding}, output_padding={output_padding}.\"\n+        )\n+\n+    if torch_output_padding >= stride:\n+        raise ValueError(\n+            f\"The padding arguments (padding={padding}) and \"\n+            f\"output_padding={output_padding}) lead to a Torch \"\n+            f\"output_padding ({torch_output_padding}) that is greater than \"\n+            f\"strides ({stride}). This is not supported. You can change the \"\n+            f\"padding arguments, kernel or stride, or run on another backend. \"\n+        )\n+\n+    return torch_padding, torch_output_padding\n+\n+\n+def compute_conv_transpose_padding_args_for_jax(\n+    input_shape,\n+    kernel_shape,\n+    strides,\n+    padding,\n+    output_padding,\n+    dilation_rate,\n+):\n+    num_spatial_dims = len(input_shape) - 2\n+    kernel_spatial_shape = kernel_shape[:-2]\n+\n+    jax_padding = []\n+    for i in range(num_spatial_dims):\n+        output_padding_i = (\n+            output_padding\n+            if output_padding is None or isinstance(output_padding, int)\n+            else output_padding[i]\n+        )\n+        strides_i = strides if isinstance(strides, int) else strides[i]\n+        dilation_rate_i = (\n+            dilation_rate\n+            if isinstance(dilation_rate, int)\n+            else dilation_rate[i]\n+        )\n+        (\n+            pad_left,\n+            pad_right,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_jax(\n+            kernel_size=kernel_spatial_shape[i],\n+            stride=strides_i,\n+            dilation_rate=dilation_rate_i,\n+            padding=padding,\n+            output_padding=output_padding_i,\n+        )\n+        jax_padding.append((pad_left, pad_right))\n+\n+    return jax_padding\n+\n+\n+def compute_conv_transpose_padding_args_for_torch(\n+    input_shape,\n+    kernel_shape,\n+    strides,\n+    padding,\n+    output_padding,\n+    dilation_rate,\n+):\n+    num_spatial_dims = len(input_shape) - 2\n+    kernel_spatial_shape = kernel_shape[:-2]\n+\n+    torch_paddings = []\n+    torch_output_paddings = []\n+    for i in range(num_spatial_dims):\n+        output_padding_i = (\n+            output_padding\n+            if output_padding is None or isinstance(output_padding, int)\n+            else output_padding[i]\n+        )\n+        strides_i = strides if isinstance(strides, int) else strides[i]\n+        dilation_rate_i = (\n+            dilation_rate\n+            if isinstance(dilation_rate, int)\n+            else dilation_rate[i]\n+        )\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=kernel_spatial_shape[i],\n+            stride=strides_i,\n+            dilation_rate=dilation_rate_i,\n+            padding=padding,\n+            output_padding=output_padding_i,\n+        )\n+        torch_paddings.append(torch_padding)\n+        torch_output_paddings.append(torch_output_padding)\n+\n+    return torch_paddings, torch_output_paddings\n+\n+\n+def _get_output_shape_given_tf_padding(\n+    input_size, kernel_size, strides, padding, output_padding, dilation_rate\n+):\n+    assert padding.lower() in {\"valid\", \"same\"}\n+\n+    kernel_size = (kernel_size - 1) * dilation_rate + 1\n+\n+    if padding.lower() == \"valid\":\n+        output_padding = (\n+            max(kernel_size, strides) - kernel_size\n+            if output_padding is None\n+            else output_padding\n+        )\n+        return (input_size - 1) * strides + kernel_size + output_padding\n+\n+    else:\n+        if output_padding is None:\n+            return input_size * strides\n+        else:\n+            return (input_size - 1) * strides + kernel_size % 2 + output_padding\n \n \n def compute_conv_transpose_output_shape(\n@@ -62,94 +236,19 @@ def compute_conv_transpose_output_shape(\n         current_output_padding = (\n             None if output_padding is None else output_padding[i]\n         )\n-        output_shape.append(\n-            compute_conv_transpose_output_length(\n-                input_spatial_shape[i],\n-                kernel_spatial_shape[i],\n+\n+        shape_i = _get_output_shape_given_tf_padding(\n+            input_size=input_spatial_shape[i],\n+            kernel_size=kernel_spatial_shape[i],\n+            strides=strides[i],\n             padding=padding,\n             output_padding=current_output_padding,\n-                stride=strides[i],\n-                dilation=dilation_rate[i],\n-            )\n+            dilation_rate=dilation_rate[i],\n         )\n+        output_shape.append(shape_i)\n \n     if data_format == \"channels_last\":\n         output_shape = [input_shape[0]] + output_shape + [filters]\n     else:\n         output_shape = [input_shape[0], filters] + output_shape\n     return output_shape\n-\n-\n-def _compute_conv_transpose_padding_one_dim(\n-    input_length,\n-    output_length,\n-    kernel_size,\n-    stride,\n-    padding,\n-    dilation_rate,\n-):\n-    \"\"\"Computes adjusted padding for `conv_transpose` in one dim.\"\"\"\n-    kernel_size = (kernel_size - 1) * dilation_rate + 1\n-    if padding == \"valid\":\n-        padding_before = 0\n-    else:\n-        # padding == \"same\".\n-        padding_needed = max(\n-            0, (input_length - 1) * stride + kernel_size - output_length\n-        )\n-        padding_before = padding_needed // 2\n-\n-    expanded_input_length = (input_length - 1) * stride + 1\n-    padded_out_length = output_length + kernel_size - 1\n-    pad_before = kernel_size - 1 - padding_before\n-    pad_after = padded_out_length - expanded_input_length - pad_before\n-    return (pad_before, pad_after)\n-\n-\n-def compute_conv_transpose_padding(\n-    input_shape,\n-    kernel_shape,\n-    strides=1,\n-    padding=\"valid\",\n-    output_padding=None,\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    \"\"\"Computes adjusted padding for `conv_transpose`.\"\"\"\n-    num_spatial_dims = len(input_shape) - 2\n-    if isinstance(output_padding, int):\n-        output_padding = (output_padding,) * num_spatial_dims\n-    if isinstance(strides, int):\n-        strides = (strides,) * num_spatial_dims\n-    if isinstance(dilation_rate, int):\n-        dilation_rate = (dilation_rate,) * num_spatial_dims\n-\n-    kernel_spatial_shape = kernel_shape[:-2]\n-    if data_format == \"channels_last\":\n-        input_spatial_shape = input_shape[1:-1]\n-    else:\n-        input_spatial_shape = input_shape[2:]\n-    padding_values = []\n-    for i in range(num_spatial_dims):\n-        input_length = input_spatial_shape[i]\n-        current_output_padding = (\n-            None if output_padding is None else output_padding[i]\n-        )\n-        output_length = compute_conv_transpose_output_length(\n-            input_spatial_shape[i],\n-            kernel_spatial_shape[i],\n-            padding=padding,\n-            output_padding=current_output_padding,\n-            stride=strides[i],\n-            dilation=dilation_rate[i],\n-        )\n-        padding_value = _compute_conv_transpose_padding_one_dim(\n-            input_length,\n-            output_length,\n-            kernel_spatial_shape[i],\n-            strides[i],\n-            padding=padding,\n-            dilation_rate=dilation_rate[i],\n-        )\n-        padding_values.append(padding_value)\n-    return padding_values\n\n@@ -6,7 +6,7 @@ from jax import nn as jnn\n \n from keras_core.backend import standardize_data_format\n from keras_core.backend.common.backend_utils import (\n-    compute_conv_transpose_padding,\n+    compute_conv_transpose_padding_args_for_jax,\n )\n from keras_core.backend.config import epsilon\n from keras_core.backend.jax.core import cast\n@@ -361,14 +361,13 @@ def conv_transpose(\n ):\n     data_format = standardize_data_format(data_format)\n     num_spatial_dims = inputs.ndim - 2\n-    padding_values = compute_conv_transpose_padding(\n-        inputs.shape,\n-        kernel.shape,\n-        strides,\n-        padding,\n-        output_padding,\n-        data_format,\n-        dilation_rate,\n+    padding_values = compute_conv_transpose_padding_args_for_jax(\n+        input_shape=inputs.shape,\n+        kernel_shape=kernel.shape,\n+        strides=strides,\n+        padding=padding,\n+        output_padding=output_padding,\n+        dilation_rate=dilation_rate,\n     )\n     dimension_numbers = _convert_to_lax_conv_dimension_numbers(\n         num_spatial_dims,\n\n@@ -5,7 +5,7 @@ from jax import numpy as jnp\n \n from keras_core.backend import standardize_data_format\n from keras_core.backend.common.backend_utils import (\n-    compute_conv_transpose_padding,\n+    compute_conv_transpose_padding_args_for_jax,\n )\n from keras_core.backend.config import epsilon\n from keras_core.backend.numpy.core import cast\n@@ -372,14 +372,13 @@ def conv_transpose(\n ):\n     data_format = standardize_data_format(data_format)\n     num_spatial_dims = inputs.ndim - 2\n-    padding_values = compute_conv_transpose_padding(\n-        inputs.shape,\n-        kernel.shape,\n-        strides,\n-        padding,\n-        output_padding,\n-        data_format,\n-        dilation_rate,\n+    padding_values = compute_conv_transpose_padding_args_for_jax(\n+        input_shape=inputs.shape,\n+        kernel_shape=kernel.shape,\n+        strides=strides,\n+        padding=padding,\n+        output_padding=output_padding,\n+        dilation_rate=dilation_rate,\n     )\n     dimension_numbers = _convert_to_lax_conv_dimension_numbers(\n         num_spatial_dims,\n\n@@ -4,7 +4,7 @@ import torch.nn.functional as tnn\n \n from keras_core.backend import standardize_data_format\n from keras_core.backend.common.backend_utils import (\n-    compute_conv_transpose_padding,\n+    compute_conv_transpose_padding_args_for_torch,\n )\n from keras_core.backend.config import epsilon\n from keras_core.backend.torch.core import cast\n@@ -446,40 +446,32 @@ def conv_transpose(\n     strides = standardize_tuple(strides, num_spatial_dims, \"strides\")\n \n     data_format = standardize_data_format(data_format)\n-    padding_values = compute_conv_transpose_padding(\n-        inputs.shape,\n-        kernel.shape,\n-        strides,\n-        padding,\n-        output_padding,\n-        data_format,\n-        dilation_rate,\n+    (\n+        torch_padding,\n+        torch_output_padding,\n+    ) = compute_conv_transpose_padding_args_for_torch(\n+        input_shape=inputs.shape,\n+        kernel_shape=kernel.shape,\n+        strides=strides,\n+        padding=padding,\n+        output_padding=output_padding,\n+        dilation_rate=dilation_rate,\n     )\n     if data_format == \"channels_last\":\n         inputs = _transpose_spatial_inputs(inputs)\n     # Transpose kernel from keras format to torch format.\n     kernel = _transpose_conv_kernel(kernel)\n     kernel_spatial_shape = kernel.shape[2:]\n-    padding_arg = []\n-    output_padding_arg = []\n     if isinstance(dilation_rate, int):\n         dilation_rate = [dilation_rate] * len(kernel_spatial_shape)\n-    for i, value in enumerate(padding_values):\n-        both_side_padding = value[0]\n-        longer_side_padding = value[1] - value[0]\n-        padding_arg.append(\n-            dilation_rate[i] * (kernel_spatial_shape[i] - 1)\n-            - both_side_padding,\n-        )\n-        output_padding_arg.append(longer_side_padding)\n \n     if num_spatial_dims == 1:\n         outputs = tnn.conv_transpose1d(\n             inputs,\n             kernel,\n             stride=strides,\n-            padding=padding_arg,\n-            output_padding=output_padding_arg,\n+            padding=torch_padding,\n+            output_padding=torch_output_padding,\n             dilation=dilation_rate,\n         )\n     elif num_spatial_dims == 2:\n@@ -487,8 +479,8 @@ def conv_transpose(\n             inputs,\n             kernel,\n             stride=strides,\n-            padding=padding_arg,\n-            output_padding=output_padding_arg,\n+            padding=torch_padding,\n+            output_padding=torch_output_padding,\n             dilation=dilation_rate,\n         )\n     elif num_spatial_dims == 3:\n@@ -496,8 +488,8 @@ def conv_transpose(\n             inputs,\n             kernel,\n             stride=strides,\n-            padding=padding_arg,\n-            output_padding=output_padding_arg,\n+            padding=torch_padding,\n+            output_padding=torch_output_padding,\n             dilation=dilation_rate,\n         )\n     else:\n\n@@ -5,11 +5,14 @@ from absl.testing import parameterized\n from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n+from keras_core.backend.common.backend_utils import (\n+    _convert_conv_tranpose_padding_args_from_keras_to_torch,\n+)\n from keras_core.backend.common.backend_utils import (\n     compute_conv_transpose_output_shape,\n )\n from keras_core.backend.common.backend_utils import (\n-    compute_conv_transpose_padding,\n+    compute_conv_transpose_padding_args_for_jax,\n )\n \n \n@@ -290,16 +293,15 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format,\n             dilation_rate,\n         )\n-        (h_pad,) = compute_conv_transpose_padding(\n-            x.shape,\n-            kernel_weights.shape,\n-            strides,\n-            padding,\n-            output_padding,\n-            data_format,\n-            dilation_rate,\n+        jax_padding = compute_conv_transpose_padding_args_for_jax(\n+            input_shape=x.shape,\n+            kernel_shape=kernel_weights.shape,\n+            strides=strides,\n+            padding=padding,\n+            output_padding=output_padding,\n+            dilation_rate=dilation_rate,\n         )\n-        h_pad_side1 = h_kernel - 1 - h_pad[0]\n+        h_pad_side1 = h_kernel - 1 - jax_padding[0][0]\n \n         if h_dilation > 1:\n             # Increase kernel size\n@@ -366,17 +368,16 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format,\n             dilation_rate,\n         )\n-        h_pad, w_pad = compute_conv_transpose_padding(\n-            x.shape,\n-            kernel_weights.shape,\n-            strides,\n-            padding,\n-            output_padding,\n-            data_format,\n-            dilation_rate,\n+        jax_padding = compute_conv_transpose_padding_args_for_jax(\n+            input_shape=x.shape,\n+            kernel_shape=kernel_weights.shape,\n+            strides=strides,\n+            padding=padding,\n+            output_padding=output_padding,\n+            dilation_rate=dilation_rate,\n         )\n-        h_pad_side1 = h_kernel - 1 - h_pad[0]\n-        w_pad_side1 = w_kernel - 1 - w_pad[0]\n+        h_pad_side1 = h_kernel - 1 - jax_padding[0][0]\n+        w_pad_side1 = w_kernel - 1 - jax_padding[1][0]\n \n         if h_dilation > 1 or w_dilation > 1:\n             # Increase kernel size\n@@ -458,18 +459,17 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format,\n             dilation_rate,\n         )\n-        h_pad, w_pad, d_pad = compute_conv_transpose_padding(\n-            x.shape,\n-            kernel_weights.shape,\n-            strides,\n-            padding,\n-            output_padding,\n-            data_format,\n-            dilation_rate,\n+        jax_padding = compute_conv_transpose_padding_args_for_jax(\n+            input_shape=x.shape,\n+            kernel_shape=kernel_weights.shape,\n+            strides=strides,\n+            padding=padding,\n+            output_padding=output_padding,\n+            dilation_rate=dilation_rate,\n         )\n-        h_pad_side1 = h_kernel - 1 - h_pad[0]\n-        w_pad_side1 = w_kernel - 1 - w_pad[0]\n-        d_pad_side1 = d_kernel - 1 - d_pad[0]\n+        h_pad_side1 = h_kernel - 1 - jax_padding[0][0]\n+        w_pad_side1 = w_kernel - 1 - jax_padding[1][0]\n+        d_pad_side1 = d_kernel - 1 - jax_padding[2][0]\n \n         if h_dilation > 1 or w_dilation > 1 or d_dilation > 1:\n             # Increase kernel size\n@@ -749,3 +749,89 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate,\n         )\n         self.assertAllClose(outputs, expected, atol=1e-5)\n+\n+    @parameterized.product(\n+        kernel_size=list(range(1, 5)),\n+        strides=list(range(1, 5)),\n+        padding=[\"same\", \"valid\"],\n+        output_padding=[None] + list(range(1, 5)),\n+    )\n+    def test_conv1d_transpose_consistency(\n+        self, kernel_size, strides, padding, output_padding\n+    ):\n+        \"\"\"Test conv transpose, on an 1D array of size 3, against several\n+        convolution parameters. In particular, tests if Torch inconsistencies\n+        are raised.\n+        \"\"\"\n+\n+        # output_padding cannot be greater than strides\n+        if isinstance(output_padding, int) and output_padding >= strides:\n+            pytest.skip(\n+                \"`output_padding` greater than `strides` is not supported\"\n+            )\n+\n+        input = np.ones(shape=(1, 3, 1))\n+        kernel_weights = np.arange(1, kernel_size + 1).reshape(\n+            (kernel_size, 1, 1)\n+        )\n+\n+        # Exepected result\n+        expected_res = self.np_conv1d_transpose(\n+            x=input,\n+            kernel_weights=kernel_weights,\n+            bias_weights=np.zeros(shape=(1,)),\n+            strides=strides,\n+            padding=padding,\n+            output_padding=output_padding,\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+        )\n+\n+        # keras-core layer\n+        kc_layer = layers.Conv1DTranspose(\n+            filters=1,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            output_padding=output_padding,\n+            dilation_rate=1,\n+        )\n+        kc_layer.build(input_shape=(1, 3, 1))\n+        kc_layer.kernel.assign(kernel_weights)\n+\n+        # Special cases for Torch\n+        if backend.backend() == \"torch\":\n+            # The following set of arguments lead to Torch output padding to be\n+            # greater than strides, which is not supported by Torch.\n+            # An error is raised.\n+            if (kernel_size, strides, padding, output_padding) in [\n+                (2, 1, \"same\", None),\n+                (4, 1, \"same\", None),\n+            ]:\n+                with pytest.raises(ValueError):\n+                    kc_res = kc_layer(input)\n+                return\n+\n+            # When both torch_padding and torch_output_padding are greater\n+            # than 0, Torch outputs are inconsistent with the ones from\n+            # Tensorflow. A warning is raised, and we expect the results to be\n+            # different.\n+            (\n+                torch_padding,\n+                torch_output_padding,\n+            ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+                kernel_size=kernel_size,\n+                stride=strides,\n+                dilation_rate=1,\n+                padding=padding,\n+                output_padding=output_padding,\n+            )\n+            if torch_padding > 0 and torch_output_padding > 0:\n+                with pytest.raises(AssertionError):\n+                    kc_res = kc_layer(input)\n+                    self.assertAllClose(expected_res, kc_res, atol=1e-5)\n+                return\n+\n+        # Compare results\n+        kc_res = kc_layer(input)\n+        self.assertAllClose(expected_res, kc_res, atol=1e-5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6963eae4103ffb79124a6831de9a1dc5ec6b3f81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 23 | Files Changed: 3 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/8 | Churn Δ: 84 | Churn Cumulative: 2851 | Contributors (this commit): 11 | Commits (past 90d): 45 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -282,6 +282,13 @@ def stft(\n     else:\n         win = torch.ones((sequence_length,), dtype=x.dtype, device=get_device())\n \n+    need_unpack = False\n+    *batch_shape, samples = x.shape\n+    if len(x.shape) > 2:\n+        need_unpack = True\n+        flat_batchsize = math.prod(batch_shape)\n+        x = torch.reshape(x, (flat_batchsize, samples))\n+\n     x = torch.stft(\n         x,\n         n_fft=fft_length,\n@@ -291,6 +298,10 @@ def stft(\n         center=center,\n         return_complex=True,\n     )\n+    if need_unpack:\n+        fft_unique_bins, num_sequences = x.shape[-2:]\n+        x = torch.reshape(x, (*batch_shape, fft_unique_bins, num_sequences))\n+\n     x = torch.swapaxes(x, -2, -1)\n     return x.real, x.imag\n \n@@ -304,36 +315,68 @@ def istft(\n     window=\"hann\",\n     center=True,\n ):\n-    # ref:\n-    # torch: aten/src/ATen/native/SpectralOps.cpp\n-    # tf: tf.signal.inverse_stft_window_fn\n-    x = irfft(x, fft_length)\n-\n-    expected_output_len = fft_length + sequence_stride * (x.shape[-2] - 1)\n-\n+    complex_input = _get_complex_tensor_from_tuple(x)\n+    dtype = complex_input.real.dtype\n+    win = None\n     if window is not None:\n         if isinstance(window, str):\n             if window == \"hann\":\n                 win = torch.hann_window(\n                     sequence_length,\n                     periodic=True,\n-                    dtype=x.dtype,\n+                    dtype=dtype,\n                     device=get_device(),\n                 )\n             else:\n                 win = torch.hamming_window(\n                     sequence_length,\n                     periodic=True,\n-                    dtype=x.dtype,\n+                    dtype=dtype,\n                     device=get_device(),\n                 )\n         else:\n-            win = convert_to_tensor(window, dtype=x.dtype)\n+            win = convert_to_tensor(window, dtype=dtype)\n         if len(win.shape) != 1 or win.shape[-1] != sequence_length:\n             raise ValueError(\n                 \"The shape of `window` must be equal to [sequence_length].\"\n                 f\"Received: window shape={win.shape}\"\n             )\n+\n+    if sequence_length == fft_length and center is True and win is not None:\n+        # can be falled back to torch.istft\n+        need_unpack = False\n+        *batch_shape, num_sequences, fft_unique_bins = complex_input.shape\n+        if len(complex_input.shape) > 3:\n+            need_unpack = True\n+            flat_batchsize = math.prod(batch_shape)\n+            complex_input = torch.reshape(\n+                complex_input, (flat_batchsize, num_sequences, fft_unique_bins)\n+            )\n+        complex_input = torch.swapaxes(complex_input, -2, -1)\n+        x = torch.istft(\n+            complex_input,\n+            n_fft=fft_length,\n+            hop_length=sequence_stride,\n+            win_length=sequence_length,\n+            window=win,\n+            center=center,\n+            length=length,\n+            return_complex=False,\n+        )\n+        if need_unpack:\n+            samples = x.shape[-1]\n+            x = torch.reshape(x, (*batch_shape, samples))\n+        return x\n+\n+    # custom implementation with irfft and _overlap_sequences\n+    # references:\n+    # torch: aten/src/ATen/native/SpectralOps.cpp\n+    # tf: tf.signal.inverse_stft_window_fn\n+    x = irfft(x, fft_length)\n+\n+    expected_output_len = fft_length + sequence_stride * (x.shape[-2] - 1)\n+\n+    if win is not None:\n         l_pad = (fft_length - sequence_length) // 2\n         r_pad = fft_length - sequence_length - l_pad\n         win = pad(win, [[l_pad, r_pad]], \"constant\")\n\n@@ -691,11 +691,6 @@ class STFT(Operation):\n         self.center = center\n \n     def compute_output_spec(self, x):\n-        if len(x.shape) not in {1, 2}:\n-            raise ValueError(\n-                f\"Input should have rank 1 (single sequence) or 2 \"\n-                f\"(batched sequences). Received: input shape = {x.shape}\"\n-            )\n         if x.shape[-1] is not None:\n             padded = 0 if self.center is False else (self.fft_length // 2) * 2\n             num_sequences = (\n\n@@ -746,8 +746,8 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(real_ref, real_output, atol=1e-5, rtol=1e-5)\n         self.assertAllClose(imag_ref, imag_output, atol=1e-5, rtol=1e-5)\n \n-        # Test 2D case.\n-        x = np.random.random((3, 32))\n+        # Test N-D case.\n+        x = np.random.random((2, 3, 32))\n         real_output, imag_output = kmath.stft(\n             x, sequence_length, sequence_stride, fft_length, window, center\n         )\n@@ -794,16 +794,16 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             window=window,\n             center=center,\n         )\n-        if backend.backend() in (\"numpy\", \"jax\"):\n-            # numpy and jax have different implementation for the boundary of\n+        if backend.backend() in (\"numpy\", \"jax\", \"torch\"):\n+            # these backends have different implementation for the boundary of\n             # the output, so we need to truncate 5% befroe assertAllClose\n             truncated_len = int(output.shape[-1] * 0.05)\n             output = output[..., truncated_len:-truncated_len]\n             ref = ref[..., truncated_len:-truncated_len]\n         self.assertAllClose(output, ref, atol=1e-5, rtol=1e-5)\n \n-        # Test 2D case.\n-        x = np.random.random((3, 256))\n+        # Test N-D case.\n+        x = np.random.random((2, 3, 256))\n         real_x, imag_x = _stft(\n             x, sequence_length, sequence_stride, fft_length, window, center\n         )\n@@ -823,8 +823,8 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             window=window,\n             center=center,\n         )\n-        if backend.backend() in (\"numpy\", \"jax\"):\n-            # numpy and jax have different implementation for the boundary of\n+        if backend.backend() in (\"numpy\", \"jax\", \"torch\"):\n+            # these backends have different implementation for the boundary of\n             # the output, so we need to truncate 5% befroe assertAllClose\n             truncated_len = int(output.shape[-1] * 0.05)\n             output = output[..., truncated_len:-truncated_len]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#09c2979818bd72ac9eacbcb10c1e398642f11822", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 173 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 18 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 181 | Churn Cumulative: 232 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -222,20 +222,18 @@ def reduce_shape(shape, axis=None, keepdims=False):\n     shape = list(shape)\n     if axis is None:\n         if keepdims:\n-            output_shape = [1 for _ in range(shape)]\n+            return tuple([1 for _ in shape])\n         else:\n-            output_shape = []\n-        return output_shape\n+            return tuple([])\n \n     if keepdims:\n         for ax in axis:\n             shape[ax] = 1\n-        return shape\n+        return tuple(shape)\n     else:\n-        for ax in axis:\n-            shape[ax] = -1\n-        output_shape = list(filter((-1).__ne__, shape))\n-        return output_shape\n+        for ax in sorted(axis, reverse=True):\n+            del shape[ax]\n+        return tuple(shape)\n \n \n @keras_core_export(\"keras_core.utils.get_source_inputs\")\n\n@@ -17,3 +17,170 @@ class OperationUtilsTest(testing.TestCase):\n     def test_get_source_inputs_return_input_tensor(self):\n         inputs = input_layer.Input(shape=(10,))\n         self.assertIs(operation_utils.get_source_inputs(inputs)[0], inputs)\n+\n+    def test_compute_pooling_output_shape(self):\n+        input_shape = (1, 4, 4, 1)\n+        pool_size = (2, 2)\n+        strides = (2, 2)\n+        output_shape = operation_utils.compute_pooling_output_shape(\n+            input_shape, pool_size, strides\n+        )\n+        expected_output_shape = (1, 2, 2, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n+    def test_compute_pooling_output_shape_with_none(self):\n+        input_shape = (None, 4, 4, 1)\n+        pool_size = (2, 2)\n+        strides = (2, 2)\n+        output_shape = operation_utils.compute_pooling_output_shape(\n+            input_shape, pool_size, strides\n+        )\n+        expected_output_shape = (None, 2, 2, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n+    def test_compute_pooling_output_shape_valid_padding(self):\n+        input_shape = (1, 4, 4, 1)\n+        pool_size = (2, 2)\n+        strides = (2, 2)\n+        output_shape = operation_utils.compute_pooling_output_shape(\n+            input_shape, pool_size, strides, padding=\"valid\"\n+        )\n+        self.assertEqual(output_shape, (1, 2, 2, 1))\n+\n+    def test_compute_pooling_output_shape_channels_last(self):\n+        input_shape = (1, 4, 4, 3)\n+        pool_size = (2, 2)\n+        strides = (2, 2)\n+        output_shape = operation_utils.compute_pooling_output_shape(\n+            input_shape,\n+            pool_size,\n+            strides,\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+        )\n+        self.assertEqual(output_shape, (1, 2, 2, 3))\n+\n+    def test_compute_pooling_output_shape_same_padding_stride1(self):\n+        input_shape = (1, 4, 4, 3)\n+        pool_size = (2, 2)\n+        strides = (1, 1)\n+        output_shape = operation_utils.compute_pooling_output_shape(\n+            input_shape,\n+            pool_size,\n+            strides,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+        )\n+        self.assertEqual(output_shape, (1, 4, 4, 3))\n+\n+    def test_compute_conv_output_shape(self):\n+        input_shape = (1, 4, 4, 1)\n+        filters = 1\n+        kernel_size = (3, 3)\n+        strides = (1, 1)\n+        output_shape = operation_utils.compute_conv_output_shape(\n+            input_shape, filters, kernel_size, strides\n+        )\n+        expected_output_shape = (1, 2, 2, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n+    def test_compute_conv_output_shape_with_none(self):\n+        input_shape = (None, 4, 4, 1)\n+        kernel_size = (3, 3)\n+        filters = 1\n+        strides = (1, 1)\n+        output_shape = operation_utils.compute_conv_output_shape(\n+            input_shape, filters, kernel_size, strides\n+        )\n+        expected_output_shape = (None, 2, 2, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n+    def test_compute_conv_output_shape_valid_padding(self):\n+        input_shape = (1, 4, 4, 1)\n+        kernel_size = (3, 3)\n+        filters = 1\n+        strides = (2, 2)\n+        output_shape = operation_utils.compute_conv_output_shape(\n+            input_shape, filters, kernel_size, strides, padding=\"valid\"\n+        )\n+        self.assertEqual(output_shape, (1, 1, 1, 1))\n+\n+    def test_compute_conv_output_shape_channels_last(self):\n+        input_shape = (1, 4, 4, 3)\n+        kernel_size = (3, 3)\n+        filters = 3\n+        strides = (2, 2)\n+        output_shape = operation_utils.compute_conv_output_shape(\n+            input_shape,\n+            filters,\n+            kernel_size,\n+            strides,\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+        )\n+        self.assertEqual(output_shape, (1, 1, 1, 3))\n+\n+    def test_compute_conv_output_shape_same_padding_stride1(self):\n+        input_shape = (1, 4, 4, 3)\n+        kernel_size = (3, 3)\n+        filters = 3\n+        strides = (1, 1)\n+        output_shape = operation_utils.compute_conv_output_shape(\n+            input_shape,\n+            filters,\n+            kernel_size,\n+            strides,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+        )\n+        self.assertEqual(output_shape, (1, 4, 4, 3))\n+\n+    def test_compute_reshape_output_shape(self):\n+        input_shape = (1, 4, 4, 1)\n+        target_shape = (16, 1)\n+        output_shape = operation_utils.compute_reshape_output_shape(\n+            input_shape, new_shape=target_shape, new_shape_arg_name=\"New shape\"\n+        )\n+        self.assertEqual(output_shape, target_shape)\n+\n+    def test_reduce_shape_no_axes_no_keepdims(self):\n+        input_shape = (1, 4, 4, 1)\n+        output_shape = operation_utils.reduce_shape(input_shape)\n+        expected_output_shape = ()\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n+    def test_reduce_shape_no_axes_with_keepdims(self):\n+        input_shape = (1, 4, 4, 1)\n+        output_shape = operation_utils.reduce_shape(input_shape, keepdims=True)\n+        expected_output_shape = (1, 1, 1, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n+    def test_reduce_shape_single_axis_no_keepdims(self):\n+        input_shape = (1, 4, 4, 1)\n+        axes = [1]\n+        output_shape = operation_utils.reduce_shape(input_shape, axes)\n+        expected_output_shape = (1, 4, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n+    def test_reduce_shape_single_axis_with_keepdims(self):\n+        input_shape = (1, 4, 4, 1)\n+        axes = [1]\n+        output_shape = operation_utils.reduce_shape(\n+            input_shape, axes, keepdims=True\n+        )\n+        expected_output_shape = (1, 1, 4, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n+    def test_reduce_shape_multiple_axes_no_keepdims(self):\n+        input_shape = (1, 4, 4, 1)\n+        axes = [1, 2]\n+        output_shape = operation_utils.reduce_shape(input_shape, axes)\n+        expected_output_shape = (1, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n+    def test_reduce_shape_out_of_order_axes_no_keepdims(self):\n+        input_shape = (1, 4, 4, 1)\n+        axes = [2, 1]\n+        output_shape = operation_utils.reduce_shape(input_shape, axes)\n+        expected_output_shape = (1, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
