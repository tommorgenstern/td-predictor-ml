{"custom_id": "scrapy#cb3007c06647867ac53072c01fc7fabe778a8fcc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 28 | Churn Cumulative: 447 | Contributors (this commit): 16 | Commits (past 90d): 20 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ import lxml.etree as etree\n \n from scrapy.selector import Selector\n from scrapy.link import Link\n-from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list\n from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.utils.response import get_base_url\n@@ -62,7 +62,7 @@ class LxmlParserLinkExtractor(object):\n             # to fix relative links after process_value\n             url = urljoin(response_url, url)\n             link = Link(url, _collect_string_content(el) or u'',\n-                nofollow=True if el.get('rel') == 'nofollow' else False)\n+                        nofollow=rel_has_nofollow(el.get('rel')))\n             links.append(link)\n \n         return unique_list(links, key=lambda link: link.url) \\\n\n@@ -9,7 +9,7 @@ from w3lib.url import safe_url_string\n from scrapy.selector import Selector\n from scrapy.link import Link\n from scrapy.linkextractors import FilteringLinkExtractor\n-from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list, to_unicode\n from scrapy.utils.response import get_base_url\n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -80,7 +80,7 @@ class BaseSgmlLinkExtractor(SGMLParser):\n                 if self.scan_attr(attr):\n                     url = self.process_value(value)\n                     if url is not None:\n-                        link = Link(url=url, nofollow=True if dict(attrs).get('rel') == 'nofollow' else False)\n+                        link = Link(url=url, nofollow=rel_has_nofollow(dict(attrs).get('rel')))\n                         self.links.append(link)\n                         self.current_link = link\n \n\n@@ -112,3 +112,8 @@ def md5sum(file):\n             break\n         m.update(d)\n     return m.hexdigest()\n+\n+def rel_has_nofollow(rel):\n+    \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n+    return True if rel is not None and 'nofollow' in rel.split() else False\n+    \n\n@@ -96,12 +96,14 @@ class LinkExtractorTestCase(unittest.TestCase):\n         html = \"\"\"\n         <a href=\"page.html?action=print\" rel=\"nofollow\">Printer-friendly page</a>\n         <a href=\"about.html\">About us</a>\n+        <a href=\"http://google.com/something\" rel=\"external nofollow\">Something</a>\n         \"\"\"\n         response = HtmlResponse(\"http://example.org/page.html\", body=html)\n         lx = SgmlLinkExtractor()\n         self.assertEqual([link for link in lx.extract_links(response)], [\n             Link(url='http://example.org/page.html?action=print', text=u'Printer-friendly page', nofollow=True),\n             Link(url='http://example.org/about.html', text=u'About us', nofollow=False),\n+            Link(url='http://google.com/something', text=u'Something', nofollow=True),\n         ])\n \n \n@@ -205,6 +207,9 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         <div>\n         <p><a href=\"/nofollow2.html\" rel=\"blah\">Choose to follow or not</a></p>\n         </div>\n+        <div>\n+        <p><a href=\"http://google.com/something\" rel=\"external nofollow\">External link not to follow</a></p>\n+        </div>\n         </body></html>\"\"\"\n         response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n \n@@ -214,6 +219,7 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n             Link(url='http://example.org/follow.html', text=u'Follow this link'),\n             Link(url='http://example.org/nofollow.html', text=u'Dont follow this one', nofollow=True),\n             Link(url='http://example.org/nofollow2.html', text=u'Choose to follow or not'),\n+            Link(url='http://google.com/something', text=u'External link not to follow', nofollow=True),\n         ])\n \n     def test_matches(self):\n@@ -467,6 +473,9 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n     <div>\n     <p><a href=\"/nofollow2.html\" rel=\"blah\">Choose to follow or not</a></p>\n     </div>\n+    <div>\n+    <p><a href=\"http://google.com/something\" rel=\"external nofollow\">External link not to follow</a></p>\n+    </div>\n </body>\n </html>\n         \"\"\"\n@@ -478,7 +487,8 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://example.com/about.html', text=u'About us', fragment='', nofollow=False),\n                           Link(url='http://example.com/follow.html', text=u'Follow this link', fragment='', nofollow=False),\n                           Link(url='http://example.com/nofollow.html', text=u'Dont follow this one', fragment='', nofollow=True),\n-                          Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False)]\n+                          Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False),\n+                          Link(url='http://google.com/something', text=u'External link not to follow', nofollow=True)]\n                         )\n \n         response = XmlResponse(\"http://example.com/index.xhtml\", body=xhtml)\n@@ -488,7 +498,8 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://example.com/about.html', text=u'About us', fragment='', nofollow=False),\n                           Link(url='http://example.com/follow.html', text=u'Follow this link', fragment='', nofollow=False),\n                           Link(url='http://example.com/nofollow.html', text=u'Dont follow this one', fragment='', nofollow=True),\n-                          Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False)]\n+                          Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False),\n+                          Link(url='http://google.com/something', text=u'External link not to follow', nofollow=True)]\n                         )\n \n     def test_link_wrong_href(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f46a4500803dac7bd64284862c3b5937bb525a64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 33 | Files Changed: 1 | Hunks: 16 | Methods Changed: 8 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 88 | Churn Cumulative: 163 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,8 @@\n import re\n import unittest\n+\n+import pytest\n+\n from scrapy.linkextractors.regex import RegexLinkExtractor\n from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.link import Link\n@@ -9,7 +12,7 @@ from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n from tests import get_testdata\n \n \n-class LinkExtractorTestCase(unittest.TestCase):\n+class BaseSgmlLinkExtractorTestCase(unittest.TestCase):\n     def test_basic(self):\n         html = \"\"\"<html><head><title>Page title<title>\n         <body><p><a href=\"item/12.html\">Item 12</a></p>\n@@ -92,30 +95,21 @@ class LinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.matches(url1), True)\n         self.assertEqual(lx.matches(url2), True)\n \n-    def test_link_nofollow(self):\n-        html = \"\"\"\n-        <a href=\"page.html?action=print\" rel=\"nofollow\">Printer-friendly page</a>\n-        <a href=\"about.html\">About us</a>\n-        \"\"\"\n-        response = HtmlResponse(\"http://example.org/page.html\", body=html)\n-        lx = SgmlLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/page.html?action=print', text=u'Printer-friendly page', nofollow=True),\n-            Link(url='http://example.org/about.html', text=u'About us', nofollow=False),\n-        ])\n \n-\n-class SgmlLinkExtractorTestCase(unittest.TestCase):\n-    extractor_cls = SgmlLinkExtractor\n+class BaseLinkExtractorTestCase(unittest.TestCase):\n+    extractor_cls = None\n \n     def setUp(self):\n+        if self.extractor_cls is None:\n+            raise unittest.SkipTest()\n         body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n         self.response = HtmlResponse(url='http://example.com/index', body=body)\n \n     def test_urls_type(self):\n-        '''Test that the resulting urls are regular strings and not a unicode objects'''\n+        ''' Test that the resulting urls are str objects '''\n         lx = self.extractor_cls()\n-        self.assertTrue(all(isinstance(link.url, str) for link in lx.extract_links(self.response)))\n+        self.assertTrue(all(isinstance(link.url, str)\n+                            for link in lx.extract_links(self.response)))\n \n     def test_extraction(self):\n         '''Test the extractor's behaviour among different situations'''\n@@ -271,7 +265,7 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n     def test_restrict_xpaths_with_html_entities(self):\n         html = '<html><body><p><a href=\"/&hearts;/you?c=&euro;\">text</a></p></body></html>'\n         response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html, encoding='iso8859-15')\n-        links = SgmlLinkExtractor(restrict_xpaths='//p').extract_links(response)\n+        links = self.extractor_cls(restrict_xpaths='//p').extract_links(response)\n         self.assertEqual(links,\n                          [Link(url='http://example.org/%E2%99%A5/you?c=%E2%82%AC', text=u'text')])\n \n@@ -326,7 +320,8 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n             Link(url='http://known.fm/AC%2FDC/?page=2', text=u'BinB', fragment='', nofollow=False),\n         ])\n \n-    def test_deny_extensions(self):\n+    def test_ignored_extensions(self):\n+        # jpg is ignored by default\n         html = \"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n         response = HtmlResponse(\"http://example.org/\", body=html)\n         lx = self.extractor_cls()\n@@ -334,9 +329,10 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n             Link(url='http://example.org/page.html', text=u'asd'),\n         ])\n \n-        lx = SgmlLinkExtractor(deny_extensions=\"jpg\")\n+        # override denied extensions\n+        lx = self.extractor_cls(deny_extensions=['html'])\n         self.assertEqual(lx.extract_links(response), [\n-            Link(url='http://example.org/page.html', text=u'asd'),\n+            Link(url='http://example.org/photo.jpg'),\n         ])\n \n     def test_process_value(self):\n@@ -388,13 +384,6 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         lx = self.extractor_cls(attrs=None)\n         self.assertEqual(lx.extract_links(self.response), [])\n \n-        html = \"\"\"<html><area href=\"sample1.html\"></area><a ref=\"sample2.html\">sample text 2</a></html>\"\"\"\n-        response = HtmlResponse(\"http://example.com/index.html\", body=html)\n-        lx = SgmlLinkExtractor(attrs=(\"href\"))\n-        self.assertEqual(lx.extract_links(response), [\n-            Link(url='http://example.com/sample1.html', text=u''),\n-        ])\n-\n     def test_tags(self):\n         html = \"\"\"<html><area href=\"sample1.html\"></area><a href=\"sample2.html\">sample 2</a><img src=\"sample2.jpg\"/></html>\"\"\"\n         response = HtmlResponse(\"http://example.com/index.html\", body=html)\n@@ -505,7 +494,7 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         ])\n \n \n-class LxmlLinkExtractorTestCase(SgmlLinkExtractorTestCase):\n+class LxmlLinkExtractorTestCase(BaseLinkExtractorTestCase):\n     extractor_cls = LxmlLinkExtractor\n \n     def test_link_wrong_href(self):\n@@ -521,6 +510,10 @@ class LxmlLinkExtractorTestCase(SgmlLinkExtractorTestCase):\n             Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n         ])\n \n+    @pytest.mark.xfail\n+    def test_restrict_xpaths_with_html_entities(self):\n+        super(LxmlLinkExtractorTestCase, self).test_restrict_xpaths_with_html_entities()\n+\n \n class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n \n@@ -552,6 +545,39 @@ class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n         ])\n \n \n+class SgmlLinkExtractorTestCase(BaseLinkExtractorTestCase):\n+    extractor_cls = SgmlLinkExtractor\n+\n+    def test_deny_extensions(self):\n+        html = \"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n+        response = HtmlResponse(\"http://example.org/\", body=html)\n+        lx = SgmlLinkExtractor(deny_extensions=\"jpg\")\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://example.org/page.html', text=u'asd'),\n+        ])\n+\n+    def test_attrs_sgml(self):\n+        html = \"\"\"<html><area href=\"sample1.html\"></area>\n+        <a ref=\"sample2.html\">sample text 2</a></html>\"\"\"\n+        response = HtmlResponse(\"http://example.com/index.html\", body=html)\n+        lx = SgmlLinkExtractor(attrs=\"href\")\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://example.com/sample1.html', text=u''),\n+        ])\n+\n+    def test_link_nofollow(self):\n+        html = \"\"\"\n+        <a href=\"page.html?action=print\" rel=\"nofollow\">Printer-friendly page</a>\n+        <a href=\"about.html\">About us</a>\n+        \"\"\"\n+        response = HtmlResponse(\"http://example.org/page.html\", body=html)\n+        lx = SgmlLinkExtractor()\n+        self.assertEqual([link for link in lx.extract_links(response)], [\n+            Link(url='http://example.org/page.html?action=print', text=u'Printer-friendly page', nofollow=True),\n+            Link(url='http://example.org/about.html', text=u'About us', nofollow=False),\n+        ])\n+\n+\n class RegexLinkExtractorTestCase(unittest.TestCase):\n \n     def setUp(self):\n@@ -579,7 +605,3 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n             Link(url='http://example.org/item1.html', text=u'Item 1', nofollow=False),\n             Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n         ])\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d5984bbea99f81765596e1aa57d03aff51612576", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 18 | Files Changed: 6 | Hunks: 29 | Methods Changed: 13 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 54 | Churn Cumulative: 251 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 16 | DMM Complexity: 0.875\n\nDIFF:\n@@ -6,14 +6,17 @@ See documentation in docs/topics/spiders.rst\n \"\"\"\n \n import copy\n+import six\n \n from scrapy.http import Request, HtmlResponse\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.spiders import Spider\n \n+\n def identity(x):\n     return x\n \n+\n class Rule(object):\n \n     def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=identity):\n@@ -27,6 +30,7 @@ class Rule(object):\n         else:\n             self.follow = follow\n \n+\n class CrawlSpider(Spider):\n \n     rules = ()\n@@ -49,7 +53,8 @@ class CrawlSpider(Spider):\n             return\n         seen = set()\n         for n, rule in enumerate(self._rules):\n-            links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]\n+            links = [lnk for lnk in rule.link_extractor.extract_links(response)\n+                     if lnk not in seen]\n             if links and rule.process_links:\n                 links = rule.process_links(links)\n             for link in links:\n@@ -77,7 +82,7 @@ class CrawlSpider(Spider):\n         def get_method(method):\n             if callable(method):\n                 return method\n-            elif isinstance(method, basestring):\n+            elif isinstance(method, six.string_types):\n                 return getattr(self, method, None)\n \n         self._rules = [copy.copy(r) for r in self.rules]\n\n@@ -1,6 +1,7 @@\n from scrapy.spiders import Spider\n from scrapy.utils.spider import iterate_spider_output\n \n+\n class InitSpider(Spider):\n     \"\"\"Base Spider with initialization facilities\"\"\"\n \n\n@@ -1,5 +1,6 @@\n import re\n import logging\n+import six\n \n from scrapy.spiders import Spider\n from scrapy.http import Request, XmlResponse\n@@ -20,13 +21,14 @@ class SitemapSpider(Spider):\n         super(SitemapSpider, self).__init__(*a, **kw)\n         self._cbs = []\n         for r, c in self.sitemap_rules:\n-            if isinstance(c, basestring):\n+            if isinstance(c, six.string_types):\n                 c = getattr(self, c)\n             self._cbs.append((regex(r), c))\n         self._follow = [regex(x) for x in self.sitemap_follow]\n \n     def start_requests(self):\n-        return (Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls)\n+        for url in self.sitemap_urls:\n+            yield Request(url, self._parse_sitemap)\n \n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n@@ -52,8 +54,8 @@ class SitemapSpider(Spider):\n                             break\n \n     def _get_sitemap_body(self, response):\n-        \"\"\"Return the sitemap body contained in the given response, or None if the\n-        response is not a sitemap.\n+        \"\"\"Return the sitemap body contained in the given response,\n+        or None if the response is not a sitemap.\n         \"\"\"\n         if isinstance(response, XmlResponse):\n             return response.body\n@@ -64,11 +66,13 @@ class SitemapSpider(Spider):\n         elif response.url.endswith('.xml.gz'):\n             return gunzip(response.body)\n \n+\n def regex(x):\n-    if isinstance(x, basestring):\n+    if isinstance(x, six.string_types):\n         return re.compile(x)\n     return x\n \n+\n def iterloc(it, alt=False):\n     for d in it:\n         yield d['loc']\n\n@@ -7,6 +7,7 @@ except ImportError:\n \n from gzip import GzipFile\n \n+\n def gunzip(data):\n     \"\"\"Gunzip the given data and return as much data as possible.\n \n@@ -31,7 +32,8 @@ def gunzip(data):\n                 raise\n     return output\n \n+\n def is_gzipped(response):\n     \"\"\"Return True if the response is gzipped, or False otherwise\"\"\"\n-    ctype = response.headers.get('Content-Type', '')\n-    return ctype in ('application/x-gzip', 'application/gzip')\n+    ctype = response.headers.get('Content-Type', b'')\n+    return ctype in (b'application/x-gzip', b'application/gzip')\n\n@@ -301,26 +301,32 @@ class SitemapSpiderTest(SpiderTest):\n     g.close()\n     GZBODY = f.getvalue()\n \n-    def test_get_sitemap_body(self):\n+    def assertSitemapBody(self, response, body):\n         spider = self.spider_class(\"example.com\")\n+        self.assertEqual(spider._get_sitemap_body(response), body)\n \n+    def test_get_sitemap_body(self):\n         r = XmlResponse(url=\"http://www.example.com/\", body=self.BODY)\n-        self.assertEqual(spider._get_sitemap_body(r), self.BODY)\n+        self.assertSitemapBody(r, self.BODY)\n \n         r = HtmlResponse(url=\"http://www.example.com/\", body=self.BODY)\n-        self.assertEqual(spider._get_sitemap_body(r), None)\n+        self.assertSitemapBody(r, None)\n \n         r = Response(url=\"http://www.example.com/favicon.ico\", body=self.BODY)\n-        self.assertEqual(spider._get_sitemap_body(r), None)\n+        self.assertSitemapBody(r, None)\n \n-        r = Response(url=\"http://www.example.com/sitemap\", body=self.GZBODY, headers={\"content-type\": \"application/gzip\"})\n-        self.assertEqual(spider._get_sitemap_body(r), self.BODY)\n+    def test_get_sitemap_body_gzip_headers(self):\n+        r = Response(url=\"http://www.example.com/sitemap\", body=self.GZBODY,\n+                     headers={\"content-type\": \"application/gzip\"})\n+        self.assertSitemapBody(r, self.BODY)\n \n+    def test_get_sitemap_body_xml_url(self):\n         r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=self.BODY)\n-        self.assertEqual(spider._get_sitemap_body(r), self.BODY)\n+        self.assertSitemapBody(r, self.BODY)\n \n+    def test_get_sitemap_body_xml_url_compressed(self):\n         r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.GZBODY)\n-        self.assertEqual(spider._get_sitemap_body(r), self.BODY)\n+        self.assertSitemapBody(r, self.BODY)\n \n \n class BaseSpiderDeprecationTest(unittest.TestCase):\n\n@@ -7,7 +7,7 @@ from tests import tests_datadir\n SAMPLEDIR = join(tests_datadir, 'compressed')\n \n \n-class GzTest(unittest.TestCase):\n+class GunzipTest(unittest.TestCase):\n \n     def test_gunzip_basic(self):\n         with open(join(SAMPLEDIR, 'feed-sample1.xml.gz'), 'rb') as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ff24cbbc477e8bd5459034e702bafd0c5ea1fc43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 32/16 | Churn Δ: 15 | Churn Cumulative: 19 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -35,14 +35,18 @@ class DepthMiddleware(object):\n                 if self.prio:\n                     request.priority -= depth * self.prio\n                 if self.maxdepth and depth > self.maxdepth:\n-                    logger.debug(\"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n+                    logger.debug(\n+                        \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                         {'maxdepth': self.maxdepth, 'requrl': request.url},\n-                                 extra={'spider': spider})\n+                        extra={'spider': spider}\n+                    )\n                     return False\n                 elif self.stats:\n                     if self.verbose_stats:\n-                        self.stats.inc_value('request_depth_count/%s' % depth, spider=spider)\n-                    self.stats.max_value('request_depth_max', depth, spider=spider)\n+                        self.stats.inc_value('request_depth_count/%s' % depth,\n+                                             spider=spider)\n+                    self.stats.max_value('request_depth_max', depth,\n+                                         spider=spider)\n             return True\n \n         # base case (depth=0)\n\n@@ -13,6 +13,7 @@ from scrapy.utils.httpobj import urlparse_cached\n \n logger = logging.getLogger(__name__)\n \n+\n class OffsiteMiddleware(object):\n \n     def __init__(self, stats):\n\n@@ -17,5 +17,5 @@ class TestRefererMiddleware(TestCase):\n \n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n         self.assertEquals(out[0].headers.get('Referer'),\n-                          'http://scrapytest.org')\n+                          b'http://scrapytest.org')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f7052413e092346b7d460d589d23fecaa4351930", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 49 | Churn Cumulative: 146 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,10 @@ This module defines the Link object used in Link extractors.\n For actual link extractors implementation see scrapy.linkextractors, or\n its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n-from scrapy.utils.python import to_native_str\n+import warnings\n+import six\n+\n+from scrapy.utils.python import to_bytes\n \n \n class Link(object):\n@@ -14,9 +17,13 @@ class Link(object):\n \n     def __init__(self, url, text='', fragment='', nofollow=False):\n         if not isinstance(url, str):\n-            import warnings\n-            warnings.warn(\"Link urls must be str objects.\")\n-            url = to_native_str(url)\n+            if six.PY2:\n+                warnings.warn(\"Link urls must be str objects. \"\n+                              \"Assuming utf-8 encoding (which could be wrong)\")\n+                url = to_bytes(url, encoding='utf8')\n+            else:\n+                got = url.__class__.__name__\n+                raise TypeError(\"Link urls must be str objects, got %s\" % got)\n         self.url = url\n         self.text = text\n         self.fragment = fragment\n\n@@ -16,44 +16,42 @@ class LinkTest(unittest.TestCase):\n         self.assertNotEqual(hash(link1), hash(link2))\n \n     def test_eq_and_hash(self):\n-        l1 = Link(b\"http://www.example.com\")\n-        l2 = Link(b\"http://www.example.com/other\")\n-        l3 = Link(b\"http://www.example.com\")\n+        l1 = Link(\"http://www.example.com\")\n+        l2 = Link(\"http://www.example.com/other\")\n+        l3 = Link(\"http://www.example.com\")\n \n         self._assert_same_links(l1, l1)\n         self._assert_different_links(l1, l2)\n         self._assert_same_links(l1, l3)\n \n-        l4 = Link(b\"http://www.example.com\", text=\"test\")\n-        l5 = Link(b\"http://www.example.com\", text=\"test2\")\n-        l6 = Link(b\"http://www.example.com\", text=\"test\")\n+        l4 = Link(\"http://www.example.com\", text=\"test\")\n+        l5 = Link(\"http://www.example.com\", text=\"test2\")\n+        l6 = Link(\"http://www.example.com\", text=\"test\")\n \n         self._assert_same_links(l4, l4)\n         self._assert_different_links(l4, l5)\n         self._assert_same_links(l4, l6)\n \n-        l7 = Link(b\"http://www.example.com\", text=\"test\", fragment='something', nofollow=False)\n-        l8 = Link(b\"http://www.example.com\", text=\"test\", fragment='something', nofollow=False)\n-        l9 = Link(b\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n-        l10 = Link(b\"http://www.example.com\", text=\"test\", fragment='other', nofollow=False)\n+        l7 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=False)\n+        l8 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=False)\n+        l9 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n+        l10 = Link(\"http://www.example.com\", text=\"test\", fragment='other', nofollow=False)\n         self._assert_same_links(l7, l8)\n         self._assert_different_links(l7, l9)\n         self._assert_different_links(l7, l10)\n \n     def test_repr(self):\n-        l1 = Link(b\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n+        l1 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n         l2 = eval(repr(l1))\n         self._assert_same_links(l1, l2)\n \n-    def test_non_str_url(self):\n-        with warnings.catch_warnings(record=True) as w:\n+    def test_non_str_url_py2(self):\n         if six.PY2:\n+            with warnings.catch_warnings(record=True) as w:\n                 link = Link(u\"http://www.example.com/\\xa3\")\n                 self.assertIsInstance(link.url, str)\n                 self.assertEqual(link.url, b'http://www.example.com/\\xc2\\xa3')\n-            else:\n-                link = Link(b\"http://www.example.com/\\xc2\\xa3\")\n-                self.assertIsInstance(link.url, str)\n-                self.assertEqual(link.url, u'http://www.example.com/\\xa3')\n-\n             assert len(w) == 1, \"warning not issued\"\n+        else:\n+            with self.assertRaises(TypeError):\n+                Link(b\"http://www.example.com/\\xc2\\xa3\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#44bfcbcf0f26a469c96feed35c40715b8b58c6a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 15 | Churn Cumulative: 394 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,9 +24,7 @@ class Base:\n             self.assertTrue(all(isinstance(link.url, str)\n                                 for link in lx.extract_links(self.response)))\n \n-        def test_extraction(self):\n-            '''Test the extractor's behaviour among different situations'''\n-\n+        def test_extract_all_links(self):\n             lx = self.extractor_cls()\n             self.assertEqual([link for link in lx.extract_links(self.response)], [\n                 Link(url='http://example.com/sample1.html', text=u''),\n@@ -36,6 +34,7 @@ class Base:\n                 Link(url='http://example.com/innertag.html', text=u'inner tag'),\n             ])\n \n+        def test_extract_filter_allow(self):\n             lx = self.extractor_cls(allow=('sample', ))\n             self.assertEqual([link for link in lx.extract_links(self.response)], [\n                 Link(url='http://example.com/sample1.html', text=u''),\n@@ -43,6 +42,7 @@ class Base:\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n             ])\n \n+        def test_extract_filter_allow_with_duplicates(self):\n             lx = self.extractor_cls(allow=('sample', ), unique=False)\n             self.assertEqual([link for link in lx.extract_links(self.response)], [\n                 Link(url='http://example.com/sample1.html', text=u''),\n@@ -51,19 +51,14 @@ class Base:\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n             ])\n \n-            lx = self.extractor_cls(allow=('sample', ))\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=u''),\n-                Link(url='http://example.com/sample2.html', text=u'sample 2'),\n-                Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n-            ])\n-\n+        def test_extract_filter_allow_and_deny(self):\n             lx = self.extractor_cls(allow=('sample', ), deny=('3', ))\n             self.assertEqual([link for link in lx.extract_links(self.response)], [\n                 Link(url='http://example.com/sample1.html', text=u''),\n                 Link(url='http://example.com/sample2.html', text=u'sample 2'),\n             ])\n \n+        def test_extract_filter_allowed_domains(self):\n             lx = self.extractor_cls(allow_domains=('google.com', ))\n             self.assertEqual([link for link in lx.extract_links(self.response)], [\n                 Link(url='http://www.google.com/something', text=u''),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e5f26078fa6c49c37aa0523f98b39977989454f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 34 | Churn Cumulative: 203 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -733,6 +733,40 @@ class FormRequestTest(RequestTest):\n         self.assertRaises(ValueError, self.request_class.from_response,\n                           response, formxpath=\"//form/input[@name='abc']\")\n \n+    def test_from_response_button_submit(self):\n+        response = _buildresponse(\n+            \"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n+            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n+            <button type=\"submit\" name=\"button1\" value=\"submit1\">Submit</button>\n+            </form>\"\"\",\n+            url=\"http://www.example.com/this/list.html\")\n+        req = self.request_class.from_response(response)\n+        self.assertEqual(req.method, 'POST')\n+        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n+        fs = _qs(req)\n+        self.assertEqual(fs[b'test1'], [b'val1'])\n+        self.assertEqual(fs[b'test2'], [b'val2'])\n+        self.assertEqual(fs[b'button1'], [b'submit1'])\n+\n+    def test_from_response_button_notype(self):\n+        response = _buildresponse(\n+            \"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n+            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n+            <button name=\"button1\" value=\"submit1\">Submit</button>\n+            </form>\"\"\",\n+            url=\"http://www.example.com/this/list.html\")\n+        req = self.request_class.from_response(response)\n+        self.assertEqual(req.method, 'POST')\n+        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n+        fs = _qs(req)\n+        self.assertEqual(fs[b'test1'], [b'val1'])\n+        self.assertEqual(fs[b'test2'], [b'val2'])\n+        self.assertEqual(fs[b'button1'], [b'submit1'])\n+\n def _buildresponse(body, **kwargs):\n     kwargs.setdefault('body', body)\n     kwargs.setdefault('url', 'http://example.com')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#121d7535beb327d1d63a92ae39b9974d8eec7bb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 713 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -150,14 +150,16 @@ def _get_clickable(clickdata, form):\n     if the latter is given. If not, it returns the first\n     clickable element found\n     \"\"\"\n-    clickables = [el for el in form.xpath('.//input[@type=\"submit\"]')]\n+    clickables = [el for el in form.xpath('descendant::input[@type=\"submit\"]'\n+                                          '|descendant::button[@type=\"submit\"]'\n+                                          '|descendant::button[not(@type)]')]\n     if not clickables:\n         return\n \n     # If we don't have clickdata, we just use the first clickable element\n     if clickdata is None:\n         el = clickables[0]\n-        return (el.name, el.value)\n+        return (el.get('name'), el.get('value'))\n \n     # If clickdata is given, we compare it to the clickable elements to find a\n     # match. We first look to see if the number is specified in clickdata,\n@@ -169,7 +171,7 @@ def _get_clickable(clickdata, form):\n         except IndexError:\n             pass\n         else:\n-            return (el.name, el.value)\n+            return (el.get('name'), el.get('value'))\n \n     # We didn't find it, so now we build an XPath expression out of the other\n     # arguments, because they can be used as such\n@@ -177,7 +179,7 @@ def _get_clickable(clickdata, form):\n             u''.join(u'[@%s=\"%s\"]' % c for c in six.iteritems(clickdata))\n     el = form.xpath(xpath)\n     if len(el) == 1:\n-        return (el[0].name, el[0].value)\n+        return (el[0].get('name'), el[0].get('value'))\n     elif len(el) > 1:\n         raise ValueError(\"Multiple elements found (%r) matching the criteria \"\n                          \"in clickdata: %r\" % (el, clickdata))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45101829a52d9681b59fc401d659c0ee34792e2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 14 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ class AjaxCrawlMiddlewareTest(unittest.TestCase):\n         self.mw = AjaxCrawlMiddleware.from_crawler(crawler)\n \n     def _ajaxcrawlable_body(self):\n-        return '<html><head><meta name=\"fragment\" content=\"!\"/></head><body></body></html>'\n+        return b'<html><head><meta name=\"fragment\" content=\"!\"/></head><body></body></html>'\n \n     def _req_resp(self, url, req_kwargs=None, resp_kwargs=None):\n         req = Request(url, **(req_kwargs or {}))\n@@ -53,6 +53,6 @@ class AjaxCrawlMiddlewareTest(unittest.TestCase):\n         assert resp3 is resp2\n \n     def test_noncrawlable_body(self):\n-        req, resp = self._req_resp('http://example.com/', {}, {'body': '<html></html>'})\n+        req, resp = self._req_resp('http://example.com/', {}, {'body': b'<html></html>'})\n         resp2 = self.mw.process_response(req, resp, self.spider)\n         self.assertIs(resp, resp2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3cf1911a9237c8cf23f6bbb7cba35335a75623e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 22 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,10 @@\n from unittest import TestCase\n-import six\n \n from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\n from scrapy.http import Request\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n+from scrapy.utils.python import to_bytes\n \n \n class TestDefaultHeadersMiddleware(TestCase):\n@@ -12,8 +12,10 @@ class TestDefaultHeadersMiddleware(TestCase):\n     def get_defaults_spider_mw(self):\n         crawler = get_crawler(Spider)\n         spider = crawler._create_spider('foo')\n-        defaults = dict([(k, [v]) for k, v in \\\n-            six.iteritems(crawler.settings.get('DEFAULT_REQUEST_HEADERS'))])\n+        defaults = {\n+            to_bytes(k): [to_bytes(v)]\n+            for k, v in crawler.settings.get('DEFAULT_REQUEST_HEADERS').items()\n+        }\n         return defaults, spider, DefaultHeadersMiddleware.from_crawler(crawler)\n \n     def test_process_request(self):\n@@ -25,9 +27,10 @@ class TestDefaultHeadersMiddleware(TestCase):\n     def test_update_headers(self):\n         defaults, spider, mw = self.get_defaults_spider_mw()\n         headers = {'Accept-Language': ['es'], 'Test-Header': ['test']}\n+        bytes_headers = {b'Accept-Language': [b'es'], b'Test-Header': [b'test']}\n         req = Request('http://www.scrapytest.org', headers=headers)\n-        self.assertEquals(req.headers, headers)\n+        self.assertEquals(req.headers, bytes_headers)\n \n         mw.process_request(req, spider)\n-        defaults.update(headers)\n+        defaults.update(bytes_headers)\n         self.assertEquals(req.headers, defaults)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#78a4cd0f1c32477d42f14e4c48daedb24be44ebf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 17 | Churn Cumulative: 21 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,5 +27,5 @@ class HttpAuthMiddleware(object):\n \n     def process_request(self, request, spider):\n         auth = getattr(self, 'auth', None)\n-        if auth and 'Authorization' not in request.headers:\n-            request.headers['Authorization'] = auth\n+        if auth and b'Authorization' not in request.headers:\n+            request.headers[b'Authorization'] = auth\n\n@@ -4,10 +4,12 @@ from scrapy.http import Request\n from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\n from scrapy.spiders import Spider\n \n+\n class TestSpider(Spider):\n     http_user = 'foo'\n     http_pass = 'bar'\n \n+\n class HttpAuthMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n@@ -21,13 +23,10 @@ class HttpAuthMiddlewareTest(unittest.TestCase):\n     def test_auth(self):\n         req = Request('http://scrapytest.org/')\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers['Authorization'], 'Basic Zm9vOmJhcg==')\n+        self.assertEquals(req.headers['Authorization'], b'Basic Zm9vOmJhcg==')\n \n     def test_auth_already_set(self):\n-        req = Request('http://scrapytest.org/', headers=dict(Authorization='Digest 123'))\n+        req = Request('http://scrapytest.org/',\n+                      headers=dict(Authorization='Digest 123'))\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers['Authorization'], 'Digest 123')\n-\n-\n-if __name__ == '__main__':\n-    unittest.main()\n+        self.assertEquals(req.headers['Authorization'], b'Digest 123')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a9c73bc5d95fa59543522d0e997ce339c3a6c99", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 35 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,10 @@ from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n \n+class MyException(Exception):\n+    pass\n+\n+\n class TestDownloaderStats(TestCase):\n \n     def setUp(self):\n@@ -18,21 +22,28 @@ class TestDownloaderStats(TestCase):\n         self.req = Request('http://scrapytest.org')\n         self.res = Response('scrapytest.org', status=400)\n \n+    def assertStatsEqual(self, key, value):\n+        self.assertEqual(\n+            self.crawler.stats.get_value(key, spider=self.spider),\n+            value,\n+            str(self.crawler.stats.get_stats(self.spider))\n+        )\n+\n     def test_process_request(self):\n         self.mw.process_request(self.req, self.spider)\n-        self.assertEqual(self.crawler.stats.get_value('downloader/request_count', \\\n-            spider=self.spider), 1)\n+        self.assertStatsEqual('downloader/request_count', 1)\n \n     def test_process_response(self):\n         self.mw.process_response(self.req, self.res, self.spider)\n-        self.assertEqual(self.crawler.stats.get_value('downloader/response_count', \\\n-            spider=self.spider), 1)\n+        self.assertStatsEqual('downloader/response_count', 1)\n \n     def test_process_exception(self):\n-        self.mw.process_exception(self.req, Exception(), self.spider)\n-        self.assertEqual(self.crawler.stats.get_value('downloader/exception_count', \\\n-            spider=self.spider), 1)\n+        self.mw.process_exception(self.req, MyException(), self.spider)\n+        self.assertStatsEqual('downloader/exception_count', 1)\n+        self.assertStatsEqual(\n+            'downloader/exception_type_count/tests.test_downloadermiddleware_stats.MyException',\n+            1\n+        )\n \n     def tearDown(self):\n         self.crawler.stats.close_spider(self.spider, '')\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c44cafe4f58d6c5e5a3a8142f81ee0201fb8268a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 11 | Churn Cumulative: 20 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,4 +20,4 @@ class UserAgentMiddleware(object):\n \n     def process_request(self, request, spider):\n         if self.user_agent:\n-            request.headers.setdefault('User-Agent', self.user_agent)\n+            request.headers.setdefault(b'User-Agent', self.user_agent)\n\n@@ -17,7 +17,7 @@ class UserAgentMiddlewareTest(TestCase):\n         spider, mw = self.get_spider_and_mw('default_useragent')\n         req = Request('http://scrapytest.org/')\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.headers['User-Agent'], 'default_useragent')\n+        self.assertEquals(req.headers['User-Agent'], b'default_useragent')\n \n     def test_remove_agent(self):\n         # settings UESR_AGENT to None should remove the user agent\n@@ -34,15 +34,16 @@ class UserAgentMiddlewareTest(TestCase):\n         mw.spider_opened(spider)\n         req = Request('http://scrapytest.org/')\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.headers['User-Agent'], 'spider_useragent')\n+        self.assertEquals(req.headers['User-Agent'], b'spider_useragent')\n \n     def test_header_agent(self):\n         spider, mw = self.get_spider_and_mw('default_useragent')\n         spider.user_agent = 'spider_useragent'\n         mw.spider_opened(spider)\n-        req = Request('http://scrapytest.org/', headers={'User-Agent': 'header_useragent'})\n+        req = Request('http://scrapytest.org/',\n+                      headers={'User-Agent': 'header_useragent'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.headers['User-Agent'], 'header_useragent')\n+        self.assertEquals(req.headers['User-Agent'], b'header_useragent')\n \n     def test_no_agent(self):\n         spider, mw = self.get_spider_and_mw(None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2c28b53cc08bae41c0a00ffeae3c8d277a1d3b84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 34 | Churn Cumulative: 237 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -767,6 +767,40 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs[b'test2'], [b'val2'])\n         self.assertEqual(fs[b'button1'], [b'submit1'])\n \n+    def test_from_response_submit_novalue(self):\n+        response = _buildresponse(\n+            \"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n+            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n+            <input type=\"submit\" name=\"button1\">Submit</button>\n+            </form>\"\"\",\n+            url=\"http://www.example.com/this/list.html\")\n+        req = self.request_class.from_response(response)\n+        self.assertEqual(req.method, 'POST')\n+        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n+        fs = _qs(req)\n+        self.assertEqual(fs[b'test1'], [b'val1'])\n+        self.assertEqual(fs[b'test2'], [b'val2'])\n+        self.assertEqual(fs[b'button1'], [b''])\n+\n+    def test_from_response_button_novalue(self):\n+        response = _buildresponse(\n+            \"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n+            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n+            <button type=\"submit\" name=\"button1\">Submit</button>\n+            </form>\"\"\",\n+            url=\"http://www.example.com/this/list.html\")\n+        req = self.request_class.from_response(response)\n+        self.assertEqual(req.method, 'POST')\n+        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n+        fs = _qs(req)\n+        self.assertEqual(fs[b'test1'], [b'val1'])\n+        self.assertEqual(fs[b'test2'], [b'val2'])\n+        self.assertEqual(fs[b'button1'], [b''])\n+\n def _buildresponse(body, **kwargs):\n     kwargs.setdefault('body', body)\n     kwargs.setdefault('url', 'http://example.com')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
