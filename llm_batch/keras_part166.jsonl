{"custom_id": "keras#8d317c986721371f6d11ae9fbc0afa0212a088dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 160 | Lines Deleted: 190 | Files Changed: 4 | Hunks: 85 | Methods Changed: 67 | Complexity Δ (Sum/Max): 55/66 | Churn Δ: 350 | Churn Cumulative: 21250 | Contributors (this commit): 94 | Commits (past 90d): 138 | Contributors (cumulative): 149 | DMM Complexity: 0.0\n\nDIFF:\n@@ -789,9 +789,10 @@ def moving_average_update(variable, value, momentum):\n \n def dot(x, y):\n     \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n-    When attempting to multiply a ND tensor\n-    with a ND tensor, it reproduces the Theano behavior.\n-    (e.g. (2, 3).(4, 3, 5) = (2, 4, 5))\n+\n+    When attempting to multiply a nD tensor\n+    with a nD tensor, it reproduces the Theano behavior.\n+    (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)\n \n     # Arguments\n         x: Tensor or variable.\n@@ -867,7 +868,8 @@ def batch_dot(x, y, axes=None):\n     we use `expand_dims` to make sure that ndim is at least 2.\n \n     # Arguments\n-        x, y: Keras tensors or variables with `ndim >= 2`\n+        x: Keras tensor or variable with `ndim >= 2`.\n+        y: Keras tensor or variable with `ndim >= 2`.\n         axes: list of (or single) int with target dimensions.\n             The lengths of `axes[0]` and `axes[1]` should be the same.\n \n@@ -983,6 +985,15 @@ def gather(reference, indices):\n # ELEMENT-WISE OPERATIONS\n \n def _normalize_axis(axis, ndim):\n+    \"\"\"Converts negative axes to positive values.\n+\n+    # Arguments\n+        axis: Integer axis (possibly negative).\n+        ndim: Rank of the tensor considered.\n+\n+    # Returns\n+        Positive integer axis.\n+    \"\"\"\n     if isinstance(axis, tuple):\n         axis = list(axis)\n     if isinstance(axis, list):\n@@ -991,7 +1002,7 @@ def _normalize_axis(axis, ndim):\n                 axis[i] = a % ndim\n     else:\n         if axis is not None and axis < 0:\n-            axis = axis % ndim\n+            axis %= ndim\n     return axis\n \n \n@@ -1132,7 +1143,7 @@ def any(x, axis=None, keepdims=False):\n     \"\"\"Bitwise reduction (logical OR).\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         axis: axis along which to perform the reduction.\n         keepdims: whether the drop or broadcast the reduction axes.\n \n@@ -1149,7 +1160,7 @@ def all(x, axis=None, keepdims=False):\n     \"\"\"Bitwise reduction (logical AND).\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         axis: axis along which to perform the reduction.\n         keepdims: whether the drop or broadcast the reduction axes.\n \n@@ -1166,7 +1177,7 @@ def argmax(x, axis=-1):\n     \"\"\"Returns the index of the maximum value along an axis.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         axis: axis along which to perform the reduction.\n \n     # Returns\n@@ -1181,7 +1192,7 @@ def argmin(x, axis=-1):\n     \"\"\"Returns the index of the minimum value along an axis.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         axis: axis along which to perform the reduction.\n \n     # Returns\n@@ -1196,7 +1207,7 @@ def square(x):\n     \"\"\"Element-wise square.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1208,7 +1219,7 @@ def abs(x):\n     \"\"\"Element-wise absolute value.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1220,7 +1231,7 @@ def sqrt(x):\n     \"\"\"Element-wise square root.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1235,7 +1246,7 @@ def exp(x):\n     \"\"\"Element-wise exponential.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1247,7 +1258,7 @@ def log(x):\n     \"\"\"Element-wise log.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1261,7 +1272,7 @@ def round(x):\n     In case of tie, the rounding mode used is \"half to even\".\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1273,7 +1284,7 @@ def sign(x):\n     \"\"\"Element-wise sign.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1285,7 +1296,7 @@ def pow(x, a):\n     \"\"\"Element-wise exponentiation.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         a: Python integer.\n \n     # Returns\n@@ -1298,7 +1309,7 @@ def clip(x, min_value, max_value):\n     \"\"\"Element-wise value clipping.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         min_value: Python float or integer.\n         max_value: Python float or integer.\n \n@@ -1318,8 +1329,8 @@ def equal(x, y):\n     \"\"\"Element-wise equality between two tensors.\n \n     # Arguments\n-        x: input tensor.\n-        y: input tensor.\n+        x: Tensor or variable.\n+        y: Tensor or variable.\n \n     # Returns\n         A bool tensor.\n@@ -1330,6 +1341,10 @@ def equal(x, y):\n def not_equal(x, y):\n     \"\"\"Element-wise inequality between two tensors.\n \n+    # Arguments\n+        x: Tensor or variable.\n+        y: Tensor or variable.\n+\n     # Returns\n         A bool tensor.\n     \"\"\"\n@@ -1340,8 +1355,8 @@ def greater(x, y):\n     \"\"\"Element-wise truth value of (x > y).\n \n     # Arguments\n-        x: input tensor.\n-        y: input tensor.\n+        x: Tensor or variable.\n+        y: Tensor or variable.\n \n     # Returns\n         A bool tensor.\n@@ -1352,6 +1367,10 @@ def greater(x, y):\n def greater_equal(x, y):\n     \"\"\"Element-wise truth value of (x >= y).\n \n+    # Arguments\n+        x: Tensor or variable.\n+        y: Tensor or variable.\n+\n     # Returns\n         A bool tensor.\n     \"\"\"\n@@ -1362,8 +1381,8 @@ def lesser(x, y):\n     \"\"\"Element-wise truth value of (x < y).\n \n     # Arguments\n-        x: input tensor.\n-        y: input tensor.\n+        x: Tensor or variable.\n+        y: Tensor or variable.\n \n     # Returns\n         A bool tensor.\n@@ -1375,8 +1394,8 @@ def lesser_equal(x, y):\n     \"\"\"Element-wise truth value of (x <= y).\n \n     # Arguments\n-        x: input tensor.\n-        y: input tensor.\n+        x: Tensor or variable.\n+        y: Tensor or variable.\n \n     # Returns\n         A bool tensor.\n@@ -1388,8 +1407,8 @@ def maximum(x, y):\n     \"\"\"Element-wise maximum of two tensors.\n \n     # Arguments\n-        x: input tensor.\n-        y: input tensor.\n+        x: Tensor or variable.\n+        y: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1401,8 +1420,8 @@ def minimum(x, y):\n     \"\"\"Element-wise minimum of two tensors.\n \n     # Arguments\n-        x: input tensor.\n-        y: input tensor.\n+        x: Tensor or variable.\n+        y: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1414,7 +1433,7 @@ def sin(x):\n     \"\"\"Computes sin of x element-wise.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1426,7 +1445,7 @@ def cos(x):\n     \"\"\"Computes cos of x element-wise.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1439,6 +1458,12 @@ def normalize_batch_in_training(x, gamma, beta,\n     \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n \n     # Arguments\n+        x: Input tensor or variable.\n+        gamma: Tensor by which to scale the input.\n+        beta: Tensor with which to center the input.\n+        reduction_axes: iterable of integers,\n+            axes over which to normalize.\n+        epsilon: Fuzz factor.\n \n     # Returns\n         A tuple length of 3, `(normalized_tensor, mean, variance)`.\n@@ -1475,7 +1500,12 @@ def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n     `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n \n     # Arguments\n-\n+        x: Input tensor or variable.\n+        mean: Mean of batch.\n+        var: Variance of batch.\n+        gamma: Tensor by which to scale the input.\n+        beta: Tensor with which to center the input.\n+        epsilon: Fuzz factor.\n \n     # Returns\n         A tensor.\n@@ -1489,6 +1519,8 @@ def concatenate(tensors, axis=-1):\n     \"\"\"Concatenates a list of tensors alongside the specified axis.\n \n     # Arguments\n+        tensors: list of tensors to concatenate.\n+        axis: concatenation axis.\n \n     # Returns\n         A tensor.\n@@ -1516,6 +1548,8 @@ def reshape(x, shape):\n     \"\"\"Reshapes a tensor to the specified shape.\n \n     # Arguments\n+        x: Tensor or variable.\n+        shape: Target shape tuple.\n \n     # Returns\n         A tensor.\n@@ -1527,7 +1561,7 @@ def permute_dimensions(x, pattern):\n     \"\"\"Permutes axes in a tensor.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         pattern: A tuple of\n             dimension indices, e.g. `(0, 2, 1)`.\n \n@@ -1537,10 +1571,14 @@ def permute_dimensions(x, pattern):\n     return tf.transpose(x, perm=pattern)\n \n \n-def resize_images(X, height_factor, width_factor, data_format):\n+def resize_images(x, height_factor, width_factor, data_format):\n     \"\"\"Resizes the images contained in a 4D tensor.\n \n     # Arguments\n+        x: Tensor or variable to resize.\n+        height_factor: Positive integer.\n+        width_factor: Positive integer.\n+        data_format: One of `\"channels_first\"`, `\"channels_last\"`.\n \n     # Returns\n         A tensor.\n@@ -1550,23 +1588,23 @@ def resize_images(X, height_factor, width_factor, data_format):\n             `channels_last` or `channels_first`.\n     \"\"\"\n     if data_format == 'channels_first':\n-        original_shape = int_shape(X)\n-        new_shape = tf.shape(X)[2:]\n+        original_shape = int_shape(x)\n+        new_shape = tf.shape(x)[2:]\n         new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\n-        X = permute_dimensions(X, [0, 2, 3, 1])\n-        X = tf.image.resize_nearest_neighbor(X, new_shape)\n-        X = permute_dimensions(X, [0, 3, 1, 2])\n-        X.set_shape((None, None, original_shape[2] * height_factor if original_shape[2] is not None else None,\n+        x = permute_dimensions(x, [0, 2, 3, 1])\n+        x = tf.image.resize_nearest_neighbor(x, new_shape)\n+        x = permute_dimensions(x, [0, 3, 1, 2])\n+        x.set_shape((None, None, original_shape[2] * height_factor if original_shape[2] is not None else None,\n                      original_shape[3] * width_factor if original_shape[3] is not None else None))\n-        return X\n+        return x\n     elif data_format == 'channels_last':\n-        original_shape = int_shape(X)\n-        new_shape = tf.shape(X)[1:3]\n+        original_shape = int_shape(x)\n+        new_shape = tf.shape(x)[1:3]\n         new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\n-        X = tf.image.resize_nearest_neighbor(X, new_shape)\n-        X.set_shape((None, original_shape[1] * height_factor if original_shape[1] is not None else None,\n+        x = tf.image.resize_nearest_neighbor(x, new_shape)\n+        x.set_shape((None, original_shape[1] * height_factor if original_shape[1] is not None else None,\n                      original_shape[2] * width_factor if original_shape[2] is not None else None, None))\n-        return X\n+        return x\n     else:\n         raise ValueError('Invalid data_format:', data_format)\n \n@@ -1575,6 +1613,11 @@ def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n     \"\"\"Resizes the volume contained in a 5D tensor.\n \n     # Arguments\n+        x: Tensor or variable to resize.\n+        depth_factor: Positive integer.\n+        height_factor: Positive integer.\n+        width_factor: Positive integer.\n+        data_format: One of `\"channels_first\"`, `\"channels_last\"`.\n \n     # Returns\n         A tensor.\n@@ -1604,6 +1647,12 @@ def repeat_elements(x, rep, axis):\n     will have shape `(s1, s2 * rep, s3)`.\n \n     # Arguments\n+        x: Tensor or variable.\n+        rep: Python integer, number of times to repeat.\n+        axis: Axis along which to repeat.\n+\n+    # Raises\n+        ValueError: In case `x.shape[axis]` is undefined.\n \n     # Returns\n         A tensor.\n@@ -1632,6 +1681,8 @@ def repeat(x, n):\n     the output will have shape `(samples, 2, dim)`.\n \n     # Arguments\n+        x: Tensor or variable.\n+        n: Python integer, number of times to repeat.\n \n     # Returns\n         A tensor.\n@@ -1686,6 +1737,7 @@ def flatten(x):\n     \"\"\"Flatten a tensor.\n \n     # Arguments\n+        x: A tensor or variable.\n \n     # Returns\n         A tensor, reshaped into 1-D\n@@ -1699,6 +1751,7 @@ def batch_flatten(x):\n     In other words, it flattens each data samples of a batch.\n \n     # Arguments\n+        x: A tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -1707,21 +1760,25 @@ def batch_flatten(x):\n     return x\n \n \n-def expand_dims(x, dim=-1):\n+def expand_dims(x, axis=-1):\n     \"\"\"Adds a 1-sized dimension at index \"dim\".\n \n     # Arguments\n+        x: A tensor or variable.\n+        axis: Position where to add a new axis.\n \n     # Returns\n         A tensor with expended dimensions.\n     \"\"\"\n-    return tf.expand_dims(x, dim)\n+    return tf.expand_dims(x, axis)\n \n \n def squeeze(x, axis):\n     \"\"\"Removes a 1-dimension from the tensor at index \"axis\".\n \n     # Arguments\n+        x: A tensor or variable.\n+        axis: Axis to drop.\n \n     # Returns\n         A tensor with the same data as `x` but reduced dimensions.\n@@ -1729,37 +1786,29 @@ def squeeze(x, axis):\n     return tf.squeeze(x, [axis])\n \n \n-def temporal_padding(x, padding=1):\n+def temporal_padding(x, padding=(1, 1)):\n     \"\"\"Pads the middle dimension of a 3D tensor.\n \n     # Arguments\n+        x: Tensor or variable.\n+        padding: Tuple of 2 integers, how many zeros to\n+            add at the start and end of dim 1.\n \n     # Returns\n         A padded 3D tensor.\n     \"\"\"\n-    pattern = [[0, 0], [padding, padding], [0, 0]]\n+    assert len(padding) == 2\n+    pattern = [[0, 0], [padding[0], padding[1]], [0, 0]]\n     return tf.pad(x, pattern)\n \n \n-def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n-    \"\"\"Pad the middle dimension of a 3D tensor.\n-\n-    # Arguments\n-\n-    # Returns\n-        A padded 3D tensor.\n-    \"\"\"\n-    pattern = [[0, 0], [left_pad, right_pad], [0, 0]]\n-    return tf.pad(x, pattern)\n-\n-\n-def spatial_2d_padding(x, padding=(1, 1), data_format=None):\n+def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     \"\"\"Pads the 2nd and 3rd dimensions of a 4D tensor.\n \n-    Pads these dimensions with\n-    \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n-\n     # Arguments\n+        x: Tensor or variable.\n+        padding: Tuple of 2 tuples, padding pattern.\n+        data_format: One of `channels_last` or `channels_first`.\n \n     # Returns\n         A padded 4D tensor.\n@@ -1768,38 +1817,9 @@ def spatial_2d_padding(x, padding=(1, 1), data_format=None):\n         ValueError: if `data_format` is neither\n             `channels_last` or `channels_first`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n-\n-    if data_format == 'channels_first':\n-        pattern = [[0, 0], [0, 0],\n-                   [padding[0], padding[0]], [padding[1], padding[1]]]\n-    else:\n-        pattern = [[0, 0],\n-                   [padding[0], padding[0]], [padding[1], padding[1]],\n-                   [0, 0]]\n-    return tf.pad(x, pattern)\n-\n-\n-def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1,\n-                                  left_pad=1, right_pad=1,\n-                                  data_format=None):\n-    \"\"\"Pad the rows and columns of a 4D tensor.\n-\n-    Pads these dimensions with\n-    \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\" (resp.) zeros\n-    rows on top, bottom; cols on left, right.\n-\n-    # Arguments\n-\n-    # Returns\n-        A padded 4D tensor.\n-\n-    # Raises\n-        ValueError: if `data_format` is neither `channels_last` or `channels_first`.\n-    \"\"\"\n+    assert len(padding) == 2\n+    assert len(padding[0]) == 2\n+    assert len(padding[1]) == 2\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n@@ -1808,12 +1828,11 @@ def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1,\n     if data_format == 'channels_first':\n         pattern = [[0, 0],\n                    [0, 0],\n-                   [top_pad, bottom_pad],\n-                   [left_pad, right_pad]]\n+                   list(padding[0]),\n+                   list(padding[1])]\n     else:\n         pattern = [[0, 0],\n-                   [top_pad, bottom_pad],\n-                   [left_pad, right_pad],\n+                   list(padding[0]), list(padding[1]),\n                    [0, 0]]\n     return tf.pad(x, pattern)\n \n@@ -1830,6 +1849,9 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     the 3rd, 4th and 5th dimension will be padded.\n \n     # Arguments\n+        x: Tensor or variable.\n+        padding: Tuple of 3 tuples, padding pattern.\n+        data_format: One of `channels_last` or `channels_first`.\n \n     # Returns\n         A padded 5D tensor.\n@@ -1839,6 +1861,10 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n             `channels_last` or `channels_first`.\n \n     \"\"\"\n+    assert len(padding) == 3\n+    assert len(padding[0]) == 2\n+    assert len(padding[1]) == 2\n+    assert len(padding[2]) == 2\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n@@ -1867,7 +1893,7 @@ def stack(x):\n     \"\"\"Stacks a list of rank `R` tensors into a rank `R+1` tensor.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n \n     # Returns\n         A tensor.\n@@ -2645,7 +2671,7 @@ def l2_normalize(x, axis):\n     \"\"\"Normalizes a tensor wrt the L2 norm alongside the specified axis.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         axis: axis along which to perform normalization.\n \n     # Returns\n@@ -2752,7 +2778,7 @@ def conv1d(x, kernel, strides=1, padding='valid',\n     \"\"\"1D convolution.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         kernel: kernel tensor.\n         strides: stride integer.\n         padding: string, `\"same\"` or `\"valid\"`.\n@@ -2783,7 +2809,7 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n     \"\"\"2D convolution.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         kernel: kernel tensor.\n         strides: strides tuple.\n         padding: string, `\"same\"` or `\"valid\"`.\n@@ -2823,7 +2849,7 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n     \"\"\"2D deconvolution (i.e. transposed convolution).\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         kernel: kernel tensor.\n         output_shape: 1D int tensor for the output shape.\n         strides: strides tuple.\n@@ -2897,7 +2923,7 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n     \"\"\"3D convolution.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         kernel: kernel tensor.\n         strides: strides tuple.\n         padding: string, `\"same\"` or `\"valid\"`.\n@@ -2938,7 +2964,7 @@ def pool2d(x, pool_size, strides=(1, 1),\n     \"\"\"2D Pooling.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         pool_size: tuple of 2 integers.\n         strides: tuple of 2 integers.\n         padding: one of `\"valid\"`, `\"same\"`.\n@@ -2978,7 +3004,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n     \"\"\"3D Pooling.\n \n     # Arguments\n-        x: input tensor.\n+        x: Tensor or variable.\n         pool_size: tuple of 3 integers.\n         strides: tuple of 3 integers.\n         padding: one of `\"valid\"`, `\"same\"`.\n@@ -3018,7 +3044,7 @@ def bias_add(x, bias, data_format=None):\n     \"\"\"Adds a bias vector to a tensor.\n \n     # Arguments\n-        x: Input tensor.\n+        x: Tensor or variable.\n         bias: Bias tensor to add.\n         data_format: Data format for 3D, 4D or 5D tensors:\n             one of \"channels_first\", \"channels_last\".\n@@ -3159,7 +3185,7 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n     num_batches_tns = tf.stack([label_shape[0]])\n     max_num_labels_tns = tf.stack([label_shape[1]])\n \n-    def range_less_than(current_input):\n+    def range_less_than(_, current_input):\n         return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(\n             max_num_labels_tns, current_input)\n \n\n@@ -771,17 +771,17 @@ def batch_flatten(x):\n     return x\n \n \n-def expand_dims(x, dim=-1):\n+def expand_dims(x, axis=-1):\n     \"\"\"Add a 1-sized dimension at index \"dim\".\n     \"\"\"\n     # TODO: `keras_shape` inference.\n     pattern = [i for i in range(x.type.ndim)]\n-    if dim < 0:\n+    if axis < 0:\n         if x.type.ndim == 0:\n-            dim = 0\n+            axis = 0\n         else:\n-            dim = dim % x.type.ndim + 1\n-    pattern.insert(dim, 'x')\n+            axis = axis % x.type.ndim + 1\n+    pattern.insert(axis, 'x')\n     return x.dimshuffle(pattern)\n \n \n@@ -794,7 +794,7 @@ def squeeze(x, axis):\n     return T.reshape(x, tuple(shape))\n \n \n-def temporal_padding(x, padding=1):\n+def temporal_padding(x, padding=(1, 1)):\n     \"\"\"Pad the middle dimension of a 3D tensor\n     with \"padding\" zeros left and right.\n \n@@ -802,74 +802,25 @@ def temporal_padding(x, padding=1):\n     really hard.\n     \"\"\"\n     # TODO: `keras_shape` inference.\n+    assert len(padding) == 2\n     input_shape = x.shape\n     output_shape = (input_shape[0],\n-                    input_shape[1] + 2 * padding,\n+                    input_shape[1] + padding[0] + padding[1],\n                     input_shape[2])\n     output = T.zeros(output_shape)\n-    return T.set_subtensor(output[:, padding:x.shape[1] + padding, :], x)\n+    return T.set_subtensor(output[:, padding[0]:x.shape[1] + padding[1], :], x)\n \n \n-def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n-    \"\"\"Pad the middle dimension of a 3D tensor\n-    with \"left_pad\" zeros left and \"right_pad\" right.\n-\n-    Apologies for the inane API, but Theano makes this\n-    really hard.\n-    \"\"\"\n-    # TODO: `keras_shape` inference.\n-    input_shape = x.shape\n-    output_shape = (input_shape[0],\n-                    input_shape[1] + left_pad + right_pad,\n-                    input_shape[2])\n-    output = T.zeros(output_shape)\n-    return T.set_subtensor(output[:, left_pad:x.shape[1] + left_pad, :], x)\n-\n-\n-def spatial_2d_padding(x, padding=(1, 1), data_format=None):\n+def spatial_2d_padding(x, padding=((1, 1),  (1, 1)), data_format=None):\n     \"\"\"Pad the 2nd and 3rd dimensions of a 4D tensor\n     with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n     \"\"\"\n     # TODO: `keras_shape` inference.\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n-\n-    input_shape = x.shape\n-    if data_format == 'channels_first':\n-        output_shape = (input_shape[0],\n-                        input_shape[1],\n-                        input_shape[2] + 2 * padding[0],\n-                        input_shape[3] + 2 * padding[1])\n-        output = T.zeros(output_shape)\n-        indices = (slice(None),\n-                   slice(None),\n-                   slice(padding[0], input_shape[2] + padding[0]),\n-                   slice(padding[1], input_shape[3] + padding[1]))\n-\n-    elif data_format == 'channels_last':\n-        output_shape = (input_shape[0],\n-                        input_shape[1] + 2 * padding[0],\n-                        input_shape[2] + 2 * padding[1],\n-                        input_shape[3])\n-        output = T.zeros(output_shape)\n-        indices = (slice(None),\n-                   slice(padding[0], input_shape[1] + padding[0]),\n-                   slice(padding[1], input_shape[2] + padding[1]),\n-                   slice(None))\n-    else:\n-        raise ValueError('Invalid data_format:', data_format)\n-    return T.set_subtensor(output[indices], x)\n-\n-\n-def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1,\n-                                  left_pad=1, right_pad=1,\n-                                  data_format=None):\n-    \"\"\"Pad the rows and columns of a 4D tensor\n-    with \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\" (resp.) zeros\n-    rows on top, bottom; cols on left, right.\n-    \"\"\"\n+    assert len(padding) == 2\n+    assert len(padding[0]) == 2\n+    assert len(padding[1]) == 2\n+    top_pad, bottom_pad = padding[0]\n+    left_pad, right_pad = padding[1]\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n\n@@ -1215,20 +1215,16 @@ class ZeroPadding1D(Layer):\n     def __init__(self, padding=1, **kwargs):\n         super(ZeroPadding1D, self).__init__(**kwargs)\n         self.padding = conv_utils.normalize_tuple(padding, 2, 'padding')\n-        self.left_pad = self.padding[0]\n-        self.right_pad = self.padding[1]\n         self.input_spec = InputSpec(ndim=3)\n \n     def compute_output_shape(self, input_shape):\n-        length = input_shape[1] + self.left_pad + self.right_pad if input_shape[1] is not None else None\n+        length = input_shape[1] + self.padding[0] + self.padding[1] if input_shape[1] is not None else None\n         return (input_shape[0],\n                 length,\n                 input_shape[2])\n \n     def call(self, inputs):\n-        return K.asymmetric_temporal_padding(inputs,\n-                                             left_pad=self.left_pad,\n-                                             right_pad=self.right_pad)\n+        return K.temporal_padding(inputs, padding=self.padding)\n \n     def get_config(self):\n         config = {'padding': self.padding}\n@@ -1324,11 +1320,8 @@ class ZeroPadding2D(Layer):\n             raise ValueError('Invalid data_format:', self.data_format)\n \n     def call(self, inputs):\n-        return K.asymmetric_spatial_2d_padding(inputs,\n-                                               top_pad=self.padding[0][0],\n-                                               bottom_pad=self.padding[0][1],\n-                                               left_pad=self.padding[1][0],\n-                                               right_pad=self.padding[1][1],\n+        return K.spatial_2d_padding(inputs,\n+                                    padding=self.padding,\n                                     data_format=self.data_format)\n \n     def get_config(self):\n\n@@ -116,8 +116,8 @@ class TestBackend(object):\n                                       pattern=(2, 0, 1))\n         check_single_tensor_operation('repeat', (4, 1), n=3)\n         check_single_tensor_operation('flatten', (4, 1))\n-        check_single_tensor_operation('expand_dims', (4, 3), dim=-1)\n-        check_single_tensor_operation('expand_dims', (4, 3, 2), dim=1)\n+        check_single_tensor_operation('expand_dims', (4, 3), axis=-1)\n+        check_single_tensor_operation('expand_dims', (4, 3, 2), axis=1)\n         check_single_tensor_operation('squeeze', (4, 3, 1), axis=2)\n         check_single_tensor_operation('squeeze', (4, 1, 1), axis=1)\n         check_composed_tensor_operations('reshape', {'shape': (4, 3, 1, 1)},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf1d29c8c3d51ddbb7b2fb639525c07827be4fa0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 41 | Files Changed: 1 | Hunks: 31 | Methods Changed: 21 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 128 | Churn Cumulative: 7255 | Contributors (this commit): 61 | Commits (past 90d): 57 | Contributors (cumulative): 61 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import tensorflow as tf\n from tensorflow.python.training import moving_averages\n from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import functional_ops\n try:\n     from tensorflow.python.ops import ctc_ops as ctc\n except ImportError:\n@@ -210,8 +211,7 @@ def is_sparse(tensor):\n \n \n def to_dense(tensor):\n-    \"\"\"Converts a sparse tensor into a dense tensor\n-    and returns it.\n+    \"\"\"Converts a sparse tensor into a dense tensor and returns it.\n \n     # Arguments\n         tensor: A tensor instance (potentially sparse).\n@@ -291,6 +291,8 @@ def variable(value, dtype=None, name=None):\n \n \n def _initialize_variables():\n+    \"\"\"Utility to initialize uninitialized variables on the fly.\n+    \"\"\"\n     if hasattr(tf, 'global_variables'):\n         variables = tf.global_variables()\n     else:\n@@ -386,8 +388,7 @@ def shape(x):\n \n \n def int_shape(x):\n-    \"\"\"Returns the shape of a Keras tensor or a Keras variable as a tuple of\n-    integers or None entries.\n+    \"\"\"Returns the shape tensor or variable as a tuple of int or None entries.\n \n     # Arguments\n         x: Tensor or variable.\n@@ -475,7 +476,6 @@ def dtype(x):\n \n def eval(x):\n     \"\"\"Evaluates the value of a variable.\n-    Returns a Numpy array.\n \n     # Arguments\n         x: A variable.\n@@ -579,8 +579,7 @@ def eye(size, dtype=None, name=None):\n \n \n def zeros_like(x, dtype=None, name=None):\n-    \"\"\"Instantiates an all-zeros Keras variable\n-    of the same shape as another Keras variable or tensor and returns it.\n+    \"\"\"Instantiates an all-zeros variable of the same shape as another tensor.\n \n     # Arguments\n         x: Keras variable or Keras tensor.\n@@ -604,8 +603,7 @@ def zeros_like(x, dtype=None, name=None):\n \n \n def ones_like(x, dtype=None, name=None):\n-    \"\"\"Instantiates an all-ones Keras variable\n-    of the same shape as another Keras variable or tensor and returns it.\n+    \"\"\"Instantiates an all-ones variable of the same shape as another tensor.\n \n     # Arguments\n         x: Keras variable or tensor.\n@@ -630,8 +628,7 @@ def ones_like(x, dtype=None, name=None):\n \n def random_uniform_variable(shape, low, high, dtype=None,\n                             name=None, seed=None):\n-    \"\"\"Instantiates an Keras variable filled with\n-    samples drawn from a uniform distribution and returns it.\n+    \"\"\"Instantiates a variable with values drawn from a uniform distribution.\n \n     # Arguments\n         shape: Tuple of integers, shape of returned Keras variable.\n@@ -669,8 +666,7 @@ def random_uniform_variable(shape, low, high, dtype=None,\n \n def random_normal_variable(shape, mean, scale, dtype=None,\n                            name=None, seed=None):\n-    \"\"\"Instantiates an Keras variable filled with\n-    samples drawn from a normal distribution and returns it.\n+    \"\"\"Instantiates a variable with values drawn from a normal distribution.\n \n     # Arguments\n         shape: Tuple of integers, shape of returned Keras variable.\n@@ -776,13 +772,9 @@ def update_sub(x, decrement):\n     return tf.assign_sub(x, decrement)\n \n \n-def moving_average_update(variable, value, momentum):\n-    try:\n+def moving_average_update(x, value, momentum):\n     return moving_averages.assign_moving_average(\n-            variable, value, momentum, zero_debias=False)\n-    except TypeError:\n-        return moving_averages.assign_moving_average(\n-            variable, value, momentum)\n+        x, value, momentum, zero_debias=False)\n \n \n # LINEAR ALGEBRA\n@@ -1183,8 +1175,7 @@ def argmax(x, axis=-1):\n     # Returns\n         A tensor.\n     \"\"\"\n-    if axis < 0:\n-        axis = axis % len(x.get_shape())\n+    axis = _normalize_axis(axis, ndim(x))\n     return tf.argmax(x, axis)\n \n \n@@ -1198,8 +1189,7 @@ def argmin(x, axis=-1):\n     # Returns\n         A tensor.\n     \"\"\"\n-    if axis < 0:\n-        axis = axis % len(x.get_shape())\n+    axis = _normalize_axis(axis, ndim(x))\n     return tf.argmin(x, axis)\n \n \n@@ -1495,8 +1485,9 @@ def normalize_batch_in_training(x, gamma, beta,\n \n \n def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n-    \"\"\"Applies batch normalization on x given mean, var, beta and gamma:\n+    \"\"\"Applies batch normalization on x given mean, var, beta and gamma.\n \n+    I.e. returns:\n     `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n \n     # Arguments\n@@ -1670,7 +1661,7 @@ def repeat_elements(x, rep, axis):\n     except TypeError:\n         splits = tf.split(value=x, num_split=x_shape[axis], split_dim=axis)\n     # repeat each slice the given number of reps\n-    x_rep = [s for s in splits for i in range(rep)]\n+    x_rep = [s for s in splits for _ in range(rep)]\n     return concatenate(x_rep, axis)\n \n \n@@ -1694,7 +1685,7 @@ def repeat(x, n):\n \n \n def arange(start, stop=None, step=1, dtype='int32'):\n-    \"\"\"Creates a 1-D tensor containing a sequence of integers.\n+    \"\"\"Creates a 1D tensor containing a sequence of integers.\n \n     The function arguments use the same convention as\n     Theano's arange: if only one argument is provided,\n@@ -1704,8 +1695,13 @@ def arange(start, stop=None, step=1, dtype='int32'):\n     match TensorFlow's default.\n \n     # Arguments\n+        start: Start value.\n+        stop: Stop value.\n+        step: Difference between two successive values.\n+        dtype: Integer dtype to use.\n \n     # Returns\n+        An integer tensor.\n \n     \"\"\"\n     # Match the behavior of numpy and Theano by returning an empty seqence.\n@@ -2046,6 +2042,13 @@ def print_tensor(x, message=''):\n # GRAPH MANIPULATION\n \n class Function(object):\n+    \"\"\"Runs a computation graph.\n+\n+    # Arguments\n+        inputs: Feed placeholders to the computation graph.\n+        outputs: Output tensors to fetch.\n+        updates: Additional update ops to be run at function call.\n+    \"\"\"\n \n     def __init__(self, inputs, outputs, updates=[]):\n         if not isinstance(inputs, (list, tuple)):\n@@ -2091,11 +2094,15 @@ def function(inputs, outputs, updates=[], **kwargs):\n     \"\"\"Instantiates a Keras function.\n \n     # Arguments\n-        inputs: list of placeholder/variable tensors.\n-        outputs: list of output tensors.\n-        updates: list of update tuples (old_tensor, new_tensor).\n+        inputs: List of placeholder tensors.\n+        outputs: List of output tensors.\n+        updates: List of update ops.\n+        **kwargs: Not used with TensorFlow.\n+\n+    # Returns\n+        Output values as Numpy arrays.\n     \"\"\"\n-    if len(kwargs) > 0:\n+    if kwargs:\n         msg = [\n             'Expected no kwargs, you passed %s' % len(kwargs),\n             'kwargs passed to function are ignored with Tensorflow backend'\n@@ -2105,15 +2112,26 @@ def function(inputs, outputs, updates=[], **kwargs):\n \n \n def gradients(loss, variables):\n-    \"\"\"Returns the gradients of `variables` (list of tensor variables)\n-    with regard to `loss`.\n+    \"\"\"Returns the gradients of `variables` w.r.t. `loss`.\n+\n+    # Arguments\n+        loss: Scalar tensor to minimize.\n+        variables: List of variables.\n+\n+    # Returns\n+        A gradients tensor.\n     \"\"\"\n     return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n \n \n def stop_gradient(variables):\n-    \"\"\"Returns `variables` but with zero gradient with respect to every other\n-    variables.\n+    \"\"\"Returns `variables` but with zero gradient w.r.t. every other variable.\n+\n+    # Arguments\n+        variables: List of variables.\n+\n+    # Returns\n+        The same list of variables.\n     \"\"\"\n     return tf.stop_gradient(variables)\n \n@@ -2202,8 +2220,8 @@ def rnn(step_function, inputs, initial_states,\n             if go_backwards:\n                 mask_list.reverse()\n \n-            for input, mask_t in zip(input_list, mask_list):\n-                output, new_states = step_function(input, states + constants)\n+            for inp, mask_t in zip(input_list, mask_list):\n+                output, new_states = step_function(inp, states + constants)\n \n                 # tf.where needs its condition tensor\n                 # to be the same shape as its two\n@@ -2218,7 +2236,7 @@ def rnn(step_function, inputs, initial_states,\n                 tiled_mask_t = tf.tile(mask_t,\n                                        tf.stack([1, tf.shape(output)[1]]))\n \n-                if len(successive_outputs) == 0:\n+                if not successive_outputs:\n                     prev_output = zeros_like(output)\n                 else:\n                     prev_output = successive_outputs[-1]\n@@ -2240,8 +2258,8 @@ def rnn(step_function, inputs, initial_states,\n                 new_states = successive_states[-1]\n                 outputs = tf.stack(successive_outputs)\n         else:\n-            for input in input_list:\n-                output, states = step_function(input, states + constants)\n+            for inp in input_list:\n+                output, states = step_function(inp, states + constants)\n                 successive_outputs.append(output)\n                 successive_states.append(states)\n             last_output = successive_outputs[-1]\n@@ -2267,7 +2285,7 @@ def rnn(step_function, inputs, initial_states,\n         time = tf.constant(0, dtype='int32', name='time')\n \n         if mask is not None:\n-            if len(states) == 0:\n+            if not states:\n                 raise ValueError('No initial states provided! '\n                                  'When using masking in an RNN, you should '\n                                  'provide initial states '\n@@ -2284,6 +2302,16 @@ def rnn(step_function, inputs, initial_states,\n             mask_ta = mask_ta.unstack(mask)\n \n             def _step(time, output_ta_t, *states):\n+                \"\"\"RNN step function.\n+\n+                # Arguments\n+                    time: Current timestep value.\n+                    output_ta_t: TensorArray.\n+                    *states: List of states.\n+\n+                # Returns\n+                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n+                \"\"\"\n                 current_input = input_ta.read(time)\n                 mask_t = mask_ta.read(time)\n                 output, new_states = step_function(current_input,\n@@ -2299,6 +2327,16 @@ def rnn(step_function, inputs, initial_states,\n                 return (time + 1, output_ta_t) + tuple(new_states)\n         else:\n             def _step(time, output_ta_t, *states):\n+                \"\"\"RNN step function.\n+\n+                # Arguments\n+                    time: Current timestep value.\n+                    output_ta_t: TensorArray.\n+                    *states: List of states.\n+\n+                # Returns\n+                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n+                \"\"\"\n                 current_input = input_ta.read(time)\n                 output, new_states = step_function(current_input,\n                                                    tuple(states) +\n@@ -3180,7 +3218,15 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n \n def ctc_label_dense_to_sparse(labels, label_lengths):\n-    from tensorflow.python.ops import functional_ops\n+    \"\"\"Converts CTC labels from dense to sparse.\n+\n+    # Arguments\n+        labels: dense CTC labels.\n+        label_lengths: length of the labels.\n+\n+    # Returns\n+        A sparse tensor representation of the lablels.\n+    \"\"\"\n     label_shape = tf.shape(labels)\n     num_batches_tns = tf.stack([label_shape[0]])\n     max_num_labels_tns = tf.stack([label_shape[1]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e25f8b1c28263ebbb16b662c1f73ed212204634", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 12 | Methods Changed: 11 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 30 | Churn Cumulative: 7285 | Contributors (this commit): 61 | Commits (past 90d): 58 | Contributors (cumulative): 61 | DMM Complexity: 1.0\n\nDIFF:\n@@ -93,6 +93,9 @@ def learning_phase():\n     The learning phase flag is a bool tensor (0 = test, 1 = train)\n     to be passed as input to any Keras function\n     that uses a different behavior at train time and test time.\n+\n+    # Returns\n+        Learning phase (scalar integer tensor or Python integer).\n     \"\"\"\n     graph = tf.get_default_graph()\n     if graph not in _GRAPH_LEARNING_PHASES:\n@@ -103,8 +106,10 @@ def learning_phase():\n \n \n def set_learning_phase(value):\n-    \"\"\"Sets the learning phase to a fixed value,\n-    either 0 or 1 (integers).\n+    \"\"\"Sets the learning phase to a fixed value.\n+\n+    # Arguments\n+        value: Learning phase value, either 0 or 1 (integers).\n \n     # Raises\n         ValueError: if `value` is neither `0` nor `1`.\n@@ -151,7 +156,10 @@ def get_session():\n \n \n def set_session(session):\n-    \"\"\"Sets the global TF session.\n+    \"\"\"Sets the global TensorFlow session.\n+\n+    # Arguments\n+        session: A TF Session.\n     \"\"\"\n     global _SESSION\n     _SESSION = session\n@@ -327,6 +335,7 @@ def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n             At least one of {`shape`, `ndim`} must be specified.\n             If both are specified, `shape` is used.\n         dtype: Placeholder type.\n+        sparse: Boolean, whether the placeholder should have a sparse type.\n         name: Optional name string for the placeholder.\n \n     # Returns\n@@ -585,6 +594,7 @@ def zeros_like(x, dtype=None, name=None):\n         x: Keras variable or Keras tensor.\n         dtype: String, dtype of returned Keras variable.\n              None uses the dtype of x.\n+        name: String, name for the variable to create.\n \n     # Returns\n         A Keras variable with the shape of x filled with zeros.\n@@ -609,6 +619,7 @@ def ones_like(x, dtype=None, name=None):\n         x: Keras variable or tensor.\n         dtype: String, dtype of returned Keras variable.\n              None uses the dtype of x.\n+        name: String, name for the variable to create.\n \n     # Returns\n         A Keras variable with the shape of x filled with ones.\n@@ -1494,8 +1505,8 @@ def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n         x: Input tensor or variable.\n         mean: Mean of batch.\n         var: Variance of batch.\n-        gamma: Tensor by which to scale the input.\n         beta: Tensor with which to center the input.\n+        gamma: Tensor by which to scale the input.\n         epsilon: Fuzz factor.\n \n     # Returns\n@@ -1517,9 +1528,9 @@ def concatenate(tensors, axis=-1):\n         A tensor.\n     \"\"\"\n     if axis < 0:\n-        dims = ndim(tensors[0])\n-        if dims:\n-            axis = axis % dims\n+        rank = ndim(tensors[0])\n+        if rank:\n+            axis %= rank\n         else:\n             axis = 0\n \n@@ -2090,7 +2101,7 @@ class Function(object):\n         return updated[:len(self.outputs)]\n \n \n-def function(inputs, outputs, updates=[], **kwargs):\n+def function(inputs, outputs, updates=None, **kwargs):\n     \"\"\"Instantiates a Keras function.\n \n     # Arguments\n@@ -2102,6 +2113,7 @@ def function(inputs, outputs, updates=[], **kwargs):\n     # Returns\n         Output values as Numpy arrays.\n     \"\"\"\n+    updates = updates or []\n     if kwargs:\n         msg = [\n             'Expected no kwargs, you passed %s' % len(kwargs),\n@@ -2146,7 +2158,7 @@ def rnn(step_function, inputs, initial_states,\n     # Arguments\n         inputs: tensor of temporal data of shape `(samples, time, ...)`\n             (at least 3D).\n-        step_function:\n+        step_function: RNN step function.\n             Parameters:\n                 input: tensor with shape `(samples, ...)` (no time dimension),\n                     representing input for the batch of samples at a certain\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0f308d4b72c420168d8c5fd3c834c61513a958fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 7293 | Contributors (this commit): 61 | Commits (past 90d): 59 | Contributors (cumulative): 61 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2061,7 +2061,8 @@ class Function(object):\n         updates: Additional update ops to be run at function call.\n     \"\"\"\n \n-    def __init__(self, inputs, outputs, updates=[]):\n+    def __init__(self, inputs, outputs, updates=None):\n+        updates = updates or []\n         if not isinstance(inputs, (list, tuple)):\n             raise TypeError('`inputs` to a TensorFlow backend function '\n                             'should be a list or tuple.')\n@@ -2113,7 +2114,6 @@ def function(inputs, outputs, updates=None, **kwargs):\n     # Returns\n         Output values as Numpy arrays.\n     \"\"\"\n-    updates = updates or []\n     if kwargs:\n         msg = [\n             'Expected no kwargs, you passed %s' % len(kwargs),\n@@ -2156,8 +2156,6 @@ def rnn(step_function, inputs, initial_states,\n     \"\"\"Iterates over the time dimension of a tensor.\n \n     # Arguments\n-        inputs: tensor of temporal data of shape `(samples, time, ...)`\n-            (at least 3D).\n         step_function: RNN step function.\n             Parameters:\n                 input: tensor with shape `(samples, ...)` (no time dimension),\n@@ -2170,6 +2168,8 @@ def rnn(step_function, inputs, initial_states,\n                 new_states: list of tensors, same length and shapes\n                     as 'states'. The first state in the list must be the\n                     output tensor at the previous timestep.\n+        inputs: tensor of temporal data of shape `(samples, time, ...)`\n+            (at least 3D).\n         initial_states: tensor with shape (samples, output_dim)\n             (no time dimension),\n             containing the initial values for the states used in\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d046cea1a1d32372cdf27a1c947969c9d09bbde8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 348 | Lines Deleted: 86 | Files Changed: 5 | Hunks: 53 | Methods Changed: 20 | Complexity Δ (Sum/Max): 31/27 | Churn Δ: 434 | Churn Cumulative: 20848 | Contributors (this commit): 98 | Commits (past 90d): 105 | Contributors (cumulative): 121 | DMM Complexity: 0.6082474226804123\n\nDIFF:\n@@ -1276,11 +1276,13 @@ class InputLayer(Layer):\n         self.dtype = dtype\n \n         if input_tensor is None:\n+            self.is_placeholder = True\n             input_tensor = K.placeholder(shape=batch_input_shape,\n                                          dtype=dtype,\n                                          sparse=self.sparse,\n                                          name=self.name)\n         else:\n+            self.is_placeholder = False\n             input_tensor._keras_shape = batch_input_shape\n         # Create an input node to add to self.outbound_node\n         # and set output_tensors' _keras_history.\n@@ -1539,8 +1541,15 @@ class Container(Layer):\n         # Build self.input_names and self.output_names.\n         self.input_names = []\n         self.output_names = []\n-        for layer in self.input_layers:\n+        self._feed_input_names = []\n+        self._feed_inputs = []\n+        self._feed_input_shapes = []\n+        for i, layer in enumerate(self.input_layers):\n             self.input_names.append(layer.name)\n+            if layer.is_placeholder:\n+                self._feed_input_names.append(layer.name)\n+                self._feed_inputs.append(layer.input)\n+                self._feed_input_shapes.append(self.inputs[i]._keras_shape)\n         for layer in self.output_layers:\n             self.output_names.append(layer.name)\n \n\n@@ -49,6 +49,8 @@ def _standardize_input_data(data, names, shapes=None,\n     # Raises\n         ValueError: in case of improperly formatted user-provided data.\n     \"\"\"\n+    if data is None:\n+        return [None for _ in range(len(names))]\n     if isinstance(data, dict):\n         arrays = []\n         for name in names:\n@@ -205,26 +207,29 @@ def _check_array_lengths(inputs, targets, weights):\n     y_lengths = [y.shape[0] for y in targets]\n     w_lengths = [w.shape[0] for w in weights]\n     set_x = set(x_lengths)\n-    if len(set_x) != 1:\n+    if len(set_x) > 1:\n         raise ValueError('All input arrays (x) should have '\n-                         'the same number of samples.')\n+                         'the same number of samples. Got array shapes: ' +\n+                         str([x.shape for x in inputs]))\n     set_y = set(y_lengths)\n-    if len(set_y) != 1:\n+    if len(set_y) > 1:\n         raise ValueError('All target arrays (y) should have '\n-                         'the same number of samples.')\n+                         'the same number of samples. Got array shapes: ' +\n+                         str([y.shape for y in targets]))\n     set_w = set(w_lengths)\n-    if len(set_w) != 1:\n+    if len(set_w) > 1:\n         raise ValueError('All sample_weight arrays should have '\n-                         'the same number of samples.')\n-    if list(set_x)[0] != list(set_y)[0]:\n+                         'the same number of samples. Got array shapes: ' +\n+                         str([w.shape for w in weights]))\n+    if set_x and set_y and list(set_x)[0] != list(set_y)[0]:\n         raise ValueError('Input arrays should have '\n                          'the same number of samples as target arrays. '\n                          'Found ' + str(list(set_x)[0]) + ' input samples '\n                          'and ' + str(list(set_y)[0]) + ' target samples.')\n-    if list(set_x)[0] != list(set_w)[0]:\n+    if set_y and set_w and list(set_y)[0] != list(set_w)[0]:\n         raise ValueError('Sample_weight arrays should have '\n-                         'the same number of samples as input arrays. Found ' +\n-                         str(list(set_x)[0]) + ' input samples and ' +\n+                         'the same number of samples as target arrays. Got ' +\n+                         str(list(set_y)[0]) + ' input samples and ' +\n                          str(list(set_w)[0]) + ' target samples.')\n \n \n@@ -246,6 +251,8 @@ def _check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n                   'binary_crossentropy',\n                   'categorical_crossentropy'}\n     for y, loss, shape in zip(targets, loss_fns, output_shapes):\n+        if loss is None:\n+            continue\n         if loss.__name__ == 'categorical_crossentropy':\n             if y.shape[-1] == 1:\n                 raise ValueError(\n@@ -403,6 +410,9 @@ def _weighted_masked_objective(fn):\n     # Returns\n         A function with signature `fn(y_true, y_pred, weights, mask)`.\n     \"\"\"\n+    if fn is None:\n+        return None\n+\n     def weighted(y_true, y_pred, weights, mask=None):\n         \"\"\"Wrapper function.\n \n@@ -685,11 +695,60 @@ class Model(Container):\n             ValueError: In case of invalid arguments for\n                 `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n         \"\"\"\n+        loss = loss or {}\n         self.optimizer = optimizers.get(optimizer)\n         self.sample_weight_mode = sample_weight_mode\n         self.loss = loss\n         self.loss_weights = loss_weights\n \n+        # Prepare loss functions.\n+        if isinstance(loss, dict):\n+            for name in loss:\n+                if name not in self.output_names:\n+                    raise ValueError('Unknown entry in loss '\n+                                     'dictionary: \"' + name + '\". '\n+                                     'Only expected the following keys: ' +\n+                                     str(self.output_names))\n+            loss_functions = []\n+            for name in self.output_names:\n+                if name not in loss:\n+                    warnings.warn('Output \"' + name +\n+                                  '\" missing from loss dictionary. '\n+                                  'We assume this was done on purpose, '\n+                                  'and we will not be expecting '\n+                                  'any data to be passed to \"' + name +\n+                                  '\" during training.')\n+                loss_functions.append(losses.get(loss.get(name)))\n+        elif isinstance(loss, list):\n+            if len(loss) != len(self.outputs):\n+                raise ValueError('When passing a list as loss, '\n+                                 'it should have one entry per model outputs. '\n+                                 'The model has ' + str(len(self.outputs)) +\n+                                 ' outputs, but you passed loss=' +\n+                                 str(loss))\n+            loss_functions = [losses.get(l) for l in loss]\n+        else:\n+            loss_function = losses.get(loss)\n+            loss_functions = [loss_function for _ in range(len(self.outputs))]\n+        self.loss_functions = loss_functions\n+        weighted_losses = [_weighted_masked_objective(fn) for fn in loss_functions]\n+        skip_indices = []\n+        self._feed_outputs = []\n+        self._feed_output_names = []\n+        for i in range(len(weighted_losses)):\n+            if weighted_losses[i] is None:\n+                skip_indices.append(i)\n+            else:\n+                self._feed_outputs.append(self.outputs[i])\n+                self._feed_output_names.append(self.output_names[i])\n+\n+        # Prepare output masks.\n+        masks = self.compute_mask(self.inputs, mask=None)\n+        if masks is None:\n+            masks = [None for _ in self.outputs]\n+        if not isinstance(masks, list):\n+            masks = [masks]\n+\n         # Prepare loss weights.\n         if loss_weights is None:\n             loss_weights_list = [1. for _ in range(len(self.outputs))]\n@@ -716,42 +775,9 @@ class Model(Container):\n                             str(loss_weights) +\n                             ' - expected a list of dicts.')\n \n-        # Prepare loss functions.\n-        if isinstance(loss, dict):\n-            for name in loss:\n-                if name not in self.output_names:\n-                    raise ValueError('Unknown entry in loss '\n-                                     'dictionary: \"' + name + '\". '\n-                                     'Only expected the following keys: ' +\n-                                     str(self.output_names))\n-            loss_functions = []\n-            for name in self.output_names:\n-                if name not in loss:\n-                    raise ValueError('Output \"' + name +\n-                                     '\" missing from loss dictionary.')\n-                loss_functions.append(losses.get(loss[name]))\n-        elif isinstance(loss, list):\n-            if len(loss) != len(self.outputs):\n-                raise ValueError('When passing a list as loss, '\n-                                 'it should have one entry per model outputs. '\n-                                 'The model has ' + str(len(self.outputs)) +\n-                                 ' outputs, but you passed loss=' +\n-                                 str(loss))\n-            loss_functions = [losses.get(l) for l in loss]\n-        else:\n-            loss_function = losses.get(loss)\n-            loss_functions = [loss_function for _ in range(len(self.outputs))]\n-        self.loss_functions = loss_functions\n-        weighted_losses = [_weighted_masked_objective(fn) for fn in loss_functions]\n-\n-        # Prepare output masks.\n-        masks = self.compute_mask(self.inputs, mask=None)\n-        if masks is None:\n-            masks = [None for _ in self.outputs]\n-        if not isinstance(masks, list):\n-            masks = [masks]\n-\n         # Prepare sample weights.\n+        sample_weights = []\n+        sample_weight_modes = []\n         if isinstance(sample_weight_mode, dict):\n             for name in sample_weight_mode:\n                 if name not in self.output_names:\n@@ -760,9 +786,11 @@ class Model(Container):\n                                      name + '\". '\n                                      'Only expected the following keys: ' +\n                                      str(self.output_names))\n-            sample_weights = []\n-            sample_weight_modes = []\n-            for name in self.output_names:\n+            for i, name in enumerate(self.output_names):\n+                if i in skip_indices:\n+                    weight = None\n+                    sample_weight_modes.append(None)\n+                else:\n                     if name not in sample_weight_mode:\n                         raise ValueError('Output \"' + name +\n                                          '\" missing from sample_weight_modes '\n@@ -784,9 +812,13 @@ class Model(Container):\n                                  ' outputs, but you passed '\n                                  'sample_weight_mode=' +\n                                  str(sample_weight_mode))\n-            sample_weights = []\n-            sample_weight_modes = []\n-            for mode, name in zip(sample_weight_mode, self.output_names):\n+            for i in range(len(self.output_names)):\n+                if i in skip_indices:\n+                    weight = None\n+                    sample_weight_modes.append(None)\n+                else:\n+                    mode = sample_weight_mode[i]\n+                    name = self.output_names[i]\n                     if mode == 'temporal':\n                         weight = K.placeholder(ndim=2,\n                                                name=name + '_sample_weights')\n@@ -797,28 +829,39 @@ class Model(Container):\n                         sample_weight_modes.append(None)\n                 sample_weights.append(weight)\n         else:\n-            if sample_weight_mode == 'temporal':\n-                sample_weights = [K.placeholder(ndim=2,\n-                                                name=name + '_sample_weights')\n-                                  for name in self.output_names]\n-                sample_weight_modes = ['temporal'\n-                                       for name in self.output_names]\n+            for i, name in enumerate(self.output_names):\n+                if i in skip_indices:\n+                    sample_weight_modes.append(None)\n+                    sample_weights.append(None)\n                 else:\n-                sample_weights = [K.placeholder(ndim=1,\n-                                                name=name + '_sample_weights')\n-                                  for name in self.output_names]\n-                sample_weight_modes = [None for name in self.output_names]\n+                    if sample_weight_mode == 'temporal':\n+                        sample_weights.append(\n+                            K.placeholder(ndim=2,\n+                                          name=name + '_sample_weights'))\n+                        sample_weight_modes.append('temporal')\n+                    else:\n+                        sample_weights.append(\n+                            K.placeholder(ndim=1,\n+                                          name=name + '_sample_weights'))\n+                        sample_weight_modes.append(None)\n         self.sample_weight_modes = sample_weight_modes\n \n         # Prepare targets of model.\n         self.targets = []\n+        self._feed_targets = []\n         for i in range(len(self.outputs)):\n+            if i in skip_indices:\n+                self.targets.append(None)\n+            else:\n                 shape = self.internal_output_shapes[i]\n                 name = self.output_names[i]\n-            self.targets.append(K.placeholder(ndim=len(shape),\n+                target = K.placeholder(ndim=len(shape),\n                                        name=name + '_target',\n                                        sparse=K.is_sparse(self.outputs[i]),\n-                                              dtype=K.dtype(self.outputs[i])))\n+                                       dtype=K.dtype(self.outputs[i]))\n+                self.targets.append(target)\n+                self._feed_targets.append(target)\n+\n         # Prepare metrics.\n         self.metrics = metrics\n         self.metrics_names = ['loss']\n@@ -827,6 +870,8 @@ class Model(Container):\n         # Compute total loss.\n         total_loss = None\n         for i in range(len(self.outputs)):\n+            if i in skip_indices:\n+                continue\n             y_true = self.targets[i]\n             y_pred = self.outputs[i]\n             weighted_loss = weighted_losses[i]\n@@ -842,6 +887,12 @@ class Model(Container):\n                 total_loss = loss_weight * output_loss\n             else:\n                 total_loss += loss_weight * output_loss\n+        if total_loss is None:\n+            if not self.losses:\n+                raise RuntimeError('The model cannot be compiled '\n+                                   'because it has no loss to optimize.')\n+            else:\n+                total_loss = 0.\n \n         # Add regularization penalties\n         # and other layer-specific losses.\n@@ -860,10 +911,11 @@ class Model(Container):\n             self.metrics_tensors.append(metric_tensor)\n \n         for i in range(len(self.outputs)):\n+            if i in skip_indices:\n+                continue\n             y_true = self.targets[i]\n             y_pred = self.outputs[i]\n             output_metrics = nested_metrics[i]\n-\n             for metric in output_metrics:\n                 if metric == 'accuracy' or metric == 'acc':\n                     # custom handling of accuracy\n@@ -894,6 +946,10 @@ class Model(Container):\n         # Prepare gradient updates and state updates.\n         self.total_loss = total_loss\n         self.sample_weights = sample_weights\n+        self._feed_sample_weights = []\n+        for i in range(len(self.sample_weights)):\n+            if i not in skip_indices:\n+                self._feed_sample_weights.append(sample_weights[i])\n \n         # Functions for train, test and predict will\n         # be compiled lazily when required.\n@@ -914,11 +970,20 @@ class Model(Container):\n                 trainable_weights.sort(key=lambda x: x.name)\n         self._collected_trainable_weights = trainable_weights\n \n+        self._feed_sample_weight_modes = []\n+        self._feed_output_shapes = []\n+        self._feed_loss_fns = []\n+        for i in range(len(self.outputs)):\n+            if i not in skip_indices:\n+                self._feed_sample_weight_modes.append(self.sample_weight_modes[i])\n+                self._feed_output_shapes.append(self.internal_output_shapes[i])\n+                self._feed_loss_fns.append(self.loss_functions[i])\n+\n     def _make_train_function(self):\n         if not hasattr(self, 'train_function'):\n             raise RuntimeError('You must compile your model before using it.')\n         if self.train_function is None:\n-            inputs = self.inputs + self.targets + self.sample_weights\n+            inputs = self._feed_inputs + self._feed_targets + self._feed_sample_weights\n             if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n                 inputs += [K.learning_phase()]\n \n@@ -937,7 +1002,7 @@ class Model(Container):\n         if not hasattr(self, 'test_function'):\n             raise RuntimeError('You must compile your model before using it.')\n         if self.test_function is None:\n-            inputs = self.inputs + self.targets + self.sample_weights\n+            inputs = self._feed_inputs + self._feed_targets + self._feed_sample_weights\n             if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n                 inputs += [K.learning_phase()]\n             # Return loss and metrics, no gradient updates.\n@@ -952,9 +1017,9 @@ class Model(Container):\n             self.predict_function = None\n         if self.predict_function is None:\n             if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n-                inputs = self.inputs + [K.learning_phase()]\n+                inputs = self._feed_inputs + [K.learning_phase()]\n             else:\n-                inputs = self.inputs\n+                inputs = self._feed_inputs\n             # Gets network outputs. Does not update weights.\n             # Does update the network states.\n             kwargs = getattr(self, '_function_kwargs', {})\n@@ -1096,7 +1161,15 @@ class Model(Container):\n             or list of arrays of predictions\n             (if the model has multiple outputs).\n         \"\"\"\n+        if ins and hasattr(ins[0], 'shape'):\n             samples = ins[0].shape[0]\n+        else:\n+            # May happen if we are running `predict` without Numpy input data,\n+            # i.e. if all inputs to the models are data tensors\n+            # instead of placeholders.\n+            # In that case we will run `predict` over a single batch.\n+            samples = batch_size\n+            verbose = 2\n         outs = []\n         if verbose == 1:\n             progbar = Progbar(target=samples)\n@@ -1104,7 +1177,7 @@ class Model(Container):\n         index_array = np.arange(samples)\n         for batch_index, (batch_start, batch_end) in enumerate(batches):\n             batch_ids = index_array[batch_start:batch_end]\n-            if isinstance(ins[-1], float):\n+            if ins and isinstance(ins[-1], float):\n                 # do not slice the training phase flag\n                 ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n             else:\n@@ -1185,31 +1258,33 @@ class Model(Container):\n \n         output_shapes = []\n         for output_shape, loss_fn in zip(self.internal_output_shapes, self.loss_functions):\n-            if loss_fn.__name__ == 'sparse_categorical_crossentropy':\n+            if loss_fn is None:\n+                continue\n+            elif loss_fn.__name__ == 'sparse_categorical_crossentropy':\n                 output_shapes.append(output_shape[:-1] + (1,))\n             elif getattr(losses, loss_fn.__name__, None) is None:\n                 output_shapes.append(None)\n             else:\n                 output_shapes.append(output_shape)\n-        x = _standardize_input_data(x, self.input_names,\n-                                    self.internal_input_shapes,\n+        x = _standardize_input_data(x, self._feed_input_names,\n+                                    self._feed_input_shapes,\n                                     check_batch_axis=False,\n                                     exception_prefix='model input')\n-        y = _standardize_input_data(y, self.output_names,\n-                                    output_shapes,\n+        y = _standardize_input_data(y, self._feed_output_names,\n+                                    self._feed_output_shapes,\n                                     check_batch_axis=False,\n                                     exception_prefix='model target')\n         sample_weights = _standardize_sample_weights(sample_weight,\n-                                                     self.output_names)\n+                                                     self._feed_output_names)\n         class_weights = _standardize_class_weights(class_weight,\n-                                                   self.output_names)\n+                                                   self._feed_output_names)\n         sample_weights = [_standardize_weights(ref, sw, cw, mode)\n                           for (ref, sw, cw, mode)\n-                          in zip(y, sample_weights, class_weights, self.sample_weight_modes)]\n+                          in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n         _check_array_lengths(x, y, sample_weights)\n         _check_loss_and_target_compatibility(y,\n-                                             self.loss_functions,\n-                                             self.internal_output_shapes)\n+                                             self._feed_loss_fns,\n+                                             self._feed_output_shapes)\n         if self.stateful and batch_size:\n             if x[0].shape[0] % batch_size != 0:\n                 raise ValueError('In a stateful network, '\n@@ -1433,8 +1508,8 @@ class Model(Container):\n                 that is not a multiple of the batch size.\n         \"\"\"\n         # validate user data\n-        x = _standardize_input_data(x, self.input_names,\n-                                    self.internal_input_shapes,\n+        x = _standardize_input_data(x, self._feed_input_names,\n+                                    self._feed_input_shapes,\n                                     check_batch_axis=False)\n         if self.stateful:\n             if x[0].shape[0] > batch_size and x[0].shape[0] % batch_size != 0:\n@@ -1557,8 +1632,8 @@ class Model(Container):\n         # Returns\n             Numpy array(s) of predictions.\n         \"\"\"\n-        x = _standardize_input_data(x, self.input_names,\n-                                    self.internal_input_shapes)\n+        x = _standardize_input_data(x, self._feed_input_names,\n+                                    self._feed_input_shapes)\n         if self.uses_learning_phase and not isinstance(K.learning_phase, int):\n             ins = x + [0.]\n         else:\n\n@@ -83,6 +83,8 @@ def deserialize(name, custom_objects=None):\n \n \n def get(identifier):\n+    if identifier is None:\n+        return None\n     if isinstance(identifier, six.string_types):\n         identifier = str(identifier)\n         return deserialize(identifier)\n\n@@ -526,6 +526,8 @@ class Sequential(Model):\n         self.container_nodes = self.model.container_nodes\n         self.output_names = self.model.output_names\n         self.input_names = self.model.input_names\n+        self._feed_input_names = self.model._feed_input_names\n+        self._feed_inputs = self.model._feed_inputs\n \n         # Make sure child model callbacks\n         # will call the parent Sequential model.\n\n@@ -232,7 +232,7 @@ def test_check_last_is_one():\n     with pytest.raises(Exception) as exc:\n         _check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [a.shape])\n \n-    assert \"You are passing a target array\" in str(exc)\n+    assert 'You are passing a target array' in str(exc)\n \n \n @keras_test\n@@ -241,7 +241,181 @@ def test_check_bad_shape():\n     with pytest.raises(Exception) as exc:\n         _check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [(2, 3, 6)])\n \n-    assert \"targets to have the same shape\" in str(exc)\n+    assert 'targets to have the same shape' in str(exc)\n+\n+\n+@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n+@keras_test\n+def test_model_with_input_feed_tensor():\n+    \"\"\"We test building a model with a TF variable as input.\n+    We should be able to call fit, evaluate, predict,\n+    by only passing them data for the placeholder inputs\n+    in the model.\n+    \"\"\"\n+    import tensorflow as tf\n+\n+    input_a_np = np.random.random((10, 3))\n+    input_b_np = np.random.random((10, 3))\n+\n+    output_a_np = np.random.random((10, 4))\n+    output_b_np = np.random.random((10, 3))\n+\n+    a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n+    b = Input(shape=(3,), name='input_b')\n+\n+    a_2 = Dense(4, name='dense_1')(a)\n+    dp = Dropout(0.5, name='dropout')\n+    b_2 = dp(b)\n+\n+    model = Model([a, b], [a_2, b_2])\n+    model.summary()\n+\n+    optimizer = 'rmsprop'\n+    loss = 'mse'\n+    loss_weights = [1., 0.5]\n+    model.compile(optimizer, loss, metrics=['mean_squared_error'],\n+                  loss_weights=loss_weights,\n+                  sample_weight_mode=None)\n+\n+    # test train_on_batch\n+    out = model.train_on_batch(input_b_np,\n+                               [output_a_np, output_b_np])\n+    out = model.train_on_batch({'input_b': input_b_np},\n+                               [output_a_np, output_b_np])\n+    out = model.test_on_batch({'input_b': input_b_np},\n+                              [output_a_np, output_b_np])\n+    out = model.predict_on_batch({'input_b': input_b_np})\n+\n+    # test fit\n+    out = model.fit({'input_b': input_b_np},\n+                    [output_a_np, output_b_np], epochs=1, batch_size=10)\n+    out = model.fit(input_b_np,\n+                    [output_a_np, output_b_np], epochs=1, batch_size=10)\n+\n+    # test evaluate\n+    out = model.evaluate({'input_b': input_b_np},\n+                         [output_a_np, output_b_np], batch_size=10)\n+    out = model.evaluate(input_b_np,\n+                         [output_a_np, output_b_np], batch_size=10)\n+\n+    # test predict\n+    out = model.predict({'input_b': input_b_np}, batch_size=10)\n+    out = model.predict(input_b_np, batch_size=10)\n+    assert len(out) == 2\n+\n+    # Now test a model with a single input\n+    # i.e. we don't pass any data to fit the model.\n+    a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n+\n+    a_2 = Dense(4, name='dense_1')(a)\n+    a_2 = Dropout(0.5, name='dropout')(a_2)\n+\n+    model = Model(a, a_2)\n+    model.summary()\n+\n+    optimizer = 'rmsprop'\n+    loss = 'mse'\n+    model.compile(optimizer, loss, metrics=['mean_squared_error'])\n+\n+    # test train_on_batch\n+    out = model.train_on_batch(None,\n+                               output_a_np)\n+    out = model.train_on_batch(None,\n+                               output_a_np)\n+    out = model.test_on_batch(None,\n+                              output_a_np)\n+    out = model.predict_on_batch(None)\n+    out = model.train_on_batch([],\n+                               output_a_np)\n+    out = model.train_on_batch({},\n+                               output_a_np)\n+\n+    # test fit\n+    out = model.fit(None,\n+                    output_a_np, epochs=1, batch_size=10)\n+    out = model.fit(None,\n+                    output_a_np, epochs=1, batch_size=10)\n+\n+    # test evaluate\n+    out = model.evaluate(None,\n+                         output_a_np, batch_size=10)\n+    out = model.evaluate(None,\n+                         output_a_np, batch_size=10)\n+\n+    # test predict\n+    out = model.predict(None, batch_size=10)\n+    out = model.predict(None, batch_size=10)\n+    assert out.shape == (10, 4)\n+\n+    # Same, without learning phase\n+    # i.e. we don't pass any data to fit the model.\n+    a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n+\n+    a_2 = Dense(4, name='dense_1')(a)\n+\n+    model = Model(a, a_2)\n+    model.summary()\n+\n+    optimizer = 'rmsprop'\n+    loss = 'mse'\n+    model.compile(optimizer, loss, metrics=['mean_squared_error'])\n+\n+    # test train_on_batch\n+    out = model.train_on_batch(None,\n+                               output_a_np)\n+    out = model.train_on_batch(None,\n+                               output_a_np)\n+    out = model.test_on_batch(None,\n+                              output_a_np)\n+    out = model.predict_on_batch(None)\n+    out = model.train_on_batch([],\n+                               output_a_np)\n+    out = model.train_on_batch({},\n+                               output_a_np)\n+\n+    # test fit\n+    out = model.fit(None,\n+                    output_a_np, epochs=1, batch_size=10)\n+    out = model.fit(None,\n+                    output_a_np, epochs=1, batch_size=10)\n+\n+    # test evaluate\n+    out = model.evaluate(None,\n+                         output_a_np, batch_size=10)\n+    out = model.evaluate(None,\n+                         output_a_np, batch_size=10)\n+\n+    # test predict\n+    out = model.predict(None, batch_size=10)\n+    out = model.predict(None, batch_size=10)\n+    assert out.shape == (10, 4)\n+\n+\n+@keras_test\n+def test_model_with_partial_loss():\n+    a = Input(shape=(3,), name='input_a')\n+\n+    a_2 = Dense(4, name='dense_1')(a)\n+    dp = Dropout(0.5, name='dropout')\n+    a_3 = dp(a_2)\n+\n+    model = Model(a, [a_2, a_3])\n+\n+    optimizer = 'rmsprop'\n+    loss = {'dropout': 'mse'}\n+    model.compile(optimizer, loss, metrics=['mae'],\n+                  sample_weight_mode=None)\n+\n+    input_a_np = np.random.random((10, 3))\n+    output_a_np = np.random.random((10, 4))\n+\n+    # test train_on_batch\n+    out = model.train_on_batch(input_a_np, output_a_np)\n+    out = model.test_on_batch(input_a_np, output_a_np)\n+    # fit\n+    out = model.fit(input_a_np, [output_a_np])\n+    # evaluate\n+    out = model.evaluate(input_a_np, [output_a_np])\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
