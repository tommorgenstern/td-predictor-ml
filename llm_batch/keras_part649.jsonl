{"custom_id": "keras#fd5760fa11f1b5d8e379f097a6410d54092b6c2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 1645 | Contributors (this commit): 10 | Commits (past 90d): 83 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -301,6 +301,15 @@ class Layer(BackendLayer, Operation):\n \n     @utils.default\n     def build(self, input_shape):\n+        if utils.is_default(self.build) and might_have_unbuilt_state(self):\n+            warnings.warn(\n+                f\"`build()` was called on layer '{self.name}', however \"\n+                \"the layer does not have a `build()` method implemented \"\n+                \"and it looks like it has unbuilt state. This will cause \"\n+                \"the layer to be marked as built, despite not being \"\n+                \"actually built, which may cause failures down the line. \"\n+                \"Make sure to implement a proper `build()` method.\"\n+            )\n         self.built = True\n \n     def get_build_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e08f41b3eb0990268415fbec2bd621a9e3a6b95e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 11 | Churn Cumulative: 2232 | Contributors (this commit): 4 | Commits (past 90d): 40 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -39,11 +39,12 @@ class JAXTrainer(base_trainer.Trainer):\n         return loss, (y_pred, non_trainable_variables)\n \n     def _eager_build(self, data_batch):\n+        model_unbuilt = not all(layer.built for layer in self._flatten_layers())\n         compile_metrics_unbuilt = (\n             self._compile_metrics is not None\n             and not self._compile_metrics.built\n         )\n-        if not self.built or compile_metrics_unbuilt:\n+        if model_unbuilt or compile_metrics_unbuilt:\n             # Build the model on one batch of data.\n             (\n                 x,\n@@ -318,7 +319,7 @@ class JAXTrainer(base_trainer.Trainer):\n         )\n \n         needs_building = (\n-            not self.built\n+            not all(layer.built for layer in self._flatten_layers())\n             or not self.optimizer.built\n             or (\n                 self._compile_metrics is not None\n@@ -475,7 +476,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 steps_per_execution=self.steps_per_execution,\n             )\n \n-        if not self.built:\n+        if not all(layer.built for layer in self._flatten_layers()):\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 data_batch = data[0]\n@@ -549,7 +550,7 @@ class JAXTrainer(base_trainer.Trainer):\n             steps_per_execution=self.steps_per_execution,\n         )\n \n-        if not self.built:\n+        if not all(layer.built for layer in self._flatten_layers()):\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 # Build model\n@@ -703,7 +704,7 @@ class JAXTrainer(base_trainer.Trainer):\n         return self._flatten_metrics_in_order(logs)\n \n     def predict_on_batch(self, x):\n-        if not self.built:\n+        if not all(layer.built for layer in self._flatten_layers()):\n             # Build model\n             with backend.StatelessScope():\n                 self(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc99b70b1ec23a634d4defe87f51bc8e30dafb09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 25 | Churn Cumulative: 246 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,8 @@ class TFDatasetAdapter(DataAdapter):\n     @property\n     def num_batches(self):\n         cardinality = int(self._dataset.cardinality())\n-        if cardinality == -1:\n+        # Return None for Unknown and Infiite cardinality datasets\n+        if cardinality < 0:\n             return None\n         return cardinality\n \n\n@@ -77,3 +77,25 @@ class TestTFDatasetAdapter(testing.TestCase):\n \n     def test_class_weights_categorical_targets(self):\n         self._test_class_weights(target_encoding=\"categorical\")\n+\n+    def test_num_batches(self):\n+        dataset = tf.data.Dataset.range(42)\n+        cardinality = int(dataset.cardinality())\n+        self.assertEqual(cardinality, 42)\n+        adapter = tf_dataset_adapter.TFDatasetAdapter(dataset)\n+        self.assertEqual(adapter.num_batches, 42)\n+\n+        # Test for Infiniate Cardinality\n+        dataset = tf.data.Dataset.range(42)\n+        dataset = dataset.repeat()\n+        cardinality = int(dataset.cardinality())\n+        self.assertEqual(cardinality, tf.data.INFINITE_CARDINALITY)\n+        adapter = tf_dataset_adapter.TFDatasetAdapter(dataset)\n+        self.assertIsNone(adapter.num_batches)\n+\n+        # Test for Unknown Cardinality\n+        dataset = dataset.filter(lambda x: True)\n+        cardinality = int(dataset.cardinality())\n+        self.assertEqual(cardinality, tf.data.UNKNOWN_CARDINALITY)\n+        adapter = tf_dataset_adapter.TFDatasetAdapter(dataset)\n+        self.assertIsNone(adapter.num_batches)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#def79d5bde18fdc6e1e72f0743e7018582f42a7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 26 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 30 | Churn Cumulative: 419 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -103,11 +103,12 @@ class ReduceLROnPlateau(Callback):\n \n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}\n-        logs[\"lr\"] = float(np.array(self.model.optimizer.learning_rate))\n+        logs[\"learning_rate\"] = float(\n+            np.array(self.model.optimizer.learning_rate)\n+        )\n         current = logs.get(self.monitor)\n \n         if current is None:\n-            print(\"tacos\")\n             warnings.warn(\n                 \"Learning rate reduction is conditioned on metric \"\n                 f\"`{self.monitor}` which is not available. Available metrics \"\n\n@@ -29,8 +29,7 @@ class TFDatasetAdapter(DataAdapter):\n     @property\n     def num_batches(self):\n         cardinality = int(self._dataset.cardinality())\n-        # Return None for Unknown and Infiite cardinality datasets\n-        if cardinality < 0:\n+        if cardinality == -1:\n             return None\n         return cardinality\n \n\n@@ -77,25 +77,3 @@ class TestTFDatasetAdapter(testing.TestCase):\n \n     def test_class_weights_categorical_targets(self):\n         self._test_class_weights(target_encoding=\"categorical\")\n-\n-    def test_num_batches(self):\n-        dataset = tf.data.Dataset.range(42)\n-        cardinality = int(dataset.cardinality())\n-        self.assertEqual(cardinality, 42)\n-        adapter = tf_dataset_adapter.TFDatasetAdapter(dataset)\n-        self.assertEqual(adapter.num_batches, 42)\n-\n-        # Test for Infiniate Cardinality\n-        dataset = tf.data.Dataset.range(42)\n-        dataset = dataset.repeat()\n-        cardinality = int(dataset.cardinality())\n-        self.assertEqual(cardinality, tf.data.INFINITE_CARDINALITY)\n-        adapter = tf_dataset_adapter.TFDatasetAdapter(dataset)\n-        self.assertIsNone(adapter.num_batches)\n-\n-        # Test for Unknown Cardinality\n-        dataset = dataset.filter(lambda x: True)\n-        cardinality = int(dataset.cardinality())\n-        self.assertEqual(cardinality, tf.data.UNKNOWN_CARDINALITY)\n-        adapter = tf_dataset_adapter.TFDatasetAdapter(dataset)\n-        self.assertIsNone(adapter.num_batches)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#792619d364a2db63aa7ea4e04f67f4e44a36c728", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 25 | Churn Cumulative: 296 | Contributors (this commit): 3 | Commits (past 90d): 16 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,8 @@ class TFDatasetAdapter(DataAdapter):\n     @property\n     def num_batches(self):\n         cardinality = int(self._dataset.cardinality())\n-        if cardinality == -1:\n+        # Return None for Unknown and Infiite cardinality datasets\n+        if cardinality < 0:\n             return None\n         return cardinality\n \n\n@@ -77,3 +77,25 @@ class TestTFDatasetAdapter(testing.TestCase):\n \n     def test_class_weights_categorical_targets(self):\n         self._test_class_weights(target_encoding=\"categorical\")\n+\n+    def test_num_batches(self):\n+        dataset = tf.data.Dataset.range(42)\n+        cardinality = int(dataset.cardinality())\n+        self.assertEqual(cardinality, 42)\n+        adapter = tf_dataset_adapter.TFDatasetAdapter(dataset)\n+        self.assertEqual(adapter.num_batches, 42)\n+\n+        # Test for Infiniate Cardinality\n+        dataset = tf.data.Dataset.range(42)\n+        dataset = dataset.repeat()\n+        cardinality = int(dataset.cardinality())\n+        self.assertEqual(cardinality, tf.data.INFINITE_CARDINALITY)\n+        adapter = tf_dataset_adapter.TFDatasetAdapter(dataset)\n+        self.assertIsNone(adapter.num_batches)\n+\n+        # Test for Unknown Cardinality\n+        dataset = dataset.filter(lambda x: True)\n+        cardinality = int(dataset.cardinality())\n+        self.assertEqual(cardinality, tf.data.UNKNOWN_CARDINALITY)\n+        adapter = tf_dataset_adapter.TFDatasetAdapter(dataset)\n+        self.assertIsNone(adapter.num_batches)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e4860f0536a458a3d5057788f7d9248b68c70bbd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 2 | Churn Cumulative: 210 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -56,7 +56,7 @@ class ZeroPadding1D(Layer):\n         [ 0  0  0]]], shape=(2, 6, 3), dtype=int64)\n \n     Args:\n-        padding: Int, or tuple of int (length 2), or dictionary.\n+        padding: Int, or tuple of int (length 2).\n             - If int:\n             How many zeros to add at the beginning and end of\n             the padding dimension (axis 1).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83e2498b9078244829625c4089e0ae4a67fad295", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 12910 | Contributors (this commit): 118 | Commits (past 90d): 9 | Contributors (cumulative): 118 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2611,10 +2611,6 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n                 # If the train_function is a `tf.function`, we can write out a\n                 # graph\n                 if hasattr(train_fn, \"function_spec\"):\n-                    # TODO(b/243822285): Use _variable_creation_fn directly.\n-                    if hasattr(train_fn, \"_concrete_stateful_fn\"):\n-                        tf.summary.graph(train_fn._concrete_stateful_fn.graph)\n-                    else:\n                     tf.summary.graph(\n                         train_fn._concrete_variable_creation_fn.graph\n                     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2a516c2e4b570552d09a4a53ff4df62dd8b1bb6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 32 | Churn Cumulative: 205 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,12 +11,11 @@ from keras_core import layers\n import keras_core\n import numpy as np\n \n-\n # Model / data parameters\n num_classes = 10\n input_shape = (28, 28, 1)\n learning_rate = 0.01\n-batch_size = 128\n+batch_size = 64\n num_epochs = 1\n \n # Load the data and split it between train and test sets\n@@ -50,40 +49,14 @@ model = keras_core.Sequential(\n ######## Writing a torch training loop for a Keras model ########\n #################################################################\n \n-\n-def get_keras_model():\n-    pass\n-\n-\n-model = get_keras_model()\n-\n # Instantiate the torch optimizer\n+print(\"Num params:\", len(list(model.parameters())))\n optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n \n # Instantiate the torch loss function\n loss_fn = nn.CrossEntropyLoss()\n \n \n-def train_step(data):\n-    x, y = data\n-    y_pred = model(x)\n-    loss = loss_fn(y_pred, y)\n-    optimizer.zero_grad()\n-    loss.backward()\n-    optimizer.step()\n-    return loss\n-\n-\n-# Create a TensorDataset\n-dataset = torch.utils.data.TensorDataset(\n-    torch.from_numpy(x_train), torch.from_numpy(y_train)\n-)\n-\n-for data in dataset:\n-    loss = train_step(data)\n-    print(\"Loss:\", float(loss))\n-\n-\n def train(model, train_loader, num_epochs, optimizer, loss_fn):\n     for epoch in range(num_epochs):\n         running_loss = 0.0\n@@ -126,7 +99,6 @@ train(model, train_loader, num_epochs, optimizer, loss_fn)\n ######## Using a Keras model or layer in a torch Module ########\n ################################################################\n \n-\n class MyModel(nn.Module):\n     def __init__(self):\n         super().__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4fd3ea10fba98a51bd08fdb1b972bc0c68df06c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 676 | Contributors (this commit): 4 | Commits (past 90d): 18 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -201,6 +201,10 @@ class BatchNormalization(Layer):\n             self.moving_mean.assign(\n                 self.moving_mean * self.momentum + mean * (1.0 - self.momentum)\n             )\n+            self.moving_variance.assign(\n+                self.moving_variance * self.momentum\n+                + variance * (1.0 - self.momentum)\n+            )\n         else:\n             moving_mean = ops.reshape(self.moving_mean, broadcast_shape)\n             moving_variance = ops.reshape(self.moving_variance, broadcast_shape)\n\n@@ -61,10 +61,23 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.product(\n         axis=(-1, 1),\n         input_shape=((5, 2, 3), (5, 3, 3, 2)),\n+        moving_mean_initializer=(\"zeros\", \"ones\"),\n+        moving_variance_initializer=(\"zeros\", \"ones\"),\n     )\n-    def test_correctness(self, axis, input_shape):\n+    def test_correctness(\n+        self,\n+        axis,\n+        input_shape,\n+        moving_mean_initializer,\n+        moving_variance_initializer,\n+    ):\n         # Training\n-        layer = layers.BatchNormalization(axis=axis, momentum=0)\n+        layer = layers.BatchNormalization(\n+            axis=axis,\n+            momentum=0,\n+            moving_mean_initializer=moving_mean_initializer,\n+            moving_variance_initializer=moving_variance_initializer,\n+        )\n         # Random data centered on 5.0, variance 10.0\n         x = np.random.normal(loc=5.0, scale=10.0, size=input_shape)\n         out = x\n@@ -83,12 +96,19 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         reduction_axes = tuple(reduction_axes)\n         self.assertAllClose(np.mean(out, axis=reduction_axes), 0.0, atol=1e-3)\n         self.assertAllClose(np.std(out, axis=reduction_axes), 1.0, atol=1e-3)\n+        self.assertAllClose(layer.moving_mean, 0.0, atol=1e-3)\n+        self.assertAllClose(layer.moving_variance, 1.0, atol=1e-3)\n \n-        # Inference\n+        # Inference done before training shouldn't match.\n         inference_out = layer(x, training=False)\n         training_out = layer(x, training=True)\n         self.assertNotAllClose(inference_out, training_out)\n \n+        # Since momentum is zero, inference after training should match.\n+        training_out = layer(x, training=True)\n+        inference_out = layer(x, training=False)\n+        self.assertAllClose(inference_out, training_out)\n+\n     def test_trainable_behavior(self):\n         layer = layers.BatchNormalization(axis=-1, momentum=0.8, epsilon=1e-7)\n         layer.build((1, 4, 4, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d65881b6948c2d46bfd26245ae7510b19f083c0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 386 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 9 | Methods Changed: 14 | Complexity Δ (Sum/Max): 78/74 | Churn Δ: 392 | Churn Cumulative: 3369 | Contributors (this commit): 7 | Commits (past 90d): 50 | Contributors (cumulative): 15 | DMM Complexity: 0.339041095890411\n\nDIFF:\n@@ -111,8 +111,13 @@ def convert_to_tensor(x, dtype=None):\n             return x.to(dtype)\n         return x\n \n-    # Convert to np first in case of any non-numpy, numpy-compatible array.\n+    # Convert to np in case of any array-like that is not list or tuple.\n+    if not isinstance(x, (list, tuple)):\n         x = np.array(x)\n+    # Handle list or tuple of torch tensors\n+    elif len(x) > 0 and isinstance(x[0], torch.Tensor):\n+        return torch.stack(x)\n+\n     return torch.as_tensor(x, dtype=dtype, device=get_device())\n \n \n\n@@ -76,6 +76,12 @@ def zeros(shape, dtype=\"float32\"):\n     return torch.zeros(size=shape, dtype=dtype)\n \n \n+def zeros_like(x, dtype=None):\n+    x = convert_to_tensor(x)\n+    dtype = to_torch_dtype(dtype)\n+    return torch.zeros_like(x, dtype=dtype)\n+\n+\n def absolute(x):\n     return abs(x)\n \n\n@@ -1,5 +1,372 @@\n-def rnn(*args, **kwargs):\n-    raise NotImplementedError\n+import torch\n+from tensorflow import nest\n+\n+from keras_core.backend.torch.core import convert_to_tensor\n+\n+\n+def rnn(\n+    step_function,\n+    inputs,\n+    initial_states,\n+    go_backwards=False,\n+    mask=None,\n+    constants=None,\n+    unroll=False,\n+    input_length=None,\n+    time_major=False,\n+    zero_output_for_mask=False,\n+    return_all_outputs=True,\n+):\n+    input_length = input_length or inputs.shape[1]\n+\n+    def swap_batch_timestep(input_t):\n+        # Swap the batch and timestep dim for the incoming tensor.\n+        axes = list(range(len(input_t.shape)))\n+        axes[0], axes[1] = 1, 0\n+        return torch.permute(input_t, axes)\n+\n+    if not time_major:\n+        inputs = nest.map_structure(swap_batch_timestep, inputs)\n+\n+    flattened_inputs = nest.flatten(inputs)\n+    time_steps = flattened_inputs[0].shape[0]\n+    time_steps_t = time_steps\n+\n+    if mask is not None:\n+        if mask.dtype != torch.bool:\n+            mask = mask.type(torch.bool)\n+        if len(mask.shape) == 2:\n+            mask = torch.unsqueeze(mask, -1)\n+        if not time_major:\n+            mask = swap_batch_timestep(mask)\n+\n+    if constants is None:\n+        constants = []\n+\n+    def _expand_mask(mask_t, input_t, fixed_dim=1):\n+        if nest.is_nested(mask_t):\n+            raise ValueError(\n+                f\"mask_t is expected to be tensor, but got {mask_t}\"\n+            )\n+        if nest.is_nested(input_t):\n+            raise ValueError(\n+                f\"input_t is expected to be tensor, but got {input_t}\"\n+            )\n+        rank_diff = len(input_t.shape) - len(mask_t.shape)\n+        for _ in range(rank_diff):\n+            mask_t = torch.unsqueeze(mask_t, -1)\n+        multiples = [1] * fixed_dim + list(input_t.shape[fixed_dim:])\n+        return torch.tile(mask_t, multiples)\n+\n+    if unroll:\n+        if not time_steps:\n+            raise ValueError(\"Unrolling requires a fixed number of timesteps.\")\n+        states = tuple(initial_states)\n+        successive_states = []\n+        successive_outputs = []\n+\n+        # Process the input tensors. The input tensor need to be split on the\n+        # time_step dim, and reverse if go_backwards is True. In the case of\n+        # nested input, the input is flattened and then transformed\n+        # individually.  The result of this will be a tuple of lists, each of\n+        # the item in tuple is list of the tensor with shape (batch, feature)\n+        def _process_single_input_t(input_t):\n+            input_t = torch.unbind(input_t)  # unstack for time_step dim\n+            if go_backwards:\n+                input_t = input_t[::-1]\n+            return input_t\n+\n+        if nest.is_nested(inputs):\n+            processed_input = nest.map_structure(\n+                _process_single_input_t, inputs\n+            )\n+        else:\n+            processed_input = (_process_single_input_t(inputs),)\n+\n+        def _get_input_tensor(time):\n+            inp = [t_[time] for t_ in processed_input]\n+            return nest.pack_sequence_as(inputs, inp)\n+\n+        if mask is not None:\n+            mask_list = torch.unbind(mask)\n+            if go_backwards:\n+                mask_list = torch.flip(mask_list, dims=mask_list.shape)\n+\n+            for i in range(time_steps):\n+                inp = _get_input_tensor(i)\n+                mask_t = mask_list[i]\n+                output, new_states = step_function(\n+                    inp, tuple(states) + tuple(constants)\n+                )\n+                tiled_mask_t = _expand_mask(mask_t, output)\n+\n+                if not successive_outputs:\n+                    prev_output = torch.zeros_like(output)\n+                else:\n+                    prev_output = successive_outputs[-1]\n+\n+                output = torch.where(tiled_mask_t, output, prev_output)\n+\n+                flat_states = nest.flatten(states)\n+                flat_new_states = nest.flatten(new_states)\n+                tiled_mask_t = tuple(\n+                    _expand_mask(mask_t, s) for s in flat_states\n+                )\n+                flat_final_states = tuple(\n+                    torch.where(m, s, ps)\n+                    for m, s, ps in zip(\n+                        tiled_mask_t, flat_new_states, flat_states\n+                    )\n+                )\n+                states = nest.pack_sequence_as(states, flat_final_states)\n+\n+                if return_all_outputs:\n+                    successive_outputs.append(output)\n+                    successive_states.append(states)\n+                else:\n+                    successive_outputs = [output]\n+                    successive_states = [states]\n+            last_output = successive_outputs[-1]\n+            new_states = successive_states[-1]\n+            outputs = torch.stack(successive_outputs)\n+\n+            if zero_output_for_mask:\n+                last_output = torch.where(\n+                    _expand_mask(mask_list[-1], last_output),\n+                    last_output,\n+                    torch.zeros_like(last_output),\n+                )\n+                outputs = torch.where(\n+                    _expand_mask(mask, outputs, fixed_dim=2),\n+                    outputs,\n+                    torch.zeros_like(outputs),\n+                )\n+\n+        else:  # mask is None\n+            for i in range(time_steps):\n+                inp = _get_input_tensor(i)\n+                output, states = step_function(\n+                    inp, tuple(states) + tuple(constants)\n+                )\n+                if return_all_outputs:\n+                    successive_outputs.append(output)\n+                    successive_states.append(states)\n+                else:\n+                    successive_outputs = [output]\n+                    successive_states = [states]\n+            last_output = successive_outputs[-1]\n+            new_states = successive_states[-1]\n+            outputs = torch.stack(successive_outputs)\n+\n+    else:  # Unroll == False\n+        states = tuple(initial_states)\n+\n+        # Create input tensor array, if the inputs is nested tensors, then it\n+        # will be flattened first, and tensor array will be created one per\n+        # flattened tensor.\n+\n+        input_ta = tuple(\n+            list(torch.unbind(input_))\n+            if not go_backwards\n+            else list(torch.unbind(torch.flip(input_, [0])))\n+            for input_ in flattened_inputs\n+        )\n+\n+        # Get the time(0) input and compute the output for that.\n+        input_time_zero = nest.pack_sequence_as(\n+            inputs, [inp[0] for inp in flattened_inputs]\n+        )\n+        # output_time_zero is used to determine the cell output shape.\n+        output_time_zero, _ = step_function(\n+            input_time_zero, tuple(initial_states) + tuple(constants)\n+        )\n+\n+        output_ta_size = time_steps_t if return_all_outputs else 1\n+        output_ta = []\n+        for out in nest.flatten(output_time_zero):\n+            out_list = list(out)\n+            if len(out) < output_ta_size:\n+                out_list.extend([[]] * (output_ta_size - len(out)))\n+            output_ta.append(out_list)\n+\n+        time = torch.tensor(0, dtype=torch.int32)\n+\n+        if input_length is None:\n+            max_iterations = time_steps_t\n+        else:\n+            if hasattr(input_length, \"__len__\"):\n+                input_length = convert_to_tensor(input_length)\n+                max_iterations = torch.max(input_length)\n+            else:\n+                max_iterations = input_length\n+\n+        if mask is not None:\n+            if go_backwards:\n+                mask = torch.flip(mask, [0])\n+\n+            mask_ta = list(torch.unbind(mask))\n+\n+            def masking_fn(time):\n+                return mask_ta[time]\n+\n+            def compute_masked_output(mask_t, flat_out, flat_mask):\n+                tiled_mask_t = tuple(\n+                    _expand_mask(mask_t, o, fixed_dim=len(mask_t.shape))\n+                    for o in flat_out\n+                )\n+                return tuple(\n+                    torch.where(m, o, fm)\n+                    for m, o, fm in zip(tiled_mask_t, flat_out, flat_mask)\n+                )\n+\n+        elif isinstance(input_length, torch.Tensor):\n+            if go_backwards:\n+                max_len = torch.max(input_length, dim=0)\n+                rev_input_length = torch.subtract(max_len - 1, input_length)\n+\n+                def masking_fn(time):\n+                    return torch.less(rev_input_length, time)\n+\n+            else:\n+\n+                def masking_fn(time):\n+                    return torch.greater(input_length, time)\n+\n+            def compute_masked_output(mask_t, flat_out, flat_mask):\n+                return tuple(\n+                    torch.where(mask_t, o, zo)\n+                    for (o, zo) in zip(flat_out, flat_mask)\n+                )\n+\n+        else:\n+            masking_fn = None\n+\n+        if masking_fn is not None:\n+            # Mask for the T output will be base on the output of T - 1. In the\n+            # case T = 0, a zero filled tensor will be used.\n+            flat_zero_output = tuple(\n+                torch.zeros_like(o) for o in nest.flatten(output_time_zero)\n+            )\n+\n+            def _step(time, output_ta_t, prev_output, *states):\n+                \"\"\"RNN step function.\n+\n+                Args:\n+                    time: Current timestep value.\n+                    output_ta_t: TensorArray.\n+                    prev_output: tuple of outputs from time - 1.\n+                    *states: List of states.\n+\n+                Returns:\n+                    Tuple: `(time + 1, output_ta_t, output) + tuple(new_states)`\n+                \"\"\"\n+                current_input = tuple(ta[time] for ta in input_ta)\n+                # maybe set shape.\n+                current_input = nest.pack_sequence_as(inputs, current_input)\n+                mask_t = masking_fn(time)\n+                output, new_states = step_function(\n+                    current_input, tuple(states) + tuple(constants)\n+                )\n+                # mask output\n+                flat_output = nest.flatten(output)\n+                flat_mask_output = (\n+                    flat_zero_output\n+                    if zero_output_for_mask\n+                    else nest.flatten(prev_output)\n+                )\n+                flat_new_output = compute_masked_output(\n+                    mask_t, flat_output, flat_mask_output\n+                )\n+\n+                # mask states\n+                flat_state = nest.flatten(states)\n+                flat_new_state = nest.flatten(new_states)\n+                flat_final_state = compute_masked_output(\n+                    mask_t, flat_new_state, flat_state\n+                )\n+                new_states = nest.pack_sequence_as(new_states, flat_final_state)\n+\n+                ta_index_to_write = time if return_all_outputs else 0\n+                for ta, out in zip(output_ta_t, flat_new_output):\n+                    ta[ta_index_to_write] = out\n+\n+                return (time + 1, output_ta_t, tuple(flat_new_output)) + tuple(\n+                    new_states\n+                )\n+\n+            it = 0\n+            output_ta_t, new_states, prev_output = (\n+                output_ta,\n+                states,\n+                flat_zero_output,\n+            )\n+            while time < time_steps_t and it < max_iterations:\n+                final_outputs = _step(\n+                    time, output_ta_t, prev_output, *new_states\n+                )\n+                time, output_ta_t, prev_output = final_outputs[:3]\n+                new_states = final_outputs[3:]\n+                it += 1\n+\n+        else:\n+\n+            def _step(time, output_ta_t, *states):\n+                \"\"\"RNN step function.\n+\n+                Args:\n+                    time: Current timestep value.\n+                    output_ta_t: TensorArray.\n+                    *states: List of states.\n+\n+                Returns:\n+                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n+                \"\"\"\n+                current_input = tuple(ta[time] for ta in input_ta)\n+                current_input = nest.pack_sequence_as(inputs, current_input)\n+                output, new_states = step_function(\n+                    current_input, tuple(states) + tuple(constants)\n+                )\n+                flat_new_state = nest.flatten(new_states)\n+\n+                flat_output = nest.flatten(output)\n+                ta_index_to_write = time if return_all_outputs else 0\n+                for ta, out in zip(output_ta_t, flat_output):\n+                    ta[ta_index_to_write] = out\n+\n+                new_states = nest.pack_sequence_as(\n+                    initial_states, flat_new_state\n+                )\n+                return (time + 1, output_ta_t) + tuple(new_states)\n+\n+            it = 0\n+            output_ta_t = output_ta\n+            new_states = states\n+            while time < time_steps_t and it < max_iterations:\n+                final_outputs = _step(time, output_ta_t, *new_states)\n+                time, output_ta_t = final_outputs[:2]\n+                new_states = final_outputs[2:]\n+                it += 1\n+\n+        def _stack(tensor_list):\n+            max_ndims = max([t.ndim for t in tensor_list])\n+            max_list = []\n+            for i, t in enumerate(tensor_list):\n+                if t.ndim == max_ndims:\n+                    max_list.append(t)\n+            return torch.stack(max_list)\n+\n+        output_ta = final_outputs[1]\n+\n+        outputs = tuple(_stack(o) for o in output_ta)\n+        last_output = tuple(o[-1] for o in outputs)\n+\n+        outputs = nest.pack_sequence_as(output_time_zero, outputs)\n+        last_output = nest.pack_sequence_as(output_time_zero, last_output)\n+\n+    if not time_major:\n+        outputs = nest.map_structure(swap_batch_timestep, outputs)\n+\n+    return last_output, outputs, new_states\n \n \n def lstm(*args, **kwargs):\n\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+from keras_core import backend\n from keras_core import initializers\n from keras_core import testing\n from keras_core.layers.rnn.conv_lstm import ConvLSTM\n@@ -21,11 +22,11 @@ class ConvLSTMCellTest(testing.TestCase):\n             recurrent_initializer=initializers.Constant(0.02),\n         )\n         output = layer(x, [s1, s2])\n-        checksum_0 = np.sum(output[0])\n+        checksum_0 = np.sum(backend.convert_to_numpy(output[0]))\n         self.assertAllClose(checksum_0, 188.89502)\n-        checksum_1 = np.sum(output[1][0])\n+        checksum_1 = np.sum(backend.convert_to_numpy(output[1][0]))\n         self.assertAllClose(checksum_1, 188.89502)\n-        checksum_2 = np.sum(output[1][1])\n+        checksum_2 = np.sum(backend.convert_to_numpy(output[1][1]))\n         self.assertAllClose(checksum_2, 2170.444)\n \n \n@@ -44,4 +45,5 @@ class ConvLSTMTest(testing.TestCase):\n             recurrent_initializer=initializers.Constant(0.02),\n         )\n         output = layer(x, initial_state=[s1, s2])\n+        output = backend.convert_to_numpy(output)\n         self.assertAllClose(np.sum(output), 119.812454)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#517f920f68b6d048bdb22298bc984e968543c84d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 21 | Churn Cumulative: 3011 | Contributors (this commit): 8 | Commits (past 90d): 50 | Contributors (cumulative): 13 | DMM Complexity: 0.1111111111111111\n\nDIFF:\n@@ -60,7 +60,7 @@ class Variable(KerasVariable):\n         self._value = torch.nn.Parameter(\n             convert_to_tensor(value, dtype=self._dtype),\n             requires_grad=self.trainable,\n-        )\n+        ).to(get_device())\n \n     def _direct_assign(self, value):\n         with torch.no_grad():\n@@ -102,26 +102,30 @@ class Variable(KerasVariable):\n \n \n def convert_to_tensor(x, dtype=None):\n-    # TODO: Need to address device placement arg of `as_tensor`\n     dtype = to_torch_dtype(dtype or getattr(x, \"dtype\", None))\n     if isinstance(x, Variable):\n         x = x.value\n     if is_tensor(x):\n         if dtype and dtype != x.dtype:\n-            return x.to(dtype)\n-        return x\n+            x = x.to(dtype)\n+        return x.to(get_device())\n \n     # Convert to np in case of any array-like that is not list or tuple.\n     if not isinstance(x, (list, tuple)):\n         x = np.array(x)\n-    # Handle list or tuple of torch tensors\n     elif len(x) > 0 and isinstance(x[0], torch.Tensor):\n+        # Handle list or tuple of torch tensors\n         return torch.stack(x)\n \n     return torch.as_tensor(x, dtype=dtype, device=get_device())\n \n \n def convert_to_numpy(x):\n+    if isinstance(x, KerasVariable):\n+        x = x.value\n+    if is_tensor(x) and x.is_cuda:\n+        # Tensor has to be moved to CPU before converting to numpy.\n+        x = x.cpu()\n     if is_tensor(x) and x.requires_grad:\n         return x.detach().numpy()\n     return np.array(x)\n\n@@ -39,6 +39,8 @@ def multiply(x1, x2):\n \n \n def mean(x, axis=None, keepdims=False):\n+    if isinstance(x, list):\n+        x = stack(x)\n     x = convert_to_tensor(x)\n     # Conversion to float necessary for `torch.mean`\n     x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n@@ -845,6 +847,8 @@ def var(x, axis=None, keepdims=False):\n \n \n def sum(x, axis=None, keepdims=False):\n+    if isinstance(x, list):\n+        x = stack(x)\n     x = convert_to_tensor(x)\n     if axis is not None:\n         return torch.sum(x, axis=axis, keepdim=keepdims)\n\n@@ -28,10 +28,11 @@ class SoftmaxTest(testing.TestCase):\n     def test_softmax_correctness_with_mask(self):\n         softmax_layer = softmax.Softmax(axis=(1, 0))\n         input = np.array([[1.0, 2.0, 1.0], [1.0, 2.0, 1.0]])\n+        mask = np.array([[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]])\n         expected_output = np.array(\n             [[0.21194154, 0.0, 0.21194154], [0.0, 0.57611686, 0.0]]\n         )\n-        result = softmax_layer(input, mask=[[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]])\n+        result = softmax_layer(input, mask=mask)\n         self.assertAllClose(result, expected_output)\n \n     def test_softmax_correctness_with_axis(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c30a635589798422465329fcccd8313400e8a637", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 399 | Contributors (this commit): 6 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -228,16 +228,22 @@ if \"KERAS_BACKEND\" in os.environ:\n         _BACKEND = _backend\n \n \n-@keras_core_export(\"keras_core.backend.backend\")\n+@keras_core_export(\n+    [\n+        \"keras_core.config.backend\",\n+        \"keras_core.backend.backend\",\n+    ]\n+)\n def backend():\n     \"\"\"Publicly accessible method for determining the current backend.\n \n     Returns:\n-        String, the name of the backend Keras is currently using.\n+        String, the name of the backend Keras is currently using. One of\n+            `\"tensorflow\"`, `\"torch\"`, or `\"jax\"`.\n \n     Example:\n \n-    >>> keras.backend.backend()\n+    >>> keras.config.backend()\n     'tensorflow'\n     \"\"\"\n     return _BACKEND\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1d36709468e2f58d980c8b83c6e50a62e85a1ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 461 | Contributors (this commit): 6 | Commits (past 90d): 27 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -72,7 +72,7 @@ class Loss:\n \n \n def standardize_reduction(reduction):\n-    allowed = {\"sum_over_batch_size\", \"sum\", None}\n+    allowed = {\"sum_over_batch_size\", \"sum\", None, \"none\"}\n     if reduction not in allowed:\n         raise ValueError(\n             \"Invalid value for argument `reduction`. \"\n@@ -100,6 +100,7 @@ def squeeze_to_same_rank(x1, x2):\n def reduce_values(values, reduction=\"sum_over_batch_size\"):\n     if (\n         reduction is None\n+        or reduction == \"none\"\n         or tuple(values.shape) == ()\n         or tuple(values.shape) == (0,)\n     ):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f51709d41f09f68a868dcff94aa5aface9dbf9b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 373 | Lines Deleted: 15 | Files Changed: 6 | Hunks: 11 | Methods Changed: 20 | Complexity Δ (Sum/Max): 43/31 | Churn Δ: 388 | Churn Cumulative: 4015 | Contributors (this commit): 9 | Commits (past 90d): 76 | Contributors (cumulative): 20 | DMM Complexity: 0.44\n\nDIFF:\n@@ -50,7 +50,6 @@ model = keras_core.Sequential(\n #################################################################\n \n # Instantiate the torch optimizer\n-print(\"Num params:\", len(list(model.parameters())))\n optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n \n # Instantiate the torch loss function\n@@ -122,7 +121,6 @@ class MyModel(nn.Module):\n torch_module = MyModel()\n \n # Instantiate the torch optimizer\n-print(\"Num params:\", len(list(torch_module.parameters())))\n optimizer = optim.Adam(torch_module.parameters(), lr=learning_rate)\n \n # Instantiate the torch loss function\n\n@@ -228,22 +228,16 @@ if \"KERAS_BACKEND\" in os.environ:\n         _BACKEND = _backend\n \n \n-@keras_core_export(\n-    [\n-        \"keras_core.config.backend\",\n-        \"keras_core.backend.backend\",\n-    ]\n-)\n+@keras_core_export(\"keras_core.backend.backend\")\n def backend():\n     \"\"\"Publicly accessible method for determining the current backend.\n \n     Returns:\n-        String, the name of the backend Keras is currently using. One of\n-            `\"tensorflow\"`, `\"torch\"`, or `\"jax\"`.\n+        String, the name of the backend Keras is currently using.\n \n     Example:\n \n-    >>> keras.config.backend()\n+    >>> keras.backend.backend()\n     'tensorflow'\n     \"\"\"\n     return _BACKEND\n\n@@ -39,7 +39,7 @@ def multiply(x1, x2):\n \n \n def mean(x, axis=None, keepdims=False):\n-    if isinstance(x, list):\n+    if isinstance(x, (list, tuple)):\n         x = stack(x)\n     x = convert_to_tensor(x)\n     # Conversion to float necessary for `torch.mean`\n@@ -847,7 +847,7 @@ def var(x, axis=None, keepdims=False):\n \n \n def sum(x, axis=None, keepdims=False):\n-    if isinstance(x, list):\n+    if isinstance(x, (list, tuple)):\n         x = stack(x)\n     x = convert_to_tensor(x)\n     if axis is not None:\n\n@@ -72,7 +72,7 @@ class Loss:\n \n \n def standardize_reduction(reduction):\n-    allowed = {\"sum_over_batch_size\", \"sum\", None, \"none\"}\n+    allowed = {\"sum_over_batch_size\", \"sum\", None}\n     if reduction not in allowed:\n         raise ValueError(\n             \"Invalid value for argument `reduction`. \"\n@@ -100,7 +100,6 @@ def squeeze_to_same_rank(x1, x2):\n def reduce_values(values, reduction=\"sum_over_batch_size\"):\n     if (\n         reduction is None\n-        or reduction == \"none\"\n         or tuple(values.shape) == ()\n         or tuple(values.shape) == (0,)\n     ):\n\n@@ -0,0 +1,284 @@\n+from tensorflow import nest\n+\n+from keras_core import backend\n+from keras_core import utils\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers import Input\n+from keras_core.layers import InputLayer\n+from keras_core.models.functional import Functional\n+from keras_core.models.functional import functional_like_constructor\n+from keras_core.models.sequential import Sequential\n+from keras_core.saving import serialization_lib\n+\n+\n+@keras_core_export(\"keras_core.models.clone_model\")\n+def clone_model(model, input_tensors=None, clone_function=None):\n+    \"\"\"Clone a Functional or Sequential `Model` instance.\n+\n+    Model cloning is similar to calling a model on new inputs,\n+    except that it creates new layers (and thus new weights) instead\n+    of sharing the weights of the existing layers.\n+\n+    Note that\n+    `clone_model` will not preserve the uniqueness of shared objects within the\n+    model (e.g. a single variable attached to two distinct layers will be\n+    restored as two separate variables).\n+\n+    Args:\n+        model: Instance of `Model`\n+            (could be a Functional model or a Sequential model).\n+        input_tensors: optional list of input tensors or InputLayer objects\n+            to build the model upon. If not provided,\n+            new `Input` objects will be created.\n+        clone_function: Callable to be used to clone each layer in the target\n+            model (except `Input` instances). It takes as argument the\n+            layer instance to be cloned, and returns the corresponding layer\n+            instance to be used in the model copy. If unspecified, this callable\n+            becomes the following serialization/deserialization function:\n+            `lambda layer: layer.__class__.from_config(layer.get_config())`.\n+            By passing a custom callable, you can customize your copy of the\n+            model, e.g. by wrapping certain layers of interest (you might want\n+            to replace all `LSTM` instances with equivalent\n+            `Bidirectional(LSTM(...))` instances, for example).\n+            Defaults to `None`.\n+\n+    Returns:\n+        An instance of `Model` reproducing the behavior\n+        of the original model, on top of new inputs tensors,\n+        using newly instantiated weights. The cloned model may behave\n+        differently from the original model if a custom `clone_function`\n+        modifies the layer.\n+\n+    Examples:\n+\n+    Basic usage:\n+\n+    ```python\n+    # Create a test Sequential model.\n+    model = keras_core.Sequential([\n+        keras_core.layers.Input(shape=(728,)),\n+        keras_core.layers.Dense(32, activation='relu'),\n+        keras_core.layers.Dense(1, activation='sigmoid'),\n+    ])\n+    # Create a copy of the test model (with freshly initialized weights).\n+    new_model = clone_model(model)\n+    ```\n+\n+    Using a `clone_function` to make a model deterministic by setting the\n+    random seed everywhere:\n+\n+    ```python\n+    def clone_function(layer):\n+        config = layer.get_config()\n+        if \"seed\" in config:\n+            config[\"seed\"] = 1337\n+        return layer.__class__.from_config(config)\n+\n+    new_model = clone_model(model)\n+    ```\n+\n+    Note that subclassed models cannot be cloned by default,\n+    since their internal layer structure is not known.\n+    To achieve equivalent functionality\n+    as `clone_model` in the case of a subclassed model, simply make sure\n+    that the model class implements `get_config()`\n+    (and optionally `from_config()`), and call:\n+\n+    ```python\n+    new_model = model.__class__.from_config(model.get_config())\n+    ```\n+\n+    In the case of a subclassed model, you cannot using a custom\n+    `clone_function`.\n+    \"\"\"\n+    if isinstance(model, Sequential):\n+        return _clone_sequential_model(\n+            model, input_tensors=input_tensors, clone_function=clone_function\n+        )\n+    if isinstance(model, Functional):\n+        # If the get_config() method is the same as a regular Functional\n+        # model, we're safe to use _clone_functional_model (which relies\n+        # on a Functional constructor). In the case where the get_config\n+        # is custom, this may not necessarily work, but if clone_function\n+        # or input_tensors are passed, we attempt it anyway\n+        # in order to preserve backwards compatibility.\n+        if utils.is_default(model.get_config) or (\n+            clone_function or input_tensors\n+        ):\n+            return _clone_functional_model(\n+                model,\n+                input_tensors=input_tensors,\n+                clone_function=clone_function,\n+            )\n+\n+    # Case of a custom model class\n+    if clone_function or input_tensors:\n+        raise ValueError(\n+            \"Arguments clone_function and input_tensors \"\n+            \"are only supported for Sequential models \"\n+            \"or Functional models. Received model of \"\n+            f\"type '{model.__class__.__name__}', with \"\n+            f\"clone_function={clone_function} and \"\n+            f\"input_tensors={input_tensors}\"\n+        )\n+    config = serialization_lib.serialize_keras_object(model)\n+    return serialization_lib.deserialize_keras_object(\n+        config, custom_objects={model.__class__.__name__: model.__class__}\n+    )\n+\n+\n+def _clone_sequential_model(model, input_tensors=None, clone_function=None):\n+    \"\"\"Clone a `Sequential` model instance.\n+\n+    Model cloning is similar to calling a model on new inputs,\n+    except that it creates new layers (and thus new weights) instead\n+    of sharing the weights of the existing layers.\n+\n+    Args:\n+        model: Instance of `Sequential`.\n+        input_tensors: optional list of input tensors\n+            to build the model upon. If not provided,\n+            placeholders will be created.\n+        clone_function: callable to be applied on non-input layers in the model.\n+            By default, it clones the layer (without copying the weights).\n+\n+    Returns:\n+        An instance of `Sequential` reproducing the behavior\n+        of the original model, on top of new inputs tensors,\n+        using newly instantiated weights.\n+    \"\"\"\n+    if clone_function is None:\n+\n+        def _clone_layer(layer):\n+            return layer.__class__.from_config(layer.get_config())\n+\n+        clone_function = _clone_layer\n+\n+    if not isinstance(model, Sequential):\n+        raise ValueError(\n+            \"Expected `model` argument \"\n+            \"to be a `Sequential` model instance. \"\n+            f\"Received: model={model}\"\n+        )\n+\n+    if not callable(clone_function):\n+        raise ValueError(\n+            \"Expected `clone_function` argument to be a callable. \"\n+            f\"Received: clone_function={clone_function}\"\n+        )\n+\n+    new_layers = [clone_function(layer) for layer in model.layers]\n+\n+    if isinstance(model._layers[0], InputLayer):\n+        ref_input_layer = model._layers[0]\n+        input_name = ref_input_layer.name\n+        input_batch_shape = ref_input_layer.batch_shape\n+        input_dtype = ref_input_layer._dtype\n+    else:\n+        input_name = None\n+        input_dtype = None\n+        input_batch_shape = None\n+\n+    if input_tensors:\n+        if isinstance(input_tensors, (list, tuple)):\n+            if len(input_tensors) != 1:\n+                raise ValueError(\n+                    \"Argument `input_tensors` must contain a single tensor.\"\n+                )\n+            input_tensors = input_tensors[0]\n+        if not isinstance(input_tensors, backend.KerasTensor):\n+            raise ValueError(\n+                \"Argument `input_tensors` must be a KerasTensor. \"\n+                f\"Received invalid value: input_tensors={input_tensors}\"\n+            )\n+        inputs = Input(tensor=input_tensors, name=input_name)\n+        new_layers = [inputs] + new_layers\n+    else:\n+        if input_batch_shape is not None:\n+            inputs = Input(\n+                tensor=input_tensors,\n+                batch_shape=input_batch_shape,\n+                dtype=input_dtype,\n+                name=input_name,\n+            )\n+            new_layers = [inputs] + new_layers\n+    return Sequential(new_layers, name=model.name, trainable=model.trainable)\n+\n+\n+def _clone_functional_model(model, input_tensors=None, clone_function=None):\n+    \"\"\"Clone a `Functional` model instance.\n+\n+    Model cloning is similar to calling a model on new inputs,\n+    except that it creates new layers (and thus new weights) instead\n+    of sharing the weights of the existing layers.\n+\n+    Input layers are always cloned.\n+\n+    Args:\n+        model: Instance of `Functional`.\n+        input_tensors: optional list of input tensors\n+            to build the model upon. If not provided,\n+            placeholders will be created.\n+        clone_function: callable to be applied on non-input layers in the model.\n+            By default, it clones the layer (without copying the weights).\n+\n+    Returns:\n+        An instance of `Functional` reproducing the behavior\n+        of the original model, on top of new inputs tensors,\n+        using newly instantiated weights.\n+    \"\"\"\n+    if clone_function is None:\n+        seen = {}\n+\n+        def _clone_layer(layer):\n+            if layer in seen:\n+                return seen[layer]\n+            new_layer = layer.__class__.from_config(layer.get_config())\n+            seen[layer] = new_layer\n+            return new_layer\n+\n+        clone_function = _clone_layer\n+\n+    if not callable(clone_function):\n+        raise ValueError(\n+            \"Expected `clone_function` argument to be a callable. \"\n+            f\"Received: clone_function={clone_function}\"\n+        )\n+\n+    if not isinstance(model, Functional):\n+        raise ValueError(\n+            \"Expected `model` argument \"\n+            f\"to be a Functional Model instance. Received: model={model}\"\n+        )\n+\n+    if input_tensors is not None:\n+        input_tensors = nest.flatten(input_tensors)\n+        if not all(isinstance(x, backend.KerasTensor) for x in input_tensors):\n+            raise ValueError(\n+                \"All entries in `input_tensors` must be KerasTensors. \"\n+                f\"Received invalid values: inputs_tensors={input_tensors}\"\n+            )\n+    else:\n+        input_tensors = nest.map_structure(\n+            lambda x: Input(x.shape, dtype=x.dtype, name=x.name), model.input\n+        )\n+\n+    def operation_fn(layer):\n+        new_layer = clone_function(layer)\n+        return new_layer\n+\n+    output_tensors = model._run_through_graph(\n+        input_tensors, operation_fn=operation_fn\n+    )\n+\n+    if functional_like_constructor(model.__class__):\n+        new_model = model.__class__(\n+            input_tensors, output_tensors, name=model.name\n+        )\n+    else:\n+        # This may be incorrect: the new model will end up having a different\n+        # class than the original. However various existing models rely\n+        # on this behavior, so we keep it.\n+        new_model = Functional(input_tensors, output_tensors, name=model.name)\n+\n+    return new_model\n\n@@ -0,0 +1,83 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras_core import layers\n+from keras_core import models\n+from keras_core import testing\n+from keras_core.models.cloning import clone_model\n+\n+\n+def get_functional_model(shared_layers=False):\n+    inputs = layers.Input(shape=(3,))\n+    x = layers.Dense(2)(inputs)\n+    if shared_layers:\n+        layer = layers.Dense(2, name=\"shared\")\n+        x = layer(x)\n+        x = layer(x)\n+    outputs = layers.Dense(2)(x)\n+    model = models.Model(inputs, outputs)\n+    return model\n+\n+\n+def get_sequential_model(explicit_input=True):\n+    model = models.Sequential()\n+    if explicit_input:\n+        model.add(layers.Input(shape=(3,)))\n+    model.add(layers.Dense(2))\n+    model.add(layers.Dense(2))\n+    return model\n+\n+\n+def get_subclassed_model():\n+    class ExampleModel(models.Model):\n+        def __init__(self, **kwargs):\n+            super().__init__(**kwargs)\n+            self.d1 = layers.Dense(2)\n+            self.d2 = layers.Dense(2)\n+\n+        def call(self, x):\n+            return self.d2(self.d1(x))\n+\n+    return ExampleModel()\n+\n+\n+class CloneModelTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        (\"functional\", get_functional_model),\n+        (\"sequential\", get_sequential_model),\n+        (\n+            \"deferred_sequential\",\n+            lambda: get_sequential_model(explicit_input=False),\n+        ),\n+        (\"subclassed\", get_subclassed_model),\n+    )\n+    def test_cloning_correctness(self, model_fn):\n+        ref_input = np.random.random((2, 3))\n+        model = model_fn()\n+        new_model = clone_model(model)\n+        ref_output = model(ref_input)\n+        new_model(ref_input)  # Maybe needed to build the model\n+        new_model.set_weights(model.get_weights())\n+        output = new_model(ref_input)\n+        self.assertAllClose(ref_output, output)\n+\n+    @parameterized.named_parameters(\n+        (\"functional\", get_functional_model),\n+        (\"sequential\", get_sequential_model),\n+    )\n+    def test_custom_clone_function(self, model_fn):\n+        def clone_function(layer):\n+            config = layer.get_config()\n+            config[\"name\"] = config[\"name\"] + \"_custom\"\n+            return layer.__class__.from_config(config)\n+\n+        model = model_fn()\n+        new_model = clone_model(model, clone_function=clone_function)\n+        for l1, l2 in zip(model.layers, new_model.layers):\n+            if not isinstance(l1, layers.InputLayer):\n+                self.assertEqual(l2.name, l1.name + \"_custom\")\n+\n+    def test_shared_layers_cloning(self):\n+        model = get_functional_model(shared_layers=True)\n+        new_model = clone_model(model)\n+        self.assertLen(new_model.layers, 4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
