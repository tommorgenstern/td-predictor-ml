{"custom_id": "keras#4ef6a96e659df867817c55ebd360d3d13276a74f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 145 | Files Changed: 17 | Hunks: 34 | Methods Changed: 25 | Complexity Δ (Sum/Max): -14/2 | Churn Δ: 231 | Churn Cumulative: 14778 | Contributors (this commit): 23 | Commits (past 90d): 297 | Contributors (cumulative): 110 | DMM Complexity: 0.09375\n\nDIFF:\n@@ -17,6 +17,7 @@ from keras_core.backend.jax.core import scatter\n from keras_core.backend.jax.core import shape\n from keras_core.backend.jax.core import stop_gradient\n from keras_core.backend.jax.core import vectorized_map\n+from keras_core.backend.jax.rnn import cudnn_ok\n from keras_core.backend.jax.rnn import gru\n from keras_core.backend.jax.rnn import lstm\n from keras_core.backend.jax.rnn import rnn\n\n@@ -287,10 +287,3 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n def stop_gradient(variable):\n     return jax.lax.stop_gradient(variable)\n-\n-\n-def unstack(x, num=None, axis=0):\n-    return [\n-        jax.lax.index_in_dim(x, i, axis, keepdims=False)\n-        for i in range(x.shape[axis])\n-    ]\n\n@@ -201,6 +201,10 @@ def rnn(\n     return last_output, outputs, new_states\n \n \n+def cudnn_ok(*args, **kwargs):\n+    return False\n+\n+\n def lstm(*args, **kwargs):\n     raise NotImplementedError\n \n\n@@ -15,6 +15,7 @@ from keras_core.backend.numpy.core import is_tensor\n from keras_core.backend.numpy.core import name_scope\n from keras_core.backend.numpy.core import shape\n from keras_core.backend.numpy.core import vectorized_map\n+from keras_core.backend.numpy.rnn import cudnn_ok\n from keras_core.backend.numpy.rnn import gru\n from keras_core.backend.numpy.rnn import lstm\n from keras_core.backend.numpy.rnn import rnn\n\n@@ -217,8 +217,3 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n def stop_gradient(x):\n     pass\n-\n-\n-def unstack(x, num=None, axis=0):\n-    x = np.moveaxis(x, axis, 0)\n-    return [x[i] for i in range(x.shape[0])]\n\n@@ -234,3 +234,7 @@ def numpy_scan(f, init, xs, reverse=False, mask=None):\n         outputs = np.flip(outputs, axis=0)\n \n     return states, outputs\n+\n+\n+def cudnn_ok(*args, **kwargs):\n+    return False\n\n@@ -18,6 +18,7 @@ from keras_core.backend.tensorflow.core import scatter\n from keras_core.backend.tensorflow.core import shape\n from keras_core.backend.tensorflow.core import stop_gradient\n from keras_core.backend.tensorflow.core import vectorized_map\n+from keras_core.backend.tensorflow.rnn import cudnn_ok\n from keras_core.backend.tensorflow.rnn import gru\n from keras_core.backend.tensorflow.rnn import lstm\n from keras_core.backend.tensorflow.rnn import rnn\n\n@@ -183,7 +183,3 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n def stop_gradient(variable):\n     return tf.stop_gradient(variable)\n-\n-\n-def unstack(x, num=None, axis=0):\n-    return tf.unstack(x, num=num, axis=axis)\n\n@@ -456,15 +456,15 @@ def gru(\n     time_major=False,\n     reset_after=True,\n ):\n-    args_supported = _do_gru_arguments_support_cudnn(\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        unroll=unroll,\n-        bias=bias,\n+    inputs_supported = _do_rnn_inputs_support_cudnn(mask, time_major)\n+    cudnn_supported = cudnn_ok(\n+        activation,\n+        recurrent_activation,\n+        unroll,\n+        use_bias=bias is not None,\n         reset_after=reset_after,\n     )\n-    inputs_supported = _do_rnn_inputs_support_cudnn(mask, time_major)\n-    if not args_supported or not inputs_supported or not _is_gpu_available():\n+    if not cudnn_supported or not inputs_supported:\n         raise NotImplementedError\n \n     from keras_core.backend.tensorflow import Variable\n@@ -500,7 +500,7 @@ def _do_gru_arguments_support_cudnn(\n     activation,\n     recurrent_activation,\n     unroll,\n-    bias,\n+    use_bias,\n     reset_after,\n ):\n     from keras_core import activations\n@@ -511,7 +511,7 @@ def _do_gru_arguments_support_cudnn(\n         and recurrent_activation\n         in (activations.sigmoid, tf.sigmoid, ops.sigmoid)\n         and not unroll\n-        and bias is not None\n+        and use_bias\n         and reset_after\n     )\n \n@@ -520,7 +520,7 @@ def _do_lstm_arguments_support_cudnn(\n     activation,\n     recurrent_activation,\n     unroll,\n-    bias,\n+    use_bias,\n ):\n     from keras_core import activations\n     from keras_core import ops\n@@ -530,7 +530,7 @@ def _do_lstm_arguments_support_cudnn(\n         and recurrent_activation\n         in (activations.sigmoid, tf.sigmoid, ops.sigmoid)\n         and not unroll\n-        and bias is not None\n+        and use_bias\n     )\n \n \n@@ -776,6 +776,31 @@ def _cudnn_gru(\n     )\n \n \n+def cudnn_ok(\n+    activation,\n+    recurrent_activation,\n+    unroll,\n+    use_bias,\n+    reset_after=None,\n+):\n+    if reset_after is None:\n+        args_supported = _do_lstm_arguments_support_cudnn(\n+            activation=activation,\n+            recurrent_activation=recurrent_activation,\n+            unroll=unroll,\n+            use_bias=use_bias,\n+        )\n+    else:\n+        args_supported = _do_gru_arguments_support_cudnn(\n+            activation=activation,\n+            recurrent_activation=recurrent_activation,\n+            unroll=unroll,\n+            use_bias=use_bias,\n+            reset_after=reset_after,\n+        )\n+    return args_supported and _is_gpu_available()\n+\n+\n def lstm(\n     inputs,\n     initial_state_h,\n@@ -791,14 +816,11 @@ def lstm(\n     unroll=False,\n     time_major=False,\n ):\n-    args_supported = _do_lstm_arguments_support_cudnn(\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        unroll=unroll,\n-        bias=bias,\n-    )\n     inputs_supported = _do_rnn_inputs_support_cudnn(mask, time_major)\n-    if not args_supported or not inputs_supported or not _is_gpu_available():\n+    cudnn_supported = cudnn_ok(\n+        activation, recurrent_activation, unroll, use_bias=bias is not None\n+    )\n+    if not cudnn_supported or not inputs_supported:\n         raise NotImplementedError\n \n     from keras_core.backend.tensorflow import Variable\n\n@@ -4,7 +4,6 @@ import warnings\n import numpy as np\n import tensorflow as tf\n import tree\n-\n from packaging.version import Version\n from tensorflow.python.eager import context as tf_context\n \n@@ -24,11 +23,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         self.test_function = None\n         self.predict_function = None\n \n-        if Version(tf.__version__) >= Version('2.9.0'):\n-            self._support_reduce_retracing = True\n-        else:\n-            self._support_reduce_retracing = False\n-\n         # Model must be created under scope of DistStrat it will be trained\n         # with.\n         if tf.distribute.has_strategy():\n@@ -37,6 +31,9 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             self._distribute_strategy = None\n \n         self._distribute_reduction_method = None\n+        self._supports_reduce_retracing = Version(tf.__version__) >= Version(\n+            \"2.9.0\"\n+        )\n \n     @property\n     def distribute_strategy(self):\n@@ -106,9 +103,9 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             return self.train_step(data)\n \n         if not self.run_eagerly:\n-            kwargs = {'jit_compile': self.jit_compile}\n-            if self._support_reduce_retracing:\n-                kwargs.update({'reduce_retracing': True})\n+            kwargs = {\"jit_compile\": self.jit_compile}\n+            if self._supports_reduce_retracing:\n+                kwargs.update({\"reduce_retracing\": True})\n             one_step_on_data = tf.function(one_step_on_data, **kwargs)\n \n         @tf.autograph.experimental.do_not_convert\n@@ -138,8 +135,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n         if not self.run_eagerly:\n             kwargs = {}\n-            if self._support_reduce_retracing:\n-                kwargs.update({'reduce_retracing': True})\n+            if self._supports_reduce_retracing:\n+                kwargs.update({\"reduce_retracing\": True})\n             train_function = tf.function(train_function, **kwargs)\n \n         self.train_function = train_function\n@@ -154,9 +151,9 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             return self.test_step(data)\n \n         if not self.run_eagerly and self.jit_compile:\n-            kwargs = {'jit_compile': True}\n-            if self._support_reduce_retracing:\n-                kwargs.update({'reduce_retracing': True})\n+            kwargs = {\"jit_compile\": True}\n+            if self._supports_reduce_retracing:\n+                kwargs.update({\"reduce_retracing\": True})\n             one_step_on_data = tf.function(one_step_on_data, **kwargs)\n \n         @tf.autograph.experimental.do_not_convert\n@@ -186,8 +183,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n         if not self.run_eagerly:\n             kwargs = {}\n-            if self._support_reduce_retracing:\n-                kwargs.update({'reduce_retracing': True})\n+            if self._supports_reduce_retracing:\n+                kwargs.update({\"reduce_retracing\": True})\n             test_function = tf.function(test_function, **kwargs)\n \n         self.test_function = test_function\n@@ -202,9 +199,9 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             return self.predict_step(data)\n \n         if not self.run_eagerly and self.jit_compile:\n-            kwargs = {'jit_compile': True}\n-            if self._support_reduce_retracing:\n-                kwargs.update({'reduce_retracing': True})\n+            kwargs = {\"jit_compile\": True}\n+            if self._supports_reduce_retracing:\n+                kwargs.update({\"reduce_retracing\": True})\n             one_step_on_data = tf.function(one_step_on_data, **kwargs)\n \n         @tf.autograph.experimental.do_not_convert\n@@ -237,8 +234,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n         if not self.run_eagerly:\n             kwargs = {}\n-            if self._support_reduce_retracing:\n-                kwargs.update({'reduce_retracing': True})\n+            if self._supports_reduce_retracing:\n+                kwargs.update({\"reduce_retracing\": True})\n \n             predict_function = tf.function(predict_function, **kwargs)\n \n\n@@ -34,6 +34,7 @@ from keras_core.backend.torch.core import shape\n from keras_core.backend.torch.core import stop_gradient\n from keras_core.backend.torch.core import to_torch_dtype\n from keras_core.backend.torch.core import vectorized_map\n+from keras_core.backend.torch.rnn import cudnn_ok\n from keras_core.backend.torch.rnn import gru\n from keras_core.backend.torch.rnn import lstm\n from keras_core.backend.torch.rnn import rnn\n\n@@ -353,7 +353,3 @@ def stop_gradient(variable):\n     # We can't use `.requires_grad_(False)` here since it only\n     # works when the tensor is a leaf node in the graph.\n     return variable.detach()\n-\n-\n-def unstack(x, num=None, axis=0):\n-    return x.unbind(axis)\n\n@@ -368,6 +368,10 @@ def rnn(\n     return last_output, outputs, new_states\n \n \n+def cudnn_ok(*args, **kwargs):\n+    return False\n+\n+\n def lstm(*args, **kwargs):\n     raise NotImplementedError\n \n\n@@ -509,6 +509,14 @@ class GRU(RNN):\n             **kwargs,\n         )\n         self.input_spec = InputSpec(ndim=3)\n+        if backend.backend() == \"tensorflow\" and backend.cudnn_ok(\n+            activation,\n+            recurrent_activation,\n+            unroll,\n+            use_bias,\n+            reset_after=reset_after,\n+        ):\n+            self.supports_jit = False\n \n     def inner_loop(self, sequences, initial_state, mask, training=False):\n         if tree.is_nested(initial_state):\n\n@@ -491,6 +491,10 @@ class LSTM(RNN):\n             **kwargs,\n         )\n         self.input_spec = InputSpec(ndim=3)\n+        if backend.backend() == \"tensorflow\" and backend.cudnn_ok(\n+            activation, recurrent_activation, unroll, use_bias\n+        ):\n+            self.supports_jit = False\n \n     def inner_loop(self, sequences, initial_state, mask, training=False):\n         if tree.is_nested(mask):\n\n@@ -344,60 +344,6 @@ def fori_loop(lower, upper, body_fun, init_val):\n     return backend.core.fori_loop(lower, upper, body_fun, init_val)\n \n \n-class Unstack(Operation):\n-    def __init__(self, num=None, axis=0):\n-        super().__init__()\n-        self.num = num\n-        self.axis = axis\n-\n-    def call(self, x):\n-        return backend.core.unstack(x, self.num, self.axis)\n-\n-    def compute_output_spec(self, x):\n-        axis = self.axis\n-        if axis < 0:\n-            axis = len(x.shape) + axis\n-        output_shapes = x.shape[:axis] + x.shape[axis + 1 :]\n-        num = self.num\n-        if num is None:\n-            num = x.shape[axis]\n-        if num is None:\n-            raise ValueError(\n-                \"Cannot infer argument `num` from shape \"\n-                f\"{x.shape}. Either provide a tensor with a \"\n-                \"concrete shape in the `axis` dimension or \"\n-                \"explicitly pass the `num` argument.\"\n-            )\n-        output = [\n-            KerasTensor(shape=output_shapes, dtype=x.dtype) for _ in range(num)\n-        ]\n-        return output\n-\n-\n-@keras_core_export(\"keras_core.ops.unstack\")\n-def unstack(x, num=None, axis=0):\n-    \"\"\"Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n-\n-    Args:\n-        x: The input tensor.\n-        num: The length of the dimension axis. Automatically inferred\n-            if `None`.\n-        axis: The axis along which to unpack.\n-\n-    Returns:\n-        A list of tensors unpacked along the given axis.\n-\n-    Example:\n-\n-    >>> x = keras_core.ops.array([[1, 2], [3, 4]])\n-    >>> keras_core.ops.unstack(x, axis=0)\n-    [array([1, 2]), array([3, 4])]\n-    \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Unstack(num, axis).symbolic_call(x)\n-    return backend.core.unstack(x, num=num, axis=axis)\n-\n-\n @keras_core_export(\"keras_core.ops.shape\")\n def shape(x):\n     \"\"\"Gets the shape of the tensor input.\n\n@@ -56,26 +56,6 @@ class CoreOpsStaticShapeTest(testing.TestCase):\n         result = core.fori_loop(0, 10, body_fun, initial_value)\n         self.assertEqual(result.shape, (3, 5, 7))\n \n-    def test_unstack(self):\n-        x = KerasTensor((2, 3, 4))\n-        axis = 1\n-        out = core.unstack(x, axis=axis)\n-        self.assertEqual(len(out), 3)\n-        for o in out:\n-            self.assertEqual(o.shape, (2, 4))\n-\n-        x = KerasTensor((2, None, None))\n-        axis, num = 1, 3\n-        out = core.unstack(x, num=num, axis=axis)\n-        self.assertEqual(len(out), 3)\n-        for o in out:\n-            self.assertEqual(o.shape, (2, None))\n-\n-        with self.assertRaisesRegex(\n-            ValueError, r\"Cannot infer argument `num` from shape\"\n-        ):\n-            core.unstack(x, axis=axis)\n-\n \n class CoreOpsCorrectnessTest(testing.TestCase):\n     def test_scatter(self):\n@@ -318,15 +298,3 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n                 lambda: KerasTensor((3,)),\n                 lambda: KerasTensor((4,)),\n             )\n-\n-    def test_unstack(self):\n-        rng = np.random.default_rng(0)\n-        x = rng.uniform(size=(2, 3, 4))\n-        x_tensor = ops.convert_to_tensor(x)\n-        axis = 1\n-        out = ops.unstack(x_tensor, axis=axis)\n-        out_ex = [x[:, i, :] for i in range(x.shape[axis])]\n-        self.assertEqual(len(out), len(out_ex))\n-        for o, o_e in zip(out, out_ex):\n-            o = ops.convert_to_numpy(o)\n-            self.assertAllClose(o, o_e)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4a17e0b5c104c7a3cff1d8f390298821e62dca96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 106 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 7 | Methods Changed: 9 | Complexity Δ (Sum/Max): 21/9 | Churn Δ: 106 | Churn Cumulative: 2844 | Contributors (this commit): 15 | Commits (past 90d): 136 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -287,3 +287,10 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n def stop_gradient(variable):\n     return jax.lax.stop_gradient(variable)\n+\n+\n+def unstack(x, num=None, axis=0):\n+    return [\n+        jax.lax.index_in_dim(x, i, axis, keepdims=False)\n+        for i in range(x.shape[axis])\n+    ]\n\n@@ -217,3 +217,8 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n def stop_gradient(x):\n     pass\n+\n+\n+def unstack(x, num=None, axis=0):\n+    x = np.moveaxis(x, axis, 0)\n+    return [x[i] for i in range(x.shape[0])]\n\n@@ -183,3 +183,7 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n def stop_gradient(variable):\n     return tf.stop_gradient(variable)\n+\n+\n+def unstack(x, num=None, axis=0):\n+    return tf.unstack(x, num=num, axis=axis)\n\n@@ -353,3 +353,7 @@ def stop_gradient(variable):\n     # We can't use `.requires_grad_(False)` here since it only\n     # works when the tensor is a leaf node in the graph.\n     return variable.detach()\n+\n+\n+def unstack(x, num=None, axis=0):\n+    return x.unbind(axis)\n\n@@ -344,6 +344,60 @@ def fori_loop(lower, upper, body_fun, init_val):\n     return backend.core.fori_loop(lower, upper, body_fun, init_val)\n \n \n+class Unstack(Operation):\n+    def __init__(self, num=None, axis=0):\n+        super().__init__()\n+        self.num = num\n+        self.axis = axis\n+\n+    def call(self, x):\n+        return backend.core.unstack(x, self.num, self.axis)\n+\n+    def compute_output_spec(self, x):\n+        axis = self.axis\n+        if axis < 0:\n+            axis = len(x.shape) + axis\n+        output_shapes = x.shape[:axis] + x.shape[axis + 1 :]\n+        num = self.num\n+        if num is None:\n+            num = x.shape[axis]\n+        if num is None:\n+            raise ValueError(\n+                \"Cannot infer argument `num` from shape \"\n+                f\"{x.shape}. Either provide a tensor with a \"\n+                \"concrete shape in the `axis` dimension or \"\n+                \"explicitly pass the `num` argument.\"\n+            )\n+        output = [\n+            KerasTensor(shape=output_shapes, dtype=x.dtype) for _ in range(num)\n+        ]\n+        return output\n+\n+\n+@keras_core_export(\"keras_core.ops.unstack\")\n+def unstack(x, num=None, axis=0):\n+    \"\"\"Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n+\n+    Args:\n+        x: The input tensor.\n+        num: The length of the dimension axis. Automatically inferred\n+            if `None`.\n+        axis: The axis along which to unpack.\n+\n+    Returns:\n+        A list of tensors unpacked along the given axis.\n+\n+    Example:\n+\n+    >>> x = keras_core.ops.array([[1, 2], [3, 4]])\n+    >>> keras_core.ops.unstack(x, axis=0)\n+    [array([1, 2]), array([3, 4])]\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Unstack(num, axis).symbolic_call(x)\n+    return backend.core.unstack(x, num=num, axis=axis)\n+\n+\n @keras_core_export(\"keras_core.ops.shape\")\n def shape(x):\n     \"\"\"Gets the shape of the tensor input.\n\n@@ -56,6 +56,26 @@ class CoreOpsStaticShapeTest(testing.TestCase):\n         result = core.fori_loop(0, 10, body_fun, initial_value)\n         self.assertEqual(result.shape, (3, 5, 7))\n \n+    def test_unstack(self):\n+        x = KerasTensor((2, 3, 4))\n+        axis = 1\n+        out = core.unstack(x, axis=axis)\n+        self.assertEqual(len(out), 3)\n+        for o in out:\n+            self.assertEqual(o.shape, (2, 4))\n+\n+        x = KerasTensor((2, None, None))\n+        axis, num = 1, 3\n+        out = core.unstack(x, num=num, axis=axis)\n+        self.assertEqual(len(out), 3)\n+        for o in out:\n+            self.assertEqual(o.shape, (2, None))\n+\n+        with self.assertRaisesRegex(\n+            ValueError, r\"Cannot infer argument `num` from shape\"\n+        ):\n+            core.unstack(x, axis=axis)\n+\n \n class CoreOpsCorrectnessTest(testing.TestCase):\n     def test_scatter(self):\n@@ -298,3 +318,15 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n                 lambda: KerasTensor((3,)),\n                 lambda: KerasTensor((4,)),\n             )\n+\n+    def test_unstack(self):\n+        rng = np.random.default_rng(0)\n+        x = rng.uniform(size=(2, 3, 4))\n+        x_tensor = ops.convert_to_tensor(x)\n+        axis = 1\n+        out = ops.unstack(x_tensor, axis=axis)\n+        out_ex = [x[:, i, :] for i in range(x.shape[axis])]\n+        self.assertEqual(len(out), len(out_ex))\n+        for o, o_e in zip(out, out_ex):\n+            o = ops.convert_to_numpy(o)\n+            self.assertAllClose(o, o_e)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#70dae2555933ee69eb9820864085aca0583e759d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 4093 | Contributors (this commit): 9 | Commits (past 90d): 44 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -510,10 +510,10 @@ class GRU(RNN):\n         )\n         self.input_spec = InputSpec(ndim=3)\n         if backend.backend() == \"tensorflow\" and backend.cudnn_ok(\n-            activation,\n-            recurrent_activation,\n-            unroll,\n-            use_bias,\n+            cell.activation,\n+            cell.recurrent_activation,\n+            self.unroll,\n+            cell.use_bias,\n             reset_after=reset_after,\n         ):\n             self.supports_jit = False\n\n@@ -492,7 +492,10 @@ class LSTM(RNN):\n         )\n         self.input_spec = InputSpec(ndim=3)\n         if backend.backend() == \"tensorflow\" and backend.cudnn_ok(\n-            activation, recurrent_activation, unroll, use_bias\n+            cell.activation,\n+            cell.recurrent_activation,\n+            self.unroll,\n+            cell.use_bias,\n         ):\n             self.supports_jit = False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ac3962de3c7348f2a2e5d259a02a8e02a60e733", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 253 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 253 | Churn Cumulative: 253 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,253 @@\n+\"\"\"\n+Title: Sequence to sequence learning for performing number addition\n+Author: [Smerity](https://twitter.com/Smerity) and others\n+Date created: 2015/08/17\n+Last modified: 2020/04/17\n+Description: A model that learns to add strings of numbers, e.g. \"535+61\" -> \"596\".\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+In this example, we train a model to learn to add two numbers, provided as strings.\n+\n+**Example:**\n+\n+- Input: \"535+61\"\n+- Output: \"596\"\n+\n+Input may optionally be reversed, which was shown to increase performance in many tasks\n+ in: [Learning to Execute](http://arxiv.org/abs/1410.4615) and\n+[Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).\n+\n+Theoretically, sequence order inversion introduces shorter term dependencies between\n+ source and target for this problem.\n+\n+**Results:**\n+\n+For two digits (reversed):\n+\n++ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n+\n+Three digits (reversed):\n+\n++ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n+\n+Four digits (reversed):\n+\n++ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n+\n+Five digits (reversed):\n+\n++ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import keras_core as keras\n+from keras_core import layers\n+import numpy as np\n+\n+# Parameters for the model and dataset.\n+TRAINING_SIZE = 50000\n+DIGITS = 3\n+REVERSE = True\n+\n+# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n+# int is DIGITS.\n+MAXLEN = DIGITS + 1 + DIGITS\n+\n+\"\"\"\n+## Generate the data\n+\"\"\"\n+\n+\n+class CharacterTable:\n+    \"\"\"Given a set of characters:\n+    + Encode them to a one-hot integer representation\n+    + Decode the one-hot or integer representation to their character output\n+    + Decode a vector of probabilities to their character output\n+    \"\"\"\n+\n+    def __init__(self, chars):\n+        \"\"\"Initialize character table.\n+        # Arguments\n+            chars: Characters that can appear in the input.\n+        \"\"\"\n+        self.chars = sorted(set(chars))\n+        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n+        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n+\n+    def encode(self, C, num_rows):\n+        \"\"\"One-hot encode given string C.\n+        # Arguments\n+            C: string, to be encoded.\n+            num_rows: Number of rows in the returned one-hot encoding. This is\n+                used to keep the # of rows for each data the same.\n+        \"\"\"\n+        x = np.zeros((num_rows, len(self.chars)))\n+        for i, c in enumerate(C):\n+            x[i, self.char_indices[c]] = 1\n+        return x\n+\n+    def decode(self, x, calc_argmax=True):\n+        \"\"\"Decode the given vector or 2D array to their character output.\n+        # Arguments\n+            x: A vector or a 2D array of probabilities or one-hot representations;\n+                or a vector of character indices (used with `calc_argmax=False`).\n+            calc_argmax: Whether to find the character index with maximum\n+                probability, defaults to `True`.\n+        \"\"\"\n+        if calc_argmax:\n+            x = x.argmax(axis=-1)\n+        return \"\".join(self.indices_char[x] for x in x)\n+\n+\n+# All the numbers, plus sign and space for padding.\n+chars = \"0123456789+ \"\n+ctable = CharacterTable(chars)\n+\n+questions = []\n+expected = []\n+seen = set()\n+print(\"Generating data...\")\n+while len(questions) < TRAINING_SIZE:\n+    f = lambda: int(\n+        \"\".join(\n+            np.random.choice(list(\"0123456789\"))\n+            for i in range(np.random.randint(1, DIGITS + 1))\n+        )\n+    )\n+    a, b = f(), f()\n+    # Skip any addition questions we've already seen\n+    # Also skip any such that x+Y == Y+x (hence the sorting).\n+    key = tuple(sorted((a, b)))\n+    if key in seen:\n+        continue\n+    seen.add(key)\n+    # Pad the data with spaces such that it is always MAXLEN.\n+    q = \"{}+{}\".format(a, b)\n+    query = q + \" \" * (MAXLEN - len(q))\n+    ans = str(a + b)\n+    # Answers can be of maximum size DIGITS + 1.\n+    ans += \" \" * (DIGITS + 1 - len(ans))\n+    if REVERSE:\n+        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n+        # space used for padding.)\n+        query = query[::-1]\n+    questions.append(query)\n+    expected.append(ans)\n+print(\"Total questions:\", len(questions))\n+\n+\"\"\"\n+## Vectorize the data\n+\"\"\"\n+\n+print(\"Vectorization...\")\n+x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=bool)\n+y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=bool)\n+for i, sentence in enumerate(questions):\n+    x[i] = ctable.encode(sentence, MAXLEN)\n+for i, sentence in enumerate(expected):\n+    y[i] = ctable.encode(sentence, DIGITS + 1)\n+\n+# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n+# digits.\n+indices = np.arange(len(y))\n+np.random.shuffle(indices)\n+x = x[indices]\n+y = y[indices]\n+\n+# Explicitly set apart 10% for validation data that we never train over.\n+split_at = len(x) - len(x) // 10\n+(x_train, x_val) = x[:split_at], x[split_at:]\n+(y_train, y_val) = y[:split_at], y[split_at:]\n+\n+print(\"Training Data:\")\n+print(x_train.shape)\n+print(y_train.shape)\n+\n+print(\"Validation Data:\")\n+print(x_val.shape)\n+print(y_val.shape)\n+\n+\"\"\"\n+## Build the model\n+\"\"\"\n+\n+print(\"Build model...\")\n+num_layers = 1  # Try to add more LSTM layers!\n+\n+model = keras.Sequential()\n+# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n+# Note: In a situation where your input sequences have a variable length,\n+# use input_shape=(None, num_feature).\n+model.add(layers.Input((MAXLEN, len(chars))))\n+model.add(layers.LSTM(128))\n+# As the decoder RNN's input, repeatedly provide with the last output of\n+# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n+# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n+model.add(layers.RepeatVector(DIGITS + 1))\n+# The decoder RNN could be multiple layers stacked or a single layer.\n+for _ in range(num_layers):\n+    # By setting return_sequences to True, return not only the last output but\n+    # all the outputs so far in the form of (num_samples, timesteps,\n+    # output_dim). This is necessary as TimeDistributed in the below expects\n+    # the first dimension to be the timesteps.\n+    model.add(layers.LSTM(128, return_sequences=True))\n+\n+# Apply a dense layer to the every temporal slice of an input. For each of step\n+# of the output sequence, decide which character should be chosen.\n+model.add(layers.Dense(len(chars), activation=\"softmax\"))\n+model.compile(\n+    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n+)\n+model.summary()\n+\n+\"\"\"\n+## Train the model\n+\"\"\"\n+\n+epochs = 30\n+batch_size = 32\n+\n+\n+# Train the model each generation and show predictions against the validation\n+# dataset.\n+for epoch in range(1, epochs):\n+    print()\n+    print(\"Iteration\", epoch)\n+    model.fit(\n+        x_train,\n+        y_train,\n+        batch_size=batch_size,\n+        epochs=1,\n+        validation_data=(x_val, y_val),\n+    )\n+    # Select 10 samples from the validation set at random so we can visualize\n+    # errors.\n+    for i in range(10):\n+        ind = np.random.randint(0, len(x_val))\n+        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n+        preds = np.argmax(model.predict(rowx), axis=-1)\n+        q = ctable.decode(rowx[0])\n+        correct = ctable.decode(rowy[0])\n+        guess = ctable.decode(preds[0], calc_argmax=False)\n+        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n+        print(\"T\", correct, end=\" \")\n+        if correct == guess:\n+            print(\"☑ \" + guess)\n+        else:\n+            print(\"☒ \" + guess)\n+\n+\"\"\"\n+You'll get to 99+% validation accuracy after ~30 epochs.\n+\n+Example available on HuggingFace.\n+\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![Generic badge](https://img.shields.io/badge/🤗%20Model-Addition%20LSTM-black.svg)](https://huggingface.co/keras-io/addition-lstm) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-Addition%20LSTM-black.svg)](https://huggingface.co/spaces/keras-io/addition-lstm) |\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bdfb8aa16507bb9468e063a43e3980013c404ace", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 73 | Files Changed: 1 | Hunks: 17 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 105 | Churn Cumulative: 445 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,76 +14,64 @@\n # ==============================================================================\n \"\"\"E2E Tests for mnist_model.\"\"\"\n \n+import numpy as np\n import tensorflow.compat.v2 as tf\n+from tensorflow.compat.v2.experimental import dtensor\n \n from keras import backend\n-from keras.dtensor import dtensor_api as dtensor\n from keras.dtensor import integration_test_utils\n+from keras.dtensor import layout_map as layout_map_lib\n from keras.dtensor import test_util\n from keras.optimizers import adam\n from keras.utils import tf_utils\n \n \n class MnistTest(test_util.DTensorBaseTest):\n-    def test_mnist_training_cpu(self):\n-        devices = tf.config.list_physical_devices(\"CPU\")\n-        tf.config.set_logical_device_configuration(\n-            devices[0],\n-            [\n-                tf.config.LogicalDeviceConfiguration(),\n-            ]\n-            * 8,\n-        )\n-\n-        mesh = dtensor.create_mesh(\n-            devices=[\"CPU:%d\" % i for i in range(8)], mesh_dims=[(\"batch\", 8)]\n-        )\n-\n+    def setUp(self):\n+        super().setUp()\n         backend.enable_tf_random_generator()\n-        # Needed by keras initializers.\n         tf_utils.set_random_seed(1337)\n-\n-        model = integration_test_utils.get_model_with_layout_map(\n-            integration_test_utils.get_all_replicated_layout_map(mesh)\n+        global_ids = test_util.create_device_ids_array((2,))\n+        local_device_ids = np.ravel(global_ids).tolist()\n+        mesh_dict = {\n+            device: tf.experimental.dtensor.Mesh(\n+                [\"batch\"],\n+                global_ids,\n+                local_device_ids,\n+                test_util.create_device_list((2,), device),\n             )\n+            for device in (\"CPU\", \"GPU\", \"TPU\")\n+        }\n+        self.mesh = self.configTestMesh(mesh_dict)\n \n-        optimizer = adam.Adam(learning_rate=0.001, mesh=mesh)\n+    def test_mnist_training(self):\n+        layout_map = layout_map_lib.LayoutMap(self.mesh)\n+        with layout_map.scope():\n+            model = integration_test_utils.get_model()\n+\n+        optimizer = adam.Adam(learning_rate=0.001, mesh=self.mesh)\n         optimizer.build(model.trainable_variables)\n \n         train_losses = integration_test_utils.train_mnist_model_batch_sharded(\n             model,\n             optimizer,\n-            mesh,\n+            self.mesh,\n             num_epochs=3,\n-            steps_per_epoch=100,\n+            steps_per_epoch=20,\n             global_batch_size=64,\n         )\n         # Make sure the losses are decreasing\n         self.assertEqual(train_losses, sorted(train_losses, reverse=True))\n \n     def test_model_fit(self):\n-        devices = tf.config.list_physical_devices(\"CPU\")\n-        tf.config.set_logical_device_configuration(\n-            devices[0],\n-            [\n-                tf.config.LogicalDeviceConfiguration(),\n-            ]\n-            * 8,\n-        )\n+        if self.mesh.device_type() == \"GPU\":\n+            self.skipTest(\"TODO(b/292596476)\")\n \n-        mesh = dtensor.create_mesh(\n-            devices=[\"CPU:%d\" % i for i in range(8)], mesh_dims=[(\"batch\", 8)]\n-        )\n+        layout_map = layout_map_lib.LayoutMap(self.mesh)\n+        with layout_map.scope():\n+            model = integration_test_utils.get_model()\n \n-        backend.enable_tf_random_generator()\n-        # Needed by keras initializers.\n-        tf_utils.set_random_seed(1337)\n-\n-        model = integration_test_utils.get_model_with_layout_map(\n-            integration_test_utils.get_all_replicated_layout_map(mesh)\n-        )\n-\n-        optimizer = adam.Adam(learning_rate=0.001, mesh=mesh)\n+        optimizer = adam.Adam(learning_rate=0.001, mesh=self.mesh)\n         optimizer.build(model.trainable_variables)\n \n         global_batch_size = 64\n@@ -100,7 +88,7 @@ class MnistTest(test_util.DTensorBaseTest):\n             def _create_batch_layout(tensor_spec):\n                 rank = len(tensor_spec.shape) + 1\n                 return dtensor.Layout.batch_sharded(\n-                    mesh, batch_dim=\"batch\", rank=rank\n+                    self.mesh, batch_dim=\"batch\", rank=rank\n                 )\n \n             layouts = tf.nest.map_structure(\n@@ -109,7 +97,7 @@ class MnistTest(test_util.DTensorBaseTest):\n \n             return dtensor.DTensorDataset(\n                 dataset=dataset,\n-                mesh=mesh,\n+                mesh=self.mesh,\n                 layouts=layouts,\n                 global_batch_size=global_batch_size,\n                 dataset_already_batched=False,\n@@ -123,35 +111,6 @@ class MnistTest(test_util.DTensorBaseTest):\n         model.fit(train_ds, steps_per_epoch=10)\n         model.evaluate(eval_ds, steps=10)\n \n-    def DISABLED_test_mnist_training_tpu(self):\n-        # TODO(scottzhu): Enable TPU test once the dtensor_test rule is migrated\n-        # out of learning/brain\n-        dtensor.initialize_accelerator_system()\n-        total_tpu_device_count = dtensor.num_global_devices(\"TPU\")\n-        mesh_shape = [total_tpu_device_count]\n-        mesh = dtensor.create_tpu_mesh([\"batch\"], mesh_shape, \"tpu_mesh\")\n-\n-        # Needed by keras initializers.\n-        tf_utils.set_random_seed(1337)\n-\n-        model = integration_test_utils.get_model_with_layout_map(\n-            integration_test_utils.get_all_replicated_layout_map(mesh)\n-        )\n-\n-        optimizer = adam.Adam(learning_rate=0.001, mesh=mesh)\n-        optimizer.build(model.trainable_variables)\n-\n-        train_losses = integration_test_utils.train_mnist_model_batch_sharded(\n-            model,\n-            optimizer,\n-            mesh,\n-            num_epochs=3,\n-            steps_per_epoch=100,\n-            global_batch_size=64,\n-        )\n-        # Make sure the losses are decreasing\n-        self.assertEqual(train_losses, sorted(train_losses, reverse=True))\n-\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8163f753f886cd24bbbee53310d9f65a7bf95b30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 1789 | Contributors (this commit): 8 | Commits (past 90d): 33 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -100,6 +100,13 @@ class Trainer:\n \n     @jit_compile.setter\n     def jit_compile(self, value):\n+        if value and not model_supports_jit(self):\n+            warnings.warn(\n+                \"Model doesn't support `jit_compile=True`. \"\n+                \"Proceeding with `jit_compile=False`.\"\n+            )\n+            self._jit_compile = False\n+        else:\n             self._jit_compile = value\n \n     @property\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#20839098df6381c1e008033773ee326d0613c148", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 306 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 306 | Churn Cumulative: 306 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,306 @@\n+\"\"\"\n+Title: Image Classification using BigTransfer (BiT)\n+Author: [Sayan Nath](https://twitter.com/sayannath2350)\n+Date created: 2021/09/24\n+Last modified: 2021/09/24\n+Description: BigTransfer (BiT) State-of-the-art transfer learning for image classification.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+BigTransfer (also known as BiT) is a state-of-the-art transfer learning method for image\n+classification. Transfer of pre-trained representations improves sample efficiency and\n+simplifies hyperparameter tuning when training deep neural networks for vision. BiT\n+revisit the paradigm of pre-training on large supervised datasets and fine-tuning the\n+model on a target task. The importance of appropriately choosing normalization layers and\n+scaling the architecture capacity as the amount of pre-training data increases.\n+\n+BigTransfer(BiT) is trained on public datasets, along with code in\n+[TF2, Jax and Pytorch](https://github.com/google-research/big_transfer). This will help anyone to reach\n+state of the art performance on their task of interest, even with just a handful of\n+labeled images per class.\n+\n+You can find BiT models pre-trained on\n+[ImageNet](https://image-net.org/challenges/LSVRC/2012/index) and ImageNet-21k in\n+[TFHub](https://tfhub.dev/google/collections/bit/1) as TensorFlow2 SavedModels that you\n+can use easily as Keras Layers. There are a variety of sizes ranging from a standard\n+ResNet50 to a ResNet152x4 (152 layers deep, 4x wider than a typical ResNet50) for users\n+with larger computational and memory budgets but higher accuracy requirements.\n+\n+![](https://i.imgur.com/XeWVfe7.jpeg)\n+Figure: The x-axis shows the number of images used per class, ranging from 1 to the full\n+dataset. On the plots on the left, the curve in blue above is our BiT-L model, whereas\n+the curve below is a ResNet-50 pre-trained on ImageNet (ILSVRC-2012).\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+import os\n+os.environ['KERAS_BACKEND'] = 'tensorflow'\n+\n+import keras_core as keras\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+import tensorflow as tf\n+import tensorflow_hub as hub\n+import tensorflow_datasets as tfds\n+\n+tfds.disable_progress_bar()\n+\n+SEEDS = 42\n+\n+np.random.seed(SEEDS)\n+keras.utils.set_random_seed(SEEDS)\n+\n+\"\"\"\n+## Gather Flower Dataset\n+\"\"\"\n+\n+train_ds, validation_ds = tfds.load(\n+    \"tf_flowers\",\n+    split=[\"train[:85%]\", \"train[85%:]\"],\n+    as_supervised=True,\n+)\n+\n+\"\"\"\n+## Visualise the dataset\n+\"\"\"\n+\n+plt.figure(figsize=(10, 10))\n+for i, (image, label) in enumerate(train_ds.take(9)):\n+    ax = plt.subplot(3, 3, i + 1)\n+    plt.imshow(image)\n+    plt.title(int(label))\n+    plt.axis(\"off\")\n+\n+\"\"\"\n+## Define hyperparameters\n+\"\"\"\n+\n+RESIZE_TO = 384\n+CROP_TO = 224\n+BATCH_SIZE = 64\n+STEPS_PER_EPOCH = 10\n+AUTO = tf.data.AUTOTUNE  # optimise the pipeline performance\n+NUM_CLASSES = 5  # number of classes\n+SCHEDULE_LENGTH = 500  # we will train on lower resolution images and will still attain good results\n+SCHEDULE_BOUNDARIES = [\n+    200,\n+    300,\n+    400,\n+]  # more the dataset size the schedule length increase\n+\n+\"\"\"\n+The hyperparamteres like `SCHEDULE_LENGTH` and `SCHEDULE_BOUNDARIES` are determined based\n+on empirical results. The method has been explained in the [original\n+paper](https://arxiv.org/abs/1912.11370) and in their [Google AI Blog\n+Post](https://ai.googleblog.com/2020/05/open-sourcing-bit-exploring-large-scale.html).\n+\n+The `SCHEDULE_LENGTH` is aslo determined whether to use [MixUp\n+Augmentation](https://arxiv.org/abs/1710.09412) or not. You can also find an easy MixUp\n+Implementation in [Keras Coding Examples](https://keras.io/examples/vision/mixup/).\n+\n+![](https://i.imgur.com/oSaIBYZ.jpeg)\n+\"\"\"\n+\n+\"\"\"\n+## Define preprocessing helper functions\n+\"\"\"\n+\n+SCHEDULE_LENGTH = SCHEDULE_LENGTH * 512 / BATCH_SIZE\n+\n+\n+@tf.function\n+def preprocess_train(image, label):\n+    image = tf.image.random_flip_left_right(image)\n+    image = tf.image.resize(image, (RESIZE_TO, RESIZE_TO))\n+    image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))\n+    image = image / 255.0\n+    return (image, label)\n+\n+\n+@tf.function\n+def preprocess_test(image, label):\n+    image = tf.image.resize(image, (RESIZE_TO, RESIZE_TO))\n+    image = image / 255.0\n+    return (image, label)\n+\n+\n+DATASET_NUM_TRAIN_EXAMPLES = train_ds.cardinality().numpy()\n+\n+repeat_count = int(\n+    SCHEDULE_LENGTH * BATCH_SIZE / DATASET_NUM_TRAIN_EXAMPLES * STEPS_PER_EPOCH\n+)\n+repeat_count += 50 + 1  # To ensure at least there are 50 epochs of training\n+\n+\"\"\"\n+## Define the data pipeline\n+\"\"\"\n+\n+# Training pipeline\n+pipeline_train = (\n+    train_ds.shuffle(10000)\n+    .repeat(repeat_count)  # Repeat dataset_size / num_steps\n+    .map(preprocess_train, num_parallel_calls=AUTO)\n+    .batch(BATCH_SIZE)\n+    .prefetch(AUTO)\n+)\n+\n+# Validation pipeline\n+pipeline_validation = (\n+    validation_ds.map(preprocess_test, num_parallel_calls=AUTO)\n+    .batch(BATCH_SIZE)\n+    .prefetch(AUTO)\n+)\n+\n+\"\"\"\n+## Visualise the training samples\n+\"\"\"\n+\n+image_batch, label_batch = next(iter(pipeline_train))\n+\n+plt.figure(figsize=(10, 10))\n+for n in range(25):\n+    ax = plt.subplot(5, 5, n + 1)\n+    plt.imshow(image_batch[n])\n+    plt.title(label_batch[n].numpy())\n+    plt.axis(\"off\")\n+\n+\"\"\"\n+## Load pretrained TF-Hub model into a `KerasLayer`\n+\"\"\"\n+\n+bit_model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n+bit_module = hub.KerasLayer(bit_model_url)\n+\n+\"\"\"\n+## Create BigTransfer (BiT) model\n+\n+To create the new model, we:\n+\n+1. Cut off the BiT model’s original head. This leaves us with the “pre-logits” output.\n+We do not have to do this if we use the ‘feature extractor’ models (i.e. all those in\n+subdirectories titled `feature_vectors`), since for those models the head has already\n+been cut off.\n+\n+2. Add a new head with the number of outputs equal to the number of classes of our new\n+task. Note that it is important that we initialise the head to all zeroes.\n+\"\"\"\n+\n+\n+class MyBiTModel(keras.Model):\n+    def __init__(self, num_classes, module, **kwargs):\n+        super().__init__(**kwargs)\n+\n+        self.num_classes = num_classes\n+        self.head = keras.layers.Dense(num_classes, kernel_initializer=\"zeros\")\n+        self.bit_model = module\n+\n+    def call(self, images):\n+        bit_embedding = self.bit_model(images)\n+        return self.head(bit_embedding)\n+\n+\n+model = MyBiTModel(num_classes=NUM_CLASSES, module=bit_module)\n+\n+\"\"\"\n+## Define optimizer and loss\n+\"\"\"\n+\n+learning_rate = 0.003 * BATCH_SIZE / 512\n+\n+# Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES.\n+lr_schedule = keras.optimizers.schedules.PiecewiseConstantDecay(\n+    boundaries=SCHEDULE_BOUNDARIES,\n+    values=[\n+        learning_rate,\n+        learning_rate * 0.1,\n+        learning_rate * 0.01,\n+        learning_rate * 0.001,\n+    ],\n+)\n+optimizer = keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n+\n+loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n+\n+\"\"\"\n+## Compile the model\n+\"\"\"\n+\n+model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n+\n+\"\"\"\n+## Set up callbacks\n+\"\"\"\n+\n+train_callbacks = [\n+    keras.callbacks.EarlyStopping(\n+        monitor=\"val_accuracy\", patience=2, restore_best_weights=True\n+    )\n+]\n+\n+\"\"\"\n+## Train the model\n+\"\"\"\n+\n+history = model.fit(\n+    pipeline_train,\n+    batch_size=BATCH_SIZE,\n+    epochs=int(SCHEDULE_LENGTH / STEPS_PER_EPOCH),\n+    steps_per_epoch=STEPS_PER_EPOCH,\n+    validation_data=pipeline_validation,\n+    callbacks=train_callbacks,\n+)\n+\n+\"\"\"\n+## Plot the training and validation metrics\n+\"\"\"\n+\n+\n+def plot_hist(hist):\n+    plt.plot(hist.history[\"accuracy\"])\n+    plt.plot(hist.history[\"val_accuracy\"])\n+    plt.plot(hist.history[\"loss\"])\n+    plt.plot(hist.history[\"val_loss\"])\n+    plt.title(\"Training Progress\")\n+    plt.ylabel(\"Accuracy/Loss\")\n+    plt.xlabel(\"Epochs\")\n+    plt.legend(\n+        [\"train_acc\", \"val_acc\", \"train_loss\", \"val_loss\"], loc=\"upper left\"\n+    )\n+    plt.show()\n+\n+\n+plot_hist(history)\n+\n+\"\"\"\n+## Evaluate the model\n+\"\"\"\n+\n+accuracy = model.evaluate(pipeline_validation)[1] * 100\n+print(\"Accuracy: {:.2f}%\".format(accuracy))\n+\n+\"\"\"\n+## Conclusion\n+\n+BiT performs well across a surprisingly wide range of data regimes\n+-- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on\n+ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark\n+(VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class,\n+and 97.0% on CIFAR-10 with 10 examples per class.\n+\n+![](https://i.imgur.com/b1Lw5fz.png)\n+\n+You can experiment further with the BigTransfer Method by following the\n+[original paper](https://arxiv.org/abs/1912.11370).\n+\n+\n+**Example available on HuggingFace**\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-bit-black.svg)](https://huggingface.co/keras-io/bit) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-bit-black.svg)](https://huggingface.co/spaces/keras-io/siamese-contrastive) |\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
