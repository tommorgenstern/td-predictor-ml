{"custom_id": "scrapy#010edfe85caa72b4c366f2dada0f79f1f91e43ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 316 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -129,6 +129,9 @@ class Request(object_ref):\n                      :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n                      may modify the :class:`~scrapy.http.Request` object.\n \n+        To translate a cURL command into a Scrapy request,\n+        you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n+\n        \"\"\"\n         request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n         request_kwargs.update(kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#16f2cb4a83033b63f3adf7d7cd8b7982b8b094ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 514 | Contributors (this commit): 22 | Commits (past 90d): 3 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -30,6 +30,11 @@ setup(\n     name='Scrapy',\n     version=version,\n     url='https://scrapy.org',\n+    project_urls = {\n+        'Documentation': 'https://docs.scrapy.org/',\n+        'Source': 'https://github.com/scrapy/scrapy',\n+        'Tracker': 'https://github.com/scrapy/scrapy/issues',\n+    },\n     description='A high-level Web Crawling and Web Scraping framework',\n     long_description=open('README.rst').read(),\n     author='Scrapy developers',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2d5d357a7ae50eaee957d1a2f8fc8ad1d9f3f24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 23 | Files Changed: 7 | Hunks: 13 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 54 | Churn Cumulative: 3944 | Contributors (this commit): 45 | Commits (past 90d): 14 | Contributors (cumulative): 88 | DMM Complexity: 1.0\n\nDIFF:\n@@ -90,8 +90,7 @@ class Command(ScrapyCommand):\n             'module': module,\n             'name': name,\n             'domain': domain,\n-            'classname': '%sSpider' % ''.join(s.capitalize() \\\n-                for s in module.split('_'))\n+            'classname': '%sSpider' % ''.join(s.capitalize() for s in module.split('_'))\n         }\n         if self.settings.get('NEWSPIDER_MODULE'):\n             spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n@@ -102,8 +101,8 @@ class Command(ScrapyCommand):\n         spider_file = \"%s.py\" % join(spiders_dir, module)\n         shutil.copyfile(template_file, spider_file)\n         render_templatefile(spider_file, **tvars)\n-        print(\"Created spider %r using template %r \" % (name, \\\n-            template_name), end=('' if spiders_module else '\\n'))\n+        print(\"Created spider %r using template %r \"\n+              % (name, template_name), end=('' if spiders_module else '\\n'))\n         if spiders_module:\n             print(\"in module:\\n  %s.%s\" % (spiders_module.__name__, module))\n \n\n@@ -37,7 +37,7 @@ class Command(ScrapyCommand):\n             help=\"evaluate the code in the shell, print the result and exit\")\n         parser.add_option(\"--spider\", dest=\"spider\",\n             help=\"use this spider\")\n-        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", \\\n+        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\",\n             default=False, help=\"do not handle HTTP 3xx status codes and print response as-is\")\n \n     def update_vars(self, vars):\n\n@@ -35,8 +35,10 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n             for method in self.methods['process_request']:\n                 response = yield deferred_from_coro(method(request=request, spider=spider))\n                 if response is not None and not isinstance(response, (Response, Request)):\n-                    raise _InvalidOutput('Middleware %s.process_request must return None, Response or Request, got %s' % \\\n-                                         (method.__self__.__class__.__name__, response.__class__.__name__))\n+                    raise _InvalidOutput(\n+                        \"Middleware %s.process_request must return None, Response or Request, got %s\"\n+                        % (method.__self__.__class__.__name__, response.__class__.__name__)\n+                    )\n                 if response:\n                     defer.returnValue(response)\n             defer.returnValue((yield download_func(request=request, spider=spider)))\n@@ -50,8 +52,10 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n             for method in self.methods['process_response']:\n                 response = yield deferred_from_coro(method(request=request, response=response, spider=spider))\n                 if not isinstance(response, (Response, Request)):\n-                    raise _InvalidOutput('Middleware %s.process_response must return Response or Request, got %s' % \\\n-                                         (method.__self__.__class__.__name__, type(response)))\n+                    raise _InvalidOutput(\n+                        \"Middleware %s.process_response must return Response or Request, got %s\"\n+                        % (method.__self__.__class__.__name__, type(response))\n+                    )\n                 if isinstance(response, Request):\n                     defer.returnValue(response)\n             defer.returnValue(response)\n@@ -62,8 +66,10 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n             for method in self.methods['process_exception']:\n                 response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))\n                 if response is not None and not isinstance(response, (Response, Request)):\n-                    raise _InvalidOutput('Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n-                                         (method.__self__.__class__.__name__, type(response)))\n+                    raise _InvalidOutput(\n+                        \"Middleware %s.process_exception must return None, Response or Request, got %s\"\n+                        % (method.__self__.__class__.__name__, type(response))\n+                    )\n                 if response:\n                     defer.returnValue(response)\n             defer.returnValue(_failure)\n\n@@ -277,10 +277,9 @@ class ExecutionEngine:\n         next loop and this function is guaranteed to be called (at least) once\n         again for this spider.\n         \"\"\"\n-        res = self.signals.send_catch_log(signal=signals.spider_idle, \\\n+        res = self.signals.send_catch_log(signal=signals.spider_idle,\n             spider=spider, dont_log=DontCloseSpider)\n-        if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n-                for _, x in res):\n+        if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) for _, x in res):\n             return\n \n         if self.spider_is_idle(spider):\n\n@@ -115,8 +115,8 @@ class MailSender:\n         from twisted.mail.smtp import ESMTPSenderFactory\n         msg = BytesIO(msg)\n         d = defer.Deferred()\n-        factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, \\\n-            to_addrs, msg, d, heloFallback=True, requireAuthentication=False, \\\n+        factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom,\n+            to_addrs, msg, d, heloFallback=True, requireAuthentication=False,\n             requireTransportSecurity=self.smtptls)\n         factory.noisy = False\n \n\n@@ -7,9 +7,12 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n \n def attribute(obj, oldattr, newattr, version='0.12'):\n     cname = obj.__class__.__name__\n-    warnings.warn(\"%s.%s attribute is deprecated and will be no longer supported \"\n-        \"in Scrapy %s, use %s.%s attribute instead\" % \\\n-        (cname, oldattr, version, cname, newattr), ScrapyDeprecationWarning, stacklevel=3)\n+    warnings.warn(\n+        \"%s.%s attribute is deprecated and will be no longer supported \"\n+        \"in Scrapy %s, use %s.%s attribute instead\"\n+        % (cname, oldattr, version, cname, newattr),\n+        ScrapyDeprecationWarning,\n+        stacklevel=3)\n \n \n def create_deprecated_class(name, new_class, clsdict=None,\n@@ -17,10 +20,10 @@ def create_deprecated_class(name, new_class, clsdict=None,\n                             warn_once=True,\n                             old_class_path=None,\n                             new_class_path=None,\n-                            subclass_warn_message=\"{cls} inherits from \"\\\n-                                    \"deprecated class {old}, please inherit \"\\\n+                            subclass_warn_message=\"{cls} inherits from \"\n+                                    \"deprecated class {old}, please inherit \"\n                                     \"from {new}.\",\n-                            instance_warn_message=\"{cls} is deprecated, \"\\\n+                            instance_warn_message=\"{cls} is deprecated, \"\n                                     \"instantiate {new} instead.\"):\n     \"\"\"\n     Return a \"deprecated\" class that causes its subclasses to issue a warning.\n\n@@ -325,7 +325,8 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item['results'], [(True, 'ITSME')])\n-        self.assertEqual(self.pipe._mockcalled, \\\n+        self.assertEqual(\n+            self.pipe._mockcalled,\n             ['get_media_requests', 'media_to_download', 'item_completed'])\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4270e0a0da66a2cb3a8e904c5ea74f84b7f9d041", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 32 | Files Changed: 10 | Hunks: 18 | Methods Changed: 18 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 77 | Churn Cumulative: 2950 | Contributors (this commit): 40 | Commits (past 90d): 22 | Contributors (cumulative): 102 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,8 +49,8 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         if len(args) != 1 or not is_url(args[0]):\n             raise UsageError()\n-        cb = lambda x: self._print_response(x, opts)\n-        request = Request(args[0], callback=cb, dont_filter=True)\n+        request = Request(args[0], callback=self._print_response,\n+                          cb_kwargs={\"opts\": opts}, dont_filter=True)\n         # by default, let the framework handle redirects,\n         # i.e. command handles all codes expect 3xx\n         if not opts.no_redirect:\n\n@@ -147,8 +147,8 @@ class Command(ScrapyCommand):\n                 logger.error('Unable to find spider for: %(url)s', {'url': url})\n \n         # Request requires callback argument as callable or None, not string\n-        request = Request(url, None)\n-        _start_requests = lambda s: [self.prepare_request(s, request, opts)]\n+        def _start_requests(spider):\n+            yield self.prepare_request(spider, Request(url, None), opts)\n         self.spidercls.start_requests = _start_requests\n \n     def start_parsing(self, url, opts):\n\n@@ -14,13 +14,12 @@ from scrapy.responsetypes import responsetypes\n def _parsed_url_args(parsed):\n     # Assume parsed is urlparse-d from Request.url,\n     # which was passed via safe_url_string and is ascii-only.\n-    b = lambda s: to_bytes(s, encoding='ascii')\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n-    path = b(path)\n-    host = b(parsed.hostname)\n+    path = to_bytes(path, encoding=\"ascii\")\n+    host = to_bytes(parsed.hostname, encoding=\"ascii\")\n     port = parsed.port\n-    scheme = b(parsed.scheme)\n-    netloc = b(parsed.netloc)\n+    scheme = to_bytes(parsed.scheme, encoding=\"ascii\")\n+    netloc = to_bytes(parsed.netloc, encoding=\"ascii\")\n     if port is None:\n         port = 443 if scheme == b'https' else 80\n     return scheme, netloc, host, port, path\n\n@@ -45,8 +45,14 @@ IGNORED_EXTENSIONS = [\n \n \n _re_type = type(re.compile(\"\", 0))\n-_matches = lambda url, regexs: any(r.search(url) for r in regexs)\n-_is_valid_url = lambda url: url.split('://', 1)[0] in {'http', 'https', 'file', 'ftp'}\n+\n+\n+def _matches(url, regexs):\n+    return any(r.search(url) for r in regexs)\n+\n+\n+def _is_valid_url(url):\n+    return url.split('://', 1)[0] in {'http', 'https', 'file', 'ftp'}\n \n \n class FilteringLinkExtractor:\n\n@@ -98,11 +98,9 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n                  unique=True, process_value=None, deny_extensions=None, restrict_css=(),\n                  strip=True, restrict_text=None):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n-        tag_func = lambda x: x in tags\n-        attr_func = lambda x: x in attrs\n         lx = LxmlParserLinkExtractor(\n-            tag=tag_func,\n-            attr=attr_func,\n+            tag=lambda x: x in tags,\n+            attr=lambda x: x in attrs,\n             unique=unique,\n             process=process_value,\n             strip=strip,\n\n@@ -13,10 +13,9 @@ from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n class CookiesMiddlewareTest(TestCase):\n \n     def assertCookieValEqual(self, first, second, msg=None):\n-        cookievaleq = lambda cv: re.split(r';\\s*', cv.decode('latin1'))\n-        return self.assertEqual(\n-            sorted(cookievaleq(first)),\n-            sorted(cookievaleq(second)), msg)\n+        def split_cookies(cookies):\n+            return sorted(re.split(r\";\\s*\", cookies.decode(\"latin1\")))\n+        return self.assertEqual(split_cookies(first), split_cookies(second), msg=msg)\n \n     def setUp(self):\n         self.spider = Spider('foo')\n\n@@ -215,11 +215,12 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         return CsvItemExporter(self.output, **kwargs)\n \n     def assertCsvEqual(self, first, second, msg=None):\n-        first = to_unicode(first)\n-        second = to_unicode(second)\n-        csvsplit = lambda csv: [sorted(re.split(r'(,|\\s+)', line))\n-                                for line in csv.splitlines(True)]\n-        return self.assertEqual(csvsplit(first), csvsplit(second), msg)\n+        def split_csv(csv):\n+            return [\n+                sorted(re.split(r\"(,|\\s+)\", line))\n+                for line in to_unicode(csv).splitlines(True)\n+            ]\n+        return self.assertEqual(split_csv(first), split_csv(second), msg=msg)\n \n     def _check_output(self):\n         self.assertCsvEqual(to_unicode(self.output.getvalue()), u'age,name\\r\\n22,John\\xa3\\r\\n')\n\n@@ -199,12 +199,19 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n \n     pipeline_class = MockedMediaPipeline\n \n+    def _callback(self, result):\n+        self.pipe._mockcalled.append('request_callback')\n+        return result\n+\n+    def _errback(self, result):\n+        self.pipe._mockcalled.append('request_errback')\n+        return result\n+\n     @inlineCallbacks\n     def test_result_succeed(self):\n-        cb = lambda _: self.pipe._mockcalled.append('request_callback') or _\n-        eb = lambda _: self.pipe._mockcalled.append('request_errback') or _\n         rsp = Response('http://url1')\n-        req = Request('http://url1', meta=dict(response=rsp), callback=cb, errback=eb)\n+        req = Request('http://url1', meta=dict(response=rsp),\n+                      callback=self._callback, errback=self._errback)\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item['results'], [(True, rsp)])\n@@ -215,10 +222,9 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     @inlineCallbacks\n     def test_result_failure(self):\n         self.pipe.LOG_FAILED_RESULTS = False\n-        cb = lambda _: self.pipe._mockcalled.append('request_callback') or _\n-        eb = lambda _: self.pipe._mockcalled.append('request_errback') or _\n         fail = Failure(Exception())\n-        req = Request('http://url1', meta=dict(response=fail), callback=cb, errback=eb)\n+        req = Request('http://url1', meta=dict(response=fail),\n+                      callback=self._callback, errback=self._errback)\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item['results'], [(False, fail)])\n\n@@ -145,7 +145,9 @@ class UtilsPythonTestCase(unittest.TestCase):\n \n         get_z = operator.itemgetter('z')\n         get_meta = operator.attrgetter('meta')\n-        compare_z = lambda obj: get_z(get_meta(obj))\n+\n+        def compare_z(obj):\n+            return get_z(get_meta(obj))\n \n         self.assertTrue(equal_attributes(a, b, [compare_z, 'x']))\n         # fail z equality\n\n@@ -90,8 +90,10 @@ class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n class SendCatchLogTest2(unittest.TestCase):\n \n     def test_error_logged_if_deferred_not_supported(self):\n+        def test_handler():\n+            return defer.Deferred()\n+\n         test_signal = object()\n-        test_handler = lambda: defer.Deferred()\n         dispatcher.connect(test_handler, test_signal)\n         with LogCapture() as l:\n             send_catch_log(test_signal)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c887fe37adfe529ed2afabd2d08c3aac00a819c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 706 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -146,9 +146,8 @@ class Command(ScrapyCommand):\n             if not self.spidercls:\n                 logger.error('Unable to find spider for: %(url)s', {'url': url})\n \n-        # Request requires callback argument as callable or None, not string\n         def _start_requests(spider):\n-            yield self.prepare_request(spider, Request(url, None), opts)\n+            yield self.prepare_request(spider, Request(url), opts)\n         self.spidercls.start_requests = _start_requests\n \n     def start_parsing(self, url, opts):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#862f0301e2cb166ca87ce985d51683b46dcf56ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 2035 | Contributors (this commit): 23 | Commits (past 90d): 11 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -341,20 +341,6 @@ class ScrapyAgent:\n             headers.removeHeader(b'Proxy-Authorization')\n         if request.body:\n             bodyproducer = _RequestBodyProducer(request.body)\n-        elif method == b'POST':\n-            # Setting Content-Length: 0 even for POST requests is not a\n-            # MUST per HTTP RFCs, but it's common behavior, and some\n-            # servers require this, otherwise returning HTTP 411 Length required\n-            #\n-            # RFC 7230#section-3.3.2:\n-            # \"a Content-Length header field is normally sent in a POST\n-            # request even when the value is 0 (indicating an empty payload body).\"\n-            #\n-            # Twisted < 17 will not add \"Content-Length: 0\" by itself;\n-            # Twisted >= 17 fixes this;\n-            # Using a producer with an empty-string sends `0` as Content-Length\n-            # for all versions of Twisted.\n-            bodyproducer = _RequestBodyProducer(b'')\n         else:\n             bodyproducer = None\n         start_time = time()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c4a5e3f0da3e674ecb7393c0894098984d6aa571", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 15 | Methods Changed: 16 | Complexity Δ (Sum/Max): 13/12 | Churn Δ: 24 | Churn Cumulative: 2675 | Contributors (this commit): 32 | Commits (past 90d): 21 | Contributors (cumulative): 47 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -33,9 +33,8 @@ logger = logging.getLogger(__name__)\n class HTTP11DownloadHandler:\n     lazy = False\n \n-    def __init__(self, settings, crawler=None, source=\"http11\"):\n+    def __init__(self, settings, crawler=None):\n         self._crawler = crawler\n-        self._source = source\n \n         from twisted.internet import reactor\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n@@ -71,8 +70,8 @@ class HTTP11DownloadHandler:\n         self._disconnect_timeout = 1\n \n     @classmethod\n-    def from_crawler(cls, crawler, **kwargs):\n-        return cls(crawler.settings, crawler, **kwargs)\n+    def from_crawler(cls, crawler):\n+        return cls(crawler.settings, crawler)\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n@@ -83,7 +82,6 @@ class HTTP11DownloadHandler:\n             warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),\n             fail_on_dataloss=self._fail_on_dataloss,\n             crawler=self._crawler,\n-            source=self._source,\n         )\n         return agent.download_request(request)\n \n@@ -281,7 +279,7 @@ class ScrapyAgent:\n     _TunnelingAgent = TunnelingAgent\n \n     def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,\n-                 maxsize=0, warnsize=0, fail_on_dataloss=True, crawler=None, source=None):\n+                 maxsize=0, warnsize=0, fail_on_dataloss=True, crawler=None):\n         self._contextFactory = contextFactory\n         self._connectTimeout = connectTimeout\n         self._bindAddress = bindAddress\n@@ -291,7 +289,6 @@ class ScrapyAgent:\n         self._fail_on_dataloss = fail_on_dataloss\n         self._txresponse = None\n         self._crawler = crawler\n-        self._source = source\n \n     def _get_agent(self, request, timeout):\n         from twisted.internet import reactor\n@@ -430,7 +427,6 @@ class ScrapyAgent:\n                 warnsize=warnsize,\n                 fail_on_dataloss=fail_on_dataloss,\n                 crawler=self._crawler,\n-                source=self._source,\n             )\n         )\n \n@@ -468,9 +464,7 @@ class _RequestBodyProducer:\n \n class _ResponseReader(protocol.Protocol):\n \n-    def __init__(\n-        self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss, crawler, source\n-    ):\n+    def __init__(self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss, crawler):\n         self._finished = finished\n         self._txresponse = txresponse\n         self._request = request\n@@ -483,7 +477,6 @@ class _ResponseReader(protocol.Protocol):\n         self._bytes_received = 0\n         self._certificate = None\n         self._crawler = crawler\n-        self._source = source\n \n     def connectionMade(self):\n         if self._certificate is None:\n@@ -503,7 +496,6 @@ class _ResponseReader(protocol.Protocol):\n             data=bodyBytes,\n             request=self._request,\n             spider=self._crawler.spider,\n-            source=self._source,\n         )\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n\n@@ -73,7 +73,6 @@ class S3DownloadHandler:\n             objcls=httpdownloadhandler,\n             settings=settings,\n             crawler=crawler,\n-            source=\"s3\",\n         )\n         self._download_http = _http_handler.download_request\n \n\n@@ -112,7 +112,6 @@ class CrawlerRun:\n         self.itemerror = []\n         self.itemresp = []\n         self.bytes = defaultdict(lambda: list())\n-        self.bytes_source = set()\n         self.signals_caught = {}\n         self.spider_class = spider_class\n \n@@ -166,9 +165,8 @@ class CrawlerRun:\n     def item_scraped(self, item, spider, response):\n         self.itemresp.append((item, response))\n \n-    def bytes_received(self, data, request, spider, source):\n+    def bytes_received(self, data, request, spider):\n         self.bytes[request].append(data)\n-        self.bytes_source.add(source)\n \n     def request_scheduled(self, request, spider):\n         self.reqplug.append((request, spider))\n@@ -281,7 +279,6 @@ class EngineTest(unittest.TestCase):\n \n     def _assert_bytes_received(self):\n         self.assertEqual(9, len(self.run.bytes))\n-        self.assertEqual(self.run.bytes_source, set([\"http11\"]))\n         for request, data in self.run.bytes.items():\n             joined_data = b\"\".join(data)\n             if self.run.getpath(request.url) == \"/\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#24a1d9acae776bc195e6078394ee159b42275833", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 210 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -14,6 +14,7 @@\n import sys\n from datetime import datetime\n from os import path\n+from pathlib import Path\n \n # If your extensions are in another directory, add it here. If the directory\n # is relative to the documentation root, use os.path.abspath to make it\n@@ -59,10 +60,10 @@ copyright = '2008–{}, Scrapy developers'.format(datetime.now().year)\n #\n # The short X.Y version.\n try:\n-    import scrapy\n-    version = '.'.join(map(str, scrapy.version_info[:2]))\n-    release = scrapy.__version__\n-except ImportError:\n+    version_path = Path(__file__).parent.absolute().parent.joinpath(\"scrapy/VERSION\")\n+    version = version_path.read_text().strip()\n+    release = version.rsplit(\".\", 1)[0]\n+except Exception:\n     version = ''\n     release = ''\n \n@@ -295,3 +296,5 @@ intersphinx_mapping = {\n # ------------------------------------\n \n hoverxref_auto_ref = True\n+hoverxref_project = \"scrapy\"\n+hoverxref_version = release\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4383f452999464b623393288361ecf7f383666e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 215 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -13,14 +13,13 @@\n \n import sys\n from datetime import datetime\n-from os import path\n from pathlib import Path\n \n # If your extensions are in another directory, add it here. If the directory\n # is relative to the documentation root, use os.path.abspath to make it\n # absolute, like shown here.\n-sys.path.append(path.join(path.dirname(__file__), \"_ext\"))\n-sys.path.insert(0, path.dirname(path.dirname(__file__)))\n+sys.path.append(str(Path(__file__).absolute().parent / \"_ext\"))\n+sys.path.insert(0, str(Path(__file__).absolute().parent.parent))\n \n \n # General configuration\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2205f04631d97103f98a28f865e8ac6511c15c82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 222 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -297,3 +297,10 @@ intersphinx_mapping = {\n hoverxref_auto_ref = True\n hoverxref_project = \"scrapy\"\n hoverxref_version = release\n+hoverxref_role_types = {\n+    \"class\": \"tooltip\",\n+    \"confval\": \"tooltip\",\n+    \"hoverxref\": \"tooltip\",\n+    \"mod\": \"tooltip\",\n+    \"ref\": \"tooltip\",\n+}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#83a0cc6cdf4b8d55ebc594f2635beb75d93898cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 857 | Contributors (this commit): 21 | Commits (past 90d): 7 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -417,7 +417,7 @@ class FilesPipeline(MediaPipeline):\n             self.inc_stats(info.spider, 'uptodate')\n \n             checksum = result.get('checksum', None)\n-            return {'url': request.url, 'path': path, 'checksum': checksum}\n+            return {'url': request.url, 'path': path, 'checksum': checksum, 'status': 'uptodate'}\n \n         path = self.file_path(request, info=info)\n         dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n@@ -494,7 +494,7 @@ class FilesPipeline(MediaPipeline):\n             )\n             raise FileException(str(exc))\n \n-        return {'url': request.url, 'path': path, 'checksum': checksum}\n+        return {'url': request.url, 'path': path, 'checksum': checksum, 'status': status}\n \n     def inc_stats(self, spider, status):\n         spider.crawler.stats.inc_value('file_count', spider=spider)\n\n@@ -94,6 +94,11 @@ class FileDownloadCrawlTestCase(TestCase):\n         file_dl_success = 'File (downloaded): Downloaded file from'\n         self.assertEqual(logs.count(file_dl_success), 3)\n \n+        # check that the images/files status is `downloaded`\n+        for item in items:\n+            for i in item[self.media_key]:\n+                self.assertEqual(i['status'], 'downloaded')\n+\n         # check that the images/files checksums are what we know they should be\n         if self.expected_checksums is not None:\n             checksums = set(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1bd8f392c92d5e856332ab99f547d2a4359bd5d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 2041 | Contributors (this commit): 24 | Commits (past 90d): 10 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -40,14 +40,14 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                         % (method.__self__.__class__.__name__, response.__class__.__name__)\n                     )\n                 if response:\n-                    defer.returnValue(response)\n-            defer.returnValue((yield download_func(request=request, spider=spider)))\n+                    return response\n+            return (yield download_func(request=request, spider=spider))\n \n         @defer.inlineCallbacks\n         def process_response(response):\n             assert response is not None, 'Received None in process_response'\n             if isinstance(response, Request):\n-                defer.returnValue(response)\n+                return response\n \n             for method in self.methods['process_response']:\n                 response = yield deferred_from_coro(method(request=request, response=response, spider=spider))\n@@ -57,12 +57,12 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                         % (method.__self__.__class__.__name__, type(response))\n                     )\n                 if isinstance(response, Request):\n-                    defer.returnValue(response)\n-            defer.returnValue(response)\n+                    return response\n+            return response\n \n         @defer.inlineCallbacks\n-        def process_exception(_failure):\n-            exception = _failure.value\n+        def process_exception(failure):\n+            exception = failure.value\n             for method in self.methods['process_exception']:\n                 response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))\n                 if response is not None and not isinstance(response, (Response, Request)):\n@@ -71,8 +71,8 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                         % (method.__self__.__class__.__name__, type(response))\n                     )\n                 if response:\n-                    defer.returnValue(response)\n-            defer.returnValue(_failure)\n+                    return response\n+            return failure\n \n         deferred = mustbe_deferred(process_request, request)\n         deferred.addErrback(process_exception)\n\n@@ -433,7 +433,7 @@ class FeedExportTest(unittest.TestCase):\n             for file_path in FEEDS.keys():\n                 os.remove(str(file_path))\n \n-        defer.returnValue(content)\n+        return content\n \n     @defer.inlineCallbacks\n     def exported_data(self, items, settings):\n@@ -448,7 +448,7 @@ class FeedExportTest(unittest.TestCase):\n                     yield item\n \n         data = yield self.run_and_export(TestSpider, settings)\n-        defer.returnValue(data)\n+        return data\n \n     @defer.inlineCallbacks\n     def exported_no_data(self, settings):\n@@ -462,7 +462,7 @@ class FeedExportTest(unittest.TestCase):\n                 pass\n \n         data = yield self.run_and_export(TestSpider, settings)\n-        defer.returnValue(data)\n+        return data\n \n     @defer.inlineCallbacks\n     def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n\n@@ -292,7 +292,7 @@ class TestSpiderMiddleware(TestCase):\n         crawler = get_crawler(spider)\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n-        raise defer.returnValue(log)\n+        return log\n \n     @defer.inlineCallbacks\n     def test_recovery(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4023d5db33b588b4df861581948e39b41c0d1678", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 35 | Churn Cumulative: 552 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n import functools\n import logging\n from collections import defaultdict\n-from twisted.internet.defer import Deferred, DeferredList, _DefGen_Return\n+from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n \n from scrapy.settings import Settings\n@@ -141,24 +141,26 @@ class MediaPipeline:\n             # This code fixes a memory leak by avoiding to keep references to\n             # the Request and Response objects on the Media Pipeline cache.\n             #\n-            # Twisted inline callbacks pass return values using the function\n-            # twisted.internet.defer.returnValue, which encapsulates the return\n-            # value inside a _DefGen_Return base exception.\n-            #\n-            # What happens when the media_downloaded callback raises another\n+            # What happens when the media_downloaded callback raises an\n             # exception, for example a FileException('download-error') when\n-            # the Response status code is not 200 OK, is that it stores the\n-            # _DefGen_Return exception on the FileException context.\n+            # the Response status code is not 200 OK, is that the original\n+            # StopIteration exception (which in turn contains the failed\n+            # Response and by extension, the original Request) gets encapsulated\n+            # within the FileException context.\n+            #\n+            # Originally, Scrapy was using twisted.internet.defer.returnValue\n+            # inside functions decorated with twisted.internet.defer.inlineCallbacks,\n+            # encapsulating the returned Response in a _DefGen_Return exception\n+            # instead of a StopIteration.\n             #\n             # To avoid keeping references to the Response and therefore Request\n             # objects on the Media Pipeline cache, we should wipe the context of\n-            # the exception encapsulated by the Twisted Failure when its a\n-            # _DefGen_Return instance.\n+            # the encapsulated exception when it is a StopIteration instance\n             #\n             # This problem does not occur in Python 2.7 since we don't have\n             # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).\n             context = getattr(result.value, '__context__', None)\n-            if isinstance(context, _DefGen_Return):\n+            if isinstance(context, StopIteration):\n                 setattr(result.value, '__context__', None)\n \n         info.downloading.remove(fp)\n\n@@ -2,7 +2,7 @@ from testfixtures import LogCapture\n from twisted.trial import unittest\n from twisted.python.failure import Failure\n from twisted.internet import reactor\n-from twisted.internet.defer import Deferred, inlineCallbacks, returnValue\n+from twisted.internet.defer import Deferred, inlineCallbacks\n \n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n@@ -124,9 +124,8 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         # Simulate the Media Pipeline behavior to produce a Twisted Failure\n         try:\n             # Simulate a Twisted inline callback returning a Response\n-            # The returnValue method raises an exception encapsulating the value\n-            returnValue(response)\n-        except BaseException as exc:\n+            raise StopIteration(response)\n+        except StopIteration as exc:\n             def_gen_return_exc = exc\n             try:\n                 # Simulate the media_downloaded callback raising a FileException\n@@ -140,7 +139,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n         # The Failure should encapsulate a FileException ...\n         self.assertEqual(failure.value, file_exc)\n-        # ... and it should have the returnValue exception set as its context\n+        # ... and it should have the StopIteration exception set as its context\n         self.assertEqual(failure.value.__context__, def_gen_return_exc)\n \n         # Let's calculate the request fingerprint and fake some runtime data...\n@@ -155,7 +154,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         self.assertEqual(info.downloaded[fp], failure)\n         # ... encapsulating the original FileException ...\n         self.assertEqual(info.downloaded[fp].value, file_exc)\n-        # ... but it should not store the returnValue exception on its context\n+        # ... but it should not store the StopIteration exception on its context\n         context = getattr(info.downloaded[fp].value, '__context__', None)\n         self.assertIsNone(context)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee4ee486b1b4f66deffccbbe15f056edaf135982", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 236 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -13,13 +13,13 @@\n \n import sys\n from datetime import datetime\n-from pathlib import Path\n+from os import path\n \n # If your extensions are in another directory, add it here. If the directory\n # is relative to the documentation root, use os.path.abspath to make it\n # absolute, like shown here.\n-sys.path.append(str(Path(__file__).absolute().parent / \"_ext\"))\n-sys.path.insert(0, str(Path(__file__).absolute().parent.parent))\n+sys.path.append(path.join(path.dirname(__file__), \"_ext\"))\n+sys.path.insert(0, path.dirname(path.dirname(__file__)))\n \n \n # General configuration\n@@ -59,10 +59,10 @@ copyright = '2008–{}, Scrapy developers'.format(datetime.now().year)\n #\n # The short X.Y version.\n try:\n-    version_path = Path(__file__).parent.absolute().parent.joinpath(\"scrapy/VERSION\")\n-    version = version_path.read_text().strip()\n-    release = version.rsplit(\".\", 1)[0]\n-except Exception:\n+    import scrapy\n+    version = '.'.join(map(str, scrapy.version_info[:2]))\n+    release = scrapy.__version__\n+except ImportError:\n     version = ''\n     release = ''\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94d7ad76cb96f1623d5944c28db24744955103cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 32 | Files Changed: 4 | Hunks: 30 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 91 | Churn Cumulative: 2999 | Contributors (this commit): 26 | Commits (past 90d): 21 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -500,7 +500,7 @@ class FilesPipeline(MediaPipeline):\n         spider.crawler.stats.inc_value('file_count', spider=spider)\n         spider.crawler.stats.inc_value('file_status_count/%s' % status, spider=spider)\n \n-    ### Overridable Interface\n+    # Overridable Interface\n     def get_media_requests(self, item, info):\n         return [Request(x) for x in item.get(self.files_urls_field, [])]\n \n\n@@ -166,7 +166,7 @@ class MediaPipeline:\n         for wad in info.waiting.pop(fp):\n             defer_result(result).chainDeferred(wad)\n \n-    ### Overridable Interface\n+    # Overridable Interface\n     def media_to_download(self, request, info):\n         \"\"\"Check request before starting download\"\"\"\n         pass\n\n@@ -147,9 +147,9 @@ class CrawlTestCase(TestCase):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n         crawler = CrawlerRunner(settings).create_crawler(BrokenStartRequestsSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        #self.assertTrue(False, crawler.spider.seedsseen)\n-        #self.assertTrue(crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),\n-        #                crawler.spider.seedsseen)\n+        self.assertTrue(\n+            crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),\n+            crawler.spider.seedsseen)\n \n     @defer.inlineCallbacks\n     def test_start_requests_dupes(self):\n\n@@ -541,7 +541,8 @@ class TestReferrerOnRedirect(TestRefererMiddleware):\n \n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.UnsafeUrlPolicy'}\n     scenarii = [\n-        (   'http://scrapytest.org/1',      # parent\n+        (\n+            'http://scrapytest.org/1',      # parent\n             'http://scrapytest.org/2',      # target\n             (\n                 # redirections: code, URL\n@@ -551,7 +552,8 @@ class TestReferrerOnRedirect(TestRefererMiddleware):\n             b'http://scrapytest.org/1',  # expected initial referer\n             b'http://scrapytest.org/1',  # expected referer for the redirection request\n         ),\n-        (   'https://scrapytest.org/1',\n+        (\n+            'https://scrapytest.org/1',\n             'https://scrapytest.org/2',\n             (\n                 # redirecting to non-secure URL\n@@ -560,7 +562,8 @@ class TestReferrerOnRedirect(TestRefererMiddleware):\n             b'https://scrapytest.org/1',\n             b'https://scrapytest.org/1',\n         ),\n-        (   'https://scrapytest.org/1',\n+        (\n+            'https://scrapytest.org/1',\n             'https://scrapytest.com/2',\n             (\n                 # redirecting to non-secure URL: different origin\n@@ -602,7 +605,8 @@ class TestReferrerOnRedirectNoReferrer(TestReferrerOnRedirect):\n     \"\"\"\n     settings = {'REFERRER_POLICY': 'no-referrer'}\n     scenarii = [\n-        (   'http://scrapytest.org/1',      # parent\n+        (\n+            'http://scrapytest.org/1',      # parent\n             'http://scrapytest.org/2',      # target\n             (\n                 # redirections: code, URL\n@@ -612,7 +616,8 @@ class TestReferrerOnRedirectNoReferrer(TestReferrerOnRedirect):\n             None,  # expected initial \"Referer\"\n             None,  # expected \"Referer\" for the redirection request\n         ),\n-        (   'https://scrapytest.org/1',\n+        (\n+            'https://scrapytest.org/1',\n             'https://scrapytest.org/2',\n             (\n                 (301, 'http://scrapytest.org/3'),\n@@ -620,7 +625,8 @@ class TestReferrerOnRedirectNoReferrer(TestReferrerOnRedirect):\n             None,\n             None,\n         ),\n-        (   'https://scrapytest.org/1',\n+        (\n+            'https://scrapytest.org/1',\n             'https://example.com/2',    # different origin\n             (\n                 (301, 'http://scrapytest.com/3'),\n@@ -641,7 +647,8 @@ class TestReferrerOnRedirectSameOrigin(TestReferrerOnRedirect):\n     \"\"\"\n     settings = {'REFERRER_POLICY': 'same-origin'}\n     scenarii = [\n-        (   'http://scrapytest.org/101',      # origin\n+        (\n+            'http://scrapytest.org/101',      # origin\n             'http://scrapytest.org/102',      # target\n             (\n                 # redirections: code, URL\n@@ -651,7 +658,8 @@ class TestReferrerOnRedirectSameOrigin(TestReferrerOnRedirect):\n             b'http://scrapytest.org/101',  # expected initial \"Referer\"\n             b'http://scrapytest.org/101',  # expected referer for the redirection request\n         ),\n-        (   'https://scrapytest.org/201',\n+        (\n+            'https://scrapytest.org/201',\n             'https://scrapytest.org/202',\n             (\n                 # redirecting from secure to non-secure URL == different origin\n@@ -660,7 +668,8 @@ class TestReferrerOnRedirectSameOrigin(TestReferrerOnRedirect):\n             b'https://scrapytest.org/201',\n             None,\n         ),\n-        (   'https://scrapytest.org/301',\n+        (\n+            'https://scrapytest.org/301',\n             'https://scrapytest.org/302',\n             (\n                 # different domain == different origin\n@@ -683,7 +692,8 @@ class TestReferrerOnRedirectStrictOrigin(TestReferrerOnRedirect):\n     \"\"\"\n     settings = {'REFERRER_POLICY': POLICY_STRICT_ORIGIN}\n     scenarii = [\n-        (   'http://scrapytest.org/101',\n+        (\n+            'http://scrapytest.org/101',\n             'http://scrapytest.org/102',\n             (\n                 (301, 'http://scrapytest.org/103'),\n@@ -692,7 +702,8 @@ class TestReferrerOnRedirectStrictOrigin(TestReferrerOnRedirect):\n             b'http://scrapytest.org/',  # send origin\n             b'http://scrapytest.org/',  # redirects to same origin: send origin\n         ),\n-        (   'https://scrapytest.org/201',\n+        (\n+            'https://scrapytest.org/201',\n             'https://scrapytest.org/202',\n             (\n                 # redirecting to non-secure URL: no referrer\n@@ -701,7 +712,8 @@ class TestReferrerOnRedirectStrictOrigin(TestReferrerOnRedirect):\n             b'https://scrapytest.org/',\n             None,\n         ),\n-        (   'https://scrapytest.org/301',\n+        (\n+            'https://scrapytest.org/301',\n             'https://scrapytest.org/302',\n             (\n                 # redirecting to non-secure URL (different domain): no referrer\n@@ -710,7 +722,8 @@ class TestReferrerOnRedirectStrictOrigin(TestReferrerOnRedirect):\n             b'https://scrapytest.org/',\n             None,\n         ),\n-        (   'http://scrapy.org/401',\n+        (\n+            'http://scrapy.org/401',\n             'http://example.com/402',\n             (\n                 (301, 'http://scrapytest.org/403'),\n@@ -718,7 +731,8 @@ class TestReferrerOnRedirectStrictOrigin(TestReferrerOnRedirect):\n             b'http://scrapy.org/',\n             b'http://scrapy.org/',\n         ),\n-        (   'https://scrapy.org/501',\n+        (\n+            'https://scrapy.org/501',\n             'https://example.com/502',\n             (\n                 # HTTPS all along, so origin referrer is kept as-is\n@@ -728,7 +742,8 @@ class TestReferrerOnRedirectStrictOrigin(TestReferrerOnRedirect):\n             b'https://scrapy.org/',\n             b'https://scrapy.org/',\n         ),\n-        (   'https://scrapytest.org/601',\n+        (\n+            'https://scrapytest.org/601',\n             'http://scrapytest.org/602',                # TLS to non-TLS: no referrer\n             (\n                 (301, 'https://scrapytest.org/603'),    # TLS URL again: (still) no referrer\n@@ -750,7 +765,8 @@ class TestReferrerOnRedirectOriginWhenCrossOrigin(TestReferrerOnRedirect):\n     \"\"\"\n     settings = {'REFERRER_POLICY': POLICY_ORIGIN_WHEN_CROSS_ORIGIN}\n     scenarii = [\n-        (   'http://scrapytest.org/101',      # origin\n+        (\n+            'http://scrapytest.org/101',      # origin\n             'http://scrapytest.org/102',      # target + redirection\n             (\n                 # redirections: code, URL\n@@ -760,7 +776,8 @@ class TestReferrerOnRedirectOriginWhenCrossOrigin(TestReferrerOnRedirect):\n             b'http://scrapytest.org/101',  # expected initial referer\n             b'http://scrapytest.org/101',  # expected referer for the redirection request\n         ),\n-        (   'https://scrapytest.org/201',\n+        (\n+            'https://scrapytest.org/201',\n             'https://scrapytest.org/202',\n             (\n                 # redirecting to non-secure URL: send origin\n@@ -769,7 +786,8 @@ class TestReferrerOnRedirectOriginWhenCrossOrigin(TestReferrerOnRedirect):\n             b'https://scrapytest.org/201',\n             b'https://scrapytest.org/',\n         ),\n-        (   'https://scrapytest.org/301',\n+        (\n+            'https://scrapytest.org/301',\n             'https://scrapytest.org/302',\n             (\n                 # redirecting to non-secure URL (different domain): send origin\n@@ -778,7 +796,8 @@ class TestReferrerOnRedirectOriginWhenCrossOrigin(TestReferrerOnRedirect):\n             b'https://scrapytest.org/301',\n             b'https://scrapytest.org/',\n         ),\n-        (   'http://scrapy.org/401',\n+        (\n+            'http://scrapy.org/401',\n             'http://example.com/402',\n             (\n                 (301, 'http://scrapytest.org/403'),\n@@ -786,7 +805,8 @@ class TestReferrerOnRedirectOriginWhenCrossOrigin(TestReferrerOnRedirect):\n             b'http://scrapy.org/',\n             b'http://scrapy.org/',\n         ),\n-        (   'https://scrapy.org/501',\n+        (\n+            'https://scrapy.org/501',\n             'https://example.com/502',\n             (\n                 # all different domains: send origin\n@@ -796,7 +816,8 @@ class TestReferrerOnRedirectOriginWhenCrossOrigin(TestReferrerOnRedirect):\n             b'https://scrapy.org/',\n             b'https://scrapy.org/',\n         ),\n-        (   'https://scrapytest.org/301',\n+        (\n+            'https://scrapytest.org/301',\n             'http://scrapytest.org/302',                # TLS to non-TLS: send origin\n             (\n                 (301, 'https://scrapytest.org/303'),    # TLS URL again: send origin (also)\n@@ -820,7 +841,8 @@ class TestReferrerOnRedirectStrictOriginWhenCrossOrigin(TestReferrerOnRedirect):\n     \"\"\"\n     settings = {'REFERRER_POLICY': POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN}\n     scenarii = [\n-        (   'http://scrapytest.org/101',      # origin\n+        (\n+            'http://scrapytest.org/101',      # origin\n             'http://scrapytest.org/102',      # target + redirection\n             (\n                 # redirections: code, URL\n@@ -830,7 +852,8 @@ class TestReferrerOnRedirectStrictOriginWhenCrossOrigin(TestReferrerOnRedirect):\n             b'http://scrapytest.org/101',  # expected initial referer\n             b'http://scrapytest.org/101',  # expected referer for the redirection request\n         ),\n-        (   'https://scrapytest.org/201',\n+        (\n+            'https://scrapytest.org/201',\n             'https://scrapytest.org/202',\n             (\n                 # redirecting to non-secure URL: do not send the \"Referer\" header\n@@ -839,7 +862,8 @@ class TestReferrerOnRedirectStrictOriginWhenCrossOrigin(TestReferrerOnRedirect):\n             b'https://scrapytest.org/201',\n             None,\n         ),\n-        (   'https://scrapytest.org/301',\n+        (\n+            'https://scrapytest.org/301',\n             'https://scrapytest.org/302',\n             (\n                 # redirecting to non-secure URL (different domain): send origin\n@@ -848,7 +872,8 @@ class TestReferrerOnRedirectStrictOriginWhenCrossOrigin(TestReferrerOnRedirect):\n             b'https://scrapytest.org/301',\n             None,\n         ),\n-        (   'http://scrapy.org/401',\n+        (\n+            'http://scrapy.org/401',\n             'http://example.com/402',\n             (\n                 (301, 'http://scrapytest.org/403'),\n@@ -856,7 +881,8 @@ class TestReferrerOnRedirectStrictOriginWhenCrossOrigin(TestReferrerOnRedirect):\n             b'http://scrapy.org/',\n             b'http://scrapy.org/',\n         ),\n-        (   'https://scrapy.org/501',\n+        (\n+            'https://scrapy.org/501',\n             'https://example.com/502',\n             (\n                 # all different domains: send origin\n@@ -866,7 +892,8 @@ class TestReferrerOnRedirectStrictOriginWhenCrossOrigin(TestReferrerOnRedirect):\n             b'https://scrapy.org/',\n             b'https://scrapy.org/',\n         ),\n-        (   'https://scrapytest.org/601',\n+        (\n+            'https://scrapytest.org/601',\n             'http://scrapytest.org/602',                # TLS to non-TLS: do not send \"Referer\"\n             (\n                 (301, 'https://scrapytest.org/603'),    # TLS URL again: (still) send nothing\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
