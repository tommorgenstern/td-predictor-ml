{"custom_id": "keras#3b61e0b6543e5c05aeff6d75d9c2d0fddc152115", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 10 | Files Changed: 6 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 1826 | Contributors (this commit): 6 | Commits (past 90d): 13 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,7 +82,9 @@ class TokenAndPositionEmbedding(layers.Layer):\n         self.token_emb = keras.layers.Embedding(\n             input_dim=vocab_size, output_dim=embed_dim\n         )\n-        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n+        self.pos_emb = keras.layers.Embedding(\n+            input_dim=maxlen, output_dim=embed_dim\n+        )\n \n     def call(self, inputs):\n         maxlen = keras.ops.backend.shape(inputs)[-1]\n@@ -99,10 +101,18 @@ class TokenAndPositionEmbedding(layers.Layer):\n \n class NERModel(keras.Model):\n     def __init__(\n-        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n+        self,\n+        num_tags,\n+        vocab_size,\n+        maxlen=128,\n+        embed_dim=32,\n+        num_heads=2,\n+        ff_dim=32,\n     ):\n         super().__init__()\n-        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n+        self.embedding_layer = TokenAndPositionEmbedding(\n+            maxlen, vocab_size, embed_dim\n+        )\n         self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n         self.dropout1 = layers.Dropout(0.1)\n         self.ff = layers.Dense(ff_dim, activation=\"relu\")\n@@ -166,7 +176,9 @@ have a total of 10 labels: 9 from the NER dataset and one for padding.\n def make_tag_lookup_table():\n     iob_labels = [\"B\", \"I\"]\n     ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n-    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n+    all_labels = [\n+        (label1, label2) for label2 in ner_labels for label1 in iob_labels\n+    ]\n     all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n     all_labels = [\"[PAD]\", \"O\"] + all_labels\n     return dict(zip(range(0, len(all_labels) + 1), all_labels))\n@@ -215,6 +227,7 @@ print(list(train_data.take(1).as_numpy_iterator()))\n We will be using the following map function to transform the data in the dataset:\n \"\"\"\n \n+\n # Data preprocessing using Tensorflow\n def map_record_to_training_data(record):\n     record = tf_strings.split(record, sep=\"\\t\")\n\n@@ -281,7 +281,9 @@ Moreover, we use `sparse_categorical_crossentropy` since our labels are integers\n model.compile(\n     loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n )\n-model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))\n+model.fit(\n+    x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val)\n+)\n \n \"\"\"\n ## Export an end-to-end model\n\n@@ -39,7 +39,8 @@ the curve below is a ResNet-50 pre-trained on ImageNet (ILSVRC-2012).\n ## Setup\n \"\"\"\n import os\n-os.environ['KERAS_BACKEND'] = 'tensorflow'\n+\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import keras_core as keras\n import numpy as np\n\n@@ -354,7 +354,9 @@ initial_model.save_weights(\"initial_weights.weights.h5\")\n model = training_model()\n model.load_weights(\"initial_weights.weights.h5\")\n \n-model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n+model.compile(\n+    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n+)\n model.fit(train_ds_cmu, validation_data=test_ds, epochs=15)\n \n test_loss, test_accuracy = model.evaluate(test_ds)\n@@ -366,7 +368,9 @@ print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))\n \n model = training_model()\n model.load_weights(\"initial_weights.weights.h5\")\n-model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n+model.compile(\n+    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n+)\n model.fit(train_ds_simple, validation_data=test_ds, epochs=15)\n \n test_loss, test_accuracy = model.evaluate(test_ds)\n\n@@ -19,6 +19,7 @@ difference between weights trained on a mini-batch of never-seen-before data and\n model weights prior to training over a fixed number of meta-iterations.\n \"\"\"\n import os\n+\n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import keras_core as keras\n\n@@ -121,11 +121,12 @@ class Patches(layers.Layer):\n         num_patches_w = width // self.patch_size\n         patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n         patches = ops.reshape(\n-            patches, (\n+            patches,\n+            (\n                 batch_size,\n                 num_patches_h * num_patches_w,\n                 self.patch_size * self.patch_size * channels,\n-            )\n+            ),\n         )\n         return patches\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab566fdc9ae83a88dc4bf149e055773e4d9edf86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 92 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -14,13 +14,5 @@\n # ==============================================================================\n \"\"\"Keras' DTensor library.\"\"\"\n \n-_DTENSOR_API_ENABLED = True\n \n-\n-# Conditional import the dtensor API, since it is currently broken in OSS.\n-if _DTENSOR_API_ENABLED:\n from tensorflow.compat.v2.experimental import dtensor as dtensor_api\n-else:\n-    # Leave it with a placeholder, so that the import line from other python\n-    # file will not break.\n-    dtensor_api = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#94f9e0af8a5d22f8e2c30b8ae1f13ae15062e82a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 319 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -5,6 +5,9 @@ from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n from keras_core import optimizers\n+import keras_core as keras\n+\n+keras.config.disable_traceback_filtering()\n \n inputs = layers.Input((100,))\n x = layers.Dense(512, activation=\"relu\")(inputs)\n\n@@ -0,0 +1,20 @@\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+\n+if backend.backend() == \"tensorflow\":\n+    BackendVariable = backend.tensorflow.core.Variable\n+elif backend.backend() == \"jax\":\n+    BackendVariable = backend.jax.core.Variable\n+elif backend.backend() == \"torch\":\n+    BackendVariable = backend.torch.core.Variable\n+elif backend.backend() == \"numpy\":\n+    from keras_core.backend.numpy.core import Variable as NumpyVariable\n+\n+    BackendVariable = NumpyVariable\n+else:\n+    raise RuntimeError(\"Invalid backend.\")\n+\n+\n+@keras_core_export(\"keras_core.backend.Variable\")\n+class Variable(BackendVariable):\n+    pass\n\n@@ -184,7 +184,6 @@ def get_json_type(obj):\n     #     return obj.__wrapped__\n \n     if tf.available and isinstance(obj, tf.TypeSpec):\n-\n         from tensorflow.python.framework import type_spec_registry\n \n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56a6ab1c1466e384b0705ec40c7233ccff7621f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 300 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 11 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 300 | Churn Cumulative: 1127 | Contributors (this commit): 6 | Commits (past 90d): 16 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -586,6 +586,38 @@ def append(\n     x2,\n     axis=None,\n ):\n+    \"\"\"Append tensor `x2` to the end of tensor `x1`.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+        axis: Axis along which tensor `x2` is appended to tensor `x1`.\n+            If `None`, both tensors are flattened before use.\n+\n+    Returns:\n+        A tensor with the values of `x2` appended to `x1`.\n+\n+    Examples:\n+\n+    >>> x1 = keras_core.ops.convert_to_tensor([1, 2, 3])\n+    >>> x2 = keras_core.ops.convert_to_tensor([[4, 5, 6], [7, 8, 9]])\n+    >>> keras_core.ops.append(x1, x2)\n+    array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)\n+\n+    When `axis` is specified, `x1` and `x2` must have compatible shapes.\n+    >>> x1 = keras_core.ops.convert_to_tensor([[1, 2, 3], [4, 5, 6]])\n+    >>> x2 = keras_core.ops.convert_to_tensor([[7, 8, 9]])\n+    >>> keras_core.ops.append(x1, x2, axis=0)\n+    array([[1, 2, 3],\n+            [4, 5, 6],\n+            [7, 8, 9]], dtype=int32)\n+    >>> x3 = keras_core.ops.convert_to_tensor([7, 8, 9])\n+    >>> keras_core.ops.append(x1, x3, axis=0)\n+    Traceback (most recent call last):\n+        ...\n+    TypeError: Cannot concatenate arrays with different numbers of\n+    dimensions: got (2, 3), (3,).\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Append(axis=axis).symbolic_call(x1, x2)\n     return backend.numpy.append(x1, x2, axis=axis)\n@@ -604,6 +636,51 @@ class Arange(Operation):\n \n @keras_core_export([\"keras_core.ops.arange\", \"keras_core.ops.numpy.arange\"])\n def arange(start, stop=None, step=1, dtype=None):\n+    \"\"\"Return evenly spaced values within a given interval.\n+\n+    `arange` can be called with a varying number of positional arguments:\n+    * `arange(stop)`: Values are generated within the half-open interval\n+        `[0, stop)` (in other words, the interval including start but excluding\n+        stop).\n+    * `arange(start, stop)`: Values are generated within the half-open interval\n+        `[start, stop)`.\n+    * `arange(start, stop, step)`: Values are generated within the half-open\n+        interval `[start, stop)`, with spacing between values given by step.\n+\n+    Args:\n+        start: Integer or real, representing the start of the interval. The\n+            interval includes this value.\n+        stop: Integer or real, representing the end of the interval. The\n+            interval does not include this value, except in some cases where\n+            `step` is not an integer and floating point round-off affects the\n+            lenght of `out`. Defaults to `None`.\n+        step: Integer or real, represent the spacing between values. For any\n+            output `out`, this is the distance between two adjacent values,\n+            `out[i+1] - out[i]`. The default step size is 1. If `step` is\n+            specified as a position argument, `start` must also be given.\n+        dtype: The type of the output array. If `dtype` is not given, infer the\n+            data type from the other input arguments.\n+\n+    Returns:\n+        Tensor of evenly spaced values.\n+        For floating point arguments, the length of the result is\n+        `ceil((stop - start)/step)`. Because of floating point overflow, this\n+        rule may result in the last element of out being greater than stop.\n+\n+    Examples:\n+\n+    >>> keras_core.ops.arange(3)\n+    array([0, 1, 2], dtype=int32)\n+\n+    >>> keras_core.ops.arange(3.0)\n+    array([0., 1., 2.], dtype=float32)\n+\n+    >>> keras_core.ops.arange(3, 7)\n+    array([3, 4, 5, 6], dtype=int32)\n+\n+    >>> keras_core.ops.arange(3, 7, 2)\n+    array([3, 5], dtype=int32)\n+    \"\"\"\n     return backend.numpy.arange(start, stop, step=step, dtype=dtype)\n \n \n@@ -617,6 +694,20 @@ class Arccos(Operation):\n \n @keras_core_export([\"keras_core.ops.arccos\", \"keras_core.ops.numpy.arccos\"])\n def arccos(x):\n+    \"\"\"Trigonometric inverse cosine, element-wise. The inverse of `cos` so that,\n+    if `y = cos(x)`, then `x = arccos(y)`.\n+\n+    Arg:\n+        x: Input tensor.\n+\n+    Returns:\n+        Tensor of the angle of the ray intersecting the unit circle at the given\n+        x-coordinate in radians [0, pi].\n+\n+    Example:\n+    >>> x = keras_core.ops.convert_to_tensor([1, -1])\n+    array([0., 3.1415927], dtype=float32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Arccos().symbolic_call(x)\n     return backend.numpy.arccos(x)\n@@ -632,6 +723,20 @@ class Arcsin(Operation):\n \n @keras_core_export([\"keras_core.ops.arcsin\", \"keras_core.ops.numpy.arcsin\"])\n def arcsin(x):\n+    \"\"\"Inverse sine, element-wise.\n+\n+    Arg:\n+        x: Input tensor.\n+\n+    Returns:\n+        Tensor of the inverse sine of each element in `x`, in radians and in\n+        the closed interval `[-pi/2, pi/2]`.\n+\n+    Example:\n+    >>> x = keras_core.ops.convert_to_tensor([1, -1, 0])\n+    >>> keras_core.ops.arcsin(x)\n+    array([ 1.5707964, -1.5707964,  0.], dtype=float32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Arcsin().symbolic_call(x)\n     return backend.numpy.arcsin(x)\n@@ -647,6 +752,20 @@ class Arctan(Operation):\n \n @keras_core_export([\"keras_core.ops.arctan\", \"keras_core.ops.numpy.arctan\"])\n def arctan(x):\n+    \"\"\"Trigonometric inverse tangent, element-wise.\n+\n+    Arg:\n+        x: Input tensor.\n+\n+    Returns:\n+        Tensor of the inverse tangent of each element in `x`, in the interval\n+        `[-pi/2, pi/2]`.\n+\n+    Example:\n+    >>> x = keras_core.ops.convert_to_tensor([0, 1])\n+    >>> keras_core.ops.arctan(x)\n+    array([0., 0.7853982], dtype=float32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Arctan().symbolic_call(x)\n     return backend.numpy.arctan(x)\n@@ -665,6 +784,43 @@ class Arctan2(Operation):\n \n @keras_core_export([\"keras_core.ops.arctan2\", \"keras_core.ops.numpy.arctan2\"])\n def arctan2(x1, x2):\n+    \"\"\"Element-wise arc tangent of `x1/x2` choosing the quadrant correctly.\n+\n+    The quadrant (i.e., branch) is chosen so that `arctan2(x1, x2)` is the\n+    signed angle in radians between the ray ending at the origin and passing\n+    through the point `(1, 0)`, and the ray ending at the origin and passing\n+    through the point `(x2, x1)`. (Note the role reversal: the \"y-coordinate\"\n+    is the first function parameter, the \"x-coordinate\" is the second.) By IEEE\n+    convention, this function is defined for `x2 = +/-0` and for either or both\n+    of `x1` and `x2` `= +/-inf`.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Tensor of angles in radians, in the range `[-pi, pi]`.\n+\n+    Examples:\n+    Consider four points in different quadrants:\n+    >>> x = keras_core.ops.convert_to_tensor([-1, +1, +1, -1])\n+    >>> y = keras_core.ops.convert_to_tensor([-1, -1, +1, +1])\n+    >>> keras_core.ops.arctan2(y, x) * 180 / numpy.pi\n+    array([-135., -45., 45., 135.], dtype=float32)\n+\n+    Note the order of the parameters. `arctan2` is defined also when x2=0 and\n+    at several other points, obtaining values in the range `[-pi, pi]`:\n+    >>> keras_core.ops.arctan2(\n+    ...     keras_core.ops.array([1., -1.]),\n+    ...     keras_core.ops.array([0., 0.]),\n+    ... )\n+    array([ 1.5707964, -1.5707964], dtype=float32)\n+    >>> keras_core.ops.arctan2(\n+    ...     keras_core.ops.array([0., 0., numpy.inf]),\n+    ...     keras_core.ops.array([+0., -0., numpy.inf]),\n+    ... )\n+    array([0., 3.1415925, 0.7853982], dtype=float32)\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Arctan2().symbolic_call(x1, x2)\n     return backend.numpy.arctan2(x1, x2)\n@@ -688,6 +844,29 @@ class Argmax(Operation):\n \n @keras_core_export([\"keras_core.ops.argmax\", \"keras_core.ops.numpy.argmax\"])\n def argmax(x, axis=None):\n+    \"\"\"Returns the indices of the maximum values along an axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: By default, the index is into the flattened tensor, otherwise\n+            along the specified axis.\n+\n+    Returns:\n+        Tensor of indices. It has the same shape as `x`, with the dimension\n+        along `axis` removed.\n+\n+    Example:\n+    >>> x = keras_core.ops.arange(6).reshape(2, 3) + 10\n+    >>> x\n+    array([[10, 11, 12],\n+           [13, 14, 15]], dtype=int32)\n+    >>> keras_core.ops.argmax(x)\n+    array(5, dtype=int32)\n+    >>> keras_core.ops.argmax(x, axis=0)\n+    array([1, 1, 1], dtype=int32)\n+    >>> keras_core.ops.argmax(x, axis=1)\n+    array([2, 2], dtype=int32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Argmax(axis=axis).symbolic_call(x)\n     return backend.numpy.argmax(x, axis=axis)\n@@ -711,6 +890,29 @@ class Argmin(Operation):\n \n @keras_core_export([\"keras_core.ops.argmin\", \"keras_core.ops.numpy.argmin\"])\n def argmin(x, axis=None):\n+    \"\"\"Returns the indices of the minium values along an axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: By default, the index is into the flattened tensor, otherwise\n+            along the specified axis.\n+\n+    Returns:\n+        Tensor of indices. It has the same shape as `x`, with the dimension\n+        along `axis` removed.\n+\n+    Example:\n+    >>> x = keras_core.ops.arange(6).reshape(2, 3) + 10\n+    >>> x\n+    array([[10, 11, 12],\n+           [13, 14, 15]], dtype=int32)\n+    >>> keras_core.ops.argmin(x)\n+    array(0, dtype=int32)\n+    >>> keras_core.ops.argmin(x, axis=0)\n+    array([0, 0, 0], dtype=int32)\n+    >>> keras_core.ops.argmin(x, axis=1)\n+    array([0, 0], dtype=int32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Argmin(axis=axis).symbolic_call(x)\n     return backend.numpy.argmin(x, axis=axis)\n@@ -732,6 +934,37 @@ class Argsort(Operation):\n \n @keras_core_export([\"keras_core.ops.argsort\", \"keras_core.ops.numpy.argsort\"])\n def argsort(x, axis=-1):\n+    \"\"\"Returns the indices that would sort an tensor.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis along which to sort. Default is `-1` (the last axis). If\n+        `None`, the flattened tensor is used.\n+\n+    Returns:\n+        Tensor of indices that sort `x` along the specified `axis`.\n+\n+    Examples:\n+    One dimensional array:\n+    >>> x = keras_core.ops.array([3, 1, 2])\n+    >>> keras_core.ops.argsort(x)\n+    array([1, 2, 0], dtype=int32)\n+\n+    Two-dimensional array:\n+    >>> x = keras_core.ops.array([[0, 3], [3, 2], [4, 5]])\n+    >>> x\n+    array([[0, 3],\n+           [3, 2],\n+           [4, 5]], dtype=int32)\n+    >>> keras_core.ops.argsort(x, axis=0)\n+    array([[0, 1],\n+           [1, 0],\n+           [2, 2]], dtype=int32)\n+    >>> keras_core.ops.argsort(x, axis=1)\n+    array([[0, 1],\n+           [1, 0],\n+           [0, 1]], dtype=int32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Argsort(axis=axis).symbolic_call(x)\n     return backend.numpy.argsort(x, axis=axis)\n@@ -747,6 +980,22 @@ class Array(Operation):\n \n @keras_core_export([\"keras_core.ops.array\", \"keras_core.ops.numpy.array\"])\n def array(x, dtype=None):\n+    \"\"\"Create a tensor.\n+\n+    Args:\n+        x: Input tensor.\n+        dtype: The desired data-type for the tensor.\n+\n+    Returns:\n+        A tensor.\n+\n+    Examples:\n+    >>> keras_core.ops.array([1, 2, 3])\n+    array([1, 2, 3], dtype=int32)\n+\n+    >>> keras_core.ops.array([1, 2, 3], dtype=\"float32\")\n+    array([1., 2., 3.], dtype=float32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Array().symbolic_call(x, dtype=dtype)\n     return backend.numpy.array(x, dtype=dtype)\n@@ -800,6 +1049,57 @@ class Average(Operation):\n \n @keras_core_export([\"keras_core.ops.average\", \"keras_core.ops.numpy.average\"])\n def average(x, axis=None, weights=None):\n+    \"\"\"Compute the weighted average along the specified axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Integer along which to average `x`. The default, `axis=None`,\n+            will average over all of the elements of the input tendor. If axis\n+            is negative it counts from the last to the first axis.\n+        weights: Tensor of wieghts associated with the values in `x`. Each\n+            value in `x` contributes to the average according to its\n+            associated weight. The weights array can either be 1-D (in which\n+            case its length must be the size of a along the given axis) or of\n+            the same shape as `x`. If `weights=None` (default), then all data\n+            in `x` are assumed to have a weight equal to one.\n+\n+            The 1-D calculation is: `avg = sum(a * weights) / sum(weights)`.\n+            The only constraint on weights is that `sum(weights)` must not be 0.\n+\n+    Returns:\n+        Return the average along the specified axis.\n+\n+    Examples:\n+    >>> data = keras_core.ops.arange(1, 5)\n+    >>> data\n+    array([1, 2, 3, 4], dtype=int32)\n+    >>> keras_core.ops.average(data)\n+    array(2.5, dtype=float32)\n+    >>> keras_core.ops.average(\n+    ...     keras_core.ops.arange(1, 11),\n+    ...     weights=keras_core.ops.arange(10, 0, -1)\n+    ... )\n+    array(4., dtype=float32)\n+\n+    >>> data = keras_core.ops.arange(6).reshape((3, 2))\n+    >>> data\n+    array([[0, 1],\n+           [2, 3],\n+           [4, 5]], dtype=int32)\n+    >>> keras_core.ops.average(\n+    ...     data,\n+    ...     axis=1,\n+    ...     weights=keras_core.ops.array([1./4, 3./4])\n+    ... )\n+    array([0.75, 2.75, 4.75], dtype=float32)\n+    >>> keras_core.ops.average(\n+    ...     data,\n+    ...     weights=keras_core.ops.array([1./4, 3./4])\n+    ... )\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Axis must be specified when shapes of a and weights differ.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Average(axis=axis).symbolic_call(x, weights=weights)\n     return backend.numpy.average(x, weights=weights, axis=axis)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30121f4b5f507cbce55efc3ea6c3d5a7526330bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 13 | Churn Cumulative: 2615 | Contributors (this commit): 17 | Commits (past 90d): 82 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -624,9 +624,6 @@ class Layer(BackendLayer, Operation):\n         # 1. Convert any array arguments to tensors of correct dtype.\n         def maybe_convert(x):\n             if backend.is_tensor(x):\n-                # Handle Torch device placement.\n-                if backend.backend() == \"torch\":\n-                    x = backend.convert_to_tensor(x)\n                 if (\n                     self.autocast\n                     and backend.is_float_dtype(x.dtype)\n@@ -646,7 +643,13 @@ class Layer(BackendLayer, Operation):\n                 return backend.convert_to_tensor(x, dtype=self.compute_dtype)\n             return x\n \n-        if self._convert_input_args:\n+        # Used to avoid expensive `tree` operations in the most common case.\n+        if (\n+            kwargs\n+            or len(args) != 1\n+            or not backend.is_tensor(args[0])\n+            or backend.standardize_dtype(args[0].dtype) != self.compute_dtype\n+        ) and self._convert_input_args:\n             args = tree.map_structure(maybe_convert, args)\n             kwargs = tree.map_structure(maybe_convert, kwargs)\n \n\n@@ -9,7 +9,7 @@ class SymbolicArguments:\n         self.kwargs = tree.map_structure(lambda x: x, kwargs)\n         self._flat_arguments = tree.flatten((self.args, self.kwargs))\n \n-        # Used to avoid expensive `nest` operations in the most common case.\n+        # Used to avoid expensive `tree` operations in the most common case.\n         if (\n             not self.kwargs\n             and len(self.args) == 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#239ac5e981e45bae23833d9bd015a4c68067668c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 684 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -232,7 +232,7 @@ All layers you've seen so far in this guide work with all Keras backends.\n The `keras.ops` namespace gives you access to:\n \n - The NumPy API, e.g. `ops.matmul`, `ops.sum`, `ops.reshape`, `ops.stack`, etc.\n-- Neural networks-specific APIs such as `ops.softmax`, `ops`.conv`, `ops.binary_crossentropy`, `ops.relu`, etc.\n+- Neural networks-specific APIs such as `ops.softmax`, `ops.conv`, `ops.binary_crossentropy`, `ops.relu`, etc.\n \n You can also use backend-native APIs in your layers (such as `tf.nn` functions),\n but if you do this, then your layer will only be usable with the backend in question.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a0673fef4e1410745b87cc6955006a79bb7f2a89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 417 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -61,7 +61,7 @@ class Merge(Layer):\n \n     def build(self, input_shape):\n         # Used purely for shape validation.\n-        if not isinstance(input_shape[0], tuple):\n+        if not isinstance(input_shape[0], (tuple, list)):\n             raise ValueError(\n                 \"A merge layer should be called on a list of inputs. \"\n                 f\"Received: input_shape={input_shape} (not a list of shapes)\"\n\n@@ -39,7 +39,9 @@ class Concatenate(Merge):\n \n     def build(self, input_shape):\n         # Used purely for shape validation.\n-        if len(input_shape) < 1 or not isinstance(input_shape[0], tuple):\n+        if len(input_shape) < 1 or not isinstance(\n+            input_shape[0], (tuple, list)\n+        ):\n             raise ValueError(\n                 \"A `Concatenate` layer should be called on a list of \"\n                 f\"at least 1 input. Received: input_shape={input_shape}\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47c2cf4d74e0c7ed3430c35d31d70926b46d6a61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 335 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 335 | Churn Cumulative: 335 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,335 @@\n+\"\"\"\n+Title: Learning to Resize in Computer Vision\n+Author: [Sayak Paul](https://twitter.com/RisingSayak)\n+Date created: 2021/04/30\n+Last modified: 2023/07/26\n+Description: How to optimally learn representations of images for a given resolution.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+It is a common belief that if we constrain vision models to perceive things as humans do,\n+their performance can be improved. For example, in [this work](https://arxiv.org/abs/1811.12231),\n+Geirhos et al. showed that the vision models pre-trained on the ImageNet-1k dataset are\n+biased towards texture, whereas human beings mostly use the shape descriptor to develop a\n+common perception. But does this belief always apply, especially when it comes to improving\n+the performance of vision models?\n+\n+It turns out it may not always be the case. When training vision models, it is common to\n+resize images to a lower dimension ((224 x 224), (299 x 299), etc.) to allow mini-batch\n+learning and also to keep up the compute limitations.  We generally make use of image\n+resizing methods like **bilinear interpolation** for this step and the resized images do\n+not lose much of their perceptual character to the human eyes. In\n+[Learning to Resize Images for Computer Vision Tasks](https://arxiv.org/abs/2103.09950v1), Talebi et al. show\n+that if we try to optimize the perceptual quality of the images for the vision models\n+rather than the human eyes, their performance can further be improved. They investigate\n+the following question:\n+\n+**For a given image resolution and a model, how to best resize the given images?**\n+\n+As shown in the paper, this idea helps to consistently improve the performance of the\n+common vision models (pre-trained on ImageNet-1k) like DenseNet-121, ResNet-50,\n+MobileNetV2, and EfficientNets. In this example, we will implement the learnable image\n+resizing module as proposed in the paper and demonstrate that on the\n+[Cats and Dogs dataset](https://www.microsoft.com/en-us/download/details.aspx?id=54765)\n+using the [DenseNet-121](https://arxiv.org/abs/1608.06993) architecture.\n+\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+from keras_core import layers\n+import keras_core as keras\n+from keras_core import ops\n+\n+from tensorflow import data as tf_data\n+from tensorflow import image as tf_image\n+from tensorflow import one_hot as tf_one_hot\n+\n+import tensorflow_datasets as tfds\n+\n+tfds.disable_progress_bar()\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\"\"\"\n+## Define hyperparameters\n+\"\"\"\n+\n+\"\"\"\n+In order to facilitate mini-batch learning, we need to have a fixed shape for the images\n+inside a given batch. This is why an initial resizing is required. We first resize all\n+the images to (300 x 300) shape and then learn their optimal representation for the\n+(150 x 150) resolution.\n+\"\"\"\n+\n+INP_SIZE = (300, 300)\n+TARGET_SIZE = (150, 150)\n+INTERPOLATION = \"bilinear\"\n+\n+AUTO = tf_data.AUTOTUNE\n+BATCH_SIZE = 50\n+EPOCHS = 5\n+\n+\"\"\"\n+In this example, we will use the bilinear interpolation but the learnable image resizer\n+module is not dependent on any specific interpolation method. We can also use others,\n+such as bicubic.\n+\"\"\"\n+\n+\"\"\"\n+## Load and prepare the dataset\n+\n+For this example, we will only use 40% of the total training dataset.\n+\"\"\"\n+\n+train_ds, validation_ds = tfds.load(\n+    \"cats_vs_dogs\",\n+    # Reserve 10% for validation\n+    split=[\"train[:40%]\", \"train[40%:50%]\"],\n+    as_supervised=True,\n+)\n+\n+\n+def preprocess_dataset(image, label):\n+    image = tf_image.resize(image, (INP_SIZE[0], INP_SIZE[1]))\n+    label = tf_one_hot(label, depth=2)\n+    return (image, label)\n+\n+\n+train_ds = (\n+    train_ds.shuffle(BATCH_SIZE * 100)\n+    .map(preprocess_dataset, num_parallel_calls=AUTO)\n+    .batch(BATCH_SIZE)\n+    .prefetch(AUTO)\n+)\n+validation_ds = (\n+    validation_ds.map(preprocess_dataset, num_parallel_calls=AUTO)\n+    .batch(BATCH_SIZE)\n+    .prefetch(AUTO)\n+)\n+\n+\"\"\"\n+## Define the learnable resizer utilities\n+\n+The figure below (courtesy: [Learning to Resize Images for Computer Vision Tasks](https://arxiv.org/abs/2103.09950v1))\n+presents the structure of the learnable resizing module:\n+\n+![](https://i.ibb.co/gJYtSs0/image.png)\n+\"\"\"\n+\n+\n+def conv_block(\n+    x, filters, kernel_size, strides, activation=layers.LeakyReLU(0.2)\n+):\n+    x = layers.Conv2D(\n+        filters, kernel_size, strides, padding=\"same\", use_bias=False\n+    )(x)\n+    x = layers.BatchNormalization()(x)\n+    if activation:\n+        x = activation(x)\n+    return x\n+\n+\n+def res_block(x):\n+    inputs = x\n+    x = conv_block(x, 16, 3, 1)\n+    x = conv_block(x, 16, 3, 1, activation=None)\n+    return layers.Add()([inputs, x])\n+\n+\n+def get_learnable_resizer(\n+    filters=16, num_res_blocks=1, interpolation=INTERPOLATION\n+):\n+    inputs = layers.Input(shape=[None, None, 3])\n+\n+    # First, perform naive resizing.\n+    naive_resize = layers.Resizing(*TARGET_SIZE, interpolation=interpolation)(\n+        inputs\n+    )\n+\n+    # First convolution block without batch normalization.\n+    x = layers.Conv2D(\n+        filters=filters, kernel_size=7, strides=1, padding=\"same\"\n+    )(inputs)\n+    x = layers.LeakyReLU(0.2)(x)\n+\n+    # Second convolution block with batch normalization.\n+    x = layers.Conv2D(\n+        filters=filters, kernel_size=1, strides=1, padding=\"same\"\n+    )(x)\n+    x = layers.LeakyReLU(0.2)(x)\n+    x = layers.BatchNormalization()(x)\n+\n+    # Intermediate resizing as a bottleneck.\n+    bottleneck = layers.Resizing(*TARGET_SIZE, interpolation=interpolation)(x)\n+\n+    # Residual passes.\n+    for _ in range(num_res_blocks):\n+        x = res_block(bottleneck)\n+\n+    # Projection.\n+    x = layers.Conv2D(\n+        filters=filters,\n+        kernel_size=3,\n+        strides=1,\n+        padding=\"same\",\n+        use_bias=False,\n+    )(x)\n+    x = layers.BatchNormalization()(x)\n+\n+    # Skip connection.\n+    x = layers.Add()([bottleneck, x])\n+\n+    # Final resized image.\n+    x = layers.Conv2D(filters=3, kernel_size=7, strides=1, padding=\"same\")(x)\n+    final_resize = layers.Add()([naive_resize, x])\n+\n+    return keras.Model(inputs, final_resize, name=\"learnable_resizer\")\n+\n+\n+learnable_resizer = get_learnable_resizer()\n+\n+\"\"\"\n+## Visualize the outputs of the learnable resizing module\n+\n+Here, we visualize how the resized images would look like after being passed through the\n+random weights of the resizer.\n+\"\"\"\n+\n+sample_images, _ = next(iter(train_ds))\n+\n+get_np = lambda image: ops.convert_to_numpy(\n+    ops.squeeze(image)\n+)  # Helper to convert image from any backend to numpy\n+\n+plt.figure(figsize=(16, 10))\n+for i, image in enumerate(sample_images[:6]):\n+    image = image / 255\n+\n+    ax = plt.subplot(3, 4, 2 * i + 1)\n+    plt.title(\"Input Image\")\n+    plt.imshow(image.numpy().squeeze())\n+    plt.axis(\"off\")\n+\n+    ax = plt.subplot(3, 4, 2 * i + 2)\n+    resized_image = learnable_resizer(image[None, ...])\n+    plt.title(\"Resized Image\")\n+    plt.imshow(get_np(resized_image))\n+    plt.axis(\"off\")\n+\n+\"\"\"\n+## Model building utility\n+\"\"\"\n+\n+\n+def get_model():\n+    backbone = keras.applications.DenseNet121(\n+        weights=None,\n+        include_top=True,\n+        classes=2,\n+        input_shape=((TARGET_SIZE[0], TARGET_SIZE[1], 3)),\n+    )\n+    backbone.trainable = True\n+\n+    inputs = layers.Input((INP_SIZE[0], INP_SIZE[1], 3))\n+    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n+    x = learnable_resizer(x)\n+    outputs = backbone(x)\n+\n+    return keras.Model(inputs, outputs)\n+\n+\n+\"\"\"\n+The structure of the learnable image resizer module allows for flexible integrations with\n+different vision models.\n+\"\"\"\n+\n+\"\"\"\n+## Compile and train our model with learnable resizer\n+\"\"\"\n+\n+model = get_model()\n+model.compile(\n+    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n+    optimizer=\"sgd\",\n+    metrics=[\"accuracy\"],\n+)\n+model.fit(train_ds, validation_data=validation_ds, epochs=EPOCHS)\n+\n+\"\"\"\n+## Visualize the outputs of the trained visualizer\n+\"\"\"\n+\n+plt.figure(figsize=(16, 10))\n+for i, image in enumerate(sample_images[:6]):\n+    image = image / 255\n+\n+    ax = plt.subplot(3, 4, 2 * i + 1)\n+    plt.title(\"Input Image\")\n+    plt.imshow(image.numpy().squeeze())\n+    plt.axis(\"off\")\n+\n+    ax = plt.subplot(3, 4, 2 * i + 2)\n+    resized_image = learnable_resizer(image[None, ...])\n+    plt.title(\"Resized Image\")\n+    plt.imshow(get_np(resized_image) / 10)\n+    plt.axis(\"off\")\n+\n+\"\"\"\n+The plot shows that the visuals of the images have improved with training. The following\n+table shows the benefits of using the resizing module in comparison to using the bilinear\n+interpolation:\n+\n+|           Model           \t| Number of  parameters (Million) \t| Top-1 accuracy \t|\n+|:-------------------------:\t|:-------------------------------:\t|:--------------:\t|\n+|   With the learnable resizer  \t|             7.051717            \t|      67.67%     \t|\n+| Without the learnable resizer \t|             7.039554            \t|      60.19%      \t|\n+\n+For more details, you can check out [this repository](https://github.com/sayakpaul/Learnable-Image-Resizing).\n+Note the above-reported models were trained for 10 epochs on 90% of the training set of\n+Cats and Dogs unlike this example. Also, note that the increase in the number of\n+parameters due to the resizing module is very negligible. To ensure that the improvement\n+in the performance is not due to stochasticity, the models were trained using the same\n+initial random weights.\n+\n+Now, a question worth asking here is -  _isn't the improved accuracy simply a consequence\n+of adding more layers (the resizer is a mini network after all) to the model, compared to\n+the baseline?_\n+\n+To show that it is not the case, the authors conduct the following experiment:\n+\n+* Take a pre-trained model trained some size, say (224 x 224).\n+\n+* Now, first, use it to infer predictions on images resized to a lower resolution. Record\n+the performance.\n+\n+* For the second experiment, plug in the resizer module at the top of the pre-trained\n+model and warm-start the training. Record the performance.\n+\n+Now, the authors argue that using the second option is better because it helps the model\n+learn how to adjust the representations better with respect to the given resolution.\n+Since the results purely are empirical, a few more experiments such as analyzing the\n+cross-channel interaction would have been even better. It is worth noting that elements\n+like [Squeeze and Excitation (SE) blocks](https://arxiv.org/abs/1709.01507), [Global Context (GC) blocks](https://arxiv.org/pdf/1904.11492) also add a few\n+parameters to an existing network but they are known to help a network process\n+information in systematic ways to improve the overall performance.\n+\"\"\"\n+\n+\"\"\"\n+## Notes\n+\n+* To impose shape bias inside the vision models, Geirhos et al. trained them with a\n+combination of natural and stylized images. It might be interesting to investigate if\n+this learnable resizing module could achieve something similar as the outputs seem to\n+discard the texture information.\n+\n+* The resizer module can handle arbitrary resolutions and aspect ratios which is very\n+important for tasks like object detection and segmentation.\n+\n+* There is another closely related topic on ***adaptive image resizing*** that attempts\n+to resize images/feature maps adaptively during training. [EfficientV2](https://arxiv.org/pdf/2104.00298)\n+uses this idea.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e8b245765055000b8add41f67b234d85ac45c74b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 346 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 346 | Churn Cumulative: 346 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.7422680412371134\n\nDIFF:\n@@ -0,0 +1,346 @@\n+\"\"\"\n+Title: FixRes: Fixing train-test resolution discrepancy\n+Author: [Sayak Paul](https://twitter.com/RisingSayak)\n+Date created: 2021/10/08\n+Last modified: 2021/10/10\n+Description: Mitigating resolution discrepancy between training and test sets.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+It is a common practice to use the same input image resolution while training and testing\n+vision models. However, as investigated in\n+[Fixing the train-test resolution discrepancy](https://arxiv.org/abs/1906.06423)\n+(Touvron et al.), this practice leads to suboptimal performance. Data augmentation\n+is an indispensable part of the training process of deep neural networks. For vision models, we\n+typically use random resized crops during training and center crops during inference.\n+This introduces a discrepancy in the object sizes seen during training and inference.\n+As shown by Touvron et al., if we can fix this discrepancy, we can significantly\n+boost model performance.\n+\n+In this example, we implement the **FixRes** techniques introduced by Touvron et al.\n+to fix this discrepancy.\n+\"\"\"\n+\n+\"\"\"\n+## Imports\n+\"\"\"\n+\n+import keras_core as keras\n+from keras_core import layers\n+import tensorflow as tf  # just for image processing and pipeline\n+\n+import tensorflow_datasets as tfds\n+\n+tfds.disable_progress_bar()\n+\n+import matplotlib.pyplot as plt\n+\n+\"\"\"\n+## Load the `tf_flowers` dataset\n+\"\"\"\n+\n+train_dataset, val_dataset = tfds.load(\n+    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True\n+)\n+\n+num_train = train_dataset.cardinality()\n+num_val = val_dataset.cardinality()\n+print(f\"Number of training examples: {num_train}\")\n+print(f\"Number of validation examples: {num_val}\")\n+\n+\"\"\"\n+## Data preprocessing utilities\n+\"\"\"\n+\n+\"\"\"\n+We create three datasets:\n+\n+1. A dataset with a smaller resolution - 128x128.\n+2. Two datasets with a larger resolution - 224x224.\n+\n+We will apply different augmentation transforms to the larger-resolution datasets.\n+\n+The idea of FixRes is to first train a model on a smaller resolution dataset and then fine-tune\n+it on a larger resolution dataset. This simple yet effective recipe leads to non-trivial performance\n+improvements. Please refer to the [original paper](https://arxiv.org/abs/1906.06423) for\n+results.\n+\"\"\"\n+\n+# Reference: https://github.com/facebookresearch/FixRes/blob/main/transforms_v2.py.\n+\n+batch_size = 32\n+auto = tf.data.AUTOTUNE\n+smaller_size = 128\n+bigger_size = 224\n+\n+size_for_resizing = int((bigger_size / smaller_size) * bigger_size)\n+central_crop_layer = layers.CenterCrop(bigger_size, bigger_size)\n+\n+\n+def preprocess_initial(train, image_size):\n+    \"\"\"Initial preprocessing function for training on smaller resolution.\n+\n+    For training, do random_horizontal_flip -> random_crop.\n+    For validation, just resize.\n+    No color-jittering has been used.\n+    \"\"\"\n+\n+    def _pp(image, label, train):\n+        if train:\n+            channels = image.shape[-1]\n+            begin, size, _ = tf.image.sample_distorted_bounding_box(\n+                tf.shape(image),\n+                tf.zeros([0, 0, 4], tf.float32),\n+                area_range=(0.05, 1.0),\n+                min_object_covered=0,\n+                use_image_if_no_bounding_boxes=True,\n+            )\n+            image = tf.slice(image, begin, size)\n+\n+            image.set_shape([None, None, channels])\n+            image = tf.image.resize(image, [image_size, image_size])\n+            image = tf.image.random_flip_left_right(image)\n+        else:\n+            image = tf.image.resize(image, [image_size, image_size])\n+\n+        return image, label\n+\n+    return _pp\n+\n+\n+def preprocess_finetune(image, label, train):\n+    \"\"\"Preprocessing function for fine-tuning on a higher resolution.\n+\n+    For training, resize to a bigger resolution to maintain the ratio ->\n+        random_horizontal_flip -> center_crop.\n+    For validation, do the same without any horizontal flipping.\n+    No color-jittering has been used.\n+    \"\"\"\n+    image = tf.image.resize(image, [size_for_resizing, size_for_resizing])\n+    if train:\n+        image = tf.image.random_flip_left_right(image)\n+    image = central_crop_layer(image[None, ...])[0]\n+\n+    return image, label\n+\n+\n+def make_dataset(\n+    dataset: tf.data.Dataset,\n+    train: bool,\n+    image_size: int = smaller_size,\n+    fixres: bool = True,\n+    num_parallel_calls=auto,\n+):\n+    if image_size not in [smaller_size, bigger_size]:\n+        raise ValueError(f\"{image_size} resolution is not supported.\")\n+\n+    # Determine which preprocessing function we are using.\n+    if image_size == smaller_size:\n+        preprocess_func = preprocess_initial(train, image_size)\n+    elif not fixres and image_size == bigger_size:\n+        preprocess_func = preprocess_initial(train, image_size)\n+    else:\n+        preprocess_func = preprocess_finetune\n+\n+    if train:\n+        dataset = dataset.shuffle(batch_size * 10)\n+\n+    return (\n+        dataset.map(\n+            lambda x, y: preprocess_func(x, y, train),\n+            num_parallel_calls=num_parallel_calls,\n+        )\n+        .batch(batch_size)\n+        .prefetch(num_parallel_calls)\n+    )\n+\n+\n+\"\"\"\n+Notice how the augmentation transforms vary for the kind of dataset we are preparing.\n+\"\"\"\n+\n+\"\"\"\n+## Prepare datasets\n+\"\"\"\n+\n+initial_train_dataset = make_dataset(\n+    train_dataset, train=True, image_size=smaller_size\n+)\n+initial_val_dataset = make_dataset(\n+    val_dataset, train=False, image_size=smaller_size\n+)\n+\n+finetune_train_dataset = make_dataset(\n+    train_dataset, train=True, image_size=bigger_size\n+)\n+finetune_val_dataset = make_dataset(\n+    val_dataset, train=False, image_size=bigger_size\n+)\n+\n+vanilla_train_dataset = make_dataset(\n+    train_dataset, train=True, image_size=bigger_size, fixres=False\n+)\n+vanilla_val_dataset = make_dataset(\n+    val_dataset, train=False, image_size=bigger_size, fixres=False\n+)\n+\n+\"\"\"\n+## Visualize the datasets\n+\"\"\"\n+\n+\n+def visualize_dataset(batch_images):\n+    plt.figure(figsize=(10, 10))\n+    for n in range(25):\n+        ax = plt.subplot(5, 5, n + 1)\n+        plt.imshow(batch_images[n].numpy().astype(\"int\"))\n+        plt.axis(\"off\")\n+    plt.show()\n+\n+    print(f\"Batch shape: {batch_images.shape}.\")\n+\n+\n+# Smaller resolution.\n+initial_sample_images, _ = next(iter(initial_train_dataset))\n+visualize_dataset(initial_sample_images)\n+\n+# Bigger resolution, only for fine-tuning.\n+finetune_sample_images, _ = next(iter(finetune_train_dataset))\n+visualize_dataset(finetune_sample_images)\n+\n+# Bigger resolution, with the same augmentation transforms as\n+# the smaller resolution dataset.\n+vanilla_sample_images, _ = next(iter(vanilla_train_dataset))\n+visualize_dataset(vanilla_sample_images)\n+\n+\"\"\"\n+## Model training utilities\n+\n+We train multiple variants of ResNet50V2\n+([He et al.](https://arxiv.org/abs/1603.05027)):\n+\n+1. On the smaller resolution dataset (128x128). It will be trained from scratch.\n+2. Then fine-tune the model from 1 on the larger resolution (224x224) dataset.\n+3. Train another ResNet50V2 from scratch on the larger resolution dataset.\n+\n+As a reminder, the larger resolution datasets differ in terms of their augmentation\n+transforms.\n+\"\"\"\n+\n+\n+def get_training_model(num_classes=5):\n+    inputs = layers.Input((None, None, 3))\n+    resnet_base = keras.applications.ResNet50V2(\n+        include_top=False, weights=None, pooling=\"avg\"\n+    )\n+    resnet_base.trainable = True\n+\n+    x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(inputs)\n+    x = resnet_base(x)\n+    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n+    return keras.Model(inputs, outputs)\n+\n+\n+def train_and_evaluate(\n+    model,\n+    train_ds,\n+    val_ds,\n+    epochs,\n+    learning_rate=1e-3,\n+    use_early_stopping=False,\n+):\n+    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n+    model.compile(\n+        optimizer=optimizer,\n+        loss=\"sparse_categorical_crossentropy\",\n+        metrics=[\"accuracy\"],\n+    )\n+\n+    if use_early_stopping:\n+        es_callback = keras.callbacks.EarlyStopping(patience=5)\n+        callbacks = [es_callback]\n+    else:\n+        callbacks = None\n+\n+    model.fit(\n+        train_ds,\n+        validation_data=val_ds,\n+        epochs=epochs,\n+        callbacks=callbacks,\n+    )\n+\n+    _, accuracy = model.evaluate(val_ds)\n+    print(f\"Top-1 accuracy on the validation set: {accuracy*100:.2f}%.\")\n+    return model\n+\n+\n+\"\"\"\n+## Experiment 1: Train on 128x128 and then fine-tune on 224x224\n+\"\"\"\n+\n+epochs = 30\n+\n+smaller_res_model = get_training_model()\n+smaller_res_model = train_and_evaluate(\n+    smaller_res_model, initial_train_dataset, initial_val_dataset, epochs\n+)\n+\n+\"\"\"\n+### Freeze all the layers except for the final Batch Normalization layer\n+\n+For fine-tuning, we train only two layers:\n+\n+* The final Batch Normalization ([Ioffe et al.](https://arxiv.org/abs/1502.03167)) layer.\n+* The classification layer.\n+\n+We are unfreezing the final Batch Normalization layer to compensate for the change in\n+activation statistics before the global average pooling layer. As shown in\n+[the paper](https://arxiv.org/abs/1906.06423), unfreezing the final Batch\n+Normalization layer is enough.\n+\n+For a comprehensive guide on fine-tuning models in Keras, refer to\n+[this tutorial](https://keras.io/guides/transfer_learning/).\n+\"\"\"\n+\n+for layer in smaller_res_model.layers[2].layers:\n+    layer.trainable = False\n+\n+smaller_res_model.layers[2].get_layer(\"post_bn\").trainable = True\n+\n+epochs = 10\n+\n+# Use a lower learning rate during fine-tuning.\n+bigger_res_model = train_and_evaluate(\n+    smaller_res_model,\n+    finetune_train_dataset,\n+    finetune_val_dataset,\n+    epochs,\n+    learning_rate=1e-4,\n+)\n+\n+\"\"\"\n+## Experiment 2: Train a model on 224x224 resolution from scratch\n+\n+Now, we train another model from scratch on the larger resolution dataset. Recall that\n+the augmentation transforms used in this dataset are different from before.\n+\"\"\"\n+\n+epochs = 30\n+\n+vanilla_bigger_res_model = get_training_model()\n+vanilla_bigger_res_model = train_and_evaluate(\n+    vanilla_bigger_res_model, vanilla_train_dataset, vanilla_val_dataset, epochs\n+)\n+\n+\"\"\"\n+As we can notice from the above cells, FixRes leads to a better performance. Another\n+advantage of FixRes is the improved total training time and reduction in GPU memory usage.\n+FixRes is model-agnostic, you can use it on any image classification model\n+to potentially boost performance.\n+\n+You can find more results\n+[here](https://tensorboard.dev/experiment/BQOg28w0TlmvuJYeqsVntw)\n+that were gathered by running the same code with different random seeds.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
