{"custom_id": "scrapy#6313b7ff4cf1b6df39fed2531682e1684e4e4023", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 636 | Contributors (this commit): 3 | Commits (past 90d): 25 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -177,3 +177,8 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n             Deprecated = create_deprecated_class('Deprecated', NewName, {'foo': 'bar'})\n \n         self.assertEqual(Deprecated.foo, 'bar')\n+\n+    def test_deprecate_a_class_with_custom_metaclass(self):\n+        Meta1 = type('Meta1', (type,), {})\n+        New = Meta1('New', (), {})\n+        Deprecated = create_deprecated_class('Deprecated', New)\n\n@@ -45,7 +45,7 @@ def create_deprecated_class(name, new_class, clsdict=None,\n     OldName.\n     \"\"\"\n \n-    class DeprecatedClass(type):\n+    class DeprecatedClass(new_class.__class__):\n \n         deprecated_class = None\n         warned_on_subclass = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#116f01f2d20602e58a9e98985f4a3a04252cb912", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 33 | Churn Cumulative: 669 | Contributors (this commit): 3 | Commits (past 90d): 27 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,6 +46,25 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         )\n         self.assertEqual(w[0].lineno, inspect.getsourcelines(UserClass)[1])\n \n+    def test_custom_class_paths(self):\n+        Deprecated = create_deprecated_class('Deprecated', NewName,\n+                                             new_class_path='foo.NewClass',\n+                                             old_class_path='bar.OldClass',\n+                                             warn_category=MyWarning)\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            class UserClass(Deprecated):\n+                pass\n+\n+            _ = Deprecated()\n+\n+        w = self._mywarnings(w)\n+        self.assertEqual(len(w), 2)\n+        self.assertIn('foo.NewClass', str(w[0].message))\n+        self.assertIn('bar.OldClass', str(w[0].message))\n+        self.assertIn('foo.NewClass', str(w[1].message))\n+        self.assertIn('bar.OldClass', str(w[1].message))\n+\n     def test_subclassing_warns_only_on_direct_childs(self):\n         Deprecated = create_deprecated_class('Deprecated', NewName,\n                                              warn_once=False,\n\n@@ -15,6 +15,8 @@ def attribute(obj, oldattr, newattr, version='0.12'):\n def create_deprecated_class(name, new_class, clsdict=None,\n                             warn_category=ScrapyDeprecationWarning,\n                             warn_once=True,\n+                            old_class_path=None,\n+                            new_class_path=None,\n                             subclass_warn_message=\"{cls} inherits from \"\\\n                                     \"deprecated class {old}, please inherit \"\\\n                                     \"from {new}.\",\n@@ -62,8 +64,8 @@ def create_deprecated_class(name, new_class, clsdict=None,\n             if old in bases and not (warn_once and meta.warned_on_subclass):\n                 meta.warned_on_subclass = True\n                 msg = subclass_warn_message.format(cls=_clspath(cls),\n-                                                   old=_clspath(old),\n-                                                   new=_clspath(new_class))\n+                                                   old=_clspath(old, old_class_path),\n+                                                   new=_clspath(new_class, new_class_path))\n                 if warn_once:\n                     msg += ' (warning only on first subclass, there may be others)'\n                 warnings.warn(msg, warn_category, stacklevel=2)\n@@ -88,8 +90,8 @@ def create_deprecated_class(name, new_class, clsdict=None,\n             meta = cls.__class__\n             old = meta.deprecated_class\n             if cls is old:\n-                msg = instance_warn_message.format(cls=_clspath(cls),\n-                                                   new=_clspath(new_class))\n+                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n+                                                   new=_clspath(new_class, new_class_path))\n                 warnings.warn(msg, warn_category, stacklevel=2)\n             return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n \n@@ -102,5 +104,7 @@ def create_deprecated_class(name, new_class, clsdict=None,\n     return deprecated_cls\n \n \n-def _clspath(cls):\n+def _clspath(cls, forced=None):\n+    if forced is not None:\n+        return forced\n     return '{}.{}'.format(cls.__module__, cls.__name__)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31dd3dc4a1060e5e2ab053270e31529aa60819c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 696 | Contributors (this commit): 3 | Commits (past 90d): 29 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -201,3 +201,27 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         Meta1 = type('Meta1', (type,), {})\n         New = Meta1('New', (), {})\n         Deprecated = create_deprecated_class('Deprecated', New)\n+\n+    def test_deprecate_subclass_of_deprecated_class(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter('always')\n+            Deprecated = create_deprecated_class('Deprecated', NewName,\n+                                                 warn_category=MyWarning)\n+            AlsoDeprecated = create_deprecated_class('AlsoDeprecated', Deprecated,\n+                                                     new_class_path='foo.Bar',\n+                                                     warn_category=MyWarning)\n+\n+        w = self._mywarnings(w)\n+        self.assertEqual(len(w), 0, str(map(str, w)))\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            AlsoDeprecated()\n+            class UserClass(AlsoDeprecated):\n+                pass\n+\n+        w = self._mywarnings(w)\n+        self.assertEqual(len(w), 2)\n+        self.assertIn('AlsoDeprecated', str(w[0].message))\n+        self.assertIn('foo.Bar', str(w[0].message))\n+        self.assertIn('AlsoDeprecated', str(w[1].message))\n+        self.assertIn('foo.Bar', str(w[1].message))\n\n@@ -87,8 +87,7 @@ def create_deprecated_class(name, new_class, clsdict=None,\n             return any(c in candidates for c in mro)\n \n         def __call__(cls, *args, **kwargs):\n-            meta = cls.__class__\n-            old = meta.deprecated_class\n+            old = DeprecatedClass.deprecated_class\n             if cls is old:\n                 msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                    new=_clspath(new_class, new_class_path))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#968141cd42b108e4343e3e6775ea223ad8d9b4b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 132 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 13/6 | Churn Δ: 132 | Churn Cumulative: 923 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,60 @@\n+import re\n+from six.moves import cStringIO\n+from mock import patch\n+from twisted.trial import unittest\n+\n+import scrapy.commands.deploy as deploy\n+from scrapy.exceptions import UsageError\n+from scrapy.tests.test_commands import CommandMockTest\n+\n+\n+@patch('urllib2.install_opener')\n+class DeployMockTest(CommandMockTest, unittest.TestCase):\n+\n+    Command = deploy.Command\n+\n+    def test_wrong_target(self, install_opener_mock):\n+        self.assertRaisesRegexp(UsageError, r'^Unknown target: wrong$', self.run_command, ['wrong'])\n+\n+    @patch('scrapy.commands.deploy._get_targets', autospec=True)\n+    def test_list_targets(self, _get_targets_mock, install_opener_mock):\n+        _get_targets_mock.return_value = {\n+            'target1': {'url': 'url1'},\n+            'target2': {'url': 'url2'},\n+        }\n+        stream = cStringIO()\n+        with patch('sys.stdout') as stdout_mock:\n+            stdout_mock.write = stream.write\n+            self.run_command(['--list-targets'])\n+        # We need to sort output strings and replace arbitrary number of spaces with 1 space to make comparison exact\n+        sorted_output = '\\n'.join(x for x in sorted(re.sub(r'\\s{2,}', ' ', stream.getvalue()).splitlines()) if x.strip())\n+        self.assertEqual(sorted_output, 'target1 url1\\ntarget2 url2')\n+\n+    @patch('urllib2.urlopen', autospec=True)\n+    @patch('scrapy.commands.deploy._get_targets', autospec=True)\n+    def test_list_projects(self, _get_targets_mock, urlopen_mock, install_opener_mock):\n+        urlopen_mock.return_value = cStringIO('{\"projects\": [\"project1\", \"project2\"]}')\n+        _get_targets_mock.return_value = {\n+            'target1': {'url': 'http://localhost/target1'},\n+            'target2': {'url': 'http://localhost/target2'},\n+        }\n+        stream = cStringIO()\n+        with patch('sys.stdout') as stdout_mock:\n+            stdout_mock.write = stream.write\n+            self.run_command(['--list-projects', 'target1'])\n+        self.assertEqual(stream.getvalue().strip(), 'project1\\nproject2')\n+\n+    @patch('scrapy.commands.deploy._build_egg', autospec=True)\n+    @patch('shutil.copyfile', autospec=True)\n+    @patch('shutil.rmtree', autospec=True)\n+    def test_build_egg(self, rmtree_mock, copyfile_mock, _build_egg_mock, install_opener_mock):\n+        _build_egg_mock.return_value = ('egg', '/egg_temp_dir')\n+        stream = cStringIO()\n+        with patch('sys.stderr') as stdout_mock:\n+            stdout_mock.write = stream.write\n+            self.run_command(['--build-egg', '/target/egg'])\n+        self.assertEqual(stream.getvalue().strip(), 'Writing egg to /target/egg')\n+        self.assertEqual(copyfile_mock.call_count, 1)\n+        self.assertEqual(copyfile_mock.call_args[0], ('egg', '/target/egg'))\n+        self.assertEqual(rmtree_mock.call_count, 1)\n+        self.assertEqual(rmtree_mock.call_args[0], ('/egg_temp_dir',))\n\n@@ -1,3 +1,4 @@\n+import optparse\n import os\n import sys\n import subprocess\n@@ -7,6 +8,7 @@ from shutil import rmtree\n from tempfile import mkdtemp\n \n from twisted.trial import unittest\n+from scrapy.settings import CrawlerSettings\n \n from scrapy.utils.python import retry_on_eintr\n from scrapy.utils.test import get_testenv\n@@ -235,3 +237,28 @@ class BenchCommandTest(CommandTest):\n                 '-s', 'CLOSESPIDER_TIMEOUT=0.01')\n         log = p.stderr.read()\n         self.assert_('INFO: Crawled' in log, log)\n+\n+\n+class CommandMockTest(object):\n+    \"\"\"This class is used to test commands and improve code coverage\n+    without invoking actual command subprocess.\n+\n+    Subclass must:\n+    1. Define Command = CommandClassToTest in class scope.\n+    2. Call super.setUp() and super.tearDown() if methods are overridden.\n+    3. Call run_command(argv) in test methods.\n+    \"\"\"\n+\n+    def setUp(self):\n+        self.cmd = self.__class__.Command()\n+        self.cmd.settings = CrawlerSettings(object())\n+        self.parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), conflict_handler='resolve')\n+        self.cmd.add_options(self.parser)\n+\n+    def tearDown(self):\n+        pass\n+\n+    def run_command(self, argv):\n+        opts, args = self.parser.parse_args(argv)\n+        self.cmd.process_options(args, opts)\n+        self.cmd.run(args, opts)\n\n@@ -2,6 +2,7 @@ import re\n import unittest\n from scrapy.http import HtmlResponse\n from scrapy.link import Link\n+from scrapy.contrib.linkextractors.htmlparser import HtmlParserLinkExtractor\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor, BaseSgmlLinkExtractor\n from scrapy.tests import get_testdata\n \n@@ -294,5 +295,22 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n \n+class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n+        self.response = HtmlResponse(url='http://example.com/index', body=body)\n+\n+    def test_extraction(self):\n+        # Default arguments\n+        lx = HtmlParserLinkExtractor()\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+            Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+            Link(url='http://example.com/sample3.html', text='sample 3 repetition'),\n+            Link(url='http://www.google.com/something', text=''),\n+        ])\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -0,0 +1,27 @@\n+import mock\n+from twisted.internet import reactor\n+from twisted.internet.defer import Deferred\n+from twisted.trial import unittest\n+from scrapy.contrib.downloadermiddleware.robotstxt import RobotsTxtMiddleware\n+from scrapy.exceptions import IgnoreRequest\n+from scrapy.http import Request, Response\n+from scrapy.settings import CrawlerSettings\n+\n+\n+class RobotsTxtMiddlewareTest(unittest.TestCase):\n+\n+    def test(self):\n+        crawler = mock.MagicMock()\n+        crawler.settings = CrawlerSettings()\n+        crawler.settings.overrides['USER_AGENT'] = 'CustomAgent'\n+        crawler.settings.overrides['ROBOTSTXT_OBEY'] = True\n+        crawler.engine.download = mock.MagicMock()\n+        deferred = Deferred()\n+        crawler.engine.download.return_value = deferred\n+        ROBOTS = ''\n+        response = Response('http://site.local/robots.txt', body=ROBOTS)\n+        reactor.callLater(0, deferred.callback, response)\n+        middleware = RobotsTxtMiddleware(crawler)\n+        spider = None # Not actually used\n+        self.assertIsNone(middleware.process_request(Request('http://site.local/dummy'), spider))\n+        self.assertRaises(IgnoreRequest, middleware.process_request, Request('http://site.local/forbidden'), spider)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a54e31cebc267339ba805aa06157fba8cb1a6dbb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 19 | Churn Cumulative: 387 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import re\n import unittest\n+from scrapy.contrib.linkextractors.regex import RegexLinkExtractor\n from scrapy.http import HtmlResponse\n from scrapy.link import Link\n from scrapy.contrib.linkextractors.htmlparser import HtmlParserLinkExtractor\n@@ -312,5 +313,23 @@ class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n         ])\n \n \n+class RegexLinkExtractorTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n+        self.response = HtmlResponse(url='http://example.com/index', body=body)\n+\n+    def test_extraction(self):\n+        # Default arguments\n+        lx = RegexLinkExtractor()\n+        # Note that RegexLinkExtractor returns links in arbitrary order,\n+        # so we need to sort them for comparison\n+        self.assertEqual(sorted(lx.extract_links(self.response), key=lambda x: x.url), [\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+            Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n+            Link(url='http://www.google.com/something', text=u''),\n+        ])\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4862a7d7fea87e0861ff5429673b8ab131f6a1e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 35 | Churn Cumulative: 62 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,10 @@\n+import re\n import mock\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n from scrapy.contrib.downloadermiddleware.robotstxt import RobotsTxtMiddleware\n-from scrapy.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response\n from scrapy.settings import CrawlerSettings\n \n@@ -14,14 +15,32 @@ class RobotsTxtMiddlewareTest(unittest.TestCase):\n         crawler = mock.MagicMock()\n         crawler.settings = CrawlerSettings()\n         crawler.settings.overrides['USER_AGENT'] = 'CustomAgent'\n+        self.assertRaises(NotConfigured, RobotsTxtMiddleware, crawler)\n         crawler.settings.overrides['ROBOTSTXT_OBEY'] = True\n         crawler.engine.download = mock.MagicMock()\n-        deferred = Deferred()\n-        crawler.engine.download.return_value = deferred\n-        ROBOTS = ''\n+        ROBOTS = re.sub(r'^\\s+(?m)', '', '''\n+        User-Agent: *\n+        Disallow: /admin/\n+        Disallow: /static/\n+        ''')\n         response = Response('http://site.local/robots.txt', body=ROBOTS)\n-        reactor.callLater(0, deferred.callback, response)\n+        def return_response(request, spider):\n+            deferred = Deferred()\n+            reactor.callFromThread(deferred.callback, response)\n+            return deferred\n+        crawler.engine.download.side_effect = return_response\n         middleware = RobotsTxtMiddleware(crawler)\n-        spider = None # Not actually used\n-        self.assertIsNone(middleware.process_request(Request('http://site.local/dummy'), spider))\n-        self.assertRaises(IgnoreRequest, middleware.process_request, Request('http://site.local/forbidden'), spider)\n\\ No newline at end of file\n+        spider = None  # not actually used\n+        # There is a bit of neglect in robotstxt.py: robots.txt is fetched asynchronously,\n+        # and it is actually fetched only *after* first process_request completes.\n+        # So, first process_request will always succeed.\n+        # We defer test() because otherwise robots.txt download mock will be called after assertRaises failure.\n+        self.assertIsNone(middleware.process_request(Request('http://site.local'), spider))  # not affected by robots.txt\n+        def test(r):\n+            self.assertIsNone(middleware.process_request(Request('http://site.local/allowed'), spider))\n+            self.assertRaises(IgnoreRequest, middleware.process_request, Request('http://site.local/admin/main'), spider)\n+            self.assertRaises(IgnoreRequest, middleware.process_request, Request('http://site.local/static/'), spider)\n+        deferred = Deferred()\n+        deferred.addCallback(test)\n+        reactor.callFromThread(deferred.callback, None)\n+        return deferred\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d46534cc547b4da79f9b47b4ccec40a16f1ca3c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 93 | Churn Cumulative: 1022 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ XPath selectors based on lxml\n \"\"\"\n \n from lxml import etree\n+import copy\n \n from scrapy.utils.misc import extract_regex\n from scrapy.utils.trackref import object_ref\n@@ -46,6 +47,45 @@ class Selector(object_ref):\n                  '__weakref__', '_parser', '_csstranslator', '_tostring_method']\n \n     _default_type = None\n+    _default_namespaces = {\n+        \"regexp\": \"http://exslt.org/regular-expressions\",\n+\n+        # supported in libxslt:\n+        # set:difference\n+        # set:has-same-node\n+        # set:intersection\n+        # set:leading\n+        # set:trailing\n+        \"set\": \"http://exslt.org/sets\",\n+\n+        # supported in libxslt:\n+        # math:abs()\n+        # math:acos()\n+        # math:asin()\n+        # math:atan()\n+        # math:atan2()\n+        # math:constant()\n+        # math:cos()\n+        # math:exp()\n+        # math:highest()\n+        # math:log()\n+        # math:lowest()\n+        # math:max()\n+        # math:min()\n+        # math:power()\n+        # math:random()\n+        # math:sin()\n+        # math:sqrt()\n+        # math:tan()\n+        \"math\": \"http://exslt.org/math\",\n+\n+        # supported in libxslt:\n+        # str:align\n+        # str:concat\n+        # str:padding\n+        # str:tokenize\n+        \"str\": \"http://exslt.org/strings\",\n+    }\n \n     def __init__(self, response=None, text=None, type=None, namespaces=None,\n                  _root=None, _expr=None):\n@@ -61,7 +101,9 @@ class Selector(object_ref):\n             _root = LxmlDocument(response, self._parser)\n \n         self.response = response\n-        self.namespaces = namespaces\n+        ns = copy.copy(self._default_namespaces)\n+        ns.update(namespaces or {})\n+        self.namespaces = ns\n         self._root = _root\n         self._expr = _expr\n \n\n@@ -333,3 +333,52 @@ class DeprecatedXpathSelectorTest(unittest.TestCase):\n             self.assertEqual(xs.select(\"//div\").extract(),\n                              [u'<div><img src=\"a.jpg\"><p>Hello</p></img></div>'])\n             self.assertRaises(RuntimeError, xs.css, 'div')\n+\n+\n+\n+class ExsltTestCase(unittest.TestCase):\n+\n+    sscls = Selector\n+\n+    def test_regexp(self):\n+        \"\"\"EXSLT regular expression tests\"\"\"\n+        body = \"\"\"\n+        <p><input name='a' value='1'/><input name='b' value='2'/></p>\n+        <div class=\"links\">\n+        <a href=\"/first.html\">first link</a>\n+        <a href=\"/second.html\">second link</a>\n+        <a href=\"http://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.xml\">EXSLT match example</a>\n+        </div>\n+        \"\"\"\n+        response = TextResponse(url=\"http://example.com\", body=body)\n+        sel = self.sscls(response)\n+\n+        # regexp:test()\n+        self.assertEqual(sel.xpath('//input[regexp:test(@name, \"[A-Z]+\", \"i\")]').extract(),\n+                         [x.extract() for x in sel.xpath('//input[regexp:test(@name, \"[A-Z]+\", \"i\")]')])\n+        self.assertEqual([x.extract() for x in sel.xpath('//a[regexp:test(@href, \"\\.html$\")]/text()')],\n+                         [u'first link', u'second link'])\n+        self.assertEqual([x.extract() for x in sel.xpath('//a[regexp:test(@href, \"first\")]/text()')],\n+                         [u'first link'])\n+        self.assertEqual([x.extract() for x in sel.xpath('//a[regexp:test(@href, \"second\")]/text()')],\n+                         [u'second link'])\n+\n+        # regexp:match() is rather special: it returns a node-set of <match> nodes\n+        #[u'<match>http://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.xml</match>',\n+        #u'<match>http</match>',\n+        #u'<match>www.bayes.co.uk</match>',\n+        #u'<match></match>',\n+        #u'<match>/xml/index.xml?/xml/utils/rechecker.xml</match>']\n+        self.assertEqual(sel.xpath(''\n+            'regexp:match(//a[regexp:test(@href, \"\\.xml$\")]/@href,'\n+            '\"(\\w+):\\/\\/([^/:]+)(:\\d*)?([^# ]*)\")/text()').extract(),\n+                         [u'http://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.xml',\n+                             u'http',\n+                             u'www.bayes.co.uk',\n+                             u'',\n+                             u'/xml/index.xml?/xml/utils/rechecker.xml'])\n+\n+        # regexp:replace()\n+        self.assertEqual(sel.xpath('regexp:replace(//a[regexp:test(@href, \"\\.xml$\")]/@href,'\n+            '\"(\\w+)://(.+)(\\.xml)\", \"\",\"https://\\\\2.html\")').extract(),\n+                         [u'https://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.html'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5df56a975884ed0680283ff6e301f8a4d32f4712", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 267 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,6 @@ XPath selectors based on lxml\n \"\"\"\n \n from lxml import etree\n-import copy\n \n from scrapy.utils.misc import extract_regex\n from scrapy.utils.trackref import object_ref\n@@ -101,9 +100,9 @@ class Selector(object_ref):\n             _root = LxmlDocument(response, self._parser)\n \n         self.response = response\n-        ns = copy.copy(self._default_namespaces)\n-        ns.update(namespaces or {})\n-        self.namespaces = ns\n+        self.namespaces = dict(self._default_namespaces)\n+        if namespaces is not None:\n+            self.namespaces.update(namespaces)\n         self._root = _root\n         self._expr = _expr\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#29fc9f3466a0ca50012f253fff27573a929d7275", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 55 | Churn Cumulative: 817 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -382,3 +382,58 @@ class ExsltTestCase(unittest.TestCase):\n         self.assertEqual(sel.xpath('regexp:replace(//a[regexp:test(@href, \"\\.xml$\")]/@href,'\n             '\"(\\w+)://(.+)(\\.xml)\", \"\",\"https://\\\\2.html\")').extract(),\n                          [u'https://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.html'])\n+\n+    def test_set(self):\n+        \"\"\"EXSLT set manipulation tests\"\"\"\n+        # microdata example from http://schema.org/Event\n+        body=\"\"\"\n+        <div itemscope itemtype=\"http://schema.org/Event\">\n+          <a itemprop=\"url\" href=\"nba-miami-philidelphia-game3.html\">\n+          NBA Eastern Conference First Round Playoff Tickets:\n+          <span itemprop=\"name\"> Miami Heat at Philadelphia 76ers - Game 3 (Home Game 1) </span>\n+          </a>\n+\n+          <meta itemprop=\"startDate\" content=\"2016-04-21T20:00\">\n+            Thu, 04/21/16\n+            8:00 p.m.\n+\n+          <div itemprop=\"location\" itemscope itemtype=\"http://schema.org/Place\">\n+            <a itemprop=\"url\" href=\"wells-fargo-center.html\">\n+            Wells Fargo Center\n+            </a>\n+            <div itemprop=\"address\" itemscope itemtype=\"http://schema.org/PostalAddress\">\n+              <span itemprop=\"addressLocality\">Philadelphia</span>,\n+              <span itemprop=\"addressRegion\">PA</span>\n+            </div>\n+          </div>\n+\n+          <div itemprop=\"offers\" itemscope itemtype=\"http://schema.org/AggregateOffer\">\n+            Priced from: <span itemprop=\"lowPrice\">$35</span>\n+            <span itemprop=\"offerCount\">1938</span> tickets left\n+          </div>\n+        </div>\n+        \"\"\"\n+        response = TextResponse(url=\"http://example.com\", body=body)\n+        sel = self.sscls(response)\n+\n+        self.assertEqual(\n+            sel.xpath('''//div[@itemtype=\"http://schema.org/Event\"]\n+                            //@itemprop''').extract(),\n+            [u'url',\n+             u'name',\n+             u'startDate',\n+             u'location',\n+             u'url',\n+             u'address',\n+             u'addressLocality',\n+             u'addressRegion',\n+             u'offers',\n+             u'lowPrice',\n+             u'offerCount']\n+)\n+        self.assertEqual(sel.xpath('''\n+                set:difference(//div[@itemtype=\"http://schema.org/Event\"]\n+                                    //@itemprop,\n+                               //div[@itemtype=\"http://schema.org/Event\"]\n+                                    //*[@itemscope]/*/@itemprop)''').extract(),\n+            [u'url', u'name', u'startDate', u'location', u'offers'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a83c6f545c440edb901e706beb2cb4e6c24b8381", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 236 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ MemoryUsage extension\n \n See documentation in docs/topics/extensions.rst\n \"\"\"\n-\n+import sys\n import socket\n from pprint import pformat\n from importlib import import_module\n@@ -41,7 +41,11 @@ class MemoryUsage(object):\n         return cls(crawler)\n \n     def get_virtual_size(self):\n-        return self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss * 1024\n+        size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n+        if sys.platform != 'darwin':\n+            # on Mac OS X ru_maxrss is in bytes, on Linux it is in KB\n+            size *= 1024\n+        return size\n \n     def engine_started(self):\n         self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26538b01b5865c4b5f68fc0ea0f22268d4768376", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 184 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -21,13 +21,13 @@ class MemoryDebugger(object):\n         if not crawler.settings.getbool('MEMDEBUG_ENABLED'):\n             raise NotConfigured\n         o = cls(crawler.stats)\n-        crawler.signals.connect(o.engine_stopped, signals.engine_stopped)\n+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n \n-    def engine_stopped(self):\n+    def spider_closed(self, spider, reason):\n         gc.collect()\n-        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage))\n+        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage), spider=spider)\n         for cls, wdict in live_refs.iteritems():\n             if not wdict:\n                 continue\n-            self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict))\n+            self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict), spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ab6496647c1e4b379898e5942ef5085e640fd63c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 230 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -129,6 +129,7 @@ else:\n         'lxml',\n         'pyOpenSSL',\n         'cssselect>=0.9',\n+        'six>=1.5.2',\n     ]\n \n setup(**setup_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#25759bdf5c891fa4084abe8bc8c86b56dae0a38f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 87 | Files Changed: 2 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 87 | Churn Cumulative: 615 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,60 +0,0 @@\n-import re\n-from six.moves import cStringIO\n-from mock import patch\n-from twisted.trial import unittest\n-\n-import scrapy.commands.deploy as deploy\n-from scrapy.exceptions import UsageError\n-from scrapy.tests.test_commands import CommandMockTest\n-\n-\n-@patch('urllib2.install_opener')\n-class DeployMockTest(CommandMockTest, unittest.TestCase):\n-\n-    Command = deploy.Command\n-\n-    def test_wrong_target(self, install_opener_mock):\n-        self.assertRaisesRegexp(UsageError, r'^Unknown target: wrong$', self.run_command, ['wrong'])\n-\n-    @patch('scrapy.commands.deploy._get_targets', autospec=True)\n-    def test_list_targets(self, _get_targets_mock, install_opener_mock):\n-        _get_targets_mock.return_value = {\n-            'target1': {'url': 'url1'},\n-            'target2': {'url': 'url2'},\n-        }\n-        stream = cStringIO()\n-        with patch('sys.stdout') as stdout_mock:\n-            stdout_mock.write = stream.write\n-            self.run_command(['--list-targets'])\n-        # We need to sort output strings and replace arbitrary number of spaces with 1 space to make comparison exact\n-        sorted_output = '\\n'.join(x for x in sorted(re.sub(r'\\s{2,}', ' ', stream.getvalue()).splitlines()) if x.strip())\n-        self.assertEqual(sorted_output, 'target1 url1\\ntarget2 url2')\n-\n-    @patch('urllib2.urlopen', autospec=True)\n-    @patch('scrapy.commands.deploy._get_targets', autospec=True)\n-    def test_list_projects(self, _get_targets_mock, urlopen_mock, install_opener_mock):\n-        urlopen_mock.return_value = cStringIO('{\"projects\": [\"project1\", \"project2\"]}')\n-        _get_targets_mock.return_value = {\n-            'target1': {'url': 'http://localhost/target1'},\n-            'target2': {'url': 'http://localhost/target2'},\n-        }\n-        stream = cStringIO()\n-        with patch('sys.stdout') as stdout_mock:\n-            stdout_mock.write = stream.write\n-            self.run_command(['--list-projects', 'target1'])\n-        self.assertEqual(stream.getvalue().strip(), 'project1\\nproject2')\n-\n-    @patch('scrapy.commands.deploy._build_egg', autospec=True)\n-    @patch('shutil.copyfile', autospec=True)\n-    @patch('shutil.rmtree', autospec=True)\n-    def test_build_egg(self, rmtree_mock, copyfile_mock, _build_egg_mock, install_opener_mock):\n-        _build_egg_mock.return_value = ('egg', '/egg_temp_dir')\n-        stream = cStringIO()\n-        with patch('sys.stderr') as stdout_mock:\n-            stdout_mock.write = stream.write\n-            self.run_command(['--build-egg', '/target/egg'])\n-        self.assertEqual(stream.getvalue().strip(), 'Writing egg to /target/egg')\n-        self.assertEqual(copyfile_mock.call_count, 1)\n-        self.assertEqual(copyfile_mock.call_args[0], ('egg', '/target/egg'))\n-        self.assertEqual(rmtree_mock.call_count, 1)\n-        self.assertEqual(rmtree_mock.call_args[0], ('/egg_temp_dir',))\n\n@@ -1,4 +1,3 @@\n-import optparse\n import os\n import sys\n import subprocess\n@@ -8,7 +7,6 @@ from shutil import rmtree\n from tempfile import mkdtemp\n \n from twisted.trial import unittest\n-from scrapy.settings import CrawlerSettings\n \n from scrapy.utils.python import retry_on_eintr\n from scrapy.utils.test import get_testenv\n@@ -237,28 +235,3 @@ class BenchCommandTest(CommandTest):\n                 '-s', 'CLOSESPIDER_TIMEOUT=0.01')\n         log = p.stderr.read()\n         self.assert_('INFO: Crawled' in log, log)\n-\n-\n-class CommandMockTest(object):\n-    \"\"\"This class is used to test commands and improve code coverage\n-    without invoking actual command subprocess.\n-\n-    Subclass must:\n-    1. Define Command = CommandClassToTest in class scope.\n-    2. Call super.setUp() and super.tearDown() if methods are overridden.\n-    3. Call run_command(argv) in test methods.\n-    \"\"\"\n-\n-    def setUp(self):\n-        self.cmd = self.__class__.Command()\n-        self.cmd.settings = CrawlerSettings(object())\n-        self.parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), conflict_handler='resolve')\n-        self.cmd.add_options(self.parser)\n-\n-    def tearDown(self):\n-        pass\n-\n-    def run_command(self, argv):\n-        opts, args = self.parser.parse_args(argv)\n-        self.cmd.process_options(args, opts)\n-        self.cmd.run(args, opts)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2421a4b3f7d90ee256599ea60be8d6d924625a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 78 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,9 @@\n from __future__ import print_function\n import os\n-from scrapy.utils.request import request_fingerprint\n-from scrapy.utils.job import job_dir\n+\n from scrapy import log\n+from scrapy.utils.job import job_dir\n+from scrapy.utils.request import request_fingerprint\n \n \n class BaseDupeFilter(object):\n@@ -14,6 +15,9 @@ class BaseDupeFilter(object):\n     def request_seen(self, request):\n         return False\n \n+    def request_fingerprint(self, request):\n+        pass\n+\n     def open(self):  # can return deferred\n         pass\n \n@@ -23,6 +27,7 @@ class BaseDupeFilter(object):\n     def log(self, request, spider):  # log that a request has been filtered\n         pass\n \n+\n class RFPDupeFilter(BaseDupeFilter):\n     \"\"\"Request Fingerprint duplicates filter\"\"\"\n \n@@ -39,13 +44,19 @@ class RFPDupeFilter(BaseDupeFilter):\n         return cls(job_dir(settings))\n \n     def request_seen(self, request):\n-        fp = request_fingerprint(request)\n+        fp = self.request_fingerprint(request)\n         if fp in self.fingerprints:\n             return True\n         self.fingerprints.add(fp)\n         if self.file:\n             self.file.write(fp + os.linesep)\n \n+    def request_fingerprint(self, request):\n+        \"\"\"Override this method to implement a different duplicate check.\n+\n+        \"\"\"\n+        return request_fingerprint(request)\n+\n     def close(self, reason):\n         if self.file:\n             self.file.close()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a3eba68aca804e67f9a083c4d9c490da9a243b59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 37 | Files Changed: 2 | Hunks: 11 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 61 | Churn Cumulative: 1145 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,35 +55,7 @@ class Selector(object_ref):\n         # set:intersection\n         # set:leading\n         # set:trailing\n-        \"set\": \"http://exslt.org/sets\",\n-\n-        # supported in libxslt:\n-        # math:abs()\n-        # math:acos()\n-        # math:asin()\n-        # math:atan()\n-        # math:atan2()\n-        # math:constant()\n-        # math:cos()\n-        # math:exp()\n-        # math:highest()\n-        # math:log()\n-        # math:lowest()\n-        # math:max()\n-        # math:min()\n-        # math:power()\n-        # math:random()\n-        # math:sin()\n-        # math:sqrt()\n-        # math:tan()\n-        \"math\": \"http://exslt.org/math\",\n-\n-        # supported in libxslt:\n-        # str:align\n-        # str:concat\n-        # str:padding\n-        # str:tokenize\n-        \"str\": \"http://exslt.org/strings\",\n+        \"set\": \"http://exslt.org/sets\"\n     }\n \n     def __init__(self, response=None, text=None, type=None, namespaces=None,\n\n@@ -335,7 +335,6 @@ class DeprecatedXpathSelectorTest(unittest.TestCase):\n             self.assertRaises(RuntimeError, xs.css, 'div')\n \n \n-\n class ExsltTestCase(unittest.TestCase):\n \n     sscls = Selector\n@@ -354,23 +353,35 @@ class ExsltTestCase(unittest.TestCase):\n         sel = self.sscls(response)\n \n         # regexp:test()\n-        self.assertEqual(sel.xpath('//input[regexp:test(@name, \"[A-Z]+\", \"i\")]').extract(),\n+        self.assertEqual(\n+            sel.xpath(\n+                '//input[regexp:test(@name, \"[A-Z]+\", \"i\")]').extract(),\n             [x.extract() for x in sel.xpath('//input[regexp:test(@name, \"[A-Z]+\", \"i\")]')])\n-        self.assertEqual([x.extract() for x in sel.xpath('//a[regexp:test(@href, \"\\.html$\")]/text()')],\n+        self.assertEqual(\n+            [x.extract()\n+             for x in sel.xpath(\n+                 '//a[regexp:test(@href, \"\\.html$\")]/text()')],\n             [u'first link', u'second link'])\n-        self.assertEqual([x.extract() for x in sel.xpath('//a[regexp:test(@href, \"first\")]/text()')],\n+        self.assertEqual(\n+            [x.extract()\n+             for x in sel.xpath(\n+                 '//a[regexp:test(@href, \"first\")]/text()')],\n             [u'first link'])\n-        self.assertEqual([x.extract() for x in sel.xpath('//a[regexp:test(@href, \"second\")]/text()')],\n+        self.assertEqual(\n+            [x.extract()\n+             for x in sel.xpath(\n+                 '//a[regexp:test(@href, \"second\")]/text()')],\n             [u'second link'])\n \n+\n         # regexp:match() is rather special: it returns a node-set of <match> nodes\n         #[u'<match>http://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.xml</match>',\n         #u'<match>http</match>',\n         #u'<match>www.bayes.co.uk</match>',\n         #u'<match></match>',\n         #u'<match>/xml/index.xml?/xml/utils/rechecker.xml</match>']\n-        self.assertEqual(sel.xpath(''\n-            'regexp:match(//a[regexp:test(@href, \"\\.xml$\")]/@href,'\n+        self.assertEqual(\n+            sel.xpath('regexp:match(//a[regexp:test(@href, \"\\.xml$\")]/@href,'\n                       '\"(\\w+):\\/\\/([^/:]+)(:\\d*)?([^# ]*)\")/text()').extract(),\n             [u'http://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.xml',\n              u'http',\n@@ -378,8 +389,11 @@ class ExsltTestCase(unittest.TestCase):\n              u'',\n              u'/xml/index.xml?/xml/utils/rechecker.xml'])\n \n+\n+\n         # regexp:replace()\n-        self.assertEqual(sel.xpath('regexp:replace(//a[regexp:test(@href, \"\\.xml$\")]/@href,'\n+        self.assertEqual(\n+            sel.xpath('regexp:replace(//a[regexp:test(@href, \"\\.xml$\")]/@href,'\n                       '\"(\\w+)://(.+)(\\.xml)\", \"\",\"https://\\\\2.html\")').extract(),\n             [u'https://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.html'])\n \n@@ -431,6 +445,7 @@ class ExsltTestCase(unittest.TestCase):\n              u'lowPrice',\n              u'offerCount']\n         )\n+\n         self.assertEqual(sel.xpath('''\n                 set:difference(//div[@itemtype=\"http://schema.org/Event\"]\n                                     //@itemprop,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#54116959056559c32fada45fd9b543e6563bb194", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 84 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,9 +15,6 @@ class BaseDupeFilter(object):\n     def request_seen(self, request):\n         return False\n \n-    def request_fingerprint(self, request):\n-        pass\n-\n     def open(self):  # can return deferred\n         pass\n \n@@ -52,9 +49,6 @@ class RFPDupeFilter(BaseDupeFilter):\n             self.file.write(fp + os.linesep)\n \n     def request_fingerprint(self, request):\n-        \"\"\"Override this method to implement a different duplicate check.\n-\n-        \"\"\"\n         return request_fingerprint(request)\n \n     def close(self, reason):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#827c0cf51f47ab87b1de1c517ee7d5095fdb9c32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 10 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 1167 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class Selector(object_ref):\n \n     _default_type = None\n     _default_namespaces = {\n-        \"regexp\": \"http://exslt.org/regular-expressions\",\n+        \"re\": \"http://exslt.org/regular-expressions\",\n \n         # supported in libxslt:\n         # set:difference\n\n@@ -352,36 +352,36 @@ class ExsltTestCase(unittest.TestCase):\n         response = TextResponse(url=\"http://example.com\", body=body)\n         sel = self.sscls(response)\n \n-        # regexp:test()\n+        # re:test()\n         self.assertEqual(\n             sel.xpath(\n-                '//input[regexp:test(@name, \"[A-Z]+\", \"i\")]').extract(),\n-            [x.extract() for x in sel.xpath('//input[regexp:test(@name, \"[A-Z]+\", \"i\")]')])\n+                '//input[re:test(@name, \"[A-Z]+\", \"i\")]').extract(),\n+            [x.extract() for x in sel.xpath('//input[re:test(@name, \"[A-Z]+\", \"i\")]')])\n         self.assertEqual(\n             [x.extract()\n              for x in sel.xpath(\n-                 '//a[regexp:test(@href, \"\\.html$\")]/text()')],\n+                 '//a[re:test(@href, \"\\.html$\")]/text()')],\n             [u'first link', u'second link'])\n         self.assertEqual(\n             [x.extract()\n              for x in sel.xpath(\n-                 '//a[regexp:test(@href, \"first\")]/text()')],\n+                 '//a[re:test(@href, \"first\")]/text()')],\n             [u'first link'])\n         self.assertEqual(\n             [x.extract()\n              for x in sel.xpath(\n-                 '//a[regexp:test(@href, \"second\")]/text()')],\n+                 '//a[re:test(@href, \"second\")]/text()')],\n             [u'second link'])\n \n \n-        # regexp:match() is rather special: it returns a node-set of <match> nodes\n+        # re:match() is rather special: it returns a node-set of <match> nodes\n         #[u'<match>http://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.xml</match>',\n         #u'<match>http</match>',\n         #u'<match>www.bayes.co.uk</match>',\n         #u'<match></match>',\n         #u'<match>/xml/index.xml?/xml/utils/rechecker.xml</match>']\n         self.assertEqual(\n-            sel.xpath('regexp:match(//a[regexp:test(@href, \"\\.xml$\")]/@href,'\n+            sel.xpath('re:match(//a[re:test(@href, \"\\.xml$\")]/@href,'\n                       '\"(\\w+):\\/\\/([^/:]+)(:\\d*)?([^# ]*)\")/text()').extract(),\n             [u'http://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.xml',\n              u'http',\n@@ -391,9 +391,9 @@ class ExsltTestCase(unittest.TestCase):\n \n \n \n-        # regexp:replace()\n+        # re:replace()\n         self.assertEqual(\n-            sel.xpath('regexp:replace(//a[regexp:test(@href, \"\\.xml$\")]/@href,'\n+            sel.xpath('re:replace(//a[re:test(@href, \"\\.xml$\")]/@href,'\n                       '\"(\\w+)://(.+)(\\.xml)\", \"\",\"https://\\\\2.html\")').extract(),\n             [u'https://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.html'])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
