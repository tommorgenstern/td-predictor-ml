{"custom_id": "scrapy#d4c7cc848b83c2ec38ea90b76369885daba7375c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 432 | Files Changed: 65 | Hunks: 63 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 433 | Churn Cumulative: 12829 | Contributors (this commit): 57 | Commits (past 90d): 65 | Contributors (cumulative): 376 | DMM Complexity: None\n\nDIFF:\n@@ -13,19 +13,12 @@ collect_ignore = [\n     \"scrapy/conf.py\",\n     \"scrapy/stats.py\",\n     \"scrapy/project.py\",\n-    \"scrapy/utils/decorator.py\",\n-    \"scrapy/statscol.py\",\n-    \"scrapy/squeue.py\",\n     \"scrapy/log.py\",\n-    \"scrapy/dupefilter.py\",\n-    \"scrapy/command.py\",\n-    \"scrapy/linkextractor.py\",\n-    \"scrapy/spider.py\",\n \n     # not a test, but looks like a test\n     \"scrapy/utils/testsite.py\",\n \n-] + _py_files(\"scrapy/contrib\") + _py_files(\"scrapy/contrib_exp\")\n+]\n \n if (twisted_version.major, twisted_version.minor, twisted_version.micro) >= (15, 5, 0):\n     collect_ignore += _py_files(\"scrapy/xlib/tx\")\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.command` is deprecated, \"\n-              \"use `scrapy.commands` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.commands import *\n\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.closespider` is deprecated, \"\n-              \"use `scrapy.extensions.closespider` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.closespider import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.corestats` is deprecated, \"\n-              \"use `scrapy.extensions.corestats` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.corestats import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.debug` is deprecated, \"\n-              \"use `scrapy.extensions.debug` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.debug import *\n\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.ajaxcrawl` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.ajaxcrawl` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.ajaxcrawl import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.chunked` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.chunked` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.chunked import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.cookies` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.cookies` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.cookies import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.decompression` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.decompression` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.decompression import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.defaultheaders` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.defaultheaders` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.defaultheaders import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.downloadtimeout` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.downloadtimeout` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.downloadtimeout import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.httpauth` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.httpauth` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.httpauth import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.httpcache` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.httpcache` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.httpcache import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.httpcompression` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.httpcompression` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.httpcompression import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.httpproxy` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.httpproxy` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.httpproxy import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.redirect` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.redirect` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.redirect import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.retry` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.retry` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.retry import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.robotstxt` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.robotstxt` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.robotstxt import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.stats` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.stats` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.stats import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.downloadermiddleware.useragent` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.useragent` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.useragent import *\n\n@@ -1,8 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.exporter` is deprecated, \"\n-              \"use `scrapy.exporters` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.exporters import *\n-from scrapy.exporters import PythonItemExporter\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.feedexport` is deprecated, \"\n-              \"use `scrapy.extensions.feedexport` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.feedexport import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.httpcache` is deprecated, \"\n-              \"use `scrapy.extensions.httpcache` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.httpcache import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.linkextractors` is deprecated, \"\n-              \"use `scrapy.linkextractors` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.linkextractors import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.linkextractors.htmlparser` is deprecated, \"\n-              \"use `scrapy.linkextractors.htmlparser` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.linkextractors.htmlparser import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.linkextractors.lxmlhtml` is deprecated, \"\n-              \"use `scrapy.linkextractors.lxmlhtml` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.linkextractors.lxmlhtml import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.linkextractors.regex` is deprecated, \"\n-              \"use `scrapy.linkextractors.regex` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.linkextractors.regex import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.linkextractors.sgml` is deprecated, \"\n-              \"use `scrapy.linkextractors.sgml` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.linkextractors.sgml import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.loader` is deprecated, \"\n-              \"use `scrapy.loader` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.loader import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.loader.common` is deprecated, \"\n-              \"use `scrapy.loader.common` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.loader.common import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.loader.processor` is deprecated, \"\n-              \"use `scrapy.loader.processors` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.loader.processors import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.logstats` is deprecated, \"\n-              \"use `scrapy.extensions.logstats` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.logstats import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.memdebug` is deprecated, \"\n-              \"use `scrapy.extensions.memdebug` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.memdebug import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.memusage` is deprecated, \"\n-              \"use `scrapy.extensions.memusage` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.memusage import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.pipeline` is deprecated, \"\n-              \"use `scrapy.pipelines` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.pipelines import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.pipeline.files` is deprecated, \"\n-              \"use `scrapy.pipelines.files` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.pipelines.files import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.pipeline.images` is deprecated, \"\n-              \"use `scrapy.pipelines.images` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.pipelines.images import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.pipeline.media` is deprecated, \"\n-              \"use `scrapy.pipelines.media` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.pipelines.media import *\n\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spidermiddleware.depth` is deprecated, \"\n-              \"use `scrapy.spidermiddlewares.depth` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spidermiddlewares.depth import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spidermiddleware.httperror` is deprecated, \"\n-              \"use `scrapy.spidermiddlewares.httperror` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spidermiddlewares.httperror import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spidermiddleware.offsite` is deprecated, \"\n-              \"use `scrapy.spidermiddlewares.offsite` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spidermiddlewares.offsite import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spidermiddleware.referer` is deprecated, \"\n-              \"use `scrapy.spidermiddlewares.referer` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spidermiddlewares.referer import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spidermiddleware.urllength` is deprecated, \"\n-              \"use `scrapy.spidermiddlewares.urllength` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spidermiddlewares.urllength import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spiders` is deprecated, \"\n-              \"use `scrapy.spiders` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spiders import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spiders.crawl` is deprecated, \"\n-              \"use `scrapy.spiders.crawl` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spiders.crawl import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spiders.feed` is deprecated, \"\n-              \"use `scrapy.spiders.feed` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spiders.feed import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spiders.init` is deprecated, \"\n-              \"use `scrapy.spiders.init` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spiders.init import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spiders.sitemap` is deprecated, \"\n-              \"use `scrapy.spiders.sitemap` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spiders.sitemap import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.spiderstate` is deprecated, \"\n-              \"use `scrapy.extensions.spiderstate` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.spiderstate import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.statsmailer` is deprecated, \"\n-              \"use `scrapy.extensions.statsmailer` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.statsmailer import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib.throttle` is deprecated, \"\n-              \"use `scrapy.extensions.throttle` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.throttle import *\n\n\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib_exp.downloadermiddleware.decompression` is deprecated, \"\n-              \"use `scrapy.downloadermiddlewares.decompression` instead\",\n-    ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware\n\n@@ -1,6 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.contrib_exp.iterators` is deprecated, use `scrapy.utils.iterators` instead\",\n-    ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.utils.iterators import xmliter_lxml\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.dupefilter` is deprecated, \"\n-              \"use `scrapy.dupefilters` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.dupefilters import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.linkextractor` is deprecated, \"\n-              \"use `scrapy.linkextractors` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.linkextractors import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.spider` is deprecated, \"\n-              \"use `scrapy.spiders` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.spiders import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.squeue` is deprecated, \"\n-              \"use `scrapy.squeues` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.squeues import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.statscol` is deprecated, \"\n-              \"use `scrapy.statscollectors` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.statscollectors import *\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.utils.decorator` is deprecated, \"\n-              \"use `scrapy.utils.decorators` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.utils.decorators import *\n\n@@ -124,25 +124,7 @@ def _clspath(cls, forced=None):\n \n \n DEPRECATION_RULES = [\n-    ('scrapy.contrib_exp.downloadermiddleware.decompression.', 'scrapy.downloadermiddlewares.decompression.'),\n-    ('scrapy.contrib_exp.iterators.', 'scrapy.utils.iterators.'),\n-    ('scrapy.contrib.downloadermiddleware.', 'scrapy.downloadermiddlewares.'),\n-    ('scrapy.contrib.exporter.', 'scrapy.exporters.'),\n-    ('scrapy.contrib.linkextractors.', 'scrapy.linkextractors.'),\n-    ('scrapy.contrib.loader.processor.', 'scrapy.loader.processors.'),\n-    ('scrapy.contrib.loader.', 'scrapy.loader.'),\n-    ('scrapy.contrib.pipeline.', 'scrapy.pipelines.'),\n-    ('scrapy.contrib.spidermiddleware.', 'scrapy.spidermiddlewares.'),\n-    ('scrapy.contrib.spiders.', 'scrapy.spiders.'),\n-    ('scrapy.contrib.', 'scrapy.extensions.'),\n-    ('scrapy.command.', 'scrapy.commands.'),\n-    ('scrapy.dupefilter.', 'scrapy.dupefilters.'),\n-    ('scrapy.linkextractor.', 'scrapy.linkextractors.'),\n     ('scrapy.telnet.', 'scrapy.extensions.telnet.'),\n-    ('scrapy.spider.', 'scrapy.spiders.'),\n-    ('scrapy.squeue.', 'scrapy.squeues.'),\n-    ('scrapy.statscol.', 'scrapy.statscollectors.'),\n-    ('scrapy.utils.decorator.', 'scrapy.utils.decorators.'),\n     ('scrapy.spidermanager.SpiderManager', 'scrapy.spiderloader.SpiderLoader'),\n ]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36453348fad9babc96558ab10af9b2942eb5431e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 27 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 224 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -11,8 +11,6 @@ def _py_files(folder):\n collect_ignore = [\n     # deprecated or moved modules\n     \"scrapy/conf.py\",\n-    \"scrapy/stats.py\",\n-    \"scrapy/project.py\",\n     \"scrapy/log.py\",\n \n     # not a test, but looks like a test\n\n@@ -1,17 +0,0 @@\n-\n-\"\"\"\n-Obsolete module, kept for giving a meaningful error message when trying to\n-import.\n-\"\"\"\n-\n-raise ImportError(\"\"\"scrapy.project usage has become obsolete.\n-\n-If you want to get the Scrapy crawler from your extension, middleware or\n-pipeline implement the `from_crawler` class method (or look up for extending\n-components that have already done it, such as spiders).\n-\n-For example:\n-\n-    @classmethod\n-    def from_crawler(cls, crawler):\n-        return cls(crawler)\"\"\")\n\n@@ -1,8 +0,0 @@\n-\n-\"\"\"\n-Obsolete module, kept for giving a meaningful error message when trying to\n-import.\n-\"\"\"\n-\n-raise ImportError(\"scrapy.stats usage has become obsolete, use \"\n-                  \"`crawler.stats` attribute instead\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f531b66822491140740a6d86af2f3f11f0443d38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 706 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -16,7 +16,3 @@ class ISpiderLoader(Interface):\n     def find_by_request(request):\n         \"\"\"Return the list of spiders names that can handle the given request\"\"\"\n \n-\n-# ISpiderManager is deprecated, don't use it!\n-# An alias is kept for backwards compatibility.\n-ISpiderManager = ISpiderLoader\n\n@@ -1,7 +0,0 @@\n-\"\"\"\n-Backwards compatibility shim. Use scrapy.spiderloader instead.\n-\"\"\"\n-from scrapy.spiderloader import SpiderLoader\n-from scrapy.utils.deprecate import create_deprecated_class\n-\n-SpiderManager = create_deprecated_class('SpiderManager', SpiderLoader)\n\n@@ -125,7 +125,6 @@ def _clspath(cls, forced=None):\n \n DEPRECATION_RULES = [\n     ('scrapy.telnet.', 'scrapy.extensions.telnet.'),\n-    ('scrapy.spidermanager.SpiderManager', 'scrapy.spiderloader.SpiderLoader'),\n ]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#17e9914b8a12e5a96cc40016b74f301fb9835cbf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 97 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,7 @@ try:\n     from twisted.conch import manhole, telnet\n     from twisted.conch.insults import insults\n     TWISTED_CONCH_AVAILABLE = True\n-except ImportError:\n+except (ImportError, SyntaxError):\n     TWISTED_CONCH_AVAILABLE = False\n \n from scrapy.exceptions import NotConfigured\n@@ -40,7 +40,8 @@ class TelnetConsole(protocol.ServerFactory):\n         if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n             raise NotConfigured\n         if not TWISTED_CONCH_AVAILABLE:\n-            raise NotConfigured\n+            raise NotConfigured('TelnetConsole not enabled: failed to import '\n+                                'required twisted modules.')\n         self.crawler = crawler\n         self.noisy = False\n         self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4f39057cbbfa4daf66f82061e57101b88d88d05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 332 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -98,8 +98,9 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n     \"\"\"\n \n     encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'\n-    def _getrow(csv_r):\n-        return [to_unicode(field, encoding) for field in next(csv_r)]\n+\n+    def row_to_unicode(row_):\n+        return [to_unicode(field, encoding) for field in row_]\n \n     # Python 3 csv reader input object needs to return strings\n     if six.PY3:\n@@ -113,10 +114,14 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n     csv_r = csv.reader(lines, **kwargs)\n \n     if not headers:\n-        headers = _getrow(csv_r)\n+        try:\n+            row = next(csv_r)\n+        except StopIteration:\n+            return\n+        headers = row_to_unicode(row)\n \n-    while True:\n-        row = _getrow(csv_r)\n+    for row in csv_r:\n+        row = row_to_unicode(row)\n         if len(row) != len(headers):\n             logger.warning(\"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n                            \"should be: %(csvheader)d)\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92b504eae5379dadade2d78efba4e54b201cbd93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 587 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,4 @@\n import logging\n-import os\n import tempfile\n import warnings\n import unittest\n@@ -14,8 +13,9 @@ from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n-from scrapy.utils.test import get_crawler\n from scrapy.extensions.throttle import AutoThrottle\n+from scrapy.extensions import telnet\n+\n \n class BaseCrawlerTest(unittest.TestCase):\n \n@@ -100,6 +100,8 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n                 custom_settings = {\n                     'LOG_LEVEL': 'INFO',\n                     'LOG_FILE': log_file.name,\n+                    # disable telnet if not available to avoid an extra warning\n+                    'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,\n                 }\n \n             configure_logging()\n\n@@ -10,6 +10,7 @@ from twisted.python.failure import Failure\n from scrapy.utils.log import (failure_to_exc_info, TopLevelFormatter,\n                               LogCounterHandler, StreamLogger)\n from scrapy.utils.test import get_crawler\n+from scrapy.extensions import telnet\n \n \n class FailureToExcInfoTest(unittest.TestCase):\n@@ -65,10 +66,14 @@ class TopLevelFormatterTest(unittest.TestCase):\n class LogCounterHandlerTest(unittest.TestCase):\n \n     def setUp(self):\n+        settings = {'LOG_LEVEL': 'WARNING'}\n+        if not telnet.TWISTED_CONCH_AVAILABLE:\n+            # disable it to avoid the extra warning\n+            settings['TELNETCONSOLE_ENABLED'] = False\n         self.logger = logging.getLogger('test')\n         self.logger.setLevel(logging.NOTSET)\n         self.logger.propagate = False\n-        self.crawler = get_crawler(settings_dict={'LOG_LEVEL': 'WARNING'})\n+        self.crawler = get_crawler(settings_dict=settings)\n         self.handler = LogCounterHandler(self.crawler)\n         self.logger.addHandler(self.handler)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f6778aa7332aecd15f7672a0852d1f49596969b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 107 | Files Changed: 2 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): -16/0 | Churn Δ: 108 | Churn Cumulative: 1279 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,7 +6,6 @@ from collections import MutableMapping\n from importlib import import_module\n from pprint import pformat\n \n-from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n from . import default_settings\n@@ -405,30 +404,6 @@ class BaseSettings(MutableMapping):\n         else:\n             p.text(pformat(self.copy_to_dict()))\n \n-    @property\n-    def overrides(self):\n-        warnings.warn(\"`Settings.overrides` attribute is deprecated and won't \"\n-                      \"be supported in Scrapy 0.26, use \"\n-                      \"`Settings.set(name, value, priority='cmdline')` instead\",\n-                      category=ScrapyDeprecationWarning, stacklevel=2)\n-        try:\n-            o = self._overrides\n-        except AttributeError:\n-            self._overrides = o = _DictProxy(self, 'cmdline')\n-        return o\n-\n-    @property\n-    def defaults(self):\n-        warnings.warn(\"`Settings.defaults` attribute is deprecated and won't \"\n-                      \"be supported in Scrapy 0.26, use \"\n-                      \"`Settings.set(name, value, priority='default')` instead\",\n-                      category=ScrapyDeprecationWarning, stacklevel=2)\n-        try:\n-            o = self._defaults\n-        except AttributeError:\n-            self._defaults = o = _DictProxy(self, 'default')\n-        return o\n-\n \n class _DictProxy(MutableMapping):\n \n@@ -479,29 +454,6 @@ class Settings(BaseSettings):\n         self.update(values, priority)\n \n \n-class CrawlerSettings(Settings):\n-\n-    def __init__(self, settings_module=None, **kw):\n-        self.settings_module = settings_module\n-        Settings.__init__(self, **kw)\n-\n-    def __getitem__(self, opt_name):\n-        if opt_name in self.overrides:\n-            return self.overrides[opt_name]\n-        if self.settings_module and hasattr(self.settings_module, opt_name):\n-            return getattr(self.settings_module, opt_name)\n-        if opt_name in self.defaults:\n-            return self.defaults[opt_name]\n-        return Settings.__getitem__(self, opt_name)\n-\n-    def __str__(self):\n-        return \"<CrawlerSettings module=%r>\" % self.settings_module\n-\n-CrawlerSettings = create_deprecated_class(\n-    'CrawlerSettings', CrawlerSettings,\n-    new_class_path='scrapy.settings.Settings')\n-\n-\n def iter_default_settings():\n     \"\"\"Return the default settings as an iterator of (name, value) tuples\"\"\"\n     for name in dir(default_settings):\n\n@@ -3,8 +3,7 @@ import unittest\n import warnings\n \n from scrapy.settings import (BaseSettings, Settings, SettingsAttribute,\n-                             CrawlerSettings, SETTINGS_PRIORITIES,\n-                             get_settings_priority)\n+                             SETTINGS_PRIORITIES, get_settings_priority)\n from tests import mock\n from . import default_settings\n \n@@ -341,35 +340,6 @@ class BaseSettingsTest(unittest.TestCase):\n         self.assertTrue(frozencopy.frozen)\n         self.assertIsNot(frozencopy, self.settings)\n \n-    def test_deprecated_attribute_overrides(self):\n-        self.settings.set('BAR', 'fuz', priority='cmdline')\n-        with warnings.catch_warnings(record=True) as w:\n-            self.settings.overrides['BAR'] = 'foo'\n-            self.assertIn(\"Settings.overrides\", str(w[0].message))\n-            self.assertEqual(self.settings.get('BAR'), 'foo')\n-            self.assertEqual(self.settings.overrides.get('BAR'), 'foo')\n-            self.assertIn('BAR', self.settings.overrides)\n-\n-            self.settings.overrides.update(BAR='bus')\n-            self.assertEqual(self.settings.get('BAR'), 'bus')\n-            self.assertEqual(self.settings.overrides.get('BAR'), 'bus')\n-\n-            self.settings.overrides.setdefault('BAR', 'fez')\n-            self.assertEqual(self.settings.get('BAR'), 'bus')\n-\n-            self.settings.overrides.setdefault('FOO', 'fez')\n-            self.assertEqual(self.settings.get('FOO'), 'fez')\n-            self.assertEqual(self.settings.overrides.get('FOO'), 'fez')\n-\n-    def test_deprecated_attribute_defaults(self):\n-        self.settings.set('BAR', 'fuz', priority='default')\n-        with warnings.catch_warnings(record=True) as w:\n-            self.settings.defaults['BAR'] = 'foo'\n-            self.assertIn(\"Settings.defaults\", str(w[0].message))\n-            self.assertEqual(self.settings.get('BAR'), 'foo')\n-            self.assertEqual(self.settings.defaults.get('BAR'), 'foo')\n-            self.assertIn('BAR', self.settings.defaults)\n-\n \n class SettingsTest(unittest.TestCase):\n \n@@ -422,33 +392,5 @@ class SettingsTest(unittest.TestCase):\n         self.assertEqual(mydict['key'], 'val')\n \n \n-class CrawlerSettingsTest(unittest.TestCase):\n-\n-    def test_deprecated_crawlersettings(self):\n-        def _get_settings(settings_dict=None):\n-            settings_module = type('SettingsModuleMock', (object,), settings_dict or {})\n-            return CrawlerSettings(settings_module)\n-\n-        with warnings.catch_warnings(record=True) as w:\n-            settings = _get_settings()\n-            self.assertIn(\"CrawlerSettings is deprecated\", str(w[0].message))\n-\n-            # test_global_defaults\n-            self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 180)\n-\n-            # test_defaults\n-            settings.defaults['DOWNLOAD_TIMEOUT'] = '99'\n-            self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 99)\n-\n-            # test_settings_module\n-            settings = _get_settings({'DOWNLOAD_TIMEOUT': '3'})\n-            self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 3)\n-\n-            # test_overrides\n-            settings = _get_settings({'DOWNLOAD_TIMEOUT': '3'})\n-            settings.overrides['DOWNLOAD_TIMEOUT'] = '15'\n-            self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 15)\n-\n-\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9428a4a3aa4c679c15eb2c606de7b49ad832ee6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 104 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in docs/topics/telnetconsole.rst\n \n import pprint\n import logging\n+import traceback\n \n from twisted.internet import protocol\n try:\n@@ -13,6 +14,7 @@ try:\n     from twisted.conch.insults import insults\n     TWISTED_CONCH_AVAILABLE = True\n except (ImportError, SyntaxError):\n+    _TWISTED_CONCH_TRACEBACK = traceback.format_exc()\n     TWISTED_CONCH_AVAILABLE = False\n \n from scrapy.exceptions import NotConfigured\n@@ -40,8 +42,9 @@ class TelnetConsole(protocol.ServerFactory):\n         if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n             raise NotConfigured\n         if not TWISTED_CONCH_AVAILABLE:\n-            raise NotConfigured('TelnetConsole not enabled: failed to import '\n-                                'required twisted modules.')\n+            raise NotConfigured(\n+                'TELNETCONSOLE_ENABLED setting is True but required twisted '\n+                'modules failed to import:\\n' + _TWISTED_CONCH_TRACEBACK)\n         self.crawler = crawler\n         self.noisy = False\n         self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7e18db179f2e45aa38a5bbdb0abba7d983cdce7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 16 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 57 | Churn Cumulative: 923 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,10 +8,7 @@ from twisted.internet import defer\n from scrapy import Spider, Request\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n-\n-\n-# TEST_URL = 'http://example.org'\n-TEST_URL = 'http://localhost:8998'\n+from tests.spiders import MockServerSpider\n \n \n class LogExceptionMiddleware:\n@@ -21,16 +18,18 @@ class LogExceptionMiddleware:\n \n \n # ================================================================================\n-# recover from an exception on a spider callback\n+# (0) recover from an exception on a spider callback\n class RecoverySpider(Spider):\n     name = 'RecoverySpider'\n-    start_urls = [TEST_URL]\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n             __name__ + '.RecoveryMiddleware': 10,\n         },\n     }\n \n+    def start_requests(self):\n+        yield Request(self.mockserver.url('/status?n=200'))\n+\n     def parse(self, response):\n         yield {'test': 1}\n         self.logger.warn('DONT_FAIL: %s', response.meta.get('dont_fail'))\n@@ -61,10 +60,11 @@ class ProcessSpiderInputSpider(Spider):\n     }\n \n     def start_requests(self):\n-        yield Request(TEST_URL, callback=self.parse, errback=self.errback)\n+        yield Request(url=self.mockserver.url('/status?n=200'),\n+                      callback=self.parse, errback=self.errback)\n \n     def parse(self, response):\n-        return [{'test': 1}, {'test': 2}]\n+        return {'from': 'callback'}\n \n     def errback(self, failure):\n         self.logger.warn('Got a Failure on the Request errback')\n@@ -80,13 +80,15 @@ class FailProcessSpiderInputMiddleware:\n # (2) exceptions from a spider callback (generator)\n class GeneratorCallbackSpider(Spider):\n     name = 'GeneratorCallbackSpider'\n-    start_urls = [TEST_URL]\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n             __name__ + '.LogExceptionMiddleware': 10,\n         },\n     }\n \n+    def start_requests(self):\n+        yield Request(self.mockserver.url('/status?n=200'))\n+\n     def parse(self, response):\n         yield {'test': 1}\n         yield {'test': 2}\n@@ -97,13 +99,15 @@ class GeneratorCallbackSpider(Spider):\n # (3) exceptions from a spider callback (not a generator)\n class NotAGeneratorCallbackSpider(Spider):\n     name = 'NotAGeneratorCallbackSpider'\n-    start_urls = [TEST_URL]\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n             __name__ + '.LogExceptionMiddleware': 10,\n         },\n     }\n \n+    def start_requests(self):\n+        yield Request(self.mockserver.url('/status?n=200'))\n+\n     def parse(self, response):\n         return [{'test': 1}, {'test': 1/0}]\n \n@@ -123,20 +127,20 @@ class TestSpiderMiddleware(TestCase):\n     def crawl_log(self, spider):\n         crawler = get_crawler(spider)\n         with LogCapture() as log:\n-            yield crawler.crawl()\n+            yield crawler.crawl(mockserver=self.mockserver)\n         raise defer.returnValue(log)\n \n-    # @defer.inlineCallbacks\n-    # def test_recovery(self):\n-    #     \"\"\"\n-    #     Recover from an exception from a spider's callback. The final item count should be 3\n-    #     (one from the spider before raising the exception, one from the middleware and one\n-    #     from the spider when processing the response that was enqueued from the middleware)\n-    #     \"\"\"\n-    #     log = yield self.crawl_log(RecoverySpider)\n-    #     self.assertIn(\"Middleware: ModuleNotFoundError exception caught\", str(log))\n-    #     self.assertEqual(str(log).count(\"Middleware: ModuleNotFoundError exception caught\"), 1)\n-    #     self.assertIn(\"'item_scraped_count': 3\", str(log))\n+    @defer.inlineCallbacks\n+    def test_recovery(self):\n+        \"\"\"\n+        (0) Recover from an exception in a spider callback. The final item count should be 2\n+        (one directly from the recovery middleware and one from the spider when processing\n+        the request that was enqueued from the recovery middleware)\n+        \"\"\"\n+        log = yield self.crawl_log(RecoverySpider)\n+        self.assertIn(\"Middleware: ModuleNotFoundError exception caught\", str(log))\n+        self.assertEqual(str(log).count(\"Middleware: ModuleNotFoundError exception caught\"), 1)\n+        self.assertIn(\"'item_scraped_count': 2\", str(log))\n \n     @defer.inlineCallbacks\n     def test_process_spider_input_errback(self):\n@@ -148,21 +152,24 @@ class TestSpiderMiddleware(TestCase):\n         self.assertNotIn(\"Middleware: IndexError exception caught\", str(log1))\n         self.assertIn(\"Middleware: will raise IndexError\", str(log1))\n         self.assertIn(\"Got a Failure on the Request errback\", str(log1))\n+        self.assertIn(\"{'from': 'errback'}\", str(log1))\n+        self.assertNotIn(\"{'from': 'callback'}\", str(log1))\n+        self.assertIn(\"'item_scraped_count': 1\", str(log1))\n     \n     @defer.inlineCallbacks\n     def test_generator_callback(self):\n         \"\"\"\n-        (2) An exception from a spider's callback should\n+        (2) An exception from a spider callback (returning a generator) should\n         be caught by the process_spider_exception chain\n         \"\"\"\n         log2 = yield self.crawl_log(GeneratorCallbackSpider)\n         self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n-        self.assertIn(\"'item_scraped_count': 2\", str(log2))\n+        self.assertNotIn(\"item_scraped_count\", str(log2))\n     \n     @defer.inlineCallbacks\n     def test_not_a_generator_callback(self):\n         \"\"\"\n-        (3) An exception from a spider's callback should\n+        (3) An exception from a spider callback (returning a list) should\n         be caught by the process_spider_exception chain\n         \"\"\"\n         log3 = yield self.crawl_log(NotAGeneratorCallbackSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c579b5276f502832f375be316094b4244cf87c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 37 | Churn Cumulative: 268 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,8 @@ Spider Middleware manager\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+from itertools import chain\n+\n import six\n from twisted.python.failure import Failure\n from scrapy.exceptions import _InvalidOutput\n@@ -10,9 +12,28 @@ from scrapy.middleware import MiddlewareManager\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.conf import build_component_list\n \n+\n def _isiterable(possible_iterator):\n     return hasattr(possible_iterator, '__iter__')\n \n+\n+class MutableChain:\n+    def __init__(self, *args):\n+        self.data = chain(*args)\n+\n+    def extend(self, iterable):\n+        self.data = chain(self.data, iterable)\n+\n+    def __iter__(self):\n+        return self.data.__iter__()\n+\n+    def __next__(self):  # py3\n+        return self.data.__next__()\n+\n+    def next(self):  # py2\n+        return self.data.next()\n+\n+\n class SpiderMiddlewareManager(MiddlewareManager):\n \n     component_name = 'spider middleware'\n@@ -68,28 +89,32 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             return _failure\n \n         def process_spider_output(result, index):\n-            def wrapper(result_iterable):\n+            # items in this iterable do not need to go through the process_spider_output\n+            # chain, they went through it already from the process_spider_exception method\n+            recovered = MutableChain()\n+\n+            def evaluate_result(result_iterable, index):\n                 try:\n                     for r in result_iterable:\n                         yield r\n                 except Exception as ex:\n-                    # process the exception with the method from the next middleware\n                     exception_result = process_spider_exception(Failure(ex), index)\n                     if exception_result is None or isinstance(exception_result, Failure):\n                         raise\n-                    for output in exception_result:\n-                        yield output\n+                    recovered.extend(exception_result)\n+\n             for i, method in enumerate(self.methods['process_spider_output']):\n                 if i < index or method is None:\n                     continue\n                 result = method(response=response, result=result, spider=spider)\n                 index += 1\n                 if _isiterable(result):\n-                    result = wrapper(result)\n+                    result = evaluate_result(result, index)\n                 else:\n                     raise _InvalidOutput('Middleware {} must return an iterable, got {}' \\\n                                          .format(fname(method), type(result)))\n-            return result\n+\n+            return chain(result, recovered)\n \n         dfd = mustbe_deferred(process_spider_input, response)\n         dfd.addErrback(process_spider_exception, index=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c5fa0ae6bc536d6bc5370c6f3b634c33848971e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 308 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,8 @@ Spider Middleware manager\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+from itertools import chain\n+\n import six\n from twisted.python.failure import Failure\n from scrapy.exceptions import _InvalidOutput\n@@ -10,9 +12,31 @@ from scrapy.middleware import MiddlewareManager\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.conf import build_component_list\n \n+\n def _isiterable(possible_iterator):\n     return hasattr(possible_iterator, '__iter__')\n \n+\n+class MutableChain:\n+    \"\"\"\n+    Thin wrapper around itertools.chain, allowing to add iterables \"in-place\"\n+    \"\"\"\n+    def __init__(self, *args):\n+        self.data = chain(*args)\n+\n+    def extend(self, *iterables):\n+        self.data = chain(self.data, *iterables)\n+\n+    def __iter__(self):\n+        return self.data.__iter__()\n+\n+    def __next__(self):  # py3\n+        return self.data.__next__()\n+\n+    def next(self):  # py2\n+        return self.data.next()\n+\n+\n class SpiderMiddlewareManager(MiddlewareManager):\n \n     component_name = 'spider middleware'\n@@ -68,28 +92,32 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             return _failure\n \n         def process_spider_output(result, index):\n-            def wrapper(result_iterable):\n+            # items in this iterable do not need to go through the process_spider_output\n+            # chain, they went through it already from the process_spider_exception method\n+            recovered = MutableChain()\n+\n+            def evaluate_result(result_iterable, index):\n                 try:\n                     for r in result_iterable:\n                         yield r\n                 except Exception as ex:\n-                    # process the exception with the method from the next middleware\n                     exception_result = process_spider_exception(Failure(ex), index)\n                     if exception_result is None or isinstance(exception_result, Failure):\n                         raise\n-                    for output in exception_result:\n-                        yield output\n+                    recovered.extend(exception_result)\n+\n             for i, method in enumerate(self.methods['process_spider_output']):\n                 if i < index or method is None:\n                     continue\n                 result = method(response=response, result=result, spider=spider)\n                 index += 1\n                 if _isiterable(result):\n-                    result = wrapper(result)\n+                    result = evaluate_result(result, index)\n                 else:\n                     raise _InvalidOutput('Middleware {} must return an iterable, got {}' \\\n                                          .format(fname(method), type(result)))\n-            return result\n+\n+            return chain(result, recovered)\n \n         dfd = mustbe_deferred(process_spider_input, response)\n         dfd.addErrback(process_spider_exception, index=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
