{"custom_id": "keras#c11954843d60d5fc5356783871cfcf2f409e9fbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 497 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -56,7 +56,7 @@ from keras import layers\n from tensorflow import clip_by_value\n from tensorflow import data as tf_data\n from tensorflow import image as tf_image\n-from tensorflow.random import gamma as tf_random_gamma\n+from tensorflow import random as tf_random\n \n keras.utils.set_random_seed(42)\n \n@@ -154,8 +154,8 @@ and pad this image in the final padded image at the same location.\n \n \n def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n-    gamma_1_sample = tf_random_gamma(shape=[size], alpha=concentration_1)\n-    gamma_2_sample = tf_random_gamma(shape=[size], alpha=concentration_0)\n+    gamma_1_sample = tf_random.gamma(shape=[size], alpha=concentration_1)\n+    gamma_2_sample = tf_random.gamma(shape=[size], alpha=concentration_0)\n     return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n \n \n@@ -168,9 +168,9 @@ def get_box(lambda_value):\n     cut_h = IMG_SIZE * cut_rat  # rh\n     cut_h = keras.ops.cast(cut_h, \"int32\")\n \n-    cut_x = keras.random.random.uniform((1,), minval=0, maxval=IMG_SIZE)  # rx\n+    cut_x = keras.random.uniform((1,), minval=0, maxval=IMG_SIZE)  # rx\n     cut_x = keras.ops.cast(cut_x, \"int32\")\n-    cut_y = keras.random.random.uniform((1,), minval=0, maxval=IMG_SIZE)  # ry\n+    cut_y = keras.random.uniform((1,), minval=0, maxval=IMG_SIZE)  # ry\n     cut_y = keras.ops.cast(cut_y, \"int32\")\n \n     boundaryx1 = clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#233761df6184e87e1ec5eb512e5c87534a33d141", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 38 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -196,6 +196,10 @@ def gamma(shape, alpha, dtype=None, seed=None):\n     dtype = to_torch_dtype(dtype)\n     alpha = torch.ones(shape) * torch.tensor(alpha)\n     beta = torch.ones(shape)\n-    # TODO: seed / generator not supported\n+    prev_rng_state = torch.random.get_rng_state()\n+    first_seed, second_seed = draw_seed(seed)\n+    torch.manual_seed(first_seed + second_seed)\n     gamma_distribution = torch.distributions.gamma.Gamma(alpha, beta)\n-    return gamma_distribution.sample().type(dtype)\n+    sample = gamma_distribution.sample().type(dtype)\n+    torch.random.set_rng_state(prev_rng_state)\n+    return sample\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa7bb6749161b24d7c21d46c3dcfd7bd1cc83b25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/4 | Churn Δ: 58 | Churn Cumulative: 225 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 0.058823529411764705\n\nDIFF:\n@@ -1205,15 +1205,15 @@ class Layer(BackendLayer, Operation):\n \n         # Otherwise, attempt to build the layer by calling it on symbolic input.\n         if might_have_unbuilt_state(self):\n-            if len(shapes_dict) == 1:\n-                success = self._build_by_run_for_single_pos_arg(first_shape)\n-            else:\n-                success = self._build_by_run_for_kwargs(shapes_dict)\n-            if not success:\n+            try:\n+                backend.compute_output_spec(\n+                    self.call, **call_spec.arguments_dict\n+                )\n+            except Exception as e:\n                 if call_spec.eager:\n                     # Will let the actual eager call do state-building\n                     return\n-                raise ValueError(\n+                warnings.warn(\n                     f\"Layer '{self.name}' looks like it has unbuilt state, but \"\n                     \"Keras is not able to trace the layer `call()` in order to \"\n                     \"build it automatically. Possible causes:\\n\"\n@@ -1225,22 +1225,11 @@ class Layer(BackendLayer, Operation):\n                     \"to implement the `def build(self, input_shape)` method on \"\n                     \"your layer. It should create all variables used by the \"\n                     \"layer (e.g. by calling `layer.build()` on all its \"\n-                    \"children layers).\"\n+                    \"children layers).\\n\"\n+                    f\"Exception encoutered: ''{e}''\"\n                 )\n-\n         self.build(first_shape)\n \n-    def _build_by_run(self, *args, **kwargs):\n-        call_spec = CallSpec(self._call_signature, args, kwargs)\n-        shapes_dict = get_shapes_dict(call_spec)\n-        if len(shapes_dict) == 1:\n-            success = self._build_by_run_for_single_pos_arg(\n-                tuple(shapes_dict.values())[0]\n-            )\n-        else:\n-            success = self._build_by_run_for_kwargs(shapes_dict)\n-        return success\n-\n     def _build_by_run_for_single_pos_arg(self, input_shape):\n         # Case: all inputs are in the first arg (possibly nested).\n         input_tensors = map_shape_structure(\n@@ -1249,7 +1238,8 @@ class Layer(BackendLayer, Operation):\n         try:\n             backend.compute_output_spec(self.call, input_tensors)\n             return True\n-        except:\n+        except Exception as e:\n+            raise e\n             return False\n \n     def _build_by_run_for_kwargs(self, shapes_dict):\n\n@@ -1,4 +1,5 @@\n import copy\n+import inspect\n \n import tree\n \n@@ -178,6 +179,21 @@ class Sequential(Model):\n                 # Can happen if shape inference is not implemented.\n                 # TODO: consider reverting inbound nodes on layers processed.\n                 return\n+            except TypeError as e:\n+                signature = inspect.signature(layer.call)\n+                positional_args = [\n+                    param\n+                    for param in signature.parameters.values()\n+                    if param.default == inspect.Parameter.empty\n+                ]\n+                if len(positional_args) != 1:\n+                    raise ValueError(\n+                        \"Layers added to a Sequential model \"\n+                        \"can only have a single positional argument, \"\n+                        f\"the input tensor. Layer {layer.__class__.__name__} \"\n+                        f\"has multiple positional arguments: {positional_args}\"\n+                    )\n+                raise e\n         outputs = x\n         self._functional = Functional(inputs=inputs, outputs=outputs)\n         self.built = True\n\n@@ -250,3 +250,15 @@ class SequentialTest(testing.TestCase):\n         model = Sequential(name=\"seq\")\n         with self.assertRaisesRegex(ValueError, \"Only instances of\"):\n             model.add({})\n+\n+        model = Sequential(name=\"seq\")\n+\n+        class BadLayer(layers.Layer):\n+            def call(self, inputs, training):\n+                return inputs\n+\n+        model.add(BadLayer())\n+        with self.assertRaisesRegex(\n+            ValueError, \"can only have a single positional\"\n+        ):\n+            model.build((None, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#315d0e61a32140c45675aa02bb052e201e1fb316", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 538 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 28 | Complexity Δ (Sum/Max): 46/46 | Churn Δ: 538 | Churn Cumulative: 538 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.9405594405594405\n\nDIFF:\n@@ -0,0 +1,538 @@\n+\"\"\"\n+Title: Automatic Speech Recognition with Transformer\n+Author: [Apoorv Nandan](https://twitter.com/NandanApoorv)\n+Date created: 2021/01/13\n+Last modified: 2021/01/13\n+Description: Training a sequence-to-sequence Transformer for automatic speech recognition.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+Automatic speech recognition (ASR) consists of transcribing audio speech segments into text.\n+ASR can be treated as a sequence-to-sequence problem, where the\n+audio can be represented as a sequence of feature vectors\n+and the text as a sequence of characters, words, or subword tokens.\n+\n+For this demonstration, we will use the LJSpeech dataset from the\n+[LibriVox](https://librivox.org/) project. It consists of short\n+audio clips of a single speaker reading passages from 7 non-fiction books.\n+Our model will be similar to the original Transformer (both encoder and decoder)\n+as proposed in the paper, \"Attention is All You Need\".\n+\n+\n+**References:**\n+\n+- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n+- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/pdf/1904.13377.pdf)\n+- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n+- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)\n+\"\"\"\n+\n+\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+from glob import glob\n+import tensorflow as tf\n+import keras\n+from keras import layers\n+\n+\n+\"\"\"\n+## Define the Transformer Input Layer\n+\n+When processing past target tokens for the decoder, we compute the sum of\n+position embeddings and token embeddings.\n+\n+When processing audio features, we apply convolutional layers to downsample\n+them (via convolution strides) and process local relationships.\n+\"\"\"\n+\n+\n+class TokenEmbedding(layers.Layer):\n+    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n+        super().__init__()\n+        self.emb = keras.layers.Embedding(num_vocab, num_hid)\n+        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n+\n+    def call(self, x):\n+        maxlen = tf.shape(x)[-1]\n+        x = self.emb(x)\n+        positions = tf.range(start=0, limit=maxlen, delta=1)\n+        positions = self.pos_emb(positions)\n+        return x + positions\n+\n+\n+class SpeechFeatureEmbedding(layers.Layer):\n+    def __init__(self, num_hid=64, maxlen=100):\n+        super().__init__()\n+        self.conv1 = keras.layers.Conv1D(\n+            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n+        )\n+        self.conv2 = keras.layers.Conv1D(\n+            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n+        )\n+        self.conv3 = keras.layers.Conv1D(\n+            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n+        )\n+\n+    def call(self, x):\n+        x = self.conv1(x)\n+        x = self.conv2(x)\n+        return self.conv3(x)\n+\n+\n+\"\"\"\n+## Transformer Encoder Layer\n+\"\"\"\n+\n+\n+class TransformerEncoder(layers.Layer):\n+    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n+        super().__init__()\n+        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n+        self.ffn = keras.Sequential(\n+            [\n+                layers.Dense(feed_forward_dim, activation=\"relu\"),\n+                layers.Dense(embed_dim),\n+            ]\n+        )\n+        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n+        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n+        self.dropout1 = layers.Dropout(rate)\n+        self.dropout2 = layers.Dropout(rate)\n+\n+    def call(self, inputs, training=False):\n+        attn_output = self.att(inputs, inputs)\n+        attn_output = self.dropout1(attn_output, training=training)\n+        out1 = self.layernorm1(inputs + attn_output)\n+        ffn_output = self.ffn(out1)\n+        ffn_output = self.dropout2(ffn_output, training=training)\n+        return self.layernorm2(out1 + ffn_output)\n+\n+\n+\"\"\"\n+## Transformer Decoder Layer\n+\"\"\"\n+\n+\n+class TransformerDecoder(layers.Layer):\n+    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n+        super().__init__()\n+        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n+        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n+        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n+        self.self_att = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=embed_dim\n+        )\n+        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n+        self.self_dropout = layers.Dropout(0.5)\n+        self.enc_dropout = layers.Dropout(0.1)\n+        self.ffn_dropout = layers.Dropout(0.1)\n+        self.ffn = keras.Sequential(\n+            [\n+                layers.Dense(feed_forward_dim, activation=\"relu\"),\n+                layers.Dense(embed_dim),\n+            ]\n+        )\n+\n+    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n+        \"\"\"Masks the upper half of the dot product matrix in self attention.\n+\n+        This prevents flow of information from future tokens to current token.\n+        1's in the lower triangle, counting from the lower right corner.\n+        \"\"\"\n+        i = tf.range(n_dest)[:, None]\n+        j = tf.range(n_src)\n+        m = i >= j - n_src + n_dest\n+        mask = tf.cast(m, dtype)\n+        mask = tf.reshape(mask, [1, n_dest, n_src])\n+        mult = tf.concat(\n+            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n+        )\n+        return tf.tile(mask, mult)\n+\n+    def call(self, enc_out, target):\n+        input_shape = tf.shape(target)\n+        batch_size = input_shape[0]\n+        seq_len = input_shape[1]\n+        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n+        target_att = self.self_att(target, target, attention_mask=causal_mask)\n+        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n+        enc_out = self.enc_att(target_norm, enc_out)\n+        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n+        ffn_out = self.ffn(enc_out_norm)\n+        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n+        return ffn_out_norm\n+\n+\n+\"\"\"\n+## Complete the Transformer model\n+\n+Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n+During training, we give the decoder the target character sequence shifted to the left\n+as input. During inference, the decoder uses its own past predictions to predict the\n+next token.\n+\"\"\"\n+\n+\n+class Transformer(keras.Model):\n+    def __init__(\n+        self,\n+        num_hid=64,\n+        num_head=2,\n+        num_feed_forward=128,\n+        source_maxlen=100,\n+        target_maxlen=100,\n+        num_layers_enc=4,\n+        num_layers_dec=1,\n+        num_classes=10,\n+    ):\n+        super().__init__()\n+        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n+        self.num_layers_enc = num_layers_enc\n+        self.num_layers_dec = num_layers_dec\n+        self.target_maxlen = target_maxlen\n+        self.num_classes = num_classes\n+\n+        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n+        self.dec_input = TokenEmbedding(\n+            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n+        )\n+\n+        self.encoder = keras.Sequential(\n+            [self.enc_input]\n+            + [\n+                TransformerEncoder(num_hid, num_head, num_feed_forward)\n+                for _ in range(num_layers_enc)\n+            ]\n+        )\n+\n+        for i in range(num_layers_dec):\n+            setattr(\n+                self,\n+                f\"dec_layer_{i}\",\n+                TransformerDecoder(num_hid, num_head, num_feed_forward),\n+            )\n+\n+        self.classifier = layers.Dense(num_classes)\n+\n+    def decode(self, enc_out, target):\n+        y = self.dec_input(target)\n+        for i in range(self.num_layers_dec):\n+            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n+        return y\n+\n+    def call(self, inputs):\n+        source = inputs[0]\n+        target = inputs[1]\n+        x = self.encoder(source)\n+        y = self.decode(x, target)\n+        return self.classifier(y)\n+\n+    @property\n+    def metrics(self):\n+        return [self.loss_metric]\n+\n+    def train_step(self, batch):\n+        \"\"\"Processes one batch inside model.fit().\"\"\"\n+        source = batch[\"source\"]\n+        target = batch[\"target\"]\n+        dec_input = target[:, :-1]\n+        dec_target = target[:, 1:]\n+        with tf.GradientTape() as tape:\n+            preds = self([source, dec_input])\n+            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n+            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n+            loss = model.compute_loss(None, one_hot, preds, sample_weight=mask)\n+        trainable_vars = self.trainable_variables\n+        gradients = tape.gradient(loss, trainable_vars)\n+        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n+        self.loss_metric.update_state(loss)\n+        return {\"loss\": self.loss_metric.result()}\n+\n+    def test_step(self, batch):\n+        source = batch[\"source\"]\n+        target = batch[\"target\"]\n+        dec_input = target[:, :-1]\n+        dec_target = target[:, 1:]\n+        preds = self([source, dec_input])\n+        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n+        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n+        loss = model.compute_loss(None, one_hot, preds, sample_weight=mask)\n+        self.loss_metric.update_state(loss)\n+        return {\"loss\": self.loss_metric.result()}\n+\n+    def generate(self, source, target_start_token_idx):\n+        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n+        bs = tf.shape(source)[0]\n+        enc = self.encoder(source)\n+        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n+        dec_logits = []\n+        for i in range(self.target_maxlen - 1):\n+            dec_out = self.decode(enc, dec_input)\n+            logits = self.classifier(dec_out)\n+            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n+            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n+            dec_logits.append(last_logit)\n+            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n+        return dec_input\n+\n+\n+\"\"\"\n+## Download the dataset\n+\n+Note: This requires ~3.6 GB of disk space and\n+takes ~5 minutes for the extraction of files.\n+\"\"\"\n+\n+keras.utils.get_file(\n+    os.path.join(os.getcwd(), \"data.tar.gz\"),\n+    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n+    extract=True,\n+    archive_format=\"tar\",\n+    cache_dir=\".\",\n+)\n+\n+\n+saveto = \"./datasets/LJSpeech-1.1\"\n+wavs = glob(\"{}/**/*.wav\".format(saveto), recursive=True)\n+\n+id_to_text = {}\n+with open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n+    for line in f:\n+        id = line.strip().split(\"|\")[0]\n+        text = line.strip().split(\"|\")[2]\n+        id_to_text[id] = text\n+\n+\n+def get_data(wavs, id_to_text, maxlen=50):\n+    \"\"\"returns mapping of audio paths and transcription texts\"\"\"\n+    data = []\n+    for w in wavs:\n+        id = w.split(\"/\")[-1].split(\".\")[0]\n+        if len(id_to_text[id]) < maxlen:\n+            data.append({\"audio\": w, \"text\": id_to_text[id]})\n+    return data\n+\n+\n+\"\"\"\n+## Preprocess the dataset\n+\"\"\"\n+\n+\n+class VectorizeChar:\n+    def __init__(self, max_len=50):\n+        self.vocab = (\n+            [\"-\", \"#\", \"<\", \">\"]\n+            + [chr(i + 96) for i in range(1, 27)]\n+            + [\" \", \".\", \",\", \"?\"]\n+        )\n+        self.max_len = max_len\n+        self.char_to_idx = {}\n+        for i, ch in enumerate(self.vocab):\n+            self.char_to_idx[ch] = i\n+\n+    def __call__(self, text):\n+        text = text.lower()\n+        text = text[: self.max_len - 2]\n+        text = \"<\" + text + \">\"\n+        pad_len = self.max_len - len(text)\n+        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n+\n+    def get_vocabulary(self):\n+        return self.vocab\n+\n+\n+max_target_len = 200  # all transcripts in out data are < 200 characters\n+data = get_data(wavs, id_to_text, max_target_len)\n+vectorizer = VectorizeChar(max_target_len)\n+print(\"vocab size\", len(vectorizer.get_vocabulary()))\n+\n+\n+def create_text_ds(data):\n+    texts = [_[\"text\"] for _ in data]\n+    text_ds = [vectorizer(t) for t in texts]\n+    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n+    return text_ds\n+\n+\n+def path_to_audio(path):\n+    # spectrogram using stft\n+    audio = tf.io.read_file(path)\n+    audio, _ = tf.audio.decode_wav(audio, 1)\n+    audio = tf.squeeze(audio, axis=-1)\n+    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n+    x = tf.math.pow(tf.abs(stfts), 0.5)\n+    # normalisation\n+    means = tf.math.reduce_mean(x, 1, keepdims=True)\n+    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n+    x = (x - means) / stddevs\n+    audio_len = tf.shape(x)[0]\n+    # padding to 10 seconds\n+    pad_len = 2754\n+    paddings = tf.constant([[0, pad_len], [0, 0]])\n+    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n+    return x\n+\n+\n+def create_audio_ds(data):\n+    flist = [_[\"audio\"] for _ in data]\n+    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n+    audio_ds = audio_ds.map(path_to_audio, num_parallel_calls=tf.data.AUTOTUNE)\n+    return audio_ds\n+\n+\n+def create_tf_dataset(data, bs=4):\n+    audio_ds = create_audio_ds(data)\n+    text_ds = create_text_ds(data)\n+    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n+    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n+    ds = ds.batch(bs)\n+    ds = ds.prefetch(tf.data.AUTOTUNE)\n+    return ds\n+\n+\n+split = int(len(data) * 0.99)\n+train_data = data[:split]\n+test_data = data[split:]\n+ds = create_tf_dataset(train_data, bs=64)\n+val_ds = create_tf_dataset(test_data, bs=4)\n+\n+\"\"\"\n+## Callbacks to display predictions\n+\"\"\"\n+\n+\n+class DisplayOutputs(keras.callbacks.Callback):\n+    def __init__(\n+        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n+    ):\n+        \"\"\"Displays a batch of outputs after every epoch\n+\n+        Args:\n+            batch: A test batch containing the keys \"source\" and \"target\"\n+            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n+            target_start_token_idx: A start token index in the target vocabulary\n+            target_end_token_idx: An end token index in the target vocabulary\n+        \"\"\"\n+        self.batch = batch\n+        self.target_start_token_idx = target_start_token_idx\n+        self.target_end_token_idx = target_end_token_idx\n+        self.idx_to_char = idx_to_token\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        if epoch % 5 != 0:\n+            return\n+        source = self.batch[\"source\"]\n+        target = self.batch[\"target\"].numpy()\n+        bs = tf.shape(source)[0]\n+        preds = self.model.generate(source, self.target_start_token_idx)\n+        preds = preds.numpy()\n+        for i in range(bs):\n+            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n+            prediction = \"\"\n+            for idx in preds[i, :]:\n+                prediction += self.idx_to_char[idx]\n+                if idx == self.target_end_token_idx:\n+                    break\n+            print(f\"target:     {target_text.replace('-','')}\")\n+            print(f\"prediction: {prediction}\\n\")\n+\n+\n+\"\"\"\n+## Learning rate schedule\n+\"\"\"\n+\n+\n+class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n+    def __init__(\n+        self,\n+        init_lr=0.00001,\n+        lr_after_warmup=0.001,\n+        final_lr=0.00001,\n+        warmup_epochs=15,\n+        decay_epochs=85,\n+        steps_per_epoch=203,\n+    ):\n+        super().__init__()\n+        self.init_lr = init_lr\n+        self.lr_after_warmup = lr_after_warmup\n+        self.final_lr = final_lr\n+        self.warmup_epochs = warmup_epochs\n+        self.decay_epochs = decay_epochs\n+        self.steps_per_epoch = steps_per_epoch\n+\n+    def calculate_lr(self, epoch):\n+        \"\"\"linear warm up - linear decay\"\"\"\n+        warmup_lr = (\n+            self.init_lr\n+            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n+        )\n+        decay_lr = tf.math.maximum(\n+            self.final_lr,\n+            self.lr_after_warmup\n+            - (epoch - self.warmup_epochs)\n+            * (self.lr_after_warmup - self.final_lr)\n+            / self.decay_epochs,\n+        )\n+        return tf.math.minimum(warmup_lr, decay_lr)\n+\n+    def __call__(self, step):\n+        epoch = step // self.steps_per_epoch\n+        epoch = tf.cast(epoch, \"float32\")\n+        return self.calculate_lr(epoch)\n+\n+\n+\"\"\"\n+## Create & train the end-to-end model\n+\"\"\"\n+\n+batch = next(iter(val_ds))\n+\n+# The vocabulary to convert predicted indices into characters\n+idx_to_char = vectorizer.get_vocabulary()\n+display_cb = DisplayOutputs(\n+    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n+)  # set the arguments as per vocabulary index for '<' and '>'\n+\n+model = Transformer(\n+    num_hid=200,\n+    num_head=2,\n+    num_feed_forward=400,\n+    target_maxlen=max_target_len,\n+    num_layers_enc=4,\n+    num_layers_dec=1,\n+    num_classes=34,\n+)\n+loss_fn = keras.losses.CategoricalCrossentropy(\n+    from_logits=True,\n+    label_smoothing=0.1,\n+)\n+\n+learning_rate = CustomSchedule(\n+    init_lr=0.00001,\n+    lr_after_warmup=0.001,\n+    final_lr=0.00001,\n+    warmup_epochs=15,\n+    decay_epochs=85,\n+    steps_per_epoch=len(ds),\n+)\n+optimizer = keras.optimizers.Adam(learning_rate)\n+model.compile(optimizer=optimizer, loss=loss_fn)\n+\n+history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs=1)\n+\n+\"\"\"\n+In practice, you should train for around 100 epochs or more.\n+\n+Some of the predicted text at or around epoch 35 may look as follows:\n+```\n+target:     <as they sat in the car, frazier asked oswald where his lunch was>\n+prediction: <as they sat in the car frazier his lunch ware mis lunch was>\n+\n+target:     <under the entry for may one, nineteen sixty,>\n+prediction: <under the introus for may monee, nin the sixty,>\n+```\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#862a9419009ca0410e9d48e64b0034bd7a40019f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 540 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ as proposed in the paper, \"Attention is All You Need\".\n **References:**\n \n - [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n-- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/pdf/1904.13377.pdf)\n+- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/abs/1904.13377)\n - [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n - [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fae2b0dd1f8f8c5c26cd1aba99266aaee928167e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 3594 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -659,7 +659,6 @@ class SensitivitySpecificityBase(Metric):\n             ops.nonzero(predicate(constrained, self.value))\n         )\n \n-        print(feasible)\n         feasible_exists = ops.greater(ops.size(feasible), 0)\n         max_dependent = ops.max(ops.take(dependent, feasible), initial=0)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#05652b129708ccad7b8c538630aee5428b38d441", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 275 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1238,8 +1238,7 @@ class Layer(BackendLayer, Operation):\n         try:\n             backend.compute_output_spec(self.call, input_tensors)\n             return True\n-        except Exception as e:\n-            raise e\n+        except:\n             return False\n \n     def _build_by_run_for_kwargs(self, shapes_dict):\n\n@@ -435,7 +435,7 @@ class Model(Trainer, Layer):\n         elif \"shapes_dict\" in config:\n             # Case: inputs were recorded as multiple keyword arguments.\n             if utils.is_default(self.build):\n-                status = self._build_for_kwargs(config[\"shapes_dict\"])\n+                status = self._build_by_run_for_kwargs(config[\"shapes_dict\"])\n             else:\n                 try:\n                     self.build(**config[\"shapes_dict\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cea017b22558f5b00f9f02bb39b9ce0eb13bd0dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 405 | Contributors (this commit): 5 | Commits (past 90d): 12 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -46,7 +46,7 @@ def scatter(indices, values, shape):\n     Args:\n         indices: A tensor or list/tuple specifying\n             indices for the values in `values`.\n-        updates: A tensor, the values to be set at `indices`.\n+        values: A tensor, the values to be set at `indices`.\n         shape: Shape of the output tensor.\n \n     Example:\n\n@@ -718,7 +718,7 @@ def average_pool(\n \n     Args:\n         inputs: Tensor of rank N+2. `inputs` has shape\n-            `(batch_size,)` + inputs_spatial_shape + (num_channels,)` if\n+            `(batch_size,) + inputs_spatial_shape + (num_channels,)` if\n             `data_format=\"channels_last\"`, or\n             `(batch_size, num_channels) + inputs_spatial_shape` if\n             `data_format=\"channels_first\"`. Pooling happens over the spatial\n@@ -910,7 +910,7 @@ def depthwise_conv(\n \n     Args:\n         inputs: Tensor of rank N+2. `inputs` has shape\n-            `(batch_size,)` + inputs_spatial_shape + (num_channels,)` if\n+            `(batch_size,) + inputs_spatial_shape + (num_channels,)` if\n             `data_format=\"channels_last\"`, or\n             `(batch_size, num_channels) + inputs_spatial_shape` if\n             `data_format=\"channels_first\"`.\n@@ -1021,7 +1021,7 @@ def separable_conv(\n \n     Args:\n         inputs: Tensor of rank N+2. `inputs` has shape\n-            `(batch_size,)` + inputs_spatial_shape + (num_channels,)` if\n+            `(batch_size,) + inputs_spatial_shape + (num_channels,)` if\n             `data_format=\"channels_last\"`, or\n             `(batch_size, num_channels) + inputs_spatial_shape` if\n             `data_format=\"channels_first\"`.\n@@ -1142,7 +1142,7 @@ def conv_transpose(\n \n     Args:\n         inputs: Tensor of rank N+2. `inputs` has shape\n-            `(batch_size,)` + inputs_spatial_shape + (num_channels,)` if\n+            `(batch_size,) + inputs_spatial_shape + (num_channels,)` if\n             `data_format=\"channels_last\"`, or\n             `(batch_size, num_channels) + inputs_spatial_shape` if\n             `data_format=\"channels_first\"`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00aad90ff20ecebefda0c5ec7db7e1470431c34d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 72 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -180,7 +180,7 @@ class BaseOptimizer:\n         if hasattr(reference_variable, \"path\"):\n             name = reference_variable.path.replace(\"/\", \"_\") + \"_\" + name\n         else:\n-            name = reference_variable.name + \"_\" + name\n+            name = str(reference_variable.name).replace(\":\", \"_\") + \"_\" + name\n         return self.add_variable(\n             shape=reference_variable.shape,\n             initializer=initializer,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#144d2af38522cd82c52d9b8d2e827a21f4c53297", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 15 | Churn Cumulative: 775 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 0.9\n\nDIFF:\n@@ -48,7 +48,7 @@ class KerasVariable:\n             if callable(initializer):\n                 self._value = None\n                 self._initializer = initializer\n-                self._shape = standardize_shape(shape)\n+                self._shape = self._validate_shape(shape)\n                 register_uninitialized_variable(self)\n             else:\n                 raise ValueError(\n@@ -70,6 +70,7 @@ class KerasVariable:\n                 )\n         else:\n             if callable(initializer):\n+                shape = self._validate_shape(shape)\n                 value = initializer(shape, dtype=dtype)\n             else:\n                 value = initializer\n@@ -91,6 +92,16 @@ class KerasVariable:\n         value = self._initializer(self._shape, dtype=self._dtype)\n         self._initialize(value)\n \n+    def _validate_shape(self, shape):\n+        shape = standardize_shape(shape)\n+        if None in shape:\n+            raise ValueError(\n+                \"Shapes used to initialize variables must be \"\n+                \"fully-defined (no `None` dimensions). Received: \"\n+                f\"shape={shape} for variable path='{self.path}'\"\n+            )\n+        return shape\n+\n     def _maybe_autocast(self, value):\n         autocast_scope = get_autocast_scope()\n         if autocast_scope is not None:\n\n@@ -135,7 +135,7 @@ class FBetaScore(Metric):\n             )\n         num_classes = y_pred_shape[-1]\n         if self.average != \"micro\":\n-            init_shape = num_classes\n+            init_shape = (num_classes,)\n         else:\n             init_shape = ()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc419118657277030e604240921c22ed64b1d55d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 990 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,11 @@\n \"\"\"\n Title: Image classification with EANet (External Attention Transformer)\n Author: [ZhiYong Chang](https://github.com/czy00000)\n-Converted to Keras 3: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2021/10/19\n Last modified: 2023/07/18\n Description: Image classification with a Transformer that leverages external attention.\n Accelerator: GPU\n+Converted to Keras 3: [Muhammad Anas Raza](https://anasrz.com)\n \"\"\"\n \n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ca063bb637df8ab8883ba008ab5351e5281a3ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 434 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,11 @@\n \"\"\"\n Title: Video Classification with Transformers\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Converted to Keras 3 by: [Soumik Rakshit](http://github.com/soumik12345)\n Date created: 2021/06/08\n Last modified: 2023/22/07\n Description: Training a video classifier with hybrid transformers.\n Accelerator: GPU\n+Converted to Keras 3 by: [Soumik Rakshit](http://github.com/soumik12345)\n \"\"\"\n \"\"\"\n This example is a follow-up to the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#218b110f4b4f18847eb3fa7f588f0d506cf18906", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 440 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,9 +44,6 @@ tar -xf ucf101_top5.tar.gz\n \"\"\"\n \n import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n-\n import keras\n from keras import layers\n from keras.applications.densenet import DenseNet121\n@@ -244,6 +241,9 @@ class PositionalEmbedding(layers.Layer):\n         self.sequence_length = sequence_length\n         self.output_dim = output_dim\n \n+    def build(self, input_shape):\n+        self.position_embeddings.build(input_shape)\n+\n     def call(self, inputs):\n         # The inputs are of shape: `(batch_size, frames, num_features)`\n         inputs = keras.backend.cast(inputs, self.compute_dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#34b77dcb91b4965ac9c298eacdc33ef219a765f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 446 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -246,9 +246,9 @@ class PositionalEmbedding(layers.Layer):\n \n     def call(self, inputs):\n         # The inputs are of shape: `(batch_size, frames, num_features)`\n-        inputs = keras.backend.cast(inputs, self.compute_dtype)\n-        length = keras.backend.shape(inputs)[1]\n-        positions = keras.ops.numpy.arange(start=0, stop=length, step=1)\n+        inputs = keras.ops.cast(inputs, self.compute_dtype)\n+        length = keras.ops.shape(inputs)[1]\n+        positions = keras.ops.arange(start=0, stop=length, step=1)\n         embedded_positions = self.position_embeddings(positions)\n         return inputs + embedded_positions\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f86c38483f9cdd8fd71702f5f904d24b04b12bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 172 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,11 @@\n \"\"\"\n Title: Object detection with Vision Transformers\n Author: [Karan V. Dave](https://www.linkedin.com/in/karan-dave-811413164/)\n-Converted to Keras 3 by: [Gabriel Rasskin](https://github.com/grasskin), [Soumik Rakshit](http://github.com/soumik12345)\n Date created: 2022/03/27\n Last modified: 2022/03/27\n Description: A simple Keras implementation of object detection using Vision Transformers.\n Accelerator: GPU\n+Converted to Keras 3 by: [Gabriel Rasskin](https://github.com/grasskin), [Soumik Rakshit](http://github.com/soumik12345)\n \"\"\"\n \n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#778303089b75c55f6336a60c8b2e8fc19aaba9ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 557 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -25,6 +25,8 @@ example.\n \"\"\"\n \n import numpy as np\n+import keras\n+from keras import layers\n \n \n def readucr(filename):\n@@ -60,12 +62,7 @@ timeseries.\n \n You can replace your classification RNN layers with this one: the\n inputs are fully compatible!\n-\"\"\"\n \n-import keras\n-from keras import layers\n-\n-\"\"\"\n We include residual connections, layer normalization, and dropout.\n The resulting layer can be stacked multiple times.\n \n@@ -156,7 +153,7 @@ model.fit(\n     x_train,\n     y_train,\n     validation_split=0.2,\n-    epochs=2,\n+    epochs=150,\n     batch_size=64,\n     callbacks=callbacks,\n )\n@@ -173,6 +170,4 @@ with less than 100k parameters. Of course, parameter count and accuracy could be\n improved by a hyperparameter search and a more sophisticated learning rate\n schedule, or a different optimizer.\n \n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/timeseries_transformer_classification)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/timeseries_transformer_classification).\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#897b269811047627039a9214b459db9331f3e5a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 1476 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -46,8 +46,6 @@ import pandas as pd\n import math\n \n \n-_dtype = \"float32\"\n-\n \"\"\"\n ## Prepare the data\n \"\"\"\n@@ -211,7 +209,7 @@ def create_model_inputs():\n     for feature_name in FEATURE_NAMES:\n         if feature_name in NUMERIC_FEATURE_NAMES:\n             inputs[feature_name] = layers.Input(\n-                name=feature_name, shape=(), dtype=_dtype\n+                name=feature_name, shape=(), dtype=\"float32\"\n             )\n         else:\n             inputs[feature_name] = layers.Input(\n@@ -286,14 +284,14 @@ class NeuralDecisionTree(keras.Model):\n             np.arange(num_features), num_used_features, replace=False\n         )\n         self.used_features_mask = ops.convert_to_tensor(\n-            one_hot[sampled_feature_indices], dtype=_dtype\n+            one_hot[sampled_feature_indices], dtype=\"float32\"\n         )\n \n         # Initialize the weights of the classes in leaves.\n         self.pi = self.add_weight(\n             initializer=\"random_normal\",\n             shape=[self.num_leaves, self.num_classes],\n-            dtype=_dtype,\n+            dtype=\"float32\",\n             trainable=True,\n         )\n \n@@ -476,9 +474,3 @@ def create_forest_model():\n forest_model = create_forest_model()\n \n run_experiment(forest_model)\n-\n-\n-\"\"\"\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/neural-decision-forest)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Neural-Decision-Forest).\n-\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7cb0ad534209c834d69ea1499f701fed6b294298", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 376 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -49,6 +49,11 @@ Target | Diagnosis of heart disease (1 = true; 0 = false) | Target\n ## Setup\n \"\"\"\n \n+import os\n+\n+# TensorFlow is the only backend that supports string inputs.\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n import tensorflow as tf\n import pandas as pd\n import keras\n@@ -172,14 +177,10 @@ then one-hot encode these integer indices.\n - `encode_integer_categorical_feature` to one-hot encode integer categorical features.\n \"\"\"\n \n-from keras.layers import IntegerLookup\n-from keras.layers import Normalization\n-from keras.layers import StringLookup\n-\n \n def encode_numerical_feature(feature, name, dataset):\n     # Create a Normalization layer for our feature\n-    normalizer = Normalization()\n+    normalizer = layers.Normalization()\n \n     # Prepare a Dataset that only yields our feature\n     feature_ds = dataset.map(lambda x, y: x[name])\n@@ -194,7 +195,7 @@ def encode_numerical_feature(feature, name, dataset):\n \n \n def encode_categorical_feature(feature, name, dataset, is_string):\n-    lookup_class = StringLookup if is_string else IntegerLookup\n+    lookup_class = layers.StringLookup if is_string else layers.IntegerLookup\n     # Create a lookup layer which will turn strings into integer indices\n     lookup = lookup_class(output_mode=\"binary\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#39169ad5e64b38483c2f37098a45403f144ad887", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 398 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -59,13 +59,14 @@ Target | Diagnosis of heart disease (1 = true; 0 = false) | Target\n ## Setup\n \"\"\"\n \n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n import tensorflow as tf\n import pandas as pd\n import keras\n from keras.utils import FeatureSpace\n \n-keras.config.disable_traceback_filtering()\n-\n \"\"\"\n ## Preparing the data\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
