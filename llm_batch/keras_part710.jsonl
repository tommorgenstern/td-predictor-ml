{"custom_id": "keras#cbfae2d9b1daf799c096f53dde9612b7dfe655c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 2621 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: 0.5\n\nDIFF:\n@@ -1,8 +1,8 @@\n import jax\n import numpy as np\n-import scipy.ndimage\n \n from keras_core.backend.numpy.core import convert_to_tensor\n+from keras_core.utils.module_utils import scipy\n \n RESIZE_INTERPOLATIONS = (\n     \"bilinear\",\n\n@@ -10,6 +10,7 @@ from keras_core.backend.common.backend_utils import (\n from keras_core.backend.config import epsilon\n from keras_core.backend.numpy.core import cast\n from keras_core.backend.numpy.core import is_tensor\n+from keras_core.utils.module_utils import scipy\n \n \n def relu(x):\n@@ -78,9 +79,7 @@ def gelu(x, approximate=True):\n             )\n         )\n     else:\n-        from scipy.stats import norm\n-\n-        return x * norm.cdf(x)\n+        return x * scipy.stats.norm.cdf(x)\n \n \n def softmax(x, axis=None):\n\n@@ -13,6 +13,7 @@ from keras_core.api_export import keras_core_export\n from keras_core.trainers.data_adapters.py_dataset_adapter import PyDataset\n from keras_core.utils import image_utils\n from keras_core.utils import io_utils\n+from keras_core.utils.module_utils import scipy\n \n \n @keras_core_export(\"keras_core._legacy.preprocessing.image.Iterator\")\n@@ -1799,8 +1800,6 @@ def apply_affine_transform(\n \n     DEPRECATED.\n     \"\"\"\n-    from scipy import ndimage\n-\n     # Input sanity checks:\n     # 1. x must 2D image with one or more channels (i.e., a 3D tensor)\n     # 2. channels must be either first or last dimension\n@@ -1883,7 +1882,7 @@ def apply_affine_transform(\n         final_offset = transform_matrix[:2, 2]\n \n         channel_images = [\n-            ndimage.interpolation.affine_transform(\n+            scipy.ndimage.interpolation.affine_transform(\n                 x_channel,\n                 final_affine_matrix,\n                 final_offset,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#29b13842269fac6df70e7a10d6c9d86caa9002ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 278/275 | Churn Δ: 33 | Churn Cumulative: 20882 | Contributors (this commit): 41 | Commits (past 90d): 4 | Contributors (cumulative): 57 | DMM Complexity: 0.88\n\nDIFF:\n@@ -20,6 +20,7 @@ import collections\n import contextlib\n import functools\n import itertools\n+import re\n import textwrap\n import threading\n import warnings\n@@ -71,6 +72,8 @@ metrics_mod = generic_utils.LazyLoader(\n # Prefix that is added to the TF op layer names.\n _TF_OP_LAYER_NAME_PREFIX = \"tf_op_layer_\"\n \n+_VALID_INPUT_NAME_REGEX = r\"^[A-Za-z0-9.][A-Za-z0-9_.\\\\/>-]*$\"\n+\n # TODO(mdan): Should we have a single generic type for types that can be passed\n # to tf.cast?\n _AUTOCAST_TYPES = (tf.Tensor, tf.SparseTensor, tf.RaggedTensor)\n@@ -1052,6 +1055,18 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         inputs, args, kwargs = self._call_spec.split_out_first_arg(args, kwargs)\n         input_list = tf.nest.flatten(inputs)\n \n+        def _check_valid_input_names(x):\n+            if not re.match(_VALID_INPUT_NAME_REGEX, x):\n+                raise ValueError(\n+                    \"Received an invalid input name: \"\n+                    f\"`{x}`. Please ensure that all input names do \"\n+                    \"not contain invalid characters such as spaces, \"\n+                    \"semicolons, etc.\"\n+                )\n+\n+        if isinstance(inputs, dict):\n+            tf.nest.map_structure(_check_valid_input_names, inputs.keys())\n+\n         # Functional Model construction mode is invoked when `Layer`s are called\n         # on symbolic `KerasTensor`s, i.e.:\n         # >> inputs = tf.keras.Input(10)\n\n@@ -466,7 +466,6 @@ class BaseLayerTest(test_combinations.TestCase):\n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def test_composite_variable_assignment(self):\n         class Spec(tf.TypeSpec):\n-\n             value_type = property(lambda self: CompositeVariable)\n \n             def _component_specs(self):\n@@ -527,6 +526,22 @@ class BaseLayerTest(test_combinations.TestCase):\n         ):\n             base_layer.Layer(name=0)\n \n+    def test_exception_if_call_invalid_input(self):\n+        class MyModel(training_lib.Model):\n+            def call(self, inputs):\n+                return inputs[\"a feature\"] + inputs[\"b_feature\"]\n+\n+        inputs = {\n+            \"a feature\": tf.constant([1.0]),\n+            \"b_feature\": tf.constant([2.0]),\n+        }\n+\n+        model = MyModel()\n+        with self.assertRaisesRegex(\n+            ValueError, \"Received an invalid input name\"\n+        ):\n+            _ = model(inputs)\n+\n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n@@ -1649,7 +1664,6 @@ class NameScopingTest(test_combinations.TestCase):\n )\n class AutographControlFlowTest(test_combinations.TestCase):\n     def test_disabling_in_context_is_matched(self):\n-\n         test_obj = self\n \n         class MyLayer(base_layer.Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1e3dd861dc0e5aa372f91425f6e782deff48c938", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 16 | Churn Cumulative: 960 | Contributors (this commit): 10 | Commits (past 90d): 56 | Contributors (cumulative): 13 | DMM Complexity: 0.7\n\nDIFF:\n@@ -109,6 +109,9 @@ class Variable(KerasVariable):\n \n def convert_to_tensor(x, dtype=None):\n     if is_tensor(x):\n+        device = get_device()\n+        if x.device != device:\n+            x = x.to(device)\n         if dtype is None:\n             return x\n         return x.to(to_torch_dtype(dtype))\n\n@@ -94,3 +94,16 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n     def test_normalization_errors(self):\n         # TODO\n         pass\n+\n+    @pytest.mark.skipif(\n+        backend.backend() != \"torch\",\n+        reason=\"Test symbolic call for torch meta device.\",\n+    )\n+    def test_call_on_meta_device_after_built(self):\n+        from keras_core.backend.torch import core\n+\n+        layer = layers.Normalization()\n+        data = np.random.random((32, 4))\n+        layer.adapt(data)\n+        with core.device_scope(\"meta\"):\n+            layer(data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9d56469d5799d483d20a286ae5e6530e62d721d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 278 | Lines Deleted: 62 | Files Changed: 3 | Hunks: 22 | Methods Changed: 9 | Complexity Δ (Sum/Max): 21/21 | Churn Δ: 340 | Churn Cumulative: 2630 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,5 @@\n-import math\n-\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n from numpy.lib.stride_tricks import as_strided\n \n@@ -249,11 +246,11 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def _same_padding(self, input_size, kernel_size, stride):\n-        # P = ((S-1)*W-S+K)/2, with K = kernel size, S = stride, W = input size\n-        padding = int(\n-            math.ceil(((stride - 1) * input_size - stride + kernel_size) / 2)\n-        )\n-        return padding\n+        if input_size % stride == 0:\n+            padding = max(kernel_size - stride, 0)\n+        else:\n+            padding = max(kernel_size - (input_size % stride), 0)\n+        return padding // 2, padding - padding // 2\n \n     def _np_conv1d(\n         self,\n@@ -292,9 +289,9 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             h_pad = self._same_padding(h_x, kernel_size, h_stride)\n             npad = [(0, 0)] * x.ndim\n             if padding == \"causal\":\n-                npad[1] = (h_pad * 2, 0)\n+                npad[1] = (h_pad[0] + h_pad[1], 0)\n             else:\n-                npad[1] = (h_pad, h_pad)\n+                npad[1] = h_pad\n             x = np.pad(x, pad_width=npad, mode=\"constant\", constant_values=0)\n \n         n_batch, h_x, _ = x.shape\n@@ -325,7 +322,192 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n             ]\n             out_grps.append(x_strided @ kernel_weights_grp + bias_weights_grp)\n-        return np.concatenate(out_grps, axis=-1)\n+        out = np.concatenate(out_grps, axis=-1)\n+        if data_format == \"channels_first\":\n+            out = out.swapaxes(1, 2)\n+        return out\n+\n+    def _np_conv2d(\n+        self,\n+        x,\n+        kernel_weights,\n+        bias_weights,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+        groups,\n+    ):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 1))\n+        if isinstance(strides, (tuple, list)):\n+            h_stride, w_stride = strides\n+        else:\n+            h_stride = strides\n+            w_stride = strides\n+        if isinstance(dilation_rate, (tuple, list)):\n+            h_dilation, w_dilation = dilation_rate\n+        else:\n+            h_dilation = dilation_rate\n+            w_dilation = dilation_rate\n+        h_kernel, w_kernel, ch_in, ch_out = kernel_weights.shape\n+\n+        if h_dilation > 1 or w_dilation > 1:\n+            new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n+            new_w_kernel = w_kernel + (w_dilation - 1) * (w_kernel - 1)\n+            new_kenel_size_tuple = (new_h_kernel, new_w_kernel)\n+            new_kernel_weights = np.zeros(\n+                (*new_kenel_size_tuple, ch_in, ch_out),\n+                dtype=kernel_weights.dtype,\n+            )\n+            new_kernel_weights[::h_dilation, ::w_dilation] = kernel_weights\n+            kernel_weights = new_kernel_weights\n+            h_kernel, w_kernel = kernel_weights.shape[:2]\n+\n+        if padding == \"same\":\n+            n_batch, h_x, w_x, _ = x.shape\n+            h_pad = self._same_padding(h_x, h_kernel, h_stride)\n+            w_pad = self._same_padding(w_x, w_kernel, w_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = h_pad\n+            npad[2] = w_pad\n+            x = np.pad(x, pad_width=npad, mode=\"constant\", constant_values=0)\n+\n+        n_batch, h_x, w_x, _ = x.shape\n+        h_out = int((h_x - h_kernel) / h_stride) + 1\n+        w_out = int((w_x - w_kernel) / w_stride) + 1\n+\n+        out_grps = []\n+        for grp in range(1, groups + 1):\n+            x_in = x[..., (grp - 1) * ch_in : grp * ch_in]\n+            stride_shape = (n_batch, h_out, w_out, h_kernel, w_kernel, ch_in)\n+            strides = (\n+                x_in.strides[0],\n+                h_stride * x_in.strides[1],\n+                w_stride * x_in.strides[2],\n+                x_in.strides[1],\n+                x_in.strides[2],\n+                x_in.strides[3],\n+            )\n+            inner_dim = h_kernel * w_kernel * ch_in\n+            x_strided = as_strided(\n+                x_in, shape=stride_shape, strides=strides\n+            ).reshape(-1, inner_dim)\n+            ch_out_groups = ch_out // groups\n+            kernel_weights_grp = kernel_weights[\n+                ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n+            ].reshape(-1, ch_out_groups)\n+            bias_weights_grp = bias_weights[\n+                ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n+            ]\n+            out_grps.append(x_strided @ kernel_weights_grp + bias_weights_grp)\n+        out = np.concatenate(out_grps, axis=-1).reshape(\n+            n_batch, h_out, w_out, ch_out\n+        )\n+        if data_format == \"channels_first\":\n+            out = out.transpose((0, 3, 1, 2))\n+        return out\n+\n+    def _np_conv3d(\n+        self,\n+        x,\n+        kernel_weights,\n+        bias_weights,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+        groups,\n+    ):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 4, 1))\n+        if isinstance(strides, (tuple, list)):\n+            h_stride, w_stride, d_stride = strides\n+        else:\n+            h_stride = strides\n+            w_stride = strides\n+            d_stride = strides\n+        if isinstance(dilation_rate, (tuple, list)):\n+            h_dilation, w_dilation, d_dilation = dilation_rate\n+        else:\n+            h_dilation = dilation_rate\n+            w_dilation = dilation_rate\n+            d_dilation = dilation_rate\n+\n+        h_kernel, w_kernel, d_kernel, ch_in, ch_out = kernel_weights.shape\n+\n+        if h_dilation > 1 or w_dilation > 1 or d_dilation > 1:\n+            new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n+            new_w_kernel = w_kernel + (w_dilation - 1) * (w_kernel - 1)\n+            new_d_kernel = d_kernel + (d_dilation - 1) * (d_kernel - 1)\n+            new_kenel_size_tuple = (new_h_kernel, new_w_kernel, new_d_kernel)\n+            new_kernel_weights = np.zeros(\n+                (*new_kenel_size_tuple, ch_in, ch_out),\n+                dtype=kernel_weights.dtype,\n+            )\n+            new_kernel_weights[\n+                ::h_dilation, ::w_dilation, ::d_dilation\n+            ] = kernel_weights\n+            kernel_weights = new_kernel_weights\n+            h_kernel, w_kernel, d_kernel = kernel_weights.shape[:3]\n+\n+        if padding == \"same\":\n+            n_batch, h_x, w_x, d_x, _ = x.shape\n+            h_pad = self._same_padding(h_x, h_kernel, h_stride)\n+            w_pad = self._same_padding(w_x, w_kernel, w_stride)\n+            d_pad = self._same_padding(d_x, d_kernel, d_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = h_pad\n+            npad[2] = w_pad\n+            npad[3] = d_pad\n+            x = np.pad(x, pad_width=npad, mode=\"constant\", constant_values=0)\n+\n+        n_batch, h_x, w_x, d_x, _ = x.shape\n+        h_out = int((h_x - h_kernel) / h_stride) + 1\n+        w_out = int((w_x - w_kernel) / w_stride) + 1\n+        d_out = int((d_x - d_kernel) / d_stride) + 1\n+\n+        out_grps = []\n+        for grp in range(1, groups + 1):\n+            x_in = x[..., (grp - 1) * ch_in : grp * ch_in]\n+            stride_shape = (\n+                n_batch,\n+                h_out,\n+                w_out,\n+                d_out,\n+                h_kernel,\n+                w_kernel,\n+                d_kernel,\n+                ch_in,\n+            )\n+            strides = (\n+                x_in.strides[0],\n+                h_stride * x_in.strides[1],\n+                w_stride * x_in.strides[2],\n+                d_stride * x_in.strides[3],\n+                x_in.strides[1],\n+                x_in.strides[2],\n+                x_in.strides[3],\n+                x_in.strides[4],\n+            )\n+            inner_dim = h_kernel * w_kernel * d_kernel * ch_in\n+            x_strided = as_strided(\n+                x_in, shape=stride_shape, strides=strides\n+            ).reshape(-1, inner_dim)\n+            ch_out_groups = ch_out // groups\n+            kernel_weights_grp = kernel_weights[\n+                ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n+            ].reshape(-1, ch_out_groups)\n+            bias_weights_grp = bias_weights[\n+                ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n+            ]\n+            out_grps.append(x_strided @ kernel_weights_grp + bias_weights_grp)\n+        out = np.concatenate(out_grps, axis=-1).reshape(\n+            n_batch, h_out, w_out, d_out, ch_out\n+        )\n+        if data_format == \"channels_first\":\n+            out = out.transpose((0, 4, 1, 2, 3))\n+        return out\n \n     @parameterized.parameters(\n         {\n@@ -364,6 +546,15 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"dilation_rate\": 1,\n             \"groups\": 2,\n         },\n+        {\n+            \"filters\": 6,\n+            \"kernel_size\": (2,),\n+            \"strides\": (2,),\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_first\",\n+            \"dilation_rate\": 1,\n+            \"groups\": 2,\n+        },\n     )\n     def test_conv1d(\n         self,\n@@ -435,6 +626,15 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"dilation_rate\": (2, 2),\n             \"groups\": 2,\n         },\n+        {\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"same\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (2, 3),\n+            \"groups\": 2,\n+        },\n         {\n             \"filters\": 6,\n             \"kernel_size\": (4, 3),\n@@ -444,6 +644,15 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"dilation_rate\": (1, 1),\n             \"groups\": 2,\n         },\n+        {\n+            \"filters\": 6,\n+            \"kernel_size\": (4, 3),\n+            \"strides\": (2, 1),\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_first\",\n+            \"dilation_rate\": (1, 1),\n+            \"groups\": 2,\n+        },\n     )\n     def test_conv2d(\n         self,\n@@ -464,31 +673,27 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n-        tf_keras_layer = tf.keras.layers.Conv2D(\n-            filters=filters,\n-            kernel_size=kernel_size,\n+\n+        inputs = np.random.normal(size=[2, 8, 8, 4])\n+        layer.build(input_shape=inputs.shape)\n+\n+        kernel_shape = layer.kernel.shape\n+        kernel_weights = np.random.normal(size=kernel_shape)\n+        bias_weights = np.random.normal(size=(filters,))\n+        layer.kernel.assign(kernel_weights)\n+        layer.bias.assign(bias_weights)\n+\n+        outputs = layer(inputs)\n+        expected = self._np_conv2d(\n+            inputs,\n+            kernel_weights,\n+            bias_weights,\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n-\n-        inputs = np.random.normal(size=[2, 8, 8, 4])\n-        layer.build(input_shape=inputs.shape)\n-        tf_keras_layer.build(input_shape=inputs.shape)\n-\n-        kernel_shape = layer.kernel.shape\n-        kernel_weights = np.random.normal(size=kernel_shape)\n-        bias_weights = np.random.normal(size=(filters,))\n-        layer.kernel.assign(kernel_weights)\n-        tf_keras_layer.kernel.assign(kernel_weights)\n-\n-        layer.bias.assign(bias_weights)\n-        tf_keras_layer.bias.assign(bias_weights)\n-\n-        outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected, rtol=5e-4)\n \n     @parameterized.parameters(\n@@ -510,6 +715,15 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"dilation_rate\": (2, 2, 2),\n             \"groups\": 2,\n         },\n+        {\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"same\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (2, 3, 4),\n+            \"groups\": 2,\n+        },\n         {\n             \"filters\": 6,\n             \"kernel_size\": (2, 2, 3),\n@@ -519,6 +733,15 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"dilation_rate\": (1, 1, 1),\n             \"groups\": 2,\n         },\n+        {\n+            \"filters\": 6,\n+            \"kernel_size\": (2, 2, 3),\n+            \"strides\": (2, 1, 2),\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_first\",\n+            \"dilation_rate\": (1, 1, 1),\n+            \"groups\": 2,\n+        },\n     )\n     def test_conv3d(\n         self,\n@@ -539,29 +762,25 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n-        tf_keras_layer = tf.keras.layers.Conv3D(\n-            filters=filters,\n-            kernel_size=kernel_size,\n+\n+        inputs = np.random.normal(size=[2, 8, 8, 8, 4])\n+        layer.build(input_shape=inputs.shape)\n+\n+        kernel_shape = layer.kernel.shape\n+        kernel_weights = np.random.normal(size=kernel_shape)\n+        bias_weights = np.random.normal(size=(filters,))\n+        layer.kernel.assign(kernel_weights)\n+        layer.bias.assign(bias_weights)\n+\n+        outputs = layer(inputs)\n+        expected = self._np_conv3d(\n+            inputs,\n+            kernel_weights,\n+            bias_weights,\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n-\n-        inputs = np.random.normal(size=[2, 8, 8, 8, 4])\n-        layer.build(input_shape=inputs.shape)\n-        tf_keras_layer.build(input_shape=inputs.shape)\n-\n-        kernel_shape = layer.kernel.shape\n-        kernel_weights = np.random.normal(size=kernel_shape)\n-        bias_weights = np.random.normal(size=(filters,))\n-        layer.kernel.assign(kernel_weights)\n-        tf_keras_layer.kernel.assign(kernel_weights)\n-\n-        layer.bias.assign(bias_weights)\n-        tf_keras_layer.bias.assign(bias_weights)\n-        outputs = layer(inputs)\n-\n-        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected, rtol=5e-4)\n\n@@ -323,7 +323,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n \n         outputs = layer(inputs)\n         expected = tf_keras_layer(inputs)\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, atol=1e-5)\n \n     @parameterized.parameters(\n         {\n@@ -407,7 +407,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n \n         outputs = layer(inputs)\n         expected = tf_keras_layer(inputs)\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, atol=1e-5)\n \n     @parameterized.parameters(\n         {\n\n@@ -1,5 +1,4 @@\n import numpy as np\n-import tensorflow as tf\n from absl.testing import parameterized\n \n from keras_core import backend\n@@ -56,22 +55,20 @@ class TestNumericalUtils(testing.TestCase, parameterized.TestCase):\n \n     @parameterized.parameters([1, 2, 3])\n     def test_normalize(self, order):\n-        xnp = np.random.random((3, 3))\n         xb = backend.random.uniform((3, 3), seed=1337)\n+        xnp = backend.convert_to_numpy(xb)\n+\n+        # Expected result\n+        l2 = np.atleast_1d(np.linalg.norm(xnp, order, axis=-1))\n+        l2[l2 == 0] = 1\n+        expected = xnp / np.expand_dims(l2, axis=-1)\n \n         # Test NumPy\n         out = numerical_utils.normalize(xnp, axis=-1, order=order)\n         self.assertTrue(isinstance(out, np.ndarray))\n-        self.assertAllClose(\n-            tf.keras.utils.normalize(xnp, axis=-1, order=order), out\n-        )\n+        self.assertAllClose(out, expected)\n \n         # Test backend\n         out = numerical_utils.normalize(xb, axis=-1, order=order)\n         self.assertTrue(backend.is_tensor(out))\n-        self.assertAllClose(\n-            tf.keras.utils.normalize(\n-                backend.convert_to_numpy(xb), axis=-1, order=order\n-            ),\n-            backend.convert_to_numpy(out),\n-        )\n+        self.assertAllClose(backend.convert_to_numpy(out), expected)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#804c11d7307aeb70b994cc57f65aede6eb657597", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 31 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 33 | Churn Cumulative: 20915 | Contributors (this commit): 41 | Commits (past 90d): 6 | Contributors (cumulative): 57 | DMM Complexity: 0.12\n\nDIFF:\n@@ -20,7 +20,6 @@ import collections\n import contextlib\n import functools\n import itertools\n-import re\n import textwrap\n import threading\n import warnings\n@@ -72,8 +71,6 @@ metrics_mod = generic_utils.LazyLoader(\n # Prefix that is added to the TF op layer names.\n _TF_OP_LAYER_NAME_PREFIX = \"tf_op_layer_\"\n \n-_VALID_INPUT_NAME_REGEX = r\"^[A-Za-z0-9.][A-Za-z0-9_.\\\\/>-]*$\"\n-\n # TODO(mdan): Should we have a single generic type for types that can be passed\n # to tf.cast?\n _AUTOCAST_TYPES = (tf.Tensor, tf.SparseTensor, tf.RaggedTensor)\n@@ -1055,18 +1052,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         inputs, args, kwargs = self._call_spec.split_out_first_arg(args, kwargs)\n         input_list = tf.nest.flatten(inputs)\n \n-        def _check_valid_input_names(x):\n-            if not re.match(_VALID_INPUT_NAME_REGEX, x):\n-                raise ValueError(\n-                    \"Received an invalid input name: \"\n-                    f\"`{x}`. Please ensure that all input names do \"\n-                    \"not contain invalid characters such as spaces, \"\n-                    \"semicolons, etc.\"\n-                )\n-\n-        if isinstance(inputs, dict):\n-            tf.nest.map_structure(_check_valid_input_names, inputs.keys())\n-\n         # Functional Model construction mode is invoked when `Layer`s are called\n         # on symbolic `KerasTensor`s, i.e.:\n         # >> inputs = tf.keras.Input(10)\n\n@@ -466,6 +466,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def test_composite_variable_assignment(self):\n         class Spec(tf.TypeSpec):\n+\n             value_type = property(lambda self: CompositeVariable)\n \n             def _component_specs(self):\n@@ -526,22 +527,6 @@ class BaseLayerTest(test_combinations.TestCase):\n         ):\n             base_layer.Layer(name=0)\n \n-    def test_exception_if_call_invalid_input(self):\n-        class MyModel(training_lib.Model):\n-            def call(self, inputs):\n-                return inputs[\"a feature\"] + inputs[\"b_feature\"]\n-\n-        inputs = {\n-            \"a feature\": tf.constant([1.0]),\n-            \"b_feature\": tf.constant([2.0]),\n-        }\n-\n-        model = MyModel()\n-        with self.assertRaisesRegex(\n-            ValueError, \"Received an invalid input name\"\n-        ):\n-            _ = model(inputs)\n-\n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n@@ -1664,6 +1649,7 @@ class NameScopingTest(test_combinations.TestCase):\n )\n class AutographControlFlowTest(test_combinations.TestCase):\n     def test_disabling_in_context_is_matched(self):\n+\n         test_obj = self\n \n         class MyLayer(base_layer.Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#090a83fea7bac667f1a1883394a474f633aac485", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 148 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 37/26 | Churn Δ: 151 | Churn Cumulative: 26255 | Contributors (this commit): 146 | Commits (past 90d): 8 | Contributors (cumulative): 148 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,7 +39,6 @@ from keras.engine import base_layer_utils\n from keras.engine import compile_utils\n from keras.engine import data_adapter\n from keras.engine import input_layer as input_layer_module\n-from keras.engine import steps_per_execution_tuning\n from keras.engine import training_utils\n from keras.metrics import base_metric\n from keras.mixed_precision import loss_scale_optimizer as lso\n@@ -55,6 +54,7 @@ from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import io_utils\n from keras.utils import layer_utils\n+from keras.utils import steps_per_execution_tuning\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n from keras.utils import traceback_utils\n\n@@ -19,8 +19,10 @@ import threading\n import time\n \n import numpy as np\n+from tensorflow.python.util.tf_export import keras_export\n \n \n+@keras_export(\"keras.utils.StepsPerExecutionTuner\")\n class StepsPerExecutionTuner:\n     \"\"\"Steps per execution tuner class.\n \n@@ -37,7 +39,82 @@ class StepsPerExecutionTuner:\n             before tuning. Defaults to 10.\n         change_threshold: Optional float, the percent different in throughput to\n             trigger a `steps_per_execution` change. For example, `0.1` triggers\n-            changes if throughput ()\n+            changes if throughput changes more than 10%.\n+\n+    Examples:\n+\n+    If you're using `model.compile` and `model.fit`, this functionality is\n+    available at compile time with `steps_per_execution='auto'`\n+\n+    ```python\n+    model.compile(..., steps_per_execution='auto')\n+    ```\n+\n+    Custom training loop usage:\n+\n+    ```python\n+    # Get model\n+    inputs = keras.Input(shape=(784,), name=\"digits\")\n+    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n+    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n+    outputs = layers.Dense(10, name=\"predictions\")(x)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+\n+    # Instantiate an optimizer to train the model.\n+    optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n+    # Instantiate a loss function.\n+    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n+\n+    # Prepare the training dataset.\n+    batch_size = 64\n+    (x_train, y_train), (_, _) = keras.datasets.mnist.load_data()\n+    x_train = np.reshape(x_train, (-1, 784))\n+    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+\n+    # Create our steps per execution variable\n+    steps_per_execution = tf.Variable(\n+        1,\n+        dtype=\"int64\",\n+        aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA\n+    )\n+\n+    # Create the tuner\n+    tuner = StepsPerExecutionTuner(\n+        optimizer, steps_per_execution\n+    )\n+\n+    # Create a step function that runs a single training step\n+    @tf.function\n+    def step_fn(iterator):\n+        batch_data, labels = next(iterator)\n+        with tf.GradientTape() as tape:\n+            logits = model(batch_data, training=True)\n+            loss_value = loss_fn(labels, logits)\n+        grads = tape.gradient(loss_value, model.trainable_weights)\n+        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n+\n+    # We can now pack multiple execution steps into one call\n+    @tf.function\n+    def multi_step_train_fn(iterator, steps_per_execution):\n+        for _ in tf.range(steps_per_execution):\n+            outputs = step_fn(iterator)\n+        return\n+\n+    initial_steps_per_execution = 1\n+    steps_per_epoch = 100\n+    epochs = 2\n+\n+    # Start the tuner before training\n+    tuner.start()\n+\n+    # We can now call our multi step training with our data\n+    for epoch in range(epochs):\n+        for _ in range(steps_per_epoch):\n+            multi_step_train_fn(iterator, steps_per_execution)\n+\n+    # End the tuner after training\n+    tuner.stop()\n+    ```\n     \"\"\"\n \n     def __init__(\n\n@@ -18,8 +18,13 @@ import time\n \n import tensorflow.compat.v2 as tf\n \n-from keras.engine import steps_per_execution_tuning\n+from keras import Input\n+from keras import Model\n+from keras import losses\n+from keras import optimizers\n+from keras.layers import Dense\n from keras.testing_infra import test_combinations\n+from keras.utils import steps_per_execution_tuning\n \n \n class mockOptimizer:\n@@ -67,6 +72,69 @@ class StepsPerExecutionTuningTest(test_combinations.TestCase):\n         assert spe_variable.numpy().item() == 5\n         assert tuner.init_spe == 5\n \n+    def test_custom_training_loop(self):\n+        dataset = _get_dataset()\n+        iterator = iter(dataset)\n+\n+        inputs = Input(shape=(784,), name=\"digits\")\n+        x = Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n+        x = Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n+        outputs = Dense(10, name=\"predictions\")(x)\n+        model = Model(inputs=inputs, outputs=outputs)\n+        optimizer = optimizers.SGD(learning_rate=1e-3)\n+        loss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\n+\n+        # Create our steps per execution variable\n+        steps_per_execution = tf.Variable(\n+            1,\n+            dtype=\"int64\",\n+            aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,\n+        )\n+\n+        # Create the tuner\n+        tuner = steps_per_execution_tuning.StepsPerExecutionTuner(\n+            optimizer, steps_per_execution\n+        )\n+\n+        # Create a step function that runs a single training step\n+        @tf.function\n+        def step_fn(iterator):\n+            batch_data, labels = next(iterator)\n+            print(batch_data.shape, labels.shape)\n+            with tf.GradientTape() as tape:\n+                logits = model(batch_data, training=True)\n+                loss_value = loss_fn(labels, logits)\n+            grads = tape.gradient(loss_value, model.trainable_weights)\n+            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n+\n+        # We can now pack multiple execution steps into one call\n+        @tf.function\n+        def multi_step_train_fn(iterator, steps_per_execution):\n+            for _ in tf.range(steps_per_execution):\n+                step_fn(iterator)\n+            return\n+\n+        steps_per_epoch = 10\n+        epochs = 2\n+\n+        # Start the tuner before training\n+        tuner.start()\n+\n+        for _ in range(epochs):\n+            for _ in range(steps_per_epoch):\n+                multi_step_train_fn(iterator, steps_per_execution)\n+\n+        # End the tuner after training\n+        tuner.stop()\n+\n+\n+def _get_dataset():\n+    inputs = tf.zeros((1000, 784), dtype=tf.float32)\n+    targets = tf.zeros((1000,), dtype=tf.float32)\n+    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n+    dataset = dataset.batch(10)\n+    return dataset\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#31fb21f68fc270606e93ccf0af3ac088561b1769", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 16 | Methods Changed: 17 | Complexity Δ (Sum/Max): 9/7 | Churn Δ: 93 | Churn Cumulative: 40292 | Contributors (this commit): 152 | Commits (past 90d): 11 | Contributors (cumulative): 172 | DMM Complexity: 0.9375\n\nDIFF:\n@@ -320,7 +320,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self._checkpoint = tf.train.Checkpoint(root=weakref.ref(self))\n \n         self._steps_per_execution = None\n-        self._enable_tune_steps_per_execution = False\n+        self._steps_per_execution_tuner = None\n+        self._autotune_steps_per_execution = False\n \n         self._layout_map = layout_map_lib.get_current_layout_map()\n \n@@ -804,12 +805,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             )\n \n             if steps_per_execution == \"auto\":\n+                if self._steps_per_execution is None:\n                     self._configure_steps_per_execution(1)\n                 self._steps_per_execution_tuner = (\n                     steps_per_execution_tuning.StepsPerExecutionTuner(\n                         self.optimizer, self._steps_per_execution\n                     )\n                 )\n+                self._autotune_steps_per_execution = True\n             else:\n                 self._configure_steps_per_execution(steps_per_execution or 1)\n \n@@ -1007,12 +1010,33 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self._run_eagerly = value\n \n     @property\n-    def enable_tune_steps_per_execution(self):\n-        return self._enable_tune_steps_per_execution\n+    def autotune_steps_per_execution(self):\n+        \"\"\"Settable property to enable tuning for steps_per_execution\"\"\"\n+        return self._autotune_steps_per_execution\n \n-    @enable_tune_steps_per_execution.setter\n-    def enable_tune_steps_per_execution(self, value):\n-        self._enable_tune_steps_per_execution = value\n+    @autotune_steps_per_execution.setter\n+    def autotune_steps_per_execution(self, value):\n+        self._autotune_steps_per_execution = value\n+        if value and self._steps_per_execution_tuner is None:\n+            if self._steps_per_execution is None:\n+                self._configure_steps_per_execution(1)\n+            self._steps_per_execution_tuner = (\n+                steps_per_execution_tuning.StepsPerExecutionTuner(\n+                    self.optimizer, self._steps_per_execution\n+                )\n+            )\n+\n+    @property\n+    def steps_per_execution(self):\n+        \"\"\"Settable `steps_per_execution variable. Requires a compiled model.\"\"\"\n+        return self._steps_per_execution\n+\n+    @steps_per_execution.setter\n+    def steps_per_execution(self, value):\n+        if self._steps_per_execution is None:\n+            self._configure_steps_per_execution(value)\n+        else:\n+            self._steps_per_execution.assign(value)\n \n     @property\n     def jit_compile(self):\n@@ -1369,7 +1393,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n-            and not self.enable_tune_steps_per_execution\n+            and not self.autotune_steps_per_execution\n         ):\n \n             def train_function(iterator):\n@@ -1752,7 +1776,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             self._train_counter.assign(0)\n             callbacks.on_train_begin()\n             training_logs = None\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.start()\n             # Handle fault-tolerance for multi-worker.\n             # TODO(omalleyt): Fix the ordering issues that mean this has to\n@@ -1860,7 +1884,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             # If eval data_handler exists, delete it after all epochs are done.\n             if getattr(self, \"_eval_data_handler\", None) is not None:\n                 del self._eval_data_handler\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.stop()\n             callbacks.on_train_end(logs=training_logs)\n             return self.history\n@@ -2034,7 +2058,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n-            and not self.enable_tune_steps_per_execution\n+            and not self.autotune_steps_per_execution\n         ):\n \n             def test_function(iterator):\n@@ -2256,7 +2280,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             test_function_runner = self._get_test_function_runner(callbacks)\n             self._test_counter.assign(0)\n             callbacks.on_test_begin()\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.start()\n             for (\n                 _,\n@@ -2282,7 +2306,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 logs = self._aggregate_exact_metrics(logs)\n             else:\n                 logs = self._validate_and_get_metrics_result(logs)\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.stop()\n             callbacks.on_test_end(logs=logs)\n \n@@ -2408,7 +2432,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n-            and not self.enable_tune_steps_per_execution\n+            and not self.autotune_steps_per_execution\n         ):\n \n             def predict_function(iterator):\n@@ -2621,7 +2645,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             self.predict_function = self.make_predict_function()\n             self._predict_counter.assign(0)\n             callbacks.on_predict_begin()\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.start()\n             batch_outputs = None\n             for _, iterator in data_handler.enumerate_epochs():  # Single epoch.\n@@ -2661,7 +2685,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     \"information of where went wrong, or file a \"\n                     \"issue/bug to `tf.keras`.\"\n                 )\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.stop()\n             callbacks.on_predict_end()\n         all_outputs = tf.__internal__.nest.map_structure_up_to(\n\n@@ -2472,9 +2472,44 @@ class TestAutotuneSPE(test_combinations.TestCase):\n         x, y = np.ones((10, 1)), np.ones((10, 1))\n         model.fit(x, y, epochs=2)\n         model.evaluate(x, y)\n-        model.enable_tune_steps_per_execution = False\n+        model.autotune_steps_per_execution = False\n         model.predict(x)\n-        assert model.enable_tune_steps_per_execution == False\n+        assert model.autotune_steps_per_execution == False\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_spe_tune_set_after_compile(self):\n+        model = sequential.Sequential([layers_module.Dense(1)])\n+        model.compile(\n+            \"sgd\",\n+            loss=\"mse\",\n+            run_eagerly=False,\n+            jit_compile=True,\n+            steps_per_execution=5,\n+        )\n+        x, y = np.ones((10, 1)), np.ones((10, 1))\n+        model.fit(x, y, epochs=2)\n+        assert model._steps_per_execution_tuner is None\n+        model.autotune_steps_per_execution = True\n+        model.fit(x, y, epochs=2)\n+        assert model.steps_per_execution.numpy().item() == 5\n+        assert model._steps_per_execution_tuner\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_spe_tune_set_before_compile(self):\n+        model = sequential.Sequential([layers_module.Dense(1)])\n+        model.steps_per_execution = 5\n+        model.compile(\n+            \"sgd\",\n+            loss=\"mse\",\n+            run_eagerly=False,\n+            jit_compile=True,\n+            steps_per_execution=\"auto\",\n+        )\n+        assert model.steps_per_execution.numpy().item() == 5\n+        assert model._steps_per_execution_tuner\n+\n+        x, y = np.ones((10, 1)), np.ones((10, 1))\n+        model.fit(x, y, epochs=2)\n \n \n class TestExceptionsAndWarnings(test_combinations.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f01ac7129e981829e0f49572886b9c793889863", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 2429 | Contributors (this commit): 12 | Commits (past 90d): 34 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -180,7 +180,7 @@ def map_graph(inputs, outputs):\n \n     Returns:\n         A tuple `(nodes, nodes_by_depth, operations, operations_by_depth)`.\n-        - nodes: list of Node instances.\n+        - network_nodes: dict mapping unique node keys to the Node instances\n         - nodes_by_depth: dict mapping ints (depth) to lists of node instances.\n         - operations: list of Operation instances.\n         - operations_by_depth: dict mapping ints (depth) to lists of Operation\n\n@@ -48,10 +48,11 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n \n     Example:\n \n-    >>> data = keras_core.ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n-    >>> segment_ids = keras_core.ops.convert_to_tensor([0, 1, 0, 1, 0, 1])\n-    >>> segment_sum(data, segment_ids)\n-    array([9 12], shape=(2,), dtype=int32)\n+    >>> data = keras_core.ops.convert_to_tensor([1, 2, 10, 20, 100, 200])\n+    >>> segment_ids = keras_core.ops.convert_to_tensor([0, 0, 1, 1, 2, 2])\n+    >>> num_segments = 3\n+    >>> keras_core.ops.segment_sum(data, segment_ids,num_segments)\n+    array([3, 30, 300], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((data,)):\n         return SegmentSum(num_segments, sorted).symbolic_call(data, segment_ids)\n@@ -100,10 +101,11 @@ def segment_max(data, segment_ids, num_segments=None, sorted=False):\n \n     Example:\n \n-    >>> data = keras_core.ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n-    >>> segment_ids = keras_core.ops.convert_to_tensor([0, 1, 0, 1, 0, 1])\n-    >>> segment_max(data, segment_ids)\n-    array([9 12], shape=(2,), dtype=int32)\n+    >>> data = keras_core.ops.convert_to_tensor([1, 2, 10, 20, 100, 200])\n+    >>> segment_ids = keras_core.ops.convert_to_tensor([0, 0, 1, 1, 2, 2])\n+    >>> num_segments = 3\n+    >>> keras_core.ops.segment_max(data, segment_ids, num_segments)\n+    array([2, 20, 200], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((data,)):\n         return SegmentMax(num_segments, sorted).symbolic_call(data, segment_ids)\n@@ -696,8 +698,9 @@ class Rsqrt(Operation):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([2., 3., -2.])\n-    >>> rsqrt(x)\n+    >>> data = keras_core.ops.convert_to_tensor([1.0, 10.0, 100.0])\n+    >>> keras_core.ops.rsqrt(data)\n+    array([1.0, 0.31622776, 0.1], dtype=float32)\n     \"\"\"\n \n     def call(self, x):\n\n@@ -1501,6 +1501,30 @@ class MultiHot(Operation):\n     ]\n )\n def multi_hot(inputs, num_tokens, axis=-1, dtype=None):\n+    \"\"\"Encodes integer labels as multi-hot vectors.\n+\n+    This function encodes integer labels as multi-hot vectors, where each label\n+    is mapped to a binary value in the resulting vector.\n+\n+    Args:\n+        inputs: Tensor of integer labels to be converted to multi-hot vectors.\n+        num_tokens: Integer, the total number of unique tokens or classes.\n+        axis: (optional) Axis along which the multi-hot encoding should be\n+            added. Default is -1, which corresponds to the last dimension.\n+        dtype: (optional) The data type of the resulting tensor. Default\n+            is backend's float type.\n+\n+    Returns:\n+        Tensor: The multi-hot encoded tensor.\n+\n+\n+    Example:\n+\n+    >>> data = keras_core.ops.convert_to_tensor([0, 4])\n+    >>> keras_core.ops.multi_hot(data, num_tokens=5)\n+    array([1.0, 0.0, 0.0, 0.0, 1.0], dtype=float32)\n+\n+    \"\"\"\n     if any_symbolic_tensors((inputs,)):\n         return MultiHot(num_tokens, axis, dtype).symbolic_call(inputs)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f2b46e46b29155e9a2e3232c59dfb21e3d78042f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 5 | Churn Cumulative: 1265 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -30,15 +30,14 @@ def weight_memory_size(weights):\n     Returns:\n         The total memory size (in Bytes) of the weights.\n     \"\"\"\n-    unique_weights_ids = set(id(w) for w in weights)\n-    unique_weights = [w for w in weights if id(w) in unique_weights_ids]\n+    unique_weights = {id(w): w for w in weights}.values()\n     total_memory_size = 0\n     for w in unique_weights:\n         weight_shape = math.prod(w.shape)\n         dtype = backend.standardize_dtype(w.dtype)\n         per_param_size = dtype_utils.dtype_size(dtype)\n         total_memory_size += weight_shape * per_param_size\n-    return total_memory_size\n+    return total_memory_size / 8\n \n \n def readable_memory_size(weight_memory_size):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
