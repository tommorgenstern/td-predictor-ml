{"custom_id": "keras#01d5e7bc4782daafcfa99e035c1bdbe13a985145", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 607 | Contributors (this commit): 13 | Commits (past 90d): 9 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,9 +22,9 @@ maxlen = 100\n embedding_size = 128\n \n # Convolution\n-filter_length = 3\n+filter_length = 5\n nb_filter = 64\n-pool_length = 2\n+pool_length = 4\n \n # LSTM\n lstm_output_size = 70\n\n@@ -14,6 +14,7 @@ from __future__ import print_function\n from keras.models import Sequential\n from keras.layers import Dense, Activation, Dropout\n from keras.layers import LSTM\n+from keras.optimizers import RMSprop\n from keras.utils.data_utils import get_file\n import numpy as np\n import random\n@@ -50,20 +51,22 @@ for i, sentence in enumerate(sentences):\n # build the model: 2 stacked LSTM\n print('Build model...')\n model = Sequential()\n-model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))\n-model.add(LSTM(512, return_sequences=False))\n-model.add(Dropout(0.2))\n+model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n model.add(Dense(len(chars)))\n model.add(Activation('softmax'))\n \n-model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+optimizer = RMSprop(lr=0.01)\n+model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n \n \n-def sample(a, temperature=1.0):\n+def sample(preds, temperature=1.0):\n     # helper function to sample an index from a probability array\n-    a = np.log(a) / temperature\n-    a = np.exp(a) / np.sum(np.exp(a))\n-    return np.argmax(np.random.multinomial(1, a, 1))\n+    preds = np.asarray(preds).astype('float64')\n+    preds = np.log(preds) / temperature\n+    exp_preds = np.exp(preds)\n+    preds = exp_preds / np.sum(exp_preds)\n+    probas = np.random.multinomial(1, preds, 1)\n+    return np.argmax(probas)\n \n # train the model, output generated text after each iteration\n for iteration in range(1, 60):\n\n@@ -99,6 +99,7 @@ class Tokenizer(object):\n         wcounts = list(self.word_counts.items())\n         wcounts.sort(key=lambda x: x[1], reverse=True)\n         sorted_voc = [wc[0] for wc in wcounts]\n+        # note that index 0 is reserved, never assigned to an existing word\n         self.word_index = dict(list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))\n \n         self.index_docs = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d5289141eb183df209391c8f63a7d8fd9281b3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 137 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 137 | Churn Cumulative: 137 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,137 @@\n+'''This script loads pre-trained word embeddings (GloVe embeddings)\n+into a frozen Keras Embedding layer, and uses it to\n+train a text classification model on the 20 Newsgroup dataset\n+(classication of newsgroup messages into 20 different categories).\n+\n+GloVe embedding data can be found at:\n+http://nlp.stanford.edu/data/glove.6B.zip\n+(source page: http://nlp.stanford.edu/projects/glove/)\n+\n+20 Newsgroup data can be found at:\n+http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n+'''\n+\n+from __future__ import print_function\n+import os\n+import numpy as np\n+np.random.seed(1337)\n+\n+from keras.preprocessing.text import Tokenizer\n+from keras.preprocessing.sequence import pad_sequences\n+from keras.utils.np_utils import to_categorical\n+from keras.layers import Dense, Input, Flatten\n+from keras.layers import Conv1D, MaxPooling1D, Embedding\n+from keras.models import Model\n+\n+BASE_DIR = ''\n+GLOVE_DIR = BASE_DIR + '/glove.6B/'\n+TEXT_DATA_DIR = BASE_DIR + '/20_newsgroup/'\n+MAX_SEQUENCE_LENGTH = 1000\n+MAX_NB_WORDS = 20000\n+EMBEDDING_DIM = 100\n+VALIDATION_SPLIT = 0.2\n+\n+# first, build index mapping words in the embeddings set\n+# to their embedding vector\n+\n+print('Indexing word vectors.')\n+\n+embeddings_index = {}\n+f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n+for line in f:\n+    values = line.split()\n+    word = values[0]\n+    coefs = np.asarray(values[1:], dtype='float32')\n+    embeddings_index[word] = coefs\n+f.close()\n+\n+print('Found %s word vectors.' % len(embeddings_index))\n+\n+# second, prepare text samples and their labels\n+print('Processing text dataset')\n+\n+texts = []  # list of text samples\n+labels_index = {}  # dictionary mapping label name to numeric id\n+labels = []  # list of label ids\n+for name in sorted(os.listdir(TEXT_DATA_DIR)):\n+    path = os.path.join(TEXT_DATA_DIR, name)\n+    if os.path.isdir(path):\n+        label_id = len(labels_index)\n+        labels_index[name] = label_id\n+        for fname in sorted(os.listdir(path)):\n+            if fname.isdigit():\n+                fpath = os.path.join(path, fname)\n+                f = open(fpath)\n+                texts.append(f.read())\n+                f.close()\n+                labels.append(label_id)\n+\n+print('Found %s texts.' % len(texts))\n+\n+# finally, vectorize the text samples into a 2D integer tensor\n+tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n+tokenizer.fit_on_texts(texts)\n+sequences = tokenizer.texts_to_sequences(texts)\n+\n+word_index = tokenizer.word_index\n+print('Found %s unique tokens.' % len(word_index))\n+\n+data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n+\n+labels = to_categorical(np.asarray(labels))\n+print('Shape of data tensor:', data.shape)\n+print('Shape of label tensor:', labels.shape)\n+\n+# split the data into a training set and a validation set\n+indices = np.arange(data.shape[0])\n+np.random.shuffle(indices)\n+data = data[indices]\n+labels = labels[indices]\n+nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n+\n+x_train = data[:-nb_validation_samples]\n+y_train = labels[:-nb_validation_samples]\n+x_val = data[-nb_validation_samples:]\n+y_val = labels[-nb_validation_samples:]\n+\n+print('Preparing embedding matrix.')\n+\n+# prepare embedding matrix\n+embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n+for word, i in word_index.items():\n+    embedding_vector = embeddings_index.get(word)\n+    if embedding_vector is not None:\n+        # words not found in embedding index will be all-zeros.\n+        embedding_matrix[i] = embedding_vector\n+\n+# load pre-trained word embeddings into an Embedding layer\n+# note that we set trainable = False so as to keep the embeddings fixed\n+embedding_layer = Embedding(len(word_index) + 1,\n+                            EMBEDDING_DIM,\n+                            weights=[embedding_matrix],\n+                            input_length=MAX_SEQUENCE_LENGTH,\n+                            trainable=False)\n+\n+print('Training model.')\n+\n+# train a 1D convnet with global maxpooling\n+sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n+embedded_sequences = embedding_layer(sequence_input)\n+x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n+x = MaxPooling1D(5)(x)\n+x = Conv1D(128, 5, activation='relu')(x)\n+x = MaxPooling1D(5)(x)\n+x = Conv1D(128, 5, activation='relu')(x)\n+x = MaxPooling1D(35)(x)\n+x = Flatten()(x)\n+x = Dense(128, activation='relu')(x)\n+preds = Dense(len(labels_index), activation='softmax')(x)\n+\n+model = Model(sequence_input, preds)\n+model.compile(loss='categorical_crossentropy',\n+              optimizer='rmsprop',\n+              metrics=['acc'])\n+\n+# happy learning!\n+model.fit(x_train, y_train, validation_data=(x_val, y_val),\n+          nb_epoch=2, batch_size=128)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b686b85b52c95f227b18bb9d6301b443a6168140", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2067 | Contributors (this commit): 24 | Commits (past 90d): 25 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -652,7 +652,7 @@ def batch_flatten(x):\n     '''Turn a n-D tensor into a 2D tensor where\n     the first dimension is conserved.\n     '''\n-    x = tf.reshape(x, [-1, np.prod(x.get_shape()[1:].as_list())])\n+    x = tf.reshape(x, [-1, prod(shape(x)[1:])])\n     return x\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1b158ff4ed9d4ca88d659d50f89a1b248da50ead", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 8529 | Contributors (this commit): 48 | Commits (past 90d): 14 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -172,6 +172,9 @@ class Sequential(Model):\n         else:\n             self.layers[-1].outbound_nodes = []\n             self.outputs = [self.layers[-1].output]\n+            # update self.inbound_nodes\n+            self.inbound_nodes[0].output_tensors = self.outputs\n+            self.inbound_nodes[0].output_shapes = [self.outputs[0]._keras_shape]\n         self.built = False\n \n     def call(self, x, mask=None):\n\n@@ -22,6 +22,22 @@ batch_size = 32\n nb_epoch = 1\n \n \n+def test_sequential_pop():\n+    model = Sequential()\n+    model.add(Dense(nb_hidden, input_dim=input_dim))\n+    model.add(Dense(nb_class))\n+    model.compile(loss='mse', optimizer='sgd')\n+    x = np.random.random((batch_size, input_dim))\n+    y = np.random.random((batch_size, nb_class))\n+    model.fit(x, y)\n+    model.pop()\n+    assert len(model.layers) == 1\n+    assert model.output_shape == (None, nb_hidden)\n+    model.compile(loss='mse', optimizer='sgd')\n+    y = np.random.random((batch_size, nb_hidden))\n+    model.fit(x, y)\n+\n+\n def _get_test_data():\n     np.random.seed(1234)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f7300b8cbd9511277732f912aeffe18033333d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1611 | Contributors (this commit): 22 | Commits (past 90d): 10 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -212,6 +212,7 @@ class History(Callback):\n         for k, v in logs.items():\n             self.history.setdefault(k, []).append(v)\n \n+\n class ModelCheckpoint(Callback):\n     '''Save the model after every epoch.\n \n@@ -319,7 +320,8 @@ class EarlyStopping(Callback):\n \n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '\n-                          'fallback to auto mode.' % (self.mode), RuntimeWarning)\n+                          'fallback to auto mode.' % (self.mode),\n+                          RuntimeWarning)\n             mode = 'auto'\n \n         if mode == 'min':\n@@ -433,8 +435,9 @@ class TensorBoard(Callback):\n         histogram_freq: frequency (in epochs) at which to compute activation\n             histograms for the layers of the model. If set to 0,\n             histograms won't be computed.\n-        write_graph: whether to visualize the graph in Tensorboard. The log file can\n-            become quite large when write_graph is set to True.\n+        write_graph: whether to visualize the graph in Tensorboard.\n+            The log file can become quite large when\n+            write_graph is set to True.\n     '''\n \n     def __init__(self, log_dir='./logs', histogram_freq=0, write_graph=True):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#40195c2fa276cf9c7d2125e525133dd83b25e23d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 20 | Churn Cumulative: 2188 | Contributors (this commit): 26 | Commits (past 90d): 29 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -64,3 +64,8 @@ def set_image_dim_ordering(dim_ordering):\n def get_uid(prefix=''):\n     _UID_PREFIXES[prefix] += 1\n     return _UID_PREFIXES[prefix]\n+\n+\n+def reset_uids():\n+    global _UID_PREFIXES\n+    _UID_PREFIXES = defaultdict(int)\n\n@@ -1,9 +1,10 @@\n import tensorflow as tf\n+from tensorflow.python.framework.ops import reset_default_graph\n import numpy as np\n import os\n import copy\n import warnings\n-from .common import _FLOATX, _EPSILON, _IMAGE_DIM_ORDERING\n+from .common import _FLOATX, _EPSILON, _IMAGE_DIM_ORDERING, reset_uids\n \n # INTERNAL UTILS\n \n@@ -12,6 +13,13 @@ _LEARNING_PHASE = tf.placeholder(dtype='uint8', name='keras_learning_phase')  #\n _MANUAL_VAR_INIT = False\n \n \n+def clear_session():\n+    global _SESSION\n+    reset_default_graph()\n+    reset_uids()\n+    _SESSION = None\n+\n+\n def manual_variable_initialization(value):\n     '''Whether variables should be initialized\n     as they are instantiated (default), or if\n@@ -744,14 +752,15 @@ class Function(object):\n         self.inputs = list(inputs)\n         self.outputs = list(outputs)\n         with tf.control_dependencies(self.outputs):\n-            self.updates = [tf.assign(p, new_p) for (p, new_p) in updates]\n+            updates = [tf.assign(p, new_p) for (p, new_p) in updates]\n+            self.updates_op = tf.group(*updates)\n \n     def __call__(self, inputs):\n         assert type(inputs) in {list, tuple}\n         names = [v.name for v in self.inputs]\n         feed_dict = dict(zip(names, inputs))\n         session = get_session()\n-        updated = session.run(self.outputs + self.updates, feed_dict=feed_dict)\n+        updated = session.run(self.outputs + [self.updates_op], feed_dict=feed_dict)\n         return updated[:len(self.outputs)]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff3f00d845addec5cc5189471978a6393211b589", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 248 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -52,8 +52,6 @@ model.compile(loss='binary_crossentropy',\n               metrics=['accuracy'])\n \n print('Train...')\n-print(X_train.shape)\n-print(y_train.shape)\n model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n           validation_data=(X_test, y_test))\n score, acc = model.evaluate(X_test, y_test,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc0e60c1012b7c72eeb5ea0c41b8a2045177ae5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 57 | Churn Cumulative: 2880 | Contributors (this commit): 26 | Commits (past 90d): 33 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -97,6 +97,13 @@ def _convert_string_dtype(dtype):\n         raise ValueError('Unsupported dtype:', dtype)\n \n \n+def _to_tensor(x, dtype):\n+    x = tf.python.framework.ops.convert_to_tensor(x)\n+    if x.dtype != dtype:\n+        x = tf.cast(x, dtype)\n+    return x\n+\n+\n def variable(value, dtype=_FLOATX, name=None):\n     '''Instantiates a tensor.\n \n@@ -446,8 +453,9 @@ def abs(x):\n def sqrt(x):\n     '''Element-wise square root.\n     '''\n-    x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n-                         tf.cast(np.inf, dtype=_FLOATX))\n+    zero = _to_tensor(0., x.dtype.base_dtype)\n+    inf = _to_tensor(np.inf, x.dtype.base_dtype)\n+    x = tf.clip_by_value(x, zero, inf)\n     return tf.sqrt(x)\n \n \n@@ -486,8 +494,9 @@ def clip(x, min_value, max_value):\n     '''\n     if max_value < min_value:\n         max_value = min_value\n-    return tf.clip_by_value(x, tf.cast(min_value, dtype=_FLOATX),\n-                            tf.cast(max_value, dtype=_FLOATX))\n+    min_value = _to_tensor(min_value, x.dtype.base_dtype)\n+    max_value = _to_tensor(max_value, x.dtype.base_dtype)\n+    return tf.clip_by_value(x, min_value, max_value)\n \n \n def equal(x, y):\n@@ -752,8 +761,15 @@ class Function(object):\n         self.inputs = list(inputs)\n         self.outputs = list(outputs)\n         with tf.control_dependencies(self.outputs):\n-            updates = [tf.assign(p, new_p) for (p, new_p) in updates]\n-            self.updates_op = tf.group(*updates)\n+            updates_ops = []\n+            for update in updates:\n+                if type(update) is tuple:\n+                    p, new_p = update\n+                    updates_ops.append(tf.assign(p, new_p))\n+                else:\n+                    # assumed already an op\n+                    updates_ops.append(update)\n+            self.updates_op = tf.group(*updates_ops)\n \n     def __call__(self, inputs):\n         assert type(inputs) in {list, tuple}\n@@ -958,13 +974,15 @@ def relu(x, alpha=0., max_value=None):\n         alpha: slope of negative section.\n         max_value: saturation threshold.\n     '''\n+    if alpha != 0.:\n         negative_part = tf.nn.relu(-x)\n     x = tf.nn.relu(x)\n     if max_value is not None:\n-        x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n-                             tf.cast(max_value, dtype=_FLOATX))\n-    if isinstance(alpha, (tuple, list, np.ndarray)) or np.isscalar(alpha):\n-        alpha = tf.constant(alpha, dtype=_FLOATX)\n+        max_value = _to_tensor(max_value, x.dtype.base_dtype)\n+        zero = _to_tensor(0., x.dtype.base_dtype)\n+        x = tf.clip_by_value(x, zero, max_value)\n+    if alpha != 0.:\n+        alpha = _to_tensor(alpha, x.dtype.base_dtype)\n         x -= alpha * negative_part\n     return x\n \n@@ -998,8 +1016,8 @@ def categorical_crossentropy(output, target, from_logits=False):\n                                 reduction_indices=len(output.get_shape()) - 1,\n                                 keep_dims=True)\n         # manual computation of crossentropy\n-        output = tf.clip_by_value(output, tf.cast(_EPSILON, dtype=_FLOATX),\n-                                  tf.cast(1. - _EPSILON, dtype=_FLOATX))\n+        epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n+        output = tf.clip_by_value(output, epsilon, 1. - epsilon)\n         return - tf.reduce_sum(target * tf.log(output),\n                                reduction_indices=len(output.get_shape()) - 1)\n     else:\n@@ -1013,8 +1031,8 @@ def sparse_categorical_crossentropy(output, target, from_logits=False):\n     # Note: tf.nn.softmax_cross_entropy_with_logits\n     # expects logits, Keras expects probabilities.\n     if not from_logits:\n-        output = tf.clip_by_value(output, tf.cast(_EPSILON, dtype=_FLOATX),\n-                                  tf.cast(1.-_EPSILON, dtype=_FLOATX))\n+        epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n+        output = tf.clip_by_value(output, epsilon, 1 - epsilon)\n         output = tf.log(output)\n \n     output_shape = output.get_shape()\n@@ -1035,8 +1053,8 @@ def binary_crossentropy(output, target, from_logits=False):\n     # expects logits, Keras expects probabilities.\n     if not from_logits:\n         # transform back to logits\n-        output = tf.clip_by_value(output, tf.cast(_EPSILON, dtype=_FLOATX),\n-                                  tf.cast(1.-_EPSILON, dtype=_FLOATX))\n+        epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n+        output = tf.clip_by_value(output, epsilon, 1 - epsilon)\n         output = tf.log(output / (1 - output))\n     return tf.nn.sigmoid_cross_entropy_with_logits(output, target)\n \n@@ -1052,8 +1070,9 @@ def hard_sigmoid(x):\n     Faster than sigmoid.\n     '''\n     x = (0.2 * x) + 0.5\n-    x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n-                         tf.cast(1., dtype=_FLOATX))\n+    zero = _to_tensor(0., x.dtype.base_dtype)\n+    one = _to_tensor(1., x.dtype.base_dtype)\n+    x = tf.clip_by_value(x, zero, one)\n     return x\n \n \n\n@@ -38,6 +38,7 @@ def check_two_tensor_operation(function_name, x_input_shape,\n     assert zth.shape == ztf.shape\n     assert_allclose(zth, ztf, atol=1e-05)\n \n+\n def check_composed_tensor_operations(first_function_name, first_function_args,\n                                      second_function_name, second_function_args,\n                                      input_shape):\n@@ -59,6 +60,7 @@ def check_composed_tensor_operations(first_function_name, first_function_args,\n     assert zth.shape == ztf.shape\n     assert_allclose(zth, ztf, atol=1e-05)\n \n+\n class TestBackend(object):\n \n     def test_linear_operations(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#946ccd32285ed762743fafce486e7f13d6d4f012", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 137 | Lines Deleted: 59 | Files Changed: 18 | Hunks: 134 | Methods Changed: 16 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 196 | Churn Cumulative: 10878 | Contributors (this commit): 48 | Commits (past 90d): 89 | Contributors (cumulative): 93 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,9 +15,11 @@ _MANUAL_VAR_INIT = False\n \n def clear_session():\n     global _SESSION\n+    global _LEARNING_PHASE\n     reset_default_graph()\n     reset_uids()\n     _SESSION = None\n+    _LEARNING_PHASE = tf.placeholder(dtype='uint8', name='keras_learning_phase')\n \n \n def manual_variable_initialization(value):\n\n@@ -418,7 +418,6 @@ def generator_queue(generator, max_q_size=10,\n         _stop = threading.Event()\n \n     try:\n-\n         def data_generator_task():\n             while not _stop.is_set():\n                 try:\n@@ -444,7 +443,6 @@ def generator_queue(generator, max_q_size=10,\n             generator_threads.append(thread)\n             thread.daemon = True\n             thread.start()\n-\n     except:\n         _stop.set()\n         if pickle_safe:\n\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n from ..engine import Layer\n from .. import backend as K\n+import numpy as np\n \n \n class GaussianNoise(Layer):\n@@ -71,7 +72,7 @@ class GaussianDropout(Layer):\n     def call(self, x, mask=None):\n         if 0 < self.p < 1:\n             noise_x = x * K.random_normal(shape=K.shape(x), mean=1.0,\n-                                          std=K.sqrt(self.p / (1.0 - self.p)))\n+                                          std=np.sqrt(self.p / (1.0 - self.p)))\n             return K.in_train_phase(noise_x, x)\n         return x\n \n\n@@ -102,3 +102,14 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n \n     # for further checks in the caller function\n     return actual_output\n+\n+\n+def keras_test(func):\n+    '''Clean up after tensorflow tests.\n+    '''\n+    def wrapper(*args, **kwargs):\n+        output = func(*args, **kwargs)\n+        if K._BACKEND == 'tensorflow':\n+            K.clear_session()\n+        return output\n+    return wrapper\n\n@@ -7,8 +7,10 @@ from keras.engine import merge, Input, get_source_inputs\n from keras.models import Model\n from keras import backend as K\n from keras.models import model_from_json, model_from_yaml\n+from keras.utils.test_utils import keras_test\n \n \n+@keras_test\n def test_learning_phase():\n     a = Input(shape=(32,), name='input_a')\n     b = Input(shape=(32,), name='input_b')\n@@ -50,6 +52,7 @@ def test_learning_phase():\n     assert fn_outputs_no_dp[1].sum() != fn_outputs_dp[1].sum()\n \n \n+@keras_test\n def test_node_construction():\n     ####################################################\n     # test basics\n@@ -128,6 +131,7 @@ def test_node_construction():\n     assert dense.get_output_mask_at(1) is None\n \n \n+@keras_test\n def test_multi_input_layer():\n     ####################################################\n     # test multi-input layer\n@@ -209,6 +213,7 @@ def test_multi_input_layer():\n     assert [x.shape for x in fn_outputs] == [(10, 64), (10, 5)]\n \n \n+@keras_test\n def test_recursion():\n     ####################################################\n     # test recursion\n@@ -400,6 +405,7 @@ def test_recursion():\n         y = Dense(2)(x)\n \n \n+@keras_test\n def test_functional_guide():\n     # MNIST\n     from keras.layers import Input, Dense, LSTM\n@@ -492,6 +498,7 @@ def test_functional_guide():\n     assert shared_lstm.input_shape == (None, 4, 25)\n \n \n+@keras_test\n def test_sequential_regression():\n     from keras.models import Sequential, Model\n     from keras.layers import Merge, Embedding, BatchNormalization, LSTM, InputLayer, Input\n\n@@ -7,8 +7,10 @@ from keras.engine.topology import merge, Input\n from keras.engine.training import Model\n from keras.models import Sequential, Graph\n from keras import backend as K\n+from keras.utils.test_utils import keras_test\n \n \n+@keras_test\n def test_model_methods():\n     a = Input(shape=(3,), name='input_a')\n     b = Input(shape=(3,), name='input_b')\n@@ -167,6 +169,7 @@ def test_model_methods():\n     out = model.predict([input_a_np, input_b_np], batch_size=4)\n \n \n+@keras_test\n def test_trainable_argument():\n     x = np.random.random((5, 3))\n     y = np.random.random((5, 2))\n\n@@ -1,7 +1,8 @@\n import pytest\n-from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import layer_test, keras_test\n \n \n+@keras_test\n def test_leaky_relu():\n     from keras.layers.advanced_activations import LeakyReLU\n     for alpha in [0., .5, -1.]:\n@@ -9,12 +10,14 @@ def test_leaky_relu():\n                    input_shape=(2, 3, 4))\n \n \n+@keras_test\n def test_prelu():\n     from keras.layers.advanced_activations import PReLU\n     layer_test(PReLU, kwargs={},\n                input_shape=(2, 3, 4))\n \n \n+@keras_test\n def test_elu():\n     from keras.layers.advanced_activations import ELU\n     for alpha in [0., .5, -1.]:\n@@ -22,6 +25,7 @@ def test_elu():\n                    input_shape=(2, 3, 4))\n \n \n+@keras_test\n def test_parametric_softplus():\n     from keras.layers.advanced_activations import ParametricSoftplus\n     for alpha in [0., .5, -1.]:\n@@ -31,12 +35,14 @@ def test_parametric_softplus():\n                    input_shape=(2, 3, 4))\n \n \n+@keras_test\n def test_thresholded_relu():\n     from keras.layers.advanced_activations import ThresholdedReLU\n     layer_test(ThresholdedReLU, kwargs={'theta': 0.5},\n                input_shape=(2, 3, 4))\n \n \n+@keras_test\n def test_srelu():\n     from keras.layers.advanced_activations import SReLU\n     layer_test(SReLU, kwargs={},\n\n@@ -2,11 +2,12 @@ import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n \n-from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import layer_test, keras_test\n from keras import backend as K\n from keras.layers import convolutional\n \n \n+@keras_test\n def test_convolution_1d():\n     nb_samples = 2\n     nb_steps = 8\n@@ -36,6 +37,7 @@ def test_convolution_1d():\n                        input_shape=(nb_samples, nb_steps, input_dim))\n \n \n+@keras_test\n def test_maxpooling_1d():\n     for stride in [1, 2]:\n         layer_test(convolutional.MaxPooling1D,\n@@ -44,6 +46,7 @@ def test_maxpooling_1d():\n                    input_shape=(3, 5, 4))\n \n \n+@keras_test\n def test_averagepooling_1d():\n     for stride in [1, 2]:\n         layer_test(convolutional.AveragePooling1D,\n@@ -52,6 +55,7 @@ def test_averagepooling_1d():\n                    input_shape=(3, 5, 4))\n \n \n+@keras_test\n def test_convolution_2d():\n     nb_samples = 2\n     nb_filter = 3\n@@ -84,6 +88,7 @@ def test_convolution_2d():\n                        input_shape=(nb_samples, stack_size, nb_row, nb_col))\n \n \n+@keras_test\n def test_atrous_conv_2d():\n     nb_samples = 2\n     nb_filter = 3\n@@ -121,6 +126,7 @@ def test_atrous_conv_2d():\n                            input_shape=(nb_samples, stack_size, nb_row, nb_col))\n \n \n+@keras_test\n @pytest.mark.skipif(K._BACKEND != 'tensorflow', reason=\"Requires TF backend\")\n def test_separable_conv_2d():\n     nb_samples = 2\n@@ -160,6 +166,7 @@ def test_separable_conv_2d():\n                            input_shape=(nb_samples, stack_size, nb_row, nb_col))\n \n \n+@keras_test\n def test_maxpooling_2d():\n     pool_size = (3, 3)\n \n@@ -171,6 +178,7 @@ def test_maxpooling_2d():\n                    input_shape=(3, 4, 11, 12))\n \n \n+@keras_test\n def test_averagepooling_2d():\n     pool_size = (3, 3)\n \n@@ -184,6 +192,7 @@ def test_averagepooling_2d():\n                            input_shape=(3, 4, 11, 12))\n \n \n+@keras_test\n def test_convolution_3d():\n     nb_samples = 2\n     nb_filter = 5\n@@ -225,6 +234,7 @@ def test_convolution_3d():\n                                     input_len_dim1, input_len_dim2, input_len_dim3))\n \n \n+@keras_test\n def test_maxpooling_3d():\n     pool_size = (3, 3, 3)\n \n@@ -236,6 +246,7 @@ def test_maxpooling_3d():\n                    input_shape=(3, 4, 11, 12, 10))\n \n \n+@keras_test\n def test_averagepooling_3d():\n     pool_size = (3, 3, 3)\n \n@@ -247,6 +258,7 @@ def test_averagepooling_3d():\n                    input_shape=(3, 4, 11, 12, 10))\n \n \n+@keras_test\n def test_zero_padding_2d():\n     nb_samples = 2\n     stack_size = 7\n@@ -300,12 +312,14 @@ def test_zero_padding_3d():\n     layer.get_config()\n \n \n+@keras_test\n def test_upsampling_1d():\n     layer_test(convolutional.UpSampling1D,\n                kwargs={'length': 2},\n                input_shape=(3, 5, 4))\n \n \n+@keras_test\n def test_upsampling_2d():\n     nb_samples = 2\n     stack_size = 7\n\n@@ -3,15 +3,17 @@ import numpy as np\n \n from keras import backend as K\n from keras.layers import core\n-from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import layer_test, keras_test\n \n \n+@keras_test\n def test_masking():\n     layer_test(core.Masking,\n                kwargs={},\n                input_shape=(3, 2, 3))\n \n \n+@keras_test\n def test_merge():\n     from keras.layers import Input, merge, Merge\n     from keras.models import Model\n@@ -83,6 +85,7 @@ def test_merge():\n     model.compile('rmsprop', 'mse')\n \n \n+@keras_test\n def test_merge_mask_2d():\n     from keras.layers import Input, merge, Masking\n     from keras.models import Model\n@@ -112,6 +115,7 @@ def test_merge_mask_2d():\n     model_concat.fit([rand(2, 3), rand(2, 3)], [rand(2, 6)], nb_epoch=1)\n \n \n+@keras_test\n def test_merge_mask_3d():\n     from keras.layers import Input, merge, Embedding, SimpleRNN\n     from keras.models import Model\n@@ -137,12 +141,14 @@ def test_merge_mask_3d():\n     model.fit([rand(2, 3), rand(2, 3)], [rand(2, 3, 6)])\n \n \n+@keras_test\n def test_dropout():\n     layer_test(core.Dropout,\n                kwargs={'p': 0.5},\n                input_shape=(3, 2))\n \n \n+@keras_test\n def test_activation():\n     # with string argument\n     layer_test(core.Activation,\n@@ -155,30 +161,35 @@ def test_activation():\n                input_shape=(3, 2))\n \n \n+@keras_test\n def test_reshape():\n     layer_test(core.Reshape,\n                kwargs={'target_shape': (8, 1)},\n                input_shape=(3, 2, 4))\n \n \n+@keras_test\n def test_permute():\n     layer_test(core.Permute,\n                kwargs={'dims': (2, 1)},\n                input_shape=(3, 2, 4))\n \n \n+@keras_test\n def test_flatten():\n     layer_test(core.Flatten,\n                kwargs={},\n                input_shape=(3, 2, 4))\n \n \n+@keras_test\n def test_repeat_vector():\n     layer_test(core.RepeatVector,\n                kwargs={'n': 3},\n                input_shape=(3, 2))\n \n \n+@keras_test\n def test_lambda():\n     from keras.utils.layer_utils import layer_from_config\n     Lambda = core.Lambda\n@@ -212,6 +223,7 @@ def test_lambda():\n     ld = layer_from_config({'class_name': 'Lambda', 'config': config})\n \n \n+@keras_test\n def test_dense():\n     from keras import regularizers\n     from keras import constraints\n@@ -230,6 +242,7 @@ def test_dense():\n                input_shape=(3, 2))\n \n \n+@keras_test\n def test_activity_regularization():\n     from keras.engine import Input, Model\n \n@@ -250,6 +263,7 @@ def test_activity_regularization():\n     model.compile('rmsprop', 'mse')\n \n \n+@keras_test\n def test_maxout_dense():\n     from keras import regularizers\n     from keras import constraints\n@@ -268,6 +282,7 @@ def test_maxout_dense():\n                input_shape=(3, 2))\n \n \n+@keras_test\n def test_highway():\n     from keras import regularizers\n     from keras import constraints\n@@ -285,6 +300,7 @@ def test_highway():\n                input_shape=(3, 2))\n \n \n+@keras_test\n def test_timedistributeddense():\n     from keras import regularizers\n     from keras import constraints\n\n@@ -1,9 +1,10 @@\n import pytest\n-from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import layer_test, keras_test\n from keras.layers.embeddings import Embedding\n import keras.backend as K\n \n \n+@keras_test\n def test_embedding():\n     layer_test(Embedding,\n                kwargs={'output_dim': 4., 'input_dim': 10, 'input_length': 2},\n\n@@ -1,9 +1,10 @@\n import pytest\n \n-from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import layer_test, keras_test\n from keras.layers import local\n \n \n+@keras_test\n def test_locallyconnected_1d():\n     nb_samples = 2\n     nb_steps = 8\n@@ -33,6 +34,7 @@ def test_locallyconnected_1d():\n                        input_shape=(nb_samples, nb_steps, input_dim))\n \n \n+@keras_test\n def test_locallyconnected_2d():\n     nb_samples = 8\n     nb_filter = 3\n\n@@ -1,14 +1,16 @@\n import pytest\n-from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import layer_test, keras_test\n from keras.layers import noise\n \n \n+@keras_test\n def test_GaussianNoise():\n     layer_test(noise.GaussianNoise,\n                kwargs={'sigma': 1.},\n                input_shape=(3, 2, 3))\n \n \n+@keras_test\n def test_GaussianDropout():\n     layer_test(noise.GaussianDropout,\n                kwargs={'p': 0.5},\n\n@@ -3,18 +3,18 @@ import numpy as np\n from numpy.testing import assert_allclose\n \n from keras.layers.core import Dense, Activation\n-from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import layer_test, keras_test\n from keras.layers import normalization\n from keras.models import Sequential, Graph\n from keras import backend as K\n \n-\n input_1 = np.arange(10)\n input_2 = np.zeros(10)\n input_3 = np.ones((10))\n input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n \n \n+@keras_test\n def basic_batchnorm_test():\n     layer_test(normalization.BatchNormalization,\n                kwargs={'mode': 1},\n@@ -24,6 +24,7 @@ def basic_batchnorm_test():\n                input_shape=(3, 4, 2))\n \n \n+@keras_test\n def test_batchnorm_mode_0_or_2():\n     for mode in [0, 2]:\n         model = Sequential()\n@@ -42,6 +43,7 @@ def test_batchnorm_mode_0_or_2():\n         assert_allclose(out.std(), 1.0, atol=1e-1)\n \n \n+@keras_test\n def test_batchnorm_mode_0_convnet():\n     model = Sequential()\n     norm_m0 = normalization.BatchNormalization(mode=0, axis=1, input_shape=(3, 4, 4))\n@@ -59,6 +61,7 @@ def test_batchnorm_mode_0_convnet():\n     assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)\n \n \n+@keras_test\n def test_batchnorm_mode_1():\n     norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1)\n     norm_m1.build(input_shape=(None, 10))\n\n@@ -7,6 +7,7 @@ from keras.layers import recurrent, embeddings\n from keras.models import Sequential\n from keras.layers.core import Masking\n from keras import regularizers\n+from keras.utils.test_utils import keras_test\n \n from keras import backend as K\n \n@@ -107,18 +108,22 @@ def _runner(layer_class):\n     K.eval(layer.output)\n \n \n+@keras_test\n def test_SimpleRNN():\n     _runner(recurrent.SimpleRNN)\n \n \n+@keras_test\n def test_GRU():\n     _runner(recurrent.GRU)\n \n \n+@keras_test\n def test_LSTM():\n     _runner(recurrent.LSTM)\n \n \n+@keras_test\n def test_masking_layer():\n     ''' This test based on a previously failing issue here:\n     https://github.com/fchollet/keras/issues/1567\n\n@@ -1,12 +1,13 @@\n import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n-\n+from keras.utils.test_utils import keras_test\n from keras.layers import wrappers, Input\n from keras.layers import core, convolutional\n from keras.models import Sequential, Model, model_from_json\n \n \n+@keras_test\n def test_TimeDistributed():\n     # first, test with Dense layer\n     model = Sequential()\n\n@@ -10,6 +10,7 @@ from keras.models import Graph, Sequential\n from keras.layers.core import Dense, Activation, Merge, Lambda\n from keras.utils.test_utils import get_test_data\n from keras.models import model_from_json, model_from_yaml\n+from keras.utils.test_utils import keras_test\n \n \n batch_size = 32\n@@ -26,6 +27,7 @@ batch_size = 32\n                                                                                  output_shape=(1,))\n \n \n+@keras_test\n def test_graph_fit_generator():\n     def data_generator_graph(train):\n         while 1:\n@@ -69,6 +71,7 @@ def test_graph_fit_generator():\n     gen_loss = graph.evaluate_generator(data_generator_graph(True), 128, verbose=0)\n \n \n+@keras_test\n def test_1o_1i():\n     # test a non-sequential graph with 1 input and 1 output\n     np.random.seed(1337)\n@@ -112,6 +115,7 @@ def test_1o_1i():\n               nb_epoch=1)\n \n \n+@keras_test\n def test_1o_1i_2():\n     # test a more complex non-sequential graph with 1 input and 1 output\n     graph = Graph()\n@@ -151,6 +155,7 @@ def test_1o_1i_2():\n     new_graph = model_from_yaml(yaml_str)\n \n \n+@keras_test\n def test_1o_2i():\n     # test a non-sequential graph with 2 inputs and 1 output\n     graph = Graph()\n@@ -187,6 +192,7 @@ def test_1o_2i():\n     new_graph = model_from_yaml(yaml_str)\n \n \n+@keras_test\n def test_siamese_1():\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n@@ -274,6 +280,7 @@ since the Graph model is deprecated.\n #     new_graph = model_from_yaml(yaml_str)\n \n \n+@keras_test\n def test_2o_1i_save_weights():\n     # test a non-sequential graph with 1 input and 2 outputs\n     graph = Graph()\n@@ -321,6 +328,7 @@ def test_2o_1i_save_weights():\n               nb_epoch=1)\n \n \n+@keras_test\n def test_2o_1i_sample_weights():\n     # test a non-sequential graph with 1 input and 2 outputs with sample weights\n     graph = Graph()\n@@ -354,6 +362,7 @@ def test_2o_1i_sample_weights():\n                           sample_weight={'output1': weights1, 'output2': weights2})\n \n \n+@keras_test\n def test_recursive():\n     # test layer-like API\n     graph = Graph()\n@@ -386,6 +395,7 @@ def test_recursive():\n     new_graph = model_from_yaml(yaml_str)\n \n \n+@keras_test\n def test_create_output():\n     # test create_output argument\n     graph = Graph()\n@@ -416,6 +426,7 @@ def test_create_output():\n     out = graph.predict({'input1': X_test_graph})\n \n \n+@keras_test\n def test_count_params():\n     # test count params\n     nb_units = 100\n\n@@ -3,13 +3,13 @@ import pytest\n import numpy as np\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n+from keras.utils.test_utils import keras_test\n \n \n+@keras_test\n def test_multiprocessing_training():\n-\n     reached_end = False\n-\n-    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_data = np.random.randint(0, 256, (500, 2))\n     arr_labels = np.random.randint(0, 2, 500)\n \n     def myGenerator():\n@@ -27,10 +27,7 @@ def test_multiprocessing_training():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(200, )))\n-    model.add(Activation('relu'))\n-    model.add(Dense(1))\n-    model.add(Activation('linear'))\n+    model.add(Dense(10, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n     model.fit_generator(myGenerator(),\n@@ -53,11 +50,12 @@ def test_multiprocessing_training():\n     assert reached_end\n \n \n+@keras_test\n def test_multiprocessing_training_fromfile():\n \n     reached_end = False\n \n-    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_data = np.random.randint(0, 256, (500, 2))\n     arr_labels = np.random.randint(0, 2, 500)\n     np.savez(\"data.npz\", **{\"data\": arr_data, \"labels\": arr_labels})\n \n@@ -78,10 +76,7 @@ def test_multiprocessing_training_fromfile():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(200, )))\n-    model.add(Activation('relu'))\n-    model.add(Dense(1))\n-    model.add(Activation('linear'))\n+    model.add(Dense(10, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n     model.fit_generator(myGenerator(),\n@@ -103,11 +98,10 @@ def test_multiprocessing_training_fromfile():\n     assert reached_end\n \n \n+@keras_test\n def test_multiprocessing_predicting():\n-\n     reached_end = False\n-\n-    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_data = np.random.randint(0, 256, (500, 2))\n \n     def myGenerator():\n \n@@ -123,10 +117,7 @@ def test_multiprocessing_predicting():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(200, )))\n-    model.add(Activation('relu'))\n-    model.add(Dense(1))\n-    model.add(Activation('linear'))\n+    model.add(Dense(10, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n     model.predict_generator(myGenerator(),\n                             val_samples=320,\n@@ -142,11 +133,10 @@ def test_multiprocessing_predicting():\n     assert reached_end\n \n \n+@keras_test\n def test_multiprocessing_evaluating():\n-\n     reached_end = False\n-\n-    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_data = np.random.randint(0, 256, (500, 2))\n     arr_labels = np.random.randint(0, 2, 500)\n \n     def myGenerator():\n@@ -164,10 +154,7 @@ def test_multiprocessing_evaluating():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(200, )))\n-    model.add(Activation('relu'))\n-    model.add(Dense(1))\n-    model.add(Activation('linear'))\n+    model.add(Dense(10, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n     model.evaluate_generator(myGenerator(),\n@@ -185,5 +172,4 @@ def test_multiprocessing_evaluating():\n \n \n if __name__ == '__main__':\n-\n     pytest.main([__file__])\n\n@@ -9,7 +9,7 @@ from keras import backend as K\n from keras.models import Graph, Sequential\n from keras.layers.core import Dense, Activation, Merge, Lambda\n from keras.utils import np_utils\n-from keras.utils.test_utils import get_test_data\n+from keras.utils.test_utils import get_test_data, keras_test\n from keras.models import model_from_json, model_from_yaml\n from keras import objectives\n from keras.engine.training import make_batches\n@@ -22,6 +22,7 @@ batch_size = 32\n nb_epoch = 1\n \n \n+@keras_test\n def test_sequential_pop():\n     model = Sequential()\n     model.add(Dense(nb_hidden, input_dim=input_dim))\n@@ -29,13 +30,13 @@ def test_sequential_pop():\n     model.compile(loss='mse', optimizer='sgd')\n     x = np.random.random((batch_size, input_dim))\n     y = np.random.random((batch_size, nb_class))\n-    model.fit(x, y)\n+    model.fit(x, y, nb_epoch=1)\n     model.pop()\n     assert len(model.layers) == 1\n     assert model.output_shape == (None, nb_hidden)\n     model.compile(loss='mse', optimizer='sgd')\n     y = np.random.random((batch_size, nb_hidden))\n-    model.fit(x, y)\n+    model.fit(x, y, nb_epoch=1)\n \n \n def _get_test_data():\n@@ -54,6 +55,7 @@ def _get_test_data():\n     return (X_train, y_train), (X_test, y_test)\n \n \n+@keras_test\n def test_sequential_fit_generator():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n \n@@ -85,10 +87,10 @@ def test_sequential_fit_generator():\n     model.fit_generator(data_generator(True), len(X_train), nb_epoch,\n                         validation_data=data_generator(False), nb_val_samples=batch_size * 3)\n     model.fit_generator(data_generator(True), len(X_train), nb_epoch, max_q_size=2)\n-\n-    loss = model.evaluate(X_train, y_train)\n+    model.evaluate(X_train, y_train)\n \n \n+@keras_test\n def test_sequential():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n \n@@ -146,16 +148,17 @@ def test_sequential():\n \n     # test serialization\n     config = model.get_config()\n-    new_model = Sequential.from_config(config)\n+    Sequential.from_config(config)\n \n     model.summary()\n     json_str = model.to_json()\n-    new_model = model_from_json(json_str)\n+    model_from_json(json_str)\n \n     yaml_str = model.to_yaml()\n-    new_model = model_from_yaml(yaml_str)\n+    model_from_yaml(yaml_str)\n \n \n+@keras_test\n def test_nested_sequential():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n \n@@ -208,16 +211,17 @@ def test_nested_sequential():\n \n     # test serialization\n     config = model.get_config()\n-    new_model = Sequential.from_config(config)\n+    Sequential.from_config(config)\n \n     model.summary()\n     json_str = model.to_json()\n-    new_model = model_from_json(json_str)\n+    model_from_json(json_str)\n \n     yaml_str = model.to_yaml()\n-    new_model = model_from_yaml(yaml_str)\n+    model_from_yaml(yaml_str)\n \n \n+@keras_test\n def test_merge_sum():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n@@ -267,16 +271,17 @@ def test_merge_sum():\n \n     # test serialization\n     config = model.get_config()\n-    new_model = Sequential.from_config(config)\n+    Sequential.from_config(config)\n \n     model.summary()\n     json_str = model.to_json()\n-    new_model = model_from_json(json_str)\n+    model_from_json(json_str)\n \n     yaml_str = model.to_yaml()\n-    new_model = model_from_yaml(yaml_str)\n+    model_from_yaml(yaml_str)\n \n \n+@keras_test\n def test_merge_dot():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n \n@@ -311,6 +316,7 @@ def test_merge_dot():\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n \n+@keras_test\n def test_merge_concat():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n \n@@ -350,6 +356,7 @@ def test_merge_concat():\n     assert(loss == nloss)\n \n \n+@keras_test\n def test_merge_recursivity():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n@@ -396,16 +403,17 @@ def test_merge_recursivity():\n \n     # test serialization\n     config = model.get_config()\n-    new_model = Sequential.from_config(config)\n+    Sequential.from_config(config)\n \n     model.summary()\n     json_str = model.to_json()\n-    new_model = model_from_json(json_str)\n+    model_from_json(json_str)\n \n     yaml_str = model.to_yaml()\n-    new_model = model_from_yaml(yaml_str)\n+    model_from_yaml(yaml_str)\n \n \n+@keras_test\n def test_merge_overlap():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n@@ -443,16 +451,17 @@ def test_merge_overlap():\n \n     # test serialization\n     config = model.get_config()\n-    new_model = Sequential.from_config(config)\n+    Sequential.from_config(config)\n \n     model.summary()\n     json_str = model.to_json()\n-    new_model = model_from_json(json_str)\n+    model_from_json(json_str)\n \n     yaml_str = model.to_yaml()\n-    new_model = model_from_yaml(yaml_str)\n+    model_from_yaml(yaml_str)\n \n \n+@keras_test\n def test_sequential_count_params():\n     input_dim = 20\n     nb_units = 10\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#726c9fc8a6344a6a0ebc90d2d64b2bd0f044c2ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 19 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 1255 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -126,8 +126,8 @@ def test_atrous_conv_2d():\n                            input_shape=(nb_samples, stack_size, nb_row, nb_col))\n \n \n-@keras_test\n @pytest.mark.skipif(K._BACKEND != 'tensorflow', reason=\"Requires TF backend\")\n+@keras_test\n def test_separable_conv_2d():\n     nb_samples = 2\n     nb_filter = 8\n\n@@ -3,13 +3,13 @@ import pytest\n import numpy as np\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n-from keras.utils.test_utils import keras_test\n \n \n-@keras_test\n def test_multiprocessing_training():\n+\n     reached_end = False\n-    arr_data = np.random.randint(0, 256, (500, 2))\n+\n+    arr_data = np.random.randint(0,256, (500, 200))\n     arr_labels = np.random.randint(0, 2, 500)\n \n     def myGenerator():\n@@ -27,7 +27,10 @@ def test_multiprocessing_training():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(2, )))\n+    model.add(Dense(10, input_shape=(200, )))\n+    model.add(Activation('relu'))\n+    model.add(Dense(1))\n+    model.add(Activation('linear'))\n     model.compile(loss='mse', optimizer='adadelta')\n \n     model.fit_generator(myGenerator(),\n@@ -50,12 +53,11 @@ def test_multiprocessing_training():\n     assert reached_end\n \n \n-@keras_test\n def test_multiprocessing_training_fromfile():\n \n     reached_end = False\n \n-    arr_data = np.random.randint(0, 256, (500, 2))\n+    arr_data = np.random.randint(0,256, (500, 200))\n     arr_labels = np.random.randint(0, 2, 500)\n     np.savez(\"data.npz\", **{\"data\": arr_data, \"labels\": arr_labels})\n \n@@ -76,7 +78,10 @@ def test_multiprocessing_training_fromfile():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(2, )))\n+    model.add(Dense(10, input_shape=(200, )))\n+    model.add(Activation('relu'))\n+    model.add(Dense(1))\n+    model.add(Activation('linear'))\n     model.compile(loss='mse', optimizer='adadelta')\n \n     model.fit_generator(myGenerator(),\n@@ -98,10 +103,11 @@ def test_multiprocessing_training_fromfile():\n     assert reached_end\n \n \n-@keras_test\n def test_multiprocessing_predicting():\n+\n     reached_end = False\n-    arr_data = np.random.randint(0, 256, (500, 2))\n+\n+    arr_data = np.random.randint(0,256, (500, 200))\n \n     def myGenerator():\n \n@@ -117,7 +123,10 @@ def test_multiprocessing_predicting():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(2, )))\n+    model.add(Dense(10, input_shape=(200, )))\n+    model.add(Activation('relu'))\n+    model.add(Dense(1))\n+    model.add(Activation('linear'))\n     model.compile(loss='mse', optimizer='adadelta')\n     model.predict_generator(myGenerator(),\n                             val_samples=320,\n@@ -133,10 +142,11 @@ def test_multiprocessing_predicting():\n     assert reached_end\n \n \n-@keras_test\n def test_multiprocessing_evaluating():\n+\n     reached_end = False\n-    arr_data = np.random.randint(0, 256, (500, 2))\n+\n+    arr_data = np.random.randint(0,256, (500, 200))\n     arr_labels = np.random.randint(0, 2, 500)\n \n     def myGenerator():\n@@ -154,7 +164,10 @@ def test_multiprocessing_evaluating():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(2, )))\n+    model.add(Dense(10, input_shape=(200, )))\n+    model.add(Activation('relu'))\n+    model.add(Dense(1))\n+    model.add(Activation('linear'))\n     model.compile(loss='mse', optimizer='adadelta')\n \n     model.evaluate_generator(myGenerator(),\n@@ -172,4 +185,5 @@ def test_multiprocessing_evaluating():\n \n \n if __name__ == '__main__':\n+\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
