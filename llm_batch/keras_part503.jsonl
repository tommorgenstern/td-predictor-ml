{"custom_id": "keras#d59fa77f27ea9234d8c626fd09ce6e6683c46581", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 40 | Churn Cumulative: 1700 | Contributors (this commit): 23 | Commits (past 90d): 10 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,6 +63,7 @@ from keras.layers.core.activation import Activation\n from keras.layers.core.dense import Dense\n from keras.layers.core.einsum_dense import EinsumDense\n from keras.layers.core.embedding import Embedding\n+from keras.layers.core.identity import Identity\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n from keras.layers.core.tf_op_layer import ClassMethod\n\n@@ -18,6 +18,7 @@ from keras.layers.core.activation import Activation\n from keras.layers.core.dense import Dense\n from keras.layers.core.einsum_dense import EinsumDense\n from keras.layers.core.embedding import Embedding\n+from keras.layers.core.identity import Identity\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n \n\n@@ -0,0 +1,38 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the Identity layer.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras.engine.base_layer import Layer\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export(\"keras.layers.Identity\")\n+class Identity(Layer):\n+    \"\"\"Identity layer.\n+\n+    This layer should be used as a placeholder when no operation is to be\n+    performed. The layer is argument insensitive, and returns its `inputs`\n+    argument as output.\n+\n+    Args:\n+        name: Optional name for the layer instance.\n+    \"\"\"\n+\n+    def call(self, inputs):\n+        return tf.identity(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a0a9ff3dce302f29d483b4733883e1d2ed0cc9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 127 | Lines Deleted: 155 | Files Changed: 2 | Hunks: 21 | Methods Changed: 18 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 282 | Churn Cumulative: 17054 | Contributors (this commit): 53 | Commits (past 90d): 28 | Contributors (cumulative): 69 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1446,10 +1446,15 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             return inputs\n         ```\n \n-        This method can also be called directly on a Functional Model during\n-        construction. In this case, any loss Tensors passed to this Model must\n-        be symbolic and be able to be traced back to the model's `Input`s. These\n-        losses become part of the model's topology and are tracked in\n+        The same code works in distributed training: the input to `add_loss()`\n+        is treated like a regularization loss and averaged across replicas\n+        by the training loop (both built-in `Model.fit()` and compliant custom\n+        training loops).\n+\n+        The `add_loss` method can also be called directly on a Functional Model\n+        during construction. In this case, any loss Tensors passed to this Model\n+        must be symbolic and be able to be traced back to the model's `Input`s.\n+        These losses become part of the model's topology and are tracked in\n         `get_config`.\n \n         Example:\n\n@@ -53,25 +53,14 @@ class Loss:\n         return tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)\n     ```\n \n-    When used with `tf.distribute.Strategy`, outside of built-in training loops\n-    such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction\n-    types, and reduce losses explicitly in your training loop. Using 'AUTO' or\n-    'SUM_OVER_BATCH_SIZE' will raise an error.\n-\n-    Please see this custom training [tutorial](\n-      https://www.tensorflow.org/tutorials/distribute/custom_training) for more\n-    details on this.\n-\n-    You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like:\n-\n-    ```python\n-    with strategy.scope():\n-      loss_obj = tf.keras.losses.CategoricalCrossentropy(\n-          reduction=tf.keras.losses.Reduction.NONE)\n-      ....\n-      loss = (tf.reduce_sum(loss_obj(labels, predictions)) *\n-              (1. / global_batch_size))\n-    ```\n+    When using a Loss under a `tf.distribute.Strategy`, except passing it\n+    to `Model.compile()` for use by `Model.fit()`, please use reduction\n+    types 'SUM' or 'NONE', and reduce losses explicitly. Using 'AUTO' or\n+    'SUM_OVER_BATCH_SIZE' will raise an error when calling the Loss object\n+    from a custom training loop or from user-defined code in `Layer.call()`.\n+    Please see this custom training\n+    [tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training)\n+    for more details on this.\n     \"\"\"\n \n     def __init__(self, reduction=losses_utils.ReductionV2.AUTO, name=None):\n@@ -81,10 +70,9 @@ class Loss:\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE`\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n             will raise an error. Please see this custom training [tutorial](\n               https://www.tensorflow.org/tutorials/distribute/custom_training)\n               for more details.\n@@ -217,17 +205,11 @@ class Loss:\n             raise ValueError(\n                 \"Please use `tf.keras.losses.Reduction.SUM` or \"\n                 \"`tf.keras.losses.Reduction.NONE` for loss reduction when \"\n-                \"losses are used with `tf.distribute.Strategy` outside \"\n-                \"of the built-in training loops. You can implement \"\n-                \"`tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE` using \"\n-                \"global batch size like:\\n```\\nwith strategy.scope():\\n\"\n-                \"    loss_obj = tf.keras.losses.CategoricalCrossentropy(\"\n-                \"reduction=tf.keras.losses.Reduction.NONE)\\n....\\n\"\n-                \"    loss = tf.reduce_sum(loss_obj(labels, predictions)) * \"\n-                \"(1. / global_batch_size)\\n```\\nPlease see \"\n-                \"https://www.tensorflow.org/tutorials\"\n-                \"/distribute/custom_training\"\n-                \" for more details.\"\n+                \"losses are used with `tf.distribute.Strategy`, \"\n+                \"except for specifying losses in `Model.compile()` \"\n+                \"for use by the built-in training looop `Model.fit()`.\\n\"\n+                \"Please see https://www.tensorflow.org/tutorials\"\n+                \"/distribute/custom_training for more details.\"\n             )\n \n         if self.reduction == losses_utils.ReductionV2.AUTO:\n@@ -250,13 +232,12 @@ class LossFunctionWrapper(Loss):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance.\n           **kwargs: The keyword arguments that are passed on to `fn`.\n         \"\"\"\n@@ -363,13 +344,12 @@ class MeanSquaredError(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to\n             'mean_squared_error'.\n         \"\"\"\n@@ -425,13 +405,12 @@ class MeanAbsoluteError(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to\n             'mean_absolute_error'.\n         \"\"\"\n@@ -493,13 +472,12 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to\n             'mean_absolute_percentage_error'.\n         \"\"\"\n@@ -558,13 +536,12 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to\n             'mean_squared_logarithmic_error'.\n         \"\"\"\n@@ -662,13 +639,12 @@ class BinaryCrossentropy(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Name for the op. Defaults to 'binary_crossentropy'.\n         \"\"\"\n         super().__init__(\n@@ -809,12 +785,12 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n       reduction: Type of `tf.keras.losses.Reduction` to apply to\n         loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n         option will be determined by the usage context. For almost all cases\n-        this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-        `tf.distribute.Strategy`, outside of built-in training loops such as\n-        `tf.keras`, `compile()` and `fit()`, using `SUM_OVER_BATCH_SIZE` or\n-        `AUTO` will raise an error. Please see this custom training [tutorial](\n-        https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-        more details.\n+        this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+        `tf.distribute.Strategy`, except via `Model.compile()` and\n+        `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+        will raise an error. Please see this custom training [tutorial](\n+        https://www.tensorflow.org/tutorials/distribute/custom_training)\n+        for more details.\n       name: Name for the op. Defaults to 'binary_focal_crossentropy'.\n     \"\"\"\n \n@@ -925,13 +901,12 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance.\n             Defaults to 'categorical_crossentropy'.\n         \"\"\"\n@@ -1014,13 +989,12 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to\n             'sparse_categorical_crossentropy'.\n         \"\"\"\n@@ -1081,13 +1055,12 @@ class Hinge(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to 'hinge'.\n         \"\"\"\n         super().__init__(hinge, name=name, reduction=reduction)\n@@ -1143,13 +1116,12 @@ class SquaredHinge(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to 'squared_hinge'.\n         \"\"\"\n         super().__init__(squared_hinge, name=name, reduction=reduction)\n@@ -1203,13 +1175,12 @@ class CategoricalHinge(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to 'categorical_hinge'.\n         \"\"\"\n         super().__init__(categorical_hinge, name=name, reduction=reduction)\n@@ -1260,13 +1231,12 @@ class Poisson(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to 'poisson'.\n         \"\"\"\n         super().__init__(poisson, name=name, reduction=reduction)\n@@ -1320,13 +1290,12 @@ class LogCosh(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to 'log_cosh'.\n         \"\"\"\n         super().__init__(log_cosh, name=name, reduction=reduction)\n@@ -1381,13 +1350,12 @@ class KLDivergence(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to 'kl_divergence'.\n         \"\"\"\n         super().__init__(kl_divergence, name=name, reduction=reduction)\n@@ -1451,13 +1419,12 @@ class Huber(LossFunctionWrapper):\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n-            this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n-            `tf.distribute.Strategy`, outside of built-in training loops such as\n-            `tf.keras` `compile` and `fit`, using `AUTO` or\n-            `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n-            training [tutorial](\n-            https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-            more details.\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n           name: Optional name for the instance. Defaults to 'huber_loss'.\n         \"\"\"\n         super().__init__(huber, name=name, reduction=reduction, delta=delta)\n@@ -2541,12 +2508,12 @@ class CosineSimilarity(LossFunctionWrapper):\n       reduction: Type of `tf.keras.losses.Reduction` to apply to loss.\n         Default value is `AUTO`. `AUTO` indicates that the reduction option will\n         be determined by the usage context. For almost all cases this defaults\n-        to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`,\n-        outside of built-in training loops such as `tf.keras` `compile` and\n-        `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE` will raise an error. Please\n-        see this custom training [tutorial](\n-        https://www.tensorflow.org/tutorials/distribute/custom_training) for\n-        more details.\n+        to `SUM_OVER_BATCH_SIZE`. When used under a\n+        `tf.distribute.Strategy`, except via `Model.compile()` and\n+        `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+        will raise an error. Please see this custom training [tutorial](\n+        https://www.tensorflow.org/tutorials/distribute/custom_training)\n+        for more details.\n       name: Optional name for the instance.\n     \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a78c8d81fbee84b33e80ce4c01c0fe162657e058", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 14 | Files Changed: 7 | Hunks: 19 | Methods Changed: 10 | Complexity Δ (Sum/Max): 4/6 | Churn Δ: 55 | Churn Cumulative: 37747 | Contributors (this commit): 179 | Commits (past 90d): 77 | Contributors (cumulative): 234 | DMM Complexity: 0.0\n\nDIFF:\n@@ -40,6 +40,7 @@ from keras.engine import node as node_module\n from keras.mixed_precision import autocast_variable\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import policy\n+from keras.saving import serialization_lib\n from keras.saving.legacy.saved_model import layer_serialization\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n@@ -769,7 +770,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         try:\n             instance._auto_get_config = auto_get_config\n             if auto_get_config:\n-                instance._auto_config = generic_utils.Config(**kwargs)\n+                instance._auto_config = serialization_lib.Config(**kwargs)\n         except RecursionError:\n             # Setting an instance attribute in __new__ has the potential\n             # to trigger an infinite recursion if a subclass overrides\n\n@@ -39,6 +39,7 @@ from keras.optimizers import optimizer_v1\n from keras.saving import pickle_utils\n from keras.saving import saving_api\n from keras.saving import saving_lib\n+from keras.saving import serialization_lib\n from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import json_utils\n from keras.saving.legacy.saved_model import model_serialization\n@@ -697,7 +698,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             **kwargs: Arguments supported for backwards compatibility only.\n         \"\"\"\n         base_layer.keras_api_gauge.get_cell(\"compile\").set(True)\n-        self._compile_config = generic_utils.Config(\n+        self._compile_config = serialization_lib.Config(\n             optimizer=optimizer,\n             loss=loss,\n             metrics=metrics,\n\n@@ -38,6 +38,7 @@ from keras.saving import object_registration\n from keras.saving.legacy import model_config\n from keras.saving.legacy import save\n from keras.saving.legacy import serialization\n+from keras.saving.legacy.saved_model import utils as saved_model_utils\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n@@ -1509,4 +1510,7 @@ class TestWholeModelSavingWithNesting(tf.test.TestCase, parameterized.TestCase):\n \n \n if __name__ == \"__main__\":\n+    with saved_model_utils.keras_option_scope(\n+        save_traces=False, in_tf_saved_model_scope=True\n+    ):\n         tf.test.main()\n\n@@ -40,6 +40,7 @@ from keras.saving import object_registration\n from keras.saving.legacy.saved_model import json_utils\n from keras.saving.legacy.saved_model import load as keras_load\n from keras.saving.legacy.saved_model import save_impl as keras_save\n+from keras.saving.legacy.saved_model import utils as saved_model_utils\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import control_flow_util\n@@ -1614,4 +1615,7 @@ class TestUpdateMetadata(tf.test.TestCase):\n \n \n if __name__ == \"__main__\":\n+    with saved_model_utils.keras_option_scope(\n+        save_traces=False, in_tf_saved_model_scope=True\n+    ):\n         tf.test.main()\n\n@@ -25,9 +25,9 @@ import tensorflow.compat.v2 as tf\n from keras import backend\n from keras.engine import base_layer_utils\n from keras.utils import control_flow_util\n-from keras.utils import layer_utils\n from keras.utils import tf_contextlib\n from keras.utils.generic_utils import LazyLoader\n+from keras.utils.layer_utils import CallFunctionSpec\n \n training_lib = LazyLoader(\"training_lib\", globals(), \"keras.engine.training\")\n \n@@ -160,7 +160,7 @@ def maybe_add_training_arg(\n     arg_spec = set_training_arg_spec(\n         call_spec.full_argspec, default_training_value\n     )\n-    call_spec = layer_utils.CallFunctionSpec(arg_spec)\n+    call_spec = CallFunctionSpec(arg_spec)\n \n     def wrap_with_training_arg(*args, **kwargs):\n         \"\"\"Wrap the `wrapped_call` function, and set training argument.\"\"\"\n\n@@ -22,6 +22,8 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n from keras.saving import object_registration\n+from keras.saving.legacy import serialization as legacy_serialization\n+from keras.saving.legacy.saved_model.utils import in_tf_saved_model_scope\n from keras.utils import generic_utils\n \n # isort: off\n@@ -31,6 +33,14 @@ PLAIN_TYPES = (str, int, float, bool)\n SHARED_OBJECTS = threading.local()\n \n \n+class Config:\n+    def __init__(self, **config):\n+        self.config = config\n+\n+    def serialize(self):\n+        return serialize_keras_object(self.config)\n+\n+\n class ObjectSharingScope:\n     \"\"\"Scope to enable detection and reuse of previously seen objects.\"\"\"\n \n@@ -86,6 +96,12 @@ def serialize_keras_object(obj):\n       A python dict that represents the object. The python dict can be\n       deserialized via `deserialize_keras_object()`.\n     \"\"\"\n+    # Fall back to legacy serialization for all TF1 users or if\n+    # wrapped by in_tf_saved_model_scope() to explicitly use legacy\n+    # saved_model logic.\n+    if not tf.__internal__.tf2.enabled() or in_tf_saved_model_scope():\n+        return legacy_serialization.serialize_keras_object(obj)\n+\n     if obj is None:\n         return obj\n     if isinstance(obj, PLAIN_TYPES):\n@@ -222,7 +238,7 @@ def serialize_dict(obj):\n     return {key: serialize_keras_object(value) for key, value in obj.items()}\n \n \n-def deserialize_keras_object(config, custom_objects=None):\n+def deserialize_keras_object(config, custom_objects=None, **kwargs):\n     \"\"\"Retrieve the object by deserializing the config dict.\n \n     The config dict is a Python dictionary that consists of a set of key-value\n@@ -316,7 +332,17 @@ def deserialize_keras_object(config, custom_objects=None):\n       The object described by the `config` dictionary.\n \n     \"\"\"\n+    module_objects = kwargs.pop(\"module_objects\", None)\n     custom_objects = custom_objects or {}\n+\n+    # Fall back to legacy deserialization for all TF1 users or if\n+    # wrapped by in_tf_saved_model_scope() to explicitly use legacy\n+    # saved_model logic.\n+    if not tf.__internal__.tf2.enabled() or in_tf_saved_model_scope():\n+        return legacy_serialization.deserialize_keras_object(\n+            config, module_objects, custom_objects\n+        )\n+\n     if config is None:\n         return None\n     if isinstance(config, PLAIN_TYPES):\n\n@@ -27,7 +27,6 @@ import types as python_types\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n-from keras.saving import serialization_lib\n from keras.utils import io_utils\n from keras.utils import tf_inspect\n \n@@ -556,11 +555,3 @@ class LazyLoader(python_types.ModuleType):\n     def __getattr__(self, item):\n         module = self._load()\n         return getattr(module, item)\n-\n-\n-class Config:\n-    def __init__(self, **config):\n-        self.config = config\n-\n-    def serialize(self):\n-        return serialization_lib.serialize_keras_object(self.config)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#510df69a824ed41e761fedd9a24a9a0b664ebe74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 20976 | Contributors (this commit): 140 | Commits (past 90d): 33 | Contributors (cumulative): 140 | DMM Complexity: None\n\nDIFF:\n@@ -3198,7 +3198,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             positions: Relative or absolute positions of log elements\n                 in each line. If not provided,\n                 defaults to `[.33, .55, .67, 1.]`.\n-            print_fn: Print function to use. Defaults to `print`.\n+            print_fn: Print function to use. By default, prints to `stdout`.\n+                If `stdout` doesn't work in your environment, change to `print`.\n                 It will be called on each line of the summary.\n                 You can set it to a custom function\n                 in order to capture the string summary.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4ed7cbe53706535cc930e45ba0969afa7f89d71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 53 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -839,20 +839,24 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n           average of the weights of the model (as the weight values change after\n           each training batch), and periodically overwriting the weights with\n           their moving average.\n-      ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`. This is  # noqa: E501\n-        the momentum to use when computing the EMA of the model's weights:\n+      ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n+          This is the momentum to use when computing\n+          the EMA of the model's weights:\n           `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n           current_variable_value`.\n       ema_overwrite_frequency: Int or None, defaults to None. Only used if\n-        `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations, we\n-        overwrite the model variable by its moving average. If None, the optimizer  # noqa: E501\n+          `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n+          we overwrite the model variable by its moving average.\n+          If None, the optimizer\n           does not overwrite model variables in the middle of training, and you\n           need to explicitly overwrite the variables at the end of training\n-        by calling `optimizer.finalize_variable_values()` (which updates the model  # noqa: E501\n-        variables in-place). When using the built-in `fit()` training loop, this\n-        happens automatically after the last epoch, and you don't need to do\n-        anything.\n-      jit_compile: Boolean, defaults to True. If True, the optimizer will use XLA  # noqa: E501\n+          by calling `optimizer.finalize_variable_values()`\n+          (which updates the model\n+          variables in-place). When using the built-in `fit()` training loop,\n+          this happens automatically after the last epoch,\n+          and you don't need to do anything.\n+      jit_compile: Boolean, defaults to True.\n+          If True, the optimizer will use XLA\n           compilation. If no GPU device is found, this flag will be ignored.\n       **kwargs: keyword arguments only used for backward compatibility.\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a624b447146b9cf6596267c5447f1cd79ce183b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 127 | Lines Deleted: 20 | Files Changed: 7 | Hunks: 30 | Methods Changed: 19 | Complexity Δ (Sum/Max): 127/118 | Churn Δ: 147 | Churn Cumulative: 2843 | Contributors (this commit): 14 | Commits (past 90d): 25 | Contributors (cumulative): 29 | DMM Complexity: 0.38461538461538464\n\nDIFF:\n@@ -23,7 +23,8 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n from keras.engine.base_layer import Layer\n-from keras.saving.legacy import serialization\n+from keras.saving import serialization_lib\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.utils import generic_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n@@ -381,13 +382,23 @@ class Lambda(Layer):\n         function_type = config.pop(func_type_attr_name)\n         if function_type == \"function\":\n             # Simple lookup in custom objects\n-            function = serialization.deserialize_keras_object(\n+            function = legacy_serialization.deserialize_keras_object(\n                 config[func_attr_name],\n                 custom_objects=custom_objects,\n                 printable_module_name=\"function in Lambda layer\",\n             )\n         elif function_type == \"lambda\":\n-            # Unsafe deserialization from bytecode\n+            if serialization_lib.in_safe_mode():\n+                raise ValueError(\n+                    \"Requested the deserialization of a Lambda layer with a \"\n+                    \"Python `lambda` inside it. \"\n+                    \"This carries a potential risk of arbitrary code execution \"\n+                    \"and thus it is disallowed by default. If you trust the \"\n+                    \"source of the saved model, you can pass `safe_mode=False` \"\n+                    \"to the loading function in order to allow \"\n+                    \"Lambda layer loading.\"\n+                )\n+            # /!\\ Unsafe deserialization from bytecode! Danger! /!\\\n             function = generic_utils.func_load(\n                 config[func_attr_name], globs=globs\n             )\n\n@@ -31,7 +31,8 @@ import tensorflow.compat.v2 as tf\n \n from keras.layers.rnn import lstm\n from keras.layers.rnn.abstract_rnn_cell import AbstractRNNCell\n-from keras.saving.legacy import serialization\n+from keras.saving import serialization_lib\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.utils import generic_utils\n from keras.utils import tf_inspect\n \n@@ -658,12 +659,22 @@ def _parse_config_to_function(\n     function_type = config.pop(func_type_attr_name)\n     if function_type == \"function\":\n         # Simple lookup in custom objects\n-        function = serialization.deserialize_keras_object(\n+        function = legacy_serialization.deserialize_keras_object(\n             config[func_attr_name],\n             custom_objects=custom_objects,\n             printable_module_name=\"function in wrapper\",\n         )\n     elif function_type == \"lambda\":\n+        if serialization_lib.in_safe_mode():\n+            raise ValueError(\n+                \"Requested the deserialization of a layer with a \"\n+                \"Python `lambda` inside it. \"\n+                \"This carries a potential risk of arbitrary code execution \"\n+                \"and thus it is disallowed by default. If you trust the \"\n+                \"source of the saved model, you can pass `safe_mode=False` to \"\n+                \"the loading function in order to allow \"\n+                \"`lambda` loading.\"\n+            )\n         # Unsafe deserialization from bytecode\n         function = generic_utils.func_load(config[func_attr_name], globs=globs)\n     else:\n\n@@ -152,7 +152,9 @@ def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n \n \n @keras_export(\"keras.models.load_model\")\n-def load_model(filepath, custom_objects=None, compile=True, **kwargs):\n+def load_model(\n+    filepath, custom_objects=None, compile=True, safe_mode=True, **kwargs\n+):\n     \"\"\"Loads a model saved via `model.save()`.\n \n     Args:\n@@ -161,6 +163,10 @@ def load_model(filepath, custom_objects=None, compile=True, **kwargs):\n             (strings) to custom classes or functions to be\n             considered during deserialization.\n         compile: Boolean, whether to compile the model after loading.\n+        safe_mode: Boolean, whether to disallow unsafe `lambda` deserialization.\n+            When `safe_mode=False`, loading an object has the potential to\n+            trigger arbitrary code execution. This argument is only\n+            applicable to the Keras v3 model format. Defaults to True.\n \n     SavedModel format arguments:\n         options: Only applies to SavedModel format.\n@@ -196,7 +202,10 @@ def load_model(filepath, custom_objects=None, compile=True, **kwargs):\n                 f\"with the native Keras format: {list(kwargs.keys())}\"\n             )\n         return saving_lib.load_model(\n-            filepath, custom_objects=custom_objects, compile=compile\n+            filepath,\n+            custom_objects=custom_objects,\n+            compile=compile,\n+            safe_mode=safe_mode,\n         )\n \n     # Legacy case.\n\n@@ -206,7 +206,7 @@ def save_model(model, filepath, weights_format=\"h5\"):\n         _SAVING_V3_ENABLED.value = saving_v3_enabled_value\n \n \n-def load_model(filepath, custom_objects=None, compile=True):\n+def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n     \"\"\"Load a zip archive representing a Keras model.\"\"\"\n \n     filepath = str(filepath)\n@@ -235,7 +235,9 @@ def load_model(filepath, custom_objects=None, compile=True):\n                 config_dict[\"compile_config\"] = None\n             # Construct the model from the configuration file in the archive.\n             with ObjectSharingScope():\n-                model = deserialize_keras_object(config_dict, custom_objects)\n+                model = deserialize_keras_object(\n+                    config_dict, custom_objects, safe_mode=safe_mode\n+                )\n \n             all_filenames = zf.namelist()\n             if _VARS_FNAME + \".h5\" in all_filenames:\n\n@@ -406,7 +406,7 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n         y = np.random.random((1000, 1))\n         functional_model.fit(x, y, epochs=3)\n         functional_model._save_experimental(temp_filepath)\n-        loaded_model = saving_lib.load_model(temp_filepath)\n+        loaded_model = saving_lib.load_model(temp_filepath, safe_mode=False)\n         self.assertEqual(\n             functional_model._is_compiled, loaded_model._is_compiled\n         )\n@@ -689,6 +689,19 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n                 temp_filepath, include_optimizer=False, save_format=\"keras_v3\"\n             )\n \n+    def test_safe_mode(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"unsafe_model.keras\")\n+        model = keras.Sequential(\n+            [\n+                keras.Input(shape=(3,)),\n+                keras.layers.Lambda(lambda x: x * 2),\n+            ]\n+        )\n+        model.save(temp_filepath, save_format=\"keras_v3\")\n+        with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n+            model = saving_lib.load_model(temp_filepath)\n+        model = saving_lib.load_model(temp_filepath, safe_mode=False)\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n@@ -15,8 +15,10 @@\n \"\"\"Object config serialization and deserialization logic.\"\"\"\n \n import importlib\n+import inspect\n import threading\n import types\n+import warnings\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -31,6 +33,7 @@ from tensorflow.python.util import tf_export\n \n PLAIN_TYPES = (str, int, float, bool)\n SHARED_OBJECTS = threading.local()\n+SAFE_MODE = threading.local()\n \n \n class Config:\n@@ -41,6 +44,24 @@ class Config:\n         return serialize_keras_object(self.config)\n \n \n+class SafeModeScope:\n+    \"\"\"Scope to propagate safe mode flag to nested deserialization calls.\"\"\"\n+\n+    def __init__(self, safe_mode=True):\n+        self.safe_mode = safe_mode\n+\n+    def __enter__(self):\n+        self.original_value = in_safe_mode()\n+        SAFE_MODE.safe_mode = self.safe_mode\n+\n+    def __exit__(self, *args, **kwargs):\n+        SAFE_MODE.safe_mode = self.original_value\n+\n+\n+def in_safe_mode():\n+    return getattr(SAFE_MODE, \"safe_mode\", None)\n+\n+\n class ObjectSharingScope:\n     \"\"\"Scope to enable detection and reuse of previously seen objects.\"\"\"\n \n@@ -143,6 +164,15 @@ def serialize_keras_object(obj):\n     if isinstance(obj, tf.DType):\n         return obj.name\n     if isinstance(obj, types.FunctionType) and obj.__name__ == \"<lambda>\":\n+        warnings.warn(\n+            \"The object being serialized includes a `lambda`. This is unsafe. \"\n+            \"In order to reload the object, you will have to pass \"\n+            \"`safe_mode=False` to the loading function. \"\n+            \"Please avoid using `lambda` in the \"\n+            \"future, and use named Python functions instead. \"\n+            f\"This is the `lambda` being serialized: {inspect.getsource(obj)}\",\n+            stacklevel=2,\n+        )\n         return {\n             \"class_name\": \"__lambda__\",\n             \"config\": {\n@@ -238,7 +268,9 @@ def serialize_dict(obj):\n     return {key: serialize_keras_object(value) for key, value in obj.items()}\n \n \n-def deserialize_keras_object(config, custom_objects=None, **kwargs):\n+def deserialize_keras_object(\n+    config, custom_objects=None, safe_mode=True, **kwargs\n+):\n     \"\"\"Retrieve the object by deserializing the config dict.\n \n     The config dict is a Python dictionary that consists of a set of key-value\n@@ -325,13 +357,20 @@ def deserialize_keras_object(config, custom_objects=None, **kwargs):\n     ```\n \n     Args:\n-      config_dict: the python dict structure to deserialize the Keras object\n-        from.\n+        config: Python dict describing the object.\n+        custom_objects: Python dict containing a mapping between custom\n+            object names the corresponding classes or functions.\n+        safe_mode: Boolean, whether to disallow unsafe `lambda` deserialization.\n+            When `safe_mode=False`, loading an object has the potential to\n+            trigger arbitrary code execution. This argument is only\n+            applicable to the Keras v3 model format. Defaults to True.\n \n     Returns:\n       The object described by the `config` dictionary.\n \n     \"\"\"\n+    safe_mode = in_safe_mode() or safe_mode\n+\n     module_objects = kwargs.pop(\"module_objects\", None)\n     custom_objects = custom_objects or {}\n \n@@ -353,7 +392,9 @@ def deserialize_keras_object(config, custom_objects=None, **kwargs):\n         return config\n     if isinstance(config, (list, tuple)):\n         return [\n-            deserialize_keras_object(x, custom_objects=custom_objects)\n+            deserialize_keras_object(\n+                x, custom_objects=custom_objects, safe_mode=safe_mode\n+            )\n             for x in config\n         ]\n     if not isinstance(config, dict):\n@@ -361,7 +402,9 @@ def deserialize_keras_object(config, custom_objects=None, **kwargs):\n \n     if \"class_name\" not in config or \"config\" not in config:\n         return {\n-            key: deserialize_keras_object(value, custom_objects=custom_objects)\n+            key: deserialize_keras_object(\n+                value, custom_objects=custom_objects, safe_mode=safe_mode\n+            )\n             for key, value in config.items()\n         }\n \n@@ -376,6 +419,14 @@ def deserialize_keras_object(config, custom_objects=None, **kwargs):\n     if config[\"class_name\"] == \"__bytes__\":\n         return inner_config[\"value\"].encode(\"utf-8\")\n     if config[\"class_name\"] == \"__lambda__\":\n+        if safe_mode:\n+            raise ValueError(\n+                \"Requested the deserialization of a `lambda` object. \"\n+                \"This carries a potential risk of arbitrary code execution \"\n+                \"and thus it is disallowed by default. If you trust the \"\n+                \"source of the saved model, you can pass `safe_mode=False` to \"\n+                \"the loading function in order to allow `lambda` loading.\"\n+            )\n         return generic_utils.func_load(inner_config[\"value\"])\n     if config[\"class_name\"] == \"__typespec__\":\n         obj = _retrieve_class_or_fn(\n@@ -434,9 +485,13 @@ def deserialize_keras_object(config, custom_objects=None, **kwargs):\n             f\"the class is missing a `from_config()` method. \"\n             f\"Full object config: {config}\"\n         )\n+\n     # Instantiate the class from its config inside a custom object scope\n     # so that we can catch any custom objects that the config refers to.\n-    with object_registration.custom_object_scope(custom_objects):\n+    with (\n+        object_registration.custom_object_scope(custom_objects),\n+        SafeModeScope(safe_mode),\n+    ):\n         instance = cls.from_config(inner_config)\n         build_config = config.get(\"build_config\", None)\n         if build_config:\n\n@@ -84,12 +84,12 @@ class WrapperLayer(keras.layers.Layer):\n \n @test_utils.run_v2_only\n class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n-    def roundtrip(self, obj, custom_objects=None):\n+    def roundtrip(self, obj, custom_objects=None, safe_mode=True):\n         serialized = serialization_lib.serialize_keras_object(obj)\n         json_data = json.dumps(serialized)\n         json_data = json.loads(json_data)\n         deserialized = serialization_lib.deserialize_keras_object(\n-            json_data, custom_objects=custom_objects\n+            json_data, custom_objects=custom_objects, safe_mode=safe_mode\n         )\n         reserialized = serialization_lib.serialize_keras_object(deserialized)\n         return serialized, deserialized, reserialized\n@@ -169,12 +169,18 @@ class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n \n     def test_lambda_fn(self):\n         obj = {\"activation\": lambda x: x**2}\n-        _, new_obj, _ = self.roundtrip(obj)\n+        with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n+            self.roundtrip(obj, safe_mode=True)\n+\n+        _, new_obj, _ = self.roundtrip(obj, safe_mode=False)\n         self.assertEqual(obj[\"activation\"](3), new_obj[\"activation\"](3))\n \n     def test_lambda_layer(self):\n         lmbda = keras.layers.Lambda(lambda x: x**2)\n-        _, new_lmbda, _ = self.roundtrip(lmbda)\n+        with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n+            self.roundtrip(lmbda, safe_mode=True)\n+\n+        _, new_lmbda, _ = self.roundtrip(lmbda, safe_mode=False)\n         x = tf.random.normal((2, 2))\n         y1 = lmbda(x)\n         y2 = new_lmbda(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
