{"custom_id": "scrapy#7729f939e9fc3177d911b041b4c40c82fc4d4820", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 1098 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -12,6 +12,7 @@ from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.http import PotentialDataLoss\n from twisted.web.iweb import IBodyProducer\n from twisted.internet.endpoints import TCP4ClientEndpoint\n+from twisted.internet.error import TimeoutError\n \n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n@@ -49,20 +50,27 @@ class ScrapyAgent(object):\n         self._pool = pool\n \n     def download_request(self, request):\n+        timeout = request.meta.get('download_timeout') or self._connectTimeout\n         url = urldefrag(request.url)[0]\n         method = request.method\n         headers = TxHeaders(request.headers)\n         bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n-        agent = self._get_agent(request)\n+        agent = self._get_agent(request, timeout)\n         start_time = time()\n         d = agent.request(method, url, headers, bodyproducer)\n-        d.addBoth(self._download_latency, request, start_time)\n-        d.addCallback(self._agentrequest_downloaded, request)\n-        d.addErrback(self._agentrequest_failed, request)\n+        d.addBoth(self._both_cb, request, start_time, url, timeout)\n+        d.addCallback(self._downloaded, request)\n+        self._timeout_cl = reactor.callLater(timeout, d.cancel)\n         return d\n \n-    def _get_agent(self, request):\n-        timeout = request.meta.get('download_timeout') or self._connectTimeout\n+    def _both_cb(self, result, request, start_time, url, timeout):\n+        request.meta['download_latency'] = time() - start_time\n+        if self._timeout_cl.active():\n+            self._timeout_cl.cancel()\n+            return result\n+        raise TimeoutError(\"Getting %s took longer than %s seconds.\" % (url, timeout))\n+\n+    def _get_agent(self, request, timeout):\n         bindaddress = request.meta.get('bindaddress') or self._bindAddress\n         proxy = request.meta.get('proxy')\n         if proxy:\n@@ -74,11 +82,7 @@ class ScrapyAgent(object):\n         return self._Agent(reactor, contextFactory=self._contextFactory,\n             connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)\n \n-    def _download_latency(self, any_, request, start_time):\n-        request.meta['download_latency'] = time() - start_time\n-        return any_\n-\n-    def _agentrequest_downloaded(self, txresponse, request):\n+    def _downloaded(self, txresponse, request):\n         if txresponse.length == 0:\n             return self._build_response(('', None), txresponse, request)\n         finished = defer.Deferred()\n@@ -95,11 +99,6 @@ class ScrapyAgent(object):\n         respcls = responsetypes.from_args(headers=headers, url=url)\n         return respcls(url=url, status=status, headers=headers, body=body)\n \n-    def _agentrequest_failed(self, failure, request):\n-        # be clear it is an HTTP failure with new downloader\n-        log.err(failure, 'HTTP11 failure: %s' % request)\n-        return failure\n-\n \n class _RequestBodyProducer(object):\n     implements(IBodyProducer)\n\n@@ -105,7 +105,7 @@ class HttpTestCase(unittest.TestCase):\n         return d\n \n     def test_timeout_download_from_spider(self):\n-        request = Request(self.getURL('wait'), meta=dict(download_timeout=0.000001))\n+        request = Request(self.getURL('wait'), meta=dict(download_timeout=0.1))\n         d = self.download_request(request, BaseSpider('foo'))\n         return self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb806fa0f8f77cba59506153e33cab96f91008ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 27 | Churn Cumulative: 682 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,8 +53,8 @@ DOWNLOAD_DELAY = 0\n DOWNLOAD_HANDLERS = {}\n DOWNLOAD_HANDLERS_BASE = {\n     'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n-    'http': 'scrapy.core.downloader.handlers.http11.Http11DownloadHandler',\n-    'https': 'scrapy.core.downloader.handlers.http11.Http11DownloadHandler',\n+    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n+    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n     's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n }\n \n\n@@ -1,4 +1,5 @@\n import os\n+import twisted\n \n from twisted.trial import unittest\n from twisted.protocols.policies import WrappingFactory\n@@ -143,6 +144,9 @@ class Http11TestCase(HttpTestCase):\n     \"\"\"HTTP 1.1 test case\"\"\"\n     download_handler_cls = Http11DownloadHandler\n \n+    if twisted.__version__.split('.') < (11, 1, 0):\n+        skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n+\n \n class UriResource(resource.Resource):\n     \"\"\"Return the full uri that was requested\"\"\"\n@@ -198,6 +202,9 @@ class HttpProxyTestCase(unittest.TestCase):\n class Http11ProxyTestCase(HttpProxyTestCase):\n     download_handler_cls = Http11DownloadHandler\n \n+    if twisted.__version__.split('.') < (11, 1, 0):\n+        skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n+\n \n class HttpDownloadHandlerMock(object):\n     def __init__(self, settings):\n\n@@ -1,9 +1,19 @@\n import twisted\n-if twisted.__version__.split('.') < (13, 0, 1):\n-    from . import client, endpoints\n-else:\n+txver = twisted.__version__.split('.')\n+if txver > (13, 0, 0):\n     from twisted.web import client\n     from twisted.internet import endpoints\n+if txver >= (11, 1, 0):\n+    from . import client, endpoints\n+else:\n+    from scrapy.exceptions import NotSupported\n+    class _Mocked(object):\n+        def __init__(self, *args, **kw):\n+            raise NotSupported('HTTP1.1 not supported')\n+    class _Mock(object):\n+        def __getattr__(self, name):\n+            return _Mocked\n+    client = endpoints = _Mock()\n \n \n Agent = client.Agent\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf10474b8b6da1028a7a6cea13b509b0c6502306", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 41 | Files Changed: 7 | Hunks: 16 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/4 | Churn Δ: 114 | Churn Cumulative: 1797 | Contributors (this commit): 9 | Commits (past 90d): 16 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -43,3 +43,8 @@ except ImportError:\n     pass\n else:\n     optional_features.add('django')\n+\n+from twisted import version as _txv\n+twisted_version = (_txv.major, _txv.minor, _txv.micro)\n+if twisted_version >= (11, 1, 0):\n+    optional_features.add('http11')\n\n@@ -1,25 +1,11 @@\n-\"\"\"Download handlers for http and https schemes\n-\"\"\"\n-from twisted.internet import reactor\n-from scrapy.utils.misc import load_object\n+from scrapy import optional_features\n+from .http10 import HTTP10DownloadHandler\n \n-\n-class HttpDownloadHandler(object):\n-\n-    def __init__(self, settings):\n-        self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n-        self.ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n-\n-    def download_request(self, request, spider):\n-        \"\"\"Return a deferred for the HTTP download\"\"\"\n-        factory = self.HTTPClientFactory(request)\n-        self._connect(factory)\n-        return factory.deferred\n-\n-    def _connect(self, factory):\n-        host, port = factory.host, factory.port\n-        if factory.scheme == 'https':\n-            return reactor.connectSSL(host, port, factory,\n-                                      self.ClientContextFactory())\n+if 'http11' in optional_features:\n+    from .http11 import HTTP11DownloadHandler as HTTPDownloadHandler\n else:\n-            return reactor.connectTCP(host, port, factory)\n+    HTTPDownloadHandler = HTTP10DownloadHandler\n+\n+\n+# backwards compatibility\n+HttpDownloadHandler = HTTP10DownloadHandler\n\n@@ -0,0 +1,25 @@\n+\"\"\"Download handlers for http and https schemes\n+\"\"\"\n+from twisted.internet import reactor\n+from scrapy.utils.misc import load_object\n+\n+\n+class HTTP10DownloadHandler(object):\n+\n+    def __init__(self, settings):\n+        self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n+        self.ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n+\n+    def download_request(self, request, spider):\n+        \"\"\"Return a deferred for the HTTP download\"\"\"\n+        factory = self.HTTPClientFactory(request)\n+        self._connect(factory)\n+        return factory.deferred\n+\n+    def _connect(self, factory):\n+        host, port = factory.host, factory.port\n+        if factory.scheme == 'https':\n+            return reactor.connectSSL(host, port, factory,\n+                                      self.ClientContextFactory())\n+        else:\n+            return reactor.connectTCP(host, port, factory)\n\n@@ -21,7 +21,7 @@ from scrapy import log\n \n \n \n-class Http11DownloadHandler(object):\n+class HTTP11DownloadHandler(object):\n \n     def __init__(self, settings):\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n\n@@ -53,8 +53,8 @@ DOWNLOAD_DELAY = 0\n DOWNLOAD_HANDLERS = {}\n DOWNLOAD_HANDLERS_BASE = {\n     'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n-    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n-    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n+    'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n+    'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n     's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n }\n \n\n@@ -12,8 +12,9 @@ from twisted.web.test.test_webclient import ForeverTakingResource, \\\n from w3lib.url import path_to_file_uri\n \n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n-from scrapy.core.downloader.handlers.http import HttpDownloadHandler\n-from scrapy.core.downloader.handlers.http11 import Http11DownloadHandler\n+from scrapy.core.downloader.handlers.http import HTTPDownloadHandler, HttpDownloadHandler\n+from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n+from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n@@ -48,7 +49,7 @@ class FileTestCase(unittest.TestCase):\n \n class HttpTestCase(unittest.TestCase):\n \n-    download_handler_cls = HttpDownloadHandler\n+    download_handler_cls = HTTPDownloadHandler\n \n     def setUp(self):\n         name = self.mktemp()\n@@ -140,11 +141,20 @@ class HttpTestCase(unittest.TestCase):\n         return d\n \n \n+class DeprecatedHttpTestCase(HttpTestCase):\n+    \"\"\"HTTP 1.0 test case\"\"\"\n+    download_handler_cls = HttpDownloadHandler\n+\n+\n+class Http10TestCase(HttpTestCase):\n+    \"\"\"HTTP 1.0 test case\"\"\"\n+    download_handler_cls = HTTP10DownloadHandler\n+\n+\n class Http11TestCase(HttpTestCase):\n     \"\"\"HTTP 1.1 test case\"\"\"\n-    download_handler_cls = Http11DownloadHandler\n-\n-    if twisted.__version__.split('.') < (11, 1, 0):\n+    download_handler_cls = HTTP11DownloadHandler\n+    if 'http11' not in optional_features:\n         skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n \n \n@@ -159,8 +169,7 @@ class UriResource(resource.Resource):\n \n \n class HttpProxyTestCase(unittest.TestCase):\n-\n-    download_handler_cls = HttpDownloadHandler\n+    download_handler_cls = HTTPDownloadHandler\n \n     def setUp(self):\n         site = server.Site(UriResource(), timeout=None)\n@@ -199,10 +208,18 @@ class HttpProxyTestCase(unittest.TestCase):\n         return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n \n \n-class Http11ProxyTestCase(HttpProxyTestCase):\n-    download_handler_cls = Http11DownloadHandler\n+class DeprecatedHttpProxyTestCase(unittest.TestCase):\n+    \"\"\"Old deprecated reference to http10 downloader handler\"\"\"\n+    download_handler_cls = HttpDownloadHandler\n \n-    if twisted.__version__.split('.') < (11, 1, 0):\n+\n+class Http10ProxyTestCase(HttpProxyTestCase):\n+    download_handler_cls = HTTP10DownloadHandler\n+\n+\n+class Http11ProxyTestCase(HttpProxyTestCase):\n+    download_handler_cls = HTTP11DownloadHandler\n+    if 'http11' not in optional_features:\n         skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n \n \n\n@@ -1,9 +1,8 @@\n-import twisted\n-txver = twisted.__version__.split('.')\n-if txver > (13, 0, 0):\n+from scrapy import twisted_version\n+if twisted_version > (13, 0, 0):\n     from twisted.web import client\n     from twisted.internet import endpoints\n-if txver >= (11, 1, 0):\n+if twisted_version >= (11, 1, 0):\n     from . import client, endpoints\n else:\n     from scrapy.exceptions import NotSupported\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7acc2171df5ea12b12066d159b8c5d1f7e30131", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 199 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,4 +8,12 @@ else:\n \n \n # backwards compatibility\n-HttpDownloadHandler = HTTP10DownloadHandler\n+class HttpDownloadHandler(HTTP10DownloadHandler):\n+\n+    def __init__(self, *args, **kwargs):\n+        import warnings\n+        from scrapy.exceptions import ScrapyDeprecationWarning\n+        warnings.warn('HttpDownloadHandler is deprecated, import scrapy.core.downloader'\n+                      '.handlers.http10.HTTP10DownloadHandler instead',\n+                      category=ScrapyDeprecationWarning, stacklevel=1)\n+        super(HttpDownloadHandler, self).__init__(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5a097cc19273a789e550d5140cc40d5a2402f3b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 449 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -142,3 +142,5 @@ def start_from_crawler(crawler):\n         settings['LOG_ENCODING'], crawler)\n     msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n         settings['BOT_NAME']))\n+    msg(\"Optional features available: %s\" % \", \".join(scrapy.optional_features),\n+        level=DEBUG)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#133091af3a35e0ecbdad3354c2cc83224aee2826", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 33 | Files Changed: 1 | Hunks: 10 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 78 | Churn Cumulative: 740 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -48,27 +48,6 @@ class ScrapyAgent(object):\n         self._bindAddress = bindAddress\n         self._pool = pool\n \n-    def download_request(self, request):\n-        timeout = request.meta.get('download_timeout') or self._connectTimeout\n-        url = urldefrag(request.url)[0]\n-        method = request.method\n-        headers = TxHeaders(request.headers)\n-        bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n-        agent = self._get_agent(request, timeout)\n-        start_time = time()\n-        d = agent.request(method, url, headers, bodyproducer)\n-        d.addBoth(self._both_cb, request, start_time, url, timeout)\n-        d.addCallback(self._downloaded, request)\n-        self._timeout_cl = reactor.callLater(timeout, d.cancel)\n-        return d\n-\n-    def _both_cb(self, result, request, start_time, url, timeout):\n-        request.meta['download_latency'] = time() - start_time\n-        if self._timeout_cl.active():\n-            self._timeout_cl.cancel()\n-            return result\n-        raise TimeoutError(\"Getting %s took longer than %s seconds.\" % (url, timeout))\n-\n     def _get_agent(self, request, timeout):\n         bindaddress = request.meta.get('bindaddress') or self._bindAddress\n         proxy = request.meta.get('proxy')\n@@ -81,22 +60,53 @@ class ScrapyAgent(object):\n         return self._Agent(reactor, contextFactory=self._contextFactory,\n             connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)\n \n-    def _downloaded(self, txresponse, request):\n+    def download_request(self, request):\n+        timeout = request.meta.get('download_timeout') or self._connectTimeout\n+        agent = self._get_agent(request, timeout)\n+\n+        # request details\n+        url = urldefrag(request.url)[0]\n+        method = request.method\n+        headers = TxHeaders(request.headers)\n+        bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n+\n+        start_time = time()\n+        d = agent.request(method, url, headers, bodyproducer)\n+        # check download timeout\n+        self._timeout_cl = reactor.callLater(timeout, d.cancel)\n+        d.addBoth(self._cb_timeout, request, url, timeout)\n+        # set download latency\n+        d.addCallback(self._cb_latency, request, start_time)\n+        # response body is ready to be consumed\n+        d.addCallback(self._cb_bodyready, request)\n+        d.addCallback(self._cb_bodydone, request, url)\n+        return d\n+\n+    def _cb_timeout(self, result, request, url, timeout):\n+        if self._timeout_cl.active():\n+            self._timeout_cl.cancel()\n+            return result\n+        raise TimeoutError(\"Getting %s took longer than %s seconds.\" % (url, timeout))\n+\n+    def _cb_latency(self, result, request, start_time):\n+        request.meta['download_latency'] = time() - start_time\n+        return result\n+\n+    def _cb_bodyready(self, txresponse, request):\n+        # deliverBody hangs for responses without body\n         if txresponse.length == 0:\n-            return self._build_response(('', None), txresponse, request)\n+            return txresponse, '', None\n+\n         finished = defer.Deferred()\n-        finished.addCallback(self._build_response, txresponse, request)\n-        txresponse.deliverBody(_ResponseReader(finished))\n+        txresponse.deliverBody(_ResponseReader(finished, txresponse, request))\n         return finished\n \n-    def _build_response(self, (body, flag), txresponse, request):\n-        if flag is not None:\n-            request.meta[flag] = True\n-        url = urldefrag(request.url)[0]\n+    def _cb_bodydone(self, result, request, url):\n+        txresponse, body, flags = result\n         status = int(txresponse.code)\n         headers = Headers(txresponse.headers.getAllRawHeaders())\n         respcls = responsetypes.from_args(headers=headers, url=url)\n-        return respcls(url=url, status=status, headers=headers, body=body)\n+        return respcls(url=url, status=status, headers=headers, body=body, flags=flags)\n \n \n class _RequestBodyProducer(object):\n@@ -119,8 +129,10 @@ class _RequestBodyProducer(object):\n \n class _ResponseReader(protocol.Protocol):\n \n-    def __init__(self, finished):\n+    def __init__(self, finished, txresponse, request):\n         self._finished = finished\n+        self._txresponse = txresponse\n+        self._request = request\n         self._bodybuf = StringIO()\n \n     def dataReceived(self, bodyBytes):\n@@ -129,8 +141,8 @@ class _ResponseReader(protocol.Protocol):\n     def connectionLost(self, reason):\n         body = self._bodybuf.getvalue()\n         if reason.check(ResponseDone):\n-            self._finished.callback((body, None))\n+            self._finished.callback((self._txresponse, body, None))\n         elif reason.check(PotentialDataLoss, ResponseFailed):\n-            self._finished.callback((body, 'partial_download'))\n+            self._finished.callback((self._txresponse, body, ['partial']))\n         else:\n             self._finished.errback(reason)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26809d2254ecbceb1352137815400de20fd12c76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 19 | Files Changed: 5 | Hunks: 21 | Methods Changed: 15 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 81 | Churn Cumulative: 1846 | Contributors (this commit): 5 | Commits (past 90d): 27 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -72,14 +72,14 @@ class ScrapyAgent(object):\n \n         start_time = time()\n         d = agent.request(method, url, headers, bodyproducer)\n-        # check download timeout\n-        self._timeout_cl = reactor.callLater(timeout, d.cancel)\n-        d.addBoth(self._cb_timeout, request, url, timeout)\n         # set download latency\n         d.addCallback(self._cb_latency, request, start_time)\n         # response body is ready to be consumed\n         d.addCallback(self._cb_bodyready, request)\n         d.addCallback(self._cb_bodydone, request, url)\n+        # check download timeout\n+        d.addBoth(self._cb_timeout, request, url, timeout)\n+        self._timeout_cl = reactor.callLater(timeout, d.cancel)\n         return d\n \n     def _cb_timeout(self, result, request, url, timeout):\n@@ -97,9 +97,12 @@ class ScrapyAgent(object):\n         if txresponse.length == 0:\n             return txresponse, '', None\n \n-        finished = defer.Deferred()\n-        txresponse.deliverBody(_ResponseReader(finished, txresponse, request))\n-        return finished\n+        def _cancel(_):\n+            txresponse._transport._producer.loseConnection()\n+\n+        d = defer.Deferred(_cancel)\n+        txresponse.deliverBody(_ResponseReader(d, txresponse, request))\n+        return d\n \n     def _cb_bodydone(self, result, request, url):\n         txresponse, body, flags = result\n@@ -139,6 +142,8 @@ class _ResponseReader(protocol.Protocol):\n         self._bodybuf.write(bodyBytes)\n \n     def connectionLost(self, reason):\n+        if self._finished.called:\n+            return\n         body = self._bodybuf.getvalue()\n         if reason.check(ResponseDone):\n             self._finished.callback((self._txresponse, body, None))\n\n@@ -12,6 +12,7 @@ def getarg(request, name, default=None, type=str):\n     else:\n         return default\n \n+\n class Follow(Resource):\n \n     isLeaf = True\n@@ -35,20 +36,37 @@ class Follow(Resource):\n         s += \"\"\"</body>\"\"\"\n         return s\n \n-class Delay(Resource):\n+\n+class DeferMixin(Resource):\n+\n+    def deferRequest(self, request, delay, f, *a, **kw):\n+        def _cancelrequest(_):\n+            # silence CancelledError\n+            d.addErrback(lambda _: None)\n+            d.cancel()\n+        d = deferLater(reactor, delay, f, *a, **kw)\n+        request.notifyFinish().addErrback(_cancelrequest)\n+        return d\n+\n+\n+class Delay(DeferMixin, Resource):\n \n     isLeaf = True\n \n     def render_GET(self, request):\n         n = getarg(request, \"n\", 1, type=float)\n-        d = deferLater(reactor, n, lambda: (request, n))\n-        d.addCallback(self._delayedRender)\n+        b = getarg(request, \"b\", 1, type=int)\n+        if b:\n+            # send headers now and delay body\n+            request.write('')\n+        self.deferRequest(request, n, self._delayedRender, request, n)\n         return NOT_DONE_YET\n \n-    def _delayedRender(self, (request, n)):\n+    def _delayedRender(self, request, n):\n         request.write(\"Response delayed for %0.3f seconds\\n\" % n)\n         request.finish()\n \n+\n class Status(Resource):\n \n     isLeaf = True\n@@ -58,20 +76,21 @@ class Status(Resource):\n         request.setResponseCode(n)\n         return \"\"\n \n-class Partial(Resource):\n+\n+class Partial(DeferMixin, Resource):\n \n     isLeaf = True\n \n     def render_GET(self, request):\n         request.setHeader(\"Content-Length\", \"1024\")\n-        d = deferLater(reactor, 0, lambda: request)\n-        d.addCallback(self._delayedRender)\n+        self.deferRequest(request, 0, self._delayedRender, request)\n         return NOT_DONE_YET\n \n     def _delayedRender(self, request):\n         request.write(\"partial content\\n\")\n         request.finish()\n \n+\n class Drop(Partial):\n \n     def _delayedRender(self, request):\n@@ -79,6 +98,7 @@ class Drop(Partial):\n         request.channel.transport.loseConnection()\n         request.finish()\n \n+\n class Root(Resource):\n \n     def __init__(self):\n@@ -95,6 +115,7 @@ class Root(Resource):\n     def render(self, request):\n         return 'Scrapy mock HTTP server\\n'\n \n+\n class MockServer():\n \n     def __enter__(self):\n\n@@ -45,15 +45,16 @@ class DelaySpider(MetaSpider):\n \n     name = 'delay'\n \n-    def __init__(self, n=1, *args, **kwargs):\n+    def __init__(self, n=1, b=0, *args, **kwargs):\n         super(DelaySpider, self).__init__(*args, **kwargs)\n         self.n = n\n+        self.b = b\n         self.t1 = self.t2 = self.t2_err = 0\n \n     def start_requests(self):\n         self.t1 = time.time()\n-        yield Request(\"http://localhost:8998/delay?n=%s\" % self.n, \\\n-            callback=self.parse, errback=self.errback)\n+        url = \"http://localhost:8998/delay?n=%s&b=%s\" % (self.n, self.b)\n+        yield Request(url, callback=self.parse, errback=self.errback)\n \n     def parse(self, response):\n         self.t2 = time.time()\n\n@@ -51,6 +51,13 @@ class CrawlTestCase(TestCase):\n         self.assertTrue(spider.t2 == 0)\n         self.assertTrue(spider.t2_err > 0)\n         self.assertTrue(spider.t2_err > spider.t1)\n+        # server hangs after receiving response headers\n+        spider = DelaySpider(n=0.5, b=1)\n+        yield docrawl(spider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n+        self.assertTrue(spider.t1 > 0)\n+        self.assertTrue(spider.t2 == 0)\n+        self.assertTrue(spider.t2_err > 0)\n+        self.assertTrue(spider.t2_err > spider.t1)\n \n     @defer.inlineCallbacks\n     def test_retry_503(self):\n\n@@ -58,6 +58,7 @@ class HttpTestCase(unittest.TestCase):\n         r = static.File(name)\n         r.putChild(\"redirect\", util.Redirect(\"/file\"))\n         r.putChild(\"wait\", ForeverTakingResource())\n+        r.putChild(\"hang-after-headers\", ForeverTakingResource(write=True))\n         r.putChild(\"nolength\", NoLengthResource())\n         r.putChild(\"host\", HostHeaderResource())\n         r.putChild(\"payload\", PayloadResource())\n@@ -106,10 +107,18 @@ class HttpTestCase(unittest.TestCase):\n         d.addCallback(self.assertEquals, 302)\n         return d\n \n+    @defer.inlineCallbacks\n     def test_timeout_download_from_spider(self):\n-        request = Request(self.getURL('wait'), meta=dict(download_timeout=0.1))\n-        d = self.download_request(request, BaseSpider('foo'))\n-        return self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n+        spider = BaseSpider('foo')\n+        meta = {'download_timeout': 0.2}\n+        # client connects but no data is received\n+        request = Request(self.getURL('wait'), meta=meta)\n+        d = self.download_request(request, spider)\n+        yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n+        # client connects, server send headers and some body bytes but hangs\n+        request = Request(self.getURL('hang-after-headers'), meta=meta)\n+        d = self.download_request(request, spider)\n+        yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n \n     def test_host_header_not_in_request_headers(self):\n         def _test(response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7a4c012db498cc763f33e3180a36c3cfd4e09d7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 127 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -6,13 +6,15 @@ from shutil import rmtree\n from twisted.trial import unittest\n \n \n+skip = False\n try:\n     from PIL import Image\n except ImportError, e:\n-    skip = True\n+    skip = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n else:\n     encoders = set(('jpeg_encoder', 'jpeg_decoder'))\n-    skip = not encoders.issubset(set(Image.core.__dict__))\n+    if not encoders.issubset(set(Image.core.__dict__)):\n+        skip = 'Missing JPEG encoders'\n \n def _mocked_download_func(request, info):\n     response = request.meta.get('response')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7af94be3a9e635cc2e9f305b0f45e82063b2f90c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 92 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -15,7 +15,7 @@ class _v19_S3Connection(S3Connection):\n \n class _v20_S3Connection(S3Connection):\n     \"\"\"A dummy S3Connection wrapper that doesn't do any syncronous download\"\"\"\n-    def _mexe(self, http_request, *args):\n+    def _mexe(self, http_request, *args, **kwargs):\n         http_request.authorize(connection=self)\n         return http_request.headers\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22da89b7488f613f0aee779a2813266365a54206", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 759 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -78,8 +78,8 @@ class ScrapyAgent(object):\n         d.addCallback(self._cb_bodyready, request)\n         d.addCallback(self._cb_bodydone, request, url)\n         # check download timeout\n-        d.addBoth(self._cb_timeout, request, url, timeout)\n         self._timeout_cl = reactor.callLater(timeout, d.cancel)\n+        d.addBoth(self._cb_timeout, request, url, timeout)\n         return d\n \n     def _cb_timeout(self, result, request, url, timeout):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e06836ff485fa1ce2939d8ea446519917bfa24bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 96 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from scrapy import optional_features\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n-from .http import HttpDownloadHandler\n+from .http import HTTPDownloadHandler\n \n try:\n     from boto.s3.connection import S3Connection\n@@ -30,7 +30,7 @@ else:\n class S3DownloadHandler(object):\n \n     def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \\\n-            httpdownloadhandler=HttpDownloadHandler):\n+            httpdownloadhandler=HTTPDownloadHandler):\n         if 'boto' not in optional_features:\n             raise NotConfigured(\"missing boto library\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bef8ade95639ef30fb335e7f7fc9cd71cdc7b233", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 3255 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -174,6 +174,8 @@ class ExecutionEngine(object):\n         self.slots[spider].nextcall.schedule()\n \n     def schedule(self, request, spider):\n+        self.signals.send_catch_log(signal=signals.request_scheduled,\n+                request=request, spider=spider)\n         return self.slots[spider].scheduler.enqueue_request(request)\n \n     def download(self, request, spider):\n\n@@ -162,8 +162,6 @@ class Scraper(object):\n         from the given spider\n         \"\"\"\n         if isinstance(output, Request):\n-            self.signals.send_catch_log(signal=signals.request_received, request=output, \\\n-                spider=spider)\n             self.crawler.engine.crawl(request=output, spider=spider)\n         elif isinstance(output, BaseItem):\n             self.slot.itemproc_size += 1\n\n@@ -11,14 +11,17 @@ spider_opened = object()\n spider_idle = object()\n spider_closed = object()\n spider_error = object()\n-request_received = object()\n+request_scheduled = object()\n response_received = object()\n response_downloaded = object()\n item_scraped = object()\n item_dropped = object()\n \n+# for backwards compatibility\n stats_spider_opened = spider_opened\n stats_spider_closing = spider_closed\n stats_spider_closed = spider_closed\n \n-item_passed = item_scraped # for backwards compatibility\n+item_passed = item_scraped\n+\n+request_received = request_scheduled\n\n@@ -95,7 +95,7 @@ class CrawlerRun(object):\n         self.crawler.install()\n         self.crawler.configure()\n         self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n-        self.crawler.signals.connect(self.request_received, signals.request_received)\n+        self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n         self.crawler.signals.connect(self.response_downloaded, signals.response_downloaded)\n         self.crawler.crawl(self.spider)\n         self.crawler.start()\n@@ -122,7 +122,7 @@ class CrawlerRun(object):\n     def item_scraped(self, item, spider, response):\n         self.itemresp.append((item, response))\n \n-    def request_received(self, request, spider):\n+    def request_scheduled(self, request, spider):\n         self.reqplug.append((request, spider))\n \n     def response_downloaded(self, response, spider):\n@@ -143,7 +143,7 @@ class EngineTest(unittest.TestCase):\n         self.run = CrawlerRun()\n         yield self.run.run()\n         self._assert_visited_urls()\n-        self._assert_received_requests()\n+        self._assert_scheduled_requests()\n         self._assert_downloaded_responses()\n         self._assert_scraped_items()\n         self._assert_signals_catched()\n@@ -155,9 +155,8 @@ class EngineTest(unittest.TestCase):\n         urls_expected = set([self.run.geturl(p) for p in must_be_visited])\n         assert urls_expected <= urls_visited, \"URLs not visited: %s\" % list(urls_expected - urls_visited)\n \n-    def _assert_received_requests(self):\n-        # 3 requests should be received from the spider. start_urls and redirects don't count\n-        self.assertEqual(3, len(self.run.reqplug))\n+    def _assert_scheduled_requests(self):\n+        self.assertEqual(6, len(self.run.reqplug))\n \n         paths_expected = ['/item999.html', '/item2.html', '/item1.html']\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c70f050bdaeec6d2c32039c005511a32d3b21cc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 22 | Churn Cumulative: 140 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,7 @@ class SpiderManager(object):\n     @classmethod\n     def from_crawler(cls, crawler):\n         sm = cls.from_settings(crawler.settings)\n+        sm.crawler = crawler\n         crawler.signals.connect(sm.close_spider, signals.spider_closed)\n         return sm\n \n@@ -41,6 +42,9 @@ class SpiderManager(object):\n             spcls = self._spiders[spider_name]\n         except KeyError:\n             raise KeyError(\"Spider not found: %s\" % spider_name)\n+        if hasattr(self, 'crawler') and hasattr(spcls, 'from_crawler'):\n+            return spcls.from_crawler(self.crawler, **spider_kwargs)\n+        else:\n             return spcls(**spider_kwargs)\n \n     def find_by_request(self, request):\n\n@@ -1,6 +1,5 @@\n import sys\n import os\n-import weakref\n import shutil\n \n from zope.interface.verify import verifyObject\n@@ -9,7 +8,6 @@ from twisted.trial import unittest\n \n # ugly hack to avoid cyclic imports of scrapy.spider when running this test\n # alone\n-import scrapy.spider\n from scrapy.interfaces import ISpiderManager\n from scrapy.spidermanager import SpiderManager\n from scrapy.http import Request\n@@ -36,7 +34,7 @@ class SpiderManagerTest(unittest.TestCase):\n \n     def test_list(self):\n         self.assertEqual(set(self.spiderman.list()),\n-            set(['spider1', 'spider2', 'spider3']))\n+            set(['spider1', 'spider2', 'spider3', 'spider4']))\n \n     def test_create(self):\n         spider1 = self.spiderman.create(\"spider1\")\n@@ -66,3 +64,7 @@ class SpiderManagerTest(unittest.TestCase):\n     def test_load_base_spider(self):\n         self.spiderman = SpiderManager(['scrapy.tests.test_spidermanager.test_spiders.spider0'])\n         assert len(self.spiderman._spiders) == 0\n+\n+    def test_load_from_crawler(self):\n+        spider = self.spiderman.create('spider4', a='OK')\n+        self.assertEqual(spider.a, 'OK')\n\n@@ -0,0 +1,10 @@\n+from scrapy.spider import BaseSpider\n+\n+class Spider4(BaseSpider):\n+    name = \"spider4\"\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, **kwargs):\n+        o = cls(**kwargs)\n+        o.crawler = crawler\n+        return o\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1af47165b4ffaabd1dc9eb41603c98a705c0bafa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 190 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 5 | Methods Changed: 20 | Complexity Δ (Sum/Max): 29/16 | Churn Δ: 190 | Churn Cumulative: 900 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,104 @@\n+\"\"\"\n+An asynchronous FTP file download handler for scrapy which somehow emulates an http response.\n+\n+FTP connection parameters are passed using the request meta field:\n+- ftp_user (required)\n+- ftp_password (required)\n+- ftp_passive (by default, enabled) sets FTP connection passive mode\n+- ftp_local_filename\n+        - If not given, file data will come in the response.body, as a normal scrapy Response,\n+        which will imply that the entire file will be on memory.\n+        - if given, file data will be saved in a local file with the given name\n+        This helps when downloading very big files to avoid memory issues. In addition, for\n+        convenience the local file name will also be given in the response body.\n+\n+The status of the built html response will be, by default\n+- 200 in case of success\n+- 404 in case specified file was not found in the server (ftp code 550)\n+\n+or raise corresponding ftp exception otherwise\n+\n+The matching from server ftp command return codes to html response codes is defined in the\n+CODE_MAPPING attribute of the handler class. The key 'default' is used for any code\n+that is not explicitly present among the map keys. You may need to overwrite this\n+mapping if want a different behaviour than default.\n+\n+In case of status 200 request, response.headers will come with two keys:\n+    'Local Filename' - with the value of the local filename if given\n+    'Size' - with size of the downloaded data\n+\"\"\"\n+\n+import re\n+from urlparse import urlparse\n+from cStringIO import StringIO\n+\n+from twisted.internet import reactor\n+from twisted.protocols.ftp import FTPClient, CommandFailed\n+from twisted.internet.protocol import Protocol, ClientCreator\n+\n+from scrapy.http import Response\n+from scrapy.responsetypes import responsetypes\n+\n+class ReceivedDataProtocol(Protocol):\n+    def __init__(self, filename=None):\n+        self.__filename = filename\n+        self.body = open(filename, \"w\") if filename else StringIO()\n+        self.size = 0\n+\n+    def dataReceived(self, data):\n+        self.body.write(data)\n+        self.size += len(data)\n+\n+    @property\n+    def filename(self):\n+        return self.__filename\n+\n+    def close(self):\n+        self.body.close() if self.filename else self.body.reset()\n+\n+_CODE_RE = re.compile(\"\\d+\")\n+class FTPDownloadHandler(object):\n+\n+    CODE_MAPPING = {\n+        \"550\": 404,\n+        \"default\": 503,\n+    }\n+\n+    def __init__(self, setting):\n+        pass\n+\n+    def download_request(self, request, spider):\n+        parsed_url = urlparse(request.url)\n+        creator = ClientCreator(reactor, FTPClient, request.meta[\"ftp_user\"],\n+                                    request.meta[\"ftp_password\"],\n+                                    passive=request.meta.get(\"ftp_passive\", 1))\n+        return creator.connectTCP(parsed_url.hostname, parsed_url.port or 21).addCallback(self.gotClient,\n+                                request, parsed_url.path)\n+\n+    def gotClient(self, client, request, filepath):\n+        self.client = client\n+        protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n+        return client.retrieveFile(filepath, protocol)\\\n+                .addCallbacks(callback=self._build_response,\n+                        callbackArgs=(request, protocol),\n+                        errback=self._failed,\n+                        errbackArgs=(request,))\n+    \n+    def _build_response(self, result, request, protocol):\n+        self.result = result\n+        respcls = responsetypes.from_args(url=request.url)\n+        protocol.close()\n+        body = protocol.filename or protocol.body.read()\n+        headers = {\"local filename\": protocol.filename or '', \"size\": protocol.size}\n+        return respcls(url=request.url, status=200, body=body, headers=headers)\n+\n+    def _failed(self, result, request):\n+        message = result.getErrorMessage()\n+        if result.type == CommandFailed:\n+            m = _CODE_RE.search(message)\n+            if m:\n+                ftpcode = m.group()\n+                httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[\"default\"])\n+                return Response(url=request.url, status=httpcode, body=message)\n+        raise result.type(result.value)\n+\n\n@@ -56,6 +56,7 @@ DOWNLOAD_HANDLERS_BASE = {\n     'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n     'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n     's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n+    'ftp': 'scrapy.core.downloader.handlers.ftp.FTPDownloadHandler',\n }\n \n DOWNLOAD_TIMEOUT = 180      # 3mins\n\n@@ -9,6 +9,9 @@ from twisted.web import server, static, util, resource\n from twisted.web.test.test_webclient import ForeverTakingResource, \\\n         NoLengthResource, HostHeaderResource, \\\n         PayloadResource, BrokenDownloadResource\n+from twisted.protocols.ftp import FTPRealm, FTPFactory\n+from twisted.cred import portal, checkers, credentials\n+from twisted.protocols.ftp import FTPClient, ConnectionLost \n from w3lib.url import path_to_file_uri\n \n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n@@ -16,6 +19,8 @@ from scrapy.core.downloader.handlers.http import HTTPDownloadHandler, HttpDownlo\n from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n+from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n+\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n from scrapy.settings import Settings\n@@ -326,3 +331,83 @@ class S3TestCase(unittest.TestCase):\n         httpreq = self.download_request(req, self.spider)\n         self.assertEqual(httpreq.headers['Authorization'], \\\n                 'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')\n+\n+class FTPTestCase(unittest.TestCase):\n+    \n+    username = \"scrapy\"\n+    password = \"passwd\"\n+\n+    def setUp(self):\n+        # setup dirs and test file\n+        self.directory = self.mktemp()\n+        os.mkdir(self.directory)\n+        userdir = os.path.join(self.directory, self.username)\n+        os.mkdir(userdir)\n+        FilePath(userdir).child('file.txt').setContent(\"I have the power!\")\n+\n+        # setup server\n+        realm = FTPRealm(anonymousRoot=self.directory, userHome=self.directory)\n+        p = portal.Portal(realm)\n+        users_checker = checkers.InMemoryUsernamePasswordDatabaseDontUse()\n+        users_checker.addUser(self.username, self.password)\n+        p.registerChecker(users_checker, credentials.IUsernamePassword)\n+        self.factory = FTPFactory(portal=p)\n+        self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n+        self.portNum = self.port.getHost().port\n+        self.download_handler = FTPDownloadHandler(Settings())\n+        self.addCleanup(self.port.stopListening)\n+\n+    def _add_test_callbacks(self, deferred, callback=None, errback=None):\n+        def _clean(data):\n+            self.download_handler.client.transport.loseConnection()\n+            return data\n+        deferred.addCallback(_clean)\n+        if callback:\n+            deferred.addCallback(callback)\n+        if errback:\n+            deferred.addErrback(errback)\n+        return deferred\n+\n+    def test_ftp_download_success(self):\n+        request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n+                meta={\"ftp_user\": self.username, \"ftp_password\": self.password})\n+        d = self.download_handler.download_request(request, None)\n+\n+        def _test(r):\n+            self.assertEqual(r.status, 200)\n+            self.assertEqual(r.body, 'I have the power!')\n+            self.assertEqual(r.headers, {'Local Filename': [''], 'Size': [17]})\n+        return self._add_test_callbacks(d, _test)\n+\n+    def test_ftp_download_notexist(self):\n+        request = Request(url=\"ftp://127.0.0.1:%s/notexist.txt\" % self.portNum,\n+                meta={\"ftp_user\": self.username, \"ftp_password\": self.password})\n+        d = self.download_handler.download_request(request, None)\n+\n+        def _test(r):\n+            self.assertEqual(r.status, 404)\n+        return self._add_test_callbacks(d, _test)\n+\n+    def test_ftp_local_filename(self):\n+        local_fname = \"/tmp/file.txt\"\n+        request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n+                meta={\"ftp_user\": self.username, \"ftp_password\": self.password, \"ftp_local_filename\": local_fname})\n+        d = self.download_handler.download_request(request, None)\n+\n+        def _test(r):\n+            self.assertEqual(r.body, local_fname)\n+            self.assertEqual(r.headers, {'Local Filename': ['/tmp/file.txt'], 'Size': [17]})\n+            self.assertTrue(os.path.exists(local_fname))\n+            with open(local_fname) as f:\n+                self.assertEqual(f.read(), \"I have the power!\")\n+            os.remove(local_fname)\n+        return self._add_test_callbacks(d, _test)\n+\n+    def test_invalid_credentials(self):\n+        request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n+                meta={\"ftp_user\": self.username, \"ftp_password\": 'invalid'})\n+        d = self.download_handler.download_request(request, None)\n+\n+        def _test(r):\n+            self.assertEqual(r.type, ConnectionLost)\n+        return self._add_test_callbacks(d, errback=_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
